[{"number": 48221, "title": "tf.linalg.eigh and tf.linalg.eigvalsh errors", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  CentOS 6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):  Source\r\n- TensorFlow version (use command below):  2.0.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nI apologize in advance, I'm working on a machine that is isolated from the internet so have to type everything in by hand.  My issue is that I'm attempting to use tf.linalg.eigh within a loss function and get errors on a fraction of the test data I'm using.  For most data it works, but about 1% fails.  The matrices it fails on work fine with numpy.linalg.eig and they are symmetric matrices.\r\n\r\nThe error looks like this:\r\n\r\n```\r\nInvalidArgumentError: Got info = 5251 for batch index 49, expected info = 0.  Debug_info = heevd\r\n  [[node SelfAdjointEigV2 (defined at .../lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]] [Op:__inference_dirtributed_function_1774378]\r\n\r\nFunction call stack:\r\ndistributed_function\r\n```", "comments": ["Please provide a concise reproducible code example to investigate the issue. Thanks!", "Also, please update TensorFlow to the latest stable version v2.4.1 and check if you are facing the same error. Thanks!", "I'm working on getting updated.  I'll report back after I do.  Thanks.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48221\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48221\">No</a>\n"]}, {"number": 48220, "title": "Model to estimator docs use deprecated code", "body": "The example code for converting a keras model to a tf estimator cause a user warning that set_learning_phase is deprecated and will be removed after 2020-10-11, which has since past. \r\n\r\n## URL(s) with the issue:\r\nhttps://www.tensorflow.org/guide/migrate#using_a_keras_model_definition\r\n\r\n## Description of issue (what needs changing):\r\nUser warning in the console log:\r\n\r\n```\r\n/tmpfs/src/tf_docs_env/lib/python3.6/site-packages/tensorflow/python/keras/backend.py:434: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\r\n```\r\n", "comments": ["@rmothukuru,\r\nI was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/55b13a19457068e5b0163f7613f4d12d/48220.ipynb). Thanks!", "Tested the tutorial with TF 2.5 version, the observed warning is no longer triggered. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48220\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48220\">No</a>\n"]}, {"number": 48219, "title": "Add \"input\" label mode to tf.keras.preprocessing.image_dataset_from_directory", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.4.0\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently, there is no way to get `tf.keras.preprocessing.image_dataset_from_directory` to create a dataset the labels of which are the images themselves, for use in autoencoders. The behavior would be the same as that of `tf.keras.preprocessing.image.ImageDataGenerator.flow_from_directory` with the argument `class_mode=\"input\"`. Perhaps the preexisting argument `label_mode` could be modified to include an `\"input\"` option.\r\n\r\n**Will this change the current api? How?**\r\nMinimal change to `tf.keras.preprocessing.image_dataset_from_directory` is required. Because of the nature of what is essentially a very small addition, it would be extremely unlikely to pose a compatibility issue.\r\n\r\n**Who will benefit with this feature?**\r\nUsers wishing to benefit from the advantages of the `tf.data` API to train their image data autoencoders.\r\n\r\n**Any Other info.**\r\n", "comments": ["@FTC55 Sorry for the late response. Interesting. \r\nI am not sure whether this feature is usable for any other applications or use cases. Do you think this will be useful for any other application other than autoencoders?\r\n\r\nAre you still interested in contributing? If yes, please feel free to open a PR in  [keras-team/keras repo.](https://github.com/keras-team/keras/issues) repository.\r\n\r\nPlease note that Keras development moved to keras-team/keras repository to focus on only keras. Thanks!\r\n", "@jvishnuvardhan Hi, I'm still interested in the implementation of this feature since I frequently work with autoencoders. However, mine was more of a suggestion as I lack the skill to carry out the implementation myself. I think the feature should be implemented for consistency with the rest of the API, as I said to match `tf.keras.preprocessing.image.ImageDataGenerator.flow_from_directory` so as to satisfy all the use cases that this feature already does here. I eventually had to circumvent this limitation for the project I was working on by abandoning `tf.data.Dataset` entirely and using multiple ImageDataGenerators which to me really hurts simplicity and ease of use.", "@FTC55 Would you mind opening it in keras-team/keras repo? Just need to copy and paste.\r\n\r\nIf you are busy, i can create the same feature request in that repo? Thanks ", "@jvishnuvardhan Sure thing. Just done it. Should I close here?", "\u90ae\u4ef6\u5df2\u6536\u5230\uff01\u8c22\u8c22\uff01"]}, {"number": 48218, "title": "Remove unneeded `file_io.file_exists` checks", "body": "`file_io.recursive_create_dir` correctly handles existing directories so this PR removes unnecessary `file_io.file_exists`. This can be beneficial when working with object storage where listing files or checking for the existence of a directory might be expensive.", "comments": ["@rthadur It has still the same behaviour https://github.com/tensorflow/tensorflow/pull/43754#issuecomment-704426064"]}, {"number": 48217, "title": "[tf.data] graduate experimental `group_by_window` API to tf.data.Dataset", "body": "UPDATED: This PR graduates the `tf.data.experimental.group_by_window` API into `tf.data.Dataset.group_by_window` by making the following changes:\r\n\r\n - [x] Adds the deprecation decorator for the experimental API.\r\n - [x] Adds the necessary `group_by_window` method to `DatasetV2` class.\r\n - [x] Updates example in documentation with new API.\r\n - [x] Regenerate golden API's.\r\n - [x] Moved and updated the `group_by_window` test target from experimental/kernel_tests to kernel_tests\r\n - [x] Updated the RELEASE.md file\r\n\r\nTEST LOG\r\n```\r\nINFO: Build completed successfully, 346 total actions\r\n//tensorflow/python/data/kernel_tests:group_by_window_test               PASSED in 4.8s\r\n  Stats over 4 runs: max = 4.8s, min = 3.0s, avg = 4.2s, dev = 0.7s\r\n```\r\n\r\nThis is a pre-requisite for https://github.com/tensorflow/tensorflow/pull/48110.\r\ncc: @jsimsa", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48217) for more info**.\n\n<!-- need_author_cla -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48217) for more info**.\n\n<!-- need_author_cla -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48217) for more info**.\n\n<!-- need_author_cla -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48217) for more info**.\n\n<!-- need_author_cla -->", "@jsimsa seems like the internal builds were failing due to an API call mismatch from `grouping.group_by_window` to `Dataset.group_by_window`. This commit addresses the issue.", "All the tests are passing now!", "Similarly to the `get_single_element` API, could we move the tests for `group_by_window` out of the experimental directory and update it to use the non-experimental API? Thanks.", "@jsimsa done! Updated the PR description as well.", "Similar to my `get_single_element` comment, please update https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md adding the following item there: \"promoting `tf.data.experimental.group_by_window` API to `tf.data.Dataset.group_by_window` and deprecating the experimental endpoint\"", "@kvignesh1420 Can you please check @jsimsa's comments and keep us posted ? Thanks!", "@gbaned as per the discussion in https://github.com/tensorflow/tensorflow/pull/48048, this PR will update the `RELEASE.md` file and conflict with https://github.com/tensorflow/tensorflow/pull/48048, thus I was waiting for https://github.com/tensorflow/tensorflow/pull/48048 to be merged first.", "@jsimsa the conflicts have been resolved and the RELEASE.md file has been updated as well!", "@jsimsa seems like there are internal failures, please let me know if anything needs to be fixed! ", "yes, working on resolving these manually myself but today is a busy day", "@jsimsa  Sure, np!\r\nAlso, I understand that the checks are internal to google but can you please let me know (whenever you find some time) where the failures occurred (if allowed by google's Business Conduct Guidelines/Policies)? This will help in future PRs for graduating other APIs. Thanks again.", "doctests are failing with:\r\n\r\n```\r\nFAIL: group_by_window (google3.third_party.tensorflow.python.data.ops.dataset_ops.DatasetV2)\r\nDoctest: google3.third_party.tensorflow.python.data.ops.dataset_ops.DatasetV2.group_by_window\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"<embedded stdlib>/doctest.py\", line 2199, in runTest\r\n    raise self.failureException(self.format_failure(new.getvalue()))\r\nAssertionError: Failed doctest test for google3.third_party.tensorflow.python.data.ops.dataset_ops.DatasetV2.group_by_window\r\n  File \"/build/work/56bb78db693a83f9c5641e125d68a438167d/google3/runfiles/google3/third_party/tensorflow/python/data/ops/dataset_ops.py\", line 2524, in group_by_window\r\n\r\n----------------------------------------------------------------------\r\nFile \"/build/work/56bb78db693a83f9c5641e125d68a438167d/google3/runfiles/google3/third_party/tensorflow/python/data/ops/dataset_ops.py\", line 2539, in google3.third_party.tensorflow.python.data.ops.dataset_ops.DatasetV2.group_by_window\r\nFailed example:\r\n    reduce_func = reduce_func=lambda key, dataset:\r\nException raised:\r\n    Traceback (most recent call last):\r\n      File \"<embedded stdlib>/doctest.py\", line 1330, in __run\r\n        compileflags, 1), test.globs)\r\n      File \"<doctest google3.third_party.tensorflow.python.data.ops.dataset_ops.DatasetV2.group_by_window[3]>\", line 1\r\n        reduce_func = reduce_func=lambda key, dataset:\r\n                                                     ^\r\n    SyntaxError: invalid syntax\r\n----------------------------------------------------------------------\r\nFile \"/build/work/56bb78db693a83f9c5641e125d68a438167d/google3/runfiles/google3/third_party/tensorflow/python/data/ops/dataset_ops.py\", line 2541, in google3.third_party.tensorflow.python.data.ops.dataset_ops.DatasetV2.group_by_window\r\nFailed example:\r\n    dataset = dataset.group_by_window(\r\n              key_func=key_func,\r\n              reduce_func=reduce_func,\r\n              window_size=window_size)\r\nException raised:\r\n    Traceback (most recent call last):\r\n      File \"<embedded stdlib>/doctest.py\", line 1330, in __run\r\n        compileflags, 1), test.globs)\r\n      File \"<doctest google3.third_party.tensorflow.python.data.ops.dataset_ops.DatasetV2.group_by_window[4]>\", line 3, in <module>\r\n        reduce_func=reduce_func,\r\n    NameError: name 'reduce_func' is not defined\r\n----------------------------------------------------------------------\r\nFile \"/build/work/56bb78db693a83f9c5641e125d68a438167d/google3/runfiles/google3/third_party/tensorflow/python/data/ops/dataset_ops.py\", line 2545, in google3.third_party.tensorflow.python.data.ops.dataset_ops.DatasetV2.group_by_window\r\nFailed example:\r\n    for elem in dataset.as_numpy_iterator():\r\n      print(elem)\r\nExpected:\r\n    [0 2 4 6 8]\r\n    [1 3 5 7 9]\r\nGot:\r\n    0\r\n    1\r\n    2\r\n    3\r\n    4\r\n    5\r\n    6\r\n    7\r\n    8\r\n    9\r\n```", "@jsimsa fixed the docstring issue and resolved conflicts.", "Seems auto-merge is not happening but the changes are now committed so we can close this. Thank you for the PR."]}, {"number": 48216, "title": "slim", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["@xiongli520,\r\nIn order to expedite the trouble-shooting process, could you please explain in detail the issue you are facing and also fill in the issue template. Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 48215, "title": "Update _index.yaml", "body": "Updated broken-link to mlir repo.\r\n\r\nFixes github issue #48189", "comments": ["@bhack \r\nThank you for the suggestion, the mentioned two files have been updated."]}, {"number": 48214, "title": "[file systems] fix the fall back to legacy implementation", "body": "This PR is a follow up of https://github.com/tensorflow/tensorflow/commit/aa35d4755409474f6619c68af1bfca168db24518.\r\n\r\nThe switch to plugin-based file systems was a bit unintuitive as per the current implementation.\r\nAs per the understanding that the env var `TF_USE_MODULAR_FILESYSTEM` is unset by default, the users will be using the legacy implementation of fs (i.e we ensure backward compatibility). However, if the env var is set then it is inferred that they want to use the plugins based modular fs from tfio. This PR addresses this confusion by simplifying the condition block.\r\n\r\ncc: @mihaimaruseac @yongtang  @vnvo2409   please let me know if there is some confusion with this approach. A PR https://github.com/tensorflow/io/pull/1348 has been raised in tfio to make the necessary changes.", "comments": ["addressed by https://github.com/tensorflow/tensorflow/commit/f8aee66b8c227da41273a8d3cef558d9dcf431de", "Sorry, haven't seen this in time. apologies"]}, {"number": 48211, "title": "kindly help", "body": "Epoch 1/20\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-107-c74f48714daf> in <module>()\r\n      2 # history = model.fit_generator(datagen.flow(train_x,train_y, batch_size=batch_size),\r\n      3 #                               epochs = epochs, validation_data = (val_x,val_y), steps_per_epoch=train_x.shape[0] // batch_size)\r\n----> 4 history = model.fit(train_x, train_y, epochs=20, batch_size=128, validation_split=0.1)\r\n\r\n9 frames\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n    975           except Exception as e:  # pylint:disable=broad-except\r\n    976             if hasattr(e, \"ag_error_metadata\"):\r\n--> 977               raise e.ag_error_metadata.to_exception(e)\r\n    978             else:\r\n    979               raise\r\n\r\nValueError: in user code:\r\n\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\r\n        return step_function(self, iterator)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\r\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\r\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\r\n        return self._call_for_each_replica(fn, args, kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\r\n        return fn(*args, **kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\r\n        outputs = model.train_step(data)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:754 train_step\r\n        y_pred = self(x, training=True)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:998 __call__\r\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/input_spec.py:259 assert_input_compatibility\r\n        ' but received input with shape ' + display_shape(x.shape))\r\n\r\n\r\n    ValueError: Input 0 of layer sequential_13 is incompatible with the layer: expected axis -1 of input shape to have value 1 but received input with shape (None, 28, 28, 3)", "comments": ["Please share code to reproduce the issue and share your platform details.", "@augustinemune,\r\nIn order to reproduce the issue reported here, could you please provide the TensorFlow version, the complete code and the dataset you are using. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 48209, "title": "Documentation for Time Distributed Layer", "body": "Documentation for ```Time Distributed layer``` in Keras mentions any ```Layer``` applied with it will be ```applied to every temporal slice``` \r\n\r\nBut in my assumption - for example consider ```mant-to-many``` sequence model trained for ```NER``` or ```Language Model``` with the following code, ```(if return_sequences==True)``` in the previous ```BiLSTM layer``` then ```Dense(n)``` and ```TimeDistributed(Dense(n))```  are exactly the same and either of them can be used. Is my assumption correct? \r\n   \r\n```\r\nmodel = Sequential()\r\nmodel.add(Embedding(input_dim=voc_size, output_dim=embed_dim, input_length=50))\r\nmodel.add(Bidirectional(LSTM(units=lstm_units,return_sequences=True),\r\n                                         merge_mode=\"ave\"))\r\nmodel.add(TimeDistributed(Dense(n)))\r\n```\r\n    \r\n ", "comments": ["@vignesh0710 \r\nKindly open a [stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow) issue for this as it is not a bug or feature request, Please post this kind of support questions at Stackoverflow. There is a big community to support and learn from your questions.\r\nThanks!", "Hi, Thanks for the response. My thinking was in case the assumption is correct, the docs can be modified to assert that as I see so many posts about this very confusion. ", "Created [a CL](https://critique-ng.corp.google.com/cl/367437963) with this change. Thanks!", "@rmothukuru can you please confirm if the initially mentioned ```assertion``` is correct or not?", "@vignesh0710,\r\nYes, the assertion is correct. Please find [this comment from Francois](https://github.com/keras-team/keras/issues/1029#issuecomment-157786333). Thanks!", "@rmothukuru thanks. Can I close this issue thread then?\r\n", "@vignesh0710,\r\nNo, please. The issue will be closed automatically once the CL is successfully submitted. ", "@vignesh0710,\r\n\r\nI've taken a look at this CL posted and Francois has mentioned that the CL should be reverted as documentation already says this,\r\n```\r\nYou can then use `TimeDistributed` to apply the same `Conv2D` layer to each\r\n  of the 10 timesteps, independently:\r\n\r\n  >>> inputs = tf.keras.Input(shape=(10, 128, 128, 3))\r\n  >>> conv_2d_layer = tf.keras.layers.Conv2D(64, (3, 3))\r\n  >>> outputs = tf.keras.layers.TimeDistributed(conv_2d_layer)(inputs)\r\n  >>> outputs.shape\r\n  TensorShape([None, 10, 126, 126, 64])\r\n```\r\n\r\nCan you confirm if we are good to close this issue? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Please go ahead and close.\r\n"]}, {"number": 48208, "title": "Fix kernel_l2_pool_2d_test for the Xtensa target", "body": " * Changed the comparison to use 1e-5. We should not be comparing floats with tolerance = 0.\r\n * manually confirmed that the following command now passes:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade test_kernel_l2_pool_2d_test -j8\r\n```\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 48207, "title": "fix lambda dynamic shape inference", "body": "Fix #44906", "comments": ["Have you tested this also with https://github.com/tensorflow/tensorflow/issues/44906#issuecomment-729868988?", "> Have you tested this also with [#44906 (comment)](https://github.com/tensorflow/tensorflow/issues/44906#issuecomment-729868988)?\r\n\r\nIt's another bug which is related to #47816, this PR fix RecusionError bug.", "According Lambda's document, I found the layer's behavior is similar to ```tf.py_function```, does the layer need to be refactored with ```tf.py_function```?\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/Lambda?version=nightly", "I'm seeing an infinite recursion error for `tensorflow/python/keras/layers:core_test`.", "Rerun test cases, I will check it.", "I'm still seeing an infinite recursion error, at this line:\r\n\r\n```\r\nreturn super(Lambda, self).compute_output_shape(input_shape)\r\n  File \".../tensorflow/python/keras/engine/base_layer_v1.py\", line 570, in compute_output_shape\r\n```\r\n\r\nTest: `tensorflow/python/keras/layers:core_test`", "> I'm still seeing an infinite recursion error, at this line:\r\n> \r\n> ```\r\n> return super(Lambda, self).compute_output_shape(input_shape)\r\n>   File \".../tensorflow/python/keras/engine/base_layer_v1.py\", line 570, in compute_output_shape\r\n> ```\r\n> \r\n> Test: `tensorflow/python/keras/layers:core_test`\r\n\r\nI fix keras v1 too.", "@fsx950223 Can you please address Ubuntu Sanity errors? Thanks!"]}, {"number": 48206, "title": "transpose op is deleted when i use tf.lite.TFLiteConverter.from_keras_model to convert tflite model.", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (Linux Ubuntu 16.04):\r\n- TensorFlow installation (pip package):\r\n- TensorFlow library (2.0.0-rc0):\r\n\r\n### 2. Code\r\n...\r\n    output = tf.transpose(output, [0, 3, 1, 2])\r\n    output_shape = output.shape\r\n    output = Reshape([output_shape[1] * output_shape[3], output_shape[2]])(tf.transpose(output, [0, 1, 3, 2]))\r\n    output = tf.transpose(output, [0, 2, 1]) **# if output shape is [1, 64, 1], the transpose op is deleted, when i convert keras to tflite. If the output shape is [1, 64, 2], the transpose op is reserved.**\r\n    bn_w = state_dict[bn_name+'.weight'].numpy()\r\n    bn_b = state_dict[bn_name+'.bias'].numpy()\r\n    bn_mean = state_dict[bn_name+'.running_mean'].numpy()\r\n    bn_var = state_dict[bn_name+'.running_var'].numpy()\r\n    output = tf.add(tf.multiply(\r\n        tf.divide(tf.subtract(output, bn_mean), tf.sqrt(bn_var+1e-05)), bn_w), bn_b)\r\n    output = tf.transpose(output, [0, 2, 1])\r\n    output = tf.transpose(Reshape([output_shape[1], output_shape[3],  output_shape[2]])(output), [0, 1, 3, 2])\r\n\r\n...\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n\r\n### 3. Failure after conversion\r\nThe conversion is successful, but the generated model is wrong:\r\n- transpose op is deleted in tflite model, which leads to the dimension mismatch after the transpose.  \r\n\r\n", "comments": ["@fhahaha could you provide minimal, reproducible steps as a format of gist to reproduce your problem at our end?", "Could you try your conversion with the recent TF version? There is possibility that the issue has been fixed in the recent TF version already.", "@abattery Thanks, the issue has been fixed in recent TF version 2.4.1. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48206\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48206\">No</a>\n", "Thanks @fhahaha for the confirmation. Closed this issue since the recent TF version can resolve this issue."]}, {"number": 48203, "title": "ifdef out newly added depthwise_conv test for hifimini.", "body": "See http://b/184087246 for additional context.\r\n\r\nManually confirmed that hifimimini tests pass with:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=hifimini XTENSA_CORE=mini1m1m_RG test -j8\r\n```", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 48201, "title": "Fix Roadmap description", "body": "The Roadmap is only related to the  model optimization ", "comments": []}, {"number": 48200, "title": "There is no Digital Object Identifiers (DOI) to make the code citable for academic references", "body": "## URL(s) with the issue:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/README.md\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe source code should be made citable for academic references: https://guides.github.com/activities/citable-code/\r\n\r\n### Clear description\r\n\r\n_\"A DOI, or Digital Object Identifier, is a string of numbers, letters and symbols used to permanently identify an article or document and link to it on the web. A DOI will help your reader easily locate a document from your citation.\"_  - [source](https://library.uic.edu/help/article/1966/what-is-a-doi-and-how-do-i-use-them-in-citations).\r\n\r\n### Usage example\r\n\r\nSee GitHub instructions on \"making your code citable\":  https://guides.github.com/activities/citable-code/\r\n", "comments": ["https://www.tensorflow.org/about/bib could help", "@mihaimaruseac it's good but it doesn't allow us to cite specific versions of the framework so that the published paper ages well in terms of reproducibility.", "Looking for an owner from the product team for this.", "We can use another specific entry for the library: https://ctan.org/tex-archive/macros/latex/contrib/biblatex-contrib/biblatex-software", "@bhack that wouldn't produce a DOI, though.", "If you want an automated DOI we could explore https://guides.github.com/activities/citable-code/\n\n/cc @MarkDaoust @lamberta ", "@bhack, yes. That's what's in the description of this issue.", "FWIW, the Google quantum projects have started doing this with Zenodo: https://quantumai.google/cirq/citation\r\nBut, yes, currently we point to the [TensorFlow white paper](https://www.tensorflow.org/about/bib) for the citation.\r\ncc: @theadactyl @joanafilipa", "cc / @iamtimdavis from product\r\n\r\n", "@angerson implemented a solution for this and we should have a DOI with the 2.5 release.", "> @angerson implemented a solution for this and we should have a DOI with the 2.5 release.\r\n\r\n@mihaimaruseac @angerson that's great to hear! Looking forward to it.", "TensorFlow now has a DOI page set up with Zenodo: https://doi.org/10.5281/zenodo.4724125\r\n\r\nI added a badge on the README and made a change that should appear on tensorflow.org/about/bib in the next few days.", "@angerson Great! Thank you to everyone who made it happen. I look forward to citing the TensorFlow DOI on my next publication."]}, {"number": 48199, "title": "TFLite ERROR: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Xiaomi Redmi Note 7\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.5.0-dev20210322\r\n- Python version: 3.8\r\n\r\n**Describe the current behavior**\r\nWhen I am running the TFLite benchmark on my phone with my model I get the following error `ERROR: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.` when I try to run on GPU although every tensor are in static size.\r\n\r\n**Describe the expected behavior**\r\nI took care of having static-sized tensors everywhere so I expect to be able to run the model fully on GPU.\r\n\r\n**Standalone code to reproduce the issue**\r\nThe network can be downloaded [removed].\r\n\r\nI run the benchmark which can be downloaded from Tensorflow [here](https://storage.googleapis.com/tensorflow-nightly-public/prod/tensorflow/release/lite/tools/nightly/latest/android_arm_benchmark_model).\r\nI run it with the following commands\r\n```\r\nadb push android_arm_benchmark_model /data/local/tmp/benchmark\r\nadb shell chmod +x /data/local/tmp/benchmark\r\nadb push model.tflite /data/local/tmp/model.tflite\r\nadb shell \"/data/local/tmp/benchmark\"  --graph=\"/data/local/tmp/model.tflite\" --input_layer=input --input_layer_shape=1,360,640,3 --use_gpu=true\r\n```\r\n\r\nRegarding conversion, I am converting my model using this script:\r\n```\r\nmodel = load_model() # I would rather not share this\r\n\r\n# Create dataset\r\ndef get_func():\r\n    return lambda obj: func(obj)\r\n\r\ndef func(obj, y=640, x=360):\r\n    image = obj[\"image\"]\r\n    shape = tf.shape(image)\r\n    height, width = shape[0], shape[1]\r\n    ratio_y = y / height\r\n    ratio_x = x / width\r\n    image = tf.image.resize(image, (y, x))\r\n    scale = tf.convert_to_tensor([ratio_y, ratio_x, ratio_y, ratio_x], dtype=tf.float32)\r\n    return {\"Input\": image, \"Scale_Input\": scale}\r\n\r\ndataset = tfds.load(\r\n    name=\"coco/2017\",\r\n    split=\"train\",\r\n)\r\ndataset = dataset.map(get_func())\r\n\r\n# Transform the dataset into a representative dataset as in the TF guide\r\ndef representative_dataset_generator():\r\n    for input_value in dataset.batch(1).take(10):\r\n        yield [input_value[\"Input\"], input_value[\"Scale_Input\"]]\r\n\r\n\r\n# Converter\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\r\n# Set the representative dataset in order to quantize the activations\r\nconverter.representative_dataset = representative_data_gen\r\n\r\n# Ensure that if any ops can't be quantized, the converter throws an error\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.target_spec.supported_types = [tf.int8]\r\n\r\n# Set the input and output tensors to uint8 (APIs added in r2.3)\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\n\r\n# Additional tricks\r\nconverter.experimental_new_converter = True\r\nconverter.experimental_new_quantizer = True\r\nconverter.target_spec.supported_ops = [\r\n    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.\r\n]\r\n\r\ntf_lite_quant_model = converter.convert()\r\n\r\n# saving converted model in TFLite file\r\nwith open(\"model.tflite\", \"wb\") as tf_file:\r\n    tf_file.write(tf_lite_quant_model)\r\n```\r\n", "comments": ["Tensor index can be easily verified if you add a print statement in the `HasDynamicTensorImpl` function...", "I am sorry but I do not find any documentation about this function. Could you please tell me where it is defined/how can I use it?", "The problem was that I indeed had a \"hidden\" dynamic-sized tensor.\r\nChanging from: \r\n```\r\nindices = tf.where(cond)\r\ncount = tf.shape(indices)[0]\r\n```\r\nto:\r\n```\r\ncount = tf.math.reduce_sum(tf.where(cond, 1, 0))\r\n```\r\nfixed the problem.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48199\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48199\">No</a>\n"]}, {"number": 48198, "title": "Remove the div kernel.", "body": "Manually confirmed that the Xtensa tests now pass with:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade test -j8\r\n```\r\n\r\nWorkaround for http://b/184059002\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 48197, "title": "@tf.function in TF2.x", "body": "Where is tf.function defined? \r\nWhere can I see its implementation?\r\nHow can it be invoked (as a decorator) when we compile the model using TF.Keras?", "comments": ["@Alok-Ranjan23 \r\n1) & 2) Please refer to [this link](https://www.tensorflow.org/api_docs/python/tf/function) for the function details.\r\n3) When you compile a keras model it is invoked internally until and unless you specify the run_eagerly argument to be true.\r\n\r\nFor any further queries on this kindly open a [stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow) issue for this as it is not a bug or feature request, Please post this kind of support questions at Stackoverflow. There is a big community to support and learn from your questions.\r\nThanks!"]}, {"number": 48196, "title": "Getting sub-layer output of a custom layer breaks in a Tensorflow 2.4.1", "body": "**System information**\r\n- Custom code:\r\n\r\n```\r\nclass MyCustomLayer(keras.layers.Layer):\r\n    def __init__(self, num_filters=64, kernel_size=3):\r\n        keras.layers.Layer.__init__(self)\r\n        self.conv_1 = keras.layers.Conv2D(filters=num_filters, kernel_size=kernel_size)\r\n        \r\n    def call(self, inputs):\r\n        return self.conv_1(inputs)\r\n\r\nx = keras.Input(shape=(None, None, 3))\r\nmy_custom_layer = MyCustomLayer()\r\ny = my_custom_layer(x)\r\n```\r\n\r\n- OS Platform and Distribution: Mac OS Big Sur v 11.2.2, Linux Ubuntu 20.04\r\n- TensorFlow installed from binary\r\n- TensorFlow version: code fails using TF 2.4.1, but it works using TF  2.3.1\r\n- Python version: 3.8\r\n\r\n**Describe the current behavior**\r\n\r\nThe following line works:\r\n\r\n```\r\nmy_custom_layer.output\r\n# Out: <KerasTensor: shape=(None, None, None, 64) dtype=float32 (created by layer 'my_custom_layer')>\r\n```\r\n\r\nThe following line breaks in Tensorflow 2.4.1:\r\n```\r\nmy_custom_layer.conv_1.output\r\n```\r\n\r\nHowever, it works in Tensorflow 2.3.1 without any problem.\r\n\r\n**Describe the expected behavior**\r\n\r\n`my_custom_layer.conv_1.output` should return the output tensor of the convolutional layer.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nLink to Colab: https://colab.research.google.com/drive/16a2IZfzrv4V0oOdxWp3ctt0VOj3wkyaI?usp=sharing\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-6-74d9fc3b4fbb> in <module>\r\n----> 1 my_custom_layer.conv_1.output\r\n\r\n~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py in output(self)\r\n   2152     \"\"\"\r\n   2153     if not self._inbound_nodes:\r\n-> 2154       raise AttributeError('Layer ' + self.name + ' has no inbound nodes.')\r\n   2155     return self._get_node_attribute_at_index(0, 'output_tensors', 'output')\r\n   2156 \r\n\r\nAttributeError: Layer conv2d has no inbound nodes.\r\n```\r\n\r\nHere there is the question that I initially raised on stackoverflow: https://stackoverflow.com/questions/66872434\r\n", "comments": ["@rmothukuru,\r\nI was able to run the code without any errors with [TF v2.3.2](https://colab.research.google.com/gist/amahendrakar/fd243f4375b1d4082dedbc19d6e2ae62/48196-2-3.ipynb). \r\n\r\nHowever with [TF v2.4.1](https://colab.research.google.com/gist/amahendrakar/b67fd59f860e8513b0f9286c058370ab/48196.ipynb#scrollTo=uZScYo7BRcNM) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/66c6be46befdc6e73073c1ab6c50199f/48196-tf-nightly.ipynb), I am facing an error stating `AttributeError: Layer conv2d has no inbound nodes.`. Please check the linked gist for reference. Thanks!", "Run into the exact same issue in #46605. Apparently it is consider as a *feature* and not a bug. However it makes impossible to access any layer's output once the model has been created. \r\n\r\nHere is the temporary *fix* I've found so far, you can disable the use of *keras_tensor* as follow:\r\n\r\n```python\r\nfrom tensorflow.python.keras.engine import keras_tensor\r\nkeras_tensor.disable_keras_tensors()\r\n\r\n#\u00a0.... Rest of your code\r\n```\r\n\r\nThen you can have access to inner component (*using your example*):\r\n\r\n```python\r\nmy_custom_layer.conv_1.output \r\n# <tf.Tensor 'my_custom_layer_2/conv2d_2/BiasAdd:0' shape=(None, None, None, 64) dtype=float32>\r\n```\r\n\r\n\r\n", "Any update on this issue, or we just have to live with it?", "There has been many changes have been made in the keras Layers behavior and so the error which you are facing. \r\nYou can follow [this](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer?version=nightly) document which explains the methods and the arguments which can be used in subclassed layer. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48196\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48196\">No</a>\n"]}, {"number": 48194, "title": "Fall back on xtensa reference when filter does not fit in input.", "body": "Xtensa currently does not support filter width > input width and filter height > input height.\r\n\r\nProgress towards b/183497550", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 48193, "title": "TOSA legalization updates for spec v0.22, Part 1", "body": "- Updated gather, gather_nd and resize.\r\n- Fix precision of TFL avgpool2d, quantize. Add squareddifference\r\n- Add left/right shift, leaky_relu, one_hot\r\n- Update relu6/relu_n1_to1\r\n- Numerical precision in tf.fakequant legalization\r\n\r\nSigned-off-by: Suraj Sudhir <suraj.sudhir@arm.com>\r\nChange-Id: Idf23c3b24342f75ee7d1eb22dab6ffe27e8710b1", "comments": ["@stellaraccident and @rsuderman , here are the part 1 set of updates to legalizations from TF/TFLite to TOSA aligned to the recent LLVM side update. That change was picked up by TensorFlow yesterday so we're pushing this out. ", "> @stellaraccident and @rsuderman , here are the part 1 set of updates to legalizations from TF/TFLite to TOSA aligned to the recent LLVM side update. That change was picked up by TensorFlow yesterday so we're pushing this out.\r\n\r\nShould be fine. We can revalidate internally when we land the CL.", "I agree the cross-pass design is bad, but that was the only way I thought I can achieve what I want.\r\n\r\nBriefly explain what I'm doing here:\r\nWe're trying to match the rounding behavior to TFL::AveragePool2DOp, which has different rounding behavior between QI8 and QU8 (since it's rounding on its storage type/range).\r\n\r\nIn the TFL to TOSA pass pipeline, the convert_tfl_uint8 pass is called first, and converts all the QU8 into QI8, so the pass after, e.g. legalize_tfl, only need to deal with QI8, but at the same time can't distinguish between those two cases.\r\n\r\nThe trick we played here (which is bad I agree) is to annotate the override_zeropoint attribute based on if it's QU8 or QI8 before QU8 is converted. When pass reaches legalize_tfl, it checks if such attribute exists. If it does, then we override that zeropoint with what's stored in the attribute, and we build TOSA::AveragePool2dOp with it.\r\n\r\nCould something you mentioned above solve the problem without using the cross-pass design?", "Again thank for all the feedbacks. They're all pretty helpful and we'll prepare next round of review as soon as possible.", "We've removed avgpool until the cross pass dependency is resolved, but have updated other things. We hope we can upstream this, and follow up the remaining pieces in a separate PR related to the remaining TOSA v0.22 changes still to follow. \r\n", "We're updating test_one_hot to follow the suggested approach. If that looks good, we'll follow up with a clean up of the entire set of tests in a follow up PR since it is an independent task. However we intend to add the gather tests to this PR, hopefully enabling us to close this one out. ", "Please let us know if we can update all the TF + TFL tests in a separate PR, using the suggested template of test_one_hot . Since this is largely independent of this PR and the ones to follow it, we can parallelize the test update work. ", "> We can land under the guarantee that the followup appears reasonably soon. We tend to avoid landing tests with brittle checks in case TF canonicalizations are updated. This can cause some unexpected failures.\r\n\r\nCurrent estimate is for the updated tests for both TF and TFL legalization to constitute a new PR within the next week. Is that reasonable ? ", "Found a set of changes required to fix some internal failures, mostly unused variable issues.", "Working on landing this internally patching the changes in myself.", "> Working on landing this internally patching the changes in myself.\r\n\r\nOops didn't see this. Thanks for handling this! "]}, {"number": 48192, "title": "Micro transpose op ported and tested for TFLM", "body": "Fixes #45695 \r\nFixes #43472\r\n\r\nAddition of TRANSPOSE operation and its relevant test file to TF Lite for Microcontrollers. This operation has been successfully tested in the following ways:\r\n\r\n1. Running `transpose_test` with Bazel\r\n2. Building TFLM for a target nRF52840 DK (Cortex-M) and verifying that the target error `Didn't find op for builtin opcode 'TRANSPOSE' version '2'` disappeared\r\n\r\n**More details abut point 1.**\r\nThe command used is:\r\n```\r\nbazel test //tensorflow/lite/micro/kernels:transpose_test --verbose_failures\r\n```\r\n\r\nIt returns:\r\n```\r\n.....\r\nRemoved for brevity\r\n.....\r\nINFO: Analyzed target //tensorflow/lite/micro/kernels:transpose_test (0 packages loaded, 0 targets configured).\r\nINFO: Found 1 test target...\r\nTarget //tensorflow/lite/micro/kernels:transpose_test up-to-date:\r\n  bazel-bin/tensorflow/lite/micro/kernels/transpose_test\r\nINFO: Elapsed time: 0.156s, Critical Path: 0.00s\r\nINFO: 1 process: 1 internal.\r\nINFO: Build completed successfully, 1 total action\r\n//tensorflow/lite/micro/kernels:transpose_test                  (cached) PASSED in 0.0s\r\n\r\nExecuted 0 out of 1 test: 1 test passes.\r\nINFO: Build completed successfully, 1 total action\r\n```\r\n\r\n**More details abut point 2.**\r\nI always build locally my version of TFLM with the command:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile \\\r\n    TARGET=cortex_m_generic \\\r\n    TARGET_ARCH=cortex-m4+fp \\\r\n    TARGET_TOOLCHAIN_ROOT=/opt/gcc-arm-none-eabi-9-2020-q2-update/bin/ \\\r\n    OPTIMIZED_KERNEL_DIR=cmsis_nn microlite\r\n```\r\n\r\nBefore applying the fixes of this PR, the error on the target nRF52840 DK was:\r\n```\r\n[ERR] ./model/debug_log.cc:12: Didn't find op for builtin opcode 'TRANSPOSE' version '2'. An older version of this builtin might be supported. Are you using an old TFLite binary with a newer model?\r\n[ERR] ./model/debug_log.cc:12: \r\n[ERR] ./model/debug_log.cc:12: Failed to get registration from op code TRANSPOSE\r\n[ERR] ./model/debug_log.cc:12: \r\n[ERR] ./model/debug_log.cc:12: Failed starting model allocation.\r\n[ERR] ./model/debug_log.cc:12: \r\n[ERR] ./model/debug_log.cc:12: AllocateTensors() failed\r\n```\r\n\r\nNow, after applying the fixes of this PR, the error disappears and the code can process further at runtime.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48192) for more info**.\n\n<!-- need_author_cla -->", "Hi, please note that commits [b0174fc](https://github.com/tensorflow/tensorflow/pull/48192/commits/b0174fc08598bb5ac7ef03079d1fcb23a0b7a4a6) and [ba839f8](https://github.com/tensorflow/tensorflow/pull/48192/commits/ba839f82264a3626101d5f1cb96733282c733a61) are already accepted in this other PR [48144](https://github.com/tensorflow/tensorflow/pull/48144).", "@googlebot I fixed it.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48192) for more info**.\n\n<!-- need_author_cla -->", "CLA provided", "@googlebot I fixed it.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48192) for more info**.\n\n<!-- need_author_cla -->", "@googlebot I fixed it.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48192) for more info**.\n\n<!-- need_author_cla -->", "Hi @victorromeo, I think I succesfully signed the CLA, but the bot keeps complaining. What about your side? Did you sign with the same email used for the commit?\r\n\r\nThanks!", "@dmpiergiacomo I have determined that the email the commits were done under was not a verifiable email address and as a result, I am unable to sign the CLA, without modifying the email addresses first.\r\n\r\nI've tried a `git filter-branch` style replace on a separate branch to swap the email addresses, but this appears to have modified more than I had anticipated. Am hoping to get my historical commits assigned to `victorromeo.gh@gmail.com` if you know how to do this?", "@victorromeo, thank you for the explanation, I understand the problem.\r\n\r\nI think the easiest solution would be the manual intervention of a Googler. I can see [here](https://opensource.google/docs/cla/#verify) that they should have the power to override the `cla: yes issue label`. @gbaned is this correct? Could you help us?\r\n\r\nAnother solution could be a git rebase through which I could amend your commits switching to the email address `victorromeo.gh@gmail.com`. I could do this from the same branch of this PR. I would however prefer the first solution, if possible for a Googler.", "Alternative 3) Please let me know if you'd like me to re-commit my changes on a clean branch, using the correct email account.\r\n\r\n> I would however prefer the first solution, if possible for a Googler.\r\n\r\nAgreed, as using a Googler will maintain the code base in its current state, however either solution is fine by me.  \r\n", "@dmpiergiacomo Can you please make sure to use same GitHub username and email-id associated with it.", "Hi @gbaned apologies for the delay, I had some hard deadlines to respect.\r\n\r\n> @dmpiergiacomo Can you please make sure to use same GitHub username and email-id associated with it.\r\n\r\nI believe I am using the same GitHub username and email-id associated with it. If not, could you please clarify?\r\n\r\nFrom my understanding the issue is that the email used by @victorromeo to push his changes can no longer be accessed to sign Google CLA. We would therefore kindly ask you to override the CLA flag with your admin power. Would this be possible?\r\n\r\nThank you.", "Thank you @gbaned for forcing the CLA flag to yes.\r\n\r\nIt looks like the community CI build now fails, probably something has changed in the meanwhile. Does it make sense to debug it, or better to wait for @advaitjain review first? \r\n\r\nThank you.", "Sorry for the delay here. I'm going to close the current PR since https://github.com/tensorflow/tensorflow/pull/48192 is doing the same and is soon going to be merged.", "Hi @advaitjain, I believe you might have provided a wrong link in your last message. If it's the case, could you provide the correct one? Thanks.\r\n\r\nAre we talking about [#47446](https://github.com/tensorflow/tensorflow/pull/47446) maybe?"]}, {"number": 48191, "title": "Regenerate Keras Python code based on model.summary() ", "body": "Is it possible to generate python code based on model.summary() information, the generated code (maybe write it to `filename.py` file) is the regular Keras model Sequential like the following:\r\n\r\n```\r\nmodel = Sequential()\r\nmodel.add(Conv1D(filters=256, kernel_size=5, padding='same', activation='relu',\r\n                 input_shape=(time_window_size, 1)))\r\nmodel.add(MaxPooling1D(pool_size=4))\r\nmodel.add(LSTM(64))\r\nmodel.add(Dense(units=time_window_size, activation='linear'))\r\nmodel.compile(optimizer='adam', loss='mean_squared_error', metrics=[metric])\r\nmodel.compile(optimizer='adam', loss='mean_squared_error', metrics=[metric])\r\nmodel.compile(optimizer=\"sgd\", loss=\"mse\", metrics=[metric])\r\n```\r\n\r\nI am sure someone somewhere did it but I searched GitHub and Google and could not find it, you may have already came across such function. Please share your thoughts.", "comments": ["@notsinkingtitanic AFAIK, there is no API that currently supports conversion of `model.summary()` to a model. I think it will be possible but as of now there is no effort to create it. \r\n\r\nThis is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!\r\n "]}, {"number": 48190, "title": "High Inference time on warmup state in android  ", "body": "**System information**\r\n- OS Platform and Distribution (Windows 10 Pro/Andriod 10,11):\r\n- Mobile device (Redmi note 8)\r\n- TensorFlow installed from (source or binary): tflite .so built from source\r\n- TensorFlow version: 2.4\r\n- Python version: 3.8\r\n\r\n**Problem:** I am working on Real time Audio processing application in android studio. I made a model using LSTMs and converted the model to the TFLITE format. After that when I tested model by giving some data it gives inconsistent inference time every time. Model inference time is 3-4ms. If I run inference in a loop, 3-4ms is the average inference time. If I shift towards the real-time scenario, I have to perform inference every 10ms. My inference time increases in this scenario. This is probably because the inference is not running back to back but actually waits till we get audio data again (10ms callback -3ms inference time = 7ms wait in thread for more audio data). I read on tensorflow site (https://www.tensorflow.org/lite/performance/measurement) that this is due to the warmup state and steady state. For warmup state it gives 9ms inference time and on steady state it gives almost 3ms inference time. But in my case, since audio is coming continuously and I want the model to run in steady state. Is there any way to run the TFLITE model in steady state once its loaded in the application ? \r\n   \r\n\r\n**Describe the current behavior**\r\nAndroid tflite model warmup everytime when inference function is called\r\n**Describe the expected behavior**\r\nAndroid tflite model should warmup once at start and stay in steady state after that.\r\n\r\nI run the model on tensorflow benchmark tool with and without delay. When I run model without any delay it gives avg 2ms time for inference. When I put 5ms delay between each inference then model inference time inreased to 6ms. Please refer to screenshots below:\r\n\r\n**Without any delay**\r\n![without delay](https://user-images.githubusercontent.com/63999516/113251302-80b87f00-92db-11eb-9a7f-1f160c506476.png)\r\n\r\n**With 5ms delay between each inference**\r\n![with 5m delay](https://user-images.githubusercontent.com/63999516/113251321-87df8d00-92db-11eb-9a71-56d535b7558a.png)\r\n\r\nAny help is greatly appreciated, thank you!", "comments": ["@srjoglekar246 could you take a look?", "@impjdi @ymodak Please reveiw", "@Humza1996,\r\nIn order to expedite the trouble-shooting process, could you please provide a minimal code snippet so that we can reproduce the issue on our end. Thanks!", "@amahendrakar Please find below\r\n**loadModelFile** is loading function\r\nIf I use **tflite.runForMultipleInputsOutputs(inputs, outputs);** it gives high inference time i.e 13-14 ms\r\nBut when I run this line in for loop it gives me around 3-4ms inference time. Althou I have declared tflite outside the while loop. \r\n------------------------------------------------------------ code ---------------------------------------------------------------\r\n\r\n```\r\nprivate MappedByteBuffer loadModelFile() throws IOException {\r\n            AssetFileDescriptor fileDescriptor = this.getAssets().openFd(\"lstm.tflite\");\r\n            FileInputStream fileInputStream = new FileInputStream(fileDescriptor.getFileDescriptor());\r\n            FileChannel fileChannel = fileInputStream.getChannel();\r\n            long startOffSets = fileDescriptor.getStartOffset();\r\n            long declaredLength = fileDescriptor.getDeclaredLength();\r\n            return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffSets, declaredLength);\r\n    \r\n        }\r\n    \t\r\n    Interpreter tflite;\r\n    float[][][] model_output;\r\n    float[][][][] model_st;\r\n    \r\n    private void foo() {\r\n            Thread bar = new Thread(new Runnable() {\r\n                @Override\r\n                public void run() {\r\n    \r\n                    try {\r\n                        tflite = new Interpreter(loadModelFile());\r\n    \r\n                    } catch (Exception e) {\r\n                        e.printStackTrace();\r\n                    }\r\n    \r\n                    while (true) {\r\n    \r\n                        float[] data = network.getData(); // blocking call // Audiorecord is being used in the audioHandler Object // Data is returned every 10ms\r\n    \r\n    \r\n    \t\t\t\t\t\r\n                        Object[] inputs = {data, states};\r\n    \t\t\t\t\toutputs.put(0, model_output);\r\n    \t\t\t\t\toutputs.put(1, model_st);\r\n    \r\n    \t\t\t\t\t\r\n    \t\t\t\t\t/////////// Inference ////////////////\r\n    \t\t\t\t\ttfLiteLock.lock();\r\n    \t\t\t\t\ttry {\r\n    \t\t\t\t\t\ttflite.runForMultipleInputsOutputs(inputs, outputs);\r\n    \t\t\t\t\t} finally {\r\n    \t\t\t\t\t\ttfLiteLock.unlock();\r\n    \t\t\t\t\t}\r\n    \t\t\t\t\tSystem.out.println(\"inference_time\",\"\"+tflite.getLastNativeInferenceDurationNanoseconds())\r\n    \t\t\t\t\t\r\n    \t\t\t\t\t//Computing \r\n    \t\t\t\t\t//for(int i=0;i<500;i++){\r\n    \t\t\t\t\t\r\n    \t\t\t\t\t//try {\r\n    \t\t\t\t\t//\ttflite.runForMultipleInputsOutputs(inputs, outputs);\r\n    \t\t\t\t\t//} finally {\r\n    \t\t\t\t\t//\ttfLiteLock.unlock();\r\n    \t\t\t\t\t//}\r\n    \t\t\t\t\t\r\n    \t\t\t\t\t//System.out.println(\"inference_time\",\"\"+tflite.getLastNativeInferenceDurationNanoseconds())\r\n    \t\t\t\t\t//}\r\n    \t\t\t\t\t\r\n                        foo.bar(model_output); \r\n    \r\n                    }\r\n                }\r\n            });\r\n            bar.setPriority(Thread.MAX_PRIORITY);\r\n            bar.start();\r\n```\r\n     ", "@amahendrakar any update ?", "I replied to the same question posted at https://stackoverflow.com/questions/66882466/high-inference-time-on-warmup-state-in-android-studio", "@abattery Thankyou for your feedback but I am not able to solve the issue. Model is working fine when its run in a continuous loop (without any delay). Pipeline is, app gets voice data  and take its FFT. Then pass it to the model. Model outputs some data and I take IFFT of that data. Issue is , when there is a delay of getting microphone data, model takes long time for prediction as well. I guess model terminate itself or go on sleep. I want , model should take same time for everytime irrespective of input delay.", "Model does not terminate and goes on sleep unless the inference object is destroyed. Instead, the related memory space can be evicted from the cache menory, which can impact the latency.\r\n\r\nTFLite model is just another program so it also is under controlled by CPU scheduling and its performance is related to the memory usage patterns. So, in my view point, it is kind of complicated problems and hard to analyze the overall behaviors with the limited information. To get the consistent result, I suggest doing some more experiments on the runtime environments, model's characteristics, and threading strategy.\r\n\r\nAlso you can rely on the other performance techniques, model optimizations and hardware acceleration paths in order to reduce overall latency.", "@sganeshb  @abattery I have tested all optimizations technique to the model. Now my model is fully optimized and in [Quantized form](https://www.tensorflow.org/lite/performance/post_training_quantization#:~:text=Post%2Dtraining%20quantization%20is%20a,little%20degradation%20in%20model%20accuracy.).\r\nI am just asking about the result from[ tensorflow benchmark tool](https://github.com/tensorflow/benchmarks) which are attached above. While testing TFLITE model on tensorflow benchmark tool, model also gives 2-3ms inference time. There is also an option of introducing delay between two inference in tensorflow benchmark tool. \r\n\r\n1. What is the purpose of that option of delay in benchmark tool ?\r\n2.  If I introduce delay between inference then why inference time increases ? \r\n\r\n**I am attaching benchmark results again below :**\r\n\r\nWithout any Delay between Inference, I got 2-3 ms inference time \r\n(time is in microsec)\r\n![without delay](https://user-images.githubusercontent.com/63999516/113472535-759d5480-947d-11eb-9d2b-f93390305dad.png)\r\n\r\nWith 5ms Delay between Inference, I got 7ms inference time\r\n![with 5m delay](https://user-images.githubusercontent.com/63999516/113472558-9f567b80-947d-11eb-816b-c9122592fa20.png)\r\n", "@multiverse-tf any thoughts on https://github.com/tensorflow/tensorflow/issues/48190#issuecomment-812831324 ?", "@rjpower @gbaned @amahendrakar  Please check #48190  [comment](https://github.com/tensorflow/tensorflow/issues/48190#issuecomment-812831324)", "I'm not sure, this looks TF-Lite specific, so perhaps someone from that team can chime in.\r\n\r\nMy guess for why the performance is different: by sleeping between inferences, you are letting the CPU state drop back to idle. When the inference request starts, the task then has to wake up before it can run the inference. So this is giving you an idea of how much time the model takes if it is starting \"cold\".", "@multiverse-tf Please see the [comment](https://github.com/tensorflow/tensorflow/issues/48190#issuecomment-812831324)", "> @multiverse-tf Please see the [comment](https://github.com/tensorflow/tensorflow/issues/48190#issuecomment-812831324)\r\n\r\nAcked. I agree w/ @rjpower's [comment](https://github.com/tensorflow/tensorflow/issues/48190#issuecomment-813453287) above that \"sleeping between inferences, you are letting the CPU state drop back to idle\" and then the \"CPU\" starts \"cold\" in terms of CPU frequency or execution being gradually migrated from little core to big core etc. Note CPUs on mobile phones are prone to being throttled to save power consumption.\r\n\r\nTo validate this idea, if possible, could you lock the CPU frequency and set the cpu affinity (see [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark#reducing-variance-between-runs-on-android) for reference) and check the perf. again?\r\n\r\nBtw, I noticed \"tfLiteLock.lock()\" and \"tfLiteLock.unlock();\" in the code you pasted above. Just wondering whether the locking here will introduce additional overhead here or not.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 48189, "title": "Broken Link in MLIR docs", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/mlir\r\n\r\n<img width=\"1406\" alt=\"Screen Shot 2021-03-30 at 10 25 10 PM\" src=\"https://user-images.githubusercontent.com/8815362/112996050-cc1d4100-91a6-11eb-8b25-54934afe1d09.png\">\r\n\r\n`MLIR on GitHub` box link is broken. That link is pointing `https://github.com/llvm/llvm-project/tree/master/mlir` that returns 404. I think `https://github.com/llvm/llvm-project/tree/main/mlir` is valid link.\r\n", "comments": ["@jeongukjae \r\nCreated a PR to fix this issue, this issue will be closed once the [PR](https://github.com/tensorflow/tensorflow/pull/48215) is merged.\r\n\r\nThanks!."]}, {"number": 48188, "title": "RaggedTensor with irregular shape breaks when yielded from `tf.keras.utils.Sequence`", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOs\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.2\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nI am trying to pass tensors with irregular shapes to a custom keras model. I am using a custom data generator which inherits from `tf.keras.utils.Sequence`. This custom data generator is important because we want to control how data is batched after each epoch. I've reproduced my issue in this sample code:\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport random\r\n\r\nclass BatchGen(tf.keras.utils.Sequence):\r\n    def __len__(self):\r\n        return 100\r\n    \r\n    def __getitem__(self, idx):\r\n        for i in range(100):\r\n            return self.gen_ragged_tensors()\r\n    \r\n    @staticmethod\r\n    def gen_ragged_tensors():\r\n        seq_len = np.random.randint(1,3)\r\n        a = np.zeros((seq_len,2), dtype=np.float32)\r\n\r\n        seq_len = np.random.randint(1,3)\r\n        b = np.ones((seq_len,2), dtype=np.float32)\r\n        c = tf.ragged.constant([a,b], dtype=tf.float32)\r\n        return c\r\n\r\nclass Model(tf.keras.models.Model):\r\n    \r\n    def calc_loss(self, batch_in):\r\n        # dummy operation\r\n        return tf.reduce_mean(batch_in - tf.constant(0, dtype=tf.float32))\r\n    \r\n    @tf.function\r\n    def train_step(self, batch_in):\r\n        \r\n        with tf.GradientTape(persistent=True) as tape:\r\n            prediction_loss = self.calc_loss(batch_in)\r\n        prediction_gradients = tape.gradient(prediction_loss, self.trainable_variables)\r\n        self.optimizer.apply_gradients(zip(prediction_gradients, self.trainable_variables))\r\n        self.add_loss(lambda: prediction_loss)\r\n        return {\"loss\": prediction_loss}\r\n        \r\n    def call(self, inputs, training):\r\n        return self.train_step(inputs)\r\n\r\nmodel = Model()\r\nmodel.compile(optimizer=tf.keras.optimizers.Adam(0.001))\r\n\r\n\r\ndata_gen = BatchGen()\r\nmodel.fit(data_gen, epochs=10)\r\n```\r\n\r\nRunning the above code gives the following error - \r\n```\r\nEpoch 1/10\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-5-71ab935a83be> in <module>\r\n----> 1 model.fit(data_gen, epochs=10)\r\n\r\n~/miniconda3/envs/rasa_master/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)\r\n    106   def _method_wrapper(self, *args, **kwargs):\r\n    107     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\r\n--> 108       return method(self, *args, **kwargs)\r\n    109 \r\n    110     # Running inside `run_distribute_coordinator` already.\r\n\r\n~/miniconda3/envs/rasa_master/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n   1096                 batch_size=batch_size):\r\n   1097               callbacks.on_train_batch_begin(step)\r\n-> 1098               tmp_logs = train_function(iterator)\r\n   1099               if data_handler.should_sync:\r\n   1100                 context.async_wait()\r\n\r\n~/miniconda3/envs/rasa_master/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    778       else:\r\n    779         compiler = \"nonXla\"\r\n--> 780         result = self._call(*args, **kwds)\r\n    781 \r\n    782       new_tracing_count = self._get_tracing_count()\r\n\r\n~/miniconda3/envs/rasa_master/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    844               *args, **kwds)\r\n    845       # If we did not create any variables the trace we have is good enough.\r\n--> 846       return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access\r\n    847 \r\n    848     def fn_with_cond(*inner_args, **inner_kwds):\r\n\r\n~/miniconda3/envs/rasa_master/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _filtered_call(self, args, kwargs, cancellation_manager)\r\n   1846                            resource_variable_ops.BaseResourceVariable))],\r\n   1847         captured_inputs=self.captured_inputs,\r\n-> 1848         cancellation_manager=cancellation_manager)\r\n   1849 \r\n   1850   def _call_flat(self, args, captured_inputs, cancellation_manager=None):\r\n\r\n~/miniconda3/envs/rasa_master/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1922       # No tape is watching; skip to running the function.\r\n   1923       return self._build_call_outputs(self._inference_function.call(\r\n-> 1924           ctx, args, cancellation_manager=cancellation_manager))\r\n   1925     forward_backward = self._select_forward_and_backward_functions(\r\n   1926         args,\r\n\r\n~/miniconda3/envs/rasa_master/lib/python3.7/site-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager)\r\n    548               inputs=args,\r\n    549               attrs=attrs,\r\n--> 550               ctx=ctx)\r\n    551         else:\r\n    552           outputs = execute.execute_with_cancellation(\r\n\r\n~/miniconda3/envs/rasa_master/lib/python3.7/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     58     ctx.ensure_initialized()\r\n     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n---> 60                                         inputs, attrs, num_outputs)\r\n     61   except core._NotOkStatusException as e:\r\n     62     if name is not None:\r\n\r\nInvalidArgumentError:  TypeError: `generator` yielded an element that could not be converted to the expected type. The expected type was float32, but the yielded element was <tf.RaggedTensor [[[0.0, 0.0], [0.0, 0.0]], [[1.0, 1.0], [1.0, 1.0]]]>.\r\nTraceback (most recent call last):\r\n\r\n  File \"/Users/daksh/miniconda3/envs/rasa_master/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 843, in generator_py_func\r\n    ret, dtype=dtype.as_numpy_dtype))\r\n\r\n  File \"/Users/daksh/miniconda3/envs/rasa_master/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 204, in _convert\r\n    result = np.asarray(value, dtype=dtype, order=\"C\")\r\n\r\n  File \"/Users/daksh/miniconda3/envs/rasa_master/lib/python3.7/site-packages/numpy/core/numeric.py\", line 538, in asarray\r\n    return array(a, dtype, copy=False, order=order)\r\n\r\nValueError: setting an array element with a sequence.\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n\r\nTraceback (most recent call last):\r\n\r\n  File \"/Users/daksh/miniconda3/envs/rasa_master/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 244, in __call__\r\n    ret = func(*args)\r\n\r\n  File \"/Users/daksh/miniconda3/envs/rasa_master/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 302, in wrapper\r\n    return func(*args, **kwargs)\r\n\r\n  File \"/Users/daksh/miniconda3/envs/rasa_master/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 848, in generator_py_func\r\n    \"element was %s.\" % (dtype.name, ret)), sys.exc_info()[2])\r\n\r\n  File \"/Users/daksh/miniconda3/envs/rasa_master/lib/python3.7/site-packages/six.py\", line 702, in reraise\r\n    raise value.with_traceback(tb)\r\n\r\n  File \"/Users/daksh/miniconda3/envs/rasa_master/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 843, in generator_py_func\r\n    ret, dtype=dtype.as_numpy_dtype))\r\n\r\n  File \"/Users/daksh/miniconda3/envs/rasa_master/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 204, in _convert\r\n    result = np.asarray(value, dtype=dtype, order=\"C\")\r\n\r\n  File \"/Users/daksh/miniconda3/envs/rasa_master/lib/python3.7/site-packages/numpy/core/numeric.py\", line 538, in asarray\r\n    return array(a, dtype, copy=False, order=order)\r\n\r\nTypeError: `generator` yielded an element that could not be converted to the expected type. The expected type was float32, but the yielded element was <tf.RaggedTensor [[[0.0, 0.0], [0.0, 0.0]], [[1.0, 1.0], [1.0, 1.0]]]>.\r\n\r\n\r\n\t [[{{node PyFunc}}]]\r\n\t [[IteratorGetNext]] [Op:__inference_train_function_170]\r\n\r\nFunction call stack:\r\ntrain_function\r\n```\r\n\r\nThe error seems to be coming because the inner rows of the ragged tensor have different size of the first dimension.\r\n\r\nNote, that there is no error when I replace `gen_ragged_tensors` with:\r\n\r\n```\r\ndef gen_ragged_tensors():\r\n        seq_len = np.random.randint(1,3)\r\n        a = np.zeros((seq_len,2), dtype=np.float32)\r\n        b = np.ones((seq_len,2), dtype=np.float32)\r\n        c = tf.ragged.constant([a,b], dtype=tf.float32, inner_shape=(2, seq_len, 2))\r\n        return c\r\n```\r\n\r\nThis is not useful in reality because this needs me to firstly use the same size of first dimension for all rows of the ragged tensor and secondly requires me to specify the `inner_shape` which is not guaranteed to be known because we want to pass irregular shaped tensors to our model.\r\n\r\nAny suggestions on what might be going wrong are very helpful.\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nSuccessfully being able to pass irregular shaped ragged tensors into a custom keras model. These ragged tensors should be yielded by a data generator of type `tf.keras.utils.Sequence`.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvided above\r\n", "comments": ["@dakshvar22 \r\nI ran the code shared and face \"invalid syntax: error, can you please check you code and share a colab gist with the error reported.", "@Saduf2019 There was a missing parenthesis. Updated the code. It should work now.", "I am able to replicate this issue on 2.3,2.4 and nightly, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/33af235b715ba87e27cbe0243e277283/untitled579.ipynb).", "Hi @jvishnuvardhan , do you have an update on the bug?", "Was able to replicate the issue in TF nightly-2.6.0.dev20210603,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/13c2de2d81f4820f0a17cf1281d8f0ac/untitled211.ipynb)..Thanks !", "@dakshvar22 Looks like this was resolved in recent `tf-nightly`. [Here](https://colab.research.google.com/gist/jvishnuvardhan/d37c0552f50a0c5aba0ee3f127105d62/untitled211.ipynb) is a gist for reference.\r\n\r\nCan you please verify once and close the issue if this was resolved for you.\r\n\r\nIf this was not resolved, can you please open the issue in  [keras-team/keras repo](https://github.com/keras-team/keras/issues) repo as keras development moved to that repo to focus mainly on Keras. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48188\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48188\">No</a>\n"]}, {"number": 48187, "title": " [ROCm] Update to use ROCm 4.1 (when building TF with --config=rocm) ", "body": "/cc @cheshire @chsigg @sanjoy ", "comments": [" @cheshire @chsigg @sanjoy gentle ping", "@cheshire @chsigg @sanjoy gentle ping", "@gbaned this PR seems to have failed the internal review/check....anything I can do to move this PR forward?\r\n\r\nthanks", "> @gbaned this PR seems to have failed the internal review/check....anything I can do to move this PR forward?\r\n> \r\n> thanks\r\n\r\nI did the fixes, it is merged now.", "Thank you @akuegel , really appreciate your help."]}, {"number": 48185, "title": "#Question on efficient data input pipeline.", "body": "Hello,\r\nI trained a small ML model, where I created and extracted training data in a Pythonic way. According to TF docs, it seems there is an efficient alternative using the module tf.data. I'm learning about tf.data from a Stanford Gitbub code written in TF Version 1. Thus, it is necessary o use tf.compat.v1 to use these commands on TF version 2. Since my goal is to build efficient solutions, is it still a good idea to learn these tf.compat.v1 commands or their purpose is solely to migrate codes written in TF v1 to TFv2?\r\n", "comments": ["@Zardoua-Yassir \r\nWe have V2 version for all V1 functions, you should look out at the V2 implementation in the API docs to modify current model,\r\nrequest you to refer to this [link](https://www.tensorflow.org/guide/upgrade) and move this issue to closed status and open an issue in [stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow) in case of any further queries as it is not a bug or feature request, Please post this kind of support questions at Stackoverflow. There is a big community to support and learn from your questions.\r\n\r\nThanks!", "Thank you for the kind answer. I'm closing this issue"]}, {"number": 48184, "title": "Minor improvement to documentation", "body": "* Added offline planner documentation to memory management markdown, since it was missing.\r\n* Minor updates to CMSIS-NN and Ethos-U documentation to reflect current functionality.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "A question/comment. Should I create an RFC folder inside the docs folder? Right now there's a mix between RFC's (e.g. pre-allocated tensor) and 'general documentation' in the docs root folder. @advaitjain let me know what you think.", "Solving #48183", "> A question/comment. Should I create an RFC folder inside the docs folder? Right now there's a mix between RFC's (e.g. pre-allocated tensor) and 'general documentation' in the docs root folder. @advaitjain let me know what you think.\r\n\r\nYes, that's a good idea. An RFC sub-folder would be very nice.", "@advaitjain done, pushed to this PR"]}]