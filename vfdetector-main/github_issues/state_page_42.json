[{"number": 45849, "title": "Inexact numeric jacobian causes test failures", "body": "Using TensorFlow 2.4 with GPUs\r\n\r\n**Describe the current behavior**\r\n\r\n//tensorflow/python/keras/integration_test:gradients_test fails when run on a node with GPUs. I traced the problem to inexact computation in `_compute_numeric_jacobian`\r\n\r\nOutput is:\r\n```\r\nFAIL: testLSTMBatchJacobian (__main__.GradientsTest)\r\nGradientsTest.testLSTMBatchJacobian\r\n...\r\nAssertionError: \r\nNot equal to tolerance rtol=0.01, atol=1e-06\r\nMismatched value: a is different from b. \r\nnot close where = (array([0]), array([0]), array([2]))\r\nnot close lhs = [0.00074506]\r\nnot close rhs = [0.00076706]\r\nnot close dif = [2.20043e-05]\r\nnot close tol = [8.670623e-06]\r\ndtype = float32, shape = (1, 1, 6)\r\nMismatch: 16.7%\r\nMax absolute difference: 2.20043e-05\r\nMax relative difference: 0.02868646\r\n x: array([[[-0.013396,  0.007078,  0.000745, -0.02031 ,  0.010461,\r\n         -0.004366]]], dtype=float32)\r\n y: array([[[-0.0134  ,  0.007075,  0.000767, -0.020303,  0.010446,\r\n         -0.004386]]], dtype=float32)\r\n```\r\n\r\nThe numeric result is `array([-0.013396,  0.007078,  0.000745, -0.02031 ,  0.010461, -0.004366], dtype=float32)`\r\nwhile the eager_result, function_result and backprop_result all are `y: array([-0.0134  ,  0.007075,  0.000767, -0.020303,  0.010446, -0.004386], dtype=float32)`\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nReduced extracted test code:\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nclass GradientsTest(tf.test.TestCase):\r\n  def testLSTMBatchJacobian(self):\r\n    class HasLSTM(tf.keras.Model):\r\n\r\n      def __init__(self):\r\n        super(HasLSTM, self).__init__()\r\n        self.lstm = tf.keras.layers.LSTM(units=5)\r\n        self.dense = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)\r\n\r\n      def call(self, x):\r\n        return self.dense(self.lstm(x))\r\n\r\n    m = HasLSTM()\r\n\r\n    def jacobian(x):\r\n      with tf.GradientTape() as tape:\r\n        tape.watch(x)\r\n        y = m(x)  # pylint: disable=not-callable\r\n      return tape.batch_jacobian(y, x)\r\n\r\n    inp = tf.nn.l2_normalize(tf.ones([1, 2, 3]), axis=[1, 2])\r\n    eager_result = jacobian(inp)\r\n    #function_result = tf.function(jacobian)(inp)\r\n    #self.assertAllClose(eager_result, function_result)\r\n    backprop_result, numeric_result = tf.test.compute_gradient(\r\n        m, [inp], delta=1e-3)\r\n\r\n    self.assertAllClose(numeric_result, backprop_result, rtol=1e-2)\r\n    self.assertAllClose(tf.reshape(numeric_result, [-1]),\r\n                        tf.reshape(eager_result, [-1]), rtol=1e-2)\r\n\r\nif __name__ == \"__main__\":\r\n  tf.test.main()\r\n```\r\n\r\n\r\n**Other info / logs**\r\nIncreasing the delta to `1e-2` makes the test pass.\r\n\r\nSide note: Uncommenting the `tf.function` line yields a couple wrong-looking errors/warnings:\r\n```\r\nWARNING:tensorflow:Using a while_loop for converting CudnnRNNBackprop\r\npfor.py:1052] Using a while_loop for converting CudnnRNNBackprop\r\nI tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\nE tensorflow/core/grappler/optimizers/meta_optimizer.cc:592] function_optimizer failed: Invalid argument: Input 0 of node loop_body/PartitionedCall/pfor/PartitionedCall/gradients/zeros_like/pfor/ZerosLike was passed float from has_lstm/lstm/PartitionedCall:6 incompatible with expected variant.\r\nE tensorflow/core/grappler/optimizers/meta_optimizer.cc:592] function_optimizer failed: Invalid argument: Input 0 of node loop_body/PartitionedCall/pfor/PartitionedCall/gradients/zeros_like/pfor/ZerosLike was passed float from has_lstm/lstm/PartitionedCall:6 incompatible with expected variant.\r\n W tensorflow/core/common_runtime/process_function_library_runtime.cc:805] Ignoring multi-device function optimization failure: Invalid argument: Input 0 of node loop_body/PartitionedCall/pfor/PartitionedCall/gradients/zeros_like/pfor/ZerosLike was passed float from has_lstm/lstm/PartitionedCall:6 incompatible with expected variant.\r\n```\r\n\r\n\r\n", "comments": ["Was able to reproduce the issue with TF v2.3 and TF v2.4. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/3c18dcfe8acc0183de0281eaf35abca9/45849.ipynb). Thanks!"]}, {"number": 45845, "title": "Failed to apply delegate: TfLiteGpuDelegate Init: MUL: Expected a 3D tensor of shape HxWxC or a 4D tensor of shape 1xHxWxC but got 98x8 (Android)", "body": "**System information**\r\n- OS Platform and Distribution: Android 9, 10 , 11\r\n- Mobile device : Pixel 3a, Nokia 6.1 \r\n- TensorFlow installed from:\r\n  https://bintray.com/google/tensorflow/tensorflow-lite-gpu\r\n  https://bintray.com/google/tensorflow/tensorflow-lite\r\n- TensorFlow version : 2.4.0\r\n\r\n**Describe the current behavior**\r\nI upgraded tensorflow-lite and tensorflow-lite-gpu from 2.3.0 to 2.4.0 \r\nAnd getting this error on initialization interpeteur \r\n\r\n` java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: TfLiteGpuDelegate Init: MUL: Expected a 3D tensor of shape HxWxC or a 4D tensor of shape 1xHxWxC but got 98x8\r\n    TfLiteGpuDelegate Prepare: delegate is not initialized\r\nNode number 329 (TfLiteGpuDelegateV2) failed to prepare.`\r\n\r\n`  val tfliteOptions = Interpreter\r\n                .Options()\r\n                .setNumThreads(THREADS_COUNT)\r\n                .addDelegate(GpuDelegate())\r\n\r\n  Interpreter(loadModelFile(context), tfliteOptions)`\r\n\r\nThe problem is somewhere in tensorflow-lite-gpu 2.4.0\r\n\r\nIf i'm using such configuration with older version all works well \r\n\r\nimplementation(\"org.tensorflow:tensorflow-lite:2.4.0\") \r\nimplementation(\"org.tensorflow:tensorflow-lite-gpu:2.3.0\")\r\n\r\n", "comments": ["Still the same issue after updating to 2.5.0", "![1](https://user-images.githubusercontent.com/20426405/121814783-89361a00-cc7b-11eb-85e1-87b939928419.png)\r\nLooks like on GpuDelegate initialization there is some issue with 2 input(\"style_weights) from tflite model. Something is changed in 2.4.0 and 2.5.0 in this part. This is very critical bug, that makes gpu library unusefull. I can provide my model if needed.  @renjie-liu @jvishnuvardhan @saikumarchalla @impjdi Thanks!", "Sorry, must have slipped.  Uh, I'm not familiar with `style_weights`.  IIRC our `MUL` essentially only does element-wise multiplication.  The dimension size of 170 doesn't make sense to me either; what op is this?", "> Sorry, must have slipped. Uh, I'm not familiar with `style_weights`. IIRC our `MUL` essentially only does element-wise multiplication. The dimension size of 170 doesn't make sense to me either; what op is this?\r\n\r\nThis is model for style transfer, first option is image for styling and second option is pretrained style. The model was trained with this guide https://github.com/magenta/magenta/tree/master/magenta/models/image_stylization\r\n", "Here, I attached model structure\r\n![main_model_1024_768 tflite](https://user-images.githubusercontent.com/20426405/122030555-8e5bac00-cdd6-11eb-8646-1b3c604671f9.png)\r\n", "@impjdi I build custom .aar from 2.5.0 version with additional logs and found that issue is happend in model_builder.cc.\r\nLooks like the problem is somewhere with shape determination. On gpu delegate initialization its trying to read constant tensor as HWC shape. \r\n[\r\n![2021-06-28](https://user-images.githubusercontent.com/20426405/123614903-712ed080-d80d-11eb-937b-e87a2d7c4760.png)\r\n](url)", "I don't know enough about neural networks or TF, but let's see from a mathematical point of view.  If you have two tensors A and B with shapes [1, 1280, 960, 3] and [170], respectively, how do you perform matrix multiplication?  That is the screenshot that you showed in https://github.com/tensorflow/tensorflow/issues/45845#issuecomment-860236437", "> I don't know enough about neural networks or TF, but let's see from a mathematical point of view. If you have two tensors A and B with shapes [1, 1280, 960, 3] and [170], respectively, how do you perform matrix multiplication? That is the screenshot that you showed in [#45845 (comment)](https://github.com/tensorflow/tensorflow/issues/45845#issuecomment-860236437)\r\n\r\nYeah I'm the same as you don't know enough. About shapes, I have added aditional logs and found that the problem is not with initial inputs nodes the problem is with initialization of some nodes in chain. I found a name of node where is exception occurs. About this node it is a MUL node with 2 inputs A and B. As you can see in attached screnshot they have the same shape [170,1] and [170,8] . So the problem may occur whith parsing or casting of input shapes. If youre ready to help. I can provide more information about. TY\r\n![image](https://user-images.githubusercontent.com/20426405/123852220-beea2c80-d924-11eb-8d26-4e10f8ef1a4c.png)\r\n", "In case of [170, 1] and [170, 8], there is a concept of broadcast, where a value at [i, 0] is applied (multiplied) to all elements in [i, 0..7].  https://numpy.org/doc/stable/user/basics.broadcasting.html\r\n\r\nHowever, [1, 1280, 960, 3] and [170] seem quite off.  I can understand if the 2nd matrix is, e.g. [1, 1280, 960, 1] or [1, 1, 1, 3], and then broadcast would be applicable, but a combination of [1, 1280, 960, 3] and [170] is beyond my understanding =/", "> In case of [170, 1] and [170, 8], there is a concept of broadcast, where a value at [i, 0] is applied (multiplied) to all elements in [i, 0..7]. https://numpy.org/doc/stable/user/basics.broadcasting.html\r\n> \r\n> However, [1, 1280, 960, 3] and [170] seem quite off. I can understand if the 2nd matrix is, e.g. [1, 1280, 960, 1] or [1, 1, 1, 3], and then broadcast would be applicable, but a combination of [1, 1280, 960, 3] and [170] is beyond my understanding =/\r\n\r\n170 is a number of pretrained style, and for single style transfer i'm setting for specific style value 1.0 and for other 0.0 for example [0 , 1.0 , 0 , 0 , 0 , 0...] and for multiple style selection i'm diving 1 / for style count, and as result it's [0,33, 0, 0,33, 0,33,0....] \r\n\r\nActual the problem I think is somewhere chained with shapes because the exception is occurs when the input with shape \r\n[170, 8] is casting as HWC shape thats why the error occurs, something is changed between 2.3.0 and 2.4.0 in this logic.\r\n\r\nhere is information about NN im using https://arxiv.org/pdf/1610.07629.pdf\r\n![image](https://user-images.githubusercontent.com/20426405/123949064-ceac5400-d9aa-11eb-84ca-458e9672cd43.png)\r\n", "> In case of [170, 1] and [170, 8], there is a concept of broadcast, where a value at [i, 0] is applied (multiplied) to all elements in [i, 0..7]. https://numpy.org/doc/stable/user/basics.broadcasting.html\r\n> \r\n> However, [1, 1280, 960, 3] and [170] seem quite off. I can understand if the 2nd matrix is, e.g. [1, 1280, 960, 1] or [1, 1, 1, 3], and then broadcast would be applicable, but a combination of [1, 1280, 960, 3] and [170] is beyond my understanding =/\r\n\r\nSo, the problem is with multiplication 2 2d matrix  \r\naccording to scheme here is them formula [170,1] x [170,8] = [170,8]\r\nOn initilalization looks like no rules for this case \r\n\r\n![image](https://user-images.githubusercontent.com/20426405/124351885-af5d3300-dc05-11eb-9ac8-640959244989.png)\r\n\r\n", "Now after yesterday merges in 2.3.0 by @mihaimaruseac this version have the same error", "Please provide a minimal reproducer. If you can, please bisect which cherry-pick causes the error.\r\n\r\nIt is very likely that the code you are using has a security issue, that is the only reason we do cherry-picks in older branches.", "> Please provide a minimal reproducer. If you can, please bisect which cherry-pick causes the error.\r\n> \r\n> It is very likely that the code you are using has a security issue, that is the only reason we do cherry-picks in older branches.\r\n\r\nI'm sorry. It was false alarm, looks like the local 2.5.0 lib cached. All fine with 2.3.0 ", "Hello again, here I made a simple android project with bug reproduce. There in gradle file is using new 2.6.0 version and the last working version 2.3.0 is commented. Maybe it can help. @impjdi https://github.com/OleksandrGrument/GpuDelegateBugReproduce", "Hello @impjdi , the new version 2.7.0 still have this issue", "With [170, 1] and [170, 8], it is now a legit broadcast MUL, but I don't see logic around broadcast MUL; I only see broadcast logic inside ADD.  You might have to do some similar plumbing to accommodate broadcast MUL.", "I have no more Ideas what to do. Here I used tf.lite.experimental.Analyzer.analyze(model_content=fb_model, gpu_compatibility=True) to analyze model, and it see no problems. The main question is why GpuDelegate 2.30 and lower working fine and new since 2.40 have this problem. This ploblem is unresolved almost for 2 years. \r\n\r\nDear TF contributers! Please can you help with this annoying bug. \r\n@daverim , @miaout17 @impjdi @terryheo @sirakiin \r\n\r\n```\r\nYour TFLite model has '1' subgraph(s). In the subgraph description below,\r\nT# represents the Tensor numbers. For example, in Subgraph#0, the MIRROR_PAD op takes\r\ntensor #0 and tensor #31 as input and produces tensor #66 as output.\r\n\r\nSubgraph#0 main(T#0, T#1) -> [T#363]\r\n  Op#0 MIRROR_PAD(T#0, T#31) -> [T#66]\r\n  Op#1 RESHAPE(T#1, T#9) -> [T#67]\r\n  Op#2 MUL(T#67, T#41) -> [T#68]\r\n  Op#3 SUM(T#68, T#8) -> [T#69]\r\n  Op#4 RESHAPE(T#69, T#62) -> [T#70]\r\n  Op#5 MUL(T#67, T#40) -> [T#71]\r\n  Op#6 SUM(T#71, T#8) -> [T#72]\r\n  Op#7 RESHAPE(T#72, T#62) -> [T#73]\r\n  Op#8 CONV_2D(T#66, T#43, T#42) -> [T#74]\r\n  Op#9 MEAN(T#74, T#4) -> [T#75]\r\n  Op#10 SQUARED_DIFFERENCE(T#74, T#75) -> [T#76]\r\n  Op#11 MEAN(T#76, T#4) -> [T#77]\r\n  Op#12 ADD(T#77, T#7) -> [T#78]\r\n  Op#13 RSQRT(T#78) -> [T#79]\r\n  Op#14 MUL(T#79, T#73) -> [T#80]\r\n  Op#15 MUL(T#74, T#80) -> [T#81]\r\n  Op#16 MUL(T#75, T#80) -> [T#82]\r\n  Op#17 SUB(T#70, T#82) -> [T#83]\r\n  Op#18 ADD(T#81, T#83) -> [T#84]\r\n  Op#19 MIRROR_PAD(T#84, T#12) -> [T#85]\r\n  Op#20 MUL(T#67, T#39) -> [T#86]\r\n  Op#21 SUM(T#86, T#8) -> [T#87]\r\n  Op#22 RESHAPE(T#87, T#63) -> [T#88]\r\n  Op#23 MUL(T#67, T#38) -> [T#89]\r\n  Op#24 SUM(T#89, T#8) -> [T#90]\r\n  Op#25 RESHAPE(T#90, T#63) -> [T#91]\r\n  Op#26 CONV_2D(T#85, T#45, T#44) -> [T#92]\r\n  Op#27 MEAN(T#92, T#4) -> [T#93]\r\n  Op#28 SQUARED_DIFFERENCE(T#92, T#93) -> [T#94]\r\n  Op#29 MEAN(T#94, T#4) -> [T#95]\r\n  Op#30 ADD(T#95, T#7) -> [T#96]\r\n  Op#31 RSQRT(T#96) -> [T#97]\r\n  Op#32 MUL(T#97, T#91) -> [T#98]\r\n  Op#33 MUL(T#92, T#98) -> [T#99]\r\n  Op#34 MUL(T#93, T#98) -> [T#100]\r\n  Op#35 SUB(T#88, T#100) -> [T#101]\r\n  Op#36 ADD(T#99, T#101) -> [T#102]\r\n  Op#37 MIRROR_PAD(T#102, T#12) -> [T#103]\r\n  Op#38 MUL(T#67, T#37) -> [T#104]\r\n  Op#39 SUM(T#104, T#8) -> [T#105]\r\n  Op#40 RESHAPE(T#105, T#64) -> [T#106]\r\n  Op#41 MUL(T#67, T#36) -> [T#107]\r\n  Op#42 SUM(T#107, T#8) -> [T#108]\r\n  Op#43 RESHAPE(T#108, T#64) -> [T#109]\r\n  Op#44 CONV_2D(T#103, T#47, T#46) -> [T#110]\r\n  Op#45 MEAN(T#110, T#4) -> [T#111]\r\n  Op#46 SQUARED_DIFFERENCE(T#110, T#111) -> [T#112]\r\n  Op#47 MEAN(T#112, T#4) -> [T#113]\r\n  Op#48 ADD(T#113, T#7) -> [T#114]\r\n  Op#49 RSQRT(T#114) -> [T#115]\r\n  Op#50 MUL(T#115, T#109) -> [T#116]\r\n  Op#51 MUL(T#110, T#116) -> [T#117]\r\n  Op#52 MUL(T#111, T#116) -> [T#118]\r\n  Op#53 SUB(T#106, T#118) -> [T#119]\r\n  Op#54 ADD(T#117, T#119) -> [T#120]\r\n  Op#55 MUL(T#67, T#35) -> [T#121]\r\n  Op#56 SUM(T#121, T#8) -> [T#122]\r\n  Op#57 RESHAPE(T#122, T#63) -> [T#123]\r\n  Op#58 MUL(T#67, T#34) -> [T#124]\r\n  Op#59 SUM(T#124, T#8) -> [T#125]\r\n  Op#60 RESHAPE(T#125, T#63) -> [T#126]\r\n  Op#61 MUL(T#67, T#33) -> [T#127]\r\n  Op#62 SUM(T#127, T#8) -> [T#128]\r\n  Op#63 RESHAPE(T#128, T#62) -> [T#129]\r\n  Op#64 MUL(T#67, T#32) -> [T#130]\r\n  Op#65 SUM(T#130, T#8) -> [T#131]\r\n  Op#66 RESHAPE(T#131, T#62) -> [T#132]\r\n  Op#67 MUL(T#67, T#30) -> [T#133]\r\n  Op#68 SUM(T#133, T#8) -> [T#134]\r\n  Op#69 RESHAPE(T#134, T#65) -> [T#135]\r\n  Op#70 MUL(T#67, T#29) -> [T#136]\r\n  Op#71 SUM(T#136, T#8) -> [T#137]\r\n  Op#72 RESHAPE(T#137, T#65) -> [T#138]\r\n  Op#73 MIRROR_PAD(T#120, T#12) -> [T#139]\r\n  Op#74 MUL(T#67, T#28) -> [T#140]\r\n  Op#75 SUM(T#140, T#8) -> [T#141]\r\n  Op#76 RESHAPE(T#141, T#64) -> [T#142]\r\n  Op#77 MUL(T#67, T#27) -> [T#143]\r\n  Op#78 SUM(T#143, T#8) -> [T#144]\r\n  Op#79 RESHAPE(T#144, T#64) -> [T#145]\r\n  Op#80 CONV_2D(T#139, T#48, T#46) -> [T#146]\r\n  Op#81 MEAN(T#146, T#4) -> [T#147]\r\n  Op#82 SQUARED_DIFFERENCE(T#146, T#147) -> [T#148]\r\n  Op#83 MEAN(T#148, T#4) -> [T#149]\r\n  Op#84 ADD(T#149, T#7) -> [T#150]\r\n  Op#85 RSQRT(T#150) -> [T#151]\r\n  Op#86 MUL(T#151, T#145) -> [T#152]\r\n  Op#87 MUL(T#146, T#152) -> [T#153]\r\n  Op#88 MUL(T#147, T#152) -> [T#154]\r\n  Op#89 SUB(T#142, T#154) -> [T#155]\r\n  Op#90 ADD(T#153, T#155) -> [T#156]\r\n  Op#91 MIRROR_PAD(T#156, T#12) -> [T#157]\r\n  Op#92 MUL(T#67, T#26) -> [T#158]\r\n  Op#93 SUM(T#158, T#8) -> [T#159]\r\n  Op#94 RESHAPE(T#159, T#64) -> [T#160]\r\n  Op#95 MUL(T#67, T#25) -> [T#161]\r\n  Op#96 SUM(T#161, T#8) -> [T#162]\r\n  Op#97 RESHAPE(T#162, T#64) -> [T#163]\r\n  Op#98 CONV_2D(T#157, T#49, T#46) -> [T#164]\r\n  Op#99 MEAN(T#164, T#4) -> [T#165]\r\n  Op#100 SQUARED_DIFFERENCE(T#164, T#165) -> [T#166]\r\n  Op#101 MEAN(T#166, T#4) -> [T#167]\r\n  Op#102 ADD(T#167, T#7) -> [T#168]\r\n  Op#103 RSQRT(T#168) -> [T#169]\r\n  Op#104 MUL(T#169, T#163) -> [T#170]\r\n  Op#105 MUL(T#164, T#170) -> [T#171]\r\n  Op#106 MUL(T#165, T#170) -> [T#172]\r\n  Op#107 SUB(T#160, T#172) -> [T#173]\r\n  Op#108 ADD(T#171, T#173) -> [T#174]\r\n  Op#109 ADD(T#120, T#174) -> [T#175]\r\n  Op#110 MIRROR_PAD(T#175, T#12) -> [T#176]\r\n  Op#111 MUL(T#67, T#24) -> [T#177]\r\n  Op#112 SUM(T#177, T#8) -> [T#178]\r\n  Op#113 RESHAPE(T#178, T#64) -> [T#179]\r\n  Op#114 MUL(T#67, T#23) -> [T#180]\r\n  Op#115 SUM(T#180, T#8) -> [T#181]\r\n  Op#116 RESHAPE(T#181, T#64) -> [T#182]\r\n  Op#117 CONV_2D(T#176, T#50, T#46) -> [T#183]\r\n  Op#118 MEAN(T#183, T#4) -> [T#184]\r\n  Op#119 SQUARED_DIFFERENCE(T#183, T#184) -> [T#185]\r\n  Op#120 MEAN(T#185, T#4) -> [T#186]\r\n  Op#121 ADD(T#186, T#7) -> [T#187]\r\n  Op#122 RSQRT(T#187) -> [T#188]\r\n  Op#123 MUL(T#188, T#182) -> [T#189]\r\n  Op#124 MUL(T#183, T#189) -> [T#190]\r\n  Op#125 MUL(T#184, T#189) -> [T#191]\r\n  Op#126 SUB(T#179, T#191) -> [T#192]\r\n  Op#127 ADD(T#190, T#192) -> [T#193]\r\n  Op#128 MIRROR_PAD(T#193, T#12) -> [T#194]\r\n  Op#129 MUL(T#67, T#22) -> [T#195]\r\n  Op#130 SUM(T#195, T#8) -> [T#196]\r\n  Op#131 RESHAPE(T#196, T#64) -> [T#197]\r\n  Op#132 MUL(T#67, T#21) -> [T#198]\r\n  Op#133 SUM(T#198, T#8) -> [T#199]\r\n  Op#134 RESHAPE(T#199, T#64) -> [T#200]\r\n  Op#135 CONV_2D(T#194, T#51, T#46) -> [T#201]\r\n  Op#136 MEAN(T#201, T#4) -> [T#202]\r\n  Op#137 SQUARED_DIFFERENCE(T#201, T#202) -> [T#203]\r\n  Op#138 MEAN(T#203, T#4) -> [T#204]\r\n  Op#139 ADD(T#204, T#7) -> [T#205]\r\n  Op#140 RSQRT(T#205) -> [T#206]\r\n  Op#141 MUL(T#206, T#200) -> [T#207]\r\n  Op#142 MUL(T#201, T#207) -> [T#208]\r\n  Op#143 MUL(T#202, T#207) -> [T#209]\r\n  Op#144 SUB(T#197, T#209) -> [T#210]\r\n  Op#145 ADD(T#208, T#210) -> [T#211]\r\n  Op#146 ADD(T#175, T#211) -> [T#212]\r\n  Op#147 MIRROR_PAD(T#212, T#12) -> [T#213]\r\n  Op#148 MUL(T#67, T#20) -> [T#214]\r\n  Op#149 SUM(T#214, T#8) -> [T#215]\r\n  Op#150 RESHAPE(T#215, T#64) -> [T#216]\r\n  Op#151 MUL(T#67, T#19) -> [T#217]\r\n  Op#152 SUM(T#217, T#8) -> [T#218]\r\n  Op#153 RESHAPE(T#218, T#64) -> [T#219]\r\n  Op#154 CONV_2D(T#213, T#52, T#46) -> [T#220]\r\n  Op#155 MEAN(T#220, T#4) -> [T#221]\r\n  Op#156 SQUARED_DIFFERENCE(T#220, T#221) -> [T#222]\r\n  Op#157 MEAN(T#222, T#4) -> [T#223]\r\n  Op#158 ADD(T#223, T#7) -> [T#224]\r\n  Op#159 RSQRT(T#224) -> [T#225]\r\n  Op#160 MUL(T#225, T#219) -> [T#226]\r\n  Op#161 MUL(T#220, T#226) -> [T#227]\r\n  Op#162 MUL(T#221, T#226) -> [T#228]\r\n  Op#163 SUB(T#216, T#228) -> [T#229]\r\n  Op#164 ADD(T#227, T#229) -> [T#230]\r\n  Op#165 MIRROR_PAD(T#230, T#12) -> [T#231]\r\n  Op#166 MUL(T#67, T#18) -> [T#232]\r\n  Op#167 SUM(T#232, T#8) -> [T#233]\r\n  Op#168 RESHAPE(T#233, T#64) -> [T#234]\r\n  Op#169 MUL(T#67, T#17) -> [T#235]\r\n  Op#170 SUM(T#235, T#8) -> [T#236]\r\n  Op#171 RESHAPE(T#236, T#64) -> [T#237]\r\n  Op#172 CONV_2D(T#231, T#53, T#46) -> [T#238]\r\n  Op#173 MEAN(T#238, T#4) -> [T#239]\r\n  Op#174 SQUARED_DIFFERENCE(T#238, T#239) -> [T#240]\r\n  Op#175 MEAN(T#240, T#4) -> [T#241]\r\n  Op#176 ADD(T#241, T#7) -> [T#242]\r\n  Op#177 RSQRT(T#242) -> [T#243]\r\n  Op#178 MUL(T#243, T#237) -> [T#244]\r\n  Op#179 MUL(T#238, T#244) -> [T#245]\r\n  Op#180 MUL(T#239, T#244) -> [T#246]\r\n  Op#181 SUB(T#234, T#246) -> [T#247]\r\n  Op#182 ADD(T#245, T#247) -> [T#248]\r\n  Op#183 ADD(T#212, T#248) -> [T#249]\r\n  Op#184 MIRROR_PAD(T#249, T#12) -> [T#250]\r\n  Op#185 MUL(T#67, T#16) -> [T#251]\r\n  Op#186 SUM(T#251, T#8) -> [T#252]\r\n  Op#187 RESHAPE(T#252, T#64) -> [T#253]\r\n  Op#188 MUL(T#67, T#15) -> [T#254]\r\n  Op#189 SUM(T#254, T#8) -> [T#255]\r\n  Op#190 RESHAPE(T#255, T#64) -> [T#256]\r\n  Op#191 CONV_2D(T#250, T#54, T#46) -> [T#257]\r\n  Op#192 MEAN(T#257, T#4) -> [T#258]\r\n  Op#193 SQUARED_DIFFERENCE(T#257, T#258) -> [T#259]\r\n  Op#194 MEAN(T#259, T#4) -> [T#260]\r\n  Op#195 ADD(T#260, T#7) -> [T#261]\r\n  Op#196 RSQRT(T#261) -> [T#262]\r\n  Op#197 MUL(T#262, T#256) -> [T#263]\r\n  Op#198 MUL(T#257, T#263) -> [T#264]\r\n  Op#199 MUL(T#258, T#263) -> [T#265]\r\n  Op#200 SUB(T#253, T#265) -> [T#266]\r\n  Op#201 ADD(T#264, T#266) -> [T#267]\r\n  Op#202 MIRROR_PAD(T#267, T#12) -> [T#268]\r\n  Op#203 MUL(T#67, T#14) -> [T#269]\r\n  Op#204 SUM(T#269, T#8) -> [T#270]\r\n  Op#205 RESHAPE(T#270, T#64) -> [T#271]\r\n  Op#206 MUL(T#67, T#13) -> [T#272]\r\n  Op#207 SUM(T#272, T#8) -> [T#273]\r\n  Op#208 RESHAPE(T#273, T#64) -> [T#274]\r\n  Op#209 CONV_2D(T#268, T#55, T#46) -> [T#275]\r\n  Op#210 MEAN(T#275, T#4) -> [T#276]\r\n  Op#211 SQUARED_DIFFERENCE(T#275, T#276) -> [T#277]\r\n  Op#212 MEAN(T#277, T#4) -> [T#278]\r\n  Op#213 ADD(T#278, T#7) -> [T#279]\r\n  Op#214 RSQRT(T#279) -> [T#280]\r\n  Op#215 MUL(T#280, T#274) -> [T#281]\r\n  Op#216 MUL(T#275, T#281) -> [T#282]\r\n  Op#217 MUL(T#276, T#281) -> [T#283]\r\n  Op#218 SUB(T#271, T#283) -> [T#284]\r\n  Op#219 ADD(T#282, T#284) -> [T#285]\r\n  Op#220 ADD(T#249, T#285) -> [T#286]\r\n  Op#221 MIRROR_PAD(T#286, T#12) -> [T#287]\r\n  Op#222 MUL(T#67, T#11) -> [T#288]\r\n  Op#223 SUM(T#288, T#8) -> [T#289]\r\n  Op#224 RESHAPE(T#289, T#64) -> [T#290]\r\n  Op#225 MUL(T#67, T#10) -> [T#291]\r\n  Op#226 SUM(T#291, T#8) -> [T#292]\r\n  Op#227 RESHAPE(T#292, T#64) -> [T#293]\r\n  Op#228 CONV_2D(T#287, T#56, T#46) -> [T#294]\r\n  Op#229 MEAN(T#294, T#4) -> [T#295]\r\n  Op#230 SQUARED_DIFFERENCE(T#294, T#295) -> [T#296]\r\n  Op#231 MEAN(T#296, T#4) -> [T#297]\r\n  Op#232 ADD(T#297, T#7) -> [T#298]\r\n  Op#233 RSQRT(T#298) -> [T#299]\r\n  Op#234 MUL(T#299, T#293) -> [T#300]\r\n  Op#235 MUL(T#294, T#300) -> [T#301]\r\n  Op#236 MUL(T#295, T#300) -> [T#302]\r\n  Op#237 SUB(T#290, T#302) -> [T#303]\r\n  Op#238 ADD(T#301, T#303) -> [T#304]\r\n  Op#239 MIRROR_PAD(T#304, T#12) -> [T#305]\r\n  Op#240 MUL(T#67, T#6) -> [T#306]\r\n  Op#241 SUM(T#306, T#8) -> [T#307]\r\n  Op#242 RESHAPE(T#307, T#64) -> [T#308]\r\n  Op#243 MUL(T#67, T#5) -> [T#309]\r\n  Op#244 SUM(T#309, T#8) -> [T#310]\r\n  Op#245 RESHAPE(T#310, T#64) -> [T#311]\r\n  Op#246 CONV_2D(T#305, T#57, T#46) -> [T#312]\r\n  Op#247 MEAN(T#312, T#4) -> [T#313]\r\n  Op#248 SQUARED_DIFFERENCE(T#312, T#313) -> [T#314]\r\n  Op#249 MEAN(T#314, T#4) -> [T#315]\r\n  Op#250 ADD(T#315, T#7) -> [T#316]\r\n  Op#251 RSQRT(T#316) -> [T#317]\r\n  Op#252 MUL(T#317, T#311) -> [T#318]\r\n  Op#253 MUL(T#312, T#318) -> [T#319]\r\n  Op#254 MUL(T#313, T#318) -> [T#320]\r\n  Op#255 SUB(T#308, T#320) -> [T#321]\r\n  Op#256 ADD(T#319, T#321) -> [T#322]\r\n  Op#257 ADD(T#286, T#322) -> [T#323]\r\n  Op#258 RESIZE_NEAREST_NEIGHBOR(T#323, T#3) -> [T#324]\r\n  Op#259 MIRROR_PAD(T#324, T#12) -> [T#325]\r\n  Op#260 CONV_2D(T#325, T#58, T#44) -> [T#326]\r\n  Op#261 MEAN(T#326, T#4) -> [T#327]\r\n  Op#262 SQUARED_DIFFERENCE(T#326, T#327) -> [T#328]\r\n  Op#263 MEAN(T#328, T#4) -> [T#329]\r\n  Op#264 ADD(T#329, T#7) -> [T#330]\r\n  Op#265 RSQRT(T#330) -> [T#331]\r\n  Op#266 MUL(T#331, T#126) -> [T#332]\r\n  Op#267 MUL(T#326, T#332) -> [T#333]\r\n  Op#268 MUL(T#327, T#332) -> [T#334]\r\n  Op#269 SUB(T#123, T#334) -> [T#335]\r\n  Op#270 ADD(T#333, T#335) -> [T#336]\r\n  Op#271 RESIZE_NEAREST_NEIGHBOR(T#336, T#2) -> [T#337]\r\n  Op#272 MIRROR_PAD(T#337, T#12) -> [T#338]\r\n  Op#273 CONV_2D(T#338, T#59, T#42) -> [T#339]\r\n  Op#274 MEAN(T#339, T#4) -> [T#340]\r\n  Op#275 SQUARED_DIFFERENCE(T#339, T#340) -> [T#341]\r\n  Op#276 MEAN(T#341, T#4) -> [T#342]\r\n  Op#277 ADD(T#342, T#7) -> [T#343]\r\n  Op#278 RSQRT(T#343) -> [T#344]\r\n  Op#279 MUL(T#344, T#132) -> [T#345]\r\n  Op#280 MUL(T#339, T#345) -> [T#346]\r\n  Op#281 MUL(T#340, T#345) -> [T#347]\r\n  Op#282 SUB(T#129, T#347) -> [T#348]\r\n  Op#283 ADD(T#346, T#348) -> [T#349]\r\n  Op#284 RESIZE_NEAREST_NEIGHBOR(T#349, T#2) -> [T#350]\r\n  Op#285 MIRROR_PAD(T#350, T#31) -> [T#351]\r\n  Op#286 CONV_2D(T#351, T#61, T#60) -> [T#352]\r\n  Op#287 MEAN(T#352, T#4) -> [T#353]\r\n  Op#288 SQUARED_DIFFERENCE(T#352, T#353) -> [T#354]\r\n  Op#289 MEAN(T#354, T#4) -> [T#355]\r\n  Op#290 ADD(T#355, T#7) -> [T#356]\r\n  Op#291 RSQRT(T#356) -> [T#357]\r\n  Op#292 MUL(T#357, T#138) -> [T#358]\r\n  Op#293 MUL(T#352, T#358) -> [T#359]\r\n  Op#294 MUL(T#353, T#358) -> [T#360]\r\n  Op#295 SUB(T#135, T#360) -> [T#361]\r\n  Op#296 ADD(T#359, T#361) -> [T#362]\r\n  Op#297 LOGISTIC(T#362) -> [T#363]\r\n\r\nTensors of Subgraph#0\r\n  T#0(input_image) shape:[1, 640, 480, 3], type:FLOAT32\r\n  T#1(style_weights) shape:[170], type:FLOAT32\r\n  T#2(transformer/expand/conv2/ResizeNearestNeighbor/size) shape:[2], type:INT32 RO 8 bytes\r\n  T#3(transformer/expand/conv1/ResizeNearestNeighbor/size) shape:[2], type:INT32 RO 8 bytes\r\n  T#4(transformer/contract/conv1/InstanceNorm/moments/mean/reduction_indices) shape:[2], type:INT32 RO 8 bytes\r\n  T#5(transformer/residual/residual5/conv2/InstanceNorm/gamma) shape:[170, 32], type:FLOAT32 RO 21760 bytes\r\n  T#6(transformer/residual/residual5/conv2/InstanceNorm/beta) shape:[170, 32], type:FLOAT32 RO 21760 bytes\r\n  T#7(transformer/contract/conv1/InstanceNorm/batchnorm/add/y) shape:n/a, type:FLOAT32 RO 4 bytes\r\n  T#8(transformer/contract/conv1/InstanceNorm/Sum/reduction_indices) shape:n/a, type:INT32 RO 4 bytes\r\n  T#9(transformer/contract/conv1/InstanceNorm/Reshape/shape) shape:[2], type:INT32 RO 8 bytes\r\n  T#10(transformer/residual/residual5/conv1/InstanceNorm/gamma) shape:[170, 32], type:FLOAT32 RO 21760 bytes\r\n  T#11(transformer/residual/residual5/conv1/InstanceNorm/beta) shape:[170, 32], type:FLOAT32 RO 21760 bytes\r\n  T#12(transformer/contract/MirrorPad_1/paddings) shape:[4, 2], type:INT32 RO 32 bytes\r\n  T#13(transformer/residual/residual4/conv2/InstanceNorm/gamma) shape:[170, 32], type:FLOAT32 RO 21760 bytes\r\n  T#14(transformer/residual/residual4/conv2/InstanceNorm/beta) shape:[170, 32], type:FLOAT32 RO 21760 bytes\r\n  T#15(transformer/residual/residual4/conv1/InstanceNorm/gamma) shape:[170, 32], type:FLOAT32 RO 21760 bytes\r\n  T#16(transformer/residual/residual4/conv1/InstanceNorm/beta) shape:[170, 32], type:FLOAT32 RO 21760 bytes\r\n  T#17(transformer/residual/residual3/conv2/InstanceNorm/gamma) shape:[170, 32], type:FLOAT32 RO 21760 bytes\r\n  T#18(transformer/residual/residual3/conv2/InstanceNorm/beta) shape:[170, 32], type:FLOAT32 RO 21760 bytes\r\n  T#19(transformer/residual/residual3/conv1/InstanceNorm/gamma) shape:[170, 32], type:FLOAT32 RO 21760 bytes\r\n  T#20(transformer/residual/residual3/conv1/InstanceNorm/beta) shape:[170, 32], type:FLOAT32 RO 21760 bytes\r\n  T#21(transformer/residual/residual2/conv2/InstanceNorm/gamma) shape:[170, 32], type:FLOAT32 RO 21760 bytes\r\n  T#22(transformer/residual/residual2/conv2/InstanceNorm/beta) shape:[170, 32], type:FLOAT32 RO 21760 bytes\r\n  T#23(transformer/residual/residual2/conv1/InstanceNorm/gamma) shape:[170, 32], type:FLOAT32 RO 21760 bytes\r\n  T#24(transformer/residual/residual2/conv1/InstanceNorm/beta) shape:[170, 32], type:FLOAT32 RO 21760 bytes\r\n  T#25(transformer/residual/residual1/conv2/InstanceNorm/gamma) shape:[170, 32], type:FLOAT32 RO 21760 bytes\r\n  T#26(transformer/residual/residual1/conv2/InstanceNorm/beta) shape:[170, 32], type:FLOAT32 RO 21760 bytes\r\n  T#27(transformer/residual/residual1/conv1/InstanceNorm/gamma) shape:[170, 32], type:FLOAT32 RO 21760 bytes\r\n  T#28(transformer/residual/residual1/conv1/InstanceNorm/beta) shape:[170, 32], type:FLOAT32 RO 21760 bytes\r\n  T#29(transformer/expand/conv3/conv/InstanceNorm/gamma) shape:[170, 3], type:FLOAT32 RO 2040 bytes\r\n  T#30(transformer/expand/conv3/conv/InstanceNorm/beta) shape:[170, 3], type:FLOAT32 RO 2040 bytes\r\n  T#31(transformer/contract/MirrorPad/paddings) shape:[4, 2], type:INT32 RO 32 bytes\r\n  T#32(transformer/expand/conv2/conv/InstanceNorm/gamma) shape:[170, 8], type:FLOAT32 RO 5440 bytes\r\n  T#33(transformer/expand/conv2/conv/InstanceNorm/beta) shape:[170, 8], type:FLOAT32 RO 5440 bytes\r\n  T#34(transformer/expand/conv1/conv/InstanceNorm/gamma) shape:[170, 16], type:FLOAT32 RO 10880 bytes\r\n  T#35(transformer/expand/conv1/conv/InstanceNorm/beta) shape:[170, 16], type:FLOAT32 RO 10880 bytes\r\n  T#36(transformer/contract/conv3/InstanceNorm/gamma) shape:[170, 32], type:FLOAT32 RO 21760 bytes\r\n  T#37(transformer/contract/conv3/InstanceNorm/beta) shape:[170, 32], type:FLOAT32 RO 21760 bytes\r\n  T#38(transformer/contract/conv2/InstanceNorm/gamma) shape:[170, 16], type:FLOAT32 RO 10880 bytes\r\n  T#39(transformer/contract/conv2/InstanceNorm/beta) shape:[170, 16], type:FLOAT32 RO 10880 bytes\r\n  T#40(transformer/contract/conv1/InstanceNorm/gamma) shape:[170, 8], type:FLOAT32 RO 5440 bytes\r\n  T#41(transformer/contract/conv1/InstanceNorm/beta) shape:[170, 8], type:FLOAT32 RO 5440 bytes\r\n  T#42(transformer/expand/conv2/conv/Conv2D) shape:[8], type:FLOAT32 RO 32 bytes\r\n  T#43(transformer/contract/conv1/Conv2D) shape:[8, 9, 9, 3], type:INT8 RO 1944 bytes\r\n  T#44(transformer/expand/conv1/conv/Conv2D) shape:[16], type:FLOAT32 RO 64 bytes\r\n  T#45(transformer/contract/conv2/Conv2D) shape:[16, 3, 3, 8], type:INT8 RO 1152 bytes\r\n  T#46(transformer/residual/residual5/conv2/Conv2D) shape:[32], type:FLOAT32 RO 128 bytes\r\n  T#47(transformer/contract/conv3/Conv2D) shape:[32, 3, 3, 16], type:INT8 RO 4608 bytes\r\n  T#48(transformer/residual/residual1/conv1/Conv2D) shape:[32, 3, 3, 32], type:INT8 RO 9216 bytes\r\n  T#49(transformer/residual/residual1/conv2/Conv2D) shape:[32, 3, 3, 32], type:INT8 RO 9216 bytes\r\n  T#50(transformer/residual/residual2/conv1/Conv2D) shape:[32, 3, 3, 32], type:INT8 RO 9216 bytes\r\n  T#51(transformer/residual/residual2/conv2/Conv2D) shape:[32, 3, 3, 32], type:INT8 RO 9216 bytes\r\n  T#52(transformer/residual/residual3/conv1/Conv2D) shape:[32, 3, 3, 32], type:INT8 RO 9216 bytes\r\n  T#53(transformer/residual/residual3/conv2/Conv2D) shape:[32, 3, 3, 32], type:INT8 RO 9216 bytes\r\n  T#54(transformer/residual/residual4/conv1/Conv2D) shape:[32, 3, 3, 32], type:INT8 RO 9216 bytes\r\n  T#55(transformer/residual/residual4/conv2/Conv2D) shape:[32, 3, 3, 32], type:INT8 RO 9216 bytes\r\n  T#56(transformer/residual/residual5/conv1/Conv2D) shape:[32, 3, 3, 32], type:INT8 RO 9216 bytes\r\n  T#57(transformer/residual/residual5/conv2/Conv2D1) shape:[32, 3, 3, 32], type:INT8 RO 9216 bytes\r\n  T#58(transformer/expand/conv1/conv/Conv2D1) shape:[16, 3, 3, 32], type:INT8 RO 4608 bytes\r\n  T#59(transformer/expand/conv2/conv/Conv2D1) shape:[8, 3, 3, 16], type:INT8 RO 1152 bytes\r\n  T#60(transformer/expand/conv3/conv/Conv2D) shape:[3], type:FLOAT32 RO 12 bytes\r\n  T#61(transformer/expand/conv3/conv/Conv2D1) shape:[3, 9, 9, 8], type:INT8 RO 1944 bytes\r\n  T#62(transformer/expand/conv2/conv/InstanceNorm/ExpandDims_3) shape:[4], type:INT32 RO 16 bytes\r\n  T#63(transformer/expand/conv1/conv/InstanceNorm/ExpandDims_3) shape:[4], type:INT32 RO 16 bytes\r\n  T#64(transformer/residual/residual5/conv2/InstanceNorm/ExpandDims_3) shape:[4], type:INT32 RO 16 bytes\r\n  T#65(transformer/expand/conv3/conv/InstanceNorm/ExpandDims_3) shape:[4], type:INT32 RO 16 bytes\r\n  T#66(transformer/contract/MirrorPad) shape:[1, 648, 488, 3], type:FLOAT32\r\n  T#67(transformer/contract/conv1/InstanceNorm/Reshape) shape:[170, 1], type:FLOAT32\r\n  T#68(transformer/contract/conv1/InstanceNorm/mul) shape:[170, 8], type:FLOAT32\r\n  T#69(transformer/contract/conv1/InstanceNorm/Sum) shape:[1, 8], type:FLOAT32\r\n  T#70(transformer/contract/conv1/InstanceNorm/ExpandDims_1) shape:[1, 1, 1, 8], type:FLOAT32\r\n  T#71(transformer/contract/conv1/InstanceNorm/mul_1) shape:[170, 8], type:FLOAT32\r\n  T#72(transformer/contract/conv1/InstanceNorm/Sum_1) shape:[1, 8], type:FLOAT32\r\n  T#73(transformer/contract/conv1/InstanceNorm/ExpandDims_3) shape:[1, 1, 1, 8], type:FLOAT32\r\n  T#74(transformer/contract/conv1/Conv2D1) shape:[1, 640, 480, 8], type:FLOAT32\r\n  T#75(transformer/contract/conv1/InstanceNorm/moments/mean) shape:[1, 1, 1, 8], type:FLOAT32\r\n  T#76(transformer/contract/conv1/InstanceNorm/moments/SquaredDifference) shape:[1, 640, 480, 8], type:FLOAT32\r\n  T#77(transformer/contract/conv1/InstanceNorm/moments/variance) shape:[1, 1, 1, 8], type:FLOAT32\r\n  T#78(transformer/contract/conv1/InstanceNorm/batchnorm/add) shape:[1, 1, 1, 8], type:FLOAT32\r\n  T#79(transformer/contract/conv1/InstanceNorm/batchnorm/Rsqrt) shape:[1, 1, 1, 8], type:FLOAT32\r\n  T#80(transformer/contract/conv1/InstanceNorm/batchnorm/mul) shape:[1, 1, 1, 8], type:FLOAT32\r\n  T#81(transformer/contract/conv1/InstanceNorm/batchnorm/mul_1) shape:[1, 640, 480, 8], type:FLOAT32\r\n  T#82(transformer/contract/conv1/InstanceNorm/batchnorm/mul_2) shape:[1, 1, 1, 8], type:FLOAT32\r\n  T#83(transformer/contract/conv1/InstanceNorm/batchnorm/sub) shape:[1, 1, 1, 8], type:FLOAT32\r\n  T#84(transformer/contract/conv1/Relu;transformer/contract/conv1/InstanceNorm/batchnorm/add_1) shape:[1, 640, 480, 8], type:FLOAT32\r\n  T#85(transformer/contract/MirrorPad_1) shape:[1, 642, 482, 8], type:FLOAT32\r\n  T#86(transformer/contract/conv2/InstanceNorm/mul) shape:[170, 16], type:FLOAT32\r\n  T#87(transformer/contract/conv2/InstanceNorm/Sum) shape:[1, 16], type:FLOAT32\r\n  T#88(transformer/contract/conv2/InstanceNorm/ExpandDims_1) shape:[1, 1, 1, 16], type:FLOAT32\r\n  T#89(transformer/contract/conv2/InstanceNorm/mul_1) shape:[170, 16], type:FLOAT32\r\n  T#90(transformer/contract/conv2/InstanceNorm/Sum_1) shape:[1, 16], type:FLOAT32\r\n  T#91(transformer/contract/conv2/InstanceNorm/ExpandDims_3) shape:[1, 1, 1, 16], type:FLOAT32\r\n  T#92(transformer/contract/conv2/Conv2D1) shape:[1, 320, 240, 16], type:FLOAT32\r\n  T#93(transformer/contract/conv2/InstanceNorm/moments/mean) shape:[1, 1, 1, 16], type:FLOAT32\r\n  T#94(transformer/contract/conv2/InstanceNorm/moments/SquaredDifference) shape:[1, 320, 240, 16], type:FLOAT32\r\n  T#95(transformer/contract/conv2/InstanceNorm/moments/variance) shape:[1, 1, 1, 16], type:FLOAT32\r\n  T#96(transformer/contract/conv2/InstanceNorm/batchnorm/add) shape:[1, 1, 1, 16], type:FLOAT32\r\n  T#97(transformer/contract/conv2/InstanceNorm/batchnorm/Rsqrt) shape:[1, 1, 1, 16], type:FLOAT32\r\n  T#98(transformer/contract/conv2/InstanceNorm/batchnorm/mul) shape:[1, 1, 1, 16], type:FLOAT32\r\n  T#99(transformer/contract/conv2/InstanceNorm/batchnorm/mul_1) shape:[1, 320, 240, 16], type:FLOAT32\r\n  T#100(transformer/contract/conv2/InstanceNorm/batchnorm/mul_2) shape:[1, 1, 1, 16], type:FLOAT32\r\n  T#101(transformer/contract/conv2/InstanceNorm/batchnorm/sub) shape:[1, 1, 1, 16], type:FLOAT32\r\n  T#102(transformer/contract/conv2/Relu;transformer/contract/conv2/InstanceNorm/batchnorm/add_1) shape:[1, 320, 240, 16], type:FLOAT32\r\n  T#103(transformer/contract/MirrorPad_2) shape:[1, 322, 242, 16], type:FLOAT32\r\n  T#104(transformer/contract/conv3/InstanceNorm/mul) shape:[170, 32], type:FLOAT32\r\n  T#105(transformer/contract/conv3/InstanceNorm/Sum) shape:[1, 32], type:FLOAT32\r\n  T#106(transformer/contract/conv3/InstanceNorm/ExpandDims_1) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#107(transformer/contract/conv3/InstanceNorm/mul_1) shape:[170, 32], type:FLOAT32\r\n  T#108(transformer/contract/conv3/InstanceNorm/Sum_1) shape:[1, 32], type:FLOAT32\r\n  T#109(transformer/contract/conv3/InstanceNorm/ExpandDims_3) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#110(transformer/contract/conv3/Conv2D1) shape:[1, 160, 120, 32], type:FLOAT32\r\n  T#111(transformer/contract/conv3/InstanceNorm/moments/mean) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#112(transformer/contract/conv3/InstanceNorm/moments/SquaredDifference) shape:[1, 160, 120, 32], type:FLOAT32\r\n  T#113(transformer/contract/conv3/InstanceNorm/moments/variance) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#114(transformer/contract/conv3/InstanceNorm/batchnorm/add) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#115(transformer/contract/conv3/InstanceNorm/batchnorm/Rsqrt) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#116(transformer/contract/conv3/InstanceNorm/batchnorm/mul) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#117(transformer/contract/conv3/InstanceNorm/batchnorm/mul_1) shape:[1, 160, 120, 32], type:FLOAT32\r\n  T#118(transformer/contract/conv3/InstanceNorm/batchnorm/mul_2) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#119(transformer/contract/conv3/InstanceNorm/batchnorm/sub) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#120(transformer/contract/conv3/Relu;transformer/contract/conv3/InstanceNorm/batchnorm/add_1) shape:[1, 160, 120, 32], type:FLOAT32\r\n  T#121(transformer/expand/conv1/conv/InstanceNorm/mul) shape:[170, 16], type:FLOAT32\r\n  T#122(transformer/expand/conv1/conv/InstanceNorm/Sum) shape:[1, 16], type:FLOAT32\r\n  T#123(transformer/expand/conv1/conv/InstanceNorm/ExpandDims_1) shape:[1, 1, 1, 16], type:FLOAT32\r\n  T#124(transformer/expand/conv1/conv/InstanceNorm/mul_1) shape:[170, 16], type:FLOAT32\r\n  T#125(transformer/expand/conv1/conv/InstanceNorm/Sum_1) shape:[1, 16], type:FLOAT32\r\n  T#126(transformer/expand/conv1/conv/InstanceNorm/ExpandDims_31) shape:[1, 1, 1, 16], type:FLOAT32\r\n  T#127(transformer/expand/conv2/conv/InstanceNorm/mul) shape:[170, 8], type:FLOAT32\r\n  T#128(transformer/expand/conv2/conv/InstanceNorm/Sum) shape:[1, 8], type:FLOAT32\r\n  T#129(transformer/expand/conv2/conv/InstanceNorm/ExpandDims_1) shape:[1, 1, 1, 8], type:FLOAT32\r\n  T#130(transformer/expand/conv2/conv/InstanceNorm/mul_1) shape:[170, 8], type:FLOAT32\r\n  T#131(transformer/expand/conv2/conv/InstanceNorm/Sum_1) shape:[1, 8], type:FLOAT32\r\n  T#132(transformer/expand/conv2/conv/InstanceNorm/ExpandDims_31) shape:[1, 1, 1, 8], type:FLOAT32\r\n  T#133(transformer/expand/conv3/conv/InstanceNorm/mul) shape:[170, 3], type:FLOAT32\r\n  T#134(transformer/expand/conv3/conv/InstanceNorm/Sum) shape:[1, 3], type:FLOAT32\r\n  T#135(transformer/expand/conv3/conv/InstanceNorm/ExpandDims_1) shape:[1, 1, 1, 3], type:FLOAT32\r\n  T#136(transformer/expand/conv3/conv/InstanceNorm/mul_1) shape:[170, 3], type:FLOAT32\r\n  T#137(transformer/expand/conv3/conv/InstanceNorm/Sum_1) shape:[1, 3], type:FLOAT32\r\n  T#138(transformer/expand/conv3/conv/InstanceNorm/ExpandDims_31) shape:[1, 1, 1, 3], type:FLOAT32\r\n  T#139(transformer/residual/residual1/MirrorPad) shape:[1, 162, 122, 32], type:FLOAT32\r\n  T#140(transformer/residual/residual1/conv1/InstanceNorm/mul) shape:[170, 32], type:FLOAT32\r\n  T#141(transformer/residual/residual1/conv1/InstanceNorm/Sum) shape:[1, 32], type:FLOAT32\r\n  T#142(transformer/residual/residual1/conv1/InstanceNorm/ExpandDims_1) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#143(transformer/residual/residual1/conv1/InstanceNorm/mul_1) shape:[170, 32], type:FLOAT32\r\n  T#144(transformer/residual/residual1/conv1/InstanceNorm/Sum_1) shape:[1, 32], type:FLOAT32\r\n  T#145(transformer/residual/residual1/conv1/InstanceNorm/ExpandDims_3) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#146(transformer/residual/residual1/conv1/Conv2D1) shape:[1, 160, 120, 32], type:FLOAT32\r\n  T#147(transformer/residual/residual1/conv1/InstanceNorm/moments/mean) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#148(transformer/residual/residual1/conv1/InstanceNorm/moments/SquaredDifference) shape:[1, 160, 120, 32], type:FLOAT32\r\n  T#149(transformer/residual/residual1/conv1/InstanceNorm/moments/variance) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#150(transformer/residual/residual1/conv1/InstanceNorm/batchnorm/add) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#151(transformer/residual/residual1/conv1/InstanceNorm/batchnorm/Rsqrt) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#152(transformer/residual/residual1/conv1/InstanceNorm/batchnorm/mul) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#153(transformer/residual/residual1/conv1/InstanceNorm/batchnorm/mul_1) shape:[1, 160, 120, 32], type:FLOAT32\r\n  T#154(transformer/residual/residual1/conv1/InstanceNorm/batchnorm/mul_2) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#155(transformer/residual/residual1/conv1/InstanceNorm/batchnorm/sub) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#156(transformer/residual/residual1/conv1/Relu;transformer/residual/residual1/conv1/InstanceNorm/batchnorm/add_1) shape:[1, 160, 120, 32], type:FLOAT32\r\n  T#157(transformer/residual/residual1/MirrorPad_1) shape:[1, 162, 122, 32], type:FLOAT32\r\n  T#158(transformer/residual/residual1/conv2/InstanceNorm/mul) shape:[170, 32], type:FLOAT32\r\n  T#159(transformer/residual/residual1/conv2/InstanceNorm/Sum) shape:[1, 32], type:FLOAT32\r\n  T#160(transformer/residual/residual1/conv2/InstanceNorm/ExpandDims_1) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#161(transformer/residual/residual1/conv2/InstanceNorm/mul_1) shape:[170, 32], type:FLOAT32\r\n  T#162(transformer/residual/residual1/conv2/InstanceNorm/Sum_1) shape:[1, 32], type:FLOAT32\r\n  T#163(transformer/residual/residual1/conv2/InstanceNorm/ExpandDims_3) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#164(transformer/residual/residual1/conv2/Conv2D1) shape:[1, 160, 120, 32], type:FLOAT32\r\n  T#165(transformer/residual/residual1/conv2/InstanceNorm/moments/mean) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#166(transformer/residual/residual1/conv2/InstanceNorm/moments/SquaredDifference) shape:[1, 160, 120, 32], type:FLOAT32\r\n  T#167(transformer/residual/residual1/conv2/InstanceNorm/moments/variance) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#168(transformer/residual/residual1/conv2/InstanceNorm/batchnorm/add) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#169(transformer/residual/residual1/conv2/InstanceNorm/batchnorm/Rsqrt) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#170(transformer/residual/residual1/conv2/InstanceNorm/batchnorm/mul) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#171(transformer/residual/residual1/conv2/InstanceNorm/batchnorm/mul_1) shape:[1, 160, 120, 32], type:FLOAT32\r\n  T#172(transformer/residual/residual1/conv2/InstanceNorm/batchnorm/mul_2) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#173(transformer/residual/residual1/conv2/InstanceNorm/batchnorm/sub) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#174(transformer/residual/residual1/conv2/InstanceNorm/batchnorm/add_1) shape:[1, 160, 120, 32], type:FLOAT32\r\n  T#175(transformer/residual/residual1/add) shape:[1, 160, 120, 32], type:FLOAT32\r\n  T#176(transformer/residual/residual2/MirrorPad) shape:[1, 162, 122, 32], type:FLOAT32\r\n  T#177(transformer/residual/residual2/conv1/InstanceNorm/mul) shape:[170, 32], type:FLOAT32\r\n  T#178(transformer/residual/residual2/conv1/InstanceNorm/Sum) shape:[1, 32], type:FLOAT32\r\n  T#179(transformer/residual/residual2/conv1/InstanceNorm/ExpandDims_1) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#180(transformer/residual/residual2/conv1/InstanceNorm/mul_1) shape:[170, 32], type:FLOAT32\r\n  T#181(transformer/residual/residual2/conv1/InstanceNorm/Sum_1) shape:[1, 32], type:FLOAT32\r\n  T#182(transformer/residual/residual2/conv1/InstanceNorm/ExpandDims_3) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#183(transformer/residual/residual2/conv1/Conv2D1) shape:[1, 160, 120, 32], type:FLOAT32\r\n  T#184(transformer/residual/residual2/conv1/InstanceNorm/moments/mean) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#185(transformer/residual/residual2/conv1/InstanceNorm/moments/SquaredDifference) shape:[1, 160, 120, 32], type:FLOAT32\r\n  T#186(transformer/residual/residual2/conv1/InstanceNorm/moments/variance) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#187(transformer/residual/residual2/conv1/InstanceNorm/batchnorm/add) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#188(transformer/residual/residual2/conv1/InstanceNorm/batchnorm/Rsqrt) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#189(transformer/residual/residual2/conv1/InstanceNorm/batchnorm/mul) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#190(transformer/residual/residual2/conv1/InstanceNorm/batchnorm/mul_1) shape:[1, 160, 120, 32], type:FLOAT32\r\n  T#191(transformer/residual/residual2/conv1/InstanceNorm/batchnorm/mul_2) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#192(transformer/residual/residual2/conv1/InstanceNorm/batchnorm/sub) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#193(transformer/residual/residual2/conv1/Relu;transformer/residual/residual2/conv1/InstanceNorm/batchnorm/add_1) shape:[1, 160, 120, 32], type:FLOAT32\r\n  T#194(transformer/residual/residual2/MirrorPad_1) shape:[1, 162, 122, 32], type:FLOAT32\r\n  T#195(transformer/residual/residual2/conv2/InstanceNorm/mul) shape:[170, 32], type:FLOAT32\r\n  T#196(transformer/residual/residual2/conv2/InstanceNorm/Sum) shape:[1, 32], type:FLOAT32\r\n  T#197(transformer/residual/residual2/conv2/InstanceNorm/ExpandDims_1) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#198(transformer/residual/residual2/conv2/InstanceNorm/mul_1) shape:[170, 32], type:FLOAT32\r\n  T#199(transformer/residual/residual2/conv2/InstanceNorm/Sum_1) shape:[1, 32], type:FLOAT32\r\n  T#200(transformer/residual/residual2/conv2/InstanceNorm/ExpandDims_3) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#201(transformer/residual/residual2/conv2/Conv2D1) shape:[1, 160, 120, 32], type:FLOAT32\r\n  T#202(transformer/residual/residual2/conv2/InstanceNorm/moments/mean) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#203(transformer/residual/residual2/conv2/InstanceNorm/moments/SquaredDifference) shape:[1, 160, 120, 32], type:FLOAT32\r\n  T#204(transformer/residual/residual2/conv2/InstanceNorm/moments/variance) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#205(transformer/residual/residual2/conv2/InstanceNorm/batchnorm/add) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#206(transformer/residual/residual2/conv2/InstanceNorm/batchnorm/Rsqrt) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#207(transformer/residual/residual2/conv2/InstanceNorm/batchnorm/mul) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#208(transformer/residual/residual2/conv2/InstanceNorm/batchnorm/mul_1) shape:[1, 160, 120, 32], type:FLOAT32\r\n  T#209(transformer/residual/residual2/conv2/InstanceNorm/batchnorm/mul_2) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#210(transformer/residual/residual2/conv2/InstanceNorm/batchnorm/sub) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#211(transformer/residual/residual2/conv2/InstanceNorm/batchnorm/add_1) shape:[1, 160, 120, 32], type:FLOAT32\r\n  T#212(transformer/residual/residual2/add) shape:[1, 160, 120, 32], type:FLOAT32\r\n  T#213(transformer/residual/residual3/MirrorPad) shape:[1, 162, 122, 32], type:FLOAT32\r\n  T#214(transformer/residual/residual3/conv1/InstanceNorm/mul) shape:[170, 32], type:FLOAT32\r\n  T#215(transformer/residual/residual3/conv1/InstanceNorm/Sum) shape:[1, 32], type:FLOAT32\r\n  T#216(transformer/residual/residual3/conv1/InstanceNorm/ExpandDims_1) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#217(transformer/residual/residual3/conv1/InstanceNorm/mul_1) shape:[170, 32], type:FLOAT32\r\n  T#218(transformer/residual/residual3/conv1/InstanceNorm/Sum_1) shape:[1, 32], type:FLOAT32\r\n  T#219(transformer/residual/residual3/conv1/InstanceNorm/ExpandDims_3) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#220(transformer/residual/residual3/conv1/Conv2D1) shape:[1, 160, 120, 32], type:FLOAT32\r\n  T#221(transformer/residual/residual3/conv1/InstanceNorm/moments/mean) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#222(transformer/residual/residual3/conv1/InstanceNorm/moments/SquaredDifference) shape:[1, 160, 120, 32], type:FLOAT32\r\n  T#223(transformer/residual/residual3/conv1/InstanceNorm/moments/variance) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#224(transformer/residual/residual3/conv1/InstanceNorm/batchnorm/add) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#225(transformer/residual/residual3/conv1/InstanceNorm/batchnorm/Rsqrt) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#226(transformer/residual/residual3/conv1/InstanceNorm/batchnorm/mul) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#227(transformer/residual/residual3/conv1/InstanceNorm/batchnorm/mul_1) shape:[1, 160, 120, 32], type:FLOAT32\r\n  T#228(transformer/residual/residual3/conv1/InstanceNorm/batchnorm/mul_2) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#229(transformer/residual/residual3/conv1/InstanceNorm/batchnorm/sub) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#230(transformer/residual/residual3/conv1/Relu;transformer/residual/residual3/conv1/InstanceNorm/batchnorm/add_1) shape:[1, 160, 120, 32], type:FLOAT32\r\n  T#231(transformer/residual/residual3/MirrorPad_1) shape:[1, 162, 122, 32], type:FLOAT32\r\n  T#232(transformer/residual/residual3/conv2/InstanceNorm/mul) shape:[170, 32], type:FLOAT32\r\n  T#233(transformer/residual/residual3/conv2/InstanceNorm/Sum) shape:[1, 32], type:FLOAT32\r\n  T#234(transformer/residual/residual3/conv2/InstanceNorm/ExpandDims_1) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#235(transformer/residual/residual3/conv2/InstanceNorm/mul_1) shape:[170, 32], type:FLOAT32\r\n  T#236(transformer/residual/residual3/conv2/InstanceNorm/Sum_1) shape:[1, 32], type:FLOAT32\r\n  T#237(transformer/residual/residual3/conv2/InstanceNorm/ExpandDims_3) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#238(transformer/residual/residual3/conv2/Conv2D1) shape:[1, 160, 120, 32], type:FLOAT32\r\n  T#239(transformer/residual/residual3/conv2/InstanceNorm/moments/mean) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#240(transformer/residual/residual3/conv2/InstanceNorm/moments/SquaredDifference) shape:[1, 160, 120, 32], type:FLOAT32\r\n  T#241(transformer/residual/residual3/conv2/InstanceNorm/moments/variance) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#242(transformer/residual/residual3/conv2/InstanceNorm/batchnorm/add) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#243(transformer/residual/residual3/conv2/InstanceNorm/batchnorm/Rsqrt) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#244(transformer/residual/residual3/conv2/InstanceNorm/batchnorm/mul) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#245(transformer/residual/residual3/conv2/InstanceNorm/batchnorm/mul_1) shape:[1, 160, 120, 32], type:FLOAT32\r\n  T#246(transformer/residual/residual3/conv2/InstanceNorm/batchnorm/mul_2) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#247(transformer/residual/residual3/conv2/InstanceNorm/batchnorm/sub) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#248(transformer/residual/residual3/conv2/InstanceNorm/batchnorm/add_1) shape:[1, 160, 120, 32], type:FLOAT32\r\n  T#249(transformer/residual/residual3/add) shape:[1, 160, 120, 32], type:FLOAT32\r\n  T#250(transformer/residual/residual4/MirrorPad) shape:[1, 162, 122, 32], type:FLOAT32\r\n  T#251(transformer/residual/residual4/conv1/InstanceNorm/mul) shape:[170, 32], type:FLOAT32\r\n  T#252(transformer/residual/residual4/conv1/InstanceNorm/Sum) shape:[1, 32], type:FLOAT32\r\n  T#253(transformer/residual/residual4/conv1/InstanceNorm/ExpandDims_1) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#254(transformer/residual/residual4/conv1/InstanceNorm/mul_1) shape:[170, 32], type:FLOAT32\r\n  T#255(transformer/residual/residual4/conv1/InstanceNorm/Sum_1) shape:[1, 32], type:FLOAT32\r\n  T#256(transformer/residual/residual4/conv1/InstanceNorm/ExpandDims_3) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#257(transformer/residual/residual4/conv1/Conv2D1) shape:[1, 160, 120, 32], type:FLOAT32\r\n  T#258(transformer/residual/residual4/conv1/InstanceNorm/moments/mean) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#259(transformer/residual/residual4/conv1/InstanceNorm/moments/SquaredDifference) shape:[1, 160, 120, 32], type:FLOAT32\r\n  T#260(transformer/residual/residual4/conv1/InstanceNorm/moments/variance) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#261(transformer/residual/residual4/conv1/InstanceNorm/batchnorm/add) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#262(transformer/residual/residual4/conv1/InstanceNorm/batchnorm/Rsqrt) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#263(transformer/residual/residual4/conv1/InstanceNorm/batchnorm/mul) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#264(transformer/residual/residual4/conv1/InstanceNorm/batchnorm/mul_1) shape:[1, 160, 120, 32], type:FLOAT32\r\n  T#265(transformer/residual/residual4/conv1/InstanceNorm/batchnorm/mul_2) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#266(transformer/residual/residual4/conv1/InstanceNorm/batchnorm/sub) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#267(transformer/residual/residual4/conv1/Relu;transformer/residual/residual4/conv1/InstanceNorm/batchnorm/add_1) shape:[1, 160, 120, 32], type:FLOAT32\r\n  T#268(transformer/residual/residual4/MirrorPad_1) shape:[1, 162, 122, 32], type:FLOAT32\r\n  T#269(transformer/residual/residual4/conv2/InstanceNorm/mul) shape:[170, 32], type:FLOAT32\r\n  T#270(transformer/residual/residual4/conv2/InstanceNorm/Sum) shape:[1, 32], type:FLOAT32\r\n  T#271(transformer/residual/residual4/conv2/InstanceNorm/ExpandDims_1) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#272(transformer/residual/residual4/conv2/InstanceNorm/mul_1) shape:[170, 32], type:FLOAT32\r\n  T#273(transformer/residual/residual4/conv2/InstanceNorm/Sum_1) shape:[1, 32], type:FLOAT32\r\n  T#274(transformer/residual/residual4/conv2/InstanceNorm/ExpandDims_3) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#275(transformer/residual/residual4/conv2/Conv2D1) shape:[1, 160, 120, 32], type:FLOAT32\r\n  T#276(transformer/residual/residual4/conv2/InstanceNorm/moments/mean) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#277(transformer/residual/residual4/conv2/InstanceNorm/moments/SquaredDifference) shape:[1, 160, 120, 32], type:FLOAT32\r\n  T#278(transformer/residual/residual4/conv2/InstanceNorm/moments/variance) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#279(transformer/residual/residual4/conv2/InstanceNorm/batchnorm/add) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#280(transformer/residual/residual4/conv2/InstanceNorm/batchnorm/Rsqrt) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#281(transformer/residual/residual4/conv2/InstanceNorm/batchnorm/mul) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#282(transformer/residual/residual4/conv2/InstanceNorm/batchnorm/mul_1) shape:[1, 160, 120, 32], type:FLOAT32\r\n  T#283(transformer/residual/residual4/conv2/InstanceNorm/batchnorm/mul_2) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#284(transformer/residual/residual4/conv2/InstanceNorm/batchnorm/sub) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#285(transformer/residual/residual4/conv2/InstanceNorm/batchnorm/add_1) shape:[1, 160, 120, 32], type:FLOAT32\r\n  T#286(transformer/residual/residual4/add) shape:[1, 160, 120, 32], type:FLOAT32\r\n  T#287(transformer/residual/residual5/MirrorPad) shape:[1, 162, 122, 32], type:FLOAT32\r\n  T#288(transformer/residual/residual5/conv1/InstanceNorm/mul) shape:[170, 32], type:FLOAT32\r\n  T#289(transformer/residual/residual5/conv1/InstanceNorm/Sum) shape:[1, 32], type:FLOAT32\r\n  T#290(transformer/residual/residual5/conv1/InstanceNorm/ExpandDims_1) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#291(transformer/residual/residual5/conv1/InstanceNorm/mul_1) shape:[170, 32], type:FLOAT32\r\n  T#292(transformer/residual/residual5/conv1/InstanceNorm/Sum_1) shape:[1, 32], type:FLOAT32\r\n  T#293(transformer/residual/residual5/conv1/InstanceNorm/ExpandDims_3) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#294(transformer/residual/residual5/conv1/Conv2D1) shape:[1, 160, 120, 32], type:FLOAT32\r\n  T#295(transformer/residual/residual5/conv1/InstanceNorm/moments/mean) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#296(transformer/residual/residual5/conv1/InstanceNorm/moments/SquaredDifference) shape:[1, 160, 120, 32], type:FLOAT32\r\n  T#297(transformer/residual/residual5/conv1/InstanceNorm/moments/variance) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#298(transformer/residual/residual5/conv1/InstanceNorm/batchnorm/add) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#299(transformer/residual/residual5/conv1/InstanceNorm/batchnorm/Rsqrt) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#300(transformer/residual/residual5/conv1/InstanceNorm/batchnorm/mul) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#301(transformer/residual/residual5/conv1/InstanceNorm/batchnorm/mul_1) shape:[1, 160, 120, 32], type:FLOAT32\r\n  T#302(transformer/residual/residual5/conv1/InstanceNorm/batchnorm/mul_2) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#303(transformer/residual/residual5/conv1/InstanceNorm/batchnorm/sub) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#304(transformer/residual/residual5/conv1/Relu;transformer/residual/residual5/conv1/InstanceNorm/batchnorm/add_1) shape:[1, 160, 120, 32], type:FLOAT32\r\n  T#305(transformer/residual/residual5/MirrorPad_1) shape:[1, 162, 122, 32], type:FLOAT32\r\n  T#306(transformer/residual/residual5/conv2/InstanceNorm/mul) shape:[170, 32], type:FLOAT32\r\n  T#307(transformer/residual/residual5/conv2/InstanceNorm/Sum) shape:[1, 32], type:FLOAT32\r\n  T#308(transformer/residual/residual5/conv2/InstanceNorm/ExpandDims_1) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#309(transformer/residual/residual5/conv2/InstanceNorm/mul_1) shape:[170, 32], type:FLOAT32\r\n  T#310(transformer/residual/residual5/conv2/InstanceNorm/Sum_1) shape:[1, 32], type:FLOAT32\r\n  T#311(transformer/residual/residual5/conv2/InstanceNorm/ExpandDims_31) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#312(transformer/residual/residual5/conv2/Conv2D2) shape:[1, 160, 120, 32], type:FLOAT32\r\n  T#313(transformer/residual/residual5/conv2/InstanceNorm/moments/mean) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#314(transformer/residual/residual5/conv2/InstanceNorm/moments/SquaredDifference) shape:[1, 160, 120, 32], type:FLOAT32\r\n  T#315(transformer/residual/residual5/conv2/InstanceNorm/moments/variance) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#316(transformer/residual/residual5/conv2/InstanceNorm/batchnorm/add) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#317(transformer/residual/residual5/conv2/InstanceNorm/batchnorm/Rsqrt) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#318(transformer/residual/residual5/conv2/InstanceNorm/batchnorm/mul) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#319(transformer/residual/residual5/conv2/InstanceNorm/batchnorm/mul_1) shape:[1, 160, 120, 32], type:FLOAT32\r\n  T#320(transformer/residual/residual5/conv2/InstanceNorm/batchnorm/mul_2) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#321(transformer/residual/residual5/conv2/InstanceNorm/batchnorm/sub) shape:[1, 1, 1, 32], type:FLOAT32\r\n  T#322(transformer/residual/residual5/conv2/InstanceNorm/batchnorm/add_1) shape:[1, 160, 120, 32], type:FLOAT32\r\n  T#323(transformer/residual/residual5/add) shape:[1, 160, 120, 32], type:FLOAT32\r\n  T#324(transformer/expand/conv1/ResizeNearestNeighbor) shape:[1, 320, 240, 32], type:FLOAT32\r\n  T#325(transformer/expand/conv1/MirrorPad) shape:[1, 322, 242, 32], type:FLOAT32\r\n  T#326(transformer/expand/conv1/conv/Conv2D2) shape:[1, 320, 240, 16], type:FLOAT32\r\n  T#327(transformer/expand/conv1/conv/InstanceNorm/moments/mean) shape:[1, 1, 1, 16], type:FLOAT32\r\n  T#328(transformer/expand/conv1/conv/InstanceNorm/moments/SquaredDifference) shape:[1, 320, 240, 16], type:FLOAT32\r\n  T#329(transformer/expand/conv1/conv/InstanceNorm/moments/variance) shape:[1, 1, 1, 16], type:FLOAT32\r\n  T#330(transformer/expand/conv1/conv/InstanceNorm/batchnorm/add) shape:[1, 1, 1, 16], type:FLOAT32\r\n  T#331(transformer/expand/conv1/conv/InstanceNorm/batchnorm/Rsqrt) shape:[1, 1, 1, 16], type:FLOAT32\r\n  T#332(transformer/expand/conv1/conv/InstanceNorm/batchnorm/mul) shape:[1, 1, 1, 16], type:FLOAT32\r\n  T#333(transformer/expand/conv1/conv/InstanceNorm/batchnorm/mul_1) shape:[1, 320, 240, 16], type:FLOAT32\r\n  T#334(transformer/expand/conv1/conv/InstanceNorm/batchnorm/mul_2) shape:[1, 1, 1, 16], type:FLOAT32\r\n  T#335(transformer/expand/conv1/conv/InstanceNorm/batchnorm/sub) shape:[1, 1, 1, 16], type:FLOAT32\r\n  T#336(transformer/expand/conv1/conv/Relu;transformer/expand/conv1/conv/InstanceNorm/batchnorm/add_1) shape:[1, 320, 240, 16], type:FLOAT32\r\n  T#337(transformer/expand/conv2/ResizeNearestNeighbor) shape:[1, 640, 480, 16], type:FLOAT32\r\n  T#338(transformer/expand/conv2/MirrorPad) shape:[1, 642, 482, 16], type:FLOAT32\r\n  T#339(transformer/expand/conv2/conv/Conv2D2) shape:[1, 640, 480, 8], type:FLOAT32\r\n  T#340(transformer/expand/conv2/conv/InstanceNorm/moments/mean) shape:[1, 1, 1, 8], type:FLOAT32\r\n  T#341(transformer/expand/conv2/conv/InstanceNorm/moments/SquaredDifference) shape:[1, 640, 480, 8], type:FLOAT32\r\n  T#342(transformer/expand/conv2/conv/InstanceNorm/moments/variance) shape:[1, 1, 1, 8], type:FLOAT32\r\n  T#343(transformer/expand/conv2/conv/InstanceNorm/batchnorm/add) shape:[1, 1, 1, 8], type:FLOAT32\r\n  T#344(transformer/expand/conv2/conv/InstanceNorm/batchnorm/Rsqrt) shape:[1, 1, 1, 8], type:FLOAT32\r\n  T#345(transformer/expand/conv2/conv/InstanceNorm/batchnorm/mul) shape:[1, 1, 1, 8], type:FLOAT32\r\n  T#346(transformer/expand/conv2/conv/InstanceNorm/batchnorm/mul_1) shape:[1, 640, 480, 8], type:FLOAT32\r\n  T#347(transformer/expand/conv2/conv/InstanceNorm/batchnorm/mul_2) shape:[1, 1, 1, 8], type:FLOAT32\r\n  T#348(transformer/expand/conv2/conv/InstanceNorm/batchnorm/sub) shape:[1, 1, 1, 8], type:FLOAT32\r\n  T#349(transformer/expand/conv2/conv/Relu;transformer/expand/conv2/conv/InstanceNorm/batchnorm/add_1) shape:[1, 640, 480, 8], type:FLOAT32\r\n  T#350(transformer/expand/conv3/ResizeNearestNeighbor) shape:[1, 640, 480, 8], type:FLOAT32\r\n  T#351(transformer/expand/conv3/MirrorPad) shape:[1, 648, 488, 8], type:FLOAT32\r\n  T#352(transformer/expand/conv3/conv/Conv2D2) shape:[1, 640, 480, 3], type:FLOAT32\r\n  T#353(transformer/expand/conv3/conv/InstanceNorm/moments/mean) shape:[1, 1, 1, 3], type:FLOAT32\r\n  T#354(transformer/expand/conv3/conv/InstanceNorm/moments/SquaredDifference) shape:[1, 640, 480, 3], type:FLOAT32\r\n  T#355(transformer/expand/conv3/conv/InstanceNorm/moments/variance) shape:[1, 1, 1, 3], type:FLOAT32\r\n  T#356(transformer/expand/conv3/conv/InstanceNorm/batchnorm/add) shape:[1, 1, 1, 3], type:FLOAT32\r\n  T#357(transformer/expand/conv3/conv/InstanceNorm/batchnorm/Rsqrt) shape:[1, 1, 1, 3], type:FLOAT32\r\n  T#358(transformer/expand/conv3/conv/InstanceNorm/batchnorm/mul) shape:[1, 1, 1, 3], type:FLOAT32\r\n  T#359(transformer/expand/conv3/conv/InstanceNorm/batchnorm/mul_1) shape:[1, 640, 480, 3], type:FLOAT32\r\n  T#360(transformer/expand/conv3/conv/InstanceNorm/batchnorm/mul_2) shape:[1, 1, 1, 3], type:FLOAT32\r\n  T#361(transformer/expand/conv3/conv/InstanceNorm/batchnorm/sub) shape:[1, 1, 1, 3], type:FLOAT32\r\n  T#362(transformer/expand/conv3/conv/InstanceNorm/batchnorm/add_1) shape:[1, 640, 480, 3], type:FLOAT32\r\n  T#363(transformer/expand/conv3/conv/Sigmoid) shape:[1, 640, 480, 3], type:FLOAT32\r\n\r\n\r\nYour model looks compatibile with GPU delegate with TFLite runtime version 2.8.0.\r\nBut it doesn't guarantee that your model works well with GPU delegate.\r\nThere could be some runtime incompatibililty happen.\r\n```", "Hello @impjdi , the new version 2.8.0 still have this issue\r\nhere is tflite model to inspect and sample project how to reproduce https://github.com/OleksandrGrument/GpuDelegateBugReproduce/blob/master/app/src/main/assets/model.tflite\r\n", "@impjdi I found a workaround for this issue, I changed tf.math.multiply(weights ,var) to tf.linalg.matmul(weights ,var,transpose_a=True ). Looks like from version 2.4.0 Gpu Delegate can't proceed this operation to multiply 2d arrays with tf.math.multiply. Also I found a some check in model builder that cheking all tensors for batch, it also was added in 2.4.0, I'm not sure what is purpose of this check, but I had to also change start input tensor with 1d array to 2d array with fake batch size. \r\nThere was a lot o changes in 2.4.0 that break working tflite model. I you need some more information I can provide you. Anyway I'm not sure where is the issue on converter side or on GpuDelegate, who should find a solution to fix this behavior.", "Internally (at Google), we don't have those TF versions and we're always using the master branch.  As a non-TF person (believe it or not, I'm not in the TF organization), it's hard for me to keep track of what is in a specific release, or to know how significant of a change a release was.  Having said that, I'm not sure what changes were introduced in 2.4.0, maybe it be in TF, or TFLite, or TFLite GPU, or tflite_convert.  These components are all owned by different teams.\r\n\r\n> I found a some check in model builder that checking all tensors for batch, it also was added in 2.4.0, I'm not sure what is purpose of this check\r\n\r\nFailing for a bad batch size is better than secretly running and producing bad results when the shader is written with a certain assumption.\r\n\r\n> I had to also change start input tensor with 1d array to 2d array with fake batch size.\r\n\r\nAs long as it works, we're absolutely fine with that solution.  In fact, we do that internally all the time too.\r\n\r\n> who should find a solution to fix this behavior.\r\n\r\nIdeally yes, but unfortunately we don't have enough people to keep up with minor issues that can be worked around.", "For OSS releases, always try to use the latest version of TF/TFLite. We patch some fixes to old releases, but not many."]}, {"number": 45838, "title": "Why model can't converge when batch size > 1?", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, but only demo.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 18.04):\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): tf-nightly-2.5.0\r\n- Python version: 3.7.4\r\n- CUDA/cuDNN version: CUDA 11.1 / cuDNN 8.0.4\r\n- GPU model and memory: RTX3090 24GB\r\n- RAM: 32GB\r\n\r\nThis question seems lkie not related to the environment.This is behaviour:\r\n\r\nI wrote a code to build the Faster RCNN model. I train it by set batch size = 1. Model can converge, and predict exactly.\r\nBut when I set batch size = 2 or greater, The faster_rcnn_regr_loss(smooth L1)  can't converge.\r\n\r\nAt first I thought it was my something wrong with the program. However I checked my code, I didn't find any error. So I try to  set batch size = 8 but still backpropagation one sample losses in one training step, like this:\r\n\r\n**Batch size = 8, backpropagation 8 sample average loss in one training step**\r\n**Mode can not converge**\r\n```\r\nloss_class = classifier_train(model_classifier,\r\n                              [image[valid_roi], x_roi],\r\n                              [y_class_label, y_classifier])\r\n```\r\n\r\n**Batch size = 8, backpropagation 1 sample losses in one training step**\r\n**Mode can converge**\r\n```\r\nloss_class = [0, 0]\r\nfor j in range(len(valid_roi)):\r\n    loss = classifier_train(model_classifier,\r\n                            [np.expand_dims(image[valid_roi[j]], axis=0),\r\n                             np.expand_dims(x_roi[j], axis=0)],\r\n                            [np.expand_dims(y_class_label[j], axis=0),\r\n                             np.expand_dims(y_classifier[j], axis=0)])\r\n    loss_class[0] += loss[0]\r\n    loss_class[1] += loss[1]\r\n\r\nloss_class[0] /= len(valid_roi)\r\nloss_class[1] /= len(valid_roi)\r\n```\r\n\r\n**Batch size = 8, backpropagation 1 sample losses 1 times in one training step**\r\n**Mode can converge**\r\n```\r\nloss_class = classifier_train(model_classifier,\r\n                              [np.expand_dims(image[valid_roi[0]], axis=0),\r\n                               np.expand_dims(x_roi[0], axis=0)],\r\n                              [np.expand_dims(y_class_label[0], axis=0),\r\n                               np.expand_dims(y_classifier[0], axis=0)])\r\n```\r\nI think this is a bug.\r\n\r\nThis is my faster_rcnn_regr_loss fuction:\r\n```\r\ndef class_loss_regr(num_classes):\r\n    epsilon = 1e-4\r\n\r\n    def class_loss_regr_fixed_num(y_true, y_pred):\r\n        \"\"\"\r\n\r\n        :param y_true:  [batch_size, num_rois, num_classes * 8]\r\n                        [:, :, :num_classes * 4] is label index, [:, :, num_classes * 4:] is ground true boxes coordinate.\r\n        :param y_pred: [batch_size, num_rois, num_classes * 4]\r\n        :return: classifier regr_loss\r\n        \"\"\"\r\n        regr_loss = 0\r\n        batch_size = len(y_true)\r\n        for i in range(batch_size):\r\n            x = y_true[i, :, 4 * num_classes:] - y_pred[i, :, :]                  \r\n            x_abs = backend.abs(x)                                                  \r\n            x_bool = backend.cast(backend.less_equal(x_abs, 1.0), 'float32')       \r\n\r\n            loss = 4 * backend.sum(\r\n                y_true[i, :, :4 * num_classes] * (x_bool * (0.5 * x * x) + (1 - x_bool) * (x_abs - 0.5))) / backend.sum(\r\n                epsilon + y_true[i, :, :4 * num_classes])\r\n            regr_loss += loss\r\n\r\n        return regr_loss / backend.constant(batch_size)\r\n\r\n    return class_loss_regr_fixed_num\r\n```\r\n\r\nThis is my training code:\r\n```\r\ndef main():\r\n    global rpn_optimizer, classifier_optimizer\r\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\r\n\r\n    gpus = tf.config.experimental.list_physical_devices(\"GPU\")\r\n    if gpus:\r\n        for gpu in gpus:\r\n            tf.config.experimental.set_memory_growth(gpu, True)\r\n\r\n    img_input = Input(shape=(None, None, 3))\r\n    roi_input = Input(shape=(None, 4))\r\n\r\n    share_layer = ResNet50(img_input)\r\n    rpn = frcnn.rpn(share_layer, num_anchors=len(cfg.anchor_box_ratios) * len(cfg.anchor_box_scales))\r\n    classifier = frcnn.classifier(share_layer, roi_input, cfg.num_rois, nb_classes=cfg.num_classes)\r\n\r\n    model_rpn = models.Model(img_input, rpn)\r\n    model_classifier = models.Model([img_input, roi_input], classifier)\r\n    model_all = models.Model([img_input, roi_input], rpn + classifier)\r\n\r\n    anchors = get_anchors(cfg.share_layer_shape, cfg.input_shape)\r\n\r\n    box_parse = BoundingBox(anchors, max_threshold=cfg.rpn_max_overlap, min_threshold=cfg.rpn_min_overlap)\r\n\r\n    reader = DataReader(cfg.annotation_path, box_parse, cfg.batch_size)\r\n    train_data = reader.generate()\r\n    train_step = len(reader.train_lines) // cfg.batch_size\r\n\r\n    losses = np.zeros((train_step, 4))\r\n    best_loss = np.Inf\r\n\r\n    rpn_lr = CosineAnnealSchedule(cfg.epoch, train_step, cfg.rpn_lr_max, cfg.rpn_lr_min)\r\n    cls_lr = CosineAnnealSchedule(cfg.epoch, train_step, cfg.cls_lr_max, cfg.cls_lr_min)\r\n\r\n    rpn_optimizer = optimizers.Adam(rpn_lr)\r\n    classifier_optimizer = optimizers.Adam(cls_lr)\r\n\r\n    for e in range(cfg.epoch):\r\n        invalid_data = 0        \r\n        print(\"Learning rate adjustment, rpn_lr: {}, cls_lr: {}\".\r\n              format(rpn_optimizer._decayed_lr(\"float32\").numpy(),\r\n                     classifier_optimizer._decayed_lr(\"float32\").numpy()))\r\n\r\n\r\n        progbar = utils.Progbar(train_step)\r\n        print('Epoch {}/{}'.format(e+1, cfg.epoch))\r\n        for i in range(train_step):\r\n\r\n            image, rpn_y, bbox = next(train_data)\r\n            loss_rpn = rpn_train(model_rpn, image, rpn_y)\r\n            predict_rpn = model_rpn(image)\r\n\r\n            predict_boxes = box_parse.detection_out(predict_rpn, confidence_threshold=0)\r\n            height, width = np.shape(image[0])[:2]\r\n            x_roi, y_class_label, y_classifier, valid_roi = get_classifier_train_data(predict_boxes,\r\n                                                                                      bbox,\r\n                                                                                      width,\r\n                                                                                      height,\r\n                                                                                      cfg.batch_size,\r\n                                                                                      cfg.num_classes)\r\n\r\n            invalid_data += (cfg.batch_size - len(valid_roi))\r\n            if len(x_roi) == 0:\r\n                progbar.update(i+1, [('rpn_cls', np.mean(losses[:i+1, 0])),\r\n                                     ('rpn_regr', np.mean(losses[:i+1, 1])),\r\n                                     ('detector_cls', np.mean(losses[:i+1, 2])),\r\n                                     ('detector_regr', np.mean(losses[:i+1, 3]))])\r\n                continue\r\n            \r\n            #######you can test code here#########\r\n            loss_class = classifier_train(model_classifier,\r\n                                          [image[valid_roi], x_roi],\r\n                                          [y_class_label, y_classifier])\r\n            #################################\r\n\r\n            losses[i, 0] = loss_rpn[0].numpy()\r\n            losses[i, 1] = loss_rpn[1].numpy()\r\n            losses[i, 2] = loss_class[0].numpy()\r\n            losses[i, 3] = loss_class[1].numpy()\r\n\r\n            progbar.update(i+1, [('rpn_cls', np.mean(losses[:i+1, 0])),\r\n                                 ('rpn_regr', np.mean(losses[:i+1, 1])),\r\n                                 ('detector_cls', np.mean(losses[:i+1, 2])),\r\n                                 ('detector_regr', np.mean(losses[:i+1, 3]))])\r\n\r\n        else:\r\n            loss_rpn_cls = np.mean(losses[:, 0])\r\n            loss_rpn_regr = np.mean(losses[:, 1])\r\n            loss_class_cls = np.mean(losses[:, 2])\r\n            loss_class_regr = np.mean(losses[:, 3])\r\n\r\n            curr_loss = loss_rpn_cls + loss_rpn_regr + loss_class_cls + loss_class_regr\r\n\r\n            print('\\nLoss RPN classifier: {:.4f}'.format(loss_rpn_cls))\r\n            print('Loss RPN regression: {:.4f}'.format(loss_rpn_regr))\r\n            print('Loss Detector classifier: {:.4f}'.format(loss_class_cls))\r\n            print('Loss Detector regression: {:.4f}'.format(loss_class_regr))\r\n            print(\"{} picture can't detect any roi.\".format(invalid_data))\r\n\r\n            print('The best loss is {:.4f}. The current loss is {:.4f}.'.format(best_loss, curr_loss))\r\n            if curr_loss < best_loss:\r\n                best_loss = curr_loss\r\n\r\n            print('Saving weights.\\n')\r\n            model_all.save_weights(\"./logs/model/voc_{:.4f}.h5\".format(curr_loss))\r\n```\r\nI try to show it colab,but I can't download the VOC dataset.And it only occured in my project.I can't write demo to show it because faster_rcnn_regr need rpn predict data.\r\nSo maybe you should download my [code](https://github.com/Runist/Faster_RCNN).And you should do this:\r\n\r\n1. Download the code.\r\n2. Run this code to get voc annotation 'train.txt' file:\r\n```\r\niimport xml.etree.ElementTree as ET\r\nimport os\r\n\r\ndef convert_annotation(image_id, list_file, class_names):\r\n    in_file = open('/content/VOCdevkit/VOC2012/Annotations/%s.xml' % (image_id))\r\n    tree = ET.parse(in_file)\r\n    root = tree.getroot()\r\n\r\n    for obj in root.iter('object'):\r\n\r\n        difficult = obj.find('difficult').text\r\n        cls = obj.find('name').text\r\n\r\n        cls_id = class_names.index(cls)\r\n        xmlbox = obj.find('bndbox')\r\n\r\n        b = (int(xmlbox.find('xmin').text),\r\n            int(xmlbox.find('ymin').text),\r\n            int(xmlbox.find('xmax').text),\r\n            int(xmlbox.find('ymax').text))\r\n\r\n        list_file.write(\" \" + \",\".join([str(a) for a in b]) + ',' + str(cls_id))\r\n\r\n\r\nif __name__ == '__main__':\r\n    label = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog',\r\n         'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\r\n    \r\n    xmlfilepath = '/content/VOCdevkit/VOC2012/Annotations'\r\n    temp_xml = os.listdir(xmlfilepath)\r\n    total_xml = []\r\n\r\n    for xml in temp_xml:\r\n        if xml.endswith(\".xml\"):\r\n            total_xml.append(xml[:-4])\r\n\r\n    files = open('train.txt', 'w')\r\n    for image_id in total_xml:\r\n        files.write('/content/VOCdevkit/VOC2012/JPEGImages/{}.jpg'.format(image_id))\r\n        convert_annotation(image_id, files, label)\r\n        files.write('\\n')\r\n    files.close()\r\n```\r\n3. Move the train.txt to './config/' directory.\r\n4. Edit the './config/config.py' some training paramters.\r\n5. Run 'python train.py'.\r\n\r\n", "comments": []}, {"number": 45822, "title": "[INTEL MKL] Convert necessary PlaceholderWithDefault or Placeholder to Constant f\u2026", "body": "Convert necessary PlaceholderWithDefault or Placeholder to Constant for Inference. \r\n\r\nSome old models have Placeholder / PlaceholderWithDefault for keras_learning_phase  which decide training / inference phase, can be easily converted to Constant when running Inference.\r\nChanges in this PR will help enable further optimizations like ConstantFolding and RemoveDeadBranches() of Merge in loop_optimization grappler pass. \r\nThis change along with further enabled fusions will provide performance improvements projected to be around 25-30% improvements.", "comments": ["@penpornk Can you please review this PR ? Thanks!", "@penpornk Can you please review this PR ? Thanks!", "@penpornk Can you please review this PR ?\r\n\r\n@gbaned - is there someone else who can review?", "@penpornk  Can you please review this PR ? Thanks!", "@penpornk Can you please review this PR ? Thanks!", "@penpornk Can you please review this PR ? Thanks!", "@penpornk Can you please review this PR ? Thanks!"]}, {"number": 45820, "title": "MHLO/LHLO Fusion lower", "body": "It seems mhlo-fusion op is unable to legalize to lhlo\uff0c and lhlo-fusion op is also unable to lower to linalg  dialect. Is there any plan to support the fusion op lower? Linalg fusion on tensor is not perfect for multi-output elemwise ops. Following is a  example\r\n```\r\n// CHECK-LABEL: func @multi_outputs_same_2\r\nfunc @multi_outputs_same_2(%arg0: tensor<?x?xf32>, %arg1: tensor<?x?xf32>) -> (tensor<?x?xf32>, tensor<?x?xf32>, tensor<?x?xf32>) {\r\n  %0 = \"mhlo.abs\"(%arg0) : (tensor<?x?xf32>) -> tensor<?x?xf32>\r\n  %1 = \"mhlo.abs\"(%arg1) : (tensor<?x?xf32>) -> tensor<?x?xf32>\r\n  %2 = \"mhlo.add\"(%0, %1) : (tensor<?x?xf32>, tensor<?x?xf32>) -> tensor<?x?xf32>\r\n  %3 = \"mhlo.abs\"(%0) : (tensor<?x?xf32>) -> tensor<?x?xf32>\r\n  %4 = \"mhlo.abs\"(%1) : (tensor<?x?xf32>) -> tensor<?x?xf32>\r\n  // CHECK: %[[RET:.*]]:3 = \"mhlo.fusion\"\r\n  // CHECK-NEXT: mhlo.abs\r\n  // CHECK-NEXT: mhlo.abs\r\n  // CHECK-NEXT: mhlo.add\r\n  // CHECK-NEXT: mhlo.abs\r\n  // CHECK-NEXT: mhlo.abs\r\n  // CHECK-NEXT: mhlo.return\r\n  return %2, %3, %4 : tensor<?x?xf32>, tensor<?x?xf32>, tensor<?x?xf32>\r\n}\r\n```\r\nwith command`mlir-hlo-opt ./lhlo-test.mlir -hlo-legalize-to-linalg -linalg-bufferize -convert-linalg-to-affine-loops`\uff0cget the following result\r\n```\r\n#map0 = affine_map<(d0, d1) -> (d0, d1)>\r\n#map1 = affine_map<() -> (0)>\r\n#map2 = affine_map<()[s0] -> (s0)>\r\nmodule {\r\n  func @multi_outputs_same_2(%arg0: tensor<?x?xf32>, %arg1: tensor<?x?xf32>) -> (tensor<?x?xf32>, tensor<?x?xf32>, tensor<?x?xf32>) {\r\n    %c0 = constant 0 : index\r\n    %c1 = constant 1 : index\r\n    %0 = tensor_to_memref %arg0 : memref<?x?xf32>\r\n    %1 = dim %arg0, %c0 : tensor<?x?xf32>\r\n    %2 = dim %arg0, %c1 : tensor<?x?xf32>\r\n    %3 = alloc(%1, %2) : memref<?x?xf32>\r\n    %4 = dim %0, %c0 : memref<?x?xf32>\r\n    %5 = dim %0, %c1 : memref<?x?xf32>\r\n    affine.for %arg2 = 0 to %4 {\r\n      affine.for %arg3 = 0 to %5 {\r\n        %18 = affine.load %0[%arg2, %arg3] : memref<?x?xf32>\r\n        %19 = absf %18 : f32\r\n        affine.store %19, %3[%arg2, %arg3] : memref<?x?xf32>\r\n      }\r\n    }\r\n    %6 = tensor_to_memref %arg1 : memref<?x?xf32>\r\n    %7 = dim %arg1, %c0 : tensor<?x?xf32>\r\n    %8 = dim %arg1, %c1 : tensor<?x?xf32>\r\n    %9 = alloc(%7, %8) : memref<?x?xf32>\r\n    %10 = dim %6, %c0 : memref<?x?xf32>\r\n    %11 = dim %6, %c1 : memref<?x?xf32>\r\n    affine.for %arg2 = 0 to %10 {\r\n      affine.for %arg3 = 0 to %11 {\r\n        %18 = affine.load %6[%arg2, %arg3] : memref<?x?xf32>\r\n        %19 = absf %18 : f32\r\n        affine.store %19, %9[%arg2, %arg3] : memref<?x?xf32>\r\n      }\r\n    }\r\n    %12 = alloc(%1, %2) : memref<?x?xf32>\r\n    affine.for %arg2 = 0 to %1 {\r\n      affine.for %arg3 = 0 to %2 {\r\n        %18 = affine.load %3[%arg2, %arg3] : memref<?x?xf32>\r\n        %19 = affine.load %9[%arg2, %arg3] : memref<?x?xf32>\r\n        %20 = addf %18, %19 : f32\r\n        affine.store %20, %12[%arg2, %arg3] : memref<?x?xf32>\r\n      }\r\n    }\r\n    %13 = tensor_load %12 : memref<?x?xf32>\r\n    %14 = alloc(%1, %2) : memref<?x?xf32>\r\n    affine.for %arg2 = 0 to %1 {\r\n      affine.for %arg3 = 0 to %2 {\r\n        %18 = affine.load %3[%arg2, %arg3] : memref<?x?xf32>\r\n        %19 = absf %18 : f32\r\n        affine.store %19, %14[%arg2, %arg3] : memref<?x?xf32>\r\n      }\r\n    }\r\n    %15 = tensor_load %14 : memref<?x?xf32>\r\n    %16 = alloc(%7, %8) : memref<?x?xf32>\r\n    affine.for %arg2 = 0 to %7 {\r\n      affine.for %arg3 = 0 to %8 {\r\n        %18 = affine.load %9[%arg2, %arg3] : memref<?x?xf32>\r\n        %19 = absf %18 : f32\r\n        affine.store %19, %16[%arg2, %arg3] : memref<?x?xf32>\r\n      }\r\n    }\r\n    %17 = tensor_load %16 : memref<?x?xf32>\r\n    return %13, %15, %17 : tensor<?x?xf32>, tensor<?x?xf32>, tensor<?x?xf32>\r\n  }\r\n}\r\n\r\n```\r\n\r\nI expect one affine nest,  but it get three affine nests. Will mhlo-fusion lowering solve this problem, or is there any other way to fuse the three for loops into one?\r\n\r\n", "comments": ["@sherhut hi , can you give me some help on this question?", "If you want to fuse on tensors, you can use `-linalg-fusion-for-tensor-ops` from mlir directly. This would go before the `-linalg-bufferize`. See https://github.com/llvm/llvm-project/blob/main/mlir/test/Dialect/Linalg/fusion-tensor.mlir for an example.\r\n\r\nFor fusion on buffers, you can use `-lho-fuse-linalg` as shown in the example at https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/compiler/mlir/hlo/tests/lhlo-fuse-linalg.mlir. This would go after the `-linalg-bufferize`. \r\n\r\nBoth of these passes work at the LinAlg level and combine the `linalg.generic` operations that come out of the lowering of the `mhlo` operations into a single `linalg.generic`. The version on buffers also does tiling. These fused operations are then mapped to a single `affine.for` nest.\r\n\r\n@bondhugula might have input on how to do loop fusion at the affine level but that likely is not what you want here.\r\n\r\nAs you also mention the `fusion` operation from the `mhlo` and `lmhlo` dialects: These operations are meant to guide fusion. The two passes I have mentioned above are greedy in the sense that they will fuse all operations in the region that are fusible. The idea behind the `fusion` operation is to first group operations that you want fused, then have a pass that fuses them (and drops the fusion operation once it only contains a single operation).\r\n\r\nThere is no ready to use fusion pass for this available at the moment and you would need to write your own.\r\n\r\n", "@feiyuvl  I notice that you have dynamic shapes. The affine loop fusion pass currently works with constant loop bounds - not hard to extend it though. If you replace your dynamic sizes with static ones just to try it out, the `-affine-loop-fusion` will fuse your whole test case into a single affine loop nest. Try:\r\n```\r\ntf-opt -hlo-legalize-to-lhlo -lhlo-legalize-to-linalg -convert-linalg-to-affine-loops -lhlo-legalize-to-affine -canonicalize -affine-loop-fusion\r\n```\r\nyour input is transformed to:\r\n\r\n```\r\n affine.for %arg2 = 0 to 1024 {\r\n      affine.for %arg3 = 0 to 1024 {\r\n        %5 = affine.load %arg0[%arg2, %arg3] : memref<1024x1024xf32>\r\n        %6 = absf %5 : f32\r\n        affine.store %6, %0[0, 0] : memref<1x1xf32>\r\n        %7 = affine.load %arg1[%arg2, %arg3] : memref<1024x1024xf32>\r\n        %8 = absf %7 : f32\r\n        affine.store %8, %1[0, 0] : memref<1x1xf32>\r\n        %9 = affine.load %0[0, 0] : memref<1x1xf32>\r\n        %10 = affine.load %1[0, 0] : memref<1x1xf32>\r\n        %11 = addf %9, %10 : f32\r\n        affine.store %11, %2[%arg2, %arg3] : memref<1024x1024xf32>\r\n        %12 = affine.load %0[0, 0] : memref<1x1xf32>\r\n        %13 = absf %12 : f32\r\n        affine.store %13, %3[%arg2, %arg3] : memref<1024x1024xf32>\r\n        %14 = affine.load %1[0, 0] : memref<1x1xf32>\r\n        %15 = absf %14 : f32\r\n        affine.store %15, %4[%arg2, %arg3] : memref<1024x1024xf32>\r\n      }\r\n    }\r\n```\r\n\r\nYou may want to always run `-memref-dataflow-opt` on the output of fusion (it'll fwd the stores to the loads), and you'll get:\r\n\r\n```\r\n  %0 = alloc() : memref<1024x1024xf32>\r\n    %1 = alloc() : memref<1024x1024xf32>\r\n    %2 = alloc() : memref<1024x1024xf32>\r\n    affine.for %arg2 = 0 to 1024 {\r\n      affine.for %arg3 = 0 to 1024 {\r\n        %3 = affine.load %arg0[%arg2, %arg3] : memref<1024x1024xf32>\r\n        %4 = absf %3 : f32\r\n        %5 = affine.load %arg1[%arg2, %arg3] : memref<1024x1024xf32>\r\n        %6 = absf %5 : f32\r\n        %7 = addf %4, %6 : f32\r\n        affine.store %7, %0[%arg2, %arg3] : memref<1024x1024xf32>\r\n        %8 = absf %4 : f32\r\n        affine.store %8, %1[%arg2, %arg3] : memref<1024x1024xf32>\r\n        %9 = absf %6 : f32\r\n        affine.store %9, %2[%arg2, %arg3] : memref<1024x1024xf32>\r\n      }\r\n    }\r\n```\r\n\r\nI'm sure this is close to the output you (or anyone else :-)) would expect given the original input.\r\n\r\n\r\n", "@sherhut @bondhugula Thank you for your replies. @bondhugula  Loop fusion works for static shapes, however  is there a scheduler to support dynamic shapes?", "> @sherhut @bondhugula Thank you for your replies. @bondhugula Loop fusion works for static shapes, however is there a scheduler to support dynamic shapes?\r\n\r\n`scheduler`? Did you mean a plan to support it? This has come up several times (in posts on the MLIR/LLVM discourse channel and with @dcaballe as well) but I think no one is working actively on it. So, contributions are welcome here.\r\n", ">  I think no one is working actively on it.\r\n\r\nWe did some internal hacks to understand if the core affine fusion algorithm was ready for dynamic shapes and we were able to have some loops fused. I think the code is not reusable, though, but I'll check. Other than that, we are not working on adding support for it right now but I'll be happy to help with the reviews if someone is interested. \r\n\r\n> not perfect for multi-output elemwise ops\r\n\r\nCurrently, there are also some limitations in affine fusion wrt multi-producer loops. However, we are introducing proper support for them here: https://reviews.llvm.org/D92876. Hopefully, we can land this soon :)", "@bondhugula @dcaballe thx\uff0cfor the above example with static shapes,, if i add -buffer-results-to-out-params, it will still produce multiple affine loop. By the way, it seems very diffcult to implement memory plan pass in mlir  which is common in traditional deep learning framework.", "> @bondhugula @dcaballe thx\uff0cfor the above example with static shapes,, if i add -buffer-results-to-out-params, it will still produce multiple affine loop. \r\n\r\nThis issue might go away with Diego's patch that will soon be committed (linked in Diego's post above).\r\n\r\n> By the way, it seems very diffcult to implement memory plan pass in mlir which is common in traditional deep learning \r\n\r\nIt's not clear to me what this means - it looks unrelated to the fusion issue in discussion here.\r\n\r\n\r\n", "> By the way, it seems very diffcult to implement memory plan pass in mlir which is common in traditional deep learning framework.\r\n\r\nCan you elaborate on this? I would be interested in understanding this better.\r\n", "I just committed the fusion patch. Please, give it a try and let me know if it doesn't work. Thanks!", "@dcaballe Thank you, I have tried the new version. For static shape, it handles multi-output well using affine-loop-fusion, but still fails when using -linalg-fusion-for-tensor-ops after the -hlo-legalize-to-linalg pass. \r\n@sherhut What I mean is memory sharing between different tensors,  which is simliar to register allocation. onnx-mlir has implemented a simple version in this pr https://github.com/onnx/onnx-mlir/pull/346", "> @dcaballe Thank you, I have tried the new version. For static shape, it handles multi-output well using affine-loop-fusion, but still fails when using -linalg-fusion-for-tensor-ops after the -hlo-legalize-to-linalg pass.\r\n\r\nGood to hear. Yeah, my patch only covered affine loop fusion."]}, {"number": 45813, "title": "Shape of model output can differ from target data shape", "body": "**System information**\r\n\r\nPython 3.6, Tensorflow 2.3, Windows 10/ Ubuntu 16.04\r\n\r\n**Describe the current behavior**\r\n\r\nIt is possible to train a model with an output shape other than the shape of the target data.\r\nI don\u00b4t know what it\u00b4s doing there (and hope I made no mistake when thinking this trough), but shouldn\u00b4t this cause and error ?\r\nSee Section of how to reproduce.\r\n\r\n**Describe the expected behavior**\r\n\r\nThrow an Error if model outputshape  != target data shape.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n\r\nCan be reproduced using Colab of\r\n\r\nhttps://www.tensorflow.org/tutorials/keras/regression\r\n\r\nas a basis (https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/keras/regression.ipynb,\r\n\r\nhttps://github.com/tensorflow/docs/blob/master/site/en/tutorials/keras/regression.ipynb?short_path=38f0bfa)\r\n\r\nand change Line 574\r\n` \"    layers.Dense(units=1)\\n\",`\r\nto\r\n` \"    layers.Dense(units=3)\\n\",`\r\n\r\n\r\nThe output-shape of the Neural Network is now (batchsize, 3) however it successfully fits target data of of shape (batchsize, 1).\r\n\r\n\r\n", "comments": ["Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/943083f25965f05ae2d8c0134562ec82/45813.ipynb). Thanks!", "@JohannesKnecht It is owing to the fact that the loss used is \"mean_squared_error\".  Try using a different loss function like \"categorical_crossentropy\" , and you will get an error when used with different dimensions. As for the \"mean_squared_error\" loss function , I think it is using the (N, 1) y_true data to compare with each (N,1) in the output of (N,3) y_pred. Hope it helps.", "Was able to replicate the issue in TF 2.6.0-dev20210602,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/4a7ec55884bc4f2a4de625d7b4351cf5/untitled183.ipynb)..Thanks !"]}, {"number": 45795, "title": "Keras add_loss()/kernel_regularizer incompatability", "body": "**System information**\r\n- OS Platform and Distribution : Ubuntu 18.04.5 LTS (Dockerized)\r\n- TensorFlow installed from : Docker tensorflow/tensorflow:latest-gpu-jupyter\r\n- TensorFlow version : 2.3.1\r\n- Python version: 3.6.9\r\n- GPU model and memory: No GPU\r\n\r\n**Describe the current behavior**\r\n\r\nGiven a Model with a Dense layer that includes a kernel_regularizer, using add_loss() and then compiling results in the following error during .fit():  ValueError: Shapes must be equal rank, but are 0 and 1.\r\n\r\nNote: this issue does not manifest when the loss is passed directly to .compile().\r\n\r\n**Describe the expected behavior**\r\n\r\nThe Keras framework should seamlessly combine various loss functions.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nhttps://gist.github.com/TheBeaNerd/e123c0e4a6245c9069bf645da5501fd7", "comments": ["Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/211375a272ab543fe8a2c7610a365560/45795.ipynb). Thanks!", "@TheBeaNerd,\r\nCan you try using the code mentioned in the [Documentation for Add_Loss](https://www.tensorflow.org/guide/keras/custom_layers_and_models#the_add_loss_method) and see if the **`Error`** Persists? Thanks!", "@rmothukuru\r\nCan you clarify which of the following actions you are suggesting?\r\n\r\n1. Perform add_loss() at a Layer level as with the ActivityRegularizationLayer in the referenced documentation\r\n\r\n  - Note that in my current example, add_loss() is being performed at the Model level.\r\n  - Because the loss is between the input and output of the network, it isn't local to a specific Layer.\r\n\r\n2. Perform kernel regularization as with the OuterLayerWithKernelRegularizer in the referenced documentation\r\n\r\n  - I can try that, but it doesn't seem substantially different from the current example.\r\n\r\n\r\n\r\n", "I ran into this issue, too, and would appreciate any help.\r\n\r\nOne issue might be that the regularizer loss (first row) show up as tf-scalars while the `model.add_loss` output (second row) shows up as `(None,)`-size/batch-size vectors? \r\n![Screenshot 2021-03-18 at 5 10 59 PM](https://user-images.githubusercontent.com/3221512/111713481-f49f5580-880c-11eb-89d4-d8ada4e9009f.png)\r\n"]}, {"number": 45794, "title": "[C++] tensorflow::ops::Unstack Documentation and Implementation Mismatch", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.4.0\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): gcc-10\r\n- CUDA/cuDNN version: 11.2 (installed but not used)\r\n- GPU model and memory: MX150\r\n\r\n**Describe the current behavior**\r\nthe input tensor has shape [1,150,150,3]. According to the documentation in the array_ops.h, unstack the input along 0th axis should produce [150,150,3] tensor. However, putting 0 as input cause segmentation fault.\r\n`Segmentation fault (core dumped)`\r\nIf unstack along 1st axis, the output tensor has shape of [150,150,3]\r\n\r\n**Describe the expected behavior**\r\nunstack along 0th axis of a tensor with shape [a,b,c,d] should output a tensor with shape [b,c,d].\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\ntensorflow::ops::Unstack unstack_image_node{load_image_scope_.NewSubScope(\"unstack_image\"), normalize_image_node, 0};\r\noutput_image_tensor_node_ = unstack_image_node[0]; // and then std::cout<<out_tensor.DebugString(); to examine the shape\r\n```\r\n\r\n**Other info / logs**\r\nStart from line 6958 to line 6982 in arrays_ops.h describes the expected behavior of class Unstack, which is basically as same as the documentation of its [python counterpart](https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/stack)\r\n```\r\n/// Unpacks a given dimension of a rank-`R` tensor into `num` rank-`(R-1)` tensors.\r\n///\r\n/// Unpacks `num` tensors from `value` by chipping it along the `axis` dimension.\r\n/// For example, given a tensor of shape `(A, B, C, D)`;\r\n///\r\n/// If `axis == 0` then the i'th tensor in `output` is the slice `value[i, :, :, :]`\r\n///   and each tensor in `output` will have shape `(B, C, D)`. (Note that the\r\n///   dimension unpacked along is gone, unlike `split`).\r\n///\r\n/// If `axis == 1` then the i'th tensor in `output` is the slice `value[:, i, :, :]`\r\n///   and each tensor in `output` will have shape `(A, C, D)`.\r\n/// Etc.\r\n///\r\n/// This is the opposite of `pack`.\r\n///\r\n/// Arguments:\r\n/// * scope: A Scope object\r\n/// * value: 1-D or higher, with `axis` dimension size equal to `num`.\r\n///\r\n/// Optional attributes (see `Attrs`):\r\n/// * axis: Dimension along which to unpack.  Negative values wrap around, so the\r\n/// valid range is `[-R, R)`.\r\n///\r\n/// Returns:\r\n/// * `OutputList`: The list of tensors unpacked from `value`.\r\nclass Unstack {\r\n```\r\nhowever, using the following code with axis = 1\r\n```\r\ntensorflow::ops::Unstack unstack_image_node{\r\n  load_image_scope_.NewSubScope(\"unstack_image\"), normalize_image_node, 1};\r\noutput_image_tensor_node_ = unstack_image_node[0];\r\n```\r\nproduce the expected behavior when axis = 0, which is\r\n```\r\n2020-12-17 11:45:05.233760: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-12-17 11:45:05.252351: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2099940000 Hz\r\nTensor<type: float shape: [1,150,150,3] values: [[[0.0705882385 0.0941176564 0.298039228]]]...>\r\nTensor<type: float shape: [150,150,3] values: [[0.0705882385 0.0941176564 0.298039228]]...>\r\n```\r\nIf axis = 0\r\n```\r\ntensorflow::ops::Unstack unstack_image_node{\r\n  load_image_scope_.NewSubScope(\"unstack_image\"), normalize_image_node, 0};\r\noutput_image_tensor_node_ = unstack_image_node[0];\r\n```\r\nsegmentation fault occurs.\r\n```\r\n2020-12-17 11:38:37.604552: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-12-17 11:38:37.624902: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2099940000 Hz\r\nTensor<type: float shape: [1,150,150,3] values: [[[0.0705882385 0.0941176564 0.298039228]]]...>\r\nSegmentation fault (core dumped)\r\n```", "comments": ["+1 I've noticed the same that the axis parameter doesn't seem to be effective and always points to dimension 0.", "I figured it out how to specify the axis dimension (which default to 0):\r\n```\r\n auto unstack_image_node = tensorflow::ops::Unstack(root.WithOpName(\"unstack_image\"), resized, 3, tensorflow::ops::Unstack::Attrs().Axis(3));\r\n```"]}, {"number": 45793, "title": "[C++] Cannot Construct tensorflow::Status with std::string", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.4.0\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): gcc-10\r\n- CUDA/cuDNN version: 11.2 (installed but NOT used)\r\n- GPU model and memory: MX150\r\n\r\n**Describe the current behavior**\r\nAll the below cannot successfully construct a tensorflow::Status instance\r\n1. ```\r\n    tensorflow::StringPiece err_msg = \"NOT jpg\";\r\n    tensorflow::Status{tensorflow::errors::Code::INVALID_ARGUMENT, err_msg};\r\n    ```\r\n2. `tensorflow::Status{tensorflow::errors::Code::INVALID_ARGUMENT, \"string\"};`\r\n\r\n3. `tensorflow::errors::InvalidArgument(\"Image must be jpeg encoded\");`\r\nOnly this work\r\n`tensorflow::Status::OK();`\r\n\r\nAll of them causes the following error\r\n```\r\n[build] /usr/bin/ld: src/tutorial_02/CatDogCnn/libCatDogCnn.a(CatDogCnn.cpp.o): in function `tensorflow::Status::Status(tensorflow::error::Code, std::basic_string_view<char, std::char_traits<char> >)':\r\n[build] /usr/local/include/tensorflow/bazel-bin/tensorflow/include/tensorflow/core/platform/status.h:54: undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, std::basic_string_view<char, std::char_traits<char> >, std::vector<tensorflow::StackFrame, std::allocator<tensorflow::StackFrame> >&&)'\r\n```\r\n\r\n**Describe the expected behavior**\r\nShould compile without error\r\n\r\n**Standalone code to reproduce the issue**\r\nclass definition \r\n```\r\n#include <tensorflow/cc/client/client_session.h>\r\n#include <tensorflow/cc/framework/gradients.h>\r\n#include <tensorflow/cc/ops/image_ops.h>\r\n#include <tensorflow/cc/ops/standard_ops.h>\r\n#include <tensorflow/cc/tools/freeze_saved_model.h>\r\n#include <tensorflow/core/framework/tensor.h>\r\n#include <tensorflow/core/lib/io/path.h>\r\n#include <tensorflow/core/public/session.h>\r\n#include <tensorflow/core/summary/summary_file_writer.h>\r\n\r\n#include <filesystem>\r\n#include <fstream>\r\n#include <iostream>\r\n#include <string>\r\n#include <vector>\r\nclass CatDogCnn {\r\n public:\r\n  /**\r\n   * @brief Construct a new Cat Dog Cnn object. Assume images are square.\r\n   *\r\n   * @param image_size the size of image\r\n   * @param num_of_channels number of channels\r\n   */\r\n  CatDogCnn(const int image_size, const int num_of_channels);\r\n  /**\r\n   * @brief Create a Graph for Input Image. If unstack is false, the\r\n   * output_image_tensor_node_ will have shape of [batch, width, channel]. If\r\n   * unstack is true, the ouput_image_tensor_node_ will have shape of [batch,\r\n   * height, width, channel]. In addition, only the first element will be stored\r\n   * to output_image_tensor_node_\r\n   *\r\n   * @param unstack ouput tensor or not\r\n   * @return tensorflow::Status\r\n   */\r\n  tensorflow::Status CreateGraphForInputImage(bool unstack);\r\n\r\n  /**\r\n   * @brief Convert an image to a tensor\r\n   *\r\n   * @param path_to_image path to the jpeg file to be converted\r\n   * @param out_tensor pointer to output tensor\r\n   * @return tensorflow::Status\r\n   */\r\n  tensorflow::Status ConvertImageToTensor(\r\n      const std::filesystem::path& path_to_image,\r\n      tensorflow::Tensor* out_tensor);\r\n\r\n private:\r\n  const int kImageSize;      // assume squre picture\r\n  const int kNumOfChannels;  // RGB\r\n  // load image\r\n  tensorflow::Scope load_image_scope_;\r\n  tensorflow::Output input_image_filename_node_;\r\n  tensorflow::Output output_image_tensor_node_;\r\n};\r\n```\r\n\r\nmethods implementation\r\n```\r\nCatDogCnn::CatDogCnn(const int image_size, const int num_of_channels)\r\n    : kImageSize{image_size},\r\n      kNumOfChannels{num_of_channels},\r\n      load_image_scope_{tensorflow::Scope::NewRootScope()} { /*empty*/\r\n}\r\n\r\ntensorflow::Status CatDogCnn::CreateGraphForInputImage(bool unstack) {\r\n  input_image_filename_node_ = tensorflow::ops::Placeholder{\r\n      load_image_scope_.NewSubScope(\"input_file_name\"),\r\n      tensorflow::DataType::DT_STRING};\r\n  tensorflow::ops::ReadFile read_file_node{\r\n      load_image_scope_.NewSubScope(\"read_file\"), input_image_filename_node_};\r\n  tensorflow::ops::DecodeJpeg decode_image_node{\r\n      load_image_scope_.NewSubScope(\"decode_image\"), read_file_node};\r\n  // convert each pixel to float\r\n  tensorflow::ops::Cast cast_float_node{\r\n      load_image_scope_.NewSubScope(\"cast_float\"), decode_image_node,\r\n      tensorflow::DataType::DT_FLOAT};\r\n  // [height, width channel] -> [batch, height, width, channel]\r\n  tensorflow::ops::ExpandDims expand_batch_dim_node{\r\n      load_image_scope_.NewSubScope(\"exapnd_batch_dim\"), cast_float_node, 0};\r\n  // resize image to square\r\n  tensorflow::ops::ResizeBilinear resize_image_node(\r\n      load_image_scope_.NewSubScope(\"resize\"), expand_batch_dim_node,\r\n      {kImageSize, kImageSize});\r\n  // divide each pixel by 255 so that each pixel is [0, 1]\r\n  tensorflow::ops::Div normalize_image_node{\r\n      load_image_scope_.NewSubScope(\"normalize_image\"),\r\n      resize_image_node,\r\n      {255.f}};\r\n  if (unstack) {\r\n    // unstack along height axis\r\n    // array of [batch, width, channel]\r\n    tensorflow::ops::Unstack unstack_image_node{\r\n        load_image_scope_.NewSubScope(\"unstack_image\"), normalize_image_node,\r\n        1};\r\n    output_image_tensor_node_ = unstack_image_node[0];\r\n  } else {\r\n    output_image_tensor_node_ = normalize_image_node;\r\n  }\r\n  return load_image_scope_.status();\r\n}\r\n\r\ntensorflow::Status CatDogCnn::ConvertImageToTensor(\r\n    const std::filesystem::path& path_to_image,\r\n    tensorflow::Tensor* out_tensor) {\r\n  if (load_image_scope_.ok() == false) {\r\n    return load_image_scope_.status();\r\n  }\r\n  if (path_to_image.extension().string() != \".jpg\" ||\r\n      path_to_image.extension().string() != \".jpeg\") {\r\n    tensorflow::StringPiece err_msg = \"NOT jpg\";\r\n    return tensorflow::errors::InvalidArgument(\"Image must be jpeg encoded\");\r\n    // return tensorflow::Status{tensorflow::errors::Code::INVALID_ARGUMENT,\r\n    //                           err_msg,};\r\n    // return tensorflow::Status::OK();\r\n  }\r\n  std::vector<tensorflow::Tensor> out_tensors;\r\n  tensorflow::ClientSession client_session{load_image_scope_};\r\n  TF_CHECK_OK(\r\n      client_session.Run({{input_image_filename_node_, path_to_image.string()}},\r\n                         {output_image_tensor_node_}, &out_tensors));\r\n  (*out_tensor) = std::move(out_tensors[0]);\r\n  return load_image_scope_.status();\r\n}\r\n```\r\nentry point\r\n```\r\n#include <iostream>\r\n#include <tensorflow_tutorial/tutorial_02/CatDogCnn/CatDogCnn.hpp>\r\nint main(int argc, char** argv) {\r\n  if (argc != 2) {\r\n    std::cerr << \"./tutorial_02 /path/to/image\" << std::endl;\r\n    exit(EXIT_FAILURE);\r\n  }\r\n  CatDogCnn cat_dog_model{150, 3};\r\n  tensorflow::Tensor image_tensor;\r\n  cat_dog_model.CreateGraphForInputImage(false);\r\n  cat_dog_model.ConvertImageToTensor(std::filesystem::path{argv[1]},\r\n                                     &image_tensor);\r\n  std::cout << image_tensor.DebugString() << std::endl;\r\n  cat_dog_model.CreateGraphForInputImage(true);\r\n  cat_dog_model.ConvertImageToTensor(std::filesystem::path{argv[1]},\r\n                                     &image_tensor);\r\n  std::cout << image_tensor.DebugString() << std::endl;\r\n  return 0;\r\n}\r\n```\r\nCMakeLists.txt for CatDogCnn\r\n```\r\nadd_library(CatDogCnn STATIC CatDogCnn.cpp)\r\n\r\ntarget_link_libraries(CatDogCnn PUBLIC TensorflowCC::TensorflowCC)\r\n\r\nif(CUDA_FOUND)\r\n  target_link_libraries(CatDogCnn PUBLIC ${CUDA_LIBRARIES})\r\nendif(CUDA_FOUND)\r\n```\r\nCMakeLists.txt for entry point\r\n```\r\nadd_subdirectory(CatDogCnn)\r\n\r\nadd_executable(tutorial_02 tutorial_02.cpp)\r\n\r\ntarget_link_libraries(tutorial_02 PRIVATE CatDogCnn)\r\n\r\ntarget_link_libraries(tutorial_02 PRIVATE TensorflowCC::TensorflowCC)\r\n\r\n# if(CUDA_FOUND)\r\n#   target_link_libraries(tutorial_02 ${CUDA_LIBRARIES})\r\n# endif(CUDA_FOUND)\r\n\r\nset_target_properties(tutorial_02 PROPERTIES\r\n  RUNTIME_OUTPUT_DIRECTORY \"${CMAKE_SOURCE_DIR}/bin/tutorial_02\"\r\n)\r\n```\r\nlibrary TensorflowCC::TensorflowCC is generated by [FloopCZ/tensorflow_cc](https://github.com/FloopCZ/tensorflow_cc/tree/tf-v2.4.0), which compiles and installs Tensorflow with CMake support.\r\nI don't think TensorflowCC is the smoking gun here since tensorflow::Status{} is working. I think there is a missing pieces that needs to be added to create Status instance with std::string\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n[main] Building folder: tensorflow_cpp_tutorial \r\n[build] Starting build\r\n[proc] Executing command: /usr/bin/cmake --build /home/fred/Documents/research/tensorflow_cpp_tutorial/build --config Debug --target all -- -j 10\r\n[build] [1/3  33% :: 11.613] Building CXX object src/tutorial_02/CatDogCnn/CMakeFiles/CatDogCnn.dir/CatDogCnn.cpp.o\r\n[build] [2/3  66% :: 11.648] Linking CXX static library src/tutorial_02/CatDogCnn/libCatDogCnn.a\r\n[build] [3/3 100% :: 14.248] Linking CXX executable ../bin/tutorial_02/tutorial_02\r\n[build] FAILED: ../bin/tutorial_02/tutorial_02 \r\n[build] : && /bin/g++-10  -g   src/tutorial_02/CMakeFiles/tutorial_02.dir/tutorial_02.cpp.o  -o ../bin/tutorial_02/tutorial_02  -Wl,-rpath,/usr/local/lib  src/tutorial_02/CatDogCnn/libCatDogCnn.a  /usr/local/lib/libtensorflow_cc.so.2  -lpthread  /usr/local/cuda/lib64/libcudart_static.a  -lpthread  -ldl  /usr/lib/x86_64-linux-gnu/librt.so && :\r\n[build] /usr/bin/ld: src/tutorial_02/CatDogCnn/libCatDogCnn.a(CatDogCnn.cpp.o): in function `tensorflow::Status::Status(tensorflow::error::Code, std::basic_string_view<char, std::char_traits<char> >)':\r\n[build] /usr/local/include/tensorflow/bazel-bin/tensorflow/include/tensorflow/core/platform/status.h:54: undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, std::basic_string_view<char, std::char_traits<char> >, std::vector<tensorflow::StackFrame, std::allocator<tensorflow::StackFrame> >&&)'\r\n[build] collect2: error: ld returned 1 exit status\r\n[build] ninja: build stopped: subcommand failed.\r\n[build] Build finished with exit code 1", "comments": ["Based on my investigation, `tensorflow::StringPiece` is an alias for `absl::string_view`.\r\nHowever, inside the \"absl/strings/string_view.h\", there is the following code:\r\n```\r\n#ifdef ABSL_USES_STD_STRING_VIEW\r\n\r\n#include <string_view>  // IWYU pragma: export\r\n\r\nnamespace absl {\r\nABSL_NAMESPACE_BEGIN\r\nusing std::string_view;\r\nABSL_NAMESPACE_END\r\n}  // namespace absl\r\n\r\n#else  // ABSL_USES_STD_STRING_VIEW\r\n```\r\n, which redirects the absl::string_view to std::string_view if `ABSL_USES_STD_STRING_VIEW` is being defined.\r\n\r\nI think it is because I am using CXX_STANDARD 20 to compile my code and the `ABSL_USES_STD_STRING_VIEW` is automatically enabled, and therefore causes the whole mess.\r\n\r\nI temporarily resolved issue by adding `#undef ABSL_USES_STD_STRING_VIEW` one line prior to `#ifdef #ABSL_USES_STD_STRING_VIEW`. But I think the library should be modified to be compatible wiht the std::string_view.\r\n\r\nThis problem affects all the classes in the API that uses tensorflow::StringPiece, including tensorflow::Conv2D.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45793\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45793\">No</a>\n"]}, {"number": 45792, "title": "The problem with the application performing speech command recognition on the mobile phone", "body": "Good afternoon. I ran your example on android (https://github.com/tensorflow/examples/tree/master/lite/examples/speech_commands/android) and it works great for me. And I decided to change the buffer type for AudioRecord to Byte, but the application stopped recognizing. If I change the type to short, it works fine. Please tell me what I need to change to make everything work.\r\n```\r\nclass MainActivity : AppCompatActivity() {\r\n\r\n    companion object {\r\n\r\n        private const val TAG = \"MainActivity\"\r\n        private const val SAMPLE_RATE = 16_000\r\n        private const val BUFFER_SIZE_SECONDS = 0.3F\r\n        private const val DETECTION_THRESHOLD = 0.50F\r\n        private const val SUPPRESSION_MS = 1500\r\n        private const val MINIMUM_COUNT = 3\r\n        private const val MINIMUM_TIME_BETWEEN_SAMPLES_MS = 30L\r\n        private const val AVERAGE_WINDOW_DURATION_MS = 1_000L\r\n        private const val REQUEST_AUDIO_RECORD_PERMISSION = 200\r\n    }\r\n\r\n    private val labels = listOf(\"_silence_\", \"_unknown_\", \"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", \"stop\", \"go\")\r\n    private val tfLiteOptions = Interpreter.Options()\r\n    private val recordingBufferLock = ReentrantLock()\r\n\r\n    private var recordingOffset = 0\r\n    private var shouldContinue = true\r\n    private var recordingThread: Thread? = null\r\n    private var shouldContinueRecognition = true\r\n    private var recognitionThread: Thread? = null\r\n    private var recordingBuffer = ByteArray(SAMPLE_RATE)\r\n\r\n    private lateinit var tfLite: Interpreter\r\n    private lateinit var tfLiteModel: MappedByteBuffer\r\n    private lateinit var recognizeCommands: RecognizeCommands\r\n\r\n    override fun onCreate(savedInstanceState: Bundle?) {\r\n        super.onCreate(savedInstanceState)\r\n        setContentView(R.layout.activity_main)\r\n\r\n        recognizeCommands = RecognizeCommands(\r\n            labels,\r\n            AVERAGE_WINDOW_DURATION_MS,\r\n            DETECTION_THRESHOLD,\r\n            SUPPRESSION_MS,\r\n            MINIMUM_COUNT,\r\n            MINIMUM_TIME_BETWEEN_SAMPLES_MS\r\n        )\r\n\r\n        if (ContextCompat.checkSelfPermission(this, Manifest.permission.RECORD_AUDIO) == PermissionChecker.PERMISSION_GRANTED) {\r\n            initTfLite()\r\n        } else {\r\n            ActivityCompat.requestPermissions(this, arrayOf(Manifest.permission.RECORD_AUDIO), REQUEST_AUDIO_RECORD_PERMISSION)\r\n        }\r\n    }\r\n\r\n    private fun initTfLite() {\r\n        try {\r\n            tfLiteModel = loadModelFile()\r\n            tfLite = Interpreter(tfLiteModel, tfLiteOptions)\r\n\r\n            tfLite.resizeInput(0, intArrayOf(SAMPLE_RATE, 1))\r\n            tfLite.resizeInput(1, intArrayOf(1))\r\n\r\n            startRecording()\r\n            startRecognition()\r\n        } catch (exc: IOException) {\r\n            Log.e(TAG, \"Error: ${exc.message}\")\r\n        }\r\n    }\r\n\r\n    private fun startRecording() {\r\n        if (recordingThread == null) {\r\n            shouldContinue = true\r\n            recordingThread = Thread { record() }\r\n            recordingThread?.start()\r\n        }\r\n    }\r\n\r\n    private fun record() {\r\n        Process.setThreadPriority(Process.THREAD_PRIORITY_AUDIO)\r\n\r\n        val bufferSize = (SAMPLE_RATE.toFloat() * BUFFER_SIZE_SECONDS).roundToInt() * 2\r\n        val record = AudioRecord(\r\n            MediaRecorder.AudioSource.DEFAULT,\r\n            SAMPLE_RATE,\r\n            AudioFormat.CHANNEL_IN_MONO,\r\n            AudioFormat.ENCODING_PCM_16BIT,\r\n            bufferSize\r\n        )\r\n\r\n        if (record.state != AudioRecord.STATE_INITIALIZED) {\r\n            Log.e(TAG,\"Audio Record can't initialize!\")\r\n            return\r\n        }\r\n\r\n        record.startRecording()\r\n\r\n        while (shouldContinue) {\r\n            val audioBuffer = ByteArray(bufferSize)\r\n            val numberRead = record.read(audioBuffer, 0, audioBuffer.size)\r\n            val newRecordingOffset = recordingOffset + numberRead\r\n            val secondCopyLength = Math.max(0, newRecordingOffset - recordingBuffer.size)\r\n            val firstCopyLength = numberRead - secondCopyLength\r\n\r\n            recordingBufferLock.lock()\r\n            try {\r\n                System.arraycopy(audioBuffer, 0, recordingBuffer, recordingOffset, firstCopyLength)\r\n                System.arraycopy(audioBuffer, firstCopyLength, recordingBuffer, 0, secondCopyLength)\r\n                recordingOffset = newRecordingOffset % recordingBuffer.size\r\n            } finally {\r\n                recordingBufferLock.unlock()\r\n            }\r\n        }\r\n\r\n        record.stop()\r\n        record.release()\r\n    }\r\n\r\n    private fun startRecognition() {\r\n        if (recognitionThread == null) {\r\n            shouldContinueRecognition = true\r\n            recognitionThread = Thread { recognize() }\r\n            recognitionThread?.start()\r\n        }\r\n    }\r\n\r\n    private fun recognize() {\r\n        val inputBuffer = ByteArray(SAMPLE_RATE)\r\n        val floatInputBuffer = Array(SAMPLE_RATE) { FloatArray(1) }\r\n        val outputScores = Array(1) { FloatArray(labels.size) }\r\n        val sampleRateList = intArrayOf(SAMPLE_RATE)\r\n\r\n        while (shouldContinueRecognition) {\r\n            recordingBufferLock.lock()\r\n\r\n            try {\r\n                val maxLength = recordingBuffer.size\r\n                val firstCopyLength = maxLength - recordingOffset\r\n                val secondCopyLength = recordingOffset\r\n                System.arraycopy(recordingBuffer, recordingOffset, inputBuffer, 0, firstCopyLength)\r\n                System.arraycopy(recordingBuffer, 0, inputBuffer, firstCopyLength, secondCopyLength)\r\n            } finally {\r\n                recordingBufferLock.unlock()\r\n            }\r\n\r\n            for (i in 0 until SAMPLE_RATE) {\r\n                floatInputBuffer[i][0] = inputBuffer[i] / Byte.MAX_VALUE.toFloat()\r\n            }\r\n\r\n            val inputArray = arrayOf<Any>(floatInputBuffer, sampleRateList)\r\n            val outputMap: MutableMap<Int, Any> = HashMap()\r\n            outputMap[0] = outputScores\r\n\r\n            tfLite.runForMultipleInputsOutputs(inputArray, outputMap)\r\n\r\n            val result = recognizeCommands.processLatestResults(outputScores[0], System.currentTimeMillis())\r\n\r\n            if (!result.foundCommand.startsWith(\"_\") && result.isNewCommand) {\r\n                Log.d(TAG, \"Command: ${result.foundCommand} (${result.score})\")\r\n            }\r\n\r\n            try {\r\n                Thread.sleep(MINIMUM_TIME_BETWEEN_SAMPLES_MS)\r\n            } catch (exc: InterruptedException) {\r\n                Log.d(TAG, \"Error: ${exc.message}\")\r\n            }\r\n        }\r\n    }\r\n\r\n    @Throws(IOException::class)\r\n    private fun loadModelFile(): MappedByteBuffer {\r\n        val fileDescriptor = assets.openFd(\"conv_actions_frozen.tflite\")\r\n        val inputStream = FileInputStream(fileDescriptor.fileDescriptor)\r\n        val fileChannel = inputStream.channel\r\n        val startOffset = fileDescriptor.startOffset\r\n        val declaredLength = fileDescriptor.declaredLength\r\n        return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength)\r\n    }\r\n\r\n    override fun onRequestPermissionsResult(\r\n        requestCode: Int,\r\n        permissions: Array<out String>,\r\n        grantResults: IntArray\r\n    ) {\r\n        super.onRequestPermissionsResult(requestCode, permissions, grantResults)\r\n        if (requestCode == REQUEST_AUDIO_RECORD_PERMISSION) {\r\n            initTfLite()\r\n        } else {\r\n            Toast.makeText(this, \"\u041d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u043f\u0440\u0435\u0434\u043e\u0441\u0442\u0430\u0432\u0438\u0442\u044c \u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u0438\u0435\", Toast.LENGTH_LONG).show()\r\n            finish()\r\n        }\r\n    }\r\n}\r\n```", "comments": []}, {"number": 45789, "title": "Invalid index list in batch_scatter_ops_test.py ", "body": "**Describe the current behavior**\r\n\r\nFor current master the tests in batch_scatter_ops_test.py produce an invalid index list/tensor.\r\n\r\nCheck: https://github.com/tensorflow/tensorflow/blob/dec8e0b11f4f87693b67e125e67dfbc68d26c205/tensorflow/python/kernel_tests/batch_scatter_ops_test.py#L59-L62\r\n\r\nThe comment says, non-duplicate values are required but `randint` is used which does produce duplicates. Hence the test fails.\r\n\r\n**Standalone code to reproduce the issue**\r\nA reduced test code which reproduces this on my machine:\r\n\r\n```\r\nimport numpy as np\r\nfrom tensorflow.python.ops import state_ops\r\nfrom tensorflow.python.ops import variables\r\nfrom tensorflow.python.framework import ops\r\n\r\ndef _NumpyUpdate(ref, indices, updates):\r\n  for i, indx in np.ndenumerate(indices):\r\n    indx = i[:-1] + (indx,)\r\n    ref[indx] = updates[i]\r\n\r\ndef _VariableRankTest(vtype, itype):\r\n  np.random.seed(8)\r\n  indices_shape = (2,)\r\n  for extra_shape in (), (5,):\r\n    # Generate random indices with no duplicates for easy numpy comparison\r\n    sparse_dim = len(indices_shape) - 1\r\n    indices = np.random.randint(indices_shape[sparse_dim], size=indices_shape, dtype=itype)\r\n    updates = np.random.randn(*(indices_shape + extra_shape)).astype(vtype)\r\n\r\n    old = np.random.randn(*(indices_shape + extra_shape)).astype(vtype)\r\n    print(\"indices: %s\" % indices)\r\n    if not extra_shape:\r\n      continue\r\n\r\n    # Scatter via numpy\r\n    new = old.copy()\r\n    _NumpyUpdate(new, indices, updates)\r\n    # Scatter via tensorflow\r\n    ref = variables.Variable(old)\r\n    variables.variables_initializer([ref])\r\n\r\n    #state_ops.batch_scatter_update(ref, indices, updates)\r\n    ref.batch_scatter_update(ops.IndexedSlices(indices=indices, values=updates))\r\n    ref = ref.numpy()\r\n    assert np.allclose(ref, new, rtol=1e-6, atol=1e-6), \"Failed:\\nlhs: %s\\nrhs: %s\" % (ref, new)\r\n\r\n_VariableRankTest(np.float32, np.int32)\r\n```\r\n\r\nI see an output of `indices: [1 1]` followed by:\r\n```\r\nAssertionError: Failed:\r\nlhs: [[-0.37835857 -0.79161525  0.8595481  -0.23078899 -0.06566103]\r\n [-2.2964916   2.4098344   1.7278361   2.2045562   0.79482764]]\r\nrhs: [[-0.37835857 -0.79161525  0.8595481  -0.23078899 -0.06566103]\r\n [ 0.9764211  -1.1834271   1.9163636  -1.1233268  -0.6640355 ]]\r\n```\r\n\r\n**Other info / logs**\r\ntensorflow/python/kernel_tests/scatter_ops_test.py contains a valid implementation using `arange` and `shuffle` which would fix the issue\r\n\r\nSide note: the code raises a warning:\r\n> tensorflow/python/ops/resource_variable_ops.py:1124: batch_scatter_update (from tensorflow.python.ops.state_ops) is deprecated and will be removed after 2018-11-29.\r\nInstructions for updating:\r\nUse the batch_scatter_update method of Variable instead.\r\n\r\nNote how the date is well passed but there is not even a way to avoid this as I'm already doing what is suggested.", "comments": ["@Flamefire,\r\nI did not get any `AssertionError` on running the code with TF v2.4 or with TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/393508384b95889b6e6a3d7dffafb133/45789-tf-nightly.ipynb).\r\n\r\nCould you please provide the Python script you are running, so that we can reproduce the issue on our end. Alternatively you can  share a gist of the Colab notebook with us. Thanks!", "Maybe this is only happening when run on GPU as the update happens in parallel there (I suppose), could you retry with GPU?\r\n\r\nAlso: As the comment states unique indices are required. As the output shows the indices are `[1,1]` which are clear non-unique. Doesn't this suffice already?", "Was able to reproduce the issue with [TF v2.4](https://colab.research.google.com/gist/amahendrakar/38ff325c100f418d4cf8cd65a52a9112/45789-2-4.ipynb) on GPU. However, I did not face the `AssertionError` with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/421ed59f7646fc5f2a3f12c95ddc8715/45789-2-3.ipynb). \r\n\r\nColab does not detect GPU with TF-nightly. Please check the linked gist for reference. Thanks!", "@Flamefire, I tried to run your code on `Colab` using `TF v2.4`, `2.5`, `2.6` and `tf-nightly`. I didn't face the error reported above using `GPU`. Please find the [gist](https://colab.research.google.com/gist/chunduriv/019c409e88cbc300403e176e5cb18ede/untitled33.ipynb) here for reference and confirm the same. Could you please let us know if the issue is resolved for you ?Thanks!\r\n\r\n\r\n", "@chunduriv As explained above the issue is (very likely) a concurrency issue i.e. the error depends on the (random) execution order of the parallel update operation. As also shown by your colab and explained above the test is clearly wrong in not producing unique indices.\r\nSo the test is still wrong hence the issue is not resolved as the code at https://github.com/tensorflow/tensorflow/blob/ff9eb3563a112d2caa33b7c161ccf89fa02f66a9/tensorflow/python/kernel_tests/batch_scatter_ops_test.py#L61-L62 is still unchanged.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 45777, "title": "[Documentation bug] Format issue in document of `tf.raw_ops.For`", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/raw_ops/For\r\n\r\n## Description of issue (what needs changing):\r\nThe signature of the function has format issue\r\n![image](https://user-images.githubusercontent.com/24580222/102424794-f1ac8300-3fd9-11eb-81d1-72dec90f96f1.png)\r\n\r\n## System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS Mojave 10.14\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 2.3.0\r\n- **Python version**: 3.7.6", "comments": ["hello @yashk2810 and @lamberta \r\ncan anyone please review this PR?\r\nThanks"]}, {"number": 45759, "title": "int8 quantization with int16 activations support for TFMicro and Cortex-M", "body": "Hello community,\r\n\r\nThere is a support of int16 activations in TFLite, which helps a lot in keeping precision of an int8-quantized network, but not in TFMicro.\r\n\r\nIt should be noted that the TFMicro users are ones who will benefit from this features the most, because the platforms where one can run standard TFLite interpreter are often powerful enough/have enough memory so that things can often be done without any quantization at all, so this may not bring as much added value for the deployment on TFLite-compatible systems in this case(but it definitely does for research).\r\n\r\nFor TFMicro int16 activations support is absolutely paramount.\r\n\r\nJust wondering if there is a plan to put this feature into production soon?\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):\r\nTF 2.4rc3\r\n\r\n- Are you willing to contribute it (Yes/No):\r\nNo\r\n\r\n**Describe the feature and the current behavior/state.**\r\nWhen the model with int8 weights and int16 activations is run with TFMicro interpreter the following error is observed: \"Node X failed to prepare\"\r\n\r\n**Will this change the current api? How?**\r\nNo, this shouldn't change APIs. It is already supported by TFLite but not by TFMicro.\r\n\r\n**Who will benefit with this feature?**\r\nTFMicro users will benefit from this feature.\r\n\r\n**Any Other info.**\r\n", "comments": []}, {"number": 45756, "title": "SVD Function has (very) different GPU and CPU outputs ", "body": "------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: No\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Colab Notebook \r\n-   **TensorFlow installed from (source or binary)**: pip install v2.3.0\r\n-   **Python version**: Colab Default 3.6.9\r\n-   **CUDA/cuDNN version**: 10.1, V10.1.243\r\n-   **GPU model and memory**: Colab Default GPU (Persistence-M) ~15GB\r\n-   **Exact command to reproduce**:\r\n```python\r\nimport tensorflow as tf \r\nimport numpy as np\r\ntf.debugging.set_log_device_placement(True)\r\nprint(tf.__version__)\r\n\r\nA= tf.random.uniform((50,9))\r\n\r\nwith tf.device('cpu:0'):\r\n    _ ,_, v = tf.linalg.svd(A, full_matrices=True, compute_uv=True)\r\n    res_cpu= v[0]\r\n    \r\nwith tf.device('gpu:0'):\r\n    _ ,_, v = tf.linalg.svd(A, full_matrices=True, compute_uv=True)\r\n    res_gpu= v[0]\r\n    \r\nnp.array_equal(res_cpu,res_gpu) #this will return False but expected should be true- \r\nnp.allclose(res_cpu,res_gpu) #this will also return False but expected should be true- \r\n```\r\n\r\n### Describe the problem\r\nComputing SVD (singular value decomp) operation on gpu matches numpy results more closely and more accurately. The cpu implementation is far off (sometimes gives nans.) I presume there maybe a different solver used but there doesn't seem to be much explanation on that in source. I would like to use the CPU method in my training pipeline because I cannot afford any more GPU cost. \r\n\r\n", "comments": ["@eddymina \r\n\r\nI have tried in TF nightly version (`2.5.0-dev20201216`) and i am not seeing any issue. Please, find the [gist here](https://colab.research.google.com/gist/ravikyram/6c7ae99c1520ede3d96bb8beaf5bd3bd/untitled580.ipynb).Please, verify once and close the issue.Thanks!", "@ravikyram In your gist, the operations in TF nightly are executed always on CPU:\r\n```\r\n2.5.0-dev20201216\r\nExecuting op RandomUniform in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op Svd in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op StridedSlice in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op Svd in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op StridedSlice in device /job:localhost/replica:0/task:0/device:CPU:0\r\n```\r\nTherefore, the results are not conclusive.", "Note that according to https://github.com/googlecolab/colabtools/issues/1574 , Colab does not offer CUDA 11, so TF 2.4 and TF nightly does not use GPUs on colab.\r\n\r\nThat can be verified by using\r\n```python\r\nprint(tf.test.gpu_device_name())\r\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\n```\r\nwhich indeed shows\r\n```\r\n\r\nNum GPUs Available:  0\r\n```\r\non TF 2.4.", "@ravikyram I would prefer to use tf 2.3 if possible as I am unsure what differences and other potential bugs exist with bleeding edge tf and @foxik brings up a good point as to why. ", "Still an issue in TF 2.5 stable version on GPU. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/bc9b27b6f0a297bee1b59061169a1762/untitled89.ipynb).Thanks!"]}, {"number": 45748, "title": "Passing tensorflow::ops::DecodeJpeg::Attrs to the tensorflow::ops::DecodeJpeg constructor causes std::logic_error", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): It's custom code. However, it's based on this [article](https://itnext.io/creating-a-tensorflow-dnn-in-c-part-1-54ce69bbd586).\r\n- OS Platform and Distribution: Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.4\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): gcc-10\r\n- CUDA/cuDNN version: CUDA 11.2 (installed but not used)\r\n- GPU model and memory: NVIDIA GeForce MX150\r\n\r\n\r\n**Describe the current behavior**\r\nPassing tensorflow::ops::DecodeJpeg::Attrs to the tensorflow::ops::DecodeJpeg constructor causes std::logic_error\r\n\r\nconsole output\r\n```\r\nterminate called after throwing an instance of 'std::logic_error'\r\n  what():  basic_string::_M_construct null not valid\r\nAborted (core dumped)\r\n```\r\n\r\n\r\n**Describe the expected behavior**\r\nExit without error.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\ntensorflow::Status ReadTensorFromImageFile(\r\n    const std::string& file_name, const int input_height, const int input_width,\r\n    const float input_mean, const float input_std,\r\n    std::vector<tensorflow::Tensor>* out_tensors, bool writeGraph) {\r\n  // build the graph\r\n  tensorflow::Scope root = tensorflow::Scope::NewRootScope();\r\n  tensorflow::ops::Placeholder input_file_name{root.WithOpName(\"input\"),\r\n                                               tensorflow::DataType::DT_STRING};\r\n  tensorflow::ops::ReadFile file_reader{root.WithOpName(\"file_reader\"),\r\n                                        input_file_name};\r\n  constexpr int kNumOfChannels = 3;\r\n  tensorflow::ops::DecodeJpeg decode_file{\r\n      root.WithOpName(\"decode_file\"), file_reader,\r\n      tensorflow::ops::DecodeJpeg::Channels(kNumOfChannels)};\r\n```\r\nExit without error iff tensorflow::ops::DecodeJpeg::Attrs is not used.\r\n```\r\ntensorflow::ops::DecodeJpeg decode_file{root.WithOpName(\"decode_file\"), file_reader};\r\n```\r\n\r\n**Other info / logs**\r\ngdb backtrace\r\n```\r\n[New Thread 0x7fffe8839700 (LWP 104836)]\r\nterminate called after throwing an instance of 'std::logic_error'\r\n  what():  basic_string::_M_construct null not valid\r\n\r\nThread 1 \"tutorial_01\" received signal SIGABRT, Aborted.\r\n__GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:50\r\n50      ../sysdeps/unix/sysv/linux/raise.c: No such file or directory.\r\n(gdb) backtrace \r\n#0  __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:50\r\n#1  0x00007fffe8a2b859 in __GI_abort () at abort.c:79\r\n#2  0x00007fffe8cb1951 in ?? () from /lib/x86_64-linux-gnu/libstdc++.so.6\r\n#3  0x00007fffe8cbd47c in ?? () from /lib/x86_64-linux-gnu/libstdc++.so.6\r\n#4  0x00007fffe8cbd4e7 in std::terminate() () from /lib/x86_64-linux-gnu/libstdc++.so.6\r\n#5  0x00007fffe8cbd799 in __cxa_throw () from /lib/x86_64-linux-gnu/libstdc++.so.6\r\n#6  0x00007fffe8cb425e in std::__throw_logic_error(char const*) () from /lib/x86_64-linux-gnu/libstdc++.so.6\r\n#7  0x00007fffeb6b316c in void std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_construct<char const*>(char const*, char const*, std::forward_iterator_tag) ()\r\n   from /usr/local/lib/libtensorflow_cc.so.2\r\n#8  0x00007ffff5617b4c in tensorflow::SetAttrValue(absl::lts_2020_02_25::string_view, tensorflow::AttrValue*) () from /usr/local/lib/libtensorflow_cc.so.2\r\n#9  0x00007ffff551fc13 in tensorflow::NodeDefBuilder::Attr(absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view) () from /usr/local/lib/libtensorflow_cc.so.2\r\n#10 0x00007fffebceff2a in tensorflow::ops::DecodeJpeg::DecodeJpeg(tensorflow::Scope const&, tensorflow::Input, tensorflow::ops::DecodeJpeg::Attrs const&) ()\r\n   from /usr/local/lib/libtensorflow_cc.so.2\r\n#11 0x000055555555a095 in ReadTensorFromImageFile (file_name=\"../data/grace_hopper.jpg\", input_height=299, input_width=299, input_mean=0, input_std=255, out_tensors=0x7fffffffd610, \r\n    writeGraph=true) at ../src/tutorial_01/tutorial_01.cpp:48\r\n#12 0x0000555555559d11 in main (argc=2, argv=0x7fffffffd788) at ../src/tutorial_01/tutorial_01.cpp:22\r\n```", "comments": []}, {"number": 45745, "title": "TF Lite for MCU porting, why not provide the generating the static library flow.", "body": "\r\nWhy not provide the generating the static library flow and link with the MCU tools environment. In this way, we only need to focus on the model generation, instead of compiling the source code for examples again and again.\r\n\r\nCould anybody tell me the reason? Thanks so much.\r\n\r\n", "comments": []}, {"number": 45717, "title": "Different fill value for different channel with ImageProjectiveTransformV3", "body": "**System information**\r\n- TensorFlow version (you are using): 2.4\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nCurrently, `tf.raw_ops.ImageProjectiveTransformV3` accepts a scalar `fill_value` that fill out of bound pixel with `fill_value` when `fill_mode` is \"constant\". However, in some tensorflow repos like tensorflow-addons and tensorflow-models have some additional functionality to suppport different fill value for different channel. It's doable and less burden to do such thing in the C++ ops by checking if fill_value is either a scalar or a vector of #channels. With pure C++ implementation, we do not need compositions of python ops, which is suppposed to be slow.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/image/image_ops.cc#L88-L96\r\nhttps://github.com/tensorflow/models/blob/master/research/object_detection/utils/autoaugment_utils.py#L1189-L1224\r\nhttps://github.com/tensorflow/addons/blob/master/tensorflow_addons/image/utils.py#L112-L151\r\n\r\n**Will this change the current api? How?**\r\n\r\nNo. Only change underlying implementation.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nUsers of Tensorflow KPL layers, addons and models.\r\n\r\n**Any Other info.**\r\n", "comments": ["Hi @tanzhenyu, is there any suggestion to this? Thannk you!", "> Hi @tanzhenyu, is there any suggestion to this? Thannk you!\r\n\r\nscalar/vector checking at OpKernel level sounds good to me. Please file a PR"]}, {"number": 45687, "title": "StringLookup on GPU and disable_eager_mode gives incosistent results", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Unknown\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: release 10.1, V10.1.243\r\n- GPU model and memory: \r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nWhen using StringLookup and GPU and disable_eager_mode, on each run I get different results - accuracy, (minimal reproducible code).\r\nOn training, the trained model don't converge.\r\nOn CPU, or when eager mode in enabled, I get consistent results.\r\n\r\n**Describe the expected behavior**\r\n\r\nconsistent accuracy - results among runs (on GPU)\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nupdated link:\r\nhttps://colab.research.google.com/gist/IsaacDayan/85c8c7492b6a2f80857ee1795f1d8e45/stringlookupdiscrepancyaccuracybetweeneagerandgraphminimal.ipynb\r\n\r\n\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@IsaacDayan \r\n\r\nWill it be possible to share the supporting files to reproduce the issue. It helps us in localizing the issue faster. Thanks!", "@ravikyram \r\n\r\nUpdated colab . Now it download files from github\r\n\r\nhttps://colab.research.google.com/gist/IsaacDayan/85c8c7492b6a2f80857ee1795f1d8e45/stringlookupdiscrepancyaccuracybetweeneagerandgraphminimal.ipynb\r\nThanks!"]}, {"number": 45676, "title": "Tensorflow 2.4 takes 3 seconds per epoch during training versus 1 second with TensorFlow 2.3", "body": "Tensorflow 2.4 takes 3 seconds per epoch during training versus 1 second with TensorFlow 2.3\r\n\r\nWhy is is slower??\r\n\r\nWhen I train, I get this standard log (in case it's helful)\r\n\r\n`2020-12-14 18:34:49.552097: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-12-14 18:34:49.572089: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2020-12-14 18:34:49.671571: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:83:00.0 name: TITAN V computeCapability: 7.0\r\ncoreClock: 1.455GHz coreCount: 80 deviceMemorySize: 12.00GiB deviceMemoryBandwidth: 607.97GiB/s\r\n2020-12-14 18:34:49.671911: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-12-14 18:34:50.350471: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-12-14 18:34:50.350673: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2020-12-14 18:34:50.456348: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2020-12-14 18:34:50.513180: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2020-12-14 18:34:50.881235: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2020-12-14 18:34:51.192188: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2020-12-14 18:34:51.216230: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2020-12-14 18:34:51.216525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2020-12-14 18:34:51.627424: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:83:00.0 name: TITAN V computeCapability: 7.0\r\ncoreClock: 1.455GHz coreCount: 80 deviceMemorySize: 12.00GiB deviceMemoryBandwidth: 607.97GiB/s\r\n2020-12-14 18:34:51.627803: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-12-14 18:34:51.627987: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-12-14 18:34:51.628149: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2020-12-14 18:34:51.628318: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2020-12-14 18:34:51.628495: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2020-12-14 18:34:51.628657: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2020-12-14 18:34:51.628843: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2020-12-14 18:34:51.629021: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2020-12-14 18:34:51.629239: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2020-12-14 18:34:52.792115: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-12-14 18:34:52.792321: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n2020-12-14 18:34:52.792441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n2020-12-14 18:34:52.792890: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10243 MB memory) -> physical GPU (device: 0, name: TITAN V, pci bus id: 0000:83:00.0, compute capability: 7.0)\r\n2020-12-14 18:34:52.830535: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set`\r\n\r\n<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below):\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.0 / 8.02\r\n- GPU model and memory: NVIDIA Titan V 12 GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@nectario \r\n\r\nPlease, share colab link or simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "@ravikyram \r\n\r\nHere is a Google Colab with the code that exhibits this issue:\r\n\r\nhttps://colab.research.google.com/drive/1E_IZ3zmPrQHbuiYl47gvL591hHfS-VkN#scrollTo=L5nAouOwIcnE\r\n\r\n**Please Note:** This needs to run on a V100 GPU to recreate this issue. With TF 2.3 each epoch takes about 1 second versus 3 seconds with TF 2.4", "Going from 1 second per epoch to 3 seconds, is a huge deal. Especially since my model is trained on AWS,  which means I now  experienced a 300% rise in costs...", "@nectario \r\n\r\nPlease, grant me the access for colab link. Thanks!", "Just did!", "@nectario \r\nWill it be possible to share supporting data files(`Eg:stock_data_df.pkl `) to reproduce the issue.Thanks!", "Here you go:\r\n\r\n[supporting_files.zip](https://github.com/tensorflow/tensorflow/files/5703675/supporting_files.zip)\r\n\r\nUnzip the file to get all supporting files.\r\n\r\n", "Hi @nectario, does this reproduce on CPU also? Or just GPU?\r\nAdditionally, I tried to reproduce this with 1 V100. I copied over the notebook and the supporting files folder to my vm but I get an error in model.fit: \r\n```\r\nEpoch 1/100\r\n2020-12-21 22:21:41.411410: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11\r\n2020-12-21 22:21:41.742131: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8\r\n2020-12-21 22:21:42.059676: F tensorflow/stream_executor/cuda/cuda_dnn.cc:1186] Check failed: cudnnSetRNNMatrixMathType(rnn_desc.get(), math_type) == CUDNN_STATUS_SUCCESS (3 vs. 0)\r\nAborted\r\n```", "Not sure if this is relevant to CPU as it takes 10 times longer, and I never measured the CPU times from 2.3.0 vs 2.4.0.\r\n\r\nI am not sure about the above error. Are you using the correct CUDA/CUDNN versions?", "With GPU is 1 second vs 3 seconds.", "Hmm not sure what is causing the `Check failed` error message. \r\n\r\nAt any rate, I ran the code with 2.4 and I'm seeing 2 seconds:\r\n\r\n<img width=\"1647\" alt=\"Screen Shot 2020-12-21 at 6 06 25 PM\" src=\"https://user-images.githubusercontent.com/16965738/102833679-5a6b7500-43b7-11eb-89e9-cb052eae4102.png\">\r\n", "It used to be 1 second. That is still 50% more. Can you run it with 2.3?", "The key thing with this issue is the timings between 2.3 and 2.4. If both timings are equal, there is no issue. If 2.3 is faster, that is the issue that needs to be looked into.", "Yes, I am trying to run it with 2.3 but I'm getting the error message I shared earlier. I need to debug that first.", "CUDA and CUDNN versions for 2.3 are different.", "Btw, it has been very difficult to train this model till completion. Was getting similar issues you are getting which I posted in a different thread a few months back.", "Adding this code at the start, dropped my per epoch time to 1 second!\r\n\r\n```\r\nfrom tensorflow.keras.mixed_precision import experimental as mixed_precision\r\npolicy = mixed_precision.Policy('mixed_float16')\r\nmixed_precision.set_policy(policy)\r\n```", "@nectario I ran the colab code locally with both TF2.3.1 and TF2.4 and interestingly enough didn't observe that much of a difference.\r\n\r\n**Original (batch_size=600 to fit my gpu)**\r\n__tf2.4__\r\n2s epoch, 316ms/step\r\n\r\n__tf2.3.1__\r\n2s epoch, 295ms/step\r\n\r\n**Mixed Precision (batch_size=620)**\r\n__tf2.4__\r\n1s epoch, ~190ms/step \r\n1s epoch, ~170ms/step (increased LSTM unit from 150 to 152 to have x%8=0)\r\n\r\n__tf2.3.1__\r\n1s epoch, ~232ms/step (default XLA settings)\r\n1s epoch, ~171ms/step (default XLA settings, increased LSTM unit from 150 to 152 to have x%8=0)\r\n2s epoch, ~235ms/step (XLA disabled with `tf.config.optimizer.set_jit(False)`)\r\n\r\nIt looks like one thing different between TF2.4 and TF2.3.1 is XLA no longer being enabled by default. Although not sure how much of it's automated magic was enabled in 2.3.1. This can be seen also from the TF \"startup\" log. Windows and XLA in TF2.4 doesn't seem to play along that well at the moment (`TF_XLA_FLAGS=--tf_xla_enable_xla_devices --tf_xla_auto_jit=fusible` causing `tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 0` and eventual hanging).\r\n\r\nTF2.3.1: Win 10, graphics driver, 460.89, CUDA 10.1, cudnn-7.6.5.32\r\nTF2.4: Win 10, graphics driver, 460.89, CUDA 11.0, cudnn-8.0.5.39\r\n\r\nPossibly not relevant but mentioning just to be sure: I do have Hardware-accelerated GPU scheduling enabled in Windows.\r\nhttps://devblogs.microsoft.com/directx/hardware-accelerated-gpu-scheduling/ (having it off was claimed to be causing some other issues with the newer cards at https://github.com/tensorflow/tensorflow/issues/45716)", "Nice, thank you @ahtik This is interesting.\r\n\r\nI just ran a 1500 epochs training and finished fine with no crashing with TF 2.4 (this is in reference to the crashing issue). Although, each epoch takes 3 seconds for me and has all your optimizations (mixed precision and LSTM output at 152).\r\n\r\n```\r\nEpoch 1500/1500\r\n7/7 - 3s - loss: 5.3210e-04 - daily_loss: 2.0926e-05 - weekly_loss: 4.1037e-05 - monthly_loss: 6.4028e-05 - three_month_loss: 1.6983e-04 - yearly_loss: 2.0743e-04 - direction_loss: 4.8010e-05 - daily_rmse: 0.0046 - daily_direction_accuracy: 0.8102 - weekly_rmse: 0.0064 - weekly_direction_accuracy: 0.8785 - monthly_rmse: 0.0080 - monthly_direction_accuracy: 0.9457 - three_month_rmse: 0.0130 - three_month_direction_accuracy: 0.9472 - yearly_rmse: 0.0144 - yearly_direction_accuracy: 0.9809 - direction_daily_accuracy: 1.0000 - val_loss: 3.2617 - val_daily_loss: 2.3174e-04 - val_weekly_loss: 7.3195e-04 - val_monthly_loss: 0.0015 - val_three_month_loss: 9.3365e-04 - val_yearly_loss: 0.0014 - val_direction_loss: 5.4297 - val_daily_rmse: 0.0152 - val_daily_direction_accuracy: 0.4769 - val_weekly_rmse: 0.0271 - val_weekly_direction_accuracy: 0.5692 - val_monthly_rmse: 0.0393 - val_monthly_direction_accuracy: 0.7077 - val_three_month_rmse: 0.0306 - val_three_month_direction_accuracy: 0.9077 - val_yearly_rmse: 0.0376 - val_yearly_direction_accuracy: 0.8615 - val_direction_daily_accuracy: 0.5385\r\n{'loss': 0.0005321009666658938, 'daily_loss': 2.0926070646964945e-05, 'weekly_loss': 4.1037234041141346e-05, 'monthly_loss': 6.402765575330704e-05, 'three_month_loss': 0.0001698337437119335, 'yearly_loss': 0.0002074312506010756, 'direction_loss': 4.8009744205046445e-05, 'daily_rmse': 0.004576211795210838, 'daily_direction_accuracy': 0.8101887702941895, 'weekly_rmse': 0.006405814550817013, 'weekly_direction_accuracy': 0.8784587383270264, 'monthly_rmse': 0.008001040667295456, 'monthly_direction_accuracy': 0.9456943273544312, 'three_month_rmse': 0.013031532056629658, 'three_month_direction_accuracy': 0.9472459554672241, 'yearly_rmse': 0.014403595589101315, 'yearly_direction_accuracy': 0.9808636903762817, 'direction_daily_accuracy': 1.0, 'val_loss': 3.26171875, 'val_daily_loss': 0.00023174285888671875, 'val_weekly_loss': 0.0007319450378417969, 'val_monthly_loss': 0.0015411376953125, 'val_three_month_loss': 0.0009336471557617188, 'val_yearly_loss': 0.0014104843139648438, 'val_direction_loss': 5.4296875, 'val_daily_rmse': 0.015223219990730286, 'val_daily_direction_accuracy': 0.4769230782985687, 'val_weekly_rmse': 0.027050945907831192, 'val_weekly_direction_accuracy': 0.5692307949066162, 'val_monthly_rmse': 0.039255402982234955, 'val_monthly_direction_accuracy': 0.7076923251152039, 'val_three_month_rmse': 0.03055468387901783, 'val_three_month_direction_accuracy': 0.9076923131942749, 'val_yearly_rmse': 0.037555813789367676, 'val_yearly_direction_accuracy': 0.8615384697914124, 'val_direction_daily_accuracy': 0.5384615659713745}\r\nWeights File: /tmp/validation_weights/DJI/val_daily_return_dir_accuracy-0.615_val_weekly_return_dir_accuracy-0.569_val_monthly_return_dir_accuracy-0.677_val_three_month_return_dir_accuracy-0.923_val_daily_rmse-0.014_val_direction_accuracy-0.585_epoch-1059.h5\r\n```", "@ahtik Now it seems with or without your optimization suggestions, it takes 3 seconds per epoch.", "Is this 100% the same code as in colab, other than the renamed losses? Output looks a bit different, for example no step time info?\r\n\r\nMy 150 epoch run finished in ~4 minutes (`0:04:13`), (mixed precision, LSTM with 152, tf2.4, everything else default):\r\n```\r\nEpoch 150/150\r\n7/7 [==============================] - 1s 175ms/step - loss: -3.8389 - dense_loss: 2.2464e-04 - dense_1_loss: 5.3154e-04 - dense_2_loss: 6.7071e-04 - dense_3_loss: 7.6445e-04 - dense_4_loss: 8.4391e-04 - dense_5_loss: -6.3981 - dense_rmse: 0.0150 - dense_direction_accuracy: 0.5012 - dense_1_rmse: 0.0231 - dense_1_direction_accuracy: 0.6162 - dense_2_rmse: 0.0259 - dense_2_direction_accuracy: 0.8292 - dense_3_rmse: 0.0276 - dense_3_direction_accuracy: 0.9069 - dense_4_rmse: 0.0290 - dense_4_direction_accuracy: 0.9085 - dense_5_daily_accuracy: 0.0000e+00\r\n```\r\n\r\nRTX 2070 Super", "@ahtik The code in colab is different than the other code. This one is a bit more elaborate but same core architecture.", "@ahtik I renamed the outputs to more generic names.", "[new_data.zip](https://github.com/tensorflow/tensorflow/files/5736862/new_data.zip)\r\n\r\n@ahtik With this data, I get 3 seconds per epoch.", "With the `new_data`, batch_size=600 (to fit my gpu in all scenarios), 150 epochs, RTX2070S.\r\n\r\nI'm a bit curious, why your log output does not show step timing like `177ms/step`?\r\n\r\nTF 2.4, LSTM 152, Mixed Precision, ~4.25 minutes (0:04:14)\r\nEpoch 150/150\r\n`7/7 [==============================] - 1s 177ms/step - loss: -3.9754 - dense_loss: 1.9943e-04 - dense_1_loss: 4.2309e-04 - dense_2_loss: 5.6400e-04 - dense_3_loss: 6.8907e-04 - dense_4_loss: 6.8543e-04 - dense_5_loss: -6.6267 - dense_rmse: 0.0141 - dense_direction_accuracy: 0.5165 - dense_1_rmse: 0.0206 - dense_1_direction_accuracy: 0.6749 - dense_2_rmse: 0.0237 - dense_2_direction_accuracy: 0.8393 - dense_3_rmse: 0.0262 - dense_3_direction_accuracy: 0.9117 - dense_4_rmse: 0.0262 - dense_4_direction_accuracy: 0.9108 - dense_5_daily_accuracy: 0.0000e+00`\r\n\r\nTF 2.4, LSTM 152, Regular (\"unmixed\"), ~6.75 minutes (0:06:46)\r\nEpoch 150/150\r\n`7/7 [==============================] - 2s 327ms/step - loss: -3.7065 - dense_loss: 2.2837e-04 - dense_1_loss: 5.4915e-04 - dense_2_loss: 6.1801e-04 - dense_3_loss: 0.0012 - dense_4_loss: 0.0011 - dense_5_loss: -6.1837 - dense_rmse: 0.0151 - dense_direction_accuracy: 0.4959 - dense_1_rmse: 0.0234 - dense_1_direction_accuracy: 0.6139 - dense_2_rmse: 0.0249 - dense_2_direction_accuracy: 0.8466 - dense_3_rmse: 0.0343 - dense_3_direction_accuracy: 0.8807 - dense_4_rmse: 0.0336 - dense_4_direction_accuracy: 0.8896 - dense_5_daily_accuracy: 0.0000e+00`", "@ahtik I have never seen the ms portion when I train. Don't ever remember seeing it. I just see the seconds portion...", "TF 2.3.1, LSTM 152, Mixed Precision, ~4.5 minutes (0:04:29)\r\nEpoch 150/150\r\n`7/7 [==============================] - 1s 164ms/step - loss: -3.7234 - dense_loss: 2.0466e-04 - dense_1_loss: 5.1873e-04 - dense_2_loss: 5.6820e-04 - dense_3_loss: 8.1529e-04 - dense_4_loss: 7.8634e-04 - dense_5_loss: -6.2101 - dense_rmse: 0.0143 - dense_direction_accuracy: 0.4912 - dense_1_rmse: 0.0228 - dense_1_direction_accuracy: 0.6092 - dense_2_rmse: 0.0238 - dense_2_direction_accuracy: 0.8431 - dense_3_rmse: 0.0286 - dense_3_direction_accuracy: 0.9141 - dense_4_rmse: 0.0280 - dense_4_direction_accuracy: 0.9176 - dense_5_daily_accuracy: 0.0000e+00`\r\n\r\nTF 2.3.1, LSTM 152, Regular (\"unmixed\"), ~6 minutes\r\nEpoch 150/150\r\n`7/7 [==============================] - 2s 303ms/step - loss: -3.7612 - dense_loss: 1.9700e-04 - dense_1_loss: 4.7340e-04 - dense_2_loss: 6.3550e-04 - dense_3_loss: 8.6339e-04 - dense_4_loss: 7.9784e-04 - dense_5_loss: -6.2736 - dense_rmse: 0.0140 - dense_direction_accuracy: 0.5128 - dense_1_rmse: 0.0218 - dense_1_direction_accuracy: 0.6382 - dense_2_rmse: 0.0252 - dense_2_direction_accuracy: 0.8350 - dense_3_rmse: 0.0294 - dense_3_direction_accuracy: 0.9080 - dense_4_rmse: 0.0282 - dense_4_direction_accuracy: 0.9141 - dense_5_daily_accuracy: 0.0000e+00`\r\n\r\nI wish I knew what else to look for to figure out the difference in setup?", "@ahtik You have an RTX 3070 and I have a Titan V. It may be because the RTX has big differences in the architecture compared to the older Titan V.", "Mine is RTX 2070 Super, a bit older than 3070. Compute capability 7.5. I do \nhave gtx 1080 with Ubuntu somewhere on the cloud...\n\nOn December 23, 2020 10:15:47 PM Nektarios Kalogridis \n<notifications@github.com> wrote:\n>\n> @ahtik You have an RTX 3070 and I have a Titan V. It may be because the RTX \n> has big differences in the architecture compared to the older Titan V.\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or unsubscribe.\n\n", "> Adding this code at the start, dropped my per epoch time to 1 second!\r\n> \r\n> ```\r\n> from tensorflow.keras.mixed_precision import experimental as mixed_precision\r\n> policy = mixed_precision.Policy('mixed_float16')\r\n> mixed_precision.set_policy(policy)\r\n> ```\r\n\r\nIs it possible that you were using mixed precision in 2.3 but not in 2.4?\r\n\r\nCC @reedwm ", "@sanjoy Actually I take it back. At the moment with that enabled and not, I get 3 seconds per epoch. There was a discrepancy in the amount of data earlier when I observed this. I used the same data when I measured this now which is this:\r\n[new_data.zip](https://github.com/tensorflow/tensorflow/files/5736862/new_data.zip)", "@sanjoy @ahtik \r\n\r\nI just downgraded to TF 2.3.1 with CUDA 10.1/CUDNN 7.6\r\n\r\nTraining the same data takes 2 seconds per epoch versus 3 seconds per epoch on TF 2.4\r\n\r\nSo:\r\n\r\n**TF 2.4.0**: 3 seconds\r\n**TF 2.3.1**: 2 seconds\r\n\r\nBoth setups were identical. I simply downgraded the TF and CUDA versions.\r\n\r\n```\r\nEpoch 198/1500\r\n{'loss': 0.4114486277103424, 'daily_loss': 0.00019866823276970536, 'weekly_loss': 0.0008702529012225568, 'monthly_loss': 0.0005495575023815036, 'three_month_loss': 0.0010519209317862988, 'yearly_loss': 0.0016778710996732116, 'direction_loss': 0.6785006523132324, 'daily_rmse': 0.014094972051680088, 'daily_direction_accuracy': 0.49185416102409363, 'weekly_rmse': 0.029500048607587814, 'weekly_direction_accuracy': 0.4587535560131073, 'monthly_rmse': 0.023442642763257027, 'monthly_direction_accuracy': 0.839927613735199, 'three_month_rmse': 0.03243333101272583, 'three_month_direction_accuracy': 0.8919058442115784, 'yearly_rmse': 0.04096182435750961, 'yearly_direction_accuracy': 0.9420739412307739, 'direction_daily_accuracy': 0.5637444853782654, 'val_loss': 0.42898598313331604, 'val_daily_loss': 0.00019334981334395707, 'val_weekly_loss': 0.0019688063766807318, 'val_monthly_loss': 0.0023292433470487595, 'val_three_month_loss': 0.001890965853817761, 'val_yearly_loss': 0.0008342369110323489, 'val_direction_loss': 0.7029489278793335, 'val_daily_rmse': 0.013905028812587261, 'val_daily_direction_accuracy': 0.446153849363327, 'val_weekly_rmse': 0.04437123239040375, 'val_weekly_direction_accuracy': 0.3384615480899811, 'val_monthly_rmse': 0.04826223477721214, 'val_monthly_direction_accuracy': 0.6769230961799622, 'val_three_month_rmse': 0.043485235422849655, 'val_three_month_direction_accuracy': 0.892307698726654, 'val_yearly_rmse': 0.028883159160614014, 'val_yearly_direction_accuracy': 0.8153846263885498, 'val_direction_daily_accuracy': 0.5076923370361328}\r\n7/7 - 2s - loss: 0.4114 - daily_loss: 1.9867e-04 - weekly_loss: 8.7025e-04 - monthly_loss: 5.4956e-04 - three_month_loss: 0.0011 - yearly_loss: 0.0017 - direction_loss: 0.6785 - daily_rmse: 0.0141 - daily_direction_accuracy: 0.4919 - weekly_rmse: 0.0295 - weekly_direction_accuracy: 0.4588 - monthly_rmse: 0.0234 - monthly_direction_accuracy: 0.8399 - three_month_rmse: 0.0324 - three_month_direction_accuracy: 0.8919 - yearly_rmse: 0.0410 - yearly_direction_accuracy: 0.9421 - direction_daily_accuracy: 0.5637 - val_loss: 0.4290 - val_daily_loss: 1.9335e-04 - val_weekly_loss: 0.0020 - val_monthly_loss: 0.0023 - val_three_month_loss: 0.0019 - val_yearly_loss: 8.3424e-04 - val_direction_loss: 0.7029 - val_daily_rmse: 0.0139 - val_daily_direction_accuracy: 0.4462 - val_weekly_rmse: 0.0444 - val_weekly_direction_accuracy: 0.3385 - val_monthly_rmse: 0.0483 - val_monthly_direction_accuracy: 0.6769 - val_three_month_rmse: 0.0435 - val_three_month_direction_accuracy: 0.8923 - val_yearly_rmse: 0.0289 - val_yearly_direction_accuracy: 0.8154 - val_direction_daily_accuracy: 0.5077\r\nEpoch 199/1500\r\n{'loss': 0.41261425614356995, 'daily_loss': 0.00021032587392255664, 'weekly_loss': 0.000582002685405314, 'monthly_loss': 0.0005726460367441177, 'three_month_loss': 0.0009110110113397241, 'yearly_loss': 0.0015689559513702989, 'direction_loss': 0.681282103061676, 'daily_rmse': 0.014502615667879581, 'daily_direction_accuracy': 0.5011637210845947, 'weekly_rmse': 0.024124732241034508, 'weekly_direction_accuracy': 0.5919317007064819, 'monthly_rmse': 0.023930024355649948, 'monthly_direction_accuracy': 0.844840943813324, 'three_month_rmse': 0.030182959511876106, 'three_month_direction_accuracy': 0.8952676653862, 'yearly_rmse': 0.03961004689335823, 'yearly_direction_accuracy': 0.9475045204162598, 'direction_daily_accuracy': 0.5549521446228027, 'val_loss': 0.412332147359848, 'val_daily_loss': 0.00014936340448912233, 'val_weekly_loss': 0.0008999488200061023, 'val_monthly_loss': 0.002337433397769928, 'val_three_month_loss': 0.0011293195420876145, 'val_yearly_loss': 0.0006307148723863065, 'val_direction_loss': 0.6786422729492188, 'val_daily_rmse': 0.01222143229097128, 'val_daily_direction_accuracy': 0.4615384638309479, 'val_weekly_rmse': 0.0299991462379694, 'val_weekly_direction_accuracy': 0.5846154093742371, 'val_monthly_rmse': 0.04834701120853424, 'val_monthly_direction_accuracy': 0.6153846383094788, 'val_three_month_rmse': 0.03360534831881523, 'val_three_month_direction_accuracy': 1.0, 'val_yearly_rmse': 0.025114037096500397, 'val_yearly_direction_accuracy': 0.9076923131942749, 'val_direction_daily_accuracy': 0.5692307949066162}\r\n7/7 - 2s - loss: 0.4126 - daily_loss: 2.1033e-04 - weekly_loss: 5.8200e-04 - monthly_loss: 5.7265e-04 - three_month_loss: 9.1101e-04 - yearly_loss: 0.0016 - direction_loss: 0.6813 - daily_rmse: 0.0145 - daily_direction_accuracy: 0.5012 - weekly_rmse: 0.0241 - weekly_direction_accuracy: 0.5919 - monthly_rmse: 0.0239 - monthly_direction_accuracy: 0.8448 - three_month_rmse: 0.0302 - three_month_direction_accuracy: 0.8953 - yearly_rmse: 0.0396 - yearly_direction_accuracy: 0.9475 - direction_daily_accuracy: 0.5550 - val_loss: 0.4123 - val_daily_loss: 1.4936e-04 - val_weekly_loss: 8.9995e-04 - val_monthly_loss: 0.0023 - val_three_month_loss: 0.0011 - val_yearly_loss: 6.3071e-04 - val_direction_loss: 0.6786 - val_daily_rmse: 0.0122 - val_daily_direction_accuracy: 0.4615 - val_weekly_rmse: 0.0300 - val_weekly_direction_accuracy: 0.5846 - val_monthly_rmse: 0.0483 - val_monthly_direction_accuracy: 0.6154 - val_three_month_rmse: 0.0336 - val_three_month_direction_accuracy: 1.0000 - val_yearly_rmse: 0.0251 - val_yearly_direction_accuracy: 0.9077 - val_direction_daily_accuracy: 0.5692\r\nEpoch 200/1500\r\n{'loss': 0.417228639125824, 'daily_loss': 0.00025663431733846664, 'weekly_loss': 0.0010450570844113827, 'monthly_loss': 0.0007138350629247725, 'three_month_loss': 0.0010068160481750965, 'yearly_loss': 0.002297411672770977, 'direction_loss': 0.686514675617218, 'daily_rmse': 0.016019809991121292, 'daily_direction_accuracy': 0.49211275577545166, 'weekly_rmse': 0.03232734277844429, 'weekly_direction_accuracy': 0.5898629426956177, 'monthly_rmse': 0.026717692613601685, 'monthly_direction_accuracy': 0.7913110852241516, 'three_month_rmse': 0.03173036500811577, 'three_month_direction_accuracy': 0.8929402828216553, 'yearly_rmse': 0.04793132096529007, 'yearly_direction_accuracy': 0.9330230355262756, 'direction_daily_accuracy': 0.5358158946037292, 'val_loss': 0.4141582250595093, 'val_daily_loss': 0.00014011705934535712, 'val_weekly_loss': 0.0018782862462103367, 'val_monthly_loss': 0.0017431046580895782, 'val_three_month_loss': 0.0013259940315037966, 'val_yearly_loss': 0.0009762737900018692, 'val_direction_loss': 0.6801573634147644, 'val_daily_rmse': 0.011837105266749859, 'val_daily_direction_accuracy': 0.4769230782985687, 'val_weekly_rmse': 0.04333920031785965, 'val_weekly_direction_accuracy': 0.32307693362236023, 'val_monthly_rmse': 0.041750505566596985, 'val_monthly_direction_accuracy': 0.6769230961799622, 'val_three_month_rmse': 0.03641420230269432, 'val_three_month_direction_accuracy': 1.0, 'val_yearly_rmse': 0.031245380640029907, 'val_yearly_direction_accuracy': 0.8153846263885498, 'val_direction_daily_accuracy': 0.5538461804389954}\r\n7/7 - 2s - loss: 0.4172 - daily_loss: 2.5663e-04 - weekly_loss: 0.0010 - monthly_loss: 7.1384e-04 - three_month_loss: 0.0010 - yearly_loss: 0.0023 - direction_loss: 0.6865 - daily_rmse: 0.0160 - daily_direction_accuracy: 0.4921 - weekly_rmse: 0.0323 - weekly_direction_accuracy: 0.5899 - monthly_rmse: 0.0267 - monthly_direction_accuracy: 0.7913 - three_month_rmse: 0.0317 - three_month_direction_accuracy: 0.8929 - yearly_rmse: 0.0479 - yearly_direction_accuracy: 0.9330 - direction_daily_accuracy: 0.5358 - val_loss: 0.4142 - val_daily_loss: 1.4012e-04 - val_weekly_loss: 0.0019 - val_monthly_loss: 0.0017 - val_three_month_loss: 0.0013 - val_yearly_loss: 9.7627e-04 - val_direction_loss: 0.6802 - val_daily_rmse: 0.0118 - val_daily_direction_accuracy: 0.4769 - val_weekly_rmse: 0.0433 - val_weekly_direction_accuracy: 0.3231 - val_monthly_rmse: 0.0418 - val_monthly_direction_accuracy: 0.6769 - val_three_month_rmse: 0.0364 - val_three_month_direction_accuracy: 1.0000 - val_yearly_rmse: 0.0312 - val_yearly_direction_accuracy: 0.8154 - val_direction_daily_accuracy: 0.5538\r\nEpoch 201/1500\r\n{'loss': 0.41835010051727295, 'daily_loss': 0.0002527496835682541, 'weekly_loss': 0.0012075683334842324, 'monthly_loss': 0.0007212499622255564, 'three_month_loss': 0.001039160997606814, 'yearly_loss': 0.004198853857815266, 'direction_loss': 0.6848840713500977, 'daily_rmse': 0.015898102894425392, 'daily_direction_accuracy': 0.5024566650390625, 'weekly_rmse': 0.034750085324048996, 'weekly_direction_accuracy': 0.4915955662727356, 'monthly_rmse': 0.02685609646141529, 'monthly_direction_accuracy': 0.826997697353363, 'three_month_rmse': 0.032236021012067795, 'three_month_direction_accuracy': 0.887251079082489, 'yearly_rmse': 0.06479856371879578, 'yearly_direction_accuracy': 0.8704422116279602, 'direction_daily_accuracy': 0.5492630004882812, 'val_loss': 0.416068434715271, 'val_daily_loss': 0.00016652456542942673, 'val_weekly_loss': 0.0010100970976054668, 'val_monthly_loss': 0.002100505866110325, 'val_three_month_loss': 0.0014713642885908484, 'val_yearly_loss': 0.0015848802868276834, 'val_direction_loss': 0.6828917264938354, 'val_daily_rmse': 0.012904440052807331, 'val_daily_direction_accuracy': 0.446153849363327, 'val_weekly_rmse': 0.03178202360868454, 'val_weekly_direction_accuracy': 0.4307692348957062, 'val_monthly_rmse': 0.04583127424120903, 'val_monthly_direction_accuracy': 0.6461538672447205, 'val_three_month_rmse': 0.03835836797952652, 'val_three_month_direction_accuracy': 0.9692307710647583, 'val_yearly_rmse': 0.039810553193092346, 'val_yearly_direction_accuracy': 0.9076923131942749, 'val_direction_daily_accuracy': 0.5538461804389954}\r\n7/7 - 2s - loss: 0.4184 - daily_loss: 2.5275e-04 - weekly_loss: 0.0012 - monthly_loss: 7.2125e-04 - three_month_loss: 0.0010 - yearly_loss: 0.0042 - direction_loss: 0.6849 - daily_rmse: 0.0159 - daily_direction_accuracy: 0.5025 - weekly_rmse: 0.0348 - weekly_direction_accuracy: 0.4916 - monthly_rmse: 0.0269 - monthly_direction_accuracy: 0.8270 - three_month_rmse: 0.0322 - three_month_direction_accuracy: 0.8873 - yearly_rmse: 0.0648 - yearly_direction_accuracy: 0.8704 - direction_daily_accuracy: 0.5493 - val_loss: 0.4161 - val_daily_loss: 1.6652e-04 - val_weekly_loss: 0.0010 - val_monthly_loss: 0.0021 - val_three_month_loss: 0.0015 - val_yearly_loss: 0.0016 - val_direction_loss: 0.6829 - val_daily_rmse: 0.0129 - val_daily_direction_accuracy: 0.4462 - val_weekly_rmse: 0.0318 - val_weekly_direction_accuracy: 0.4308 - val_monthly_rmse: 0.0458 - val_monthly_direction_accuracy: 0.6462 - val_three_month_rmse: 0.0384 - val_three_month_direction_accuracy: 0.9692 - val_yearly_rmse: 0.0398 - val_yearly_direction_accuracy: 0.9077 - val_direction_daily_accuracy: 0.5538\r\nEpoch 202/1500\r\n```", "@nectario, have you tried collecting [TensorBoard profiles](https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras) for 2.4 and 2.3 to see if something jumps out?", "> @nectario, have you tried collecting [TensorBoard profiles](https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras) for 2.4 and 2.3 to see if something jumps out?\r\n\r\nI can try doing this.", "Is there any luck with this issue? Will it be addressed in any of the upcoming releases?", "Hello, this is also a relevant issue to me. I have a personal rtx 2070 super and each epoch takes around 5s\r\n\r\nthese are the logs\r\n\r\n2021-04-01 20:03:46.712589: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2.4.1\r\n2021-04-01 20:03:48.725664: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-04-01 20:03:48.727030: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2021-04-01 20:03:48.759345: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2070 Super computeCapability: 7.5\r\ncoreClock: 1.38GHz coreCount: 40 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2021-04-01 20:03:48.759725: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-04-01 20:03:48.765863: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-04-01 20:03:48.766005: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-04-01 20:03:48.768815: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-04-01 20:03:48.769891: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-04-01 20:03:48.774016: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-04-01 20:03:48.776639: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-04-01 20:03:48.777397: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-04-01 20:03:48.777662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-04-01 20:03:48.778345: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-04-01 20:03:48.779352: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2070 Super computeCapability: 7.5\r\ncoreClock: 1.38GHz coreCount: 40 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2021-04-01 20:03:48.779854: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-04-01 20:03:48.780161: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-04-01 20:03:48.780471: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-04-01 20:03:48.780708: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-04-01 20:03:48.780930: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-04-01 20:03:48.781206: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-04-01 20:03:48.781430: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-04-01 20:03:48.781575: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-04-01 20:03:48.781744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-04-01 20:03:49.268423: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-04-01 20:03:49.268576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n2021-04-01 20:03:49.268669: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n2021-04-01 20:03:49.268882: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6611 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070 Super, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2021-04-01 20:03:49.269599: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n...\r\n1875/1875 [==============================] - 7s 3ms/step - loss: 0.5143 - accuracy: 0.8183\r\nEpoch 2/100\r\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.2595 - accuracy: 0.9065\r\nEpoch 3/100\r\n...."]}, {"number": 45668, "title": "Multi-GPU selection in map_fn and vectorized_map", "body": "**System information**\r\n- TensorFlow version (you are using): 2.3.1\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Current Behaviour**\r\nCurrently, both these functions run on a single GPU, even on a multi-GPU machine. This restricts the maximim number of parallel iterations (map_fn) and the amount of memory available for mapping (both).\r\n\r\n**Will this change the current api? How?**\r\nThis will add another parameter to `tf.map_fn` and `tf.vectorized_map` that specifies the GPUs to use for parallelizing. By default, all visible GPUs will be used.\r\n\r\n**Who will benefit with this feature?**\r\nAnyone having a setup with two or more GPUs can benefit from this feature.\r\n\r\n**Any Other info.**\r\nFor simple parallelism, this may even make it possible to call a model on multiple GPUs without GPU Strategy APIs.", "comments": ["@Susmit-A,\r\nSorry for the delayed response.  Can you please refer to the documentation of [Distribute Strategy](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy) and let us know if it helps? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 45664, "title": "Out of memory in some tests due to GPU memory limit confusion", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.4.0rc4\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source): 3.4.1\r\n- GCC/Compiler version (if compiling from source): GCC 8.3.0\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: V100\r\n\r\n**Describe the current behavior**\r\n\r\nI have V100 GPUs with ~32GB memory. During startup of the test (many tests show this) I see lines like \r\n` W tensorflow/stream_executor/stream_executor_pimpl.cc:490] Not enough memory to allocate 31614597888 on device 0 within provided limit. [used=0, limit=1073741824]`\r\n\r\nSome tests then fail after allocating about 1GB of memory trying to allocate more. The failure message includes the 31GB and shows almost 1GB as used.\r\nE.g. //tensorflow/python/keras/applications:applications_test or //tensorflow/python/keras/layers:convolutional_recurrent_test\r\n\r\n**Standalone code to reproduce the issue**\r\nRun bazel test\r\n\r\n**Other info / logs**\r\n```\r\n2020-12-14 12:27:32.863559: W tensorflow/stream_executor/stream_executor_pimpl.cc:490] Not enough memory to allocate 30637607680 on device 0 within provided limit. [used=0, limit=1073741824]\r\n2020-12-14 12:27:32.863607: W tensorflow/stream_executor/stream_executor_pimpl.cc:490] Not enough memory to allocate 30637607680 on device 0 within provided limit. [used=0, limit=1073741824]\r\n2020-12-14 12:27:42.864373: W tensorflow/stream_executor/stream_executor_pimpl.cc:490] Not enough memory to allocate 30637607680 on device 0 within provided limit. [used=0, limit=1073741824]\r\n2020-12-14 12:27:42.864404: W tensorflow/stream_executor/stream_executor_pimpl.cc:490] Not enough memory to allocate 30637607680 on device 0 within provided limit. [used=0, limit=1073741824]\r\n2020-12-14 12:27:42.864421: W tensorflow/core/common_runtime/bfc_allocator.cc:433] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.00MiB (rounded to 4194304)requested by op Tanh\r\nCurrent allocation summary follows.\r\n2020-12-14 12:27:42.864434: I tensorflow/core/common_runtime/bfc_allocator.cc:972] BFCAllocator dump for GPU_0_bfc\r\n...\r\n2020-12-14 12:27:42.866982: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Sum Total of in-use chunks: 928.00MiB\r\n2020-12-14 12:27:42.866989: I tensorflow/core/common_runtime/bfc_allocator.cc:1042] total_region_allocated_bytes_: 976990208 memory_limit_: 31614597888 available bytes: 30637607680 curr_region_allocation_bytes_: 63229195776\r\n2020-12-14 12:27:42.867000: I tensorflow/core/common_runtime/bfc_allocator.cc:1048] Stats: \r\nLimit:                     31614597888\r\nInUse:                       973083648\r\nMaxInUse:                    973083648\r\nNumAllocs:                        2238\r\nMaxAllocSize:                 86081536\r\nReserved:                            0\r\nPeakReserved:                        0\r\nLargestFreeBlock:                    0\r\n\r\n2020-12-14 12:27:42.867013: W tensorflow/core/common_runtime/bfc_allocator.cc:441] ****************************************************************************************************\r\n2020-12-14 12:27:42.867035: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at cwise_op_gpu_base.cc:97 : Resource exhausted: OOM when allocating tensor with shape[32,32,32,32] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n\r\n```", "comments": ["@Flamefire,\r\nIn order to expedite the trouble-shooting process, could you please provide the exact sequence of commands / steps that you executed before running into the error. \r\n\r\nAlso, TensorFlow 2.4 was build and tested against CUDA 11. Could you please update CUDA to v11 and check if you are facing the same issue. Thanks!", "Sure, although the full command line is massive and I expect that to be a general error in the code or test, as the way TF was build shouldn't affect the limits it uses. Also upgrading to CUDA 11 is not possible on that machine yet, and also here I don't see how that would affect the limits.\r\nAnyway full command line and config:\r\n```\r\nexport CC_OPT_FLAGS='-O2 -ftree-vectorize -mcpu=native -fno-math-errno -fPIC'\r\nexport CUDA_TOOLKIT_PATH='/sw/installed/CUDA/10.1.243-GCC-8.3.0'\r\nexport CUDNN_INSTALL_PATH='/sw/installed/cuDNN/7.6.4.38-gcccuda-2019b'\r\nexport GCC_HOST_COMPILER_PATH='/scratch/ws/1/s3248973-EasyBuild/easybuild-ml/software/GCCcore/8.3.0/bin/gcc'\r\nexport GCC_HOST_COMPILER_PREFIX='/sw/installed/binutils/2.32-GCCcore-8.3.0/bin'\r\nexport MPI_HOME='/sw/installed/OpenMPI/3.1.4-gcccuda-2019b'\r\nexport NCCL_INSTALL_PATH='/sw/installed/NCCL/2.4.8-gcccuda-2019b'\r\nexport PYTHON_BIN_PATH='/scratch/ws/1/s3248973-EasyBuild/easybuild-ml/software/Python/3.7.4-GCCcore-8.3.0/bin/python'\r\nexport PYTHON_LIB_PATH='/tmp/ebinstall/software/TensorFlow/2.4.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages'\r\nexport TF_CUBLAS_VERSION='10.2.1'\r\nexport TF_CUDA_CLANG='0'\r\nexport TF_CUDA_COMPUTE_CAPABILITIES='3.7,6.1,7.0'\r\nexport TF_CUDA_PATHS='/sw/installed/CUDA/10.1.243-GCC-8.3.0'\r\nexport TF_CUDA_VERSION='10.1'\r\nexport TF_CUDNN_VERSION='7.6.4'\r\nexport TF_DOWNLOAD_CLANG='0'\r\nexport TF_ENABLE_XLA='1'\r\nexport TF_NCCL_VERSION='2.4.8'\r\nexport TF_NEED_AWS='0'\r\nexport TF_NEED_CUDA='1'\r\nexport TF_NEED_GCP='0'\r\nexport TF_NEED_GDR='0'\r\nexport TF_NEED_HDFS='0'\r\nexport TF_NEED_JEMALLOC='1'\r\nexport TF_NEED_KAFKA='0'\r\nexport TF_NEED_MPI='1'\r\nexport TF_NEED_OPENCL='0'\r\nexport TF_NEED_OPENCL_SYCL='0'\r\nexport TF_NEED_ROCM='0'\r\nexport TF_NEED_S3='0'\r\nexport TF_NEED_TENSORRT='0'\r\nexport TF_NEED_VERBS='0'\r\nexport TF_SET_ANDROID_WORKSPACE='0'\r\nexport TF_SYSTEM_LIBS='curl,boringssl,double_conversion,flatbuffers,gif,hwloc,icu,jsoncpp_git,libjpeg_turbo,png,lmdb,nasm,nsync,pcre,com_google_protobuf,pybind11,snappy,org_sqlite,zlib,absl_py,astor_archive,astunparse_archive,cython,dill_archive,enum34_archive,functools32_archive,gast_archive,opt_einsum_archive,pasta,six_archive,tblib_archive,termcolor_archive,typing_extensions_archive,wrapt'\r\nexport PYTHONPATH='/tmp/ebinstall/software/TensorFlow/2.4.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages:/tmp/ebinstall/software/TensorFlow/2.4.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages:/scratch/ws/1/s3248973-EasyBuild/easybuild-ml/software/typing-extensions/3.7.4.3-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages:/scratch/ws/1/s3248973-EasyBuild/easybuild-ml/software/flatbuffers-python/1.12-GCCcore-8.3.0-Python-3.7.4/lib/python3.7/site-packages:/scratch/ws/1/s3248973-EasyBuild/easybuild-ml/software/protobuf-python/3.10.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages:/sw/installed/h5py/2.10.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages:/sw/installed/SciPy-bundle/2019.10-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages:/scratch/ws/1/s3248973-EasyBuild/easybuild-ml/software/Python/3.7.4-GCCcore-8.3.0/easybuild/python'\r\n\r\nbazel --output_user_root=/tmp/bazel-tf --host_jvm_args=-Xms512m --host_jvm_args=-Xmx4096m test --compilation_mode=opt --config=opt --subcommands --verbose_failures --config=noaws --jobs=64 --strip=never --copt=\"-ggdb\" --copt=\"-fPIC\" --action_env=CPATH='/sw/installed/cURL/7.66.0-GCCcore-8.3.0/include:/sw/installed/double-conversion/3.1.4-GCCcore-8.3.0/include:/scratch/ws/1/s3248973-EasyBuild/easybuild-ml/software/flatbuffers/1.12.0-GCCcore-8.3.0/include:/scratch/ws/1/s3248973-EasyBuild/easybuild-ml/software/giflib/5.2.1-GCCcore-8.3.0/include:/sw/installed/hwloc/1.11.12-GCCcore-8.3.0/include:/sw/installed/ICU/64.2-GCCcore-8.3.0/include:/scratch/ws/1/s3248973-EasyBuild/easybuild-ml/software/JsonCpp/1.9.3-GCCcore-8.3.0/include:/sw/installed/libjpeg-turbo/2.0.3-GCCcore-8.3.0/include:/sw/installed/libpng/1.6.37-GCCcore-8.3.0/include:/scratch/ws/1/s3248973-EasyBuild/easybuild-ml/software/LMDB/0.9.24-GCCcore-8.3.0/include:/scratch/ws/1/s3248973-EasyBuild/easybuild-ml/software/nsync/1.24.0-GCCcore-8.3.0/include:/sw/installed/PCRE/8.43-GCCcore-8.3.0/include:/sw/installed/protobuf/3.10.0-GCCcore-8.3.0/include:/sw/installed/pybind11/2.4.3-GCCcore-8.3.0-Python-3.7.4/include:/scratch/ws/1/s3248973-EasyBuild/easybuild-ml/software/snappy/1.1.7-GCCcore-8.3.0/include:/sw/installed/SQLite/3.29.0-GCCcore-8.3.0/include:/sw/installed/zlib/1.2.11-GCCcore-8.3.0/include' --action_env=LIBRARY_PATH='/sw/installed/cURL/7.66.0-GCCcore-8.3.0/lib:/sw/installed/double-conversion/3.1.4-GCCcore-8.3.0/lib:/scratch/ws/1/s3248973-EasyBuild/easybuild-ml/software/flatbuffers/1.12.0-GCCcore-8.3.0/lib64:/scratch/ws/1/s3248973-EasyBuild/easybuild-ml/software/giflib/5.2.1-GCCcore-8.3.0/lib:/sw/installed/hwloc/1.11.12-GCCcore-8.3.0/lib:/sw/installed/ICU/64.2-GCCcore-8.3.0/lib:/scratch/ws/1/s3248973-EasyBuild/easybuild-ml/software/JsonCpp/1.9.3-GCCcore-8.3.0/lib64:/sw/installed/libjpeg-turbo/2.0.3-GCCcore-8.3.0/lib64:/sw/installed/libpng/1.6.37-GCCcore-8.3.0/lib:/scratch/ws/1/s3248973-EasyBuild/easybuild-ml/software/LMDB/0.9.24-GCCcore-8.3.0/lib:/scratch/ws/1/s3248973-EasyBuild/easybuild-ml/software/nsync/1.24.0-GCCcore-8.3.0/lib64:/sw/installed/PCRE/8.43-GCCcore-8.3.0/lib:/sw/installed/protobuf/3.10.0-GCCcore-8.3.0/lib64:/scratch/ws/1/s3248973-EasyBuild/easybuild-ml/software/snappy/1.1.7-GCCcore-8.3.0/lib64:/sw/installed/SQLite/3.29.0-GCCcore-8.3.0/lib:/sw/installed/zlib/1.2.11-GCCcore-8.3.0/lib' --action_env=PYTHONPATH --action_env=PYTHONNOUSERSITE=1 --distinct_host_configuration=false --test_output=errors --test_env=TF_GPU_COUNT=6 --test_env=TF_TESTS_PER_GPU=1 --local_test_jobs=6 --build_tests_only --test_tag_filters='-no_gpu,-no_oss,-oss_serial,-benchmark-test,-no_oss_py37,-v1only' --build_tag_filters='-no_gpu,-no_oss,-oss_serial,-benchmark-test,-no_oss_py37,-v1only' --run_under=//tensorflow/tools/ci_build/gpu_build:parallel_gpu_execute -- //tensorflow/python/... -//tensorflow/python/integration_testing/...\r\n```\r\n\r\nThis can of course be refined to only run 1 of the 2 mentioned tests which leads to the same issue.", "Hi @Flamefire,\r\n\r\nPlease run the tests as follows:\r\n\r\n```\r\nbazel test \\\r\n  --build_tests_only \\\r\n\\\r\n  --test_env=TF_GPU_COUNT=8 \\\r\n  --test_env=TF_TESTS_PER_GPU=8 \\\r\n  --local_test_jobs=64 \\\r\n  --run_under=//tensorflow/tools/ci_build/gpu_build:parallel_gpu_execute \\\r\n\\\r\n  --nodistinct_host_configuration \\\r\n\\\r\n  --test_tag_filters=gpu,-no_gpu,-nogpu,-benchmark-test,-no_oss,-oss_serial,-v1only,-no_gpu_presubmit,-no_cuda11 \\\r\n\\\r\n  -- \\\r\n   //tensorflow/... \\\r\n  -//tensorflow/python/integration_testing/... \\\r\n  -//tensorflow/compiler/mlir/tosa/... \\\r\n  -//tensorflow/compiler/xrt/... \\\r\n  -//tensorflow/lite/micro/examples/... \\\r\n  -//tensorflow/core/tpu/... \\\r\n  -//tensorflow/lite/...\r\n```\r\n\r\nWhere `TF_GPU_COUNT` is the number of GPUs available on your machine and the number passed to `--local_test_jobs` is `TF_GPU_COUNT * TF_TESTS_PER_GPU`.  Set `TF_TESTS_PER_GPU` low enough that your GPU has at least `2 * TF_TESTS_PER_GPU` GB of memory.\r\n\r\nOf course the list of tests and the test tag filters should vary based on what you want to test.", "I think I'm doing that already beeing even more conservative. The relevant parts from my above command also reformatted:\r\n\r\n```\r\nbazel \\\r\n--compilation_mode=opt --config=opt --config=noaws --jobs=64 \\\r\n--distinct_host_configuration=false \\\r\n--test_env=TF_GPU_COUNT=6 \\\r\n--test_env=TF_TESTS_PER_GPU=1 \\\r\n--local_test_jobs=6 \\\r\n--build_tests_only \\\r\n--test_tag_filters='-no_gpu,-no_oss,-oss_serial,-benchmark-test,-no_oss_py37,-v1only' \\\r\n--build_tag_filters='-no_gpu,-no_oss,-oss_serial,-benchmark-test,-no_oss_py37,-v1only' \\\r\n--run_under=//tensorflow/tools/ci_build/gpu_build:parallel_gpu_execute \\\r\n-- //tensorflow/python/... -//tensorflow/python/integration_testing/...\r\n```\r\n\r\nThis is on a node with 6 GPUs and IIUC this runs 1 test per GPU only. So this is more conservative than your suggestion where I should be able to run 16 tests per GPU in parallel (32GB Volta) Is this correct?\r\n\r\nAlso quoting the message from the OP:\r\n> W tensorflow/stream_executor/stream_executor_pimpl.cc:490] Not enough memory to allocate 31614597888 on device 0 within provided limit. [used=0, limit=1073741824]\r\n\r\nThis looks like only 1GB should be used at most for the tests due to that limit (although I'm unsure where this limit is set at)\r\nSo only explanation I have is that the test requires more than this 1GB", "> I think I'm doing that already beeing even more conservative. The relevant parts from my above command also reformatted:\r\n\r\nThanks, the reformat made it much easier to read. :)\r\n\r\nThe full set of tags is `gpu,-no_gpu,-nogpu,-benchmark-test,-no_oss,-oss_serial,-v1only,-no_gpu_presubmit,-no_cuda11` (note that we only run tests that are explicitly tagged with `gpu`, amongst other constraints).  Other tests are not guaranteed to pass on GPUs.\r\n\r\nAnd you'll notice that e.g. `//tensorflow/python/keras/applications:applications_test` is not tagged with `gpu`.  So the command line I proposed above will filter that test out.", "> Other tests are not guaranteed to pass on GPUs.\r\n\r\nThis makes me very surprised. My understanding of unit tests is, that they should test code for correctness. The availability of GPUs is optional in \"normal\" usage of TensorFlow.\r\nHow can a test fail when a GPU is available? Doesn't that mean that either the TF code it tests has a bug (i.e. the test code uses it like a user would but it returns wrong results) or that the test code has a bug (e.g. it runs TF code that is only there for CPU and the test fails to set CPU as the device)?\r\n\r\nTo put it another way: My expectation would be: When running unit tests, all tests are run by default. When some tests require additional constraints (e.g. a GPU or a certain amount of memory) they are marked as such and automatically skipped when it is detected that the current system doesn't has this. The filter-tags allow to limit the amount of tests gathered to exclude e.g. long-running tests (e.g. benchmarks) or to not consider the GPU tests at all (as you know already they will be skipped)\r\n\r\nMy use case is that I want to run the TF unit tests at the end of a build on HPC clusters. The whole install process is heavily automated as it is unfeasible for HPC admins worldwide to manually go through each softwares unique build process. Here it is not known if the build machine contains GPUs or not. Hence I would like to additionally run the GPU tests if there are GPUs available. If now some tests fail only because a GPU is available then it renders the units tests unusable in that scenario and IMO the tests should be fixed.", "> How can a test fail when a GPU is available?\r\n\r\nIn this specific case it looks like the test needs more than 1GB to run.\r\n\r\n> Hence I would like to additionally run the GPU tests if there are GPUs available. If now some tests fail only because a GPU is available then it renders the units tests unusable in that scenario and IMO the tests should be fixed.\r\n\r\nI don't think we'll be able to change how we run our tests anytime soon.  (We could make spot fixes here and there, but these kinds of things will likely regress over time.)\r\n\r\nSo I recommend you do two runs, one without GPUs and one with GPUs.  The first one excludes all tests tagged with `gpu` and run with `CUDA_VISIBLE_DEVICES=-1` to ensure that the tests don't see any GPUs.  The second one runs only GPU tests, and runs only if GPUs are present on the machine.", "> So I recommend you do two runs, one without GPUs and one with GPUs\r\n\r\nAlthough inconvenient (and I wonder what the `nogpu` and `no_gpu` tags are for then) I tried that. So based on the CI files I have \r\n```\r\n        'test_tag_filters_gpu': 'gpu,-no_gpu,-nogpu,-no_oss,-oss_serial,-benchmark-test,-v1only',\r\n        'test_tag_filters_cpu': '-gpu,-tpu,-no_oss,-oss_serial,-benchmark-test,-v1only',\r\n```\r\n\r\nHowever there are failures in the CPU tests now. E.g. `//tensorflow/core/common_runtime:ring_reducer_test` tries to run `RingReducerTest.DaTyFLOAT_DevTyGPU_Wkr1_Dev2_Sdiv1_Len1_Abrt0` which then fails with \"F tensorflow/core/common_runtime/ring_reducer_test.cc:187] Unsupported device_type GPU\"\r\n\r\nFrom checking the BUILD files I see that there is a ring_reducer_test and ring_reducer_test_gpu added through tf_cuda_cc_test\r\n\r\nAnd in the cc file the GPU section is guarded by `#if GOOGLE_CUDA`, but as I verified that ring_reducer_test (w/o _gpu suffix) is run there seems to be GOOGLE_CUDA wrongly(?) defined.\r\nCan you advise what to do now? Why and where is GOOGLE_CUDA defined? And why am I seeing a GPU related failure in a supposedly CPU test which seems to not happen on the TF CI? Is that test excluded there?", "Ok this seems to be another issue which can be workaround by excluding `no_cuda_on_cpu_tap`, see https://github.com/tensorflow/tensorflow/issues/46425 for an argument against that tag.\r\n\r\nIn a similar case the presence of the `no_gpu` (or `nogpu`, why 2?) is odd. As noted in https://github.com/tensorflow/tensorflow/issues/45664#issuecomment-755798751 it seems to be unsupported or impossible to run tests *not* marked with `gpu` in an environment which has GPUs. Hence tests need to be started with a test_tag_filter of either `gpu` or `-gpu` anyway, so what are the `no_gpu` tags for?\r\nMay I suggest to remove those? Or could you clarify the meaning of those?", "The test tags could use some cleaning up, but that's more complicated than it sounds since there are several bazel macros involved here (and more blaze macros internally at Google).  So I suggest not blocking progress on this.\r\n\r\nRegarding `GOOGLE_CUDA` -- it is set via `--config=cuda`.  The non-GPU tests should not be built or tested with `--config=cuda`.  The `configure.py` workflow makes this difficult by writing out `build --config=cuda` to `.tf_configure.bazelrc` the user asked for it.  This means so if you configure a source tree on a machine with a GPU present (and enable CUDA) and build/test on a machine without a GPU then `--config=cuda` is set even for the second run.  Building & running TensorFlow itself this way is fine (TF built with `--config=cuda` silently falls back to the CPU if no GPU is present), but unit tests may fail when run this way.  So I would recommend reconfiguring TF before running the non-GPU test suite.", ">  So I would recommend reconfiguring TF before running the non-GPU test suite.\r\n\r\nThis is highly inconvenient. Doing it this way I can't be sure that the TF I built (and installed) is the one that is tested as (I think) if I reconfigure then Bazel will rebuild also the core TF parts which are being tested.\r\nIMO the correct approach would be to not rely on a flag/define which is intended to signal GPU support (as you wrote it is fine to have GPU support but no GPU) in a test which is meant to test the CPU implementation (on which TF should fall back). This basically makes it impossible, to test if the CPU fallback is (also) correctly built.\r\n\r\nSo I would suggest to add a new flag which tells the tests that they are CPU tests (I mean: those are even added twice: Once for cpu once for gpu and a workaround via a test tag was neccessary to \"fix\" this), see my comment in #46425", "Somehow this slipped. I think with the upcoming refactor we can target this too"]}, {"number": 45663, "title": "No registered 'ResourceScatterNdUpdate' OpKernel for 'GPU' ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.4.0rc4\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source): 3.4.1\r\n- GCC/Compiler version (if compiling from source): GCC 8.3.0\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: V100\r\n\r\n**Describe the current behavior**\r\nA test shows that a GPU implementation for BOOL inputs of ResourceScatterNdUpdate is seemingly missing.\r\nThe test is //tensorflow/python/kernel_tests:batch_scatter_ops_test -> ScatterTest.testBooleanScatterUpdate\r\n\r\n**Standalone code to reproduce the issue**\r\nRun bazel test\r\n\r\n**Other info / logs**\r\n```\r\nERROR: testBooleanScatterUpdate (__main__.ScatterTest)\r\nScatterTest.testBooleanScatterUpdate\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/batch_scatter_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/batch_scatter_ops_test.py\", line 91, in testBooleanScatterUpdate\r\n    update0 = state_ops.batch_scatter_update(var, [1], [True])\r\n  File \"/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/batch_scatter_ops_test.runfiles/org_tensorflow/tensorflow/python/util/deprecation.py\", line 340, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/batch_scatter_ops_test.runfiles/org_tensorflow/tensorflow/python/ops/state_ops.py\", line 915, in batch_scatter_update\r\n    ref, final_indices, updates, use_locking=use_locking)\r\n  File \"/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/batch_scatter_ops_test.runfiles/org_tensorflow/tensorflow/python/ops/state_ops.py\", line 368, in scatter_nd_update\r\n    name=name))\r\n  File \"/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/batch_scatter_ops_test.runfiles/org_tensorflow/tensorflow/python/ops/gen_state_ops.py\", line 740, in resource_scatter_nd_update\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/batch_scatter_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py\", line 6862, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.NotFoundError: No registered 'ResourceScatterNdUpdate' OpKernel for 'GPU' devices compatible with node {{node ResourceScatterNdUpdate}}\r\n\t (OpKernel was found, but attributes didn't match) Requested Attributes: T=DT_BOOL, Tindices=DT_INT32, use_locking=true\r\n\t.  Registered:  device='GPU'; T in [DT_COMPLEX128]; Tindices in [DT_INT64]\r\n  device='GPU'; T in [DT_COMPLEX128]; Tindices in [DT_INT32]\r\n  device='GPU'; T in [DT_COMPLEX64]; Tindices in [DT_INT64]\r\n  device='GPU'; T in [DT_COMPLEX64]; Tindices in [DT_INT32]\r\n  device='GPU'; T in [DT_DOUBLE]; Tindices in [DT_INT64]\r\n  device='GPU'; T in [DT_DOUBLE]; Tindices in [DT_INT32]\r\n  device='GPU'; T in [DT_FLOAT]; Tindices in [DT_INT64]\r\n  device='GPU'; T in [DT_FLOAT]; Tindices in [DT_INT32]\r\n  device='GPU'; T in [DT_HALF]; Tindices in [DT_INT64]\r\n  device='GPU'; T in [DT_HALF]; Tindices in [DT_INT32]\r\n  device='GPU'; T in [DT_INT64]; Tindices in [DT_INT64]\r\n  device='GPU'; T in [DT_INT64]; Tindices in [DT_INT32]\r\n  device='GPU'; T in [DT_INT32]; Tindices in [DT_INT64]\r\n  device='GPU'; T in [DT_INT32]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_BOOL]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_BOOL]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_STRING]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_STRING]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_COMPLEX128]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_COMPLEX128]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_COMPLEX64]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_COMPLEX64]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_DOUBLE]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_DOUBLE]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_BFLOAT16]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_BFLOAT16]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_HALF]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_HALF]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_INT32]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_INT32]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_INT8]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_INT8]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_UINT8]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_UINT8]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_INT16]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_INT16]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_UINT16]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_UINT16]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_UINT32]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_UINT32]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_INT64]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_INT64]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_UINT64]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_UINT64]; Tindices in [DT_INT32]\r\n [Op:ResourceScatterNdUpdate]\r\n```\r\n\r\n", "comments": ["@Flamefire,\r\nTensorFlow 2.4 is tested and built against CUDA 11 and cuDNN 8. For more information, please take a look at the [tested build configurations](https://www.tensorflow.org/install/source#gpu).\r\n\r\nCould you please check if you are facing the same error with CUDA 11 and cuDNN 8 as well? Thanks!", "As I can't use CUDA 11 I retested that with TF 2.3 and `python git/tensorflow/tensorflow/python/kernel_tests/batch_scatter_ops_test.py ScatterTest.testBooleanScatterUpdate` and am seeing the same issue. So I'd assume that this is not related to the version.", "The issue is fully expected looking at the source: The bool kernel is simply never declared for GPU, the relevant line is https://github.com/tensorflow/tensorflow/blob/b5c8d770f553c529165292c607bcda46ce370c08/tensorflow/core/kernels/scatter_nd_op_gpu.cu.cc#L201\r\n\r\nThat uses https://github.com/tensorflow/tensorflow/blob/b5c8d770f553c529165292c607bcda46ce370c08/tensorflow/core/framework/register_types.h#L195-L196\r\n\r\nAnd as one can see the Bool type is missing. I guess using `TF_CALL_GPU_ALL_TYPES` would be the right choice here. And looking at the other CPU implementations I'd say `TF_CALL_INTEGRAL_TYPES` instead of the `TF_CALL_int32/TF_CALL_int64` should also be preferred"]}, {"number": 45661, "title": "Test failures with \"OpError not raised\"", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.4.0rc4\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source): 3.4.1\r\n- GCC/Compiler version (if compiling from source): GCC 8.3.0\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: V100\r\n\r\n**Describe the current behavior**\r\n\r\nSome tests fail with \"OpError not raised\": \r\n- //tensorflow/python/feature_column:feature_column_test  & //tensorflow/python/feature_column:feature_column_v2_test : IdentityCategoricalColumnTest.test_get_sparse_tensors_with_inputs_too_big\r\n- //tensorflow/python/kernel_tests:batch_scatter_ops_test : ScatterTest.testScatterOutOfRange\r\n\r\n**Standalone code to reproduce the issue**\r\nRun bazel test\r\n", "comments": ["@Flamefire,\r\nIn order to expedite the trouble-shooting process, could you please provide the exact sequence of commands / steps that you executed before running into the error. Thanks!", "Running `python git/tensorflow/tensorflow/python/feature_column/feature_column_test.py IdentityCategoricalColumnTest.test_get_sparse_tensors_with_inputs_too_big` or the `bazel test` seems to be enough to trigger this.\r\n\r\nI was able to reproduce this with the custom build TF 2.4 and the pip installed TF 2.3 (can't use 2.4 as we still have CUDA 10.1 on our systems and the driver update is pending)", "Note that this seems to only affect the CPU dispatched version, while when a GPU that is used (by default  when any is detected) the bug does not appear. Furthermore it seems that out-of-bounds indices are seemingly ignored which means there is a chance those cause actual OOB writes which are somehow not noticed otherwise\r\n\r\nEdit: I was just able to verify this: The GPU code simply ignores it: https://github.com/tensorflow/tensorflow/blob/b5c8d770f553c529165292c607bcda46ce370c08/tensorflow/core/kernels/scatter_nd_op_gpu.cu.cc#L115-L120 \r\nAnd later always returns \"no error\": https://github.com/tensorflow/tensorflow/blob/b5c8d770f553c529165292c607bcda46ce370c08/tensorflow/core/kernels/scatter_nd_op_gpu.cu.cc#L162"]}, {"number": 45660, "title": "Seg faults in various tests", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.4.0rc4\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source): 3.4.1\r\n- GCC/Compiler version (if compiling from source): GCC 8.3.0\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: V100\r\n\r\n**Describe the current behavior**\r\n\r\nSome tests error with Signal 11 / Segfault: //tensorflow/python:convert_to_constants_test, //tensorflow/python/distribute:parameter_server_strategy_v2_test, //tensorflow/python/keras/tests:convert_to_constants_test, //tensorflow/python/kernel_tests:map_ops_test\r\n\r\n**Standalone code to reproduce the issue**\r\nRun bazel test on the above targets\r\n\r\n**Other info / logs**\r\nExample log:\r\n\r\n```\r\n2020-12-14 13:06:37.124411: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:928] Optimization results for grappler item: graph_to_optimize\r\n  function_optimizer: Graph size after: 5 nodes (0), 4 edges (0), time = 0.346ms.\r\n  function_optimizer: Graph size after: 5 nodes (0), 4 edges (0), time = 0.343ms.\r\nOptimization results for grappler item: while_1_body_2134\r\n  function_optimizer: function_optimizer did nothing. time = 0ms.\r\n  function_optimizer: function_optimizer did nothing. time = 0ms.\r\nOptimization results for grappler item: while_1_cond_2133\r\n  function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n  function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n\r\nFatal Python error: Segmentation fault\r\n\r\nThread 0x00002000000484c0 (most recent call first):\r\n  File \"/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/convert_to_constants_test.runfiles/org_tensorflow/tensorflow/python/eager/execute.py\", line 60 in quick_execute\r\n  File \"/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/convert_to_constants_test.runfiles/org_tensorflow/tensorflow/python/eager/function.py\", line 560 in call\r\n  File \"/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/convert_to_constants_test.runfiles/org_tensorflow/tensorflow/python/eager/function.py\", line 1919 in _call_flat\r\n  File \"/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/convert_to_constants_test.runfiles/org_tensorflow/tensorflow/python/eager/function.py\", line 1736 in _call_with_flat_signature\r\n  File \"/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/convert_to_constants_test.runfiles/org_tensorflow/tensorflow/python/eager/function.py\", line 1687 in _call_impl\r\n  File \"/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/convert_to_constants_test.runfiles/org_tensorflow/tensorflow/python/eager/wrap_function.py\", line 247 in _call_impl\r\n  File \"/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/convert_to_constants_test.runfiles/org_tensorflow/tensorflow/python/eager/function.py\", line 1669 in __call__\r\n  File \"/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/convert_to_constants_test.runfiles/org_tensorflow/tensorflow/python/framework/convert_to_constants_test.py\", line 186 in _testConvertedFunction\r\n  File \"/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/convert_to_constants_test.runfiles/org_tensorflow/tensorflow/python/framework/convert_to_constants_test.py\", line 466 in testStatelessWhile\r\n  File \"/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/convert_to_constants_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1436 in decorated\r\n  File \"/scratch/ws/1/s3248973-EasyBuild/easybuild-ml/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/unittest/case.py\", line 628 in run\r\n  File \"/scratch/ws/1/s3248973-EasyBuild/easybuild-ml/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/unittest/case.py\", line 676 in __call__\r\n  File \"/scratch/ws/1/s3248973-EasyBuild/easybuild-ml/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/unittest/suite.py\", line 122 in run\r\n  File \"/scratch/ws/1/s3248973-EasyBuild/easybuild-ml/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/unittest/suite.py\", line 84 in __call__\r\n  File \"/scratch/ws/1/s3248973-EasyBuild/easybuild-ml/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/unittest/suite.py\", line 122 in run\r\n  File \"/scratch/ws/1/s3248973-EasyBuild/easybuild-ml/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/unittest/suite.py\", line 84 in __call__\r\n  File \"/scratch/ws/1/s3248973-EasyBuild/easybuild-ml/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/unittest/runner.py\", line 176 in run\r\n  File \"/scratch/ws/1/s3248973-EasyBuild/easybuild-ml/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/unittest/main.py\", line 271 in runTests\r\n  File \"/scratch/ws/1/s3248973-EasyBuild/easybuild-ml/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/unittest/main.py\", line 101 in __init__\r\n  File \"/tmp/ebinstall/software/TensorFlow/2.4.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/absl/testing/absltest.py\", line 2404 in _run_and_get_tests_result\r\n  File \"/tmp/ebinstall/software/TensorFlow/2.4.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/absl/testing/absltest.py\", line 2434 in run_tests\r\n  File \"/tmp/ebinstall/software/TensorFlow/2.4.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/absl/testing/absltest.py\", line 2109 in _run_in_app\r\n  File \"/tmp/ebinstall/software/TensorFlow/2.4.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/absl/testing/absltest.py\", line 2002 in main\r\n  File \"/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/convert_to_constants_test.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py\", line 56 in g_main\r\n  File \"/tmp/ebinstall/software/TensorFlow/2.4.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/absl/app.py\", line 251 in _run_main\r\n  File \"/tmp/ebinstall/software/TensorFlow/2.4.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/absl/app.py\", line 303 in run\r\n  File \"/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/convert_to_constants_test.runfiles/org_tensorflow/tensorflow/python/platform/app.py\", line 40 in run\r\n  File \"/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/convert_to_constants_test.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py\", line 65 in main_wrapper\r\n  File \"/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/convert_to_constants_test.runfiles/org_tensorflow/tensorflow/python/platform/benchmark.py\", line 486 in benchmarks_main\r\n  File \"/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/convert_to_constants_test.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py\", line 66 in main\r\n  File \"/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/convert_to_constants_test.runfiles/org_tensorflow/tensorflow/python/platform/test.py\", line 58 in main\r\n  File \"/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/convert_to_constants_test.runfiles/org_tensorflow/tensorflow/python/framework/convert_to_constants_test.py\", line 1185 in <module>\r\n*** Received signal 11 ***\r\n*** BEGIN MANGLED STACK TRACE ***\r\n/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/convert_to_constants_test.runfiles/org_tensorflow/tensorflow/python/../libtensorflow_framework.so.2(+0xbeae3c)[0x2002f0cbae3c]\r\n[0x2000000504d8]\r\n/scratch/ws/1/s3248973-EasyBuild/easybuild-ml/software/Python/3.7.4-GCCcore-8.3.0/lib/libpython3.7m.so.1.0(+0x9089c)[0x20000010089c]\r\n[0x2000000504d8]\r\n[0x83bce103d9edaebe]\r\n[0x49ad6880]\r\n/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/convert_to_constants_test.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN10tensorflow8BinaryOpIN5Eigen16ThreadPoolDeviceENS_7functor3addIiEEE7ComputeEPNS_15OpKernelContextE+0x560)[0x2002d4b0a9b0]\r\n/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/convert_to_constants_test.runfiles/org_tensorflow/tensorflow/python/../libtensorflow_framework.so.2(_ZN10tensorflow13BaseGPUDevice7ComputeEPNS_8OpKernelEPNS_15OpKernelContextE+0x298)[0x2002f0b142e8]\r\n/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/convert_to_constants_test.runfiles/org_tensorflow/tensorflow/python/../libtensorflow_framework.so.2(+0xb377d0)[0x2002f0c077d0]\r\n/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/convert_to_constants_test.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN5Eigen15ThreadPoolTemplIN10tensorflow6thread16EigenEnvironmentEE10WorkerLoopEi+0x3a8)[0x2002cf8d00e8]\r\n/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/convert_to_constants_test.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so(_ZNSt17_Function_handlerIFvvEZN5Eigen15ThreadPoolTemplIN10tensorflow6thread16EigenEnvironmentEEC4EibS5_EUlvE_E9_M_invokeERKSt9_Any_data+0x28)[0x2002cf8d1298]\r\n/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/convert_to_constants_test.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so(_ZNSt17_Function_handlerIFvvEZN10tensorflow6thread16EigenEnvironment12CreateThreadESt8functionIS0_EEUlvE_E9_M_invokeERKSt9_Any_data+0x84)[0x2002cf8cc1f4]\r\n/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/convert_to_constants_test.runfiles/org_tensorflow/tensorflow/python/../libtensorflow_framework.so.2(+0xbf8b00)[0x2002f0cc8b00]\r\n/lib64/libpthread.so.0(+0x8b94)[0x200000708b94]\r\n/lib64/libc.so.6(clone+0xe4)[0x2000008985f4]\r\n*** END MANGLED STACK TRACE ***\r\n\r\n*** Begin stack trace ***\r\n\ttensorflow::CurrentStackTrace[abi:cxx11]()\r\n\t\r\n\t__kernel_sigtramp_rt64\r\n\t\r\n\t__kernel_sigtramp_rt64\r\n\t\r\n\t\r\n\ttensorflow::BinaryOp<Eigen::ThreadPoolDevice, tensorflow::functor::add<int> >::Compute(tensorflow::OpKernelContext*)\r\n\ttensorflow::BaseGPUDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*)\r\n\t\r\n\tEigen::ThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int)\r\n\tstd::_Function_handler<void (), Eigen::ThreadPoolTempl<tensorflow::thread::EigenEnvironment>::ThreadPoolTempl(int, bool, tensorflow::thread::EigenEnvironment)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\r\n\tstd::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\r\n\t\r\n\t\r\n\tclone\r\n*** End stack trace ***\r\n```\r\n", "comments": ["@Flamefire \r\n\r\nPlease, share colab link or simple standalone code with supporting files to reproduce the issue in our environment. It helps us in localizing the issue faster.Thanks!", "As this is a python test it can be run via e.g. `python git/tensorflow/tensorflow/python/framework/convert_to_constants_test.py VariablesToConstantsTest.testStatelessWhile` (adapt the path as required) which reproduces this on TF 2.4 and 2.3 (the latter installed via pip)"]}, {"number": 45659, "title": "Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.4.0rc4\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source): 3.4.1\r\n- GCC/Compiler version (if compiling from source): GCC 8.3.0\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: V100\r\n\r\n**Describe the current behavior**\r\n\r\nThe following tests fail with the error \"Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string\": //tensorflow/python/ops/ragged:ragged_dispatch_test, //tensorflow/python/ops/ragged:ragged_map_fn_op_test, //tensorflow/python/ops/ragged:ragged_print_op_test, //tensorflow/python/ops/ragged:ragged_tensor_test\r\n\r\n**Standalone code to reproduce the issue**\r\nRunning `bazel test` on the above targets\r\n\r\n**Other info / logs**\r\n\r\nExample log:\r\n\r\n```\r\nERROR: testRaggedToStringescape (__main__.RaggedToStringTest)\r\nRaggedToStringTest.testRaggedToStringescape\r\ntestRaggedToStringescape([[\"a'b\"], ['c\\\\d']], \"[['a\\\\'b'], ['c\\\\\\\\d']]\")\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/ops/ragged/ragged_print_op_test.runfiles/org_tensorflow/tensorflow/python/client/session.py\", line 1375, in _do_call\r\n    return fn(*args)\r\n  File \"/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/ops/ragged/ragged_print_op_test.runfiles/org_tensorflow/tensorflow/python/client/session.py\", line 1360, in _run_fn\r\n    target_list, run_metadata)\r\n  File \"/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/ops/ragged/ragged_print_op_test.runfiles/org_tensorflow/tensorflow/python/client/session.py\", line 1453, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument: 2 root error(s) found.\r\n  (0) Invalid argument: 2 root error(s) found.\r\n  (0) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string\r\n  (1) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string\r\n0 successful operations.\r\n0 derived errors ignored.\r\n  (1) Invalid argument: 2 root error(s) found.\r\n  (0) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string\r\n  (1) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string\r\n0 successful operations.\r\n0 derived errors ignored.\r\n0 successful operations.\r\n0 derived errors ignored.\r\n\t [[{{node AsString/map/TensorArrayUnstack/TensorListFromTensor}}]]\r\n\t [[Func/AsString/map/while/body/_1/input/_41/_24]]\r\n  (1) Invalid argument: 2 root error(s) found.\r\n  (0) Invalid argument: 2 root error(s) found.\r\n  (0) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string\r\n  (1) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string\r\n0 successful operations.\r\n0 derived errors ignored.\r\n  (1) Invalid argument: 2 root error(s) found.\r\n  (0) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string\r\n  (1) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string\r\n0 successful operations.\r\n0 derived errors ignored.\r\n0 successful operations.\r\n0 derived errors ignored.\r\n\t [[{{node AsString/map/TensorArrayUnstack/TensorListFromTensor}}]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n```\r\n", "comments": ["@Flamefire \r\n\r\nPlease, share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "This is simply running the mentioned test case via bazel. I could also reproduce this with `python git/tensorflow/tensorflow/python/ops/ragged/ragged_print_op_test.py RaggedToStringTest.testRaggedToStringescape` with TF installed.\r\n\r\nReducing that to a minimal code is cumbersome as the error happens in graph mode with a `test_session` created spanning over many functions. My attempt in doing that failed, i.e. I could not extract enough from the test to make it fail"]}, {"number": 45651, "title": "Mask RCNN tflite inference on android", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): nightly\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\r\nINFO:tensorflow:Assets written to: /tmp/tmpjivs_7gq/assets\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\n\r\nint imageTensorIndex = 0;\r\n        int[] imageShape = tflite.getInputTensor(imageTensorIndex).shape(); // {1, height, width, 3}\r\n        imageSizeY = imageShape[1];\r\n        imageSizeX = imageShape[2];\r\n        DataType imageDataType = tflite.getInputTensor(imageTensorIndex).dataType();\r\n\r\nTensorImage inputImageBuffer = new TensorImage(imageDataType);\r\n inputImageBuffer.load(bitmap);\r\n\r\nint[] probabilityShape = tflite.getOutputTensor(probabilityTensorIndex).shape(); // {1, NUM_CLASSES}\r\nDataType probabilityDataType = tflite.getOutputTensor(probabilityTensorIndex).dataType();\r\n// Creates the output tensor and its processor.\r\noutputProbabilityBuffer = TensorBuffer.createFixedSize(probabilityShape, probabilityDataType);\r\ntflite.run(inputImageBuffer.getBuffer(), outputProbabilityBuffer.getBuffer().rewind());\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n[{'name': 'input_image', 'index': 0, 'shape': array([   1, 1024, 1024,    3], dtype=int32), 'shape_signature': array([  -1, 1024, 1024,    3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'input_image_meta', 'index': 1, 'shape': array([ 1, 18], dtype=int32), 'shape_signature': array([-1, 18], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'input_anchors', 'index': 2, 'shape': array([     1, 261888,      4], dtype=int32), 'shape_signature': array([    -1, 261888,      4], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\r\n[{'name': 'Identity', 'index': 662, 'shape': array([1, 1, 1], dtype=int32), 'shape_signature': array([ 1, -1, -1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'Identity_1', 'index': 740, 'shape': array([   1, 1000,    6,    4], dtype=int32), 'shape_signature': array([   1, 1000,    6,    4], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'Identity_2', 'index': 744, 'shape': array([   1, 1000,    6], dtype=int32), 'shape_signature': array([   1, 1000,    6], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'Identity_3', 'index': 864, 'shape': array([  1, 100,   6], dtype=int32), 'shape_signature': array([  1, 100,   6], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'Identity_4', 'index': 936, 'shape': array([  1, 100,  28,  28,   6], dtype=int32), 'shape_signature': array([  1, 100,  28,  28,   6], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'Identity_5', 'index': 578, 'shape': array([1, 1, 4], dtype=int32), 'shape_signature': array([-1, -1,  4], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'Identity_6', 'index': 591, 'shape': array([1, 1, 2], dtype=int32), 'shape_signature': array([-1, -1,  2], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\r\n\r\nThese are the input and output tensors of the model.\r\n\r\n--------- beginning of crash\r\n2020-12-11 17:27:42.138 16752-16752/com.objdetector A/libc: Fatal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x0 in tid 16752 (com.objdetector), pid 16752 (com.objdetector)\r\n\r\nThis is the segmentation fault error\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem.\r\nIf including tracebacks, please include the full traceback. Large logs and files\r\nshould be attached.\r\n\r\nHow exactly should I run inference on android for Mask RCNN tflite model? I have added these dependencies:\r\n\r\ndependencies {\r\n    implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'\r\n    // This dependency adds the necessary TF op support.\r\n    implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly'\r\n}\r\n\r\nThere seem to be 3 inputs and 7 outputs to the model.. what do i need to pass as input, should I pass image, image_meta as well as input anchors? Which of the outputs will provide the result? Do I need to map all the outputs? How can I get the result?\r\n\r\n", "comments": ["@MeghaGhosh What is your use-case? If you need bounding boxes, you can check out the [SSD models on the TF2 detection zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md) Mask-RCNN is not very well supported on TFLite, mainly because the models in the TF1 detection zoo are too large to perform well on-device.", "I need Mask RCNN itself.. I need the tflite model to match my h5 model in this use case.. and that cannot be changed from Mask RCNN. What do you mean \"not very well\" supported? Is it supported though? Even just the bounding box values are enough.. can do without mask.. Is it possible at all to get the output using android interpreter?", "@MeghaGhosh Support for the TF1 Mask RCNNmodels in TFlite is pretty limited. That said, feel free to follow the steps in [this Github issue](https://github.com/tensorflow/tensorflow/issues/34845#issuecomment-573156336) to get a TFLite model. Its latency is pretty large (~1-2s), and you will need to change some of the pipeline config params based on which model you are using. Feel free to give it a try :-)\r\nWe are also working on CenterNet & EfficientDet support in the TF2 detection zoo for mobile phones, so hopefully we will models with greater mAP soon.", "Okay sure I will check that out, thank you. In addition, I did actually get a tflite model from the matterport Mask RCNN model.. i used select tf ops for conversion, python inference did not work but android seems to support select ops from what I've been reading.. what I could use help with is the android inference. Currently facing a segmentation fault issue that I have outlined, would appreciate any direction you could point me in with reference to that.", "@MeghaGhosh Actually Select TF ops are already supported on both Python and Android. Could you share the minimum steps to reproduce your problem at our side? For example, can you share model files if possible and the android application code for reproducing your case? The above information is not enough and also not easy for us to reproduce your problem at our side. Also it would be nice to have more android log outputs related to the SIGSEGV crash. \r\n\r\n@terryheo do you have any ideas on the above crashing?\r\n\r\n", "If the crashing happens on x86 emulator, it's likely the same issue with https://github.com/tensorflow/tensorflow/issues/38025\r\n\r\nAs a workaround, you need to use download nightly manually.\r\nhttps://storage.googleapis.com/tensorflow-nightly-public/prod/tensorflow/google3/ubuntu_16/lite/nightly/511/20201214-223707/tensorflow-lite-select-tf-ops.aar\r\n\r\nAnd use it as\r\n```\r\ncompile files('libs/tensorflow-lite-select-tf-ops.aar')\r\n```\r\ninstead of\r\n```\r\nimplementation('org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly') { changing = true }\r\n```", "So the segmentation fault is actually happening when we create the tflite model with tf-nightly version. When we create the model with tensorflow 2.3.0 we are getting this error:\r\n\r\n\r\n2020-12-18 16:03:35.209 12244-12244/com.objdetector W/System.err: java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: tensorflow/lite/kernels/reduce.cc:534 reference_ops::ReduceGeneric<T>( GetTensorData<T>(op_context->input), op_context->input->dims->data, op_context->input->dims->size, GetTensorData<T>(op_context->output), op_context->output->dims->data, op_context->output->dims->size, GetTensorData<int>(op_context->axis), num_axis, op_context->params->keep_dims, GetTensorData<int>(temp_index), GetTensorData<int>(resolved_axis), init_value, reducer) was not true.2020-12-18 16:03:35.209 12244-12244/com.objdetector W/System.err: No2020-12-18 16:03:35.221 12244-12244/com.objdetector W/System.err: \u00a0 \u00a0 at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)2020-12-18 16:03:35.221 12244-12244/com.objdetector W/System.err: \u00a0 \u00a0 at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:158)2020-12-18 16:03:35.221 12244-12244/com.objdetector W/System.err: \u00a0 \u00a0 at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:343)2020-12-18 16:03:35.221 12244-12244/com.objdetector W/System.err: \u00a0 \u00a0 at com.objdetector.deepmodel.MObjectDetector.detectObjects(MObjectDetector.java:198)2020-12-18 16:03:35.222 12244-12244/com.objdetector W/System.err: \u00a0 \u00a0 at com.objdetector.MainActivitynew$1.onClick(MainActivitynew.java:69)2020-12-18 16:03:35.222 12244-12244/com.objdetector W/System.err: \u00a0 \u00a0 at android.view.View.performClick(View.java:6733)2020-12-18 16:03:35.222 12244-12244/com.objdetector W/System.err: \u00a0 \u00a0 at android.view.View.performClickInternal(View.java:6691)2020-12-18 16:03:35.222 12244-12244/com.objdetector W/System.err: \u00a0 \u00a0 at android.view.View.access$3400(View.java:802)2020-12-18 16:03:35.222 12244-12244/com.objdetector W/System.err: \u00a0 \u00a0 at android.view.View$PerformClick.run(View.java:26499)2020-12-18 16:03:35.222 12244-12244/com.objdetector W/System.err: \u00a0 \u00a0 at android.os.Handler.handleCallback(Handler.java:873)2020-12-18 16:03:35.222 12244-12244/com.objdetector W/System.err: \u00a0 \u00a0 at android.os.Handler.dispatchMessage(Handler.java:99)2020-12-18 16:03:35.222 12244-12244/com.objdetector W/System.err: \u00a0 \u00a0 at android.os.Looper.loop(Looper.java:226)2020-12-18 16:03:35.222 12244-12244/com.objdetector W/System.err: \u00a0 \u00a0 at android.app.ActivityThread.main(ActivityThread.java:7178)2020-12-18 16:03:35.222 12244-12244/com.objdetector W/System.err: \u00a0 \u00a0 at java.lang.reflect.Method.invoke(Native Method)2020-12-18 16:03:35.222 12244-12244/com.objdetector W/System.err: \u00a0 \u00a0 at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:503)2020-12-18 16:03:35.223 12244-12244/com.objdetector W/System.err: \u00a0 \u00a0 at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:942)2020-12-18 16:03:35.224 12244-12244/com.objdetector E/Edgar: []2020-12-18 16:03:35.245 12244-12244/com.objdetector E/ANR_LOG: >>> msg's executing time is too long2020-12-18 16:03:35.245 12244-12244/com.objdetector E/ANR_LOG: Blocked msg = { when=-44s561ms what=0 target=android.view.ViewRootImpl$ViewRootHandler callback=android.view.View$PerformClick } , cost \u00a0= 44558 ms2020-12-18 16:03:35.245 12244-12244/com.objdetector E/ANR_LOG: >>>Current msg List is:2020-12-18 16:03:35.246 12244-12244/com.objdetector E/ANR_LOG: Current msg <1> \u00a0= { when=-44s561ms what=0 target=android.view.ViewRootImpl$ViewRootHandler callback=android.view.View$UnsetPressedState }2020-12-18 16:03:35.246 12244-12244/com.objdetector E/ANR_LOG: >>>CURRENT MSG DUMP OVER<<<2020-12-18 16:03:35.282 12244-12244/com.objdetector I/Choreographer: Skipped 2675 frames!\u00a0 The application may be doing too much work on its main thread.2020-12-18 16:03:35.355 12244-12244/com.objdetector I/Choreographer: Skipped 3 frames!\u00a0 The application may be doing too much work on its main thread.2020-12-18 16:03:35.361 12244-12423/com.objdetector I/OpenGLRenderer: Davey! duration=44679ms; Flags=0, IntendedVsync=204496322680101, Vsync=204540920879301, OldestInputEvent=9223372036854775807, NewestInputEvent=0, HandleInputStart=204540928240420, AnimationStart=204540928558343, PerformTraversalsStart=204540930867420, DrawStart=204540931626113, SyncQueued=204540934693343, SyncStart=204540934836959, IssueDrawCommandsStart=204540987101266, SwapBuffers=204540994920651, FrameCompleted=204541002585651, DequeueBufferDuration=553000, QueueBufferDuration=6118000,\u00a0ReplyForward | \u00a0 | ReplyForward | \u00a0 | \u00a0\r\n-- | -- | -- | -- | --\r\n\u00a0 | ReplyForward\r\n\r\nWe have tried 2-3 different versions of code and they all result in the same error - I am sharing all the different versions here:\r\n\r\n1. Generating an image of input tensor size and passing that as input (have tried passing a real image with the same result):\r\n\r\nint imageTensorIndex = 0;\r\nint[] imageShape = tfLite.getInputTensor(imageTensorIndex).shape(); // {1, height, width, 3}\r\nint imageSizeY = imageShape[1];\r\nint imageSizeX = imageShape[2];\r\nDataType imageDataType = tfLite.getInputTensor(imageTensorIndex).dataType();\r\n\r\n// input tensor\r\n\r\nint[] inputshape;\r\nDataType OinputputDataType = null;\r\n\r\nHashMap inputProbabilityBuffers = new HashMap<>();\r\nByteBuffer xc;\r\nfor (int k = 0; k < tfLite.getInputTensorCount(); k++) {\r\n    inputshape = tfLite.getInputTensor(k).shape();\r\n    OinputputDataType = tfLite.getInputTensor(k).dataType();\r\n    xc = TensorBuffer.createFixedSize(inputshape, OinputputDataType).getBuffer();\r\n    inputProbabilityBuffers.put(k, xc);\r\n}\r\n\r\nTensorImage inputImageBuffer = new TensorImage(OinputputDataType);\r\ninputImageBuffer.load(generateImage());\r\n\r\nint[] OutputShape;\r\nDataType OutputDataType;\r\nHashMap outputProbabilityBuffers = new HashMap<>();\r\nByteBuffer x;\r\nfor (int i = 0; i < tfLite.getOutputTensorCount(); i++) {\r\n    OutputShape = tfLite.getOutputTensor(i).shape();\r\n    OutputDataType = tfLite.getOutputTensor(i).dataType();\r\n    x = TensorBuffer.createFixedSize(OutputShape, OutputDataType).getBuffer();\r\n    outputProbabilityBuffers.put(i, x);\r\n    \r\n}\r\n\r\nObject[] Inputs = {inputImageBuffer.getBuffer()};\r\n\r\ntry {\r\n    tfLite.runForMultipleInputsOutputs(Inputs, outputProbabilityBuffers);\r\n} catch (Exception e){\r\n    e.printStackTrace();\r\n}\r\n\r\n\r\n2. Passing random float values into all 3 input tensors and sending those to the model:\r\n\r\nint imageTensorIndex = 0;\r\n  int[] imageShape = tfLite.getInputTensor(imageTensorIndex).shape(); // {1, height, width, 3}\r\n  int imageSizeY = imageShape[1];\r\n  int imageSizeX = imageShape[2];\r\n  DataType imageDataType = tfLite.getInputTensor(imageTensorIndex).dataType();\r\n\r\n  // input tensor\r\n\r\n  int[] inputshape;\r\n  DataType OinputputDataType = null;\r\n\r\n  HashMap inputProbabilityBuffers = new HashMap<>();\r\n  ByteBuffer xc;\r\n  Object[] input = new Object[3];\r\n\r\n  for (int k = 0; k < tfLite.getInputTensorCount(); k++) {\r\n      inputshape = tfLite.getInputTensor(k).shape();\r\n      Log.e(\"Edgar\",\"\"+ inputshape.toString());\r\n\r\n      OinputputDataType = tfLite.getInputTensor(k).dataType();\r\n      xc = TensorBuffer.createFixedSize(inputshape, OinputputDataType).getBuffer();\r\n      inputProbabilityBuffers.put(k, xc);\r\n\r\n      ByteBuffer temp = ByteBuffer.allocateDirect(xc.limit()); // Allocate with size of float\r\n      temp.putFloat(inputshape[k]); // May be an array containing actual values\r\n      input[k] = temp;\r\n  }\r\n \r\n  TensorImage inputImageBuffer = new TensorImage(OinputputDataType);\r\n  inputImageBuffer.load(generateImage());\r\n\r\n  int[] OutputShape;\r\n  DataType OutputDataType;\r\n  HashMap outputProbabilityBuffers = new HashMap<>();\r\n  ByteBuffer x;\r\n  for (int i = 0; i < tfLite.getOutputTensorCount(); i++) {\r\n      OutputShape = tfLite.getOutputTensor(i).shape();\r\n      OutputDataType = tfLite.getOutputTensor(i).dataType();\r\n      x = TensorBuffer.createFixedSize(OutputShape, OutputDataType).getBuffer();\r\n      outputProbabilityBuffers.put(i, x);\r\n\r\n  }\r\n  try {\r\n      tfLite.runForMultipleInputsOutputs(input, outputProbabilityBuffers);\r\n\r\n      Log.e(\"Edgar\", \"\" + outputProbabilityBuffers.toString());\r\n\r\n  } catch (Exception e){\r\n      e.printStackTrace();\r\n  }\r\n\r\n3. Creating empty buffers for inputs and passing those:\r\n\r\nint[] inputshape;\r\nDataType OinputputDataType;\t\r\nObject inputs[] = new Object[tfLite.getInputTensorCount()];\r\nfor (int i=0; i < tfLite.getInputTensorCount(); i++) {\r\n\tinputshape = tfLite.getInputTensor(k).shape();\r\n    OinputputDataType = tfLite.getInputTensor(k).dataType();\r\n    xc = TensorBuffer.createFixedSize(inputshape, OinputputDataType).getBuffer();\r\n    inputProbabilityBuffers.put(k, xc);\r\n\tByteBuffer i_bytes = ByteBuffer.allocate(xc);\r\n\tinputs[i] = i_bytes;\r\n    }\r\nint[] OutputShape;\r\nDataType OutputDataType;\r\nHashMap outputProbabilityBuffers = new HashMap<>();\r\nByteBuffer x;\r\nfor (int i = 0; i < tfLite.getOutputTensorCount(); i++) {\r\n  OutputShape = tfLite.getOutputTensor(i).shape();\r\n  OutputDataType = tfLite.getOutputTensor(i).dataType();\r\n  x = TensorBuffer.createFixedSize(OutputShape, OutputDataType).getBuffer();\r\n  outputProbabilityBuffers.put(i, x);\r\n}\r\ninterpreter.runForMultipleInputsOutputs(inputs, outputProbabilityBuffers);\r\n\r\nThey all result in above error. This is my model file (it is the converted tflite model from mask rcnn coco weights file):\r\n\r\nhttps://drive.google.com/file/d/1gGjMK_YyyHuOqyZytsoh5sJ9jSPSxn7J/view\r\n\r\nThis is the implementation while building gradle:\r\n\r\nassert file(project.ext.ASSET_DIR + \"/model.tflite\").exists()\r\nassert file(project.ext.ASSET_DIR + \"/mod.txt\").exists()\r\napply plugin: 'com.android.application'\r\n\r\nandroid {\r\n    compileSdkVersion 29\r\n    buildToolsVersion \"29.0.2\"\r\n    defaultConfig {\r\n        applicationId \"com.objdetector\"\r\n        minSdkVersion 23\r\n        targetSdkVersion 29\r\n        versionCode 1\r\n        versionName \"1.0\"\r\n        testInstrumentationRunner \"androidx.test.runner.AndroidJUnitRunner\"\r\n        ndk{\r\n            abiFilters  'armeabi-v7a',  'arm64-v8a'\r\n        }\r\n    }\r\n    buildTypes {\r\n        release {\r\n            minifyEnabled false\r\n            proguardFiles getDefaultProguardFile('proguard-android-optimize.txt'), 'proguard-rules.pro'\r\n        }\r\n    }\r\n\r\n    compileOptions {\r\n        sourceCompatibility JavaVersion.VERSION_1_8\r\n        targetCompatibility JavaVersion.VERSION_1_8\r\n    }\r\n\r\n    dependencies {\r\n        implementation 'org.tensorflow:tensorflow-android:1.6.0'\r\n    }\r\n\r\n    aaptOptions {\r\n        noCompress \"tflite\"\r\n    }\r\n\r\n    sourceSets { main { assets.srcDirs = ['src/main/assets', 'src/main/assets/'] } }\r\n}\r\n\r\ndependencies {\r\n    implementation fileTree(dir: 'libs', include: ['*.jar'])\r\n    implementation 'androidx.appcompat:appcompat:1.2.0'\r\n    implementation 'androidx.constraintlayout:constraintlayout:1.1.3'\r\n//    implementation 'org.tensorflow:tensorflow-lite:2.3.0'\r\n   implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'\r\n    implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly'\r\n    implementation 'org.tensorflow:tensorflow-lite-support:0.0.0-nightly'\r\n\r\n}\r\n\r\nWhat are we missing? Is there some other way we need to pass the input values? \r\n", "@MeghaGhosh I confirmed that your shared model, which is included in the above comment, was able to run with the benchmark_model_plus_flex tool. Looks like the model itself is runnable.\r\n\r\nCould you follow the @terryheo 's comment with the shared model?\r\n\r\nYou can run your model with TensorFlow nightly aar files.", "Yes, we are using the nightly aar files. Still facing the same issue:\r\n\r\n2020-12-22 17:10:08.127 16299-16299/com.objdetector W/System.err: java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: tensorflow/lite/kernels/reduce.cc:534 reference_ops::ReduceGeneric<T>( GetTensorData<T>(op_context->input), op_context->input->dims->data, op_context->input->dims->size, GetTensorData<T>(op_context->output), op_context->output->dims->data, op_context->output->dims->size, GetTensorData<int>(op_context->axis), num_axis, op_context->params->keep_dims, GetTensorData<int>(temp_index), GetTensorData<int>(resolved_axis), init_value, reducer) was not true.\r\n\r\n\r\n", "@MeghaGhosh there is possibility that model won't work when the given input shape is not compatible with the model. Could you make sure that the input shapes being used meet input constraints?", "Hi @abattery yes we are taking the input shapes from the model itself and filling the object, the size of our object matches that expected by the model. This is our latest attempt, could you let us know if we're missing something?\r\n\r\n\r\n// input tensor\r\n        int[] inputshape;\r\n        DataType OinputputDataType = null;\r\n        HashMap inputProbabilityBuffers = new HashMap<>();\r\n        ByteBuffer xc;\r\n        Object[] input = new Object[3];\r\n        for (int k = 0; k < tfLite.getInputTensorCount(); k++) {\r\n            inputshape = tfLite.getInputTensor(k).shape();\r\n            OinputputDataType = tfLite.getInputTensor(k).dataType();\r\n            xc = TensorBuffer.createFixedSize(inputshape, OinputputDataType).getBuffer();\r\n            inputProbabilityBuffers.put(k, xc);\r\n            ByteBuffer temp = ByteBuffer.allocateDirect(xc.limit()); // Allocate with size of float\r\n            int[] shape = tfLite.getInputTensor(k).shape();\r\n\r\n            int na = xc.limit()/4;\r\n            for (int a = 0; a <na; a++) {\r\n\r\n                temp.putFloat(0);\r\n  \r\n            }\r\n            input[k] = temp;\r\n\r\n            Log.e(\"Edgar\", \"Shape of input tensor \" + \": \" + Arrays.toString(input));\r\n            tfLite.resizeInput(k, inputshape);\r\n        }\r\n        int[] OutputShape;\r\n        DataType OutputDataType;\r\n        HashMap outputProbabilityBuffers = new HashMap<>();\r\n        ByteBuffer x;\r\n        for (int i = 0; i < tfLite.getOutputTensorCount(); i++) {\r\n            OutputShape = tfLite.getOutputTensor(i).shape();\r\n            OutputDataType = tfLite.getOutputTensor(i).dataType();\r\n            x = TensorBuffer.createFixedSize(OutputShape, OutputDataType).getBuffer();\r\n            outputProbabilityBuffers.put(i, x);\r\n        }\r\n        try {\r\n            tfLite.runForMultipleInputsOutputs(input, outputProbabilityBuffers);\r\n            Log.e(\"Edgar\", \"\" + outputProbabilityBuffers.toString());\r\n        } catch (Exception e){\r\n            e.printStackTrace();\r\n        }\r\n\r\n\r\nShape of input tensor : [java.nio.DirectByteBuffer[pos=12582912 lim=12582912 cap=12582912], java.nio.DirectByteBuffer[pos=372 lim=372 cap=372], java.nio.DirectByteBuffer[pos=4190208 lim=4190208 cap=4190208]]\r\n6:42", "Could you share your input shape sizes? I would make sure that the benchmark tool can reproduce the problem with those sizes or not.", "Thanks! These are the input shape sizes:\r\n\r\n[1, 1024, 1024, 3]\r\n[1, 93]\r\n[1, 261888, 4]", "The tf-nightly version of benchmark tool on the desktop works well with the model and the above input shapes.\r\n\r\n@MeghaGhosh Can you share which version of aar files you are using?\r\n@thaink could you take a look at this issue?", "I'm using the aar file from the link shared in @terryheo 's comment.\r\n\r\nhttps://storage.googleapis.com/tensorflow-nightly-public/prod/tensorflow/google3/ubuntu_16/lite/nightly/511/20201214-223707/tensorflow-lite-select-tf-ops.aar", "@MeghaGhosh could you try the same version of tensorflow-lite.aar as well?\r\n\r\nhttps://storage.googleapis.com/tensorflow-nightly-public/prod/tensorflow/google3/ubuntu_16/lite/nightly/511/20201214-223707/tensorflow-lite.aar", "Yes we've been trying this, however this is leading to us not being able to use tensorflow-lite-support.\r\n\r\nThese implementations are leading to this error:\r\n\r\nimplementation 'org.tensorflow:tensorflow-lite-support:0.0.0-nightly'\r\ncompile files ('libs/tensorflow-lite-select-tf-ops.aar')\r\ncompile files ('libs/tensorflow-lite.aar')\r\n\r\nDuplicate class org.tensorflow.lite.DataType found in modules jetified-tensorflow-lite-0.0.0-nightly-runtime.jar (org.tensorflow:tensorflow-lite:0.0.0-nightly) and jetified-tensorflow-lite-runtime.jar (tensorflow-lite.aar)\r\nDuplicate class org.tensorflow.lite.DataType$1 found in modules jetified-tensorflow-lite-0.0.0-nightly-runtime.jar (org.tensorflow:tensorflow-lite:0.0.0-nightly) and jetified-tensorflow-lite-runtime.jar (tensorflow-lite.aar)\r\nDuplicate class org.tensorflow.lite.Delegate found in modules jetified-tensorflow-lite-0.0.0-nightly-runtime.jar (org.tensorflow:tensorflow-lite:0.0.0-nightly) and jetified-tensorflow-lite-runtime.jar (tensorflow-lite.aar)\r\nDuplicate class org.tensorflow.lite.Interpreter found in modules jetified-tensorflow-lite-0.0.0-nightly-runtime.jar (org.tensorflow:tensorflow-lite:0.0.0-nightly) and jetified-tensorflow-lite-runtime.jar (tensorflow-lite.aar)\r\nDuplicate class org.tensorflow.lite.Interpreter$Options found in modules jetified-tensorflow-lite-0.0.0-nightly-runtime.jar (org.tensorflow:tensorflow-lite:0.0.0-nightly) and jetified-tensorflow-lite-runtime.jar (tensorflow-lite.aar)\r\nDuplicate class org.tensorflow.lite.NativeInterpreterWrapper found in modules jetified-tensorflow-lite-0.0.0-nightly-runtime.jar (org.tensorflow:tensorflow-lite:0.0.0-nightly) and jetified-tensorflow-lite-runtime.jar (tensorflow-lite.aar)\r\nDuplicate class org.tensorflow.lite.Tensor found in modules jetified-tensorflow-lite-0.0.0-nightly-runtime.jar (org.tensorflow:tensorflow-lite:0.0.0-nightly) and jetified-tensorflow-lite-runtime.jar (tensorflow-lite.aar)\r\nDuplicate class org.tensorflow.lite.Tensor$QuantizationParams found in modules jetified-tensorflow-lite-0.0.0-nightly-runtime.jar (org.tensorflow:tensorflow-lite:0.0.0-nightly) and jetified-tensorflow-lite-runtime.jar (tensorflow-lite.aar)\r\nDuplicate class org.tensorflow.lite.TensorFlowLite found in modules jetified-tensorflow-lite-0.0.0-nightly-runtime.jar (org.tensorflow:tensorflow-lite:0.0.0-nightly) and jetified-tensorflow-lite-runtime.jar (tensorflow-lite.aar)\r\nDuplicate class org.tensorflow.lite.annotations.UsedByReflection found in modules jetified-tensorflow-lite-0.0.0-nightly-runtime.jar (org.tensorflow:tensorflow-lite:0.0.0-nightly) and jetified-tensorflow-lite-runtime.jar (tensorflow-lite.aar)\r\nDuplicate class org.tensorflow.lite.nnapi.NnApiDelegate found in modules jetified-tensorflow-lite-0.0.0-nightly-runtime.jar (org.tensorflow:tensorflow-lite:0.0.0-nightly) and jetified-tensorflow-lite-runtime.jar (tensorflow-lite.aar)\r\nDuplicate class org.tensorflow.lite.nnapi.NnApiDelegate$Options found in modules jetified-tensorflow-lite-0.0.0-nightly-runtime.jar (org.tensorflow:tensorflow-lite:0.0.0-nightly) and jetified-tensorflow-lite-runtime.jar (tensorflow-lite.aar)\r\nGo to the documentation to learn how to Fix dependency resolution errors.\r\n\r\nThis is solved as we remove implementation 'org.tensorflow:tensorflow-lite-support:0.0.0-nightly', however then we can't use the support functions such as TensorBuffer. \r\n\r\nHow can we work around this?\r\n\r\n", "Hey thanks for all the support, we could run the model on android with the following code: (It finally did run with the tensorflow-lite.aar version)\r\n\r\nObject[] inputs = new Object[] {new float[1][1024][1024][3], new float[1][93], new float[1][261888][4]};\r\nMap<Integer, Object> outputs = new HashMap<>();\r\noutputs.put(0, new float[1][1000][4]);\r\noutputs.put(1, new float[1][1000][6][4]);\r\noutputs.put(2, new float[1][1000][6]);\r\noutputs.put(3, new float[1][100][6]);\r\noutputs.put(4, new float[1][100][28][28][6]);\r\noutputs.put(5, new float[1][261888][4]);\r\noutputs.put(6, new float[1][261888][2]);\r\ntry {\r\n    tfLite.runForMultipleInputsOutputs(inputs, outputs);\r\n} catch (Exception e){\r\n    e.printStackTrace();\r\n}\r\n\r\nThe output tensor shapes returned by \"tflite.GetOutputTensor\" did not match 3 of the output shapes given here.. ", "Also, it's taking about 52 seconds to run (without GPU), is it taking the same amount of time on the benchmark tool as well?", "I read through the discussion. Thanks for the information and learned.\r\nCould I know how the performance of running Mask RCNN tflite on android (such as inference time) is? \r\nI successfully converted my trained Mask RCNN model to a tflite model, but still didn't get how to run it on Android by using Java codes. Are there any successful examples and demo codes?\r\nThanks", "I've the same issue.\r\n\r\n1. I used same config for Mask_RCNN from samples --> [Balloon](https://github.com/matterport/Mask_RCNN/tree/master/samples/balloon).\r\n2. Trained model on custom dataset. \r\n3. Successfully converted model from .h5(255.9MB) to .tflite(65.8MB), using this [article](https://wathek.medium.com/convert-mask-r-cnn-model-to-tflite-with-tensorflow-2-3-57160d3be18d).\r\n4. Simple pass .tflite file to [label_image.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/python/label_image.py) for inference check.\r\n\r\nAfter running this file, I also checked activity monitor to look at resources, it's taking ~90% CPU usage, but after 2/3 minutes, I got an error like this,\r\n**`zsh: segmentation fault  python label_image.py`**\r\n\r\nDon't know, what's the problem, because I didn't even modify any code. \r\n\r\nConfig:\r\nTF-2.4.0 on MacOs 10.15 using conda (CPU only)\r\n\r\n", "Hey @abattery @terryheo I am facing issues inferencing the code and getting the output in android. \r\nI have populated all the input values - image, image meta and anchors , however the outputs are not as expected at all. I have shared the codes here. \r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/46951\r\n\r\nAny help will be much appreciated!! "]}, {"number": 45630, "title": "Help with object detection project", "body": "Hi all,\r\n\r\nI successfully trained my custom ssd_mobilenet_v2_quantized_300x300_coco model with 7 classes to detect.\r\n\r\nHowever, I'm having the next issue when tried to run [this code](https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/blob/master/TFLite_detection_webcam.py) for object detection:\r\n\r\n`RuntimeError: tensorflow/lite/kernels/detection_postprocess.cc:404 ValidateBoxes(decoded_boxes, num_boxes) was not true.Node number 98 (TFLite_Detection_PostProcess) failed to invoke.`\r\n\r\nThe weirdest thing of all is the fact that if I run the exact same code in my workstation (with tf 1.15.1), the code works flawlessly, so it makes me think that there's something wrong with the raspberry.\r\n\r\nThe model was trained in Colab with tensorflow 1.15, and compiled with the following sintax:\r\n\r\n!tflite_convert --graph_def_file=compiler/tflite_graph.pb --output_file=compiler/detect.tflite --output_format=TFLITE --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' --inference_type=QUANTIZED_UINT8 --mean_values=128 --std_dev_values=128 --change_concat_input_ranges=false --allow_custom_ops\r\n\r\nAny help will be so pleased. Thanks in advance!", "comments": ["I also googled some answers, which said that downgrading to tf 1.13 will solve the issue. Done that, downgrading and upgrading to every version that i can but the issue still occurs...Thanks for all", "@egamez099 \r\nCan you please refer to [the comment](https://github.com/tensorflow/models/issues/7918#issuecomment-696805292) and let us know if it helps. [ also [this](https://stackoverflow.com/questions/65267578/project-with-object-detection-on-raspberry-pi-tensorflow) issue]", "Yes, actually tried to open the issue on EdjeElectronics repository as it's the author of the code that i'm using to test. Is there something that I've done wrong?", "@egamez099 How did you run the model on your workstation? Also, could you share what commands you ran before TFlite conversion? (As in, how did you get the intermediate graph for conversion to TFLite?) ", "I am trying to run the same object detection model and I get the same error message. \r\nI am using tf 2.3 and tf-nightly 2.5 though.\r\n\r\nBefore the conversion I got the intermediate graph via export_tflite_graph_tf2.py", "@LSnyd There is no quantized model in the TF2 [detection zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md). Which one did you download? And what command did you run for conversion? Note that the quantization parameters should not be specified if the model itself is not quantized.", "Ah sorry about that! I've used the SSD MobileNet v2 320x320 from the detection zoo. \r\n\r\nFor the conversion I ran\r\n`tflite_convert --saved_model_dir=/../my_model/saved_model --output_file=detect.tflite`\r\n\r\nI tried to use the  tf.lite.TFLiteConverter.from_saved_model() approach before, but that didn't work either. ", "@LSnyd  If you are following the instructions from [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tf2.md), the model *should* work fine. I am not fully sure of the inference code that @egamez099 pointed to, but anything equivalent to the `Test .tflite model` section in [this notebook](https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tflite.ipynb) should work (in whichever language you are coding).", "So I tried the following:\r\n\r\n1. Exported the inference graph with:\r\n\r\n`!python export_tflite_ssd_graph.py --pipeline_config_path=\"/content/drive/My Drive/Colab Data/models/research/object_detection/training/ssd_mobilenet_v1_quantized_300x300_coco14_sync.config\" --trained_checkpoint_prefix=training/model.ckpt-2178 --output_directory=compiler/ --add_postprocessing_op=true\r\n`\r\n\r\n2. Converted the model with:\r\n\r\n```\r\noutput_file=detect.t!tflite_convert \\\r\n--graph_def_file=compiler/tflite_graph.pb \\\r\n--output_file=compiler/detect.tflite \\\r\n--output_format=TFLITE \\\r\n--input_shapes=1,300,300,3 \\\r\n--input_arrays=normalized_input_image_tensor \\\r\n--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3'  \\\r\n--inference_type=QUANTIZED_UINT8 \\\r\n--mean_values=128 \\\r\n--std_dev_values=128 \\\r\n--change_concat_input_ranges=false \\\r\n--allow_custom_ops\r\n\r\n```\r\n\r\nSo, as you might notice, I changed the model to mobilenet v2 to v1, and surprisingly now runs fine on the rpi....Don't know why and how, but it runs flawlessly with the exact same code.\r\n\r\nNow the error happens when I compile the model on Ubuntu for the Coral TPU, and running with the same script...Things only got weirder at this time ", "@srjoglekar246  the inference code works fine, I've tested it on a pretrained model. \r\n\r\nWhen I followed the instructions that you pointed to, I didn't receive a meaningful model after conversion. The file was only a couple bytes large and netron didn't show any meaningful content within the model. I did not add a representative dataset though, so maybe that's the reason. \r\n\r\nWith `tflite_convert --saved_model_dir=/../my_model/saved_model --output_file=detect.tflite`\r\nI've received only a meaningful model for \r\n\r\n- ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8 \r\n- ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8\r\n\r\n when using tf-nightly==2.5.0.dev20201213. The newest tf-nightly release didn't work either. \r\n\r\nThe conversion for the ssd_mobilenet_v2_320x320_coco17_tpu-8 still does not work via the same approach. \r\nI'll just keep going with the ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8 model now. Thank you for your help! ", "Hello,\r\n\r\nI hope this is not hijacking the original problem from this ticket. But my issue looks quite similar to @egamez099 yours and it sounds like you got past to the edge-tpu step. Perhaps anyone can shed some light how they got past the conversion step? \r\n\r\nI'm on Google Colab.\r\n\r\nI've transferred learned the [ssd_mobilenet_v2_320x320_coco17_tpu-8](http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz) from tf2 model zoo. I'm trying to convert this model to tflite to uint8 (quantized) to work on the edge tpu for object detection. However, I am currently stuck on the converter step, not the edge-tpu compiling step. Here's my training [gist](https://gist.github.com/dragonSwords98/b6bfae0c5021044041a65040f03a281f) and my converting [gist](https://gist.github.com/dragonSwords98/bab7796969c7684b333aa5259cdaa351). They are a bit lengthy, the important things are at the bottom of the gist.\r\n\r\nIn my converting gist, I took the problem in a few directions and took clues from recent forums on this topic.\r\n\r\nWithout tf-nightly, using tensorflow 2.5 as of 2020/12/24, I encounter this error:\r\n```\r\n<unknown>:0: error: loc(callsite(callsite(\"Postprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/SortByField_1/Size@__inference_call_func_21233\" at \"StatefulPartitionedCall@__inference_signature_wrapper_21902\") at \"StatefulPartitionedCall\")): 'tf.Size' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):\r\n\ttf.Size {device = \"\"}\r\n```\r\n\r\nOnce I install tf-nightly and as of the v.12/24 and for the past few weeks of builds, have all brought me to the same errors.\r\n\r\nAttempt: _Whenever I try a representative dataset that casts my training images into float32_\r\n```python\r\nimport numpy as np\r\n\r\ndef representative_data_gen():\r\n  path = '/content/gdrive/My Drive/Colab Notebooks/deeppicar/images/lego-stop-signs/train'\r\n  dataset_list = tf.data.Dataset.list_files(path + '/*.jpg')  \r\n  for i in range(17):\r\n    image = next(iter(dataset_list))\r\n    image = tf.io.read_file(image)\r\n    image = tf.io.decode_jpeg(image, channels=3)\r\n    image = tf.image.resize(image, [300, 300])\r\n    image = tf.cast(image / 255., tf.float32)\r\n    image = tf.expand_dims(image, 0)\r\n    print(image)\r\n    yield [image]\r\n\r\nimport tensorflow as tf\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('/content/gdrive/MyDrive/Colab Notebooks/fine_tuned_model/saved_model')\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\nconverter.representative_dataset = tf.lite.RepresentativeDataset(representative_data_gen)\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.target_spec.supported_types = [tf.int8]\r\nquantized_model_tflite = converter.convert()\r\n\r\nwith open('/content/gdrive/MyDrive/Colab Notebooks/export-tflite/model-quantized-for-edge-tpu.tflite', 'wb') as f:\r\n  f.write(quantized_model_tflite)\r\n```\r\nError: \r\n`Cannot set tensor: Got value of type FLOAT32 but expected type UINT8 for input 0, name: serving_default_input_tensor:0`\r\n\r\n\r\nThere is not really any web search that yields someone with this exact FLOAT32 - UINT8 issue. However, I would assume this is because the input tensor images from the rep dataset maybe need to be in uint8. However:\r\n\r\nAttempt: _Whenever I try a representative dataset that casts my training images into uint8_\r\n\r\n```python\r\n...\r\n    image = tf.image.convert_image_dtype(\r\n    image, tf.uint8, saturate=False, name=None\r\n    )\r\n    image = tf.expand_dims(image, 0)\r\n    yield [image]\r\n```\r\n\r\nError:\r\n`Max and min for dynamic tensors should be recorded during calibration: Failed for tensor Cast\r\nEmpty min/max for tensor Cast`\r\n\r\nAttempt: Dummy dataset\r\nI also noticed that with my saved model, it does not even work with the dummy representative dataset provided in the tensorflow documentation. So maybe the issue is my own model, which trained successfully from loss of 3 to 0.48? Or is the issue with how I am using the tfliteconverter?\r\n\r\nDummy dataset I tried:\r\n```python\r\ndef representative_dataset():\r\n    for _ in range(100):\r\n      data = np.random.rand(1, 300, 300, 3)\r\n      yield [data.astype(np.float32)]\r\n```\r\nError: \r\n`Cannot set tensor: Got value of type FLOAT32 but expected type UINT8 for input 0, name: serving_default_input_tensor:0`\r\n\r\nI must be missing some important dtype knowledge, because most code snippets I have seen through web searching leave the representative dataset in float32. This current open ticket and this open ticket #45302 are the closest matches to my issue.\r\n\r\nI'm wondering if there's some model fundamentals or dtypes logic I need to read up on?\r\n\r\nIs anyone else trying to use tf2 detection models zoo to make some full integer, edge tpu compatible models too? Did anyone come up with a fallback if this method did not work?\r\n\r\nWhat's a good way to debug this?\r\n\r\nHopefully, others may benefit from solving this problem too?\r\n\r\nThanks, Happy Holidays all~\r\n", "@dragonSwords98 Before we look into quantization, it might be good to check if floating-point conversion & inference works for you as intended. Maybe using our Python API like the `Test .tflite model` section [here](https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tflite.ipynb).\r\n\r\nIn your gist, I see that the SavedModel path given to the TFLite Converter is not the same as what you output from `export_tflite_graph_tf2.py`. Or am I missing something? That script essentially creates an SSD model that is TFLite-compatible. The error you see seems to be from trying to convert a non-TFLite-compatible model.\r\n\r\nFor the representative dataset gen function, you can do something like this:\r\n\r\n```\r\nfor i in range(num_calibration_steps):\r\n    image = tf.io.read_file(os.path.join(data_dir, image_files[i]))\r\n    image = tf.compat.v1.image.decode_jpeg(image)\r\n    image = preprocess_fn(image)\r\n```\r\nYou can find an implementation of `preprocess_fn` [here](https://github.com/tensorflow/models/blob/master/research/slim/preprocessing/inception_preprocessing.py#L258). I *think* the preprocessing you are using is similar, though I haven't taken a deeper look.\r\n\r\nIf the model converts & does inferences fine on float, the quantization should happen as expected with the representative data gen code above.\r\n", "@srjoglekar246 Hello Sachin, thanks for the response.\r\n\r\nOne of the models I tried did end up passing inference tests as you suggested. Thanks for that. And that same model did work on the edge tpu.\r\n\r\nI may have too many models, so likely provided different pieces from various attempts. Apologies for that.\r\n\r\nAs you mentioned in your last paragraph, all these points checked out when I finally got it working. Thanks!\r\n\r\n"]}, {"number": 45607, "title": "TFLM: Adding ported and optimized operations for CEVA-BX1", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu\r\n- TensorFlow installed from (source or binary): source\r\n- Tensorflow version (commit SHA if source): nightly\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): CEVA-BX1\r\n\r\nI will be adding optimized operations for the CEVA-BX1 platform (later on for other CEVA platforms as well).\r\n\r\nStarting with fully connected but many others will follow: see PR: https://github.com/tensorflow/tensorflow/pull/45606\r\n\r\n", "comments": []}, {"number": 45594, "title": "CancelledError: [_Derived_]RecvAsync is cancelled. ", "body": "## Note\r\n\r\nI am opening this issue because the error I am describing seem to affect quite some people. See **Related Issues**, but those have been closed \"due to inactivity\".\r\n\r\n----\r\n\r\n**System information**\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16, 18\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1\r\n- Python version: 3.8\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: GeForce 1080 TI, 11 GB\r\n\r\n**Describe the current behavior**\r\n\r\nThe training fails seemingly randomly with `CancelledError:  [_Derived_]RecvAsync is cancelled`. All cases seem to have recurrent layer in common (see **Related Issues**).\r\n\r\nAfter starting, the training will run (in my case) for some time and then just crash with the above error. \r\n\r\n**Describe the expected behavior**\r\n\r\nDon't crash.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nThere are some in https://github.com/tensorflow/tensorflow/issues/33721.\r\n\r\n**Other info / logs**\r\n\r\n```\r\n2020-12-05 18:06:59.383572: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at cudnn_rnn_ops.cc:1517 : Unknown: CUDNN\r\n_STATUS_BAD_PARAM\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1484): 'cudnnSetTensorNdDescriptor( tensor_desc.get(), data_type, sizeof(dims) / sizeof(dims[0]), dims, strides)'\r\n2020-12-05 18:06:59.383906: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at cudnn_rnn_ops.cc:1517 : Unknown: CUDNN_STATUS_BAD_PARAM\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1484): 'cudnnSetTensorNdDescriptor( tensor_desc.get(), data_type, sizeof(dims) / sizeof(dims[0]), dims, strides)'\r\n2020-12-05 18:06:59.384114: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at cudnn_rnn_ops.cc:1517 : Unknown: CUDNN_STATUS_BAD_PARAM\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1484): 'cudnnSetTensorNdDescriptor( tensor_desc.get(), data_type, sizeof(dims) / sizeof(dims[0]), dims, strides)'\r\nTraceback (most recent call last):\r\n  File \"asr/bin/train_keras.py\", line 300, in <module>\r\n    app.run(main)\r\n  File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"asr/bin/train_keras.py\", line 236, in main\r\n    model.fit(\r\n  File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 108, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n  File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 1098, in fit\r\n    tmp_logs = train_function(iterator)\r\n  File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 780, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 807, in _call\r\n    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n  File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 2829, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1843, in _filtered_call\r\n    return self._call_flat(\r\n  File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1923, in _call_flat\r\n    return self._build_call_outputs(self._inference_function.call(\r\n  File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 545, in call\r\n    outputs = execute.execute(\r\n  File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.CancelledError:  [_Derived_]RecvAsync is cancelled.\r\n         [[{{node div_no_nan/ReadVariableOp_5/_1620}}]]\r\n         [[GroupCrossDeviceControlEdges_2/Identity_6/_1699]] [Op:__inference_train_function_159151]\r\n\r\nFunction call stack:\r\ntrain_function\r\n```\r\n\r\n**Related Issues**\r\n\r\n- https://github.com/tensorflow/tensorflow/issues/33721\r\n- https://github.com/tensorflow/tensorflow/issues/35523\r\n", "comments": ["@stefan-falk \r\n\r\nPlease, share colab link or simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "@ravikyram I am going to try and build a small example but the nature of this issue does not make this easy, as nobody seems to know where this issue is coming from (if one follows what people post e.g. in https://github.com/tensorflow/tensorflow/issues/33721).\r\n\r\nAt this point I think it would help if somebody could help us track the source of this issue down on our side. E.g. somebody who has _any_ clue whatsoever what `RecvAsync` means and does. From the name and the stack trace it seems to be an issue that occurs when using multiple GPUs although this might not always be the case?\r\n\r\nCould you mention somebody here who you think could help us all track this issue down somehow?", "@stefan-falk Is it possible to test with recent TF versions (`TF2.4rc4` and/or `tf-nightly`). Thanks!", "@jvishnuvardhan I can certainly try that. Can I simply upgrade to `TF2.4rc4` or does anything else change like CUDA/CUDNN?", "@jvishnuvardhan Apparently I cannot do this with nightly just like so. It appears that there are some breaking changes which I have to adapt first.\r\n\r\n### Update:\r\n\r\nI switched to `TF2.4rc4` because `tf-nightly` seems to require more changes. The training is running. In a few hours I should see whether it is still crashing.", "@jvishnuvardhan @ravikyram I have upgraded to ~2.4.rc4~ 2.4.0 (stable) and started some experiments. \r\n\r\nThe problem persists though. After a few hours of training, the program crashes.\r\n\r\nIs there anything else I can do i.o. to track this issue down?", "One of my latest changes was using `tf.data.experimental.bucket_by_sequence_length`. I've just removed it i.o. to see if the issue comes from there but it does not. \r\n\r\nWith that being said, I have no clue where this is coming from. I am 100% sure I didn't have this in 2.1.0 and I am not even sure if I had it with 2.3.0 but I certainly got that issue with 2.3.1. Not saying the problem is Tensorflow, maybe I am doing something wrong somewhere, but I have no idea how I could possibly track the source of this down.", "@jvishnuvardhan @ravikyram I think I have fixed the issue. The root of this was [`bucket_by_sequence_length`](https://www.tensorflow.org/api_docs/python/tf/data/experimental/bucket_by_sequence_length) and me setting `drop_remainder=False`.\r\n\r\nWhat seems to happen here is that there are batches which do not have enough samples s.t. there weren't enough examples for all cards. Since I set `drop_remainder=True` I don't get this error anymore. \r\n\r\nI don't know whether it is possible to raise an error or log a warning in such a case because the current error message is not really a good indicator i.o. to get an idea where to look.", "This code does not reproduces the error from above exactly but I think this is what is happening.\r\n\r\nIf I run the code below on e.g. 4 GPUs it will simply crash because there will be a batch with just one example.\r\n\r\nI guess we can expect something like this but to me it was not very obvious where to look.\r\n\r\n```python\r\nimport tensorflow as tf  # v2.4.0\r\nimport numpy as np\r\n\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\n\r\n\r\ndef sample_generator(nb_samples):\r\n\r\n    for i in range(nb_samples):\r\n        l = np.random.randint(6, 20)\r\n        yield np.random.rand(l, 8), np.random.rand(1, 1)\r\n\r\n    # One example for bucket (1, 5)\r\n    yield np.random.rand(3, 8), np.random.rand(1, 1)\r\n\r\n\r\ndef sample_len(sample, *_):\r\n    return tf.shape(sample)[0]\r\n\r\n\r\nnb_replica = max(1, len(tf.config.experimental.list_physical_devices('GPU')))\r\nassert nb_replica > 1, f'Number of GPUs must be >1 got {nb_replica}'\r\n\r\ndataset = tf.data.Dataset.from_generator(\r\n    lambda: sample_generator(500),\r\n    output_types=(tf.float32, tf.float32),\r\n    output_shapes=((None, 8), (None, 1))\r\n)\r\noptions = tf.data.Options()\r\noptions.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\r\ndataset = dataset.with_options(options)\r\n\r\nboundaries = [5, 10]\r\nbatch_sizes = [i * nb_replica for i in range(len(boundaries) + 1)]\r\n\r\nbucketing = tf.data.experimental.bucket_by_sequence_length(\r\n    sample_len,\r\n    bucket_boundaries=boundaries,\r\n    bucket_batch_sizes=batch_sizes,\r\n    drop_remainder=True\r\n)\r\n\r\ndataset = dataset.apply(bucketing).repeat()\r\n\r\nstrategy = tf.distribute.MirroredStrategy()\r\n\r\nwith strategy.scope():\r\n    inputs = layers.Input(shape=(None, 8))\r\n    x = inputs\r\n    x = layers.LSTM(16)(x)\r\n    x = layers.Dense(1)(x)\r\n    model = keras.Model(inputs=inputs, outputs=x)\r\n    model.compile(loss='mse')\r\n\r\nmodel.fit(\r\n    dataset,\r\n    epochs=2,\r\n    steps_per_epoch=100,\r\n)\r\n```\r\n\r\nOutput:\r\n\r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: 5 root error(s) found.\r\n  (0) Invalid argument:  Window size must be greater than zero, but got 0.\r\n\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\r\n\t [[RemoteCall]]\r\n\t [[IteratorGetNext]]\r\n  (1) Invalid argument:  Window size must be greater than zero, but got 0.\r\n\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\r\n\t [[RemoteCall]]\r\n\t [[IteratorGetNext]]\r\n\t [[RMSprop/Cast_10/ReadVariableOp/_8]]\r\n  (2) Invalid argument:  Window size must be greater than zero, but got 0.\r\n\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\r\n\t [[RemoteCall]]\r\n\t [[IteratorGetNext]]\r\n\t [[div_no_nan/ReadVariableOp_1/_64]]\r\n  (3) Invalid argument:  Window size must be greater than zero, but got 0.\r\n\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\r\n\t [[RemoteCall]]\r\n\t [[IteratorGetNext]]\r\n\t [[group_deps/_111]]\r\n  (4) Invalid argument:  Window size must be greater than zero, but got 0.\r\n\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\r\n\t [[RemoteCall]]\r\n\t [[IteratorGetNext]]\r\n\t [[RMSprop/Cast_3/ReadVariableOp/_6]]\r\n````", "I was running RNN on Kaggle. The error message I encounter is\r\n\r\n```\r\nCancelledError:  [_Derived_]RecvAsync is cancelled.\r\n\t [[{{node gradient_tape/sequential_4/embedding_4/embedding_lookup/Reshape/_24}}]] [Op:__inference_train_function_81303]\r\n```\r\n\r\nApparently something was not right with the embedding layer. This is my embedding layer:\r\n\r\n```python\r\nlayers.Embedding(\r\n    input_dim=SIZE_VOCAB,\r\n    output_dim=EMBED_DIM,\r\n    mask_zero=True,\r\n    input_length=MAX_SEQ_LEN,\r\n),\r\n```\r\n\r\nThe suspect is `mask_zero=True`. I set it to true according to [Understanding masking & padding](https://keras.io/guides/understanding_masking_and_padding/#masking), which allows the embedding layer to ignore the padded zeros.\r\n\r\nAfter I comment out `mask_zero=True`, the error does not occur anymore when using GPU, with all default settings.\r\n\r\nThe code example is available [here](https://www.kaggle.com/fanchenbao/movie-review-sentiment)", "Also getting this error after training for ~6 epochs. I am using a GRU with embeddings (tf nightly gpu 2.5.0-dev20210115).\r\nError below:\r\n```\r\nCancelledError                            Traceback (most recent call last)\r\n<timed exec> in <module>\r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n   1133                 _r=1):\r\n   1134               callbacks.on_train_batch_begin(step)\r\n-> 1135               tmp_logs = self.train_function(iterator)\r\n   1136               if data_handler.should_sync:\r\n   1137                 context.async_wait()\r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\def_function.py in __call__(self, *args, **kwds)\r\n    795     tracing_count = self.experimental_get_tracing_count()\r\n    796     with trace.Trace(self._name) as tm:\r\n--> 797       result = self._call(*args, **kwds)\r\n    798       compiler = \"xla\" if self._jit_compile else \"nonXla\"\r\n    799       new_tracing_count = self.experimental_get_tracing_count()\r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\def_function.py in _call(self, *args, **kwds)\r\n    823       # In this case we have created variables on the first call, so we run the\r\n    824       # defunned version which is guaranteed to never create variables.\r\n--> 825       return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n    826     elif self._stateful_fn is not None:\r\n    827       # Release the lock early so that multiple threads can perform the call\r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py in __call__(self, *args, **kwargs)\r\n   2970        filtered_flat_args) = self._maybe_define_function(args, kwargs)\r\n   2971     return graph_function._call_flat(\r\n-> 2972         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\r\n   2973 \r\n   2974   @property\r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1946       # No tape is watching; skip to running the function.\r\n   1947       return self._build_call_outputs(self._inference_function.call(\r\n-> 1948           ctx, args, cancellation_manager=cancellation_manager))\r\n   1949     forward_backward = self._select_forward_and_backward_functions(\r\n   1950         args,\r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py in call(self, ctx, args, cancellation_manager)\r\n    559               inputs=args,\r\n    560               attrs=attrs,\r\n--> 561               ctx=ctx)\r\n    562         else:\r\n    563           outputs = execute.execute_with_cancellation(\r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     58     ctx.ensure_initialized()\r\n     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n---> 60                                         inputs, attrs, num_outputs)\r\n     61   except core._NotOkStatusException as e:\r\n     62     if name is not None:\r\n\r\nCancelledError:  [_Derived_]RecvAsync is cancelled.\r\n\t [[{{node gradient_tape/model/txn_type_emb/embedding_lookup/Reshape/_352}}]] [Op:__inference_train_function_6250]\r\n\r\nFunction call stack:\r\ntrain_function\r\n```", "Any solution to this Multi GPU?\r\n\r\n```\r\nW tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at cudnn_rnn_ops.cc:1562 : UNKNOWN: CUDNN_STATUS_BAD_PARAM in tensorflow/stream_executor/cuda/cuda_dnn.cc(1588): 'cudnnSetTensorNdDescriptor( tensor_desc.get(), data_type, sizeof(dims) / sizeof(dims[0]), dims, strides)' Traceback (most recent call last):\r\n\r\n tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.CancelledError:  [_Derived_]RecvAsync is cancelled.\r\n\t [[{{node div_no_nan_1/ReadVariableOp_3/_40}}]] [Op:__inference_test_function_29082]\r\n```\r\n\r\nI am using \r\n\r\n``` tf.keras.utils.timeseries_dataset_from_array ```\r\nfrom [this link](https://www.tensorflow.org/api_docs/python/tf/keras/utils/timeseries_dataset_from_array)", "@stefan-falk thanks for digging into this! Definitely looks like a batch issue. I was using `batch_size=2` across 2 GPUs and it was failing because, like you said, the final batch would only have 1 piece of data to send across both GPUs (odd number of training samples). Bigger batch size has solved the problem.\r\n\r\nThanks for your help :)", "I have also suffered from the same issue in my NER model with mirrored strategy using 2 GPUs.\r\nI have tested with the example code in [Distributed training with Keras](https://www.tensorflow.org/tutorials/distribute/keras) and get to a conclusion that this issue is related to the number of samples in the last batch of data.\r\n\r\nThe environment I used is as follows:\r\n**OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\nTensorFlow installed from (source or binary): binary\r\nTensorFlow version (use command below): v2.5.0\r\nPython version: 3.8\r\nCUDA/cuDNN version: 11.2 (V11.2.152)\r\nGPU model and memory: GeForce 1080, 11 GB * 2 EA**\r\n\r\nThe example code uses the MNIST data, which has 60000 examples in training data and 10000 examples in test data.\r\nI fixed the (global) BATCH_SIZE to 100, and changed the number of samples in training data in range of [59900 - 60000].\r\nWith the 2 GPUs, each GPU gets 50 samples in each step (this is the replica batch size). With changing the number of samples in training data, the number of samples in the last batch also changes in range of [1 - 100]. I set drop_remainder to False (the default value) when making batch:\r\n\r\n`train_dataset = mnist_train.map(scale).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=False)`\r\n\r\nIn this experiment, when the number of samples in the last batch was 1 to 50, model.fit caused the CUDNN_STATUS_BAD_PARAM, while the number between 51 to 100 does not cause any error (It learned correctly with 2 GPUs). From this experiment, I get to a conclusion that **the issue seems to occur when the last batch has samples less than or equal to the replica batch size**. It seems that each replica does not get the same number of samples from one batch of data (I mean that non-even distribution among replica). For example, assume 2 GPUs (GPU0 and GPU1), 100 global batch size, 50 samples in the last batch. GPU0 seems to get the all first 50 samples and GPU1 gets no samples. I am not sure but I think this situation seems to cause the issue. To prevent this situation, **we could simply set drop_remainder to True** as commented by @stefan-falk or **select insufficient number of samples from training data (randomly or based on some criteria like sample's weight) and add them to training data** to make all batch have the same number (i.e., global batch size) of samples. The former is a very simple solution but loses some number of training samples while the latter does not. Through this approach, I could resolved this issue in my NER model training.\r\n\r\nValidation / test data may cause the same error. When I used the MNIST test data for validation with Model.fit(), the insufficient number of samples in the last validation batch does not cause any error. However, in my NER model training with CoNLL2003 data, the insufficient number of samples in the last validation batch caused the same error:\r\n```\r\nFunction call stack:\r\ntest_function\r\n\r\n```\r\nFor the validation or test data, I didn't use dropping or adding method because this method may make the result of evaluation incomparable. Instead of dropping or adding, I **dynamically changed the validation or test data batch size by increasing one by one until the number of samples in the last batch gets larger than replica batch size**. Through this method, I could solved the same issue in validation or test data.\r\n"]}, {"number": 45592, "title": "Keras doesn't release the memory after fit (single thread and multiprocessing)", "body": "**System information**\r\n- OS Platform: Windows, Linux\r\n- TensorFlow version ('v2.3.0-54-gfcc4b966f1', '2.3.1')\r\n- Python version: 3.8.6\r\n\r\n**Current behavior**\r\nThis problem is unfortunately blocking my production code.\r\nThe example below is oversimplified but it's perfectly able to reproduce the issue.\r\nI run many different models using different combinations of:\r\n- hyperparameters \r\n- features\r\n- samples\r\n\r\nFinally I save all the models for further analysis.\r\nIn the example below, for the sake of simplicity, I fit and save the same model many times, but as I said, in the production code it would be a different model for each run. \r\nI provide both single thread and parallel implementation .\r\n\r\n**1. Import** \r\n```\r\n# built in\r\nimport psutil\r\n\r\n# third party\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nfrom dask import compute, delayed\r\nfrom dask.distributed import Client\r\nfrom scikeras._utils import make_model_picklable \r\nfrom tensorflow.keras import Model\r\nfrom tensorflow.keras import regularizers\r\nfrom tensorflow.keras.backend import clear_session\r\nfrom tensorflow.keras.layers import (\r\n    Dense,\r\n    Input,\r\n    GRU,\r\n    BatchNormalization,\r\n    Dropout,\r\n    Conv1D,\r\n    Flatten\r\n)\r\n```\r\n\r\n**2. Keras model build function**\r\n```\r\ndef build_keras_model(\r\n        X,\r\n        nodes=15,\r\n        activation=\"selu\",\r\n        kernel_initializer='random_uniform',\r\n        regularizer_l1=0.05,\r\n        regularizer_l2=0.0,\r\n        recurrent_dropout=0.0,\r\n        dropout=0.0,\r\n        dense_units=tuple(),\r\n        batchnorm=True,\r\n        batchnorm_trainable=True,\r\n        batchnorm_training=False,\r\n        use_bias=False,\r\n        loss='mse',\r\n        optimizer='adam'\r\n):\r\n    shape = X.shape[1:]\r\n    inputs = Input(shape=shape, name='inputs')\r\n\r\n    x = Dropout(dropout)(inputs) if dropout else inputs\r\n\r\n    x = GRU(\r\n        nodes,\r\n        activation=activation,\r\n        recurrent_dropout=recurrent_dropout,\r\n        return_sequences=False,\r\n        kernel_initializer=kernel_initializer,\r\n        kernel_regularizer=regularizers.l1_l2(regularizer_l1, regularizer_l2),\r\n        activity_regularizer=regularizers.l1(0.0),\r\n        use_bias=use_bias\r\n    )(x)\r\n\r\n    dense_units = dense_units or ()\r\n    for n in dense_units:\r\n        x = Dense(\r\n            n,\r\n            activation=activation,\r\n            name=f'extra_dense{n}',\r\n            kernel_regularizer=regularizers.l1_l2(0.01, 0.01)\r\n        )(x)\r\n\r\n    if batchnorm:\r\n        x = BatchNormalization(trainable=batchnorm_trainable)(x, training=batchnorm_training)\r\n\r\n    x = Dense(\r\n        1,\r\n        activation='linear',\r\n        use_bias=use_bias,\r\n        kernel_initializer='random_uniform',\r\n        name='prediction'\r\n    )(x)\r\n\r\n    model = Model(inputs=inputs, outputs=x)\r\n    model.compile(\r\n        optimizer=optimizer,\r\n        loss=loss\r\n    )\r\n    make_model_picklable(model)\r\n    return model\r\n```\r\n\r\n**3. Functions**\r\n```\r\ndef get_memory_usage():\r\n    m_perc = psutil.virtual_memory().percent\r\n    m_used = round(psutil.virtual_memory().used / 10e8, 4)\r\n    return m_used, m_perc\r\n\r\n\r\ndef print_memory_usage(msg=0):\r\n    m_used, m_perc = get_memory_usage()\r\n    print(f'{msg}: memory used is {m_used}, {m_perc}% of total memory')\r\n\r\n\r\ndef fit_keras(model, X, y, **fit_params):\r\n    model.fit(X, y, batch_size=fit_params.pop('batch_size', len(X)), **fit_params)\r\n    return model\r\n\r\n\r\ndef run_keras_single(X, y, n_runs):\r\n    models = []\r\n    for i in range(n_runs):\r\n        print_memory_usage(i)\r\n        model = build_keras_model(X)\r\n        model.fit(X, y, batch_size=len(X), verbose=0)\r\n        models.append(model)\r\n    return models\r\n\r\n\r\ndef run_keras_multi(X, y, n_runs):\r\n    print_memory_usage('before')\r\n    keras_model = build_keras_model(X)\r\n    models = compute(*[delayed(fit_keras)(keras_model, X, y, epochs=20, verbose=0) for i in range(n_runs)])\r\n    print_memory_usage('after')\r\n    return models\r\n```\r\n\r\n **4. Data**\r\n```\r\nrng = np.random.default_rng(7)\r\nn_samples = 500\r\nn_timesteps = 10\r\nn_features = 50\r\nloc = 0\r\nscale = 0.01\r\n\r\nX = rng.normal(loc=loc, scale=scale, size=(n_samples, n_timesteps, n_features))\r\ny = rng.normal(loc=loc, scale=scale, size=(n_samples,))\r\n```\r\n\r\n**5. Run**\r\n```\r\nn_runs = 1000\r\n\r\nmodels_single = run_keras_single(X, y, n_runs)\r\nmodels_multi = run_keras_multi(X, y, n_runs)\r\n```\r\n\r\n**Conclusion**\r\nAfter running 1000 models and saving the results to the disk the saved models file size is about 10Mb. However the whole process both with and without multiprocessing requires almost 30GB of RAM which is not released after fitting.\r\n\r\nAny suggestion on how to solve or mitigate the issue would be much appreciated.\r\nMany thanks \r\nGio", "comments": ["Was able to reproduce the issue with TF v2.2, TF v2.3 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/03fd62c7da0095df4f73aef22bb3cd56/45592-tf-nightly.ipynb). Thanks!", "Thank you for the answer!\r\nI hope this can be solved soon.\r\nIf you need any test from my end please let me know.\r\n\r\n@amahendrakar when you say you were able to reproduce the issue with TF v2.2, TF v2.3 and TF-nightly, does that mean that the issue is not present on TF v2.1?\r\nBecause if that is the case I can try to downgrade version while I wait for this issue to be solved.\r\n\r\nMany thanks\r\nGio", "@jvishnuvardhan any update on this? ", "I was able to replicate this issue in TF 2.7 too. Attaching [Gist](https://colab.research.google.com/gist/mohantym/0956a766f12c176e50b8be82f4f8fb94/45592-tf-nightly.ipynb#scrollTo=n_PBYOJXr-X-) for Reference.Thanks!", "still nothing?"]}]