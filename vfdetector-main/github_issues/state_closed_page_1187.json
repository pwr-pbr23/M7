[{"number": 17577, "title": "Hide `os` from docs generator", "body": "Hide `os` so we don't generate api_docs for it", "comments": []}, {"number": 17576, "title": "Hide `os` from docs generator.", "body": " Delete `os` so the docs generator doesn't build docs for it.", "comments": []}, {"number": 17575, "title": "Fix pylint error in single_return.py", "body": "", "comments": ["cc @alexbw"]}, {"number": 17574, "title": "Correct curly brace typo", "body": "Curly brace required instead of right bracket for code display on getting started guide", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to `go/cla#troubleshoot`.\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "All these build checks for a typo? Haha\r\nChange just needed for r1.6; already fixed in r1.7 & master. Could someone merge? Thanks!"]}, {"number": 17573, "title": "contrib/lite: add missing include assert.h (spectrogram.cc)", "body": "tensorflow/contrib/lite/build_ios_universal_lib.sh fails with:\r\n\r\n    error: use of undeclared identifier 'assert'", "comments": ["@deltheil You legend!\r\nI can confirm this works on my Mac where current master 7ad74a0d66c5b8547382dfd3aad503288f051ae9 does not work!\r\n\r\nI followed instructions here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/ios.md\r\nI now have:\r\n```\r\n/tensorflow/contrib/lite/gen/lib/libtensorflow-lite.a\r\n```", "I can also confirm the app compiles and works with this code fix above! \ud83d\udc4d "]}, {"number": 17572, "title": "Update version string to 1.7.0rc0 everywhere.", "body": "", "comments": []}, {"number": 17571, "title": "keras.model_to_estimator feature request", "body": "Really does not depend on system information, just a completely new feature. \r\n\r\nUsing the:\r\n```\r\ntf.keras.estimator.model_to_estimator(keras_model=model,\r\n                                                    model_dir=model_dir)\r\n```\r\n\r\ndoes not allow us to log images in tensor board while training the model. I believe this is because the keras model is serialized and then rebuilt in this function and it does not capture the summaries that you put in the original graph. \r\n\r\nI am able to get around this by using a Lambda layer that looks like this:\r\n\r\n```\r\ndef viz_layer(x):\r\n    from tensorflow.summary import histogram\r\n    histogram('hist', x)\r\n    return x\r\n\r\nmodel.add(layers.Lambda(viz_layer))\r\n```\r\n\r\nSo two problems with this:\r\n\r\n1. It's pretty hacky to have to add identity layers in to the network to visualize\r\n2. I can't get the lambda layer to work without importing tensor flow inside the layer which is pretty bad\r\n\r\nIt would be great if the original summaries were captured or if there were a more streamlined way to add summaries. ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code: No\r\nOS Platform and Distribution:NA\r\nTensorFlow installed from: NA\r\nTensorFlow version: 1.6\r\nBazel version: NA\r\nCUDA/cuDNN version: NA\r\nGPU model and memory: NA\r\nExact command to reproduce: NA", "Is it related to https://github.com/tensorflow/tensorflow/issues/14879#issuecomment-351996902?", "Yes exactly!", "There are many users in sparse tickets that are asking full docs coverage of high level api. See also https://github.com/tensorflow/tensorflow/issues/14504#issuecomment-371195181", "Yeah, multi-gpu was literally the next question I was going to have. I'm not sure if I'd call this just docs coverage. I really don't think there is an effective way to add summary visualizations to keras models going into estimators. Seems to be a problem with: keras.models.clone_model", "You can use [SessionRunHook](https://www.tensorflow.org/api_docs/python/tf/train/SessionRunHook) but I don't know if it is the official solution.", "Is there an example doing this to modify a keras graph?", "No but you can take variables from the graph and add summary with standard TF calls inside this hook.", "This seems very funky. I can only use the hook in a train or evaluate. So for each evaluate I'll need to add the summaries to the graph? Do I need to do it just once?", "Was partially related to https://github.com/tensorflow/tensorflow/issues/15332", "Just a small followup here. I have gone ahead and added image summaries to the model, however I am unable to output image summaries to TB with the evaluation hook:\r\n\r\n```\r\n    est = tf.keras.estimator.model_to_estimator(\r\n        keras_model=model, model_dir=model_dir)\r\n\r\n    eval_summary_hook = tf.train.SummarySaverHook(save_steps=validation_steps,\r\n                                                  output_dir=os.path.join(\r\n                                                      model_dir, 'test_images'),\r\n                                                  summary_op=tf.summary.merge_all())\r\n\r\n    for _ in range(epochs):\r\n        est.train(lambda: make_train_iter(\r\n            'image_input'), steps=steps_per_epoch)\r\n        est.evaluate(lambda: make_test_iter('image_input'),\r\n                     steps=validation_steps, name='test', hooks=[eval_summary_hook])\r\n```\r\n\r\nThe error I get is: \r\n\r\n`ValueError: Fetch argument <tf.Tensor 'Merge/MergeSummary:0' shape=() dtype=string> cannot be interpreted as a Tensor. (Tensor Tensor(\"Merge/MergeSummary:0\", shape=(), dtype=string) is not an element of this graph.)`\r\n\r\nThis seems to imply the summaries that I merged from the keras estimator originally are forgotten when keras clones the model, so I only reference the old summaries. Super related to: #15332", "Nagging Assignee @fchollet: It has been 167 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This may be a case where using a model_fn that wraps your Keras model is the best strategy-- see option 2 [here](https://github.com/tensorflow/tensorflow/issues/19295#issuecomment-415892696). That would allow you to add summaries without trying to hack them into the graph, as the current expectation is that summaries are separate from the graph, controlled by callbacks.\r\n\r\nCC @omalleyt12 -- this is a good example of a use-case where users expect to have summaries in the graph. Do your planned updates for TB offer an easier solution here?\r\n\r\n@knathanieltucker , in the meantime, does the model_fn route work for you?", "That certainly works. \ud83d\udc4d ", "Great. Closing for now-- please reopen a new issue if you encounter any difficulties."]}, {"number": 17570, "title": "Run selective registration tool even if it's just been built", "body": "Follow-on to #14421 - when running `build_all_ios.sh -g path/to/model.pb` for the first time, the selective registration tool would be built, but not used.", "comments": []}, {"number": 17569, "title": "Problem w/ leaky_relu", "body": "### Describe the problem\r\nHaving problems using `tf.nn.leaky_relu`.  Attempted to deal with the problem by trying out `tf.nn.elu` and `tf.nn.relu`, but both functions resulted in a fairly significant drop in accuracy.  I was originally building the model in Keras, but switched to TensorFlow after having a problem with the corresponding leaky_relu function in Keras.  The TensorFlow and Keras source code and error logs are below.  Thank you in advance!\r\n\r\n### Source code / logs\r\nTensorFlow Source Code\r\n```\r\ndef conv2d_w_leakyrelu(x, W, b, s = 1, name = \"conlayer_i\"): \r\n    x = tf.nn.conv2d(input = x, filter = W, strides = [1, s, s, 1], data_format = \"NHWC\", padding = \"SAME\", name = name)\r\n    x = tf.nn.bias_add(value = x, bias = b)\r\n    # return tf.nn.elu(features = x) # TENSORFLOW PROBLEMS WITH LEAKY_RELU, SO USING ELU ACTIVATION FOR NOW\r\n    return tf.nn.leaky_relu(features = x, alpha = lr_alpha)\r\n```\r\n\r\nKeras Source Code\r\n```\r\nmodel.add(Conv2D(32, kernel_size = (im_h, im_w), strides = (s, s), padding = \"same\", kernel_initializer = KI, input_shape = input_shape))\r\n    model.add(LeakyReLU(alpha = leakiness))\r\n```\r\n\r\nBelow is the TensorFlow error message\r\n<img width=\"962\" alt=\"screen shot 2018-03-08 at 3 13 12 pm\" src=\"https://user-images.githubusercontent.com/10000384/37174004-489b7656-22e3-11e8-9118-060eba773965.png\">\r\n\r\nBelow is part of the Keras error message\r\n<img width=\"850\" alt=\"screen shot 2018-03-08 at 3 23 54 pm\" src=\"https://user-images.githubusercontent.com/10000384/37174559-df014b2e-22e4-11e8-961b-3be3f247cd6a.png\">\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "**Custom Code?**: No \r\n**TensorFlow Version:** 1.3.0\r\n**CUDA/cuDNN Version:** V8.0.61\r\n**GPU model and memory**: Tesla K80\r\n**Exact Command to Reproduce**: Call the functions", "Your TF version is too old.", "@ppwwyyxx Thank you."]}, {"number": 17568, "title": "tf.control_dependencies does not respect fed nodes", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian 9\r\n- **TensorFlow installed from (source or binary)**: pip\r\n- **TensorFlow version (use command below)**: 1.3.0\r\n- **Python version**: 2.7.13\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\n\r\n**TLDR**: I have a node `X` which has a control dependency on `Y`. I do `session.run(fetches=[X], feed_dict={Y: value})`. TensorFlow still tries to run `Y` despite it already being fed.\r\n\r\n**More details**:\r\n\r\nConsider the following code:\r\n\r\n    import tensorflow as tf\r\n    \r\n    p = tf.placeholder(tf.float32, name='p')\r\n    q = tf.add(p, 2.0)\r\n    with tf.control_dependencies([q]):\r\n      r = tf.constant(1.0)\r\n    \r\n    sess = tf.Session()\r\n    sess.run(r, feed_dict={q: 5})\r\n    \r\n    # InvalidArgumentError: You must feed a value for placeholder tensor 'p' with dtype float\r\n\r\nI would have expected the `sess.run` call to succeed because I'm already feeding `q`. ", "comments": ["A hacky workaround: `with tf.control_dependencies([tf.identity(q)]):`.\r\nThis is because control_dependencies add dependency to ops, not tensors.", "Nagging Assignee @bignamehyp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Agreed with ppwwyyxx. Does his answer resolve your issue?\r\n", "It works as a workaround, but it's not very nice as it requires figuring out which nodes to wrap, and then wrapping them with `tf.identity`. This is especially difficult when working with already exported models. Intuitively I would expect `control_dependencies` to behave as if added dependencies to Tensors, not Ops. Are there plans to change this behaviour in the future?", "It's not actually a workaround. Tensors are edges and Ops are nodes. Control dependencies are for nodes. Construction of nodes  within the \"with block\" depends on nodes in the control list. Control_dependencies is not a conditional.  You can't really have control dependencies over the edges/tensors. \r\n\r\nI agree it's a bit confusing to allow tensors in the control inputs. But when control input is a tensor, it will depend on tensor.op, which is the `Operation` that produces this tensor as an output (tf.add in your example).\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/ops.py#L4648\r\n\r\n", "Nagging Assignee @bignamehyp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @bignamehyp: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 17567, "title": "Update tensorrt import exception.", "body": "", "comments": ["cc @jjsjann123", "Thanks for making the change! I've pulled in and internal tests are running. Will update once they finish running.", "Internal tests have passed, FYI.", "Thanks for making the changes & re-running the test @yifeif @aaroey @gunan "]}, {"number": 17566, "title": "tensorflow-gpu pip package is not compatible with cuda9 docker image", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: \r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary (`pip install tensorflow-gpu`)\r\n- **TensorFlow version (use command below)**:\r\n1.6.0\r\n- **Python version**: \r\n2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\nCUDA 9, cuDNN 7\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\nI was trying to build a horovod image, but this would affect anyone using the `nvidia/cuda:9.0-cudnn7-devel-ubuntu16.04` base image:\r\n\r\n```shell\r\ndocker build -t horovod https://raw.githubusercontent.com/uber/horovod/master/Dockerfile\r\ndocker run -it --rm horovod python tensorflow_mnist.py\r\n```\r\n### Describe the problem\r\nWhen building a docker image based on `nvidia/cuda:9.0-cudnn7-devel-ubuntu16.04` and doing a `pip install tensorflow-gpu==1.6.0`, the resulting image causes a crash because the base image contains cuDNN 7.1, while the tensorflow-gpu pip package was built against cuDNN 7.0.\r\n\r\n### Source code / logs\r\nError messages:\r\n```shell\r\n2018-03-08 17:46:50.845206: E tensorflow/stream_executor/cuda/cuda_dnn.cc:378] Loaded runtime CuDNN library: 7101 (compatibility version 7100) but source was compiled with 7004 (compatibility version 7000).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.\r\n2018-03-08 17:46:50.845868: F tensorflow/core/kernels/conv_ops.cc:717] Check failed: stream->parent()->GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(), &algorithms) \r\n```\r\n\r\n@flx42 ", "comments": ["@gunan do you remember why you have a check on major/minor cuDNN version instead of just major version?", "@zheng-xq @martinwicke do you know why we check for cuDNN minor version?", "Hello,\r\n\r\nI had to rebuild my computer and am now experiencing the one of the errors described in the original post (see below).  Is there a recommended workaround?\r\n\r\nLoaded runtime CuDNN library: 7101 (compatibility version 7100) but source was compiled with 7004 (compatibility version 7000).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.", "If you use docker, you should pin the version of cuDNN you are installing. For instance:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/Dockerfile.devel-gpu#L16-L17\r\nBut that means not using the cuDNN official images, but a regular CUDA one.\r\n\r\nIf you are not using docker, you can still downgrade cuDNN with a similar command.", "Thank you for your reply.\r\n\r\nI had just solved it by updating Tensorflow.  Type \"pip install update\"", "@wdma How could you solve it by upgrading TF? I'm getting this error on the latest (1.6) TF.", "+1", "@adampl  Installing tensorflow per these instructions (https://www.tensorflow.org/install/) generates the above error.  Typing \"pip install update\" fixes it.  I hope this helps!", "I ended up doing what @flx42 advised (https://github.com/tensorflow/tensorflow/issues/17566#issuecomment-371901878) though it's not a perfect solution.", "If you use docker, I think you have 3 options:\r\n* Use the cuda base image (e.g. `nvidia/cuda:9.0-devel-ubuntu16.04`; note this doesn't have cuDNN), and install cuDNN 7.0 yourself, as I've done for horovod (https://github.com/uber/horovod/pull/206).\r\n* Use the cuda+cudnn base image (e.g. `nvidia/cuda:9.0-cudnn7-devel-ubuntu16.04`), but downgrade cuDNN to 7.0.  You need to do `apt-get install --allow-downgrades libcudnn7=7.0.5.15-1+cuda9.0`.\r\n* Use Tensorflow's docker image (`tensorflow/tensorflow:1.6.0-gpu`) as base.\r\n\r\nIf you don't use docker, just make sure your machine has cuDNN 7.0, not 7.1.", "@rongou I implemented your second suggestion in my [Dockerfile](https://github.com/Luke035/nvidia-anaconda-docker) and I've been able to run TF 1.6 along with KERAS 2.15  within the base image nvidia/cuda:9.0-cudnn7-runtime-ubuntu16.04.\r\nThe only thing I had to do was to add a RUN layer in my docker file for executing \"apt-get install --allow-downgrades libcudnn7=7.0.5.15-1+cuda9.0\". ", "@zheng-xq @martinwicke let me know if there is a problem with cuDNN that warrants this strict check!", "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:pip3 install --upgrade tensorflow-gpu\r\n- **TensorFlow version (use command below)**:1.6.0\r\n- **Python version**: Python 3.5.2\r\n- **Bazel version (if compiling from source)**:no\r\n- **GCC/Compiler version (if compiling from source)**:gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\n- **CUDA/cuDNN version**:CUDA 9, cuDNN v7.1.1 Library for Linux\r\n- **GPU model and memory**:1070\r\n\r\nHello, I leave my story installing cuda with the problems related to the messages below.\r\n\r\n```\r\n2018-03-13 10:19:33.118216: E tensorflow/stream_executor/cuda/cuda_dnn.cc:378] Loaded runtime CuDNN library: 7101 (compatibility version 7100) but source was compiled with 7004 (compatibility version 7000).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.\r\n2018-03-13 10:19:33.118929: F tensorflow/core/kernels/conv_ops.cc:717] Check failed: stream->parent()->GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(), &algorithms) \r\nAborted\r\n```\r\n\r\nI installed cuDNN v7.0.4 Library for Linux(the oldest version for cuda9.0) ([link](https://developer.nvidia.com/rdp/cudnn-download)) like belows.\r\ntar xzvf cudnn-9.0-linux-x64-v7.tgz\r\nsudo cp cuda/lib64/* /usr/local/cuda-9.0/lib64/\r\nsudo cp cuda/include/* /usr/local/cuda-9.0/include/\r\nsudo chmod a+r /usr/local/cuda-9.0/lib64/libcudnn*\r\nsudo chmod a+r /usr/local/cuda-9.0/include/cudnn.h\r\n\r\nNo Errors, works well.", "@flx42 I am tracking this down tomorrow.  The nightly docker is also broken due to the same issue, which means I cannot run nightly tests so I have some direct motivation.  I see two issues maybe more in summary of this thread:\r\n\r\n1.  Does the check need to be so stricket\r\n2. Why is the nightly docker broken?  I have a nightly perf test and should have caught this earlier but that is not the purpose of my perf tests and I was distracted.  \r\n\r\nMy suggestion will be to move cuDNN forward for the next release if possible, e.g. 1.7 unless it is too late and at a minimum switch the nightly as well as followup on the version check.   \r\n", "I think the biggest problem is the \"latest\" nvidia-docker images are cuda 9.1, cudnn 7.1.\r\nAnd our builds look for cuda 9 and cudnn 7.\r\nIn our nightlies, or tests the fix is to avoid using \"latest\" nvidia docker images.\r\n\r\nAlso, it is too late to change anything for 1.7. RC0 is almost out.", "The nightly docker we release is broken.  I ran into the problem trying to figure out why my perf tests stopped running.  We can at least pin the nightly docker image until 1.8 nightlies.  I do not think we need  to move to 7.1 per say as that also break people but I will ask XQ tomorrow about major/minor versions to see what he thinks and Martin as well.  I know you can but just to help track stuff down to help.  \r\n\r\nThat unblocks me and anyone using nightly docker.  I cannot believe you were awake to see my message.  :-)", "Thanks for following up on this.\r\nIt is a good idea to pin the nightly docker image bases, and going forward, I wonder if we can remove minor version checks on both CUDA and cuDNN.", "> I think the biggest problem is the \"latest\" nvidia-docker images are cuda 9.1, cudnn 7.1.\r\n\r\nWhat do you mean? Which dockerfile is that?", "Sorry, you are right.\r\nNot latest image, but rather all cudnn versions are marked \"cudnn7\" without minor version specification.", "If there is a good reason to have this check on the minor version (e.g. an incompatibility despite the SONAME), I might split future images with the cuDNN minor version.", "I believe that we have run into incompatibilities between some minor versions of CUDA, but I'm not sure whether we've ever seen that in cuDNN (@jlebar are there details?)", "@martinwicke, yeah, e.g. CUDA 9.0 and CUDA 9.1 are quite different in the respects we care about.\r\n\r\nFor cudnn, I have not seen a statement specifying their level of backwards compatibility.  I would *naively expect* that if you build against cudnn x.y and run with cudnn x.z for z >= y, it *probably will* work.  But to be comfortable with blessing that I'd want a statement in writing from nvidia.  (Perhaps such a statement already exists.)\r\n\r\nWhether or not it should be a fatal error in TF vs a \"good luck, you're on your own\" warning (like we do for known-broken ptxas versions), I don't have an opinion on.", "CUDA toolkit libraries and cuDNN have different SONAME, so that's actually expected.\r\n```\r\n$ objdump -p  /usr/lib/x86_64-linux-gnu/libcudnn.so.7 | grep SONAME\r\n  SONAME               libcudnn.so.7\r\n\r\n$ objdump -p  /usr/local/cuda-9.1/targets/x86_64-linux/lib/libcublas.so.9.1 | grep SONAME\r\n  SONAME               libcublas.so.9.1\r\n```", "> For cudnn, I have not seen a statement specifying their level of backwards compatibility. I would naively expect that if you build against cudnn x.y and run with cudnn x.z for z >= y, it probably will work. But to be comfortable with blessing that I'd want a statement in writing from nvidia. (Perhaps such a statement already exists.)\r\n\r\nStatement from NVIDIA:\r\n\r\nBeginning in cuDNN 7, binary compatibility of patch and minor releases is maintained as follows:\r\n* Any patch release x.y.z is forward- or backward-compatible with applications built against another cuDNN patch release x.y.w (i.e., of the same major and minor version number, but having w!=z).\r\n* cuDNN minor releases beginning with cuDNN 7 are binary backward-compatible with applications built against the same or earlier patch release (i.e., an app built against cuDNN 7.x is binary compatible with cuDNN library 7.y, where y>=x).\r\n\r\n(Note that this compatibility was not necessarily guaranteed in prior cuDNN major releases.)\r\n\r\n--Cliff Woolley\r\nDirector, DL Frameworks Engineering, NVIDIA", "Update.  I caught up with the build people on @gunan team and the nightly Docker will be fixed.  I need to talk to a few more people but I think we should consider a warning for a minor version difference and a fatal for major difference.  I have some concerns that there might be feature differences in point releases.  I am doing research and talking with people, it will likely take a few days.  I or someone will have a final statement.  \r\n\r\nb/74600152", "We will also update the cuDNN docs to say the same as what I posted above.  Thanks for pointing out the omission.", "@cliffwoolley  Thank you.  I am opening an internal issue and looking for someone to update the code to match your statement in cuDNN.  ", "Last update until done or change in progress.  I found someone to make the changes to match Cliff's cuDNN version position update.  Will post when complete and would not expect it to take very long.  Team effort I just get the honor of updating the github issue.  :-)  ", "@gunan Should we modify [Dockerfile.gpu](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/Dockerfile.gpu) to make it more similar to [Dockerfile.gpu-devel](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/Dockerfile.devel-gpu)? That is to say, `FROM nvidia/cuda:9.0-base-ubuntu16.04`, select the CUDA packages you need, and pin the cuDNN version.", "How much space will we save by doing that?\r\nit may still be worth it just to fix the cudnn issues.", "400 MB from a quick test, 300 MB if I re-add CUPTI. So it seems worth it, and it's always better to pin the version of a key dependency like cuDNN. It's better for reproducibility.", "Then this looks like it is worthwhile.\nThanks for the analysis! I can review,if you like to send the change.\n\nOn Wed, Mar 14, 2018 at 10:15 AM, Felix Abecassis <notifications@github.com>\nwrote:\n\n> 400 MB from a quick test, 300 MB if I re-add CUPTI. So it seems worth it,\n> and it's always better to pin the version of a key dependency like cuDNN.\n> It's better for reproducibility.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/17566#issuecomment-373102392>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHlCOdUpWzmocwEQ3V3NzFgdGFXoDNZLks5teVA6gaJpZM4SjG8W>\n> .\n>\n", "I'm having the same error with pip. I guess I'll try the Docker then", "Fixed in nightly builds.  We are now checking according to Cliff's update in nightly builds now and I am going to guess in TF 1.8 (100% not 1.7) because 1.8 has not been branched yet.:\r\n\r\n> Beginning in cuDNN 7, binary compatibility of patch and minor releases is maintained as follows:\r\n> \r\n> Any patch release x.y.z is forward- or backward-compatible with applications built against another cuDNN patch release x.y.w (i.e., of the same major and minor version number, but having w!=z).\r\n> cuDNN minor releases beginning with cuDNN 7 are binary backward-compatible with applications built against the same or earlier patch release (i.e., an app built against cuDNN 7.x is binary compatible with cuDNN library 7.y, where y>=x)."]}, {"number": 17565, "title": "Remove TF_NEED_KAFKA from configure.py as it is not needed anymore.", "body": "With the new update in kafka dataset support, it looks like TF_NEED_KAFKA is not needed any more as we are loading .so dynamically. For that I think it makes sense to remove TF_NEED_KAFKA option from configure.py.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["It looks like we may need the ability to remove dependency on Kafka conditionally.\r\nKafka depends on boringSSL, and boringSSL does not work on big endian systems, such as IBM s390x.", "Thanks @gunan. I updated the PR so that now kafka build will be skipped if `with_kafka_support=False` or if in Windows. Please take a look and let me know if there are any issues.", "I also would like to see how much of extra build the test infra has to do with kafka support default on.\r\nThis is pretty important as we are at the scale of 10s of thousands of CPUs. What is the extra sources we have to build with kafka default on? If it adds aa lot, I wold still go for turning it off and letting people enable it conditionally.\r\n\r\n@martinwicke how do you feel about having the apache kafka support on by default?", "I don't mind this if the added load isn't excessive. I would want to make sure that all ops which depend on Kafka exit gracefully and with a clear error if the required library is not found. \r\n\r\nIn generally, less questions in configure are better.", "@gunan Could you please help in merging this? Will this commit available in Tensorflow v1.7.0? Once merged, we will give a try to build on s390x to see if we still face the issue mentioned in\u00a0https://github.com/tensorflow/tensorflow/issues/17587\u00a0 "]}, {"number": 17564, "title": "Provide tensor/outer product method 'tf.outer'", "body": "System information\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux ubuntu 16.04\r\nTensorFlow installed from (source or binary): binary\r\nTensorFlow version (use command below): 1.4.1 gpu\r\nPython version: 3.5.4\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version: Cuda 8.0/Cudnn 6.0\r\nGPU model and memory: Titan xp\r\nExact command to reproduce: no method for outer product\r\n\r\nDescribe the problem\r\n\r\nTensor (outer product) is the fundamental operation on tensors, but there appears to be no method `tf.outer` in `tensorflow` analogous to `np.outer` in `numpy` to compute the outer product of arbitrary tensors. A google search pulls up these implementation suggestions on stackoverflow: https://stackoverflow.com/questions/33858021/outer-product-in-tensorflow, but these require the dimensions of the tensors to be accessed/known beforehand.\r\n\r\n\r\nI have the following hack. It would be nice to express this in terms of a pairwise (given associativity) operation `tf.outer`.\r\n\r\nThe tensor product for an arbitrary collection of tensors can be computed:\r\n```\r\ndef tensor_product(*e):\r\n    \"\"\" Tensor product of elements \"\"\"\r\n    if len(e) == 1:\r\n        return e\r\n    elif len(e) == 2:\r\n        a, b = e\r\n        r_a = len(a.get_shape().as_list())\r\n        r_b = len(b.get_shape().as_list())\r\n        s_a = tf.concat([tf.shape(a), tf.constant([1] * r_b)], axis=0)\r\n        s_b = tf.concat([tf.constant([1] * r_a), tf.shape(b)], axis=0)\r\n        a_reshaped = tf.reshape(a, s_a)\r\n        b_reshaped = tf.reshape(b, s_b)\r\n        return a_reshaped * b_reshaped\r\n    prod = e[0]\r\n    for tensor in e[1:]:\r\n        prod = tensor_product(prod, tensor)\r\n    return prod\r\n\r\n\r\n```\r\n\r\nThe tensor product allows more elegant expression of rank-2 and greater tensors in a loss function. \r\nFor example here is a diagonal and elliptical (weighted terms) quadratic term:\r\n\r\n```\r\ndef diagonal_M(batch_size, d):\r\n    \"\"\" M_abij = delta_ab delta_ij \"\"\"\r\n    return tensor_product(tf.diag([1.] * batch_size), tf.diag([1.] * d))\r\n\r\n```\r\n\r\n```\r\ndef biased_diagonal_M(batch_size, d):\r\n    \"\"\" M_abij = lambda_i delta_ab delta_ij \"\"\"\r\n    return tensor_product(tf.diag([1.] * batch_size), tf.diag([5.] + [1.] * (d-1)))\r\n\r\n```\r\nref: https://github.com/4d55397500/quadratic-forms-tensorflow", "comments": ["@rmlarsen : Any thoughts/suggestions on this?", "Bump- any interest?  I've tried to justify an outer product `tf.outer` analogous to `np.outer` as a useful basic tensor operation that will find application in loss functions, but someone else will have to comment on if the implementation effort is worthwhile.", "Bump", "Bump, no interest?", "This functionality already exists in [tf.tensordot](https://www.tensorflow.org/api_docs/python/tf/tensordot), which works like the [numpy.tensordot](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.tensordot.html) which does state the functionality in the documentation.\r\n\r\nCode for outer product is the following: \r\n```python\r\ntf.tensordot(tensor_a, tensor_b, axes=0)\r\n```\r\n(akin to `np.tensordot(tensor_a, tensor_b, axes=0)`\r\n\r\nDocumenting the outer product functionality of tensordot and/or creating an alias could fix this issue.", "thanks @swierh, I appreciate it. Looks like a documentation problem. I may in fact suggest implementing a `tf.outer` operation analogous to `np.outer.` Closing this issue"]}, {"number": 17563, "title": "Fix the tpu related broken link especially for imagenet_to_gcs.py", "body": "As you can see in [using_tpus](https://www.tensorflow.org/programmers_guide/using_tpu), the link of below \"a script\" is 404 and no longer valid due to tpu-demos repo moved to tpu repo and adjust its correponding structure.\r\n\r\nThis PR is to fix the below tpu related broken links and also some minor typo.\r\n\r\n> The TPU-demos repo includes [a script](https://github.com/tensorflow/tpu-demos/blob/master/cloud_tpu/datasets/imagenet_to_gcs.py) for downloading the imagenet dataset", "comments": ["Can any admin help review this PR?"]}, {"number": 17562, "title": "Fix markdown error in layers tutorial.", "body": null, "comments": ["ping"]}, {"number": 17561, "title": "Fix the messed up list format in using_tpu.md", "body": "As you can see in [using_tpu.md](https://www.tensorflow.org/programmers_guide/using_tpu), this PR is to fix below messed up format:\r\n\r\n> 1) Set FLAGS.use_tpu to True 1) Set FLAGS.tpu_name so the tf.contrib.cluster_resolver.TPUClusterResolver can find it 1) Set FLAGS.model_dir to a Google Cloud Storage bucket url (gs://).", "comments": []}, {"number": 17560, "title": "sess.run() returns invisible string for a tf.summary.merge_all() op. ", "body": "HI, \r\n\r\nI have a problem to read the outpur of sess.run:\r\nhere is the code:\r\n`        tf.summary.scalar('Loss', loss)\r\n        tf.summary.scalar('Accuracy', accuracy)\r\n        write_op = tf.summary.merge_all()\r\n       summary_train = sess.run(write_op, feed_dict=feed_dict_train)`\r\nwhere, write_op is a op from tf.summary.merge_all().\r\n\r\nThe output **summary_train** is something like: **\\n\\nLoss??\\n\\nAccuracy?????** \r\n\r\nDo you have any idea how I can get the numbers for the loss and accuracy from **summary_train**\r\n\r\nLook forward to your response\r\n\r\nHao", "comments": ["That string is a binary-encoded protocol buffer. If you want to render something more human readable, you'll need to parse it as a `tf.summary.Summary` object:\r\n\r\n```python\r\nsummary_train = sess.run(write_op, feed_dict=feed_dict_train)\r\n\r\nsummary_pb = tf.summary.Summary()\r\nsummary_pb.ParseFromString(summary_train)\r\nprint(summary_pb)\r\n```"]}, {"number": 17559, "title": "CPU Stoped working suddendly.", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Nagging Assignee @rohan100jain: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you."]}, {"number": 17558, "title": "Timeline showing replicated processes", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.6.0-rc1\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.11.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4\r\n- **CUDA/cuDNN version**: 9.1/7.0\r\n- **GPU model and memory**: Tesla k80 (11441MiB)\r\n- **Exact command to reproduce**:python cifar10_train.py\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nI am running the standard CIPHAR example available in tensor-flow repository. I just added a few lines as suggested in this (https://github.com/tensorflow/tensorflow/issues/1824#issuecomment-225754659) by @prb12. If I look at the timeline created for this:\r\n<img width=\"1440\" alt=\"screen shot 2018-03-08 at 10 58 30 am\" src=\"https://user-images.githubusercontent.com/10864603/37161719-78b01972-22c1-11e8-98a1-fc5213711316.png\">\r\nI see two processes running exactly at the same time. Is this a bug in the timeline or am I missing something here ? The other explanation to me looks like these two processes are running simultaneously and sharing the GPU which according to my knowledge is non-trivial. If, these two processes are actually sharing the GPU is there some code or documentation to understand how its being done by tensor-flow ? Thanks a lot for any help \r\n\r\n### Source code / logs\r\ncifar10 example  : https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10\r\nmy modified cifar10_train.py : https://gist.github.com/xilenteyex/bd8802d4baaf17acb8786ef0a9b60b7c ", "comments": ["Which two processes do you refer to? Note that pid 11 shows all streams on GPU while pip 15 shows one stream that launch most of the cuda kernels. It's expected to see pid 15 looks like pid 11.\r\n\r\n", "@bignamehyp Thanks for the clarification. I totally missed that.", "@xilenteyex  I _wish_ we could remove the string `PID` from the `chrome://tracing` UI since there are no processes involved here and it frequently leads to confusion.  To make the various TensorFlow devices show up on separate timelines we needed to pretend they were 'processes and threads' to the trace UI.  For those who are interested, the JSON trace file format is documented [here](https://github.com/catapult-project/catapult/wiki/Trace-Event-Format).\r\n\r\nThis is one drawback of 'borrowing' the trace viewer that's built into the web browser.  We _could_ build a modified version of the trace viewer but then people would have to locally build and install the tool etc.   Another option would be for us to build a nicer version into TensorBoard - but again the user would need to be running a TensorBoard service to look at traces.\r\n\r\n", "@prb12 thanks for the explanation. In addition to this, is there any documentation about the format of the json which is produced as an output of timeline profiling ?\r\n\r\nActually , I am working on creating an auto placer for graph nodes of Tensorflow jobs for the scenario when each worker has multiple computer resources i.e. gpu(s), cpu(s), tpu(s) etc. For that, I might need to come up with a cost model which will be used to assign cost to each node, device (compute resource) pair to find an optimal or close to optimal placement of nodes. So, Its highly likely that I end up writing my own version of better trace viewer for this because I will be initially doing a lot of profiling. \r\nSo, if you guys are interested in building a trace viewer which can be another tool like tensor board (but a very light weight one). I am more than happy to discuss and collaborate on the list of features one might want to have in such a tool.", "@xilenteyex My previous comment included a link to the JSON documentation.\r\n\r\nRegarding cost based placement - there is much prior work in this space... please consider taking a look at '[grappler](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/grappler)'   (@benoitsteiner)\r\n\r\nRegarding trace visualization, please also take a look at [this visualization tool](https://github.com/tensorflow/tensorflow/issues/4809#issuecomment-360616500) for distributed traces written by @xldrx. ", "@prb12 thanks a lot for the pointers. I will look into these.\r\n\r\nAlso, @benoitsteiner  is there any documentation available for grappler ?", "@tensorflowbutler I am waiting for @benoitsteiner's response ", "Nagging Assignee @bignamehyp: It has been 124 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 17557, "title": "Feature request: initializing an Estimator without training", "body": "Would it be possible to implement a method in Estimator to initialize the variables in the model? For example, it would then be possible to use `predict` before using `train` what is not possible actually. In some cases, the lack of this method could be very inconvenient.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 45 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 16 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 16 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 17556, "title": "C++ gradient for StridedSlice", "body": "See https://github.com/tensorflow/tensorflow/issues/9645", "comments": ["@suharshs ping?"]}, {"number": 17555, "title": "Nested while_loop does not work for automatic gradient derivation", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes  I have\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OSX 10.11.6\r\n- **TensorFlow installed from (source or binary)**: conda-forge\r\n- **TensorFlow version (use command below)**: 1.5.0\r\n- **Python version**: 3.6.0\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\n```python\r\ncell_fw = tf.nn.rnn_cell.GRUCell(num_units=hidden_dim)\r\ncell_bw = tf.nn.rnn_cell.GRUCell(num_units=hidden_dim)\r\ndef fn(inp):\r\n    (outputs_fw, outputs_bw), _ = \\\r\n        tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, inp[0], sequence_length=inp[1], dtype=tf.float32)\r\n        return tf.concat([outputs_fw, outputs_bw], axis=2)\r\noutputs = tf.map_fn(fn, (embedded, words_length), dtype=tf.float32)\r\n```\r\n\r\n### Describe the problem\r\nThis problem seems having to do with the nested `while_loop`. I have a 4-D tensor that I want to run through a bidirectional GRU with, so I used `tf.map_fn` to map the sub tensors (which are 3-D tensor now) to `bidirectional_rnn` with the same `GRU` cells. This was going fine until I asked `TensorFlow` to generate the automatic gradient descent, which threw error suggesting `while_loop` cannot be inside another `while_loop`. \r\n\r\n**Reproducible code is here**:\r\n```python\r\nnum_words = 1000\r\nembedding_dim = 100\r\nhidden_dim = 100\r\nnum_class = 20\r\n\r\nwords = tf.placeholder(tf.int32, [None, None, None], name='words')\r\nwords_length = tf.placeholder(tf.int32, [None, None], name='words_length')\r\nsentences_length = tf.placeholder(tf.int32, [None], name='sentences_length')\r\nlabels = tf.placeholder(tf.int32, [None], name='labels')\r\n\r\nwith tf.variable_scope('embeddings'):\r\n    embedding = \\\r\n        tf.get_variable('parameter', \r\n                        shape=(num_words, embedding_dim), \r\n                        dtype=tf.float32, trainable=True)\r\n    embedded  = tf.nn.embedding_lookup(embedding, words, name='lookup')\r\nwith tf.variable_scope('words_lstm'):\r\n    cell_fw = tf.nn.rnn_cell.GRUCell(num_units=hidden_dim)\r\n    cell_bw = tf.nn.rnn_cell.GRUCell(num_units=hidden_dim)\r\n    def fn(inp):\r\n        (outputs_fw, outputs_bw), _ = \\\r\n            tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, inp[0], sequence_length=inp[1], dtype=tf.float32)\r\n        return tf.concat([outputs_fw, outputs_bw], axis=2)\r\n    outputs = tf.map_fn(fn, (embedded, words_length), dtype=tf.float32)\r\nwith tf.variable_scope('words_attention'):\r\n    hidden = tf.layers.dense(outputs, units=hidden_dim * 2, activation=tf.nn.tanh)\r\n    attention = tf.layers.dense(outputs, units=1, activation=None)\r\n    attention = tf.transpose(tf.nn.softmax(tf.transpose(attention, perm=[0, 1, 3, 2])), perm=[0, 1, 3, 2])\r\noutputs = tf.reduce_sum(outputs * attention, axis=2)\r\nwith tf.variable_scope('sentence_lstm'):\r\n    cell_fw = tf.nn.rnn_cell.GRUCell(num_units=hidden_dim)\r\n    cell_bw = tf.nn.rnn_cell.GRUCell(num_units=hidden_dim)\r\n    (outputs_fw, outputs_bw), _ = \\\r\n        tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, outputs, sequence_length=sentences_length, dtype=tf.float32)\r\noutputs = tf.concat([outputs_fw, outputs_bw], axis=2)\r\nwith tf.variable_scope('sentence_attention'):\r\n    hidden = tf.layers.dense(outputs, units=hidden_dim * 2, activation=tf.nn.tanh)\r\n    attention = tf.layers.dense(hidden, units=1, activation=None)\r\n    attention = tf.transpose(tf.nn.softmax(tf.transpose(attention, perm=[0, 2, 1])), perm=[0, 2, 1])\r\noutputs = tf.reduce_sum(outputs * attention, axis=1)\r\nlogits = tf.layers.dense(outputs, units=num_class, activation=None)\r\nloss = tf.reduce_sum(tf.one_hot(labels, num_class) * tf.nn.softmax(logits), name='loss')\r\ntraining_op = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)\r\n```\r\n\r\n**Error**\r\n```python\r\nINFO:tensorflow:Cannot use 'words_lstm/map/while/bidirectional_rnn/fw/fw/strided_slice_1' as input to 'gradients/words_lstm/map/while/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/f_acc' because 'words_lstm/map/while/bidirectional_rnn/fw/fw/strided_slice_1' is in a while loop.\r\n\r\ngradients/words_lstm/map/while/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/f_acc while context: None\r\nwords_lstm/map/while/bidirectional_rnn/fw/fw/strided_slice_1 while context: words_lstm/map/while/while_context\r\n\r\nTraceback for gradients/words_lstm/map/while/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/f_acc:\r\n  File \"/Users/shengc/anaconda/envs/py36/bin/ipython\", line 6, in <module>\r\n    sys.exit(IPython.start_ipython())\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/__init__.py\", line 125, in start_ipython\r\n    return launch_new_instance(argv=argv, **kwargs)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\r\n    app.start()\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/terminal/ipapp.py\", line 356, in start\r\n    self.shell.mainloop()\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/terminal/interactiveshell.py\", line 480, in mainloop\r\n    self.interact()\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/terminal/interactiveshell.py\", line 471, in interact\r\n    self.run_cell(code, store_history=True)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2856, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-3-cda6375ef3e5>\", line 1, in <module>\r\n    get_ipython().run_line_magic('paste', '')\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2095, in run_line_magic\r\n    result = fn(*args,**kwargs)\r\n  File \"<decorator-gen-27>\", line 2, in paste\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/magic.py\", line 187, in <lambda>\r\n    call = lambda f, *a, **k: f(*a, **k)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/terminal/magics.py\", line 199, in paste\r\n    self.store_or_execute(block, name)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/terminal/magics.py\", line 57, in store_or_execute\r\n    self.shell.run_cell(b)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-3-0176473fb528>\", line 40, in <module>\r\n    training_op = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 355, in minimize\r\n    grad_loss=grad_loss)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 456, in compute_gradients\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 609, in gradients\r\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 375, in _MaybeCompile\r\n    return grad_fn()  # Exit early\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 609, in <lambda>\r\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_grad.py\", line 131, in _TensorArrayWriteGrad\r\n    grad = g.read(index)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 859, in read\r\n    return self._implementation.read(index, name=name)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 259, in read\r\n    name=name)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 4498, in _tensor_array_read_v3\r\n    dtype=dtype, name=name)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3160, in create_op\r\n    op_def=op_def)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1674, in __init__\r\n    self._control_flow_context.AddOp(self)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2251, in AddOp\r\n    self._AddOpInternal(op)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2274, in _AddOpInternal\r\n    real_x = self.AddValue(x)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2207, in AddValue\r\n    real_val = grad_ctxt.grad_state.GetRealValue(val)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1050, in GetRealValue\r\n    history_value = cur_grad_state.AddForwardAccumulator(cur_value)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 908, in AddForwardAccumulator\r\n    name=\"f_acc\")\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 3578, in _stack_v2\r\n    stack_name=stack_name, name=name)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3160, in create_op\r\n    op_def=op_def)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1625, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nTraceback for words_lstm/map/while/bidirectional_rnn/fw/fw/strided_slice_1:\r\n  File \"/Users/shengc/anaconda/envs/py36/bin/ipython\", line 6, in <module>\r\n    sys.exit(IPython.start_ipython())\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/__init__.py\", line 125, in start_ipython\r\n    return launch_new_instance(argv=argv, **kwargs)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\r\n    app.start()\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/terminal/ipapp.py\", line 356, in start\r\n    self.shell.mainloop()\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/terminal/interactiveshell.py\", line 480, in mainloop\r\n    self.interact()\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/terminal/interactiveshell.py\", line 471, in interact\r\n    self.run_cell(code, store_history=True)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2856, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-3-cda6375ef3e5>\", line 1, in <module>\r\n    get_ipython().run_line_magic('paste', '')\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2095, in run_line_magic\r\n    result = fn(*args,**kwargs)\r\n  File \"<decorator-gen-27>\", line 2, in paste\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/magic.py\", line 187, in <lambda>\r\n    call = lambda f, *a, **k: f(*a, **k)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/terminal/magics.py\", line 199, in paste\r\n    self.store_or_execute(block, name)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/terminal/magics.py\", line 57, in store_or_execute\r\n    self.shell.run_cell(b)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-3-0176473fb528>\", line 22, in <module>\r\n    outputs = tf.map_fn(fn, (embedded, words_length), dtype=tf.float32)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/functional_ops.py\", line 409, in map_fn\r\n    swap_memory=swap_memory)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2934, in while_loop\r\n    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2720, in BuildLoop\r\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2662, in _BuildLoop\r\n    body_result = body(*packed_vars_for_body)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/functional_ops.py\", line 399, in compute\r\n    packed_fn_values = fn(packed_values)\r\n  File \"<ipython-input-3-0176473fb528>\", line 20, in fn\r\n    (outputs_fw, outputs_bw), _ =             tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, inp[0], sequence_length=inp[1], dtype=tf.float32)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\", line 414, in bidirectional_dynamic_rnn\r\n    time_major=time_major, scope=fw_scope)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\", line 629, in dynamic_rnn\r\n    dtype=dtype)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\", line 688, in _dynamic_rnn_loop\r\n    time_steps = input_shape[0]\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 573, in _slice_helper\r\n    name=name)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 737, in strided_slice\r\n    shrink_axis_mask=shrink_axis_mask)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 5501, in strided_slice\r\n    name=name)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3160, in create_op\r\n    op_def=op_def)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1625, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\n\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py in _MaybeCompile(scope, op, func, grad_fn)\r\n    369     try:\r\n--> 370       xla_compile = op.get_attr(\"_XlaCompile\")\r\n    371       xla_separate_compiled_gradients = op.get_attr(\r\n\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in get_attr(self, name)\r\n   2172         raise ValueError(\r\n-> 2173             \"No attr named '\" + name + \"' in \" + str(self._node_def))\r\n   2174       x = self._node_def.attr[name]\r\n\r\nValueError: No attr named '_XlaCompile' in name: \"words_lstm/map/while/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3\"\r\nop: \"TensorArrayWriteV3\"\r\ninput: \"words_lstm/map/while/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3/Enter\"\r\ninput: \"words_lstm/map/while/bidirectional_rnn/fw/fw/while/Identity_1\"\r\ninput: \"words_lstm/map/while/bidirectional_rnn/fw/fw/while/Select\"\r\ninput: \"words_lstm/map/while/bidirectional_rnn/fw/fw/while/Identity_2\"\r\nattr {\r\n  key: \"T\"\r\n  value {\r\n    type: DT_FLOAT\r\n  }\r\n}\r\nattr {\r\n  key: \"_class\"\r\n  value {\r\n    list {\r\n      s: \"loc:@words_lstm/map/while/bidirectional_rnn/fw/fw/while/gru_cell/add\"\r\n    }\r\n  }\r\n}\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-3-0176473fb528> in <module>()\r\n     38 logits = tf.layers.dense(outputs, units=num_class, activation=None)\r\n     39 loss = tf.reduce_sum(tf.one_hot(labels, num_class) * tf.nn.softmax(logits), name='loss')\r\n---> 40 training_op = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)\r\n\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py in minimize(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\r\n    353         aggregation_method=aggregation_method,\r\n    354         colocate_gradients_with_ops=colocate_gradients_with_ops,\r\n--> 355         grad_loss=grad_loss)\r\n    356\r\n    357     vars_with_grad = [v for g, v in grads_and_vars if g is not None]\r\n\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py in compute_gradients(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, grad_loss)\r\n    454         gate_gradients=(gate_gradients == Optimizer.GATE_OP),\r\n    455         aggregation_method=aggregation_method,\r\n--> 456         colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n    457     if gate_gradients == Optimizer.GATE_GRAPH:\r\n    458       grads = control_flow_ops.tuple(grads)\r\n\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py in gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients)\r\n    607                 # functions.\r\n    608                 in_grads = _MaybeCompile(\r\n--> 609                     grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n    610               else:\r\n    611                 # For function call ops, we add a 'SymbolicGradient'\r\n\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py in _MaybeCompile(scope, op, func, grad_fn)\r\n    373       xla_scope = op.get_attr(\"_XlaScope\").decode()\r\n    374     except ValueError:\r\n--> 375       return grad_fn()  # Exit early\r\n    376\r\n    377   if not xla_compile:\r\n\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py in <lambda>()\r\n    607                 # functions.\r\n    608                 in_grads = _MaybeCompile(\r\n--> 609                     grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n    610               else:\r\n    611                 # For function call ops, we add a 'SymbolicGradient'\r\n\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_grad.py in _TensorArrayWriteGrad(op, flow)\r\n    129                                     colocate_with_first_write_call=False)\r\n    130        .grad(source=grad_source, flow=flow))\r\n--> 131   grad = g.read(index)\r\n    132   return [None, None, grad, flow]\r\n    133\r\n\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py in read(self, index, name)\r\n    857       The tensor at index `index`.\r\n    858     \"\"\"\r\n--> 859     return self._implementation.read(index, name=name)\r\n    860\r\n    861   @tf_should_use.should_use_result\r\n\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py in read(self, index, name)\r\n    257         flow_in=self._flow,\r\n    258         dtype=self._dtype,\r\n--> 259         name=name)\r\n    260     if self._element_shape:\r\n    261       value.set_shape(self._element_shape[0].dims)\r\n\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py in _tensor_array_read_v3(handle, index, flow_in, dtype, name)\r\n   4496     _, _, _op = _op_def_lib._apply_op_helper(\r\n   4497         \"TensorArrayReadV3\", handle=handle, index=index, flow_in=flow_in,\r\n-> 4498         dtype=dtype, name=name)\r\n   4499     _result = _op.outputs[:]\r\n   4500     _inputs_flat = _op.inputs\r\n\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)\r\n    785         op = g.create_op(op_type_name, inputs, output_types, name=scope,\r\n    786                          input_types=input_types, attrs=attr_protos,\r\n--> 787                          op_def=op_def)\r\n    788       return output_structure, op_def.is_stateful, op\r\n    789\r\n\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in create_op(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\r\n   3158         input_types=input_types,\r\n   3159         original_op=self._default_original_op,\r\n-> 3160         op_def=op_def)\r\n   3161     self._create_op_helper(ret, compute_shapes=compute_shapes,\r\n   3162                            compute_device=compute_device)\r\n\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in __init__(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\r\n   1672       control_flow_util.CheckInputFromValidContext(self, input_tensor.op)\r\n   1673     if self._control_flow_context is not None:\r\n-> 1674       self._control_flow_context.AddOp(self)\r\n   1675     self._recompute_node_def()\r\n   1676\r\n\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in AddOp(self, op)\r\n   2249             op_input_ctxt._AddOpInternal(op)\r\n   2250             return\r\n-> 2251     self._AddOpInternal(op)\r\n   2252\r\n   2253   def _AddOpInternal(self, op):\r\n\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in _AddOpInternal(self, op)\r\n   2272       for index in range(len(op.inputs)):\r\n   2273         x = op.inputs[index]\r\n-> 2274         real_x = self.AddValue(x)\r\n   2275         if real_x != x:\r\n   2276           op._update_input(index, real_x)\r\n\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in AddValue(self, val)\r\n   2205               forward_ctxt = forward_ctxt.GetWhileContext()\r\n   2206           if forward_ctxt == grad_ctxt.grad_state.forward_context:\r\n-> 2207             real_val = grad_ctxt.grad_state.GetRealValue(val)\r\n   2208             self._external_values[val.name] = real_val\r\n   2209             return real_val\r\n\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in GetRealValue(self, value)\r\n   1048           # Record the history of this value in forward_ctxt.\r\n   1049           self._grad_context.Exit()\r\n-> 1050           history_value = cur_grad_state.AddForwardAccumulator(cur_value)\r\n   1051           self._grad_context.Enter()\r\n   1052           break\r\n\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in AddForwardAccumulator(self, value, dead_branch)\r\n    906             max_size=maximum_iterations,\r\n    907             elem_type=value.dtype.base_dtype,\r\n--> 908             name=\"f_acc\")\r\n    909         # pylint: enable=protected-access\r\n    910       if curr_ctxt: curr_ctxt.Exit()\r\n\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py in _stack_v2(max_size, elem_type, stack_name, name)\r\n   3576     _, _, _op = _op_def_lib._apply_op_helper(\r\n   3577         \"StackV2\", max_size=max_size, elem_type=elem_type,\r\n-> 3578         stack_name=stack_name, name=name)\r\n   3579     _result = _op.outputs[:]\r\n   3580     _inputs_flat = _op.inputs\r\n\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)\r\n    785         op = g.create_op(op_type_name, inputs, output_types, name=scope,\r\n    786                          input_types=input_types, attrs=attr_protos,\r\n--> 787                          op_def=op_def)\r\n    788       return output_structure, op_def.is_stateful, op\r\n    789\r\n\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in create_op(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\r\n   3158         input_types=input_types,\r\n   3159         original_op=self._default_original_op,\r\n-> 3160         op_def=op_def)\r\n   3161     self._create_op_helper(ret, compute_shapes=compute_shapes,\r\n   3162                            compute_device=compute_device)\r\n\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in __init__(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\r\n   1670     self._control_flow_context = g._get_control_flow_context()  # pylint: disable=protected-access\r\n   1671     for input_tensor in self.inputs:\r\n-> 1672       control_flow_util.CheckInputFromValidContext(self, input_tensor.op)\r\n   1673     if self._control_flow_context is not None:\r\n   1674       self._control_flow_context.AddOp(self)\r\n\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_util.py in CheckInputFromValidContext(op, input_op)\r\n    198         input_op.name, \"\".join(traceback.format_list(input_op.traceback)))\r\n    199     logging.info(log_msg)\r\n--> 200     raise ValueError(error_msg + \" See info log for more details.\")\r\n\r\nValueError: Cannot use 'words_lstm/map/while/bidirectional_rnn/fw/fw/strided_slice_1' as input to 'gradients/words_lstm/map/while/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/f_acc' because 'words_lstm/map/while/bidirectional_rnn/fw/fw/strided_slice_1' is in a while loop. See info log for more details.\r\n```\r\n\r\n### Source code / logs\r\nsource code is here,\r\nhttps://github.com/shengc/tf-han/blob/master/hierarchical-attention-network.ipynb\r\ncalling `HierarchicalAttentionNetwork()._make_graph_batch(graph)` will produce the above error", "comments": ["This code runs just fine for me on TF nightly.  Have you tried it with TF 1.6?", "@ebrevdo not yet ... I am using conda-forge to install TF which only has 1.5 at the moment. I will try 1.6 and come back to comment. Thanks for your quick response!", "Indeed this issue no longer exists in TF 1.6 . Will close it. "]}, {"number": 17554, "title": "tf.train.Saver() protocol buffer saving issue", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.4.1 gpu\r\n- **Python version**: 3.5.4\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: Cuda 8.0/Cudnn 6.0\r\n- **GPU model and memory**: Titan xp\r\n- **Exact command to reproduce**: tf.train.Saver()\r\n\r\n### Describe the problem\r\nWhen using tf.train.Saver() if you set max_to_keep=None to keep all checkpoint files created then the protocol buffer saved at 'checkpoint' will only save the name of the latest checkpoint created and not all created checkpoints. \r\n\r\nWhen you set max_to_keep=N to be any other value then it will save the latest N checkpoint names as intended. \r\n\r\n\r\n", "comments": ["@rjpower assigning you because I saw you worked on this code recently but please reassign if you know of a better owner.", "Is this not the defined behavior (though perhaps it is not a desired behavior)? From https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/saver.py#L737:\r\n\r\n      max_to_keep: Maximum number of checkpoints to keep.  As new checkpoints\r\n        are created, old ones are deleted.  If None or 0, no checkpoints are\r\n        deleted from the filesystem but only the last one is kept in the\r\n        `checkpoint` file.  Presently the number is only roughly enforced.  For\r\n        example in case of restarts more than max_to_keep checkpoints may be\r\n        kept.\r\n\r\nSo no checkpoint data files should be deleted but only the last one should be in the checkpoint file. Is that not what is happening?\r\n", "Yeah that is what is happening. Probably just needs a mention in the documentation then that this happens if it is desired behavior as I didn't find it there, didn't think to look at the source code apologies.", "Agreed, I'll create a pull request shortly to update the docs. However I do wonder if this should be the desired behavior going forward and what the rationale for not keeping all the checkpoints in the checkpoint file is.", "added PR #17712 "]}, {"number": 17553, "title": "tf.python_io.TFRecordWriter() results in \"UnknownError: Failed to create a NewWriteableFile\"", "body": "### System information\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):**No**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows 7, 64 Bit**\r\n- TensorFlow installed from (source or binary): **TensorFlow installed from Conda**\r\n- TensorFlow version (use command below): **1.6.0**\r\n- Python version: **Python 3.6.3**\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: **9.1.85**\r\n- GPU model and memory: **NVIDIA GeForce 820M**\r\n- Exact command to reproduce: \r\nimport tensorflow as tf\r\ntf.python_io.TFRecordWriter('C:\\\\Users\\\\user_name\\\\Documents')\r\n\r\nimport tensorflow as tf\r\ntf.python_io.TFRecordWriter('Any_path')\r\n\r\n### Problem\r\ntf.python_io.TFRecordWriter('path') doesn't create output file and returns following error:\r\n_UnknownError: Failed to create a NewWriteableFile: 'path' : Access is denied.\r\n; Input/output error._\r\n\r\n### Troubleshooting\r\nTried with different folders and drives as \"path\". But error is consistent \r\n\r\n### Source code / logs\r\n\r\nimport tensorflow as tf\r\nC:\\Users\\Sethu\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n\r\ntf.python_io.TFRecordWriter('C:\\\\Users\\\\Sethu\\\\Documents')\r\n---------------------------------------------------------------------------\r\nUnknownError                              Traceback (most recent call last)\r\n<ipython-input-2-1fe0cde88225> in <module>()\r\n----> 1 tf.python_io.TFRecordWriter('C:\\\\Users\\\\Sethu\\\\Documents')\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\lib\\io\\tf_record.py in __init__(self, path, options)\r\n    109     with errors.raise_exception_on_not_ok_status() as status:\r\n    110       self._writer = pywrap_tensorflow.PyRecordWriter_New(\r\n--> 111           compat.as_bytes(path), compat.as_bytes(compression_type), status)\r\n    112 \r\n    113   def __enter__(self):\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)\r\n    514             None, None,\r\n    515             compat.as_text(c_api.TF_Message(self.status.status)),\r\n--> 516             c_api.TF_GetCode(self.status.status))\r\n    517     # Delete the underlying status object from memory otherwise it stays alive\r\n    518     # as there is a reference to status from this from the traceback due to\r\n\r\nUnknownError: Failed to create a NewWriteableFile: C:\\Users\\Sethu\\Documents : Access is denied.\r\n; Input/output error\r\n", "comments": ["Is `\"C:\\Users\\Sethu\\Documents\"` the name of an existing directory on your computer?", "Yes, it is an existing folder in my computer.", "That explains the \"Access is denied\" error. The `path` argument to [`tf.python_io.TFRecordWriter()`](https://www.tensorflow.org/api_docs/python/tf/python_io/TFRecordWriter#__init__) must be a *file* name (typically the name of a file that does not exist) and not a directory name.", "Thanks you so much mrry. I guess your answer has helped me multiple times. I really hate tensorflow. I will move on to pytorch soon. But thanks again.", "mrry, thanks as well! Now I see 'c.\t[PATH_TO_ANNOTATIONS_FOLDER]/train.record' what this means!  And now it seems so obvious. I'm glad it was a much easier fix than it looked."]}, {"number": 17552, "title": "train on new dataset", "body": "i want to detect people wearing a hat.can i directly label those people wearing hats as a class  and train on this dataset?can cnn distinguish people wearing hats from those who don't?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "thank you for your reply .In my case the fields in the issue template are irrelevant.\r\nHave I written custom code            N/A\r\nOS Platform and Distribution          N/A\r\nTensorFlow installed from               N/A\r\nTensorFlow version                          N/A\r\nBazel version                                   N/A\r\nCUDA/cuDNN version                    N/A\r\nGPU model and memory                N/A\r\nExact command to reproduce         N/A", "@misstong Yes, CNNs can do that.\r\nBtw, this issue is not about TF bugs/feature requests. Please ask it on StackOverflow or other websites.", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 17551, "title": "SYCL with ComputeCpp: local_config_sycl has multiple matches", "body": "This patch fixes issue reported https://github.com/lukeiwanski/tensorflow/issues/197", "comments": []}, {"number": 17550, "title": "can we compute sparse matrix gradient in TF?", "body": "In tensor flow (TF) , to compute gradients we have to pass some variable. Sparse tensors cannot be used as variables. Can you please tell me is there any solution for sparse matrix gradient ?\r\nN/A", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!\r\n\r\n(And if posting the question, I'd encourage you to add more detail - for example, a code snippet that you expect to work but doesn't. Or some more detail about what you're trying to achieve. Thanks!)"]}, {"number": 17549, "title": "Setting up CI jobs for Windows Bazel build", "body": "Now tensorflow/contrib is enabled in the Bazel Windows build. (https://github.com/tensorflow/tensorflow/pull/16659)\r\n\r\nWe can start working on setting up kokoro jobs for Bazel Windows build.\r\n\r\n- [ ] Github presubmit\r\n- [ ] Github postsubmit\r\n- [ ] Internal presubmit\r\n- [ ] Internal postsubmit\r\n", "comments": ["@gunan @yifeif ", "this is all internal work. Let's create an internal bug for this."]}, {"number": 17548, "title": "Compilation flags are not always passed.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Fedora 27\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: b'v1.6.0-0-gd2e24b6039' 1.6.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: 0.10.1- (@non-git)\r\n- **GCC/Compiler version (if compiling from source)**: 6.4\r\n- **CUDA/cuDNN version**: 9.1/7.0 (also with 9.0)\r\n- **GPU model and memory**: 1070 Ti\r\n- **Exact command to reproduce**: See building script\r\n\r\nBuilding with enabled MPI against Openmpi fails, as mpicxx.h is not included. This is despite explicitly passing the flag `-DOMPI_SKIP_MPICXX`. I am also passing the `march=native` flag, and once installed, Tensorflow doesn't complain about my CPU having more capabilities, so this seems to be passed at least some of the time.\r\n\r\nAdding `#define OMPI_SKIP_MPICXX` in `tensorflow/contrib/mpi/mpi_utils.h` circumvents the problem, and Tensorflow compiles nicely.\r\n\r\nFull error message:\r\n```\r\nERROR: /home/david/gits/tensorflow/tensorflow/contrib/mpi/BUILD:60:1: C++ compilation of rule '//tensorflow/contrib/mpi:mpi_rendezvous_mgr' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd /home/david/.cache/bazel/_bazel_david/47d00ffdd2fc0515138a34f138cebd63/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    CUDA_TOOLKIT_PATH=/usr/local/cuda/ \\\r\n    CUDNN_INSTALL_PATH=/usr/local/cuda-9.1 \\\r\n    GCC_HOST_COMPILER_PATH=/home/david/.local/bin/gcc6.4 \\\r\n    LD_LIBRARY_PATH=/usr/lib64/openmpi/lib \\\r\n    PATH=/usr/lib64/openmpi/bin:/home/david/.local/bin:/home/david/.local/hmmer3.1/bin:/home/david/.virtualenvs/py36/bin:/usr/libexec/python3-sphinx:/home/david/.local/bin:/home/david/.local/hmmer3.1/bin:/home/david/.virtualenvs/py36/bin:/usr/lib64/qt-3.3/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/usr/local/cuda/bin:/home/david/.local/bin:/home/david/bin:/usr/local/cuda/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/home/david/.virtualenvs/py36/bin/python \\\r\n    PYTHON_LIB_PATH=/home/david/.virtualenvs/py36/lib/python3.6/site-packages \\\r\n    TF_CUDA_CLANG=0 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \\\r\n    TF_CUDA_VERSION=9.1 \\\r\n    TF_CUDNN_VERSION=7 \\\r\n    TF_NEED_CUDA=1 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK '-std=c++11' -MD -MF bazel-out/k8-py3-opt/bin/tensorflow/contrib/mpi/_objs/mpi_rendezvous_mgr/tensorflow/contrib/mpi/mpi_rendezvous_mgr.pic.d '-frandom-seed=bazel-out/k8-py3-opt/bin/tensorflow/contrib/mpi/_objs/mpi_rendezvous_mgr/tensorflow/contrib/mpi/mpi_rendezvous_mgr.pic.o' -fPIC -DEIGEN_MPL2_ONLY -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DTENSORFLOW_USE_JEMALLOC -DTENSORFLOW_USE_ABSL -DTF_USE_SNAPPY -DTENSORFLOW_USE_MPI -DTENSORFLOW_USE_GDR '-DGRPC_ARES=0' -iquote . -iquote bazel-out/k8-py3-opt/genfiles -iquote external/protobuf_archive -iquote bazel-out/k8-py3-opt/genfiles/external/protobuf_archive -iquote external/bazel_tools -iquote bazel-out/k8-py3-opt/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/k8-py3-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/k8-py3-opt/genfiles/external/local_config_sycl -iquote external/com_google_absl -iquote bazel-out/k8-py3-opt/genfiles/external/com_google_absl -iquote external/nsync -iquote bazel-out/k8-py3-opt/genfiles/external/nsync -iquote external/jemalloc -iquote bazel-out/k8-py3-opt/genfiles/external/jemalloc -iquote external/gif_archive -iquote bazel-out/k8-py3-opt/genfiles/external/gif_archive -iquote external/jpeg -iquote bazel-out/k8-py3-opt/genfiles/external/jpeg -iquote external/com_googlesource_code_re2 -iquote bazel-out/k8-py3-opt/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/k8-py3-opt/genfiles/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/k8-py3-opt/genfiles/external/fft2d -iquote external/highwayhash -iquote bazel-out/k8-py3-opt/genfiles/external/highwayhash -iquote external/png_archive -iquote bazel-out/k8-py3-opt/genfiles/external/png_archive -iquote external/zlib_archive -iquote bazel-out/k8-py3-opt/genfiles/external/zlib_archive -iquote external/local_config_cuda -iquote bazel-out/k8-py3-opt/genfiles/external/local_config_cuda -iquote external/grpc -iquote bazel-out/k8-py3-opt/genfiles/external/grpc -isystem external/protobuf_archive/src -isystem bazel-out/k8-py3-opt/genfiles/external/protobuf_archive/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/eigen_archive -isystem bazel-out/k8-py3-opt/genfiles/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/k8-py3-opt/genfiles/external/nsync/public -isystem external/jemalloc/include -isystem bazel-out/k8-py3-opt/genfiles/external/jemalloc/include -isystem external/gif_archive/lib -isystem bazel-out/k8-py3-opt/genfiles/external/gif_archive/lib -isystem external/farmhash_archive/src -isystem bazel-out/k8-py3-opt/genfiles/external/farmhash_archive/src -isystem external/png_archive -isystem bazel-out/k8-py3-opt/genfiles/external/png_archive -isystem external/zlib_archive -isystem bazel-out/k8-py3-opt/genfiles/external/zlib_archive -isystem external/local_config_cuda/cuda -isystem bazel-out/k8-py3-opt/genfiles/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-py3-opt/genfiles/external/local_config_cuda/cuda/cuda/include -isystem external/local_config_cuda/cuda/cuda/include/crt -isystem bazel-out/k8-py3-opt/genfiles/external/local_config_cuda/cuda/cuda/include/crt -isystem external/grpc/include -isystem bazel-out/k8-py3-opt/genfiles/external/grpc/include -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fno-canonical-system-headers -c tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc -o bazel-out/k8-py3-opt/bin/tensorflow/contrib/mpi/_objs/mpi_rendezvous_mgr/tensorflow/contrib/mpi/mpi_rendezvous_mgr.pic.o)\r\nIn file included from ./tensorflow/contrib/mpi/mpi_utils.h:28:0,\r\n                 from ./tensorflow/contrib/mpi/mpi_rendezvous_mgr.h:34,\r\n                 from tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc:18:\r\n./third_party/mpi/mpi.h:2704:41: fatal error: openmpi/ompi/mpi/cxx/mpicxx.h: No such file or directory\r\n #include \"openmpi/ompi/mpi/cxx/mpicxx.h\"\r\n                                         ^\r\ncompilation terminated.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 1153.570s, Critical Path: 141.81s\r\n```\r\n\r\nBuilding script:\r\n```\r\nset -e\r\n\r\nexport PYTHON_BIN_PATH=`which python`\r\nexport PYTHON_LIB_PATH=/home/david/.virtualenvs/py36/lib/python3.6/site-packages\r\n\r\nexport GCC_HOST_COMPILER_PATH=$HOME/.local/bin/gcc6.4\r\nexport COMPUTECPP_TOOLKIT_PATH=$HOME/.local/bin/g++6.4\r\n\r\nexport TF_NEED_CUDA=1\r\nexport TF_CUDA_VERSION=9.1\r\nexport CUDA_TOOLKIT_PATH=/usr/local/cuda/\r\nexport TF_CUDNN_VERSION=7\r\nexport CUDNN_INSTALL_PATH=$CUDA_TOOLKIT_PATH\r\nexport TF_CUDA_COMPUTE_CAPABILITIES=6.1\r\nexport TF_CUDA_CLANG=0\r\nexport TF_NEED_OPENCL_SYCL=0\r\n\r\nexport TF_NEED_JEMALLOC=1\r\nexport TF_NEED_HDFS=0\r\nexport TF_NEED_GCP=0\r\nexport TF_NEED_S3=0\r\nexport TF_ENABLE_XLA=1\r\nexport TF_NEED_GDR=1\r\nexport TF_NEED_VERBS=0\r\nexport TF_NEED_MPI=1\r\nexport CC_OPT_FLAGS=\"-DOMPI_SKIP_MPICXX -O2 -pipe -march=native\"\r\nexport TF_SET_ANDROID_WORKSPACE=0\r\nexport TF_NEED_KAFKA=0\r\nexport TF_NEED_TENSORRT=0\r\n\r\n\r\ncd tensorflow\r\n./configure &&\r\nbazel build -c opt  --config=cuda --verbose_failures //tensorflow/tools/pip_package:build_pip_package &&\r\nbazel-bin/tensorflow/tools/pip_package/build_pip_package built_wheels/\r\n```\r\n\r\nRelevant extracto of `.tf_configure.bazelrc` showing the flags being saved\r\n\r\n```build:opt --copt=-DOMPI_SKIP_MPICXX\r\nbuild:opt --copt=-O2\r\nbuild:opt --copt=-pipe\r\nbuild:opt --copt=-march=native\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\nbuild --copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK\r\nbuild --host_copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK\r\n```\r\n\r\n", "comments": ["Nagging Assignee @tatatodd: It has been 199 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "It would be great if you could help us by working on this and submitting a PR. Let us know if you need further clarification. Feel free to open this issue and link the PR when you have it.\r\nThanks!", "Sorry, this goes deep into the building procedure, way above my head. Still, I think it is wrong to just close it because the underlying bug is still there."]}]