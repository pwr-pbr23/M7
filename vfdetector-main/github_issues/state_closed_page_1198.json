[{"number": 17241, "title": "TFlite: tensorflow/contrib/lite/java/demo/app/src/main/TfLiteCameraDemo does not work", "body": "The TfLiteCameraDemo shows \"Uninitialized Classifier or invalid context\" after launching.\r\n\r\nMy environment\r\n1. android 8.1 on pixel 2\r\n2. bazel 0.11.0\r\n3. tensorflow commit c7e966b5e35ee7fc511c1efe84dba8d3558f2b1c\r\n4. stand-alone ndk-r14b, sdk within android-studio 3.0.1\r\n5. ubuntu 16.04\r\n\r\nMy step\r\n1. bazel build --cxxopt=--std=c++11 //tensorflow/contrib/lite/java/demo/app/src/main:TfLiteCameraDemo --config=android_arm64 --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a\r\n2. adb install -r bazel-bin/tensorflow/contrib/lite/java/demo/app/src/main/TfLiteCameraDemo.apk", "comments": ["The demo is ok if built from android studio, only failed when built by bazel command line.", "I have a patch for this issue, see https://github.com/tensorflow/tensorflow/pull/17094", "Nagging Assignee @jart: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Should be fixed now."]}, {"number": 17240, "title": "Tensorflow", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 17239, "title": "Support GCS URL by tf.estimator.LatestExporter", "body": "I have found that the parameter `exports_to_keep` of `tf.estimator.LatestExporter()` is ignored if export_path is GCS URL (like `gs://bucket/object/`). All exported SavedModel directories are never removed.\r\n\r\nThe root cause is that the `tf.gfile.ListDirectory` returns basenames with file separator '/' at the end if the path was GCS URL.  The `_export_version_parser` function rejects a path with trailing '/'.\r\n\r\nI don't think the behavior of `tf.gfile.ListDirectory` is wrong because the semantics of \"path\" in GCS is different from usual filesystems. For example, you can place two different objects in the GCS bucket like `gs://bucket/obj` and `gs://bucket/obj/`. So I think `tf.gfile.ListDirectory` should return basename with '/'.\r\n\r\nI try to fix the issue by eliminating trailing separator at `tf.estimator.gc._getpaths()`.", "comments": ["Nagging Assignee @protoget: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @isaprykin: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Reviewer @isaprykin: It has been 14 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @isaprykin: It has been 14 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @isaprykin: It has been 59 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @isaprykin: It has been 74 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Hi @isaprykin, \r\nIs there any chance to merge this PR before branching for 1.10?\r\n\r\nThanks.", "Hi nagachika, is it possible to create a test which shows all flow for this case. \r\nI mean, have paths, deleted right ones at the end.\r\nyou can mock listdirectories", "Hi @ispirmustafa , thank you for your advice.\r\nI have added a test in case `gfile.ListDirectory` returns subdirectory with delimiter `/` at https://github.com/tensorflow/tensorflow/pull/17239/commits/3c44a0c54ad09eed8855d017b18ffbb51b4e68e3.\r\nIs that OK? Using GCS for temporary export directory is ideal, but I don't know how to handle credentials for that.", "Nagging Reviewer @isaprykin: It has been 16 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @isaprykin: It has been 31 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Hi\r\nIs there any chance to merge this pull req before creating branch `r1.10`?", "Nagging Assignee @ispirmustafa: It has been 20 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 17237, "title": "\"ld: unknown option: --icf=all\" Bazel build tensorflow-lite label_image Error", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: max os 10.12.6\r\n- **TensorFlow installed from (source or binary)**: pip\r\n- **TensorFlow version (use command below)**: 1.5\r\n- **Python version**:   2.7 anaconda\r\n- **Bazel version (if compiling from source)**: none\r\n- **GCC/Compiler version (if compiling from source)**: none\r\n\r\n\r\n### Describe the problem\r\n```\r\nbazel build --config opt --cxxopt=-std=c++11 //tensorflow/contrib/lite/examples/label_image:label_image\r\n```\r\n\r\n### Source code / logs\r\n```\r\nWARNING: Config values are not defined in any .rc file: opt.\r\nWARNING: /private/var/tmp/_bazel_ericyue/c53d920e143a3bbcf51dc97baaf3590c/external/protobuf_archive/WORKSPACE:1: Workspace name in /private/var/tmp/_bazel_ericyue/c53d920e143a3bbcf51dc97baaf3590c/external/protobuf_archive/WORKSPACE (@com_google_protobuf) does not match the name given in the repository's definition (@protobuf_archive); this will cause a build error in future versions.\r\nINFO: Found 1 target...\r\nERROR: /Users/ericyue/workspace/tensorflow/tensorflow/contrib/lite/examples/label_image/BUILD:15:1: Linking of rule '//tensorflow/contrib/lite/examples/label_image:label_image' failed (Exit 1).\r\nld: unknown option: --icf=all\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow/contrib/lite/examples/label_image:label_image failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 1.229s, Critical Path: 0.10s\r\n```", "comments": ["I have a patch for this, see https://github.com/tensorflow/tensorflow/pull/15127", "it works. thanks @freedomtan", "@freedomtan I get the same error on Ubuntu 16.04, but unfortunately your patch doesn't solve the issue yet. Any further ideas though?", "@Johnson145 interesting. Do you really mean that you literally see \"ld: unknown option: --icf=all\" on Ubuntu 16.04? That's weird. We got the message on OS X because Apple has its own [ld](https://opensource.apple.com/source/ld64/) (neither GNU ld  nor gold) and the Apple ld doesn't support `--icf`. On Ubuntu 16.04,\r\n\r\n```\r\nbazel build --config opt --cxxopt=-std=c++11 //tensorflow/contrib/lite/examples/label_image:label_image\r\n```\r\nshould just work.", "Well, almost literally. Actually, it's an `unrecognized` option instead of an `unknown` one, but the actual content is just the same. Here is the full error message having `--verbose_failures` enabled:\r\n```\r\nERROR: tensorflow/tensorflow/contrib/lite/examples/label_image/BUILD:15:1: Linking of rule '//tensorflow/contrib/lite/examples/label_image:label_image' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd /home/user/.cache/bazel/_bazel_user/9227ee1bdf044d663b48d04003e34de8/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    CUDA_TOOLKIT_PATH=/usr/local/cuda \\\r\n    CUDNN_INSTALL_PATH=/usr/local/cuda-9.0 \\\r\n    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \\\r\n    LD_LIBRARY_PATH=/opt/intel/compilers_and_libraries_2017.0.098/linux/tbb/lib/intel64/gcc4.4:/opt/intel/compilers_and_libraries_2017.0.098/linux/compiler/lib/intel64:/opt/intel/compilers_and_libraries_2017.0.098/linux/mkl/lib/intel64:/opt/intel/compilers_and_libraries_2017.0.098/linux/compiler/lib/intel64_lin:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64 \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python3 \\\r\n    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \\\r\n    TF_CUDA_CLANG=0 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \\\r\n    TF_CUDA_VERSION=9.0 \\\r\n    TF_CUDNN_VERSION=7 \\\r\n    TF_NEED_CUDA=1 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -o bazel-out/k8-py3-opt/bin/tensorflow/contrib/lite/examples/label_image/label_image '-Wl,-rpath,$ORIGIN/../../../../../_solib_local/_U_S_Stensorflow_Scontrib_Slite_Sexamples_Slabel_Uimage_Clabel_Uimage___Utensorflow' -Lbazel-out/k8-py3-opt/bin/_solib_local/_U_S_Stensorflow_Scontrib_Slite_Sexamples_Slabel_Uimage_Clabel_Uimage___Utensorflow '-Wl,--icf=all' '-Wl,-rpath,$ORIGIN/,-rpath,$ORIGIN/..,-rpath,$ORIGIN/../..,-rpath,$ORIGIN/../../..,-rpath,$ORIGIN/../../../..' -Wl,-no-as-needed -B/usr/bin/ -pie -Wl,-z,relro,-z,now -no-canonical-prefixes -pass-exit-codes '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -Wl,--gc-sections -Wl,@bazel-out/k8-py3-opt/bin/tensorflow/contrib/lite/examples/label_image/label_image-2.params)\r\n/usr/bin/ld: unrecognized option '--icf=all'\r\n/usr/bin/ld: use the --help option for usage information\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow/contrib/lite/examples/label_image:label_image failed to build\r\n```\r\n\r\nThe only related information I could find using Google is [this GitHub issue from a different repo](https://github.com/turol/smaaDemo/issues/1). It suggests that the default Linux ld linker doesn't support this option either, doesn't it?", "I think GNU ld also supports --icf, but it seems I am wrong. I guess my Ubutun machine  uses gold. Anyway, either change your /usr/bin/ld, which should be a symbolic link, to gold or modify the default link flag should work. ", "Just changing the ld symlink from `x86_64-linux-gnu-ld` to `x86_64-linux-gnu-ld.gold` crashes with\r\n```\r\nerror: undefined reference to 'tanhf'\r\ncollect2: error: ld returned 1 exit status\r\n```\r\nBesides that, just changing the symlink was a good idea for testing purposes, but I don't really feel good about doing that permanently.\r\n\r\nAny idea how to modify the flag in the `build_def.bzl` file? If I just remove it, I get another error.", "Also experiencing the same problem, I'm using Ubuntu 16.04 and my `ld --version` is:\r\n`GNU ld (GNU Binutils for Ubuntu) 2.26.1`\r\n\r\nCommand ran: `bazel build --config opt --cxxopt=-std=c++11 //tensorflow/contrib/lite/examples/label_image:label_image` on TF's v1.6.0 source tree.\r\n\r\nError log with `--verbose_failures`:\r\n\r\n```\r\nERROR: /home/slr/tensorflow/tensorflow/contrib/lite/examples/label_image/BUILD:15:1: Linking of rule '//tensorflow/contrib/lite/examples/label_image:label_image' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd /home/slr/.cache/bazel/_bazel_slr/d2e92630c77c8e4781ef11da6a1d7c29/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    CUDA_TOOLKIT_PATH=/usr/local/cuda \\\r\n    CUDNN_INSTALL_PATH=/usr/local/cuda-9.1 \\\r\n    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/home/slr/anaconda3/bin/python \\\r\n    PYTHON_LIB_PATH=/home/slr/anaconda3/lib/python3.6/site-packages \\\r\n    TF_CUDA_CLANG=0 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=5.2 \\\r\n    TF_CUDA_VERSION=9.1 \\\r\n    TF_CUDNN_VERSION=7.1.1 \\\r\n    TF_NEED_CUDA=1 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -o bazel-out/k8-py3-opt/bin/tensorflow/contrib/lite/examples/label_image/label_image '-Wl,-rpath,$ORIGIN/../../../../../_solib_local/_U_S_Stensorflow_Scontrib_Slite_Sexamples_Slabel_Uimage_Clabel_Uimage___Utensorflow' -Lbazel-out/k8-py3-opt/bin/_solib_local/_U_S_Stensorflow_Scontrib_Slite_Sexamples_Slabel_Uimage_Clabel_Uimage___Utensorflow '-Wl,--icf=all' '-Wl,-rpath,$ORIGIN/,-rpath,$ORIGIN/..,-rpath,$ORIGIN/../..,-rpath,$ORIGIN/../../..,-rpath,$ORIGIN/../../../..' -Wl,-no-as-needed -B/usr/bin/ -pie -Wl,-z,relro,-z,now -no-canonical-prefixes -pass-exit-codes '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -Wl,--gc-sections -Wl,@bazel-out/k8-py3-opt/bin/tensorflow/contrib/lite/examples/label_image/label_image-2.params)\r\n/usr/bin/ld: unrecognized option '--icf=all'\r\n/usr/bin/ld: use the --help option for usage information\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow/contrib/lite/examples/label_image:label_image failed to build\r\n```\r\n", "@GPhilo Apparently, GNU ld doesn't support `--icf=all`, which is a [gold](https://en.wikipedia.org/wiki/Gold_(linker)) specific option. Quick solution is to remove it.\r\n```\r\ndiff --git a/tensorflow/contrib/lite/build_def.bzl b/tensorflow/contrib/lite/build_def.bzl\r\nindex 2813d1c347..2e3a9cfac5 100644\r\n--- a/tensorflow/contrib/lite/build_def.bzl\r\n+++ b/tensorflow/contrib/lite/build_def.bzl\r\n@@ -50,7 +50,6 @@ def tflite_linkopts_unstripped():\r\n       \"//tensorflow/contrib/lite:mips\": [],\r\n       \"//tensorflow/contrib/lite:mips64\": [],\r\n       \"//conditions:default\": [\r\n-          \"-Wl,--icf=all\",  # Identical code folding.\r\n       ],\r\n   })\r\n```", "it seems something is wrong in `external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc` or other CUDA environment settings, I usually build label_image on linux **without** enabling CUDA. With that, I didn't run into `--icf=all` problem, with either `gnu ld` or `gold`. And when I tried to build label_image with CUDA enabled, I got the message."]}, {"number": 17236, "title": "Error in variable_scope initialization, via estimator api, due to a panda.DataFrame without column headers", "body": "The variable_scope initialization generates an error when a panda.DataFrame object without any column headers is passed via the estimator api.\r\n\r\nTo replicate this issue, please execute the below-specified code - \r\n\r\n```python\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport tensorflow as tf\r\nimport pandas as pd\r\n\r\n\r\ndef main(argv):\r\n    # Fetch the data\r\n    (train_x, train_y), (test_x, test_y) = load_data()\r\n\r\n    # Feature columns describe how to use the input.\r\n    my_feature_columns = []\r\n    for key in train_x.keys():\r\n        my_feature_columns.append(tf.feature_column.numeric_column(key=key))\r\n\r\n    # Build 2 hidden layer DNN with 10, 10 units respectively.\r\n    classifier = tf.estimator.DNNClassifier(\r\n        feature_columns=my_feature_columns,\r\n        # Two hidden layers of 10 nodes each.\r\n        hidden_units=[10, 10],\r\n        # The model must choose between 3 classes.\r\n        n_classes=3)\r\n\r\n    # Train the Model.\r\n    classifier.train(\r\n        input_fn=lambda: train_input_fn(train_x, train_y,\r\n                                        100),\r\n        steps=1000)\r\n\r\n    # Evaluate the model.\r\n    eval_result = classifier.evaluate(\r\n        input_fn=lambda: eval_input_fn(test_x, test_y,\r\n                                       10))\r\n\r\n    print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))\r\n\r\n\r\ndef load_data():\r\n\r\n    # Note that there are no column names specified during the\r\n    # DataFrame initialization\r\n    train_x = pd.DataFrame([[6.4, 2.8, 5.6, 2.2],\r\n                            [5,\t2.3, 3.3, 1],\r\n                            [4.9, 2.5, 4.5, 1.7],\r\n                            [4.9, 3.1, 1.5, 0.1]], dtype=float)\r\n    train_y = pd.DataFrame([[2], [1], [2], [0]], dtype=float)\r\n\r\n    test_x = pd.DataFrame([[4.4, 3.2, 1.3, 0.2]], dtype=float)\r\n    test_y = pd.DataFrame([[2]], dtype=float)\r\n\r\n    print(train_x)\r\n\r\n    return (train_x, train_y), (test_x, test_y)\r\n\r\n\r\ndef train_input_fn(features, labels, batch_size):\r\n    \"\"\"An input function for training\"\"\"\r\n    # Convert the inputs to a Dataset.\r\n    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\r\n\r\n    # Shuffle, repeat, and batch the examples.\r\n    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\r\n\r\n    # Return the dataset.\r\n    return dataset\r\n\r\n\r\ndef eval_input_fn(features, labels, batch_size):\r\n    \"\"\"An input function for evaluation or prediction\"\"\"\r\n    features = dict(features)\r\n    if labels is None:\r\n        # No labels, use only features.\r\n        inputs = features\r\n    else:\r\n        inputs = (features, labels)\r\n\r\n    # Convert the inputs to a Dataset.\r\n    dataset = tf.data.Dataset.from_tensor_slices(inputs)\r\n\r\n    # Batch the examples\r\n    assert batch_size is not None, \"batch_size must not be None\"\r\n    dataset = dataset.batch(batch_size)\r\n\r\n    # Return the dataset.\r\n    return dataset\r\n\r\n\r\nif __name__ == '__main__':\r\n    tf.logging.set_verbosity(tf.logging.INFO)\r\n    tf.app.run(main)\r\n\r\n```\r\n\r\nOutput, printing the 'train_x' dataframe, before the error - \r\n\r\n```python\r\n     0    1    2    3\r\n0  6.4  2.8  5.6  2.2\r\n1  5.0  2.3  3.3  1.0\r\n2  4.9  2.5  4.5  1.7\r\n3  4.9  3.1  1.5  0.1\r\n```\r\nNote that there are no column headers provided, the respective index appears as the column header for the data frame which is of type integer.\r\n\r\nTraceback - \r\n```python\r\nTraceback (most recent call last):\r\n  File \"tf_bug.py\", line 90, in <module>\r\n    tf.app.run(main)\r\n  File \"C:\\Users\\Sagar_Jadhav03\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 124, in run\r\n    _sys.exit(main(argv))\r\n  File \"tf_bug.py\", line 30, in main\r\n    steps=1000)\r\n  File \"C:\\Users\\Sagar_Jadhav03\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 314, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"C:\\Users\\Sagar_Jadhav03\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 743, in _train_model\r\n    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\r\n  File \"C:\\Users\\Sagar_Jadhav03\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 725, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"C:\\Users\\Sagar_Jadhav03\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\estimator\\canned\\dnn.py\", line 324, in _model_fn\r\n    config=config)\r\n  File \"C:\\Users\\Sagar_Jadhav03\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\estimator\\canned\\dnn.py\", line 176, in _dnn_model_fn\r\n    logits = logit_fn(features=features, mode=mode)\r\n  File \"C:\\Users\\Sagar_Jadhav03\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\estimator\\canned\\dnn.py\", line 90, in dnn_logit_fn\r\n    features=features, feature_columns=feature_columns)\r\n  File \"C:\\Users\\Sagar_Jadhav03\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column.py\", line 280, in input_layer\r\n    trainable, cols_to_vars)\r\n  File \"C:\\Users\\Sagar_Jadhav03\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column.py\", line 193, in _internal_input_layer\r\n    None, default_name=column._var_scope_name):  # pylint: disable=protected-access\r\n  File \"C:\\Users\\Sagar_Jadhav03\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1929, in __enter__\r\n    unique_default_name = _get_unique_variable_scope(self._default_name)\r\n  File \"C:\\Users\\Sagar_Jadhav03\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1656, in _get_unique_variable_scope\r\n    name = current_scope.name + \"/\" + prefix if current_scope.name else prefix\r\nTypeError: must be str, not int\r\n```\r\n", "comments": ["Thanks for reporting and debugging this! \r\n\r\nIf I understand correctly, the issue is that the default pandas DataFrame has int as type for column headers.\r\n\r\nGiven the Estimator requires the input_fn returning a dict with string name as key (i.e., str type) [1], I think a better way to fix this is the pandas_input_fn, i.e., Line 118 of [2]. Most of the tooling around Estimator is written based on the fact that the dict has string key name. \r\n\r\nwdyt?\r\n\r\n[1] https://github.com/tensorflow/tensorflow/blob/7c324811f10eaf049735ace7b6a228ccc931d8ff/tensorflow/python/estimator/estimator.py\r\n[2] https://github.com/tensorflow/tensorflow/blob/bc69c4ceed6544c109be5693eb40ddcf3a4eb95d/tensorflow/python/estimator/inputs/pandas_io.py\r\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "Sorry for being late on responding to this and thanks for your inputs.\r\n\r\nI did try and check if we can insert the fix at the [place](https://github.com/tensorflow/tensorflow/blob/bc69c4ceed6544c109be5693eb40ddcf3a4eb95d/tensorflow/python/estimator/inputs/pandas_io.py#L118) suggested by you, but it appears that the said line is not invoked for the example given by me.\r\n\r\nAlso, I'm not completely aware of the way pandas input_fn is used or the way it integrates with the estimator api. If you can point to any relevant documentation explaining the usage, that would be helpful.\r\n\r\nReturning back to the main discussion, the other alternatives that I can think of for this problem are - \r\n1. Keep the fix at the same place which I had proposed earlier, which would localize its effect and avoid the error.\r\n2. Given that the name of the feature column is used as the partial name for constructing the name of the variable scope, cast the return value of the method - ['_var_scope_name'](https://github.com/tensorflow/tensorflow/blob/7c324811f10eaf049735ace7b6a228ccc931d8ff/tensorflow/python/feature_column/feature_column.py#L1595) to 'str'. This would ensure that a string value is returned always for other types of feature columns.\r\n3. Or else override the method specified in [2] in the class - ['_NumericColumn'](https://github.com/tensorflow/tensorflow/blob/7c324811f10eaf049735ace7b6a228ccc931d8ff/tensorflow/python/feature_column/feature_column.py#L2030) to return an 'str' value. This would localize its effect to numeric feature columns only.\r\n\r\nLet me know your thoughts on this.", "Could you please check and let me know if you agree to the proposed solution.", "Sorry the confusion. When I see the \"panda\", I am thinking pandas_input_fn [1].\r\n\r\nI re-read your code and figured out you were creating the input_fn by yourself. I have tested your scripted locally and found the following issues:\r\n1) scope name should not be int. It should be str. Of course, this is not user facing part as your repro example is using the number_column.\r\n2) tf.feature_column.numeric_column takes a string as key name [2]. Your repro example is passing int, which is not allowed. \r\n\r\nI do not think it is reasonable to hack all the framework code in order to support the unsupported \"int\" type here as most of APIs explicitly documented the argument should be string. Added @ispirmustafa \r\nfor his insights.\r\n\r\nI think there are two ways to fix this\r\n1) In your code, cast all the keys to string types. It should work. I have not tested by myself. You can either give explicit string column name to DataFrame or cast in the input_fn.\r\n\r\n2) You can also use the tf.estimator.inputs.pandas_input_fn. It needed a fix I mentioned in previous comment (converting the column name from int to str) -- which I can fix for you. See [3] for a working example. \r\n\r\nI think item 1) by adding string column name is easiest for you. See [4]\r\n\r\n\r\n[1]  https://www.tensorflow.org/api_docs/python/tf/estimator/inputs/pandas_input_fn\r\n\r\n[2] https://www.tensorflow.org/api_docs/python/tf/feature_column/numeric_column\r\n\r\n[3] https://gist.github.com/xiejw/7b4c351beec41817c62f587ad90970ad\r\n\r\n[4] https://gist.github.com/xiejw/329502f3726f139e6af85829bdad2720", "Nagging Reviewer @xiejw: It has been 23 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Close due to inactivity. "]}, {"number": 17235, "title": "Clean up output formatting of saved_model_cli.py", "body": "The output of `saved_model_cli.py` was a bit hard to read, so I added some indentation to make the variable/graph information easier to take in at a glance. \r\n\r\n**Before:**\r\n```\r\nMetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\r\n\r\nsignature_def['classify_x2_to_y3']:\r\nThe given SavedModel SignatureDef contains the following input(s):\r\ninputs['inputs'] tensor_info:\r\n    dtype: DT_FLOAT\r\n    shape: (-1, 1)\r\n    name: x2:0\r\nThe given SavedModel SignatureDef contains the following output(s):\r\noutputs['scores'] tensor_info:\r\n    dtype: DT_FLOAT\r\n    shape: (-1, 1)\r\n    name: y3:0\r\nMethod name is: tensorflow/serving/classify\r\n```\r\n\r\n**After:**\r\n```\r\nMetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\r\n\r\nsignature_def['classify_x2_to_y3']:\r\n  The given SavedModel SignatureDef contains the following input(s):\r\n    inputs['inputs'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (-1, 1)\r\n        name: x2:0\r\n  The given SavedModel SignatureDef contains the following output(s):\r\n    outputs['scores'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (-1, 1)\r\n        name: y3:0\r\n  Method name is: tensorflow/serving/classify\r\n```\r\n\r\nI updated the documentation and unit tests accordingly.\r\n\r\nI also fixed the formatting of some of the \"usage\" messages in `saved_model_cli.py`.", "comments": []}, {"number": 17234, "title": "Add nasm mirror", "body": "Fixes #6956.\r\n\r\nnasm.us serves a revoked cert for https. Doesn't matter to us, since we're checking the sha anyway, but it is a little sketchy.", "comments": ["The line was added in a wrong section. Unless Bazel does some filename-based guessing, this won't work. Even worse, users will see a difficult to interpret error message if the first two libjpeg-turbo sources fail, the nasm file is downloaded and the sha256 does not match.", "Yeah, sorry, I messed that up. Sent #17277 to fix.", "Thanks, and also thanks for the first attempt.\r\n\r\n"]}, {"number": 17233, "title": "4x slowdown in feed_dict in tf-nightly-gpu", "body": "Upgrading to `pip install tf-nightly-gpu` from `pip install tensorflow` (1.5) slows down feeding about 4x\r\n\r\nFeeding 100MB array used to take 15ms, and it takes 60ms after the change. I think this is due to change in alignment requirements for AVX-compiled binary. I want to start the thread to figure out how to regain the performance before these changes make it into official release\r\n\r\nbenchmark: [align_feed_bug.py](https://github.com/diux-dev/cluster/blob/0edf19d919ee02273c7beb3af2ea378496a95610/yuxin_numpy/align_feed_bug.py)\r\n\r\n```\r\n# version: 1.5.0\r\npython align_feed_bug.py\r\nfeed-cpu-variable   : min: 17.17, median: 19.95, mean: 19.82\r\n\r\n# After upgrading to tf nightly\r\n# version: 1.7.0-dev20180221\r\npython align_feed_bug.py\r\nfeed-cpu-variable   : min: 53.97, median: 57.15, mean: 66.60\r\n```\r\n\r\nI've tried using @eamartin 's recipe (https://github.com/numpy/numpy/issues/5312#issuecomment-299533915) to make sure numpy array are 128-byte aligned, but that didn't make any difference in speed\r\n```\r\npython align_feed_bug.py --align=1\r\nfeed-cpu-variable   : min: 49.54, median: 50.50, mean: 60.06\r\n```\r\n\r\ncc @martinwicke ", "comments": ["OK, here's a hacky way of aligning an existing array. This removes the speed penalty of speed dict, and you can copy new data into existing aligned memory using `np.copyto`\r\n\r\n```\r\ndef align_numpy(unaligned):\r\n  sess = tf.get_default_session()\r\n  aligned = sess.run(tf.ones(unaligned.shape, dtype=unaligned.dtype))\r\n  np.copyto(aligned, unaligned)\r\n  return aligned\r\n\r\n```", "@alextp what is the logic to determine if existing numpy memory is aligned?", "Just to check. Does the regression also affect 1.6 RC1?", "Also, could this be related to the AVX build?\n", "I suspect an extra memcpy is the cause. The slowdown is 45ms, and 100MB/45ms = 2.2GB/s. This is about the throughput of a memcpy that causes a lot of pagefaults.\r\n\r\n```python\r\nIn [5]: x = np.ones(100 * 1 << 20, dtype=np.uint8)\r\n\r\nIn [6]: %time y = np.copy(x)\r\nCPU times: user 32 ms, sys: 8 ms, total: 40 ms\r\nWall time: 36.9 ms\r\n```\r\n\r\nGIven this, I suspect the TF changes caused an additional memcpy on feed. The memcpy theory can be further tested by running @yaroslavvb 's test at 200MB instead of 100MB. If slowdown roughly doubles to 90ms, strong indicator that memcpy (or at least something that touches every fed byte) is the source of slowdown.\r\n\r\nGood catch @yaroslavvb !", "That would be consistent with an alignment mismatch forcing a copy.\n", "@gunan it would affect it if RC1 is built with avx\r\n\r\nI regain the speed when using TensorFlow to allocate memory and writing my own data into that memory using `np.copyto`.\r\n\r\nbenchmark: [tf_numpy_benchmark.py](https://github.com/diux-dev/cluster/blob/41b318a5e5248b8e236c31c323be6d7777fa6133/yuxin_numpy/tf_numpy_benchmark.py)\r\n\r\nUsing 100 MB of data, times are in ms\r\n\r\n```\r\npython tf_numpy_benchmark.py --allocator=numpy --benchmark=feed_cpu_tensor\r\nfeed_cpu_tensor               :   2.0 GB/sec, min: 50.10, median: 53.48, mean: 54.61\r\n\r\npython tf_numpy_benchmark.py --allocator=tf --benchmark=feed_cpu_tensor\r\nfeed_cpu_tensor               :  12.9 GB/sec, min:  7.77, median:  8.53, mean:  8.88\r\n```", "Thank you very much for the analysis.\r\nSince this will require some investigation, I will release 1.6 final without a fix for this, however I want to follow up on this and once we have a fix prepare patch releases for 1.5 and 1.6.\r\n@tfboyd @tatianashp FYI", "@yaroslavvb this is the test we apply https://github.com/tensorflow/tensorflow/blob/26ae3287a12c71fccaec9ea74f55b6a51a3d33c6/tensorflow/c/c_api.cc#L197 and it hasn't changed recently so I don't know what's going on. The call site to this is in https://github.com/tensorflow/tensorflow/blob/26ae3287a12c71fccaec9ea74f55b6a51a3d33c6/tensorflow/c/c_api.cc#L197 and that hasn't changed recently either.", "@alextp I think what's going on is that `EIGEN_MAX_ALIGN_BYTES` has changed recently in `tf-nightly` due to switching to AVX build", "@alextp is there any other place that copy can happen? I'm finding that there's significant penalty in feeding read-only numpy arrays, and I can't see anything in that c_api.cc logic that checks for read-only attrs . This comes up when trying to feed data from Ray which uses read-only memory buffers\r\n\r\nIE,\r\n```\r\nsess.run(some_op, feed_dict={a:arr})  # 12.6 GB/sec\r\narr.flags['WRITEABLE']=False\r\nsess.run(some_op, feed_dict={a:arr})  # 1.2 GB/sec\r\n```\r\n\r\n[tf_numpy_benchmark.py](https://github.com/diux-dev/cluster/blob/ee5c07056a9d1dadb118aaa93e721bc81a962428/yuxin_numpy/tf_numpy_benchmark.py)\r\n\r\n```\r\npython tf_numpy_benchmark.py --benchmark=feed_cpu_tensor --allocator=tf --num-iters=51\r\nfeed_cpu_tensor               :  12.9 GB/sec, min:  7.75, median:  8.98, mean:  9.01\r\n\r\npython tf_numpy_benchmark.py --benchmark=feed_cpu_tensor --allocator=tf_readonly --num-iters=51\r\nfeed_cpu_tensor               :   1.1 GB/sec, min: 88.48, median: 91.55, mean: 93.30\r\n\r\n\r\n```", "Moving \"read-only\" slowness to separate issue https://github.com/tensorflow/tensorflow/issues/17315 because it's unrelated to alignment", "I [have to](https://stackoverflow.com/q/46712934) use feed_dict for a large (> 2 GB) embedding matrix and I'm seeing a 60 fold slowdown in my Siamese CNN between Tensorflow 1.5 and 1.6 (via nvidia-docker and the official images). While a batch takes about 1 second in 1.5 it takes > 1 minute in 1.6. I strongly suspect this could also be caused by this issue. Also it seems this gets much worse with CPU load where it slows down to 3 minutes on 1.6. I can provide code and embeddings if needed.", "CPU slowdown has also been reported in #17383, Could you please provide data about your environment and build? Does the workaround suggested by @yaroslavvb help you?", "@tatianashp the problem occurs on nvidia-docker with `gcr.io/tensorflow/tensorflow:1.6.0-gpu-py3` on an NVIDIA TITAN X (Maxwell and Pascal). Both inside Kubernetes as well as on the Ubuntu 16.04 LTS host + Docker CE.  Driver version is 390.25 but the same happened with an older version already (tried updating today during our investigation). As for the docker image there are almost no other dependencies (gensim + joblib) and those remain the same when changing from 1.5 to 1.6.\r\n\r\nI have spent some time trying to get @yaroslavvb 's align_numpy_tf() hack to work but it's a bit hairy because the embeddings' size is taken into account when building the graph and at that point there is no session yet. I got it working with a hack but couldn't really see a performance difference. Still, I think that the very large NumPy array feed is a likely culprit as other models by colleagues don't show a similar degradation but also don't feed large NumPy arrays.\r\n\r\nNote that this is with the embeddings created under a the 'cpu:0' device. When creating on the GPU it takes 1 minute per batch even on 1.5, I'm not sure about 1.6. The loss seems to develop almost identically. Interestingly with 1.5 I see 60-70% utilization in `nvidia-smi` but with 1.6 it drops to 0-2%. CPU load is 100% with both versions, though on 1.5 when using no GPU it doesn't go beyond 100% while with 1.6 it uses just one core.\r\n\r\nI can try to create a reproducer tomorrow though it will likely need a large download for the embeddings..", "@niklas88 GPU at low utilization while TF only uses 1 core is consistent with copy problem. memcpy of 1GB array would take 1 second and will use a single core. You could check if `align_numpy_tf` is working for you by feeding 1GB aligned array and checking if takes 1 sec vs 1ms", "The issue in #17383 is unrelated, as it is specific to MKL.\r\n@niklas88 Let us know if  @yaroslavvb suggestion helps you.\r\n", "@tatianashp ok I've tried integrating @yaroslavvb 's `align_numpy_tf()` again. First I can confirm that with his `tf_numpy_benchmark.py` I definitely see the slowdown on my setup. However integrating it for my embedding doesn't significantly speed things up. I also tried running with `align_numpy_tf()` on 1.5 and it's as fast as always (1 second per epoch) but on 1.6 it remains at 1 minute per epoch. So there is still something very weird going on. I still think it's got to be related. So now I'll put together a reproducer, the code isn't too pretty but also not too large.", "I've put together a reproducer [here](https://github.com/niklas88/tf_perf_regression) and also host the required data (see the README for instructions). This is the code with @yaroslavvb 's workaround just to make sure I've applied it correctly. If you need me to further simplify the code I can of course look into that still. *Note: I may remove the repository when this is fixed*", "@niklas88 have you isolated this problem to `feed_dict`? If `align_numpy_tf` doesn't help, it suggests that extra memcpy on feed-dict is not the problem. \r\n\r\nAlso, just in case, you could try running this beforehand. That's an alternative way of making your numpy arrays aligned.\r\n```\r\nsudo apt-get install -y google-perftools\r\nexport LD_PRELOAD=\"/usr/lib/libtcmalloc.so.4\"\r\n```", "@yaroslavvb so with `align_numpy_tf()` as used in my posted code I didn't see an improvement. However using  `LD_PRELOAD` does the trick and the performance returns to 1.5 levels. When doing this tcmalloc warns of `large alloc`s though.\r\n\r\nMaybe `align_numpy_tf()` doesn't work since the embedding uses [vstack](https://github.com/niklas88/tf_perf_regression/blob/master/deep_relscorer.py#L156) or because the placeholder has an [explicit shape](https://github.com/niklas88/tf_perf_regression/blob/e7bdac70ceba120ad20820cc3f93f8e4493b1a8b/deep_relscorer.py#L702)?", "OK, it looks like your issue is also due to change in alignment requirements in TF 1.6\r\n\r\nThe reason `align_numpy_tf` didn't help is possibly because you were calling it too many times. Each time you call it, it involves a memory copy into aligned memory. The way I use it is to call it once, and then do in-place operations on the resulting array.\r\n \r\nPS, you can check alignment of various allocators using this utility\r\n\r\n```\r\n# install ray, pytorch, then\r\nwget -N https://raw.githubusercontent.com/diux-dev/cluster/fa98c59d07447e2b7923fde8ee294f9a054f9fbb/yuxin_numpy/tf_numpy_benchmark.py\r\npython tf_numpy_benchmark.py --benchmark=allocator_alignment\r\n```\r\nit prints how many bytes you are off 64-byte alignment, ie, I see something like this with tcmalloc\r\n```\r\n     numpy: 0\r\n       ray: 48\r\n   pytorch: 0\r\n        tf: 0\r\n```", "@yaroslavvb that can't be it. I only call it when I build the graph which happens exactly once in the reproducer. But it was a quick addition so I might simply have screwed up.\r\n\r\nIm getting the following alignments (uncommented ray and pytorch as I didn't want to install them in nvidia-docker):\r\n\r\n     numpy: 16\r\n     tf: 0", "@tatianashp, has this issue been fixed? The last comment of #17383 states that this is fixed in 1.7.", "Nagging Assignee @tatianashp: It has been 211 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing as this is resolved, feel free to reopen if problem persists."]}, {"number": 17232, "title": "Cmake build doesn't work on ubuntu 16.04", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: Latest from master branch\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: 5.4\r\n- **CUDA/cuDNN version**: Cuda 9.1, cuDNN 7.0.5\r\n- **GPU model and memory**: GeForce GTX TITAN X, 12288 MB\r\n- **Exact command to reproduce**:\r\ngit clone https://github.com/tensorflow/tensorflow\r\ncd tensorflow/tensorflow/contrib/cmake/\r\nmkdir build && cd build \r\nccmake .. [correct cuda version and add nccl library, idk why that was mandatory though.]\r\nmake -j\r\n\r\n\r\n### Describe the problem\r\nI am trying to compile tensorflow with cmake on ubuntu 16.04. Some issues I have faced till now:\r\n\r\n**1. cuda/extras/CUPTI/include/cupti.h no found (In cupti_wrapper.h)**\r\nTemp fix: I saw \"cuda/extras/CUPTI/include\" is already included in the cmake. So, changed \"cuda/extras/CUPTI/include/cupti.h\" => \"cupti.h\" and it works.\r\n\r\n**2. src/nccl.h not found (In nccl_manager.h and nccl_ops.cc)**\r\nTemp fix: nccl v2 doesnt have src folder but nccl.h can be found in /usr/include/. So, changed\r\n\"src/nccl.h\" => nccl.h and it works\r\n\r\n**3. GRPC is not linked to libpywrap_tensorflow_internal.so (Not fixed yet)**\r\n`[100%] Running SWIG to generate Python wrappers\r\n[100%] Building CXX object CMakeFiles/pywrap_tensorflow_internal.dir/pywrap_tensorflow_internal.cc.o\r\n[100%] Linking CXX shared library libpywrap_tensorflow_internal.so\r\ngrpc/src/grpc/libgrpc_unsecure.a(grpc_ares_wrapper.cc.o): In function `on_txt_done_cb(void*, int, int, unsigned char*, int)':\r\ngrpc_ares_wrapper.cc:(.text+0x256): undefined reference to `ares_parse_txt_reply_ext'\r\ngrpc_ares_wrapper.cc:(.text+0x267): undefined reference to `ares_strerror'\r\ngrpc_ares_wrapper.cc:(.text+0x363): undefined reference to `ares_free_data'\r\ngrpc/src/grpc/libgrpc_unsecure.a(grpc_ares_wrapper.cc.o): In function `on_srv_query_done_cb(void*, int, int, unsigned char*, int)':\r\ngrpc_ares_wrapper.cc:(.text+0x65a): undefined reference to `ares_parse_srv_reply'\r\ngrpc_ares_wrapper.cc:(.text+0x677): undefined reference to `ares_free_data'\r\ngrpc_ares_wrapper.cc:(.text+0x683): undefined reference to `ares_strerror'\r\ngrpc_ares_wrapper.cc:(.text+0x7cf): undefined reference to `ares_gethostbyname'\r\ngrpc_ares_wrapper.cc:(.text+0x830): undefined reference to `ares_gethostbyname'\r\ngrpc/src/grpc/libgrpc_unsecure.a(grpc_ares_wrapper.cc.o): In function `on_hostbyname_done_cb(void*, int, int, hostent*)':\r\ngrpc_ares_wrapper.cc:(.text+0x964): undefined reference to `ares_strerror'\r\ngrpc_ares_wrapper.cc:(.text+0xb7d): undefined reference to `ares_inet_ntop'\r\ngrpc_ares_wrapper.cc:(.text+0xc62): undefined reference to `ares_inet_ntop'\r\ngrpc/src/grpc/libgrpc_unsecure.a(grpc_ares_wrapper.cc.o): In function `grpc_dns_lookup_ares_impl(char const*, char const*, char const*, grpc_pollset_set*, grpc_closure*, grpc_lb_addresses**, bool, char**)':\r\n`\r\n**4. Can't fix with GRPC OFF in ccmake (Not fixed yet)**\r\n\r\nThere are so many issues in CMake and I am not able to build directly on ubuntu 16.04. Can someone look into this? Thanks!", "comments": ["Currently, cmake build is only recommended for windows.\r\nBazel is the currently tested and officially supported build system on ubuntu.", "I can surely take a look to make it work in linux environment. In so many of my projects I have to use cmake so I will fix and send PR. Other contributors are also welcome for speedup.", "@achalshah20 I saw similar grpc error on my Mac before so this PR #17005 may fix your grpc issue.", "What do you think about using SCons as building script?", "@mskwarek It may be good but most of the c++ projects like opencv, caffe,dlib are already using CMake. So, it's so convenient if tensorflow also officially support CMake, that way anyone can easily integrate tensorflow code to their product without using bazel build to create shared library and copy pasting some includes manually and etc. etc. . ", "@achalshah20 Did you eventually manage to get the CMake build to work on Ubuntu ?", "I created a PR #18775 which should address the issue to build with CMAKE+GPU on Ubuntu. Please take a look if interested.", "Thanks. I will test and let you know if it works for me! ", "Hey @yongtang , It actually builds successfully. But currently does tensorflow cmake have install rule? ", "No. Cmake build can generate a pip package which you can install though.\r\nI dont think our cmake build has any other artifacts, other than the pip package.", "@gunan  So, If i want to use tensorflow c++ library, I don't need pip package but .so file and .h headers. So, I can't achieve that using current tensorflow cmake?"]}, {"number": 17231, "title": "Cherrypick: Don't assign device for the keras part of _saved_first_checkpoint. Fi\u2026", "body": "\u2026x #14504.\r\n\r\nPiperOrigin-RevId: 186526175", "comments": []}, {"number": 17230, "title": "Ship TF Eager header with libtensorflow tarballs.", "body": "The TF Eager headers are getting excluded from nightly tarballs because they both have the same filename and output destination. Looks like the core TF c header wins. This PR introduces shipping the eager header in the correct location.\r\n\r\nTake a look at one of the hosted tarballs here: http://ci.tensorflow.org/view/Nightly/job/nightly-libtensorflow/TYPE=cpu-slave/", "comments": []}, {"number": 17229, "title": "r1.5 cherry-pick request: Fixes issue when linking of rule '//tensorflow/contrib/lite/toco:toco\u2026", "body": "\u2026\u2026 (#16838)\r\n\r\n* Fixes issue when linking of rule '//tensorflow/contrib/lite/toco:toco' fails because LD_LIBRARY_PATH is not configured\r\n\r\n* Check if LD_LIBRARY_PATH is in environ_cp", "comments": ["@gunan Can you take a look please?", "Thank you very much for the help!"]}, {"number": 17228, "title": "r1.5 cherry-pick request: Fixes issue when linking of rule '//tensorflow/contrib/lite/toco:toco", "body": "Fixes issue when linking of rule '//tensorflow/contrib/lite/toco:toco\u2026 (#16838)", "comments": []}, {"number": 17227, "title": "Unable to load retrained MobileNet", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch Linux on x86_64\r\n- **TensorFlow installed from (source or binary)**: binary/pip\r\n- **TensorFlow version (use command below)**: 1.5.0\r\n- **Python version**: 3.6.4\r\n- **Exact command to reproduce**:\r\n`python3 label_image/label_image.py \\\r\n--graph=output_graph.pb --labels=output_labels.txt \\\r\n--input_layer=Mul \\\r\n--output_layer=final_result \\\r\n--input_mean=128 --input_std=128 \\\r\n--image=images.jpg`\r\n\r\n### Describe the problem\r\nAttempting to use a retrained MobileNet model (retrained with retrain.py) results in \r\n\r\n`Traceback (most recent call last):\r\n  File \"label_image/label_image.py\", line 144, in <module>\r\n    classify()\r\n  File \"label_image/label_image.py\", line 116, in classify\r\n    graph = load_graph(model_file)\r\n  File \"label_image/label_image.py\", line 34, in load_graph\r\n    graph_def.ParseFromString(f.read())\r\ngoogle.protobuf.message.DecodeError: Error parsing message`\r\n\r\nwhen trying to use the label_image example, and \r\n\r\n`Traceback (most recent call last):\r\n  File \"label_image/label_image.py\", line 144, in <module>\r\n    classify()\r\n  File \"label_image/label_image.py\", line 116, in classify\r\n    graph = load_graph(model_file)\r\n  File \"label_image/label_image.py\", line 34, in load_graph\r\n    graph_def.ParseFromString(f.read())\r\n  File \"/usr/lib/python3.6/site-packages/google/protobuf/message.py\", line 185, in ParseFromString\r\n    self.MergeFromString(serialized)\r\n  File \"/usr/lib/python3.6/site-packages/google/protobuf/internal/python_message.py\", line 1083, in MergeFromString\r\n    if self._InternalParse(serialized, 0, length) != length:\r\n  File \"/usr/lib/python3.6/site-packages/google/protobuf/internal/python_message.py\", line 1120, in InternalParse\r\n    pos = field_decoder(buffer, new_pos, end, self, field_dict)\r\n  File \"/usr/lib/python3.6/site-packages/google/protobuf/internal/decoder.py\", line 633, in DecodeField\r\n    if value._InternalParse(buffer, pos, new_pos) != new_pos:\r\n  File \"/usr/lib/python3.6/site-packages/google/protobuf/internal/python_message.py\", line 1120, in InternalParse\r\n    pos = field_decoder(buffer, new_pos, end, self, field_dict)\r\n  File \"/usr/lib/python3.6/site-packages/google/protobuf/internal/decoder.py\", line 612, in DecodeRepeatedField\r\n    if value.add()._InternalParse(buffer, pos, new_pos) != new_pos:\r\n  File \"/usr/lib/python3.6/site-packages/google/protobuf/internal/python_message.py\", line 1120, in InternalParse\r\n    pos = field_decoder(buffer, new_pos, end, self, field_dict)\r\n  File \"/usr/lib/python3.6/site-packages/google/protobuf/internal/decoder.py\", line 743, in DecodeMap\r\n    if submsg._InternalParse(buffer, pos, new_pos) != new_pos:\r\n  File \"/usr/lib/python3.6/site-packages/google/protobuf/internal/python_message.py\", line 1109, in InternalParse\r\n    new_pos = local_SkipField(buffer, new_pos, end, tag_bytes)\r\n  File \"/usr/lib/python3.6/site-packages/google/protobuf/internal/decoder.py\", line 850, in SkipField\r\n    return WIRETYPE_TO_SKIPPER[wire_type](buffer, pos, end)\r\n  File \"/usr/lib/python3.6/site-packages/google/protobuf/internal/decoder.py\", line 799, in _SkipGroup\r\n    new_pos = SkipField(buffer, pos, end, tag_bytes)\r\n  File \"/usr/lib/python3.6/site-packages/google/protobuf/internal/decoder.py\", line 850, in SkipField\r\n    return WIRETYPE_TO_SKIPPER[wire_type](buffer, pos, end)\r\n  File \"/usr/lib/python3.6/site-packages/google/protobuf/internal/decoder.py\", line 814, in _SkipFixed32\r\n    raise _DecodeError('Truncated message.')\r\ngoogle.protobuf.message.DecodeError: Truncated message.` \r\n\r\nwhen using `PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python` as mentioned in [https://github.com/tensorflow/tensorflow/issues/582](url)\r\n\r\nI also get the same issues when trying to quantize the model.\r\n", "comments": ["I had faced a similar issue a while ago, but I realized that my Linux shell variables, `IMAGE_SIZE` and `ARCHITECTURE` had been incorrectly populated. ", "Unfortunately, that doesn't seem to be my issue. Unsetting them before retraining, as well as setting them to the correct values, doesn't change the outcome.", "@tensorflowbutler ", "same here...:(\r\n", "I was able to fix it by using the v1.4.0 tag, something broke it since then.\n\nOn Tue, Mar 6, 2018 at 10:51 AM kaskagir <notifications@github.com> wrote:\n\n> same here...:(\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/17227#issuecomment-370825940>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AIZ-VLym-Ph6X4JdxFA5GOME7iPYqXbRks5tbrCbgaJpZM4SRXc->\n> .\n>\n", "@frap129 \r\n\r\nSame here.\r\n\r\nI'm trying to freeze graph from saved model and get the same error after setting PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION.\r\n\r\nWindows 10\r\nPython 3.5.4\r\nTensorFlow 1.5\r\n\r\nEDIT: \r\n\r\nProblem solved by using **input_saved_model_dir** instead of **input_graph** in freeze_graph.py.\r\n", "Same here, just loaded a model and got this error", "Nagging Assignee @petewarden: It has been 122 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@tensorflowbutler can you nag a little harder? Thanks", "Apologies for the very long delay on this one. I've not been able to reproduce this, so closing for now, but please reopen if you're still hitting it."]}, {"number": 17226, "title": "Branch 186777369", "body": "push after pull", "comments": ["Auto-merge issue?\r\n```\r\nFAIL: Found 1 non-whitelited pylint errors:\r\ntensorflow/python/ops/nn_ops.py:2242: [E0102(function-redefined), _get_noise_shape] function already defined line 2217\r\n```"]}, {"number": 17225, "title": "Change unicode() --> six.text_type() for Python 3", "body": "__unicode()__ was removed in Python 3 because all str are Unicode so this PR changes four calls to __unicode()__ into calls to [__six.text_type()__](http://six.readthedocs.io/#six.text_type).", "comments": ["@ekelsen Was there something more that I needed to do on this one?"]}, {"number": 17224, "title": "TF Lite: from six.moves import xrange for Python 3", "body": "Lines 1785 and 1818 contain calls to the Python 2-only builtin function __xrange()__ which was removed in Python 3 in favor of __range()__.  This PR adds the line [__from six.moves import xrange__](http://six.readthedocs.io/#module-six.moves) for compatibility with both Python 2 and Python 3.", "comments": []}, {"number": 17223, "title": "How to run a trained model of TensorFlow object detection in Java application with GPU support with boxes output (x, y, w, h)?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04/Windows 10\r\n- **TensorFlow installed from (source or binary)**: No\r\n- **TensorFlow version (use command below)**: 1.5\r\n- **Python version**:  3.5\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**: 4.8\r\n- **CUDA/cuDNN version**: 9.0\r\n- **GPU model and memory**: NVIDIA GTX 1080 8GB\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nHere I want to ask and sure that: Is this possible to run a trained model (using python to train) in a Java application with GPU support and detect the object and get the result of matches and the x, y point and the width and height? \r\nI am going to use python to train my own dataset by using one of the model e.g: `ssd_mobilenet_v1_coco  `OR `faster_rcnn_resnet101_coco` and then use the trained model in the Java application with GPU support with boxes output. So I want to sure that does the TensorFlow support this task or not? If yes, is there any good tutorial to show the usage? If not, why, and when it can be supported?\r\n\r\nThanks in advance!!\r\n", "comments": ["See the object detection API section for details on detection with boxes\r\nhttps://github.com/tensorflow/models/blob/master/research/object_detection/README.md", "@CarltonSemple I know the link you mentioned, but you did not get the question that I asked. \r\nI say that: \r\nIs there possible to run the trained object detection API model within Java application with **GPU** support in both **Windows** and **Linux** operating systems?\r\n\r\nI don't want a toy OR `helloword `program, I want to sure the stability and reliability in a real project.", "Hi,\r\nI've tried to run my demo application (https://github.com/szaza/tensorflow-java-yolo) with GPU on Linux Mint 18 with Nvidia Geforce 940MX, CUDA/CUDNN version 9.0, TensorFlow version 1.6, however I could run only on CPU until now. Theoretically, it is enough to include the `libtensorflow_jni_gpu` as dependency instead of `libtensorflow`. \r\nI'm still trying to get it work.\r\nBest regards,\r\n   Zoltan", "It seems like if TF Java was built with CUDA support it should allow running on GPU. @asimshankar  for  verification. Also, you may find that for inference, just running on CPU is fast, especially if you are doing small batches, the latency for pipeline synch in gpu may not be worth it.\r\n", "@Bahramudin : GPU on Windows isn't packaged yet (see #16660), but as @aselle said, we do support it it for Linux.\r\n\r\n@szaza : To use GPU on Linux, your dependencies should look [like this](https://docs.google.com/presentation/d/e/2PACX-1vQ6DzxNTBrJo7K5P8t5_rBRGnyJoPUPBVOJR4ooHCwi4TlBFnIriFmI719rDNpcQzojqsV58aUqmBBx/pub?start=false&loop=false&delayms=3000&slide=id.g306175dd89_0_41). From a cursory look at [your `build.gradle`](https://github.com/szaza/tensorflow-example-java/blob/9c69b58/build.gradle), it would be a two line change:\r\n- Change line 15 to `name: 'libtensorflow'`\r\n- Change line 16 to `name: 'libtensorflow_jni_gpu'`\r\n\r\n(Note the `org.tensorflow:tensorflow` artifact is a convenience one that pulls in `libtensorflow` and `libtensorflow_jni`)\r\n\r\nHope that helps.\r\nClosing this since I believe the question is answered, but let me know if I'm mistaken.\r\n", "@asimshankar OK, the question almost has been answered, we hope the TensorFlow support GPU for Java on windows platform. And also I think the API for using the trained model in Java a much complex to use, I think it can be better to divide the usage into steps, like 1. load 2. calling the API for recognition 3. release resources. But now when loading we must do a lot of work, and also when doing recognition also do a lot of works, so then there is some job can incapsulate inside the API no need the user do the duplicate job. So it will be the API be handy and friendly. Thanks!", "Thanks @asimshankar for your response, however I tried those dependencies before. As you can see the 'libtensorflow_jni_gpu' is also present in line 19, just it is commented out right now.\r\nUnfortunately, the GPU support on Linux didn't work for me, however it can happen that my configuration is wrong.", "@szaza : Note that you want to depend on *exactly one* of `libtensorflow_jni` and `libtensorflow_jni_gpu`.  Hence, I suggested replacing lines 15 and 16 (and not just uncommenting line 19).\r\n\r\n"]}, {"number": 17222, "title": "C++ gradients for MaxPool3D, AvgPool and AvgPool3D", "body": "Resolves tensorflow/tensorflow#17195\r\n\r\nAlso checked for possible flakes in the newly added tests, none showed up with\r\n```\r\nbazel test --runs_per_test=100 `tensorflow/cc:gradients_nn_grad_test\r\n...\r\n//tensorflow/cc:gradients_nn_grad_test                                   PASSED in 0.3s\r\n  Stats over 100 runs: max = 0.3s, min = 0.2s, avg = 0.2s, dev = 0.0s\r\n```\r\n", "comments": ["@suharshs @drpngx friendly pinging about reviewing the PR."]}, {"number": 17221, "title": "tf.contrib.signal.stft: Incompatible gradient shape", "body": "**Fails on TF 1.5.0 and 1.6.0rc1**\r\n\r\nI need to compute gradients of a signal through a STFT with a smaller frame length than FFT length. Assumed this would truncate the window in the gradient computation or fail on some assertion but this fails ungracefully.\r\n\r\n### Sample code\r\n```python\r\ndef test_gradient_computation(frame_length, fft_length):\r\n    graph = tf.Graph()\r\n    with graph.as_default():\r\n        x = tf.get_variable('input', [1, 16000], tf.float32)\r\n        x = tf.contrib.signal.stft(\r\n            x,\r\n            frame_length=frame_length,\r\n            frame_step=frame_length // 2,\r\n            fft_length=fft_length\r\n        )\r\n\r\n        x = tf.abs(x)\r\n        y = tf.ones_like(x)\r\n\r\n        loss = tf.losses.mean_squared_error(x, y)\r\n\r\n        optimizer = tf.train.GradientDescentOptimizer(1e-3)\r\n        train_op = optimizer.minimize(loss)\r\n        with tf.Session() as session:\r\n            session.run(tf.global_variables_initializer())\r\n            session.run(train_op)\r\n```\r\n\r\n### This works:\r\n```python\r\nn = 1024\r\ntest_gradient_computation(frame_length=n, fft_length=n)\r\ntest_gradient_computation(frame_length=n, fft_length=n*2)\r\n```\r\n\r\n### But this fails:\r\n```python\r\nn = 1024\r\ntest_gradient_computation(frame_length=n*2, fft_length=n)\r\n```\r\n> ValueError: Incompatible shapes between op input and calculated input gradient.  Forward operation: stft/rfft.  Input index: 0. Original input shape: (1, 14, 2048).  Calculated input gradient shape: (1, 14, 1024)\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 17220, "title": "Tensorflow-produced HLO graphs cannot be rendered by graphviz", "body": "- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo, I'm using standard `mnist_softmax_xla.py` with unmodified tensorflow (cloned from master)\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04: 4.13.0-32-generic #35~16.04.1-Ubuntu SMP Thu Jan 25 10:13:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n- **TensorFlow installed from (source or binary)**:\r\nSource, with XLA and CUDA\r\n- **TensorFlow version (use command below)**:\r\ntf.VERSION = 1.6.0-rc0\r\ntf.GIT_VERSION = v1.5.0-2285-g4448430\r\ntf.COMPILER_VERSION = v1.5.0-2285-g4448430\r\n- **Python version**: \r\nPython 3.5.2\r\n- **Bazel version (if compiling from source)**:\r\nbazel release 0.10.0\r\n- **GCC/Compiler version (if compiling from source)**:\r\ngcc 5.4.0 20160609\r\n- **CUDA/cuDNN version**:\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-9.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0.176\r\n/usr/local/cuda-9.1/doc/man/man7/libcudart.7\r\n/usr/local/cuda-9.1/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-9.1/targets/x86_64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-9.1/targets/x86_64-linux/lib/libcudart.so.9.1.85\r\n- **GPU model and memory**:\r\nNvidia Titan V Driver Version: 387.34, 12058MiB\r\n- **Exact command to reproduce**:\r\n    TF_XLA_FLAGS=\"--xla_hlo_graph_path=/home/user/test/hlo_graphs --xla_generate_hlo_graph=.*\" python3 mnist_softmax_xla.py\r\n\r\n### Describe the problem\r\nI want to run XLA JIT on the `mnist_softmax_xla.py` example script, and then view the generated HLO to see what was compiled. To do this, I run:\r\n\r\n    $> TF_XLA_FLAGS=\"--xla_hlo_graph_path=/home/user/test/hlo_graph --xla_generate_hlo_graph=.*\" python3 mnist_softmax_xla.py\r\n\r\nThis produces a lot of text:\r\n\r\n    **[omitted]**\r\n    2018-02-23 12:42:22.413434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1016] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10979 MB memory) -> physical GPU (device: 0, name: Graphics Device, pci bus id: 0000:01:00.0, compute capability: 7.0)\r\n    2018-02-23 12:42:22.743524: I tensorflow/compiler/xla/service/service.cc:158] XLA service 0x7fc63c001800 executing computations on platform CUDA. Devices:\r\n    2018-02-23 12:42:22.743569: I tensorflow/compiler/xla/service/service.cc:166]   StreamExecutor device (0): Graphics Device, Compute Capability 7.0\r\n    **[omitted]**\r\n    2018-02-23 12:42:22.796890: I tensorflow/compiler/xla/service/hlo_graph_dumper.cc:1395] computation cluster_0__XlaCompiledKernel_true__XlaNumConstantArgs_0__XlaNumResourceArgs_0_.v83 [GPU-ir-emit-prepare: after copy-insertion, pipeline end]: /home/user/test/hlo_graph/hlo_graph_47.mcrbox-6dffc700-28862-565e07de51010.dot\r\n    2018-02-23 12:42:22.973757: W tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:351] *** WARNING *** You are using ptxas 9.1.108, which is in range [9.0.0, 9.0.276) + [9.1.0, 9.1.121). These versions are known to miscompile XLA code, leading to incorrect results or invalid-address errors.\r\n    2018-02-23 12:42:24.527632: I tensorflow/stream_executor/dso_loader.cc:147] successfully opened CUDA library libcupti.so.9.1 locally\r\n    **[omitted]**\r\n\r\nApart from the warning for the ptxas version (which I believe there is no way for me to fix because I'm using the latest available version of CUDA that I can get from Nvidia), this appears to be successful and the compiled execution can be seen in the timeline.json. However, I want to view the HLO graph with `graphviz`, so using the above output, I try to load `hlo_graph/hlo_graph_47.mcrbox-6dffc700-28862-565e07de51010.dot`. However, I get an error:\r\n\r\n    $> dot -Tpng hlo_graph/hlo_graph_47.mcrbox-6dffc700-28862-565e07de51010.dot -o graph.png\r\n    Error: hlo_graph/hlo_graph_47.mcrbox-6dffc700-28862-565e07de51010.dot: syntax error in line 10 near ''\r\n\r\nLine 10 is the large stylesheet definition in the `.dot` file. It's very long and I am not familiar with the syntax, but it 'seems' like there are no missing/supernumerary quotations anywhere. Is this a known problem or have I done something incorrectly? Is there another renderer (i.e. not `graphviz`) that I can target out-of-the-box?\r\n\r\nI've attached the aforementioned `.dot` file as a `.txt`:\r\n[hlo_graph_47.mcrbox-6dffc700-28862-565e07de51010.txt](https://github.com/tensorflow/tensorflow/files/1751694/hlo_graph_47.mcrbox-6dffc700-28862-565e07de51010.txt)\r\n\r\nThe graphviz version I'm using is:\r\n\r\n    dot - graphviz version 2.38.0 (20140413.2041)\r\n", "comments": ["The rendering works with the 'spreadsheet' attribute entirely removed. \r\n\r\nLooking in a little more depth, we get an error when the spreadsheet is restricted to 312 lines. For example, if you remove lines 323:665 in the attached `.dot` file then no error is given. If you instead remove lines 324:665, i.e. 1 extra line, we get the error from before.\r\n\r\nIt does not seem like the syntax of lines 323 onwards is problematic, as we can move those CSS selectors earlier in the file and get no error, provided we don't go past 312 lines. \r\n\r\nThis looks like it is therefore a `graphviz` issue, but I will keep this open if we are interested in some kind of workaround for Tensorflow.", "@tatatodd in case he has any thoughts on what typically works for visualizing HLO graphs.", "@jlebar might have some thoughts here, as he's worked on HLO visualization.", "Yeah, maybe try graphviz v2.40.1 or newer?  That seems to be what we're using internally.  (I say \"seems to be because...it's complicated, and I can't just run `dot -v`.)  You may also need to pass `-Tsvg` given that we have the HTML selectors, I dunno.  I verified that `-Tsvg` doesn't work with v2.38.0.\r\n\r\nFYI the graphs can get pretty big, and at some point graphviz will be unable to render even if it can parse.  Internally we have an interactive tool which can render subsets of the graph.  There's no fundamental reason this can't be open-sourced, and it probably should be.  But...no promises on when we'd be able to do that.", "Using `dot - graphviz version 2.40.1 (20161225.0304)`, I get a more precise confirmation from graphviz about the problem:\r\n\r\n     Error: hlo_graph_47.mcrbox-6dffc700-28862-565e07de51010.dot: syntax error in line 10 scanning a quoted string (missing endquote? longer than 16384?)\r\n    String starting:\"\r\n      data:text/css,\r\n      @import url(https://fonts.googleapis.com/css?family=Roboto:4\r\n\r\nIn graphviz, the buffer size is defined as 2^14:\r\n\r\n    #define YY_BUF_SIZE 16384\r\n\r\nI put it up to 2^20 and built graphviz from source, and I can now render the HLO as SVG. \r\n\r\n> Internally we have an interactive tool which can render subsets of the graph.\r\n\r\nThis sounds good and indeed necessary for ever-larger graphs, let us know if/when there is movement on releasing it.", "Hi Richard, this really helped, thanks", "This also saved my bacon. Thanks!", "Hi @Richard-Neill , could you please tell me where is the \"#define YY_BUF_SIZE 16384\"?", "> This sounds good and indeed necessary for ever-larger graphs, let us know if/when there is movement on releasing it.\r\n\r\nThis is top of mind again, I'm hoping I can get something workable in the next few weeks.  If and when this happens, I'll announce it on the xla-dev mailing list, https://groups.google.com/forum/#!forum/xla-dev.", "The tool has landed!\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/4817dabb5aba45547c6c1f5ef79d2b0c2040d51f\r\n\r\nAs a bonus it doesn't use `dot` on your local machine (it uses an asmjs version), so you shouldn't need to rebuild dot locally.", "Hi @jlebar, apologies for the lack of response. The solution looks good, thanks for putting it out there!"]}, {"number": 17219, "title": "KeyError: u'SSTableReaderV2'", "body": "when i use pretained model **(MobileNet_v1_0.50_160)** , it results in error **(KeyError: u'SSTableReaderV2' )**.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Sorry, the problem has solved. Thank you all the same.", "HI,\r\nI have the same problem,  when i use pretained model (MobileNet_v1_0.50_160) , it results in error (KeyError: u'SSTableReaderV2' ). \r\n@youngstu how did you solve the problem?\r\n\r\nthank you!", "HI,\r\nI have the same problem, when i use pretained model (ssd_mobilenet_v1_coco_11_06_2017) , it results in error (KeyError: u'SSTableReaderV2' ).\r\n@tamizharasank how did you solve the problem?\r\n\r\nthank you!"]}, {"number": 17218, "title": "Retrained quantized mobilenet introduces unsupported Pow operator", "body": "here's a code which can reproduce the problem\r\nfrom https://github.com/tensorflow/tensorflow/issues/15122#issuecomment-363126160\r\n\r\nBUT When  I run the toco command,  Fatal error happens:\r\n```\r\n2018-02-23 19:38:31.164513: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 66 operators, 156 arrays (1 quantized)\r\n2018-02-23 19:38:31.165597: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before default min-max range propagation graph transformations: 66 operators, 156 arrays (1 quantized)\r\n2018-02-23 19:38:31.167279: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before quantization graph transformations: 66 operators, 156 arrays (1 quantized)\r\n2018-02-23 19:38:31.221783: F tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:354] Unimplemented: this graph contains an operator of type (Unsupported TensorFlow op: Pow) for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).\r\n```\r\n\r\nI'm wondering how to use quantized mobilenet with tensorflow-lite in ios ?\r\n\r\n```\r\nARCHITECTURE=mobilenet_1.0_224_quantized                                                           \r\nDATA_DIR=~/tensorflow-for-poets-2/tf_files                                                         \r\nTRAINING_DIR=/tmp/tf_files                                                                         \r\n                                                                                                   \r\npython tensorflow/tensorflow/examples/image_retraining/retrain.py \\                                \r\n  --bottleneck_dir=$TRAINING_DIR/bottlenecks \\                                                     \r\n  --how_many_training_steps=500 \\                                                                  \r\n  --model_dir=$TRAINING_DIR/models \\                                                               \r\n  --summaries_dir=$TRAINING_DIR/training_summaries/\"${ARCHITECTURE}\" \\                             \r\n  --output_graph=$TRAINING_DIR/retrained_graph.pb \\                                                \r\n  --output_labels=$TRAINING_DIR/retrained_labels.txt \\                                             \r\n  --architecture=\"${ARCHITECTURE}\" \\                                                               \r\n  --image_dir=$DATA_DIR/flower_photos  \r\n\r\nrm -f /$TRAINING_DIR/${ARCHITECTURE}.tflite                              \r\n                        \r\ntensorflow/bazel-bin/tensorflow/contrib/lite/toco/toco \\                                           \r\n  --input_file=$TRAINING_DIR/retrained_graph.pb \\                                                  \r\n  --input_format=TENSORFLOW_GRAPHDEF \\                                                             \r\n  --output_format=TFLITE \\                                                                         \r\n  --output_file=/$TRAINING_DIR/${ARCHITECTURE}.tflite \\                                            \r\n  --inference_type=QUANTIZED_UINT8 \\                                                               \r\n  --input_array=Placeholder \\                                                                      \r\n  --output_array=final_result \\                                                                    \r\n  --input_shape=1,224,224,3 \\                                                                      \r\n  --mean_value=128 \\                                                                               \r\n  --std_value=128                                                                                  \r\n```\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: max os 10.12.6\r\n- **TensorFlow installed from (source or binary)**: pip\r\n- **TensorFlow version (use command below)**: 1.5\r\n- **Python version**:   2.7 anaconda\r\n- **Bazel version (if compiling from source)**: none\r\n- **GCC/Compiler version (if compiling from source)**: none\r\n\r\n### Describe the problem\r\nWhen I run toco comand to convert model . \r\n```\r\n2018-02-23 19:38:31.221783: F tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:354] Unimplemented: this graph contains an operator of type (Unsupported TensorFlow op: Pow) for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).\r\n\r\n```\r\n### Source code / logs\r\n```\r\nARCHITECTURE=mobilenet_1.0_224_quantized                                                           \r\nDATA_DIR=~/tensorflow-for-poets-2/tf_files                                                         \r\nTRAINING_DIR=/tmp/tf_files                                                                         \r\n                                                                                                   \r\npython tensorflow/tensorflow/examples/image_retraining/retrain.py \\                                \r\n  --bottleneck_dir=$TRAINING_DIR/bottlenecks \\                                                     \r\n  --how_many_training_steps=500 \\                                                                  \r\n  --model_dir=$TRAINING_DIR/models \\                                                               \r\n  --summaries_dir=$TRAINING_DIR/training_summaries/\"${ARCHITECTURE}\" \\                             \r\n  --output_graph=$TRAINING_DIR/retrained_graph.pb \\                                                \r\n  --output_labels=$TRAINING_DIR/retrained_labels.txt \\                                             \r\n  --architecture=\"${ARCHITECTURE}\" \\                                                               \r\n  --image_dir=$DATA_DIR/flower_photos  \r\n\r\nrm -f /$TRAINING_DIR/${ARCHITECTURE}.tflite                              \r\n                        \r\ntensorflow/bazel-bin/tensorflow/contrib/lite/toco/toco \\                                           \r\n  --input_file=$TRAINING_DIR/retrained_graph.pb \\                                                  \r\n  --input_format=TENSORFLOW_GRAPHDEF \\                                                             \r\n  --output_format=TFLITE \\                                                                         \r\n  --output_file=/$TRAINING_DIR/${ARCHITECTURE}.tflite \\                                            \r\n  --inference_type=QUANTIZED_UINT8 \\                                                               \r\n  --input_array=Placeholder \\                                                                      \r\n  --output_array=final_result \\                                                                    \r\n  --input_shape=1,224,224,3 \\                                                                      \r\n  --mean_value=128 \\                                                                               \r\n  --std_value=128         \r\n```", "source code from @andrehentz  ", "@tensorflowbutler ", "@ericyue \r\nThis is a known issue, since the quantize op currently not fully implemented...\r\n\r\nTake a look at [issue15871](https://github.com/tensorflow/tensorflow/issues/15871)", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@ericyue Could you try with a recent version of TF? Those steps should work and should not introduce Pow. Let me know if you still find problems.", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 45 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 17217, "title": "Could not find a version that satisfies the requirement tensorflow-gpu (from versions: )", "body": "Can anyone help with this? I tried quite a few times even wiped the computer and started from scratch.\r\n\r\n**Environment**\r\n- CPU: Intel i7\r\n- Memory: 16GB\r\n- OS: Windows 10 64bit\r\n- Nvidia GTX 1060 (latest driver installed: 390.77-desktop-win10-64bit-international-whql)\r\n- Visual Studio Community\r\n- CUDA 8.0 for Windows 10\r\n- cuDNN 5.1 for CUDA 8.0\r\n- Python 3.6.4\r\n\r\n\r\nI have a few versions available on local hard drive but all of them are giving me the same error message.\r\n- tensorflow-1.5.0-cp36-cp36m-win_amd64.whl\r\n- tensorflow-1.6.0rc1-cp35-cp35m-win_amd64.whl\r\n- tensorflow-1.6.0rc1-cp36-cp36m-win_amd64.whl\r\n\r\n\r\n**By running `python -c \"from pip import pep425tags;print(pep425tags.supported_tags)\"`**:\r\n`[('cp36', 'cp36m', 'win32'), ('cp36', 'none', 'win32'), ('py3', 'none', 'win32'), ('cp36', 'none', 'any'), ('cp3', 'none', 'any'), ('py36', 'none', 'any'), ('py3', 'none', 'any'), ('py35', 'none', 'any'), ('py34', 'none', 'any'), ('py33', 'none', 'any'), ('py32', 'none', 'any'), ('py31', 'none', 'any'), ('py30', 'none', 'any')]`", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "- Have I written custom code: N/A\r\n- OS Platform and Distribution: Windows 10 64bit\r\n- TensorFlow installed from: local files\r\n- TensorFlow version: tensorflow-1.6.0rc1-cp35-cp35m-win_amd64.whl\r\n- Bazel version: N/A\r\n- CUDA/cuDNN version: 8.0/5.1\r\n- GPU model and memory: GTX 1060 4GB\r\n- Exact command to reproduce: `pip3 install tensorflow-1.6.0rc1-cp35-cp35m-win_amd64.whl`\r\n\r\nHope this helps.", "You need CUDA 9.0 and cuDNN v7.0, and then make sure they're referenced in the Path environment variable.\r\nSee https://www.tensorflow.org/install/install_windows", "Also, choose a command with `cp36` since you have Python 3.6.\r\nFor example\r\n~~~~\r\npip3 install tensorflow-1.6.0rc1-cp36-cp36m-win_amd64.whl\r\n~~~~", "I installed CUDA 9.1.85 and cudnn 7 for 9.1 and tried\r\n```bash\r\npip3 install tensorflow-1.6.0rc1-cp36-cp36m-win_amd64.whl\r\n```\r\n\r\nbut still having **tensorflow-1.6.0rc1-cp36-cp36m-win_amd64.whl is not a supported wheel on this platform.** issue.", "Are there any x64 versions?", "I found the issue:\r\nMy windows 10 is 64 bit but I have a 32 bit python installed. After uninstalled the 32 bit python and reinstalled the 64 bit python, problem solved.\r\nThanks for everyone.", "it still doesnt work for me", "Currently TensorFlow released Python package up to 3.6, I have met the same problem and success after using Python 3.6 instead of 3.7, wish this help..", "> Currently TensorFlow released Python package up to 3.6, I have met the same problem and success after using Python 3.6 instead of 3.7, wish this help..\r\n\r\nyes, you are right!!!!", "> I installed CUDA 9.1.85 and cudnn 7 for 9.1 and tried\r\n> \r\n> ```shell\r\n> pip3 install tensorflow-1.6.0rc1-cp36-cp36m-win_amd64.whl\r\n> ```\r\n> \r\n> but still having **tensorflow-1.6.0rc1-cp36-cp36m-win_amd64.whl is not a supported wheel on this platform.** issue.\r\n\r\nHey Hi\r\nCan you please tell me how did you install CUDA 9.1.85?\r\nIf you have any hyperlinks to install please share, I will try them.\r\n\r\nThanks"]}, {"number": 17216, "title": "terminate called after throwing an instance of 'std::system_error'", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Ubuntu 16.04.1 LTS (GNU/Linux3.13.0-105-generic x86_64)\r\n- **TensorFlow installed from (source or binary)**: binary (tf_nightly-1.head-cp36-cp36m-linux_x86_64.whl)\r\n- **TensorFlow version (use command below)**: 1.7.0-dev20180222\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: CPU(Intel Xeon E5-2620 v3)\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\n\r\n```\r\n>>> import tensorflow as tf\r\n>>> hello = tf.constant('Hello, TensorFlow!')\r\n>>> sess = tf.Session()\r\n```\r\n\r\n\r\n\r\n### Describe the problem\r\nAfter I installed tensorflow which was downloaded from the main page you provided on github\r\n\r\n[tf_nightly-1.head-cp36-cp36m-linux_x86_64.whl](http://ci.tensorflow.org/view/tf-nightly/job/tf-nightly-linux/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3.6,label=gpu-linux/lastSuccessfulBuild/artifact/pip_test/whl/tf_nightly_gpu-1.head-cp36-cp36m-linux_x86_64.whl)\r\n\r\nI tried to  test the tensorflow and when I run `sess = tf.Session()`\r\n\r\nI got the error:\r\n\r\n```\r\nterminate called after throwing an instance of 'std::system_error'\r\n  what():  Resource temporarily unavailable\r\nAborted\r\n```\r\n\r\nI use anaconda create a new python 3.6 enviroment for tensorflow. After I downloaded the .whl package I use \r\n\r\n`pip install tf_nightly-1.head-cp36-cp36m-linux_x86_64.whl`\r\n\r\nto install the tensorflow\r\n\r\nI've seen a similar problem and in that case his system is CentOS. So is anyone having a similar experience with this problem?\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "@LotusWang0723 Do you solve this problem? I have the same problem.", "I just run \"tf.test.is_gpu_available()\" and get the same error, can anyone help me?", "I upgraded the CUDA and cudnn, then the error was gone.\n\n\n\u738b\u843d\u6850\n\nLotus WANG (Ms.)\n\n\u535a\u58eb\u751f\n\n\u5317\u4eac\u5927\u5b66 \u5de5\u5b66\u9662\n\u751f\u7269\u533b\u5b66\u5de5\u7a0b\u7cfb\nPh.D Candidate\nDepartment of Biomedical Engineering\n\nCollege of Engineering, Peking University\n\n\n\u539f\u59cb\u90ae\u4ef6\n\u53d1\u4ef6\u4eba:Victor Tannotifications@github.com\n\u6536\u4ef6\u4eba:tensorflow/tensorflowtensorflow@noreply.github.com\n\u6284\u9001:LotusWangwangluotong@126.com; Mentionmention@noreply.github.com\n\u53d1\u9001\u65f6\u95f4:2019\u5e743\u670820\u65e5(\u5468\u4e09)\u200709:26\n\u4e3b\u9898:Re: [tensorflow/tensorflow] terminate called after throwing aninstance of 'std::system_error' (#17216)\n\n\nI just run \"tf.test.is_gpu_available()\" and get the same error, can anyone help me?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.", "@LotusWang0723 I have no root privilege, so I use conda to install cudatoolkit and cudnn. The versions matches with the official built binary. But I still got this error. ", "By the way, in the same context (cudatookit=10.0, cudnn=7.4), PyTorch library can run without any issues, but TensorFlow just throws this error.", "I solved this problem by increasing the max user processes.", "> I solved this problem by increasing the max user processes.\r\n\r\n@theonegis Could you please let me know what is the max user processes did you choose to fix the problem please ?\r\n\r\nAnd how did you increase it without root privilege ?", "@oksyassine Run ulimit, check the max user processes allowed (-u)\r\n\r\nRun ulimit -u 4096 (if 4096 is the max user processes allowed).\r\n\r\nThis shouldn't need root access."]}, {"number": 17215, "title": "No clear documentation about how optimizer works through tf.tile() ", "body": "If some one uses this type of code for batch learning,\r\n`w1=tf.get_variable(\"W1\", [300,300], dtype=tf.float32, initializer=init)\r\nW1=tf.tile(tf.expand_dims(w1,axis=0), [batch_size,1,1], name=\"batch_W1\")\r\nx1=tf.placeholder(name=\"context\",dtype=tf.float32,shape=[None,None,300]) ;\r\ny=tf.matmul(x1,W1)`\r\nhow the optimizer will combine the gradients from `W1 [batch_size,300,300]` to `w1[300,300]`?\r\nI asked this question in stackoverflow also but none replied.\r\nhttps://stackoverflow.com/questions/48933564/is-this-the-appropriate-way-to-set-up-weights-in-batch-learning-in-tensorflow\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I have included a custom code, rest are not required in this case", "When in doubt, check the [source code](https://github.com/tensorflow/tensorflow/blob/7c324811f10eaf049735ace7b6a228ccc931d8ff/tensorflow/python/ops/array_grad.py#L566).\r\n\r\nSo apparently, in your case, the gradient will be summed along axis 0 in back propagation.\r\n", "Thank you very much, I'll remember that from next time.\r\nYou can close the issue."]}, {"number": 17214, "title": "Unimplemented: TensorArray has size zero, but element shape [24,24,?] is not fully defined. Currently only static shapes are supported when packing zero-size TensorArrays.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: python version - pip, c++ version - makefile\r\n- **TensorFlow version (use command below)**: r1.5\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: \r\n- **GPU model and memory**: \r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI have tensorflow frozen graph. When I read it using python api everything is good but if I do it via c++ api it fails with the error:\r\n```\r\nUnimplemented: TensorArray has size zero, but element shape [24,24,?] is not fully defined. Currently only static shapes are supported when packing zero-size TensorArrays.\r\n\t [[Node: TensorArrayStack/TensorArrayGatherV3 = TensorArrayGatherV3[_class=[\"loc:@TensorArray\"], dtype=DT_INT32, element_shape=[24,24,?], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](TensorArray, TensorArrayStack/range, while/Exit_1)]]\r\n```\r\n\r\n### Source code / logs\r\nI guess this is the problem related with while_loop op. This is some part of code that constructs tensorflow graph:\r\n```\r\ndets = convert_to_square(boxes_c)\r\nnum_box = tf.shape(dets)[0]\r\nh = tf.shape(x)[0]\r\nw = tf.shape(x)[1]\r\nx0, y0, dy, dx, ex, ey, edx, edy, tmpw, tmph = pad(dets, h, w, num_box)\r\nx_int = tf.cast(x, tf.int32)\r\nims_resized = tf.TensorArray(tf.int32, num_box)\r\ni = 0\r\n[i, ims_resized, x_int, tmph, tmpw, dy, edy, dx, edx, y0, ey, x0, ex] = tf.while_loop(cond, generate,\r\n                            [i, ims_resized, x_int, tmph, tmpw, dy, edy, dx, edx, y0, ey, x0, ex])\r\n\r\nims_resized_normal_tensor0 = ims_resized.stack()\r\nims_resized_normal_tensor = (tf.cast(ims_resized_normal_tensor0, tf.float32) - 127.5) / 128.0\r\nims_resized = ims_resized.close()\r\n```\r\n", "comments": ["Is there more information you can share that can help us reproduce the problem? For example, is it possible to share the graph and the C++ code used to execute it? This error might also occur if the input somehow leads to state unexpected by your model.\r\n\r\nCould you clarify if executing the model with the same inputs works in Python but fails in C++, or does it fail in both? You mentioned \"When I read it using python api everything is good but if I do it via c++ api it fails with the error\", but I wasn't sure what you mean by \"read it using Python api\" - are you feeding the same inputs to session.run?\r\n", "Yes. I am feeding the same images into sessions. In python is numpy object with 3-dimention shape, in c++ api it is tensorflow::Tensor.\r\nUnfortunately I am not able to share frozen graph.", "Yes. Actually, there were different inputs."]}, {"number": 17213, "title": "Level 3 warnings in core headers", "body": "I found some level 3 warnings in core headers:\r\n\r\n```\r\ntensorflow/core/framework/node_def_util.h(118): warning C4267: 'return': conversion from 'size_t' to 'int', possible loss of data\r\ntensorflow/core/lib/core/arena.h(46): warning C4267: 'argument': conversion from 'size_t' to 'const int', possible loss of data\r\ntensorflow/core/graph/graph.h(475): warning C4244: 'return': conversion from 'const tensorflow::int64' to 'int', possible loss of data\r\ntensorflow/core/graph/graph.h(480): warning C4244: 'return': conversion from 'tensorflow::int64' to 'int', possible loss of data\r\ninclude\\tensorflow/core/graph/graph.h(509): warning C4267: 'return': conversion from 'size_t' to 'int', possible loss of data\r\ntensorflow/core/graph/graph.h(518): warning C4267: 'return': conversion from 'size_t' to 'int', possible loss of data\r\n```\r\n\r\nThis issue can be fixed using static_cast<>. I'm using VS2017 15.5.6 on Windows 10.", "comments": ["This warnings might be compiler specific. ", "As far as I know, gcc will also raise warnings for conversion from 'size_t' to 'int' ([see here for example](https://stackoverflow.com/questions/27490762/how-can-i-convert-to-size-t-from-int-safely)). Beyond the warning issue, is there any reason to return an int instead of a size_t since these functions are returning sizes (so positive integer)?"]}, {"number": 17212, "title": "got error when run eval.py in object detection api", "body": "Hi ,\r\ni faced with this error when i run eval.py:\r\n\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nTraceback (most recent call last):\r\nFile \"eval.py\", line 142, in\r\ntf.app.run()\r\nFile \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 124, in run\r\n_sys.exit(main(argv))\r\nFile \"eval.py\", line 138, in main\r\nFLAGS.checkpoint_dir, FLAGS.eval_dir)\r\nFile \"/home/mm/models/research/object_detection/evaluator.py\", line 240, in evaluate\r\nsave_graph_dir=(eval_dir if eval_config.save_graph else ''))\r\nFile \"/home/mm/models/research/object_detection/eval_util.py\", line 448, in repeated_checkpoint_run\r\nreturn metrics\r\nUnboundLocalError: local variable 'metrics' referenced before assignment\r\n\r\n########################################################################\r\ni once saw that must be initial metrics : in the beginning of repeated_checkpoint_run i i added this :\r\nmetrics={}\r\npr_value={}\r\nglobal_step={}\r\ni this case , that error don't face but after for a while processing don't give me any value :\r\nmm@mm:~/models/research/object_detection$ python3 eval.py \\\r\n\r\n        --logtostderr \\\r\n        --checkpoint_dir=training_ssd_mobile/model.ckpt-200000 \\\r\n        --eval_dir=eval_log\\\r\n        --pipeline_config_path=ssd_mobilenet_v1_coco_2017_11_17/ssd_mobilenet_v1_focal_loss_coco.config\r\n\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nINFO:tensorflow:depth of additional conv before box predictor: 0", "comments": ["I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "yes , i'm working to solve this problem.", "Nagging Assignee @bignamehyp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I am facing the same problem, has anyone found a workaround to solve this issue?", "Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "@bignamehyp \r\nOS: Ubuntu 16.04\r\nTF version: 1.6.0\r\n(note that it also happens to me on macOS, TF 1.6.0)\r\n\r\nCommand used:\r\n`python scripts/eval.py --logtostderr --checkpoint_dir=models/baseline --eval_dir=models/baseline/eval --pipeline_config_path=scripts/baseline.config`\r\n\r\nIn the folder models/baseline, there are 3 files:\r\nbaseline-ckpt.data-00000-of-00001\r\nbaseline-ckpt.index\r\nbaseline-ckpt.meta\r\n\r\nI also tried command\r\n`python scripts/eval.py --logtostderr --checkpoint_dir=models/baseline/baseline-ckpt --eval_dir=models/baseline/eval --pipeline_config_path=scripts/baseline.config`\r\nbecause in baseline.config, I have the setting `fine_tune_checkpoint: \"models/baseline/baseline-ckpt\"` and I used it to run training fine. \r\n\r\nAfter adding `from tensorflow.python.platform import tf_logging as logging` in eval_util.py as suggested in this [post](https://stackoverflow.com/questions/48163085/tensorflow-object-detection-api-eval-py-metrics-referenced-before-assignment), I can see that it was because of the following error\r\n\r\nWARNING:tensorflow:`shuffle` is false, but the input data stream is still slightly shuffled since `num_readers` > 1.\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nINFO:tensorflow:Starting evaluation at 2018-05-02-21:07:47\r\n**INFO:tensorflow:No model found in model/baseline. Will try again in 300 seconds\r\nINFO:tensorflow:Finished evaluation!**\r\nTraceback (most recent call last):\r\n  File \"scripts/eval.py\", line 162, in <module>\r\n    tf.app.run()\r\n  File \"/Users/siyuyang/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 126, in run\r\n    _sys.exit(main(argv))\r\n  File \"scripts/eval.py\", line 158, in main\r\n    FLAGS.checkpoint_dir, FLAGS.eval_dir)\r\n  File \"/Users/siyuyang/source/repos/TF_models/research/object_detection/evaluator.py\", line 274, in evaluate\r\n    losses_dict=losses_dict)\r\n  File \"/Users/siyuyang/source/repos/TF_models/research/object_detection/eval_util.py\", line 438, in repeated_checkpoint_run\r\n    return metrics\r\nUnboundLocalError: local variable 'metrics' referenced before assignment\r\n\r\nHere's the relevant parts in the config file:\r\n\r\n> eval_config: {\r\n>   num_examples: 16254\r\n>   num_visualizations: 300\r\n>   max_num_boxes_to_visualize: 200\r\n>   visualize_groundtruth_boxes: true\r\n>   keep_image_id_for_visualization_export: true\r\n>   max_evals: 1\r\n>   metrics_set: 'pascal_voc_detection_metrics'\r\n> }\r\n> \r\n> eval_input_reader: {\r\n>   tf_record_input_reader {\r\n>     input_path: \"train_images_processed/t3.record\"\r\n>    }\r\n>   label_map_path: \"scripts/label_map.pbtxt\"\r\n>   shuffle: false\r\n>   num_readers: 1\r\n> }\r\n\r\nNote that I set shuffle to false and num_reader to 1, but there was the warning \r\n\r\n> WARNING:tensorflow:`shuffle` is false, but the input data stream is still slightly shuffled since `num_readers` > 1.", "@bignamehyp I probably have information that can help narrow down the issue. So I had an early clone of the `model` repository which works, however the latest clone from the `model` repo does not work for me either (the same error message as above). I believe there's some check-in which introduces some backward incompatibility.\r\n\r\nThe output of `git logs` from my clone is like this, which doesn't have the issue mentioned above. \r\n\r\n\r\n```\r\ncommit aad56e4c428ce9ff5fbb1e9e1b0ef96f1e1fdfd2\r\nMerge: e3bfb9b c54c9bd\r\nAuthor: Lukasz Kaiser <lukaszkaiser@users.noreply.github.com>\r\nDate:   Mon Apr 9 15:57:25 2018 -0700\r\n\r\n    Merge pull request #3918 from andrefaraujo/master\r\n\r\n    Updating DELF README\r\n\r\ncommit c54c9bd9f028ee9ec29aa63b5303ac1282662888\r\nAuthor: Andre Araujo <andrearaujo@rodete-desktop-imager.corp.google.com>\r\nDate:   Mon Apr 9 15:21:37 2018 -0700\r\n\r\n    Updating DELF README\r\n\r\ncommit e3bfb9b8dc0161d77e7df4be4592ebbe0ec579d3\r\nMerge: 2506c9a e658d64\r\nAuthor: pkulzc <lzc@google.com>\r\nDate:   Mon Apr 9 12:01:25 2018 -0700\r\n\r\n    Merge pull request #3744 from liangxiao05/master\r\n\r\n    Update create_pascal_tf_record.py\r\n\r\ncommit 2506c9a65757b371d66a7aea3a6f973ea414f4da\r\nMerge: fbb27cf 77725c6\r\nAuthor: pkulzc <lzc@google.com>\r\nDate:   Mon Apr 9 12:01:02 2018 -0700\r\n\r\n    Merge pull request #3914 from matiji66/master\r\n\r\n    Update evaluation_protocols.md\r\n\r\ncommit fbb27cf31f09c3b4b10c1e237fd283f06db301d2\r\nAuthor: Taylor Robie <taylorrobie@google.com>\r\nDate:   Mon Apr 9 11:14:59 2018 -0700\r\n\r\n    Add fp16 support to official ResNet. (#3687)\r\n\r\n    * Add fp16 support to resnet.\r\n\r\n    * address PR comments\r\n\r\n    * add dtype checking to model definition\r\n\r\n    * delint\r\n\r\n    * more PR comments\r\n```\r\n\r\nAnd following the trace, I believe https://github.com/tensorflow/models/tree/c54c9bd9f028ee9ec29aa63b5303ac1282662888 this branch should not have the problem above (though I haven't validated)\r\n", "@bignamehyp \r\nI was able to resolve my issue! It was actually because I lacked the \"checkpoint\" text file of the format\r\n> model_checkpoint_path: \"baseline-ckpt\"\r\n> all_model_checkpoint_paths: \"baseline-ckpt\"\r\n\r\nin `checkpoint_dir`.\r\n\r\nThe `local variable 'metrics' referenced before assignment` error is probably just failing to catch that case when no checkpoints can be found. \r\n\r\nCould you also add `from tensorflow.python.platform import tf_logging as logging` to `eval_util.py` in the place of `import logging`, as it is really hard to tell what's going on without the logging messages. ", "@yangsiyu007 , `from tensorflow.python.platform import tf_logging as logging to eval_util.py` that is work. i before tested this .", "Nagging Assignee @bignamehyp: It has been 23 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@njzhaixiang the checkpoint file should be in the train_folder (i.e. together with the checkpoint file)", "@njzhaixiang yes like xiaoyongzhu said, the \"checkpoint\" text file should be in the same directory where your other 3 checkpoint files are.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Try the updated tutorial:\r\nhttps://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193"]}, {"number": 17211, "title": "Getting Redeclaration Error of \"error: redeclaration of 'P_ALL'    P_ALL,  /* Wait for any child.  */\" while cross compiling for raspberry pi 3", "body": "# I'm trying to cross compile TensorFlow for Raspberry pi 3 and every time m getting the error like \"error: redeclaration of 'P_ALL'\r\n   P_ALL,  /* Wait for any child.  */\"\r\n\r\n# I Followed these steps,\r\n1.Installed Bazel\r\n2.Cloned Tensorflow and checkout r1.5\r\n3.Running the \"./tensorflow/tools/ci_build/pi/build_raspberry_pi.sh\"\r\n\r\nError:-/home/baladev/.cache/bazel/_bazel_baladev/da0e175f87e998fe3d550279550cec2c/external/grpc/BUILD:431:1: C++ compilation of rule '@grpc//:gpr_base' failed (Exit 1): arm-linux-gnueabihf-gcc failed: error executing command \r\n  (cd /home/baladev/.cache/bazel/_bazel_baladev/da0e175f87e998fe3d550279550cec2c/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/home/baladev/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n  /home/baladev/.cache/bazel/_bazel_baladev/da0e175f87e998fe3d550279550cec2c/external/arm_compiler/bin/arm-linux-gnueabihf-gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -DRASPBERRY_PI -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK '-march=armv7-a' '-mfpu=neon-vfpv4' '-std=gnu11' '-DS_IREAD=S_IRUSR' '-DS_IWRITE=S_IWUSR' -O3 -U__GCC_HAVE_SYNC_COMPARE_AND_SWAP_1 -U__GCC_HAVE_SYNC_COMPARE_AND_SWAP_2 -U__GCC_HAVE_SYNC_COMPARE_AND_SWAP_8 -funsafe-math-optimizations -ftree-vectorize -fomit-frame-pointer '-std=c++11' -isystem /usr/include/arm-linux-gnueabihf -isystem /usr/include/python2.7 -isystem /usr/include/ -MD -MF bazel-out/armeabi-opt/bin/external/grpc/_objs/gpr_base/external/grpc/src/core/lib/support/subprocess_posix.pic.d '-frandom-seed=bazel-out/armeabi-opt/bin/external/grpc/_objs/gpr_base/external/grpc/src/core/lib/support/subprocess_posix.pic.o' -fPIC '-DGRPC_ARES=0' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -iquote external/grpc -iquote bazel-out/armeabi-opt/genfiles/external/grpc -iquote external/bazel_tools -iquote bazel-out/armeabi-opt/genfiles/external/bazel_tools -iquote external/com_google_absl -iquote bazel-out/armeabi-opt/genfiles/external/com_google_absl -isystem external/grpc/include -isystem bazel-out/armeabi-opt/genfiles/external/grpc/include -isystem external/bazel_tools/tools/cpp/gcc3 -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -no-canonical-prefixes -fno-canonical-system-headers -c external/grpc/src/core/lib/support/subprocess_posix.cc -o bazel-out/armeabi-opt/bin/external/grpc/_objs/gpr_base/external/grpc/src/core/lib/support/subprocess_posix.pic.o)\r\ncc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++\r\nIn file included from external/grpc/src/core/lib/support/subprocess_posix.cc:33:0:\r\n/home/baladev/.cache/bazel/_bazel_baladev/da0e175f87e998fe3d550279550cec2c/external/arm_compiler/bin/../arm-linux-gnueabihf/sysroot/usr/include/sys/wait.h:101:3: error: redeclaration of 'P_ALL'\r\n   P_ALL,  /* Wait for any child.  */\r\n   ^\r\nIn file included from /usr/include/stdlib.h:41:0,\r\n                 from external/grpc/src/core/lib/support/subprocess_posix.cc:30:\r\n/usr/include/bits/waitflags.h:52:3: note: previous declaration 'idtype_t P_ALL'\r\n   P_ALL,  /* Wait for any child.  */\r\n   ^\r\nIn file included from external/grpc/src/core/lib/support/subprocess_posix.cc:33:0:\r\n/home/baladev/.cache/bazel/_bazel_baladev/da0e175f87e998fe3d550279550cec2c/external/arm_compiler/bin/../arm-linux-gnueabihf/sysroot/usr/include/sys/wait.h:102:3: error: redeclaration of 'P_PID'\r\n   P_PID,  /* Wait for specified process.  */\r\n   ^\r\nIn file included from /usr/include/stdlib.h:41:0,\r\n                 from external/grpc/src/core/lib/support/subprocess_posix.cc:30:\r\n/usr/include/bits/waitflags.h:53:3: note: previous declaration 'idtype_t P_PID'\r\n   P_PID,  /* Wait for specified process.  */\r\n   ^\r\nIn file included from external/grpc/src/core/lib/support/subprocess_posix.cc:33:0:\r\n/home/baladev/.cache/bazel/_bazel_baladev/da0e175f87e998fe3d550279550cec2c/external/arm_compiler/bin/../arm-linux-gnueabihf/sysroot/usr/include/sys/wait.h:103:3: error: redeclaration of 'P_PGID'\r\n   P_PGID  /* Wait for members of process group.  */\r\n   ^\r\nIn file included from /usr/include/stdlib.h:41:0,\r\n                 from external/grpc/src/core/lib/support/subprocess_posix.cc:30:\r\n/usr/include/bits/waitflags.h:54:3: note: previous declaration 'idtype_t P_PGID'\r\n   P_PGID  /* Wait for members of process group.  */\r\n   ^\r\nIn file included from external/grpc/src/core/lib/support/subprocess_posix.cc:33:0:\r\n/home/baladev/.cache/bazel/_bazel_baladev/da0e175f87e998fe3d550279550cec2c/external/arm_compiler/bin/../arm-linux-gnueabihf/sysroot/usr/include/sys/wait.h:104:3: error: conflicting declaration 'typedef enum idtype_t idtype_t'\r\n } idtype_t;\r\n   ^\r\nIn file included from /usr/include/stdlib.h:41:0,\r\n                 from external/grpc/src/core/lib/support/subprocess_posix.cc:30:\r\n/usr/include/bits/waitflags.h:55:3: note: previous declaration as 'typedef enum idtype_t idtype_t'\r\n } idtype_t;\r\n   ^\r\nINFO: Elapsed time: 984.156s, Critical Path: 40.58s\r\nFAILED: Build did NOT complete successfully\r\n\r\n_NOTE:-I'm Cross Compiling for Raspberry PI 3 In ubuntu 16.04LTS_\r\n\r\nM I missing something??If yes kindly let me know.\r\nThanks in advance!!", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "@bignamehyp I'm using Ubuntu 16.04LTS and X86_64 Architecture for cross compilation for Raspberry PI 3, TensorFlow Version is \"1.5.0\" and Bazel Version is \"0,10.0\"\r\nTrying to cross compile the above version of Tensorflow with bazel for Raspberry PI 3, and m following the below steps for cross compilation,\r\n1:-Installing Bazel\r\n2:-Cloning tensorflow and checking out r1.5\r\n3:-Executing \"**/tensorflow/tools/ci_build/pi/build_raspberry_pi.sh**\" script present inside the given directory.\r\n4:-I'm Cross Compiling from Source Code.\r\nM i Missing something more for cross compilation?\r\nPlease help me on this..Thanks in advance!!", "@bmabey can you please help on this?", "Nagging Assignee @cy89: It has been 189 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@AnchoretBaladev can you move to version 1.9, instead, which officially supports Raspberry Pi?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}]