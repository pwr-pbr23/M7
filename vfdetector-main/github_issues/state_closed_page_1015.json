[{"number": 22899, "title": "Error reported to Coordinator: libnccl.so.2: cannot open shared object file: No such file or directory", "body": "When I use tf-version 1.11, I found above error for distribute strategy on multi-gpus", "comments": ["Calling model_fn.\r\nINFO:tensorflow:batch_all_reduce invoked for batches size = 158 with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\r\nINFO:tensorflow:Error reported to Coordinator: libnccl.so.2: cannot open shared object file: No such file or directory\r\nTraceback (most recent call last):\r\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\r\n    yield\r\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 177, in _call_for_each_tower\r\n    **merge_kwargs)\r\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 659, in _distributed_apply\r\n    variable_scope.VariableAggregation.SUM, grads_and_vars)\r\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/distribute.py\", line 773, in batch_reduce\r\n    return self._batch_reduce(aggregation, value_destination_pairs)\r\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 620, in _batch_reduce\r\n    value_destination_pairs)\r\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/cross_tower_ops.py\", line 243, in batch_reduce\r\n    return self._batch_reduce(aggregation, value_destination_pairs)\r\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/cross_tower_ops.py\", line 597, in _batch_reduce\r\n    [v[0] for v in value_destination_pairs])\r\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/cross_tower_ops.py\", line 631, in _batch_all_reduce\r\n    device_grad_packs)\r\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/cross_tower_utils.py\", line 41, in aggregate_gradients_using_nccl\r\n    agg_grads = nccl.all_sum(single_grads)\r\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/nccl/python/ops/nccl_ops.py\", line 49, in all_sum\r\n    return _apply_all_reduce('sum', tensors)\r\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/nccl/python/ops/nccl_ops.py\", line 217, in _apply_all_reduce\r\n    _validate_and_load_nccl_so()\r\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/nccl/python/ops/nccl_ops.py\", line 288, in _validate_and_load_nccl_so\r\n    _maybe_load_nccl_ops_so()\r\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/nccl/python/ops/nccl_ops.py\", line 274, in _maybe_load_nccl_ops_so\r\n    resource_loader.get_path_to_datafile('_nccl_ops.so'))\r\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/util/loader.py\", line 56, in load_op_library\r\n    ret = load_library.load_op_library(path)\r\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/load_library.py\", line 56, in load_op_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename)\r\nNotFoundError: libnccl.so.2: cannot open shared object file: No such file or directory\r\n", "@xiaohu2015 Did you install NCCL2? Please provide more information by filling out the issue template.\r\n\r\n**System information**\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): \r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary): \r\nTensorFlow version: \r\nPython version: \r\nInstalled using virtualenv? pip? conda?:\r\nBazel version (if compiling from source): \r\nGCC/Compiler version (if compiling from source): \r\nCUDA/cuDNN version: \r\nGPU model and memory:\r\nDescribe the problem\r\n\r\nProvide the exact sequence of commands / steps that you executed before running into the problem", "You should install NVIDIA Collective Communications Library (NCCL) first for contribution training.\r\nhttps://developer.nvidia.com/nccl/nccl-download", "@huizhang0110 thx, I will try", "@wt-huang thx, I got.", "Closing as this is resolved, free to reopen if problem persists\r\n\r\n\r\n\r\n"]}, {"number": 22898, "title": "Error building on Windows // no such package '@flatbuffers", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: from source\r\n- **TensorFlow version (use command below)**: 1.11\r\n- **Python version**: 3.6.6\r\n- **Bazel version (if compiling from source)**: 0.17.2\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**: bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\n### Describe the problem\r\nI followed the instructions from https://www.tensorflow.org/install/source_windows \r\nWhen I tried to make a cpu only build with the command\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\nI get the error:\r\n\r\nC:\\tensorflow>bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\nStarting local Bazel server and connecting to it...\r\nWARNING: Option 'experimental_shortened_obj_file_path' is deprecated\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@flatbuffers//': C:/users/xxxx/_bazel_xxxx/xv6zejqw/external/flatbuffers/tests (Directory not empty)\r\nINFO: Elapsed time: 106.904s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (246 packages loaded)\r\n", "comments": ["Did you find a solution, @ivopivo? I'm facing the same issue.", "I am also having this error. Did you find a solution too @arnaldog12?"]}, {"number": 22897, "title": "Conversion from pb to tflite fails", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: binary through pip\r\n- **TensorFlow version (use command below)**: tf-nightly and tf 1.11 (updated today)\r\n- **Python version**: 3.6.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: CPU only\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\nI have a trained model and exportet it to a frozen pb file. After that, I optimized this frozen graph for inference. Now I want to convert this graph to a tflite model to use on Android.\r\nThe code I used to convert it is:\r\n```\r\nimport tensorflow as tf\r\n\r\ngraph_def_file = \"graph_optimized.pb\"\r\ninput_arrays = [\"Placeholder\"]\r\noutput_arrays = [\"output\"]\r\n\r\nconverter = tf.contrib.lite.TocoConverter.from_frozen_graph(\r\n    graph_def_file, input_arrays, output_arrays, input_shapes={\"Placeholder\" : [1, 227, 227, 3]})\r\ntflite_model = converter.convert()\r\nopen(\"save_path/converted_model.tflite\", \"wb\").write(tflite_model)\r\n```\r\n\r\nThis fails with an error. I tried it both with my existing tensorflow installation and with a clean installation in a new virtualenv with tf-nightly like it was suggested in issue #22617 . Still the same errors happen\r\n### Source code / logs\r\n\r\n```\r\nWARNING:tensorflow:From C:/Users/User/Documents/Test_export/export.py:8: TocoConverter.from_frozen_graph (from tensorflow.contrib.lite.python.lite) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `lite.TFLiteConverter.from_frozen_graph` instead.\r\n2018-10-11 11:57:49.338944: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\nTraceback (most recent call last):\r\n  File \"C:/Users/User/Documents/Test_export/export.py\", line 9, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"C:\\Users\\User\\venv-tf-nightly\\lib\\site-packages\\tensorflow\\contrib\\lite\\python\\lite.py\", line 453, in convert\r\n    **converter_kwargs)\r\n  File \"C:\\Users\\User\\venv-tf-nightly\\lib\\site-packages\\tensorflow\\contrib\\lite\\python\\convert.py\", line 348, in toco_convert_impl\r\n    input_data.SerializeToString())\r\n  File \"C:\\Users\\User\\venv-tf-nightly\\lib\\site-packages\\tensorflow\\contrib\\lite\\python\\convert.py\", line 135, in toco_convert_protos\r\n    (stdout, stderr))\r\nRuntimeError: TOCO failed see console for info.\r\nb'Traceback (most recent call last):\\r\\n  File \"c:\\\\users\\\\user\\\\appdata\\\\local\\\\programs\\\\python\\\\python36\\\\lib\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\tensorflow_wrap_toco.py\", line 18, in swig_import_helper\\r\\n    fp, pathname, description = imp.find_module(\\'_tensorflow_wrap_toco\\', [dirname(__file__)])\\r\\n  File \"c:\\\\users\\\\user\\\\appdata\\\\local\\\\programs\\\\python\\\\python36\\\\lib\\\\imp.py\", line 297, in find_module\\r\\n    raise ImportError(_ERR_MSG.format(name), name=name)\\r\\nImportError: No module named \\'_tensorflow_wrap_toco\\'\\r\\n\\r\\nDuring handling of the above exception, another exception occurred:\\r\\n\\r\\nTraceback (most recent call last):\\r\\n  File \"c:\\\\users\\\\user\\\\appdata\\\\local\\\\programs\\\\python\\\\python36\\\\lib\\\\runpy.py\", line 193, in _run_module_as_main\\r\\n    \"__main__\", mod_spec)\\r\\n  File \"c:\\\\users\\\\user\\\\appdata\\\\local\\\\programs\\\\python\\\\python36\\\\lib\\\\runpy.py\", line 85, in _run_code\\r\\n    exec(code, run_globals)\\r\\n  File \"C:\\\\Users\\\\User\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python36\\\\Scripts\\\\toco_from_protos.exe\\\\__main__.py\", line 5, in <module>\\r\\n  File \"c:\\\\users\\\\user\\\\appdata\\\\local\\\\programs\\\\python\\\\python36\\\\lib\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\toco_from_protos.py\", line 22, in <module>\\r\\n    from tensorflow.contrib.lite.toco.python import tensorflow_wrap_toco\\r\\n  File \"c:\\\\users\\\\user\\\\appdata\\\\local\\\\programs\\\\python\\\\python36\\\\lib\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\tensorflow_wrap_toco.py\", line 28, in <module>\\r\\n    _tensorflow_wrap_toco = swig_import_helper()\\r\\n  File \"c:\\\\users\\\\user\\\\appdata\\\\local\\\\programs\\\\python\\\\python36\\\\lib\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\tensorflow_wrap_toco.py\", line 20, in swig_import_helper\\r\\n    import _tensorflow_wrap_toco\\r\\nModuleNotFoundError: No module named \\'_tensorflow_wrap_toco\\'\\r\\n'\r\nNone\r\n\r\n\r\nProcess finished with exit code 1\r\n```\r\n", "comments": ["i have the same problem . \r\n`Traceback (most recent call last):\r\n  File \"quant.py\", line 17, in <module>\r\n    mobilenet_tflite_file.write_bytes(converter.convert())\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/lite.py\", line 464, in convert\r\n    **converter_kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/convert.py\", line 317, in toco_convert_graph_def\r\n    input_data.SerializeToString())\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/convert.py\", line 135, in toco_convert_protos\r\n    (stdout, stderr))\r\nRuntimeError: TOCO failed see console for info.\r\nb'2018-10-12 13:08:32.732548: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1109] Converting unsupported operation: TFLite_Detection_PostProcess\\n2018-10-12 13:08:32.738803: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1182] Unable to determine output type for op: TFLite_Detection_PostProcess\\n2018-10-12 13:08:32.783094: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 900 operators, 1352 arrays (0 quantized)\\n2018-10-12 13:08:32.844203: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 900 operators, 1352 arrays (0 quantized)\\n2018-10-12 13:08:33.944841: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 111 operators, 220 arrays (0 quantized)\\n2018-10-12 13:08:33.948440: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 111 operators, 220 arrays (0 quantized)\\n2018-10-12 13:08:33.953645: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 11520000 bytes, theoretical optimal value: 11520000 bytes.\\n2018-10-12 13:08:33.954096: I tensorflow/contrib/lite/toco/toco_tooling.cc:397] Estimated count of arithmetic ops: 2.49483 billion (note that a multiply-add is counted as 2 ops).\\n2018-10-12 13:08:33.954530: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954549: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954557: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_pointwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954565: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954572: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_pointwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954579: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954588: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_pointwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954597: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954606: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_pointwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954615: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954624: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_pointwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954633: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954641: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_pointwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954650: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954659: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_pointwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954668: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954676: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_pointwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954685: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954694: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_pointwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954703: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954712: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_pointwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954721: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954730: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_pointwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954739: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954747: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_pointwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954756: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954765: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_pointwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954774: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_256/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954783: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_512/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954792: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_128/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954801: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_256/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954810: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_128/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954819: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_256/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954828: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_64/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954836: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_128/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954845: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_0/BoxEncodingPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954854: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_0/ClassPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954863: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_1/BoxEncodingPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954872: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_1/ClassPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954881: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_2/BoxEncodingPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954890: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_2/ClassPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954899: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_3/BoxEncodingPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954906: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_3/ClassPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954913: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_4/BoxEncodingPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954921: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_4/ClassPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954930: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_5/BoxEncodingPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954939: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_5/ClassPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954950: F tensorflow/contrib/lite/toco/tflite/export.cc:460] Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.contrib.lite.TFLiteConverter(). Here is a list of operators for which  you will need custom implementations: TFLite_Detection_PostProcess.\\nAborted (core dumped)\\n'\r\nNone\r\n`", "Any word on this?", "We're still looking into this. We've just fixed the last outstanding bug blocking use of the Python conversion APIs on Windows, and will try to get this into the nightly pip for Windows soon.", "Same problem here.\r\nI got *.pb by this cmd:\r\n`python3 export_tflite_ssd_graph.py --pipeline_config_path=training/ssd_mobilenet_v1_coco.config --trained_checkpoint_prefix=RESULTS/model.ckpt-191 --output_directory=inference_graph`\r\nand I want to get *.tflite file by this cmd:\r\n`tflite_convert --output_file=./ssd_mobilenet_v1_coco.tflite --graph_def_file=./tflite_graph.pb --input_shape=0,0,0,3 --input_array=image_tensor --output_array=detection_boxes,detection_scores,detection_classes,num_detections`\r\nbut get this error:\r\n`Traceback (most recent call last):\r\n  File \"/home/baniel/MyCode/tfenv/tensorflow1.5-withpy3/bin/tflite_convert\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/home/baniel/MyCode/tfenv/tensorflow1.5-withpy3/lib/python3.5/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 401, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/home/baniel/MyCode/tfenv/tensorflow1.5-withpy3/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/home/baniel/MyCode/tfenv/tensorflow1.5-withpy3/lib/python3.5/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 397, in run_main\r\n    _convert_model(tflite_flags)\r\n  File \"/home/baniel/MyCode/tfenv/tensorflow1.5-withpy3/lib/python3.5/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 159, in _convert_model\r\n    output_data = converter.convert()\r\n  File \"/home/baniel/MyCode/tfenv/tensorflow1.5-withpy3/lib/python3.5/site-packages/tensorflow/contrib/lite/python/lite.py\", line 445, in convert\r\n    **converter_kwargs)\r\n  File \"/home/baniel/MyCode/tfenv/tensorflow1.5-withpy3/lib/python3.5/site-packages/tensorflow/contrib/lite/python/convert.py\", line 277, in toco_convert_graph_def\r\n    input_data.SerializeToString())\r\n  File \"/home/baniel/MyCode/tfenv/tensorflow1.5-withpy3/lib/python3.5/site-packages/tensorflow/contrib/lite/python/convert.py\", line 109, in toco_convert_protos\r\n    (stdout, stderr))\r\nRuntimeError: TOCO failed see console for info.\r\nb'2018-10-23 18:43:22.707313: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1055] Converting unsupported operation: TFLite_Detection_PostProcess\\n2018-10-23 18:43:22.707397: F tensorflow/contrib/lite/toco/tooling_util.cc:617] Check failed: dim >= 1 (0 vs. 1)\\nAborted\\n'\r\nNone`\r\nAnyone could help me?", "@CreateChance and @cjr0106, please file a separate bug. The issue you're seeing is unrelated to the originally reported issue, which is related to TFLite conversion on Windows.", "@gargn, @aselle: The converter should now compile/run successfully on Windows, anything else we need to do to include it with the Windows pip?", "In which version did you fix it exactly? because I just upgraded my current tf-nightly installation to 1.13.0.def20181107 and it still gives me the same error.", "Can you please point to the version that fix this issue?", "The version in tf-nightly does work ( I verified it ). \r\nMany people on this thread are saying \"they have the same problem\" as the original issue. This is not the case as @notibus' original issue was that the converter was not running at all maybe... but it is hard to tell because it is not a reproducible test case since I do not have your pb file. Try my example below.\r\n\r\n@cjr0106's issue was that the model is not supported or the arguments are wrong (but @cjr0106 would need to post what they ran exactly and what model for us to help).\r\n\r\n@CreateChance's issue has to do with an improper input shape because you are using a wrong command line\r\n*--input_shape=0,0,0,3*\r\nShape 0 is not valid. Use a real shape.\r\n\r\n@RenanSch, tf-nightly has the fix.\r\n\r\nIf you want to verify you have a working Windows converter and python interpreter path run the below python script. It makes a simple tensorflow model, converts it to tflite and runs it in tflite It should produce a tensor of all 2's\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n# Make a simple model\r\na = tf.placeholder(shape=(1,4,4,3), dtype=tf.float32)\r\nc = a+a\r\n\r\n# Convert the model to tflite (and write a tflite file for fun)\r\nsess=tf.Session()\r\nlite = tf.contrib.lite.TFLiteConverter.from_session(sess, [a],[c]).convert()\r\nwith open(\"simple_model.tflite\",\"wb\") as fp:\r\n  fp.write(lite)\r\n\r\n# Now Run the model\r\nwoot = tf.contrib.lite.Interpreter(model_path=\"simple_model.tflite\")\r\ninput = woot.get_input_details()[0]\r\nwoot.allocate_tensors()\r\nwoot.set_tensor(input[\"index\"], np.ones(dtype=np.float32,shape=(1,4,4,3)))\r\nwoot.invoke()\r\nprint(woot.get_tensor(woot.get_output_details()[0][\"index\"]))\r\n\r\n```\r\n", "The last iteration of tf-nightly indeed fixes my problem! Interestingly running my code from inside PyCharm results in the same error as before although I'm using the same interpreter of my virtual environment. However, running the script from the terminal works.", "Nagging Assignees @aselle, @gargn: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@aselle script did work but that means there's something wrong in my command,\r\ni did not build toco from source i'm using it directly from the anaconda tensorflow-gpu 1.13.1 package\r\n\r\nmy command:\r\n`toco --input_file=tiny-yolo-1c.pb --output_file=foo.tflite --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --input_arrays=input --input_shapes=1,416,416,3 --output_arrays=output`\r\n\r\nkeeps saying:\r\n`toco: error: one of the arguments --graph_def_file --saved_model_dir --keras_model_file is required`", "@iseegr8tfuldeadppl Please file a new issue with all of the details required to reproduce this issue.", "@iseegr8tfuldeadppl Change the --input_file to --graph_def_file in your command.", "Thanx @devimonica it worked on windows with the folliwing command\r\n\r\n--graph_def_file=<mylocation>/retrained_graph.pb --output_file=<location>/optimized_graph.tflite --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --input_shape=1,224,224,3 --input_array=input --output_array=final_result --inference_type=FLOAT --input_data_type=FLOAT"]}, {"number": 22896, "title": "README Updates", "body": "Clarification of code examples for fast copy-paste replication. (I did run the code myself and encountered those little missing elements)\r\n- Estimator declaration placed after config declaration\r\n- loss_fn() incoherent with previous loss variable declaration", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "@alelouis Hi, would you like to sign CLA ? If not, feel free to close this request.", "That\u2019s done :) Thank you.", "CLAs look good, thanks!\n\n<!-- ok -->", "@yuefengz  - Hi, could you please look into the kokoro failures ? Tried multiple times but no luck.", "Nagging Assignee @harshini-gadige: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 22895, "title": "Fix typo in estimator with distribution strategy using hooks", "body": "Move #22867 against master branch\r\nFix typo while extracting estimator training hooks using distribution strategy.\r\nCC @yuefengz", "comments": ["Close this PR because estimators got moved to separate repo in master: https://github.com/tensorflow/estimator", "Reopen since rollback of \"estimator move\" commit happened", "Thanks for fixing @nvcastet . Looks like there is another PR for this on the new repo: https://github.com/tensorflow/estimator/pull/3#pullrequestreview-168946557\r\nWe will get that merged. "]}, {"number": 22893, "title": "Strange issue converting Tensorflow model to TFLite using toco", "body": "\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.5 LTS\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: Tensorflow v1.9.0-rc2-5420-g411b9ba 1.11.0-rc1 \r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nI have a tensorflow model, in two formats, the first in a frozen graph definition (pb) file, and the second in a Keras model (h5) file.\r\n\r\nVia pb file:\r\n```\r\ntoco --graph_def_file=./model.pb --output_file=./model.tflite --inference_type=FLOAT --input_type=FLOAT --input_arrays=input_1 --output_arrays=out0 --allow_custom_operators\r\n```\r\nYields:\r\n```\r\n2018-10-09 12:24:55.039391: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 5705 operators, 7732 arrays (0 quantized)\r\n2018-10-09 12:24:55.664965: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 5705 operators, 7732 arrays (0 quantized)\r\n2018-10-09 12:24:56.867861: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 4908 operators, 9078 arrays (0 quantized)\r\n2018-10-09 12:24:57.400037: F tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:1442] Check failed: axis < input_shape.dimensions_count() (90746776 vs. 4)\r\nAborted (core dumped)'\r\nNone\r\n```\r\nInterestingly the 90746776 (axis) value in the error changes between successive calls\r\n\r\nVia the h5 file\r\n```\r\ntoco --keras_model_file=./model.h5 --output_file=./model.tflite --inference_type=FLOAT --input_type=FLOAT --input_arrays=input_1 --output_arrays=out0 --allow_custom_ops\r\n```\r\nYields:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/joseph/.local/bin/toco\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/home/joseph/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 412, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/home/joseph/.local/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/home/joseph/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 408, in run_main\r\n    _convert_model(tflite_flags)\r\n  File \"/home/joseph/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 100, in _convert_model\r\n    converter = _get_toco_converter(flags)\r\n  File \"/home/joseph/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 87, in _get_toco_converter\r\n    return converter_fn(**converter_kwargs)\r\n  File \"/home/joseph/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/lite.py\", line 368, in from_keras_model_file\r\n    keras_model = _keras.models.load_model(model_file)\r\n  File \"/home/joseph/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/saving.py\", line 230, in load_model\r\n    model = model_from_config(model_config, custom_objects=custom_objects)\r\n  File \"/home/joseph/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/saving.py\", line 310, in model_from_config\r\n    return deserialize(config, custom_objects=custom_objects)\r\n  File \"/home/joseph/.local/lib/python3.5/site-packages/tensorflow/python/keras/layers/serialization.py\", line 64, in deserialize\r\n    printable_module_name='layer')\r\n  File \"/home/joseph/.local/lib/python3.5/site-packages/tensorflow/python/keras/utils/generic_utils.py\", line 173, in deserialize_keras_object\r\n    list(custom_objects.items())))\r\n  File \"/home/joseph/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/network.py\", line 1298, in from_config\r\n    process_layer(layer_data)\r\n  File \"/home/joseph/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/network.py\", line 1284, in process_layer\r\n    layer = deserialize_layer(layer_data, custom_objects=custom_objects)\r\n  File \"/home/joseph/.local/lib/python3.5/site-packages/tensorflow/python/keras/layers/serialization.py\", line 64, in deserialize\r\n    printable_module_name='layer')\r\n  File \"/home/joseph/.local/lib/python3.5/site-packages/tensorflow/python/keras/utils/generic_utils.py\", line 173, in deserialize_keras_object\r\n    list(custom_objects.items())))\r\n  File \"/home/joseph/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/sequential.py\", line 340, in from_config\r\n    model.add(layer)\r\n  File \"/home/joseph/.local/lib/python3.5/site-packages/tensorflow/python/training/checkpointable/base.py\", line 474, in _method_wrapper\r\n    method(self, *args, **kwargs)\r\n  File \"/home/joseph/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/sequential.py\", line 175, in add\r\n    output_tensor = layer(self.outputs[0])\r\n  File \"/home/joseph/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 746, in __call__\r\n    self.build(input_shapes)\r\n  File \"/home/joseph/.local/lib/python3.5/site-packages/tensorflow/python/keras/layers/wrappers.py\", line 213, in build\r\n    self.layer.build(tuple(child_input_shape))\r\n  File \"/home/joseph/.local/lib/python3.5/site-packages/tensorflow/python/keras/layers/core.py\", line 933, in build\r\n    raise ValueError('The last dimension of the inputs to `Dense` '\r\nValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`.\r\n```", "comments": ["Please go through the documentation [to convert tf model into tensorflow lite](https://www.tensorflow.org/lite/convert/python_api#exporting_a_tfkeras_file_)  and [additional instructions](https://www.tensorflow.org/lite/convert/python_api#additional_instructions) if you haven't already. \r\n\r\nIn addition, can you please try tf-nightly and check if the issue still persists?", "Both links provided are broken", "I have updated the links now. Please fire them up.", "Thanks for your fast reply. I've pulled the latest revision from tensorflow master, built a new python pip package from source using [these instructions](https://www.tensorflow.org/install/source) and installed via pip using the built `whl` file. Still the same issue when trying:\r\n```\r\nimport tensorflow as tf\r\n\r\n\r\nconverter = tf.contrib.lite.TFLiteConverter.from_keras_model_file(\"model.h5\")\r\ntflite_model = converter.convert()\r\nopen(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n```\r\nResults in:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"convert.py\", line 4, in <module>\r\n    converter = tf.contrib.lite.TFLiteConverter.from_keras_model_file(\"model.h5\")\r\n  File \"/home/joseph/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/lite.py\", line 368, in from_keras_model_file\r\n    keras_model = _keras.models.load_model(model_file)\r\n  File \"/home/joseph/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/saving.py\", line 230, in load_model\r\n    model = model_from_config(model_config, custom_objects=custom_objects)\r\n  File \"/home/joseph/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/saving.py\", line 310, in model_from_config\r\n    return deserialize(config, custom_objects=custom_objects)\r\n  File \"/home/joseph/.local/lib/python3.5/site-packages/tensorflow/python/keras/layers/serialization.py\", line 64, in deserialize\r\n    printable_module_name='layer')\r\n  File \"/home/joseph/.local/lib/python3.5/site-packages/tensorflow/python/keras/utils/generic_utils.py\", line 173, in deserialize_keras_object\r\n    list(custom_objects.items())))\r\n  File \"/home/joseph/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/network.py\", line 1319, in from_config\r\n    process_layer(layer_data)\r\n  File \"/home/joseph/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/network.py\", line 1305, in process_layer\r\n    layer = deserialize_layer(layer_data, custom_objects=custom_objects)\r\n  File \"/home/joseph/.local/lib/python3.5/site-packages/tensorflow/python/keras/layers/serialization.py\", line 64, in deserialize\r\n    printable_module_name='layer')\r\n  File \"/home/joseph/.local/lib/python3.5/site-packages/tensorflow/python/keras/utils/generic_utils.py\", line 173, in deserialize_keras_object\r\n    list(custom_objects.items())))\r\n  File \"/home/joseph/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/sequential.py\", line 340, in from_config\r\n    model.add(layer)\r\n  File \"/home/joseph/.local/lib/python3.5/site-packages/tensorflow/python/training/checkpointable/base.py\", line 474, in _method_wrapper\r\n    method(self, *args, **kwargs)\r\n  File \"/home/joseph/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/sequential.py\", line 175, in add\r\n    output_tensor = layer(self.outputs[0])\r\n  File \"/home/joseph/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 746, in __call__\r\n    self.build(input_shapes)\r\n  File \"/home/joseph/.local/lib/python3.5/site-packages/tensorflow/python/keras/layers/wrappers.py\", line 214, in build\r\n    self.layer.build(tuple(child_input_shape))\r\n  File \"/home/joseph/.local/lib/python3.5/site-packages/tensorflow/python/keras/layers/core.py\", line 933, in build\r\n    raise ValueError('The last dimension of the inputs to `Dense` '\r\nValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`.\r\n```", "Nagging Assignee @gargn: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Sorry for posting on a closed issue.\r\n\r\n@josephn123 , I am having the exact issue, just wondering how did you solved this? Thank ", "This error still appears when converting any Keras model which has custom layer\r\nOn TF 1.13", "> \r\n> \r\n> This error still appears when converting any Keras model which has custom layer\r\n> On TF 1.13\r\n\r\nThe same issue, did you find a solution?", "same problem with the first in a frozen graph definition (pb) file, did you find a solution? Thankyou ", "Starting 1.14, [`TFLiteConverter.from_keras_model_file`](https://www.tensorflow.org/api_docs/python/tf/lite/TFLiteConverter#from_keras_model_file) takes in the argument `custom_objects` which is passed directly into the Keras loading function. The logic should look something like the following:\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_keras_model_file(\r\n   keras_model_file,\r\n   custom_objects={'Dense': Dense})\r\n```\r\n\r\nIf you need to use 1.13 then you can load the Keras model yourself and use `TFLiteConverter.from_session` with logic similar to the following:\r\n\r\n```\r\ntf.keras.backend.set_learning_phase(False)\r\nkeras_model = tf.keras.models.load_model(model_file, custom_objects)\r\nsess = tf.keras.backend.get_session()\r\nconverter = tf.lite.TFLiteConverter.from_session(sess, keras_model.inputs, keras_model.outputs)\r\n```"]}, {"number": 22892, "title": "How to parse the caffe-generated LMDB datasets with tf.LMDBReader", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nPC\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary\r\n- **TensorFlow version (use command below)**:\r\n1.9.0\r\n- **Python version**:\r\n2.7.15\r\n- **Bazel version (if compiling from source)**:\r\nNot compiled from source\r\n- **GCC/Compiler version (if compiling from source)**:\r\nNot compiled from source\r\n- **CUDA/cuDNN version**:\r\nCUDA 9.0\r\n- **GPU model and memory**:\r\nGTX 1060 6G\r\n\r\n### Describe the problem\r\nAfter importation through the tf.LMDBReader(), we are able to get the key and value of the .mdb. But we cannot further parse the value through the tf.parse_single_example API as no sub-keys like \"image_raw\" or \"label\" existed in the Caffe-generated .mdb dataset. As in Caffe, the properties of the image or the label data are available with this data block converted to Caffe-defined Datum structure and we can get the width through methods like Datum.width(). So what we should do in tensorflow?\r\n\r\n### Source code / logs\r\nfilename_queue = tf.train.string_input_producer(['path_to_the_data.mdb'], num_epochs=None)\r\nreader = tf.LMDBReader()\r\ncur_key_val, serialized_example = reader.read(filename_queue)\r\nfeatures = tf.parse_single_example(\r\n        serialized_example,\r\n        features={\r\n             '?????': tf.VarLenFeature(tf.string),\r\n             '?????': tf.VarLenFeature(tf.string)\r\n         })\r\nlabel = features['?????']\r\nimage = features['?????']", "comments": ["No you can not parse caffe LMDB with `tf.LMDBReader`.\r\nIn fact, using caffe's lmdb file is a bad idea because how inefficient it is: it stores image as arrays instead of jpegs, and can make data loading sometimes [many times slower](https://github.com/tensorpack/tensorpack/issues/124#issuecomment-277206505)", "> No you can not parse caffe LMDB with `tf.LMDBReader`.\r\n> In fact, using caffe's lmdb file is a bad idea because how inefficient it is: it stores image as arrays instead of jpegs, and can make data loading sometimes [many times slower](https://github.com/tensorpack/tensorpack/issues/124#issuecomment-277206505)\r\n\r\nThank you for your response and we have turned to the TFRecord method to feed the packed data in terms of string instead. \r\n", "@asjmasjm Hi, looks like your question is answered. Feel free to close this issue if you do not have any outstanding queries related to this. Thank you !", "Thanks @harshini-gadige and close this topic. "]}, {"number": 22891, "title": "Cannot obtain the output feature map value of certain tensor in tensorflow-lite", "body": "Hello, I am analysing each layer of mobilenet-ssd in tensorflow-lite. I am using python interpreter.\r\nhere is the sample of python script for accessing certain tensor of detect.tflite.\r\n\r\n```\r\ninterpreter = interpreter_wrapper.Interpreter(model_path=resource_loader.get_path_to_datafile(\"detect.tflite\"))\r\ninterpreter.allocate_tensors()\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\ninterpreter.set_tensor(input_details[0]['index'], input_data)\r\ninterpreter.invoke()\r\ntensorIndex  = 173\r\ntmpTensor = interpreter._get_tensor_details(tensorIndex)\r\nvalue = interpreter.get_tensor(tensorIndex)\r\nvalueShape = value.shape\r\ntensorName = interpreter._get_tensor_details(tensorIndex)\r\nLayerNames = tensorName['name']\r\nplt.imshow(value)\r\nplt.show()\r\n```\r\nin this way I want to check the output feature map of certain tensor. However, the output feature map seems show trash values all the time.\r\n\r\n> Note: given python script can show the image_tensor (input image) and cannot obtain any detection results\r\n\r\nHow to check the other output feature map of certain tensor in tensorflow lite?\r\n\r\nThank you.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "detect.tflite model works on some image and does not work on some image.\r\nimage preprocessing is done by subtracting mean values and scale value=1.\r\nwhat is possible hypothesis for this problem?", "Closing due to stale history, feel free to re-open if this is still an issue.", "Marking issue as resolved due to inactivity. Feel free to re-open this if it's unresolved or file a [new issue](https://github.com/tensorflow/tensorflow/issues/new/choose)"]}, {"number": 22890, "title": "Failed to load the native TensorFlow runtime.", "body": "please help me\r\n\r\npygame 1.9.4\r\nHello from the pygame community. https://www.pygame.org/contribute.html\r\n2018-10-11 18:04:48.044220: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n2018-10-11 18:04:48.293316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties:\r\nname: GeForce GTX 1060 major: 6 minor: 1 memoryClockRate(GHz): 1.6705\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 6.00GiB freeMemory: 4.97GiB\r\n2018-10-11 18:04:48.305478: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0\r\n2018-10-11 18:04:49.064648: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-10-11 18:04:49.075159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0\r\n2018-10-11 18:04:49.079229: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N\r\n2018-10-11 18:04:49.084288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4699 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2018-10-11 18:05:04.242432: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.59G (4927577088 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nWARNING:tensorflow:From D:\\Project\\practice_pysc2\\RDRL\\rl\\model.py:63: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nkeep_dims is deprecated, use keepdims instead\r\nW1011 18:05:41.685944 12328 tf_logging.py:125] From D:\\Project\\practice_pysc2\\RDRL\\rl\\model.py:63: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nkeep_dims is deprecated, use keepdims instead\r\nTraceback (most recent call last):\r\n  File \"D:\\Engineer\\Anaconda\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"D:\\Engineer\\Anaconda\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"D:\\Engineer\\Anaconda\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"D:\\Engineer\\Anaconda\\Anaconda3\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"D:\\Engineer\\Anaconda\\Anaconda3\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: \u9875\u9762\u6587\u4ef6\u592a\u5c0f\uff0c\u65e0\u6cd5\u5b8c\u6210\u64cd\u4f5c\u3002\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"D:\\Engineer\\Anaconda\\Anaconda3\\lib\\multiprocessing\\spawn.py\", line 105, in spawn_main\r\n    exitcode = _main(fd)\r\n  File \"D:\\Engineer\\Anaconda\\Anaconda3\\lib\\multiprocessing\\spawn.py\", line 114, in _main\r\n    prepare(preparation_data)\r\n  File \"D:\\Engineer\\Anaconda\\Anaconda3\\lib\\multiprocessing\\spawn.py\", line 225, in prepare\r\n    _fixup_main_from_path(data['init_main_from_path'])\r\n  File \"D:\\Engineer\\Anaconda\\Anaconda3\\lib\\multiprocessing\\spawn.py\", line 277, in _fixup_main_from_path\r\n    run_name=\"__mp_main__\")\r\n  File \"D:\\Engineer\\Anaconda\\Anaconda3\\lib\\runpy.py\", line 263, in run_path\r\n    pkg_name=pkg_name, script_name=fname)\r\n  File \"D:\\Engineer\\Anaconda\\Anaconda3\\lib\\runpy.py\", line 96, in _run_module_code\r\n    mod_name, mod_spec, pkg_name, script_name)\r\n  File \"D:\\Engineer\\Anaconda\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"D:\\Project\\practice_pysc2\\RDRL\\main.py\", line 3, in <module>\r\n    import tensorflow as tf\r\n  File \"D:\\Engineer\\Anaconda\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"D:\\Engineer\\Anaconda\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"D:\\Engineer\\Anaconda\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"D:\\Engineer\\Anaconda\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"D:\\Engineer\\Anaconda\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"D:\\Engineer\\Anaconda\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"D:\\Engineer\\Anaconda\\Anaconda3\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"D:\\Engineer\\Anaconda\\Anaconda3\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: \u9875\u9762\u6587\u4ef6\u592a\u5c0f\uff0c\u65e0\u6cd5\u5b8c\u6210\u64cd\u4f5c\u3002\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n2018-10-11 18:05:48.218204: W tensorflow/core/framework/op_kernel.cc:1275] OP_REQUIRES failed at constant_op.cc:207 : Resource exhausted: OOM when allocating tensor with shape[76800,256] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "> \r\n> \r\n> Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\r\n> Have I written custom code\r\n> OS Platform and Distribution\r\n> TensorFlow installed from\r\n> TensorFlow version\r\n> Bazel version\r\n> CUDA/cuDNN version\r\n> GPU model and memory\r\n> Exact command to reproduce\r\n> Mobile device\r\n\r\nThanks for your comments\r\nim sorry for the late reply\r\n\r\n> Have I written custom code\r\n I think I did not write custom code\r\n\r\n> OS Platform and Distribution\r\nwin10\r\n\r\n> TensorFlow installed from\r\nI installed all package by Anaconda3 include Tensorflow\r\n\r\n> TensorFlow version\r\n1.10.0\r\n\r\n> Bazel version\r\nnot installed\r\n\r\n> CUDA/cuDNN version\r\nCUDA 9.0    cuDNN 7.1.4\r\n\r\n> GPU model and memory\r\nNvidia Geforce GTX 1060   6GB\r\n\r\n> Exact command to reproduce\r\nI tried to running code from https://github.com/inoryy/pysc2-rl-agent\r\n\r\n", "Nagging Assignee @ymodak: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@Jonarain I apologize for my delayed response. Is this still an issue for you? From the snap you have provided, its complaining about running low on GPU memory for your computation. To free some memory, Can you try by aborting all python processes which require GPU computation and execute the script in a new terminal window?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 22889, "title": "train with multi-gpu with MirroredStrategy will hang-up", "body": "### System information\r\nHave I written custom code: N/A\r\nOS Platform and Distribution: CentOS Linux release 7.3.1611\r\nTensorFlow installed from:  (pip install tf-nightly-gpu)\r\nTensorFlow version: Tensorflow('v1.9.0-rc2-5345-g57d31aa599', '1.12.0-dev20181005')\r\nBazel version: N/A\r\nGPU model and memory: Tesla P40 24G\r\nExact command to reproduce: N/A\r\nMobile device: N/A\r\nCUDA/cuDNN version: cuda 9.0 with cudnn7.1.4\r\n\r\nI train with tensorflow for multi-gpu with MirroredStrategy and estimator. I got the problem:\r\nwhen I set the distribute mode with the following code it will got stuck after runing some training steps:\r\n```\r\ndistribution = tf.contrib.distribute.MirroredStrategy()\r\nconfig = tf.estimator.RunConfig(train_distribute=distribution)\r\nestimator = tf.estimator.Estimator(model_fn=mymodel_fn, model_dir='logs',\r\n        config=config)\r\n```\r\nbug when I run without distribute mode like this:\r\n```\r\ndistribution = tf.contrib.distribute.MirroredStrategy()\r\nconfig = tf.estimator.RunConfig()\r\nestimator = tf.estimator.Estimator(model_fn=mymodel_fn, model_dir='logs',\r\n        config=config)\r\n```\r\nIt runs ok. Why? \r\nIs that a bug of MirroredStrategy?\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nGPU model and memory\nExact command to reproduce\nMobile device", "OK, question updated", "have same problem. Stuck after variable initialization.\r\n\r\n\r\n\r\nHave I written custom code: N/A\r\nOS Platform and Distribution: Ubuntu 18.04\r\nTensorFlow installed from: (pip install tensorflow-gpu)\r\nTensorFlow version: 1.11.0\r\nBazel version: N/A\r\nGPU model and memory: 1080ti\r\nExact command to reproduce: see code below\r\nMobile device: N/A\r\nCUDA/cuDNN version: cuda 10.0 with cudnn7.3.1.20\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\ndef model_fn(features, labels, mode):\r\n  layer = tf.layers.Dense(1)\r\n  logits = layer(features)\r\n\r\n  if mode == tf.estimator.ModeKeys.PREDICT:\r\n    predictions = {\"logits\": logits}\r\n    return tf.estimator.EstimatorSpec(mode, predictions=predictions)\r\n\r\n  loss = tf.losses.mean_squared_error(\r\n      labels=labels, predictions=tf.reshape(logits, []))\r\n\r\n  if mode == tf.estimator.ModeKeys.EVAL:\r\n    return tf.estimator.EstimatorSpec(mode, loss=loss)\r\n\r\n  if mode == tf.estimator.ModeKeys.TRAIN:\r\n    train_op = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\r\n    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\r\n\r\ndef input_fn():\r\n  features = tf.data.Dataset.from_tensors([[1.]]).repeat(100)\r\n  labels = tf.data.Dataset.from_tensors(1.).repeat(100)\r\n  return tf.data.Dataset.zip((features, labels))\r\n\r\n\r\ndistribution = tf.contrib.distribute.MirroredStrategy()\r\nconfig = tf.estimator.RunConfig(train_distribute=distribution)\r\nclassifier = tf.estimator.Estimator(model_fn=model_fn, config=config)\r\nclassifier.train(input_fn=input_fn)\r\nclassifier.evaluate(input_fn=input_fn)\r\n```\r\n\r\n\r\n\r\noutput:\r\n\r\n```\r\nINFO:tensorflow:Initializing RunConfig with distribution strategies.\r\nINFO:tensorflow:Not using Distribute Coordinator.\r\nWARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpcqwt3jg0\r\nINFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpcqwt3jg0', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\r\ngraph_options {\r\n  rewrite_options {\r\n    meta_optimizer_iterations: ONE\r\n  }\r\n}\r\n, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': <tensorflow.contrib.distribute.python.mirrored_strategy.MirroredStrategy object at 0x7fe0e5733e80>, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe0e5733f98>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_distribute_coordinator_mode': None}\r\nINFO:tensorflow:Device is available but not used by distribute strategy: /device:CPU:0\r\nINFO:tensorflow:Configured nccl all-reduce.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:batch_all_reduce invoked for batches size = 2 with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Graph was finalized.\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpcqwt3jg0/model.ckpt.\r\nINFO:tensorflow:Initialize system\r\n```", "With TF 1.12 I still have the issue that @magnofel encountered (only difference since TF 1.11 is that it freezes before displaying `INFO:tensorflow:Initialize system`). \r\n@seemuch do you have any update on this ? Thanks a lot.", "Same problem as @magnofel\r\nr1.11 (1.12 has broken distributed training)\r\nbuilt from source\r\nnccl 2.3.5\r\n@seemuch how can we debug this problem?", "@gunan maby you can take a look at this? Looks like the problem is at the step result aggregation. I can share all logs that needed to fix this.", "Haven't had bandwidth to look at this yet. \r\nWill take a look within this week. ", "@seemuch i have resolved my issue and it can possibly help others here (@jnd77 @magnofel @honeytidy )\r\nIf you use AMD Treadripper and motherboard without PLX chips then you should go to UEFI and disable IOMMU. NCCL is not compatible with it. \r\nMore you can find [here](https://github.com/NVIDIA/nccl/issues/120) and [here](https://github.com/pytorch/pytorch/issues/1637#issuecomment-338268158) ", "Thanks a lot @Luonic. With your links, we solved the issue. \r\nWe disabled IOMMU via grub as mentioned [here](https://github.com/pytorch/pytorch/issues/1637#issuecomment-338268158).", "How would you go about this if you were encountering this issue with a cloud provider? E.g. GCP or AWS. There you couldn't edit the UEFI settings I guess :thinking: ", "@patzm I think cloud providers test thier instances for compatibility (at least GCP, AWS do I think). And you can always disable it with grub config as @jnd77 did.", "This issue seems to be the most relevant to the problem I'm facing. I tried disabling NCCL P2P which should allow for training (albeit slower training) as per the issues pointed out by @Luonic. This fix doesn't seem to work. I don't have access to disable IOMMU, I was wondering if there is any other troubleshooting steps or workarounds for this issue?\r\n\r\nFor extra information, I did build from source (r1.96) using NCCL 1.3 and NCCL 2.3.5. Attempting to train on 2-4 NVIDIA P100's. I thought my issue might be related to #25057 but I left the system for an extended period of time and training never began.", "I also encounter the same problem.  I used dataset to read data, the MirroredStrategy job will hang on at last batch if I use \"shuffle->repeat->batch\". But when I changed to \"shuffle->batch->repeat\", the job will finished correctly.\r\n\r\ndrop_remainder in batch() can't solve the problem\r\n", "Hi @honeytidy!\r\nIt seems you are using older versions(1.x versions) of Tensorflow which is not supported any more. Please visit these links to upgrade your codebase to latest versions(2.7).Ref [1](https://www.tensorflow.org/addons),[2](https://www.tensorflow.org/guide/migrate). Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 22888, "title": "Make nccl2 bazel configuration platform independent", "body": "The new NCCL2 bzl build files that build from git source are coded to only work on a single platform (X86_64).\r\nThis PR  uses a genrule to check the system architecture and uses that when continuing on to the nvlinker.\r\n\r\nNote: This does not consider Windows at the moment because NCCL is not supported on Windows.\r\n\r\nThis fixes a build break for the ppc64le community build.", "comments": ["@wdirons @gunan "]}, {"number": 22887, "title": "[TensorRT] use op_name as op_type_name.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: None\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: centos 7.2\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: None\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.11.0\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:  0.16.1\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: cudnn 7.0.4\r\n- **GPU model and memory**: p40 & 20GB\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\n\r\n1. run tensorflow 1.11.0 with tensorrt 4.0.6.\r\n2. TRTEngineOp is registered. \r\n3. I find the tf  use `op_name` as `op_type_name`.\r\n\r\n```\r\nNot found: Op type not registered 'TitleLSTM/my_trt_op_2_native_segment' in binary\r\n```\r\nI think  'TitleLSTM/my_trt_op_2_native_segment' is a op_name, this op's type is 'TRTEngineOp'\r\n\r\n### Source code / logs\r\n2018-09-30 17:23:37.482200: I external/org_tensorflow/tensorflow/core/framework/op.cc:103] Not found: Op type not registered 'TitleLSTM/my_trt_op_2_native_segment' in binary running on jshd_40_73. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.", "comments": ["@qiaohaijun step 3 is not clear to me. Can you describe what are you trying to do in a bit more detail?", "@samikama ok\r\n1. freeze `saved_model` to `freezed_model`\r\n2. use `create_inference_graph` to covert `freezed_model` to `freezed_trt_model`\r\n3. convert `freezed_trt_model` to `trt_saved_model` for tf serving\r\n4. tf serving load the `trt_saved_model`\r\n\r\nWhen the tensorflow serving is executed, an error occurs.\r\n```\r\nNot found: Op type not registered 'TitleLSTM/my_trt_op_2_native_segment' in binary\r\n```\r\n\r\n", "@samikama is it a bug or not?", "@qiaohaijun this is not a bug in TFTRT, it is a bug in your script that converts TFTRT model to serving model. As error mentions 'TitleLSTM/my_trt_op_2_native_segment' is not an OP but it is a function. It looks like your serving conversion script can't handle functions.", "@samikama \r\nadd more info.\r\n\r\n### my scripts\r\n```\r\ngraph_pb = './freezed_jce.pb'\r\n\r\ngraph_def=tf.GraphDef()\r\nwith tf.gfile.GFile(graph_pb, \"rb\") as f:\r\n    graph_def.ParseFromString(f.read())\r\n\r\ngpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.4)\r\n\r\ntrt_graph_def=trt.create_inference_graph(\r\n        input_graph_def = graph_def,\r\n        outputs = [\"final_score\"],\r\n        max_batch_size = 6000,\r\n        max_workspace_size_bytes=1<<32,\r\n        precision_mode = \"FP32\"\r\n        )   \r\n\r\ntf.reset_default_graph()\r\ng = tf.Graph()\r\n\r\nwith tf.Session(graph=g) as sess:\r\n    with g.as_default():\r\n        tf.import_graph_def(\r\n        graph_def=trt_graph_def,\r\n        name='')\r\n    graph_io.write_graph(g, '.', 'trt_jce_fp32_b6000.pb', as_text=False)\r\n\r\n```\r\n### conversion Error and Warning\r\n```\r\n2018-10-30 18:17:36.761273: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:418] Can't find a device placement for the op!\r\n2018-10-30 18:17:36.768941: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:418] Can't find a device placement for the op!\r\n2018-10-30 18:17:36.770592: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:418] Can't find a device placement for the op!\r\n2018-10-30 18:18:30.236226: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.\r\n2018-10-30 18:18:30.251184: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.\r\n2018-10-30 18:18:30.271306: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.\r\n2018-10-30 18:18:30.276792: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.\r\n2018-10-30 18:18:30.292489: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.\r\n2018-10-30 18:18:30.297720: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.\r\n\r\n### c++ source code\r\n```\r\n```c++\r\n184   if (item.id != \"tf_graph\") {\r\n185     LOG(WARNING) << name_\r\n186                  << \" is probably called on funcdef! This optimizer must *NOT* \"\r\n187                     \"be called on function objects.\";\r\n188     *optimized_graph = item.graph;\r\n189     return tensorflow::Status::OK();\r\n190   }\r\n\r\n```", "@samikama \r\n>  It looks like your serving conversion script can't handle functions.\r\n\r\nyes, my scripts didn't handle functions. But I want know how to fix it.", "@qiaohaijun , logs above seem correct to me. TRT optimization pass should not be run on function objects in the graph but current infrastructure runs all configured optimizers on all graph components thus TRT optimization pass is printing a warning about it. I don' t think problem is related with TFTRT or specific to conversion of a TRT optimized graph. It is more likely to be due to you conversion scripts ability to handle function objects in the graph. I expect it to fail similarly when more functional ops introduced in to the graphs. Perhaps you can try asking in TFServing forums, or you can take a look at https://github.com/NVIDIA/dl-inference-server which is similar and support TRT optimized graphs.", "@samikama ok, thank you very much."]}, {"number": 22886, "title": "ImportError: DLL load failed: The specified module could not be found.", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No this is just a simple installation and import of tensorflow\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nDesktop PC\r\n- **TensorFlow installed from (source or binary)**:\r\npip install tensorflow-gpu\r\n- **TensorFlow version (use command below)**:\r\ntensorflow-gpu 1.11.0\r\n- **Python version**:\r\npython 3.5.5\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: CUDA v10.0 / cudnn-10.0-windows10-x64-v7.3.1.20\r\n- **GPU model and memory**:\r\nGeforce 770 2GB\r\n- **Exact command to reproduce**:\r\nimport tensorflow as tf\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nWhen I import tensorflow I immediately get an error (see below). I have looked at discussions on stack overflow and seem to have the following path variables to get at the necessary DLLs including msvcp140.dll\r\n\r\nThe problem began after I upgraded my tensorflow, CUDA, and cuDNN\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nPython 3.5.5 | packaged by conda-forge | (default, Feb 13 2018, 06:15:35) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Ulric\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Ulric\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Ulric\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Ulric\\Anaconda3\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Ulric\\Anaconda3\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Ulric\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Ulric\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Ulric\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Ulric\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Ulric\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Ulric\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Ulric\\Anaconda3\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Ulric\\Anaconda3\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n", "comments": ["I was able to resolve this by rolling back my tf, keras, cuda, cuDNN versions to the following:\r\n* tensorflow-gpu == 1.4.0\r\n* keras == 2.0.8\r\n* CUDA == 8.0\r\n* cuDNN == 6.0\r\n\r\nI'm just wondering if this is expected?  Is it that my geforce 770 GPU is not supported in more recent versions of tensorflow with more recent versions of CUDA?", "@uwong CUDA 10.0 needs to be installed from source for Windows as documented [here](https://www.tensorflow.org/install/source_windows). You can still use the more recent versions of TensorFlow with the latest CUDA.", "Closing this issue, feel free to reopen if problem persists."]}, {"number": 22885, "title": "Segmentation fault running layers.conv2d", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Centos 7.5\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA\r\n- **TensorFlow installed from (source or binary)**: Anaconda\r\n- **TensorFlow version (use command below)**: 1.10.0\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: python test.py\r\n\r\n### Describe the problem\r\nI am trying to implement a CNN using tensorflow, but I started running into segmentation fault with increasing input size to layers.conv2d. In the provided simple test script, I can successfully run it if n_H=3000. But I got a segmentation fault when I set n_H=4000. In addition, if I run it without layars.conv2d by setting with_conv=False, the script runs successfully. I don't know if this is my lack of understanding with layers.conv2d or there is something more to this.\r\n\r\n### Source code / logs\r\nHere is the output when I get a segmentation fault:\r\n```\r\n$ python test.py\r\n(100, 4000, 100, 1) (100, 8)\r\n2018-10-10 11:57:23.825704: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2\r\n2018-10-10 11:57:23.827653: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\nFatal Python error: Segmentation fault\r\n\r\nThread 0x00007fea9af1a740 (most recent call first):\r\n  File \"/home/seng/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1350 in _call_tf_sessionrun\r\n  File \"/home/seng/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1263 in _run_fn\r\n  File \"/home/seng/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1278 in _do_call\r\n  File \"/home/seng/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1272 in _do_run\r\n  File \"/home/seng/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1100 in _run\r\n  File \"/home/seng/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 877 in run\r\n  File \"test.py\", line 43 in <module>\r\nSegmentation fault (core dumped)\r\n```\r\n\r\nHere is the simple test script test.py:\r\n```\r\nimport faulthandler; faulthandler.enable()\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nseed = 42\r\ntf.reset_default_graph()\r\ntf.set_random_seed(seed)\r\nnp.random.seed(seed)\r\n\r\nnum_examples = 100\r\nn_H = 4000\r\nwith_conv = True\r\n\r\n# generate training data\r\nX_train = np.random.randn(num_examples*n_H*100).reshape(num_examples, n_H, 100, 1)\r\nY_train = np.random.randn(num_examples*8).reshape(num_examples, 8)    \r\nprint(X_train.shape, Y_train.shape)\r\n\r\n# create placeholders\r\nX = tf.placeholder(tf.float32, shape=(None, n_H, 100, 1))\r\nY = tf.placeholder(tf.float32, shape=(None, 8))\r\n\r\n# build graph\r\nif (with_conv):\r\n    conv1 = tf.layers.conv2d(X, filters=64, kernel_size=[5, 5],strides = 1, padding='valid',activation = tf.nn.relu)    \r\n    pool1 = tf.layers.max_pooling2d(conv1, pool_size=[2, 2], strides=2, padding='valid')\r\nelse:\r\n    pool1 = tf.layers.max_pooling2d(X, pool_size=[2, 2], strides=2, padding='valid')    \r\npool1_flat = tf.layers.flatten(pool1)\r\ndense2 = tf.layers.dense(pool1_flat, units=256, activation=tf.nn.relu)\r\nH = tf.layers.dense(dense2, units=8, activation=tf.nn.relu)\r\n\r\n# compute cost\r\ncost = tf.reduce_mean(tf.square(Y - H))\r\n\r\n# initialize variables\r\ninit = tf.global_variables_initializer()\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n    a = sess.run(cost, feed_dict = {X: X_train, Y:Y_train})\r\n    print(a)\r\n\r\n```\r\n", "comments": ["I was able to execute your script successfully in TF 1.10 locally. So I think this issue is not due to tf.layers.conv2d function but due to your system memory configuration.", "Initially I thought I was saturating my RAM. I have 45 GB and my host has 12 CPUs. During the execution, my test script used up about 20 GB before segmentation fault. What system memory configuration could affect the execution? How would I fix my system memory configuration? Have you try increasing n_H to larger values (6000, 8000, etc)? I can also get segmentation fault by increasing the num_examples.", "Just an update. I just ran my script on an OS X host with 32 GB RAM and tensorflow version. It ran successfully!\r\n One thing I notice is there was no message stating like I had on my Centos 7 host:\r\n`2018-10-10 11:57:23.827653: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.`\r\nWhy does it segmentation fault on my host running Centos 7?", "I was able to run your script successfully on increasing the n_H value till 9000. Going beyond that landed me in segmentation fault similar to yours. The tensorflow/core/framework/allocator.cc script does the calculation of system memory usage. I think this script allocates less memory for computation for your Centos 7 host system config than the required threshold memory for your test.py to execute successfully. ", "Does this mean allocator.cc incorrectly calculated the system memory usage for Centos 7?", "No. It calculated correctly however the memory allocation was not good enough to execute test.py in your Centos 7 system configuration.", "I don't really know how to resolve my problem. I understand this may not be a tensorflow issue. But how do I increase the memory allocation? If insufficient memory is allocated, shouldn't an exception be thrown? Any help would be appreciated.", "@asimshankar  Can you please take a look at this issue and suggest how this should be addressed?", "@wschoong : Thanks for reporting. Yes, it is running out of memory, and yes it should fail gracefully instead of segfaulting.\r\n\r\nI haven't traced when, but I _think_ this has been fixed, though after 1.11.0, so the fix will only be visible in 1.12.0. I tried the same program out in 1.10.0, 1.11.0, and 1.12.0-rc0 and while I see a segmentation fault in the first two (as you guys did), I see a better error message:\r\n\r\n```\r\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[15353856,256] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\n         [[node dense/kernel/Initializer/random_uniform/RandomUniform (defined at ./test.py:29)  = RandomUniform[T=DT_INT32, _class=[\"loc:@dense/kernel/Assign\"], dtype=DT_FLOAT, seed=42, seed2=32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](dense/kernel/Initializer/random_uniform/shape)]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n```\r\n\r\nin 1.12.0-rc0.\r\n\r\nClosing this out since it has been fixed since, and we're unlikely to want to patch previous releases.\r\nFeel free to reopen if you think I'm mistaken.\r\n\r\nThanks!"]}, {"number": 22884, "title": "Weird crash when using tensorflow C++ API on Android", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n\r\nFollow the link https://medium.com/jim-fleming/loading-a-tensorflow-graph-with-the-c-api-4caaff88463f to load graph with C++ API:\r\n\r\nnamespace tf = tensorflow;\r\nInference::loadGraph( const std::vector<unsigned char>& graph ) \r\n{\r\n \r\n    tf::SessionOptions* options = new tf::SessionOptions();\r\n    tensorflow::Session* session = nullptr;\r\n    tf::Status status = tf::NewSession( *options, &session );\r\n    if ( status.ok() ) {\r\n        m_session.reset( session );\r\n    } else {\r\n        return false;\r\n    }\r\n\r\n    tf::GraphDef tensorflowGraph;\r\n\r\n    // The following graph parsing codes are copied from ReadBinaryProto in core/platform/env.cc\r\n    std::unique_ptr<::tensorflow::protobuf::io::CodedInputStream> coded_stream =\r\n            std::unique_ptr<::tensorflow::protobuf::io::CodedInputStream>(\r\n                new ::tensorflow::protobuf::io::CodedInputStream(\r\n                        (const google::protobuf::uint8*)graph.data(),\r\n                        (int)graph.size()));\r\n\r\n    if (!coded_stream.get()) {\r\n        return false;\r\n    }\r\n\r\n    // Total bytes hard limit / warning limit are set to 1GB and 512MB\r\n    // respectively.\r\n    coded_stream->SetTotalBytesLimit(1024LL << 20, 512LL << 20);\r\n\r\n    if (!tensorflowGraph.ParseFromCodedStream(coded_stream.get())) {\r\n        return false;\r\n    }\r\n\r\n    status = m_session->Create( tensorflowGraph );\r\n    if (!status.ok()) {\r\n        return false;\r\n    }\r\n\r\n#ifdef IOS\r\n    // Notes: Crash in SessionOptions destructor on Android for unknown reason.\r\n    // All of data member get corrupt on Android in destructor\r\n    // Use stack variable tf::SessionOptions options crash as well.\r\n    // The object may be deleted already by others on Android\r\n    // But no crash on iOS.\r\n    delete options;\r\n#endif\r\n\r\n    return true;\r\n}\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 18.04 or Mac OS\r\n\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nSamsung S9 or HuaWei P20. \r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nTensorflow mobile, build for Android from source with makefile, following official build steps.\r\n\r\n./tensorflow/contrib/makefile/build_all_android.sh -a armeabi-v7a -s tensorflow/contrib/makefile/sub_makefiles/android/Makefile.in -t \"libtensorflow_inference.so libtensorflow_demo.so all\"\r\n\r\n\r\n- **TensorFlow version (use command below)**:\r\nr1.10\r\n\r\n- **Python version**:\r\n3.5\r\n\r\n- **Bazel version (if compiling from source)**:\r\n1.5.0.  Not used because makefile is used\r\n\r\n- **GCC/Compiler version (if compiling from source)**:\r\nAndroid NDK r15c\r\n\r\n\r\n- **CUDA/cuDNN version**:\r\nNot enabled\r\n\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI created a set C++ APIs for iOS and Android with JNI wrapper on top of it. Not using Tensorflow Java API since I want the same C++ API shared between iOS and Android.\r\n\r\nEverything works on iOS.  \r\n\r\nAs comments mentioned in the code above, the SessionOptions can't be deleted on Android. It causes crash.  If the options object not deleted, load graph works on Android as well.  Please let me know what I did wrong. Thanks\r\n\r\n\r\n", "comments": ["Interesting,  whenever create a tensorflow::SessionOptions, it crashes in its destructor on Android.\r\nI can reproduce the crash with the following simple code.\r\nbool\r\nInference::loadGraph( const std::vector<unsigned char>& graph ) {\r\n    tensorflow::SessionOptions options;\r\n   return true;\r\n}", "Does the crash stack trace reveal anything? Does it repro with both debug and optimized builds?", "raw crash stack:\r\n\r\n2018-10-16 17:48:36.604 29325-29325/? A/DEBUG:     #05 pc 000cc505  /data/app/com.ml.example-lg00mRAO0Q31eDZwmSEdGg==/lib/arm/libml.so (_ZNSsD2Ev+60)\r\n2018-10-16 17:48:36.604 29325-29325/? A/DEBUG:     #06 pc 00091431  /data/app/com.ml.example-lg00mRAO0Q31eDZwmSEdGg==/lib/arm/libml.so (_ZN10tensorflow14SessionOptionsD2Ev+24)\r\n2018-10-16 17:48:36.604 29325-29325/? A/DEBUG:     #07 pc 00091453  /data/app/com.ml.example-lg00mRAO0Q31eDZwmSEdGg==/lib/arm/libml.so (_ZNKSt14default_deleteIN10tensorflow14SessionOptionsEEclEPS1_+18)\r\n2018-10-16 17:48:36.604 29325-29325/? A/DEBUG:     #08 pc 00090583  /data/app/com.ml.example-lg00mRAO0Q31eDZwmSEdGg==/lib/arm/libml.so (_ZNSt10unique_ptrIN10te\r\n\r\nthe decoded crash stack:\r\n\r\ntermediates/cmake/debug/obj/armeabi-v7a/libml.so 000cc505\r\n__gnu_cxx::new_allocator<char>::deallocate(char*, unsigned int)\r\n/usr/local/google/buildbot/src/android/ndk-r15-release/out/build/tmp/build-170085/build-gnustl/static-armeabi-v7aj4.9/build/include/ext/new_allocator.h:116\r\n\r\n==== \r\n\r\nIt seems crash when SessionOptions's target get destructed.\r\n\r\n", "BTW, I was trying to build libtensorflow_cc.so, but I can't build it. Please see the last comment in https://github.com/tensorflow/tensorflow/issues/12747\r\n\r\nI also tried building libtensorflow_inference.so with bazel, but my application has a lot of link errors: missing symbol errors, for example, SessionOptions class missing.\r\n\r\nSo finally, I built android with build_all_android.sh/build_all_ios.sh script as mentioned above (it works well on iOS):\r\n./tensorflow/contrib/makefile/build_all_android.sh -a armeabi-v7a -s tensorflow/contrib/makefile/sub_makefiles/android/Makefile.in -t \"libtensorflow_inference.so libtensorflow_demo.so all\"\r\n\r\nPlease let me know what I need to change in makefile to build optimized and debug.\r\n", "@andrehentz have you seen anything like this before?", "Does this still repro with the latest 1.12 release?", "Yes. it is reproducible on 1.12 release", "The CMake-based build path for TensorFlow Mobile is a best effort project, and it's sadly unsurprising that it's out-of-sync with the proper source tree. I would strongly encourage you to try the bazel-based build path again. I was able to build successfully using NDK r15c:\r\n\r\n```\r\nbazel build --config=android_arm -c opt --cxxopt=--std=c++11 \\\r\n  tensorflow/contrib/android:libtensorflow_inference.so \r\n```\r\n\r\nThe relevant bazelrc section after running `./configure` from the root checkout directory:\r\n\r\n```\r\nbuild --action_env ANDROID_NDK_HOME=\"$ANDROID_CODE_DIR/android-ndk-r15c\"\r\nbuild --action_env ANDROID_NDK_API_LEVEL=\"19\"\r\nbuild --action_env ANDROID_BUILD_TOOLS_VERSION=\"26.0.1\"\r\nbuild --action_env ANDROID_SDK_API_LEVEL=\"23\"\r\nbuild --action_env ANDROID_SDK_HOME=\"$ANDROID_CODE_DIR/android-sdk-linux\"\r\n```\r\n", "android_arm64 build failed which we need to support\r\n\r\nAlso it seems protobuf is not in the generated .so file\r\n\r\nundefined reference to 'google::protobuf::io::CodedInputStream::SetTotalBytesLimit\r\n", "What is the arm64 failure you're seeing? There's a known issue with certain NDK versions and arm64 builds where you need to change the NDK API version. See also issue #20192. In particular, if you change `ANDROID_NDK_API_LEVEL` in `.tf_configure.bazelrc` to 21, then `--config=android_arm64` ought to build. Can you give that a try?", "tensorflow build for arm64 now. \r\n \r\nbut I still have link issue for my app: undefined reference to 'google::protobuf::io::CodedInputStream::SetTotalBytesLimit.\r\n\r\nPlease see my source code at beginning.\r\n\r\nBTW, \r\n(1)how to build with cmake , any working branch is ok. I would like to give a try.\r\n\r\n(2) --config=android_x86 is not supported ?  \r\n\r\n(3) I know tf mobile will be deprecated. but tflite support rnn already ?\r\n", "> (1)how to build with cmake , any working branch is ok. I would like to give a try.\r\n\r\nAt the moment, we don't have concrete plans to maintain CMake support indefinitely. As stated before, this has been best-effort. Of course, we'd be happy to accept PRs that fix the CMake build issue.\r\n\r\n> (2) --config=android_x86 is not supported ?\r\n\r\nFor TensorFlow Lite, yes. For TensorFlow Mobile, again, the situation isn't quite as clear. What issues are you seeing? Does it fail to compile?\r\n\r\n> (3) I know tf mobile will be deprecated. but tflite support rnn already ?\r\n\r\nWe are working on control flow and general RNN support. If there's a specific model you'd like to support, let us know, and we can be sure to prioritize it.", "It seems missing APIs in bazel build as proposed. We have to stay for Makefile solution.\r\n\r\nI am using bidirectional_dynamic_rnn, tf.contrib.rnn.LSTMCell and tf.contrib.seq2seq. Hope all of them are available in tf.lite.  \r\n", "> I am using bidirectional_dynamic_rnn, tf.contrib.rnn.LSTMCell and tf.contrib.seq2seq. Hope all of them are available in tf.lite.\r\n\r\nYep, this is being tracked separately. For now, there's not a lot of support we can give to the CMake build, but feel free to make a pull request if you see issues in the bazel build."]}, {"number": 22883, "title": "Weird crash when using C++ ", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 22882, "title": "custom", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 22881, "title": "How to quantize the format of h5 by API tf.keras ?", "body": "import numpy as np\r\nimport tensorflow as tf\r\n\r\n**the code only generate a float tflite, but i want to get an int8 tflite, i dont't kown how to do** \r\nif somebody can solve this issue [https://github.com/tensorflow/tensorflow/issues/22880](url) , maybe this problem can be solved.\r\n\r\n# Generate tf.keras model.\r\nmodel = tf.keras.models.Sequential()\r\nmodel.add(tf.keras.layers.Dense(2, input_shape=(3,)))\r\nmodel.add(tf.keras.layers.RepeatVector(3))\r\nmodel.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(3)))\r\nmodel.compile(loss=tf.keras.losses.MSE,\r\n              optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),\r\n              metrics=[tf.keras.metrics.categorical_accuracy],\r\n              sample_weight_mode='temporal')\r\n\r\nx = np.random.random((1, 3))\r\ny = np.random.random((1, 3, 3))\r\nmodel.train_on_batch(x, y)\r\nmodel.predict(x)\r\n\r\n# Save tf.keras model in HDF5 format.\r\nkeras_file = \"keras_model.h5\"\r\ntf.keras.models.save_model(model, keras_file)\r\n\r\n# Convert to TensorFlow Lite model.\r\nconverter = tf.contrib.lite.TFLiteConverter.from_keras_model_file(keras_file)\r\ntflite_model = converter.convert()\r\nopen(\"converted_model.tflite\", \"wb\").write(tflite_model)", "comments": ["@wt-huang", "@aselle", "@dlml: @aselle correct me if I am off, but I don't think keras API supports that at the moment. To get int8 model you can first load the above `keras_model.h5` file using [tftables](https://github.com/ghcollin/tftables) to create TensorFlow graph. Next freeze the graph into .pb file. Then you can convert the resulting graph to int8 tflite using toco. \r\n\r\n", "@wt-huang correct me if I'm wrong. I thought for toco to output quantized tflite, you have to give it a fake quantized .pb file? For a keras model, if #22880's answer is no, then we can only get a float .pb model. I know there is the post-quantization method, but it's still using floating point kernels to do calculations during inference.  \r\n\r\n", "@ZJPUBG In the last step I mentioned, invoke toco to convert .pb into int8 tflite FlatBuffer, make sure to set\r\n`--inference_type=QUANTIZED_UINT8` and `--inference_input_type=QUANTIZED_UINT8`", "@wt-huang Thanks for your prompt reply. I know this last step. But my current understanding is, in order to set these flags you mentioned, the training process has to be a quantization-aware training, right? I guess this is the most confused part for me. Thanks.", "@ZJPUBG Yes, quantization-aware training would be ideal in the first step to quantize weights and activation tensors, otherwise final classification results may not be correct. Alternatively, you can use post-training quantization, which is more widely available but could potentially suffer from accuracy loss.\r\n\r\n"]}, {"number": 22880, "title": "how to get  \u201ctf. fake_quant_with_min_max_args\u201d  by  API  \"tf.keras\"", "body": "I want to quantize a h5 model, but i can't find \u201ctf. fake_quant_with_min_max_args\u201d  by   API tf.keras  ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Duplicate of #22881 "]}, {"number": 22879, "title": "Integrating imported graphs into Dataset pipelines", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nyes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nn/a\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary (pip)\r\n- **TensorFlow version (use command below)**:\r\n1.5.0\r\n- **Python version**:\r\n3.5.5\r\n- **Bazel version (if compiling from source)**:\r\nNA\r\n- **GCC/Compiler version (if compiling from source)**:\r\nNA\r\n- **CUDA/cuDNN version**:\r\ncuda 9.0/cudnn 7.0\r\n- **GPU model and memory**:\r\ngtx 1080, 12GB\r\n- **Exact command to reproduce**\r\n\r\n### Describe the problem\r\nI'd like to build parts of my `dataset` by importing pre-existing graphs. It looks like a way to do this is to import the graph from within a `dataset.map` operation as in the source code below. But this approach fails with a placeholder-not-mapped error (please see below). I know, e.g., that it is possible to import and stick a graph _after_ the iterator at the end of the dataset. But I'd like to stick them _in_ the dataset pipeline. Is this currently infeasible using datasets, or am I doing something wrong?\r\n\r\n### Source code\r\nHere is my minimal source code. It works when `USE_GRAPH` is set to `False` and fails (see below for trace) when set to `True`:\r\n```\r\nimport tensorflow as tf\r\nUSE_GRAPH = True\r\nINT = tf.int64\r\n\r\n\r\ndef sq_fun(x):\r\n    if USE_GRAPH:\r\n        g = tf.Graph()\r\n        with g.as_default() as g:\r\n            d = tf.placeholder(INT, shape=(), name='d')\r\n            _ = tf.pow(d, 2, name='e')\r\n        xx = tf.import_graph_def(\r\n            g.as_graph_def(),\r\n            input_map={'d': x},\r\n            return_elements=['e:0'])\r\n    else:\r\n        xx = tf.pow(x, 2, name='e')\r\n    return xx\r\n\r\n\r\ndataset = tf.data.Dataset.range(10)\r\ndataset = dataset.map(sq_fun)\r\nnext_element = dataset.make_one_shot_iterator().get_next()\r\n\r\nwith tf.Session() as sess:\r\n    for i in range(10):\r\n        value = sess.run(next_element)\r\n        print(value)\r\n```\r\n\r\n### Error trace\r\nHere is the error trace when the above code is run as is (wiith `USE_GRAPH` set to `True`):\r\n\r\n```\r\n(tensorflow) matthai@matthai-z400:~/projects/w4ml/src/python/w4ml$ python test1.py \r\n/home/matthai/install/anaconda/anaconda3-4.4.0/envs/tensorflow/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n2018-10-10 18:05:42.047846: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2\r\n2018-10-10 18:05:42.177377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-10-10 18:05:42.177855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: \r\nname: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531\r\npciBusID: 0000:0f:00.0\r\ntotalMemory: 11.90GiB freeMemory: 7.23GiB\r\n2018-10-10 18:05:42.177883: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:0f:00.0, compute capability: 6.1)\r\n2018-10-10 18:05:42.577441: W tensorflow/core/framework/op_kernel.cc:1198] Invalid argument: You must feed a value for placeholder tensor 'import/d' with dtype int64\r\n\t [[Node: import/d = Placeholder[dtype=DT_INT64, shape=[]]()]]\r\nTraceback (most recent call last):\r\n  File \"/home/matthai/install/anaconda/anaconda3-4.4.0/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1350, in _do_call\r\n    return fn(*args)\r\n  File \"/home/matthai/install/anaconda/anaconda3-4.4.0/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1329, in _run_fn\r\n    status, run_metadata)\r\n  File \"/home/matthai/install/anaconda/anaconda3-4.4.0/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 473, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'import/d' with dtype int64\r\n\t [[Node: import/d = Placeholder[dtype=DT_INT64, shape=[]]()]]\r\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[]], output_types=[DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"test1.py\", line 28, in <module>\r\n    value = sess.run(next_element)\r\n  File \"/home/matthai/install/anaconda/anaconda3-4.4.0/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 895, in run\r\n    run_metadata_ptr)\r\n  File \"/home/matthai/install/anaconda/anaconda3-4.4.0/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1128, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/matthai/install/anaconda/anaconda3-4.4.0/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1344, in _do_run\r\n    options, run_metadata)\r\n  File \"/home/matthai/install/anaconda/anaconda3-4.4.0/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1363, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'import/d' with dtype int64\r\n\t [[Node: import/d = Placeholder[dtype=DT_INT64, shape=[]]()]]\r\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[]], output_types=[DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator)]]\r\n```\r\n\r\n\r\n", "comments": ["@matthai I ran the above code snippet without running into any errors. Please use TensorFlow 1.10.0.", "Ah great, thanks! Will try.", "@wt-huang , I tested with TF 1.10 and it indeed does work. Thanks for your quick response!", "Glad that it worked.", "Are all your questions answered?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 22878, "title": "[Bug] Strange FIFOQueue behavior in distributed tensorflow. Is this a bug?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: pip install tensorflow \r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: No CUDA\r\n- **GPU model and memory**: No GPU\r\n- **Exact command to reproduce**: Check code below\r\n\r\n\r\n------------------------\r\n### Describe the problem\r\nThere seems to be very little documentation on distributed tensorflow, so I am unsure of whether or not this is a bug or a logical error on my part. But the behavior is still a little strange.  Essentially, the issue seems to arise due to tf.train.SummarySaverHook(), it's interaction with tf.train.MoniteredTrainingSession() and the chief worker. If you look at my code below, you will see that there are two workers in this model. The chief worker is associated with \"task:0\". I've set up the model such that the queue and it operations live on \"task:1\"(not the chief worker). There is a simple sum operation on \"task:0\". Once I launch the 3 nodes(ps, task0, task1), task0 is able to complete one simple sum operation and then all tasks hang indefinitely.  \r\n\r\nFor the moment, I've bypassed this issue by passing the summary hook object to the non-chief worker(task:0). By doing this, there don't seem to be any issues and the code works fine. I'm unsure of why this bypasses the issue. If chief workers handle summary write operations, shouldn't it still hang?\r\n\r\nThe strange behavior that I was talking about earlier occurs when you pass both the summary hook to the chief worker (Doing this will cause all workers to hang). You can get past the issue by simple removing the tf.FIFOqueue().dequeue() operation. It seems that this operation is getting called by \"task:0\" despite the fact that I've specified its location as \"task:1\" using tf.device. It is also being called despite not being an input requirement for any part of the graph that lives on \"task:0\". \r\n\r\nMy question is essentially: Is this a bug or a logical error on my part?\r\n\r\n### Source code / logs\r\n\r\n\r\n```\r\nimport tensorflow as tf \r\nimport sys\r\n\r\ns_name = str(sys.argv[1])\r\nt_num = int(sys.argv[2])\r\n\r\n\r\n#Cluster details\r\ncl_spec = tf.train.ClusterSpec({\r\n    \"worker\": [\r\n        \"localhost:2223\",\r\n        \"localhost:2224\"\r\n    ],\r\n    \"ps\": [\r\n        \"localhost:2222\"\r\n    ]\r\n})\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\n\r\n\r\n#Decides whether parameter server or worker\r\nif s_name == \"ps\":\r\n    server = tf.train.Server(cl_spec,job_name=\"ps\",task_index=0,config=config)\r\n    server.join()\r\nelse:\r\n    server = tf.train.Server(cl_spec,job_name=\"worker\",task_index=t_num,config=config)\r\n\r\n    with tf.device(\"/job:worker/replica:0/task:0\"):\r\n        a = tf.Variable(tf.constant(5))\r\n        b = tf.Variable(tf.constant(6))\r\n        z = tf.add(a,b)\r\n        z_p = tf.Print([z],[z],\"sum: \")\r\n        tf.summary.histogram(\"t0sum\",z_p)\r\n\r\n    with tf.device(\"/job:ps/replica:0/task:0\"):    \r\n        with tf.name_scope(\"train_place_holder\"):\r\n            x = tf.placeholder(tf.uint8,shape=[1,1],name=\"s_img1\")\r\n            y = tf.placeholder(tf.uint8,shape=[1,1],name=\"s_a\")    \r\n        \r\n        global_step = tf.train.create_global_step()\r\n    \r\n    with tf.device(\"/job:worker/replica:0/task:1\"):\r\n        q = tf.FIFOQueue(capacity=25,\r\n                         dtypes= (tf.uint8,tf.uint8),\r\n                         shapes= (tf.TensorShape([1,1]),tf.TensorShape([1,1])),\r\n                         name=\"tq\",shared_name=\"train_queue\")\r\n        \r\n        enqueue_op = q.enqueue((x,y),name=\"enqueue\")\r\n        xx,yy = q.dequeue(name=\"dq\")\r\n        zz = tf.add(xx,yy)\r\n        tf.summary.histogram(\"t1sum: \",zz)\r\n\r\n    summ = tf.summary.merge_all()\r\n\r\n    lap_dir = r'C:\\Users\\Vishnu\\Documents\\EngProj\\bug_tf\\log'\r\n\r\n    summary_hook = tf.train.SummarySaverHook(   save_steps=1,save_secs=None,\r\n                                                    output_dir=lap_dir,summary_writer=None,\r\n                                                    scaffold=None,summary_op=summ)\r\n\r\n    if (t_num == 0):\r\n        # THis is chief worker\r\n        saver_hook = tf.train.CheckpointSaverHook(  checkpoint_dir=lap_dir,\r\n                                                    save_secs=3600,save_steps=None,\r\n                                                    saver=tf.train.Saver(),checkpoint_basename='model.ckpt',\r\n                                                    scaffold=None)\r\n\r\n        with tf.train.MonitoredTrainingSession(master=server.target,is_chief=True,\r\n                                                hooks=[saver_hook,summary_hook], config=config) as sess:   \r\n           while True:\r\n               sess.run([z,z_p])\r\n\r\n    else:\r\n\r\n        with tf.train.MonitoredTrainingSession(master=server.target,is_chief=False,\r\n                                                save_summaries_steps=1,config=config) as sess:\r\n            while not sess.should_stop():\r\n                sess.run([zz])\r\n\r\n    \r\n```\r\n\r\n\r\nYou can run the above file and start 3 workers using the following command line commands.\r\n\r\n```\r\npython [file_name] worker 1\r\n\r\npython [file_name] worker 0\r\n\r\npython [file_name] ps 0\r\n```", "comments": ["I've also noticed that adding `checkpoint_dir` argument to the chief worker session also causes the workers to hang. \r\n", "@DevarakondaV Have you observed this FIFOQueue behavior when GPU is installed?", "@wt-huang I am not using a GPU and have not tested with one.", "Woops. I think i accidently closed the issue. Sorry. ", "@DevarakondaV It shows in the above code snippet that you are using GPU, probably double check here:\r\n```\r\nconfig.gpu_options.allow_growth = True\r\n```\r\nCheck the communications between ps and workers as well as how you set up the queue. You can always monitor the ps and worker performance online.\r\n\r\nAlso, make sure the chief worker is consistent in the setup, notice some discrepancy here:\r\n\r\n> The chief worker is associated with \"task:0\". I've set up the model such that the queue and it operations live on \"task:1\"(not the chief worker).\r\n\r\n> For the moment, I've bypassed this issue by passing the summary hook object to the non-chief worker(task:0). ", "@wt-huang\r\n\r\n`\r\nconfig.gpu_options.allow_growth = True\r\n`\r\nI've configured this option but I know for a fact that I am not using a GPU. \r\nI've also removed it before running the code again and it still hangs. Have you been able to replicate the issue? \r\n\r\nI should note that I am still running this distributed architecture completely on a local machine. Would this have an effect on whether or not the program hangs? (I am still a beginner at distributed computing in general).\r\n\r\n> Also, make sure the chief worker is consistent in the setup, notice some discrepancy here:\r\n\r\nThat's totally my fault. Sorry for the confusion this may have caused. But this sentence\r\n\r\n>For the moment, I've bypassed this issue by passing the summary hook object to the non-chief worker(task:0).\r\n\r\nShould really say\r\n\r\n>For the moment, I've bypassed this issue by passing the summary hook object to the non-chief worker(task:1).\r\n\r\nI've edited the original description to reflect the change. \r\n\r\n", "@DevarakondaV ,\r\nCan you please upgrade the Tensorflow with the latest version and confirm if the issue still persists.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 22877, "title": "1.12-rc1 cherry-pick request: Add mode_override to the TPU embedding enqueue ops. ", "body": "This allows the mode to be overridden at runtime allowing dynamic switching between inference and training modes. Not fully implemented yet.\r\n\r\nPiperOrigin-RevId: 215325071", "comments": ["(Added @george-kurian just as an FYI)"]}, {"number": 22876, "title": "1.12-rc1 cherry-pick request: Changed Adam algorithm variant formula ...", "body": "... from sqrt(max(v, epsilon**2)) to sqrt(v + epsilon**2) and changed flag name accordingly.\r\n\r\nPiperOrigin-RevId: 216240045", "comments": []}, {"number": 22875, "title": "1.12-rc1 cherry-pick request: Revert constant folding changes", "body": "Turns out that enabling constant folding for int32 will cause following ops to be constant folded as well, which might generate large output tensors (e.g. Fill/Tile/Pad). It has caused problem for both TPU and GPU folks.\r\n\r\nPiperOrigin-RevId: 215772272\r\nPiperOrigin-RevId: 215946205", "comments": []}, {"number": 22874, "title": "TF-TRT support conversion for Transpose, Reshape", "body": "Support conversion for Reshape and Transpose ops.", "comments": ["I just added infrastructure to do c++ unit testing of individual op converters, and tests for const/reshape/transpose converter. I'm now fixing the broken python tests, after that I'll merge.\r\n\r\nAlso +cc @azaks2 ", "> I just added infrastructure to do c++ unit testing of individual op converters, and tests for const/reshape/transpose converter. I'm now fixing the broken python tests, after that I'll merge.\r\n\r\nAwesome, thanks!\r\n"]}, {"number": 22873, "title": "[Intel MKL] Add scripts for creating allreduce based Tensorflow k8s deployment", "body": "Create allreduce based distributed Tensorflow k8s deployment for testing\r\n\r\nIntel optimized Tensorflow horovod dockerfile\r\n- tensorflow/tools/docker/Dockerfile.devel-mkl-horovod\r\n- tensorflow/tools/docker/Dockerfile.mkl-horovod\r\n\r\nSigned-off-by: Cong Xu <cong.xu@intel.com>", "comments": ["Thank you so much for your great review! I have addressed all the comments, please have a look at my new  commit. We use [Intel MKL] to track all the PRs submitted by Intel, it doesn't necessarily have to be related to Intel MKL code.", "Thanks a lot for the great suggestions, they are very useful! I have addressed your comments, please have a look at my new commit. :)", "The commit does not pass Ubuntu Santiy check because line 64 in k8s_generate_yaml.py is too long (84/88). I have modified that line and submitted a new commit. Please approve the new commit and see whether it can pass Ubuntu Santiy check. Thanks!", "The new commit passes the Ubuntu Santiy check. Thanks a lot!"]}, {"number": 22872, "title": "import tensorflow failed, \"ImportError: DLL load failed: The specified module could not be found\"", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 Home\r\n- **TensorFlow installed from (source or binary)**: binary ( pip install tensorflow-gpu )\r\n- **TensorFlow version (use command below)**: 1.11.0\r\n- **Python version**: python-3.5.4-amd64\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: cuda_10.0.130_411.31_win10\r\n- **GPU model and memory**: Nvidia Geforece GTX 1060 6G\r\n- **Exact command to reproduce**: python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nCannot import tensorflow successfully.\r\nTry \"visual studio 2015, Microsoft Visual C++ 2015 Redistributable Update 3\" but the update in my laptop is newer than this, so I cannot install this update.\r\n\r\n### Source code / logs\r\nThe error message after the command above\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Program Files\\Python35\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Program Files\\Python35\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Program Files\\Python35\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Program Files\\Python35\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nPlease let me know if any question or suggestion.\r\n", "comments": ["TensorFlow officially supports CUDA 9.0. However it is compatible with CUDA 10.0 but not supported currently. For using TF with cuda 10, you have to build it from sources yourself. You can also take a look at [installations](https://medium.com/@amsokol.com/update-1-how-to-build-and-install-tensorflow-gpu-cpu-for-windows-from-source-code-using-bazel-and-c2e86fec9ef2) done by another users to make it work.", "Hello @ymodak \r\nThank you for the quick response.\r\nAs you mentioned, the binary version TensorFlow works with cuda_9.0.176_win10 and cudnn-9.0-windows10-x64-v7.3.1.20 in the same machine.", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)", "Im getting the same error using cuda 9.0 \r\n", "Using TensorFlow backend.\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\aniag\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\aniag\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\aniag\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\aniag\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\aniag\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\D_CASE\\task1\\Untitled.py\", line 7, in <module>\r\n    import keras\r\n  File \"C:\\Users\\aniag\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\__init__.py\", line 3, in <module>\r\n    from . import utils\r\n  File \"C:\\Users\\aniag\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\utils\\__init__.py\", line 6, in <module>\r\n    from . import conv_utils\r\n  File \"C:\\Users\\aniag\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\utils\\conv_utils.py\", line 9, in <module>\r\n    from .. import backend as K\r\n  File \"C:\\Users\\aniag\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\backend\\__init__.py\", line 89, in <module>\r\n    from .tensorflow_backend import *\r\n  File \"C:\\Users\\aniag\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 5, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\aniag\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\aniag\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\aniag\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\aniag\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\aniag\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\aniag\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\aniag\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\aniag\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "The above is the error", "@ExiledMercenary Please open a [new issue](https://github.com/tensorflow/tensorflow/issues/new) describing your problem and provide the system information. This will help us to focus on your issue explicitly.", "same issue.  same resolution.  tensorflow 1.12 is broken.", "`conda install -c aaronzs tensorflow-gpu`\r\n`conda install -c anaconda cudatoolkit`\r\n`conda install -c anaconda cudnn`\r\n`conda install -c anaconda cudatoolkit`\r\n\r\nworks for me.\r\n\r\nat first time, anaconda install cuda9 for me. \r\nafter run `conda install -c anaconda cudnn`, it shows cudnn 7.14 for cuda 8 is installed, also cuda 9 will be down to cuda8.\r\nthen, python and import tensorflow, error: looking for cudnn files, suggest installing cuda 9.\r\njust run `conda install -c anaconda cudatoolkit` again, cuda 9 will be installed to replace cuda8.\r\nfinally, system works.\r\n\r\n\r\n", "I tried the solution of @dwindy, the installation seemed to proceed well. But when I tried to confirm that Tensorflow GPU was installed it failed.\r\nI then tried to re-run the same code `conda install -c aaronzs tensorflow-gpu` the following message appears \r\n\r\n> Solving environment: failed\r\n> UnsatisfiableError: The following specifications were found to be in conflict:\r\n>   - tensorflow-gpu\r\n> Use \"conda info <package>\" to see the dependencies for each package.\r\n", "> I just downgraded TensorFlow to 1.10.0 and it worked\r\n> \r\n> `pip install tensorflow-gpu==1.10.0`\r\n\r\n@57ar7up were you using cuda 10 ?", "CUDA 9.0 with the same issue on windows, both TensorFlow 1.13 and 2.0-alpha. So I just use the previous version as 1.12 solved this problem.\r\n`pip unistall tensorflow-gpu`\r\n`pip install tensorflow-gpu==1.12.0`", "hi, i have this error. How can i solve this problem?\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow1\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow1\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"generate_tfrecord.py\", line 17, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow1\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow1\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "i have the same error .\r\nbut when i import torch first and then import tensorflow \r\nit works :0\r\n@ymodak \r\n\r\n> \r\nPython 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)] :: Anaconda, Inc. on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\anaconda\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\anaconda\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\anaconda\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\anaconda\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\anaconda\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: \u627e\u4e0d\u5230\u6307\u5b9a\u7684\u6a21\u5757\u3002\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\anaconda\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\anaconda\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\anaconda\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\anaconda\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\anaconda\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\anaconda\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\anaconda\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\anaconda\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: \u627e\u4e0d\u5230\u6307\u5b9a\u7684\u6a21\u5757\u3002\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>> import torch\r\n>>> import tensorflow as tf\r\n>>> tf.__version__\r\n'1.13.1'\r\n>>>\r\n", "Thanks for reply I will try it\n\nOn Thu, 11 Apr 2019 at 17:43, corleonechensiyu <notifications@github.com>\nwrote:\n\n> i have the same error .\n> but when i import torch first and then import tensorflow\n> it works :0\n> @ymodak <https://github.com/ymodak>\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/22872#issuecomment-482044001>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/Ap8eqdLZqkpt4Cs1H7C0F5UoPUcYt_kQks5vfwOygaJpZM4XWCy8>\n> .\n>\n", "> I just downgraded TensorFlow to 1.10.0 and it worked\r\n> \r\n> `pip install tensorflow-gpu==1.10.0`\r\n\r\nCan confirm that this works for me too. Thanks a ton man.\r\n\r\nBTW I'm using **CUDA 9.0** and as @57ar7up said, I downgraded my tensorflow-gpu version to **1.10**\r\n\r\n", "Can someone help me, i v tryed combination of :\r\nCuda 9 , cudnn 7.4.1.5 , different tensofrlows 1.10 to latest,  (like in posts above), \r\nthen cuda 10 with tensorflow 2 .0.0. alfa but no combination works for no reason!!!!!\r\n\r\n It is possible to get the **INFORMATIVE ERROR MESSAGE** from tensorflow that **EXACTLY** the problem is,  ", "> CUDA 9.0\u5728Windows\u4e0a\u5177\u6709\u76f8\u540c\u7684\u95ee\u9898\uff0cTensorFlow 1.13\u548c2.0-alpha\u3002\u6240\u4ee5\u6211\u53ea\u4f7f\u75281.12\u4ee5\u524d\u7684\u7248\u672c\u89e3\u51b3\u4e86\u8fd9\u4e2a\u95ee\u9898\u3002\r\n> `pip unistall tensorflow-gpu`\r\n> `pip install tensorflow-gpu==1.12.0`\r\nAttributeError: module 'tensorflow._api.v1.compat' has no attribute 'v1'\r\n"]}, {"number": 22871, "title": "Dataset.shuffle gives same order with any seed if reshuffle_each_iteration=False", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OS X 10.11.6\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: binary (anaconda)\r\n- **TensorFlow version (use command below)**: b'unknown' 1.10.0\r\n- **Python version**:\r\nPython 3.6.6 |Anaconda custom (64-bit)| (default, Jun 28 2018, 11:07:29) \r\n[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\nin python interactive session\r\n\r\n```\r\n>>> op1 = tf.data.Dataset.from_tensor_slices(list(range(100))).shuffle(100, seed=0, reshuffle_each_iteration=False).repeat().make_one_shot_iterator().get_next()\r\n>>> op2 = tf.data.Dataset.from_tensor_slices(list(range(100))).shuffle(100, seed=1, reshuffle_each_iteration=False).repeat().make_one_shot_iterator().get_next()\r\n>>> sess = tf.InteractiveSession()\r\n2018-10-10 18:56:24.996286: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n>>> sess.run([op1, op2])\r\n[85, 85]\r\n>>> sess.run([op1, op2])\r\n[1, 1]\r\n>>> sess.run([op1, op2])\r\n[54, 54]\r\n```\r\n\r\nOne can see that op1 and op2 fetch elements in same order. If reshuffle_each_iteration is set to True, the element order is different as expected. I believe different seed values should result in different element order for op1 and op2, no matter what the value of reshuffle_each_iteration is.", "comments": ["Thanks for reporting this. We just noticed this bug in the last few days, and the fix is in 4ff7b81514ea1b86295bc74b620e3c1d3e127e6f."]}, {"number": 22870, "title": "Transfer Learning help", "body": "I am working on a sketch based image retrieval problem using Siamese network and have written the code in TensorFlow. Some people suggested to train the network on pretrained weights rather than training from scratch, but I am not sure how to implement it in TensorFlow. Online resources couldn't clear up my doubts completely. The repo link is given below:\r\n\r\nhttps://github.com/ArkaJU/Sketch-Retrieval---Siamese/blob/master/SketchRetrieval.ipynb\r\n\r\nI want to change the architecture to GoogLeNet and start with pretrained weights.\r\nAny help is appreciated, thank you.", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 22868, "title": "Bluepill robot tests", "body": "This is a proposed implementation of tests using Robot. If this is accepted, writing more tests like that should be fairly simple.", "comments": ["Hi @petewarden is there anything else blocking us here?"]}]