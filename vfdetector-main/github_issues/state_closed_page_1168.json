[{"number": 18158, "title": "Centos: C++, No OpKernel was registered to support Op 'RandomUniform' with these attrs.", "body": "OS Platform : Cestos7\r\npython:2.7\r\ngcc:4.8\r\nTensorFlow installed from source\r\ntf version : ('v1.4.0-19-ga52c8d9', '1.4.1')\r\nBazel version:N/A\r\nCUDA Version 8.0.61\r\nCUDNN_MAJOR : 6\r\nGPU model and memory: 22912MiB*4\r\n\r\n\r\nInvalid argument: No OpKernel was registered to support Op 'RandomUniform' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  <no registered kernels>\r\n[[Node: loss/transitions/Initializer/random_uniform/RandomUniform = RandomUniform[T=DT_INT32, _class=[\"loc:@loss/transitions\"], _output_shapes=[[71,71]], dtype=DT_FLOAT, seed=0, seed2=0](loss/transitions/Initializer/random_uniform/shape)]]\r\n\r\nthe C++ code is :\r\n\r\n```\r\nint main(int argc, char* argv[]) {\r\n\r\n\tstd::cout << \"test start----.\\n\";\r\n\tSession* session;\r\n\tStatus status = NewSession(SessionOptions(), &session);\r\n\tif (!status.ok()) {\r\n\t\tstd::cout << status.ToString() << \"\\n\";\r\n\t\treturn 1;\r\n\t}\r\n\tstd::cout << \"Session successfully created..\\n\";\r\n\r\n\r\n\t//GraphDef graph_def;\r\n\t//status = ReadBinaryProto(Env::Default(), \"../demo/deep_model/freeze_graph.pb\", &graph_def);\r\n\t//\r\n\tMetaGraphDef graph_def;\r\n\tstatus = ReadBinaryProto(Env::Default(), \"../src/checkpoints/model-1.meta\", &graph_def);\r\n\tif (!status.ok()) {\r\n\t\tstd::cout << \"readerror=\" << status.ToString() << std::endl;\r\n\t\treturn 0;\r\n\t} \r\n\telse {\r\n\t\tstd::cout << \"Load graph protobuf successfully\" << std::endl;\r\n\t}\r\n\r\n\r\n\t//status = session->Create(graph_def);\r\n\tstatus = session->Create(graph_def.graph_def());\r\n\tif (!status.ok()) {\r\n  \t\tstd::cout << std::endl << std::endl << \"error:\"<< status.ToString() << std::endl;\r\n\t} \r\n\telse {\r\n  \t\tstd::cout << \"Add graph to session successfully\" << std::endl;\r\n\t}\r\n\r\n\ttensorflow::Tensor checkpointTensor(DT_STRING, tensorflow::TensorShape());\r\n\tcheckpointTensor.scalar<string>()() = \"../src/checkpoints/model-1\";\r\n   \r\n\tstatus = session->Run(\r\n\t\t{{ graph_def.saver_def().filename_tensor_name(), checkpointTensor }},\r\n        \t{},\r\n        \t{graph_def.saver_def().restore_op_name()},\r\n       \t\tnullptr);\r\n\tif (!status.ok()) {\r\n\t\tstd::cout << \"run1:\" << status.ToString() << std::endl;\r\n\t\treturn 0;\r\n\t}\r\n\tstd::cout << \"ok\" << std::endl;\r\n\r\n\tsession->Close();\r\n\t\r\n}\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow version\nCUDA/cuDNN version\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Thanks for digging in. @jlebar would you be able to take a look?", "@skye is this somehow related to XLA?", "Ah sorry I meant to cc you on a different issue :) Nvm, I'll find someone else... ", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 37 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!", "Sorry.I meet the similar issue which is No OpKernel for Sin and Cos,could you please tell me how do you solve it? Thank you!"]}, {"number": 18157, "title": "Failed to convert object of type <class 'werkzeug.datastructures.File.Storage> to tensor.", "body": "This is my client python file that uses flask framework to create REST api.  I am running this inside a docker machine. So this take an input .txt file and read the contents of it.\r\n\r\n```\r\nfrom flask import Flask, render_template, request, url_for, jsonify\r\nimport json\r\nimport tensorflow as tf \r\nimport numpy as np \r\nimport os \r\nimport argparse\r\nimport sys\r\nfrom google.protobuf import json_format\r\nfrom datetime import datetime \r\nfrom werkzeug import secure_filename\r\n\r\nfrom grpc.beta import implementations\r\nfrom tensorflow_serving.apis import predict_pb2\r\nfrom tensorflow_serving.apis import prediction_service_pb2\r\n\r\ntf.app.flags.DEFINE_string('server', 'localhost:9000', 'PredictionService host:port')\r\nFLAGS = tf.app.flags.FLAGS\r\n\r\napp = Flask(__name__)\r\n\r\nclass mainSessRunning():\r\n    \r\n    def __init__(self):\r\n        host, port = FLAGS.server.split(':')\r\n        channel = implementations.insecure_channel(host, int(port))\r\n        self.stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\r\n\r\n        self.request = predict_pb2.PredictRequest()\r\n        self.request.model_spec.name = 'example_model'\r\n        self.request.model_spec.signature_name = 'prediction'\r\n\r\n    def inference(self, val_x):\r\n        #temp_data = numpy.random.randn(100, 3).astype(numpy.float32)\r\n        #temp_data = val_x.astype(np.float32).reshape(-1, 3)\r\n        data = val_x\r\n        self.request.inputs['input'].CopyFrom(tf.contrib.util.make_tensor_proto(data))\r\n        result = self.stub.Predict(self.request, 5.0)\r\n        return result\r\n\r\nrun = mainSessRunning()\r\n\r\nprint(\"Initialization done. \")\r\n\r\n# Define a route for the default URL, which loads the form\r\n@app.route('/inference', methods=['POST'])\r\ndef inference():\r\n    request_data = request.files['file']\r\n    result = run.inference(request_data)\r\n    r = json_format.MessageToJson(result)\r\n    return jsonify({'result':r})\r\n\r\n@app.route('/test', methods=['GET'])\r\ndef test_serv():\r\n    return (\"Hello\")\r\n        \r\nif __name__ == \"__main__\":\r\n    app.run(host= '0.0.0.0')\r\n    \r\n    \r\n\r\n\r\n```\r\n\r\n\r\nWhen i try too run it produces error, \r\n![capture](https://user-images.githubusercontent.com/26268279/38171661-117fa570-35bc-11e8-9f12-dd6242552794.JPG)\r\n\r\nIs this a bug?\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 18156, "title": "Broken links of assignment 1:https://commondatastorage.googleapis.com/books1000/", "body": "The download link for \"notMNIST_large.tar.gz\" and \"notMNIST_small.tar.gz\" is broken. Can u provide other download link and edit [the file of assignment 1](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/1_notmnist.ipynb).\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n-----------------\r\n\r\nPlease go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 18155, "title": "Update metrics_ops.py", "body": "Add deprecation notes", "comments": ["Please fix linter errors: https://source.cloud.google.com/results/invocations/282188b0-34e7-4ad0-85bd-d8f07f5ed915/log", "The failure looks transient. Running tests again."]}, {"number": 18154, "title": "Replace deprecated tf.contrib.metrics.streaming_accuracy with tf.metrics.accuracy", "body": "As tf.contrib.metrics.streaming_accuracy (tf.contrib.metrics.streaming_mean_squared_error) has been deprecated and has been replaced with tf.metrics.accuracy, this fix replaces deprecated tf.contrib.metrics.streaming_accuracy with tf.metrics.accuracy in tensorflow/contrib/training/python/training/evaluation.py.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 18153, "title": "Update build.gradle", "body": "", "comments": ["@petewarden could you take a look, please?", "Nagging Reviewers @petewarden, @aselle: It has been 14 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @petewarden, @aselle: It has been 29 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @petewarden, @aselle: It has been 44 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @petewarden, @aselle: It has been 59 days with no activity and the `awaiting review` label was assigned. Can you please take a look?"]}, {"number": 18152, "title": "Missing @grpc//third_party/address_sorting", "body": "I think a [regression was introduced recently](https://github.com/tensorflow/tensorflow/commit/f80486324807181614ac71367dbb9cf588aa2804#diff-bb845bf0c867fef59b6527c0946799f9R116) when doing a new cpu build:\r\n\r\n`failed; build aborted: no such package '@grpc//third_party/address_sorting': java.io.IOException: Error downloading [https://mirror.bazel.build/github.com/grpc/grpc/archive/bd6bdf93279a39a8cd92978fd7c9d14eccd98fc2.tar.gz, https://github.com/grpc/grpc/archive/bd6bdf93279a39a8cd92978fd7c9d14eccd98fc2.tar.gz] `\r\n\r\nReverting this commit allowed the build to continue.\r\n\r\nUpdate:  building origin/master", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "No custom code.  This was building TF from source on Ubuntu.  origin/master branch.  bazel release 0.11.1.  No CUDA/cuDNN.  No GPU at all.\r\n\r\nExact commands to reproduce the error is a simple build as described in the \"building from source\" instructions.\r\n\r\nCheers", "@apolcyn\r\n@nicolasnoble\r\n\r\n", "I'm wondering if this is an upstream issue in [grpc](https://github.com/grpc/grpc/tree/master/third_party/address_sorting).\r\n\r\n@fat-tire do you have command lines to reproduce?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Ping @apolcyn - we need to help Tensorflow add your new project into their workspace.", "I'm currently trying to repro from Tensorflow's master branch (commit 714f3c4f2f901e865bfcbf830485adafb92dca48), with ubuntu 16.04 and bazel 0.13.1 (I'll try bazel 0.11.1 next).\r\n\r\nI'm trying to follow the [instructions for install tensorflow from source](https://www.tensorflow.org/install/install_sources), but I'm stumbling on what looks like unrelated issues and I'm not able to reproduce so far. @fat-tire can you please post more details if you have any?E.g., are you following [these source build instructions](https://www.tensorflow.org/install/install_sources), and using the `configure` defaults? Or, do we know if this is still an issue?\r\n\r\nProbably a minor issue but FWIW, after following the \"install tensorflow from source\" instructions, using python 2.7, and using all of the defaults in the `configure` script, I first ran into an issue with the `enum` module not being available (probably because I tried the python2.7 route). After installing the `enum` module, my bazel build command (`bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`) now fails with:\r\n\r\n```\r\n[4,403 / 8,171] Executing genrule //tensorflow/python:framework/fast_tensor_util.pyx_cython_translation; 0s local\r\nERROR: /src/tensorflow/tensorflow/tools/api/generator/BUILD:27:1: Executing genrule //tensorflow/tools/api/generator:python_api_gen failed (Exit 1)\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/1c025c2968c1aa16bac5eca67aaf21d1/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/tools/api/generator/create_python_api.py\", line 26, in <module>\r\n    from tensorflow import python  # pylint: disable=unused-import\r\n  File \"/root/.cache/bazel/_bazel_root/1c025c2968c1aa16bac5eca67aaf21d1/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 63, in <module>\r\n    from tensorflow.python.framework.framework_lib import *  # pylint: disable=redefined-builtin\r\n  File \"/root/.cache/bazel/_bazel_root/1c025c2968c1aa16bac5eca67aaf21d1/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/framework/framework_lib.py\", line 52, in <module>\r\n    from tensorflow.python.framework.importer import import_graph_def\r\n  File \"/root/.cache/bazel/_bazel_root/1c025c2968c1aa16bac5eca67aaf21d1/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/framework/importer.py\", line 32, in <module>\r\n    from tensorflow.python.framework import function\r\n  File \"/root/.cache/bazel/_bazel_root/1c025c2968c1aa16bac5eca67aaf21d1/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/framework/function.py\", line 37, in <module>\r\n    from tensorflow.python.ops import variable_scope as vs\r\n  File \"/root/.cache/bazel/_bazel_root/1c025c2968c1aa16bac5eca67aaf21d1/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/ops/variable_scope.py\", line 197, in <module>\r\n    \"\"\"\r\nAttributeError: 'int' object attribute '__doc__' is read-only\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 35.977s, Critical Path: 21.32s\r\nINFO: 1 process, local.\r\nFAILED: Build did NOT complete successfully\r\n```", "It has been 18 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 33 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!", "WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nDEBUG: /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:91:3: \r\nAuto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\\Windows\\Temp' as default\r\nWARNING: /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/protobuf_archive/WORKSPACE:1: Workspace name in /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/protobuf_archive/WORKSPACE (@com_google_protobuf) does not match the name given in the repository's definition (@protobuf_archive); this will cause a build error in future versions\r\nWARNING: /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/absl_py/WORKSPACE:1: Workspace name in /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/absl_py/WORKSPACE (@io_abseil_py) does not match the name given in the repository's definition (@absl_py); this will cause a build error in future versions\r\nWARNING: /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/flatbuffers/WORKSPACE:1: Workspace name in /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/flatbuffers/WORKSPACE (@com_github_google_flatbuffers) does not match the name given in the repository's definition (@flatbuffers); this will cause a build error in future versions\r\nERROR: /root/tensorflow/tensorflow/tools/pip_package/BUILD:123:1: no such package '@grpc//third_party/address_sorting': java.io.IOException: thread interrupted and referenced by '//tensorflow/tools/pip_package:licenses'\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@grpc//third_party/address_sorting': java.io.IOException: thread interrupted\r\nINFO: Elapsed time: 52.940s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (172 packages loaded)\r\n"]}, {"number": 18151, "title": "tf.contrib.layers.l2_regularizer() cause warning", "body": "TF version: 1.7\r\n```\r\nimport tensorflow as tf\r\nr = tf.contrib.layers.l2_regularizer(0.001)\r\n```\r\nThen a warning message will occur:\r\n\r\n> WARNING:tensorflow:From /.../tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\r\n> Instructions for updating:\r\n> Use the retry module or similar alternatives.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "leave them as N/A", "Nagging Assignee @cy89: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 60 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 75 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "issue solved in TF 1.8"]}, {"number": 18150, "title": "Validate the shape of count for RepeatDataset", "body": "The count in RepeatDataset requires a scalar though the validation is only done in kernel, not in shape function.\r\n\r\nThis fix validates the shape of count for RepeatDataset.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 18148, "title": "Fix some rendering format in contrib doc strings", "body": "This PR is to fix mess-up math equation format in below tf.contrib related api docstrings.\r\n\r\n- This PR is to fix this math equation issue according to the [Math in markdown guideline](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/community/documentation.md#math-in-markdown).\r\nTake [ConvDiagonalFB](https://www.tensorflow.org/api_docs/python/tf/contrib/kfac/fisher_blocks/ConvDiagonalFB) as an example below:\r\nBefore:\r\n![image](https://user-images.githubusercontent.com/1680977/38165660-8fcf4dae-3549-11e8-9344-96ed64ca8378.png)\r\n\r\nAfter:\r\n![image](https://user-images.githubusercontent.com/1680977/38165680-d0372498-3549-11e8-8270-fbd72733dad5.png)\r\n\r\n- Fix a minor citation reference in [sparsemax](https://www.tensorflow.org/api_docs/python/tf/contrib/sparsemax) as below:\r\nBefore:\r\n![image](https://user-images.githubusercontent.com/1680977/38170052-d35da246-35ad-11e8-9285-54d4455e9b4f.png)\r\nAfter:\r\n> Module that implements sparsemax and sparsemax loss, see [1].\r\n> \r\n> [1]: https://arxiv.org/abs/1602.02068\r\n> ", "comments": []}, {"number": 18147, "title": "Exit code 132 on TensorFlow import", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch Linux 4.15.8\r\n- **TensorFlow installed from (source or binary)**: Binary  wheel (`pip install tensorflow`)\r\n- **TensorFlow version (use command below)**: 1.7.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: Intel graphics, 4GB RAM\r\n- **Exact command to reproduce**: `import tensorflow`\r\n\r\n### Describe the problem\r\nI've installed tensorflow v1.7.0 via PIP and I am having issues working with it. Upon entering a REPL and attempting to `import tensorflow` the REPL silently exits with an exit code of 132. I've run PDB and tried importing it and it appears it is exiting around the `from tensorflow.python import *` section of the tensorflow code. Looking at the PDB output it appears that code is exiting with error code 132 after running `importlib._bootstrap._find_and_load` which is a python internal.\r\n\r\n### Source code / logs\r\nPDB output from trying to import tensorflow (around the section that it exits):\r\n```\r\n> <frozen importlib._bootstrap>(191)_get_module_lock()->_ModuleLock('...40427194539368\r\n(Pdb) n\r\n> <frozen importlib._bootstrap>(149)__enter__()\r\n(Pdb) n\r\n--Return--\r\n> <frozen importlib._bootstrap>(149)__enter__()->None\r\n(Pdb) n\r\n> <frozen importlib._bootstrap>(969)_find_and_load()\r\n(Pdb) n\r\n> <frozen importlib._bootstrap>(970)_find_and_load()\r\n(Pdb) n\r\n> <frozen importlib._bootstrap>(971)_find_and_load()\r\n(Pdb) n\r\n\r\n```\r\nWhat I'm trying to run:\r\n```py\r\n$ python3.6\r\nPython 3.6.4 (default, Jan  5 2018, 02:35:40) \r\n[GCC 7.2.1 20171224] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n$ echo $?\r\n132\r\n```\r\n", "comments": ["I couldn't replicate this on an Ubuntu docker container.\r\n\r\nI also couldn't replicate this on an Arch linux docker container, with the same versions as yours:\r\n\r\n```bash\r\n$ docker run -it base/archlinux bash\r\n>$ pacman -Syy                                                                                                               \r\n>$ pacman -S python3                                                                                                                                                                                                          \r\n>$ pacman -S python-pip\r\n>$ pip install tensorflow                                                                                                                                                                                                    \r\n>$ python3.6\r\nPython 3.6.4 (default, Jan  5 2018, 02:35:40)                                                                                    \r\n[GCC 7.2.1 20171224] on linux                                                                                                    \r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.                                                           \r\n>>> import tensorflow as tf\r\n>>>\r\n```\r\n\r\nIt looks like this might be local, or a GPU-specific issue. Can you replicate this in Docker (ideally nvidia-docker) for us?", "Started a docker container, ran all the same commands as you and got this as a result\r\n```bash\r\n[root@216b9202260e /]# python3.6\r\nPython 3.6.4 (default, Jan  5 2018, 02:35:40) \r\n[GCC 7.2.1 20171224] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nIllegal instruction (core dumped)\r\n[root@216b9202260e /]# echo $?\r\n132\r\n```\r\n\r\nEven inside the docker container I'm receiving the 132 status code.\r\n\r\nThis was not nvidia docker as I don't have a NVIDIA graphics card (just intel integrated graphics), Not sure if that matters or not though.\r\n\r\nNormally I would assume that this is a setup issue with my Python installation. However, the 132 error code (which by definition is for an error caused by RF-Kill) threw me off.", "Interesting. This looks to me like either an issue with your computer, or perhaps a problem with integrated graphics (or something else that could be common to other users).\r\n\r\nCan you also check if this still happens with different versions of TF, like a build from master HEAD and older versions of the Python package, like 1.6?", "Tested with 1.6 yesterday with same issue. I can run a build from master HEAD now.", "Sorry, got side tracked. Just finished the build from source and that seems to me working fine. Not sure why the other packages aren't.", "@gunan Any thoughts on why the pip package would be broken here?\r\n\r\nAFAIK, the PyPi packages have some platform-specific build settings, which may mean that you'll have to build from source. Did you try an Ubuntu-targeting container, too?", "we never tested our build on arch linux. It is possible that the combination of core libraries on arch linux is different, and there is a low level conflict.", "Im having this issue on Ubuntu  16.04.3....  how might I track down the offending lib?\n\n\n\n\nOn April 4, 2018 1:41:35 PM EDT, Gunhan Gulsoy <notifications@github.com> wrote:\n>we never tested our build on arch linux. It is possible that the\n>combination of core libraries on arch linux is different, and there is\n>a low level conflict.\n>\n>-- \n>You are receiving this because you are subscribed to this thread.\n>Reply to this email directly or view it on GitHub:\n>https://github.com/tensorflow/tensorflow/issues/18147#issuecomment-378684059\n", "the same issue, ubuntu...\r\n", "@bjmuld @unnir Can you provide your platform details (same info in the [New Issue Template](https://github.com/tensorflow/tensorflow/issues/new)) and replication commands for Docker?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Yes.  Still an issue.  \r\nUbuntu 16.04 on amd64.  Tensorflow 1.7.0 installed via pip inside a fresh python 3.5 virtualenv. ", "Could you reproduce this in a dockerfile and share the file? Since on our systems this seems to work fine, we have no idea how to proceed.", "Hmm.  I'm not fluent in docker-ese.  Could perhaps read up, but not immediately.  Python crashes at 'import tensorflow'.  I should add that tensorflow==1.5 installed via pip in a virtualenv is running fine, it seems.\r\n\r\nHere's some output from GDB:\r\n\r\n```\r\nThread 1 \"python3\" received signal SIGILL, Illegal instruction.\r\n0x00007fffdaa586c0 in std::pair<std::__detail::_Node_iterator<std::pair<tensorflow::StringPiece const, std::function<bool (tensorflow::Variant*)> >, false, true>, bool> std::_Hashtable<tensorflow::StringPiece, std::pair<tensorflow::StringPiece const, std::function<bool (tensorflow::Variant*)> >, std::allocator<std::pair<tensorflow::StringPiece const, std::function<bool (tensorflow::Variant*)> > >, std::__detail::_Select1st, std::equal_to<tensorflow::StringPiece>, tensorflow::StringPieceHasher, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_emplace<std::pair<tensorflow::StringPiece, std::function<bool (tensorflow::Variant*)> > >(std::integral_constant<bool, true>, std::pair<tensorflow::StringPiece, std::function<bool (tensorflow::Variant*)> >&&)\r\n    () from /home/$$$$$$$$/working/testvenb/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n```\r\n", "Is it possible you are running on an old CPU?\r\nTensorFlow prebuilt packages require your CPU to have AVX instruction set.\r\nWhat is your CPU make and model?", "Yes, that is possible. Here's some cpuinfo (coudn't find 'avx' by grepping /proc/cpuinfo):\r\n```\r\nprocessor       : 0\r\nvendor_id       : GenuineIntel\r\ncpu family      : 6\r\nmodel           : 26\r\nmodel name      : Intel(R) Xeon(R) CPU           W3530  @ 2.80GHz\r\nstepping        : 5\r\nmicrocode       : 0x19\r\n```\r\nadditionally, this dockerfile produces the error for me:\r\n\r\n```\r\nFROM ubuntu\r\n\r\nRUN apt update && apt install -y python3 python3-virtualenv virtualenv\r\n\r\nRUN bash -c '\\\r\n /usr/bin/virtualenv -p python3 testvenv && \\\r\n source testvenv/bin/activate && \\\r\n pip install tensorflow==1.7 && \\\r\n echo \"import tensorflow\" > test.py && \\\r\n python test.py'\r\n```", "I should add that my first-cut self-compiled TF gave me the same error....", "This CPU definitely does not have AVX instruction set. So that is one issue.\r\nWhich command did you use to build TF on your machine?\r\nAlso, how did you configure?", "Yes, the CPU is missing the AVX instruction set.\r\nThe quickest recommendation I have is to build TF from sources on your machine. You can also install an older version which has more compatible binaries.", "It also cause I cannot run tensorflow > 1.5 on Paperspace Gradient\u00b0 \r\n\r\nCommand:\r\n\r\npaperspace jobs create --container tensorflow/tensorflow:1.8.0-gpu --machineType P4000 --command \"python -c 'import tensorflow'\"", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@tensorflowbutler sure, just be patient.", "duplicate of #19584 "]}, {"number": 18146, "title": "Continous Messages of  \"I tensorflow/core/common_runtime/placer.cc:874]\" in every training iteration", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu16.04\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.4\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:8.0\r\n- **GPU model and memory**: NVIDIA 1080 (8GB)\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nSo I have written a DQN code for my research which has two networks(Target Network and Eval Networks). Eval Network is for training and target network is or generating samples. But during training in every iteration I get a long message like below.My code is running well and fine in CPU but I get these messages while training them in GPU and what does '(Const)' means.\r\n\r\n### Source code / logs\r\n018-03-31 21:43:19.799483: I tensorflow/core/common_runtime/placer.cc:874] Target_Network/RNN_Layer/rnn/while/rnn/basic_lstm_cell/BiasAdd: (BiasAdd)/job:localhost/replica:0/task:0/device:GPU:1\r\nTarget_Network/RNN_Layer/rnn/while/rnn/basic_lstm_cell/split: (Split): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.799498: I tensorflow/core/common_runtime/placer.cc:874] Target_Network/RNN_Layer/rnn/while/rnn/basic_lstm_cell/split: (Split)/job:localhost/replica:0/task:0/device:GPU:1\r\nTarget_Network/RNN_Layer/rnn/while/rnn/basic_lstm_cell/Sigmoid_2: (Sigmoid): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.799513: I tensorflow/core/common_runtime/placer.cc:874] Target_Network/RNN_Layer/rnn/while/rnn/basic_lstm_cell/Sigmoid_2: (Sigmoid)/job:localhost/replica:0/task:0/device:GPU:1\r\nTarget_Network/RNN_Layer/rnn/while/rnn/basic_lstm_cell/Tanh: (Tanh): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.799529: I tensorflow/core/common_runtime/placer.cc:874] Target_Network/RNN_Layer/rnn/while/rnn/basic_lstm_cell/Tanh: (Tanh)/job:localhost/replica:0/task:0/device:GPU:1\r\nTarget_Network/RNN_Layer/rnn/while/rnn/basic_lstm_cell/Sigmoid_1: (Sigmoid): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.799544: I tensorflow/core/common_runtime/placer.cc:874] Target_Network/RNN_Layer/rnn/while/rnn/basic_lstm_cell/Sigmoid_1: (Sigmoid)/job:localhost/replica:0/task:0/device:GPU:1\r\nTarget_Network/RNN_Layer/rnn/while/rnn/basic_lstm_cell/mul_1: (Mul): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.799559: I tensorflow/core/common_runtime/placer.cc:874] Target_Network/RNN_Layer/rnn/while/rnn/basic_lstm_cell/mul_1: (Mul)/job:localhost/replica:0/task:0/device:GPU:1\r\nTarget_Network/RNN_Layer/rnn/while/rnn/basic_lstm_cell/add: (Add): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.799574: I tensorflow/core/common_runtime/placer.cc:874] Target_Network/RNN_Layer/rnn/while/rnn/basic_lstm_cell/add: (Add)/job:localhost/replica:0/task:0/device:GPU:1\r\nTarget_Network/RNN_Layer/rnn/while/rnn/basic_lstm_cell/Sigmoid: (Sigmoid): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.799588: I tensorflow/core/common_runtime/placer.cc:874] Target_Network/RNN_Layer/rnn/while/rnn/basic_lstm_cell/Sigmoid: (Sigmoid)/job:localhost/replica:0/task:0/device:GPU:1\r\nTarget_Network/RNN_Layer/rnn/while/rnn/basic_lstm_cell/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.799602: I tensorflow/core/common_runtime/placer.cc:874] Target_Network/RNN_Layer/rnn/while/rnn/basic_lstm_cell/mul: (Mul)/job:localhost/replica:0/task:0/device:GPU:1\r\nTarget_Network/RNN_Layer/rnn/while/rnn/basic_lstm_cell/add_1: (Add): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.799617: I tensorflow/core/common_runtime/placer.cc:874] Target_Network/RNN_Layer/rnn/while/rnn/basic_lstm_cell/add_1: (Add)/job:localhost/replica:0/task:0/device:GPU:1\r\nTarget_Network/RNN_Layer/rnn/while/NextIteration_2: (NextIteration): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.799633: I tensorflow/core/common_runtime/placer.cc:874] Target_Network/RNN_Layer/rnn/while/NextIteration_2: (NextIteration)/job:localhost/replica:0/task:0/device:GPU:1\r\nTarget_Network/RNN_Layer/rnn/while/rnn/basic_lstm_cell/Tanh_1: (Tanh): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.799648: I tensorflow/core/common_runtime/placer.cc:874] Target_Network/RNN_Layer/rnn/while/rnn/basic_lstm_cell/Tanh_1: (Tanh)/job:localhost/replica:0/task:0/device:GPU:1\r\nTarget_Network/RNN_Layer/rnn/while/rnn/basic_lstm_cell/mul_2: (Mul): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.799662: I tensorflow/core/common_runtime/placer.cc:874] Target_Network/RNN_Layer/rnn/while/rnn/basic_lstm_cell/mul_2: (Mul)/job:localhost/replica:0/task:0/device:GPU:1\r\nTarget_Network/RNN_Layer/rnn/while/NextIteration_3: (NextIteration): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.799677: I tensorflow/core/common_runtime/placer.cc:874] Target_Network/RNN_Layer/rnn/while/NextIteration_3: (NextIteration)/job:localhost/replica:0/task:0/device:GPU:1\r\nTarget_Network/RNN_Layer/rnn/while/TensorArrayWrite/TensorArrayWriteV3: (TensorArrayWriteV3): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.799690: I tensorflow/core/common_runtime/placer.cc:874] Target_Network/RNN_Layer/rnn/while/TensorArrayWrite/TensorArrayWriteV3: (TensorArrayWriteV3)/job:localhost/replica:0/task:0/device:GPU:1\r\nTarget_Network/RNN_Layer/rnn/while/NextIteration_1: (NextIteration): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.799705: I tensorflow/core/common_runtime/placer.cc:874] Target_Network/RNN_Layer/rnn/while/NextIteration_1: (NextIteration)/job:localhost/replica:0/task:0/device:GPU:1\r\nAdam/epsilon: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.799725: I tensorflow/core/common_runtime/placer.cc:874] Adam/epsilon: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nAdam/beta2: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.799743: I tensorflow/core/common_runtime/placer.cc:874] Adam/beta2: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nAdam/beta1: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.799760: I tensorflow/core/common_runtime/placer.cc:874] Adam/beta1: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nAdam/learning_rate: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.799777: I tensorflow/core/common_runtime/placer.cc:874] Adam/learning_rate: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/Final_Dense_Connected_Layer/Action_Output_Layer/bias/Adam_1/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.799791: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/Final_Dense_Connected_Layer/Action_Output_Layer/bias/Adam_1/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/Final_Dense_Connected_Layer/Action_Output_Layer/bias/Adam/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.799806: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/Final_Dense_Connected_Layer/Action_Output_Layer/bias/Adam/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/Final_Dense_Connected_Layer/Action_Output_Layer/kernel/Adam_1/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.799821: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/Final_Dense_Connected_Layer/Action_Output_Layer/kernel/Adam_1/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/Final_Dense_Connected_Layer/Action_Output_Layer/kernel/Adam/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.799836: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/Final_Dense_Connected_Layer/Action_Output_Layer/kernel/Adam/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/RNN_Layer/rnn/basic_lstm_cell/bias/Adam_1/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.799851: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/RNN_Layer/rnn/basic_lstm_cell/bias/Adam_1/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/RNN_Layer/rnn/basic_lstm_cell/bias/Adam/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.799866: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/RNN_Layer/rnn/basic_lstm_cell/bias/Adam/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/RNN_Layer/rnn/basic_lstm_cell/kernel/Adam_1/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.799881: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/RNN_Layer/rnn/basic_lstm_cell/kernel/Adam_1/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/RNN_Layer/rnn/basic_lstm_cell/kernel/Adam/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.799895: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/RNN_Layer/rnn/basic_lstm_cell/kernel/Adam/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/First_Fully_Connected/Dense_Layer_1_input_2/bias/Adam_1/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.799910: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/First_Fully_Connected/Dense_Layer_1_input_2/bias/Adam_1/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/First_Fully_Connected/Dense_Layer_1_input_2/bias/Adam/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.799925: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/First_Fully_Connected/Dense_Layer_1_input_2/bias/Adam/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/First_Fully_Connected/Dense_Layer_1_input_2/kernel/Adam_1/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.799940: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/First_Fully_Connected/Dense_Layer_1_input_2/kernel/Adam_1/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/First_Fully_Connected/Dense_Layer_1_input_2/kernel/Adam/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.799954: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/First_Fully_Connected/Dense_Layer_1_input_2/kernel/Adam/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/First_Fully_Connected/Dense_Layer_1_input_1/bias/Adam_1/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.799969: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/First_Fully_Connected/Dense_Layer_1_input_1/bias/Adam_1/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/First_Fully_Connected/Dense_Layer_1_input_1/bias/Adam/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.799984: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/First_Fully_Connected/Dense_Layer_1_input_1/bias/Adam/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/First_Fully_Connected/Dense_Layer_1_input_1/kernel/Adam_1/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.799999: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/First_Fully_Connected/Dense_Layer_1_input_1/kernel/Adam_1/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/First_Fully_Connected/Dense_Layer_1_input_1/kernel/Adam/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800014: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/First_Fully_Connected/Dense_Layer_1_input_1/kernel/Adam/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/conv5/Convolution_Layer_5_input_2_Biases/Adam_1/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800029: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/conv5/Convolution_Layer_5_input_2_Biases/Adam_1/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/conv5/Convolution_Layer_5_input_2_Biases/Adam/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800044: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/conv5/Convolution_Layer_5_input_2_Biases/Adam/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/conv5/Convolution_Layer_5_input_2/Adam_1/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800059: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/conv5/Convolution_Layer_5_input_2/Adam_1/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/conv5/Convolution_Layer_5_input_2/Adam/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800073: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/conv5/Convolution_Layer_5_input_2/Adam/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/conv5/Convolution_Layer_5_input_1_Biases/Adam_1/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800088: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/conv5/Convolution_Layer_5_input_1_Biases/Adam_1/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/conv5/Convolution_Layer_5_input_1_Biases/Adam/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800102: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/conv5/Convolution_Layer_5_input_1_Biases/Adam/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/conv5/Convolution_Layer_5_input_1/Adam_1/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800117: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/conv5/Convolution_Layer_5_input_1/Adam_1/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/conv5/Convolution_Layer_5_input_1/Adam/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800132: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/conv5/Convolution_Layer_5_input_1/Adam/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/conv4/Convolution_Layer_4_input_1_Biases/Adam_1/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800147: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/conv4/Convolution_Layer_4_input_1_Biases/Adam_1/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/conv4/Convolution_Layer_4_input_1_Biases/Adam/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800161: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/conv4/Convolution_Layer_4_input_1_Biases/Adam/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/conv4/Convolution_Layer_4_input_1/Adam_1/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800175: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/conv4/Convolution_Layer_4_input_1/Adam_1/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/conv4/Convolution_Layer_4_input_1/Adam/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800190: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/conv4/Convolution_Layer_4_input_1/Adam/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/conv3/Convolution_Layer_3_input_2_Biases/Adam_1/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800205: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/conv3/Convolution_Layer_3_input_2_Biases/Adam_1/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/conv3/Convolution_Layer_3_input_2_Biases/Adam/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800220: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/conv3/Convolution_Layer_3_input_2_Biases/Adam/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/conv3/Convolution_Layer_3_input_2/Adam_1/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800234: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/conv3/Convolution_Layer_3_input_2/Adam_1/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/conv3/Convolution_Layer_3_input_2/Adam/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800249: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/conv3/Convolution_Layer_3_input_2/Adam/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/conv3/Convolution_Layer_3_input_1_Biases/Adam_1/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800263: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/conv3/Convolution_Layer_3_input_1_Biases/Adam_1/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/conv3/Convolution_Layer_3_input_1_Biases/Adam/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800278: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/conv3/Convolution_Layer_3_input_1_Biases/Adam/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/conv3/Convolution_Layer_3_input_1/Adam_1/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800293: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/conv3/Convolution_Layer_3_input_1/Adam_1/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/conv3/Convolution_Layer_3_input_1/Adam/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800307: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/conv3/Convolution_Layer_3_input_1/Adam/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/conv2/Convolution_Layer_2_input_2_Biases/Adam_1/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800321: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/conv2/Convolution_Layer_2_input_2_Biases/Adam_1/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/conv2/Convolution_Layer_2_input_2_Biases/Adam/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800337: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/conv2/Convolution_Layer_2_input_2_Biases/Adam/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/conv2/Convolution_Layer_2_input_2/Adam_1/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800351: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/conv2/Convolution_Layer_2_input_2/Adam_1/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/conv2/Convolution_Layer_2_input_2/Adam/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800365: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/conv2/Convolution_Layer_2_input_2/Adam/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/conv2/Convolution_Layer_2_input_1_Biases/Adam_1/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800380: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/conv2/Convolution_Layer_2_input_1_Biases/Adam_1/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/conv2/Convolution_Layer_2_input_1_Biases/Adam/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800395: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/conv2/Convolution_Layer_2_input_1_Biases/Adam/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/conv2/Convolution_Layer_2_input_1/Adam_1/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800411: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/conv2/Convolution_Layer_2_input_1/Adam_1/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/conv2/Convolution_Layer_2_input_1/Adam/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800425: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/conv2/Convolution_Layer_2_input_1/Adam/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/conv1/Convolution_Layer_1_input_2_Biases/Adam_1/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800439: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/conv1/Convolution_Layer_1_input_2_Biases/Adam_1/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/conv1/Convolution_Layer_1_input_2_Biases/Adam/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800454: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/conv1/Convolution_Layer_1_input_2_Biases/Adam/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/conv1/Convolution_Layer_1_input_2/Adam_1/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800468: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/conv1/Convolution_Layer_1_input_2/Adam_1/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/conv1/Convolution_Layer_1_input_2/Adam/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800483: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/conv1/Convolution_Layer_1_input_2/Adam/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/conv1/Convolution_Layer_1_input_1_Biases/Adam_1/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800497: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/conv1/Convolution_Layer_1_input_1_Biases/Adam_1/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/conv1/Convolution_Layer_1_input_1_Biases/Adam/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800512: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/conv1/Convolution_Layer_1_input_1_Biases/Adam/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/conv1/Convolution_Layer_1_input_1/Adam_1/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800526: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/conv1/Convolution_Layer_1_input_1/Adam_1/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nEval_Network/conv1/Convolution_Layer_1_input_1/Adam/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800540: I tensorflow/core/common_runtime/placer.cc:874] Eval_Network/conv1/Convolution_Layer_1_input_1/Adam/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nbeta2_power/initial_value: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800554: I tensorflow/core/common_runtime/placer.cc:874] beta2_power/initial_value: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\nbeta1_power/initial_value: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800570: I tensorflow/core/common_runtime/placer.cc:874] beta1_power/initial_value: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\ngradients/Eval_Network/conv2_1/concat_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800584: I tensorflow/core/common_runtime/placer.cc:874] gradients/Eval_Network/conv2_1/concat_grad/Shape_1: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\ngradients/Eval_Network/conv2_1/concat_grad/Shape: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800599: I tensorflow/core/common_runtime/placer.cc:874] gradients/Eval_Network/conv2_1/concat_grad/Shape: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\ngradients/Eval_Network/conv2_1/concat_grad/Rank: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800614: I tensorflow/core/common_runtime/placer.cc:874] gradients/Eval_Network/conv2_1/concat_grad/Rank: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\ngradients/Eval_Network/conv2/concat_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800628: I tensorflow/core/common_runtime/placer.cc:874] gradients/Eval_Network/conv2/concat_grad/Shape_1: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\ngradients/Eval_Network/conv2/concat_grad/Shape: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800643: I tensorflow/core/common_runtime/placer.cc:874] gradients/Eval_Network/conv2/concat_grad/Shape: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\ngradients/Eval_Network/conv2/concat_grad/Rank: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800658: I tensorflow/core/common_runtime/placer.cc:874] gradients/Eval_Network/conv2/concat_grad/Rank: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\ngradients/Eval_Network/conv4_1/concat_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800672: I tensorflow/core/common_runtime/placer.cc:874] gradients/Eval_Network/conv4_1/concat_grad/Shape_1: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\ngradients/Eval_Network/conv4_1/concat_grad/Shape: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800686: I tensorflow/core/common_runtime/placer.cc:874] gradients/Eval_Network/conv4_1/concat_grad/Shape: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\ngradients/Eval_Network/conv4_1/concat_grad/Rank: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800735: I tensorflow/core/common_runtime/placer.cc:874] gradients/Eval_Network/conv4_1/concat_grad/Rank: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\ngradients/Eval_Network/conv4/concat_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800753: I tensorflow/core/common_runtime/placer.cc:874] gradients/Eval_Network/conv4/concat_grad/Shape_1: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\ngradients/Eval_Network/conv4/concat_grad/Shape: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800803: I tensorflow/core/common_runtime/placer.cc:874] gradients/Eval_Network/conv4/concat_grad/Shape: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\ngradients/Eval_Network/conv4/concat_grad/Rank: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800818: I tensorflow/core/common_runtime/placer.cc:874] gradients/Eval_Network/conv4/concat_grad/Rank: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\ngradients/Eval_Network/conv5_1/concat_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800832: I tensorflow/core/common_runtime/placer.cc:874] gradients/Eval_Network/conv5_1/concat_grad/Shape_1: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\ngradients/Eval_Network/conv5_1/concat_grad/Shape: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800848: I tensorflow/core/common_runtime/placer.cc:874] gradients/Eval_Network/conv5_1/concat_grad/Shape: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\ngradients/Eval_Network/conv5_1/concat_grad/Rank: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800863: I tensorflow/core/common_runtime/placer.cc:874] gradients/Eval_Network/conv5_1/concat_grad/Rank: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\ngradients/Eval_Network/conv5/concat_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800897: I tensorflow/core/common_runtime/placer.cc:874] gradients/Eval_Network/conv5/concat_grad/Shape_1: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\ngradients/Eval_Network/conv5/concat_grad/Shape: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800913: I tensorflow/core/common_runtime/placer.cc:874] gradients/Eval_Network/conv5/concat_grad/Shape: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\ngradients/Eval_Network/conv5/concat_grad/Rank: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800928: I tensorflow/core/common_runtime/placer.cc:874] gradients/Eval_Network/conv5/concat_grad/Rank: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\ngradients/Eval_Network/Flatten_input_2/Reshape_grad/Shape: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800943: I tensorflow/core/common_runtime/placer.cc:874] gradients/Eval_Network/Flatten_input_2/Reshape_grad/Shape: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\ngradients/Eval_Network/Flatten_input_1/Reshape_grad/Shape: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800958: I tensorflow/core/common_runtime/placer.cc:874] gradients/Eval_Network/Flatten_input_1/Reshape_grad/Shape: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\ngradients/Eval_Network/Reshape_1_grad/Shape: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800973: I tensorflow/core/common_runtime/placer.cc:874] gradients/Eval_Network/Reshape_1_grad/Shape: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\ngradients/Eval_Network/Reshape_grad/Shape: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.800987: I tensorflow/core/common_runtime/placer.cc:874] gradients/Eval_Network/Reshape_grad/Shape: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\ngradients/Eval_Network/concat_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.801002: I tensorflow/core/common_runtime/placer.cc:874] gradients/Eval_Network/concat_grad/Shape_1: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\ngradients/Eval_Network/concat_grad/Shape: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n2018-03-31 21:43:19.801017: I tensorflow/core/common_runtime/placer.cc:874] gradients/Eval_Network/concat_grad/Shape: (Const)/job:localhost/replica:0/task:0/device:GPU:1\r\ngradients/Eval_Network/concat_grad/Rank: (Const): /job:localhost/replica:0/task:0/device:GPU:1\r\n", "comments": ["This typically happens when one sets `log_device_placement` in the `tf.ConfigProto` provided when creating the `tf.Session` (see [documentation for `tf.Session`](https://www.tensorflow.org/api_docs/python/tf/Session#class_session)). This is intended behavior as enabling that logging is telling TensorFlow to spit out information on which device each operation in the graph is executing on.\r\n\r\nClosing this out since it isn't a bug or feature request. Support questions are generally better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 18145, "title": "Maxpoolwithargmax cpu", "body": "#6035", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "thx for your work guys, having Maxpoolwithargmax work with CPU is awesome!", "@nio1814 Thanks for the contribution!", "Great Me too was having a lot of problems"]}, {"number": 18144, "title": "tf.data.apply does not return output shape", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.8.0-dev20180328\r\n- **Python version**:  3.6\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\nI am using tf.data API to transform my dataset. Following is the code Transform:\r\n\r\n### Transformation Function\r\n```python\r\n\r\nwin_len = 32\r\nlook_back = 11\r\npad = int(look_back/2)\r\nslen = 1344\r\nsh = int(slen + 2*pad)\r\nbatch_size = 840\r\n\r\ndef _parse_function(example_proto):\r\n    keys_to_features = {'mix':tf.FixedLenFeature((slen), tf.float32),\r\n                        'pure':tf.FixedLenFeature((slen), tf.float32)}\r\n    parsed_features = tf.parse_single_example(example_proto, keys_to_features)\r\n    return {\"pure\":parsed_features['pure'], \"mix\":parsed_features['mix']}\r\ndef _dict_pad(sig):\r\n    return {'pure':tf.pad(sig['pure'],[[pad,pad]]),'mix':tf.pad(sig['mix'],[[pad,pad]])}\r\ndef _dict_slide_reduce(sig,sh):\r\n    return sig.apply(sliding_window_batch(window_size=look_back,stride=1))\r\ndef _pure_reduce(sig):\r\n    return {'pure': sig['pure'][:,pad],'mix':sig['mix'] }\r\n```\r\n\r\n### Data Reading:\r\n\r\n```python\r\ntrain_data = tf.data.TFRecordDataset(train_files)\\\r\n            .map(_parse_function)\\\r\n            .map(_dict_pad)\\\r\n            .apply(unbatch())\\\r\n            .apply((group_by_window(key_func= lambda sig : 1,reduce_func = lambda key,sig : _dict_slide_reduce(sig,sh),window_size=sh)))\\\r\n            .batch(win_len)\\\r\n            .map(_pure_reduce)\\\r\n            .batch(batch_size) \r\ntrain_data = train_data.prefetch(100)\r\nprint(train_data.output_shapes)\r\niterator_train = train_data.make_one_shot_iterator()\r\ntr_sig = iterator_train.get_next()\r\n```\r\n\r\noutput of `print(train_data.output_shapes) ` is following:\r\n\r\n`{'pure': TensorShape([Dimension(None), Dimension(None)]), 'mix': TensorShape([Dimension(None), Dimension(None), Dimension(None)])}\r\n`\r\n\r\noutput shape after each transformation:\r\n```python\r\ntrain_data = tf.data.TFRecordDataset(train_files)\r\ntrain_data = train_data.map(_parse_function)\r\nprint(train_data.output_shapes)\r\n# {'pure': TensorShape([Dimension(1344)]), 'mix': TensorShape([Dimension(1344)])}\r\ntrain_data = train_data.map(_dict_pad)\r\nprint(train_data.output_shapes)\r\n# {'pure': TensorShape([Dimension(1354)]), 'mix': TensorShape([Dimension(1354)])}\r\ntrain_data = train_data.apply(tf.contrib.data.unbatch())\r\nprint(train_data.output_shapes)\r\n# {'pure': TensorShape([]), 'mix': TensorShape([])}\r\ntrain_data = train_data.apply((group_by_window(key_func= lambda sig : 1,reduce_func = lambda key,sig : _dict_slide_reduce(sig,sh),window_size=32)))\r\nprint(train_data.output_shapes)\r\n# {'pure': TensorShape([Dimension(None)]), 'mix': TensorShape([Dimension(None)])}\r\ntrain_data = train_data.batch(win_len)\r\nprint(train_data.output_shapes)\r\n# {'pure': TensorShape([Dimension(None), Dimension(None)]), 'mix': TensorShape([Dimension(None), Dimension(None)])}\r\ntrain_data = train_data.map(_pure_reduce)\r\nprint(train_data.output_shapes)\r\n# {'pure': TensorShape([Dimension(None)]), 'mix': TensorShape([Dimension(None), Dimension(None)])}\r\ntrain_data = train_data.batch(batch_size)\r\nprint(train_data.output_shapes)\r\n# {'pure': TensorShape([Dimension(None), Dimension(None)]), 'mix': TensorShape([Dimension(None), Dimension(None), Dimension(None)])}\r\n```\r\n\r\nThe expected output is `{'mix': TensorShape([Dimension(None), Dimension(32), Dimension(11)]), 'pure': TensorShape([Dimension(None), Dimension(32)])}\r\n`\r\n```python\r\niterator_train = train_data.make_one_shot_iterator()\r\ntr_sig = iterator_train.get_next()\r\nwith tf.Session() as sess:\r\n    data = sess.run(tr_sig)\r\n    print(data['mix'].shape,data['pure'].shape)\r\n# (840, 32, 11) (840, 32)\r\n```\r\nBecause of this issue I am getting following error in  dynamic_rnn:\r\n\r\n`\r\nValueError: Input size (depth of inputs) must be accessible via shape inference, but saw value None.\r\n`", "comments": ["@nthakor Hi, if #17480 is merged, would you like to specify the output shape by youself?", "@facaiy Sure. That will resolve my issue. ", "#17480 has been merged, I hope it could help you :)"]}, {"number": 18143, "title": "contribute: Add the API to set the session configuration when load the model in the JAVA", "body": "System information\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): NO\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu\r\nTensorFlow installed from (source or binary): source\r\nTensorFlow version (use command below): Master\r\nPython version: 3.6\r\nBazel version (if compiling from source): N/A\r\nGCC/Compiler version (if compiling from source): N/A\r\nCUDA/cuDNN version: None\r\nGPU model and memory: None\r\nExact command to reproduce: Run the snippet below.\r\n\r\nIn the Java, only can set the run options and can't modify the session configuration when load the model. Sometimes I want to set the session strategies, ex. Set the session threads, op threads number, and so on.\r\n\r\nI write the code to add new interface in the JAVA, and also add the JNI corresponding method.\r\nI have been submit the code in the github:\r\nURL:\r\nhttps://github.com/raintung/TensorflowPatch\r\n\r\nI will use it in my company project, I think it should not conflict the Tensorflow license, I still contribute it.\r\n \r\n\r\n\r\n\r\n\r\n", "comments": ["Nagging Assignee @skye: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "If I understand correctly, you're proposing that a variant of `SavedModelBundle.load()` be added that takes in the session configuration (e.g., a serialized `ConfigProto` protocol buffer)?\r\n\r\nSure, that sounds perfectly reasonable.\r\nMarking this as contributions welcome, feel free to send in a pull request. And I would suggest that we don't add a new native method, simply modify the existing one in place."]}, {"number": 18142, "title": "+1", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 18141, "title": "Tensorflow model save using c++", "body": "I can't find any c++ api to save a tensorflow model. Is there any way to implement this?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce"]}, {"number": 18140, "title": "Fix tensor naming for tf.metrics.root_mean_squared_error", "body": "Tensors returned by `tf.metrics.root_mean_squared_error` function do not reflect the name passed in the function parameters. This patch fixes that.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Seems to be failing on this test:\r\n```python\r\nclass RootMeanSquaredErrorTest(test.TestCase):\r\n  def testVars(self):\r\n    metrics.root_mean_squared_error(\r\n        predictions=array_ops.ones((10, 1)), labels=array_ops.ones((10, 1)))\r\n    _assert_metric_variables(\r\n        self,\r\n        ('root_mean_squared_error/count:0', 'root_mean_squared_error/total:0'))\r\n\r\n  def _assert_metric_variables(test_case, expected):\r\n    test_case.assertEquals(\r\n        set(expected), set(v.name for v in variables.local_variables()))\r\n    test_case.assertEquals(\r\n        set(expected),\r\n        set(v.name for v in ops.get_collection(ops.GraphKeys.METRIC_VARIABLES)))\r\n```", "@markostam you also need to address some linter errors.", "@markostam I think you just need to update the test to look for the updated variable name:\r\n```\r\nAssertionError: Items in the first set but not the second:\r\n'root_mean_squared_error/total:0'\r\n'root_mean_squared_error/count:0'\r\nItems in the second set but not the first:\r\n'mean_squared_error/count:0'\r\n'mean_squared_error/total:0'\r\n```", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "Still working on it just havent had time lately. ", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "still here. conference last week, hoping to get some time soon.", "It has been 17 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@markostam could you take a look at the errors?\r\n\r\n```\r\nFAIL: testVars (__main__.StreamingRootMeanSquaredErrorTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/b/s/w/ir/run/bazel-out/k8-py3-opt/bin/tensorflow/contrib/metrics/metric_ops_test.runfiles/org_tensorflow/tensorflow/contrib/metrics/python/ops/metric_ops_test.py\", line 5445, in testVars\r\n    ('root_mean_squared_error/count:0', 'root_mean_squared_error/total:0'))\r\n  File \"/b/s/w/ir/run/bazel-out/k8-py3-opt/bin/tensorflow/contrib/metrics/metric_ops_test.runfiles/org_tensorflow/tensorflow/contrib/metrics/python/ops/metric_ops_test.py\", line 153, in _assert_metric_variables\r\n    set(expected), set(v.name for v in variables.local_variables()))\r\nAssertionError: Items in the first set but not the second:\r\n'root_mean_squared_error/count:0'\r\n'root_mean_squared_error/total:0'\r\nItems in the second set but not the first:\r\n'mean_squared_error/count:0'\r\n'mean_squared_error/total:0'\r\n----------------------------------------------------------------------\r\n```", "It has been 28 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 18139, "title": "Error in restoring the tensorflow model and metagraph while using seq2seq and attention model.", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: pip3 install tensorflow-gpu==1.0.0\r\n- **TensorFlow version (use command below)**: 1.0.0\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**:No\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: 8/7\r\n- **GPU model and memory**:GTX 1060X and 6GB\r\n- **Exact command to reproduce**:\r\n\r\nI am using seq2seq model with attention mechanism for a chatbot as given in the [link](https://github.com/Currie32/Chatbot-from-Movie-Dialogue/blob/master/Chatbot_Attention.ipynb), I am able to train the model and save it but when I am trying to restore the model I am getting an error:\r\n\r\nCode for loading the model:\r\n```\r\nwith tf.Session() as sess:\r\n        #try:\r\n        saver = tf.train.import_meta_graph('/media/saurabh/New Volume/nlp_ucf/project/model/model_0.ckpt.meta')\r\n        saver.restore(sess, '/media/saurabh/New Volume/nlp_ucf/project/model/model_0.ckpt.data-00000-of-00001')\r\n```\r\nThe error I am getting is\r\n\r\n```\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0)\r\nTraceback (most recent call last):\r\n  File \"start_something.py\", line 254, in <module>\r\n    saver = tf.train.import_meta_graph('/media/saurabh/New Volume/nlp_ucf/project/model/model_0.ckpt.meta')\r\n  File \"/home/saurabh/tfenv3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1577, in import_meta_graph\r\n    **kwargs)\r\n  File \"/home/saurabh/tfenv3/lib/python3.5/site-packages/tensorflow/python/framework/meta_graph.py\", line 498, in import_scoped_meta_graph\r\n    producer_op_list=producer_op_list)\r\n  File \"/home/saurabh/tfenv3/lib/python3.5/site-packages/tensorflow/python/framework/importer.py\", line 259, in import_graph_def\r\n    raise ValueError('No op named %s in defined operations.' % node.op)\r\nValueError: No op named attn_add_fun_f32f32f32 in defined operations.\r\n```\r\nI am even unable to understand the error. Does anyone have any clue about it? I was able to find one more person with a similar issue on StackOverflow but couldn't understand what exactly is the solution?", "comments": ["@sfarkya Is this issue solved?", "Euguene can you take a look?\r\n\r\nThe library code is pretty old, but relevant line seems to be  and I was able to dig it out at https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/contrib/seq2seq/python/ops/attention_decoder_fn.py\r\n\r\nMy understand was that calling the function should have added it to the graph at train time, so it puzzling a bit that on restore it cant find it.\r\n\r\n", "Try using a newer version of tensorflow for both saving and loading the graph", "Nagging Assignee @ebrevdo: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 18138, "title": "Distribute sp", "body": "", "comments": []}, {"number": 18137, "title": "Update release notes", "body": "Updating release notes:\r\n- Adding a note about tensorrt.\r\n- Adding a note about supporting Cuda >= 8.0 and cuDNN >= 6.0 starting with 1.8 release.", "comments": ["Looks good to me.  Thanks @annarev."]}, {"number": 18136, "title": "Tenorflow gpu problems", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:source\r\n- **TensorFlow version (use command below)**:sudo pip3 install --upgrade tensorflow-gpu==1.4\r\n- **Python version**: python3\r\n- **Bazel version (if compiling from source)**:no\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nTraceback (most recent call last):\r\n  File \"utils.py\", line 15, in <module>\r\n    import tensorflow as tf\r\n  File \"/home/sagar/miniconda2/lib/python2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/home/sagar/miniconda2/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/sagar/miniconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/sagar/miniconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/sagar/miniconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/sagar/miniconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n", "comments": ["I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you.", "There is the same problem in my Manjaro too. I can find libcublas.so.9.1. I guess I should install cuda9.0 instead of 9.1? But it seems yaourt command cannot install older version...... ", "There are only two following possibilities \r\n\r\n1.  You have used `pip install` or ` conda install` of TensorFlow compiled for Cuda version 9.0 ; while you do not have that on your system. \r\n\r\n2. You have Cuda version 9.0 version but it is not added to the system path.\r\n\r\nFor the first possibility you have to install TensorFlow compiled for the Cuda version on your system.\r\n\r\nFor the second possibility you have to add CUDA to your system paths.", "Problem is solved. I reinstalled everthing including tensorflow and cuda 9.0", "@SAGGSOC close the issue if it is solved."]}, {"number": 18135, "title": "updated installation instructions for Tensowflow-TensorRT integration", "body": "@aaroey @cliffwoolley @gunan ", "comments": ["/cc @tfboyd ", "I will pick this up.  The markdown has a lot of HTML that should be removed.  I will prioritize getting to this in the next 48 hours and figuring out how to get it published after it is submitted.  ", "Thanks @tfboyd.  There was already similar HTML embedded into the MD of that page (well, except the br's), so we figured it was okay.  If you'd like to reformat to be a bit more pure on the MD, that's understandable.  Let us know if we can help.", "Mind resolving the conflicts? (Should we merge this into the master branch instead?)", "@jhseu  I changed to master and I think I fixed the merge conflict.  My github/git skills are not strong in this area.  There was not a conflict with the branch and I think I realize what they is kind of a mess.  Makes more sense to push it to master than to the branch if we really want that but 1.8 is very near.  ", "Ubuntu CC seems stuck or was stuck on the last run."]}, {"number": 18134, "title": "Merge pull request #1 from tensorflow/master", "body": "Sync Base Changes", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "It looks like this was possibly created by mistake. Closing."]}, {"number": 18133, "title": "Segmentation fault with TF 1.7 built from source with MKL support", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Amazon Linux (Linux version 4.9.85-38.58.amzn1.x86_64)\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.7.0\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.11.1\r\n- **GCC/Compiler version (if compiling from source)**: 5.3.0\r\n- **CUDA/cuDNN version**: None\r\n- **GPU model and memory**: None\r\n- **Exact command to reproduce**:\r\n```\r\ngit clone -b r1.7 https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow\r\n./configure\r\nbazel build --jobs $(nproc) --config=opt --config=mkl --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" --copt=\"-DEIGEN_USE_VML\" //tensorflow/tools/pip_package:build_pip_package\r\n\r\ngit clone https://github.com/tensorflow/models.git\r\ncd models/tutorials/image/cifar10/\r\npython cifar10_train.py\r\n```\r\n**Result:**\r\n```\r\n[ec2-user@ip-xxx-xx-xx-xxx cifar10]$ python cifar10_train.py\r\nFilling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\r\nSegmentation fault\r\n```\r\n\r\n### Describe the problem\r\nI tried to compile TF 1.7 from source with MKL and AVX512 support on my AWS EC2 C5.18xlarge instance. \r\nThis Segmentation fault appeared when running cifar10_train.py. This error appeared with TF1.5 as well and TF 1.6 fixed it, but now it appears again with 1.7. \r\nI have tried to compile TF without AVX512 with following commands and this error didn't appear again:\r\n```\r\nbazel build --jobs $(nproc) --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" --config=mkl --copt=-msse4.1 --copt=-msse4.2 --copt=-mavx2 --copt=-mavx --copt=-mfma --copt=\"-DEIGEN_USE_VML\" //tensorflow/tools/pip_package:build_pip_package\r\n```\r\nSo I'm guessing this issue is related with AVX512.\r\n", "comments": ["Can you do backtrace on segfault?\r\n\r\n```\r\nulimit -Sc unlimited\r\n<crashy script>\r\ngdb python\r\ncore core\r\nbt\r\n```", "@yaroslavvb Thank you for your reply, this is what I got with your commands, but I'm not sure whether this is what you want for traceback\r\n```\r\n[ec2-user@ip cifar10]$ python cifar10_train.py\r\nFilling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\r\nTraceback (most recent call last):\r\n  File \"cifar10_train.py\", line 127, in <module>\r\n    tf.app.run()\r\n  File \"/usr/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 126, in run\r\n    _sys.exit(main(argv))\r\n  File \"cifar10_train.py\", line 123, in main\r\n    train()\r\n  File \"cifar10_train.py\", line 115, in train\r\n    mon_sess.run(train_op)\r\n  File \"/usr/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 546, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 1022, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 1113, in run\r\n    raise six.reraise(*original_exc_info)\r\n  File \"/usr/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 1098, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/usr/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 1170, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 950, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/usr/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 905, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1140, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1321, in _do_run\r\n    run_metadata)\r\n  File \"/usr/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1340, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Nan in summary histogram for: conv2/conv2/conv2/activations\r\n\t [[Node: conv2/conv2/conv2/activations = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](conv2/conv2/conv2/activations/tag, conv2/conv2)]]\r\n\r\nCaused by op u'conv2/conv2/conv2/activations', defined at:\r\n  File \"cifar10_train.py\", line 127, in <module>\r\n    tf.app.run()\r\n  File \"/usr/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 126, in run\r\n    _sys.exit(main(argv))\r\n  File \"cifar10_train.py\", line 123, in main\r\n    train()\r\n  File \"cifar10_train.py\", line 72, in train\r\n    logits = cifar10.inference(images)\r\n  File \"/home/ec2-user/src/models/tutorials/image/cifar10/cifar10.py\", line 231, in inference\r\n    _activation_summary(conv2)\r\n  File \"/home/ec2-user/src/models/tutorials/image/cifar10/cifar10.py\", line 93, in _activation_summary\r\n    tf.summary.histogram(tensor_name + '/activations', x)\r\n  File \"/usr/lib/python2.7/dist-packages/tensorflow/python/summary/summary.py\", line 196, in histogram\r\n    tag=tag, values=values, name=scope)\r\n  File \"/usr/lib/python2.7/dist-packages/tensorflow/python/ops/gen_logging_ops.py\", line 282, in _histogram_summary\r\n    \"HistogramSummary\", tag=tag, values=values, name=name)\r\n  File \"/usr/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3290, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1654, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): Nan in summary histogram for: conv2/conv2/conv2/activations\r\n\t [[Node: conv2/conv2/conv2/activations = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](conv2/conv2/conv2/activations/tag, conv2/conv2)]]\r\n\r\n[ec2-user@ip cifar10]$ gdb python\r\nGNU gdb (GDB) Amazon Linux (7.6.1-64.33.amzn1)\r\nCopyright (C) 2013 Free Software Foundation, Inc.\r\nLicense GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\r\nThis is free software: you are free to change and redistribute it.\r\nThere is NO WARRANTY, to the extent permitted by law.  Type \"show copying\"\r\nand \"show warranty\" for details.\r\nThis GDB was configured as \"x86_64-amazon-linux-gnu\".\r\nFor bug reporting instructions, please see:\r\n<http://www.gnu.org/software/gdb/bugs/>...\r\nReading symbols from /usr/bin/python2.7...(no debugging symbols found)...done.\r\nMissing separate debuginfos, use: debuginfo-install python26-2.6.9-2.89.amzn1.x86_64 python27-2.7.13-2.122.amzn1.x86_64 python34-3.4.7-1.38.amzn1.x86_64\r\n(gdb) core core\r\n/home/ec2-user/src/models/tutorials/image/cifar10/core: No such file or directory.\r\n(gdb) bt\r\nNo stack.\r\n(gdb)\r\n```\r\n", "It looks like your error is a NaN rather than segmentation fault, this is why there's no core file", "@yaroslavvb Thanks! I agree with you, but why this happened only when I compiled with AVX512 support? Do you have any idea about how to solve it?", "I've seen segfaults happening when applying avx instruction to memory that has not been 64-byte aligned. The first step would be to obtain core file and look at backtrace.", "@zhiz21 Can you check if https://github.com/tensorflow/tensorflow/pull/18411 addresses your issue?", "@jbobba @zhiz21 Not sure if #18411 will resolve this issue, because #18411 is not related to MKL.", "@zhiz21 can you please obtain the core file?\r\nThanks.", "Related to issue of: https://github.com/tensorflow/tensorflow/issues/17273 \r\nWe are working on this AVX512 issue with Eigen. As a workaround, please configure AND compile with AVX2 only (i.e. use -march=broadwell instead of -march=native).  \r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@zhiz21 Can you please try compiling wih -march=broadwell instead of -march=native and see if problems go away? FYI we are working on fixing Eigen AVX512 related issues. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 44 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 18132, "title": "Build error using bazel for r1.7 -> gen_gen_stats_ops_py_py_wrappers_cc: undefined symbol: _ZN6google8protobuf8internal26fixed_address_empty_stringB5cxx11E", "body": "Tried to build r1.7 from source for CPU only\r\nBuild logs below\r\n###############\r\nINFO: From Compiling external/kafka/src/rdkafka_op.c:\r\nexternal/kafka/src/rdkafka_op.c: In function 'rd_kafka_op_destroy':\r\nexternal/kafka/src/rdkafka_op.c:299:35: warning: variable 'res' set but not used [-Wunused-but-set-variable]\r\n                 rd_kafka_op_res_t res;\r\n                                   ^~~\r\nERROR: /tmp/tensorflow/tensorflow/contrib/tensor_forest/BUILD:324:1: Executing genrule //tensorflow/contrib/tensor_forest:gen_stats_ops_py_pygenrule failed (Exit 127)\r\nbazel-out/host/bin/tensorflow/contrib/tensor_forest/gen_gen_stats_ops_py_py_wrappers_cc: symbol lookup error: bazel-out/host/bin/tensorflow/contrib/tensor_forest/gen_gen_stats_ops_py_py_wrappers_cc: undefined symbol: _ZN6google8protobuf8internal26fixed_address_empty_stringB5cxx11E\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n\r\n####################\r\nUbuntu 16.10\r\nBazel\r\nBuild label: 0.11.1\r\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "I guess it is still is. Since I have encounter this problem under ubuntu16.04", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 36 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to the lack of full reproduction instructions. \r\nPlease resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new). Please provide all the information it asks. Thank you.\r\n"]}, {"number": 18131, "title": "What's the min version of glibc required for building TF from source?", "body": "I want to compile TF from source, what's the min version of glibc I can use for building?", "comments": ["Current tests run on Ubuntu 16.04, so glibc version 2.23 is going to be the most well tested.\r\nanything before 2.19 (Ubuntu 14.04) is likely to be not well tested.", "Nagging Assignee @cy89: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 47 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 63 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 78 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 93 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 108 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 18130, "title": "1.7.0 cherry-pick request: Prevent warning every time someone imports contrib.learn.datasets.base", "body": "Everything in contrib/learn/python/learn/datasets/base.py has been deprecated. One of the function in there is a decorator, retry. Because another function in that file is decorated with retry, the function is called upon import, which prints a warning.\r\n\r\nI have fixed this by adding a private function, _internal_retry, which is used internally, and redefining retry to simply call this. That way, using retry in user-code will still print the deprecated warning, but it's not printed upon every import.\r\n\r\nI also cleaned up the docstrings slightly.\r\n\r\nPiperOrigin-RevId: 190626717", "comments": []}, {"number": 18129, "title": "AttributeError: module 'tensorflow.contrib.slim.python.slim.nets.inception' has no attribute 'inception_v4_arg_scope'", "body": "when i use this code in my code , i get that error but for v1-3 don't problem.\r\n```\r\nwith slim.arg_scope(inception.inception_v4_arg_scope()):\r\n    \t_, end_points = inception.inception_v4(x_input, num_classes=num_classes, is_training=False)\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "that's works when i changed from `tensorflow.contrib.slim.nets import inception` to  `from nets import inception` ,\r\nMy configure : \r\nubuntu 16.04\r\nTensorflow 1.5 binary\r\ncuda 9.0 / cudnn 7.1\r\nGPU 1080 / ram 16", "It looks like this issue has been resolved, so I'm closing the issue.\r\n\r\nIf you have any more problems, please ask a question on [Stack Overflow](http://stackoverflow.com/questions/tagged/tensorflow), which is more suited to problems that are not bugs or feature requests. Thanks!"]}, {"number": 18128, "title": "fix cmake windows python 2.7 test fail", "body": "fix cmake windows python 2.7 unit test fail", "comments": []}]