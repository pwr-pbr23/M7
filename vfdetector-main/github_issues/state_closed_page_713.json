[{"number": 32190, "title": "[r1.15 Cherrypick] Make writing saved_model.pb atomic so there's a way to check that a SavedModel is complete", "body": "Make writing saved_model.pb atomic so there's a way to check that a SavedModel is complete (i.e. saved_model.pb exists).", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32190) for more info**.\n\n<!-- need_sender_cla -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32190) for more info**.\n\n<!-- cla_yes -->"]}, {"number": 32189, "title": "[r2.0:Cherrypick]Negating cosine similarity loss so that value gets minimized while training without needed a wrapper function.", "body": "PiperOrigin-RevId: 266977539Automated rollback of commit c3fb862245d7d3daa318f3e04de25fed83d29993\r\n\r\nNegating cosine similarity loss so that value gets minimized while training without needed a wrapper function.\r\n", "comments": []}, {"number": 32188, "title": "Efficient band matrix multiplication", "body": "Tensorflow appears to lack an efficient way to do a multiplication by band matrix.\r\n\r\nBy this I mean calculating \r\n\r\nC[i,j] = sum_k A[i,j+k] B[i, k] \r\n\r\nFor example, if B is\r\n\r\n[ b00 b01 b02 .. b07]\r\n[ b10 b11 b12 .. b17]\r\n[ b20 b21 b22 .. b27]\r\n\r\nthis is equivalent to multiplying A by\r\n\r\n[ b00  0   0 ...\r\n[ b10 b01 0 ...\r\n[ b20 b11 b02 ...\r\n[ 0     b21 b12 ... ]\r\n[ 0     0     b22 ... ]\r\n...\r\n[0   0 ...    b27]\r\n\r\nThis arises as part of the implementation of keras.layers.LocallyConnected1D(), which provides two possible implementations for it; both implementations are suboptimal. One creates a tensorflow op for every column, which means a lot of framework overhead, and the other expands B to the full form, which is inefficient and also seems to incur some overhead. (If B is 512x16, it should take one tensorflow op with 512x16 FMAs; implementation 1 would do 512 ops with 16 FMAs each;  implementation 2 would perform two ops, one with 512x529 float multiplications, and one with 512x529 FMAs less however much is saved internally by sparse_matmul().)\r\n\r\nI tried to replace some dense layers with LocallyConnected1D() layers in @tensor2tensor transformer. It would not load at all with implementation 1 (it was still trying to build the graph after 10 minutes) and it ran significantly slower than with dense() (despite doing fewer FMAs) with implementation 2. \r\n\r\nThe convolution would be easy to implement if one had direct access to tensor strides. (You simply set the second-to-last stride of A to stride[-2]+stride[-1] and do a regular matrix multiplication call.) And there is some internal support for it in the form of methods called Stream::ThenBlasGbmv() in stream_executor/stream.cc. But I don't see how to change strides from python, and ThenBlasGbmv() is never used in 1.x master.", "comments": ["I did some profiling of implementation 2 and it looks like it is slower because it is partly done on the CPU, necessitating lots of CPU<->GPU traffic. The following snapshot\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.python.keras import backend as K\r\ntf.enable_eager_execution()\r\n\r\nwith tf.device('/gpu:0'):\r\n\tinput = tf.convert_to_tensor(np.random.random([1024,1024]),dtype=tf.float32)\r\n\tkernel = [[(np.random.random() if abs(x-y)<10 else 0) for x in range(1024)] for y in range(1024)]\r\n\tkernel = tf.convert_to_tensor(kernel, dtype=tf.float32)\r\n\toutput = tf.sparse_matmul(input, kernel, b_is_sparse=True)\r\n```\r\n\r\nfails with\r\n```\r\n2019-09-03 14:41:49.641175: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9654 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0, compute capability: 6.1)\r\n2019-09-03 14:41:49.643464: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10468 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:81:00.0, compute capability: 6.1)\r\nTraceback (most recent call last):\r\n  File \"../sparse_perf.py\", line 12, in <module>\r\n    output = tf.sparse_matmul(input, kernel, b_is_sparse=True)\r\n  File \"/home/aidev/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 9745, in sparse_mat_mul\r\n    _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n  File \"/home/aidev/.local/lib/python2.7/site-packages/six.py\", line 737, in raise_from\r\n    raise value\r\ntensorflow.python.framework.errors_impl.NotFoundError: No registered 'SparseMatMul' OpKernel for GPU devices compatible with node {{node SparseMatMul}}\r\n\t.  Registered:  device='XLA_GPU'; Tb in [DT_FLOAT, DT_BFLOAT16]; Ta in [DT_FLOAT, DT_BFLOAT16]\r\n  device='XLA_CPU'; Tb in [DT_FLOAT, DT_BFLOAT16]; Ta in [DT_FLOAT, DT_BFLOAT16]\r\n  device='XLA_CPU_JIT'; Tb in [DT_FLOAT, DT_BFLOAT16]; Ta in [DT_FLOAT, DT_BFLOAT16]\r\n  device='XLA_GPU_JIT'; Tb in [DT_FLOAT, DT_BFLOAT16]; Ta in [DT_FLOAT, DT_BFLOAT16]\r\n  device='CPU'; Ta in [DT_FLOAT]; Tb in [DT_FLOAT]\r\n  device='CPU'; Ta in [DT_BFLOAT16]; Tb in [DT_FLOAT]\r\n  device='CPU'; Ta in [DT_FLOAT]; Tb in [DT_BFLOAT16]\r\n  device='CPU'; Ta in [DT_BFLOAT16]; Tb in [DT_BFLOAT16]\r\n [Op:SparseMatMul]\r\n```\r\n", "@ekuznetsov139, Which Tensorflow version are you using.\r\nI could reproduce the issue on Colab with Tensorflow 1.14.0. Please see the [gist here](https://colab.sandbox.google.com/gist/gadagashwini/b8020ae709122ca33858f638b93065eb/untitled123.ipynb). Thanks!", "I am using master.", "Perhaps you can use soft device placement instead to solve the error message;\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.python.keras import backend as K\r\n#tf.enable_eager_execution()\r\nwith tf.device('/gpu:0'):\r\n\tinput = tf.convert_to_tensor(np.random.random([1024,1024]),dtype=tf.float32)\r\n\tkernel = [[(np.random.random() if abs(x-y)<10 else 0) for x in range(1024)] for y in range(1024)]\r\n\tkernel = tf.convert_to_tensor(kernel, dtype=tf.float32)\r\n\toutput = tf.sparse_matmul(input, kernel, b_is_sparse=True)\r\n \r\n\r\nwith tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\r\n   sess.run(tf.global_variables_initializer())\r\n   print(sess.run(output))\r\n```\r\noutput:\r\n```python\r\n[[1.9552021 2.6956174 2.7000577 ... 5.2015905 4.306609  3.1939871]\r\n [2.8889682 3.0871844 2.7817092 ... 4.8683367 4.3100066 3.0557048]\r\n [2.7225637 3.5055962 3.1739452 ... 3.5228992 3.0882854 2.1134665]\r\n ...\r\n [2.9096322 3.844394  4.0538597 ... 3.5110736 2.2325637 2.1976418]\r\n [2.8388832 4.240377  3.5652244 ... 3.7165935 3.3939593 2.4490907]\r\n [2.362702  4.143541  3.2684267 ... 3.440366  3.225929  2.0782244]]\r\n```", "Allow me to direct your attention at the title of the issue. It contains the word \"efficient\". Soft placement is not efficient. ", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32188\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32188\">No</a>\n"]}, {"number": 32187, "title": "XLA compilation fails in replica context of distribution strategy", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian GNU/Linux 9.9 (stretch)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.14.0-0-g87989f6 1.14.0\r\n- Python version: Python 3.5.3\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA Version 10.0.130\r\n- GPU model and memory: 2 x Nvidia Tesla K80\r\n\r\n**Describe the current behavior**\r\nInvoking `xla.compile` in the function passed to `strategy.experimental_run_v2` as in the code below raises the following exception (full traceback below):\r\n```\r\nValueError: XLA compiled computations cannot be nested, (operator name: replica_1/input_0)\r\n```\r\n**Describe the expected behavior**\r\nThe expectation was to get one compiled XLA cluster for each replica.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.compiler.xla import xla\r\n\r\nstrategy = tf.distribute.experimental.CentralStorageStrategy(\r\n    compute_devices=[\"/device:GPU:0\", \"/device:GPU:1\"],\r\n    parameter_device=\"/device:CPU:0\"\r\n)\r\n\r\nwith strategy.scope():\r\n    def train_step(inputs):\r\n        def step_fn(x):\r\n            w = tf.get_variable(name=\"w\", initializer=1.0)\r\n            loss = w * x\r\n            optimizer = tf.train.GradientDescentOptimizer(0.1)\r\n            train_op = optimizer.minimize(loss)\r\n            with tf.control_dependencies([train_op]):\r\n                return tf.identity(loss)\r\n        \r\n        def compiled_step_fn(x):\r\n            [out] = xla.compile(step_fn, inputs=[x])\r\n            return out\r\n\r\n        per_replica_losses = strategy.experimental_run_v2(\r\n            compiled_step_fn, args=[inputs])\r\n        sum_loss = strategy.reduce(\r\n            tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\r\n        return sum_loss\r\n    \r\n    loss = train_step(1.0)\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        for _ in range(10):\r\n            print(sess.run(loss))\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n```\r\nTraceback (most recent call last):\r\n  File \"multi_gpu_strategy_xla.py\", line 29, in <module>\r\n    loss = train_step(1.0)\r\n  File \"multi_gpu_strategy_xla.py\", line 24, in train_step\r\n    compiled_step_fn, args=[inputs])\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/distribute/distribute_lib.py\", line 511, in experimental_run_v2\r\n    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/distribute/distribute_lib.py\", line 1555, in call_for_each_replica\r\n    return self._call_for_each_replica(fn, args, kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/distribute/parameter_server_strategy.py\", line 407, in _call_for_each_replica\r\n    self._container_strategy(), self._device_map, fn, args, kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/distribute/mirrored_strategy.py\", line 195, in _call_for_each_replica\r\n    coord.join(threads)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py\", line 389, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/usr/local/lib/python3.5/dist-packages/six.py\", line 693, in reraise\r\n    raise value\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\r\n    yield\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/distribute/mirrored_strategy.py\", line 911, in run\r\n    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\r\n  File \"multi_gpu_strategy_xla.py\", line 20, in compiled_step_fn\r\n    [out] = xla.compile(step_fn, inputs=[x])\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/compiler/xla/xla.py\", line 110, in compile\r\n    return _compile_internal(computation, inputs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/compiler/xla/xla.py\", line 338, in _compile_internal\r\n    for i, x in enumerate(flat_inputs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/compiler/xla/xla.py\", line 338, in <listcomp>\r\n    for i, x in enumerate(flat_inputs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/dispatch.py\", line 180, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py\", line 86, in identity\r\n    ret = gen_array_ops.identity(input, name=name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 4253, in identity\r\n    \"Identity\", input=input, name=name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2043, in __init__\r\n    self._control_flow_post_processing()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2054, in _control_flow_post_processing\r\n    self._control_flow_context.AddOp(self)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/compiler/xla/xla.py\", line 254, in AddOp\r\n    self._outer_context.AddInnerOp(op)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/compiler/xla/xla.py\", line 274, in AddInnerOp\r\n    self.AddOp(op)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/compiler/xla/xla.py\", line 202, in AddOp\r\n    'name: %s)' % op.name)\r\nValueError: XLA compiled computations cannot be nested, (operator name: replica_1/input_0)\r\n```", "comments": ["Issue replicating with TF-version [1.14](https://colab.sandbox.google.com/gist/oanush/7bc7560672e6df3d37a785f83bf601d0/32187.ipynb).", "Yanan (ycao@google.com) is a better person to handle this.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32187\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32187\">No</a>\n"]}, {"number": 32186, "title": "correct cosine_proximity loss", "body": "Missing minus sign in front. It was there in 1.13 but it got removed for some reason. Without that negative sign, the accuracy remains extremely low, near 0.", "comments": ["This has been fixed and has been cherrypicked into 2.0 release. Thank you!"]}, {"number": 32185, "title": "(re-re-submission) Improve Flatten to avoid using dynamic shapes when possible", "body": "- Fixes issues @ebrevdo pointed out with #30380\r\n- Added unit test for the int32 overflow situation (skip test for windows due to failures).\r\n- See [original PR](https://github.com/tensorflow/tensorflow/pull/30380) for explanation of overall changes.\r\n\r\nThanks to @trevor-m who did most of the implementation. ", "comments": [":frowning_face: Sorry, but only Googlers may change the label `cla: yes`.", "sanity check failed:\r\n\r\n```\r\nFAIL: Found 1 non-whitelisted pylint errors:\r\ntensorflow/python/layers/core_test.py:562: [E0001(syntax-error), ] invalid syntax\r\n```", "> sanity check failed:\r\n> \r\n> ```\r\n> FAIL: Found 1 non-whitelisted pylint errors:\r\n> tensorflow/python/layers/core_test.py:562: [E0001(syntax-error), ] invalid syntax\r\n> ```\r\n\r\noops, I fixed the syntax. Thanks.", "OK; the windows failure is not related.  Let's merge."]}, {"number": 32184, "title": "[2.0.0-rc0 CherryPick]: Only add ModuleWrapper when lazy loading is requested or when using TF 1.x (to print deprecation warnings).", "body": "That is, don't use TFModuleWrapper in r2.0 builds. We don't need to print deprecation warnings since deprecated 1.x symbols are already removed and we don't use lazy loading by default.", "comments": ["Windows build is breaking with this change. I will update this PR with a few more changes before merging.", "I included changes from these other commits to get it to build:\r\nhttps://github.com/tensorflow/tensorflow/commit/d82a2bbf8a0ead6d7298e652f7d4e057ca8fd83d#diff-62ec18ff8bdd93adaff55160f27a7e09\r\nhttps://github.com/tensorflow/tensorflow/commit/b7db3f4ae43f5228952f3a1bb480a3e52a2006ed#diff-15bdff9a63024759f88313469be4b11f\r\nhttps://github.com/tensorflow/tensorflow/commit/18c2cf989a2263ee212fbd5ac0b3085d9450b80a#diff-15bdff9a63024759f88313469be4b11f\r\n\r\nPlease take another look."]}, {"number": 32183, "title": "Build tflite-with-select-ops.aar for running on emulator (tf v1.14)", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.14.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n\r\n\r\n**Describe the problem**\r\n\r\n\r\nFirst of all I tried to build the `tflite-with-select-ops.aar` lib using this command : \r\n```\r\nbazel build --cxxopt='--std=c++11' \\\r\n            -c opt \\\r\n            --config=android_arm \\\r\n            --config=monolithic \\\r\n            --jobs=1 \\\r\n            //tensorflow/lite/java:tensorflow-lite-with-select-tf-ops  \r\n```\r\nBuild was successful , but then inside the emulator i was encountering this **ERROR**:\r\n`java.lang.UnsatisfiedLinkError: No implementation found for long org.tensorflow.lite.NativeInterpreterWrapper.createErrorReporter(int) (tried Java_org_tensorflow_lite_NativeInterpreterWrapper_createErrorReporter and Java_org_tensorflow_lite_NativeInterpreterWrapper_createErrorReporter__I)`\r\n\r\nI guess that this is due to the not **arm** architecture of emulator (x86, x86_64).\r\nSo I tried this builld command : \r\n```\r\nbazel build --cxxopt='--std=c++11' \\\r\n            -c opt \\\r\n            --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a \\\r\n            --config=monolithic \\\r\n            --jobs=1 \\\r\n            //tensorflow/lite/java:tensorflow-lite-with-select-tf-ops\r\n```\r\n**(also tried without monolithic argument, both cases failed to build)**\r\n\r\nThis was the error i got (**with and without monolithic arg**) :\r\n\r\n```\r\nERROR: /home/dav/Desktop/tensorflow-1.14.0/tensorflow/core/BUILD:1856:1: C++ compilation of rule '//tensorflow/core:android_tensorflow_lib_lite' failed (Exit 1)\r\nIn file included from tensorflow/core/lib/core/threadpool.cc:26:\r\nIn file included from ./tensorflow/core/platform/setround.h:19:\r\nexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/cfenv:68:9: error: no member named 'feclearexcept' in the global namespace\r\nusing ::feclearexcept;\r\n      ~~^\r\nexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/cfenv:69:9: error: no member named 'fegetexceptflag' in the global namespace\r\nusing ::fegetexceptflag;\r\n      ~~^\r\nexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/cfenv:70:9: error: no member named 'feraiseexcept' in the global namespace\r\nusing ::feraiseexcept;\r\n      ~~^\r\nexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/cfenv:71:9: error: no member named 'fesetexceptflag' in the global namespace\r\nusing ::fesetexceptflag;\r\n      ~~^\r\nexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/cfenv:72:9: error: no member named 'fetestexcept' in the global namespace\r\nusing ::fetestexcept;\r\n      ~~^\r\nexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/cfenv:73:9: error: no member named 'fegetround' in the global namespace\r\nusing ::fegetround;\r\n      ~~^\r\nexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/cfenv:74:9: error: no member named 'fesetround' in the global namespace\r\nusing ::fesetround;\r\n      ~~^\r\nexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/cfenv:75:9: error: no member named 'fegetenv' in the global namespace; did you mean 'getenv'?\r\nusing ::fegetenv;\r\n      ~~^\r\nexternal/androidndk/ndk/sysroot/usr/include/stdlib.h:61:7: note: 'getenv' declared here\r\nchar* getenv(const char* __name);\r\n      ^\r\nIn file included from tensorflow/core/lib/core/threadpool.cc:26:\r\nIn file included from ./tensorflow/core/platform/setround.h:19:\r\nexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/cfenv:76:9: error: no member named 'feholdexcept' in the global namespace\r\nusing ::feholdexcept;\r\n      ~~^\r\nexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/cfenv:77:9: error: no member named 'fesetenv' in the global namespace\r\nusing ::fesetenv;\r\n      ~~^\r\nexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/cfenv:78:9: error: no member named 'feupdateenv' in the global namespace\r\nusing ::feupdateenv;\r\n      ~~^\r\n11 errors generated.\r\n```\r\n\r\n\r\n**Additional Info**\r\n\r\n**WORKSPACE**:\r\n```\r\nbuild --action_env ANDROID_NDK_HOME=\"/home/dav/Android/Sdk/ndk/18\"\r\nbuild --action_env ANDROID_NDK_API_LEVEL=\"18\"\r\nbuild --action_env ANDROID_BUILD_TOOLS_VERSION=\"29.0.2\"\r\nbuild --action_env ANDROID_SDK_API_LEVEL=\"23\"\r\nbuild --action_env ANDROID_SDK_HOME=\"/home/dav/Android/Sdk\"\r\n```\r\n\r\nAny help would be really appreciated!! ", "comments": ["Can you try building with NDK r17c? That is the version we've been using for our stable releases.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32183\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32183\">No</a>\n", "The build has done many more steps, and the previous problem has been resolver :+1: but now gets stuck with : \r\n```\r\nERROR: /home/dav/Desktop/tensorflow-1.14.0/tensorflow/core/kernels/BUILD:6305:1: Linking of rule '//tensorflow/core/kernels:android_tensorflow_kernels' failed (Exit 1)\r\nTarget //tensorflow/lite/java:tensorflow-lite-with-select-tf-ops failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 15406.345s, Critical Path: 213.49s\r\nINFO: 5208 processes: 5197 local, 11 worker.\r\nFAILED: Build did NOT complete successfully\r\n```\r\nnew **WORKSPACE** : \r\n```\r\nbuild --action_env ANDROID_NDK_HOME=\"/home/dav/Android/Sdk/ndk/17\"\r\nbuild --action_env ANDROID_NDK_API_LEVEL=\"18\"\r\nbuild --action_env ANDROID_BUILD_TOOLS_VERSION=\"29.0.2\"\r\nbuild --action_env ANDROID_SDK_API_LEVEL=\"23\"\r\nbuild --action_env ANDROID_SDK_HOME=\"/home/dav/Android/Sdk\"\r\n```\r\n\r\nShould also ANDROID_NDK_API_LEVEL be 17?", "> ERROR: /home/dav/Desktop/tensorflow-1.14.0/tensorflow/core/kernels/BUILD:6305:1: Linking of rule '//tensorflow/core/kernels:android_tensorflow_kernels' failed (Exit 1)\r\n\r\nIs that the only error? No other failure message(s)? You might try setting the NDK_API_LEVEL to 17, though I don't think that should be necessary. You might also try 19.", "Update\r\n\r\nFound this #20192\r\nThe first error was fixed and the build was successful with this workspace:\r\n```\r\nbuild --action_env ANDROID_NDK_HOME=\"/home/dav/Android/Sdk/ndk/17\"\r\nbuild --action_env ANDROID_NDK_API_LEVEL=\"21\"\r\nbuild --action_env ANDROID_BUILD_TOOLS_VERSION=\"29.0.2\"\r\nbuild --action_env ANDROID_SDK_API_LEVEL=\"23\"\r\nbuild --action_env ANDROID_SDK_HOME=\"/home/dav/Android/Sdk\"\r\n```", "@git-davi were you able to build the tflite with select-ops ? if so, would you be kind enough to share the aar file?", "Hi, @bmabir17 ! \r\nWell, Yes but actually no!\r\nI choosed to debug the app directly from my android device instead from emulator, so I compiled only for arm architecture.\r\nHere's the file \ud83d\udc4d \r\nhttps://drive.google.com/open?id=1021SAVsnh7879-Mj45uXOAgfruaFH2rB", "@git-davi Thank you so much!! I was actually looking for the arm version rather than the emulator one.\r\nIf i may bother you with one more query. \r\nMy model contains `ResizeNearestNeighbor`, `Stack`, and `TensorFlowShape`  tf_ops. will this tf_lite build be able to run inference with this model?\r\nI have converted this model using `tf.contrib.lite.TocoConverter` python api [[code](https://gist.github.com/bmabir17/754a6e0450ec4fd5e25e462af949cde6#file-convert-py-L72)]\r\nlike the following\r\n`converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,tf.lite.OpsSet.SELECT_TF_OPS]`", "I will be honest, a lot of time has passed and I don't remember but with this prebuilt package I'm pretty sure that you can go with every tflite model!", "Thank you again for the help!! :smile: ", "You are welcome,\r\nif my aar wont work try to compile like this :\r\n```python\r\nconverter.target_ops = [tf.lite.OpsSet.SELECT_TF_OPS]\r\n```"]}, {"number": 32182, "title": "[r1.15 Cherrypick] Fix use-after-free of CancellationManager in LocalRendezvousImpl.", "body": "Previously, we were invoking the CancellationManager in ~Item, which runs after the done callback. However, the CancellationManager was borrowed from the calling RecvOp, and it will tend to be deleted synchronously when the done callback executes.\r\n\r\nPiperOrigin-RevId: 266567112", "comments": []}, {"number": 32181, "title": "[r2.0 Cherrypick] Fix use-after-free of CancellationManager in LocalRendezvousImpl.", "body": "Previously, we were invoking the CancellationManager in ~Item, which runs after the done callback. However, the CancellationManager was borrowed from the calling RecvOp, and it will tend to be deleted synchronously when the done callback executes.\r\n\r\nPiperOrigin-RevId: 266567112", "comments": []}, {"number": 32180, "title": "Keras to tflite. Multiple outputs all given name Identity", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):2.0-rc0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0/7.6\r\n- GPU model and memory: RTX 2070\r\n\r\n**Describe the current behavior**\r\nWhen I convert a keras model to a tflite model all outputs are named Identity. When you have a bunch of outputs that all have the same output size it becomes impossible to tell which one is which.\r\n\r\n**Describe the expected behavior**\r\nI would expect the names to match the layer names that the output comes from. \r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ninputs = []\r\noutputs = []\r\n\r\nx = tf.keras.layers.Input(shape=(10), batch_size=1, name='input1')\r\ninputs.append(x)\r\nx = tf.keras.layers.Dense(10, name='dense1')(x)\r\noutputs.append(x)\r\nx = tf.keras.layers.Dense(10, name='dense2')(x)\r\noutputs.append(x)\r\n\r\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\r\n\r\nmodel.summary()\r\n```\r\n```\r\nModel: \"model_1\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput1 (InputLayer)          [(1, 10)]                 0         \r\n_________________________________________________________________\r\ndense1 (Dense)               (1, 10)                   110       \r\n_________________________________________________________________\r\ndense2 (Dense)               (1, 10)                   110       \r\n=================================================================\r\nTotal params: 220\r\nTrainable params: 220\r\nNon-trainable params: 0\r\n```\r\n```\r\nmodel.outputs\r\n```\r\n```\r\n[<tf.Tensor 'dense1/Identity:0' shape=(1, 10) dtype=float32>,\r\n <tf.Tensor 'dense2/Identity:0' shape=(1, 10) dtype=float32>]\r\n```\r\n```\r\n# Convert and save the model.\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\ntflite_model = converter.convert()\r\nopen(\"./save/simple.tflite\", \"wb\").write(tflite_model)\r\n\r\n# Load TFLite model and allocate tensors.\r\ninterpreter = tf.lite.Interpreter(model_path=\"./save/simple.tflite\")\r\ninterpreter.allocate_tensors()\r\n\r\n# Get input and output tensors.\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\noutput_details\r\n```\r\n```\r\n[{'name': 'Identity',\r\n  'index': 0,\r\n  'shape': array([ 1, 10], dtype=int32),\r\n  'dtype': numpy.float32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'Identity_1',\r\n  'index': 1,\r\n  'shape': array([ 1, 10], dtype=int32),\r\n  'dtype': numpy.float32,\r\n  'quantization': (0.0, 0)}]\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I would like to work on this issue!  Please assign", "Thumbs up! Right now it is impossible to deploy models to tflite if they have multiple outputs. Is anybody working on this?", "@Cospel One work around I found was using https://lutzroeder.github.io/netron/ to view the tflite model to visually figure out what index each output corresponded to. This issue should still be fixed but this work around should help people that are currently stuck on this problem. ", "@marno1d Thank you for the idea. However this is not very friendly and scalable solution :).\r\n\r\nI hoped that creating tf.keras.layers.Lambda with the name would work:\r\n\r\n    output1 = tf.keras.layers.Lambda(lambda k: k, name='output1')(x)\r\n\r\nbut it's just creates another Identity....", "@Cospel I agree that it isn't a scalable solution. I think all outputs are automatically wrapped in an Identity layer according to the source code. I'm not familiar enough with the code to know why this is there / how to fix it. \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/base_layer.py#L774\r\n\r\n`# Wrap Tensors in \"outputs\" in \"tf.identity\" to avoid`\r\n ` # circular dependencies.`\r\n ` outputs = base_layer_utils.mark_as_return(outputs, acd)`", "@marno1d yes, making a change like this fixes the issue (and probably creates another issues \ud83d\ude04)\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/v2.1.0-rc2/tensorflow/python/framework/auto_control_deps.py#L167\r\n```diff\r\n-tensor = array_ops.identity(tensor)\r\n+# convert \"model/tf_op_layer_output/output:0\" -> \"output\"\r\n+tensor = array_ops.identity(tensor, tensor.name.split(\"/\")[-1].split(\":\")[0])\r\n```\r\n", "I'm also running into this. \r\n\r\n@ravikyram or @jdduke have you seen this? Don't have an easy way of getting around it right now", "okay, so it seems that at some point, it sorts the output tensors by name. So, you're able to control the outputs by prepending the outputs with a letter:\r\n```\r\nx = tf.keras.layers.Input(shape=(10), batch_size=1, name='input1')\r\ny = tf.keras.layers.Dense(10, name='a_dense1')(x)\r\nz = tf.keras.layers.Dense(20, name='b_dense2')(x)\r\nmodel = tf.keras.Model(x, [y, z])\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\ntflite_model = converter.convert()\r\npath = pathlib.Path('test_order1.tflite')\r\npath.write_bytes(tflite_model)\r\n\r\n# Names flipped\r\nx = tf.keras.layers.Input(shape=(10), batch_size=1, name='input1')\r\ny = tf.keras.layers.Dense(10, name='b_dense1')(x)\r\nz = tf.keras.layers.Dense(20, name='a_dense2')(x)\r\nmodel = tf.keras.Model(x, [y, z])\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\ntflite_model = converter.convert()\r\npath = pathlib.Path('test_order2.tflite')\r\npath.write_bytes(tflite_model)\r\n```\r\n### tflite_order1.tflite\r\n![image](https://user-images.githubusercontent.com/1422280/72569644-e2342f80-3888-11ea-8592-5995091297a8.png)\r\n\r\n### tflite_order2.tflite\r\n![image](https://user-images.githubusercontent.com/1422280/72569663-efe9b500-3888-11ea-971a-3443d195301e.png)\r\n\r\n\r\nThis obviously is not great, but a temporary bandaid that works for me", "Is anybody working on this ? This problem still appears in the newer version of TF. ", "Hi, I have found (temporal) solution for this problem. It seems that the invalid output names are generated within this function:\r\n\r\n```python\r\ntensorflow_core.python.framework.func_graph.func_graph_from_py_func\r\n```\r\nIt has a default argument `add_control_dependencies` set to True. Switching it to False before model conversion leads to correct names when combined with keras.Lambda and tf.identity e.g:\r\n\r\n```python\r\noutput = Lambda(lambda x: tf.identity(x, name=\"output\"), name=\"logits\")(outputs)\r\nkeras.Model(input_image, output, name=\"export\")\r\n```\r\ngives me following output details:\r\n```\r\nOutput details:\r\n{ 'dtype': <class 'numpy.float32'>,\r\n  'index': 192,\r\n  'name': 'export/logits/output',\r\n  'quantization': (0.0, 0),\r\n  'shape': array([  1, 4], dtype=int32)}\r\n```\r\n \r\nMy shitfix for this is to use Monkey patching and create `tflite_converter_shitfix.py` file which will set `add_control_dependencies` to False at runtime just before model conversion:\r\n\r\n```python\r\nimport weakref\r\nimport tensorflow as tf\r\nfrom tensorflow.python.eager import def_function as _def_function\r\nfrom tensorflow.python.eager import function as _function\r\nfrom tensorflow.python.framework import func_graph as func_graph_module\r\nfrom tensorflow.python.framework import ops\r\nfrom tensorflow.python.keras.saving import saving_utils as _saving_utils\r\nfrom tensorflow.python.util import tf_decorator\r\n\r\n\r\nkeras = tf.keras\r\n\r\n\r\ndef from_keras_model(model: keras.Model) -> tf.lite.TFLiteConverter:\r\n    \"\"\"Creates a TFLiteConverter object from a Keras model.\r\n\r\n    Args:\r\n      model: tf.Keras.Model\r\n\r\n    Returns:\r\n      TFLiteConverter object.\r\n    \"\"\"\r\n    input_signature = None\r\n    # If the model's call is not a `tf.function`, then we need to first get its\r\n    # input signature from `model_input_signature` method. We can't directly\r\n    # call `trace_model_call` because otherwise the batch dimension is set\r\n    # to None.\r\n    # Once we have better support for dynamic shapes, we can remove this.\r\n    if not isinstance(model.call, _def_function.Function):\r\n        # Pass `keep_original_batch_size=True` will ensure that we get an input\r\n        # signature including the batch dimension specified by the user.\r\n        input_signature = _saving_utils.model_input_signature(\r\n            model, keep_original_batch_size=True\r\n        )\r\n\r\n    func = _saving_utils.trace_model_call(model, input_signature)\r\n    func._defun_with_scope = lambda s: _defun_with_scope(func, s)\r\n    concrete_func = func.get_concrete_function()\r\n    return tf.lite.TFLiteConverter([concrete_func])\r\n\r\n\r\ndef _defun_with_scope(self, scope):\r\n    \"\"\"Creates a defun wrapped inside a variable creator scope.\"\"\"\r\n\r\n    weak_wrapped_fn = None\r\n\r\n    def wrapped_fn(*args, **kwds):\r\n        \"\"\"Wraps `self._python_function` in a variable creator scope.\"\"\"\r\n        with ops.get_default_graph()._variable_creator_scope(scope, priority=50):\r\n            return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n\r\n    weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n\r\n    fun = self._defun(tf_decorator.make_decorator(self._python_function, wrapped_fn))\r\n\r\n    fun._create_graph_function = lambda args, kwargs: _create_graph_function(\r\n        fun, args, kwargs\r\n    )\r\n    return fun\r\n\r\n\r\ndef _create_graph_function(self, args, kwargs, override_flat_arg_shapes=None):\r\n    \"\"\"Create a `ConcreteFunction` from `args` and `kwargs`.\"\"\"\r\n\r\n    self.tracing_count += 1\r\n    if self.input_signature is None:\r\n        arglen = len(args)\r\n    else:\r\n        arglen = len(self.input_signature)\r\n    base_arg_names = self._function_spec.arg_names[:arglen]\r\n    num_missing_args = arglen - len(self._function_spec.arg_names)\r\n    missing_arg_names = [self._function_spec.vararg_name] * num_missing_args\r\n    # Produce a list of missing args of the form [\"arg_0\", \"arg_1\", ...],\r\n    # where arg is based on the self._function_spec.vararg_name.\r\n    missing_arg_names = [\"%s_%d\" % (arg, i) for i, arg in enumerate(missing_arg_names)]\r\n    arg_names = base_arg_names + missing_arg_names\r\n\r\n    graph_function = _function.ConcreteFunction(\r\n        func_graph_module.func_graph_from_py_func(\r\n            self._name,\r\n            self._python_function,\r\n            args,\r\n            kwargs,\r\n            self.input_signature,\r\n            autograph=self._autograph,\r\n            autograph_options=self._autograph_options,\r\n            arg_names=arg_names,\r\n            override_flat_arg_shapes=override_flat_arg_shapes,\r\n            capture_by_value=self._capture_by_value,\r\n            add_control_dependencies=False,\r\n        ),\r\n        self._function_attributes,\r\n        # Tell the ConcreteFunction to clean up its graph once it goes out of\r\n        # scope. This is not the default behavior since it gets used in some\r\n        # places (like Keras) where the FuncGraph lives longer than the\r\n        # ConcreteFunction.\r\n        shared_func_graph=False,\r\n    )\r\n    return graph_function\r\n```\r\n\r\nAnd then use:\r\n\r\n```python\r\n\r\nfrom tflite_converter_shitfix import from_keras_model\r\nconverter = from_keras_model(model)\r\nconverter.experimental_new_converter = True  \r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.convert()\r\n```", "Hi, I've run into this today and figured out that I can still use the old converter code with disabled eager execution:\r\n\r\n```\r\ntf.compat.v1.disable_eager_execution()\r\n\r\nconverter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(path_model)\r\n\r\nreturn converter.convert()\r\n```\r\n\r\nthis will keep the proper names and works for my use case. I hope it helps others too.", "The problem persists in TF2.3 nightly as well. ", "I also encountered this problem\u3002", "This also happens on tensorrt conversion with TrtGraphConverterV2.\r\nI tracked down the cause to 'canonicalize_signatures' method in signature_serialization.py\r\nSeems like only the inputs are being saved in the signature and the outputs are being converted to \"Identity\" named tensors.", "I run into the problem too\u3002is it solved\uff1f tflite for multiple output", "Same issue here, All outputs ordered after their defined names but shows as Identity", "I got this also...  all my outpus were re-ordered ", "@caegomezji \r\n\r\nSignatureDef provides meaningful/generic names for inputs/outputs which doesn't rely on specific model details. More on SignatureDefs [here](https://www.tensorflow.org/guide/saved_model?hl=zh-tw#specifying_signatures_during_export).\r\nIf your saved model has a defined signatureDef then it will be exported during conversion to TFLite and then you can use the Signature inputs/outputs for inference and not relying on inputs/outputs order or tensor names.\r\n\r\nSignatureDef support is new thing available in nightly and will be available in 2.5\r\n\r\nThen use the nightly for converting to tflite. The generated tflite file will have the SignatureDef details.\r\nUsing Python API (as an example) you can do something like\r\n\r\n```\r\nmy_signature = interpreter.get_signature_runner(\"my_method\")\r\nresults = my_signature(input_1=input_tensor_1, input_2=input_tensor_2)\r\nprint(results[\"my_output\"])\r\n```", "Any update on this issue?\r\n\r\nIt is a shame that it's possible to rename a TensorFlow model's outputs for using `coremltools` but not TensorFlow itself.", "Looking forward to updates on this issue! For extreme use cases, every byte counts and would be great if we had the option to remove some of the unnecessary metadata and tensor names from the exported C array. ", "@kdonbekci @maxpv \r\n\r\nPlease see https://github.com/tensorflow/tensorflow/issues/32180#issuecomment-772140542 for the solution.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32180\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32180\">No</a>\n", "@abattery \r\n\r\nIf I'm not wrong, the issue is not solved at C++ level ?\r\nIs there any way to solve it with tflite C++ API ?\r\n\r\nThanks ", "@n-moussa There is a C++ API for using Signatures\r\n\r\n* To get list of signatures available\r\ninterpreter.signature_def_names();\r\n* To get inputs/output of a single Signature:\r\nTfLiteTensor* input_1_tensor = interpreter.input_tensor_by_signature_name(\"input1\", \"mySignature\");\r\nTfLiteTensor* input_2_tensor = interpreter.input_tensor_by_signature_name(\"input2\", \"mySignature\");\r\nconst TfLiteTensor* output_tensor = interpreter.output_tensor_by_signature_name(\"output\", \"mySignature\");\r\n\r\nExample,\r\ninterpreter.AllocateTensors()\r\n\r\nautp* input1 = interpreter.input_tensor_by_signature_name(\"input1\", \"mySignature\");\r\n// Set the data in the tensor normally e,g,: input1->data.f[0] = 1.0f;\r\nautp* input2 = interpreter.input_tensor_by_signature_name(\"input2\", \"mySignature\");\r\n// Set the data in the tensor normally e,g,: input1->data.f[0] = 1.0f;\r\n\r\n// Run\r\ninterpreter.Invoke();\r\n\r\n// Fetch outputs\r\nconst TfLiteTensor* output_tensor = interpreter.output_tensor_by_signature_name(\"output\", \"mySignature\");\r\n", "Maybe i'm missing something obvious here but I don't understand how this was fixed? I still have the same problem with nightly 2.5.0-dev20210307\r\nmy signature def is intact in the saved model. and lists my outputs correctly\r\n\r\nedit:\r\nnever mind, i now understand that i wasn't looking at the signature def by using\r\ninterpreter.get_input_details()\r\ninterpreter.get_output_details()\r\n\r\n", "Love the signature runner API! I noticed that when I convert a `concrete_function` instead of a `saved_model` to tflite using `tf.lite.TFLiteConverter.from_concrete_functions()` the signature def is missing from the model after I load it into the interpreter. Is this behaviour by design or am I missing something during the conversion (for example, some attributes that I need to add to the concrete function, so it persists in the signature def)?   \r\n\r\nSorry for adding to a closed thread. I thought this was relevant to the discussion here.  \r\n\r\nUPDATE: I can get around this by simply wrapping the concrete function into a saved_model using `tf.saved_model.save(module, tf_model_path, signatures=concrete_func)` and then converting it to tflite. So, please ignore my comment.", "@avroshk Thanks for your comment! Could you file another issue separately for the concrete function's signature def support? This can be a good one for the feature request. :-)", "Currently the Signature is supported through saved model. As Jaesung said please file another issue for a feature request.\r\n\r\nThanks", "Another this I realised is the weights and biases are getting modified in this procedure and graph structure is also changing when visualised through netron and the same changed structure is reflecting when checked via scripting to  cross validate the netron output. Conv2_block1_2 for example had no BN layer before Relu. But If want to pull out the Relu output then it adds BN layers at every node which is being pulled out", "@manp-git I am not sure i understand what you refer to by \"this procedure\"\r\nPlease file separate issue with sample and reproduce steps if you're having an issue.\r\n\r\nThanks", "this issue is partially back in the latest latest nightly and the official tf2.5.0\r\nit works in nightly 2.5.0-dev20210307\r\n\r\nWhen doing a straight conversion to tflite it works as expected, with input and output naming carried over as it should\r\n\r\nBut when quantizing a model through converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nthe signature is empty", "@LIFEPANTS Thanks for the report. We are working on a fix for signature getting dropped. It is work in progress now.\r\n\r\nSorry for the trouble.", "@MeghnaNatraj FYI", "@karimnosseir \r\nWas this ever fixed? any release i should try?", "You can try out the nightly version. See also https://www.tensorflow.org/lite/guide/signatures"]}, {"number": 32179, "title": "Corrected typo in `ones_like`", "body": "TF Website shows `zero` in place of `one`. \r\nThis fixes the issue https://github.com/tensorflow/tensorflow/issues/32129", "comments": ["This should go against `master` and maybe be cherry-picked", "This matches master, so reopening. It was not clear if this was a cherry-pick."]}, {"number": 32178, "title": "Convert invalid names to strings to prevent formatting errors", "body": "In case user specify incorrect objection for TFLite converter, the code rasise ValueError. Unfortunately, during calculation of error message another error may appear due to invalid string formatting attempt. This PR prevents this situation by forcing string conversion for error messages.\r\n\r\nBelow is the example of such an error condition.\r\n\r\n```\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/util.py in get_tensors_from_tensor_names(graph, tensor_names)\r\n      114   if invalid_tensors:\r\n      115     raise ValueError(\"Invalid tensors '{}' were found.\".format(\r\n  --> 116         \",\".join(invalid_tensors)))\r\n      117   return tensors\r\n      118\r\n\r\n  TypeError: sequence item 0: expected str instance, Tensor found\r\n  ```\r\n", "comments": ["This PR appears to mask an issue with the user not passing in the correct types. I don't think this is the proper way to solve this issue."]}, {"number": 32177, "title": "Use `patch` utility for patching sources on all platforms", "body": "Let me propose a fix for https://github.com/tensorflow/tensorflow/issues/31196 issue", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32177) for more info**.\n\n<!-- need_author_consent -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32177) for more info**.\n\n<!-- ok -->", "> This PR does more than its description says it does. Please remove unrelated changes.\r\n\r\nRebased to master. @mihaimaruseac @rthadur please check\r\n"]}, {"number": 32176, "title": "Gradient Accumulation feature for Distribution Strategy", "body": "**System information**\r\n- TensorFlow version (you are using): 1.12\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nGradient Accumulation(GA) is a workaround to enable big batches on limited memory GPUs which has been supported in Caffe and PyTorch. Instead of back-propagating for every batch feed-forward; gradients across multiple batches are accumulated. After multiple feed forwards, the accumulated gradient is back-propagated through the network layer.  It boosts performance with a couple of percentages on our several workload (e.g., XLNet, Transformer) with Distribution Strategy. Unfortunately, I am not aware of any official documentation to use such feature in Tensorflow. `tf.contrib.opt.AGNOptimizer` has a similar GA implementation but is not common for general distribution job on DistributionStrategy. Besides it will introduces OOM for large embeddings(#31637).\r\n**Will this change the current api? How?**\r\nYes. Just need to add one new parameter `iter_size` to current strategy API, which  for example:\r\n```\r\n # For MirroredStrategy, gradient accumulation is supported using iter_size parameter.\r\n distribution = tf.distribute.MirroredStrategy(num_gpus=2, iter_size=4)\r\n\r\n # For MultiWorkerMirroredStrategy:\r\n tf.distribute.experimental.MultiWorkerMirroredStrategy(..., iter_size=4)\r\n```\r\n\r\n**Who will benefit with this feature?**\r\nAnyone who use distribution strategy and want to enable big batches on limited memory GPUs \r\n**Any Other info.**\r\nNone.\r\n\r\nWe already have a concise implementation of GA on DistributionStrategy and are willing to contribute. Thanks in advance for any feedback.", "comments": ["Thank Shiqing for proposing this feature.\r\nGA is not a fresh idea, the original idea may originated from 2012 in Microsoft's speech training acceleration paper, and in pyTorch and Caffe there is already alike implementation. Such as in \r\nhttps://gist.github.com/thomwolf/ac7a7da6b1888c2eeac8ac8b9b05d3d3\r\nand \r\nhttps://github.com/BVLC/caffe/blob/128797ebe100dd87f375d2d30b0f649914d0fcc3/src/caffe/solver.cpp#L230  \r\nhttps://github.com/BVLC/caffe/wiki/Solver-Prototxt#iter_size\r\nIt is also possible that users may choose to implement this feature manually in their model script, however, doing so would be tedious, error-prone and duplicated.  We think it is better to integrate such feature into DS framework to ease users' burden. \r\nThis feature is quite useful since it brings two benefits:\r\n1). Make it capable to train large models.  For example, if the original training could successfully train with batch size 1, then GA can boost the batch size to a much larger one. For lots of DL\r\nmodel training, it is found that a decent batch size is better for model convergence than smaller batch size. So GA is useful for large model training.\r\n2).Since large batch training techniques has been adopted for lots of scenarios, such as CV, NLP, speech and search&recommendation,  GA is an useful solution to increase computation-to-communication ration which in turn improves distributed scalability.  With GA, we can conveniently increase the iter_size to a extent with which the training statistical efficiency is acceptable(with tricks such as linear-learning rate rule or LARS, etc), thus greatly improve distributed training throughput.\r\n\r\nWe have already implemented GA on top of our internal version of TensorFlow and integrated it into MultiWorkerMirroredStrategy and Mirrored strategy. Several users benefit from it with just one line of small code change. We think this feature may be useful for people outside of Alibaba so we raise this issue. If TF community like this idea, we can submit the PR very soon.\r\n\r\nThanks", "@yuefengz ", "This is indeed a must have feature for recent sequence models. Could you clarify why you implemented this inside distribution strategies instead of as an optimizer wrapper?", "@guillaumekln  Actually our GA implementation is an independent functional module which is called conditionally by the base `Optimizer` class, so that any other optimizer wrapper can make use of this basic feature easily when needed. We add interface especially for distribution strategies as many of our models run top of it for its convenience and well scalability. \r\n", "Interesting. Looking forward to see the code!", "A simple wrapper can be made to let any tensorflow 1.0 optimizer support Gradient Accumulation: https://github.com/tensorpack/tensorpack/blob/6a0bba688f2bac1c8baf59eb0c61eb90234a5424/tensorpack/tfutils/optimizer.py#L141-L224\r\n\r\nSo how is this related to distribution strategies? What types of integration with distribution strategies are you looking for that provide more benefits than simply wrapping the optimizer?", "@fanshiqing Thank for your suggestion. I think this is an important and interesting feature, too. However, it would be difficult for us to change the distribution strategy's constructors. Other options may include changing the optimizer, or doing something in your [custom train loop](https://www.tensorflow.org/beta/tutorials/distribute/training_loops).", "@yuefengz Sorry for the delayed reply as I was on vocation. Add a new attribute to strategy's constructor is a direct approach. As what we want is just to pass a control-parameter like `iter_size` to  `Optimzer` in some way, if the distribution strategy's constructors are difficult to change **explicitly**, what about add `iter_size` to `tf.estimator.RunConfig`, so that the original strategies' constructor will keep unchanged. ", "If you just want to pass a parameter to `Optimizer`, then you should only change `Optimizer`. Because:\r\n* Distribution strategy and Estimator do not even assume the existence of optimizer. They have their own scope of abstraction that are independent of what you do inside (which typically is an optimizer-based algorithm, but maybe not). Having an argument that's tied to specific types of common tasks is an abstraction leak (though I'm not saying that estimator does not have similar issues now).\r\n\r\n* If you want to add an option to dist-strategy or `RunConfig` just to let them pass something to the optimizer, why not pass them to the optimizer directly? What benefits do you get by taking a longer path to achieve the same thing? In fact I only saw a  few disadvantages, which are:\r\n\r\n* By adding such options to dist-strategy or `RunConfig` instead of to the optimizers directly, users who do not use dist-strategy or `RunConfig` (but still use optimizers) will not have access to this feature. \r\n\r\n* If you add such options to dist-strategy or `RunConfig`, what if I want to use two optimizers in the model (where both are under the same dist-strategy or estimator), one with gradient accumulation and one without?", "@ppwwyyxx Thx for your nice advice. Actually at our scenario we only want to add GA for distribution strategy from the very begging. While it could be another story if we want to add this to community. It seems that pass control parameter to `Optimizer` directly is more reasonable. \r\nI will open a PR soon for more specific discussion.", "Is there a practical way to do this in tf.keras 2.x?\r\n\r\nFrom a user perspective, having a parameter (e.g. in `.fit`) like `gradient_accumulate_batch_frequency=1` that updates gradients every N batches would be perfect. (1 as default to update every batch, as it is at the moment)", "+1, how can we achieve gradient accumulation in `tf.keras` (with `fit()` method) ?", "@ymodak @yuefengz Is there a timeline to add this gradient accumulation to tf.keras optimizers? Or will there be a guideline on how to do it in the custom training loop? My current approach is to create an accumulated gradients variable and have train step in each replica to add to this variable, but I also need a separate strategy.run to apply the gradients because that variable is under replica context.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Check https://github.com/tensorflow/addons/pull/2525"]}, {"number": 32175, "title": "(Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.)", "body": "2019-09-02 15:57:27.737 14560-14560/org.tensorflow.demo E/tensorflow: CameraActivity: Exception!\r\n    java.lang.RuntimeException: Failed to load model from 'file:///android_asset/ssd.pb'\r\n        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.<init>(TensorFlowInferenceInterface.java:113)\r\n        at org.tensorflow.demo.TensorFlowObjectDetectionAPIModel.create(TensorFlowObjectDetectionAPIModel.java:91)\r\n        at org.tensorflow.demo.DetectorActivity.onPreviewSizeChosen(DetectorActivity.java:157)\r\n        at org.tensorflow.demo.CameraActivity.onPreviewFrame(CameraActivity.java:120)\r\n        at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1209)\r\n        at android.os.Handler.dispatchMessage(Handler.java:106)\r\n        at android.os.Looper.loop(Looper.java:193)\r\n        at android.app.ActivityThread.main(ActivityThread.java:6669)\r\n        at java.lang.reflect.Method.invoke(Native Method)\r\n        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:493)\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:858)\r\n     Caused by: java.io.IOException: Not a valid TensorFlow Graph serialization: NodeDef mentions attr 'half_pixel_centers' not in Op<name=ResizeBilinear; signature=images:T, size:int32 -> resized_images:float; attr=T:type,allowed=[DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=align_corners:bool,default=false>; NodeDef: {{node Preprocessor/map/while/ResizeImage/resize/ResizeBilinear}}. (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\r\n        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.loadGraph(TensorFlowInferenceInterface.java:561)\r\n        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.<init>(TensorFlowInferenceInterface.java:105)\r\n        at org.tensorflow.demo.TensorFlowObjectDetectionAPIModel.create(TensorFlowObjectDetectionAPIModel.java:91)\u00a0\r\n        at org.tensorflow.demo.DetectorActivity.onPreviewSizeChosen(DetectorActivity.java:157)\u00a0\r\n        at org.tensorflow.demo.CameraActivity.onPreviewFrame(CameraActivity.java:120)\u00a0\r\n        at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1209)\u00a0\r\n        at android.os.Handler.dispatchMessage(Handler.java:106)\u00a0\r\n        at android.os.Looper.loop(Looper.java:193)\u00a0\r\n        at android.app.ActivityThread.main(ActivityThread.java:6669)\u00a0\r\n        at java.lang.reflect.Method.invoke(Native Method)\u00a0\r\n        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:493)\u00a0\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:858)\u00a0\r\n", "comments": ["when I use my .pb file to replace detectoractivity.java , I have above question.\r\n\r\nmy replace:\r\n\r\nprivate static final int TF_OD_API_INPUT_SIZE = 300;\r\n private static final String TF_OD_API_MODEL_FILE = \"file:///android_asset/ssd.pb\";\r\n  private static final String TF_OD_API_LABELS_FILE = \"file:///android_asset/label_list.txt\";", "@dm961027 ,\r\nPlease provide details about what platform you are using (operating system, architecture) Include your TensorFlow version.\r\n\r\nIn order to expedite the trouble-shooting process, Can you share a simple and standalone code to reproduce the issue reported here? Also provide more info on the issue.Thanks!", "I use  Windows10  , Android studio 3.3 \uff0cthe demo in tensorflow_ android\uff0cmodify the file \"DetectorActivity.java\" \r\nmy .pb file use Linux system  train \uff0cuse tensorflow 1.14.0\r\n \r\nandroid build gradle \r\n\r\nnativeBuildSystem = 'none'\r\n\r\ndependencies {\r\n    if (nativeBuildSystem == 'cmake' || nativeBuildSystem == 'none') {\r\n        implementation 'org.tensorflow:tensorflow-android:+'\r\n          }\r\n", "\u662f\u4e0d\u662f\u6211\u8bad\u7ec3\u6570\u636e\u7684tensorflow\u7248\u672c\u592a\u9ad8\u4e86\uff0c\u5e94\u8be5\u4f7f\u7528tensorflow 1.13.0\u4ee5\u4e0b\u7684\u7248\u672c", "@dm961027 \u6b3a\u8d1f\u4eba\u5bb6\u4e0d\u61c2\u4e2d\u6587\u5417[\u5978\u7b11]", "help me , please", "@dm961027 Could you summarize your issue with little more details? Thanks", "@jvishnuvardhan  when i build my app,there is no error, but when i run my app ,the error is the one when i refer to , so i do not know how to summarize more details .That is all my error ,sorry \r\n\r\n\r\n\r\n2019-09-15 11:38:39.830 19087-19087/? I/tensorflow.dem: Not late-enabling -Xcheck:jni (already on)\r\n2019-09-15 11:38:39.884 19087-19087/? W/tensorflow.dem: Unexpected CPU variant for X86 using defaults: x86\r\n2019-09-15 11:38:40.279 19087-19087/org.tensorflow.demo I/tensorflow.dem: The ClassLoaderContext is a special shared library.\r\n2019-09-15 11:38:40.627 19087-19087/org.tensorflow.demo W/tensorflow.dem: JIT profile information will not be recorded: profile file does not exits.\r\n2019-09-15 11:38:40.647 19087-19087/org.tensorflow.demo I/chatty: uid=10102(org.tensorflow.demo) identical 2 lines\r\n2019-09-15 11:38:40.647 19087-19087/org.tensorflow.demo W/tensorflow.dem: JIT profile information will not be recorded: profile file does not exits.\r\n2019-09-15 11:38:40.649 19087-19087/org.tensorflow.demo W/tensorflow.dem: JIT profile information will not be recorded: profile file does not exits.\r\n2019-09-15 11:38:40.650 19087-19087/org.tensorflow.demo W/tensorflow.dem: JIT profile information will not be recorded: profile file does not exits.\r\n2019-09-15 11:38:40.655 19087-19087/org.tensorflow.demo I/chatty: uid=10102(org.tensorflow.demo) identical 5 lines\r\n2019-09-15 11:38:40.655 19087-19087/org.tensorflow.demo W/tensorflow.dem: JIT profile information will not be recorded: profile file does not exits.\r\n2019-09-15 11:38:40.737 19087-19087/org.tensorflow.demo I/InstantRun: starting instant run server: is main process\r\n2019-09-15 11:38:40.891 19087-19087/org.tensorflow.demo D/tensorflow: CameraActivity: onCreate org.tensorflow.demo.ClassifierActivity@ddf260d\r\n2019-09-15 11:38:41.038 19087-19087/org.tensorflow.demo D/tensorflow: CameraActivity: onStart org.tensorflow.demo.ClassifierActivity@ddf260d\r\n2019-09-15 11:38:41.041 19087-19087/org.tensorflow.demo D/tensorflow: CameraActivity: onResume org.tensorflow.demo.ClassifierActivity@ddf260d\r\n2019-09-15 11:38:41.058 19087-19087/org.tensorflow.demo D/OpenGLRenderer: HWUI GL Pipeline\r\n2019-09-15 11:38:41.111 19087-19087/org.tensorflow.demo D/tensorflow: CameraActivity: onPause org.tensorflow.demo.ClassifierActivity@ddf260d\r\n2019-09-15 11:38:41.112 19087-19087/org.tensorflow.demo D/tensorflow: CameraActivity: Requesting finish\r\n2019-09-15 11:38:41.349 19087-19106/org.tensorflow.demo I/ConfigStore: android::hardware::configstore::V1_0::ISurfaceFlingerConfigs::hasWideColorDisplay retrieved: 0\r\n2019-09-15 11:38:41.354 19087-19106/org.tensorflow.demo I/ConfigStore: android::hardware::configstore::V1_0::ISurfaceFlingerConfigs::hasHDRDisplay retrieved: 0\r\n2019-09-15 11:38:41.354 19087-19106/org.tensorflow.demo I/OpenGLRenderer: Initialized EGL, version 1.4\r\n2019-09-15 11:38:41.355 19087-19106/org.tensorflow.demo D/OpenGLRenderer: Swap behavior 1\r\n2019-09-15 11:38:41.355 19087-19106/org.tensorflow.demo W/OpenGLRenderer: Failed to choose config with EGL_SWAP_BEHAVIOR_PRESERVED, retrying without...\r\n2019-09-15 11:38:41.355 19087-19106/org.tensorflow.demo D/OpenGLRenderer: Swap behavior 0\r\n2019-09-15 11:38:41.437 19087-19106/org.tensorflow.demo D/EGL_emulation: eglCreateContext: 0xf1143360: maj 2 min 0 rcv 2\r\n2019-09-15 11:38:41.470 19087-19106/org.tensorflow.demo D/EGL_emulation: eglMakeCurrent: 0xf1143360: ver 2 0 (tinfo 0xf10b1aa0)\r\n2019-09-15 11:38:41.579 19087-19106/org.tensorflow.demo D/EGL_emulation: eglMakeCurrent: 0xf1143360: ver 2 0 (tinfo 0xf10b1aa0)\r\n2019-09-15 11:38:42.612 19087-19106/org.tensorflow.demo D/EGL_emulation: eglMakeCurrent: 0xf1143360: ver 2 0 (tinfo 0xf10b1aa0)\r\n2019-09-15 11:38:42.769 19087-19087/org.tensorflow.demo D/tensorflow: CameraActivity: onStop org.tensorflow.demo.ClassifierActivity@ddf260d\r\n2019-09-15 11:38:42.784 19087-19087/org.tensorflow.demo D/tensorflow: CameraActivity: onDestroy org.tensorflow.demo.ClassifierActivity@ddf260d\r\n2019-09-15 11:39:02.133 19087-19087/org.tensorflow.demo W/ActivityThread: handleWindowVisibility: no activity for token android.os.BinderProxy@9aa9a4b\r\n2019-09-15 11:39:02.493 19087-19087/org.tensorflow.demo D/tensorflow: CameraActivity: onCreate org.tensorflow.demo.DetectorActivity@c9152e6\r\n2019-09-15 11:39:02.631 19087-19087/org.tensorflow.demo I/CameraManagerGlobal: Connecting to camera service\r\n2019-09-15 11:39:02.698 19087-19087/org.tensorflow.demo I/tensorflow: CameraActivity: Camera API lv2?: false\r\n2019-09-15 11:39:02.864 19087-19087/org.tensorflow.demo W/tensorflow.dem: Verification of java.lang.Object org.tensorflow.demo.AutoFitTextureView.access$super(org.tensorflow.demo.AutoFitTextureView, java.lang.String, java.lang.Object[]) took 128.972ms\r\n2019-09-15 11:39:02.911 19087-19087/org.tensorflow.demo D/tensorflow: CameraActivity: onStart org.tensorflow.demo.DetectorActivity@c9152e6\r\n2019-09-15 11:39:02.912 19087-19087/org.tensorflow.demo D/tensorflow: CameraActivity: onResume org.tensorflow.demo.DetectorActivity@c9152e6\r\n2019-09-15 11:39:03.413 19087-19087/org.tensorflow.demo I/tensorflow: CameraConnectionFragment: Desired size: 640x480, min size: 480x480\r\n2019-09-15 11:39:03.414 19087-19087/org.tensorflow.demo I/tensorflow: CameraConnectionFragment: Valid preview sizes: [640x480, 1280x720, 1280x960]\r\n2019-09-15 11:39:03.414 19087-19087/org.tensorflow.demo I/tensorflow: CameraConnectionFragment: Rejected preview sizes: [352x288, 320x240, 176x144]\r\n2019-09-15 11:39:03.414 19087-19087/org.tensorflow.demo I/tensorflow: CameraConnectionFragment: Exact size match found.\r\n2019-09-15 11:39:03.470 19087-19087/org.tensorflow.demo W/tensorflow: ImageUtils: Native library not found, native RGB -> YUV conversion may be unavailable.\r\n2019-09-15 11:39:03.512 19087-19106/org.tensorflow.demo D/EGL_emulation: eglMakeCurrent: 0xf1143360: ver 2 0 (tinfo 0xf10b1aa0)\r\n2019-09-15 11:39:04.495 19087-19087/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: ???\r\n2019-09-15 11:39:04.495 19087-19087/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: Awake\r\n2019-09-15 11:39:04.495 19087-19087/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: \r\n2019-09-15 11:39:04.496 19087-19087/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: Fatigue\r\n2019-09-15 11:39:04.528 19087-19087/org.tensorflow.demo I/TensorFlowInferenceInterface: Checking to see if TensorFlow native methods are already loaded\r\n2019-09-15 11:39:04.529 19087-19087/org.tensorflow.demo E/tensorflow.dem: No implementation found for long org.tensorflow.contrib.android.RunStats.allocate() (tried Java_org_tensorflow_contrib_android_RunStats_allocate and Java_org_tensorflow_contrib_android_RunStats_allocate__)\r\n2019-09-15 11:39:04.529 19087-19087/org.tensorflow.demo I/TensorFlowInferenceInterface: TensorFlow native methods not found, attempting to load via tensorflow_inference\r\n2019-09-15 11:39:04.665 19087-19098/org.tensorflow.demo I/tensorflow.dem: Background concurrent copying GC freed 1106(85KB) AllocSpace objects, 0(0B) LOS objects, 49% free, 2MB/5MB, paused 1.666ms total 184.225ms\r\n2019-09-15 11:39:04.747 19087-19087/org.tensorflow.demo W/native: cpu_feature_guard.cc:35 The TensorFlow library was compiled to use SSE instructions, but these aren't available on your machine.\r\n2019-09-15 11:39:04.747 19087-19087/org.tensorflow.demo W/native: cpu_feature_guard.cc:35 The TensorFlow library was compiled to use SSE2 instructions, but these aren't available on your machine.\r\n2019-09-15 11:39:04.747 19087-19087/org.tensorflow.demo W/native: cpu_feature_guard.cc:35 The TensorFlow library was compiled to use SSE3 instructions, but these aren't available on your machine.\r\n2019-09-15 11:39:04.771 19087-19087/org.tensorflow.demo I/TensorFlowInferenceInterface: Successfully loaded TensorFlow native methods (RunStats error may be ignored)\r\n2019-09-15 11:39:05.861 19087-19087/org.tensorflow.demo E/tensorflow: CameraActivity: Exception!\r\n    java.lang.RuntimeException: Failed to load model from 'file:///android_asset/ssd.pb'\r\n        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.<init>(TensorFlowInferenceInterface.java:113)\r\n        at org.tensorflow.demo.TensorFlowObjectDetectionAPIModel.create(TensorFlowObjectDetectionAPIModel.java:91)\r\n        at org.tensorflow.demo.DetectorActivity.onPreviewSizeChosen(DetectorActivity.java:157)\r\n        at org.tensorflow.demo.CameraActivity.onPreviewFrame(CameraActivity.java:120)\r\n        at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1209)\r\n        at android.os.Handler.dispatchMessage(Handler.java:106)\r\n        at android.os.Looper.loop(Looper.java:193)\r\n        at android.app.ActivityThread.main(ActivityThread.java:6669)\r\n        at java.lang.reflect.Method.invoke(Native Method)\r\n        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:493)\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:858)\r\n     Caused by: java.io.IOException: Not a valid TensorFlow Graph serialization: NodeDef mentions attr 'half_pixel_centers' not in Op<name=ResizeBilinear; signature=images:T, size:int32 -> resized_images:float; attr=T:type,allowed=[DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=align_corners:bool,default=false>; NodeDef: {{node Preprocessor/map/while/ResizeImage/resize/ResizeBilinear}}. (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\r\n        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.loadGraph(TensorFlowInferenceInterface.java:561)\r\n        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.<init>(TensorFlowInferenceInterface.java:105)\r\n        at org.tensorflow.demo.TensorFlowObjectDetectionAPIModel.create(TensorFlowObjectDetectionAPIModel.java:91)\u00a0\r\n        at org.tensorflow.demo.DetectorActivity.onPreviewSizeChosen(DetectorActivity.java:157)\u00a0\r\n        at org.tensorflow.demo.CameraActivity.onPreviewFrame(CameraActivity.java:120)\u00a0\r\n        at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1209)\u00a0\r\n        at android.os.Handler.dispatchMessage(Handler.java:106)\u00a0\r\n        at android.os.Looper.loop(Looper.java:193)\u00a0\r\n        at android.app.ActivityThread.main(ActivityThread.java:6669)\u00a0\r\n        at java.lang.reflect.Method.invoke(Native Method)\u00a0\r\n        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:493)\u00a0\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:858)\u00a0\r\n", "I have already solved this problem , I use tensorflow 1.13.1 replaced tensorflow 1.14.0 and add \r\n implementation 'org.tensorflow:tensorflow-android:1.13.1'  thank you everyone ", "Thanks @dm961027 I am closing the issue as it was resolved. Please feel free to open it if the issue persists again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32175\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32175\">No</a>\n", "@dm961027  \r\nimplementation 'org.tensorflow:tensorflow-android:1.13.1' \u73b0\u5728\u6700\u9ad8\u7248\u672c\u5c311.13.1\r\n\u6240\u4ee5\u8bad\u7ec3\u6a21\u578b\uff0c\u8f6c\u6210tflite\u90fd\u5f97\u7528tensorflow 1.13.1\u7684\u7248\u672c\u5417", "@dm961027 @fcu-d0571783 Did you had to retrain the model again with tensorflow 1.13.1 for it to work or you just downgraded your tensorflow version and it worked? "]}, {"number": 32174, "title": "Remove superfluous use of Processors in compute_gradients", "body": "The use of the processors in compute_gradients is that every \"variable\" has\r\n\".target\" called. The only case in which it is not a simple return is the\r\n\"_RefVariableProcessor\", in which it calls _ref(). However, tf.gradients\r\ndoes not need this as it will perform conversion using\r\nvariables._TensorConversionFunction.\r\n\r\nThe \"Processors\" are still needed for processor.update_op in line 614, however\r\nthis code could be simplified.", "comments": ["@nw89 thank you , can you please add a test case ?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 32173, "title": "Mirror Strategy slow down by adding GPUs", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["duplicate #32172 "]}, {"number": 32172, "title": "Mirror Strategy slow down by adding GPUs", "body": "I am using the custom estimator with TFrecord. I am training VGG16. By increasing the number of GPUs the training time increase. All GPUs are in a single machine. GPUs utilization is about 100%. So it seems the input function can feed the data to GPUs well. I am using tf.layers.conv2d, tf.contrib.layers.flatten, tf.layers.dense to create the VGG Model. However, I have a large number of groups_Deps nodes in the computation graph. since all GPUs are in a single machine I am wondering why increasing number of GPUs lead to decreasing the global_Steps/Sec and also increasing the training time. \r\n\r\n![image](https://user-images.githubusercontent.com/17527773/64166782-195cf380-ce48-11e9-91d8-15ceade1db54.png)\r\n\r\n", "comments": ["@sakh251 ,\r\nCan you share a simple and standalone code to reproduce to reproduce the issue reported here.Also mention the TensorFlow version being used.Thanks!", "@oanush \r\nThis some of my code,\r\nI am using tensorflow 1.11\r\n\r\n    import tensorflow as tf\r\n    import json\r\n    import os\r\n  \r\n     \r\n    def new_vgg(features, mode, n_classes):\r\n            if mode == tf.estimator.ModeKeys.TRAIN:\r\n                is_training = 1\r\n            else:\r\n                is_training = 0\r\n            conv = tf.layers.conv2d(\r\n                         inputs=features,\r\n                         filters=64,\r\n                         kernel_size=[3, 3],\r\n                         padding=\"same\",\r\n                         activation=tf.nn.relu)\r\n              \r\n            conv = tf.layers.conv2d(\r\n                         inputs=conv,\r\n                         filters=64,\r\n                         kernel_size=[3, 3],\r\n                         padding=\"same\",\r\n                         activation=tf.nn.relu)\r\n            pool = tf.layers.max_pooling2d(inputs=conv, pool_size=[2, 2], strides=2)\r\n\r\n             \r\n            conv = tf.layers.conv2d(\r\n                         inputs=pool,\r\n                         filters=128,\r\n                         kernel_size=[3, 3],\r\n                         padding=\"same\",\r\n                         activation=tf.nn.relu)\r\n            \r\n            conv = tf.layers.conv2d(\r\n                         inputs=conv,\r\n                         filters=128,\r\n                         kernel_size=[3, 3],\r\n                         padding=\"same\",\r\n                         activation=tf.nn.relu)\r\n            pool = tf.layers.max_pooling2d(inputs=conv, pool_size=[2, 2], strides=2)\r\n\r\n            \r\n            conv = tf.layers.conv2d(\r\n                         inputs=pool,\r\n                         filters=256,\r\n                         kernel_size=[3, 3],\r\n                         padding=\"same\",\r\n                         activation=tf.nn.relu)\r\n            \r\n            conv = tf.layers.conv2d(\r\n                         inputs=conv,\r\n                         filters=256,\r\n                         kernel_size=[3, 3],\r\n                         padding=\"same\",\r\n                         activation=tf.nn.relu)\r\n                         \r\n            conv = tf.layers.conv2d(\r\n                         inputs=conv,\r\n                         filters=256,\r\n                         kernel_size=[3, 3],\r\n                         padding=\"same\",\r\n                         activation=tf.nn.relu)\r\n            pool = tf.layers.max_pooling2d(inputs=conv, pool_size=[2, 2], strides=2)\r\n\r\n            \r\n            conv = tf.layers.conv2d(\r\n                         inputs=pool,\r\n                         filters=512,\r\n                         kernel_size=[3, 3],\r\n                         padding=\"same\",\r\n                         activation=tf.nn.relu)\r\n            \r\n            conv = tf.layers.conv2d(\r\n                         inputs=conv,\r\n                         filters=512,\r\n                         kernel_size=[3, 3],\r\n                         padding=\"same\",\r\n                         activation=tf.nn.relu)\r\n            \r\n            conv = tf.layers.conv2d(\r\n                         inputs=conv,\r\n                         filters=512,\r\n                         kernel_size=[3, 3],\r\n                         padding=\"same\",\r\n                         activation=tf.nn.relu)\r\n            pool = tf.layers.max_pooling2d(inputs=conv, pool_size=[2, 2], strides=2)\r\n\r\n            \r\n            conv = tf.layers.conv2d(\r\n                         inputs=pool,\r\n                         filters=512,\r\n                         kernel_size=[3, 3],\r\n                         padding=\"same\",\r\n                         activation=tf.nn.relu)\r\n            \r\n            conv = tf.layers.conv2d(\r\n                         inputs=conv,\r\n                         filters=512,\r\n                         kernel_size=[3, 3],\r\n                         padding=\"same\",\r\n                         activation=tf.nn.relu)\r\n            \r\n            conv = tf.layers.conv2d(\r\n                         inputs=conv,\r\n                         filters=512,\r\n                         kernel_size=[3, 3],\r\n                         padding=\"same\",\r\n                         activation=tf.nn.relu)\r\n            pool = tf.layers.max_pooling2d(inputs=conv, pool_size=[2, 2], strides=2)\r\n\r\n            \r\n            conv = tf.layers.conv2d(\r\n                         inputs=pool,\r\n                         filters=4096,\r\n                         kernel_size=[3, 3],\r\n                         padding=\"same\",\r\n                         activation=tf.nn.relu)\r\n            dropout = tf.layers.dropout(inputs=conv, rate=0.5, training=is_training)\r\n\r\n            \r\n            conv = tf.layers.conv2d(\r\n                         inputs=dropout,\r\n                         filters=4096,\r\n                         kernel_size=[3, 3],\r\n                         padding=\"same\",\r\n                         activation=tf.nn.relu)\r\n\r\n            out_conv = tf.contrib.layers.flatten(conv)\r\n\r\n           \r\n            dense = tf.layers.dense(inputs=out_conv, units=1024, activation=tf.nn.relu)\r\n            out = tf.layers.dense(inputs=dense, activation = None,  units=n_classes)\r\n\r\n            return out\r\n        \r\n    \r\n\r\n    def build_estimator(config, logdir, params = None):\r\n          return tf.estimator.Estimator(\r\n          model_fn=model_fn,\r\n          model_dir=logdir,\r\n          config=config,\r\n          params=params\r\n      )\r\n\r\n\r\n    def model_fn(features, labels, mode,params):\r\n      FLAGS = params\r\n      logits = new_vgg(features,mode,FLAGS.dataset.n_classes)\r\n    \r\n      class_predictions = tf.argmax(logits, axis=-1)\r\n      loss = None\r\n      train_op = None\r\n      eval_metric_ops = {}\r\n      train_hook_list= []\r\n      predictions = class_predictions\r\n      # Loss will only be tracked during training or evaluation.\r\n      if mode in (TRAIN, EVAL):\r\n\r\n          loss = tf.losses.sparse_softmax_cross_entropy(\r\n              labels=tf.cast(labels, tf.int32),\r\n              logits=logits)\r\n  \r\n          accuracy = tf.metrics.accuracy(\r\n                  labels=labels,\r\n                  predictions=class_predictions,\r\n                  name='accuracy')\r\n          tf.summary.scalar('loss', loss)\r\n         \r\n\r\n      \r\n      if mode == TRAIN:\r\n          train_op = get_train_op_fn(loss)\r\n      \r\n      if mode == EVAL:\r\n          eval_metric_ops = {\r\n              'accuracy': accuracy\r\n          }\r\n      tf.summary.scalar('accuracy', accuracy[1])\r\n\r\n\r\n      if mode == PREDICT:\r\n          predictions = {\r\n              'classes': class_predictions,\r\n              'probabilities': tf.nn.softmax(logits, name='softmax_tensor')\r\n          }\r\n\r\n\r\n      return tf.estimator.EstimatorSpec(\r\n          mode=mode,\r\n          predictions=predictions,\r\n          loss=loss,\r\n          train_op=train_op,\r\n          eval_metric_ops=eval_metric_ops\r\n\r\n      )\r\n\r\n\r\n    class Dataset():\r\n        def __init__(self, image_shape = (75,75,3),n_class=2):\r\n            self.image_shape = image_shape\r\n            self.n_classes = n_class\r\n            self._iter = 0\r\n\r\n        def get_data_train(self, filenames, batch_size,training_steps):\r\n            import pandas as pd\r\n            def parser(serialized_example):\r\n                \"\"\"Parses a single tf.Example into image and label tensors.\"\"\"\r\n                features = tf.parse_single_example(\r\n                    serialized_example,\r\n                    features={\r\n                        'image_raw': tf.FixedLenFeature([], tf.string),\r\n                        'label': tf.FixedLenFeature([1], tf.int64)\r\n                    })\r\n                im = tf.io.decode_raw(features['image_raw'],  tf.float32)\r\n                lb = tf.cast(features['label'], tf.int64)\r\n                im = tf.reshape(im,[self.image_shape[0],self.image_shape[1],3])\r\n                im = tf.cast(im, tf.float32)\r\n                return im, lb[0]\r\n            dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=2)\r\n            dataset = dataset.repeat(training_steps)\r\n            dataset = dataset.shuffle(300)\r\n            dataset = dataset.map(parser, num_parallel_calls=10)\r\n            dataset = dataset.batch(batch_size)\r\n            dataset = dataset.prefetch(buffer_size=10000)\r\n            return dataset\r\n\r\n        def get_data_val(self, filenames, batch_size):\r\n            import pandas as pd\r\n            def parser(serialized_example):\r\n                \"\"\"Parses a single tf.Example into image and label tensors.\"\"\"\r\n                features = tf.parse_single_example(\r\n                    serialized_example,\r\n                    features={\r\n                        'image_raw': tf.FixedLenFeature([], tf.string),\r\n                        'label': tf.FixedLenFeature([1], tf.int64)\r\n                    })\r\n                im = tf.io.decode_raw(features['image_raw'], tf.float32)\r\n                lb = tf.cast(features['label'], tf.int64)\r\n                im = tf.reshape(im,[self.image_shape[0],self.image_shape[1],3])\r\n                im = tf.cast(im, tf.float32)\r\n                return im, lb[0]\r\n            dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=2)\r\n            dataset = dataset.map(parser, num_parallel_calls=10)\r\n            dataset = dataset.batch(batch_size)\r\n            dataset = dataset.prefetch(buffer_size=10000)\r\n            return dataset\r\n\r\n\r\n    def get_train_op_fn(loss):\r\n      optimizer = tf.train.AdamOptimizer(learning_rate)\r\n      train_op = optimizer.minimize(\r\n          loss=loss,\r\n          global_step=tf.train.get_global_step())\r\n      return train_op\r\n    from hops import hdfs\r\n    vgg_model_path = hdfs.project_path() + 'python_resource/vgg16_weights.npz'\r\n\r\n\r\n    \r\n\r\n\r\n   \r\n\r\n    FLAGS.command = 'train'\r\n    FLAGS.augmentation_params = AUGMENTATION_PARAMS\r\n    FLAGS.metrics = ['acc', 'mean_acc']\r\n    FLAGS.data_dir = DATA_ROOT_PATH\r\n    FLAGS.augmentation_params = AUGMENTATION_PARAMS\r\n    FLAGS.n_epochs = training_steps\r\n    FLAGS.batch_size = batch_size\r\n    FLAGS.optimizer = OPTIMIZER\r\n    FLAGS.learning_rate = learning_rate\r\n    FLAGS.batch_norm = batch_norm\r\n    FLAGS.dropout_rate = drop_out\r\n    FLAGS.debug = True\r\n    FLAGS.model_name = 'new_vgg'\r\n    FLAGS.saved_variables = None\r\n    FLAGS.vgg16_weights_path = vgg_model_path\r\n    FLAGS.use_pre_trian = pre_train\r\n    FLAGS.use_transpose_conv = transpose_conv\r\n    FLAGS.init = init\r\n  \r\n        train_filenames = [hdfs.project_path() + \"Data/SAR- \r\n                  IceSea/train_iceSea_zeros_255_patchsize_32_thinIce_IA_HH_HV_scaled_mine.tfrecords\"]\r\n        validation_filenames = [hdfs.project_path() + \"Data/SAR- \r\n                  IceSea/test_iceSea_zeros_255_patchsize_32_thinIce_IA_HH_HV_scaled_mine.tfrecords\"]\r\n        dataset = Dataset((32,32,3),2)\r\n        FLAGS.input_fn_train = dataset.get_icesea\r\n        FLAGS.input_fn_val = dataset.get_icesea_val\r\n        FLAGS.dataset = dataset\r\n   \r\n\r\n   \r\n    \r\n\r\n    config = tf.estimator.RunConfig(train_distribute=tf.contrib.distribute.MirroredStrategy(),\r\n              model_dir=tensorboard.logdir(),\r\n              save_summary_steps=200,\r\n              log_step_count_steps=200,\r\n              save_checkpoints_steps=200)\r\n    \r\n    model_estimator = build_estimator(config, tensorboard.logdir(), params = FLAGS)\r\n    train_spec = tf.estimator.TrainSpec(\r\n       input_fn=lambda: FLAGS.input_fn_train(train_filenames, \r\n           batch_size,training_steps),max_steps=training_steps)\r\n\r\n    eval_spec = tf.estimator.EvalSpec(\r\n       input_fn=lambda: FLAGS.input_fn_val(validation_filenames, batch_size),\r\n       steps=None,\r\n       start_delay_secs=30,  # Start evaluating after 10 sec.\r\n       throttle_secs=10  # Evaluate only every 30 sec\r\n    )\r\n\r\n\r\n    tf.estimator.train_and_evaluate(model_estimator, train_spec, eval_spec)", "Hi!  This is odd.\r\n\r\nIs it the same if you 1) use tf.distribute.MirroredStrategy instead of the contrib one? 2) use RunConfig.experimental_distribute.train_distribute instead of RunConfig.train_distribute?\r\n\r\nIf it's the same, could you include the full picture of the graph?", "Thanks for your answer. Yes it is strange\r\n\r\nFor 1 I can not use tf.distribute.MirroredStrategy. I got 'module' object has no attribute 'distribute' error. I am using the 1.11.0 version.\r\nFor 2 Also I have same kind of error. But in this case, I am not sure if I am using it correctly. Cloud you please send me an example for this.\r\nI tried to attached complete graph. however, it is too large. I hope these can be helpful\r\n\r\n![image](https://user-images.githubusercontent.com/17527773/64701243-b94df900-d4a8-11e9-8125-a3ebc62b3041.png)\r\n\r\n![image](https://user-images.githubusercontent.com/17527773/64701288-cf5bb980-d4a8-11e9-96df-f37b37b46236.png)\r\n\r\n\r\n\r\n\r\n\r\n", "I  have the same problem", "Hi, can you please try with latest version of TF and see if you still have the same problem?", "I still have this problem and could not solve it> I need some clue to track the problem\r\n", "@guptapriya I see the same problem of slowness with MirroredStrategy today, using tf-nightly==2.4.0.dev20200701\r\n\r\nCould we please get some more elaborated answer regarding what could be the problem? Some of us have invested time coding for using this assuming that it was production ready/tested and provided performance gains.", "@oscaralvaro MirroredStrategy is certainly production ready and tested. Can you please open another bug with your use case? the current bug uses TF 1 and contrib layers etc which we are no longer supporting. \r\nPlease provide code to repro, as well as use the profiler to get profiles during training and share the profiles. See these links for how to use the profiling:\r\nhttps://www.tensorflow.org/guide/profiler\r\nhttps://www.tensorflow.org/tensorboard/tensorboard_profiling_keras\r\n\r\n\r\n\r\n\r\n", "cc @nikitamaia ", "@guptapriya thank you for your response. It is not trivial to isolate an example because I'm using it in conjunction with keras-tuner and preprocessing layers. I will try to create an example where I can observe the same slow effect when using MirroredStrategy. However, before that I wanted to know if there is any guidelines/documentation regarding the performance considerations when using TF with multi-gpu. So far in the documentation I only could find recommendations regarding the global_batch_size and learning rate tuning", "@nikitamaia will be working on such a guide specifically for multi GPU. In the meantime, some of the common guidelines from here will also apply if input pipeline is the bottleneck, which can often be the case:\r\nhttps://www.tensorflow.org/guide/data_performance\r\n\r\nFor using the profiler, you don't need to isolate the example. So perhaps you should try running the profiler. The profiler output often gives valuable suggestions like whether input pipeline is bottleneck or something else. \r\n\r\n\r\n", "Thank you @guptapriya for the suggestions. I'm going to run the profiler first and see if I can fix the issue before spending time isolating the example. Looking forward for the performance in multi-gpu guide by @nikitamaia ", "@guptapriya I've created the issue https://github.com/tensorflow/tensorflow/issues/41213, with code to reproduce the behavior. In summary I observe the following difference in training time:\r\n\r\n#average step for single-gpu\r\nEpoch 2/50\r\n8/8 [==============================] - 0s 5ms/step - loss: 0.7215 - accuracy: 0.5620 - val_loss: 0.6550 - val_accuracy: 0.7213\r\n\r\n#average step for multi-gpu, is more than 10 times slower!!\r\nEpoch 2/50\r\n2/2 [==============================] - 0s 224ms/step - loss: 0.6756 - accuracy: 0.5702 - val_loss: 0.6435 - val_accuracy: 0.6885", "Hi  @sakh251 ! we are  checking to see if you are still looking for assistance in this issue.\r\nCould you please try on latest stable version of  TF 2.6  and let us know if this is still an issue.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 32171, "title": "import tensorflow problem ", "body": "\r\n- OS Platform and Distribution (windows 64):\r\n- \r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version:3.6.8\r\n\r\n\r\n\r\nHi \r\nWhen i import tensorflow , it just quit the code without any error .\r\n\r\nappreciate any help in advance   ", "comments": ["\n\n\n\n| |\n\u6c5f\u6c11\n\u90ae\u7bb1\uff1a18810558570@163.com\n|\n\n\u7b7e\u540d\u7531 \u7f51\u6613\u90ae\u7bb1\u5927\u5e08 \u5b9a\u5236\n\nOn 09/03/2019 18:12, RZz96 wrote:\nOS Platform and Distribution (windows 64):\nTensorFlow version (use command below): 1.14.0\nPython version:3.6.8\n\nHi\nWhen i import tensorflow , it just quit the code without any error .\n\nappreciate any help in advance\n\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.", "Can you provide all the details needed for a bug, see the template?\r\n\r\nCan you provide a demo of the behavior you're seeing?", "> \r\n> \r\n> Can you provide all the details needed for a bug, see the template?\r\n> \r\n> Can you provide a demo of the behavior you're seeing?\r\n\r\n\r\n``` console\r\nC:\\>python\r\nPython 3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 18:50:55) [MSC v.1915 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nC:\\>\r\n```\r\n\r\nHi thanks for the reply \r\nAs u see after importing the tensorflow ,  it just quit.\r\n", "@RZz96\r\n\r\nAs per my understanding you are able to install tensorflow successfully but while importing tensorflow it is going to quit from command prompt. Please, let me know my understanding is correct? Thanks!", "> \r\n> \r\n> @RZz96\r\n> \r\n> As per my understanding you are able to install tensorflow successfully but while importing tensorflow it is going to quit from command prompt. Please, let me know my understanding is correct? Thanks!\r\n\r\nyes exactly . i think i have to install another version of python . may be 3.7 .\r\nis there any other way to solve the problem rather than the new installation of python ?", "Can you try `python -v` instead of `python` and repeat the same import? You'll have a large output after that, please share it.", "I did , the result is the same , just this time it printed all the modules and processes in details.\r\njust to mention that im running it on virtual env \r\n\r\n ", "Yes, but printing them might hide away the crash. That's why I suggested sharing the output", "Hi here i have attached the output.\r\n[output.txt](https://github.com/tensorflow/tensorflow/files/3578277/output.txt)\r\n", "Ok, so there's a segfault coming from a dependency. what version of `h5py` gets installed on your system?\r\n", "it is 2.9.0", "Can you try with `pip install tensorflow==2.0.0rc0` instead of `1.14`?", "Another option is to try having a file `myfile.py` with just `import tensorflow` and see if you can do `python myfile.py`", "> \r\n> \r\n> Another option is to try having a file `myfile.py` with just `import tensorflow` and see if you can do `python myfile.py`\r\nno . i already tried and didnt work . im installing the tensorflow 2", "And the third option (although you might want this first) is to run python in a debugger.\r\n\r\nFor example, here's what I'd do if `gdb` were present (needs some setup on Windows)\r\n\r\n```console\r\n(3) mihaimaruseac@ankh:/tmp/gh/3$ gdb -q python\r\nReading symbols from python...(no debugging symbols found)...done.\r\n(gdb) r\r\nStarting program: /tmp/gh/3/bin/python \r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\nPython 2.7.16 (default, Apr  6 2019, 01:42:57) \r\n[GCC 7.3.0] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n[New Thread 0x7ffff3ae9700 (LWP 142370)]\r\n[New Thread 0x7ffff32e8700 (LWP 142371)]\r\n....\r\n>>> print(tf.__version__)\r\n2.0.0-dev20190905\r\n>>> \r\n[Inferior 1 (process 142358) exited normally]\r\n(gdb) quit\r\n```", "the new installation didnt work .\r\ni never used gdb for python debugging . i will try it and let you know.", "working with gdb in windows needs a lots of set up which i dont want to go trough .\r\nif you dont know any other solution , then may be is better to install the python 3.7 to resolve the problem.", "See also #31431. Some people have had troubles installing tensorflow on python3.7\r\n\r\nThere should be other debuggers on Windows which can be used.", "Thanks i will check it ", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32171\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32171\">No</a>\n"]}, {"number": 32170, "title": "request of SparseApplyFtrlOp running on gpu", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source):  1.13.1\r\n\r\n\r\nCurrently, I notice that `SparseApplyFtrlOp` only supports running on cpu platform. Seeing from here:\r\nhttps://github.com/tensorflow/tensorflow/blob/d5c6687d9919c562bea2a01a6e1be1756bfaab33/tensorflow/core/kernels/training_ops.cc#L2448\r\n\r\nHowever, in some circumstances, there's great advantage of supporting this  `SparseApplyFtrlOp` in time performance. So is there any plan to support this op on gpu, and if not, what's the reason? Thanks", "comments": ["@jvishnuvardhan @gadagashwini Any update? Thanks", "@aaroey hi, is there any update?", "@reedwm could you help to take a look? Thanks.", "@tanzhenyu, I see you added the tf.keras Ftrl optimizer. Are there any plans to support GPUs with sparse gradients?", "There are plans to support GPU. However specifically ftrl might not be the prioritized item in the plan", "Ok, I'll assign to you for now", "@tanzhenyu hi, can you be more specific, about what kind of gpu optimizer will be supported, sparse optimizer or dense optimizer?", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32170\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32170\">No</a>\n"]}, {"number": 32169, "title": "[Intel MKL] Upgrading giflib to fix CVE-2019-15133", "body": "# [CVE-2019-15133](https://nvd.nist.gov/vuln/detail/CVE-2019-15133)\r\n\r\n**NVD**: 2019/08/17 - CVSS v2.0 Base Score: 4.3 - CVSS v3.0 Base Score: 6.5\r\nIn GIFLIB before 2019-02-16, a malformed GIF file triggers a divide-by-zero exception in the decoder function DGifSlurp in dgif_lib.c if the height field of the ImageSize data structure is equal to zero.\r\n\r\n## **References to Advisories, Solutions, and Tools**\r\n\r\nSource | Link | Type\r\n---- | ---- | ----\r\nMISC | [bugs.chromium.org](https://bugs.chromium.org/p/oss-fuzz/issues/detail?id=13008) | Mailing List, Third Party Advisory\r\nUBUNTU | [usn.ubuntu.com](https://usn.ubuntu.com/4107-1/) | Third Party Advisory\r\n\r\n", "comments": ["@rthadur This CVE doesn't just apply to MKL builds.", "@penpornk @gunan May I suggest that this be cherry-picked?", "@goldiegadde this should be cherrypicked into 1.15 and 2.0", "@gunan @claynerobison @goldiegadde I see a Windows Bazel failure (in some other PR's test results) that seems to originate from this PR.\r\n\r\n```\r\ngif.lib(gif_font.obj) : error LNK2019: unresolved external symbol strtok_r referenced in function GifDrawBoxedText8x8\r\n```\r\n[Full log](https://source.cloud.google.com/results/invocations/bd42ab39-0f56-4edc-b789-d16d76348800/log).\r\n\r\n(The error doesn't show up in this PR's test results because the Windows tests were already failing from CSRSparseMatrix changes at the time.)\r\n\r\nPlease don't cherrypick this PR yet. I'll fix this and cherrypick the changes afterwards.", "Since this is affecting all other Windows tests, we are reverting the PR for now.", "Please assign to me (or notify me) when fixing"]}, {"number": 32168, "title": "Port mul from Tensorflow Lite to Tensorflow Lite Micro", "body": "This PR port the mul kernel to Tensorflow Lite Micro. Supported data types are float, int8 and uint8.", "comments": ["@petewarden Do you have the time to look at this? Thanks!", "There seems to be some build errors related to multiple definitions of Mul(). I'll have a look at them.", "It seems that there is some problem with the \"Ubuntu CC\" test. It has been waiting for status for a few days now, and I've seen the same things for other PRs.", "Ready to merge.", "Ready to merge.", "Ready to merge.", "There's a compile error related to the Register_* functions being moved to micro_ops.h. I'll fix it ASAP.\r\n\r\nEdited: Fixed in the latest commit.", "Ready to merge.", "Ready to merge.", "Ready to merge.", "Ready to merge.", "Sorry for the delay on this one, I am chasing it up internally!", "@petewarden Thanks!", "`ERROR: /tmpfs/tensor_flow/tensorflow/lite/experimental/micro/kernels/BUILD:181:1: Couldn't build file tensorflow/lite/experimental/micro/kernels/portable_optimized_depthwise_conv_test_binary: Linking of rule '//tensorflow/lite/experimental/micro/kernels:portable_optimized_depthwise_conv_test_binary' failed (Exit 1)\r\nbazel-out/k8-opt/bin/tensorflow/lite/experimental/micro/kernels/_objs/portable_optimized_ops_resolver/all_ops_resolver.o:all_ops_resolver.cc:function tflite::ops::micro::AllOpsResolver::AllOpsResolver(): error: undefined reference to 'tflite::ops::micro::Register_MUL()'\r\ncollect2: error: ld returned 1 exit status`\r\n\r\n@jenselofsson can you please check this error ?\r\ncc @petewarden ", "This is surprising, since the .cc should automatically be included in the library sources here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/micro/tools/make/Makefile#L93\r\n\r\nI'll investigate on our end what's going wrong.", "This is an issue with one of our Bazel build rules. I have a one-line fix (adding mul.cc to :portable_optimized_micro_ops in the kernel BUILD file), but longer-term we'll fix this so that we don't have to repeat this file.\r\nWe will make this change on our side, no action is needed from you Jens!", "Changes have been merged , auto merge did not happen so closing this PR , thank you ", "Thanks for looking into this @petewarden !"]}, {"number": 32167, "title": "transformer implementation error", "body": "https://github.com/tensorflow/models/blob/master/official/transformer/model/transformer.py\r\nThe transfomer implementation seems to be incorrect. I cannot find the residual component, and the batchnorm is missed after every feed forward layer.", "comments": ["You may post this issue on tensorflow models repository since it belongs to that repository. Thanks!\r\nhttps://github.com/tensorflow/models/issues", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32167\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32167\">No</a>\n"]}, {"number": 32166, "title": "fixed __thread define usage for i386 build", "body": "- make use of the `__thread` define specified in the makefile\r\n- used inside `work_sharder.cc`", "comments": ["closing this PR as contrib folder will be depricated in 2.0, thank you.\r\nCC @mihaimaruseac", "@rthadur okay, was this issue adressed in 2.0?", "What is the problem that you try to solve, though? PR only mentions the list of changes (none of which are in master or 2.0)"]}, {"number": 32165, "title": "Include atomic headers", "body": "Change made in order to fix #32164", "comments": []}, {"number": 32164, "title": "Tensorflow lite build failed : \u2018atomic\u2019 in namespace \u2018std\u2019 does not name a type", "body": "This issue happened while building tensorflow lite into AWS Lambda docker\r\n\r\n**System information**\r\n- OS platform and distribution : [docker image lambci/lambda:build-python3.6](https://hub.docker.com/r/lambci/lambda/tags)\r\n- TensorFlowLite pip package built from source\r\n- Tensorflow version : master (currently on 496acff)\r\n- Python version: 3.6\r\n- Compiler version : \r\n  * clang version 3.6.2 (tags/RELEASE_362/final)\r\n  * gcc version 4.8.5 20150623 (Red Hat 4.8.5-28) (GCC) \r\n  * ldd (GNU libc) 2.17\r\n\r\n**Describe the problem**\r\n\r\nUsing this Dockerfile\r\n\r\n```\r\nFROM lambci/lambda:build-python3.6\r\n\r\nWORKDIR /var/task\r\n\r\nRUN yum update -y && yum -y install swig libjpeg-devel zlib-devel wget\r\n\r\nRUN git clone https://code.googlesource.com/re2\r\nWORKDIR /var/task/re2\r\nRUN make && make test && make install && make testinstall\r\n\r\nWORKDIR /var/task/\r\n\r\nRUN wget https://github.com/google/googletest/archive/release-1.8.0.tar.gz &&\\\r\n    tar xf release-1.8.0.tar.gz\r\n\t\t\r\nWORKDIR /var/task/googletest-release-1.8.0\r\n\r\nRUN cmake -DBUILD_SHARED_LIBS=ON . &&\\\r\n    make &&\\\r\n\t\tmake install\r\n\t\t\r\nWORKDIR /var/task\r\nRUN pip install numpy\r\nRUN git clone https://github.com/tensorflow/tensorflow\r\nWORKDIR /var/task/tensorflow\r\n\r\nRUN git checkout 496acff\r\nRUN bash ./tensorflow/lite/tools/pip_package/build_pip_package.sh\r\n```\r\n\r\nIt fails with : \r\n\r\n```\r\n./tensorflow/lite/kernels/acceleration_test_util_internal.h: In function \u2018absl::optional<T> tflite::GetAccelerationTestParam(std::string)\u2019:\r\n./tensorflow/lite/kernels/acceleration_test_util_internal.h:69:10: error: \u2018atomic\u2019 in namespace \u2018std\u2019 does not name a type\r\n   static std::atomic<std::vector<ConfigurationEntry<T>>*> test_config_ptr;\r\n```\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32164\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32164\">No</a>\n"]}, {"number": 32163, "title": "[1.14] Cannot use a custom op with `TPUStrategy` that works with `MirroredStrategy`", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc91b5 1.14.0\r\n- Python version: Python 3.6.8\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: 10\r\n- GPU model and memory: n/a\r\n\r\n**Describe the current behavior**\r\n\r\nCopied from https://github.com/google/sentencepiece/issues/390 since this might be a `TPUStrategy` issue, since things work with `MirroredStrategy`.\r\n\r\n[SentencePiece](https://github.com/google/sentencepiece) is a library that provides extremely fast and efficient utilities for text encoding during training. It accomplishes this by providing custom ops written in C++, instead of userland code. The custom ops are detected and work with regular TensorFlow and in distributed GPU mode with `MirroredStrategy`, but are not getting detected when using `TPUStrategy`. The custom op makes training ~3x faster, compared to using something like [SubwordTextEncoder](https://www.tensorflow.org/datasets/api_docs/python/tfds/features/text/SubwordTextEncoder).\r\n\r\n**Describe the expected behavior**\r\n\r\nThe custom ops themselves should work with TPUs so I can train on TPUs without compromising the performance of my data pipeline.\r\n\r\n**Code to reproduce the issue**\r\n\r\nhttps://colab.research.google.com/gist/suyash/286ba54a1cd6d51fba5c2f091b4900ce\r\n", "comments": ["Unfortunately, custom ops are not supported on TPUs right now, since end users cannot control the custom TensorFlow instance that is running on the TPUs.\r\n\r\nWe are investigating various ways to overcome this limitation, but don't have anything to announce right now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32163\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32163\">No</a>\n"]}, {"number": 32162, "title": "[lite doc] broken link in`TensorFlow Lite and TensorFlow operator compatibility`", "body": "The link for `tf.transpose` on page https://www.tensorflow.org/lite/guide/ops_compatibility#compatible_operations\r\nis broken.", "comments": ["Thanks. Looks like this only exists in TF2: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/transpose\r\nIt will (eventually) sort itself out, but it shouldn't be linked in TF1", "Ehh, the links on that page should all use backticks for API symbols and not absolute URLs. I'll fix it", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32162\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32162\">No</a>\n"]}, {"number": 32161, "title": "tfcompile with negative one shape", "body": "I am trying to compile a tensorflow model, using the following [tutorial](https://barkeywolf.consulting/posts/tf-aot-rust/#prepare-the-tensorflow-graph-for-compilation), which is based on [tf documentation](https://www.tensorflow.org/xla/tfcompile).\r\n<br>\r\nI am doing this in the latest `tensorflow/tensorflow` docker.\r\n<br>\r\nAll the tutorials I have seen deal with image processing, but my models receive one-dimensional vector. The config file looks like this:\r\n\r\n```\r\nfeed {\r\n  id {\r\n    node_name: \"dense_1_input\"\r\n  }\r\n  shape {\r\n    dim {\r\n      size: -1\r\n    }\r\n    dim {\r\n      size: 5\r\n    }\r\n  }\r\n}\r\nfetch {\r\n  id {\r\n    node_name: \"k2tfout_0\"\r\n  }\r\n}\r\n```\r\n\r\nIf I use it, the compilation fails. If I change `-1` for `1`, the model compiles, but whenever I try to use it, I get a core dump.\r\n<br>\r\nCould this be an issue with this negative one dimension? How do I compile such models?\r\n<br>\r\nI tried invoking my model using Python and flipping dimension numbers. (1,5) \u2013 (5,1) \u2013 (5,)...\r\n<br>E.g.:\r\n\r\n```\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\nlibmodel = np.ctypeslib.load_library('libmodel', '/folder/org_tensorflow/')\r\n\r\nlibmodel.run.argtypes = [np.ctypeslib.ndpointer(np.float32, ndim=2, shape=(1,5), flags=('c', 'a')),\r\nnp.ctypeslib.ndpointer(np.float32, ndim=2, shape=(1,5), flags=('c', 'a', 'w')),\r\nnp.ctypeslib.ctypes.c_float,\r\nnp.ctypeslib.ctypes.c_float]\r\n\r\ndf = pd.read_csv('/folder/trial.csv', header = 0, index_col = None)\r\nfeats = pd.read_table('/folder/feature_list.txt', header=None, index_col = None)\r\nfeats = feats.iloc[:,0].tolist()\r\ndf = df.reindex(columns=feats, fill_value=0)\r\n\r\nx = np.require(df.iloc[0,:], np.float32, ('c', 'a'))\r\ny = np.require(np.zeros((1,1)), np.float32, ('c', 'a', 'w'))\r\n\r\nx = x.reshape((1,5))\r\nlibmodel.run(x, y, x.size, y.size)\r\n```\r\n\r\nIs it possible to compile such models with negative one dimension? If so, it should be in documentation.", "comments": ["@lotrus28, In order to expedite the trouble-shooting process, please provide a complete code snippet to reproduce the issue reported here. Thanks!\r\n", "> @lotrus28, In order to expedite the trouble-shooting process, please provide a complete code snippet to reproduce the issue reported here. Thanks!\r\n\r\nHere is the docker file, which is basically [`stakemura/tfcompile`](https://hub.docker.com/r/stakemura/tfcompile) \r\n\r\n```\r\nFROM tensorflow/tensorflow:latest-devel-py3\r\n\r\nRUN git clone --branch=master --no-progress https://github.com/tensorflow/tensorflow.git /tensorflow\r\n\r\nWORKDIR /tensorflow\r\nENV TF_NEED_JEMALLOC=1 \\\r\n    TF_NEED_GCP=0 \\\r\n    TF_NEED_HDFS=0 \\\r\n    TF_NEED_AWS=0 \\\r\n    TF_NEED_KAFKA=0 \\\r\n    TF_ENABLE_XLA=0 \\\r\n    TF_NEED_GDR=0 \\\r\n    TF_NEED_VERBS=0 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n    TF_NEED_OPENCL=0 \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_CUDA_CLANG=0 \\\r\n    TF_NEED_ROCM=0 \\\r\n    TF_NEED_MKL=0 \\\r\n    TF_DOWNLOAD_MKL=0 \\\r\n    TF_DOWNLOAD_CLANG=0 \\\r\n    TF_NEED_MPI=0 \\\r\n    TF_NEED_S3=0 \\\r\n    TF_SET_ANDROID_WORKSPACE=0 \\\r\n    TF_NEED_COMPUTECPP=0 \\\r\n    TF_NEED_TENSORRT=0 \\\r\n    TF_CONFIGURE_IOS=0 \\    \r\n    CC_OPT_FLAGS=\"-march=native -Wno-sign-compare\" \r\nRUN export PYTHON_BIN_PATH=$(which python3);\\\r\n    export PYTHON_LIB_PATH=\"$($PYTHON_BIN_PATH -c 'import site; print(site.getsitepackages()[0])')\";\\\r\n    python3 configure.py\r\nRUN bazel build -c opt --verbose_failures //tensorflow/compiler/aot:tfcompile\r\nRUN\texit 0\r\n```\r\n\r\nThen I copy these files to the tensorflow directory:\r\n\r\n> BUILD\r\n```\r\n\r\nload('@org_tensorflow//tensorflow/compiler/aot:tfcompile.bzl', 'tf_library')\r\n\r\ntf_library(\r\n    name = 'graph',\r\n    config = 'graph.config.pbtxt',\r\n    cpp_class = 'Graph',\r\n    graph = 'graph.pb',\r\n)\r\n\r\ncc_binary(\r\n    name = \"libmodel.so\",\r\n    srcs = [\"graph.cc\"],\r\n    deps = [\":graph\", \"//third_party/eigen3\"],\r\n    linkopts = [\"-lpthread\"],\r\n    linkshared = 1,\r\n    copts = [\"-fPIC\"],\r\n)\r\n```\r\n\r\n> graph.cc\r\n```\r\n#define EIGEN_USE_THREADS\r\n#define EIGEN_USE_CUSTOM_THREAD_POOL\r\n\r\n#include \"graph.h\"\r\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\r\n\r\nextern \"C\" int run(\r\n\t\tfloat *input,\r\n\t\tfloat *output_age,\r\n\t\tint input_size,\r\n\t\tint output_age_size\r\n    ){\r\n\t// allocate an instance of the Graph, along with all its private buffers\r\n\tEigen::ThreadPool tp(std::thread::hardware_concurrency());\r\n\tEigen::ThreadPoolDevice device(&tp, tp.NumThreads());\r\n\tGraph graph;\r\n\tgraph.set_thread_pool(&device);\r\n\r\n\t// copy over the input buffer\r\n\tstd::copy(input, input + input_size, graph.arg0_data());\r\n\r\n\t// execute the inference graph\r\n\tauto ok = graph.Run();\r\n\tif (not ok) return -1;\r\n\r\n\t// copy the result into the output buffer\r\n\tstd::copy(graph.result0_data(), graph.result0_data() + output_age_size, output_age);\r\n\treturn 0;\r\n}\r\n```\r\n\r\nI have already shown you my `graph.config.pbtxt`. \r\n\r\nTo start compiling I do \r\n`bazel build --show_progress_rate_limit=60 @org_tensorflow//:libmodel.so`\r\n\r\nIf I check the model's shape in Python:\r\n```\r\n>>m.input_shape\r\n>> (None, 5)\r\n```\r\nBut saving it as `.pb` produces a `-1` dimension in the human readable layers' descriptions. \r\n\r\n\\===\r\nShould I provide anything more?", "Seems like `-1` in the inputs stands for a dynamic input. I.e. my model can receive 1 or 10 or 100 5-long vectors to produce an output. When I specify shape as (1,5) in the config, it compiles. Does that mean that the compiled model expects (1,5) input? Well, I do just that, when invoking:<br>\r\n`x = np.require(df.iloc[0,:], np.float32, ('c', 'a'))`<br>\r\nCould the problem be in the way I specify the output? I also tried `dim = 1`, but that did not work either.", "Ok, I tried to totally reduce the model completely and try to compile it without training. This time I defined the input shape explicitly, so that its input shape is `(1, 2)` in contrast to `(None, 2)`, if I defined it the way I used to.\r\n<br>\r\n```\r\n>> model.add(Dense(5, input_shape=(2,), batch_size = 1, kernel_initializer='uniform'))\r\n>> model.add(BatchNormalization())\r\n>> model.add(PReLU())\r\n>> model.add(Dropout(0.5))\r\n>> model.add(Dense(1, kernel_initializer='normal'))\r\n>> model.compile(loss='mean_absolute_error', optimizer='adam')\r\n>> model.save('../test.h5')\r\n```\r\nThe summary is then:<br>\r\n```\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ndense_1 (Dense)              (1, 5)                    15        \r\n_________________________________________________________________\r\nbatch_normalization_1 (Batch (1, 5)                    20        \r\n_________________________________________________________________\r\np_re_lu_1 (PReLU)            (1, 5)                    5         \r\n_________________________________________________________________\r\ndropout_1 (Dropout)          (1, 5)                    0         \r\n_________________________________________________________________\r\ndense_2 (Dense)              (1, 1)                    6         \r\n=================================================================\r\nTotal params: 46\r\nTrainable params: 36\r\nNon-trainable params: 10\r\n```\r\n`keras_to_tensorflow.py` identifies its output as `dense_2/BiasAdd` <br>\r\nThe model then compiles, but I fail to invoke it. Am I invoking it correctly?\r\n<br>\r\n```\r\nimport numpy as np\r\n\r\nprint('Starting script')\r\nlibmodel = np.ctypeslib.load_library('libmodel', '/tensorflow/bazel-bin/external/org_tensorflow/')\r\n\r\nlibmodel.run.argtypes = [np.ctypeslib.ndpointer(np.float32, ndim=2, shape=(1,2), flags=('c', 'a')),\r\nnp.ctypeslib.ndpointer(np.float32, ndim=2, shape=(1,1), flags=('c', 'a', 'w')),\r\nnp.ctypeslib.ctypes.c_float,\r\nnp.ctypeslib.ctypes.c_float]\r\n\r\nx = np.require(np.zeros((1,2)), np.float32, ('c', 'a'))\r\ny = np.require(np.zeros((1,1)), np.float32, ('c', 'a', 'w'))\r\n\r\nres = libmodel.run(x, y, x.size, y.size)\r\nprint(res)\r\n```\r\nIs it an invocation problem? Or is it impossible to compile this model?  If so, what modifications can I do to make it possible, while preserving the weights?", "I have attached the compilation log for a model with just 2 Dense layers.\r\nAs before, it compiles, but can't be further used. Perhaps there are some important warnings in there that could help\r\n\r\n[LOG.txt](https://github.com/tensorflow/tensorflow/files/3579259/LOG.txt)\r\n", "@lotrus28 What do you mean by the following. \r\n\r\n> > it compiles, but can't be further used.\r\n\r\n \r\nHere is the [gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/1665ec9a6ba00b2e001f8c37cf86eb54/tf32161.ipynb). Can you check my gist and add the part that is not working? Thanks!", "> \r\n> \r\n> @lotrus28 What do you mean by the following.\r\n> \r\n> > > it compiles, but can't be further used.\r\n> \r\nI mean that if I use `tfcompile` to convert a network into binary, I do get an `.so` file, but during invocation I get core dump error.<br>\r\nThe invocation fails even if I start with a model containing only 2 dense layers.\r\nI tried fixing the batch dimension and feeding and reshaping the input, but I still get a segmentation error. So, most likely the error is in this part\r\n\r\n```\r\nx = np.require(np.zeros((1,2)), np.float32, ('c', 'a'))\r\ny = np.require(np.zeros((1,1)), np.float32, ('c', 'a', 'w'))\r\nres = libmodel.run(x, y, x.size, y.size)\r\n```\r\n\r\nHow would you run this sample network?", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32161\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32161\">No</a>\n"]}]