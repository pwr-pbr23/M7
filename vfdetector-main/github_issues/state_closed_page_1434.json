[{"number": 9951, "title": "Would you please accomodate for building tensorflow with a custom clang (4.0.0) and libc++ instead of stdlibc++? ", "body": "I have a custom clang with additional optimization passes but I cant get TS compiled with it. \r\n\r\n$ bazel build --cxxopt=-std=c++11 --cxxopt=-stdlib=libc++ tensorflow:libtensorflow.so\r\nINFO: Found 1 target...\r\nINFO: From Compiling external/protobuf/src/google/protobuf/compiler/js/embed.cc [for host]:\r\nexternal/protobuf/src/google/protobuf/compiler/js/embed.cc:37:12: warning: unused variable 'output_file' [-Wunused-const-variable]\r\nconst char output_file[] = \"well_known_types_embed.cc\";\r\n           ^\r\n1 warning generated.\r\nERROR: /home/hbucher/.cache/bazel/_bazel_hbucher/ad427c7fddd5b68de5e1cfaa7cd8c8cc/external/com_googlesource_code_re2/BUILD:11:1: undeclared inclusion(s) in rule '@com_googlesource_code_re2//:re2':\r\nthis rule is missing dependency declarations for the following files included by 'external/com_googlesource_code_re2/re2/bitstate.cc':\r\n  '/home/hbucher/install/include/c++/v1/stddef.h'\r\n  '/home/hbucher/install/include/c++/v1/__config'", "comments": ["Thank  you @HFTrader, noted. Support for custom builds of clang isn't a high priority for us right now, but I'm opening this up for community support.", "Can we close this?", "who really cares - it's been +4 years", "Hi @HFTrader ,Could you please check out the new [Tensorflow manual](https://www.tensorflow.org/install/source) here for building from Source.  \r\nAlso It  seems that you are using old version of Tensorflow which is officially considered as end of life, We recommend that you upgrade to 2.6 version and let us know if the issue still persists in newer versions .Please open a new issue in case you face any errors, we will get you the right help .Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/9951\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/9951\">No</a>\n"]}, {"number": 9950, "title": "New reader for LMDB databases", "body": "Implement a reader op to access records stored in an LMDB database.\r\nThis facilitates Caffe users to switch to TensorFlow without rebuilding their (potentially very large) LMDB datasets.", "comments": ["Can one of the admins verify this patch?", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins test this please", "@bowang Your tests are failing because the key is a bytearray, not a string. Perhaps you can re-use the _ExpectRead method here: https://github.com/bowang/tensorflow/blob/89297b6c00bd00181c1522cf84e84c5e7ec10baa/tensorflow/python/kernel_tests/reader_ops_test.py#L237", "@rmlarsen Thanks for the help! Do you have any idea why cmake cannot find `lmdb.h` while others can?\r\nhttps://ci.tensorflow.org/job/tensorflow-pull-requests-cpu-cmake/2960/console\r\n", "@tensorflow-jenkins test this please", "Thank you very much for the help and guiding me through this process! @rmlarsen @jart \r\nIn cmake build, it still complains failing to find `lmdb.h`. Any idea how I can fix that?", "I think you might need to add external dependencies to third_party somewhere here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/CMakeLists.txt\r\nand possibly a .cmake for lmdb in external/ to keep it clean.", "@bowang FYI: We are waiting for confirmation that code covered by the OpenLDAP license is allowed in TensorFlow.", "@bowang @maciekcc license is approved. ", "Jenkins test this please.", "@tensorflow-jenkins test this please", "@rmlarsen Thanks a lot for checking the license!\r\n@maciekcc Thank you for the suggestion. It really helps! I added cmake files for lmdb and succeeded in a local cmake build.", "Jenkins test this please.", "@bowang can you take a look at the test failures, please?", "@tensorflow-jenkins test this please", "@bowang I also saw some bytearray vs. string literal errors  in the last set of tests (sorry, lost the link). Mind taking a look?\r\n@tensorflow-jenkins test this please", "@rmlarsen Definitely. Thanks for looking into the log. I will try test it on Python 3 locally.", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "Finally all checks have passed! Thanks for the help and guidance! @rmlarsen ", "Can we merge this PR?", "Thanks for the contribution! Merging now."]}, {"number": 9949, "title": "Convolution of zero length input gives junk gradients", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.1\r\n- **Bazel version (if compiling from source)**: 0.4.5\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: python3 junk_gradients.py\r\n\r\n### Describe the problem\r\nWhen you pass a zero length input to tf.conv1d and then calculate gradients, you will get junk values. I think it's reading from uninitialized memory because it's nondeterministic and can be very small or very large positive or negative values or NaNs.\r\n\r\nThe expected behavior is that the gradients should all be 0 because the weights aren't making any contribution to the loss.\r\n\r\nIn the application I am writing the input has a variable length (including 0) but for this minimal example I've set it to a constant (tf.ones([0,2])).\r\n\r\nIf you force the convolution to always have an input with length > 0 then the bug goes away. I've included that in the reproduction code under the variable 'remove_bug'.\r\n\r\nFor me, with this reduced example, the gradients are always junk but vary widely. You might see 0s if it happens to read from zeroed out memory. Hopefully the bug will show up if you just run it a few times.\r\n\r\n### Source code / logs\r\n```python\r\nimport tensorflow as tf\r\n\r\nremove_bug = False\r\n\r\nvals = tf.ones([0,2])\r\n\r\nif remove_bug: # hack it to not actually have zero length\r\n    vals = tf.concat([tf.ones([2, 2]), vals], 0)\r\n\r\n# At this point 'vals' will either have length 0 or 2 if the bug was removed\r\n\r\nfilter = tf.Variable(tf.ones([2, 2, 2]))\r\n\r\nconv = tf.nn.conv1d(tf.expand_dims(vals, 0), filter, 2, 'SAME')[0]\r\n\r\n# At this point 'conv' will either have length 0 or 1 if the bug was removed\r\n\r\nif remove_bug:\r\n    conv = conv[1:] # slice off hack, make 'conv' zero length again\r\n\r\n# At this point 'conv' will have length 0 whether or not the bug was removed.\r\n\r\noptimizer = tf.train.GradientDescentOptimizer(0.01)\r\n\r\ngrads = [g for g, _ in optimizer.compute_gradients(conv)]\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n\r\n    gs = sess.run(grads)\r\n\r\nprint(gs)\r\n```", "comments": ["Hi @mscheifer, thanks for reaching out. Unfortunately, I am not able to reproduce the problem. Running the code that you provided above seems to return a vector filled with None(s). This is the expected output of tf.gradients(), for None denotes that the outputs have no graph dependency on the specified inputs.", "If this continues to be a problem, please consider asking this question on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow). The larger community there may be able to help debug. If you do discover a bug, feel free to reopen this issue or open a new one. Thanks!"]}, {"number": 9948, "title": "Return GitHub links for models to tensorflow.org/code links", "body": "https://tensorflow.org/code/tensorflow_models is now redirecting properly.\r\n\r\ncc @Carmezim and @caisq", "comments": ["@nealwu great! It seems [word2Vec](https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow_models/tutorials/embedding/word2vec.py) continues not working though\r\nhttps://www.tensorflow.org/tutorials/word2vec", "Thanks for pointing that out @Carmezim. I added some changes for the word2vec tutorial as well.", "@tensorflow-jenkins test this please"]}, {"number": 9947, "title": "Branch 155393864", "body": "", "comments": ["@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "This one seems to be still running https://ci.tensorflow.org/job/tensorflow-pull-requests-multijob/4922/", "Yes, I kicked off another run.", "But yes, the MacOS buld seems to actually be making progress this time.", "@tensorflow-jenkins test this please"]}, {"number": 9946, "title": "StatSummarizer logs error messages for graph with a while loop", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, see Python script below\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: Built from source\r\n- **TensorFlow version (use command below)**: From command below: \"b'unknown' 1.1.0-rc2\", commit: 69433c1f1adef96fde2074b05d3362e88d8587de\r\n- **Bazel version (if compiling from source)**: Used CMake 3.6.3\r\n- **CUDA/cuDNN version**: Built without GPU support\r\n- **GPU model and memory**: Built without GPU support\r\n- **Exact command to reproduce**:\r\n```\r\n  <create graph.pb with Python script below>\r\n  <path to build output>/benchmark_model.exe --graph=\"graph.pb\" --input_layer=\"\" --input_layer_shape=\"\" --input_layer_type=\"\"\r\n```\r\n\r\n\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\n**Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.**\r\n\r\nWhen running a graph containing a while loop with benchmark_model.exe, StatSummarizer reports numerous errors like this:\r\n\r\nE c:\\source\\tensorflow\\tensorflow\\core\\util\\stat_summarizer.cc:146] Bad output slot '1' for 'loop_op/Switch'\r\n\r\nIt looks like StatSummarizer determines that something about the step stats produced for the Switch node in this case is invalid. This is reproducible with the simple graph in the Python script below, which does not seem like it should be an invalid graph.\r\n\r\n### Source code / logs\r\n**Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.**\r\n\r\nPython script:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ninitial_value = tf.constant(1)\r\nloop_cond = lambda i: tf.less(i, 10)\r\nloop_body = lambda i: tf.add(i, 1)\r\nloop_op = tf.while_loop(loop_cond, loop_body, [initial_value], name=\"loop_op\")\r\noutput = tf.identity(loop_op, name=\"output\")\r\n\r\nwith tf.Session() as sess:\r\n  tf.train.write_graph(sess.graph, \".\", \"graph.pb\", as_text=False)\r\n```", "comments": ["[benchmark_model.out.txt](https://github.com/tensorflow/tensorflow/files/1006073/benchmark_model.out.txt)\r\n", "@andrewharp can you comment or redirect to someone who can? Thanks!", "Is there any update on this?", "Is this issue resolved? Please update here If it was not resolved already. Thanks!"]}, {"number": 9945, "title": "Branch 155393864", "body": "", "comments": []}, {"number": 9944, "title": "Distributed tensorflow - possible memory leak on Linux", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.1.rc2 (v1.1.0-rc2-675-gd0042ed)\r\n- **Bazel version (if compiling from source)**: 0.4.5\r\n- **CUDA/cuDNN version**: 8/5.0\r\n- **GPU model and memory**: Geforce670 2GB (GPU not used in example)\r\n- **Exact command to reproduce**:\r\n\r\n`import tensorflow as tf`\r\n`server = tf.train.Server.create_local_server()`\r\n`session = tf.Session(server.target)`\r\n`c = tf.constant(\"Hello, distributed TensorFlow!\")`\r\n`init = tf.global_variables_initializer()`\r\n`session.run(init)`\r\n`session.graph.finalize()`\r\n`with session as sess:`\r\n`  ` `while True:`\r\n`    `  `    `  `out = sess.run(c)`\r\n\r\n### Describe the problem\r\nRunning the above example, memory usage keeps slowly increasing. If the session is initialized as non-distributed with:\r\n\r\nsession = tf.Session()\r\n\r\nThe amount of memory used remains stable. This is tested on several machines both with binary and source code installations. Could someone else verify, if this is a problem with tensorflow.\r\n", "comments": ["Heap profile could be helpful to see where the memory is going", "Thanks - I did a heap profile and it seems to be related to session_mgr->AssociateStepIdWithGraph.\r\nThis function no longer exists in the newest git version and the above example no longer leaks memory. "]}, {"number": 9943, "title": "Tensorflow Layers conv3d_transpose", "body": "I'm trying to use 3D transpose convolutions found [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/convolutional.py#L1376), but they are not available in the current build of Tensorflow when installing using pip. On another machine I was successfully able to build Tensorflow from source because I already had Bazel installed on my system, and then able to use the 3d transpose convolutions.\r\n\r\nHowever, on a new machine without Bazel, I am unable to install Bazel, and therefore unable to compile Tensorflow from source. It appears that the Java project has moved. Below is the result of running `sudo apt-get install openjdk-8-jdk`, as per the instructions [here](https://bazel.build/versions/master/docs/install-ubuntu.html#install-compile-source.html).\r\n\r\n\r\n`Setting up oracle-java9-installer (9b162-1~webupd8~0) ...\r\nUsing wget settings from /var/cache/oracle-jdk9-installer/wgetrc\r\nDownloading Oracle Java 9...\r\n--2017-05-16 14:23:32--  http://www.java.net/download/java/jdk9/archive/162/binaries/jdk-9-ea+162_linux-x64_bin.tar.gz\r\nResolving www.java.net (www.java.net)... 137.254.56.25\r\nConnecting to www.java.net (www.java.net)|137.254.56.25|:80... connected.\r\nHTTP request sent, awaiting response... 302 Found\r\nLocation: https://home.java.net/download/java/jdk9/archive/162/binaries/jdk-9-ea+162_linux-x64_bin.tar.gz [following]\r\n--2017-05-16 14:23:33--  https://home.java.net/download/java/jdk9/archive/162/binaries/jdk-9-ea+162_linux-x64_bin.tar.gz\r\nResolving home.java.net (home.java.net)... 156.151.59.19\r\nConnecting to home.java.net (home.java.net)|156.151.59.19|:443... connected.\r\nHTTP request sent, awaiting response... 302 Found\r\nLocation: http://www.oracle.com/splash/java.net/maintenance/index.html [following]\r\n--2017-05-16 14:23:33--  http://www.oracle.com/splash/java.net/maintenance/index.html\r\nResolving www.oracle.com (www.oracle.com)... 23.209.61.60, 2600:1418:3:1a2::2d3e, 2600:1418:3:18b::2d3e\r\nConnecting to www.oracle.com (www.oracle.com)|23.209.61.60|:80... connected.\r\nHTTP request sent, awaiting response... 503 Service Unavailable\r\n2017-05-16 14:23:33 ERROR 503: Service Unavailable.\r\ndownload failed.\r\nOracle JDK 9 is NOT installed.\r\ndpkg: error processing package oracle-java9-installer (--configure):\r\n subprocess installed post-installation script returned error exit status 1\r\nErrors were encountered while processing:\r\n oracle-java9-installer\r\nE: Sub-process /usr/bin/dpkg returned an error code (1)\r\n`\r\n\r\nIs there a way around this, or a timeframe when the new Tensorflow Layers will be included in the pip download?", "comments": ["Hi @cameronfabbri, I'd recommend asking this question on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 9942, "title": "Merged performance docs.", "body": "", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 9941, "title": "Replaced deprecated op_scope call with name_scope", "body": "This was flooding our experiment logs, so I fixed it. Should be pretty straightforward?", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 9940, "title": "Compiling Error with empty error message", "body": "OS: centos 7.2 x64\r\nTensorFlow installed from: source\r\nTensorFlow version: r1.1 \r\nBazel: 0.45\r\nCPU-only\r\n```\r\n bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-msse4.2 --copt=-msse4.1 --copt=-msse3 --copt=-mfma -k //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n![](https://cloud.githubusercontent.com/assets/18662395/26121841/2930b0ac-3aa8-11e7-8cb5-65cb3b0b847e.png)\r\n\r\nand I check log file, It's an empty file\r\nthen I tried again \r\nexit with this: \r\n```\r\n[1,801 / 2,762] Compiling tensorflow/core/graph/validate.cc\r\n\r\nServer terminated abruptly (error code: 14, error message: '', log file: '/root/.cache/bazel/_bazel_root/bc2bd5c039dfc4a14bebd99e0322728d/server/jvm.out')\r\n```", "comments": ["This is probably an environment/setup issue and not a bug, so is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow). There is also a larger community that reads questions there. Thanks!", "thx!"]}, {"number": 9939, "title": "contrib.layers.avg_pool2d raises warnings when serializing metagraph", "body": "\r\n### System information\r\n\r\n- **OS Platform and Distribution**: Ubuntu 14.04 LTS\r\n- **TensorFlow installed from**: source\r\n- **TensorFlow version**:  ('v1.1.0-rc2-1015-gf2047a3', '1.1.0-rc2')\r\n- **Bazel version**: 0.4.5\r\n- **CUDA/cuDNN version**: 8.0/5.1.5\r\n- **GPU model and memory**: Maxwell Titan X (12 GiB)\r\n\r\n\r\n### Overview of problem\r\nSince updating to the most recent version (as of yesterday) of TensorFlow, I've started seeing the following ominous warning when serializing a wide resnet variant that I've been using for acoustic modeling:\r\n\r\n    WARNING:tensorflow:Error encountered when serializing LAYER_NAME_UIDS.\r\n    Type is unsupported, or the types of the items don't match field type in CollectionDef.\r\n    'dict' object has no attribute 'name'\r\n\r\nHowever, a metagraphdef does export and I am able to successfully use it to recreate the trained model. After playing around with simpler architectures, it looks like the problem comes from the average pooling I do at the end, which involves a call to ``tf.contrib.layers.avg_pool2d``. For a trivial example that elicits this warning, please see the script at \r\n\r\n    https://gist.github.com/nryant/1f69cda71fd6a468fa5641855199f843\r\n", "comments": ["Hi @sherrym, are you able to take a look? Thanks!", "I also frequently encounter this issue since I started to use the `tf.layers` module in TF 1.1.\r\nIt seems that something related to naming went wrong in some of the modules.\r\n\r\nFor example, the following code triggered the issue:\r\n```python\r\nimport tensorflow as tf\r\n\r\nx = tf.zeros([10, 3])\r\ny = tf.layers.dense(x, 5)  # the issue is originated from this line\r\n\r\nsv = tf.train.Supervisor()\r\nsess = sv.prepare_or_wait_for_session()\r\nsv.saver.save(sess, '.')\r\n```\r\nReturn:\r\n```\r\nWARNING:tensorflow:Error encountered when serializing LAYER_NAME_UIDS.\r\nType is unsupported, or the types of the items don't match field type in CollectionDef.\r\n'dict' object has no attribute 'name'\r\n```\r\n\r\nHowever, the issue can be easily avoided (in this simple case) by specifying the `name` argument:\r\nIn contrast, using `slim`, `keras`, or `tf.Variable` doesn't trigger such an issue.\r\nHere are some examples that work smoothly without triggering the warning above.\r\n```python\r\n# The Tensorflow infrastructure\r\nw = tf.Variable(tf.zeros([3, 5]))\r\ny = tf.matmul(x, w)\r\n```\r\n\r\n```python\r\n# The `layers` module with the `name` arg specified\r\ny = tf.layers.dense(x, 5, name='y')\r\n```\r\n\r\n```python\r\n# Slim\r\nfrom tensorflow.contrib import slim\r\ny = slim.fully_connected(x, 5)\r\n```\r\n\r\n```python\r\n# Keras\r\ny = tf.contrib.keras.layers.Dense(units=5)(x)\r\n```\r\n", "@JeremyCCHsu, I use slim but am yet encountered by this issue AFTER I upgrade from tf 1.0.1 to 1.1. \r\nenvironment info:\r\npython 2.7\r\ngpu: k40c with cudn 8.0\r\nrhel: 6.5\r\ntf: binary installation\r\n\r\n", "Also getting this error on tf 1.1 even though I've passed names to all tf.layers commands (at least, I think so. If it's useful, I can try to find a minimal example of this where it doesn't come from an unnamed tf.layers).\r\n\r\nSee [here](http://stackoverflow.com/questions/43986092/warning-when-saving-variables-in-tensorflow-1-1-0) for somebody getting this issue using an RNN as well, which is similar to what I'm seeing (I'm thinking it's probably the same issue as reported above with layers, though I could be wrong).", "I'm getting the exact same error, but I'm not using any `tf.layers`. I'll see if I can create a minimal example.", "I did a quick debugging session, and it seems the issue was introduced in https://github.com/tensorflow/tensorflow/commit/1e3e5d424eaa6332314f8ad1d54089eb0f9e02e7 by adding a dictionary named `LAYER_NAME_UIDS` as a graph collection. I'm not sure on the internals, but my feeling is that collections are not meant for dictionaries, only for ops, as the serialization code apparently expects to be able to access a `name` property on all collection entries.\r\n\r\nThe dictionary serves as a global (or technically graph-local) store for tracking counters per base name to make unique names. As a proof of concept I defined `layer_name_uids` globally as a struct and just used that without using collections, and this fixed the graph serialization issue.\r\n\r\nI hope someone with more in-depth knowledge on this code could take a look. Maybe @fchollet who authored the commit in question?", "As a temporary workaround, running \r\n```python\r\ndel tf.get_collection_ref('LAYER_NAME_UIDS')[0]\r\n``` \r\nbefore exporting the graph seems to work.", "FWIW, I get the same warning when running [this](https://github.com/GoogleCloudPlatform/training-data-analyst/tree/master/blogs/timeseries/simplernn) rnn example using TF 1.2 rc0.\r\nDon't see it in TF 1.1.\r\n```\r\nWARNING:tensorflow:Error encountered when serializing LAYER_NAME_UIDS.\r\nType is unsupported, or the types of the items don't match field type in CollectionDef.\r\n'dict' object has no attribute 'name'\r\n```", "@ebrevdo has done some work on contrib.layers recently. Mind taking a look at why this warning message is being printed?", "This is a harmless warning.  Layers perform bookkeeping inside a specially\nnamed graph collection. The bookkeeping doesn't need to be serialized\nacross meta graph save/reload, so we haven't added the appropriate\nserialization classes.\n\nOn May 25, 2017 8:20 PM, \"Justine Tunney\" <notifications@github.com> wrote:\n\n> @ebrevdo <https://github.com/ebrevdo> has done some work on\n> contrib.layers recently. Mind taking a look at why this warning message is\n> being printed?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9939#issuecomment-304180819>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim-Oj1wz9-OL91mOJvcMtxJltSfEAks5r9kT6gaJpZM4Nc5RY>\n> .\n>\n", "A harmless warning is still a warning, and users keep asking the meaning of this warning.\r\n\r\nIt seems that originally the bookkeeping is done within each package (slim, layers, etc). Then the bookkeeping  dict gets moved into a special collection, so that the packages can share the bookkeeping code.\r\n\r\nIt seems that TensorFlow collections are used for collecting Tensors only. I suggest that we do not re-use the collections, and use some other means to share the bookkeeping dict.\r\n\r\nOr, at least we can set a flag so that MetaGraph Savers don't try to serialize that.", "I find that *harmless warnings* are a little silly. I don't want to be warned about harmless things. Then it becomes very difficult for me to find the real warnings. Or I just turn warnings off because it's bothersome to have so much text output with no meaning. I'll just add the fix by @EdeMeijer to my code for now, but it is a bit hacky to have to do. As I save models while I'm still training them, though (which is why the number of warnings is enough to be troublesome), I wonder if I ought to make a copy of the dictionary, then delete it when I export the model to avoid the warning, then restore it so that it's there if something in the layers expects it later? By the time the workaround is that long (admittedly only a few lines of code), I'd be glad to have something incorporated that took care of it for me.", "Thanks @ebrevdo for helping us know that there's no cause for alarm. How do you think this issue should be triaged? I agree with our friends that, ideally speaking, warnings should be as helpful as possible. So maybe contributions welcome?", "Still present in 1.2rc1:\r\n`WARNING:tensorflow:Error encountered when serializing LAYER_NAME_UIDS.\r\nType is unsupported, or the types of the items don't match field type in CollectionDef.\r\n'dict' object has no attribute 'name'`\r\n@ali01 : please reclassify, this isn't a feature-request, but a bug: This worked fine until 1.1, then models that were working without error-messages suddenly produce the error-message above.\r\nThat causes people first to assume a breaking API-change, then to look for problems in their own model definition, when finding out that model-saves cause this, and finally to start digging in tf's code for the reason of this message, which by it self sounds like the model-save did _not_ work.\r\n\r\n", "Francois and Sherry should probably come to a decision. Either keras keeps\nan internal dictionary or we find a way to signal to saver to mark some\ncollections as local.\n\nOn May 29, 2017 12:22 AM, \"Dominik Schl\u00f6sser\" <notifications@github.com>\nwrote:\n\n> Still present in 1.2rc1:\n> WARNING:tensorflow:Error encountered when serializing LAYER_NAME_UIDS.\n> Type is unsupported, or the types of the items don't match field type in\n> CollectionDef. 'dict' object has no attribute 'name'\n> @ali01 <https://github.com/ali01> : please reclassify, this isn't a\n> feature-request, but a bug: This worked fine until 1.1, then models that\n> were working without error-messages suddenly produce the error-message\n> above.\n> That causes people first to assume a breaking API-change, then to look for\n> problems in their own model definition, when finding out that model-saves\n> cause this, and finally to start digging in tf's code for the reason of\n> this message, which by it self sounds like the model-save did *not* work.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9939#issuecomment-304591228>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim8d1pUdjtbZiUzbVEJqFutfiKKiaks5r-nJAgaJpZM4Nc5RY>\n> .\n>\n", "Fix is checked in internally and should get synced today.  I am attempting to make sure it is in the next 1.2 RC as a final cherry pick.  I will close once it hits master.  ", "This is in with this https://github.com/tensorflow/tensorflow/pull/10359   and I have it on the list to get cherry-picked into 1.2 RC2.  I will double check as long as I do not forget.  ", "@tfboyd\r\n\r\nI still get this error when I tried to use tf.add_to_collection to add a dictionary/list to the collection. Is it the same issue? I tested with latest 1.2.1 build for windows.\r\n\r\nWARNING:tensorflow:Error encountered when serializing exporter_interface.\r\nType is unsupported, or the types of the items don't match field type in CollectionDef.\r\n'xxxx' object has no attribute 'name'"]}, {"number": 9938, "title": "Branch 155393864", "body": "", "comments": []}, {"number": 9937, "title": "reimplement core/util/cuda_kernel_helper.h?", "body": "Hi,\r\n\r\nI'm trying to implement a `GetCuda3DLaunchConfig` into `cuda_kernel_helper.h`, but while reading the code, I feel it's a bit confusing and there might be a better way to implement it.\r\n\r\nHere is the pointer:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/util/cuda_kernel_helper.h\r\n\r\nIn line 55-57, there is something like `block_count = physical_thread_count / thread_per_block`. Why use `thread_per_block` instead of `virtual_thread_count`?  The number of blocks can be higher than physical maximum and cuda will automatically put these blocks into queue. On the other hand, limiting the number of blocks to physical limit would make the computation incomplete when the number of threads is large.\r\nSee: http://docs.nvidia.com/cuda/cuda-c-programming-guide/graphics/automatic-scalability.png\r\n\r\nThe the kernel launch config computed is not optimal. I would suggest using the API provided by cuda >=6.5\r\nSee: https://devblogs.nvidia.com/parallelforall/cuda-pro-tip-occupancy-api-simplifies-launch-configuration/\r\n\r\nCan anyone confirm what I said? If my suggestion make sense, I would like to reimplement these functions using cuda's occupancy api while writing my `GetCuda3DLaunchConfig`.", "comments": ["@zheng-xq any opinion there? ", "The original formula is tuned on K80 with very small kernels. The convention is that each kernel would iterate through its assigned range for large number of work elements. \r\n\r\nSince these are all heuristics, for your kernels, feel free to use a different set of launch parameters. There are a number of custom kernels that do this. A common practice is to write custom logic for your kernels, and see if they are successful enough, refactor them to shared utilities. Feel free to skip this step if you want to go for a common utility function directly, which is often subject to a higher review standard.\r\n", "@zheng-xq Thanks for the clarification. I know what to do now. Closing this issue."]}, {"number": 9936, "title": "R1.1", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "Looks like an accidental PR. Closing."]}, {"number": 9935, "title": "Unable to freeze Keras layers in a Tensorflow workflow.", "body": "I'm trying to freeze Keras layers in a Tensorflow workflow. It seems that the flag trainable does not work in tf.contrib.keras. This is how I define the graph : \r\n\r\n`sess = tf.Session()`\r\n\r\n`K.set_session(sess)`\r\n    \r\n`labels = tf.placeholder(tf.float32, shape=(None, 1))`\r\n`user_id_input = tf.placeholder(tf.float32, shape=(None, 1))`\r\n`item_id_input = tf.placeholder(tf.float32, shape=(None, 1))`\r\n\r\n    \r\n\r\n`max_user_id = all_ratings['user_id'].max()`\r\n`max_item_id = all_ratings['item_id'].max()`\r\n\r\n`embedding_size = 30`\r\n`user_embedding = Embedding(output_dim=embedding_size, input_dim=max_user_id+1,\r\n                           input_length=1, name='user_embedding', trainable=all_trainable)(user_id_input)`\r\n`item_embedding = Embedding(output_dim=embedding_size, input_dim=max_item_id+1,\r\n                           input_length=1, name='item_embedding', trainable=all_trainable)(item_id_input)`\r\n\r\n\r\n\r\n`user_vecs = Flatten()(user_embedding)`\r\n`item_vecs = Flatten()(item_embedding)`\r\n\r\n\r\n`input_vecs = concatenate([user_vecs, item_vecs])`\r\n\r\n`x = Dense(30, activation='relu')(input_vecs)`\r\n`x1 = Dropout(0.5)(x)`\r\n`x2 = Dense(30, activation='relu')(x1)`\r\n`y = Dense(1, activation='sigmoid')(x2)`\r\n\r\n`loss = tf.reduce_mean(binary_crossentropy(labels, y))`\r\n\r\n`train_step = tf.train.AdamOptimizer(0.004).minimize(loss)`\r\n\r\nThen I just train the model : \r\n\r\n`with sess.as_default():`\r\n\r\n`train_step.run(..)`\r\n\r\nEverything is working fine when the trainable flag is set to `True`. Then when I set it to `False`, it does not freeze the layers.\r\n\r\nI also tried to minimize only over the variable that I want to train by using `train_step_freeze = tf.train.AdamOptimizer(0.004).minimize(loss, var_list=[user_embedding]) `, and I get : \r\n\r\n`('Trying to optimize unsupported type ', <tf.Tensor 'Placeholder_33:0' shape=(?, 1) dtype=float32>)`\r\n\r\nIs it possible to use Keras layers in Tensorflow and freeze them ? \r\n\r\n\r\n\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 9934, "title": "No OpKernel was registered to support Op 'GatherNd' with these attrs", "body": "env: macOS\r\ntf version: master brunch\r\n\r\nI built tensorflow on `macOS` using `build_all_ios.sh`. Then I tried to load `frozen.pb` data created by python program. Here is a code to load graph in `iOS`:\r\n\r\n```\r\n    Status status;\r\n    Session *session;\r\n    \r\n    status = NewSession(SessionOptions(), &session);\r\n    if (!status.ok()) {\r\n        return NO;\r\n    }\r\n    \r\n    GraphDef graph;\r\n    NSString *modelPath = [[NSBundle bundleForClass:[self class]] pathForResource:@\"frozen\" ofType:@\"pb\"];\r\n    status = ReadBinaryProto(Env::Default(), modelPath.fileSystemRepresentation, &graph);\r\n    if (!status.ok()) {\r\n        return NO;\r\n    }\r\n    \r\n    status = session->Create(graph);\r\n    if (!status.ok()) {\r\n        std::cout << status.ToString() << \"\\n\";\r\n        return NO;\r\n    }\r\n```\r\n\r\n First thing is with default build settings `libtensorflow-core.a` is extremely large - more then 400 MB that is not applicable for using in mobile devices. In any case while loading `graph` I got and error:\r\n\r\n```\r\nInvalid argument: No OpKernel was registered to support Op 'Less' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n     device='CPU'; T in [DT_FLOAT]\r\n     \r\n     [[Node: GRU-RNN/rnn/while/Less = Less[T=DT_INT32](GRU-RNN/rnn/while/Merge, GRU-RNN/rnn/while/Less/Enter)]]\r\n```\r\n\r\nI could not find how to solve it except manually change next files:\r\n1) added next code to `cwise_op_less.cc`\r\n\r\n```\r\n#if defined(__ANDROID_TYPES_SLIM__)\r\n// We only register the first type when we have multi-argument calls in the\r\n// case where we're trying to reduce executable size, but it turns out that the\r\n// int32 version of this op is needed, so explicitly include it.\r\nREGISTER(BinaryOp, CPU, \"Less\", functor::less, int32);\r\n#endif  // __ANDROID_TYPES_SLIM__\r\n```\r\n2) added next code to `cwise_op_add_2.cc`\r\n\r\n```\r\n#if defined(__ANDROID_TYPES_SLIM__)\r\n    // We only register the first type when we have multi-argument calls in the\r\n    // case where we're trying to reduce executable size, but it turns out that the\r\n    // int32 version of this op is needed, so explicitly include it.\r\n    REGISTER(BinaryOp, CPU, \"Add\", functor::add, int32);\r\n#endif  // __ANDROID_TYPES_SLIM__\r\n```\r\n\r\nThe second change was added because of the same error but for `Add` operation.\r\n\r\nThen, I have got another error:\r\n```\r\nInvalid argument: No OpKernel was registered to support Op 'GatherNd' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[Node: GatherNd = GatherNd[Tindices=DT_INT32, Tparams=DT_FLOAT](GRU-RNN/rnn/transpose, stack)]]\r\n```\r\n I think this is because python script contains `tf.gather_nd` function. I found that `tf_op_files.txt` does not contain any `gather*` functions, so I tried to add them manually but I don't know that exactly should I add and where. Can you please provide some information about that and probably add some issues about `Less` and `Add` operations for `Int32` type?", "comments": ["@petewarden @andrewharp @girving ", "Duplicate of https://github.com/tensorflow/tensorflow/issues/3764 and https://github.com/tensorflow/tensorflow/issues/9476.  iOS doesn't include all the ops.", "@girving \r\n Same here : \r\n```\r\nError creating graph: Invalid argument: No OpKernel was registered to support Op 'RandomShuffleQueue' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[Node: shuffle_batch_1/random_shuffle_queue = RandomShuffleQueue[_output_shapes=[[2]], capacity=2000, component_types=[DT_FLOAT, DT_FLOAT, DT_FLOAT], container=\"\", min_after_dequeue=1500, seed=0, seed2=0, shapes=[[1152], [62], [62]], shared_name=\"\"]()]]\r\n```\r\n\r\nWhat do you suggest?"]}, {"number": 9933, "title": "Multiplicative Integration Recurrent Neural Networks", "body": "This is the same PR as #9286(closed) after I\r\n   - resolved conflicts\r\n   - modified __init__.py\r\n   - solved cla:no problem(I hope)\r\n\r\nI implemented Multiplicative Integration variants of recurrent neural networks\r\n(RNN, GRU and LSTM) proposed in\r\n\r\nYuhuai Wu, Saizheng Zhang, Ying Zhang, Yoshua Bengio, Ruslan Salakhutdinov,\r\nOn Multiplicative Integration with Recurrent Neural Networks. NIPS, 2016.\r\nhttps://arxiv.org/abs/1606.06630\r\n\r\nThe RNNs proposed in the paper are implemented as:\r\n\r\n    MultiplicativeIntegrationRNNCell\r\n    MultiplicativeIntegrationGRUCell\r\n    MultiplicativeIntegrationLSTMCell\r\n    _multiplicative_integration as a helper function\r\nin\r\n    tensorflow/contrib/rnn/python/ops/rnn_cell.py\r\n    tensorflow/contrib/rnn/__init__.py\r\n\r\nTest codes:\r\n    tensorflow/contrib/rnn/python/kernel_tests/rnn_cell_test.py", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "@ebrevdo what do you think?", "Needs some cleanup but seems ok to accept eventually.\n\nPlease remove the input_size arguments and change __call__ to call, and\nrename \"weights\" to kernel and \"biases\" to bias.   See other rnncells in\nthe same file for more examples on style.\n\nOn Jun 26, 2017 11:47 AM, \"drpngx\" <notifications@github.com> wrote:\n\n> @ebrevdo <https://github.com/ebrevdo> what do you think?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/9933#issuecomment-311147824>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim77S8qfVoE8hBIXvV4ayi_0yCix0ks5sH_zMgaJpZM4Ncdrr>\n> .\n>\n", "ping for @cydonia999 (it looks like not all review comments have been addressed? please use the review tool to respond as needed).", "@vrv Thank you for alerting me to the review comments. I have replied all the review comments.", "@ebrevdo can you take another look when you are back in the office, please?", "@tensorflow-jenkins test this please", "Jenkins, test this please.", "I thinks this issue is waiting for @ebrevdo 's review. ", "@ebrevdo ping", "@cydonia999 could you reply to @ebrevdo comments? Also, please rebase and push again.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ebrevdo: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@cydonia999 : Are you planning to work on this PR?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 44 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 9932, "title": "Incorrect Timing Stats Reported by tfprof", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: I have added profiling code as shown below to the cifar10 code ([https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10]).\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: ('v1.1.0-rc2-773-g7fa0cf3', '1.1.0-rc2')\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:8.0/6.0\r\n- **GPU model and memory**:NVIDIA Quadra K1200 4GB\r\n\r\n### Describe the problem\r\n\r\nWhile doing profiling by tfprof I get the following stats\r\n\r\nTiming and Memory\r\n\r\n  conv1/weights (19.20KB/76.80KB, 8us/18us)\r\n    conv1/weights/ExponentialMovingAverage (19.20KB/38.40KB, 6us/8us)\r\n      conv1/weights/ExponentialMovingAverage/read (19.20KB/19.20KB, 2us/2us)\r\n    conv1/weights/read (19.20KB/19.20KB, 2us/2us)\r\n  conv2/BiasAdd (4.72MB/4.72MB, 225us/225us)\r\n  conv2/Conv2D (4.72MB/4.72MB, 2.34ms/2.34ms)\r\n  conv2/L2Loss (4B/4B, 21us/21us)\r\n  conv2/biases (256B/1.02KB, 8us/58us)\r\n    conv2/biases/ExponentialMovingAverage (256B/512B, 7us/49us)\r\n      conv2/biases/ExponentialMovingAverage/read (256B/256B, 42us/42us)\r\n\r\nFloating Point Operations\r\n\r\n_TFProfRoot (0/5.23b flops)\r\n  conv2/Conv2D (3.77b/3.77b flops)\r\n  conv1/Conv2D (707.79m/707.79m flops)\r\n  gradients/local3/MatMul_grad/MatMul (226.49m/226.49m flops)\r\n  gradients/local3/MatMul_grad/MatMul_1 (226.49m/226.49m flops)\r\n  local3/MatMul (226.49m/226.49m flops)\r\n  gradients/local4/MatMul_grad/MatMul (18.87m/18.87m flops)\r\n  gradients/local4/MatMul_grad/MatMul_1 (18.87m/18.87m flops)\r\n  local4/MatMul (18.87m/18.87m flops)\r\n  conv1/BiasAdd (4.72m/4.72m flops)\r\n  conv2/BiasAdd (1.18m/1.18m flops)\r\n  gradients/softmax_linear/MatMul_grad/MatMul (491.52k/491.52k flops)\r\n  gradients/softmax_linear/MatMul_grad/MatMul_1 (491.52k/491.52k flops)\r\n  softmax_linear/MatMul (491.52k/491.52k flops)\r\n\r\nComputing Floating Point Performance for Conv2D operation gives surprising results: It comes out to be 3.77b/2.34ms = 1618 GFLOPS which is more than the manufacturer prescribed peak performance of 1052 GFLOPS. The timing stats seem to be wrong. This is impossible.\r\n\r\n### Source code / logs\r\n```\r\n run_metadata = tf.RunMetadata()\r\n    with tf.train.MonitoredTrainingSession(\r\n        checkpoint_dir=FLAGS.train_dir,\r\n        hooks=[tf.train.StopAtStepHook(last_step=FLAGS.max_steps),\r\n               tf.train.NanTensorHook(loss),\r\n               _LoggerHook()],\r\n        config=tf.ConfigProto(\r\n            log_device_placement=FLAGS.log_device_placement, \r\n            graph_options=tf.GraphOptions(build_cost_model=1))) as mon_sess:\r\n      while not mon_sess.should_stop():\r\n        #Disable Profiling \r\n        # mon_sess.run(train_op)\r\n\r\n        #Enable Profiling \r\n        mon_sess.run(train_op, options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE), \r\n        run_metadata=run_metadata)\r\n        analysis = tf.contrib.tfprof.model_analyzer.print_model_analysis(\r\n        tf.get_default_graph(),\r\n        run_meta=run_metadata,\r\n        tfprof_options=tf.contrib.tfprof.model_analyzer.PRINT_ALL_TIMING_MEMORY)``\r\n```", "comments": ["@panyx0718 can you comment?", "float_ops is calculated here (contributed long time ago)\r\nhttps://github.com/tensorflow/tensorflow/blob/32b3f6501eb2d644398936bc4884719ba59bd168/tensorflow/python/ops/nn_ops.py#L1772\r\n\r\ntiming used to be from CostGraphDef, while the latest change switch to RunMetadata. However, I don't expect big changes here.\r\n\r\nI can see several reasons that your calculation can differ from manufacturer's specs:\r\n1. float ops is calculated, instead of the float ops of physical devices. Maybe there are some differences there. Also, I suspect the float ops calculation is no longer up-to-date. Many improvements happened in Conv2D implementation, while its float ops calculation implementation is no updated.\r\n2. The timing is a one-step result. I'm working on something to aggregate stats from multiple steps.\r\n3. The latest change (a few days ago) tried to fix a problem: A Op can fire multiple GPU kernel run, tfprof used to count only one of them. Now it's fixed to count all of them. \r\n\r\nFor now, I would treat the result as approximate. I (only me) am working on some improvements. Do let me know if the result is orders of magnitude off the reality.", "More fine-grained time profiling has been added to the profiler now."]}, {"number": 9931, "title": "Go: SIGSEGV when using int32 instead of int64 and missing error in Resize functions", "body": "## Problem\r\n\r\nIn Go, some operation causes a SIGSEGV when using an `int32` instead of an `int64` (and I have reasons to believe that the same will happen when using `float` instead of `double` and vice-versa).\r\n\r\nThe `Resize*` operations don't define the output shape correctly when the input is not a \"batch\": they just let the dimensions undefined instead of raising some errors.\r\n\r\nThe tests below are commented so I hope that's enough to let you understand what the problems are.\r\n\r\n### Source code / logs\r\n\r\n```go\r\npackage poc_test\r\n\r\nimport (\r\n        \"fmt\"\r\n        //tf \"github.com/tensorflow/tensorflow/tensorflow/go\"\r\n        \"github.com/tensorflow/tensorflow/tensorflow/go/op\"\r\n        \"testing\"\r\n)\r\n\r\nfunc TestResizeWithoutBatchIsNoSense(t *testing.T) {\r\n        // Create root scope\r\n        root := op.NewScope()\r\n\r\n        // Define graph\r\n\r\n        // 1: read image content\r\n        imagePath := \"test.jpg\"\r\n        contents := op.ReadFile(root.SubScope(\"ReadFile\"), op.Const(root.SubScope(\"filename\"), imagePath))\r\n\r\n        // 2: decode Jpeg\r\n        value := op.DecodeJpeg(root.SubScope(\"DecodeJpeg\"), contents, op.DecodeJpegChannels(3))\r\n\r\n        // I'd like to add noise to the image, so I'd like to define a nose tensor with the same shape of the image\r\n        // Just to be sure that the image shape is fully defined, I resize it\r\n        resize1 := op.ResizeNearestNeighbor(root.SubScope(\"ResizeArea\"), value, op.Const(root.SubScope(\"size\"), []int32{int32(80), int32(80)}))\r\n\r\n        // If the size parameter is an int32, no error is raised but the operation is no sense\r\n        // Because it returns ? instead of [80, 80, 3]\r\n        // The reason is taht Resize* methods requires a batch of images: should raise an error?\r\n        fmt.Println(\"Shape with int32: \", resize1.Shape().String())\r\n        if dims64, err := resize1.Shape().ToSlice(); err != nil {                                                                                                                                                                                                              \r\n                fmt.Println(dims64)                                                                                                                                                                                                                                            \r\n        } else {                                                                                                                                                                                                                                                               \r\n                t.Error(\"Error: \", err.Error())                                                                                                                                                                                                                                \r\n        }                                                                                                                                                                                                                                                                      \r\n                                                                                                                                                                                                                                                                               \r\n        // I expect a fully defined shape                                                                                                                                                                                                                                      \r\n        if !resize1.Shape().IsFullySpecified() {                                                                                                                                                                                                                               \r\n                t.Error(\"Not defined shape\")                                                                                                                                                                                                                                   \r\n        }                                                                                                                                                                                                                                                                      \r\n                                                                                                                                                                                                                                                                               \r\n        // create the batch and see how things changes                                                                                                                                                                                                                         \r\n        batch := op.ExpandDims(root.SubScope(\"expand\"), value, op.Const(root.SubScope(\"axis\"), []int32{0}))                                                                                                                                                                    \r\n        resize1 = op.ResizeNearestNeighbor(root.SubScope(\"ResizeArea\"), batch, op.Const(root.SubScope(\"size\"), []int32{int32(80), int32(80)}))                                                                                                                                 \r\n        fmt.Println(\"Shape with int32 and input as a batch: \", resize1.Shape().String())                                                                                                                                                                                       \r\n        if dims64, err := resize1.Shape().ToSlice(); err == nil {                                                                                                                                                                                                              \r\n                fmt.Println(dims64)                                                                                                                                                                                                                                            \r\n        } else {                                                                                                                                                                                                                                                               \r\n                fmt.Println(\"Error: \", err.Error())                                                                                                                                                                                                                            \r\n        }                                                                                                                                                                                                                                                                      \r\n        // Now the things have sense and the shape is defined and equals to [ 1, 80, 80, 3]                                                                                                                                                                                    \r\n}                                                                                                                                                                                                                                                                              \r\n                                                                                                                                                                                                                                                                               \r\nfunc TestResizeWithIn64ShapeSigSegvs(t *testing.T) {                                                                                                                                                                                                                           \r\n        defer func() {                                                                                                                                                                                                                                                         \r\n                if r := recover(); r != nil {                                                                                                                                                                                                                                  \r\n                        t.Error(\"Panic!\")                                                                                                                                                                                                                                      \r\n                }                                                                                                                                                                                                                                                              \r\n        }()\r\n        // Create root scope\r\n        root := op.NewScope()\r\n\r\n        // Define graph\r\n\r\n        // 1: read image content\r\n        imagePath := \"test.jpg\"\r\n        contents := op.ReadFile(root.SubScope(\"ReadFile\"), op.Const(root.SubScope(\"filename\"), imagePath))\r\n\r\n        // 2: decode Jpeg\r\n        value := op.DecodeJpeg(root.SubScope(\"DecodeJpeg\"), contents, op.DecodeJpegChannels(3))\r\n\r\n        // However, changing int32 with int64 breaks everyting (no matter if I use `batch` or `value`)\r\n        resize2 := op.ResizeArea(root.SubScope(\"ResizeArea2\"), value, op.Const(root.SubScope(\"size2\"), []int64{int64(80), int64(80)}))\r\n        // This operation causes a SIGSEGV\r\n        fmt.Println(\"Shape value: \", resize2.Shape())\r\n        fmt.Println(\"Shape with int64: \", resize2.Shape().String())\r\n\r\n        // In short, chaning int32 with int64 causes SIGSEGV. It looks like kernels are not registered to handle both types\r\n\r\n        // This can bring the code to be a mess to debug, because If I'd like to, for example, add noise to an image\r\n        // I have to generate a set of values with the same shape of the input images.\r\n        // Using the one with the defined shape (the batch) I'd like to use the output of Shape().ToSlice()\r\n        // But I can't.\r\n}\r\n\r\nfunc TestGenerateNoiseWithInt32Shape(t *testing.T) {\r\n        defer func() {\r\n                if r := recover(); r != nil {\r\n                        t.Error(\"Panic!\")\r\n                }\r\n        }()\r\n        // Create root scope\r\n        root := op.NewScope()\r\n\r\n        // Define graph\r\n\r\n        // 1: read image content\r\n        imagePath := \"test.jpg\"\r\n        contents := op.ReadFile(root.SubScope(\"ReadFile\"), op.Const(root.SubScope(\"filename\"), imagePath))\r\n        // 2: decode Jpeg\r\n        value := op.DecodeJpeg(root.SubScope(\"DecodeJpeg\"), contents, op.DecodeJpegChannels(3))\r\n        batch := op.ExpandDims(root.SubScope(\"expand\"), value, op.Const(root.SubScope(\"axis\"), []int32{0}))\r\n        resize1 := op.ResizeNearestNeighbor(root.SubScope(\"ResizeArea\"), batch, op.Const(root.SubScope(\"size\"), []int32{int32(80), int32(80)}))\r\n        fmt.Println(\"Shape with int32 and input as a batch: \", resize1.Shape().String())\r\n        if dims64, err := resize1.Shape().ToSlice(); err != nil {\r\n                fmt.Println(dims64)\r\n        } else {\r\n                fmt.Println(\"Error: \", err.Error())\r\n        }\r\n\r\n        dims64, _ := resize1.Shape().ToSlice()\r\n        noise := op.ParameterizedTruncatedNormal(root.SubScope(\"ParameterizedTruncatedNormal\"),\r\n                op.Const(root.SubScope(\"shape\"), dims64),\r\n                op.Const(root.SubScope(\"means\"), 0.),\r\n                op.Const(root.SubScope(\"stddev\"), 1.),\r\n                op.Const(root.SubScope(\"minvals\"), 0.),\r\n                op.Const(root.SubScope(\"maxvals\"), 1.))\r\n        fmt.Println(noise)\r\n\r\n        // ^ This operation causes SIGSEGV\r\n        // I have to convert dims64 to a slice of int32 and then the operation works\r\n\r\n}\r\n\r\nfunc TestGenerateNoiseWithInt64Shape(t *testing.T) {\r\n        // Create root scope\r\n        root := op.NewScope()\r\n\r\n        // Define graph\r\n\r\n        // 1: read image content\r\n        imagePath := \"test.jpg\"\r\n        contents := op.ReadFile(root.SubScope(\"ReadFile\"), op.Const(root.SubScope(\"filename\"), imagePath))\r\n        // 2: decode Jpeg\r\n        value := op.DecodeJpeg(root.SubScope(\"DecodeJpeg\"), contents, op.DecodeJpegChannels(3))\r\n        batch := op.ExpandDims(root.SubScope(\"expand\"), value, op.Const(root.SubScope(\"axis\"), []int32{0}))\r\n        resize1 := op.ResizeNearestNeighbor(root.SubScope(\"ResizeArea\"), batch, op.Const(root.SubScope(\"size\"), []int32{int32(80), int32(80)}))\r\n        fmt.Println(\"Shape with int32 and input as a batch: \", resize1.Shape().String())\r\n        if dims64, err := resize1.Shape().ToSlice(); err != nil {\r\n                fmt.Println(dims64)\r\n        } else {\r\n                fmt.Println(\"Error: \", err.Error())\r\n        }\r\n\r\n        dims64, _ := resize1.Shape().ToSlice()\r\n\r\n        var dims []int32 = make([]int32, len(dims64))\r\n        for i, dim := range dims64 {\r\n                dims[i] = int32(dim)\r\n        }\r\n\r\n        noise := op.ParameterizedTruncatedNormal(root.SubScope(\"ParameterizedTruncatedNormal\"),\r\n                op.Const(root.SubScope(\"shape\"), dims64),\r\n                op.Const(root.SubScope(\"means\"), 0.),\r\n                op.Const(root.SubScope(\"stddev\"), 1.),\r\n                op.Const(root.SubScope(\"minvals\"), 0.),\r\n                op.Const(root.SubScope(\"maxvals\"), 1.))\r\n        fmt.Println(noise.Shape().String())\r\n}\r\n\r\n```\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Archlinux\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.1.0-rc2\r\n- **Bazel version (if compiling from source)**: 0.4.5\r\n- **CUDA/cuDNN version**: cuda 8, cudnn 5.1\r\n- **GPU model and memory**:  GeForce GTX 1080\r\n- **Exact command to reproduce**: `go test`\r\n", "comments": ["Thanks for the report @galeone , there are multiple things going on here.\r\n\r\n1. There was a bug in the underlying C API where it was suppressing errors during graph construction. That is fixed at head in 7d785f1e18af9d22d940f18aac6e8c9ffd268b22, so it will be available with the 1.2 release of the TensorFlow C API\r\n\r\n2. The `Scope` type follows the \"builder pattern\" for graph construction. So, while [`op.ResizeNearestNeighbor(Scope*, ...)`](https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go/op#ResizeNearestNeighbor) doesn't return an error, errors are collected in [`Scope.Err`](https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go/op#Scope.Err). This allows for more compact graph definitions (see the [package example for the `op` package](https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go/op#ex-package)), at the cost of forgetting to check the error. Setting aside comments on the merits of this design, before using any of the returned `tf.Outputs` from the functions in the `op` package, one is encouraged to check the error. So, in line 30 of the snippet above for example, I'd suggest:\r\n\r\n    ```go\r\n    resize1 := op.ResizeNearestNeighbor(root.SubScope(\"ResizeArea\"), value, op.Const(root.SubScope(\"size\"), []int32{int32(80), int32(80)}))\r\n    if err := root.Err(); err != nil {\r\n        t.Fatal(err)\r\n    }\r\n    \r\n    // If the size parameter is an int32, no error is raised but the operation is no sense\r\n    // Because it returns ? instead of [80, 80, 3]\r\n    // The reason is taht Resize* methods requires a batch of images: should raise an error?\r\n    fmt.Println(\"Shape with int32: \", resize1.Shape().String())\r\n    ```\r\n\r\n3. Regardless, the Go API should never end up with segfaults from the underlying C API (in this case, providing an invalid `TF_Operation` pointer). So I'm going to send a fix for that.\r\n\r\nLong story short: A couple of fixes will ensure error messages/panics that are more useful than the cryptic segfaults. Additionally, it's good practice to check the error on the `Scope` object.\r\n\r\nHope that helps (will update this issue with the fix mentioned above). Comments/thoughts welcome.", "Thank you for the suggestion on how to properly check for errors!\r\nIs somewhere documented the `Scope` follows the builder pattern? I guess it would be nice to put this in `Scope`'s go-doc.\r\n\r\nAbout the point 3: what is the problem? Why it segfaults? That op has a kernel registered to handle both types or the fix is to register the missing type?", "@galeone : I'm probably misusing the phrase \"builder pattern\" :), but yeah, it does want you to check for errors. If you'd like to contribute or suggest some changes to `scope.go` or other files to help improve documentation, we'd be more than happy to look at them.\r\n\r\nRegarding point 3: It isn't that the kernel isn't registered, it's that the operation requires that the `size` argument be an int32 Tensor (see [godoc](https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go/op#ResizeNearestNeighbor)).\r\n\r\nBetween 7d785f1e18af9d22d940f18aac6e8c9ffd268b22 and a fix I'm about to make on the Go side, you shouldn't see any segfaults in the C API - but you may see a nil-pointer dereference in Go (because `Scope.AddOperation` will return nil and an error after the fix).\r\n\r\nHope that helps (the fix should be in sometime tomorrow I think)", "I'll be happy to contribute! In the next days, as soon as I have time, I'm going to give a better look.\r\n\r\nHowever, since every `size` argument of every function requires an `int32`, why the `.Shape()` method returns a `[]int64`? I mean, it seems not coherent. The documentation talks about int32 almost everywhere and this int64 just makes operations like the creation of a new tensor with a predefined shape (like the noise in my previous example) a problem because of the required casts.\r\nIsn't just better to use an int32 everywhere and thus avoid the problem? I'm pretty sure that no one will create a tensor with a dimension > 2^32 -1", "I believe the use of int32 in many operations predates efforts to accept int64 as well (see for example 91ce95d497ec2957535b2ce6a965cd8269d723e5). So, I think some of these ops need to be updated to accept int64 as well.\r\n\r\nIn general, having Tensors that reach > 2^32-1 dimensions isn't unheard of, especially when reshaping large batches of multi-dimensional tensors.", "Alright, thus instead of using `int32` everywhere I suggest using `int64` everywhere in order to maintain consistency. Or, if not everywhere, at leat use the same type for attributes that works togeather, i.e. shape and dimensions (as input and/or output parameters) should be both `int64`.", "This issue should be resolved at head (and with C library compiled from head), so I'm going to close this out.\r\n\r\nContributions for making more operations accept int64 for shape similar to https://github.com/tensorflow/tensorflow/commit/91ce95d497ec2957535b2ce6a965cd8269d723e5 are welcome.\r\n\r\n@galeone : Hope that helps. Feel free to open a new issue if you run into more trouble. Thanks!"]}, {"number": 9930, "title": "AttributeError: type object 'NewBase' has no attribute 'is_abstract'", "body": "\r\n\r\n------------------------\r\n\r\n### System information\r\n- \r\n-OS Platform and Distribution  =Ubuntu 14.04\r\n- **TensorFlow installed = \r\n i mtrying to install tensorflow using following link\r\n\r\nhttps://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.1.0-cp27-none-linux_x86_64.whl\r\n\r\n\r\n\r\n\r\n\r\n### Describe the problem\r\nwhen i am trying to import tensorflow its not working what to do you can check following logs\r\n\r\n### Source code / logs\r\nroot@ubuntu:/home/vivek# python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py\", line 104, in <module>\r\n    from tensorflow.python.platform import test\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/test.py\", line 38, in <module>\r\n    from tensorflow.python.framework import test_util as _test_util\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/test_util.py\", line 45, in <module>\r\n    from tensorflow.python.platform import googletest\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/googletest.py\", line 33, in <module>\r\n    from tensorflow.python.platform import benchmark  # pylint: disable=unused-import\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/benchmark.py\", line 119, in <module>\r\n    class Benchmark(six.with_metaclass(_BenchmarkRegistrar, object)):\r\n  File \"/usr/lib/python2.7/dist-packages/six.py\", line 617, in with_metaclass\r\n    return meta(\"NewBase\", bases, {})\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/benchmark.py\", line 114, in __new__\r\n    if not newclass.is_abstract():\r\nAttributeError: type object 'NewBase' has no attribute 'is_abstract'\r\nroot@ubuntu:/home/vivek# \r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it sounds like a setup problem. There is also a larger community that reads questions there. Thanks!"]}, {"number": 9929, "title": "Compiling tensorflow 1.1 under Centos 7", "body": "I'm trying to compile tensorflow with cuda support under linux Centos 7 distribution.\r\n\r\nI followed the instructions provides at [github: gentaiscool/tensorflow.md ](https://gist.github.com/gentaiscool/a628fab5cd98953af7f46b69463394b3) with no success.\r\n\r\nIn this page they ask for hacking the file tensorflow/third_party/gpus/crosstool/CROSSTOOL (adding the line \"cxx_builtin_include_directory : \"/usr/local/cuda/targets/x86_64-linux/include\").\r\n\r\nIn the current version of Tensorflow (1.1) there is no such a file; similar files, such as: CROSSTOOL.tpl, CROSSTOOL_nvcc.tpl, CROSSTOOL_clang.tpl, are found. I tried the proposed hacking on each of these files and didn't get successful compilation.\r\n\r\nSince I'm interested is tensorflow deployment, I'm compiling it using the instructions found in [gitgub cjweeks/tensorflow-cmake](https://github.com/cjweeks/tensorflow-cmake) (building a library to be linked with a C++ based program). My build command is:\r\n\r\n`bazel build -c opt --config=cuda tensorflow:libtensorflow_all.so`\r\n\r\nThe error I've got is:\r\n\r\n`ERROR: /root/tensorflow/tensorflow/core/kernels/BUILD:3004:1: undeclared inclusion(s) in rule '//tensorflow/core/kernels:depth_space_ops_gpu':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/core/kernels/spacetodepth_op_gpu.cu.cc':\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/limits.h'\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/syslimits.h'\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/stddef.h'\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/stdarg.h'\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/stdint.h'\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/x86intrin.h'\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/ia32intrin.h'\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/mmintrin.h'\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/xmmintrin.h'\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/mm_malloc.h'\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/emmintrin.h'\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/immintrin.h'\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/fxsrintrin.h'\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/adxintrin.h'\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/float.h'\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/stdbool.h'.`\r\n\r\nSystem information:\r\n\r\n- Linux Centos 7.2\r\n- Kernel: 3.10.0-327.36.1.el7.x86_64\r\n- Compiler: gcc version 4.8.5 20150623 (Red Hat 4.8.5-4) (GCC)\r\n- Tensorflow: 1.1, installed from source.\r\n- nvcc: Cuda compilation tools, release 8.0, V8.0.44\r\n- CUDA: 8.0\r\n- cudaa: 5.1.5\r\n- bazel: 0.4.5\r\n\r\nAny help please?\r\n\r\nThanks in advance,\r\n\r\nRon.", "comments": ["Did you try adding `/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include` as a builtin directory?", "Hi drpngx,\r\n\r\nThanks for your response...\r\n\r\nHow do I add this directory as a builtin directory?\r\n\r\nRegards,\r\nRon.", "Please add it to `CROSSTOOL.tpl` or the `nvcc` version if you are using NVCC.", "Hi drpngx,\r\n\r\nTried your proposal getting the same errors. \r\n\r\nI did the following:\r\n\r\nIn CROSSTOOL_nvcc.tpl I replaced the line:\r\n`cxx_builtin_include_directory: \"/\"`\r\nwith this line:\r\n`cxx_builtin_include_directory: \"/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include\"`\r\n\r\nAny additional proposal?\r\n\r\nThanks,\r\nRon", "Did you try in `CROSSTOOL.tpl`? Also try adding the line, not replacing the existing one.", "I remember from my experiments in a docker container that this was caused by missing header installation.\r\nreinstalling glibc gcc and g++ through yum solved the issue when I tried this.\r\n\r\nHowever, Tensorflow does not have official support for CentOS.\r\nAs the instructions you found were in a non-TF repository, I recommend reaching out to the author of those instructions.", "Tried you suggestions without success :-( - got same error...", "Using latest TF version, and bazel version 0.8.1 or later, this problem seems to be gone. For more information see #14380"]}, {"number": 9928, "title": "I'm curious about whether these two graphs are same network", "body": "![cifar](https://cloud.githubusercontent.com/assets/10510469/26090870/2fd017a4-3a3a-11e7-9303-13b912a8b0c1.png)\r\nAs we know, the upper graph is the provided example instance of CIFAR10. But when I copied these inference() code to my code, it likes as bellow. So my question is does these two graphs are equal? If yes, why they are looks like so different?\r\n![cifar-my](https://cloud.githubusercontent.com/assets/10510469/26090871/2fe2502c-3a3a-11e7-8608-c5aa91324871.png)\r\n\r\n", "comments": ["While this is an interesting question you're better off **asking this on Stack Overflow** tagged as Tensorflow as this will get closed by the devs as it's not a bug. \r\n\r\nFrom a quick glance it looks like they do share a lot of similar components but the inference code shows the graph built together instead of what looks like the model components in the top picture. Bit i'm not sure. Hopefully those on Stack Overflow can help you more!", "@jubjamie is correct, please repost on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 9927, "title": "Some small problems with the RNN and seq2seq implementations", "body": "1. In the class `AttentionCellWrapper` , https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/rnn_cell.py#L1116  , maybe the `lstm_output` should be replaced by some other token, since the wrapper is not specified for LSTM.\r\n\r\n2.  In the class `Decoder`, https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/decoder.py#L90 , I think the `step()` method should not return an output as an instance of `BasicDecoderOutput`. From an object-oriented-programming view, the `BasicDecoder` is an inheritance of `Decoder`, the basic class should not have access to something designed for the inherited class, the same problem exists in the `dynamic_decode` method.", "comments": ["@ebrevdo, any thoughts?", "Issue 2 is a documentation issue.  We only have one type of decoder right now so we just need to change it to something like \"an object containing the decoder output\".  PRs welcome.\r\n\r\nIssue 1 - we're not really focusing on that attention wrapper (i would like to mark it deprecated).  We have a more comprehensive attention wrapper in tf.contrib.seq2seq.AttentionWrapper.  In the meantime, PRs changing the variable name inside that function are welcome.", "Added pull request (https://github.com/tensorflow/tensorflow/pull/11049) addressing these two points.", "@ebrevdo This issue was fixed by #11049 and can be closed now."]}, {"number": 9926, "title": "Issues with RoCE support", "body": "When rendezvous_mgr->RecvLocalAsync fails, grpc responds with the Status, while grpc+verbs does not. Should we consider this situation ? @junshi15 ", "comments": ["@mrry ", "@ChenChuang , can you be more specific which function were you referring to? I do not see RecvLocalAsync is defined in rdma_rendezvous_mgr.cc. \r\n\r\nThe error handling part of contrib/verbs is weak, partially because I do not know what to do when verbs fails, except restarting the whole process. Your suggestions, comments and contributions (pull-request) are welcome.", "I feel I'm facing similar situation.  When trying to run two-node set up, I see following errors.\r\n\r\nOn first (chief) worker:\r\n```\r\n...\r\n2017-05-17 22:43:09.680866: I tensorflow/contrib/verbs/rdma_mgr.cc:56] connecting to remote node /job:worker/replica:0/task:1\r\n2017-05-17 22:43:30.237809: I tensorflow/contrib/verbs/rdma.cc:519] channel already connected\r\n2017-05-17 22:43:30.237851: I tensorflow/contrib/verbs/rdma_mgr.cc:56] connecting to remote node /job:ps/replica:0/task:1\r\n2017-05-17 22:43:30.239249: I tensorflow/contrib/verbs/rdma_mgr.cc:56] connecting to remote node /job:ps/replica:0/task:0\r\n2017-05-17 22:43:30.239955: I tensorflow/contrib/verbs/rdma.cc:519] channel already connected\r\n2017-05-17 22:43:31.592456: I tensorflow/contrib/verbs/rdma.cc:519] channel already connected\r\n...\r\n2017-05-17 22:44:30.982164: F tensorflow/contrib/verbs/rdma.cc:679] Check failed: status.ok() RecvLocalAsync was not ok, key/job:worker/replica:0/task:0/cpu:0;5e9e196185e311cc;/job:ps/replica:0/task:0/cpu:0;edge_15063_Mul_145;0:0;141115178875244276 error message: Step 141115178875244276\r\n```\r\n\r\nOn first parameter server:\r\n```\r\n2017-05-17 22:42:51.775275: I tensorflow/contrib/verbs/rdma_mgr.cc:56] connecting to remote node /job:worker/replica:0/task:0\r\n2017-05-17 22:43:13.055682: I tensorflow/contrib/verbs/rdma_mgr.cc:56] connecting to remote node /job:worker/replica:0/task:1\r\n2017-05-17 22:43:30.239808: I tensorflow/contrib/verbs/rdma.cc:519] channel already connected\r\n2017-05-17 22:43:35.198098: I tensorflow/contrib/verbs/rdma.cc:519] channel already connected\r\n2017-05-17 22:43:35.198150: I tensorflow/contrib/verbs/rdma_mgr.cc:56] connecting to remote node /job:ps/replica:0/task:1\r\n2017-05-17 22:43:35.198926: I tensorflow/contrib/verbs/rdma.cc:519] channel already connected\r\n2017-05-17 22:44:29.948398: F tensorflow/contrib/verbs/rdma.cc:130] Check failed: wc_[i].status == IBV_WC_SUCCESS Failed status \r\ntransport retry counter exceeded 12 738900624 129\r\n```\r\n\r\nOn second worker:\r\n```\r\n...\r\n2017-05-17 22:43:29.731111: I tensorflow/contrib/verbs/rdma_mgr.cc:56] connecting to remote node /job:worker/replica:0/task:0\r\n2017-05-17 22:43:29.733157: I tensorflow/contrib/verbs/rdma_mgr.cc:56] connecting to remote node /job:ps/replica:0/task:1\r\n2017-05-17 22:43:29.734459: I tensorflow/contrib/verbs/rdma_mgr.cc:56] connecting to remote node /job:ps/replica:0/task:0\r\n2017-05-17 22:43:30.234363: I tensorflow/contrib/verbs/rdma.cc:519] channel already connected\r\n2017-05-17 22:43:31.588210: I tensorflow/contrib/verbs/rdma.cc:519] channel already connected\r\n2017-05-17 22:43:35.194635: I tensorflow/contrib/verbs/rdma.cc:519] channel already connected\r\n...\r\n2017-05-17 22:44:30.979018: F tensorflow/contrib/verbs/rdma.cc:679] Check failed: status.ok() RecvLocalAsync was not ok, key/job:worker/replica:0/task:1/cpu:0;47caf76f804b2108;/job:ps/replica:0/task:1/cpu:0;edge_16166_Mul_85;0:0;117997835881656439 error message: Step 117997835881656439\r\n```\r\n\r\nOn second parameter server:\r\n```\r\n2017-05-17 22:43:26.510171: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:331] Started server with target: grpc://localhost:31254\r\n2017-05-17 22:43:26.510230: I tensorflow/contrib/verbs/rdma_mgr.cc:56] connecting to remote node /job:worker/replica:0/task:1\r\n2017-05-17 22:43:31.588367: I tensorflow/contrib/verbs/rdma.cc:519] channel already connected\r\n2017-05-17 22:43:31.588410: I tensorflow/contrib/verbs/rdma_mgr.cc:56] connecting to remote node /job:worker/replica:0/task:0\r\n2017-05-17 22:43:31.589394: I tensorflow/contrib/verbs/rdma.cc:519] channel already connected\r\n2017-05-17 22:43:31.589446: I tensorflow/contrib/verbs/rdma_mgr.cc:56] connecting to remote node /job:ps/replica:0/task:0\r\n2017-05-17 22:43:35.195517: I tensorflow/contrib/verbs/rdma.cc:519] channel already connected\r\n2017-05-17 22:44:29.981975: F tensorflow/contrib/verbs/rdma.cc:679] Check failed: status.ok() RecvLocalAsync was not ok, key/job:ps/replica:0/task:1/cpu:0;e73d8038df027221;/job:worker/replica:0/task:0/gpu:0;edge_17945_group_deps_2/NoOp_1;0:0;141115178875244276 error message: Dequeue operation was cancelled\r\n\t [[Node: fifo_queue_DequeueMany = QueueDequeueManyV2[component_types=[DT_BOOL], timeout_ms=-1, _device=\"/job:ps/replica:0/task:1/cpu:0\"](fifo_queue, fifo_queue_DequeueMany/n)]]\r\n```\r\n\r\n`ibv_devinfo` on first machine:\r\n```\r\nhca_id:\tmlx5_1\r\n\ttransport:\t\t\tInfiniBand (0)\r\n\tfw_ver:\t\t\t\t14.14.2320\r\n\tnode_guid:\t\t\t248a:0703:004c:2719\r\n\tsys_image_guid:\t\t\t248a:0703:004c:2718\r\n\tvendor_id:\t\t\t0x02c9\r\n\tvendor_part_id:\t\t\t4117\r\n\thw_ver:\t\t\t\t0x0\r\n\tboard_id:\t\t\tDEL2420110034\r\n\tphys_port_cnt:\t\t\t1\r\n\tDevice ports:\r\n\t\tport:\t1\r\n\t\t\tstate:\t\t\tPORT_DOWN (1)\r\n\t\t\tmax_mtu:\t\t4096 (5)\r\n\t\t\tactive_mtu:\t\t1024 (3)\r\n\t\t\tsm_lid:\t\t\t0\r\n\t\t\tport_lid:\t\t0\r\n\t\t\tport_lmc:\t\t0x00\r\n\t\t\tlink_layer:\t\tEthernet\r\n\r\nhca_id:\tmlx5_0\r\n\ttransport:\t\t\tInfiniBand (0)\r\n\tfw_ver:\t\t\t\t14.14.2320\r\n\tnode_guid:\t\t\t248a:0703:004c:2718\r\n\tsys_image_guid:\t\t\t248a:0703:004c:2718\r\n\tvendor_id:\t\t\t0x02c9\r\n\tvendor_part_id:\t\t\t4117\r\n\thw_ver:\t\t\t\t0x0\r\n\tboard_id:\t\t\tDEL2420110034\r\n\tphys_port_cnt:\t\t\t1\r\n\tDevice ports:\r\n\t\tport:\t1\r\n\t\t\tstate:\t\t\tPORT_ACTIVE (4)\r\n\t\t\tmax_mtu:\t\t4096 (5)\r\n\t\t\tactive_mtu:\t\t1024 (3)\r\n\t\t\tsm_lid:\t\t\t0\r\n\t\t\tport_lid:\t\t0\r\n\t\t\tport_lmc:\t\t0x00\r\n\t\t\tlink_layer:\t\tEthernet\r\n```\r\n\r\n`ibv_devinfo` on second machine:\r\n```\r\nhca_id:\tmlx5_1\r\n\ttransport:\t\t\tInfiniBand (0)\r\n\tfw_ver:\t\t\t\t14.14.2320\r\n\tnode_guid:\t\t\t248a:0703:004c:25f9\r\n\tsys_image_guid:\t\t\t248a:0703:004c:25f8\r\n\tvendor_id:\t\t\t0x02c9\r\n\tvendor_part_id:\t\t\t4117\r\n\thw_ver:\t\t\t\t0x0\r\n\tboard_id:\t\t\tDEL2420110034\r\n\tphys_port_cnt:\t\t\t1\r\n\tDevice ports:\r\n\t\tport:\t1\r\n\t\t\tstate:\t\t\tPORT_DOWN (1)\r\n\t\t\tmax_mtu:\t\t4096 (5)\r\n\t\t\tactive_mtu:\t\t1024 (3)\r\n\t\t\tsm_lid:\t\t\t0\r\n\t\t\tport_lid:\t\t0\r\n\t\t\tport_lmc:\t\t0x00\r\n\t\t\tlink_layer:\t\tEthernet\r\n\r\nhca_id:\tmlx5_0\r\n\ttransport:\t\t\tInfiniBand (0)\r\n\tfw_ver:\t\t\t\t14.14.2320\r\n\tnode_guid:\t\t\t248a:0703:004c:25f8\r\n\tsys_image_guid:\t\t\t248a:0703:004c:25f8\r\n\tvendor_id:\t\t\t0x02c9\r\n\tvendor_part_id:\t\t\t4117\r\n\thw_ver:\t\t\t\t0x0\r\n\tboard_id:\t\t\tDEL2420110034\r\n\tphys_port_cnt:\t\t\t1\r\n\tDevice ports:\r\n\t\tport:\t1\r\n\t\t\tstate:\t\t\tPORT_ACTIVE (4)\r\n\t\t\tmax_mtu:\t\t4096 (5)\r\n\t\t\tactive_mtu:\t\t1024 (3)\r\n\t\t\tsm_lid:\t\t\t0\r\n\t\t\tport_lid:\t\t0\r\n\t\t\tport_lmc:\t\t0x00\r\n\t\t\tlink_layer:\t\tEthernet\r\n```\r\n\r\nAs you may notice, first port in the list is DOWN.  To work around that, I made modification suggested by @bkovalev in the `verbs` to open device like this:\r\n```\r\nibv_context* open_default_device() {\r\n  ibv_device** dev_list;\r\n  ibv_device* ib_dev;\r\n  int num_devices;\r\n  dev_list = ibv_get_device_list(&num_devices);\r\n  CHECK(dev_list) << \"No InfiniBand device found\";\r\n  ib_dev = dev_list[num_devices - 1];  /// THIS IS MODIFICATION\r\n  CHECK(ib_dev) << \"No InfiniBand device found\";\r\n  ibv_context* context = ibv_open_device(ib_dev);\r\n  CHECK(context) << \"Open context failed for \" << ibv_get_device_name(ib_dev);\r\n  return context;\r\n}\r\n```\r\n\r\nAny suggestions?", "@alsrgv The error mostly likely is due to rdma connection failures.I don't have any experience in multiple IB devices in a single box. But from your ib_devinfo log, it seems the first device \"mlx5_0\" was active and the second device \"mlx5_1\" was down.  @bkovalev had a similar issue here: https://github.com/yahoo/tensorflow/issues/5. Maybe he can clarify this. ", "@alsrgv please unbind the \"mlx5_1\" and recheck. The procedure:\r\n$ mst start\r\n$ mst status\r\n$ echo 0000:05:00.0 > /sys/bus/pci/drivers/mlx5_core/unbind\r\n", "@bkovalev when I execute\r\nmst start\r\nStarting MST (Mellanox Software Tools) driver set\r\nLoading MST PCI module - Success\r\nLoading MST PCI configuration module - Success\r\nCreate devices\r\nUnloading MST PCI module (unused) - Success\r\nmst status\r\nMST modules:\r\n------------\r\n    MST PCI module is not loaded\r\n    MST PCI configuration module loaded\r\n\r\nMST devices:\r\n------------\r\n/dev/mst/mt4115_pciconf0         - PCI configuration cycles access.\r\n                                   domain:bus:dev.fn=0000:84:00.0 addr.reg=88 data.reg=92\r\n                                   Chip revision is: 00\r\necho 0000:05:00.0 > /sys/bus/pci/drivers/mlx5_core/unbind\r\nI got the following error\r\nbash: echo: write error: No such device", "I am referring to `RdmaTensorBuffer::SendNextItem()`\uff0cwhere `cb` in `channel_->adapter_->worker_env_>rendezvous_mgr->RecvLocalAsync(step_id, parsed, cb)` will get status of `RecvLocalAsync`\uff0cif `RecvLocalAsync` failed, I think the receiver is expecting to be notified ? @junshi15 ", "@fanlu to disable your device , you need change echo 0000:05:00.0 > /sys/bus/pci/drivers/mlx5_core/unbind to echo 0000:84:00.0 > /sys/bus/pci/drivers/mlx5_core/unbind\r\n\r\n", "Similar situation occurs with the following message: \r\n```\r\n2017-06-16 20:24:35.483589: F tensorflow/contrib/verbs/rdma.cc:678] Check failed: status.ok() RecvLocalAsync was not ok, key/job:worker/replica:0/task:0/gpu:0;27319926cf0b2d4c;/job:ps/replica:0/task:0/cpu:0;edge_103_report_uninitialized_variables/boolean_mask/Squeeze;0:0;97320812532467277 error message: Step 97320812532467277\r\n```\r\nWe are training a VGG16 net using Google's official benchmark script.", "Moreover, different nodes fail for different Ops and/or different devices (CPU/GPU).\r\n\r\nNode 1:\r\n```\r\nCheck failed: status.ok() RecvLocalAsync was not ok, key/job:worker/replica:0/task:0/cpu:0;64747ef5beaa3090;/job:ps/replica:0/task:1/cpu:0;edge_177_v/tower_0/gradients/AddN_4;0:0;94094948734439551 error message: Step 94094948734439551\r\n```\r\n\r\nNode 2:\r\n```\r\nCheck failed: status.ok() RecvLocalAsync was not ok, key/job:worker/replica:0/task:1/cpu:0;9c15404106788d80;/job:ps/replica:0/task:2/cpu:0;edge_126_v/tower_0/gradients/AddN;0:0;79207857884824133 error message: Step 79207857884824133\r\n```\r\n\r\nNode 3:\r\n```\r\nCheck failed: status.ok() RecvLocalAsync was not ok, key/job:worker/replica:0/task:2/cpu:0;058e67ed7fd0680e;/job:ps/replica:0/task:0/cpu:0;edge_207_group_deps_3/NoOp_1;0:0;79431287967600536 error message: Step 79431287967600536\r\n```\r\n\r\nThe same problem goes away changing server protocol back to `grpc`.", "@byronyi The original problem was there were multiple IB devices in a node, but the first device is down. The current Verbs implementation only uses the first IB device, hence the error. Does this apply to you as well?", "This has not been our case. We are using only 1 rNIC with two ports, and two ports connect to two different switches in parallel. So both ports are UP. ", "This was in RoCE config? If so, have you tried IB mode? \r\n\r\nDid this happen at the very beginning of the training, likely the very first step? Or do you have evidence that this happened in the middle of training, i.e. there were a few successful steps prior to these errors.\r\n\r\nI am wondering if this is still related to connection issues, i.e. the connection was not really established. Although, I do not know the exact cause. ", "This happens at very beginning in the training. I am wondering if the connections are setup at all, too.", "Hi @byronyi, \r\nWe did some benchmarks lately, but I don't recall seeing the exact issue you are seeing.\r\ncan u specify the test and cluster spec being used? \r\n\r\nI'm seeing a different issue, tf_cnn_benchmark gets stuck when the scale is >=4 nodes (with 8 gpus each) and working with real ImageNet data. This happens in IB as well as RoCE.\r\nNot clear what is the issue yet, the RDMA connections looks ok.\r\nAnyway, I'll open a different issue for it, and I'll update there on the findings.\r\n\r\nDid u got more information when running with TF_CPP_MIN_VLOG_LEVEL=2 ? \r\nIt's a lot of verbosity (all the VLOG(2) prints) but maybe can add good data.", "Hi @shamoya I followed the configuration guide [here](https://community.mellanox.com/docs/DOC-2911).\r\n\r\nHardware:\r\nWe are using a 4-node 40Gb RoCE topology, with 4 K40m GPUs each node and and ConnectX-3 dual port NIC that connects to 2 MSN2100-BB2F switches. The switch is configured with link-level flow control (as opposed to PFC). \r\n\r\nSoftware:\r\nWe are using Debian 9 with in-stock kernel version 4.9, and MLNX_OFED 4.0-2.0.2.0 that built from source. We are using CUDA 8.0 with cuDNN 6.0, but the bug seems to appear even when running on CPU. But it runs smoothly within the same box, i.e. there is no such error when running worker and ps on the same host only.\r\n\r\nWe regularly rebase our codebase with current TF master, and the issue seems to be rather persistent. We are using the latest ``tf_cnn_benchmark.py`` in the benchmark repository.\r\n\r\nFor the verbosity issue, I'll simply suggest you to change the ones that interest you to ``VLOG(0)`` so it prints by default.", "Just as an update, [here](https://gist.github.com/byronyi/52496c089b98bbe254f899742cf9bf93) and [there](https://gist.github.com/0942396aef46720d6791839e7423a9a3) are the complete logs when we run into errors. I do have some time lately so I could try to figure it out this week.\r\n\r\n@junshi15 Any light shed on these logs?\r\n\r\nUpdate: just as I looked into it, it appears to be caused by size mismatch:\r\n```\r\nF tensorflow/contrib/verbs/rdma.cc:786] Check failed: (buffer_size == size_ && rm.data_type_ != DT_STRING) || (buffer_size <= size_ && rm.data_type_ == DT_STRING) tensor and buffer size do not agree! buffer_size = 591 requested tensor size = 583Tensor<type: int64 shape: [0,1] values: >\r\n```\r\nI will try the patch in #10700 first and see if that could fix this issue.", "@byronyi for size mismatch, do you expect the tensor size change over the course of training?\r\n\r\nCurrently, the tensor size is assumed to be fixed except for string tensor. PR#10700 relaxes that requirement by sending a warning. Please make sure the size change is intended/expected before applying that patch.\r\n\r\nThe worker error may be due to PS error.", "@junshi15 Indeed the errors are gone after applying the patch.\r\n\r\nFor the record, I did not write the model myself; I am using the in-stock VGG16 available from [benchmark scripts](https://www.tensorflow.org/performance/performance_models), i.e. [``tf_cnn_benchmarks.py``](https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py). Shall we just consider tensors changing their sizes as a normal case? Maybe then we should have a more carefully designed patch than just relaxing the requirement as in #10700, if you already  have some idea on that.", "I did not use that script. I have been using VGG model inside tensorflow/slim. According to discussion in #10700, we should allow tensor size to vary.", "@shamoya I have successfully running the benchmark with at least one of the models in ``tf_cnn_benchmarks.py`` with the cluster spec I mentioned above. Any hints you got with more verbose logging?", "@junshi15 \r\n\r\nFor multiple IB devices in a single box. **open_default_device** may open a DOWN state device .\r\n\r\nHow about this change ? thank you :)  [#L28](https://github.com/AEGQ/CProj/blob/master/yahoo/common/rdma/rdma_adapter.cc#L28)", "@AEGQ \r\nThere was a proposal for the user to specify the device. Yours is to find the first usable device. I think your approach will work as well. If you want to make the change, please send in a pull-request. Thanks.", "@junshi15 \r\n\r\nthanks for you reply :)\r\n\r\nI will send a PR after test if it ([L69](https://github.com/AEGQ/tensorflow/blob/verbs/tensorflow/contrib/verbs/rdma.cc#L69)) is acceptable to you :) ", "Nice @AEGQ \r\nWe are working on a similar patch which adds control over some configurations from the user. \r\nBeside RDMA device we also want to have configurable port, gid index, and various QP and CQ parameters. \r\nStill debating if to use env variables (like in your case) or passing some configuration file/proto to Server creation. What do you think ? \r\n\r\n@byronyi, good to hear the script is working for you.\r\nWe are using the same script (tf_cnn_script) and we are still debugging the stuck issue.\r\nIt only happens with 6 or more GPUs per worker and when using real data (ImageNet).\r\nThe workers for some reason endlessly request the tensor \"edge_4_global_step\" (INT64 type) from the PS.\r\nThis is the part of the log from the worker that is repeated endlessly\r\nhttps://gist.github.com/shamoya/745ece658346978e43899b334a0ee4ff\r\n ", "@shamoya I'll take a look into the logs tomorrow. \n\nBtw, my GDR patch uses librdmacm so it should be easier to configure host, port, and other related issues :)", "@shamoya \r\n\r\nThanks and glad to hear that , it will be great to have fix and optimization from Mellanox , I will waiting for that .\r\n\r\nps:\r\nIn my use case, it is easy to add env variables  to the docker container other than mount the configure file :)  \r\n", "Thanks @byronyi , please ignore the log.\r\nThe GlobalStepWatcher fooled me (https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py#L229) by reading the global_step 4 times a second (while the training step was stuck).\r\nI removed it and now I have a cleaner log where things are actually stuck without traffic at all (:\r\nThis bug is driving me crazy, a lot because it only  happens when working with \"real\" data (ImageNet).\r\nCan happen after 90 steps or after 1.\r\nI'm starting to think maybe we should use UCX instead of native verbs implementation, since it abstract much of the low level verbs issues and implements already Rendezvous, etc.. \r\n\r\n@AEGQ, no problem. I think we will go for the environment variables.\r\nAnyway I must find the bug before.", "@shamoya Or maybe you could take a look at my patch. Simple design without all the hassles.", "@byronyi sure thing I will take a look.\r\nWill get to it in the weekend for sure.", "That patch is an effort we've been working on for over a year, and it's been tested on our RoCE environment extensively. Moreover, the whole design is considered carefully to integrate well with existing TF bits (and even the un-open sourced part; see [here](https://github.com/tensorflow/tensorflow/pull/10629#issuecomment-312032067)), with a memory allocator that re-use pre-pinned tensor buffer. Since the buffers are pre-pinned, we only need a single READ to retrieve the tensor buffer from remote, and this greatly simplifies my design (no need for a state machine; just read and then invalidate).", "@shamoya , I am wondering if your hang is caused by a corrupted message due to a multi-thread race condition. Let's say an Ack is corrupted, hence not parsed properly, then the remote buffer is not released. as a result, no more message can be sent. I describe the messaging system in detail [here](https://github.com/tensorflow/tensorflow/issues/11416) in case you did not know already.\r\n\r\nIf you see multiple lines like `remote_status_ is not idle`, there is a good chance the messaging system is screwed up.", "Hi @junshi15 \r\nThis is not the case I'm seeing in my case.\r\nI get ACKs to all REQUEST/RESPONSES, and everything looks good from the verbs protocol side.\r\nI even checked hash collision in the buffer table - and it's not it as well.\r\n\r\nAfter serious debug, it looks like something wrong with the step_id handling.\r\nI'm not sure it's related to the verbs code directly, but something in Distributed TF, with the timing happens only with: RDMA && large number of GPUS && loading ImageNet from storage.", "@shamoya Thanks for the explanation. It might be something else then.", "@junshi15  - I opened up https://github.com/tensorflow/tensorflow/issues/11725 \r\nI tried alone for some time (2 weeks) but still I failed to understand what exactly is the issue.\r\n", "@shamoya I have tried with the latest (1.4-dev0) and the same issue, i.e. edge_4_global_step being repeatedly requested, showed up even without any verbs nor GDR code (i.e. the default build options with gRPC). I suspect it is a bug in gRPC runtime side (perhaps not being able to handle higher request rates with the latest upgrade), and I am filing a bug report regarding to this.", "Hi @byronyi,\r\nI found out that the repeated edge_4_global request is actually coming from [here](https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py#L241).\r\nIt's a python thread that reads the global counter for counting and timing process.\r\nOnce something is stuck, this thread endlessly continues to read the global counter (4 times in a second).\r\nIt didn't related to the stuck issue - once I disabled this thread we indeed saw the runtime was stuck without further log messages (the stuck bug was fixed in #12361).\r\nSorry for not updating this.", "Hi @shamoya thanks for the heads up! I've taken a closer look and it seems to be an unrelated issue in my environment that caused the hang. Btw I've just sent a PR #13140 to remove the sync wrappers in the GDR patch, now it should be a little bit faster. ", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 9925, "title": "graph_editor copy_with_input_replacements doesn't update colocation constraints", "body": "It seems if you try to use graph_editor to make copy of a model to place on another device, the new graph will still refer to old version inside colocation constraints. \r\n\r\nThis causes errors like below when trying to run resulting graph.\r\n`tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot colocate nodes 'gradients/Max_1_1_grad/mul' and 'gradients/AddN_13: Cannot merge devices with incompatible ids: '/GPU:0' and '/GPU:1'\r\n`\r\n\r\nMore natural might be to update colocation constraints to point to newly created copies of ops.\r\n\r\nTest case\r\n```\r\n  import tensorflow.contrib.graph_editor as ge\r\n  tf.reset_default_graph()\r\n  with tf.device('/cpu:0'):\r\n    a = tf.ones((), name='a')\r\n    with tf.get_default_graph().colocate_with(a):\r\n      b = tf.add(a, 1, name='b')\r\n  g = tf.get_default_graph()\r\n  ops = g.get_operations()\r\n  copied_sgv, info = ge.copy_with_input_replacements(ge.sgv(ops),\r\n                                                       {})\r\n  print(tf.get_default_graph().as_graph_def())\r\n```\r\n\r\nYou will see that newly created `b_1` op will refer to old op `a`\r\n\r\n```\r\nnode {\r\n  name: \"b/y_1\"\r\n  op: \"Const\"\r\n  device: \"/device:CPU:0\"\r\n  attr {\r\n    key: \"_class\"\r\n    value {\r\n      list {\r\n        s: \"loc:@a\"\r\n      }\r\n    }\r\n  }\r\n```\r\n\r\n@purpledog ", "comments": ["BTW, I used following work-around to rewrite colocation constraints in copied graph to point to copied ops. I couldn't find format of colocation groups documented so just assumed they are in the form `_class: loc:@opname` and it seemed to work \r\n\r\n    from tensorflow.core.framework import attr_value_pb2\r\n    import tensorflow.contrib.graph_editor as ge\r\n\r\n    copied_sgv, info = ge.copy_with_input_replacements(ge.sgv(self.ops),\r\n                                                       replacements)\r\n\r\n    def to_bytes(s): return s.encode('ascii')\r\n    def from_bytes(s): return s.decode('ascii')\r\n\r\n    # fix colocation constraints to point to copied ops\r\n    new_ops = [info._transformed_ops[op] for op in self.ops]\r\n    for new_op in new_ops:\r\n      assert len(new_op.colocation_groups()) == 1\r\n      colocation_group = new_op.colocation_groups()[0]\r\n      assert colocation_group.startswith(b'loc:@')\r\n      colocated_with_name = from_bytes(colocation_group[len(b'loc:@'):])\r\n    \r\n      # if there were no colocation constraints, the op gets colocated with\r\n      # itself (default colocation group), ignore that constraint\r\n      if colocated_with_name == new_op.name:\r\n        continue\r\n    \r\n      colocation_op = g.get_operation_by_name(colocated_with_name)\r\n      if colocation_op in info._transformed_ops:\r\n        new_colocation_op = info._transformed_ops[colocation_op]\r\n      else:\r\n        # TODO: make it work with variables\r\n        assert colocation_op in self.input_ops\r\n        colocation_op_idx = self.input_ops.index(colocation_op)\r\n        new_colocation_op = new_inputs[colocation_op_idx].op\r\n        \r\n      # overwrite existing _class attribute with new colocation constraints\r\n      new_colocation_groups = [b'loc:@'+to_bytes(new_colocation_op.name)]\r\n      new_op.node_def.attr[\"_class\"].CopyFrom(attr_value_pb2.AttrValue(\r\n        list=attr_value_pb2.AttrValue.ListValue(s=new_colocation_groups)))\r\n", "Thanks Yaroslav, @purpledog, any thoughts?", "To avoid spamming new issues on graph_editor, here's another one I found -- `copy_with_input_replacements` scales quadratically with size of the graph. Copying 80k piece of graph several times, most of the time spent inside `assign_renamed_collections_handler`, which in turn caused 300M calls to `__equals__` method inside `ops.py`\r\n\r\nThe work-around is to call `disable_collections_handler()` function below which disables graph editor's `assign_renamed_collections_handler`. That cut down the time spent inside `copy_with_input_replacements` from 332 seconds to 48 second.\r\n\r\nI'm happy to say that with these two issues fixed/hacked-around, `graph_editor` is usable for replicating large graphs (ie 200k node pixel CNN) onto multiple devices in reasonable time (<15 minutes)\r\n\r\n```\r\n# work-around for graph_editor.copy_with_input_replacements scaling\r\n# quadratically with size of the graph\r\nfrom tensorflow.contrib.graph_editor import transform\r\noriginal_assign_renamed_collections_handler = transform.assign_renamed_collections_handler\r\ndef dummy_collections_handler(info, elem, elem_): pass\r\ndef disable_collections_handler():\r\n  transform.assign_renamed_collections_handler = dummy_collections_handler\r\ndef enable_collections_handler():\r\n  transform.assign_renamed_collections_handler = original_assign_renamed_collections_handler\r\n```\r\n<img width=\"1222\" alt=\"screenshot 2017-05-18 10 03 50\" src=\"https://cloud.githubusercontent.com/assets/23068/26214540/9a506bf6-3bb1-11e7-9e89-3986a17932f9.png\">\r\n", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "I reported a similar problem occurs in TF 1.4, the same codes work fine in TF 1.2: \r\n\r\n[https://github.com/tensorflow/tensorflow/issues/3198](url)\r\n\r\nI don't know if it is related with this issue mentioned by Yaroslav. Is there any approach or workaround\r\nto avoid errors as below?\r\n\r\n---------------------------------------------\r\nInvalidArgumentError (see above for traceback): Cannot colocate nodes 'tvars_ps35_embedding_attention_seq2seq_embedding_attention_decoder_attention_decoder_AttnOutputProjection_biases/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnOutputProjection/biases/read_tvars_ps35_embedding_attention_seq2seq_embedding_attention_decoder_attention_decoder_AttnOutputProjection_biases_0' and 'embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnOutputProjection/biases/RMSProp_1: Cannot merge devices with incompatible jobs: '/job:worker/task:1/device:CPU:0' and '/job:ps/task:0/device:CPU:0'\r\n\r\n\r\n", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/9925\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/9925\">No</a>\n"]}, {"number": 9924, "title": "Quantize conv2d transpose", "body": "This PR creates a new quantized kernel called `QuantizedDeconv2D`, which performs a deconvolution computation (also known as `conv2d_transpose` in TensorFlow) in a quantized mode.\r\n\r\nThe current progress of this new kernel includes:\r\n\r\n1. A reference implementation of deconvolution\r\n2. Two unit tests with small dimensions and different strides\r\n3. Only `VALID` padding is allowed\r\n\r\nFurther guidance on this work is welcomed!", "comments": ["Can one of the admins verify this patch?", "@andrewharp what do you think?", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "CLAs look good, thanks!\n\n<!-- ok -->", "@petewarden Hi Pete, thank you so much for your comments and review! I have fixed several of them and will come back later for the rest.", "@petewarden could you take another look, please?", "@petewarden any cycles to look at this? Feel free to assign someone else.", "@petewarden ping", "@petewarden Hi Pete, sorry for my late reply. Thank you for your kind comments. I resolved most of your issues mentioned previously.\r\n\r\nThere is only one thing left: you mentioned that I should implement a test in [here](/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/quantize_nodes_test.cc).\r\nI tried, but the problem is: there is no corresponding C++ implementation of `Conv2DTranspose`,\r\nand what we have is `Conv2DBackpropInput`. So I think maybe it is a better idea to establish a clear mapping between `QuantizedConv2DTranspose` and `Conv2DBackpropInput` before writing the test. \r\n\r\nHowever, before I start to work on it, could you please help me with the following question:\r\nShould I build `QuantizedConv2DTranspose` on a new `QuantizedConv2DBackpropInput` op and map that to `Conv2DBackpropInput`, or:\r\ndirectly map `QuantizedConv2DTranspose` to `Conv2DBackpropInput`?\r\n\r\nThanks again for all your help.", "Thanks for your flexibility on this!\r\n\r\n> directly map QuantizedConv2DTranspose to Conv2DBackpropInput?\r\n\r\nI think this approach is best.", "@petewarden Hi, I just added a simple test regarding quantizing `Conv2DBackpropInput` to `QuantizedConv2DTranspose`. \r\nThere is also a slight change in the quantize graph generation part, which can allow you convert a node to a quantized one with a custom node name, not just Quantized*.\r\n\r\nPlease let me know if you have further suggestions. Thanks!", "@petewarden can you take another look?", "@petewarden any luck with this?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "any luck with this one?", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 60 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 75 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 90 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 105 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Sorry for the long delay on this PR. We're moving away from this approach for supporting quantization inside TensorFlow (all quantized ops now live in TF Lite), so unfortunately we'll need to close this request.", "@petewarden Is it possible to convert conv2d_tranpose to quantized uint8 tflite model right now?"]}, {"number": 9923, "title": "Change SummaryWriter --> FileWriter in TensorBoard", "body": "Noticed a few spots in TensorBoard referenced `SummaryWriter` instead of `FileWriter`. This change cleans up some notifications, documentation, and one of the tests.", "comments": ["Can one of the admins verify this patch?", "Tensorboard is moving to [tensorflow/tensorboard](https://github.com/tensorflow/tensorboard). \r\n\r\nThis change may or make not make it there (@dandelion has been focusing on making this happen, which should make future contributions to Tensorboard much easier).\r\n\r\nCan you reopen the PR over there?"]}, {"number": 9922, "title": "Update README.md of tootls/benchmark", "body": "People run into this may have never seen https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android", "comments": ["Can one of the admins verify this patch?", "Jenkins test this please.", "Jenkins test this please."]}]