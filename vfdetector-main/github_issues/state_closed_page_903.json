[{"number": 26379, "title": "TF Framework ops_eager_test missing test case add", "body": "1-disable_eager_execution api test case", "comments": ["Sorry for the delay, I've been out of the office."]}, {"number": 26378, "title": "Centos7.6+python37+tensorflow1.13.1 : ImportError: /lib64/libstdc++.so.6: version `CXXABI_1.3.8' not found", "body": "```\r\nPython 3.7.1 (default, Dec 14 2018, 19:28:38) \r\n[GCC 7.3.0] :: Anaconda, Inc. on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/zhaodachuan/zhaodachuan/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/zhaodachuan/zhaodachuan/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/zhaodachuan/zhaodachuan/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/zhaodachuan/zhaodachuan/anaconda3/lib/python3.7/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/zhaodachuan/zhaodachuan/anaconda3/lib/python3.7/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: /lib64/libstdc++.so.6: version `CXXABI_1.3.8' not found (required by /home/zhaodachuan/zhaodachuan/anaconda3/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/zhaodachuan/zhaodachuan/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/home/zhaodachuan/zhaodachuan/anaconda3/lib/python3.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/zhaodachuan/zhaodachuan/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/zhaodachuan/zhaodachuan/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/zhaodachuan/zhaodachuan/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/zhaodachuan/zhaodachuan/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/zhaodachuan/zhaodachuan/anaconda3/lib/python3.7/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/zhaodachuan/zhaodachuan/anaconda3/lib/python3.7/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: /lib64/libstdc++.so.6: version `CXXABI_1.3.8' not found (required by /home/zhaodachuan/zhaodachuan/anaconda3/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n```\r\n\r\n\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nCentOS Linux release 7.6.1810 (Core)\r\n- TensorFlow installed from (source or binary):\r\npip install tensorflow\r\n- TensorFlow version:\r\n1.13.1\r\n- Python version:\r\nPython 3.7.1\r\n- Installed using virtualenv? pip? conda?:\r\npip install tensorflow\r\n\r\n\r\n\r\n\r\n**Describe the problem**\r\nI think the problem is that the version of gcc is 4.8 on Centos7.6.\r\nWhat should I do to install tensorflow1.13 with Centos7+python37 ?\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\n$ pip install tensorflow\r\n$ python \r\n>>> import tensorflow\r\n```\r\n\r\n", "comments": ["what's your libstdc++.so.6 linked version?.", "Build gcc 6.3, you would get libstdc++.so.6.0.22. Create  libstdc++.so.6 soft-link to it, may solve your problem", "Hi,@Leslie-Fang, \r\nThe libstdc++.so.6 linked version is libstdc++.so.6.0.19 on Centos7.6\r\nI don't want to change the system file in` /lib64` because it maybe bring other problems.\r\nIs there any soluiton to solve it without using `sudo`", "```strings /usr/lib64/libstdc++.so.6.0.19 | grep CXXABI_ ``` to check whether `CXXABI_1.3.8'.\r\nIf not, you could use latest lib version. Set the LD_LIBRARY_PATH before you run TF, so this lib would only be effective in this shell. ", "Hi,@Leslie-Fang \r\nthe result is :\r\n```\r\n-bash-4.2$ strings /usr/lib64/libstdc++.so.6.0.19 | grep CXXABI_\r\nCXXABI_1.3\r\nCXXABI_1.3.1\r\nCXXABI_1.3.2\r\nCXXABI_1.3.3\r\nCXXABI_1.3.4\r\nCXXABI_1.3.5\r\nCXXABI_1.3.6\r\nCXXABI_1.3.7\r\nCXXABI_TM_1\r\n```\r\nHow can I install the latest lib version without `sudo`?", "Build GCC6.3, you will get the libstdc++.so.6.0.22", "Hi,@Leslie-Fang \r\nBuilding GCC6.3 needs `sudo`.Is there any solution else?", "@DachuanZhao Could you search libstdc++.so.6.0.22 online? I think it's available.", "Has this been resolved yet?", "Just wanted to add that I have this issue, as well. Very similar configuration as OP.", "same issue here, does anyone know how to resolve?", "We have several versions of python available to use. I had this problem when I tried to use python/3.7.3, because it was compiled with gcc/6.4 (on a different system), but our system only has gcc/6.3.1 support.  I found that we have an older version of python (3.6.2) that was compiled with gcc/4.8.5, so I switched to that, and then `import tensorflow` worked just fine.\r\n\r\nSo the workaround is to install a version of python that is compatible (or older) with gcc on your system. \r\n\r\nWhen you run python, it immediately prints out the version of gcc support to the console, and you can check the version of gcc install with `gcc --version`.\r\n\r\nI bet that if you recompile python3 on your system, then tensorflow will run.\r\n\r\nThis combination worked for me:\r\n\r\n\r\n```\r\n$ python3\r\nPython 3.6.2 (default, Jul 27 2017, 20:29:23)\r\n[GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] on linux\r\n\r\n$ gcc --version\r\ngcc (GCC) 6.3.1 20170216 (Red Hat 6.3.1-3)\r\n\r\n```\r\n\r\n(python older than gcc)\r\n", "It gave me same error when I tried to install with pip. I am using conda. I tried with conda (conda install tensorflow). It worked.", "Looping back on this issue - would it be possible to add a more informative error message detailing @marklakata's [explanation](https://github.com/tensorflow/tensorflow/issues/26378#issuecomment-486345435)?\r\n\r\n@martinwicke ", "> We have several versions of python available to use. I had this problem when I tried to use python/3.7.3, because it was compiled with gcc/6.4 (on a different system), but our system only has gcc/6.3.1 support. I found that we have an older version of python (3.6.2) that was compiled with gcc/4.8.5, so I switched to that, and then `import tensorflow` worked just fine.\r\n> \r\n> So the workaround is to install a version of python that is compatible (or older) with gcc on your system.\r\n> \r\n> When you run python, it immediately prints out the version of gcc support to the console, and you can check the version of gcc install with `gcc --version`.\r\n> \r\n> I bet that if you recompile python3 on your system, then tensorflow will run.\r\n> \r\n> This combination worked for me:\r\n> \r\n> ```\r\n> $ python3\r\n> Python 3.6.2 (default, Jul 27 2017, 20:29:23)\r\n> [GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] on linux\r\n> \r\n> $ gcc --version\r\n> gcc (GCC) 6.3.1 20170216 (Red Hat 6.3.1-3)\r\n> ```\r\n> \r\n> (python older than gcc)\r\n\r\nThanks a lot! I tried to reinstall the lower version of python with the older version gcc and it worked, that is, \r\n> $ conda install python==3.6.0  (the former version is 3.6.1)", "Hi @DachuanZhao ! Could you please visit [here ](https://www.tensorflow.org/install)for more details on errors and installation procedures.  We  also see that you are using old version of Tensorflow which is officially considered as end of life, We recommend that you upgrade to 2.6 version and let us know if the issue still persists in newer versions .Please open a new issue in case you face any errors, we will get you the right help .Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26378\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26378\">No</a>\n"]}, {"number": 26377, "title": "ParseOpData function schema changed to const", "body": "Changed the schema params to const", "comments": ["@jdduke , thanks for pointing that out, i have changed the code as per our suggestion i.e used static_cast. Kindly check and approve."]}, {"number": 26376, "title": "ModuleNotFoundError: No module named 'gast'", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nCustom code - https://github.com/SeongokRyu/augmented-GCN\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nGNU/Linux \r\n\r\n- TensorFlow installed from (source or binary):\r\nconda install -c conda-forge tensorflow \r\n\r\n- TensorFlow version (use command below):\r\n1.12.0\r\n\r\n- Python version:\r\nPython 3.6.8 \r\n\r\n- GPU model and memory:\r\nProcessor type - x86_64\r\n\r\nMemory:         total - 62G,    used - 58G,   free - 4.3G\r\n\r\n**Describe the problem**\r\nBeen utilizing tensorflow 1.10.0 for a weeks or so, and the custom code I've been running gave me no issues. After updating to 1.12.0 today, I get an error rooting from the `tf.get_variable()` function: ModuleNotFoundError: No module named 'gast'.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nBefore running into the problem I updated tensorflow with `conda update tensorflow=1.12.0` then proceeded to run a script that calls the function which raised the problem.\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n<img width=\"1047\" alt=\"screen shot 2019-03-05 at 21 23 10\" src=\"https://user-images.githubusercontent.com/35546511/53857913-eca94b80-3f8c-11e9-9c9f-7a23260928f0.png\">\r\n\r\n", "comments": ["Hi,@sayred1 \r\nI guess you can run the following command to solve it:\r\n`pip install gast`", "i got this error when i install gast\r\nERROR conda.core.link:_execute_actions(337): An error occurred while installing package 'defaults::tqdm-4.32.1-py_0'.\r\nCondaError: Cannot link a source that does not exist. C:\\Python-Anaconda\\Anaconda3\\Scripts\\conda.exe\r\nAttempting to roll back.", "> Hi,@sayred1\r\n> I guess you can run the following command to solve it:\r\n> `pip install gast`\r\n\r\nwhen i did this command, cmd told me that \"Requirement already satisfied: gast in c:\\users\\asus\\anaconda3\\lib\\site-packages (0.2.2)\" , but in my Spyder still this error is repeated . please help me", "try \r\n```\r\npip uninstall gast\r\npip install tensorflow\r\n```\r\n\r\n", "```\r\nERROR: tensorflow-gpu 2.2.0 has requirement gast==0.3.3, but you'll have gast 0.2.2 which is incompatible.\r\nERROR: tensorflow-gpu 2.2.0 has requirement tensorboard<2.3.0,>=2.2.0, but you'll have tensorboard 2.0.0 which is incompatible.\r\nERROR: tensorflow-gpu 2.2.0 has requirement tensorflow-estimator<2.3.0,>=2.2.0, but you'll have tensorflow-estimator 2.0.0 which is incompatible.\r\n```\r\n\r\nso i had this error and i downgraded gast and others cause the previous message was asking for a downgrade but upon downgrading then the flag was asking upgrade, not sure if it has to do with me having both installed tensofrlow and tensorflow-gpu\r\n\r\nit was never clear to me if you are required to install both tensorflows, im upgrading my tf to match the gpu version now even tho that might make some of my book snippets not run anymore...\r\n\r\n```\r\nERROR: tensorflow 2.0.0 has requirement gast==0.2.2, but you'll have gast 0.3.3 which is incompatible.\r\nERROR: tensorflow 2.0.0 has requirement tensorboard<2.1.0,>=2.0.0, but you'll have tensorboard 2.2.2 which is incompatible.\r\nERROR: tensorflow 2.0.0 has requirement tensorflow-estimator<2.1.0,>=2.0.0, but you'll have tensorflow-estimator 2.2.0 which is incompatible.\r\n```"]}, {"number": 26375, "title": "Train Keras model with GPU Google Colab very slow when compared with my personal cpu", "body": "**Information environment when I run [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)**\r\n\r\n== cat /etc/issue ===============================================\r\nLinux dc5aaefaf4e1 4.14.79+ #1 SMP Wed Dec 19 21:19:13 PST 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== are we in docker =============================================\r\nYes\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCopyright (C) 2017 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux dc5aaefaf4e1 4.14.79+ #1 SMP Wed Dec 19 21:19:13 PST 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nmesh-tensorflow          0.0.5                \r\nmsgpack-numpy            0.4.3.2              \r\nnumpy                    1.14.6               \r\nprotobuf                 3.6.1                \r\ntensorflow               1.13.1               \r\ntensorflow-estimator     1.13.0               \r\ntensorflow-hub           0.3.0                \r\ntensorflow-metadata      0.12.1               \r\ntensorflow-probability   0.6.0                \r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.13.1\r\ntf.GIT_VERSION = b'v1.13.1-2-g09e3b09e69'\r\ntf.COMPILER_VERSION = b'v1.13.1-2-g09e3b09e69'\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /usr/lib64-nvidia\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nWed Mar  6 04:31:12 2019       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 410.79       Driver Version: 410.79       CUDA Version: 10.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\r\n| N/A   36C    P0    71W / 149W |      0MiB / 11441MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/lib/python3.6/dist-packages/torch/lib/libcudart-1581fefa.so.10.0\r\n/usr/local/lib/python2.7/dist-packages/torch/lib/libcudart-1581fefa.so.10.0\r\n/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudart.so.10.0.130\r\n/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-10.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-10.0/doc/man/man7/libcudart.7\r\n\r\n\r\n**Describe the current behavior**\r\nTrain time of each epoch on google colab gpu: ~60s\r\nTrain time of each epoch on my personal pc cpu: ~10s\r\n\r\n**Code to reproduce the issue**\r\nI train crf model use keras, keras_contrib library\r\nCode:\r\n```python\r\n    input = Input(shape=(config[\"input_dim\"],))\r\n    model = Embedding(input_dim=config[\"vocab_size\"] + 1, output_dim=config[\"embedding_dim\"],\r\n                      input_length=config[\"input_dim\"], mask_zero=True)(input)\r\n    model = Bidirectional(LSTM(units=config[\"lstm_dim\"],\r\n                               return_sequences=True, recurrent_dropout=config[\"recurrent_dropout\"]))(model)\r\n    model = TimeDistributed(Dense(config[\"dense_dim\"], activation=\"relu\"))(model)\r\n    crf = CRF(config[\"num_tags\"])  # CRF layer\r\n    output = crf(model)  # output\r\n\r\n    model = Model(input, output)\r\n    model.compile(optimizer=config[\"optimizer\"], loss=crf_loss, metrics=[crf_accuracy])\r\n    model.summary()\r\n\r\n```\r\n\r\n**Logs**\r\n2019-03-06 04:13:53.402787: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\r\n2019-03-06 04:13:53.403069: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x23c8dc0 executing computations on platform Host. Devices:\r\n2019-03-06 04:13:53.403105: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-03-06 04:13:53.491058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-03-06 04:13:53.491582: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55374a0 executing computations on platform CUDA. Devices:\r\n2019-03-06 04:13:53.491616: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\r\n2019-03-06 04:13:53.492038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:00:04.0\r\ntotalMemory: 11.17GiB freeMemory: 11.10GiB\r\n2019-03-06 04:13:53.492071: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\r\n2019-03-06 04:13:53.863400: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-03-06 04:13:53.863478: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \r\n2019-03-06 04:13:53.863513: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \r\n2019-03-06 04:13:53.863768: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\r\n2019-03-06 04:13:53.863821: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10754 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\r\n2019-03-06 04:13:55.629273: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\r\n", "comments": ["@quancq  Can you provide a code to run and reproduce the bug? Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!", "@jvishnuvardhan \r\nHi it seems that it's duet to the bug from Colab.\r\nBecause the GPU of Colab will make almost every package to run slower than that under CPU.\r\nIt's very strange", "@appleyuchi Please create a new issue and provide a small standalone code to reproduce the issue. Thanks!", "Hi !\r\nI've the same thing with my Colab.....\r\nAnd I don't get solution, if someone can help me!????", "I have the same problem here.\r\n\r\nMy laptop has a 960m gpu but it runs tensorflow object detection model training faster than google colab. Am I missing something or is it just normal of free google colab?", "@ArefAz Can you please create an issue with `colab` repository [here](https://github.com/googlecolab/colabtools/issues). thanks!"]}, {"number": 26374, "title": "Make weighted_cross_entropy_with_logits and sigmoid_cross_entropy_with_logits consistent", "body": "This fix tries to address the issue raised in #26337 where weighted_cross_entropy_with_logits and sigmoid_cross_entropy_with_logits are not consistent with funtion definitions.\r\n\r\nThis fix changes the definitions in v2 so that they are consistent, but keep the old defs in v1.\r\n\r\nThis fix fixes #26337.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Looks like the issue has been fixed in  8f75c5a, so closing."]}, {"number": 26373, "title": "Disable bincount_op_test on windows gpu.", "body": "PiperOrigin-RevId: 236907827", "comments": []}, {"number": 26372, "title": "[TF 2.0 API Docs] tf.keras.activations.softmax", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: 2.0\r\n- Doc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/activations/softmax\r\n\r\n\r\n**Describe the documentation issue**\r\n\r\n* The softmax activation function is not described in detail, and there is no recommendation about when to use it.\r\n\r\n* There is no usage example.\r\n\r\n* The description of the returned value could be more useful.\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n\r\nYes.", "comments": ["@dynamicwebpaige \r\nI would like to contribute on this.", "@Jaskaran170599 Please feel free to contribute through PRs. Thanks!", "Looks like the PR is merged already , please close the issue.", "I am closing this as this was already updated in the TF website [here](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/activations/softmax).  \r\n\r\nPlease feel free to reopen if I am mistaken. Thanks!"]}, {"number": 26371, "title": "[TF_Java] Enable Control Dependencies to While Loop Body from Outer Graph", "body": "*System information*\r\n\r\n* TensorFlow version (you are using): master\r\n* Are you willing to contribute it (Yes/No): yes\r\n\r\n*Describe the feature and the current behavior/state.*\r\n\r\nWe would like to be able to add control dependencies to ops within the body of a while loop from ops in the outer graph. \r\n\r\nWhy: we want to be able to update variables in the outer graph from within the while loop body. An important use case for this is training within a while loop. We understand that in Python, training within a loop is done by adding control dependencies within the loop body from the weight/bias update operations in the outer graph. We had hoped to use the same approach in Java.\r\n\r\nCurrent behavior: We aren't able to add control dependencies from anything outside of the body graph. When we do, we get the following error: \r\n\r\n`Exception in thread \"main\" java.lang.IllegalArgumentException: Node 'node_in_body_graph': Unknown input node '^node_in_outer_graph'`\r\n\r\nWe believe this error occurs because in Java, the loop body graph is not connected to the outer graph at the time it is constructed / the time that its control dependencies are added \u2014 it is only connected to the outer graph once FinishWhile is called at the C++ level. \r\n\r\n*Will this change the current api? How?*\r\n\r\nYes, people will be able to add control dependencies to operations within the loop body from operations in the outer graph.\r\n\r\n*Who will benefit with this feature?*\r\n\r\nAnyone who wants to train within their Java while loop, or update variables in the outer graph for any other reason. \r\n\r\n*Any Other info.*\r\n\r\n", "comments": ["cc @skye @sjamesr @karllessard", "Actually that was one of my concern, I don\u2019t know about the low-level details of the while loop functionality, but ideally there should be a relationship between the inner and the outer graph (e.g. the should be able to share the same `Scope`). \r\n\r\nRight now I don\u2019t have a quick solution for this but it worth it to take some time to think about how to achieve this.", "I assume you mean control deps _from_ the outer graph _to_ the inner while loop graph? You're not allowing to have control deps (or any kind of input) from within a loop body to outside the body.", "@skye, yes, that is what I meant -- I'll update the above to avoid confusion. ", "Got it, thanks!\r\n\r\nI agree with Karl, we need some notion of graphs nested in other graphs. In the Python API, this is represented via functions (i.e. [FuncGraphs](http://go/gh/tensorflow/tensorflow/blob/master/tensorflow/python/framework/func_graph.py#L126), which can \"capture\" Tensors from outer graphs). while_v2 takes advantage of this to handle closed-over inputs inside the loop body. As discussed in a different thread, it'll probably be a lot of work to go down this path for the C/++ APIs.\r\n\r\nA quicker solution might be to add the concept of an outer graph to Graph (or maybe just a \"friend\" Graph? Like in C++ :)), and allow Nodes to have inputs from ancestor graphs. We'd still have a different Scope for each Graph. I'm a little wary of this, because in general it's not safe to use inputs from different Graphs -- this would only work because we eventually copy the inner graph into the outer graph. But I don't think it'd be much code.", "Sounds good, I will give that approach a try! Thanks @skye and @karllessard. ", "Keep us update. There are surely tons of complications I'm not foreseeing... :)", "Just out of curiosity @melissagrueter, would eager execution be a better fit for the use case you are trying to cover (i.e. using Java native while loops)?", "@skye will do!\r\n\r\n@karllessard -- we'd like to have the option to do this in a static graph. That said, we're definitely keeping an eye on your eager mode PR and are excited to see where eager mode goes in TF Java!", "Hi @skye, \r\n\r\nI've been able to add control dependencies from the outer graph to the loop body, but I'm seeing some unexpected behavior: the control input operations are executed a nondeterministic number of times. From what I've seen so far, they run either n, n-1, or n+1 times, where n is the number of loop iterations. If there are multiple control dependencies, they are not all necessarily executed the same number of times in a single run.\r\n\r\n\r\nTo illustrate, here's some quick pseudocode of what I'm seeing:\r\n```\r\n x = 0  \r\n y = 0\r\n\r\n //control dependencies \r\nassign x = x + 1\r\nassign y = y + 1\r\n\r\n i = 0 \r\nwhile (i < 10) {\r\n  i = i + 1 \r\n }\r\n```\r\n\r\nThe assigns (assign x and assign y) are control inputs to either the const (1) or the add operation (i + 1) in the while loop body. Either way, I've seen nondeterminism. Running a session with the loop output as a target and fetching x and y, we might get x = 10 and y = 11. In the next run we might get x = 11 and y = 11, or x = 10 and  y = 9. I don't understand why this nondeterminism is happening.\r\n\r\nSo far, I've been creating enter nodes to enter the control inputs into the while loop frame (entering constant 1 and variable x and the other constant 1 and variable y). Looking at the protobuf for similar code in Python, however, it appears they enter them into the loop frame by creating enter nodes for the variables (x and y), but adding the Identity of Switch::true as a control input to the constants (both 1). We've tried this in C++, and it seems to get rid of the nondeterminism issue. However, we've only tried this with our own implementation of while loop from primitives where we have access to while loop internals. You mentioned above that control dependencies are not allowed from within the loop body to the outer graph, so I'm not sure this approach will be possible. I'm also not sure why this appears to fix the nondeterminism issue.\r\n\r\nIf you have any further insight into control dependency behavior that would be super helpful!\r\n\r\nThanks, Melissa\r\n", "Hi, sorry for the delay. FYI I switched teams so am no longer actively working on TensorFlow, but I'll continue helping you here until I can find a replacement point person.\r\n\r\nI'm a little confused -- in your example, are you trying to update x and y 10 times? And just to clarify, x and y are tf.Variables right? Not just regular tensors. In that case the assigns should be in the while loop body:\r\n```\r\n x = 0  \r\n y = 0\r\n i = 0 \r\nwhile (i < 10) {\r\n  //control dependencies \r\n  assign x = x + 1\r\n  assign y = y + 1\r\n  i = i + 1 \r\n}\r\n```\r\nI think you're somehow constructing something like this, since you're seeing x and y be updated multiple times. Did you mean to say that the i + 1 is a control input to the assigns, not the other way around?\r\n\r\nIn general, the Java while loop code should produce the same thing as the Python code. Even small deviations from what the Python code can produce strange results, as you're finding out :)", "Hi @skye,\r\n\r\nI'm sorry to hear you've left TF; I enjoyed our brief time collaborating! :) Thanks so much for all your help, and also for going above and beyond to still reply here and help us find a new point person \u2014 I really appreciate it!\r\n\r\nTo respond to your points, let me start from the code you sent. Sorry for the confusion!\r\n```\r\nx = 0  \r\ny = 0\r\ni = 0 \r\nwhile (i < 10) {\r\n  assign x = x + 1\r\n  assign y = y + 1\r\n  i = i + 1 // takes assigns as control inputs\r\n}\r\n```\r\nAlthough this code would work in Python, it doesn't currently work in Java. It breaks because x and y are not in the body graph, and we get the following error:\r\n\r\n`Exception in thread \"main\" java.lang.IllegalStateException: Input 0 ('Variable') for 'Assign' was not previously added to ShapeRefiner. `\r\n\r\nI haven't been able to think of a clean way around this. Maybe a \u201cfriend graph\u201d idea like what we discussed with regard to control dependencies earlier would be possible, but from what I saw during my exploration of adding control dependencies, it seems that adding inputs from other graphs is not explicitly disallowed anywhere, so trying to allow it could break things in lots of unexpected places that happen to depend on that assumption. Are you aware of why the implementation of the while loop in C++ creates separate graphs? It seems like it this is the core barrier to mirroring Python behavior. \r\n\r\n\r\n\r\nI found a relatively simple way to add control inputs from the outer graph to the loop body (by only changing while loop code, not allowing control inputs between separate graphs in general), so I was trying to accomplish the desired behavior this way instead:\r\n```\r\nx = 0  \r\ny = 0\r\ni = 0 \r\n \r\nassign x = x + 1\r\nassign y = y + 1\r\n\r\nwhile (i < 10) {\r\n  i = i + 1 // takes assigns as control inputs\r\n}\r\n```\r\nThis avoids the above issue with x and y since they stay in the outer graph. The dependencies are able to be successfully added due to my changes. However, there's the issue with nondeterminism I mentioned above (from what I've seen, the assigns would nondeterministically be run 9, 10, or 11 times  in the example above). \r\n\r\n In the spirit of matching Python, I can see that this is likely not a solution to move forward with. However, my understanding of control dependencies is that each time an op executes, its control dependencies are executed, so I'd still expect any operation that's a control input to _i + 1_ to be executed 10 times by the above loop. If you have any further knowledge of control dependencies that could explain where \u201ceach time an op executes, its control dependencies are executed\u201d falls short, that would be super helpful \u2014  I'd love to have a clearer understanding for reference on any future work involving control dependencies. \r\n", "This is a great explanation, thank you! My perspective has become too Python API-centric apparently...\r\n\r\nCan you share the nondeterministic GraphDef? I have some theories about what could be going on, but I think it'll be easier to explain if I can look at the GraphDef to make sure they make sense. \r\n\r\n", "If it's too big to easily post here, feel free to email it to me.", "Thanks @skye! Just sent you an email. ", "Hi Melissa, here's what I think is happening... since the Variables are entered into the loop with Enter(is_constant=True) nodes, they are available in _all frames_ of the while loop (a frame roughly corresponds to an iteration, and since is_constant=True, the Enter outputs are always available). A new frame is created every time the NextIteration node runs, meaning that on the final iteration, a new frame will be created after the final iteration ends, and then the loop will exit. I believe you're seeing 11 when that last frame, that isn't actually used, causes the Assign to run one extra time. The Python code creates a control dep on the Switch::true output to prevent the Assign from running on this last frame, since the Switch::false branch is taken when the loop exits.\r\n\r\nI'm less sure about how you're getting 9, but my guess is that the Assign is running multiple times in parallel (since nodes from different iteration frames can run in parallel as long as there's no pending dependencies), causing some of the updates to be lost. The control dep on the switch would fix this too, since it effectively causes the iterations to run in order (at least for the Assign).\r\n\r\nDoes this make sense? It barely makes sense to me :) I unfortunately don't have any clever solutions at the moment... I think we might need to change the C API while loop API, this is too hard to work with.", "Hi @skye,\r\n\r\nThanks so much for taking a look! Your explanation makes sense to me. :) \r\n\r\nAs for potential changes to the C API loop implementation, do you think making it take lambdas to build the cond and body in the outer graph instead of creating separate cond/body graphs would work? That way I think we could actually do things the Python way (reproduced below again) since x and y would be in the same graph as the loop body. \r\n\r\n```\r\nx = 0  \r\ny = 0\r\ni = 0 \r\nwhile (i < 10) {\r\n  assign x = x + 1\r\n  assign y = y + 1\r\n  i = i + 1 // takes assigns as control inputs\r\n}\r\n```\r\n\r\nOr were you envisioning other changes to the implementation?\r\n\r\nAlso, do you know who maintains the C API that we should consult on this? \r\n\r\nThanks again for all your help!!!", "That's one option. We could also revisit allowing the underlying Graph to allow inputs from Tensors outside the Graph. Like you pointed out, this could lead to a bunch of unexpected problems, but we could at least limit it to only be allowed in the TF_Graphs returned by TF_NewWhile, and the advantage would be avoiding changes to the C API. Lemme see if I can someone still working on TF to weigh in...", "cc @saxenasaurabh \r\n\r\nBased on some offline discussion, I think we're leaning towards adding the ability to \"capture\" Tensors in a TF_Graph. This is very similar to allowing inputs from other Graphs. This would be a step towards representing functions as graphs in the C API, which is probably a prerequisite for while_v2, so it'd be nice to move towards that final goal.", "Hi @skye,\r\n\r\nAfter thinking about it some more, I'm still confused about why the Assign runs in the final frame. In general, what triggers control dependencies to run? \r\n\r\nMy working assumption had been that dataflow determines the execution flow of the graph, so an op's control dependencies would only be triggered to run when its inputs flow in. \r\n\r\nIn the graph I sent you, the Assign was a control input to a constant in the loop body. In this case, the constant's inputs are obviously available (since there are none) and the Assign is also available, so I can see why the Assign might run in the final frame if it happens to be run before the constant's other control dependency on switch::true. (Side question: is my observation that an op's control dependencies are run in an unpredictable order correct?) \r\n\r\nHowever, I've also tried making the Assign a control input to an op which takes switch::true as an input, and I still see the off-by-one error in that case. There, I would have assumed that because the op's inputs never become available, the op's control dependencies would not be triggered. This must mean an op's control dependencies can be run before the op's inputs become available. \r\n\r\nIf not inputs flowing in, what triggers an op's control dependencies to run? Are all the control dependencies in a frame run once per frame, when it's created?\r\n", "The assigns run in the final frame (and every frame) because they are loop invariants due to the RefEnters having is_constant=true (since the Variables aren't loop variables). Specifically:\r\n\r\nAll constant enters are marked as loop invariants:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/executor.cc#L2132\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/executor.cc#L2779\r\nIn our case, these are the RefEnters for the Variables.\r\n\r\nThe constant enters are activated/run for every loop iteration frame:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/executor.cc#L2785 (existing frames)\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/executor.cc#L2820 (new frames added after the Enter runs for the first time)\r\n\r\nRunning the enters then flows through to any downstream ops, in our case to the Assigns. It doesn't matter to the Assigns that there's a control output to the Const, since an op running depends on only on its upstream inputs, no downstream outputs. So we need a control edge from the Switch::true to the Assigns, so the true branch is upstream of the Assigns. Does this make sense?", "Hi @skye, \r\n\r\nAhh, everything makes sense now. :) Thank you so much for all the time you've taken to write such thorough responses to all my questions! I really appreciate all your help. ", "No problem! Please let me know if you have more questions.\r\n\r\nWould you or someone on your team be willing to implement capturing in TF_Graphs (or some other solution)? Or wait until post-TF 2.0? The TF team is focused on TF 2.0 right now, so realistically there probably isn't internal bandwidth to fix this until after 2.0.", "Hi @skye,\r\n\r\nYes, I'd like to contribute whatever needed to get the while loop working! Is there anyone you'd recommend trying to pull into the discussion when I open the issue/feature request for it?", "Hi @melissagrueter \r\n\r\nFeel free to pull me into any discussions.\r\n\r\nThanks!", "Thanks @saxenasaurabh! Will do. "]}, {"number": 26370, "title": "Switch to use pip3", "body": "", "comments": []}, {"number": 26369, "title": "Update TF 2.0.0-alpha-0 release notes.", "body": "", "comments": []}, {"number": 26368, "title": "[TF 2.0] Add skip_empty argument to tf.strings.split so that tf.string_split can be deprecated.", "body": "There were two split functions defined in TensorFlow 1.13:\r\n\r\n1. [`tf.string_split`](https://www.tensorflow.org/api_docs/python/tf/string_split)\r\n2. [`string_split_v2`](https://github.com/tensorflow/tensorflow/pull/19650)\r\n\r\nWe have consolidated these two ops into [`tf.strings.split`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/strings/split) in TF 2.0:\r\n\r\n```\r\ntf.strings.split(\r\n    source,\r\n    sep=None,\r\n    maxsplit=-1\r\n)\r\n```\r\n and wish to deprecate `tf.string_split`. However, `tf.strings.split` is missing the `skip_empty` functionality included in `string_split`:\r\n\r\n```\r\ntf.string_split(\r\n    source,\r\n    delimiter=' ',\r\n    skip_empty=True\r\n)\r\n```\r\n\r\nThis feature request would be to **include `skip_empty` functionality for `tf.strings.split`**.", "comments": ["Ill take a quick look tonight at it", "So I have some code I am testing on my local fork, but an issue I am running against is that it uses the library gen_string_ops which is machine generated,  I am not quite sure how to approach this ", "> So I have some code I am testing on my local fork, but an issue I am running against is that it uses the library gen_string_ops which is machine generated, I am not quite sure how to approach this\r\n\r\nIt appears the function signature will need to be modified in the python section as well as the core/ops section and the skip_empty functionality will need to be implemented in the core/kernels section.\r\n\r\n@dynamicwebpaige, just to be clear, the skip_empty specifies that the function will not return empty strings in the list of resulting strings? Do you want to keep the 'predicate' form of the previous implementation?\r\n\r\nEdit: I believe I have completed the required edits and am building the project to test the changes.\r\n\r\nI noticed there are not many tests written in string_ops_test.cc, should there be a test case added for this behavior? I am not familiar with how the tests are written.\r\n\r\nEdit: Now I've noticed the tests are written in python. I'll add a test there.", "#26475 . I have added in the needed changes with the unit tests.  I am right now working on end to end tests(compiling takes some time on my machine) and am ensuring clean code standards.  I have opened up a PR for any feedback.", "i have added added skip_empty in string_split_v2 file and merged it accordingly", "> #26475 . I have added in the needed changes with the unit tests. I am right now working on end to end tests(compiling takes some time on my machine) and am ensuring clean code standards. I have opened up a PR for any feedback.\r\n\r\nIt's pretty rude to take me explaining what to do and opening a PR while I'm building the project. But whatever, glad it got done. Why did you change the default in python to False?\r\n\r\n> i have added added skip_empty in string_split_v2 file and merged it accordingly\r\n\r\n@shashvatshahi1998, Sorry, but the OP asked to \"include skip_empty functionality\" which requires more significant changes than modifying the Python function signature.\r\n", "I'm sorry if this appears rude, but I was also in the process of building the project as was previously stated and had work done as well.  \r\nWhen tf.strings.split is called elsewhere in the code, based on the other unit tests, it appears to expect the function to work as if the default value is false.  So I went with that approach.  Do you think it should be changed to handle a default of true?", "> When tf.strings.split is called elsewhere in the code, based on the other unit tests, it appears to expect the function to work as if the default value is false. So I went with that approach. Do you think it should be changed to handle a default of true?\r\n\r\nIt just seems as though the default value should be consistent in the Python and C code. The expected default will have to be decided by someone from the tensorflow team, but maybe they should all be False if that's how the new function is expected to behave.", "Ill update it shortly then", "Updated to default to true, Ill leave the question in the pr review as well though", "Quick update, the review requested we do this Post-processing. I am in the process of working on that and have asked him to review what I have so far for that type of solution.  Any thoughts?", "The review has requested this be done entirely in preprocessing and made a separate issue", "Design guidance (as of 06-03-2019) is that the `skip_empty` argument should be removed from `tf.strings.split` (filtering after the fact introduces difficulties). Closing out this issue as working as intended. Apologies for confusion, and thank you for working on this!"]}, {"number": 26367, "title": "Cherrypick build and test fixes for fixing the release builds for TF 2.0 alpha-0", "body": "", "comments": ["Yay, let's see."]}, {"number": 26366, "title": "How to plot train, test and validation line to evaluate your model whether overfit or underfit", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n# Copy and paste here\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 26365, "title": "Interpreter lifecycle Tensorflow Lite", "body": "**System information**\r\n- TensorFlow version: `org.tensorflow:tensorflow-lite:0.0.0-nightly`\r\n- Doc Link: there is no doc as far as I know\r\n\r\n**Describe the documentation issue**\r\nI was not able to find information about what is the proper life cycle for the [Intepreter](https://www.tensorflow.org/api_docs/python/tf/lite/Interpreter) class. I'm a contributor of this [Android library](https://github.com/the-super-toys/glimpse-android) which makes use of TensorFlow Lite. The problem is that I don't know where I should instantiate the Interpreter. If I do it with a global scope and reuse the instance over and over after a few calls  it throws the next exception:\r\n\r\n```\r\n--------- beginning of crash\r\n2019-03-05 01:15:23.444 31140-31169/glimpse.sample A/libc: Fatal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x6d36b8d51c in tid 31169 (glide-disk-cach), pid 31140 (glimpse.sample)\r\n2019-03-05 01:15:23.476 31140-31159/glimpse.sample E/libc: Access denied finding property \"vendor.debug.egl.swapinterval\"\r\n2019-03-05 01:15:23.513 31228-31228/? A/DEBUG: *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***\r\n2019-03-05 01:15:23.514 31228-31228/? A/DEBUG: Build fingerprint: 'OnePlus/OnePlus6/OnePlus6:9/PKQ1.180716.001/1901231401:user/release-keys'\r\n2019-03-05 01:15:23.514 31228-31228/? A/DEBUG: Revision: '0'\r\n2019-03-05 01:15:23.514 31228-31228/? A/DEBUG: ABI: 'arm64'\r\n2019-03-05 01:15:23.514 31228-31228/? A/DEBUG: pid: 31140, tid: 31169, name: glide-disk-cach  >>> glimpse.sample <<<\r\n2019-03-05 01:15:23.514 31228-31228/? A/DEBUG: signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x6d36b8d51c\r\n2019-03-05 01:15:23.514 31228-31228/? A/DEBUG:     x0  0000007e068702c0  x1  0000000000000000  x2  0000000000000010  x3  0000000000000004\r\n2019-03-05 01:15:23.514 31228-31228/? A/DEBUG:     x4  0000007e068702d0  x5  0000007e06870300  x6  0000000000000002  x7  0000007e068702c0\r\n2019-03-05 01:15:23.514 31228-31228/? A/DEBUG:     x8  0000000000000002  x9  0000000000000004  x10 0000000000000003  x11 fffff80dc78c320f\r\n2019-03-05 01:15:23.514 31228-31228/? A/DEBUG:     x12 ffffffef3031d1dc  x13 0000000000000004  x14 0000000000000001  x15 0000000000000003\r\n2019-03-05 01:15:23.514 31228-31228/? A/DEBUG:     x16 0000007e077e6280  x17 0000007ea829de70  x18 0000000000000010  x19 0000007e068702c0\r\n2019-03-05 01:15:23.514 31228-31228/? A/DEBUG:     x20 0000000000000004  x21 0000007e091f28c4  x22 0000007e068689c0  x23 0000000000000002\r\n2019-03-05 01:15:23.514 31228-31228/? A/DEBUG:     x24 0000007e06870300  x25 0000007e06870340  x26 0000000000000060  x27 0000000000000060\r\n2019-03-05 01:15:23.514 31228-31228/? A/DEBUG:     x28 0000007e091e8500  x29 0000007e08912ae0\r\n2019-03-05 01:15:23.514 31228-31228/? A/DEBUG:     sp  0000007e08912aa0  lr  0000007e07703254  pc  0000007e0770335c\r\n2019-03-05 01:15:23.518 31140-31159/glimpse.sample E/libc: Access denied finding property \"vendor.debug.egl.swapinterval\"\r\n2019-03-05 01:15:23.533 31140-31159/glimpse.sample E/libc: Access denied finding property \"vendor.debug.egl.swapinterval\"\r\n2019-03-05 01:15:23.557 31228-31228/? A/DEBUG: backtrace:\r\n2019-03-05 01:15:23.557 31228-31228/? A/DEBUG:     #00 pc 00000000000d035c  /data/app/glimpse.sample-Jd0f0_ct3CDvNgh38yERpA==/lib/arm64/libtensorflowlite_jni.so\r\n2019-03-05 01:15:23.557 31228-31228/? A/DEBUG:     #01 pc 00000000000cfb4c  /data/app/glimpse.sample-Jd0f0_ct3CDvNgh38yERpA==/lib/arm64/libtensorflowlite_jni.so\r\n2019-03-05 01:15:23.557 31228-31228/? A/DEBUG:     #02 pc 00000000000ccc54  /data/app/glimpse.sample-Jd0f0_ct3CDvNgh38yERpA==/lib/arm64/libtensorflowlite_jni.so\r\n2019-03-05 01:15:23.557 31228-31228/? A/DEBUG:     #03 pc 000000000011a38c  /data/app/glimpse.sample-Jd0f0_ct3CDvNgh38yERpA==/lib/arm64/libtensorflowlite_jni.so\r\n2019-03-05 01:15:23.557 31228-31228/? A/DEBUG:     #04 pc 000000000011cd98  /data/app/glimpse.sample-Jd0f0_ct3CDvNgh38yERpA==/lib/arm64/libtensorflowlite_jni.so\r\n2019-03-05 01:15:23.557 31228-31228/? A/DEBUG:     #05 pc 0000000000010f60  /data/app/glimpse.sample-Jd0f0_ct3CDvNgh38yERpA==/lib/arm64/libtensorflowlite_jni.so (Java_org_tensorflow_lite_NativeInterpreterWrapper_run+32)\r\n2019-03-05 01:15:23.557 31228-31228/? A/DEBUG:     #06 pc 000000000055e1e0  /system/lib64/libart.so (art_quick_generic_jni_trampoline+144)\r\n2019-03-05 01:15:23.557 31228-31228/? A/DEBUG:     #07 pc 000000000055544c  /system/lib64/libart.so (art_quick_invoke_static_stub+604)\r\n2019-03-05 01:15:23.557 31228-31228/? A/DEBUG:     #08 pc 00000000000cf6e8  /system/lib64/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+232)\r\n2019-03-05 01:15:23.557 31228-31228/? A/DEBUG:     #09 pc 000000000027f2b8  /system/lib64/libart.so (art::interpreter::ArtInterpreterToCompiledCodeBridge(art::Thread*, art::ArtMethod*, art::ShadowFrame*, unsigned short, art::JValue*)+344)\r\n2019-03-05 01:15:23.557 31228-31228/? A/DEBUG:     #10 pc 00000000002792c0  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+968)\r\n2019-03-05 01:15:23.557 31228-31228/? A/DEBUG:     #11 pc 0000000000526498  /system/lib64/libart.so (MterpInvokeStatic+204)\r\n2019-03-05 01:15:23.557 31228-31228/? A/DEBUG:     #12 pc 0000000000547a14  /system/lib64/libart.so (ExecuteMterpImpl+14612)\r\n2019-03-05 01:15:23.557 31228-31228/? A/DEBUG:     #13 pc 000000000026abe8  /dev/ashmem/dalvik-classes2.dex extracted in memory from /data/app/glimpse.sample-Jd0f0_ct3CDvNgh38yERpA==/base.apk!classes2.dex (deleted) (org.tensorflow.lite.NativeInterpreterWrapper.run+164)\r\n2019-03-05 01:15:23.557 31228-31228/? A/DEBUG:     #14 pc 0000000000252fc4  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.99565114+488)\r\n2019-03-05 01:15:23.557 31228-31228/? A/DEBUG:     #15 pc 0000000000258ab8  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)\r\n2019-03-05 01:15:23.557 31228-31228/? A/DEBUG:     #16 pc 00000000002792a4  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+940)\r\n2019-03-05 01:15:23.557 31228-31228/? A/DEBUG:     #17 pc 0000000000524f94  /system/lib64/libart.so (MterpInvokeVirtual+588)\r\n2019-03-05 01:15:23.557 31228-31228/? A/DEBUG:     #18 pc 0000000000547894  /system/lib64/libart.so (ExecuteMterpImpl+14228)...\r\n```\r\n\r\nTo provide more context, the `interpreter` is used from a background thread, specifically from a [Glide Transformation](https://bumptech.github.io/glide/doc/transformations.html) component. So, at this point, I don't know if the problem is due to a wrong lifecycle handling of the interpreter instance or rather something related to threading. \r\n\r\nBut beyond my particular case I wanted to ask for some guidance about the proper lifecycle handling for the Interpreter class.\r\n\r\nThanks.", "comments": ["So, the Interpreter should remain valid until you call close() on it. However, it is *not* thread-safe, so you'll need make sure any access is properly serialized. One question, are you using the GPU delegate accelerator? Or just regular CPU inference? ", "Thanks @jdduke for th clarification. We managed to call `Interpreter::run` thread safe and the issue is fixed.\r\n\r\n> are you using the GPU delegate accelerator? Or just regular CPU inference?\r\n\r\nJust regular CPU. But I think we tried at some point with the GPU delegate accelerator and we found a worse performance."]}, {"number": 26364, "title": "Win 10 Import Error: Failed to load the native TensorFlow runtime TF-GPU(both1.13 and 2.0) ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): tensorflow-gpu-1.13.1\r\n- Python version: 3.6.4\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: CUDA 10.1/cuDNN7.5\r\n- GPU model and memory: 930mx 2GB\r\n\r\n**Describe the current behavior**  \r\nFails to run because of an Import Error whenever I try to import tf-GPU,\r\n**Describe the expected behavior**  \r\nExpected it to import so I could use it.\r\n**Code to reproduce the issue**  \r\nimport tensorflow as tf\r\n\r\n**Other info / logs**  \r\n---------------------------------------------------------------------------\r\n```\r\nImportError                               Traceback (most recent call last)\r\nc:\\users\\ent\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59   from tensorflow.python.pywrap_tensorflow_internal import __version__\r\n\r\nc:\\users\\ent\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in <module>\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow_internal = swig_import_helper()\r\n     29     del swig_import_helper\r\n\r\nc:\\users\\ent\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     23             try:\r\n---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n     25             finally:\r\n\r\nc:\\users\\ent\\appdata\\local\\programs\\python\\python36\\lib\\imp.py in load_module(name, file, filename, details)\r\n    242         else:\r\n--> 243             return load_dynamic(name, filename, file)\r\n    244     elif type_ == PKG_DIRECTORY:\r\n\r\nc:\\users\\ent\\appdata\\local\\programs\\python\\python36\\lib\\imp.py in load_dynamic(name, path, file)\r\n    342             name=name, loader=loader, origin=path)\r\n--> 343         return _load(spec)\r\n    344 \r\n\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-602d2050546b> in <module>\r\n----> 1 import tensorflow as tf\r\n      2 import time\r\n      3 import os\r\n      4 from tensorflow.examples.tutorials.mnist import input_data\r\n      5 \r\n\r\nc:\\users\\ent\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\__init__.py in <module>\r\n     22 \r\n     23 # pylint: disable=g-bad-import-order\r\n---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n     25 \r\n     26 from tensorflow._api.v1 import app\r\n\r\nc:\\users\\ent\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>\r\n     47 import numpy as np\r\n     48 \r\n---> 49 from tensorflow.python import pywrap_tensorflow\r\n     50 \r\n     51 # Protocol buffers\r\n\r\nc:\\users\\ent\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     72 for some common reasons and solutions.  Include the entire stack trace\r\n     73 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 74   raise ImportError(msg)\r\n     75 \r\n     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"c:\\users\\ent\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"c:\\users\\ent\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"c:\\users\\ent\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"c:\\users\\ent\\appdata\\local\\programs\\python\\python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"c:\\users\\ent\\appdata\\local\\programs\\python\\python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.  \r\n```\r\n---------------------------------------------------------------------------", "comments": ["+1 with almost exactly same specs. GPU is a 1050 Ti and Python 3.7.2, and only TF 1.13.1", "Same is occurring for me. Similar specs: 1050Ti, Python 3.6.8, tf-nightly-gpu, same CUDA/CuDNN versions. I can also confirm there are no path issues with the CUDA components. Downgrading CUDA is not an option for me, as the 9.0 installers break my PhysX driver and Visual Studio integrations.", "> CUDA/cuDNN version: CUDA 10.1/cuDNN7.5\r\n\r\nYou have to use CUDA 10.0 for tensorflow-gpu-1.13.1 (don't forget to download cuDNN 7.5 for CUDA 10.0 too).\r\n", "I think you should use CUDA10.0 instead of CUDA10.1.The tensorflow-gpu compiled by pip install specifies the cuda version, which needs to be accurate to the specific subversion.In addition, according to my test, NVIDIA GPU without Tensor Core and FP16 support will not achieve higher performance than CUDA9 with CUDA10.", "Confirmed to be working using CUDA 10.0 and CuDNN 7.5 for 10.0.", "Thanks for the information. Will be downloading the cudnn and cuda toolkit today. Hopefully I can run them and close this issue.", "> Confirmed to be working using CUDA 10.0 and CuDNN 7.5 for 10.0.\r\n\r\nHow did you get it to work? 'mine failed yesterday. It just failed with CUDA 9.0 Too. I am beginning to feel like giving up to be honest\r\n", "@Pure-Entropy  and which version of Tensorflow did you try with CUDA 9.0? Because the 1.13 version does not work with CUDA 9.0 afaik.", "@Pure-Entropy Please use cuda 10.0 against TF 1.13.1 and in addition to that make sure you add cuda, cudnn paths to your environment. Please take a look at similar [issue](https://github.com/tensorflow/tensorflow/issues/26059)", "> @Pure-Entropy Please use cuda 10.0 against TF 1.13.1 and in addition to that make sure you add cuda, cudnn paths to your environment. Please take a look at similar [issue](https://github.com/tensorflow/tensorflow/issues/26059)\r\n\r\nI honestly do not know where I am going wrong here. I just reinstalled CUDA 10.0 and cuDNN 10.0 the other day. I am just from copying the path variables EXACTLY as they are on the install page. \r\n\r\nthe reinstalled TF13.1 still refuses to work. PyTorch manages to recognise my GPU and work without any setup whatsoever, I got it to work just as soon as I installed CUDA and cuDNN even without the path variable modifications. It is the TensorFlows that keep refusing to work. \r\n[This link](https://imgur.com/a/US9LJjj) shows my environment's PATH variable. this is the system PATH.\r\n\r\nEven 2.0 alpha is failing. Paige said TF does not support GPU compute on windows. I am beginning to think that is true", "> @Pure-Entropy and which version of Tensorflow did you try with CUDA 9.0? Because the 1.13 version does not work with CUDA 9.0 afaik.\r\n\r\nit was both 1.13 and 2.0.", "1) There is no cuDNN 10.0. The newest CuDNN version is 7.5. \r\nThe tensorflow 1.13.1 works on Windows with GPU support, I have it myself. \r\n\r\n2) Maybe a bit silly question, but are you using 64bit python? Because the version (32/64bit) of DLL must match the python version. Otherwise it complains about missing DLL even when it's present.\r\n", "You need to add sub folders: ```bin``` , ```include``` and ```lib``` to the path. I see that ```lib and include``` folder's are not added to the path. Can you please try adding them as well?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "> Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n\r\nNo need to reopen. The issue was solved by adding `include` to the path. Thank you", "> Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n\r\nI updated the docs and did a PR for the updated fork. the instructions did not tell us to add `C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\include` to the PATH variable.\r\n\r\nPlease review it.", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)\r\n", "where should i add the bin, include lib? or how should i ?", "> where should i add the bin, include lib? or how should i ?\r\nnever mind, just had to move the Cudnn files into cuda,\r\nhere is the [link](https://medium.com/@akshaysin_86681/installing-cuda-and-cudnn-on-windows-10-f735585159f7) just in case anyone needs it in the future\r\n\r\nthanks ymodak", "> I updated the docs and did a PR for the updated fork. the instructions did not tell us to add `C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\include` to the PATH variable.\r\n> Please review it.\r\n\r\nThe PR in question is https://github.com/tensorflow/docs/pull/419", "I'd like to confirm that it works on cuda 10.0, not on 10.1.\r\nI installed 2 things:\r\n- cuda_10.0.130_win10_network.exe\r\n- cudnn-10.0-windows10-x64-v7.6.0.64.zip"]}, {"number": 26363, "title": "No gen_bigquery_reader_ops", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 1.12.0\r\n- Python version: 3.5\r\n- Installed using: pip\r\n- CUDA/cuDNN version: 9.0\r\n- GPU model and memory: Jetson TX2\r\n\r\n\r\n\r\nI can't run uff conversion.\r\nThe traceback is:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/tegra/AIC_tf_to_trt_image_classification/scripts/frozen_graphs_to_uffs.py\", line 9, in <module>\r\n    from model_meta import NETS, FROZEN_GRAPHS_DIR, CHECKPOINT_DIR, UFF_DIR\r\n  File \"/tegra/AIC_tf_to_trt_image_classification/scripts/model_meta.py\", line 10, in <module>\r\n    import tensorflow.contrib.slim as slim\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/__init__.py\", line 38, in <module>\r\n    from tensorflow.contrib import cloud\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/cloud/__init__.py\", line 24, in <module>\r\n    from tensorflow.contrib.cloud.python.ops.bigquery_reader_ops import *\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/cloud/python/ops/bigquery_reader_ops.py\", line 21, in <module>\r\n    from tensorflow.contrib.cloud.python.ops import gen_bigquery_reader_ops\r\nImportError: cannot import name 'gen_bigquery_reader_ops'\r\n```\r\nThere's no such file `gen_bigquery_reader_ops.py` at this location.", "comments": ["@yarons Could you provide more details on the problem and its context? If possible, please provide a code to reproduce the issue that makes it faster to find rootcause of the issue. Thanks!", "@jvishnuvardhan\r\nhttps://github.com/tensorflow/tensorflow/blob/afab5b322f780c235c61038b49593e34b523d400/tensorflow/contrib/cloud/python/ops/bigquery_reader_ops.py#L21\r\n\r\nThis line is referencing this library but I couldn't find it anywhere.", "@yarons This is fixed in Tensorflow 1.14.0. Would you like to upgrade your Tensorflow version and try. Thanks!", "I will try that again, thank you :)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26363\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26363\">No</a>\n"]}, {"number": 26362, "title": "How can I do distributed training using Parameter Server Strategy with estimators? How to define Cluster Spec without using TF_CONFIG  for Estimators?", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": ["@yashwantptl7 This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!"]}, {"number": 26361, "title": "LIte: Conv missing test case added", "body": "Missing test case added for Conv.", "comments": ["@jdduke : Would you please help conclude on this PR, TIA!", "@jdduke: Would you please help conclude this PR, thanks!", "@jdduke: Gentle Reminder!", "@jdduke: Thanks for your comments. All are handled now, please check and approve, Thanks!", "@jdduke : Your comment is handled now, please check , Thanks!", "@jdduke : Gentle Reminder!", "@jdduke : All your comments are handled, please check, Thanks!", "@jdduke : The conflict is resolved now, please check and approve the changes, Thanks!", "@jdduke : Gentle Reminder!"]}, {"number": 26360, "title": "extend_with_weight_decay function doesn't exist? ", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: 1.12\r\n- Doc Link: \r\nhttps://www.tensorflow.org/versions/r1.12/api_docs/python/tf/contrib/opt/AdamWOptimizer\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/opt/python/training/weight_decay_optimizers.py\r\n\r\n\r\n**Describe the documentation issue**\r\nIn `MomentumWOptimizer`, `AdamWOptimizer` and `DecoupledWeightDecayExtension` the documentation refers to `extend_with_weight_decay`:\r\n\r\n```python\r\n  extend_with_weight_decay(tf.train.MomentumOptimizer,\r\n                           weight_decay=weight_decay)\r\n  ```\r\nAFAIK there is no \"`extend_with_weight_decay`\", only \"`extend_with_decoupled_weight_decay`\", which seems to serve a similar purpose:\r\n\r\n```python\r\nMyAdamW = extend_with_decoupled_weight_decay(tf.train.AdamOptimizer)\r\n  # Create a MyAdamW object\r\n  optimizer = MyAdamW(weight_decay=0.001, learning_rate=0.001)\r\n```\r\nIs my understanding correct?\r\nI could submit a PR if necessary\r\n", "comments": ["You are right, it should be extend_with_decoupled_weight_decay. \r\n\r\nWould be great if you can create a PR to fix this issue. Thanks!", "Pull request created #26433 ", "@miktoki Thanks for finding this issue! I'm currently moving these optimizers into addons (tensorflow/addons#164), I've fixed documentation there. Thanks again!", "Close the issue since it is resolved.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26360\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26360\">No</a>\n"]}, {"number": 26359, "title": "RAM memory issue in Xavier ", "body": "Hi,\r\nI am currently working on an application which runs ssd mobilenet based Object detector in Nvidia AGX Xavier Developer kit, which is flashed with Jetpack 4.1. \r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): LINUX UBUNTU 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NO\r\n- TensorFlow installed from (source or binary): pip install --pre --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v411 tensorflow-gpu\r\n- TensorFlow version (use command below): 1.13.0\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): \r\n- CUDA/cuDNN version: CUDA 10.0/cuDNN 7.3.1.20\r\n- GPU model and memory:Xavier developer kit\r\n\r\nThe sofware specifications are as follows, \r\nUbuntu 18.04\r\ntf = 1.13.0 from pip install --pre --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v411 tensorflow-gpu\r\nCUDA 10.0\r\nCUDNN 7.3.1.20\r\n\r\nI have noticed that the algorithm is taking around 12GB out of 15.5GB RAM memory available in Xavier.\r\n\r\nI have made use of \"config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=log_device)\" and \"config.gpu_options.allow_growth=allow_memory_growth\" in the code to control the memory growth. But still I could not see any improvement in the memory allocation.\r\n\r\nI have uninstalled tensorflow and installed it again too. But there is no much change.\r\n\r\nThe same algorithm allocates only about 2-3GB is GPU based Laptop.\r\n\r\nCould you help me to make a conclusion on this issue. Are there any more changes to be done specific to xavier for using the split model algorithm you have created ? Also can the split model code be used for mutilple cameras ? \r\n\r\nThanks in Advance !!!\r\n\r\n\r\nRegards,\r\nNiran", "comments": ["@nluehr Can you please help triage this?", "Yeah the issue might be, that you used (like i did ) Tensorflow and Pytorch together.\r\nIf it uses to much RAM you have to limit it manually, to do this, just add those following lines for TF2(TF1 uses a different Syntax)\r\n\r\nimport tensorflow as tf\r\ngpus = tf.config.experimental.list_physical_devices('GPU') tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=CUDARAMINMB)])\r\n\r\nCUDARAMINMB Is the CUDA RAM Limit in MB. The two lines above can be placed after the import of Tensorflow.\r\n\r\nAlso if you use Pytorch and Tensorflow together, tensorflow MUST be imported after Pytorch, otherwise it will fail with some cryptical block error.\r\n\r\nFor TF1 use those lines to change the memory fraction as you want\r\n\r\nimport tensorflow as tf\r\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.3)\r\nsess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\r\n", "@Niran89,\r\n\r\nCan you take a look at the above comment and let us know if it helps in resolving your issue.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26359\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26359\">No</a>\n"]}, {"number": 26358, "title": "Replace add_variable() with add_weight()", "body": "Changes in the rnn_cell.py file", "comments": ["Please add more context to the description. Why is this replacement being done?", "@jdduke the add_variable method here ends up calling add_weight. Kindly see the below link.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/4ca24c6e7df49881ae5770c331a29778a43d3c4c/tensorflow/python/keras/_impl/keras/engine/base_layer.py#L438\r\n", "I see, over to Renjie for further review."]}, {"number": 26357, "title": "fatal error: tensorflow/cc/ops/array_ops.h", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nI use c++ API\r\n \r\n**Describe the problem**\r\nfatal error: tensorflow/cc/ops/array_ops.h\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\ncant find array_ops.h\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\ntensorflow/tensorflow/cc/ops/standard_ops.h:19:41: fatal error: tensorflow/cc/ops/array_ops.h:", "comments": ["Could you give us more detail? How you run it? What is the error message?", "\r\n\r\n\r\n\r\n> Could you give us more detail? How you run it? What is the error message?\r\n\r\n#include <fstream>\r\n#include <utility>\r\n#include <vector>\r\n#include <Eigen/Core>\r\n#include <Eigen/Dense>\r\n\r\n#include \"tensorflow/cc/ops/const_op.h\"\r\n#include \"tensorflow/contrib/image/kernels/image_ops.h\"\r\n#include \"tensorflow/cc/ops/standard_ops.h\"\r\n#include \"tensorflow/core/framework/graph.pb.h\"\r\n#include \"tensorflow/core/framework/tensor.h\"\r\n#include \"tensorflow/core/graph/default_device.h\"\r\n#include \"tensorflow/core/graph/graph_def_builder.h\"\r\n#include \"tensorflow/core/lib/core/errors.h\"\r\n#include \"tensorflow/core/lib/core/stringpiece.h\"\r\n#include \"tensorflow/core/lib/core/threadpool.h\"\r\n#include \"tensorflow/core/lib/io/path.h\"\r\n#include \"tensorflow/core/lib/strings/stringprintf.h\"\r\n#include \"tensorflow/core/platform/env.h\"\r\n#include \"tensorflow/core/platform/init_main.h\"\r\n#include \"tensorflow/core/platform/logging.h\"\r\n#include \"tensorflow/core/platform/types.h\"\r\n#include \"tensorflow/core/public/session.h\"\r\n#include \"tensorflow/core/util/command_line_flags.h\"\r\n\r\nusing namespace std;\r\nusing namespace tensorflow;\r\nusing namespace tensorflow::ops;\r\nusing tensorflow::Flag;\r\nusing tensorflow::Tensor;\r\nusing tensorflow::Status;\r\nusing tensorflow::string;\r\nusing tensorflow::int32;\r\n\r\nerror:\r\n\r\nIn file included from /home/j/tf_test/minist/tensorflow-c-mnist/tf.cpp:11:0:\r\n/home/tensorflow/tensorflow/cc/ops/standard_ops.h:19:41: fatal error: tensorflow/cc/ops/array_ops.h: \r\ncompilation terminated.\r\nCMakeFiles/tf_test.dir/build.make:62: recipe for target 'CMakeFiles/tf_test.dir/tf.cpp.o' failed\r\nmake[2]: *** [CMakeFiles/tf_test.dir/tf.cpp.o] Error 1\r\nCMakeFiles/Makefile2:67: recipe for target 'CMakeFiles/tf_test.dir/all' failed\r\nmake[1]: *** [CMakeFiles/tf_test.dir/all] Error 2\r\nMakefile:83: recipe for target 'all' failed\r\nmake: *** [all] Error 2\r\n\r\nrepo:https://github.com/zhangcliff/tensorflow-c-mnist", "@allendred Is this still an issue ? Please make sure you're using TF v2.4 or later as TF v1.x is not actively supported. Please let us know if the issue still persists in newer versions .Please open a new issue in case you face any errors using TF v2, we will get you the right help.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26357\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26357\">No</a>\n"]}, {"number": 26356, "title": "Low GPU usage for inference when multithreaded vs multiprocess c++", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7 64-bit\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): \r\n- GCC/Compiler version (if compiling from source): vs2015\r\n- CUDA/cuDNN version: 10.0 / 7.5\r\n- GPU model and memory: RTX 2060 6GB\r\n\r\n\r\n**Describe the current behavior**\r\nI'm using Python to build a graph (using tf.keras.layers) that contains cudnnlstm, timedistributed, dense layers. It crashes the RTX 2060 (there's a bug report for that already, this is not this issue) if I don't set options allow_growth  or limit ram usage. \r\nI'm using the model for inference from C++, my batch size is known as soon as the program starts but it can vary each time it's started so the graph doesn't have a fixed input size. I'm creating several threads with one Session on each. Tensorflow GPU usage is low (38%) and it eventually gobbles up all gpu ram available if it's not manually limited, I can't simply increase the batch sizes, because different weights are loaded on each Run().\r\nNow, the issue here is that it doesn't matter much how many parallel sessions I run in multiple threads, GPU utilization is still low, **_but_** if I limit gpu RAM usage so that I can run two separate processes (not threads), then they can both use about half the ram and increase GPU utilization to 80%. Why can't Tensorflow figure out a way to do that in just one process with multiple threads?  It's the same inputs and results, just multi-threaded vs multi-process.\r\n\r\n\r\n**Describe the expected behavior**\r\nTF should utilize at least 80%+ GPU in my case since I'm just running inference sessions in parallel in one process, instead of having to resort to limiting ram usage and starting several processes.\r\n\r\n**Code to reproduce the issue**\r\nToo much code to extract to make a barebones example, but I could do it if absolutely necessary.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@alvaroslm can you confirm that when two processes are used, you are actually getting double the inference examples/sec? Measuring GPU utilization is not always accurate, and perhaps the increased utilization comes from extra overhead of running two processes.", "Closing due to lack of activity.  @alvaroslm please feel free to reopen with a reply to [Reed's question above](https://github.com/tensorflow/tensorflow/issues/26356#issuecomment-472224051).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26356\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26356\">No</a>\n"]}, {"number": 26355, "title": "running the tflite model with a video input?", "body": "I want to give the  tflite object detection model an recorded video input instead of an live streaming video\r\nis that possible?", "comments": ["This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "> I want to give the tflite object detection model an recorded video input instead of an live streaming video\r\n> is that possible?\r\n\r\n@aravindchaluvadi Hi,  I want to do this recently, do you have the python script now ? If have, can you share with me ? Thanks !"]}, {"number": 26354, "title": "Wrong outputs of \"tensorflow.python.ops.distributions.util.fill_triangular_inverse\"", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 14.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.6\r\n\r\n\r\n**Describe the current behavior**\r\ntensorflow.python.ops.distributions.util.fill_triangular_inverse didn't give the right outputs.\r\n\r\n**Code to reproduce the issue**\r\nimport tensorflow as tf\r\nfrom tensorflow.python.ops.distributions.util import fill_triangular_inverse\r\nimport numpy as np\r\n\r\n\r\nsess = tf.InteractiveSession()\r\na = np.arange(1, 17).reshape((1, 4, 4))\r\n\"\"\"\r\na = [[[ 1  2  3  4]\r\n  [ 5  6  7  8]\r\n  [ 9 10 11 12]\r\n  [13 14 15 16]]]\r\n\"\"\"\r\n\r\nT = fill_triangular_inverse(a, upper=True)\r\nt = sess.run(T)\r\n\r\n\"\"\"\r\nt = [ 1  2  3  4 21 21 21 21 21 21]]\r\n\"\"\"\r\n", "comments": ["Why was this closed, it still doesn't work? I don't understand this output\r\n", "> Why was this closed, it still doesn't work? I don't understand this output\r\n\r\nIt doesn't work the way we think. The function only works for the lower-triangular or upper-triangular matrix with the rest part equaling zero, which is also explained in its doc. \r\n\r\n  Example:\r\n  ```python\r\n  fill_triangular_inverse(\r\n    [[4, 0, 0],\r\n     [6, 5, 0],\r\n     [3, 2, 1]])\r\n  # ==> [1, 2, 3, 4, 5, 6]\r\n  fill_triangular_inverse(\r\n    [[1, 2, 3],\r\n     [0, 5, 6],\r\n     [0, 0, 4]], upper=True)\r\n  # ==> [1, 2, 3, 4, 5, 6]\r\n  ````"]}, {"number": 26353, "title": "[TF 2.0] Provide a tool to convert checkpoints for optimizers, from TF 1.x to TF 2.0.", "body": "[Checkpoints will break with RNNs](https://github.com/tensorflow/tensorflow/issues/26350) and shared embedding columns, which is unfortunate but acceptable. For optimizers, though, [checkpoint breaking is a bit more extensive](https://github.com/tensorflow/tensorflow/issues/26349). \r\n\r\nIdeally this feature request would result in a function or tool that could **take the old checkpoint and some information about the old + new optimizers**, and **replace the old checkpoint with the new**. This should involve some variable renaming, and some new variables added in. (Hyperparameters are now variables, so we will need to add those in to the checkpoints, and set with reasonable values-- either the defaults, or with user-specified values.)\r\n\r\n**Priority**:\r\nP0 - optimizers\r\nP1 - RNNs\r\nP1 - shared embeddings", "comments": ["I'm looking into this, seems reasonably straightforward.", "I am also happy to help, @dynamicwebpaige please lemme know if this issue is still open", "This Issue Looks Interesting. Anyone working on this issue? Would love to work on this.\ud83d\ude04 ", "I have been busier than I imagined, so I've just done some research and haven't started working on it. If the people above that are interested want to start outlining what such a tool would look like I'm happy to collaborate on a solution. Alternatively, I won't be offended if someone just does the whole thing. I'll continue to post as I make progress.", "This issue is still up for grabs, and we'd appreciate the help! Supporting information for building the conversion mapping can be found in the two referenced issues (#26350, #26349).", "@dynamicwebpaige am i in a right direction.\r\nwe are supposed to build a function model.load_weight(convert('old_checkpoint_path'))\r\nthat convert function should change the checkpoint data format into the format supported by tf.keras.optimizer.Adam. as tf.train.AdamOptimizer is deprecated in tf2.0.\r\n```checkpoint_dir = os.path.dirname(checkpoint_path)\r\n\r\ncp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,verbose=1)\r\n\r\nmodel = create_model(tf.train.AdamOptimizer,0.01) // model trained using tf.train.AdamOptimizer\r\n\r\n\r\nmodel.fit(train_images, train_labels,  epochs = 10, \r\n          validation_data = (test_images,test_labels),\r\n          callbacks = [cp_callback]) // model checkpoint saved\r\n\r\nloss, acc = model.evaluate(test_images, test_labels)\r\nprint(\"Trained model, accuracy: {:5.2f}%\".format(100*acc))\r\n\r\nmodel = create_model(tf.keras.optimizers.Adam,0.01)\r\n\r\nmodel.load_weights(checkpoint_path) // will cause problem as the format used to save weights by tf.keras.optimizer and tf.train.AdamOptimizer are different\r\n\r\nloss, acc = model.evaluate(test_images, test_labels)\r\nprint(\"Untrained model, accuracy: {:5.2f}%\".format(100*acc))```\r\n\r\n\r\n", "for the P0 item, A checkpoint converter tool for converting optimizers is included with the 2.0.0-beta0 release. \r\n\r\nThis checkpoint converter tool is mainly for Canned Estimators, including DNN\r\nLinear and DNNLinearCombined estimators. The allowed optimizers to be converted\r\ninclude Adam, Adagrad, Ftrl, RMSProp, and SGD.\r\n\r\nNote that, this converter is not suitable for the case where 'dnn_optimizer'\r\nand 'linear_optimizer' in DNNLinearCombined model are the same.\r\n\r\nIf your current canned estimators and checkpoints are from TF 1.x, after you\r\nmigrate the canned estimator to v2 with `tf.keras.optimizers.*`, the converted\r\ncheckpoint allow you to restore and retrain the model in TF 2.0.\r\n\r\n", "I think this was resolved already with the detailed guide on migrating checkpoints from TF1.x to 2.x.\r\n[Here](https://www.tensorflow.org/guide/migrate/migrating_checkpoints) is the link to the guide. \r\n\r\nI am closing this issue as this was already resolved. Please feel free to reopen if I am mistaken. Thanks!"]}, {"number": 26352, "title": "win10vc2017build failed", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (win10):\r\n- TensorFlow installed from (source or binary): source r1.12\r\n- TensorFlow version: 1.12\r\n- Python version: python 3.7 (anaconda)\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 0.22\r\n- GCC/Compiler version (if compiling from source): vc 2017\r\n- CUDA/cuDNN version:cuda:10.0/cudnn:7.5\r\n- GPU model and memory: Nvidia Geforce 840M\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\npython ./configure.py\r\n...\r\nbazel build -c opt --copt=/arch:AVX2  --config=cuda  --copt=-nvcc_options=disable-warnings //tensorflow/tools/pip_package:build_pip_package  --local_resources 2048,.5,1.0\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nexternal/com_google_absl\\absl/meta/type_traits.h(338): error: static assertion failed with \"Not compliant with std::is_trivially_copy_assignable; Standard: true, Implementation: false\"\r\n          detected during:\r\n            instantiation of class \"absl::is_trivially_copy_assignable<T> [with T=Eigen::Index]\"\r\nexternal/com_google_absl\\absl/types/optional.h(267): here\r\n            processing of template argument list for \"absl::optional_internal::optional_data\" based on template argument <tensorflow::int64>\r\nexternal/com_google_absl\\absl/types/optional.h(485): here\r\n            instantiation of class \"absl::optional<T> [with T=tensorflow::int64]\"\r\n.\\tensorflow/core/framework/allocator.h(68): here\r\n\r\n1 error detected in the compilation of \"D:/Temp/nvcc_inter_files_tmp_dir/dense_update_functor_gpu.cu.cpp1.ii\".\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 7092.230s, Critical Path: 846.02s\r\nINFO: 1705 processes: 1705 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\nbut std::is_trivially_copy_assignable is ok .\r\ni build the below source \r\n\r\n#include <iostream>\r\n\r\n#include <iostream>\r\n#include <utility>\r\n#include <type_traits>\r\nstruct Foo { int n; };\r\n\r\nint main()\r\n{\r\n    std::cout << \"Hello World!\\n\"; \r\n\tstd::cout << std::boolalpha\r\n\t\t<< \"Foo is trivially copy-assignable? \"\r\n\t\t<< std::is_trivially_copy_assignable<Foo>::value << '\\n'\r\n\t\t<< \"int[2] is copy-assignable? \"\r\n\t\t<< std::is_copy_assignable<int[2]>::value << '\\n'\r\n\t\t<< \"int is nothrow copy-assignable? \"\r\n\t\t<< std::is_nothrow_copy_assignable<int>::value << '\\n';\r\n}\r\n\r\noutpub the below \r\nHello World!\r\nFoo is trivially copy-assignable? true\r\nint[2] is copy-assignable? false\r\nint is nothrow copy-assignable? true\r\n\r\n", "comments": ["I tried vs 2017 too at first and I had to switch to 2015, which works ok. If I remember correctly nvcc was to blame. That was for TF 1.10, I'm now using 1.13 and vs 2015 still works ok.", "@kerwinxu Please uninstall python and tensorflow and reinstall freshly using the instructions [here](https://github.com/jvishnuvardhan/Installing-TensorFlow-GPU-on-Windows-10-TF1.12). If you want to use Bazel 0.22.0 and CUDA 10.0, then please install TF 1.13.1. Please check [here](https://www.tensorflow.org/install/source_windows#gpu) for tested build configurations. Please let me know how it progresses. Thanks!", "> I tried vs 2017 too at first and I had to switch to 2015, which works ok. If I remember correctly nvcc was to blame. That was for TF 1.10, I'm now using 1.13 and vs 2015 still works ok.\r\n\r\nThank you , the problem are vc 2017  and  cuda 10 . \r\n\r\n### **Donot build Gpu version with vc 2017  , vc 2017 is incompatible with cuda 10 .  I build gpu version with vc 2015 and cuda 9.0** ", "@jvishnuvardhan We are hitting the same but on master branch, with VC 14.0 (should be 2015ish) and CUDA 10.0.", "> @jvishnuvardhan We are hitting the same but on master branch, with VC 14.0 (should be 2015ish) and CUDA 10.0.\r\n\r\nJust to make it clear, the exact same setup works well with CUDA 10.0 and r1.13", "> > @jvishnuvardhan We are hitting the same but on master branch, with VC 14.0 (should be 2015ish) and CUDA 10.0.\r\n> \r\n> Just to make it clear, the exact same setup works well with CUDA 10.0 and r1.13\r\n\r\nOur master branch was a couple of days / few weeks old, merging current helps going over that issue."]}, {"number": 26351, "title": "Documentation fix: replace add_metric(SomeMetric()(value)) code examples...", "body": "...with `m = SomeMetric(); add_metric(m(value))`\r\n\r\nIndeed, `add_metric(SomeMetric()(value))` will only work with the functional API, not the subclassing API, as the metric needs to be created in the constructor, and then used in the `call()` method.\r\nI was confused by these code examples when trying to use `add_metric()` when using the subclassing API, so hopefully this fix will avoid others getting confused as well.\r\n", "comments": []}, {"number": 26350, "title": "[TF 2.0] Checkpoint breaking change for RNN.", "body": "**Type of breakage**: Breakage with changing code.\r\n\r\n**APIs that are affected**: \r\n\r\n1. [`tf.nn.dynamic_rnn`](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn)\r\n2. [`tf.nn.static_rnn`](https://www.tensorflow.org/api_docs/python/tf/nn/static_rnn)\r\n3. [`tf.nn.bidirectional_dynamic_rnn`](https://www.tensorflow.org/api_docs/python/tf/nn/bidirectional_dynamic_rnn)\r\n4. [`tf.nn.static_bidirectional_rnn`](https://www.tensorflow.org/api_docs/python/tf/nn/static_bidirectional_rnn)\r\n5. All cells within the [`tf.nn.rnn_cell.*`](https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell) module.\r\n\r\n**Description of change**: The current TensorFlow RNN code is going to be replaced by `tf.keras` RNNs in TensorFlow 2.0. User will need to update their code to use the new Keras API. The weights of the Keras RNN cell are different from the existing TensorFlow RNN cell.\r\n\r\n**Variable name change map**: The RNN cell weights are in a different format between existing TensorFlow RNN cells and Keras cells. There is currently no direct mapping for that.\r\n\r\n**Target time window**: Undecided since the update requires a non-trivial user side change.", "comments": ["Unassigning myself, as this is more of an FYI than a solvable problem.", "@dynamicwebpaige \r\nPlease let us know if this issue can be moved to closed status.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26350\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26350\">No</a>\n", "AttributeError: module 'tensorflow.compat.v1' has no attribute 'contrib'\r\nI am facing this error while using rnn model in my code. Although I used \r\nimport tensorflow.compat.v1 as tf\r\ntf.disable_v2_behavior()\r\nhelp me how can I solve it "]}]