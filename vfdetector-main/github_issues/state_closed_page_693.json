[{"number": 32799, "title": "Inconsistency between keras model.predict() and model.call()", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.1\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.0.0-rc0-101-gd2d2566 2.0.0-rc1\r\n- Python version: Python 3.7.4\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nI am trying to extract the features generated by the model's DenseFeatures layer by creating a new model based on the inputs of the original model and the outputs of the DenseFeature layers. When using this submodel I notice an inconsistency between model.call() and model.predict(). If we provide extra columns to the model in model.predict() the model behaves reliably by processing the inputs using keras.engine.training_utils.standardize_input_data(). However if model.call() is used this doesn't happen and the model orders the columns based on alphabetical order as per nest.flatten(), thus the model crashes with the type cast exception if the extra columns are of a different format. Please see the code provided to reproduce this issue. I can work around this by using the predict() method. \r\n\r\n**Describe the expected behavior**\r\n\r\nExpected behaviour is that the two methods to use a model are consistent. The model.call() processes the inputs using the standardize_input_data() method to ensure the input data is as expected.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\nimport urllib.request as request\r\nimport tensorflow as tf\r\nimport pandas as pd\r\n\r\n\r\ndef download_data(download_path: str):\r\n    url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data'\r\n    header_line = 'age,sex,cp,trestbps,chol,fbs,restecg,thalach,exang,oldpeak,slope,ca,thal,target\\n'\r\n\r\n    # Download the data and add the column headers\r\n    request.urlretrieve(url, 'heart.data')\r\n    with open(download_path, 'w') as output:\r\n        output.write(header_line)\r\n        with open('heart.data', 'r') as input_data:\r\n            output.writelines(input_data.readlines())\r\n\r\n\r\ndef preprocess_df(df, categorical_columns):\r\n    \"\"\"Ensure categorical columns are treated as string inputs\"\"\"\r\n    col_types = {key : str for key in categorical_columns.keys()}\r\n    df = df.astype(col_types)\r\n    return df\r\n\r\n\r\ndef df_to_dataset(dataframe, target_column='target', shuffle=True, batch_size=5):\r\n    \"\"\"Dataset preparation code from the tensorflow tutorial\"\"\"\r\n    dataframe = dataframe.copy()\r\n    labels = dataframe.pop(target_column)\r\n    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), tf.one_hot(labels, depth=2)))\r\n    if shuffle:\r\n        ds = ds.shuffle(buffer_size=len(dataframe))\r\n    ds = ds.batch(batch_size)\r\n    return ds\r\n\r\n\r\nif __name__ == '__main__':\r\n\r\n    # Download dataset\r\n    data_path = 'heart.csv'\r\n    download_data(data_path)\r\n    df = pd.read_csv(data_path)\r\n\r\n    # Setup feature columns\r\n    numeric_columns = [\"age\", \"chol\"]\r\n    categorical_columns = {\"thal\": df['thal'].unique()}\r\n    feature_columns = {}\r\n    inputs = {}\r\n    for feature_name in numeric_columns:\r\n        feature_columns[feature_name] = tf.feature_column.numeric_column(feature_name)\r\n        inputs[feature_name] = tf.keras.Input(name=feature_name, shape=(), dtype=tf.float32)\r\n\r\n    for feature_name, vocab in categorical_columns.items():\r\n        vocab.sort()\r\n        cat_col = tf.feature_column.categorical_column_with_vocabulary_list(feature_name, vocab)\r\n        feature_columns[feature_name] = tf.feature_column.indicator_column(cat_col)\r\n        inputs[feature_name] = tf.keras.Input(name=feature_name, shape=(), dtype=tf.string)\r\n\r\n    # Prepare input data\r\n    df = preprocess_df(df, categorical_columns)\r\n    batch_size = 5  # A small batch sized is used for demonstration purposes\r\n    train_ds = df_to_dataset(df, target_column='target', batch_size=batch_size)\r\n\r\n    # Create Model\r\n    input_tensors = []\r\n    feature_names = list(feature_columns.keys())\r\n    feature_names.sort()\r\n    for column_name in feature_names:\r\n        features = feature_columns[column_name]\r\n        x = tf.keras.layers.DenseFeatures(features, name=f'{column_name}_feature')(inputs)\r\n        input_tensors.append(x)\r\n\r\n    x = tf.keras.layers.Concatenate()(input_tensors)\r\n    x = tf.keras.layers.Dense(units=24, activation='relu', name='dense_0')(x)\r\n    x = tf.keras.layers.Dense(units=24, activation='relu', name='dense_1')(x)\r\n    y_pred = tf.keras.layers.Dense(units=2, activation='softmax', name='output_layer')(x)\r\n    model = tf.keras.Model(inputs=inputs, outputs=y_pred)\r\n\r\n    model.compile(optimizer=tf.keras.optimizers.Adam(),\r\n                  loss=tf.keras.losses.CategoricalCrossentropy(),\r\n                  metrics=['accuracy'],\r\n                  run_eagerly=True)\r\n\r\n    model.summary()\r\n    model.fit(train_ds, epochs=1)\r\n\r\n    # Create a new keras model to extract the features\r\n    # the actual model is using.\r\n    outputs = []\r\n    for column_name in feature_names:\r\n        outputs.append(model.get_layer(f'{column_name}_feature').output)\r\n\r\n    feature_extractor = tf.keras.Model(model.input, outputs)\r\n\r\n    for i, (X, _) in enumerate(train_ds):\r\n\r\n        # Predict works as it calls keras.engine.training_utils.standardize_input_data() internally\r\n        # this modifies the input so that if extra columns are passed they are removed and column\r\n        # order is changed as per the model inputs specified.\r\n        # out = feature_extractor.predict(X)\r\n\r\n        # Model call() doesn't use the above util method and thus fails\r\n        # as the ordering of the input columns doesn't match the input\r\n        out = feature_extractor(X)\r\n        print(out)\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n```\r\nTraceback (most recent call last):\r\n  File \"check_model_clone_4.py\", line 103, in <module>\r\n    out = feature_extractor(X)\r\n  File \"/home/user/anaconda3/envs/rc1/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 891, in __call__\r\n    outputs = self.call(cast_inputs, *args, **kwargs)\r\n  File \"/home/user/anaconda3/envs/rc1/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\", line 707, in call\r\n    convert_kwargs_to_constants=base_layer_utils.call_context().saving)\r\n  File \"/home/user/anaconda3/envs/rc1/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\", line 859, in _run_internal_graph\r\n    output_tensors = layer(computed_tensors, **kwargs)\r\n  File \"/home/user/anaconda3/envs/rc1/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 891, in __call__\r\n    outputs = self.call(cast_inputs, *args, **kwargs)\r\n  File \"/home/user/anaconda3/envs/rc1/lib/python3.7/site-packages/tensorflow_core/python/feature_column/dense_features.py\", line 133, in call\r\n    self._state_manager)\r\n  File \"/home/user/anaconda3/envs/rc1/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py\", line 4357, in get_dense_tensor\r\n    return transformation_cache.get(self, state_manager)\r\n  File \"/home/user/anaconda3/envs/rc1/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py\", line 2608, in get\r\n    transformed = column.transform_feature(self, state_manager)\r\n  File \"/home/user/anaconda3/envs/rc1/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py\", line 4296, in transform_feature\r\n    transformation_cache, state_manager)\r\n  File \"/home/user/anaconda3/envs/rc1/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py\", line 3771, in get_sparse_tensors\r\n    transformation_cache.get(self, state_manager), None)\r\n  File \"/home/user/anaconda3/envs/rc1/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py\", line 2608, in get\r\n    transformed = column.transform_feature(self, state_manager)\r\n  File \"/home/user/anaconda3/envs/rc1/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py\", line 3749, in transform_feature\r\n    return self._transform_input_tensor(input_tensor, state_manager)\r\n  File \"/home/user/anaconda3/envs/rc1/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py\", line 3726, in _transform_input_tensor\r\n    prefix='column_name: {} input_tensor'.format(self.key))\r\n  File \"/home/user/anaconda3/envs/rc1/lib/python3.7/site-packages/tensorflow_core/python/feature_column/utils.py\", line 58, in assert_string_or_int\r\n    '{} dtype must be string or integer. dtype: {}.'.format(prefix, dtype))\r\nValueError: column_name: thal input_tensor dtype must be string or integer. dtype: <dtype: 'float32'>.\r\n```\r\n", "comments": ["I was able to replicate the issue for TF-2.0rc1 and TF-2.0nightly. Kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/4928abbc6345f705adc36bc9e12aa840/32799.ipynb) of colab.thanks!", "Here is a short, self-contained example where call and predict disagree (without raising an exception).\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nprint(\"tensorflow version\",tf.__version__)\r\n\r\n# make a null base model\r\nbaseinputs = keras.Input(shape=(1,), name='base_input')\r\nbase = keras.Model(inputs=baseinputs, outputs=baseinputs)\r\n\r\n# Use it to construct a bigger model.\r\n# (I now know there is a better way to make this model.)\r\nNin,Nout,Nparallel = 1,1,2\r\nparallel_inputs = keras.Input(shape=(Nin,Nparallel), name='parallel_input0')\r\n# apply base NN to each parallel slice; each outputs (?,Nout)\r\nxs = [base(layers.Lambda(lambda x : x[:,:,i],name='view'+str(i))(parallel_inputs)) for i in range(Nparallel)]\r\n# reshape each of them to (?,Nout,1)\r\nxs = [layers.Reshape((Nout,1))(x) for x in xs]\r\n# concatenate on the third direction to get (?,Nout,Nparallel)\r\ncx = layers.Concatenate()(xs)\r\n# create input scalars for weighted sun (?,Nparallel)\r\nweight_inputs = keras.Input(shape=(Nparallel,), name='parallelScalars')\r\n# do a weighted sum over the third direction to get (?,Nout)\r\nout = layers.Dot((-1,-1))([cx,weight_inputs])\r\nwrapper = keras.Model(inputs=[weight_inputs,parallel_inputs], outputs=out, name='parallelwrapper')\r\n\r\n# Make tiny example and try predict and call\r\nw = np.array([[7.,11.]],dtype='float32')\r\nv = np.array([[[3.,5.]]],dtype='float32')\r\nwrappercall = wrapper([w,v]).numpy()\r\nwrapperpredict = wrapper.predict([w,v])\r\nprint(\"wrapper ||call - predict|| =\",np.linalg.norm(wrappercall-wrapperpredict))\r\nprint(\"wrapper predict =\",wrapperpredict,\"; correct is 3*7+5*11=\",3*7+5*11)\r\nprint(\"wrapper call =\",wrappercall,\"; appears to do 5*(7+11)=\",5*(7+11))\r\n\r\n\"\"\"\r\nOutputs:\r\n\r\n/usr/lib/python3/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\ntensorflow version 2.0.0\r\n2019-10-21 14:27:01.188059: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-10-21 14:27:01.211364: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 1800000000 Hz\r\n2019-10-21 14:27:01.211915: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x54e1d50 executing computations on platform Host. Devices:\r\n2019-10-21 14:27:01.211940: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\nwrapper ||call - predict|| = 14.0\r\nwrapper predict = [[76.]] ; correct is 3*7+5*11= 76\r\nwrapper call = [[90.]] ; appears to do 5*(7+11)= 90\r\n\r\n\r\n\"\"\"\r\n```", "same issue here.\r\n@tanzhenyu @jvishnuvardhan any updates?", "tf1.14.0 can reproduce the bug, both eager mode and graph mode.", "Gently ping @qlzh727 @rchao @pavithrasv; does this looks familiar to you?", "@byronyi  @MartinMohlenkamp  using the [standalone repro code ](https://github.com/tensorflow/tensorflow/issues/32799#issuecomment-544651131)to depict the difference between call and predict, I am **unable to reproduce** this with 2.2.0-rc1. \r\n@nikhil-mathew-td can you please try with 2.2.0-rc1  and close this if the issue is fixed for you.\r\n", "@nikhil-mathew-td @MartinMohlenkamp Thanks for the report! (And sorry for the late reply)\r\nLooks like this is fixed in tf-nightly. Would you mind confirming it before I close the issue?", "@tanzhenyu the original code to reproduce with DenseFeatures still fails but at a new location. Now the code fails at model.fit() with the following error:\r\n```\r\n2020-04-01 11:56:24.474889: W tensorflow/core/framework/op_kernel.cc:1706] OP_REQUIRES failed at cast_op.cc:124 : Unimplemented: Cast string to float is not supported\r\nTraceback (most recent call last):\r\n  File \"/home/nikhil/workspace/testing_ground/test_tf_bug/check.py\", line 86, in <module>\r\n    model.fit(train_ds, epochs=1)\r\n  File \"/opt/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 71, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n  File \"/opt/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 920, in fit\r\n    tmp_logs = train_function(iterator)\r\n  File \"/opt/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 608, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/opt/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 672, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/opt/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2420, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"/opt/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1665, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"/opt/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1746, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/opt/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 598, in call\r\n    ctx=ctx)\r\n  File \"/opt/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.UnimplementedError:  Cast string to float is not supported\r\n\t [[node model/Cast_1 (defined at home/nikhil/workspace/testing_ground/test_tf_bug/check.py:86) ]] [Op:__inference_train_function_941]\r\n\r\nFunction call stack:\r\ntrain_function\r\n```\r\nThe code goes through if the  tf.data.Dataset.from_tensor_slices only includes the columns being used by DenseFeatures and **no additional columns**. Meaning the feature_columns list provided to the DenseFeatures __init__ must include all the columns in the dataset, not a subset. Is this expected?", "> @tanzhenyu the original code to reproduce with DenseFeatures still fails but at a new location. Now the code fails at model.fit() with the following error:\r\n> \r\n> ```\r\n> 2020-04-01 11:56:24.474889: W tensorflow/core/framework/op_kernel.cc:1706] OP_REQUIRES failed at cast_op.cc:124 : Unimplemented: Cast string to float is not supported\r\n> Traceback (most recent call last):\r\n>   File \"/home/nikhil/workspace/testing_ground/test_tf_bug/check.py\", line 86, in <module>\r\n>     model.fit(train_ds, epochs=1)\r\n>   File \"/opt/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 71, in _method_wrapper\r\n>     return method(self, *args, **kwargs)\r\n>   File \"/opt/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 920, in fit\r\n>     tmp_logs = train_function(iterator)\r\n>   File \"/opt/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 608, in __call__\r\n>     result = self._call(*args, **kwds)\r\n>   File \"/opt/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 672, in _call\r\n>     return self._stateless_fn(*args, **kwds)\r\n>   File \"/opt/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2420, in __call__\r\n>     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n>   File \"/opt/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1665, in _filtered_call\r\n>     self.captured_inputs)\r\n>   File \"/opt/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1746, in _call_flat\r\n>     ctx, args, cancellation_manager=cancellation_manager))\r\n>   File \"/opt/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 598, in call\r\n>     ctx=ctx)\r\n>   File \"/opt/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n>     inputs, attrs, num_outputs)\r\n> tensorflow.python.framework.errors_impl.UnimplementedError:  Cast string to float is not supported\r\n> \t [[node model/Cast_1 (defined at home/nikhil/workspace/testing_ground/test_tf_bug/check.py:86) ]] [Op:__inference_train_function_941]\r\n> \r\n> Function call stack:\r\n> train_function\r\n> ```\r\n> \r\n> The code goes through if the tf.data.Dataset.from_tensor_slices only includes the columns being used by DenseFeatures and **no additional columns**. Meaning the feature_columns list provided to the DenseFeatures **init** must include all the columns in the dataset, not a subset. Is this expected?\r\n\r\nCan you use print(tf.__version__) and let us know the output?", "```tf.__version__```", "`tf.__version__` is the following : `'2.2.0-dev20200331'`", "That's odd. The results I'm getting is:\r\ntensorflow version 2.2.0-dev20200331\r\nwrapper ||call - predict|| = 0.0\r\nwrapper predict = [[90.]] ; correct is 3*7+5*11= 76\r\nwrapper call = [[90.]] ; appears to do 5*(7+11)= 90", "Ah I see, that's the code snippet provided by @MartinMohlenkamp, yup that one returns the result you provided. I was referring the original code snippet I provided in the issue description above. The snippet is larger, so my apologies, but that's the one that's failing.", "@tanzhenyu note that when you ran my code snippet, call and predict agreed, but both gave the incorrect output.", "Ok then it looks like there's been some changes regarding the unified model call path. Re-assigning to @qlzh727 ", "@nikhil-mathew-td  Closing this issue as it is fixed in latest TF-nightly version.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32799\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32799\">No</a>\n", "From above discussions I don't think this is fixed", "@nikhil-mathew-td Looks like this was resolved in recent tf-nightly. I cannot reproduce the issue. Output from `call` and `predict` are same.\r\n\r\n[Here](https://colab.research.google.com/gist/jvishnuvardhan/c4ad1b53afeb86644f8a8461bdbbdcbd/32799.ipynb) is the gist with the @nikhil-mathew-td code.  \r\n\r\n[Here](https://colab.research.google.com/gist/jvishnuvardhan/1466c6a88e6b2505cf7647038d051e9c/untitled967.ipynb) is the gist with @MartinMohlenkamp code.\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!\r\n", "In the gist of my code, call and predict agreed, but both gave the logically incorrect output. \r\nThe effect is not a dot product `3*7+5*11` but is instead ignoring the 3 and doing `5*(7+11)`.\r\nThis is the same state as on April 1.\r\n\r\nOr perhaps the line\r\n`xs = [base(layers.Lambda(lambda x : x[:,:,i],name='view'+str(i))(parallel_inputs)) for i in range(Nparallel)]`\r\ndoes not slice as I thought and a now-fixed bug in predict made me think it was correct. ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Any progress on that issue? I am still getting inconsistent behaviour in tensorflow 2.3.1 when using batches.", "@MartinMohlenkamp @stoyan-dixa  Could you please check in TF 2.5 and 2.6 nightly versions ad let us know if the issue still persists.Thanks!", "In TF 2.5 I get the same result as [my July 31, 2020 comment](https://github.com/tensorflow/tensorflow/issues/32799#issuecomment-667167468): call and predict agree, but both give what I thought was a logically incorrect answer. If my logic is correct, then the bug is in tensorflow.karas.layers.Lambda rather than call or predict.\r\n\r\nI don't have a convenient way to test 2.6 nightly.", "Hi There,\r\n\r\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \r\n\r\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32799\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32799\">No</a>\n"]}, {"number": 32798, "title": "can not watch the version of tensorflow 1.14.0 by tensorflow.__version__", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nhere are some information about installing some packages like python and tensorflow[CPU] \r\n```\r\nPython 3.7.4 [Clang 10.0.0 (clang-1000.11.45.5)] \r\ntensorflow                       1.14.0\r\ntensorflow-estimator             1.14.0\r\ntensorflow-federated             0.7.0\r\ntensorflow-model-optimization    0.1.2\r\n```\r\n`import tensorflow as tf` is ok, however, i want to check tensorflow version by `tf.__version__` , errors happened:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/util/module_wrapper.py\", line 171, in __getattr__\r\n    raise e\r\n  File \"/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/util/module_wrapper.py\", line 168, in __getattr__\r\n    attr = getattr(self._tfmw_wrapped_module, name)\r\nAttributeError: module 'tensorflow' has no attribute '__version__'\r\n```", "comments": ["Can you please go through #32768 and see if it helps you.Thanks!", "Can you also fill in the template in the bug please? In particular operating system, python version and the way you obtained tensorflow (pip, conda, from source, in a virtualenv or global?)", "@smartcatdog \r\nPlease, let us know if the issue still persists?. If so can you please fill the Github [new issue template.](https://github.com/tensorflow/tensorflow/issues/new/choose)Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32798\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32798\">No</a>\n"]}, {"number": 32797, "title": "[Intel Mkl] Fixing the README badge link", "body": "The old README nightly was pointing to the https://tensorflow-ci.intel.com/job/tensorflow-mkl-linux-cpu/ job that is run multiple times per day to run Unit Tests on the master branch, and which frequently fails. The correct job to point to is the https://tensorflow-ci.intel.com/job/tensorflow-mkl-build-whl-nightly/ job which runs once per day to build the whls.", "comments": ["@claynerobison Could you please check reviewer comments and keep us posted. Thanks!", "@claynerobison Wondering if you still need this PR, if yes can you please check reviewer comments and keep us posted. Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 32795, "title": "tf.config namespace not available in rc1/2 (was available in beta0)", "body": "**System information**\r\ntf_env.txt attached\r\n\r\n**The Issue**\r\ntf.config namespace is not available in TF 2.0 rc1 and rc2 while it was available in beta0 releases and before. I see no indication that this namespace is being deprecated or changed in the documentation.\r\n\r\n```\r\nAttributeError Traceback (most recent call last)\r\n<ipython-input-1-a65a6ef9c8ca> in <module>\r\n      1 import tensorflow as tf\r\n----> 2 print(tf.config.experimental.list_physical_devices('CPU'))\r\nAttributeError: module 'tensorflow' has no attribute 'config'\r\n```\r\n\r\n**Expected behavior**\r\nAPI functionality under tf.config namespace should be available unless otherwise indicated in the documentation. The latest API info says it should be there:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/config/experimental/list_physical_devices\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nprint(tf.config.experimental.list_physical_devices('CPU'))\r\n```\r\n[tf_env.txt](https://github.com/tensorflow/tensorflow/files/3650032/tf_env.txt)\r\n", "comments": ["The `tf.config` works just fine for me with TF 2.0 rc2. Looking into `tf_env.txt`, I contains the following:\r\n```\r\n== check pips ===================================================\r\nnumpy                    1.17.2              \r\nprotobuf                 3.9.2               \r\ntensorflow               2.0.0rc2            \r\n\r\n...\r\n\r\n== tensorflow import ============================================\r\ntf.version.VERSION = 1.13.1\r\n```\r\nso it seems you have both TF 2.0.0rc2 and 1.13.1 installed, and `import tensorflow` actually imports the 1.13.1 version, which of course does not have `tf.config`.", ":: slaps forehead :: I've been upgrading from one tf2.0 pre-release version to the next without issue for months but still can't believe I overlooked this possibility! tensorflow 1.13 was apparently a dependency of a conda-forge package I had installed long ago that I inadvertently reinstalled. SMH. Thanks!"]}, {"number": 32794, "title": "[r1.15-CherryPick]: Cherrypicks svoii", "body": "", "comments": ["I don't think the failing distributed gpu test is related to my PR.", "> I don't think the failing distributed gpu test is related to my PR.\r\n\r\nyes that has been failing on the branch for a while now. "]}, {"number": 32793, "title": "device_lib.list_local_devices() InvalidArgumentError: Invalid device ordinal value (1). Valid range is [0, 0].", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): gLinux\r\n- TensorFlow installed from (source or binary): pip3 installed\r\n- TensorFlow version (use command below): 2.0.0-rc1\r\n- Python version: 3.6.2\r\n- CUDA/cuDNN version: 10.0, 7.6.3\r\n- GPU model and memory:\r\n\r\noutput of ```nvidia-smi``` from the terminal:\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 430.34       Driver Version: 430.34       CUDA Version: 10.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Quadro P1000        Off  | 00000000:65:00.0  On |                  N/A |\r\n| 37%   52C    P0    N/A /  N/A |   1289MiB /  4037MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  TITAN RTX           Off  | 00000000:B3:00.0 Off |                  N/A |\r\n| 41%   29C    P8    14W / 280W |   1155MiB / 24220MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n```\r\n\r\nNOTE: in the above output it shows that it is using ```CUDA Version: 10.``` but my ```LD_LIBRARY_PATH``` environment variable is pointing to CUDA 10.0.\r\n\r\nSnippet of code that cause the problem:\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.client import device_lib\r\n\r\ndevice_lib.list_local_devices()\r\n```\r\n\r\nerror message:\r\n```\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-2-b6f1169dc7e5> in <module>\r\n     2 from tensorflow.python.client import device_lib\r\n     3 \r\n---> 4 device_lib.list_local_devices()\r\n\r\n~/.local/lib/python3.6/site-packages/tensorflow_core/python/client/device_lib.py in list_local_devices(session_config)\r\n     39   return [\r\n     40       _convert(s)\r\n---> 41       for s in pywrap_tensorflow.list_devices(session_config=session_config)\r\n     42   ]\r\n\r\n~/.local/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py in list_devices(session_config)\r\n   2247     return ListDevicesWithSessionConfig(session_config.SerializeToString())\r\n   2248   else:\r\n-> 2249     return ListDevices()\r\n   2250 \r\n   2251 \r\n\r\nInvalidArgumentError: Invalid device ordinal value (1). Valid range is [0, 0].\r\n\twhile setting up XLA_GPU_JIT device number 1\r\n```\r\n\r\nPotential cause and current workaround:\r\nIn the terminal output I notice that because the Quadro P1000  in my workstation only has 5 multiprocessor and so by default tf will not use it (minimum 8), so I added the following line to my ```.bashrc```\r\n\r\n```\r\nexport TF_MIN_GPU_MULTIPROCESSOR_COUNT=5\r\n```\r\n\r\nand run ```source .bashrc``` and it works. Another potential solution if I don't want to set the min GPU Multiprocessor count I can remove the Quadro P1000 from my workstation. I suspect that there is an inconsistency within list_local_devices() that fetch all GPUs in the workstation but didn't update base on min gpu multiprocessor count rule. So I run another experiment to see if I can reproduce the error after setting ```TF_MIN_GPU_MULTIPROCESSOR_COUNT``` to 5\r\n\r\nThe below code will reproduce the same error:\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.client import device_lib\r\n\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\ntf.config.experimental.set_visible_devices(gpus[0], 'GPU')\r\ndevice_lib.list_local_devices()\r\n```\r\nThis will produce the same error but if we call ```device_lib.list_local_devices()``` before calling ```tf.config.experimental.set_visible_devices(gpus[0], 'GPU')``` and then we call ```device_lib.list_local_devices()``` again, there is no error. I suspect that maybe setting device to visible may interact weirdly with list_local_devices().\r\n\r\n\r\n", "comments": ["Can you print the output of  ```which nvcc```? This will give you cuda runtime version whereas ```nvidia-smi``` shows latest installed cuda driver version. Both can be different. \r\n", "Hi ymodak,\r\n\r\n```which nvcc``` gives me the output ``` /usr/local/cuda/bin/nvcc```\r\n\r\nand ```nvcc --version``` gives me the following output:\r\n\r\n```\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2018 NVIDIA Corporation\r\nBuilt on Sat_Aug_25_21:08:01_CDT_2018\r\nCuda compilation tools, release 10.0, V10.0.130\r\n```\r\n\r\nI hope this helps!", "Facing the same issue. Any thoughts?", "Hi @abhoi ,\r\n\r\nmay you provide me with the output of ```nvidia-smi``` from the terminal and maybe I can help you debug :)", "@shun-lin Any chance you're willing to send a PR to fix this?", "@sanjoy what I end up doing is\r\n\r\n1) set ```export TF_MIN_GPU_MULTIPROCESSOR_COUNT=5``` in my config file to let TF use the less powerful GPU and \r\n\r\n2) Removing the less power GPU and just use the powerful GPU I have.\r\n\r\nI have not spent much time diving deeper into where the the problem occurs (i.e. it should work without removing the less powerful GPU and know to only list the available devices) but when I have time outside of work I can definitely try to help out :)", "This is somewhat strange, because you're using the same configuration (as far as I can tell) as virtually everyone else using GPU: one powerful GPU for compute, other GPU for driving graphics. I really would not expect crashes on this codepath.", "@cheshire exactly, I wanted to use my Quadro P1000 for general graphics use and Titan RTX for computing, but then I got the above error so I just remove the Quadro P1000. Do you have any insight into what may have gone wrong? Thank you so much for your time!", "Hi Shun-lin, Did you get any response on this one? I am getting the same error in the following\r\n/usr/local/cuda-10.2/bin/nvcc\r\nSun Jan 26 18:52:57 2020       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 760     On   | 00000000:01:00.0 N/A |                  N/A |\r\n| 42%   46C    P0    N/A /  N/A |   1150MiB /  4034MiB |     N/A      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce RTX 207...  On   | 00000000:02:00.0 Off |                  N/A |\r\n|  6%   55C    P2    55W / 215W |    610MiB /  7982MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0                    Not Supported                                       |\r\n|    1       701      C   ...ndon/anaconda3/envs/cuda_env/bin/python   101MiB |\r\n|    1      2629      C   ...ndon/anaconda3/envs/cuda_env/bin/python    99MiB |\r\n|    1      8061      C   ...ndon/anaconda3/envs/cuda_env/bin/python   101MiB |\r\n|    1     17581      C   ...ndon/anaconda3/envs/cuda_env/bin/python    99MiB |\r\n|    1     20290      C   ...ndon/anaconda3/envs/cuda_env/bin/python    99MiB |\r\n|    1     23923      C   ...ndon/anaconda3/envs/cuda_env/bin/python    99MiB |\r\n+-----------------------------------------------------------------------------+\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2019 NVIDIA Corporation\r\nBuilt on Wed_Oct_23_19:24:38_PDT_2019\r\nCuda compilation tools, release 10.2, V10.2.89", "@shun-lin Can you try running with tf-nightly?  The codepath that was causing the original error has been fixed.\r\n\r\n@maximuslee1226 What version of TF are you using?", "Hi Sanjoy, Thanks for your response. I am running tensorflow 1.14.0", "> Hi Sanjoy, Thanks for your response. I am running tensorflow 1.14.0\r\n\r\nThank you.  I believe this issue has been fixed; can you check if you can reproduce the problem with tf-nightly?", "How can I install tf-nightly?", "never mind pip install tf-nightly", "> @shun-lin Can you try running with tf-nightly? The codepath that was causing the original error has been fixed.\r\n> \r\n> @maximuslee1226 What version of TF are you using?\r\n\r\nThanks for the suggestion, but I no longer have access to the powerful GPU due to switching to other team and therefore I wouldn't be able to know if the new `tf-nightly` build has fixed the issue, if @maximuslee1226 issue is fixed then I think we can closed this issue. Thank you so much for your help!", "Hi Shun-lin, I tried, but it failed. I have tf-nightly installed, but it's still giving me the same error. cudnn installed and working find with samples after mnist test, but as soon as I call the tf.config.experimental.list_physical_devices(), I get the same error. I need some help! Thanks in advance. ", "Can you please share the log file?  The original error has now been replaced by a [`LOG(INFO)` line](https://github.com/tensorflow/tensorflow/blob/f54bb6f5578b931d79884302768996ba1073f685/tensorflow/compiler/jit/xla_gpu_device.cc#L161) so you're probably seeing a different error.", "Also could you say whether the crash is still there if you run under `TF_XLA_FLAGS='--tf_xla_enable_xla_devices=false'` ?", "any update about this issue? I am also facing the same problem.", "@betterze Have you tried running under the environment variable specified in the previous comment?", "@cheshire Thank you for your reply. How to run under TF_XLA_FLAGS='--tf_xla_enable_xla_devices=false' ? I am not sure where to change this variable. \r\n\r\nMy promble is solved by:\r\n\r\n~~~\r\nimport os\r\nos.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" #(or \"1\" or \"2\")\r\n~~~\r\n\r\nTry 0,1,2 that represent different index of gpu. One of my gpu is not strong enough to train DL model, so I can choose any gpu except this one.", "@betterze It's an environment variable, `env TF_XLA_FLAGS='--tf_xla_enable_xla_devices=false' ./yourscript.py`", "Was able to replicate the issue with TF v2.5 in Google Colab with and without GPU  , They are not throwing errors in both situation ,please find the [gist](https://colab.research.google.com/gist/mohantym/c830549d3e3aa202f31519cf1a238ec2/32793_g.ipynb)  here .. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32793\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32793\">No</a>\n"]}, {"number": 32792, "title": "Add momentum optimizer option to speech_commands", "body": "- Add command line argument to choose momentum optimizer option to speech_commands.\r\n- Default optimizer for speech commands is unchanged to still use GradientDescentOptimizer.\r\n- Default values for momentum optimizer are to use a momentum of .9 and use_nesterov=True.", "comments": ["This looks reasonable. Any comment on when/why someone should choose this option? Does it work better for some configurations?", "@MarkDaoust : For the audio dataset we're working with, the Nesterov Momentum Optimizer gets a better training and validation accuracy after the same number of steps compared with the standard GradientDescentOptimizer. Would recommend users to try this option if they're trying to improve audio recognition on their custom datasets!", "@MarkDaoust : Can you approve this pull request or are there changes you'd like me to make to the code? Thanks!", "Awesome thanks!", "@MarkDaoust : Fixed a line that was over 80 characters to be less than 80 characters long. Can you approve this pull request one more time? Thanks!", "Hi,\r\n\r\nIt looks like this is failing the tests.\r\n\r\nhttps://source.cloud.google.com/results/invocations/1dc48548-20e7-4cb6-9759-e406882e2228/targets/%2F%2Ftensorflow%2Fexamples%2Fspeech_commands:train_test/tests\r\n\r\nI think you just need to also define this flag in `trail_test.py` which also runs this code.", "Sorry didn't realize the tests needed to be modified and will fix as soon as I can. Thanks for letting me know!", "@MarkDaoust : Sorry for the delay. I've added the optimizer flag to train_test.py. Can you take another look at this PR? Thanks!\r\n\r\nVerified that the following tests now pass:\r\n> bazel test --jobs=2 --ram_utilization_factor=30 //tensorflow/examples/speech_commands:train_test", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32792) for more info**.\n\n<!-- need_author_cla -->", "Tried resolving a merge commit through the web browser but seems like Googlebot is no longer happy with the CLA we signed.", "Seems auto-merge is not happening but the changes are now committed so we can close this. Thank you for the PR.", "Thanks!"]}, {"number": 32791, "title": "Mixnet graph freezing issue", "body": "**System information**\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nI have been trying to covert the [Mixnet](https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet/mixnet) model to Core ML using the [tfcoreml](https://github.com/tf-coreml/tf-coreml) converter. I froze the graph, then tried converting it, and that's when I get thrown with the following error:\r\n```\r\n1199 ops in the final graph.\r\n\r\nLoading the TF graph...\r\nGraph Loaded.\r\nNow finding ops in the TF graph that can be dropped for inference\r\nCollecting all the 'Const' ops from the graph, by running it....\r\n---------------------------------------------------------------------------\r\nOutOfRangeError                           Traceback (most recent call last)\r\n~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1355     try:\r\n-> 1356       return fn(*args)\r\n   1357     except errors.OpError as e:\r\n\r\n~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1340       return self._call_tf_sessionrun(\r\n-> 1341           options, feed_dict, fetch_list, target_list, run_metadata)\r\n   1342 \r\n\r\n~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/client/session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)\r\n   1428         self._session, options, feed_dict, fetch_list, target_list,\r\n-> 1429         run_metadata)\r\n   1430 \r\n\r\nOutOfRangeError: Node 'mixnet-s/mixnet_model/stem/batch_normalization/FusedBatchNormV3' (type: 'Add', num of outputs: 1) does not have output 5\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nOutOfRangeError                           Traceback (most recent call last)\r\n<ipython-input-12-6b7b8d71599d> in <module>\r\n     48                      mlmodel_path=ml_model,\r\n     49                      output_feature_names=['logits'],\r\n---> 50                      input_name_shape_dict={})\r\n\r\n~/anaconda3/envs/tf/lib/python3.7/site-packages/tfcoreml/_tf_coreml_converter.py in convert(tf_model_path, mlmodel_path, output_feature_names, input_name_shape_dict, image_input_names, is_bgr, red_bias, green_bias, blue_bias, gray_bias, image_scale, class_labels, predicted_feature_name, predicted_probabilities_output, add_custom_layers, custom_conversion_functions, use_coreml_3)\r\n    619       predicted_probabilities_output=predicted_probabilities_output,\r\n    620       add_custom_layers=add_custom_layers,\r\n--> 621       custom_conversion_functions=custom_conversion_functions)\r\n\r\n~/anaconda3/envs/tf/lib/python3.7/site-packages/tfcoreml/_tf_coreml_converter.py in _convert_pb_to_mlmodel(tf_model_path, mlmodel_path, output_feature_names, input_name_shape_dict, image_input_names, is_bgr, red_bias, green_bias, blue_bias, gray_bias, image_scale, class_labels, predicted_feature_name, predicted_probabilities_output, add_custom_layers, custom_conversion_functions)\r\n    260     else:\r\n    261       const_tensor_names = []\r\n--> 262     tensors_evaluated = sess.run(tensors, feed_dict=input_feed_dict)\r\n    263     for i in range(len(tensor_names)):\r\n    264       if tensor_names[i] not in SHAPE_DICT:\r\n\r\n~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    948     try:\r\n    949       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 950                          run_metadata_ptr)\r\n    951       if run_metadata:\r\n    952         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1171     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n   1172       results = self._do_run(handle, final_targets, final_fetches,\r\n-> 1173                              feed_dict_tensor, options, run_metadata)\r\n   1174     else:\r\n   1175       results = []\r\n\r\n~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1348     if handle is None:\r\n   1349       return self._do_call(_run_fn, feeds, fetches, targets, options,\r\n-> 1350                            run_metadata)\r\n   1351     else:\r\n   1352       return self._do_call(_prun_fn, handle, feeds, fetches)\r\n\r\n~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1368           pass\r\n   1369       message = error_interpolation.interpolate(message, self._graph)\r\n-> 1370       raise type(e)(node_def, op, message)\r\n   1371 \r\n   1372   def _extend_graph(self):\r\n\r\nOutOfRangeError: Node 'mixnet-s/mixnet_model/stem/batch_normalization/FusedBatchNormV3' (type: 'Add', num of outputs: 1) does not have output 5\r\n\r\n```\r\nI presume this is an issue with the way I am freezing the graph. The checkpoint was taken from the official [Mixnet](https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet/mixnet) implementation. I am relatively new to TF and am still learning. Please help me resolve this issue and let me know in case further information is needed. Suggestions for converting to Core ML are also welcome. Thanks in advance!\r\n\r\n**Code to reproduce the issue**\r\nYou might have to install TF 1.14.0\r\n```python\r\nimport tensorflow as tf\r\nimport tfcoreml as tf_converter\r\nfrom tensorflow.compat.v1 import graph_util\r\n\r\ntf.logging.set_verbosity(tf.logging.ERROR)\r\n\r\n# Download the checkpoint\r\nmodel_name = 'mixnet-s'\r\n!wget https://storage.googleapis.com/cloud-tpu-checkpoints/mixnet/{model_name}.tar.gz -O {model_name}.tar.gz\r\n!tar -zxvf {model_name}.tar.gz\r\nckpt_dir = model_name\r\n\r\ndef _freeze_graph(model_folder):\r\n    # Retrieve the checkpoint fullpath\r\n    checkpoint = tf.train.get_checkpoint_state(model_folder)\r\n    input_checkpoint = checkpoint.model_checkpoint_path\r\n    \r\n    # File fullname of the freezed graph\r\n    absolute_model_folder = \"/\".join(input_checkpoint.split('/')[:-1])\r\n    output_graph = absolute_model_folder + \"/model.pb\"\r\n\r\n    # Before exporting the graph, get the output nodes\r\n    output_node_names = \"logits\"\r\n\r\n    # Clear devices to allow TF to control on which device it will load operations\r\n    clear_devices = True\r\n    \r\n    # Import the meta graph and retrieve a Saver\r\n    saver = tf.train.import_meta_graph(input_checkpoint + '.meta',\r\n                                       clear_devices=clear_devices)\r\n\r\n    # Retrieve the protobuf graph definition\r\n    graph = tf.get_default_graph()\r\n    input_graph_def = graph.as_graph_def()\r\n    \r\n    # Start a session and restore the graph weights\r\n    with tf.Session() as sess:\r\n        saver.restore(sess, input_checkpoint)\r\n\r\n        # Use a built-in TF helper to export variables to constants\r\n        output_graph_def = graph_util.convert_variables_to_constants(\r\n            sess,\r\n            input_graph_def, \r\n            output_node_names.split(\",\")\r\n        ) \r\n\r\n        # Serialize and dump the output graph to the filesystem\r\n        with tf.gfile.GFile(output_graph, \"wb\") as f:\r\n            f.write(output_graph_def.SerializeToString())\r\n        print(\"%d ops in the final graph.\" % len(output_graph_def.node))\r\n\r\n_freeze_graph(ckpt_dir)\r\n\r\nml_model = 'mixnet-s/mixnet_core.mlmodel'\r\nfrozen_model = 'mixnet-s/model.pb'\r\n\r\ntf_converter.convert(tf_model_path=frozen_model,\r\n                     mlmodel_path=ml_model,\r\n                     output_feature_names=['logits'],\r\n                     input_name_shape_dict={})\r\n```", "comments": ["@dhananjaisharma10,\r\nThanks for reporting the issue, Can you provide a simple and standalone code to reproduce the issue?", "> @dhananjaisharma10,\r\n> Thanks for reporting the issue, Can you provide a simple and standalone code to reproduce the issue?\r\n\r\nI have updated the \"code to reproduce\". Please let me know if you have trouble running it.", "Thank you for updating the code,Issue replicating for TF-1.14 and TF-1.15rc1, kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/d7823c31df1fdad9513663c91ee07528/32791.ipynb) of colab.Thanks!", "Have you considered using ```freeze_graph``` function to convert your checkpoint model to frozen model.\r\nYou can try,\r\n```python\r\nfrom tensorflow.python.tools import freeze_graph\r\n```\r\nhttps://github.com/tensorflow/tensorflow/blob/f718c5af3706d99d2acb70d27cb79231650e468b/tensorflow/python/tools/freeze_graph.py#L286", "> Have you considered using `freeze_graph` function to convert your checkpoint model to frozen model.\r\n> You can try,\r\n> \r\n> ```python\r\n> from tensorflow.python.tools import freeze_graph\r\n> ```\r\n> \r\n> https://github.com/tensorflow/tensorflow/blob/f718c5af3706d99d2acb70d27cb79231650e468b/tensorflow/python/tools/freeze_graph.py#L286\r\n\r\nYes, I have been using that as well, but was facing some issues. I was able to convert the model to Core ML without using the BN layers. I am yet to try this _with_ BN. Will update this soon.", "> Have you considered using `freeze_graph` function to convert your checkpoint model to frozen model.\r\n> You can try,\r\n> \r\n> ```python\r\n> from tensorflow.python.tools import freeze_graph\r\n> ```\r\n> \r\n> https://github.com/tensorflow/tensorflow/blob/f718c5af3706d99d2acb70d27cb79231650e468b/tensorflow/python/tools/freeze_graph.py#L286\r\n\r\nHi!\r\n\r\nI tried freezing the graph as you suggested, but I am getting the following error:\r\n```\r\nWARNING:tensorflow:From freeze.py:247: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\r\n\r\nWARNING:tensorflow:From freeze.py:247: The name tf.logging.ERROR is deprecated. Please use tf.compat.v1.logging.ERROR instead.\r\n\r\n2019-10-04 21:24:21.124841: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-10-04 21:24:21.147115: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3192000000 Hz\r\n2019-10-04 21:24:21.148346: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5634b8da0430 executing computations on platform Host. Devices:\r\n2019-10-04 21:24:21.148384: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-10-04 21:24:21.522500: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\r\nTraceback (most recent call last):\r\n  File \"freeze.py\", line 263, in <module>\r\n    feed_dict={'X:0': X})\r\n  File \"freeze.py\", line 238, in _simple_run_and_freeze\r\n    initializer_nodes=\"\")\r\n  File \"/home/dhananjai/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/tools/freeze_graph.py\", line 361, in freeze_graph\r\n    checkpoint_version=checkpoint_version)\r\n  File \"/home/dhananjai/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/tools/freeze_graph.py\", line 190, in freeze_graph_with_def_protos\r\n    var_list=var_list, write_version=checkpoint_version)\r\n  File \"/home/dhananjai/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 825, in __init__\r\n    self.build()\r\n  File \"/home/dhananjai/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 837, in build\r\n    self._build(self._filename, build_save=True, build_restore=True)\r\n  File \"/home/dhananjai/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 875, in _build\r\n    build_restore=build_restore)\r\n  File \"/home/dhananjai/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 482, in _build_internal\r\n    names_to_saveables)\r\n  File \"/home/dhananjai/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/training/saving/saveable_object_util.py\", line 342, in validate_and_slice_inputs\r\n    for converted_saveable_object in saveable_objects_for_op(op, name):\r\n  File \"/home/dhananjai/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/training/saving/saveable_object_util.py\", line 205, in saveable_objects_for_op\r\n    variable, \"\", name)\r\n  File \"/home/dhananjai/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/training/saving/saveable_object_util.py\", line 82, in __init__\r\n    self.handle_op = var.op.inputs[0]\r\n  File \"/home/dhananjai/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 2410, in __getitem__\r\n    return self._inputs[i]\r\nIndexError: list index out of range\r\n```\r\n\r\n**Code to reproduce the error:**\r\n```python\r\n# You will have to work in the mixnet directory in tensorflow/tpu/models/official/mnasnet/mixnet\r\nimport os.path as osp\r\nimport tempfile\r\nimport shutil\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom mixnet_builder import build_model\r\nfrom tensorflow.python.tools.freeze_graph import freeze_graph\r\n\r\n\r\n# A utility function to freeze rhe graph. It will be used later\r\ndef _simple_run_and_freeze(graph,\r\n                           output_name,\r\n                           frozen_model_file='',\r\n                           feed_dict={}):\r\n    model_dir = tempfile.mkdtemp()\r\n    graph_def_file = osp.join(model_dir, 'tf_graph.pbtxt')\r\n    checkpoint_file = osp.join(model_dir, 'tf_model.ckpt')\r\n\r\n    tf.reset_default_graph()\r\n    with graph.as_default() as g:\r\n        saver = tf.train.Saver()\r\n    with tf.Session(graph=graph) as sess:\r\n        # initialize\r\n        sess.run(tf.global_variables_initializer())\r\n        # run the result\r\n        fetch = graph.get_operation_by_name(output_name).outputs[0]\r\n        tf_result = sess.run(fetch, feed_dict=feed_dict)\r\n        # save graph definition somewhere\r\n        tf.train.write_graph(sess.graph, model_dir, graph_def_file)\r\n        # save the weights\r\n        saver.save(sess, checkpoint_file)\r\n\r\n    freeze_graph(input_graph=graph_def_file,\r\n                 input_saver=\"\",\r\n                 input_binary=False,\r\n                 input_checkpoint=checkpoint_file,\r\n                 output_node_names=output_name,\r\n                 restore_op_name=\"save/restore_all\",\r\n                 filename_tensor_name=\"save/Const:0\",\r\n                 output_graph=frozen_model_file,\r\n                 clear_devices=True,\r\n                 initializer_nodes=\"\")\r\n\r\n    if osp.exists(model_dir):\r\n        shutil.rmtree(model_dir)\r\n\r\n    return tf_result\r\n\r\n\r\nif __name__ == '__main__':\r\n    tf.logging.set_verbosity(tf.logging.ERROR)\r\n    BATCH_SIZE = 20\r\n    DIM1 = DIM2 = 224\r\n    # define a TF graph: input -> MixNet -> output\r\n    graph = tf.Graph()\r\n    with graph.as_default() as g:\r\n        inputs = tf.placeholder(tf.float32, shape=[BATCH_SIZE, DIM1, DIM2, 3],\r\n                                name='X')\r\n        logits, _ = build_model(inputs, 'mixnet-s', False)\r\n\r\n    output_name = logits.op.name\r\n    X = np.random.randn(BATCH_SIZE, DIM1, DIM2, 3)\r\n    frozen_model_file = 'mixnet_s_graph.pb'\r\n    out = _simple_run_and_freeze(graph,\r\n                                 output_name,\r\n                                 frozen_model_file,\r\n                                 feed_dict={'X:0': X})\r\n\r\n```\r\nThis seems something specific to [Mixnet](https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet/mixnet), but please let me know if you have any pointers.\r\n\r\nThanks in advance!", "Can you validate that the `feed_dict` is in the graph that is being passed to `freeze_graph` and that the input arrays didn't get renamed. If that doesn't work, can you provide the checkpoint and pbtxt files as well as visibility into the code that's in `build_model` (imported by `mixnet_builder`) so that we can reproduce this issue.", "> Can you validate that the `feed_dict` is in the graph that is being passed to `freeze_graph` and that the input arrays didn't get renamed. If that doesn't work, can you provide the checkpoint and pbtxt files as well as visibility into the code that's in `build_model` (imported by `mixnet_builder`) so that we can reproduce this issue.\r\n\r\nHi sir! The `feed_dict` is still there. I visualized the `.pb` file and the input and output are indeed `X` and `logits`, respectively. You can find the checkpoint for the error in the first post in this page [here](https://storage.googleapis.com/cloud-tpu-checkpoints/mixnet/mixnet-s.tar.gz). I trained on normal distribution to get around that with the following script: [train.py.zip](https://github.com/tensorflow/tensorflow/files/3774270/train.py.zip). The code for `build_model` can be found [here](https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet/mixnet). Looking forward to hearing from you. Thanks!\r\n", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32791\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32791\">No</a>\n"]}, {"number": 32790, "title": "ModuleNotFoundError: No module named 'tensorflow.examples.tutorials'", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (macOS Mojave 10.14.6):\r\n- TensorFlow installed from (source):\r\n- TensorFlow version: 2.0.0-rc1\r\n- Python version: Python 3.7.4\r\n- Installed using virtualenv? pip? conda?: pip\r\n\r\n**Describe the problem**\r\nI am trying to run a basic mnist classifier tutorial using tensorflow (without keras) and I am getting the following error:\r\nUpon running:\r\n`from tensorflow.examples.tutorials.mnist import input_data`\r\nI get the following error:\r\nModuleNotFoundError: No module named 'tensorflow.examples.tutorials'\r\n\r\nAny help of how to fix this issue is highly appreciated.\r\n\r\n", "comments": ["@hassanshallal I'm hitting the same problem, did you figure out a fix for this?", "I ended up using tfds:\r\n\r\n```\r\nimport tensorflow_datasets as tfds\r\n# Construct a tf.data.Dataset\r\ndataset = tfds.load(name=\"mnist\", split=tfds.Split.TRAIN)\r\n```\r\n", "do you install tf using docker?", "I didn't (and I don't think it matters)", "I'm also having exactly the same problem with Tensorflow 2.0.0\r\nI changed the TensorFlow version to 1.14.0 and I was able to import tensorflow.examples.tutorials.mnist\r\n\r\nI don't know why this is happening. There seems to be no change in the examples package code.", "  I think the Tensorflow 2.0.0 can't support to import tensorflow.examples.tutorials. Beacause I found the packages of tensorflow-core/examples had no the module named by turorials under my environment of anaconda installation. Finally, I changed the Tensorflow version to 1.14.0 and it did work! I'm very confused.", "I found a solution at this link: https://blog.csdn.net/weixin_41663570/article/details/102512468\r\n\r\nYou just need to download the lost files and copy them to the tensorflow-core/ examples.", "I met\r\n\r\n> I found a solution at this link: https://blog.csdn.net/weixin_41663570/article/details/102512468\r\n> \r\n> You just need to download the lost files and copy them to the tensorflow-core/ examples.\r\n\r\nI use this method to solve the problem", "This breaks a bunch of community code and tutorials.  Could the issue be reopened and fixed as part of the tensorflow or datasets packages?  A straight install of conda install tensorflow and conda install tensorflow-datasets didn't solve the issue.  \r\n\r\nWhy not conda/pip install tensorflow-examples?  Copying lost files manually doesn't seem like a good approach.  Examples should probably state that they use older version code.", "I don't get it, I buy a book called Deep Learning for Games, and know half of all the codes within the book doesn't with tensorflow 2.0, what is the use if you want to learn deep programming?????\r\nIdiots need to up their brains.", "I had the same problem with TF2. In the tutorial that I had the following codes:\r\n\r\n```\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nmnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=False)\r\n```\r\nIn in tf2, it turns out that **I don't need** that turorial package, just  write:\r\n\r\n`mnist = tf.keras.datasets.mnist`\r\n\r\n", "> I had the same problem with TF2. In the tutorial that I had the following codes:\r\n> \r\n> ```\r\n> from tensorflow.examples.tutorials.mnist import input_data\r\n> mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=False)\r\n> ```\r\n> \r\n> In in tf2, it turns out that **I don't need** that turorial package, just write:\r\n> \r\n> `mnist = tf.keras.datasets.mnist`\r\n\r\nbut because of this it doesnot act like a dataset. Now it act like a module", "For this perticular problem of not getting ModuleNotFoundError: No module named 'tensorflow.examples.tutorials, i followed the following way to solve this issue.. and it solved in my case.\r\nURL for Solution: https://www.youtube.com/watch?v=CH8uW3abI9A&feature=youtu.be", "> _Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template_\r\n> \r\n> **System information**\r\n> \r\n> * OS Platform and Distribution (macOS Mojave 10.14.6):\r\n> * TensorFlow installed from (source):\r\n> * TensorFlow version: 2.0.0-rc1\r\n> * Python version: Python 3.7.4\r\n> * Installed using virtualenv? pip? conda?: pip\r\n> \r\n> **Describe the problem**\r\n> I am trying to run a basic mnist classifier tutorial using tensorflow (without keras) and I am getting the following error:\r\n> Upon running:\r\n> `from tensorflow.examples.tutorials.mnist import input_data`\r\n> I get the following error:\r\n> ModuleNotFoundError: No module named 'tensorflow.examples.tutorials'\r\n> \r\n> Any help of how to fix this issue is highly appreciated.\r\n\r\nFor this perticular problem of not getting ModuleNotFoundError: No module named 'tensorflow.examples.tutorials, i followed the following way to solve this issue.. and it solved in my case.\r\nURL for Solution: https://www.youtube.com/watch?v=CH8uW3abI9A&feature=youtu.be", "> I found a solution at this link: https://blog.csdn.net/weixin_41663570/article/details/102512468\r\n> \r\n> You just need to download the lost files and copy them to the tensorflow-core/ examples.\r\n\r\ni use this way to copy the 'tutorials' file into the tensorflow_core/example file. but it seems not work. my ide sent me this error:ImportError: cannot import name 'tutorials' from 'tensorflow_core.examples'", "After cloning all git repo, and made it run ... had the following warning:\r\n\r\ntensorflow:\r\nread_data_sets (from tensorflow.examples.tutorials.mnist.input_data) \r\nis deprecated and will be removed in a future version.\r\n\r\nPlease use alternatives such as: tensorflow_datasets.load('mnist')\r\n", "OK :)\r\n\r\n\r\n\r\n\r\n------------------&nbsp;\u539f\u59cb\u90ae\u4ef6&nbsp;------------------\r\n\u53d1\u4ef6\u4eba: \"Nikolaos M Petridis\"<notifications@github.com&gt;; \r\n\u53d1\u9001\u65f6\u95f4: 2020\u5e744\u670822\u65e5(\u661f\u671f\u4e09) \u4e0a\u53489:18\r\n\u6536\u4ef6\u4eba: \"tensorflow/tensorflow\"<tensorflow@noreply.github.com&gt;; \r\n\u6284\u9001: \"1505253990\"<1505253990@qq.com&gt;; \"Comment\"<comment@noreply.github.com&gt;; \r\n\u4e3b\u9898: Re: [tensorflow/tensorflow] ModuleNotFoundError: No module named &#39;tensorflow.examples.tutorials&#39; (#32790)\r\n\r\n\r\n\r\n\r\n\r\n \r\nAfter cloning all git repo, and made it run ... had the following warning:\r\n \r\ntensorflow:\r\n read_data_sets (from tensorflow.examples.tutorials.mnist.input_data)\r\n is deprecated and will be removed in a future version.\r\n \r\nPlease use alternatives such as: tensorflow_datasets.load('mnist')\r\n \r\n\u2014\r\nYou are receiving this because you commented.\r\nReply to this email directly, view it on GitHub, or unsubscribe.", "**I downloaded the example/tutorials folder on github tensorflow, and then imported the input.py module by the following method:**\r\n`import sys`\r\n`sys.path.append(\"D:/anaconda3/envs/tensorflow2/Lib/site-packages/tensorflow_core/examples/tutorials/mnist/\")` # I download 'examples' file by myself in github tensorflow project\r\n`import input_data`\r\n\r\n**For deprecated API, The tf1 API be used under the tf2 env, by\uff1a**\r\n`import tensorflow.compat.v1 as tf`\r\n`tf.disable_v2_behavior()`", "Two options:\r\n\r\n1) use pip show tensorflow in somewhere you have already imported tensorflow; then you will find the location of your tensorflow under the name of Location; then go to the location; and then go to tensorflow_core under the location and then go to the examples; under the examples, download tutorials in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/tutorials; then copy the tutorials to the previous location.\r\n\r\n2) use the minist in keras that is included in tensorflow 2.0 to get MNIST dataset", "Just downgrade the version of tensorflow to 1.14 will resolve this, if you are using navigator then you can directly downgrade there if you have already created and installed the tensorflow environment. Just click the check option and mark for specific version to apply.", "> For this perticular problem of not getting ModuleNotFoundError: No module named 'tensorflow.examples.tutorials, i followed the following way to solve this issue.. and it solved in my case.\r\n> URL for Solution: https://www.youtube.com/watch?v=CH8uW3abI9A&feature=youtu.be\r\n\r\nThis worked! Thanks a ton!! ", "TF2 does not support from `tensorflow.examples.tutorials.mnist import input_data`\r\nInstead using older version of Tensorflow just go ahead with\r\n`pip install tensorflow-datasets` or `conda install tensorflow-datasets`\r\nUse the data set by following code\r\n`import tensorflow_datasets as datasets`\r\n`mnist = datasets.load(name='mnist')`", "Check the TensorFlow version. \r\n>>> tf.__version__\r\nThis may help:\r\nhttps://github.com/purosh96/NeuralNetwork/blob/main/MNIST_Tensorflow_1X_ShallowNN.ipynb", "> I found a solution at this link: https://blog.csdn.net/weixin_41663570/article/details/102512468\r\n> \r\n> You just need to download the lost files and copy them to the tensorflow-core/ examples.\r\n\r\nAre you sure of the link? It shows something in a different non-Latin script language."]}, {"number": 32789, "title": "Update version numbers for TensorFlow 1.15.0-rc2", "body": "Before merging this PR, please double check that it has correctly updated\n`core/public/version.h`, `tools/pip_package/setup.py`, and\n`tensorflow/tensorflow.bzl`. Also review the execution notes below:\n\n```\nMajor: 1 -> 1\nMinor: 15 -> 15\nPatch: 0 -> 0\n\nNo lingering old version strings \"1.15.0-rc1\" found in source directory \n\"tensorflow/\". Good.\nNo lingering old version strings \"1.15.0rc1\" found in source directory \n\"tensorflow/\". Good.\n```", "comments": []}, {"number": 32788, "title": "Fix exported_symbols.lds", "body": "This was a regression introduced in https://github.com/tensorflow/tensorflow/commit/4cc4425aee000b6358fbf219fa3dd64710b7aed4#diff-bcab48f9617a557923ec1df5bd9b840e that was breaking OSX builds of `libtensorflowlite_c.so`.", "comments": []}, {"number": 32787, "title": "Noisy loss in distributed training", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.12 & 1.13\r\n- Python version: 3.6.5\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nI am working on distributed learning in tensorflow through estimators API using below simple code template:\r\n\r\n```\r\nrunConfig = tf.estimator.RunConfig(session_config=config, \r\n                                model_dir=log_dir,\r\n                                save_summary_steps=1,\r\n                                save_checkpoints_steps=train_steps)\r\nestimator = tf.keras.estimator.model_to_estimator(model, model_dir=log_dir, config=runConfig) \r\ntrain_spec = tf.estimator.TrainSpec(input_fn=lambda: read_dataset(...), max_steps=...) \r\neval_spec = tf.estimator.EvalSpec(input_fn=lambda: read_dataset(...), start_delay_secs=1,\r\n                                throttle_secs=1, steps=None) \r\ntf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n```\r\n\r\nMy model is defined as a simple 2-layer LSTM model in keras, and read_dataset() functions return datasets that I use for training and validation purposes.  The training/validation data files  and model directory are set in a shared place available to all workers. The whole code is exactly the same for servers (ps, chief, and worker) except the task setting in TF_CONFIG.\r\nWhen I train model in single-worker configuration, the loss graph I see in tensorboard is gradually downward and reasonable.\r\n\r\n![single](https://user-images.githubusercontent.com/17579773/65541564-975a6a80-dedb-11e9-9a56-f77d2b5906a8.jpg)\r\n\r\nWhen using two machines of one chief and one worker, the total run time is less (as expected) but the loss graph is very noisy and higher than single-server case.\r\n\r\n![double](https://user-images.githubusercontent.com/17579773/65541692-d092da80-dedb-11e9-96bc-a3a3b0f0d92e.jpg)\r\n\r\nI expected to see the same performance in both cases, but it seems that training in the second server ruins the situation. Is there any special provision/setting/additional code that I should include in my work?", "comments": ["@shahriar49, Please provide the simple and standalone code to reproduce the issue reported here. Thanks!", "@gadagashwini Thanks for note, but the data that I am using for my work is not a public data, so its reproduction here may not be possible. Isn't there any possibility to discuss this issue by itself? I was thinking that maybe somebody has seen such a behavior before or there is a clear mistake in my approach.", "@shahriar49, \r\nWill it be possible to use the publicly available data and provide the code to investigate the root cause of the issue. Thanks! ", "@gadagashwini \r\nThank you very much for your attention! I tested a simpler code with mnist data and that noisy loss was not showing but I faced some other problems that I posted in another question (https://github.com/tensorflow/tensorflow/issues/32841) and I appreciate if you also have a look at it. I will investigate more my code and data and will return to this issue later.", "@shahriar49, Any update on reproducible code. Thanks!", "@gadagashwini \r\nSorry I was very busy and generating sample data and code takes a while. I will close this issue at this time and maybe return to it later.", "@gadagashwini \r\nBut I was actually thinking of something that may be a clue: The worker loads the saved model to start working on its batch, but if saved model was a bit old (because models are not saved on each batch), the chief's model may have been way better than worker model and therefore their combination will not be as good as the chief alone. What do you think about it? How such a problem can be avoided in asynchronous distributed learning?", "@shahriar49,\r\nCan you please confirm if you are using `Tensorflow Distribution Strategy` for Distributed Training. \r\n\r\nIf you are not using it, can you please try using  `Tensorflow Distribution Strategy` mentioned in [this link](https://www.tensorflow.org/guide/distributed_training) and let us know if your issue still persists. \r\n\r\nThanks!", "@rmothukuru \r\nNo I have not defined the distribution strategy, but I see the worker and chief are communicating and training the network together (and the global step per second is increased when having both chief and worker). How does it work without the strategy defined?\r\nAnd to be honest, I don't understand from the explanations in Tensorflow page which kind of strategy is suitable for my application (i.e. different machines, each have a CPU but may or may not have GPU). Can you tell me which strategy is suitable for this case?", "@shahriar49,\r\nFor your case, MultiWorkerMirroredStrategy explained in [this link](https://www.tensorflow.org/guide/distributed_training#multiworkermirroredstrategy) should work. Can you please check and let us know if it works. Thanks!", "@rmothukuru \r\nI used the below mnist_estimator implementation to check the strategy on Tensorflow 1.14. \r\n\r\n```\r\nimport json\r\nimport os\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\r\ndata_dir = '.\\\\MNIST_data'\r\nlog_dir = '.\\\\log_dist_y'\r\nbatch_size = 512\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\n\r\ndef keras_model(lr, decay):\r\n    \"\"\"Return a CNN Keras model\"\"\"\r\n    input_tensor = tf.keras.layers.Input(shape=(784,), name='input')\r\n\r\n    temp = tf.keras.layers.Reshape([28, 28, 1], name='input_image')(input_tensor)\r\n    for i, n_units in enumerate([32, 64]):\r\n        temp = tf.keras.layers.Conv2D(n_units, kernel_size=3, strides=(2, 2),\r\n                                      activation='relu', name='cnn'+str(i))(temp)\r\n        temp = tf.keras.layers.Dropout(0.5, name='dropout'+str(i))(temp)\r\n    temp = tf.keras.layers.GlobalAvgPool2D(name='average')(temp)\r\n    output = tf.keras.layers.Dense(10, activation='softmax', name='output')(temp)\r\n\r\n    model = tf.keras.models.Model(inputs=input_tensor, outputs=output)\r\n    optimizer = tf.keras.optimizers.Adam(lr=lr, decay=decay)\r\n    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\r\n    print(model.summary())\r\n    return model\r\n\r\n\r\ndef main():\r\n    \"\"\"Main function\"\"\"\r\n    data = read_data_sets(data_dir,\r\n                          one_hot=False,\r\n                          fake_data=False)\r\n    model = keras_model(lr=0.001, decay=0.001)\r\n    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n    config = tf.estimator.RunConfig(\r\n                train_distribute=strategy,\r\n                eval_distribute=strategy,\r\n                model_dir=log_dir,\r\n                save_summary_steps=1,\r\n                save_checkpoints_steps=100)\r\n    estimator = tf.keras.estimator.model_to_estimator(model, model_dir=log_dir, config=config)\r\n\r\n    train_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n                         x={'input': data.train.images},\r\n                         y=data.train.labels,\r\n                         num_epochs=None,   # run forever\r\n                         batch_size=batch_size,\r\n                         shuffle=True)\r\n    eval_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n                         x={'input': data.test.images},\r\n                         y=data.test.labels,\r\n                         num_epochs=1,\r\n                         shuffle=False)\r\n\r\n    train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn,\r\n                                        max_steps=100)\r\n    eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn,\r\n                                      #throttle_secs=1,\r\n                                      steps=None    # until the end of evaluation data\r\n                                      )\r\n\r\n    evaluate_result = tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n    print(\"Evaluation results:\")\r\n    for key in evaluate_result[0].keys():\r\n        print(\"   {}: {}\".format(key, evaluate_result[0][key]))\r\n\r\n```\r\n\r\nWithout setting TF_CONFIG and running the code, it does the job (of course, without distribution). But when I define the TF_CONFIG such as:\r\n```\r\nTF_CONFIG = {\r\n        'task': {\r\n            'type': 'chief',\r\n            'index': 0\r\n        },\r\n        'cluster': {\r\n            'chief': ['IP1:PORT1'],\r\n            'worker': ['IP2:PORT2'],\r\n            'ps': ['IP1:PORT3']\r\n        }\r\n    }\r\nos.environ['TF_CONFIG'] = json.dumps(TF_CONFIG)\r\n\r\n```\r\nand call the main(), I get below error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"mnist_estimator2.py\", line 84, in <module>\r\n    main()\r\n  File \"mnist_estimator2.py\", line 63, in main\r\n    evaluate_result = tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n  File \"D:\\Shahriar\\tf1.14_venv\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\training.py\", line 464, in train_and_evaluate\r\n    estimator, train_spec, eval_spec, _TrainingExecutor)\r\n  File \"D:\\Shahriar\\tf1.14_venv\\lib\\site-packages\\tensorflow\\python\\distribute\\estimator_training.py\", line 290, in train_and_evaluate\r\n    session_config=run_config.session_config)\r\n  File \"D:\\Shahriar\\tf1.14_venv\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_coordinator.py\", line 853, in run_distribute_coordinator\r\n    task_id, session_config, rpc_layer)\r\n  File \"D:\\Shahriar\\tf1.14_venv\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_coordinator.py\", line 360, in _run_single_worker\r\n    return worker_fn(strategy)\r\n  File \"D:\\Shahriar\\tf1.14_venv\\lib\\site-packages\\tensorflow\\python\\distribute\\estimator_training.py\", line 252, in _worker_fn\r\n    hooks=hooks)\r\n  File \"D:\\Shahriar\\tf1.14_venv\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 367, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"D:\\Shahriar\\tf1.14_venv\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 1156, in _train_model\r\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n  File \"D:\\Shahriar\\tf1.14_venv\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 1219, in _train_model_distributed\r\n    self._config._train_distribute, input_fn, hooks, saving_listeners)\r\n  File \"D:\\Shahriar\\tf1.14_venv\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 1255, in _actual_train_model_distributed\r\n    input_fn, ModeKeys.TRAIN, strategy)\r\n  File \"D:\\Shahriar\\tf1.14_venv\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 1009, in _get_iterator_from_input_fn\r\n    lambda input_context: self._call_input_fn(input_fn, mode,\r\n  File \"D:\\Shahriar\\tf1.14_venv\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\", line 774, in make_input_fn_iterator\r\n    input_fn, replication_mode)\r\n  File \"D:\\Shahriar\\tf1.14_venv\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\", line 406, in make_input_fn_iterator\r\n    input_fn, replication_mode=replication_mode)\r\n  File \"D:\\Shahriar\\tf1.14_venv\\lib\\site-packages\\tensorflow\\python\\distribute\\collective_all_reduce_strategy.py\", line 380, in _make_input_fn_iterator\r\n    self._container_strategy())\r\n  File \"D:\\Shahriar\\tf1.14_venv\\lib\\site-packages\\tensorflow\\python\\distribute\\input_lib.py\", line 558, in __init__\r\n    \"input_fn must return a tf.data.Dataset or a callable.\")\r\nValueError: input_fn must return a tf.data.Dataset or a callable.\r\nException ignored in: <bound method Server.__del__ of <tensorflow.python.training.server_lib.Server object at 0x000002AD4A35FF98>>\r\nTraceback (most recent call last):\r\n  File \"D:\\Shahriar\\tf1.14_venv\\lib\\site-packages\\tensorflow\\python\\training\\server_lib.py\", line 158, in __del__\r\nAttributeError: 'NoneType' object has no attribute 'UnimplementedError'\r\n\r\n```\r\n\r\nThis is very strange. What does it say?\r\n\r\n", "@tanzhenyu -- is this potentially related to the bug you fixed for losses in model_to_estimator?", "@karmel It shouldn't be related to that (that fixes both single training and dist training), this seems more related to estimator distributed training scheme.", "@shahriar49     The issue is that in order to use distribution strategies with Estimator the input fn needs to return a dataset.  However, numpy_input_fn doesn't return a dataset, it returns tensors instead.", "@isaprykin  OK that seems to be the case (although this lack of support was not expected). However, I am still confused do I need to put the strategy clause in my code or not. As I mentioned earlier, I see the worker and chief are communicating and training the network together (and the global step per second is increased when having both chief and worker) even without this clause.", "@shahriar49,\r\nPlease confirm if we can close this issue as the concern about Distributed Strategy is being addressed and as the problem about Distributed Training without using Distributed Strategy is being addressed in #32841. Thanks!", "@rmothukuru \r\nI am still waiting for an answer to clarify how the worker and chief are communicating and training the network together without strategy clause. If they work without it, what is the benefit of including this clause?", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32787\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32787\">No</a>\n"]}, {"number": 32786, "title": "Keras Layer.compute_output_shape calls `build` with wrong input shape", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0rc2\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nWhen using `Layer.compute_output_shape` or `Layer.compute_output_signature` on a Layer with a `build` function, the `build(input_shape=...)` argument is always set to None.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe `input_shape` should be set to the shape passed to `compute_output_shape`.\r\n\r\n**Code to reproduce the issue**\r\n``` python\r\nimport tensorflow as tf\r\n\r\nshape = (1, 2)\r\n\r\n\r\nclass MyLayer(tf.keras.layers.Layer):\r\n    def build(self, input_shape):\r\n        print(input_shape)\r\n        assert input_shape == shape\r\n\r\n    def call(self, inputs):\r\n        return inputs\r\n\r\n\r\nlayer = MyLayer()\r\nlayer.compute_output_shape(shape)\r\n```\r\n", "comments": ["I have tried on colab with TF version 2.0.0-rc2 , 2.0.0-dev20190924 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/4071357c68ee4ac7295198754d9489a0/untitled219.ipynb). Thanks!", "issue still exist in [nightly and in tensorflow 2.1](https://colab.sandbox.google.com/gist/Saduf2019/8f56913030e3bf31cd3344a9076d6b2d/32786.ipynb)", "@drasmuss I updated your code to build the layer correctly. With that update, code is not throwing any error. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/00a883bdcb7bcd40f9f1007176d99df4/32786.ipynb).\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nshape = (1, 2)\r\n\r\n\r\nclass MyLayer(tf.keras.layers.Layer):\r\n\r\n  def __init__(self, **kwargs):\r\n        super(MyLayer, self).__init__(**kwargs)\r\n        \r\n  def build(self, input_shape):\r\n      print(input_shape)\r\n      super(MyLayer, self).build(input_shape)\r\n      assert input_shape == shape\r\n\r\n  def call(self, inputs):\r\n      return inputs\r\n\r\n\r\nlayer = MyLayer()\r\nlayer.build(shape)\r\nlayer.compute_output_shape(shape)\r\n```\r\n\r\nPlease close the issue if this was resolved for you. Thanks!\r\n", "I don't think that solves the issue, it just avoids it by explicitly calling `build` ahead of time (with the correct shape). The problem that is being highlighted in this issue is that when `compute_output_shape` calls `build` automatically, it calls it with the wrong shape.\r\n\r\nIt says right in the docstring of `compute_output_shape` that it shouldn't be necessary to manually call `build`, so I don't think this is expected behaviour:\r\n> Computes the output shape of the layer.\r\n>\r\n>If the layer has not been built, this method will call build on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here.\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#compute_output_shape", "Re-assigning to Yanhui", "Hi @drasmuss there is a fix just submitted, and you may verify it with tf-nightly later. Close this for now, and feel free to reopen it if needed. Thanks.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32786\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32786\">No</a>\n"]}, {"number": 32785, "title": "Converting tf fashion mnist model with Supported Operations to TFLite breaks due to Operand Shape Mismatch", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac 10.12.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N/A\r\n- TensorFlow installed from (source or binary): binary (pip)\r\n- TensorFlow version (use command below):2.0.0-rc1\r\n- Python version: 3.7.1\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nWhen converting model to tflite a supported operation is attempted to be fused and a dimension error occurs.\r\n**Describe the expected behavior**\r\nWhen converting model to tflite the supported operation fuses correctly.\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nimport tensorflow.compat.v1 as tf\r\nimport numpy as np\r\ntf.disable_v2_behavior()\r\n\r\nfashion_mnist = tf.keras.datasets.fashion_mnist\r\n(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\r\nclass_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\r\n               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\r\ntrain_images = train_images/255.0\r\ntest_images = test_images/255.0\r\n\r\ninput = tf.placeholder(dtype=tf.float32, shape=(None,28,28), name=\"input\")\r\nreshape = tf.reshape(input, [tf.shape(input)[0],784])\r\nw1 = tf.Variable(tf.random_normal([128,784], dtype=tf.float32), name=\"w1\")\r\nb1 = tf.Variable(tf.random_normal([128], dtype=tf.float32), name=\"b1\")\r\nlayer_one_unbiased = tf.matmul(w1,tf.transpose(reshape))\r\nprint(layer_one_unbiased, b1)\r\nlayer_one_biased = tf.add(tf.transpose(layer_one_unbiased),b1)\r\nprint(layer_one_biased)\r\nactivated_layer_one = tf.nn.relu(layer_one_biased)\r\nw2 = tf.Variable(tf.random_normal([10,128]), name=\"w2\")\r\nb2 = tf.Variable(tf.random_normal([10], name=\"b2\"))\r\nprint(activated_layer_one)\r\nlayer_two_unbiased = tf.matmul(w2,tf.transpose(activated_layer_one))\r\nprint(tf.transpose(layer_two_unbiased), b2)\r\nprint(layer_two_unbiased, b2)\r\nlayer_two_biased = tf.add(tf.transpose(layer_two_unbiased), b2)\r\nprint(layer_two_biased)\r\npredictions = tf.nn.softmax(layer_two_biased, name=\"final\")\r\nlabels= tf.placeholder(dtype=tf.int32, shape=(None))\r\nloss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=layer_two_biased)\r\noptimizer = tf.train.AdamOptimizer()\r\ntrain = optimizer.minimize(loss)\r\nlabel_acc = tf.one_hot(labels,10)\r\naccuracy = 1 - tf.norm(tf.subtract(predictions,label_acc), axis=1)/2\r\naccuracy_averaged = tf.math.reduce_mean(accuracy)\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\nfeed_dict = {input: train_images, labels: train_labels}\r\nsess.run(train,feed_dict=feed_dict)\r\nfeed_dict2 = {input: test_images, labels: test_labels}\r\nprint(\"Test accuracy\", sess.run(accuracy_averaged, feed_dict = feed_dict2))\r\nprint(\"Training accuracy\",sess.run(accuracy_averaged, feed_dict = feed_dict))\r\nsaver =tf.train.Saver()\r\nsave_path = saver.save(sess, \"model.ckpt\")\r\ntf.io.write_graph(sess.graph, \"\", 'train.pbtxt')\r\n#converter = tf.lite.TFLiteConverter.from_session(sess, [input], [predictions])\r\n#converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n#tflite_model = converter.convert()\r\n#open('converted_model.tflite', \"wb\").write(tflite_model)\r\nsess.close()\r\n\r\n# Above code is the code used to generate the model\r\nfreeze_graph.py is then run on the output of the above code.\r\n# The below code is used to convert\r\nimport tensorflow.compat.v1 as tf\r\nimport numpy as np\r\ntf.disable_v2_behavior()\r\ngraph_def_file = \"freeze_graph.pbtxt\"\r\ninput_arrays = [\"input\"]\r\noutput_arrays = [\"final\"]\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(\r\n  graph_def_file, input_arrays, output_arrays)\r\ntflite_model = converter.convert()\r\nopen(\"converted_model.tflite\", \"wb\").write(tflite_model)\t\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0924 12:49:42.307250 140736348050368 deprecation.py:323] From /Users/t.capes/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nnon-resource variables are not supported in the long term\r\n2019-09-24 12:49:42.308630: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-09-24 12:49:42.324658: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fb5c10b8740 executing computations on platform Host. Devices:\r\n2019-09-24 12:49:42.324684: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\n2019-09-24 12:49:42.336278: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2019-09-24 12:49:42.336378: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2019-09-24 12:49:42.344845: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize\r\n2019-09-24 12:49:42.344864: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 27 nodes (-4), 27 edges (-4), time = 3.984ms.\r\n2019-09-24 12:49:42.344871: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 27 nodes (0), 27 edges (0), time = 0.999ms.\r\nTraceback (most recent call last):\r\n  File \"conversion.py\", line 11, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"/Users/t.capes/miniconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py\", line 983, in convert\r\n    **converter_kwargs)\r\n  File \"/Users/t.capes/miniconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/convert.py\", line 449, in toco_convert_impl\r\n    enable_mlir_converter=enable_mlir_converter)\r\n  File \"/Users/t.capes/miniconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/convert.py\", line 200, in toco_convert_protos\r\n    raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: See console for info.\r\n2019-09-24 12:49:44.232035: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 14 operators, 27 arrays (0 quantized)\r\n2019-09-24 12:49:44.232297: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 14 operators, 27 arrays (0 quantized)\r\n2019-09-24 12:49:44.232628: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 13 operators, 26 arrays (0 quantized)\r\n2019-09-24 12:49:44.232720: F tensorflow/lite/toco/graph_transformations/fuse_binary_into_preceding_affine.cc:62] Operand shape mismatch.\r\nFatal Python error: Aborted\r\n\r\nCurrent thread 0x00007fffbc0853c0 (most recent call first):\r\n  File \"/Users/t.capes/miniconda3/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 52 in execute\r\n  File \"/Users/t.capes/miniconda3/lib/python3.7/site-packages/absl/app.py\", line 251 in _run_main\r\n  File \"/Users/t.capes/miniconda3/lib/python3.7/site-packages/absl/app.py\", line 300 in run\r\n  File \"/Users/t.capes/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/platform/app.py\", line 40 in run\r\n  File \"/Users/t.capes/miniconda3/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 89 in main\r\n  File \"/Users/t.capes/miniconda3/bin/toco_from_protos\", line 10 in <module>", "comments": ["I am aware there is a bug in the model training which results in poor accuracy. I still want to be able to convert this model and it appears to use only supported operations once freeze_graph is used to replace tf.Variable with tf.constant.", "It is worth noting that this model will successfully convert if I exclude the TFLITE_BUILTINS and use only SELECT_TF_OPS but according to the documentation this should work with TFLITE_BUILTINS as every op is compatible.  This also breaks if I use both TFLITE_BUILTINS and SELECT_TF_OPS in that order.", "Hi, thanks for posting your run script and conversion  script. Could you attach the model you get from freeze_graph.py to this issue? I'm having trouble reproducing your exact error. Thanks!", "Changed name from .pbtxt to .txt because of unsupported file format. Uploaded.\r\n[freeze_graph.txt](https://github.com/tensorflow/tensorflow/files/3664520/freeze_graph.txt)\r\n", "Hi I believe the problem is that your bias vectors were one dimension row vectors instead of true \"column\" vectors that have to be specified as two dimensional vectors of size (128, 1) for b1 and (10, 1) for b2. I made this change and re-ran your script, converted with freeze_graph, and then the model was able to successfully convert with TF Lite. Here is the code:\r\n\r\nhttps://gist.github.com/talumbau/042ee5b278b4f73229333f6baf427bbe\r\n\r\n", "Looks good to me. Closing issue. Thanks.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32785\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32785\">No</a>\n"]}, {"number": 32784, "title": "module 'tensorflow_core._api.v2.nn' has no attribute 'rnn_cell'", "body": "File \"F:\\AI\\RedditChatBot\\nmt-chatbot-master\\nmt\\gnmt_model.py\", line 262, in <module>\r\n    class GNMTAttentionMultiCell(tf.nn.rnn_cell.MultiRNNCell):\r\nAttributeError: module 'tensorflow_core._api.v2.nn' has no attribute 'rnn_cell'", "comments": ["Can you please also post a code sample (using proper Markdown formatting so it's easy to read) that is a minimal reproduction example?", "@AquaDef, you may have `Tensorflow 2.0` installed, but the code you used was based on Tensorflow 1. xx. I would switch to install a lower version of TensorFlow. ", "I'm trying TensorFlow 2.0.\r\nI got same error.\r\n\r\nI used data is download from\r\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/00360/AirQualityUCI.zip\r\n\r\n```python\r\nwith ZipFile(AirQualityUCI.zip) as z:\r\n    with z.open('AirQualityUCI.xlsx') as f:\r\n        air_quality = pd.read_excel(\r\n            f,\r\n            index_col=0, parse_dates={'DateTime': [0, 1]},\r\n            na_values=[-200.0],\r\n            convert_float=False\r\n        )\r\n\r\ntarget_columns = ['T', 'AH', 'PT08.S1(CO)', 'PT08.S2(NMHC)',\r\n                  'PT08.S3(NOx)', 'PT08.S4(NO2)']\r\nair_quality = air_quality[target_columns]\r\n\r\ndataset = TimeSeriesDataSet(air_quality)\r\ntrain_dataset = dataset[dataset.times.year < 2005]\r\ntest_dataset = dataset[dataset.times.year >= 2005]\r\n\r\nsess = tf.compat.v1.InteractiveSession()\r\ntf.compat.v1.set_random_seed(12345)\r\nSERIES_LENGTH = 72\r\nFEATURE_COUNT = dataset.feature_count\r\n\r\ntf.compat.v1.disable_eager_execution()\r\nx = tf.compat.v1.placeholder(tf.float32, [None, SERIES_LENGTH, FEATURE_COUNT])\r\ny = tf.compat.v1.placeholder(tf.float32, [None, FEATURE_COUNT])\r\n\r\ncell = tf.nn.rnn_cell.BasicRNNCell(20)\r\n```", "There is no [`tf.nn.rnn_cell` in 2.0 API](https://www.tensorflow.org/api_docs/python/tf/nn/). \r\n\r\nPlease consult https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/compatibility and run the upgrade script to update your code to work on 2.0.", "> There is no [`tf.nn.rnn_cell` in 2.0 API](https://www.tensorflow.org/api_docs/python/tf/nn/).\r\n> \r\n> Please consult https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/compatibility and run the upgrade script to update your code to work on 2.0.\r\n\r\nThank you, I try that.", "@AquaDef ,@mitsuhisaT \r\nDid you get a chance to look @mihaimaruseac suggestion . Please close the issue if it was resolved already. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "> > There is no [`tf.nn.rnn_cell` in 2.0 API](https://www.tensorflow.org/api_docs/python/tf/nn/).\r\n> > Please consult https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/compatibility and run the upgrade script to update your code to work on 2.0.\r\n> \r\n> Thank you, I try that.\r\n\r\nHi Buddy,\r\n\r\ndoes it work?", "I'm trying TensorFlow 2.2.\r\nI got same error.\r\ni can't fix it  from link https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/compatibility\r\n\r\nplease help me\r\n", "TF 2.2 does not have `tensorflow_core`. Please check output of `pip lists`, it's likely you have packages predating this release.\r\n\r\nPlease open new issue, fill in issue template."]}, {"number": 32783, "title": "GELU activation Functions?", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): TF 2.0.\r\n\r\n- Are you willing to contribute it (Yes/No):\r\nYes, pending review, I would be happy to write this in.\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCreate a high-level API implementing the GELU activation function.  This was the activation function used in BERT[0], and the GELU authors prove its success on  a number of benchmarks like CIFAR-10[1].\r\n\r\nGelu is not mentioned in a search on tensorflow.org[2]. \r\n\r\n[0] \"We use a gelu activation (Hendrycks and Gimpel, 2016) rather than the standard relu, following OpenAI GPT.\" https://arxiv.org/pdf/1810.04805.pdf\r\n[1]\"Ultimately, the GELU obtains a median error rate of 7.89%, the ReLU obtains 8.16%, and the ELU obtains 8.41%.\"  https://arxiv.org/pdf/1606.08415.pdf\r\n[2] https://www.tensorflow.org/s/results?q=gelu\r\n\r\n**Will this change the current api? How?**\r\nYes, this would expand the API. A good way to implement it would be adding to activations.py [3] The implementation used in BERT, which may need some modification, is here [4].\r\n\r\n[3] https://github.com/tensorflow/tensorflow/blob/00fad90125b18b80fe054de1055770cfb8fe4ba3/tensorflow/python/keras/activations.py\r\n[4] https://github.com/google-research/bert/blob/master/modeling.py#L264\r\n\r\n**Who will benefit with this feature?**\r\nAll developers writing custom layers, particularly those using keras.layers.Dense. \r\n\r\nAfter implementing, we will not have to worry about any optimization problems which may come from mixing activation functions training on top of BERT models.\r\n\r\n**Any Other info.**\r\n", "comments": ["https://github.com/hendrycks/GELUs\r\n\r\n```\r\nB = tf.keras.backend\r\n\r\n@tf.function(experimental_relax_shapes=True)\r\ndef gelu(x):\r\n    return 0.5 * x * (1 + B.tanh(x * 0.7978845608 * (1 + 0.044715 * x * x)))\r\n\r\n@tf.function\r\ndef fast_gelu(x):\r\n    return B.hard_sigmoid(1.702 * x) * x\r\n```", "Hello,\r\n\r\nGelu has about 100 hits on ArXiv and has been used in at least one popular model. Given that it would be easy to add and easy to maintain, we can definitely add it to `activations.py`. Feel free to open a PR and assign me as a reviewer.\r\n", "@bwindsor22 Hi, Brad. Gelu activation has been implemented in Addons, you might be interested in it: please check out https://github.com/tensorflow/addons/blob/master/tensorflow_addons/activations/README.md\r\n\r\n\r\n> Feel free to open a PR and assign me as a reviewer.\r\n\r\n@fchollet @karmel  Hi, Fran\u00e7ois, Karmel. Is it time to port gelu from addons to tf-core? I believe Tzu-Wei @WindQAQ would like to help it.", "@facaiy sounds good, we should port the Addons Gelu to core. Please open a PR.", "Tagging in @karmel and @seanpmorgan, RE: graduation from `addons` to core.", "cc @Windqaq who will be submitting the PR in ~two weeks after exams. There was some discussion about whether or not we should use the C++ implementation or just composite python ops, but the reported speed up looks to be ~3x so IMO we should use the op implementation (which is pretty straight forward):\r\nhttps://github.com/tensorflow/addons/blob/master/tensorflow_addons/custom_ops/activations/cc/kernels/gelu_op.h#L37\r\n\r\nRelevant issue:\r\nhttps://github.com/tensorflow/addons/issues/550", "Just observing ALBERT followed the GELU trend too. Can't wait for the speedy implementation to move, thanks a million folks!\r\nhttps://arxiv.org/pdf/1909.11942.pdf", "Gelu is in Tensorflow now: https://github.com/tensorflow/tensorflow/pull/41178. Can we close this?", "Looks good to me! Thanks!"]}, {"number": 32782, "title": "Building TF 2.0 from source returns 1.14 wheel", "body": "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.0 (master branch)\r\n- Python version: 3.6.8\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n- CUDA/cuDNN version: 10.1/7.6.3\r\n- GPU model and memory: RTX2080TI\r\n\r\nI am trying to build tensorflow 2.0 with GPU support. During the ./configure I enable CUDA (the locations are properly found). Afterwards I start the build with:\r\n\r\nbazel build --config=v2 //tensorflow/tools/pip_package:build_pip_package\r\n\r\nI had to link python to python3, since Ubuntu 18 has no python 2 and bazel has problems, when no python link is available. The build finishes without any fails.\r\n\r\n./bazel-bin/tensorflow/tools/pip_package/build_pip_package --nightly_flag /tmp/tensorflow_pkg\r\n\r\nAfter building the wheel with the above line I can only build tensorflow 1.14\r\n\r\n$ ls /tmp/tensorflow_pkg/\r\ntf_nightly-1.14.0-cp36-cp36m-linux_x86_64.whl\r\n\r\nHave been trying it multiple times now, even on a clean and fresh Ubuntu 18 setup.\r\n\r\nCheers\r\n", "comments": ["This is because you are compiling the master branch which is the stable version of `tensorflow` version `1.14`. Your gonna need to checkout to the `r2.0` branch and recompile.\r\n\r\nRun `git checkout r2.0` inside the tensorflow repo.", "We will change version number in master after 2.0 is released.", "Thanks to both of you. Expected the master to be the nightly TF2.0 version. With the correct branch everything worked out fine.\r\n\r\nCheers"]}, {"number": 32781, "title": "GPU Conv Fusion Code is disabled in grappler remapper.cc", "body": "The grappler remapper optimizer disabled the ability to fuse conv related op in GPU.\r\n\r\nSince related code is inside \r\n#ifndef INTEL_MKL\r\n#endif \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/optimizers/remapper.cc", "comments": ["@OuHangKresnik, Please elaborate the issue with context. Thanks!", "The context is then using GPU, this optimization seems to be not happening.\r\n\r\nNo FusedConv2D exists while on cpu, it will show up.", "@OuHangKresnik, Will it be possible to provide the sample standalone code reproduce the issue reported here. Thanks!", "With any network that contains Convolution and BatchNorm stuff.\r\n\r\nconfig = tf.ConfigProto(device_count = {'GPU': 0})\r\n\r\nWith above configuration, the _FusedConv2D operator will show up (in CPU mode).\r\nWithout it, then _FusedConv2D won't show up (in GPU mode)\r\n\r\n", "@OuHangKresnik Sorry for the late response. Is this still an issue? Can you please try latest version and let us know whether the issue persists there also. Thanks!", "This is a stale issue. Please check the issue with latest TensorFlow. If the issue still persists in the newer version of TF, please feel free to reopen it by providing details about the issue and a standalone code to reproduce the issue. Thanks!", "I got same problem. \r\nI'm testing conv+bias_add+relu case, which should be fused as shown in\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/core/grappler/optimizers/remapper_test.cc#L294\r\n\r\nHowever, fused ops only exists when using CPU, but disappear when using GPU. Seems like it doesn't work when using GPU ?\r\n[test_conv.zip](https://github.com/tensorflow/tensorflow/files/4034675/test_conv.zip)\r\n"]}, {"number": 32780, "title": "How to use Tensorflowlite SDK with iOS 8", "body": "Hello @petewarden ,\r\n\r\nI want to support tensorlowlite to my existing project which is having iOS 8 min target. But I am getting error while including it using pod targeted OS version does not support use of thread local variables in __ZNK6tflite13eigen_support12_GLOBAL__N_122EigenThreadPoolWrapper15CurrentThreadIdEv for architecture\r\n\r\nCan anyone help me how to fix this issue?\r\nAny help is appreciable.\r\n\r\nThanks\r\n", "comments": ["@miaout17 : Can you please help me to achieve this? ", "We do not officially support iOS 8, but you could try building your own `TensorFlowLiteC` framework with minimum OS version as 8.0.\r\n\r\n* In `tensorflow/lite/experimental/ios/ios.bzl`, change the `TFL_MINIMUM_OS_VERSION` to `8.0`.\r\n* In `tensorflow/lite/experimental/ios/BUILD`:\r\n  * Comment out the `gpu:metal_delegate.h` from the header list.\r\n  * Comment out the `\"Metal\"` line from the `TensorFlowLiteC` target.\r\n  * Comment out the `gpu:metal_delegate` dependency from the `tensorflow_lite_c` target.\r\n\r\nOnce you've done all of this, try building the `TensorFlowLiteC_framework` target with the following command:\r\n\r\n```sh\r\nbazel build -c opt --config=ios_fat //tensorflow/lite/experimental/ios:TensorFlowLiteC_framework\r\n```\r\n\r\nYou could then try using this framework following [this guide](https://www.tensorflow.org/lite/guide/build_ios#use_in_your_own_application).\r\n\r\nLet me know if this works for you.", "@yyoon \r\n\r\nThanks for your support. I will check and get back to you on this.\r\n", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 32779, "title": "Parallel Mapping of Dataset Does Not Ensure Same Random Numbers", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES\r\n- OS Platform and Distribution: Windows10\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 1.15.0-dev20190812 \r\n- Python version: 3.7 (64-bit)\r\n\r\n**Describe the current behavior**\r\nI am doing data augmentations using Dataset map method. To ensure that the images and the labels (labels are images, too) are transformed in the same way (e.g. the same rotation, zooming, etc.), I set the random seeds at the same value.\r\n\r\nIn each of the following test, I run the test program for an adequately long time and use `tf.assert_equal` to capture the bug. If I do not set parallel in the map method, it goes well and augmentation yields the same random numbers. But if I set `num_parallel_calls=4` or `num_parallel_calls=tf.data.experimental.AUTOTUNE`, the assertion failure is soon triggered. This inconsistency does not happen for all samples but happens brokenly.\r\n\r\n**Describe the expected behavior**\r\nFix the RNG bug in parallel mode maybe?\r\n", "comments": ["@BinyanHu,\r\nPlease provide the simple standalone code to reproduce the issue. Thanks!", "@BinyanHu,\r\nAny update on reproducible code. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 32778, "title": "Incorrect number of weights in custom layer containing keras layer", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 9.0, 7.1\r\n- GPU model and memory: Quadro\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n```\r\nclass CustomLayer(tf.keras.layers.Layer):\r\n    def __init__(self, dims, **kwargs):\r\n        super(CustomLayer, self).__init__(**kwargs)\r\n        self.dims = dims\r\n        self.dense = tf.keras.layers.Dense(units=self.dims)\r\n        \r\n    def call(self, x, mask=None):\r\n        return self.dense(x)\r\n    \r\n    def get_config(self):\r\n        return {\"dims\": self.dims}\r\n    \r\ninp = tf.keras.layers.Input(shape=(10,))\r\nx = CustomLayer(32)(inp)\r\nmodel = tf.keras.models.Model(inp, x)\r\nprint(model.summary())\r\n```\r\n\r\nThe summary contains no weights:\r\n```\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_5 (InputLayer)         (None, 10)                0         \r\n_________________________________________________________________\r\ncustom_layer_2 (CustomLayer) (None, 32)                0         \r\n=================================================================\r\nTotal params: 0\r\nTrainable params: 0\r\nNon-trainable params: 0\r\n```\r\n\r\nIf I try to train, it doesn't seem to do anything\r\n```\r\nmodel.compile(\"sgd\",\"mse\")\r\nX,Y = np.random.rand(1000,10), np.zeros((1000, 32))\r\nmodel.fit(X,Y, epochs=20)\r\n```\r\n```\r\nEpoch 1/20\r\n1000/1000 [==============================] - 0s 28us/step - loss: 0.1625\r\nEpoch 2/20\r\n1000/1000 [==============================] - 0s 25us/step - loss: 0.1625\r\nEpoch 3/20\r\n1000/1000 [==============================] - 0s 29us/step - loss: 0.1625\r\nEpoch 4/20\r\n1000/1000 [==============================] - 0s 28us/step - loss: 0.1625\r\nEpoch 5/20\r\n1000/1000 [==============================] - 0s 24us/step - loss: 0.1625\r\nEpoch 6/20\r\n1000/1000 [==============================] - 0s 24us/step - loss: 0.1625\r\nEpoch 7/20\r\n1000/1000 [==============================] - 0s 26us/step - loss: 0.1625\r\nEpoch 8/20\r\n1000/1000 [==============================] - 0s 22us/step - loss: 0.1625\r\nEpoch 9/20\r\n1000/1000 [==============================] - 0s 22us/step - loss: 0.1625\r\nEpoch 10/20\r\n1000/1000 [==============================] - 0s 25us/step - loss: 0.1625\r\nEpoch 11/20\r\n1000/1000 [==============================] - 0s 22us/step - loss: 0.1625\r\nEpoch 12/20\r\n1000/1000 [==============================] - 0s 25us/step - loss: 0.1625\r\nEpoch 13/20\r\n1000/1000 [==============================] - 0s 23us/step - loss: 0.1625\r\nEpoch 14/20\r\n1000/1000 [==============================] - 0s 26us/step - loss: 0.1625\r\nEpoch 15/20\r\n1000/1000 [==============================] - 0s 23us/step - loss: 0.1625\r\nEpoch 16/20\r\n1000/1000 [==============================] - 0s 26us/step - loss: 0.1625\r\nEpoch 17/20\r\n1000/1000 [==============================] - 0s 23us/step - loss: 0.1625\r\nEpoch 18/20\r\n1000/1000 [==============================] - 0s 23us/step - loss: 0.1625\r\nEpoch 19/20\r\n1000/1000 [==============================] - 0s 23us/step - loss: 0.1625\r\nEpoch 20/20\r\n1000/1000 [==============================] - 0s 23us/step - loss: 0.1625\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nI would like to use Keras layers within the custom layer, and have them trainable.\r\n", "comments": ["Is it possible for you to try latest TF versions (`!pip install tensorflow-gpu==1.15.0-rc1`)and let us know whether the issue persists? There were lots of performance improvements in the latest versions. Thanks!", "Seems to work appropriately in 1.15. Thanks!"]}, {"number": 32777, "title": "Do not ignore the `shuffle` parameter when workers=0", "body": "Fixes the bug previously reported in https://github.com/keras-team/keras/issues/12927.", "comments": ["I am not certain whether a `ValueError` or a `UserWarning` is more appropriate when a generator can't be shuffled.", "Hi @alexeyr, the code you are currently fixing only affects the v1 training behavior for Keras. The new code for training lives in training_v2.py and training_utils_v2.py.\r\n\r\nCould u check whether your current PR is still applicable for the latest TF release?", "Closing this PR as it is not active, @alexeyr feel free to reopen this if you have address my previous question/comment."]}, {"number": 32776, "title": "Sym-links in nvcc C compiler path causes \"undeclared inclusion(s) in rule '//tensorflow/core:lib_hash_crc32c_accelerate_internal'\"", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Redhat Enterprise Linux 7.x\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.14.0\r\n- Python version: 3.7.4\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): 4.9.2\r\n- CUDA/cuDNN version: 10.0.130/7.4.2.24\r\n- GPU model and memory: Tesla P100\r\n\r\n**Describe the problem**\r\n\r\nWe have compilers installed in `/shared/ucl/apps/gcc/4.9.2`.  Unfortunately `/shared` is a sym-link to `/lustre/shared`.  Attempting to build Tensorflow with Cuda support results in this sym-link being inconsistently de-referenced meaning that some rules refer to the `/shared` location while the system include paths refer to `/lustre/shared`.  This seems to be an issue only with the C compiler used by nvcc, not with the one used to build the rest of the code.\r\n\r\nTelling the configure script to use `/lustre/shared/ucl/apps/gcc/4.9.2/bin/gcc` as the nvcc c compiler *works around the problem* but is less than ideal as this path is different on different clusters. It culd also be an issue if a user on a mult-user system uses their own install of GCC in `/home`where `/home` is a sym-link. This appears to be a bug in the way the configure script and/or bazel deals with sym-links.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nThe script at https://github.com/owainkenwayucl/install-scripts/blob/master/scripts/tensorflow/tensorflow-1.14.0-py37-gpu_install\r\n\r\nBut effectively:\r\n\r\n```\r\nCONFIG_OPTS=\"--config=cuda --copt=-march=broadwell --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0 --copt=-O3\"\r\nexport TF_CUDA_PATHS=/shared/ucl/apps/cuda/10.0.130/gnu-4.9.2,/shared/ucl/apps/cudnn/7.4.2.24/10.0/cuda\r\n./configure\r\n\r\nbazel build --verbose_failures $CONFIG_OPTS //tensorflow/tools/pip_package:build_pip_package \r\n```\r\n\r\nAnswering \"yes\" to build with cuda and selecting defaults for compute capability, nccl etc.\r\n\r\n", "comments": ["Actions I took at configure stage:\r\n\r\n```\r\nYou have bazel 0.24.1 installed.\r\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]: \r\nXLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: \r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: \r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: \r\nNo TensorRT support will be enabled for TensorFlow.\r\n\r\nFound CUDA 10.0 in:\r\n    /shared/ucl/apps/cuda/10.0.130/gnu-4.9.2/lib64\r\n    /shared/ucl/apps/cuda/10.0.130/gnu-4.9.2/include\r\nFound cuDNN 7 in:\r\n    /shared/ucl/apps/cudnn/7.4.2.24/10.0/cuda/lib64\r\n    /shared/ucl/apps/cudnn/7.4.2.24/10.0/cuda/include\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: \r\n\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]: \r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /shared/ucl/apps/gcc/4.9.2/bin/gcc]: \r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]:\r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: \r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]:\r\n```", "Build failure:\r\n```\r\nERROR: /dev/shm/tensorflow/tmp.6ooBek4IW8/tensorflow/tensorflow/core/BUILD:2499:1: undeclared inclusion(s) in rule '//tensorflow/core:lib_hash_crc32c_accelerate_internal':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/core/lib/hash/crc32c_accelerate.cc':\r\n  '/shared/ucl/apps/gcc/4.9.2/lib/gcc/x86_64-unknown-linux-gnu/4.9.2/include/stddef.h'\r\n  '/shared/ucl/apps/gcc/4.9.2/lib/gcc/x86_64-unknown-linux-gnu/4.9.2/include/stdint.h'\r\n  '/shared/ucl/apps/gcc/4.9.2/lib/gcc/x86_64-unknown-linux-gnu/4.9.2/include/nmmintrin.h'\r\n  '/shared/ucl/apps/gcc/4.9.2/lib/gcc/x86_64-unknown-linux-gnu/4.9.2/include/smmintrin.h'\r\n  '/shared/ucl/apps/gcc/4.9.2/lib/gcc/x86_64-unknown-linux-gnu/4.9.2/include/tmmintrin.h'\r\n  '/shared/ucl/apps/gcc/4.9.2/lib/gcc/x86_64-unknown-linux-gnu/4.9.2/include/pmmintrin.h'\r\n  '/shared/ucl/apps/gcc/4.9.2/lib/gcc/x86_64-unknown-linux-gnu/4.9.2/include/emmintrin.h'\r\n  '/shared/ucl/apps/gcc/4.9.2/lib/gcc/x86_64-unknown-linux-gnu/4.9.2/include/xmmintrin.h'\r\n  '/shared/ucl/apps/gcc/4.9.2/lib/gcc/x86_64-unknown-linux-gnu/4.9.2/include/mmintrin.h'\r\n  '/shared/ucl/apps/gcc/4.9.2/lib/gcc/x86_64-unknown-linux-gnu/4.9.2/include/mm_malloc.h'\r\n  '/shared/ucl/apps/gcc/4.9.2/lib/gcc/x86_64-unknown-linux-gnu/4.9.2/include/popcntintrin.h'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 108.730s, Critical Path: 1.51s\r\nINFO: 42 processes: 42 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "If I change the answer at `Please specify which gcc should be used by nvcc as the host compiler. [Default is /shared/ucl/apps/gcc/4.9.2/bin/gcc]: ` to `/lustre/shared/ucl/apps/gcc/4.9.2/bin/gcc` then Tensorflow builds successfully.", "Can we close this issue since the query is been resolved.Thanks!", "It's not been resolved.  There's a workaround which I found in this case but that's only because I've a great deal of familiarity with the system.  \r\n\r\nIf I were a user of a system where I were trying to install things in my home directory and i did not know how things were set up underneath it would cause problems.  Googling the error finds other people stuck at this point (e.g. https://github.com/bazelbuild/bazel/issues/4573 where it's clear that the path of the user's conda tools is likely to be under a sym-link) so it should really be resolved properly.", "Thanks for your detailed comments! Just to be clear, you think this would be solved if our `./configure` script resolved symlinks, right?", "I think so.  However it works it out, it needs to be consistent with how Bazel writes its rules (which I'm really not an expert in!).", "Having looked at `configure.py` I believe it's as simple as adding:\r\n\r\n```python\r\ngcc_host_compiler_path = os.path.realpath(gcc_host_compiler_path)\r\n```\r\n\r\nat line 842 in the current version, and 831 in 1.14.0.\r\n\r\nI'm happy to do this and do a pull request if you prefer?", "Yes, that would be very helpful, thank you!\n\nOn Fri, Oct 4, 2019 at 6:57 AM Dr Owain Kenway <notifications@github.com>\nwrote:\n\n> Having looked at configure.py I believe it's as simple as adding:\n>\n> gcc_host_compiler_path = os.path.realpath(gcc_host_compiler_path)\n>\n> at line 842.\n>\n> I'm happy to do this and do a pull request if you prefer?\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/32776?email_source=notifications&email_token=AHXWEQBFMDOXYSA35UDXGK3QM5DTHA5CNFSM4IZ6XHP2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEALWP3A#issuecomment-538404844>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHXWEQH4HSLPYQ2FOZLRZX3QM5DTHANCNFSM4IZ6XHPQ>\n> .\n>\n", "I met this problem too. After I used the patch, I still faced a similar problem with CUDA includes path.", "Ok, that's weird - our CUDA includes are also under the sym-link and we don't get that problem.  What error do you get?", "> Ok, that's weird - our CUDA includes are also under the sym-link and we don't get that problem. What error do you get?\r\n\r\n```\r\nERROR: /cache/home/jz748/.cache/bazel/_bazel_jz748/9a5804611367d5a3465d7ad3decb2864/external/nccl_archive/BUILD.bazel:67:1: undeclared inclusion(s) in rule '@nccl_archive//:device_lib':\r\nthis rule is missing dependency declarations for the following files included by 'external/nccl_archive/collectives/device/max_all_gather.cu.cc':       '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/cuda_runtime.h'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/crt/host_config.h'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/builtin_types.h'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/device_types.h'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/crt/host_defines.h'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/driver_types.h'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/vector_types.h'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/surface_types.h'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/texture_types.h'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/library_types.h'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/channel_descriptor.h'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/cuda_runtime_api.h'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/cuda_device_runtime_api.h'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/driver_functions.h'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/vector_functions.h'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/vector_functions.hpp'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/crt/common_functions.h'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/crt/math_functions.h'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/crt/math_functions.hpp'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/cuda_surface_types.h'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/cuda_texture_types.h'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/crt/device_functions.h'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/crt/device_functions.hpp'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/device_atomic_functions.h'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/device_atomic_functions.hpp'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/crt/device_double_functions.h'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/crt/device_double_functions.hpp'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/sm_20_atomic_functions.h'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/sm_20_atomic_functions.hpp'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/sm_32_atomic_functions.h'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/sm_32_atomic_functions.hpp'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/sm_35_atomic_functions.h'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/sm_60_atomic_functions.h'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/sm_60_atomic_functions.hpp'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/sm_20_intrinsics.h'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/sm_20_intrinsics.hpp'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/sm_30_intrinsics.h'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/sm_30_intrinsics.hpp'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/sm_32_intrinsics.h'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/sm_32_intrinsics.hpp'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/sm_35_intrinsics.h'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/sm_61_intrinsics.h'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/sm_61_intrinsics.hpp'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/crt/sm_70_rt.h'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/crt/sm_70_rt.hpp'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/surface_functions.h'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/texture_fetch_functions.h'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/texture_indirect_functions.h'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/surface_indirect_functions.h'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/device_launch_parameters.h'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/cuda_fp16.h'\r\n  '/scratch/jz748/cuda-10.1/targets/x86_64-linux/include/cuda_fp16.hpp'\r\n```\r\nDo you think it is a different issue? If so, I'll create another issue.", "@njzjz facing the same error and i think this error is related to this issue only", "> @njzjz facing the same error and i think this error is related to this issue only\r\n\r\nDowngrading from CUDA 10.1 to CUDA 10.0 solves the issue. Something wrong with CUDA 10.1? ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32776\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32776\">No</a>\n", "@owainkenwayucl, 58b236b235ce57808c917feb19b9f895f628710c should solve the original problem here. Thank you for your patience with the CLA issue in your PR.\r\n\r\nFor others -- please file a separate issue if you're still running into problems.", "No problem - thanks for solving the issue.", "> > @njzjz facing the same error and i think this error is related to this issue only\r\n> \r\n> Downgrading from CUDA 10.1 to CUDA 10.0 solves the issue. Something wrong with CUDA 10.1?\r\n\r\nI guess it is because in CUDA 10.1, `/usr/local/cuda-10.1/include` is now a symlink to `/usr/local/cuda-10.1/targets/x86_64-linux/include` and `/usr/local/cuda-10.1/lib64` is a symlink to `/usr/local/cuda-10.1/targets/x86_64-linux/lib`. This is different from CUDA 10.0, where both are real directories.\r\n\r\nPing @buchgr who seems to work on this recently. I hit the same issue setting up RBE builds with NCCL complaining missing dependency declaration. Might worth taking a look.", "> > > @njzjz facing the same error and i think this error is related to this issue only\r\n> > \r\n> > \r\n> > Downgrading from CUDA 10.1 to CUDA 10.0 solves the issue. Something wrong with CUDA 10.1?\r\n> \r\n> I guess it is because in CUDA 10.1, `/usr/local/cuda-10.1/include` is now a symlink to `/usr/local/cuda-10.1/targets/x86_64-linux/include` and `/usr/local/cuda-10.1/lib64` is a symlink to `/usr/local/cuda-10.1/targets/x86_64-linux/lib`. This is different from CUDA 10.0, where both are real directories.\r\n> \r\n> Ping @buchgr who seems to work on this recently. I hit the same issue setting up RBE builds with NCCL complaining missing dependency declaration. Might worth taking a look.\r\n\r\nSee here https://github.com/tensorflow/tensorflow/issues/35122#issuecomment-589858889. I have a temporary solution: resolving symbolic links for all CUDA paths.", "@njzjz You are welcomed to submit a PR and ping @angerson to see if he'd like to review."]}, {"number": 32775, "title": "Undefined symbol tensorflow::functor::CSRSparseMatrixTranspose", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 1.14 (github master aebcf430467918e646c2fb65372bdd9eeb320745)\r\n- Python version: 3.7.4\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 0.29\r\n- GCC/Compiler version (if compiling from source): 8.3.0\r\n- CUDA/cuDNN version: 10.1/7.6\r\n- GPU model and memory: GTX1070 8GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nBuild process completes without issue, installs without issue. However, after issuing `python -c \"import tensorflow\"` the following output is observed\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.7/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.7/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: /usr/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN10tensorflow7functor24CSRSparseMatrixTransposeIN5Eigen9GpuDeviceESt7complexIfEEclEPNS_15OpKernelContextEbRKNS_15CSRSparseMatrixEPS9_\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow/__init__.py\", line 98, in <module>\r\n    from tensorflow_core import *\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow_core/__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow/__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow/__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"/usr/lib/python3.7/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.7/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.7/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: /usr/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN10tensorflow7functor24CSRSparseMatrixTransposeIN5Eigen9GpuDeviceESt7complexIfEEclEPNS_15OpKernelContextEbRKNS_15CSRSparseMatrixEPS9_\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```sh\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n./bazel-bin/tensorflow/tools/pip_package/build_pip_package --nightly_flag /tmp/tensorflow_pkg\r\nsudo -H pip install tensorflow_pkg/tf_nightly-1.14.0-cp37-cp37m-linux_x86_64.whl\r\npython -c \"import tensorflow\"\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n```sh\r\n$ cat .tf_configure.bazelrc\r\nbuild --action_env PYTHON_BIN_PATH=\"/usr/bin/python\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/usr/lib/python3.7/site-packages\"\r\nbuild --python_path=\"/usr/bin/python\"\r\nbuild:xla --define with_xla_support=true\r\nbuild --config=xla\r\nbuild --config=tensorrt\r\nbuild --action_env TF_CUDA_VERSION=\"10.1\"\r\nbuild --action_env TF_CUDNN_VERSION=\"7.6\"\r\nbuild --action_env TF_TENSORRT_VERSION=\"6.0.1\"\r\nbuild --action_env TF_NCCL_VERSION=\"2.4.8\"\r\nbuild --action_env TF_CUDA_PATHS=\"/opt/cuda,/usr\"\r\nbuild --action_env CUDA_TOOLKIT_PATH=\"/opt/cuda\"\r\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"6.1\"\r\nbuild --action_env GCC_HOST_COMPILER_PATH=\"/usr/bin/gcc-8\"\r\nbuild --config=cuda\r\nbuild:opt --copt=-march=native\r\nbuild:opt --copt=-Wno-sign-compare\r\nbuild:opt --copt=-mtune=native\r\nbuild:opt --copt=-O3\r\nbuild:opt --copt=-fPIC\r\nbuild:opt --copt=-DNDEBUG\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\nbuild:v2 --define=tf_api_version=2\r\ntest --flaky_test_attempts=3\r\ntest --test_size_filters=small,medium\r\ntest --test_tag_filters=-benchmark-test,-no_oss,-oss_serial\r\ntest --build_tag_filters=-benchmark-test,-no_oss\r\ntest --test_tag_filters=-gpu\r\ntest --build_tag_filters=-gpu\r\nbuild --action_env TF_CONFIGURE_IOS=\"0\"\r\n```\r\n\r\nMissing symbol is\r\n`tensorflow::functor::CSRSparseMatrixTranspose<Eigen::GpuDevice, std::complex<float> >::operator()(tensorflow::OpKernelContext*, bool, tensorflow::CSRSparseMatrix const&, tensorflow::CSRSparseMatrix*)`\r\n\r\n```sh\r\n$ cat /proc/cpuinfo | head -n5\r\nprocessor\t: 0\r\nvendor_id\t: AuthenticAMD\r\ncpu family\t: 23\r\nmodel\t\t: 8\r\nmodel name\t: AMD Ryzen 7 2700X Eight-Core Processor\r\n```\r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n\n* For TF-GPU - See point 1\n* For TF-CPU - See point 2\n\n-----------------------------------------------------------------------------------------------\n\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\n*TF Version >= 1.13 requires CUDA 10.0 and TF Version < 1.13 (till TF 1.5) requires CUDA 9.0.*\n\n* If you have above configuration and using _**Windows**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n  * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n* If you have above configuration and using _**Ubuntu/Linux**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n  * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n* If error still persists then, apparently your CPU model does not support AVX instruction sets.\n  * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\n Therefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n* Try Google Colab to use TensorFlow.\n  * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true).You get pre-installed latest stable TF version. Also you can use```pip install``` to install any other preferred TF version.\n  * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n  * All you need is a good internet connection and you are all set.\n* Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*", "Try uninstalling cuda 10.1 and install cuda 10.0 and in addition to that make sure you add cuda, cudnn paths to your environment.\r\nPlease, go through below links and see if it helps you.\r\nSee #[software_requirements](https://www.tensorflow.org/install/gpu)\r\n            #[TensorFlow website](https://www.tensorflow.org/install/source) ", "@ravikyram cuda 10.1 isnt the issue (I have built tensorflow from source on other machine with cuda 10.1 without issue). Cuda and cuDNN are in the locations that the configure script found them. All software requirements are met.", "Can you try building again with the current maximum supported bazel version, `0.26` (from `configure.py`)? If the Arch package is too new, you may be able to use Bazelisk: https://github.com/bazelbuild/bazelisk ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32775\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32775\">No</a>\n"]}, {"number": 32774, "title": "Are complex variables supported in eager mode?", "body": "**System information**\r\n- TensorFlow version (you are using): 2.0.0-rc1\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nCurrently (unless I'm not doing things correctly) it seems that there is no support for complex variables. This fails:\r\n```python\r\n    a = tf.Variable(4.0+1.0j) # a = tf.Variable(4.0) works\r\n\r\n    def f(x):\r\n      return x*a\r\n\r\n    loss = lambda: tf.abs(f(5)-5)\r\n\r\n    optimizer = tf.optimizers.Adam(learning_rate=0.1)\r\n    optimizer.minimize(loss, [a])\r\n```\r\n\r\n> WARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.complex128\r\n\r\nand \r\n> ValueError: Invalid type tf.complex128 for Variable:0, expected: [tf.float32, tf.float64, tf.float16, tf.bfloat16].\r\n\r\n\r\n**Will this change the current api? How?**\r\nPotentially yes\r\n\r\n**Who will benefit with this feature?**\r\nAll of the users who used to rely on complex numbers!\r\n", "comments": ["I was able to reproduce the issue in `2.0.0rc2`. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/ecf15b48633de8064ea99a5834c523cc/32774.ipynb). Thanks!\r\n\r\nHere is the error trace\r\n```\r\nWARNING:tensorflow:The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.complex128\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-23-f65531aefcec> in <module>()\r\n      5 loss = lambda: tf.abs(f(5)-5)\r\n      6 optimizer = tf.optimizers.Adam(learning_rate=0.1)\r\n----> 7 optimizer.minimize(loss, [a])\r\n\r\n2 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py in _assert_valid_dtypes(self, tensors)\r\n    830       if dtype not in valid_dtypes:\r\n    831         raise ValueError(\"Invalid type %r for %s, expected: %s.\" %\r\n--> 832                          (dtype, t.name, [v for v in valid_dtypes]))\r\n    833 \r\n    834   def _valid_dtypes(self):\r\n\r\nValueError: Invalid type tf.complex128 for Variable:0, expected: [tf.float32, tf.float64, tf.float16, tf.bfloat16].\r\n```", "Unassigning, as I have no context on complex variable support. Sorry.", "@fchollet what do you mean by context?", "might be related to https://github.com/tensorflow/tensorflow/pull/30107", "@ziofil: That issue is slightly unrelated. I have a fix that add support for complex values in (some) optimizers that should be landing shortly.", "@jaingaurav Is there a timeframe on this fix? I am currently looking at a fix myself and wonder if it is worth it.", "@Davidvdrm: The fix is already committed and available in the nightly pip package. It will also be included in the upcoming 2.1 release.", "I built the nightly package and still get a NotFoundError concerning the needed ApplyGradient ops for Adam, SGD and AdaDelta. This is the same issue as before the upgrade to the nightly build and I created a new issue [https://github.com/tensorflow/tensorflow/issues/34098](https://github.com/tensorflow/tensorflow/issues/34098) concerning this problem. Should the change fix this issue or are they unrelated?\r\n\r\nFull error code:\r\n```\r\n> NotFoundError                             Traceback (most recent call last)\r\n> <ipython-input-41-0c0c662a0b5d> in <module>\r\n>      26         # Optimize the model\r\n>      27         loss_value, grads = grad(model, x, y)\r\n> ---> 28         optimizer.apply_gradients(zip(grads, model.trainable_variables))\r\n>      29 \r\n>      30         # Track progress\r\n> \r\n> ~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py in apply_gradients(self, grads_and_vars, name)\r\n>     442           functools.partial(self._distributed_apply, apply_state=apply_state),\r\n>     443           args=(grads_and_vars,),\r\n> --> 444           kwargs={\"name\": name})\r\n>     445 \r\n>     446   def _distributed_apply(self, distribution, grads_and_vars, name, apply_state):\r\n> \r\n> ~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py in merge_call(self, merge_fn, args, kwargs)\r\n>    1947     if kwargs is None:\r\n>    1948       kwargs = {}\r\n> -> 1949     return self._merge_call(merge_fn, args, kwargs)\r\n>    1950 \r\n>    1951   def _merge_call(self, merge_fn, args, kwargs):\r\n> \r\n> ~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py in _merge_call(self, merge_fn, args, kwargs)\r\n>    1954         distribution_strategy_context._CrossReplicaThreadMode(self._strategy))  # pylint: disable=protected-access\r\n>    1955     try:\r\n> -> 1956       return merge_fn(self._strategy, *args, **kwargs)\r\n>    1957     finally:\r\n>    1958       _pop_per_thread_mode()\r\n> \r\n> ~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py in _distributed_apply(self, distribution, grads_and_vars, name, apply_state)\r\n>     486           update_ops.extend(\r\n>     487               distribution.extended.update(\r\n> --> 488                   var, apply_grad_to_update_var, args=(grad,), group=False))\r\n>     489 \r\n>     490       any_symbolic = any(isinstance(i, ops.Operation) or\r\n> \r\n> ~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py in update(self, var, fn, args, kwargs, group)\r\n>    1541       kwargs = {}\r\n>    1542     with self._container_strategy().scope():\r\n> -> 1543       return self._update(var, fn, args, kwargs, group)\r\n>    1544 \r\n>    1545   def _update(self, var, fn, args, kwargs, group):\r\n> \r\n> ~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py in _update(self, var, fn, args, kwargs, group)\r\n>    2172     # The implementations of _update() and _update_non_slot() are identical\r\n>    2173     # except _update() passes `var` as the first argument to `fn()`.\r\n> -> 2174     return self._update_non_slot(var, fn, (var,) + tuple(args), kwargs, group)\r\n>    2175 \r\n>    2176   def _update_non_slot(self, colocate_with, fn, args, kwargs, should_group):\r\n> \r\n> ~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py in _update_non_slot(self, colocate_with, fn, args, kwargs, should_group)\r\n>    2178     # once that value is used for something.\r\n>    2179     with UpdateContext(colocate_with):\r\n> -> 2180       result = fn(*args, **kwargs)\r\n>    2181       if should_group:\r\n>    2182         return result\r\n> \r\n> ~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py in apply_grad_to_update_var(var, grad)\r\n>     468       if \"apply_state\" in self._dense_apply_args:\r\n>     469         apply_kwargs[\"apply_state\"] = apply_state\r\n> --> 470       update_op = self._resource_apply_dense(grad, var, **apply_kwargs)\r\n>     471       if var.constraint is not None:\r\n>     472         with ops.control_dependencies([update_op]):\r\n> \r\n> ~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\adadelta.py in _resource_apply_dense(self, grad, var, apply_state)\r\n>     139         coefficients['epsilon'],\r\n>     140         grad,\r\n> --> 141         use_locking=self._use_locking)\r\n>     142 \r\n>     143   def _resource_apply_sparse(self, grad, var, indices, apply_state=None):\r\n> \r\n> ~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\training\\gen_training_ops.py in resource_apply_adadelta(var, accum, accum_update, lr, rho, epsilon, grad, use_locking, name)\r\n>    1100         pass  # Add nodes to the TensorFlow graph.\r\n>    1101     except _core._NotOkStatusException as e:\r\n> -> 1102       _ops.raise_from_not_ok_status(e, name)\r\n>    1103   # Add nodes to the TensorFlow graph.\r\n>    1104   if use_locking is None:\r\n> \r\n> ~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\framework\\ops.py in raise_from_not_ok_status(e, name)\r\n>    6596   message = e.message + (\" name: \" + name if name is not None else \"\")\r\n>    6597   # pylint: disable=protected-access\r\n> -> 6598   six.raise_from(core._status_to_exception(e.code, message), None)\r\n>    6599   # pylint: enable=protected-access\r\n>    6600 \r\n> \r\n> ~\\Anaconda3\\lib\\site-packages\\six.py in raise_from(value, from_value)\r\n> \r\n> \r\n> NotFoundError: No registered 'ResourceApplyAdadelta' OpKernel for 'CPU' devices compatible with node {{node ResourceApplyAdadelta}}\r\n> \t (OpKernel was found, but attributes didn't match) Requested Attributes: T=DT_COMPLEX64, use_locking=true\r\n> \t.  Registered:  device='CPU'; T in [DT_HALF]\r\n>   device='CPU'; T in [DT_BFLOAT16]\r\n>   device='CPU'; T in [DT_FLOAT]\r\n>   device='CPU'; T in [DT_DOUBLE]\r\n>  [Op:ResourceApplyAdadelta]\r\n```\r\n\r\n`\r\n> "]}, {"number": 32773, "title": "Track initializer in StaticVocabularyTable", "body": "Fixes #32770.", "comments": ["@alextp I added a test. Thanks!", "@guillaumekln Seems like some test failed. Do you know why? ", "It does not seem like the failures are related to this change.", "@guillaumekln alright, either ping someone who can re-trigger the checks, or maybe force-push new commits to trigger a re-run of the checks", "Looks like other PRs marked with \"ready to pull\" have similar CI failures.\r\n\r\n@gbaned Should we do something else to move forward?", "@guillaumekln Sorry for the slow response, it is waiting for internal approval. We will let you know if anything needed. Thank you. "]}, {"number": 32772, "title": "Training parameter to model passed as None in 1.14.0", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary (pip)\r\n- TensorFlow version (use command below): v1.14.0-0-g87989f6959 1.14.0\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 10.1/7.5\r\n- GPU model and memory: GTX 1080Ti\r\n\r\n**Describe the current behavior**\r\nTraining parameter to model passed as None\r\n\r\n**Describe the expected behavior**\r\nTraining parameter should be True in training and False in inference\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\nclass MyModel(tf.keras.Model):\r\n  def __init__(self):\r\n    super(MyModel, self).__init__()\r\n    self.dense = tf.keras.layers.Dense(4)\r\n\r\n  def call(self, inputs, training=False):\r\n    print('Training', training)\r\n    return self.dense(inputs)\r\n    \r\n    \r\nclass Gen(tf.keras.utils.Sequence):\r\n    def __len__(self):\r\n        return 10\r\n    \r\n    def __getitem__(self, i):\r\n        return np.ones((32, 100)), np.ones((32, 4))\r\n    \r\nmodel = MyModel()\r\nmodel.compile(optimizer=tf.train.AdagradOptimizer(0.001), loss='categorical_crossentropy', metrics=['accuracy'])\r\n\r\n\r\n# training should be False\r\nmodel.build(input_shape=(32, 100))\r\n\r\n# training should be True\r\nmodel.fit_generator(generator=Gen(), epochs=1, validation_data=Gen())\r\n```\r\n\r\n**Other info / logs**\r\nOutputs:\r\n```\r\nTraining False\r\nTraining None\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.where in 2.0, which has the same broadcast rule as np.where\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\n10/10 [==============================] - 1s 127ms/step - loss: 17.8394 - acc: 1.0000 - val_loss: 16.1181 - val_acc: 1.0000\r\n<tensorflow.python.keras.callbacks.History at 0x7f4754342e80>\r\n```\r\n", "comments": ["@stefanpantic \r\nIs it possible for you to try latest TF versions (`!pip install tensorflow-gpu==1.15.0-rc0` )and let us know whether the issue persists? There were lots of performance improvements in the latest versions. Thanks!", "```\r\nTraining Tensor(\"keras_learning_phase:0\", shape=(), dtype=bool)\r\n```\r\nThis gets passed as the training parameter instead of None in the 1.15 release candidate.\r\n\r\nAn aditional question I have is: is it necessary to pass the training parameter to the Dropout/BatchNormalization layers or can they infer the appropriate value based on K.learning_phase()?", "@stefanpantic Thanks for the issue!\r\n\r\nThis is fixed in the latest tf-nightly: `pip install -U tf-nightly`", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32772\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32772\">No</a>\n"]}, {"number": 32771, "title": "TensorFlow is broken, unusable on Raspberry Pi", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, code is pasted below\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Raspbian Buster\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Raspberry Pi 4 with 4 GB RAM\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.7.3\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nOn Windows I use python-3.7.4 with TensorFlow 1.13.1, 1.14.0 and 2.0.0rc1\r\n\r\n**Describe the current behavior**\r\nI train a model on Windows (gen_test_train_data.py, then train_model.py), using the same TF version that's available on RPi. I can test the model on Windows just fine using model_visual_test.py.\r\nIf I transfer the model to the RPi, I cannot load it. I get mysterious errors when running model_visual_test.py:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"model_visual_test.py\", line 16, in <module>\r\n    model = keras.models.load_model('saved_model.h5')\r\n  File \"/home/pi/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py\", line 140, in load_model\r\n    loader_impl.parse_saved_model(filepath)\r\n  File \"/home/pi/.local/lib/python3.7/site-packages/tensorflow_core/python/saved_model/loader_impl.py\", line 83, in parse_saved_model\r\n    constants.SAVED_MODEL_FILENAME_PB))\r\nOSError: SavedModel file does not exist at: saved_model.h5/{saved_model.pbtxt|saved_model.pb}\r\n```\r\n\r\nIt doesn't work even if I train the model on the RPi, by running gen_test_train_data.py, then run train_model.py, which takes forever. It completes training, but then it fails to load with\r\n\r\n```\r\nTrain on 8000 samples\r\n8000/8000 [==============================] - 1305s 163ms/sample - loss: 0.5675 - acc: 0.8609\r\nTraceback (most recent call last):\r\n  File \"train_model.py\", line 122, in <module>\r\n    keras.models.save_model(model, 'saved_model.h5')\r\n  File \"/home/pi/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py\", line 104, in save_model\r\n    model, filepath, overwrite, include_optimizer)\r\n  File \"/home/pi/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py\", line 73, in save_model_to_hdf5\r\n    raise ImportError('`save_model` requires h5py.')\r\nImportError: `save_model` requires h5py.\r\n```\r\n\r\nNo, that's wrong. I do have the h5 module installed on the Raspberry Pi:\r\n\r\n```\r\n$ pip3 list --user | grep h5\r\nh5py                 2.10.0\r\n```\r\n\r\nIn other words, a model trained on the RPi fails to load on the RPi. A model trained on Win10 runs fine on Win10, but fails to load on the RPi. This is with a code that works perfect on Windows 10 and macOS with all kinds of TF versions between 1.13.1 and 2.0.0rc1, GPU or CPU versions.\r\n\r\n**Describe the expected behavior**\r\nI mean, it should just work, shouldn't it? Why is this so hard?\r\n\r\n**Code to reproduce the issue**\r\nhttps://github.com/FlorinAndrei/TensorAim/tree/tf-bug-report\r\nRun gen_test_train_data.py. Then on that run train_model.py. It fails to run its own model, generated on RPi.\r\nOr run gen_test_train_data.py on Windows, run train_model.py on Windows, transfer the model on the RPi, then run model_visual_test.py on it. It will fail to load.\r\nI will leave this branch untouched so you can test the bug report.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Any clue on what is happening?", "@FlorinAndrei  Is this still happening with TF 2.2 release?", "I have no time to test it nowadays, sorry. Maybe in a few weeks.", "The same is happening to me.", "@JosimarReis @FlorinAndrei I have a RasPi Zero, so it'd be hard to reproduce this for me but, take a look at https://github.com/tensorflow/tensorflow/issues/35909. They may have had a similar issue with loading H5 models. The solution that worked there was getting the h5py package via the APT package manager: `sudo apt-get install python3-h5py`. Judging by your bug report, @FlorinAndrei, you installed h5py with `pip`.\r\n\r\nNote that this was performed in Linux Raspbian and @FlorinAndrei you have Raspbian Buster (not an expert in the diff of those distros).\r\n\r\n> ... From the error log i can see that saved_model- face_recognition_model.h5 does not exist, make sure you import the file to the right path and then try loading the model using load_model...\r\n\r\n@niccle27 said:\r\n> ... the path was alright. The problem concern the h5py package installed using pip on the raspberry pi. Apparently there seem to be some issue with it. I managed to fix it by installing using the debian repository : sudo apt-get install python3-h5py.\r\n\r\nHowever, this solution didn't seem to work for some others, like @JinkaiGUAN.\r\n\r\n@petewarden Maybe you or someone from the TF Lite team can confirm this is an outstanding bug and `apt-get` is the preferred package installer for now? cc @dynamicwebpaige ", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32771\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32771\">No</a>\n"]}, {"number": 32770, "title": "Asset file not exported in the SavedModel when using tf.lookup.StaticVocabularyTable", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Ubuntu 16.04\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 2.0.0rc2\r\n- Python version: 3.6.6\r\n\r\n**Describe the current behavior**\r\n\r\nWhen using `tf.lookup.StaticVocabularyTable`, the asset file is not exported in the SavedModel assets directory. However, it is correctly saved when using `tf.lookup.StaticHashTable`.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe vocabulary file should be saved in the assets directory of the SavedModel.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport os\r\nimport shutil\r\nimport tensorflow as tf\r\n\r\nclass Model(tf.keras.layers.Layer):\r\n\r\n    def __init__(self, vocabulary_path):\r\n        super(Model, self).__init__()\r\n        initializer = tf.lookup.TextFileInitializer(\r\n            vocabulary_path,\r\n            tf.string,\r\n            tf.lookup.TextFileIndex.WHOLE_LINE,\r\n            tf.int64,\r\n            tf.lookup.TextFileIndex.LINE_NUMBER)\r\n        self.table = tf.lookup.StaticVocabularyTable(initializer, num_oov_buckets=1)\r\n        #self.table = tf.lookup.StaticHashTable(initializer, 0)\r\n\r\n    def call(self, tokens):\r\n        return self.table.lookup(tokens)\r\n\r\n    @tf.function(input_signature=(tf.TensorSpec([None], dtype=tf.string),))\r\n    def serve(self, tokens):\r\n        return self(tokens)\r\n\r\n\r\nvocabulary_path = \"/tmp/vocab.txt\"\r\nwith open(vocabulary_path, \"w\") as vocabulary_file:\r\n    vocabulary_file.write(\"a\\nb\\nc\\n\")\r\n\r\nmodel = Model(vocabulary_path)\r\n\r\nexport_dir = \"/tmp/model\"\r\nif os.path.exists(export_dir):\r\n    shutil.rmtree(export_dir)\r\ntf.saved_model.save(model, export_dir, signatures=model.serve)\r\nassets = os.listdir(os.path.join(export_dir, \"assets\"))\r\nassert len(assets) == 1\r\n```\r\n\r\n**Other info / logs**\r\n\r\nThe code above raises an `AssertionError` as the assets directory is empty.", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32770\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32770\">No</a>\n"]}, {"number": 32769, "title": "model.fit() raise FailedPreconditionError after applying \"tf.distribute.MirroredStrategy()\" ", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):NO\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N/A\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):tf1.14/tf2.0.0rc1\r\n- Python version:3.7.4\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version: 9.0/7.6.0(tf1.14.0) , 10.0.130/7.6.0(tf2.0.0rc1) \r\n- GPU model and memory: 8G*2\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI create and complie the keras model under `with mirrored_strategy.scope():` \uff0cthen i got \"FailedPreconditionError \" when i excute model.fit();\r\nif i move the code out from `with mirrored_strategy.scope():`, everything is ok.\r\n\r\n**Describe the expected behavior**\r\nwhen I create and complie the keras model under `with mirrored_strategy.scope():` , do not raise the exception and  multi GPU  can come into use.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nmirrored_strategy = tf.distribute.MirroredStrategy()\r\nwith mirrored_strategy.scope():\r\n    # Create the base model from the pre-trained model MobileNet V2\r\n    base_model = tf.keras.applications.InceptionV3(input_shape=IMG_SHAPE,\r\n                                                   include_top=False,\r\n                                                   weights='imagenet')\r\n    base_model.trainable = False\r\n    model = tf.keras.Sequential([\r\n      base_model,\r\n      keras.layers.GlobalAveragePooling2D(),\r\n      keras.layers.Dense(len(label_names), activation='sigmoid')\r\n    ])\r\n\r\n    model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),\r\n                  loss='binary_crossentropy',\r\n                  metrics=['categorical_accuracy'])\r\n\r\ncallbacks = [\r\n    tf.keras.callbacks.TensorBoard(log_dir=log_dir),\r\n    tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\r\n                                       save_weight_only=True)\r\n]\r\n\r\nepochs = 10\r\nhistory = model.fit(train_dataset, \r\n                    epochs=epochs,\r\n                    callbacks=callbacks)\r\n```\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n--------------------------------------------------------------------\r\nFailedPreconditionError                   Traceback (most recent call last)\r\n<ipython-input-28-35cd9d3d6f8f> in <module>\r\n      9                               validation_data=validation_ds,\r\n     10                               validation_steps=validation_steps,\r\n---> 11                               callbacks=callbacks)\r\n\r\n/usr/local/miniconda3/envs/tf_2.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    726         max_queue_size=max_queue_size,\r\n    727         workers=workers,\r\n--> 728         use_multiprocessing=use_multiprocessing)\r\n    729 \r\n    730   def evaluate(self,\r\n\r\n/usr/local/miniconda3/envs/tf_2.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\r\n    683         validation_steps=validation_steps,\r\n    684         validation_freq=validation_freq,\r\n--> 685         steps_name='steps_per_epoch')\r\n    686 \r\n    687   def evaluate(self,\r\n\r\n/usr/local/miniconda3/envs/tf_2.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\r\n    297           else:\r\n    298             actual_inputs = ins()\r\n--> 299           batch_outs = f(actual_inputs)\r\n    300         except errors.OutOfRangeError:\r\n    301           if is_dataset:\r\n\r\n/usr/local/miniconda3/envs/tf_2.0/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py in __call__(self, inputs)\r\n   3465 \r\n   3466     fetched = self._callable_fn(*array_vals,\r\n-> 3467                                 run_metadata=self.run_metadata)\r\n   3468     self._call_fetch_callbacks(fetched[-len(self._fetches):])\r\n   3469     output_structure = nest.pack_sequence_as(\r\n\r\n/usr/local/miniconda3/envs/tf_2.0/lib/python3.7/site-packages/tensorflow_core/python/client/session.py in __call__(self, *args, **kwargs)\r\n   1470         ret = tf_session.TF_SessionRunCallable(self._session._session,\r\n   1471                                                self._handle, args,\r\n-> 1472                                                run_metadata_ptr)\r\n   1473         if run_metadata:\r\n   1474           proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\nFailedPreconditionError: 2 root error(s) found.\r\n  (0) Failed precondition: Error while reading resource variable conv2d_64/kernel from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/conv2d_64/kernel/N10tensorflow3VarE does not exist.\r\n\t [[{{node inception_v3_1/conv2d_64/Conv2D/ReadVariableOp}}]]\r\n\t [[batch_normalization_42/Const/_189]]\r\n  (1) Failed precondition: Error while reading resource variable conv2d_64/kernel from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/conv2d_64/kernel/N10tensorflow3VarE does not exist.\r\n\t [[{{node inception_v3_1/conv2d_64/Conv2D/ReadVariableOp}}]]\r\n0 successful operations.\r\n1 derived errors ignored.\r\n", "comments": ["@BoysFight Can you provide a standalone code to reproduce the issue? Thanks!", "+1. Complete code to repro would be helpful in debugging this further.\r\nThe stack trace above is confusing because it seems like it is from TF 2.0, but also seems to be using TF session which is only a 1.x thing. Can you confirm if the stack trace if from 2.0 rc1? ", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@BoysFight Just a reminder for a standalone code to reproduce the issue. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32769\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32769\">No</a>\n"]}]