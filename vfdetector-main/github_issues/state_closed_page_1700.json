[{"number": 1900, "title": "Update roadmap.md", "body": "", "comments": []}, {"number": 1899, "title": "partial_run won't accept optimizers as fetch", "body": "### Environment info\n\nOperating System: ubuntu 14.04\ntensorflow 0.7.1\nInstalled version of CUDA and cuDNN: cuda-7.5, cuDNN 4\n\nI'd like to use partial_run to insert some lookahead processing during training attention-based image processing, but I can't complete the step because partial_run won't accept an operation as a fetch arg, and optimizers don't return any downstream variables. See the trivial case below.\nSeems like a bug in the partial_run implementation, or do I misunderstand its use?\n....\ny_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2,name='softmax')\ncross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))\ntrain_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\ntf.initialize_all_variables().run()\nfor i in range(100000):\n    batch =shapes.train.next_batch(batch_size)\n    h = sess.partial_run_setup([y_conv,train_step],[x,y_,keep_prob])\n    im_state=sess.partial_run(h,y_conv,feed_dict={x:batch[0], y_: batch[1], keep_prob: 0.5})\n    final=sess.partial_run(h,[train_step])\n\nlooking at the code, it seems any ops put in fetch are moved to the target_list, which must be empty when _do_call is running for partial_run. As a result, this error on the \"final=\" last step above:\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in _prun_fn(session, handle, feed_dict, fetch_list)\n    557     def _prun_fn(session, handle, feed_dict, fetch_list):\n    558       if target_list:\n--> 559         raise RuntimeError('partial_run() requires empty target_list.')\n    560       return tf_session.TF_PRun(session, handle, feed_dict, fetch_list)\n    561 \nRuntimeError: partial_run() requires empty target_list.\n", "comments": ["This is currently by design. You can probably do something like below. \n\n```\nwith tf.control_dependencies([train_step]):\n  dummy = tf.constant(0)\n```\n\nAnd\n`final = sess.partial_run(h, [dummy])`\n", "yup, thanks! \nHad to upgrade to HEAD to fix SegFault, but that does it. Sorry, should have thought of using  control_dependencies.\n"]}, {"number": 1898, "title": "R0.8 Release notes", "body": "", "comments": []}, {"number": 1897, "title": "Make sparse_tensor_dense_matmul_op_test medium", "body": "", "comments": []}, {"number": 1896, "title": "Fixes for release", "body": "", "comments": []}, {"number": 1895, "title": "Fix bugs in the indexing operator with latest NumPy", "body": "`np.random.random_integers()` is deprecated and recently started returning non-`int` values. This PR fixes this so that the `tf.Tensor` indexing operator can handle non-`int` (but convertable-to-`int`) values, and removes the deprecated method from the test.\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins, test this please.\n"]}, {"number": 1894, "title": "Missing types in C++ API", "body": "It looks like there are some missing types in tensorflow/core/framework/types.h, such as uint32 and uint64. This means I can't make tensors of these types in C++.\n", "comments": ["Hi, I can work on this. Starting with the open source for the first time. Can I get more details regarding the issue. Thanks.\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 1893, "title": "Problem with shape inference with diag_part operator. ", "body": "Running` from master the following small code snippet gives an error.\n\n```\nimport tensorflow as tf\nimport numpy as np\n\nW = tf.constant(3., tf.float64)\nX = tf.placeholder(tf.float64, [5,3])\nY = tf.scalar_mul( W , tf.ones(tf.pack([tf.shape(X)[0], tf.shape(X)[0]]), tf.float64) )\nZ = tf.diag_part(Y)\n\ninit = tf.initialize_all_variables()\n\nsess = tf.Session()\nsess.run(init)\nprint(sess.run(Z, feed_dict={X: np.ones((5,3))}))\n```\n\nThe relevant part of the error is \n\n> /tensorflow/python/ops/array_ops.py\", line 960, in _DiagPartShape\n>     \" do not match \")\n> ValueError: Invalid shape, shape[:mid] (?,) and shape[mid:] (?,) do not match \n\nI think it is reasonable to be able to get the diagonal of this matrix. Therefore I think this is a bug with the shape inference. \n\nThanks,\nAlex \n", "comments": ["Simpler example\n\nshape = tf.placeholder(tf.int32, shape=(2))\na = tf.ones(shape)\nsess = tf.Session()\nsess.run(tf.diag_part(a+1), feed_dict={shape:(1,1)})\n\nValueError: Invalid shape, shape[:mid](?,) and shape[mid:](?,) do not match\n", "@yaroslavvb That looks like a bug then. I'd be interested in a work around if anyone finds one. \n", "A fix has been submitted.\n"]}, {"number": 1892, "title": "Broken links in C++ API documentation", "body": "It looks like all of the links from https://www.tensorflow.org/versions/r0.7/api_docs/cc/index.html are broken. For example, https://www.tensorflow.org/versions/r0.7/api_docs/cc/classTensor.html.\n", "comments": ["It appears that they just need to be capitalized: https://www.tensorflow.org/versions/r0.7/api_docs/cc/ClassTensor.html.\n", "I reported this in #1303, and it was fixed with #1314.\n\nIt seems like the fix would need to be backported in order to update the documentation for TF 0.7 on the website. The [documentation for TF 0.8](https://www.tensorflow.org/versions/r0.8/api_docs/cc/index.html) apparently works as it's supposed to.\n", "TF 0.7 documentation has been removed, so this issue is now obsolete.\r\nClosing."]}, {"number": 1891, "title": "Gradient and unit test for MatrixTriangularSolve and BatchMatrixTriangularSolve ops", "body": "See issue #1875 for some discussion with @girving . \n", "comments": ["Can one of the admins verify this patch?\n", "Can one of the admins verify this patch?\n", "@rmlarsen What do you think of this one? Would you like me to update it? I notice the linalg_grad unit test has had some work recently. \n", "Hi,\n\nThanks for submitting the PR. I just submitted similar code with gradients\nfor {batch_}matrix_solve, {batch_}triangular_solve, and\nbatch_matrix_determinant. So this is probably not necessary anymore.\n\nCheers,\n  Rasmus\n\nOn Mon, Apr 18, 2016 at 8:58 AM, Alexander G. de G. Matthews <\nnotifications@github.com> wrote:\n\n> @rmlarsen https://github.com/rmlarsen What do you think of this one?\n> Would you like me to update it? I notice the linalg_grad unit test has had\n> some work recently.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/1891#issuecomment-211443746\n", "Hi Rasmus, Great. Thanks for letting me know. I hope the code helped. Alex \n", "I actually didn't see your PR until after I submitted, but as you know \"all great minds think alike\", or the math only has one solution... or something... ;-)\n"]}, {"number": 1890, "title": "graph.pb.h missing", "body": "The issue is quite simple: I downloaded and built TF from source, but the include files like tensorflow/core/public/session.h reference non-existing files. One culprit is tensorflow/core/framework/graph.pb.h. Attempted compilation produces the following error:\n\n22:31:27 ***\\* Build of configuration Debug for project hello_world ****\nmake all \nmake: Warning: File `objects.mk' has modification time 1,8e+02 s in the future\nBuilding file: ../src/hello_world.cpp\nInvoking: GCC C++ Compiler\ng++ -I/home/sander/tensorflow -O0 -g3 -Wall -c -fmessage-length=0 -MMD -MP -MF\"src/hello_world.d\" -MT\"src/hello_world.d\" -o \"src/hello_world.o\" \"../src/hello_world.cpp\"\nIn file included from ../src/hello_world.cpp:8:0:\n/home/sander/tensorflow/tensorflow/core/public/session.h:22:48: fatal error: tensorflow/core/framework/graph.pb.h: No such file or directory\n #include \"tensorflow/core/framework/graph.pb.h\"\n                                                ^\ncompilation terminated.\n\nI cannot rule out a mistake on my part, but I've tried a bunch of things to no avail. Either a file is missing or there may be a linking issue. Any help would be greatly appreciated.\n### Environment info\n\nOperating System: Ubuntu 14.04\n\nInstalled version of CUDA and cuDNN: \nNone\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n   I followed these steps:\n\n1 git clone --recurse-submodules https://github.com/tensorflow/tensorflow\n\n2 run ./configure\n\n3 bazel build -c opt //tensorflow/tools/pip_package:build_pip_package\n\n4 bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\n\n5 pip install /tmp/tensorflow_pkg/tensorflow-0.7.1-py2-none-linux_x86_64.whl\n1. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n   0.7.1\n\nIf installed from sources, provide the commit hash:\nnot sure, retrieved on 11 april from terminal:\n$ git clone --recurse-submodules https://github.com/tensorflow/tensorflow\n### Steps to reproduce\n1. The error occurs simply when any file from TF is included in c++\n   example:\n   #include \"tensorflow/core/public/session.h\"\n### What have you tried?\n1. Reinstall/upgrade: pip, bazel, tensorflow, gcc/g++, add additional include paths\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n", "comments": ["`tensorflow/core/framework/graph.pb.h` is a generated file: it should be produced in the `bazel-genfiles/tensorflow/core/framework` directory when you build TensorFlow using Bazel.\n\nBuilding from source without using Bazel is not fully supported right now, but you could try adding `-I/home/sander/tensorflow/bazel-genfiles` to your `g++` arguments (assuming you have previously build TF from source).\n", "Thank you for your answer. In genfiles I finally found the missing file. Should I add genfiles to my include paths?\n", "That should work, although the recommend workflow is still to use Bazel for building C++ targets (since it takes care of these issues).\n", "Ah, but the thing is, I did use bazel to build it. I used the bazel commands as shown on the TF installation page. I wonder if something else is wrong... Do you have any idea what could be the issue?\n", "I mean that it would be easier to build your own program `hello_world.cpp` using Bazel as well. From your error message it looks like you are using `make all` to build it, which is unsupported, but you might be able to set up your include path appropriately.\n", "Wow, I finally get my mistake. I built TF using bazel, but then tried to build my program in Eclipse. Thanks a lot! \n\nAs a last little question, do you know any link/tutorial on building your own project in bazel?\n", "Ah, that makes sense! The [Bazel C++ tutorial](http://bazel.io/docs/cpp.html) covers all of the concepts that you need to know, although for simple projects you might get just as far by copying the rule for one of the TensorFlow C++ binaries ([e.g.](https://github.com/tensorflow/tensorflow/blob/cf1659d1c233f8ddbee13fd298464d76e58bdccb/tensorflow/cc/BUILD#L61)).\n", "@SanderDalm After you created the bazel-gen files, what did you do for your application to build? Did you add the bazel-gen files in your include path? Did it work after that?  How did you avoid rebuilding TF again?\r\n\r\nI have a C++ project that contains several subfolders of *.h and *.cpp files. I think that creating a BUILD file for such a project will not be the fastest way to solve the issue.\r\n@mrry ", "@SanderDalm @HossamAmer12 \r\nI have the same question ,too.\r\nI would like to add the code for prediction to my cv project, \r\nI used Bazel build:  to create a libtensorflow.so file,\r\nthen I add it in the lib path,\r\nbut it dose not work.\r\nBecause the IDE always remind me that many include files are not found.\r\nI use Max OS X, QtCreator, gcc.", "@SanderDalm @YorksonChang @mrry \r\n\r\nThis is a simple Neural Network I created using C++ Tensorflow. I also created a makefile for compiling the source code via (g++); I think that this makefile will be helpful for you to understand how to build the code.\r\n[simpleNetwork.zip](https://github.com/tensorflow/tensorflow/files/943262/simpleNetwork.zip)\r\n\r\nBecause I inserted absolute paths, you need to insert your own paths in the makefile.\r\n\r\nBest of luck and let me know if you have more questions!\r\n\r\nPS: I extended the following tutorial: \r\n[https://medium.com/jim-fleming/loading-a-tensorflow-graph-with-the-c-api-4caaff88463f](url)", "@HossamAmer12 Thank you for sharing your project. There's a key step I'm failing to accomplish. When I run your code (or my code) the header files are not found. How do you resolve this?", "Thanks to @HossamAmer12 for providing a zip of his work, deriving from the Medium article example, I have the follow:\r\n\r\nFirst, after installing `protobuf` and `eigen`:\r\n```\r\n./configure\r\nbazel build  //tensorflow:libtensorflow_cc.so\r\n```\r\n\r\nThen Copy the following include headers and dynamic shared library to `/usr/local/lib` and `/usr/local/include`:\r\n```\r\nmkdir /usr/local/include/tf\r\ncp -r bazel-genfiles/ /usr/local/include/tf/\r\ncp -r tensorflow /usr/local/include/tf/\r\ncp -r third_party /usr/local/include/tf/\r\ncp -r bazel-bin/libtensorflow_cc.so /usr/local/lib/\r\n```\r\n\r\nLastly, compile using an example:\r\n```\r\ng++ -std=c++11 -o tLoader -I/usr/local/include/tf -I/usr/local/include/eigen3 -g -Wall -D_DEBUG -Wshadow -Wno-sign-compare -w  -L/usr/local/lib/libtensorflow_cc `pkg-config --cflags --libs protobuf`  -ltensorflow_cc loader.cpp\r\n```", "@lababidi Is the issue still open?", "@lababidi thanks for your tips, after some efforts, I can run successfully in your way, which makes it so comfortable when using cmake. ", "@lababidi @freesouls  I've followed @lababidi method,but  got the error below.Any suggestion?\r\n\r\n```\r\nUndefined symbols for architecture x86_64:\r\n  \"tensorflow::internal::CheckOpMessageBuilder::NewString[abi:cxx11]()\", referenced from:\r\n      std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >* tensorflow::internal::MakeCheckOpString<int, int>(int const&, int const&, char const*) in cc918f4y.o\r\nld: symbol(s) not found for architecture x86_64\r\ncollect2: error: ld returned 1 exit status\r\n```", "@haiy I had your same issue trying to compile some examples using `g++-7` from Homebrew. I solved it by using instead the compiler shipped with macOS, i.e. `/usr/bin/g++`.", "@lababidi Hi, thanks for you description of how to use standalone shared library. For me, libtensorflow_so compiled without a problem using bazel, but I have problems with includes, when i try to use it in a separate eclipse project, specifically with eigen3. I tried 2 options. \r\n1) Add tensorflow's _third_party/eigen3_ to the include path. This causes cyclic include, since _unsupported/Eigen/CXX11/Tensor_ bacisally includes itself in the first line:\r\n` #include \"unsupported/Eigen/CXX11/Tensor\". `\r\nSo this approach does not work.\r\n\r\n2) Using system installed /usr/include/eigen3, but in this case the error is:\r\n\r\n`tensorflow/core/framework/type_traits.h:69:52: error: \u2018half\u2019 is not a member of \u2018Eigen\u2019`\r\n\r\nI guess there is probably a trivial solution of just correctly configuring the include paths, but so far i have spent a lot of time on this with no luck. Does anybody have any ideas for how to do this correctly?", "@jkravanja This is some `bash` code I use to copy the necessary files for compiling a separate project (after having built tensorflow):\r\n```\r\ncp -r $TENSORFLOW_DIR/tensorflow include/third_party\r\ncp -r $TENSORFLOW_DIR/bazel-genfiles/tensorflow include/third_party\r\ncp -r $TENSORFLOW_DIR/bazel-tensorflow/../../external/protobuf/src/google include/third_party\r\ncp -r $TENSORFLOW_DIR/third_party/eigen3 include/third_party\r\ncp -r $TENSORFLOW_DIR/bazel-tensorflow/../../external/eigen_archive/. include/third_party/eigen3\r\ncp -r include/third_party/eigen3/Eigen include/third_party\r\n```\r\nwhere `TENSORFLOW_DIR` is the directory where you cloned the tensorflow repository (for example `/opt/tensorflow`), and `include/third_party` is the directory to include when compiling your project (`-I include/third_party`). This script probably copies a lot more files than the strictly necessary, but it works for me.\r\n", "Thanks that works, but i have to add both include paths, include/third_party and also include/, since some headers include from \"third_party/...\"", "@integeruser @jkravanja \r\n\r\nI am having the same problem `tensorflow/core/framework/type_traits.h:69:52: error: \u2018half\u2019 is not a member of \u2018Eigen\u2019`.\r\n\r\nI tried running all those copy commands, but it gets stuck at:\r\n\r\n`$TENSORFLOW_DIR/bazel-tensorflow/../../external/protobuf/src/google': No such file or directory`.\r\n\r\nEDIT: I managed to overcome the Eigen error by downloading the latest version of Eigen and linking to that, instead of the version available through Ubuntu 16.04 package manager.", "Probably you can try copying \r\n`cp -r $TENSORFLOW_DIR/bazel-tensorflow/external/protobuf/src/google include/third_party`", "@9thDimension @jkravanja I have a lot of undefined errors. Any idea? I did compile libtensorflow_cc.so successfully.", "@bobeo undefined errors or undefined rference erros? Probably it is a linker error, you are missing some libraries. Can you copy paste it here?", "@jkravanja undefined reference errors:\r\n```\r\n\r\n/tmp/ccEdGM3i.o: In function `LoadGraph(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::unique_ptr<tensorflow::Session, std::default_delete<tensorflow::Session> >*)':\r\n~/Downloads/simpleNetwork/simpleNetwork/loader.cpp:44: undefined reference to `tensorflow::GraphDef::GraphDef()'\r\n~/Downloads/simpleNetwork/simpleNetwork/loader.cpp:46: undefined reference to `tensorflow::Env::Default()'\r\n~/Downloads/simpleNetwork/simpleNetwork/loader.cpp:46: undefined reference to `tensorflow::ReadBinaryProto(tensorflow::Env*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, google::protobuf::MessageLite*)'\r\n~/Downloads/simpleNetwork/simpleNetwork/loader.cpp:51: undefined reference to `tensorflow::SessionOptions::SessionOptions()'\r\n~/Downloads/simpleNetwork/simpleNetwork/loader.cpp:51: undefined reference to `tensorflow::NewSession(tensorflow::SessionOptions const&)'\r\n~/Downloads/simpleNetwork/simpleNetwork/loader.cpp:44: undefined reference to `tensorflow::GraphDef::~GraphDef()'\r\n~/Downloads/simpleNetwork/simpleNetwork/loader.cpp:44: undefined reference to `tensorflow::GraphDef::~GraphDef()'\r\n/tmp/ccEdGM3i.o: In function `main':\r\n~/Downloads/simpleNetwork/simpleNetwork/loader.cpp:71: undefined reference to `tensorflow::SessionOptions::SessionOptions()'\r\n~/Downloads/simpleNetwork/simpleNetwork/loader.cpp:71: undefined reference to `tensorflow::NewSession(tensorflow::SessionOptions const&, tensorflow::Session**)'\r\n~/Downloads/simpleNetwork/simpleNetwork/loader.cpp:73: undefined reference to `tensorflow::Status::ToString[abi:cxx11]() const'\r\n~/Downloads/simpleNetwork/simpleNetwork/loader.cpp:81: undefined reference to `tensorflow::GraphDef::GraphDef()'\r\n~/Downloads/simpleNetwork/simpleNetwork/loader.cpp:84: undefined reference to `tensorflow::Env::Default()'\r\n~/Downloads/simpleNetwork/simpleNetwork/loader.cpp:84: undefined reference to `tensorflow::ReadBinaryProto(tensorflow::Env*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, google::protobuf::MessageLite*)'\r\n~/Downloads/simpleNetwork/simpleNetwork/loader.cpp:86: undefined reference to `tensorflow::Status::ToString[abi:cxx11]() const'\r\n~/Downloads/simpleNetwork/simpleNetwork/loader.cpp:93: undefined reference to `tensorflow::Status::ToString[abi:cxx11]() const'\r\n~/Downloads/simpleNetwork/simpleNetwork/loader.cpp:105: undefined reference to `tensorflow::Tensor::Tensor(tensorflow::DataType, tensorflow::TensorShape const&)'\r\n~/Downloads/simpleNetwork/simpleNetwork/loader.cpp:144: undefined reference to `tensorflow::Status::ToString[abi:cxx11]() const'\r\n~/Downloads/simpleNetwork/simpleNetwork/loader.cpp:156: undefined reference to `tensorflow::Tensor::DebugString[abi:cxx11]() const'\r\n~/Downloads/simpleNetwork/simpleNetwork/loader.cpp:127: undefined reference to `tensorflow::Tensor::~Tensor()'\r\n~/Downloads/simpleNetwork/simpleNetwork/loader.cpp:81: undefined reference to `tensorflow::GraphDef::~GraphDef()'\r\n~/Downloads/simpleNetwork/simpleNetwork/loader.cpp:105: undefined reference to `tensorflow::Tensor::~Tensor()'\r\n~/Downloads/simpleNetwork/simpleNetwork/loader.cpp:81: undefined reference to `tensorflow::GraphDef::~GraphDef()'\r\n/tmp/ccEdGM3i.o: In function `tensorflow::Status::operator=(tensorflow::Status const&)':\r\n~/Downloads/simpleNetwork/simpleNetwork/include/third_party/tensorflow/core/lib/core/status.h:106: undefined reference to `tensorflow::Status::SlowCopyFrom(tensorflow::Status::State const*)'\r\n/tmp/ccEdGM3i.o: In function `tensorflow::TensorShapeRep::TensorShapeRep(tensorflow::TensorShapeRep const&)':\r\n~/Downloads/simpleNetwork/simpleNetwork/include/third_party/tensorflow/core/framework/tensor_shape.h:477: undefined reference to `tensorflow::TensorShapeRep::SlowCopyFrom(tensorflow::TensorShapeRep const&)'\r\n/tmp/ccEdGM3i.o: In function `tensorflow::TensorShapeRep::~TensorShapeRep()':\r\n~/Downloads/simpleNetwork/simpleNetwork/include/third_party/tensorflow/core/framework/tensor_shape.h:492: undefined reference to `tensorflow::TensorShapeRep::DestructorOutOfLine()'\r\n/tmp/ccEdGM3i.o: In function `tensorflow::core::RefCounted::~RefCounted()':\r\n~/Downloads/simpleNetwork/simpleNetwork/include/third_party/tensorflow/core/lib/core/refcount.h:79: undefined reference to `tensorflow::internal::LogMessageFatal::LogMessageFatal(char const*, int)'\r\n~/Downloads/simpleNetwork/simpleNetwork/include/third_party/tensorflow/core/lib/core/refcount.h:79: undefined reference to `tensorflow::internal::LogMessageFatal::~LogMessageFatal()'\r\n/tmp/ccEdGM3i.o: In function `tensorflow::core::RefCounted::Ref() const':\r\n~/Downloads/simpleNetwork/simpleNetwork/include/third_party/tensorflow/core/lib/core/refcount.h:82: undefined reference to `tensorflow::internal::LogMessageFatal::LogMessageFatal(char const*, int)'\r\n~/Downloads/simpleNetwork/simpleNetwork/include/third_party/tensorflow/core/lib/core/refcount.h:82: undefined reference to `tensorflow::internal::LogMessageFatal::~LogMessageFatal()'\r\n~/Downloads/simpleNetwork/simpleNetwork/include/third_party/tensorflow/core/lib/core/refcount.h:82: undefined reference to `tensorflow::internal::LogMessageFatal::~LogMessageFatal()'\r\n/tmp/ccEdGM3i.o: In function `tensorflow::TensorShape::TensorShape(std::initializer_list<long long>)':\r\n~/Downloads/simpleNetwork/simpleNetwork/include/third_party/tensorflow/core/framework/tensor_shape.h:273: undefined reference to `tensorflow::TensorShapeBase<tensorflow::TensorShape>::TensorShapeBase(std::initializer_list<long long>)'\r\n/tmp/ccEdGM3i.o: In function `tensorflow::SessionOptions::~SessionOptions()':\r\n~/Downloads/simpleNetwork/simpleNetwork/include/third_party/tensorflow/core/public/session_options.h:28: undefined reference to `tensorflow::ConfigProto::~ConfigProto()'\r\n/tmp/ccEdGM3i.o: In function `std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor>::~pair()':\r\n/usr/include/c++/5/bits/stl_pair.h:96: undefined reference to `tensorflow::Tensor::~Tensor()'\r\n/tmp/ccEdGM3i.o: In function `tensorflow::Status tensorflow::errors::NotFound<char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, char const*>(char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, char const*)':\r\n~/Downloads/simpleNetwork/simpleNetwork/include/third_party/tensorflow/core/lib/core/errors.h:73: undefined reference to `tensorflow::strings::StrCat[abi:cxx11](tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&)'\r\n~/Downloads/simpleNetwork/simpleNetwork/include/third_party/tensorflow/core/lib/core/errors.h:73: undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, tensorflow::StringPiece)'\r\n/tmp/ccEdGM3i.o: In function `tensorflow::TTypes<float, 2ul, long>::Tensor tensorflow::Tensor::tensor<float, 2ul>()':\r\n~/Downloads/simpleNetwork/simpleNetwork/include/third_party/tensorflow/core/framework/tensor.h:535: undefined reference to `tensorflow::Tensor::CheckTypeAndIsAligned(tensorflow::DataType) const'\r\n/tmp/ccEdGM3i.o: In function `tensorflow::TTypes<float, 1, long>::Scalar tensorflow::Tensor::scalar<float>()':\r\n~/Downloads/simpleNetwork/simpleNetwork/include/third_party/tensorflow/core/framework/tensor.h:643: undefined reference to `tensorflow::Tensor::CheckIsAlignedAndSingleElement() const'\r\n/tmp/ccEdGM3i.o: In function `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >* tensorflow::internal::MakeCheckOpString<long, int>(long const&, int const&, char const*)':\r\n~/Downloads/simpleNetwork/simpleNetwork/include/third_party/tensorflow/core/platform/default/logging.h:184: undefined reference to `tensorflow::internal::CheckOpMessageBuilder::CheckOpMessageBuilder(char const*)'\r\n~/Downloads/simpleNetwork/simpleNetwork/include/third_party/tensorflow/core/platform/default/logging.h:186: undefined reference to `tensorflow::internal::CheckOpMessageBuilder::ForVar2()'\r\n~/Downloads/simpleNetwork/simpleNetwork/include/third_party/tensorflow/core/platform/default/logging.h:187: undefined reference to `tensorflow::internal::CheckOpMessageBuilder::NewString[abi:cxx11]()'\r\n~/Downloads/simpleNetwork/simpleNetwork/include/third_party/tensorflow/core/platform/default/logging.h:184: undefined reference to `tensorflow::internal::CheckOpMessageBuilder::~CheckOpMessageBuilder()'\r\n~/Downloads/simpleNetwork/simpleNetwork/include/third_party/tensorflow/core/platform/default/logging.h:184: undefined reference to `tensorflow::internal::CheckOpMessageBuilder::~CheckOpMessageBuilder()'\r\n/tmp/ccEdGM3i.o: In function `Eigen::DSizes<long, 2> tensorflow::TensorShape::AsEigenDSizes<2>() const':\r\n~/Downloads/simpleNetwork/simpleNetwork/include/third_party/tensorflow/core/framework/tensor_shape.h:445: undefined reference to `tensorflow::TensorShape::CheckDimsEqual(int) const'\r\n/tmp/ccEdGM3i.o: In function `Eigen::DSizes<long, 2> tensorflow::TensorShape::AsEigenDSizesWithPadding<2>() const':\r\n~/Downloads/simpleNetwork/simpleNetwork/include/third_party/tensorflow/core/framework/tensor_shape.h:452: undefined reference to `tensorflow::TensorShape::CheckDimsAtLeast(int) const'\r\n~/Downloads/simpleNetwork/simpleNetwork/include/third_party/tensorflow/core/framework/tensor_shape.h:455: undefined reference to `tensorflow::TensorShapeBase<tensorflow::TensorShape>::dims() const'\r\n~/Downloads/simpleNetwork/simpleNetwork/include/third_party/tensorflow/core/framework/tensor_shape.h:456: undefined reference to `tensorflow::TensorShapeBase<tensorflow::TensorShape>::dim_size(int) const'\r\n~/Downloads/simpleNetwork/simpleNetwork/include/third_party/tensorflow/core/framework/tensor_shape.h:458: undefined reference to `tensorflow::TensorShapeBase<tensorflow::TensorShape>::dims() const'\r\n/tmp/ccEdGM3i.o: In function `void std::_Destroy<tensorflow::Tensor>(tensorflow::Tensor*)':\r\n/usr/include/c++/5/bits/stl_construct.h:93: undefined reference to `tensorflow::Tensor::~Tensor()'\r\ncollect2: error: ld returned 1 exit status\r\n```\r\n", "Do you link against libtensorflow_cc.so? You should use a linkflag when you compile your program, -ltensorflow_cc", "@jkravanja yes i did\r\n\r\n`g++ -std=c++11 -o tLoader -Iinclude/third_party -Iinclude -g -Wall -D_DEBUG -Wshadow -Wno-sign-compare -w  -L~/tensorflow/bazel-bin/tensorflow/ `pkg-config --cflags --libs protobuf` -ltensorflow_cc loader.cpp`", "@jkravanja all good now when I run this instead : \r\n\r\ng++ -std=c++11 -o tLoader loader.cpp -Iinclude/third_party -Iinclude -g -Wall -D_DEBUG -Wshadow -Wno-sign-compare -w  -L~/tensorflow/bazel-bin/tensorflow/ -ltensorflow_cc `pkg-config --cflags --libs protobuf`", "Sir i created a trained model using Tensorflow Inception. I use the Flower data set.\r\nBut i cant find .pb file anywhere....\r\n\r\nThe codes are specified below...\r\n\r\n\r\nFLOWERS_DATA_DIR=/tmp/flowers-data/\r\n\r\ncd tensorflow-models/inception\r\nbazel build //inception:download_and_preprocess_flowers\r\n\r\nbazel-bin/inception/download_and_preprocess_flowers \"${FLOWERS_DATA_DIR}\"\r\n\r\n\r\n\r\nNow the .tfr file created sucessfully.\r\n\r\nINCEPTION_MODEL_DIR=$HOME/inception-v3-model\r\nmkdir -p ${INCEPTION_MODEL_DIR}\r\ncd ${INCEPTION_MODEL_DIR}\r\n\r\ncurl -O http://download.tensorflow.org/models/image/imagenet/inception-v3-2016-03-01.tar.gz\r\ntar xzf inception-v3-2016-03-01.tar.gz\r\n\r\n\r\n# this will create a directory called inception-v3 which contains the following files.\r\n> ls inception-v3\r\nREADME.txt\r\ncheckpoint\r\nmodel.ckpt-157585\r\n\r\n\r\n\r\ncd tensorflow-models/inception\r\nbazel build //inception:flowers_train\r\n\r\n# Path to the downloaded Inception-v3 model.\r\nMODEL_PATH=\"${INCEPTION_MODEL_DIR}/inception-v3/model.ckpt-157585\"\r\n\r\n# Directory where the flowers data resides.\r\n\r\n\r\nFLOWERS_DATA_DIR=/tmp/flowers-data/\r\n\r\n# Directory where to save the checkpoint and events files.\r\n\r\n\r\nTRAIN_DIR=/tmp/flowers_train/\r\n\r\n# Run the fine-tuning on the flowers data set starting from the pre-trained\r\n# Imagenet-v3 model.\r\n\r\n\r\nbazel-bin/inception/flowers_train \\\r\n  --train_dir=\"${TRAIN_DIR}\" \\\r\n  --data_dir=\"${FLOWERS_DATA_DIR}\" \\\r\n  --pretrained_model_checkpoint_path=\"${MODEL_PATH}\" \\\r\n  --fine_tune=True \\\r\n  --initial_learning_rate=0.001 \\\r\n  --input_queue_memory_factor=1\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nPlease help me..", "The comments in here are precious!!!\r\nAfter all of this effort, isn't it possible to have an official tensorflow_cc-dev package? isn't it possible to have a more official way of compiling tensorflow based c++ project out of the source tree? It feels like a very important feature to me!", "This instruction also save my time\r\nhttp://www.blitzblit.com/2017/06/11/creating-tensorflow-c-headers-and-libraries/\r\n", "@lababidi Hi, I did the following according to you tips,\r\nmkdir /usr/local/include/tf\r\ncp -r bazel-genfiles/ /usr/local/include/tf/\r\ncp -r tensorflow /usr/local/include/tf/\r\ncp -r third_party /usr/local/include/tf/\r\ncp -r bazel-bin/libtensorflow_cc.so /usr/local/lib/\r\n\r\nand make\r\n\r\ng++ -std=c++11 -o tLoader -I/usr/local/include/tf -I/usr/local/include/eigen3 -g -Wall -D_DEBUG -Wshadow -Wno-sign-compare -w  -L/usr/local/lib/libtensorflow_cc `pkg-config --cflags --libs protobuf`  -ltensorflow_cc loader.cpp\r\n\r\nhowever, I met an error,\r\n\r\n/usr/local/include/tf/tensorflow/core/framework/numeric_types.h:60:34: error: \u2018log\u2019 is not a template function \r\n/usr/local/include/tf/tensorflow/core/framework/numeric_types.h:66:34: error: \u2018exp\u2019 is not a template function    \r\n/usr/local/include/tf/tensorflow/core/framework/numeric_types.h:72:34: error: \u2018abs\u2019 is not a template function\r\n\r\nany suggestions?Thanks\r\n    \r\n\r\n"]}, {"number": 1889, "title": "Make diag_op_test faster", "body": "Removes high rank and large tensors from the test.\n", "comments": []}, {"number": 1888, "title": "Feature request: Ability to specify some GPUs and ignore all others", "body": "Right now, to specify a particular GPU, I do something like this:\n\n```\ngpu = 0\navailable_devices = os.environ['CUDA_VISIBLE_DEVICES'].split(',')\nos.environ['CUDA_VISIBLE_DEVICES'] = available_devices[gpu]\n```\n\nafter which TensorFlow will only allocate resources on the first GPU device. It would be nice to include a native way to do this.\n\nSide note: There isn't much documentation (any?) on `device_count`, so I'm not sure if it's meant to handle this. Either way, I have experimented with the `device_count`, but with no luck: if I use `device_count = {'GPU': 1}`, TensorFlow still allocates memory on all available GPUs.\n", "comments": ["I would like to implement this, as it's important for some things besides just convenience; for example, you cannot use `CUDA_VISIBLE_DEVICES` with CUDA IPC, so Tensorflow cannot be used with anything that needs access to the GPU memory in other processes (without copying it to CPU first). \n\nIs there a particular API that would make sense for this?\n\nSome options:\n1. Another environment variable, besides `CUDA_VISIBLE_DEVICES`, which tells Tensorflow which GPUs to use. For example, `TENSORFLOW_CUDA_DEVICES`, which _must_ be a subset of `CUDA_VISIBLE_DEVICES` (if `CUDA_VISIBLE_DEVICES` is specified).\n2. Modify the `GPUOptions` proto with a `repeated` field along the lines of `gpu_id`, which select which of the `CUDA_VISIBLE_DEVICES` to use. This seems cleaner, but I'm not sure how invasive that change would be (when does Tensorflow allocate GPU memory, call `cuGetDevice`, and otherwise initialize the StreamExecutor? Is it upon `tf.Session()`? Or is it earlier? If it's earlier, this change may be tricky to implement.)\n\nFinally, if I were to implement option (2) or (1), what would I need to do in order to get the contribution accepted to Tensorflow?\n", "@zffchen78 If I were to implement this, what strategy would be most likely to get accepted as a patch to tensorflow?\n\nThe least invasive strategy is one which uses an environment variable, because GPUs are allocated  statically; In `gpu_init.cc` `InitModule()` is called, which eventually calls `ExecutorForDevice`, which allocates the GPUs. The GPUs aren't stored in the `Session`, and are not freed after a session ends; the `CUDAPlatform` is also stored statically and initialized on load. Effectively the architecture was not designed to allow the choice of GPUs to happen dynamically at runtime, but since environment variables are also global and exist at startup, this would be straightforward to implement.\n\nAn alternative route would be to fix the main design issue: Have a `Session` track which GPUs its using, and, when it's done, deallocate the GPUs and destroy the `StreamExecutor` contexts. To do this we would probably have to move the `Platform` from a global static variable into a `Session`-local one, and have it be initialized on session startup rather than global startup.\n\nPlease, let me know which of these is most likely to be accepted by the Tensorflow team if this is implemented and tested. If this is not a viable modification, please remove the \"contributions welcome\" tag. \n", "@gibiansky, independently for another reason I essentially implemented option 2 in your first post earlier this week.  I haven't submitted it yet, but maybe it might work for you.  Concretely, what I did was:\n\nGPUOptions has a field called 'visible_gpu_devices', which is a comma-separated string.  It maps from \"visible\" devices to \"logical\" devices.  As an example, let's say the TensorFlow process has 8 GPUs visible to it.  One could pass visible_gpu_devices=\"5,3\", and it would map visible device 5 as \"/gpu:0\" and visible device 3 as \"/gpu:1\", and it would leave all other devices untouched / uninitialized.\n\nSo it's basically like CUDA_VISIBLE_DEVICES, except CUDA_VISIBLE_DEVICES acts prior to the process starting, and controls what I would call 'physical GPUs' (e.g., what the driver reports) to \"visible GPUs\", which is the order that the process sees it, and this new option would allow one to remap from \"visible\" to \"logical\", with the default empty string meaning that the \"visible\" to \"logical\" map is one to one.\n\nLet me know if you think this satisfies this feature request and I can try to send it for review.\n", "@rdipietro in addition, I believe now the device_count approach should maybe work for your case, since we now only allocate memory lazily on first memory allocation on GPU, so if you never ran an op on GPUs, it shouldn't allocate any memory on it either.\n", "@vrv Yes! This would be perfect! (In fact, I started implementing _literally_ the same thing today, in the same manner.)\n\nMinor questions:\n- Shouldn't that be part of ConfigProto not GPUOptions? (GPUOptions seems like it is settings that affect a single GPU, while ConfigProto has settings related to many GPUs, for example device_count)\n- Why pass `visible_gpu_devices` as a string, rather than as a `repeated int`?\n- To confirm: this interacts fluidly with `CUDA_VISIBLE_DEVICES`. For example, if, on an 8-GPU system, `CUDA_VISIBLE_DEVICES=5,2,0,6`, and you set `visible_gpu_devices=0,3`, Tensorflow will end up running on GPUs 5 and 6.\n- Does this change also mean that, since the GPU info is stored in the `Session`, `Session.close()` can now actually deallocate the GPU and destroy the context? If that's the case that would be _awesome_. (If you look right now, `nvidia-smi` will show that even after `Session.close()`, the GPU contexts still exist and thus you cannot launch more processes that use them.)\n\nThe most important thing is that when `visible_gpu_devices` is used, the contexts are _not_ created on all visible devices \u2013 not just that memory is not allocated on them, but that they are completely uninitialized, so that other concurrently running Tensorflow processes can use them.\n", "- Yeah, we can put it in something other than GPUOptions (I'm loathe to put it top level, I'd rather scope it appropriately).\n- Because that's how CUDA_VISIBLE_DEVICES works, and I kind of wanted it to be 'familiar' to people :).  I could make it repeated too.\n- Yes, exactly.\n- Unfortunately I think some part of the GPU initialization is still global (held by process_state), so destroying the session would not release it.  However, if you never used it, it shouldn't establish any context.  It would be nice to have it release on session close, but it will require some work to properly refcount the currently global state.  Heck, if you wanted to work on that instead, that would be nice.\n", "Great, SGTM. No strong opinions here wrt GPUOptions vs other locations; if it's in GPUOptions it may make sense to change the comment in the ConfigProto GPUOptions field to reflect that GPUOptions are not only per-GPU settings.\n\nThanks! Please ping me / close this issue when this is merged. It will be very helpful for the work I am doing right now.\n", "(I just checked, the comment on GPUOptions in ConfigProto explicitly mentions 'options that applies to all GPUs' -- maybe we'll add a repeated PerGPUOptions field in the future...)\n", "Perfect, I just misinterpreted it then when I read it, for some reason I assumed that meant it was \"single-GPU options that are applied to every single GPU uniformly\". You're right though that my interpretation doesn't make sense given the comment. Oops, sorry!\n", "Well, some of those options could apply individually with different settings for each GPU, so your interpretation is also reasonable -- we just haven't implemented it that way (yet).\n", "@vrv BTW: By implementing a very hacky version of this patch, I have confirmed that with this fix will allow you to use Tensorflow with CUDA IPC. (My hacky version just edited cuda_platform.cc to always select just one GPU based on an environment variable, and to return 1 for VisibleDeviceCount()). So definitely looking forward to this patch landing in Tensorflow master \u2013 do you know a rough ETA for this? Thank you!!\n", "It'll probably in master tonight or tomorrow\n\nOn Thu, Aug 25, 2016 at 3:24 PM, Andrew Gibiansky notifications@github.com\nwrote:\n\n> @vrv https://github.com/vrv BTW: By implementing a very hacky version\n> of this patch, I have confirmed that with this fix will allow you to use\n> Tensorflow with CUDA IPC. (My hacky version just edited cuda_platform.cc to\n> always select just one GPU based on an environment variable, and to return\n> 1 for VisibleDeviceCount()). So definitely looking forward to this patch\n> landing in Tensorflow master \u2013 do you know a rough ETA for this? Thank you!!\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1888#issuecomment-242562890,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAcTeV18AB9kj1oOIZL15pAyWHGD-V2iks5qjhYUgaJpZM4IFrBD\n> .\n", "@rmlarsen @vrv This change got rolled back automatically; please re-open this issue!\n", "It got rolled forward again with a fix.\n", "In which commit was it rolled forward? I just cloned master and do not see the change.\n", "Oh, it hasn't been pushed to master yet (so, on the next push).  Sorry about the wait...\n", "Ah, got it. How do pushes to master work? Do you do one every few hours?\n", "It's a manual process, usually once a day.  Until then, you could sync at the commit that closed this feature (the bug was related to when you run on a machine that doesn't have GPUs, but you compiled with GPU support).\n", "I just tested this patch. It does not work. Please re-open this bug!\n\nYou can verify as follows:\n\n``` python\nimport tensorflow as tf\nconfig = tf.ConfigProto(tf.GPUOptions(visible_device_list=\"0\"))\nsession = tf.Session(config=config)\n\nimport subprocess\nsubprocess.check_call(\"nvidia-smi\")\n```\n\nThe output you will see will look like this on a 2-GPU system:\n\n```\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0     19236    C   /mnt/home/gibiansky/env/bin/python           10893MiB |\n|    1     19236    C   /mnt/home/gibiansky/env/bin/python              73MiB |\n+-----------------------------------------------------------------------------+\n```\n\nSpecifically, note how there are _two_ contexts opened, not just one! That means that no other process can access _either_ GPU.\n\nThe reason this is happening is because `platform->ExecutorForDevice(device)` is being called in more places than what you have addressed. For example, take a look at these pieces of code, that are called upon session startup:\n- [platform.cc:96](https://github.com/tensorflow/tensorflow/blob/d42facc3cc9611f0c9722c81551a7404a0bd3f6b/tensorflow/stream_executor/platform.cc#L96)\n- [platform.cc:113](https://github.com/tensorflow/tensorflow/blob/d42facc3cc9611f0c9722c81551a7404a0bd3f6b/tensorflow/stream_executor/platform.cc#L113)\n- [gpu_init.cc:38](https://github.com/tensorflow/tensorflow/blob/d42facc3cc9611f0c9722c81551a7404a0bd3f6b/tensorflow/core/common_runtime/gpu/gpu_init.cc#L33)\n\nI believe there are others as well; you can get a complete list with `grep -iIr \"->ExecutorForDevice\" /path/to/tensorflow/directory`.\n\nI believe the trickiest bit is in `gpu_init.cc`. Specifically, if you look at `gpu_init.cc:161`, you see that the first time `GPUMachineManager()` is called (which happens upon Session creation), it calls `InitModule()`. `InitModule` in turn calls `InitGPU()`, which has the devilish line of code:\n\n``` c++\nint dev_count = platform->VisibleDeviceCount();\n...\nfor (int i = 0; i < dev_count; ++i) {\n  auto executor = platform->ExecutorForDevice(i);\n  ...\n}\n```\n\nWhat this means is that the first time anything related to GPUs happens (and thus a GPU machine manager is created), you find out the number of visible devices and initialize every single one of them, _no matter what_. I emphasize the \"no matter what\" because GPUMachineManager() takes no parameters, and neither does `InitModule`. Because all of this is static global state there is nothing you can do per-session that fixes this.\n\nMy approach was a bit different, and IMHO uglier but less fragile:\n- Inside `CUDAPlatform`, check for a `TENSORFLOW_VISIBLE_DEVICES` environment variable, which was exactly the same in meaning as your `visible_device_list` configuration option.\n- If that environment variable exists, then instead of returning `CUDADriver::DeviceCount()` from `VisibleDeviceCount()`, return `visible_device_list.size()`, where `visible_device_list` is the variable holding the parsed contents of `TENSORFLOW_VISIBLE_DEVICES`.\n- Whenever you call `ExecutorForDevice` (or any variant thereof), remap the provided ordinal as directed by `visible_device_list`. That way, you _know_ that Tensorflow cannot accidentally claim more GPUs than it is allowed to, because `CUDAPlatform` is the only available interface to GPUs, and it does not allow any caller to touch GPUs that it should not be seeing.\n\nWhat do you think? Is that a viable strategy? The current patch does not solve this problem at all (whereas the approach described above is uglier but fixes the issue). There _is_ an alternative which would be refactoring `CUDAPlatform` to be a part of the session state instead of the global device, and then having things like `GPUMachineManager` take and store a `CUDAPlatform`. This might be a pretty invasive change though and I'm not sure how many other pieces of global state would have to be pulled into local state in order to get this to work.\n", "Ah, I didn't realize the StreamExecutor was establishing contexts too.  We don't maintain StreamExecutor ourselves, so I'll have to talk with the owners to figure out how to get this working as intended.\n", "Thank you! Yes, sadly if you look at the code behind `ExecutorForDevice`, it eventually calls [`GetUncachedExecutor`](https://github.com/tensorflow/tensorflow/blob/d42facc3cc9611f0c9722c81551a7404a0bd3f6b/tensorflow/stream_executor/cuda/cuda_platform.cc#L149); this creates a new `CUDAExecutor` and calls `Init`; in turn, that calls `CUDADriver::GetDevice`, which then calls `cuDeviceGet`, which is the low-level call that I believe actually creates the context.\n\nIt seems like in this case StreamExecutor is doing everything right \u2013 when you call `ExecutorForDevice`, it _should_ allocate a context and prepare for running things on the device, since you're about to run things on the device! (That's what executors do, presumably.) However, the mistake is in Tensorflow core -- Tensorflow shouldn't be asking for the stream executors in the first place.\n", "FYI, if the environment variable-based approach I described above is acceptable, I am happy to make a patch and submit it as a PR.\n\nPersonally I think it's a little bit ugly and if there's another way I would prefer the other way. But the alternative may be a pretty big and annoying refactor of where `CUDAPlatform` is stored (globally vs in session state).\n", "So we can't put TF specific code in stream executor because StreamExecutor is being opensourced separately from TensorFlow, so whatever solution we choose has to work more generally.  If they are happy with the environment variable approach, we could add it.\n\nStreamExecutor was never really designed to co-exist well with other processes, unfortunately.\n", "Ah, I understand. You are correct -- `cuda_platform.cc` is part of StreamExecutor, not Tensorflow.\n\nI cannot think of a way of doing this without modifying StreamExecutor or duplicating part of its functionality. At the very least `GetPeerAccessMap` and `EnablePeerAccess` are broken in their current implementations.\n\nThank you for continuing to push on this; this is really important for being able to scale Tensorflow. Please let me know what I can do to help this along -- if you or other members of the team are busy and so it will take a while to get this going, but you think this is a patch that an external contributor will be able to push through, please let me know what I can do. I understand that this may be tricky and that it may require an internal contributor, though.\n", "I played around with some approaches only touching code in gpu_init.cc and gpu_device.cc and got it so that it didn't establish a context on a device that wasn't in gpu_device_list.  The only implication is that once established, I don't think we could de-establish it.  That is, if you started with gpu_device_list=\"0,1\" but then closed that session and reopened it with \"1\", I'm pretty sure 0 would still have the context established.\n\nBut I think we could do it so that if you did \"0\", it would only establish on 0, and in the same process, you could probably then add \"0,1\" in another session and it might still work.  Would that satisfy your requirements?\n", "Ultimately yes if it's possible to start a Tensorflow session without defining CUDA_VISIBLE_DEVICES and use `nvidia-smi` to verify that only a single context is active... then I am happy :) \n\nObviously this isn't the best in the long-term but for the short term (and for my requirements) it's definitely enough!\n", "Any progress on an updated patch? :) \n", "Yeah, I just submitted it, should be pushed by tomorrow some time.\n", "Awesome, thanks @vrv! I'll test this out today but it looks very promising.\n", "Just tested this out today.\n\nIt's closer than the previous one... but still not quite there -- reopen the issue again? The bug here is insidious and somewhat unexpected:\n\n:(\n\nIt comes from the following line: [gpu/process_state.cc:188](https://github.com/tensorflow/tensorflow/blob/d42facc3cc9611f0c9722c81551a7404a0bd3f6b/tensorflow/core/common_runtime/gpu/process_state.cc#L188)\n\nIf you take a look at that, you see that when you allocate memory on the host that is known to CUDA, you always use `ExecutorForDevice(0)`, even though `0` may not be in your `visible_device_list`. \n\nYou can test this with:\n\n``` python\nimport time\nimport tensorflow as tf\n\nc = tf.ConfigProto()\nc.gpu_options.visible_device_list=\"1\"\n\ns = tf.Session(config=c)\ns.run(tf.constant(10))\ntime.sleep(5)\n```\n\nAnd then if you look at `nvidia-smi` you will see two contexts, one on 0 and one on 1, even though only one was requested. (The difference between this test and the previous one is that you use a `s.run`)\n\nI'm not sure this is fixable without changing stream executor, but the patch isn't super helpful in its current state :(\n", "Ha, fun times.  I think it is fixable without too much work:\n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/process_state.cc#L99\n\ngpu_allocators_ is a sparse array indexed by visible gpu id.  So instead of using '0', we need to find any non-null entry in gpu_allocators_ and use the index of that executor.  I believe we are guaranteed that the gpu_allocators_ are allocated prior to the first call to GetCudaHostAllocator.\n\nIf you want to give that a try, let me know, otherwise I will try to find time to get to it.\n", "SGTM, I'll give it a shot.\n", "PR opened, I've confirmed that with that change `nvidia-smi` gives the desired output. I don't know whether there is any code style that I should be observing but am not, so just let me know what needs to be done to get this merged (whether by merging my almost-trivial PR or something else).\n", "I am seeing a problem with this implementation on some machines, where the device ordering used by nvidia-smi (and the NVML library) doesn't match the ordering in cuDeviceGet().  On those machines you end up having to specify a different number than the obvious one that you might see via nvidia-smi or via the python pynvml library.  Please see my comment in #152.\r\n\r\n\r\n", "I could use config.gpu_options.visible_device_list in python, and it works.\r\nBut when I try to set the visible device list in c++ using the protobuf methods (eg. set_visible_device_list), it not only changes the visible_device_list field, but also the allocator_type field), and I got the error \"Invalid allocator type: 0,1,2,3\".\r\n\r\nIt seems that the two fields got mapped into the same string address. Anyone knows what might be going on?", "@RexYing,  meet the same invalid allocator type issue, have you figured out the solution? thanks. ", "I met the same issue, any solutions? thanks.", "I have the same issue. When I use C++ api 'set_visible_device_list(\"0\")', the program crash and said 'E tensorflow/core/common_runtime/gpu/process_state.cc:130] Invalid allocator type: 0\r\nSegmentation fault (core dumped)'\r\n\r\nI think it's a bug. @kitterive @bravelywangexocr @RexYing ", "@kitterive @bravelywangexocr @RexYing @mattdingmeng \r\n\r\nJust want to create more context, I tried to debug a little bit and I found that when I called set_visible_device_list(\"1,2\"), it also set \"allocator_type\" to \"1,2\", which is an invalid value for allocator_type. That's why the error message is \"Invalid allocator type: 1,2\". \r\n\r\nIn other words, allocator_type and visible_device seem to share the same memory location.\r\nhttps://github.com/tensorflow/tensorflow/blob/3378865c5509dfb6d18e6f95f28757437f67d3da/tensorflow/core/protobuf/config.proto\r\n\r\nPlease see the code snippet below as an example\r\n\r\n```\r\nauto options = tensorflow::SessionOptions();\r\nconst auto& static_gpu_options = options.config.gpu_options();\r\nauto mutable_gpu_options = options.config.mutable_gpu_options();\r\n\r\nstd::cout << static_gpu_options.visible_device_list() << std::endl;     // print \"\"\r\nstd::cout << static_gpu_options.allocator_type() << std::endl;          // print \"\"\r\n\r\nmutable_gpu_options->set_visible_device_list(\"1\");\r\n\r\nstd::cout << static_gpu_options.visible_device_list() << std::endl;     // print \"1\"\r\nstd::cout << static_gpu_options.allocator_type() << std::endl;          // print \"1\"\r\n\r\nmutable_gpu_options->set_allocator_type(\"\");\r\n\r\nstd::cout << static_gpu_options.visible_device_list() << std::endl;     // print \"\"\r\nstd::cout << static_gpu_options.allocator_type() << std::endl;          // print \"\"\r\n```", "@ChengshuLi \r\nThe same issue when I use c++ API and my tensorflow version is 1.5.0. I also find that it works if I compile the source codes without `--config=monolithic` option and use `libtensorflow_cc` and `libtensorflow_framework` as a dynamic library linked into my project. Theis problem only arises when I compile with `--config=monolithic` option to get a single dynamic library `libtensorflow_cc`."]}, {"number": 1887, "title": "Adding a link to the playground", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I'm a googler, so I don't need to sign the CLA.\n", "You need to use your @google.com email address in your commit. Otherwise you need to sign the CLA.\n", "I signed it! (with my github email address)\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Jenkins, test this please.\n"]}, {"number": 1886, "title": "Make diag_op_test faster", "body": "Removes high rank and large tensors from the test.\n", "comments": []}, {"number": 1885, "title": "Fix chat link at start of README", "body": "Original formatting for this link included the pipe character at the end, so that clicking it would lead to a 404. The more explicit URL formatting makes it so that clicking the link leads to the correct chat room.\n", "comments": ["Can one of the admins verify this patch?\n"]}, {"number": 1884, "title": "Bumping version: 0.7.1 --> 0.8.0rc0", "body": "Preparing for RC0 of 0.8.0\n", "comments": []}, {"number": 1883, "title": "Bumping version: 0.7.1 --> 0.8.0rc0", "body": "Preparing for RC0 of 0.8.0\n", "comments": []}, {"number": 1882, "title": "Distortions in Retrain.py (Transfer learning w/ inception model) take exceedingly long", "body": "### Environment info\n\nOperating System:\nUbuntu 14.04\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\nCUDA: 7.5\ncuDNN: 6.5 (v2)\nUsing nVidia Quadro K2200\n\nIf installed from sources, provide the commit hash:\nfe9dafd1583da5ccc205eab776f86afcb00411d2\n### Steps to reproduce\n\nRetrain.py runs fine without any distortions. Not massively faster than CPU, but faster.\nNVIDIA-SMI shows that the GPU has 3824mb used during training.\n\nIf any distortions are used, training becomes very, very, long. It takes almost ten minutes for ten images to be trained.\n\nIs this very slow speed intended?\n\nBest\n\nOren\n", "comments": ["Hi @oweingrod,\n\nyou probably find the explanation for the slow training [here](https://github.com/tensorflow/tensorflow/blob/d4bf5e072478c92cae2bd71b96e10e77e229dfba/tensorflow/examples/image_retraining/retrain.py#L497).\nIt's said that bottleneck caching cannot be applied if distortions are enabled. That means the bottlenecks are calculated by the inception model for each image and each step.\n\nEven it makes sense that the randomized distortion is applied ad-hoc when an images is processed, I wonder if it would be possible to apply the distortions before the training is executed to create more training examples. Then we are able to precompute and cache the bottlenecks again. I see one tricky problem arising: The handling of train and validation splits will be trickier.\n\nCheers,\nMax\n", "@petewarden: Assigning you since it's about retraining.\n", "I'm moving this one to contributions welcome, since I'm unlikely to have a chance to work on this.\n", "@nealwu, could you consider this bug in your creation of garden models.", "Yep, we'll keep retraining in mind. We shouldn't run into this issue in the garden though, at least not with our current set of models. As far as this issue, I'll un-assign myself since there's nothing immediate I can help with.", "I had the same issue. How about adding some noise to precalculated bottlenecks?\r\n\r\nI'm not sure if this would work, but adding a random vector to each precalculated bottleneck before training with them is cheap enougth. Since those bottlenecks are in the end a feature vector over which there is a linear classifier, it makes sense for me... doesn't it?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 1881, "title": "Update website with correct Cuda info", "body": "I received this on twitter:\n\"Can you please help update TF website to state TF-GPU needs nvidia compute cap. >= 3.5 and to install cudNN v4 (not 5). Thanks\"\nhttps://twitter.com/AlliseeSolution/status/719789250082766848\n\nI'm not using Cuda myself, so I'm not sure of the details here, but wanted to get it logged so somebody more knowledgeable can take a look.\n", "comments": ["Thanks Pete!  I twittered that.  I'm new to github community.  Here is a screenshot. \n\nhttps://www.dropbox.com/s/b9dhbz46iqhfer7/tf.jpg?dl=0\n", "@alliseesolutions: which documentation was incorrect?  https://www.tensorflow.org/versions/r0.7/get_started/os_setup.html#optional-install-cuda-gpus-on-linux appears accurate but we may have stale docs somewhere.\n", "Oh, our pip install for 0.7.1 was only built for cuda 3.5+, but I believe our installs for 0.8.0+ will probably support 3.0 natively.\n", "If you build from sources you can use both cudnnv5 and cuda compute 3.0.  We only build pips for the latest official cudnn (v4), and the nightly pip packages now are built with 3.0 support.\n", "Compiled from source everything worked fine with my Cuda Compute Cap. 3.0 card.  \n", "Also tested that latest 0.8 pip release with cudnn v4 and it works fine with the card as well.  I may suggest adding to the cuda installation instructions that cudnn v4 should be installed if not installing from source.\n", "I believe our docs https://www.tensorflow.org/versions/r0.9/get_started/os_setup.html are currently accurate.\n"]}, {"number": 1880, "title": "make the formula to be centralized", "body": "remove the period after the formula to make the formula centralized, so it can be consistent with other formulas\n", "comments": ["Can one of the admins verify this patch?\n"]}, {"number": 1879, "title": "Evaluate dev set in Translate.py: the decoder stills fetches from decoder_inputs, instead of previous predictions.", "body": "In the training process defined in Translate.py, the model is created with `model = create_model(sess, False)` where `forward_only=False` means every step of decoding the decoder would fetch from decoder_inputs, instead of previous prediction. \n\nThat's the same for evaluation of dev set, since the command \n`_, eval_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, True)` \nwith `forward_only=True` actually doesn't change the decoding way. (It just decides the backward.)\n\nSo I guess the evaluation of dev set is not a real evaluation? Or is it a trick? I guess the correct way should be creating a model with current checkpoint params and set `forward_only=True`.\n\nThanks.\n", "comments": ["It's exactly as you say! I think it's a bit too strong to call it \"not real\" -- the reported perplexities come from teacher-forced examples, and so are indeed not fully representative for what you'll get during decoding. But some people report this kind of perplexities in their papers and they are often a good measure to judge how the training process is progressing. In particular, I think they are enough to determine if the model is overfitting or not and whether it is converging or not. You could create a second model with the same parameters and `forward_only=True` and use that for decoding, but it'd use a lot of memory, that's why it's not done by default.\n\nTo get a final measure of the model quality, it's best to take a separate test set, decode it using the `decode` function [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/translate/translate.py#L208](which uses `forward_only=True` of course) and then use any measure on the result, e.g. BLEU. I'd also recommend to use a test set that's disjoint from the dev set for that, as you might be tuning hyper-parameters on the dev set.\n"]}, {"number": 1878, "title": "Adding server/Dockerfile.test to versioning roll", "body": "", "comments": []}, {"number": 1877, "title": "Word2vec basic show id and word together ", "body": "When I first read the output, I spent a lot of time to understand these numbers to match them. Now, words along with the ids clearly show what's going on in the sample data and prediction pairs.\n\n[New output]: clearly shows the word ids and corresponding words\n      Sample data [5239, 3084, 12, 6, 195, 2, 3137, 46, 59, 156] ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n      3084 originated -> 12 as\n      3084 originated -> 5239 anarchism\n      12 as -> 6 a\n      12 as -> 3084 originated\n      6 a -> 12 as\n      6 a -> 195 term\n      195 term -> 6 a\n      195 term -> 2 of\n\n[Old output]: No words for Sample data. Word ids and words are mixed, so it's very hard to read\n      Sample data [5239, 3084, 12, 6, 195, 2, 3137, 46, 59, 156] ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n      3084 -> 5239\n      originated -> anarchism\n      3084 -> 12\n      originated -> as\n      12 -> 3084\n      as -> originated\n      12 -> 6\n      as -> a\n      6 -> 195\n      a -> term\n      6 -> 12\n      a -> as\n      195 -> 6\n      term -> a\n      195 -> 2\n      term -> of\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins: test this please\n", "Can one of the admins verify this patch?\n", "@vrv Hmm. I am not sure where/how this BR brakes those tests. It was fine in my environment. I'll double check.\n", "no, jenkins broke, we restarted it.  let me try again:  @tensorflow-jenkins test this please\n", "Jenkins keeps breaking but python3/mac tests passed so this is syntactically fine.  Merging.\n"]}, {"number": 1876, "title": "add reference for CBOW and skip-gram", "body": "add reference for CBOW and skip-gram in word-vec tutorial\n", "comments": ["Can one of the admins verify this patch?\n"]}, {"number": 1875, "title": "Gradient for MatrixTriangularSolve ", "body": "We have a gradient op in master for MatrixSolve . It seems like it shouldn't be too difficult to define a gradient for MatrixTriangularSolve in a similar way. Are there any imminent plans to do this? @rmlarsen @girving ?\n\nI actually had a look at doing it myself but I was getting some strange numerical behaviour from the finite differences test. Possibly just something I am doing wrong. If we can work out what is going on with the test I would be happy to help. \n", "comments": ["What numerical issues were you seeing, and what matrix were you differentiating around?  If the inverted matrix has bad condition number the gradients will be very unstable.\n", "Hi Geoffrey,\nThanks for your quick response. I looked here for inspiration on the code:\n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/linalg_grad.py#L79\n\nYou need to account for the fact that the matrix will change from being upper triangular to lower triangular and vice versa when you take the adjoint but that's fine. \n\nI then played with adding the MatrixTriangularSolve op here:\n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/linalg_grad_test.py#L133\n\nand using np.tril to give it lower triangular random inputs.  I'm not sure I did this in a very tidy way. But in any case I was getting big differences between the finite differences and the backprop. Perhaps as you suggest random lower triangular matrices are very ill conditioned. \n\nI could submit as a PR with help needed if you like? I thought if you guys where doing this anyway that it might be more efficient not to tread on your toes.\n", "We're happy to look at a PR.  Everything you've written sounds reasonable, so the next steps are all details. :)\n\nThe main difference between lower triangular and full here is that the determinant of a triangular matrix is the product of the diagonal entries, so you have `n` entries where a zero makes the entire matrix singular regardless of the other entries.  I'd try picking the diagonal to have random values bounded well away from zero.\n", "As mentioned on the PR, I actually just submitted support for backprop for  {batch_}matrix_solve, {batch_}triangular_solve, and batch_matrix_determinant.\n"]}, {"number": 1874, "title": "Does TensorBoard (TensorFlow) have the features to add labels for axes and legends on plots? If so, how? ", "body": "Need to know if these features are available for scalar and histogram summaries in the  python3  GPU-enabled 0.7.1 version\n", "comments": ["Hello,\n\nGitHub issues are for bugs / installation problems / feature requests.\n\nFor general support from the community, see StackOverflow.\n"]}, {"number": 1873, "title": "skflow: trivial fixup", "body": "This pull request contains trivial fixup for `examples/skflow`.  It removes mistakenly added files that related with merge conflict and fix trivial typos in comments.\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins: test this please\n"]}, {"number": 1872, "title": "How to get top N results for seq2seq?", "body": "In the example translate seq2seq model, mentioned in the tutorial and in the code here (reproduced below):\n\n```\n      # Get a 1-element batch to feed the sentence to the model.\n      encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n          {bucket_id: [(token_ids, [])]}, bucket_id)\n      # Get output logits for the sentence.\n      _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs,\n                                       target_weights, bucket_id, True)\n      # This is a greedy decoder - outputs are just argmaxes of output_logits.\n      outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n\n```\n\nIt uses only a greedy decoder, and uses argmax to find the best match. I'm wondering if there's a way to get the top N results instead of just doing it greedily. I've tried argsort, but everything apart from the 0th index is just garbage results. I've also looked into tf.nn.top_k(), but because this is batched, I get the error \"List of Tensors when single Tensor expected\" and I'm not sure how to unroll the list within TF.\n", "comments": ["To get top-N results in a sequential network you need to run a beam search. While it's not implemented in the tutorial, there were already some suggestions and code on github -- please see #654 .\n"]}, {"number": 1871, "title": "Fix trivial typos in source codes / comments", "body": "- Fix trivial typos in source codes / comments\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n", "Can one of the admins verify this patch?\n", "I'm killing the test jobs for now to make space for release tests (sorry!).\n", "I'll restart them later.\n", "Linux GPU PIP passed, and this was just doc changes, so I'll merge.\n"]}]