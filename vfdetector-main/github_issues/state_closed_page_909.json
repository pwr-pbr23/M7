[{"number": 26194, "title": "Updated operator.cc", "body": "Fixed the warnings in the operator.cc file", "comments": ["@amitsrivastava78  please club related PR's #26193 ,#26192,#26188", "@rthadur  I have merged the PRs as requested by you.", "> @rthadur I have merged the PRs as requested by you.\r\n\r\nthank you @amitsrivastava78 ", "thanks @srjoglekar246 , i have updated the code as per your comments, kindly check and approve.", "@srjoglekar246 , i have updated the code as per the comments, kindly check and approve."]}, {"number": 26193, "title": "Updated export.cc", "body": "Reserved the space for shape for speed up", "comments": ["clubbed these changes to #26194 "]}, {"number": 26192, "title": "Updated tooling_util.cc", "body": "updated the datatype to Const", "comments": ["clubbed these changes to #26194 "]}, {"number": 26191, "title": "warnings (please do not import '@grpc//third_party/nanopb:pb_common.c' directly ; depends on deprecated target ; ...) during build from source", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.2 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: r1.12\r\n- Python version: Python 3.6.8 :: Anaconda, Inc.\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 0.19.2\r\n- GCC/Compiler version (if compiling from source): 6.5.0\r\n- CUDA/cuDNN version: 9.0 / 7.5.0\r\n- GPU model and memory: GeForce GT 650M / 2 GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nI try to compile TF from source to get compute capability 3.0 support.\r\nTherefore I followed more or less the guide on [this site](https://medium.com/@mccann.matt/compiling-tensorflow-with-cuda-3-0-support-42d8fe0bf3b5).\r\nBut I get warnings and the compilation failed.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n./configure\r\n\r\nbazel build --config=opt --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow/tools/pip_package:build_pip_package\r\n\r\n(see details below)\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nNot sure if important but I added `import /home/jonathan/tensorflow/tools/bazel.rc` on top line of(hide file) \"/home/jonathan/tensorflow/.bazelrc \" as stated [here](https://github.com/tensorflow/tensorflow/issues/23401#issuecomment-435827786).\r\n\r\n\r\n\r\n```\r\n$ ./configure\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.19.2 installed.\r\nPlease specify the location of python. [Default is /home/jonathan/anaconda2/envs/tf_cu90/bin/python]: \r\n\r\n\r\nFound possible Python library paths:\r\n  /home/jonathan/anaconda2/envs/tf_cu90/lib/python3.6/site-packages\r\nPlease input the desired Python library path to use.  Default is [/home/jonathan/anaconda2/envs/tf_cu90/lib/python3.6/site-packages]\r\n\r\nDo you wish to build TensorFlow with Apache Ignite support? [Y/n]: \r\nApache Ignite support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]: \r\nXLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: \r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: \r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 9.0]: \r\n\r\n\r\nPlease specify the location where CUDA 9.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /usr/local/cuda-9.0\r\n\r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]: \r\n\r\n\r\nPlease specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda-9.0]: /usr/lib/x86_64-linux-gnu\r\n\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: \r\nNo TensorRT support will be enabled for TensorFlow.\r\n\r\nPlease specify the NCCL version you want to use. If NCCL 2.2 is not installed, then you can use version 1.3 that can be fetched automatically but it may have worse performance with multiple GPUs. [Default is 2.2]: 1.3\r\n\r\n\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.0]: \r\n\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]: \r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: /usr/bin/gcc-6\r\n\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: \r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: \r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See tools/bazel.rc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\n\t--config=gdr         \t# Build with GDR support.\r\n\t--config=verbs       \t# Build with libverbs support.\r\n\t--config=ngraph      \t# Build with Intel nGraph support.\r\nConfiguration finished\r\n\r\n```\r\n\r\n```\r\n$ bazel build --config=opt --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow/tools/pip_package:build_pip_package\r\n\r\nLoading: \r\nLoading: 0 packages loaded\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 0 targets configured)\r\nWARNING: /home/jonathan/.cache/bazel/_bazel_jonathan/e7c09fc463511989ded3d56396c466d4/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/jonathan/.cache/bazel/_bazel_jonathan/e7c09fc463511989ded3d56396c466d4/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nWARNING: /home/jonathan/.cache/bazel/_bazel_jonathan/e7c09fc463511989ded3d56396c466d4/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/jonathan/.cache/bazel/_bazel_jonathan/e7c09fc463511989ded3d56396c466d4/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nWARNING: /home/jonathan/.cache/bazel/_bazel_jonathan/e7c09fc463511989ded3d56396c466d4/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/jonathan/.cache/bazel/_bazel_jonathan/e7c09fc463511989ded3d56396c466d4/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nWARNING: /home/jonathan/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/jonathan/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/jonathan/tensorflow/tensorflow/contrib/timeseries/python/timeseries/BUILD:354:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries:ar_model: target '//tensorflow/contrib/timeseries/python/timeseries:ar_model' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /home/jonathan/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:73:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /home/jonathan/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:230:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /home/jonathan/tensorflow/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /home/jonathan/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /home/jonathan/tensorflow/tensorflow/contrib/BUILD:13:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nINFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 0 targets configured).\r\nINFO: Found 1 target...\r\n[0 / 4] [-----] ProtoCompile tensorflow/core/example/example_pb2.py\r\n[5 / 21] Compiling tensorflow/core/ops/nn_ops.cc [for host]; 2s local ... (8 actions running)\r\n[6 / 22] Compiling tensorflow/core/ops/nn_ops.cc [for host]; 6s local ... (7 actions running)\r\n[13 / 35] Compiling tensorflow/core/ops/nn_ops.cc [for host]; 9s local ... (8 actions running)\r\n[17 / 37] Compiling tensorflow/contrib/tensor_forest/hybrid/core/ops/routing_gradient_op.cc; 6s local ... (8 actions running)\r\n[24 / 46] Compiling tensorflow/contrib/tensor_forest/hybrid/core/ops/k_feature_gradient_op.cc [for host]; 6s local ... (8 actions running)\r\n[38 / 73] Compiling tensorflow/python/framework/python_op_gen_internal.cc [for host]; 7s local ... (8 actions, 7 running)\r\nINFO: From Compiling tensorflow/python/framework/python_op_gen_internal.cc [for host]:\r\ntensorflow/python/framework/python_op_gen_internal.cc: In member function 'virtual std::__cxx11::string tensorflow::python_op_gen_internal::GenPythonOp::Code()':\r\ntensorflow/python/framework/python_op_gen_internal.cc:542:44: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = op_def_.input_arg_size(); i < params_no_default.size(); ++i) {\r\n                                          ~~^~~~~~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/python/framework/python_op_gen_internal.cc:545:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = 0; i < params_with_default.size(); ++i) {\r\n                   ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nINFO: From Compiling tensorflow/python/framework/python_op_gen.cc [for host]:\r\ntensorflow/python/framework/python_op_gen.cc: In function 'std::__cxx11::string tensorflow::{anonymous}::VectorToTuple(const std::vector<std::__cxx11::basic_string<char> >&)':\r\ntensorflow/python/framework/python_op_gen.cc:65:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = 0; i < l.size(); ++i) {\r\n                   ~~^~~~~~~~~~\r\ntensorflow/python/framework/python_op_gen.cc: In function 'void tensorflow::{anonymous}::Unflatten(const string&, const std::vector<std::__cxx11::basic_string<char> >&, const string&, std::__cxx11::string*)':\r\ntensorflow/python/framework/python_op_gen.cc:77:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = 0; i < output_sizes.size(); ++i) {\r\n                   ~~^~~~~~~~~~~~~~~~~~~~~\r\n\r\n[...]\r\n\r\n\r\nERROR: /home/jonathan/tensorflow/tensorflow/core/kernels/BUILD:2951:1: output 'tensorflow/core/kernels/_objs/cwise_op_gpu/cwise_op_gpu_bitwise_and.cu.pic.o' was not created\r\nINFO: From Compiling tensorflow/core/kernels/cwise_op_gpu_ceil.cu.cc [for host]:\r\n./tensorflow/core/kernels/cwise_ops.h(190): warning: __host__ annotation on a defaulted function(\"scalar_left\") is ignored\r\n\r\n./tensorflow/core/kernels/cwise_ops.h(190): warning: __device__ annotation on a defaulted function(\"scalar_left\") is ignored\r\n\r\n./tensorflow/core/kernels/cwise_ops.h(220): warning: __host__ annotation on a defaulted function(\"scalar_right\") is ignored\r\n\r\n./tensorflow/core/kernels/cwise_ops.h(220): warning: __device__ annotation on a defaulted function(\"scalar_right\") is ignored\r\n\r\n./tensorflow/core/kernels/cwise_ops.h(190): warning: __host__ annotation on a defaulted function(\"scalar_left\") is ignored\r\n\r\n./tensorflow/core/kernels/cwise_ops.h(190): warning: __device__ annotation on a defaulted function(\"scalar_left\") is ignored\r\n\r\n./tensorflow/core/kernels/cwise_ops.h(220): warning: __host__ annotation on a defaulted function(\"scalar_right\") is ignored\r\n\r\n./tensorflow/core/kernels/cwise_ops.h(220): warning: __device__ annotation on a defaulted function(\"scalar_right\") is ignored\r\n\r\nERROR: /home/jonathan/tensorflow/tensorflow/core/kernels/BUILD:2951:1: output 'tensorflow/core/kernels/_objs/cwise_op_gpu/cwise_op_gpu_mul.cu.pic.o' was not created\r\nERROR: /home/jonathan/tensorflow/tensorflow/core/kernels/BUILD:2951:1: not all outputs were created or valid\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 10303.024s, Critical Path: 9226.48s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]\r\nINFO: 1813 processes: 1813 local.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nfor full log-file see \r\n[log.txt](https://github.com/tensorflow/tensorflow/files/2913016/log.txt)", "comments": ["@Thajobe Please check tested build configuration for your system [here](https://www.tensorflow.org/install/source#linux). For TF1.12, Bazel 0.15.0 is recommended. Could you downgrade it from Bazel 0.19.0 to 0.15.0 and follow instructions listed [here](https://github.com/rnreich/ubuntu-tensorflow-gpu-all-versions). Another suggestion is to start fresh. So it is better to uninstall python and tensorflow and then follow the instruction in the listed doc. Please let me know how it progresses. Thanks!", "@jvishnuvardhan thank you for your support.\r\nI tried with Bazel 0.15.0 and with fresh tensorflow (r1.12) download in a new conda environment but this doesn't help.\r\nWith Bazel 0.15.0 I get additional problems (the other problems still persist):\r\n```\r\nDEBUG: /home/jonathan/.cache/bazel/_bazel_jonathan/e7c09fc463511989ded3d56396c466d4/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5: \r\nAuto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\\Windows\\Temp' as default\r\n```\r\nwhat was one reason I used 0.19.2 as mentioned [here](https://github.com/tensorflow/tensorflow/issues/22654#issuecomment-464521582).\r\n\r\nAlso I tried with NCCL 2.2.13 installed though  it's optional for multiple GPU. But it didn't help.\r\nAny other ideas?", "@Thajobe,\r\n\r\nAs we have the latest stable version of tensorflow `2.6.0`, Can you try to build from source using the [guide](https://www.tensorflow.org/install/source) and let us know if it works fine. Thanks!", "@sanatmpa1 Thank you for your late response. I didnt't even expected it now, because as you said the latest version is 2.6.2 way beyond the 1.12.\r\nThat's why I didnt't close the issue, or forgot about it.\r\nBack than I coudn't get it to work with gpu, but I was able to work on a remote server instead.\r\nI currently not even have the same computer any more, so I can't try if I would work with the current version.\r\nAs I'm not trying to compile TF from source right now, I will close this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26191\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26191\">No</a>\n"]}, {"number": 26190, "title": " Adding GANAuxiliaryLoss", "body": "Add a new namedtuple to contain the auxiliary loss settings.\r\nAdd a named argument to GANEstimator constructor,\r\nso that GANEstimator can make use of auxiliary losses.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26190) for more info**.\n\n<!-- need_author_cla -->", "Use work e-mail on one of the commit, re-signed cla from work account.", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26190) for more info**.\n\n<!-- ok -->", "The ability to use special TF-GAN behavior with GANEstimator will be added when tensorflow_gan is released as its own repo. I'm just waiting for internal approval. @martinwicke ", "Any estimate on a time frame? I was also going to make a pull request to better control what is returned from GANEstimator.predict, should I hold off on that as well?", "The code has been ready to be pushed for over a month, but we're waiting on an internal process. Sorry I can't be more specific on timeframe. @martinwicke for more detail.\r\n\r\nPerhaps in the meantime you could make local code changes for whatever you're working on, and you can switch to the new version when it becomes available (it's a bit cleaner, too). Once that's available, I *would* really appreciate a PR giving finer control over the generator output. Sound reasonable?", "Can one of the admins verify this patch?", "The `tf.contrib` submodule will not be part of tensorflow after the `2.0` release.\r\n\r\nIt looks like the [tensorflow_gan repo is up](https://github.com/tensorflow/gan).\r\n\r\nSo please remake this PR on that repo. \r\n\r\n\r\nThanks."]}, {"number": 26189, "title": "Side effects induced by attribute and slice operators must be limited in Python control flow.", "body": "With `autograph=True`, wrapping ConvND in a `tf.function` works as expected with `use_bias=True`; but `use_bias=False` would fail. Code works as expected with `autograph=True`.\r\n\r\nError message received:\r\n\r\n```\r\nAttributeError: 'ConvND' object has no attribute 'b'.\r\n```", "comments": ["Sorry for the delay in closing. This was fixed on March 18th."]}, {"number": 26188, "title": "Updated lstm.cc", "body": "Fixed warning in the file", "comments": ["clubbed these changes to #26194 "]}, {"number": 26187, "title": "Updated arena_planner.cc", "body": "Fixed the warning (comparison between signed and unsigned integer)", "comments": ["@amitsrivastava78 thank you so much for your contribution , instead of multiple PR's , is it possible to combine these PR's : [26186](https://github.com/tensorflow/tensorflow/pull/26186) , [26185](https://github.com/tensorflow/tensorflow/pull/26185) , [26184](https://github.com/tensorflow/tensorflow/pull/26184)", "@rthadur , thanks for your comments, Since these PRs are approved i will try to group the similar PRs together in future.", "@amitsrivastava78  I think you should group these ones together as well. ", "@shahzadlone and @rthadur, I will merge the PRs tomorrow ", "@shahzadlone & @rthadur , i have merged all the requested PR, kindly review and approve.\r\n\r\nRegards\r\nAmit", "Thank you Amit ,we appreciate your contribution , going forward please submit related PR's in one request, so that it is easy get in to the system.", "@shahzadlone , i have rebased and merged the code again, can you kindly approve again. ", "@rthadur, this PR is approved, can you please change the label to Ready to pull.", "Closing the PR as similar PR is merged."]}, {"number": 26186, "title": "Updated test_util.cc", "body": "fixed warning in the file (comparison between signed and unsigned integer)", "comments": ["changes are ported here  #26187 "]}, {"number": 26185, "title": "Update test_util.h", "body": "Fixed the warning issue in the file  (comparison between signed and unsigned integer)", "comments": ["changes are ported here #26187 "]}, {"number": 26184, "title": "Removed the unused variable usage warning", "body": "Fixed the unused variable warning in the PR", "comments": ["changes are ported here #26187 "]}, {"number": 26183, "title": "bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n$ uname -a\r\nLinux e1.carbonate.uits.iu.edu 3.10.0-862.6.3.el7.x86_64 #1 SMP Fri Jun 15 17:57:37 EDT 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.12.0\r\n- Python version: 3.7.2\r\n- Installed using virtualenv? pip? conda?: no\r\n- Bazel version (if compiling from source): 0.22.0\r\n- git version: 2.13.0\r\n- GCC/Compiler version (if compiling from source): gcc (GCC) 6.3.0\r\n- CUDA/cuDNN version: 10.0/7.4.2\r\n- GPU model and memory: NVIDIA V100 16GB and NVIDIA P100 16GB\r\n\r\n**Describe the problem**\r\nAttempting to build from source, build does not complete.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package \r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nINFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (383 packages loaded, 19566 targets configured). INFO: Found 1 target... ERROR: /gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/gif_archive/BUILD.bazel:8:1: undeclared inclusion(s) in rule '@gif_archive//:gif': this rule is missing dependency declarations for the following files included by 'external/gif_archive/lib/openbsd-reallocarray.c': '/N/soft/rhel7/gcc/6.3.0/lib/gcc/x86_64-pc-linux-gnu/6.3.0/include/stddef.h' '/N/soft/rhel7/gcc/6.3.0/lib/gcc/x86_64-pc-linux-gnu/6.3.0/include/stdint.h' Target //tensorflow/tools/pip_package:build_pip_package failed to build Use --verbose_failures to see the command lines of failed build steps. INFO: Elapsed time: 811.872s, Critical Path: 46.61s INFO: 20 processes: 20 local. FAILED: Build did NOT complete successfully", "comments": ["@steige2 Please check the tested build configurations [here](https://www.tensorflow.org/install/source#linux). There could be path reference issues in CUDA/cuDNN folders. Please check whether CUDA and cuDNN paths are referencing correctly or not. \r\nEither upgrade TF1.12 to TF1.13rc2 or downgrade Bazel and CUDA versions. Thanks!", "Fixed it, I believe some of the dependencies were guilt with a different version of gcc, blew away the entire stack, started over with TF1.12, everything built just fine", "thinks your for your contribution.\r\n"]}, {"number": 26182, "title": "ImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- Linux Mint\r\n-Anaconda - pip install tensorflow-gpu\r\n- 9.0/7.5:\r\n- 1080 ti\r\n\r\nI was using tensorflow gpu last year. I wanted to set it up again. I got it running on my Windows 10 partition. Now I have tried to set it up again on my Mint partition. I always get the following error. \r\nImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory.\r\nI thought TF needs cuda 9.0 and not 10.0?\r\n\r\nThe error occurs if I execute the following code.\r\n\r\n\r\nimport tensorflow as tf\r\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))", "comments": ["+ Latest TensorFlow supports cuda 8-10. cudnn 6-7.\r\n+ Each TensorFlow binary has to work with the version of cuda and cudnn it was built with. If they don't match, you have to change either the TensorFlow binary or the Nvidia softwares.\r\n+ Official `tensorflow-gpu` binaries (the one downloaded by pip or conda) are built with cuda 9.0, cudnn 7 since TF 1.5, and cuda 10.0, cudnn 7 since TF 1.13. These are written in the [release notes](https://github.com/tensorflow/tensorflow/releases). You have to use the matching version of cuda if using the official binaries.\r\n+ If you don't like to change your Nvidia software, you can:\r\n(1) Use a different version of TensorFlow\r\n(2) Use non-official binaries built by others. e.g.: https://github.com/mind/wheels/releases, https://github.com/hadim/docker-tensorflow-builder#builds, \r\nhttps://github.com/inoryy/tensorflow-optimized-wheels\r\n(3) Build the binaries by yourself from source with your version of Nvidia software.", "@gian1312 I think it is looking for CUDA10 file. The error is due to mismatch is CUDA version. Best approach is install TF from clean state. Please follow @ppwwyyxx suggestion to select best versions (TF1.12, CUDA9.0 or TF1.13,CUDA10.0) for your need. Please uninstall python and tensorflow and then follow the [instructions](https://github.com/rnreich/ubuntu-tensorflow-gpu-all-versions) to install TF fresh. Please let me know how it progresses. Thanks!", "identical problem here.\r\n\r\nclean installation of Nvidia drivers, CUDA 10.1 and TF\r\n\r\nlibcublas.so.10.0 error as soon as TF is called.\r\n\r\nUbuntu 18.04.2 LTS; Also Anaconda install of Python 3.7 (is the anaconda install relevant?); 2070", "@rhinsall Which TF version you are trying to install? Could you install CUDA10 or correctly reference the CUDA10.1 path in cuDNN. Thanks", "It does not seem possible to install Tensorflow with default packaging on Ubuntu 18.04.  You have to either build TF from scratch, which requires sourcing an older version of bazel than is available through the default repositories, or manually install specific versions of nvidia drivers and libraries.\r\n\r\nNone of the linked wheels from upthread are yet built against CUDA 10.1.", "Thanks a lot. I relyed on the website and haven't realised, that a new version came out a few days ago. I am sorry. I downgraded to 1.12. Now, my graphic card gets found with the mentioned code. \r\n\r\nSadly, the code (an example from a lecture I attend) which runs on my Windows installation perfectly fine (30 s) takes 6 min on my Linux installation an puts the CPU under load. Is there a work around to force Tensorflow to use the GPU?", "> @rhinsall Which TF version you are trying to install? Could you install CUDA10 or correctly reference the CUDA10.1 path in cuDNN. Thanks\r\n\r\nI'll come home much later and report the exact numbers and paths - but it's a fresh install, downloaded yesterday, CUDA 10.1 per Nvidia's instructions and TF clean install using PIP & Python 3.7", "@rhinsall \r\nI just found this out myself, not sure if it's common knowledge, but got around this by doing\r\n``` bash\r\nconda install cudatoolkit\r\nconda install cudnn\r\n```\r\nI have cuda-10.1 installed on my box, this installed a local conda-only cuda-10.0. Obviously this is to just keep tensorflow working while waiting for better support.", "Excellent advice.  Immediate rescue.  Thank you very much fabricatedmath.", "@gian1312 That is strange. There is a guide on using gpu [here](https://www.tensorflow.org/guide/using_gpu). Using those instructions you can force TF to use a gpu. Some times it is better to uninstall and reinstall TF. Please let me know how it progresses. If the issue was resolved, please close the ticket. Thanks! ", "hi, \r\nI am having the similar problem. So , I created new conda environment and installed tensorflow-gpu as  \r\n`\r\nconda install tensorflow-gpu\r\nCollecting package metadata: done\r\nSolving environment: done\r\n\r\n## Package Plan ##\r\n\r\n  environment location: /home/lasii/anaconda3/envs/drunk2\r\n\r\n  added / updated specs:\r\n    - tensorflow-gpu\r\n\r\n\r\nThe following packages will be downloaded:\r\n\r\n    package                    |            build\r\n    ---------------------------|-----------------\r\n    _tflow_select-2.1.0        |              gpu           2 KB  defaults\r\n    absl-py-0.4.1              |           py35_0         144 KB  defaults\r\n    astor-0.7.1                |           py35_0          43 KB  defaults\r\n    cupti-9.2.148              |                0         1.7 MB  defaults\r\n    gast-0.2.0                 |           py35_0          15 KB  defaults\r\n    grpcio-1.12.1              |   py35hdbcaa40_0         1.7 MB  defaults\r\n    libprotobuf-3.6.0          |       hdbcaa40_0         4.1 MB  defaults\r\n    markdown-2.6.11            |           py35_0         104 KB  defaults\r\n    mkl_fft-1.0.6              |   py35h7dd41cf_0         149 KB  defaults\r\n    mkl_random-1.0.1           |   py35h4414c95_1         362 KB  defaults\r\n    numpy-1.15.2               |   py35h1d66e8a_0          47 KB  defaults\r\n    numpy-base-1.15.2          |   py35h81de0dd_0         4.2 MB  defaults\r\n    protobuf-3.6.0             |   py35hf484d3e_0         615 KB  defaults\r\n    six-1.11.0                 |           py35_1          21 KB  defaults\r\n    tensorboard-1.10.0         |   py35hf484d3e_0         3.3 MB  defaults\r\n    tensorflow-1.10.0          |gpu_py35hd9c640d_0           3 KB  defaults\r\n    tensorflow-base-1.10.0     |gpu_py35had579c0_0       190.6 MB  defaults\r\n    tensorflow-gpu-1.10.0      |       hf154084_0           2 KB  defaults\r\n    termcolor-1.1.0            |           py35_1           7 KB  defaults\r\n    ------------------------------------------------------------\r\n                                           Total:       207.1 MB\r\n\r\nThe following NEW packages will be INSTALLED:\r\n\r\n  _tflow_select      pkgs/main/linux-64::_tflow_select-2.1.0-gpu\r\n  absl-py            pkgs/main/linux-64::absl-py-0.4.1-py35_0\r\n  astor              pkgs/main/linux-64::astor-0.7.1-py35_0\r\n  blas               pkgs/main/linux-64::blas-1.0-mkl\r\n  cudatoolkit        pkgs/main/linux-64::cudatoolkit-9.2-0\r\n  cudnn              pkgs/main/linux-64::cudnn-7.3.1-cuda9.2_0\r\n  cupti              pkgs/main/linux-64::cupti-9.2.148-0\r\n  gast               pkgs/main/linux-64::gast-0.2.0-py35_0\r\n  grpcio             pkgs/main/linux-64::grpcio-1.12.1-py35hdbcaa40_0\r\n  intel-openmp       pkgs/main/linux-64::intel-openmp-2019.1-144\r\n  libgfortran-ng     pkgs/main/linux-64::libgfortran-ng-7.3.0-hdf63c60_0\r\n  libprotobuf        pkgs/main/linux-64::libprotobuf-3.6.0-hdbcaa40_0\r\n  markdown           pkgs/main/linux-64::markdown-2.6.11-py35_0\r\n  mkl                pkgs/main/linux-64::mkl-2018.0.3-1\r\n  mkl_fft            pkgs/main/linux-64::mkl_fft-1.0.6-py35h7dd41cf_0\r\n  mkl_random         pkgs/main/linux-64::mkl_random-1.0.1-py35h4414c95_1\r\n  numpy              pkgs/main/linux-64::numpy-1.15.2-py35h1d66e8a_0\r\n  numpy-base         pkgs/main/linux-64::numpy-base-1.15.2-py35h81de0dd_0\r\n  protobuf           pkgs/main/linux-64::protobuf-3.6.0-py35hf484d3e_0\r\n  six                pkgs/main/linux-64::six-1.11.0-py35_1\r\n  tensorboard        pkgs/main/linux-64::tensorboard-1.10.0-py35hf484d3e_0\r\n  tensorflow         pkgs/main/linux-64::tensorflow-1.10.0-gpu_py35hd9c640d_0\r\n  tensorflow-base    pkgs/main/linux-64::tensorflow-base-1.10.0-gpu_py35had579c0_0\r\n  tensorflow-gpu     pkgs/main/linux-64::tensorflow-gpu-1.10.0-hf154084_0\r\n  termcolor          pkgs/main/linux-64::termcolor-1.1.0-py35_1\r\n  werkzeug           pkgs/main/linux-64::werkzeug-0.14.1-py35_0\r\n`\r\nAfter installation . I just imported tensorflow and got the error.\r\n\r\n`Traceback (most recent call last):\r\n  File \"/home/lasii/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/lasii/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/lasii/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/lasii/anaconda3/envs/drunk2/lib/python3.5/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/lasii/anaconda3/envs/drunk2/lib/python3.5/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/lasii/.local/lib/python3.5/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/home/lasii/.local/lib/python3.5/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/lasii/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/lasii/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/lasii/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/lasii/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/lasii/anaconda3/envs/drunk2/lib/python3.5/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/lasii/anaconda3/envs/drunk2/lib/python3.5/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n`\r\n\r\nI just started using github. Guide me if I am posting improperly.", "@ivineetm007 , Can you check the CUDA version!", "@codexponent\r\nIt's 9.20\r\nConda automatically installed it while installing tensorflow-gpu. ", "I think you should update your CUDA version to 10 along.\r\nThis link will help you \r\nLink: https://www.nvidia.com/Download/index.aspx?lang=en-us ", "@codexponent \r\nI installed cuda 10.0 in conda by\r\n`conda install -c fragcolor cuda10.0 `\r\n\r\nNow , there are two cuda in conda environment package list.\r\ncudatoolkit 9.2\r\ncuda 10.0\r\n\r\nBut the same error occurs on importing tensorflow.", "@ivineetm007 , Can you do nvidia-smi and check the head of the table! I am sure that you need to update cuda by downloading the nvidia driver from their website.", "@codexponent\r\nheader \r\nNVIDIA-SMI 396.54                 Driver Version: 396.54 \r\n\r\nI am working on a PC in college which is alloted to two or three students. I am not sure if I install cuda by downloading , it will not affect the other environment in conda.\r\nA little history...\r\nI am using code in the link \r\n(https://github.com/DevendraPratapYadav/gsoc18_RedHenLab/tree/master/video_processing_pipeline)\r\nIn this link, setup is done on conda . Two weeks ago, tensorflow was [running] perfectly while running the above code.  \r\nBut someone updated conda in the PC. Now, I am having libculas.so.10.0 error.", "@ivineetm007 , if this is not your pc i suggest you don't update it as it might break other environments working for cuda 9. Do one thing, create a new environment, install tensorflow with the specific version number\r\npip install tensorfow==1.10.0 and then test a very simple code like addition of 2 numbers(tf.add). See if this runs or not.", "@codexponent \r\nI tried your suggestion. It worked fine . Then I  tried to install tf-gpu and keras as -\r\nconda install -y -c anaconda tensorflow-gpu==1.7.0\r\nconda install -y keras\r\nNow I am having error-\r\nAttributeError: module 'tensorflow.python.training.checkpointable' has no attribute 'CheckpointableBase'\r\n I followed the solution for this error in the link \r\n(https://github.com/tensorflow/tensorflow/issues/20499l)\r\nwhich suggested reinstalling.\r\nI think some other version of tensorflow-gpu will work ", "@ivineetm007 , try to do the same thing with opening tf session on the gpu. This link may help\r\nLink: https://www.tensorflow.org/guide/using_gpu\r\n\r\n**Another solution**: Don't install anything from conda, just install from pip\r\nSteps:\r\n1) Create a fresh environment\r\n2) pip install tensorflow==1.12.0\r\n3) pip install tensorflow-gpu==1.12.0\r\n4) pip install keras==2.1.3\r\nIf you have anything that you want to install from conda, check if it is available on the pip version. If it is not then,\r\nLet's say that your env name is my_env_1\r\nafter activating that environment, type ```which conda```, \r\nif this gives the path to your created environment (...\\my_env_1\\...), then you can install other essential environments. If this gives (..\\\\...), then type ```pip install conda```, then install other essential environments. (be sure to check again by typing ```which conda```)", "Same problem.My cuda version is 10.1,but the the libcublas.so.10.0 file is not in the catalogue of lib64.I am installing the tensorflow-gpu with the command 'pip install tensorflow-gpu'.", "> Same problem.My cuda version is 10.1,but the the libcublas.so.10.0 file is not in the catalogue of lib64.I am installing the tensorflow-gpu with the command 'pip install tensorflow-gpu'.\r\n\r\nIt seems that the libcublas-version is removed by the cuda 10 ", "@lipingbj , did you update the cuda version from conda command or through nvidia official site, I think doing from the actual site might help t get those .so files\r\nLink: https://www.nvidia.com/Download/index.aspx?lang=en-us", "@lipingbj so i had a similar issue, when pushing an upgrade to a tensorflow code which would  call 200 sagemakers in parallel. i solved it by fixing the numpy version to numpy==1.14.5  and tensorflow-gpu to 1.12.0. If you would you like i can paste the dockerfile i created to ensure it works?", "> > Same problem.My cuda version is 10.1,but the the libcublas.so.10.0 file is not in the catalogue of lib64.I am installing the tensorflow-gpu with the command 'pip install tensorflow-gpu'.\r\n> \r\n> It seems that the libcublas-version is removed by the cuda 10\r\n\r\nAfter installing CUDA 10 I have found `libcublas.so.10` under `/usr/lib/x86_64-linux-gnu/`.\r\nSo you need to add `/usr/lib/x86_64-linux-gnu/` to your library path by calling:\r\n```\r\n> export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/x86_64-linux-gnu/\r\n```\r\n\r\nAnd also since TensorFlow is looking for  `libcublas.so.10.0` rather than `libcublas.so.10` (without the last .0) you need to create a symlink:\r\n```\r\nln -s /usr/lib/x86_64-linux-gnu/libcublas.so.10 /usr/lib/x86_64-linux-gnu/libcublas.so.10.0\r\n```", "Please look at the instructions here after installing CUDA 10:\r\nhttps://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#environment-setup", "hi all,\r\n\r\nI am facilng the same issue, but my problem is little different, i am able to install and import tensorflow-gpu on my local machine as well as when building the docker container, everything is working fine. but when I am building my docker image from Dockerfile and docker-compose-up...build, i am getting this error.\r\nPlease help me out, I really dont know why this is happening in the building of docker image.", "After installing cuda, you need to export `$PATH` and `$LD_LIBRARY_PATH`. Tensorflow will use these environment variables to load package. For example, if you install cuda at `/usr/local/`, you can add this to your `.zshrc` or `.bashrc` (depend on the shell you using)\r\n\r\n```bash\r\nCUDA_VERSION=10.0\r\n\r\nexport PATH=/usr/local/cuda-$CUDA_VERSION/bin:$PATH\r\nexport LD_LIBRARY_PATH=/usr/local/cuda-$CUDA_VERSION/lib64:$LD_LIBRARY_PATH\r\n```\r\n\r\nThis trick can be used to change the version of cuda you want to use. ", "@mostafaelhoushi I did the simlink but this does not make the trick:\r\n\r\n```sh\r\nln -s /usr/lib/x86_64-linux-gnu/libcublas.so.10 /usr/lib/x86_64-linux-gnu/libcublas.so.10\r\nroot@b55736f184ff:/notebooks# python3.6 -c \"import tensorflow as tf; print(tf.__version__);\"\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\n```", "@dattran2346 This should work with CUDA10 installed already, but if you start from older Docker images, you may have installed \r\n```\r\nroot@b55736f184ff:/notebooks# echo  $CUDA_VERSION\r\n9.0.176\r\n```", "> @mostafaelhoushi I did the simlink but this does not make the trick:\r\n> \r\n> ```shell\r\n> ln -s /usr/lib/x86_64-linux-gnu/libcublas.so.10 /usr/lib/x86_64-linux-gnu/libcublas.so.10\r\n> root@b55736f184ff:/notebooks# python3.6 -c \"import tensorflow as tf; print(tf.__version__);\"\r\n> Traceback (most recent call last):\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n>     from tensorflow.python.pywrap_tensorflow_internal import *\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n>     _pywrap_tensorflow_internal = swig_import_helper()\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n>   File \"/usr/lib/python3.6/imp.py\", line 243, in load_module\r\n>     return load_dynamic(name, filename, file)\r\n>   File \"/usr/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n>     return _load(spec)\r\n> ImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"<string>\", line 1, in <module>\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\", line 24, in <module>\r\n>     from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n>     from tensorflow.python import pywrap_tensorflow\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n>     raise ImportError(msg)\r\n> ImportError: Traceback (most recent call last):\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n>     from tensorflow.python.pywrap_tensorflow_internal import *\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n>     _pywrap_tensorflow_internal = swig_import_helper()\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n>   File \"/usr/lib/python3.6/imp.py\", line 243, in load_module\r\n>     return load_dynamic(name, filename, file)\r\n>   File \"/usr/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n>     return _load(spec)\r\n> ImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory\r\n> \r\n> \r\n> Failed to load the native TensorFlow runtime.\r\n> ```\r\n\r\nDid you make sure you installed CUDA10.0? Or which version is installed?", "@dattran2346 @mostafaelhoushi \r\nDo i need to install cuda and cudnn during the build of the docker image also, like this :\r\n```\r\nconda install -c fragcolor cuda10.0\r\n```", "@dattran2346 \r\ni have exported the paths as you suggested, but still not working... ", "- Nvidia-docker only give us access to the driver, the `cudatoolkit` and `cudnn` stuff we have to install by ourselves. `cudatoolkit` and `cudnn` are just dynamic libraries and their location depends on where we installed.\r\n\r\n- If we install `tensorflow-gpu` with `pip` (inside conda or not), `tensorflow-gpu` will look at `$PATH` and `$LD_LIBRARY_PATH` to find `cudatoolkit` and `cudnn` at runtime. In this approach, we need to install `cudatoolkit` and `cudnn` before hand and do `export $PATH`. We also need to ensure the version compatible between `cudatoolkit`, `cudnn` and `tensorflow`. Check version compatibility [here](https://www.tensorflow.org/install/source#tested_build_configurations)\r\n\r\n- If we install `tensorflow-gpu` with `conda`, `conda` will also install **appropriate** version of `cudatoolkit` and `cudnn`. In this approach, we do not need to install `cudatoolkit` and `cudnn` before hand.\r\n\r\n![Tensorflow-gpu conda](https://user-images.githubusercontent.com/17774491/54659497-9dbef280-4b04-11e9-9a8d-809ca08b954a.png)\r\n\r\n@priyakansal, you may need to  `conda uninstall cuda10.0` and run `conda install tensorflow-gpu`. \r\n@loretoparisi, may be try lower version of tensorflow or use conda to install or even upgrade your cuda version \ud83e\udd14 \r\n\r\nPs 1: for installing `cudatoolkit` and `cudnn`, I found [this guide](https://gist.github.com/zhanwenchen/e520767a409325d9961072f666815bb8#install-cuda) very useful.\r\nPs 2: Install `cudatoolkit` and `cudnn` by `runtime` file will install the library in `/usr/local/` while install by `.deb` file will install in `/usr/lib/x86_64-linux-gnu/`. So your `$PATH` and `$LD_LIBRARY_PATH` need to change accordingly. Install `cudatoolkit` and `cudnn` by conda will install the library `~/miniconda3/envs/<name>/lib`. And you do not need to `export`\r\nPs 3: What if I have installed `cudatoolkit` and `cudnn` and also install `tensorflow-gpu` using `conda`. `Tensoflow-gpu` will use the libaries install within `conda` enviroment.\r\n\r\nHope this help,\r\nCorrect me if I'm wrong \ud83d\ude04 \r\nCheers", "@dattran2346 \r\nThankyou so much for so detailed explanation.\r\nif i am running ``` conda install tensorflow-gpu ```, then also it is not working, however, i have not tried is with  ```conda uninstall cuda10.0 ```. Here, the problem is that i also want to install ``` tensorflow-serving-api-gpu```, which is not available for conda-install, so need to install using pip, but when installing this.. i am getting the same error. \r\nplease note that, i am doing all this inside the docker. On my local machine(ubuntu), everything is working fine.\r\n\r\n", "What I did was this https://gist.github.com/loretoparisi/4a096fc3625f60403c8734de9660cbcc\r\n\r\n```sh\r\nadd-apt-repository ppa:jonathonf/python-3.6\r\napt-get update & apt-get install -y python3.6\r\ncurl https://bootstrap.pypa.io/get-pip.py > get-pip.py\r\npython3.6 get-pip.py\r\npip3 uninstall tensorflow-gpu\r\npip3.6 install tensorflow-gpu==1.12.0\r\npython3.6 -c \"import tensorflow as tf; print(tf.__version__);\"\r\n```\r\n\r\nBasically you will get Python3.6, CUDA 9 and TF 1.12.0. We have to remote TF-GPU 1.13.0, and then install TF 1.12.0 GPU.", "@loretoparisi\r\nHi , \r\nSorry I am bit new to docker ... so when I am building some image ... either <tensorflow :tensorflow- latest-gpu> or < tensorflow-serving: latest -gpu> using docker run or nivida-docker rum and importing the packages related to tensorflow every thing is working fine ... but when i am building my custom image with anaconda as a base image using docker-compose or nivida-docker- compose build command .. it is not working.. ", "> @dattran2346\r\n> i have exported the paths as you suggested, but still not working...\r\n\r\nCan you try to search for the missing file `libcublas.so.10.0` on your file system. e.g. by using\r\n```\r\nfind / -name \"libcublas.so.10.0\"\r\n```\r\n\r\nand then when you find the path add it to `LD_LIBRARY_PATH` environment variable.\r\nIf you can't find it, then you probably need to install the correct version.", "@mostafaelhoushi \r\nWhen i am running this command  ``` find / -name \"libcublas.so.10.0\" ```\r\nthe output is \r\n```\r\n/var/lib/docker/overlay2/33ff618e94595ffbdc09016439dc6a469fa8adc3ec3b5231f776d6065aab7968/diff/root/.cache/bazel/_bazel_root/e53bbb0b0da4e26d24b415310219b953/execroot/tf_serving/bazel-out/k8-opt/bin/tensorflow_serving/model_servers/tensorflow_model_server.runfiles/tf_serving/external/local_config_cuda/cuda/cuda/lib/libcublas.so.10.0\r\n/var/lib/docker/overlay2/33ff618e94595ffbdc09016439dc6a469fa8adc3ec3b5231f776d6065aab7968/diff/root/.cache/bazel/_bazel_root/e53bbb0b0da4e26d24b415310219b953/execroot/tf_serving/bazel-out/k8-opt/bin/tensorflow_serving/model_servers/tensorflow_model_server.runfiles/tf_serving/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/libcublas.so.10.0\r\n/var/lib/docker/overlay2/33ff618e94595ffbdc09016439dc6a469fa8adc3ec3b5231f776d6065aab7968/diff/root/.cache/bazel/_bazel_root/e53bbb0b0da4e26d24b415310219b953/execroot/tf_serving/bazel-out/k8-opt/bin/tensorflow_serving/model_servers/tensorflow_model_server.runfiles/local_config_cuda/cuda/cuda/lib/libcublas.so.10.0\r\n/var/lib/docker/overlay2/33ff618e94595ffbdc09016439dc6a469fa8adc3ec3b5231f776d6065aab7968/diff/root/.cache/bazel/_bazel_root/e53bbb0b0da4e26d24b415310219b953/execroot/tf_serving/bazel-out/k8-opt/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/libcublas.so.10.0\r\n/var/lib/docker/overlay2/33ff618e94595ffbdc09016439dc6a469fa8adc3ec3b5231f776d6065aab7968/diff/root/.cache/bazel/_bazel_root/e53bbb0b0da4e26d24b415310219b953/execroot/tf_serving/bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda/cuda/lib/libcublas.so.10.0\r\n/var/lib/docker/overlay2/33ff618e94595ffbdc09016439dc6a469fa8adc3ec3b5231f776d6065aab7968/diff/root/.cache/bazel/_bazel_root/e53bbb0b0da4e26d24b415310219b953/execroot/tf_serving/bazel-out/host/genfiles/external/local_config_cuda/cuda/cuda/lib/libcublas.so.10.0\r\n/var/lib/docker/overlay2/97cb0c942535cde4622f53bf094251cd1aef1cfc744e8ddda1472ee691f87618/diff/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcublas.so.10.0\r\n/var/lib/docker/overlay2/2fb234250d278545f55a004fcd436b4cba5e847c40503b990ffe800f3b440cb5/diff/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcublas.so.10.0\r\n/var/lib/docker/overlay2/c704b6be3bc1a5d25119fa46216a4e64f872d8001d8bed6d40930f6420ffb091/diff/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcublas.so.10.0\r\n/usr/local/cuda-10.0/lib64/libcublas.so.10.0\r\n```", "@mostafaelhoushi \r\nWhen i am running this command  ``` find / -name \"libcublas.so.10.0\" ```\r\nthe output is \r\n```\r\n/var/lib/docker/overlay2/33ff618e94595ffbdc09016439dc6a469fa8adc3ec3b5231f776d6065aab7968/diff/root/.cache/bazel/_bazel_root/e53bbb0b0da4e26d24b415310219b953/execroot/tf_serving/bazel-out/k8-opt/bin/tensorflow_serving/model_servers/tensorflow_model_server.runfiles/tf_serving/external/local_config_cuda/cuda/cuda/lib/libcublas.so.10.0\r\n/var/lib/docker/overlay2/33ff618e94595ffbdc09016439dc6a469fa8adc3ec3b5231f776d6065aab7968/diff/root/.cache/bazel/_bazel_root/e53bbb0b0da4e26d24b415310219b953/execroot/tf_serving/bazel-out/k8-opt/bin/tensorflow_serving/model_servers/tensorflow_model_server.runfiles/tf_serving/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/libcublas.so.10.0\r\n/var/lib/docker/overlay2/33ff618e94595ffbdc09016439dc6a469fa8adc3ec3b5231f776d6065aab7968/diff/root/.cache/bazel/_bazel_root/e53bbb0b0da4e26d24b415310219b953/execroot/tf_serving/bazel-out/k8-opt/bin/tensorflow_serving/model_servers/tensorflow_model_server.runfiles/local_config_cuda/cuda/cuda/lib/libcublas.so.10.0\r\n/var/lib/docker/overlay2/33ff618e94595ffbdc09016439dc6a469fa8adc3ec3b5231f776d6065aab7968/diff/root/.cache/bazel/_bazel_root/e53bbb0b0da4e26d24b415310219b953/execroot/tf_serving/bazel-out/k8-opt/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/libcublas.so.10.0\r\n/var/lib/docker/overlay2/33ff618e94595ffbdc09016439dc6a469fa8adc3ec3b5231f776d6065aab7968/diff/root/.cache/bazel/_bazel_root/e53bbb0b0da4e26d24b415310219b953/execroot/tf_serving/bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda/cuda/lib/libcublas.so.10.0\r\n/var/lib/docker/overlay2/33ff618e94595ffbdc09016439dc6a469fa8adc3ec3b5231f776d6065aab7968/diff/root/.cache/bazel/_bazel_root/e53bbb0b0da4e26d24b415310219b953/execroot/tf_serving/bazel-out/host/genfiles/external/local_config_cuda/cuda/cuda/lib/libcublas.so.10.0\r\n/var/lib/docker/overlay2/97cb0c942535cde4622f53bf094251cd1aef1cfc744e8ddda1472ee691f87618/diff/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcublas.so.10.0\r\n/var/lib/docker/overlay2/2fb234250d278545f55a004fcd436b4cba5e847c40503b990ffe800f3b440cb5/diff/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcublas.so.10.0\r\n/var/lib/docker/overlay2/c704b6be3bc1a5d25119fa46216a4e64f872d8001d8bed6d40930f6420ffb091/diff/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcublas.so.10.0\r\n/usr/local/cuda-10.0/lib64/libcublas.so.10.0\r\n```", "> @mostafaelhoushi\r\n> When i am running this command `find / -name \"libcublas.so.10.0\"`\r\n> the output is\r\n> \r\n> ```\r\n> /var/lib/docker/overlay2/33ff618e94595ffbdc09016439dc6a469fa8adc3ec3b5231f776d6065aab7968/diff/root/.cache/bazel/_bazel_root/e53bbb0b0da4e26d24b415310219b953/execroot/tf_serving/bazel-out/k8-opt/bin/tensorflow_serving/model_servers/tensorflow_model_server.runfiles/tf_serving/external/local_config_cuda/cuda/cuda/lib/libcublas.so.10.0\r\n> /var/lib/docker/overlay2/33ff618e94595ffbdc09016439dc6a469fa8adc3ec3b5231f776d6065aab7968/diff/root/.cache/bazel/_bazel_root/e53bbb0b0da4e26d24b415310219b953/execroot/tf_serving/bazel-out/k8-opt/bin/tensorflow_serving/model_servers/tensorflow_model_server.runfiles/tf_serving/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/libcublas.so.10.0\r\n> /var/lib/docker/overlay2/33ff618e94595ffbdc09016439dc6a469fa8adc3ec3b5231f776d6065aab7968/diff/root/.cache/bazel/_bazel_root/e53bbb0b0da4e26d24b415310219b953/execroot/tf_serving/bazel-out/k8-opt/bin/tensorflow_serving/model_servers/tensorflow_model_server.runfiles/local_config_cuda/cuda/cuda/lib/libcublas.so.10.0\r\n> /var/lib/docker/overlay2/33ff618e94595ffbdc09016439dc6a469fa8adc3ec3b5231f776d6065aab7968/diff/root/.cache/bazel/_bazel_root/e53bbb0b0da4e26d24b415310219b953/execroot/tf_serving/bazel-out/k8-opt/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/libcublas.so.10.0\r\n> /var/lib/docker/overlay2/33ff618e94595ffbdc09016439dc6a469fa8adc3ec3b5231f776d6065aab7968/diff/root/.cache/bazel/_bazel_root/e53bbb0b0da4e26d24b415310219b953/execroot/tf_serving/bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda/cuda/lib/libcublas.so.10.0\r\n> /var/lib/docker/overlay2/33ff618e94595ffbdc09016439dc6a469fa8adc3ec3b5231f776d6065aab7968/diff/root/.cache/bazel/_bazel_root/e53bbb0b0da4e26d24b415310219b953/execroot/tf_serving/bazel-out/host/genfiles/external/local_config_cuda/cuda/cuda/lib/libcublas.so.10.0\r\n> /var/lib/docker/overlay2/97cb0c942535cde4622f53bf094251cd1aef1cfc744e8ddda1472ee691f87618/diff/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcublas.so.10.0\r\n> /var/lib/docker/overlay2/2fb234250d278545f55a004fcd436b4cba5e847c40503b990ffe800f3b440cb5/diff/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcublas.so.10.0\r\n> /var/lib/docker/overlay2/c704b6be3bc1a5d25119fa46216a4e64f872d8001d8bed6d40930f6420ffb091/diff/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcublas.so.10.0\r\n> /usr/local/cuda-10.0/lib64/libcublas.so.10.0\r\n> ```\r\n\r\nOK. I see `libcublas.so.10.0` is found in `/usr/local/cuda-10.0/lib64/`.\r\nTry running this command:\r\n```\r\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-10.0/lib64/\r\n```\r\nand try again.\r\n\r\nNOTE: I see the library is also found in your docker system. I am not familiar with dockers, so maybe someone else could help here. But try the above command and see.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26182\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26182\">No</a>\n", "It happened to me when I installed cuda-10.1 not cuda-10.0 , downgrading to 10.0 did fix it", "@littlehome-eugene\r\nBut I am using cuda-10.0 only\r\nBtw, have you done it for docker\r\n", "> > > Same problem.My cuda version is 10.1,but the the libcublas.so.10.0 file is not in the catalogue of lib64.I am installing the tensorflow-gpu with the command 'pip install tensorflow-gpu'.\r\n> > \r\n> > \r\n> > It seems that the libcublas-version is removed by the cuda 10\r\n> \r\n> After installing CUDA 10 I have found `libcublas.so.10` under `/usr/lib/x86_64-linux-gnu/`.\r\n> So you need to add `/usr/lib/x86_64-linux-gnu/` to your library path by calling:\r\n> \r\n> ```\r\n> > export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/x86_64-linux-gnu/\r\n> ```\r\n> And also since TensorFlow is looking for `libcublas.so.10.0` rather than `libcublas.so.10` (without the last .0) you need to create a symlink:\r\n> \r\n> ```\r\n> ln -s /usr/lib/x86_64-linux-gnu/libcublas.so.10 /usr/lib/x86_64-linux-gnu/libcublas.so.10\r\n> ```\r\n\r\nThere is a typo in the last command, it should be:\r\n`ln -s /usr/lib/x86_64-linux-gnu/libcublas.so.10 /usr/lib/x86_64-linux-gnu/libcublas.so.10.0`\r\nAlso consider issuing that command with root privileges (sudo) or you will get a permission denied error...", "> > > > Same problem.My cuda version is 10.1,but the the libcublas.so.10.0 file is not in the catalogue of lib64.I am installing the tensorflow-gpu with the command 'pip install tensorflow-gpu'.\r\n> > > \r\n> > > \r\n> > > It seems that the libcublas-version is removed by the cuda 10\r\n> > \r\n> > \r\n> > After installing CUDA 10 I have found `libcublas.so.10` under `/usr/lib/x86_64-linux-gnu/`.\r\n> > So you need to add `/usr/lib/x86_64-linux-gnu/` to your library path by calling:\r\n> > ```\r\n> > > export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/x86_64-linux-gnu/\r\n> > ```\r\n> > And also since TensorFlow is looking for `libcublas.so.10.0` rather than `libcublas.so.10` (without the last .0) you need to create a symlink:\r\n> > ```\r\n> > ln -s /usr/lib/x86_64-linux-gnu/libcublas.so.10 /usr/lib/x86_64-linux-gnu/libcublas.so.10\r\n> > ```\r\n> \r\n> There is a typo in the last command, it should be:\r\n> `ln -s /usr/lib/x86_64-linux-gnu/libcublas.so.10 /usr/lib/x86_64-linux-gnu/libcublas.so.10.0`\r\n> Also consider issuing that command with root privileges (sudo) or you will get a permission denied error...\r\n\r\nThanks @plche ! I fixed it", "just remove everything about 10.1 and downgrade it to Cuda 10.0 and it will work. Nothing else worked for me.", "> > @mostafaelhoushi\r\n> > When i am running this command `find / -name \"libcublas.so.10.0\"`\r\n> > the output is\r\n> > ```\r\n> > /var/lib/docker/overlay2/33ff618e94595ffbdc09016439dc6a469fa8adc3ec3b5231f776d6065aab7968/diff/root/.cache/bazel/_bazel_root/e53bbb0b0da4e26d24b415310219b953/execroot/tf_serving/bazel-out/k8-opt/bin/tensorflow_serving/model_servers/tensorflow_model_server.runfiles/tf_serving/external/local_config_cuda/cuda/cuda/lib/libcublas.so.10.0\r\n> > /var/lib/docker/overlay2/33ff618e94595ffbdc09016439dc6a469fa8adc3ec3b5231f776d6065aab7968/diff/root/.cache/bazel/_bazel_root/e53bbb0b0da4e26d24b415310219b953/execroot/tf_serving/bazel-out/k8-opt/bin/tensorflow_serving/model_servers/tensorflow_model_server.runfiles/tf_serving/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/libcublas.so.10.0\r\n> > /var/lib/docker/overlay2/33ff618e94595ffbdc09016439dc6a469fa8adc3ec3b5231f776d6065aab7968/diff/root/.cache/bazel/_bazel_root/e53bbb0b0da4e26d24b415310219b953/execroot/tf_serving/bazel-out/k8-opt/bin/tensorflow_serving/model_servers/tensorflow_model_server.runfiles/local_config_cuda/cuda/cuda/lib/libcublas.so.10.0\r\n> > /var/lib/docker/overlay2/33ff618e94595ffbdc09016439dc6a469fa8adc3ec3b5231f776d6065aab7968/diff/root/.cache/bazel/_bazel_root/e53bbb0b0da4e26d24b415310219b953/execroot/tf_serving/bazel-out/k8-opt/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/libcublas.so.10.0\r\n> > /var/lib/docker/overlay2/33ff618e94595ffbdc09016439dc6a469fa8adc3ec3b5231f776d6065aab7968/diff/root/.cache/bazel/_bazel_root/e53bbb0b0da4e26d24b415310219b953/execroot/tf_serving/bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda/cuda/lib/libcublas.so.10.0\r\n> > /var/lib/docker/overlay2/33ff618e94595ffbdc09016439dc6a469fa8adc3ec3b5231f776d6065aab7968/diff/root/.cache/bazel/_bazel_root/e53bbb0b0da4e26d24b415310219b953/execroot/tf_serving/bazel-out/host/genfiles/external/local_config_cuda/cuda/cuda/lib/libcublas.so.10.0\r\n> > /var/lib/docker/overlay2/97cb0c942535cde4622f53bf094251cd1aef1cfc744e8ddda1472ee691f87618/diff/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcublas.so.10.0\r\n> > /var/lib/docker/overlay2/2fb234250d278545f55a004fcd436b4cba5e847c40503b990ffe800f3b440cb5/diff/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcublas.so.10.0\r\n> > /var/lib/docker/overlay2/c704b6be3bc1a5d25119fa46216a4e64f872d8001d8bed6d40930f6420ffb091/diff/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcublas.so.10.0\r\n> > /usr/local/cuda-10.0/lib64/libcublas.so.10.0\r\n> > ```\r\n> \r\n> OK. I see `libcublas.so.10.0` is found in `/usr/local/cuda-10.0/lib64/`.\r\n> Try running this command:\r\n> \r\n> ```\r\n> export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-10.0/lib64/\r\n> ```\r\n> and try again.\r\n> \r\n> NOTE: I see the library is also found in your docker system. I am not familiar with dockers, so maybe someone else could help here. But try the above command and see.\r\n\r\n@mostafaelhoushi have given the best solution. Anyone who is confused see this answer. :)", "Unfortunately my underlying question is a bit unrelated to this thread - I have the wrong version installed. However I'm hoping there's someone more knowledgeable here that can answer my actual query below.\r\n\r\nI'm running arch linux; I installed tensorflow 2:\r\n\r\n```\r\npip install tensorflow-gpu==2.0.0-alpha0\r\n```\r\nI had previously been running an older version of the `cuda` and `cudnn` packages in order to work with tensorflow 1. I removed these and installed the latest in the AUR:\r\n```\r\n[stiege@archie ~]$ sudo pacman -S cuda cudnn\r\n[sudo] password for stiege: \r\nwarning: cuda-10.1.105-6 is up to date -- reinstalling\r\nwarning: cudnn-7.5.0.56-1 is up to date -- reinstalling\r\nresolving dependencies...\r\nlooking for conflicting packages...\r\n\r\nPackages (2) cuda-10.1.105-6  cudnn-7.5.0.56-1\r\n\r\nTotal Installed Size:  4390.26 MiB\r\nNet Upgrade Size:         0.00 MiB\r\n\r\n:: Proceed with installation? [Y/n] Y\r\n```\r\nNote the cuda version is actually `10.1`; however I get the same error as others in the thread:\r\n```sh\r\nImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory\r\n```\r\nBut - \r\n```\r\n[stiege@archie ~]$ ldconfig -p 2>/dev/null | grep libcublas.so\r\n\tlibcublas.so.10 (libc6,x86-64) => /opt/cuda/lib64/libcublas.so.10\r\n\tlibcublas.so (libc6,x86-64) => /opt/cuda/lib64/libcublas.so\r\n```\r\n\r\nI can find nothing about why only these two `libcublas.so*` links are created - why is it just for the major version and not the minor and patch versions? Is this by a convention / standard? Links/Docs? I also still can't find these in the \"standard place\" - which I assumed is what `ldconfig` was doing:\r\n```\r\n[stiege@archie ~]$ find /usr/lib/ -name libcublas.so*\r\n[stiege@archie ~]$ find /lib/ -name libcublas.so*\r\n[stiege@archie ~]$\r\n```\r\n\r\nAnd this is what makes me concerned about the issue of the actual thread - it appears that even `libcublas.so.10.1` isn't even available:\r\n\r\n```py\r\nIn [38]: l = ctypes.cdll.LoadLibrary(\"libcublas.so\")                                                                          \r\n\r\nIn [39]: l = ctypes.cdll.LoadLibrary(\"libcublas.so.10\")                                                                       \r\n\r\nIn [40]: l = ctypes.cdll.LoadLibrary(\"libcublas.so.10.1\")                                                                     \r\n---------------------------------------------------------------------------\r\nOSError                                   Traceback (most recent call last)\r\n<ipython-input-40-9eb0347ef2f9> in <module>\r\n----> 1 l = ctypes.cdll.LoadLibrary(\"libcublas.so.10.1\")\r\n```\r\n\r\n```\r\n[stiege@archie ~]$ cat /etc/ld.so.conf.d/cuda.conf \r\n/opt/cuda/lib64\r\n/opt/cuda/nvvm/lib64\r\n/opt/cuda/extras/CUPTI/lib64\r\n```\r\n^ Again there are lots of shared objects in these directories; I'm not sure why only the 2 mentioned above end up being processed by `ldconfig`; is this basically all by the underlying convention? It seems reasonable to me to ask for a specific minor version, however much of the guidance (I could find at short notice) seems to really push that only the MAJOR version is important - https://unix.stackexchange.com/questions/475/how-do-so-shared-object-numbers-work\r\n\r\n---\r\n\r\nFound `libcrypto` as a counter-example to the convention I inferred. This links to a major.minor version, the major alone is actually not provided.\r\n\r\n```\r\nIn [50]: l = ctypes.cdll.LoadLibrary(\"libcrypto.so.1.0.0\")                                                                    \r\n\r\nIn [51]: l = ctypes.cdll.LoadLibrary(\"libcrypto.so.1.1\")                                                                      \r\n\r\nIn [52]: l = ctypes.cdll.LoadLibrary(\"libcrypto.so.1\")                                                                        \r\n---------------------------------------------------------------------------\r\nOSError                                   Traceback (most recent call last)\r\n<ipython-input-52-3d67cbbd3826> in <module>\r\n----> 1 l = ctypes.cdll.LoadLibrary(\"libcrypto.so.1\")\r\n```\r\nBut this is exactly what I'd expect from the listing in `/lib/`:\r\n```sh\r\n[stiege@archie tensorflow]$ find /lib/ -name libcrypto.so*\r\n/lib/libcrypto.so\r\n/lib/libcrypto.so.1.1\r\n/lib/libcrypto.so.1.0.0\r\n```\r\n\r\nSo my main question appears to be that even though `/opt/cuda/lib64/libcublas.so.10.1` seems to be available and configured via the `ldconfig` system, why is it unavailable for import with python.\r\n\r\n---\r\n\r\nWeird\r\n```sh\r\n[stiege@archie tensorflow]$ sudo cp /opt/cuda/lib64/libcublas.so.10.1 /opt/cuda/lib64/libcublas.so.10.2\r\n[stiege@archie tensorflow]$ ldconfig -v | grep libcublas\r\nldconfig: Can't unlink /opt/cuda/lib64/libcublas.so.10\r\n\tlibcublas.so.10 -> libcublas.so.10.2 (SKIPPED)\r\n\tlibcublasLt.so.10 -> libcublasLt.so.10.1.0.105\r\n[stiege@archie tensorflow]$ sudo cp /opt/cuda/lib64/libcublas.so.10.1 /opt/cuda/lib64/libcublas.so.11\r\n[stiege@archie tensorflow]$ sudo cp /opt/cuda/lib64/libcublas.so.10.1 /opt/cuda/lib64/libcublas.so.11.2\r\n[stiege@archie tensorflow]$ ldconfig -v | grep libcublas\r\nldconfig: Can't unlink /opt/cuda/lib64/libcublas.so.10\r\n\tlibcublas.so.10 -> libcublas.so.11.2 (SKIPPED)\r\n\tlibcublasLt.so.10 -> libcublasLt.so.10.1.0.105\r\n```\r\nI was expecting a new key \"libcublas.so.11\" to be created, but instead `ldconfig` seems to be trying to link  10 to 11.2 - no idea how this works.", "I had the same problem, \r\nafter remove tensorflow 1.13, install 1.12, problem was solved! \r\n\r\npip install tensorflow-gpu==1.12.0\r\n\r\nmy environment is \r\nnvidia-driver-390\r\ncuda9.0 ", "> @ivineetm007 , try to do the same thing with opening tf session on the gpu. This link may help\r\n> Link: https://www.tensorflow.org/guide/using_gpu\r\n> \r\n> **Another solution**: Don't install anything from conda, just install from pip\r\n> Steps:\r\n> \r\n> 1. Create a fresh environment\r\n> 2. pip install tensorflow==1.12.0\r\n> 3. pip install tensorflow-gpu==1.12.0\r\n> 4. pip install keras==2.1.3\r\n>    If you have anything that you want to install from conda, check if it is available on the pip version. If it is not then,\r\n>    Let's say that your env name is my_env_1\r\n>    after activating that environment, type `which conda`,\r\n>    if this gives the path to your created environment (...\\my_env_1...), then you can install other essential environments. If this gives (..\\...), then type `pip install conda`, then install other essential environments. (be sure to check again by typing `which conda`)\r\n\r\nHi I'm having the same problem as the OP for your post. Trying to install TF GPU on a shared GPU cluster. I tried this but still no luck. Any suggestions? \r\n\r\n`   _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory`", "> I had the same problem,\r\n> after remove tensorflow 1.13, install 1.12, problem was solved!\r\n> \r\n> pip install tensorflow-gpu==1.12.0\r\n> \r\n> my environment is\r\n> nvidia-driver-390\r\n> cuda9.0\r\n\r\nThis worked for me in Ubuntu 18.04.2 LTS,\r\nFrom terminal: \r\n    `source activate \"name of current Virtual Environment\"`    \r\n    `conda uninstall tensorflow-gpu`\r\n    `conda install tensorflow-gpu==1.12.0`\r\n\r\nAfter that i had to reinstall some of my dependent packages like tensorLayer\r\n**NOTE:**\r\nNvidia driver was installed through Ubuntu software update: nvidia-driver-418\r\n", "The issue is closed but this is the top search result for the error on some search engines. \r\n\r\nMy mistake was that an installation into ~/.local/lib/python3.7 was showing up earlier in the python path and masking the systemwide installation (arch linux, packages). Removing it fixed the problem\r\n\r\n", "## Update it seems that even the cpu version of tensorflow 2 causes the kernel to restart when imported :disappointed: .\r\n## only happening in one computer, might be because its old .. but its not really that old :thinking: \r\nHello I was having this error i tried fixing with the above recommended\r\n```\r\nln -s /opt/cuda/lib64/libcublas.so.10 /opt/cuda/lib64/libcublas.so.10.0\r\n```\r\nand it work but now im having a similar error with libcusolver.so.10\r\nand after trying the same fix it doesn't work .. \r\n```\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n~/VENV/Science/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59   from tensorflow.python.pywrap_tensorflow_internal import __version__\r\n\r\n~/VENV/Science/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py in <module>\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow_internal = swig_import_helper()\r\n     29     del swig_import_helper\r\n\r\n~/VENV/Science/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py in swig_import_helper()\r\n     23             try:\r\n---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n     25             finally:\r\n\r\n/usr/lib/python3.7/imp.py in load_module(name, file, filename, details)\r\n    241         else:\r\n--> 242             return load_dynamic(name, filename, file)\r\n    243     elif type_ == PKG_DIRECTORY:\r\n\r\n/usr/lib/python3.7/imp.py in load_dynamic(name, path, file)\r\n    341             name=name, loader=loader, origin=path)\r\n--> 342         return _load(spec)\r\n    343 \r\n\r\nImportError: /opt/cuda/lib64/libcusolver.so.10.0: version `libcusolver.so.10.0' not found (required by /home/archangel/VENV/Science/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-3-0bc7e94a7704> in <module>\r\n      8 get_ipython().run_line_magic('matplotlib', 'inline')\r\n      9 \r\n---> 10 import tensorflow as tf\r\n     11 import os\r\n\r\n~/VENV/Science/lib/python3.7/site-packages/tensorflow/__init__.py in <module>\r\n     25 import sys as _sys\r\n     26 \r\n---> 27 from tensorflow._api.v2 import audio\r\n     28 from tensorflow._api.v2 import autograph\r\n     29 from tensorflow._api.v2 import bitwise\r\n\r\n~/VENV/Science/lib/python3.7/site-packages/tensorflow/_api/v2/audio/__init__.py in <module>\r\n      6 from __future__ import print_function as _print_function\r\n      7 \r\n----> 8 from tensorflow.python.ops.gen_audio_ops import decode_wav\r\n      9 from tensorflow.python.ops.gen_audio_ops import encode_wav\r\n     10 \r\n\r\n~/VENV/Science/lib/python3.7/site-packages/tensorflow/python/__init__.py in <module>\r\n     47 import numpy as np\r\n     48 \r\n---> 49 from tensorflow.python import pywrap_tensorflow\r\n     50 \r\n     51 # Protocol buffers\r\n\r\n~/VENV/Science/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>\r\n     72 for some common reasons and solutions.  Include the entire stack trace\r\n     73 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 74   raise ImportError(msg)\r\n     75 \r\n     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/archangel/VENV/Science/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/archangel/VENV/Science/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/archangel/VENV/Science/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.7/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.7/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: /opt/cuda/lib64/libcusolver.so.10.0: version `libcusolver.so.10.0' not found (required by /home/archangel/VENV/Science/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n\r\n```\r\n\r\nhere is my ls -l of the dir filtering for libcu\r\n```\r\n\u25b6 ls -l | ack libcu\r\nlrwxrwxrwx 1 root root        25 Apr 22 18:22 libcublasLt.so -> libcublasLt.so.10.1.0.105\r\nlrwxrwxrwx 1 root root        25 Apr 22 18:22 libcublasLt.so.10 -> libcublasLt.so.10.1.0.105\r\nlrwxrwxrwx 1 root root        25 Apr 22 18:22 libcublasLt.so.10.1 -> libcublasLt.so.10.1.0.105\r\nlrwxrwxrwx 1 root root        25 Apr 22 18:22 libcublasLt.so.10.1.0 -> libcublasLt.so.10.1.0.105\r\n-rwxr-xr-x 1 root root  37058992 Apr 22 18:22 libcublasLt.so.10.1.0.105\r\n-rw-r--r-- 1 root root  23519634 Apr 22 18:22 libcublasLt_static.a\r\nlrwxrwxrwx 1 root root        23 Apr 22 18:22 libcublas.so -> libcublas.so.10.1.0.105\r\nlrwxrwxrwx 1 root root        23 Apr 22 18:22 libcublas.so.10 -> libcublas.so.10.1.0.105\r\nlrwxrwxrwx 1 root root        15 Apr 25 21:08 libcublas.so.10.0 -> libcublas.so.10\r\nlrwxrwxrwx 1 root root        23 Apr 22 18:22 libcublas.so.10.1 -> libcublas.so.10.1.0.105\r\nlrwxrwxrwx 1 root root        23 Apr 22 18:22 libcublas.so.10.1.0 -> libcublas.so.10.1.0.105\r\n-rwxr-xr-x 1 root root  78315120 Apr 22 18:22 libcublas.so.10.1.0.105\r\n-rw-r--r-- 1 root root  90743938 Apr 22 18:22 libcublas_static.a\r\n-rw-r--r-- 1 root root    717772 Apr 22 18:22 libcudadevrt.a\r\nlrwxrwxrwx 1 root root        21 Apr 22 18:22 libcudart.so -> libcudart.so.10.1.105\r\nlrwxrwxrwx 1 root root        21 Apr 22 18:22 libcudart.so.10 -> libcudart.so.10.1.105\r\nlrwxrwxrwx 1 root root        15 Apr 25 21:11 libcudart.so.10.0 -> libcudart.so.10\r\nlrwxrwxrwx 1 root root        21 Apr 22 18:22 libcudart.so.10.1 -> libcudart.so.10.1.105\r\n-rwxr-xr-x 1 root root    504480 Apr 22 18:22 libcudart.so.10.1.105\r\n-rw-r--r-- 1 root root    888488 Apr 22 18:22 libcudart_static.a\r\nlrwxrwxrwx 1 root root        20 Apr 22 18:22 libcufft.so -> libcufft.so.10.1.105\r\nlrwxrwxrwx 1 root root        20 Apr 22 18:22 libcufft.so.10 -> libcufft.so.10.1.105\r\nlrwxrwxrwx 1 root root        20 Apr 22 18:22 libcufft.so.10.1 -> libcufft.so.10.1.105\r\n-rwxr-xr-x 1 root root 116778696 Apr 22 18:22 libcufft.so.10.1.105\r\n-rw-r--r-- 1 root root 137788484 Apr 22 18:22 libcufft_static.a\r\n-rw-r--r-- 1 root root 124248792 Apr 22 18:22 libcufft_static_nocallback.a\r\nlrwxrwxrwx 1 root root        21 Apr 22 18:22 libcufftw.so -> libcufftw.so.10.1.105\r\nlrwxrwxrwx 1 root root        21 Apr 22 18:22 libcufftw.so.10 -> libcufftw.so.10.1.105\r\nlrwxrwxrwx 1 root root        21 Apr 22 18:22 libcufftw.so.10.1 -> libcufftw.so.10.1.105\r\n-rwxr-xr-x 1 root root    500600 Apr 22 18:22 libcufftw.so.10.1.105\r\n-rw-r--r-- 1 root root     33250 Apr 22 18:22 libcufftw_static.a\r\nlrwxrwxrwx 1 root root        22 Apr 22 18:22 libcuinj64.so -> libcuinj64.so.10.1.105\r\nlrwxrwxrwx 1 root root        22 Apr 22 18:22 libcuinj64.so.10 -> libcuinj64.so.10.1.105\r\nlrwxrwxrwx 1 root root        22 Apr 22 18:22 libcuinj64.so.10.1 -> libcuinj64.so.10.1.105\r\n-rwxr-xr-x 1 root root   7792248 Apr 22 18:22 libcuinj64.so.10.1.105\r\n-rw-r--r-- 1 root root     31954 Apr 22 18:22 libculibos.a\r\nlrwxrwxrwx 1 root root        21 Apr 22 18:22 libcurand.so -> libcurand.so.10.1.105\r\nlrwxrwxrwx 1 root root        21 Apr 22 18:22 libcurand.so.10 -> libcurand.so.10.1.105\r\nlrwxrwxrwx 1 root root        21 Apr 22 18:22 libcurand.so.10.1 -> libcurand.so.10.1.105\r\n-rwxr-xr-x 1 root root  59811768 Apr 22 18:22 libcurand.so.10.1.105\r\n-rw-r--r-- 1 root root  59841762 Apr 22 18:22 libcurand_static.a\r\nlrwxrwxrwx 1 root root        23 Apr 22 18:22 libcusolver.so -> libcusolver.so.10.1.105\r\nlrwxrwxrwx 1 root root        23 Apr 22 18:22 libcusolver.so.10 -> libcusolver.so.10.1.105\r\nlrwxrwxrwx 1 root root        17 Apr 25 21:19 libcusolver.so.10.0 -> libcusolver.so.10\r\nlrwxrwxrwx 1 root root        23 Apr 22 18:22 libcusolver.so.10.1 -> libcusolver.so.10.1.105\r\n-rwxr-xr-x 1 root root 182557912 Apr 22 18:22 libcusolver.so.10.1.105\r\n-rw-r--r-- 1 root root  91260642 Apr 22 18:22 libcusolver_static.a\r\nlrwxrwxrwx 1 root root        23 Apr 22 18:22 libcusparse.so -> libcusparse.so.10.1.105\r\nlrwxrwxrwx 1 root root        23 Apr 22 18:22 libcusparse.so.10 -> libcusparse.so.10.1.105\r\nlrwxrwxrwx 1 root root        23 Apr 22 18:22 libcusparse.so.10.1 -> libcusparse.so.10.1.105\r\n-rwxr-xr-x 1 root root  90326888 Apr 22 18:22 libcusparse.so.10.1.105\r\n-rw-r--r-- 1 root root 100712188 Apr 22 18:22 libcusparse_static.a\r\n```\r\n\r\ni tried what @fabricatedmath  suggested but now my kernel restarts in jupyter lab everytime i try to import tensorflow 2\r\n\r\nhere is my conda\r\n\r\n```\r\n\u25b6 conda env export\r\nname: Science\r\nchannels:\r\n  - defaults\r\ndependencies:\r\n  - cudatoolkit=10.0.130=0\r\n  - cudnn=7.3.1=cuda10.0_0\r\nprefix: /home/archangel/anaconda3/envs/Science\r\n\r\n\u25b6 echo $path\r\n/home/archangel/anaconda3/envs/Science/bin /home/archangel/anaconda3/condabin /home/archangel/anaconda3/bin /home/archangel/bin /usr/local/bin /home/archangel/.cargo/bin /home/archangel/bin /usr/local/bin /home/archangel/.cargo/bin /usr/local/sbin /usr/local/bin /usr/bin /opt/cuda/bin /var/lib/flatpak/exports/bin /usr/lib/jvm/default/bin /usr/bin/site_perl /usr/bin/vendor_perl /usr/bin/core_perl /home/archangel/.gem/ruby/2.1.0/bin /home/archangel/.cargo/bin /home/archangel/Android/android-sdk-linux/tools /home/archangel/Android/android-sdk-linux/platform-tools /home/archangel/Android/android-sdk-linux/build-tools /opt/cuda/bin /var/lib/flatpak/exports/bin /usr/lib/jvm/default/bin /usr/bin/site_perl /usr/bin/vendor_perl /usr/bin/core_perl /home/archangel/.gem/ruby/2.1.0/bin /home/archangel/.cargo/bin /home/archangel/Android/android-sdk-linux/tools /home/archangel/Android/android-sdk-linux/platform-tools /home/archangel/Android/android-sdk-linux/build-tools\r\n\r\n\u25b6 echo $LD_LIBRARY_PATH\r\n/home/archangel/anaconda3/envs/Science/lib::/opt/cuda/lib64:/home/archangel/VENV/_Resources/cudnn/cuda_v5/lib64:/home/archangel/VENV/_Resources/cudnn/cuda_v10/lib64\r\n\r\n\u25b6 echo $CUDA_HOME\r\n/home/archangel/anaconda3/envs/Science/lib\r\n\r\n```", "> @rhinsall\r\n> I just found this out myself, not sure if it's common knowledge, but got around this by doing\r\n> \r\n> ```shell\r\n> conda install cudatoolkit\r\n> conda install cudnn\r\n> ```\r\n> \r\n> I have cuda-10.1 installed on my box, this installed a local conda-only cuda-10.0. Obviously this is to just keep tensorflow working while waiting for better support.\r\n\r\nit works very well", "Thank you.  Yes, a conda-specific environment worked perfectly for me as well.  It seems to get all the dependencies right.", "> @rhinsall\r\n> I just found this out myself, not sure if it's common knowledge, but got around this by doing\r\n> \r\n> ```shell\r\n> conda install cudatoolkit\r\n> conda install cudnn\r\n> ```\r\n> \r\n> I have cuda-10.1 installed on my box, this installed a local conda-only cuda-10.0. Obviously this is to just keep tensorflow working while waiting for better support.\r\n\r\nThe simplest and fastest solution! Also do this after\r\n\r\nconda install tensorflow-gpu\r\n\r\nand this is what is installed\r\n\r\nabsl-py==0.7.1\r\nastor==0.7.1\r\ncertifi==2019.3.9\r\ngast==0.2.2\r\ngrpcio==1.16.1\r\nh5py==2.9.0\r\nKeras-Applications==1.0.7\r\nKeras-Preprocessing==1.0.9\r\nMarkdown==3.1\r\nmkl-fft==1.0.12\r\nmkl-random==1.0.2\r\nmock==2.0.0\r\nnumpy==1.16.3\r\npbr==5.1.3\r\nprotobuf==3.7.1\r\nscipy==1.2.1\r\nsix==1.12.0\r\ntensorboard==1.13.1\r\ntensorflow==1.13.1\r\ntensorflow-estimator==1.13.0\r\ntermcolor==1.1.0\r\nWerkzeug==0.15.2\r\n", "I also encountered this issue on the tf2.0 nightly GPU, I overcame the issue by doing a conda install of CUDA 10.0.  However, I would like to have things up and running with my ubuntu install of CUDA 10.1.  ", "**Possible Solution:**\r\nDocker container which handles all dependencies and provides a working tensorflow2-cuda environment:\r\n\r\n**Link Docker Hub:**\r\n[https://hub.docker.com/r/twodarkmessiah/tensorflow2-cuda)](https://hub.docker.com/r/twodarkmessiah/tensorflow2-cuda)\r\n", "In my case, the issue was that the location of libcublas changed with cuda 10.1 and needed me to update my LD_LIBRARY_PATH", "> In my case, the issue was that the location of libcublas changed with cuda 10.1 and needed me to update my LD_LIBRARY_PATH\r\n\r\nExactly. \r\n1. find your CUDA install path, in my case it is `/usr/local/cuda`\r\n2. `export LD_LIBRARY_PATH=/usr/local/cuda/lib64`\r\n\r\nThen TF follow LD_LIBRARY_PATH to locate libcublas.so.10.0", "I found that the issue was due to the version of installed CUDA. In my case, the cuda version is 9.0 which is not supported by the latest stable version of tensorflow-gpu. BTW, I use  `dpkg -l | grep cuda`  to check the version of CUDA in Ubuntu 16.04. \r\nTherefore, my solution is downgrading the version of tensorflow-gpu from 1.13 to 1.12 and the issue was solved. ", "My problem was incompatible CUDA and Tensorflow. So I check CUDA version and Tensorflow Version on my Ubuntu. You can find tensorflow and cuda version compatibility on [link](https://www.tensorflow.org/install/source#tested_build_configurations) \r\n\r\n- Check CUDA version:\r\ncat /usr/local/cuda/version.txt\r\n\r\n- Check Tensorflow version:\r\npip list | grep tensorflow\r\n\r\nI had : \r\ntensorflow                                1.12.0      \r\ntensorflow-estimator                1.13.0      \r\ntensorflow-gpu                         1.13.0  \r\n\r\nSo I re-installed \r\ntensorflow-gpu  with version 1.12.0  using <pip install tensorflow-gpu==1.12.0> \r\n\r\nNow I can import tensorflow without trouble!\r\n", "Hello,\r\n\r\nI had the same issue. I fixed it by adding the below command to the '**.bashrc**' file.\r\n\r\n> export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-10.0/lib64/\r\n\r\n**System configuration:**\r\n\r\n```\r\nUbuntu 16.04 LTS\r\nTensorflow GPU 2.0beta1\r\nCuda 10.0\r\ncuDNN 7.6.0 for Cuda 10.0\r\n```\r\n\r\nI  used conda to configure my system.", "for a cuda-10.1 user, the  cublas lib has been moved to  /usr/lib/x86_64-linux-gnu , just creat a symbolic link to that file bypass the problem.", "if you are using Conda and want to use tf2 I just find a work around :\r\nFirst install:\r\nconda install cudatoolkit\r\nconda install cudnn\r\nthen \r\npip install tensorflow-gpu\r\nif this dosent work\r\nenter on your terminal \r\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/**YOURUSERNAME**/anaconda3/pkgs/cudatoolkit-10.**X.Y-Z**/lib/\r\n\r\nif this work then add the line above to your **.bashrc**\r\n\r\nX.Y.Z is the values for your coda version you can check this with \r\nls /home/**YOURUSERNAME**/anaconda3/pkgs/ \r\n", "Look for compatible tensorflow and cuda versions: \r\n[https://www.tensorflow.org/install/source#tested_build_configurations](https://www.tensorflow.org/install/source#tested_build_configurations)\r\nLook for campatible tensorflow and keras versoins: \r\n[https://docs.floydhub.com/guides/environments/](https://docs.floydhub.com/guides/environments/)", "Install CUDA 10.0 into /usr/local/cuda110.0/ where your program will find the new libraries.\r\n\r\nIf you have CUDA 10.1 installed into /usr/local/cuda-10.1/ along with the nvidia drivers.  In that event, skip installing the drivers and only install the cuda libraries.  Don't link /usr/local/cuda to 10.0.  Leave it linked to 10.1.  We just need the libraries in the location tensorflow looks for it.  cuda 10.0 successfully shares the drivers 10.1 installed.", "On a fresh ubuntu 18.04, this does purge nvidia* to try and get a clean install.\r\n\r\n```\r\nsudo apt-get install build-essential -y\r\nsudo apt-get install cmake git unzip zip -y\r\nsudo apt-get install python-dev python3-dev python-pip python3-pip -y\r\nsudo apt-get install linux-headers-$(uname -r) -y\r\nsudo apt-get purge nvidia* -y\r\nsudo apt-get autoremove -y\r\nsudo apt-get autoclean -y\r\nsudo rm -rf /usr/local/cuda* \r\nsudo apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub\r\necho \"deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64 /\" | sudo tee /etc/apt/sources.list.d/cuda.list\r\nsudo apt-get update \r\nsudo apt-get -o Dpkg::Options::=\"--force-overwrite\" install cuda-10-0 cuda-drivers -y\r\n```\r\nReboot, no really.\r\n\r\n```\r\necho 'export PATH=/usr/local/cuda-10.0/bin${PATH:+:${PATH}}' >> ~/.bashrc\r\necho 'export LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}' >> ~/.bashrc\r\nsource ~/.bashrc\r\nsudo ldconfig\r\n```\r\n\r\n**CUDNN**\r\n\r\nCUDNN can be downloaded from [here](https://developer.nvidia.com/cudnn) note if you're redirected to the NVIDIA home page you either need to sign in, or create an account, come back here click that link again it'll take you to the download page.\r\n\r\nSelect the one that says 'Download cuDNN v7.something, for CUDA 10.0' **make sure it's for version 10.0**\r\n\r\n```\r\ntar -xzvf cudnn-10.0-linux-x64-v7.6.*.tgz\r\nsudo cp cuda/include/cudnn.h /usr/local/cuda/include\r\nsudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64\r\nsudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn*\r\n```\r\n\r\nReboot, welcome to the party. This is the most reliable fiddle free way I can find, it always seems to work whereas conda etc seems to be flakey for sometimes.", "Why is this closed? It is definitely an issue, where tensorflow doesn't seem to be observing the naming conventions of unix shared libraries. So, for tensorflow-gpu==2.1.0, it's trying to load `libcublas.so.10.0`, but it _should_ be trying to load `libcublas.so.10` for cuda10.\r\n\r\nEither that, or there's more stringent requirements on the individual cuda versions. (Is Cuda really that unstable from version to version?)\r\n\r\nWhatever the case, tensorflow 2.1.0 is supposed to be compatible with cuda 10.1 -- the installation instructions show installing cuda 10.1. But it's then trying to load a 10.0 shared library. That is a bug in tensorflow, or in the instructions.", "`pip3 install tensorflow-gpu==2.0.0b1`\r\n`2.0.0b1` this version works for me.", "> > > @mostafaelhoushi\r\n> > > When i am running this command `find / -name \"libcublas.so.10.0\"`\r\n> > > the output is\r\n> > > ```\r\n> > > /var/lib/docker/overlay2/33ff618e94595ffbdc09016439dc6a469fa8adc3ec3b5231f776d6065aab7968/diff/root/.cache/bazel/_bazel_root/e53bbb0b0da4e26d24b415310219b953/execroot/tf_serving/bazel-out/k8-opt/bin/tensorflow_serving/model_servers/tensorflow_model_server.runfiles/tf_serving/external/local_config_cuda/cuda/cuda/lib/libcublas.so.10.0\r\n> > > /var/lib/docker/overlay2/33ff618e94595ffbdc09016439dc6a469fa8adc3ec3b5231f776d6065aab7968/diff/root/.cache/bazel/_bazel_root/e53bbb0b0da4e26d24b415310219b953/execroot/tf_serving/bazel-out/k8-opt/bin/tensorflow_serving/model_servers/tensorflow_model_server.runfiles/tf_serving/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/libcublas.so.10.0\r\n> > > /var/lib/docker/overlay2/33ff618e94595ffbdc09016439dc6a469fa8adc3ec3b5231f776d6065aab7968/diff/root/.cache/bazel/_bazel_root/e53bbb0b0da4e26d24b415310219b953/execroot/tf_serving/bazel-out/k8-opt/bin/tensorflow_serving/model_servers/tensorflow_model_server.runfiles/local_config_cuda/cuda/cuda/lib/libcublas.so.10.0\r\n> > > /var/lib/docker/overlay2/33ff618e94595ffbdc09016439dc6a469fa8adc3ec3b5231f776d6065aab7968/diff/root/.cache/bazel/_bazel_root/e53bbb0b0da4e26d24b415310219b953/execroot/tf_serving/bazel-out/k8-opt/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/libcublas.so.10.0\r\n> > > /var/lib/docker/overlay2/33ff618e94595ffbdc09016439dc6a469fa8adc3ec3b5231f776d6065aab7968/diff/root/.cache/bazel/_bazel_root/e53bbb0b0da4e26d24b415310219b953/execroot/tf_serving/bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda/cuda/lib/libcublas.so.10.0\r\n> > > /var/lib/docker/overlay2/33ff618e94595ffbdc09016439dc6a469fa8adc3ec3b5231f776d6065aab7968/diff/root/.cache/bazel/_bazel_root/e53bbb0b0da4e26d24b415310219b953/execroot/tf_serving/bazel-out/host/genfiles/external/local_config_cuda/cuda/cuda/lib/libcublas.so.10.0\r\n> > > /var/lib/docker/overlay2/97cb0c942535cde4622f53bf094251cd1aef1cfc744e8ddda1472ee691f87618/diff/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcublas.so.10.0\r\n> > > /var/lib/docker/overlay2/2fb234250d278545f55a004fcd436b4cba5e847c40503b990ffe800f3b440cb5/diff/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcublas.so.10.0\r\n> > > /var/lib/docker/overlay2/c704b6be3bc1a5d25119fa46216a4e64f872d8001d8bed6d40930f6420ffb091/diff/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcublas.so.10.0\r\n> > > /usr/local/cuda-10.0/lib64/libcublas.so.10.0\r\n> > > ```\r\n> > \r\n> > \r\n> > OK. I see `libcublas.so.10.0` is found in `/usr/local/cuda-10.0/lib64/`.\r\n> > Try running this command:\r\n> > ```\r\n> > export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-10.0/lib64/\r\n> > ```\r\n> > \r\n> > \r\n> > and try again.\r\n> > NOTE: I see the library is also found in your docker system. I am not familiar with dockers, so maybe someone else could help here. But try the above command and see.\r\n> \r\n> @mostafaelhoushi have given the best solution. Anyone who is confused see this answer. :)\r\n\r\nthis works for me! thanks! ", "in case anyone still facing this:\r\ni got cuda 10.2 and i just ran into this problem and here's how i solved it : \r\n```\r\ncd ~\r\ngedit .bashrc\r\n```\r\n```\r\n#add this in the end : \r\nexport PATH=/usr/local/cuda-10.2/bin${PATH:+:${PATH}}\r\nexport LD_LIBRARY_PATH=/usr/local/cuda-10.2/lib64\\${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\r\nexport LD_LIBRARY_PATH=/usr/local/cuda10.2/targets/x86_64linux\\${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\r\n```\r\n", "> > In my case, the issue was that the location of libcublas changed with cuda 10.1 and needed me to update my LD_LIBRARY_PATH\r\n> \r\n> Exactly.\r\n> \r\n> 1. find your CUDA install path, in my case it is `/usr/local/cuda`\r\n> 2. `export LD_LIBRARY_PATH=/usr/local/cuda/lib64`\r\n> \r\n> Then TF follow LD_LIBRARY_PATH to locate libcublas.so.10.0\r\n\r\nThis worked out for me. I had a tensorflow-gpu 1.13.1 installed inside a python virutal environment. CUDA installation is 10.0. Tried symlinks but didn't work. Setting LD_LIBRARY_PATH did the job. ", "> in case anyone still facing this:\r\n> i got cuda 10.2 and i just ran into this problem and here's how i solved it :\r\n> \r\n> ```\r\n> cd ~\r\n> gedit .bashrc\r\n> ```\r\n> \r\n> ```\r\n> #add this in the end : \r\n> export PATH=/usr/local/cuda-10.2/bin${PATH:+:${PATH}}\r\n> export LD_LIBRARY_PATH=/usr/local/cuda-10.2/lib64\\${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\r\n> export LD_LIBRARY_PATH=/usr/local/cuda10.2/targets/x86_64linux\\${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\r\n> ```\r\n\r\nThanks, I was looking for a solution that would allow me to run various versions of tensorflow and pytorch on the same machine.  This worked out great!", "> in case anyone still facing this:\r\n> i got cuda 10.2 and i just ran into this problem and here's how i solved it :\r\n> \r\n> ```\r\n> cd ~\r\n> gedit .bashrc\r\n> ```\r\n> \r\n> ```\r\n> #add this in the end : \r\n> export PATH=/usr/local/cuda-10.2/bin${PATH:+:${PATH}}\r\n> export LD_LIBRARY_PATH=/usr/local/cuda-10.2/lib64\\${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\r\n> export LD_LIBRARY_PATH=/usr/local/cuda10.2/targets/x86_64linux\\${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\r\n> ```\r\n\r\nThanks! It worked for me! \r\nI only had a problem with `nvcc` command, that was not found anymore.\r\n\r\nAs a workaround, I exported both `cuda` and `cuda-10.2` in the `LD_LIBRARY_PATH`:\r\n\r\n```\r\nexport PATH=$PATH:/usr/local/cuda/bin\r\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\r\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-10.2/lib64:/usr/local/cuda-10.2/extras/CUPTI/lib64\r\n```\r\n\r\nIn this way tensorflow is working, alongside `nvcc`", "I found this topic while I was trying to resolve similar problem but seems with newer version. So want to add for those who get here too.\r\n\r\nRight now actual Python version 3.8.5 installs TensorFlow 2.3.0.\r\nIt requires cuda 10.1 and default.\r\n\r\nEasiest way to install it together with nvidia drivers (thanks @marcfielding1) is:\r\n```\r\nsudo apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub\r\necho \"deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64 /\" | sudo tee /etc/apt/sources.list.d/cuda.list\r\nsudo apt-get update\r\nsudo apt-get -o Dpkg::Options::=\"--force-overwrite\" install cuda-10-1 cuda-drivers -y\r\n```\r\n\r\nOnce done, it cause the same:\r\n```\r\n Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory\r\n```\r\n\r\nThat happens because `libcublas.so.10` sits in `/usr/local/cuda-10.2/lib64` (surprise from nvidia - installation of 10.1 installs some 10.2 stuff) but only `/usr/local/cuda` is in include path which points to `/usr/local/cuda-10.1`.\r\n\r\nAdding it to include path helps and everything seems working:\r\n```\r\nexport LD_LIBRARY_PATH=/usr/local/cuda-10.2/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\r\n```", "> I found this topic while I was trying to resolve similar problem but seems with newer version. So want to add for those who get here too.\r\n> \r\n> Right now actual Python version 3.8.5 installs TensorFlow 2.3.0.\r\n> It requires cuda 10.1 and default.\r\n> \r\n> Easiest way to install it together with nvidia drivers (thanks @marcfielding1) is:\r\n> \r\n> ```\r\n> sudo apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub\r\n> echo \"deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64 /\" | sudo tee /etc/apt/sources.list.d/cuda.list\r\n> sudo apt-get update\r\n> sudo apt-get -o Dpkg::Options::=\"--force-overwrite\" install cuda-10-1 cuda-drivers -y\r\n> ```\r\n> \r\n> Once done, it cause the same:\r\n> \r\n> ```\r\n>  Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory\r\n> ```\r\n> \r\n> That happens because `libcublas.so.10` sits in `/usr/local/cuda-10.2/lib64` (surprise from nvidia - installation of 10.1 installs some 10.2 stuff) but only `/usr/local/cuda` is in include path which points to `/usr/local/cuda-10.1`.\r\n> \r\n> Adding it to include path helps and everything seems working:\r\n> \r\n> ```\r\n> export LD_LIBRARY_PATH=/usr/local/cuda-10.2/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\r\n> ```\r\n\r\nAdding \r\n```\r\nexport LD_LIBRARY_PATH=/usr/local/cuda-10.2/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\r\n```\r\nto your .bashrc or .zshrc would do the trick.\r\nBefore that make sure you have both cuda-10.1 and cuda-10.2 available at `/usr/local/`", "> > in case anyone still facing this:\r\n> > i got cuda 10.2 and i just ran into this problem and here's how i solved it :\r\n> > ```\r\n> > cd ~\r\n> > gedit .bashrc\r\n> > ```\r\n> > \r\n> > \r\n> > ```\r\n> > #add this in the end : \r\n> > export PATH=/usr/local/cuda-10.2/bin${PATH:+:${PATH}}\r\n> > export LD_LIBRARY_PATH=/usr/local/cuda-10.2/lib64\\${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\r\n> > export LD_LIBRARY_PATH=/usr/local/cuda10.2/targets/x86_64linux\\${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\r\n> > ```\r\n> \r\n> Thanks! It worked for me!\r\n> I only had a problem with `nvcc` command, that was not found anymore.\r\n> \r\n> As a workaround, I exported both `cuda` and `cuda-10.2` in the `LD_LIBRARY_PATH`:\r\n> \r\n> ```\r\n> export PATH=$PATH:/usr/local/cuda/bin\r\n> export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\r\n> export LD_LIBRARY_PATH=$LD_LIBRARY_PATH\r\n> In this way tensorflow is working, alongside nvcc\r\n\r\nThis worked perfectly for me, plus I added `export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-10.2/targets/x86_64-linux/lib/`.\r\n\r\nAfter that tensorflow found physical GPU.\r\nGood luck.\r\n\r\n", "> I found this topic while I was trying to resolve similar problem but seems with newer version. So want to add for those who get here too.\r\n> \r\n> Right now actual Python version 3.8.5 installs TensorFlow 2.3.0.\r\n> It requires cuda 10.1 and default.\r\n> \r\n> Easiest way to install it together with nvidia drivers (thanks @marcfielding1) is:\r\n> \r\n> ```\r\n> sudo apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub\r\n> echo \"deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64 /\" | sudo tee /etc/apt/sources.list.d/cuda.list\r\n> sudo apt-get update\r\n> sudo apt-get -o Dpkg::Options::=\"--force-overwrite\" install cuda-10-1 cuda-drivers -y\r\n> ```\r\n> \r\n> Once done, it cause the same:\r\n> \r\n> ```\r\n>  Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory\r\n> ```\r\n> \r\n> That happens because `libcublas.so.10` sits in `/usr/local/cuda-10.2/lib64` (surprise from nvidia - installation of 10.1 installs some 10.2 stuff) but only `/usr/local/cuda` is in include path which points to `/usr/local/cuda-10.1`.\r\n> \r\n> Adding it to include path helps and everything seems working:\r\n> \r\n> ```\r\n> export LD_LIBRARY_PATH=/usr/local/cuda-10.2/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\r\n> ```\r\n\r\nSpecifically, the 10-2 packages come from libcublas10 (which may get installed automatically with the others, but not if apt thinks it's already installed e.g., when I deleted the \"surprise\" files). A possible corner case for some.\r\n\r\n`sudo apt-get -o Dpkg::Options::=\"--force-overwrite\" install --reinstall cuda-10-1 cuda-drivers libcublas10  -y`\r\n\r\nI then symbolic linked it into cuda.\r\n\r\n```\r\nsudo ln -s /usr/local/cuda-10.1 /usr/local/cuda\r\nsudo ln -s /usr/local/cuda-10.2/lib64/* /usr/local/cuda/lib64/\r\n```", "SUCESS!\r\n\r\n`\r\n$ export LD_LIBRARY_PATH=/usr/local/cuda-10.2/lib64\r\n`\r\n\r\n`\r\nrfrye@SunWukong:~/Code/General-Development/Roger/c2111_whole_anomaly_ML/experiments$ python3.8 img_stack.py CHANNELS Machine_Learning_Recoat_Interaction_2_Stable_Indra_FullScans 2 -f ted,tep -l 130,131 --template 704,608 -vt 0b010 -A VH\r\n2020-10-13 15:58:35.873728: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\nInitial config length 2\r\n0: Build Machine_Learning_Recoat_Interaction_2_Stable_Indra_FullScans\r\nconfig {5: [126, 127, 128, 129, 130, 131, 132, 133, 134, 135], 2: [128, 131, 132, 133, 134]}\r\n2020-10-13 15:58:36.762308: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2020-10-13 15:58:36.807565: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:3b:00.0 name: Quadro P6000 computeCapability: 6.1\r\ncoreClock: 1.645GHz coreCount: 30 deviceMemorySize: 23.88GiB deviceMemoryBandwidth: 403.49GiB/s\r\n2020-10-13 15:58:36.807615: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-10-13 15:58:36.810036: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-10-13 15:58:36.812030: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-10-13 15:58:36.812371: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-10-13 15:58:36.814467: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-10-13 15:58:36.815718: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-10-13 15:58:36.820158: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-10-13 15:58:36.822504: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-10-13 15:58:36.822858: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-10-13 15:58:36.839247: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2400000000 Hz\r\n2020-10-13 15:58:36.842438: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e531c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-10-13 15:58:36.842512: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-10-13 15:58:36.967553: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5670d20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-10-13 15:58:36.967624: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro P6000, Compute Capability 6.1\r\n2020-10-13 15:58:36.970244: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:3b:00.0 name: Quadro P6000 computeCapability: 6.1\r\ncoreClock: 1.645GHz coreCount: 30 deviceMemorySize: 23.88GiB deviceMemoryBandwidth: 403.49GiB/s\r\n2020-10-13 15:58:36.970326: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-10-13 15:58:36.970366: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-10-13 15:58:36.970393: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-10-13 15:58:36.970419: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-10-13 15:58:36.970467: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-10-13 15:58:36.970494: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-10-13 15:58:36.970521: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-10-13 15:58:36.974574: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-10-13 15:58:36.974639: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-10-13 15:58:37.468505: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-10-13 15:58:37.468542: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-10-13 15:58:37.468548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2020-10-13 15:58:37.470189: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22737 MB memory) -> physical GPU (device: 0, name: Quadro P6000, pci bus id: 0000:3b:00.0, compute capability: 6.1)\r\n`\r\n\r\nand it ran about 5x faster on this very simple problem.", "Check what do you have in \r\n`cd /usr/local/cuda` + hit tab \r\n\r\nif for example you get \r\n\r\n`cuda/      cuda-10.1/     cuda-10.2`\r\n\r\nyou need to export cuda-10.1 and cuda-10.2 as following \r\n\r\n`export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-10.1/lib64/`\r\n\r\nand \r\n\r\n`export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-10.2/lib64/`\r\n\r\nThis is what works for me \r\n", "> > I found this topic while I was trying to resolve similar problem but seems with newer version. So want to add for those who get here too.\r\n> > Right now actual Python version 3.8.5 installs TensorFlow 2.3.0.\r\n> > It requires cuda 10.1 and default.\r\n> > Easiest way to install it together with nvidia drivers (thanks @marcfielding1) is:\r\n> > ```\r\n> > sudo apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub\r\n> > echo \"deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64 /\" | sudo tee /etc/apt/sources.list.d/cuda.list\r\n> > sudo apt-get update\r\n> > sudo apt-get -o Dpkg::Options::=\"--force-overwrite\" install cuda-10-1 cuda-drivers -y\r\n> > ```\r\n> > \r\n> > \r\n> > Once done, it cause the same:\r\n> > ```\r\n> >  Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory\r\n> > ```\r\n> > \r\n> > \r\n> > That happens because `libcublas.so.10` sits in `/usr/local/cuda-10.2/lib64` (surprise from nvidia - installation of 10.1 installs some 10.2 stuff) but only `/usr/local/cuda` is in include path which points to `/usr/local/cuda-10.1`.\r\n> > Adding it to include path helps and everything seems working:\r\n> > ```\r\n> > export LD_LIBRARY_PATH=/usr/local/cuda-10.2/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\r\n> > ```\r\n> \r\n> Specifically, the 10-2 packages come from libcublas10 (which may get installed automatically with the others, but not if apt thinks it's already installed e.g., when I deleted the \"surprise\" files). A possible corner case for some.\r\n> \r\n> `sudo apt-get -o Dpkg::Options::=\"--force-overwrite\" install --reinstall cuda-10-1 cuda-drivers libcublas10 -y`\r\n> \r\n> I then symbolic linked it into cuda.\r\n> \r\n> ```\r\n> sudo ln -s /usr/local/cuda-10.1 /usr/local/cuda\r\n> sudo ln -s /usr/local/cuda-10.2/lib64/* /usr/local/cuda/lib64/\r\n> ```\r\n\r\nDoesn't this section fix that? Just asking as I was about to do a clean install :-)\r\n\r\n```\r\necho 'export PATH=/usr/local/cuda-10.0/bin${PATH:+:${PATH}}' >> ~/.bashrc\r\necho 'export LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}' >> ~/.bashrc\r\nsource ~/.bashrc\r\nsudo ldconfig\r\n```", "I just ran into this issue installing tensorflow 2.3.1 with the current instructions at https://www.tensorflow.org/install/gpu which are currently:\r\n\r\n```# Add NVIDIA package repositories\r\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-repo-ubuntu1804_10.1.243-1_amd64.deb\r\nsudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub\r\nsudo dpkg -i cuda-repo-ubuntu1804_10.1.243-1_amd64.deb\r\nsudo apt-get update\r\nwget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb\r\nsudo apt install ./nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb\r\nsudo apt-get update\r\n\r\n# Install NVIDIA driver\r\nsudo apt-get install --no-install-recommends nvidia-driver-450\r\n# Reboot. Check that GPUs are visible using the command: nvidia-smi\r\n\r\n# Install development and runtime libraries (~4GB)\r\nsudo apt-get install --no-install-recommends \\\r\n    cuda-10-1 \\\r\n    libcudnn7=7.6.5.32-1+cuda10.1  \\\r\n    libcudnn7-dev=7.6.5.32-1+cuda10.1\r\n\r\n\r\n# Install TensorRT. Requires that libcudnn7 is installed above.\r\nsudo apt-get install -y --no-install-recommends libnvinfer6=6.0.1-1+cuda10.1 \\\r\n    libnvinfer-dev=6.0.1-1+cuda10.1 \\\r\n    libnvinfer-plugin6=6.0.1-1+cuda10.1\r\n```\r\n\r\nI fixed it by just adding these steps at the end:\r\n\r\n`sudo apt install --reinstall libcublas10`\r\n\r\nAdd to ~/.bashrc:\r\n\r\n```shell\r\nexport LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda-10.2/lib64:/usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH\r\n```", "_Massive kudos to the scholar and gentleman @alexkyllo ._ His two simple additions to the official guide saved my life.\r\nRunning a GeForce 1660 Ti on Ubuntu 20.10, Python 3.7, Tensorflow 2.3.1.\r\n\r\nTo reiterate once more:\r\n\r\n1. Purge everything NVidia and CUDA related. Yes, some of these lines are redundant. Just make sure everything gets deleted and you start off on a clean machine.\r\n```\r\nsudo apt-get --purge remove \"*cublas*\" \"cuda*\" \"nsight*\" \r\nsudo apt-get --purge remove \"*nvidia*\"\r\nsudo rm -rf /usr/local/cuda*\r\nsudo apt-get purge nvidia*\r\n```\r\n2. Follow official guide **precisely** and **exactly** (https://www.tensorflow.org/install/gpu#ubuntu_1804_cuda_101). (Yes, even if you're running Ubuntu 20.10, because there is nothing newer posted on this guide.)\r\n3. Then when trying to use TF: \"Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory\"\r\n4. Use @alexkyllo s fixes:\r\n4.1 `sudo apt install --reinstall libcublas10`\r\n4.2 add this to ~/.bashrc:\r\n`export LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda-10.2/lib64:/usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH`\r\n\r\n5. Reboot one last time. Just do it.\r\n6. ENJOY A WORKING TENSORFLOW.\r\n\r\nWhat a mess the tensorflow installation process is. Hours wasted for nothing. This thread alone goes back 1.5 years.", "may anaconda evrioment is wrong,  uninstall cudunn in anaconda ", "> I found this topic while I was trying to resolve similar problem but seems with newer version. So want to add for those who get here too.\r\n> \r\n> Right now actual Python version 3.8.5 installs TensorFlow 2.3.0.\r\n> It requires cuda 10.1 and default.\r\n> \r\n> Easiest way to install it together with nvidia drivers (thanks @marcfielding1) is:\r\n> \r\n> ```\r\n> sudo apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub\r\n> echo \"deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64 /\" | sudo tee /etc/apt/sources.list.d/cuda.list\r\n> sudo apt-get update\r\n> sudo apt-get -o Dpkg::Options::=\"--force-overwrite\" install cuda-10-1 cuda-drivers -y\r\n> ```\r\n> \r\n> Once done, it cause the same:\r\n> \r\n> ```\r\n>  Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory\r\n> ```\r\n> \r\n> That happens because `libcublas.so.10` sits in `/usr/local/cuda-10.2/lib64` (surprise from nvidia - installation of 10.1 installs some 10.2 stuff) but only `/usr/local/cuda` is in include path which points to `/usr/local/cuda-10.1`.\r\n> \r\n> Adding it to include path helps and everything seems working:\r\n> \r\n> ```\r\n> export LD_LIBRARY_PATH=/usr/local/cuda-10.2/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\r\n> ```\r\n\r\nThis is Work for me \r\n**export LD_LIBRARY_PATH=/usr/local/cuda-10.2/lib64**\r\n- Ubuntu 18.04\r\n- Cuda 10.1\r\n- cudnn 7.6.5\r\n- Tensorflow 2.1.0\r\n- RTX 3090\r\n\r\nThanks \r\n\r\n\r\n"]}, {"number": 26181, "title": "[TF Java] Adds support for adding control inputs via Ops API", "body": "Adds withControlDependencies to Ops API as discussed here: https://github.com/tensorflow/tensorflow/issues/26024\r\n\r\ncc: @karllessard @sjamesr ", "comments": ["@melissagrueter @samdow ", "friendly ping :) @mingxingtan ", "Thanks for the PR.  It looks good to me, but I am not very familiar to specific Java APIs.  Will ask someone else to review it later. Thanks!", "@mingxingtan Thanks! Should we be adding someone as a reviewer to this PR?\r\n\r\ncc @rthadur ", "Probably me (@sjamesr), I'll take a look soon\n\nOn Wed, Mar 6, 2019 at 10:41 AM Irene Dea <notifications@github.com> wrote:\n\n> @mingxingtan <https://github.com/mingxingtan> Thanks! Should we be adding\n> someone as a reviewer to this PR?\n>\n> cc @rthadur <https://github.com/rthadur>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/26181#issuecomment-470224445>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AEbTAzLLOor8k0z_mn6SqfdNHzaSZdtiks5vUAvDgaJpZM4bVCo->\n> .\n>\n", "Thanks @sjamesr! How can I add you as a reviewer for this PR?", "Just added @sjamesr as a reviewer. Thanks James!", "friendly ping :) @sjamesr ", "Hey @irenedea ,  you might probably want to rebase your PR now to avoid any merge conflicts after approval, since [this huge one](https://github.com/tensorflow/tensorflow/pull/24858) has just been merged.", "@irenedea please resolve conflicts", "@sjamesr @rthadur Rebased and resolved conflicts, please take another look, thanks!", "@sjamesr @rthadur friendly ping :) ", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26181) for more info**.\n\n<!-- need_sender_cla -->", "@irenedea can you please sign CLA", "CLA issue fixed, should be signed now", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26181) for more info**.\n\n<!-- need_author_cla -->", "coauthors should have CLA now", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26181) for more info**.\n\n<!-- ok -->", "> CLA issue fixed, should be signed now\r\n\r\n@irenedea it still shows CLA is pending , can you use please make sure to use same GitHub username and email-id associated with it.", "@rthadur Should be fixed now, is it still pending? ", "> @rthadur Should be fixed now, is it still pending?\r\n\r\nIts resolved , starting internal merge, thank you"]}, {"number": 26180, "title": "Update link in log message", "body": "The contrib README just redirects you to the other README.", "comments": []}, {"number": 26179, "title": "Datasets not reshuffling between epochs in eager mode", "body": "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 10.14\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13.0-dev20190227\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\nIterating a shuffled dataset returns elements in the same order each time it is iterated over, and the order is unaffected by the random seed.\r\n\r\n**Describe the expected behavior**\r\nThe order should be deterministic given the random seed, and there should be a way to reshuffle between epochs.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ntf.enable_eager_execution()\r\n\r\nd = tf.data.Dataset.range(5).shuffle(5)\r\ntf.set_random_seed(0)\r\nelems = [item.numpy() for item in d]\r\nprint(\"First epoch: \", elems)\r\nelems = [item.numpy() for item in d]\r\nprint(\"Second epoch: \", elems)\r\n```\r\n\r\nRunning this twice produces:\r\n\r\nFirst run:\r\n```\r\nFirst epoch:  [4, 2, 1, 3, 0]\r\nSecond epoch:  [4, 2, 1, 3, 0]\r\n```\r\n\r\nSecond run:\r\n```\r\nFirst epoch:  [3, 1, 0, 2, 4]\r\nSecond epoch:  [3, 1, 0, 2, 4]\r\n```\r\n\r\nSo even though the same random seed is used in both runs, the order of elements differs between them. And additionally, the order is the same between epochs, which you wouldn't necessarily want in your training loop. ", "comments": ["Actually, it looks like this is all explained by `d.shuffle` caching the random seed when it's called instead of when the dataset is iterated over. That seems a bit strange since it means I need to recall `shuffle` at the start of each epoch and so eg passing a dataset to Keras's `fit` method is going to use the same iteration order for every epoch. ", "Thanks for reporting this problem! This is definitely surprising behavior, and we should find a way to make it easier.\r\n\r\n@rohan100jain Do you think it would be possible to extend your solution from b397935bae2530e1cf5022ca6f85a0b898a795d6 to work as expected in the eager case?\r\n\r\nAlso CCing @wangpengmit because whatever happens here ought to be compatible with [https://github.com/tensorflow/community/blob/master/rfcs/20181217-tf2-random-numbers.md](the TF 2.0 randomness interface), and @jsimsa FYI.", "This still occurs in tf2.0.0-beta1. In my case I have a multitask training scenario where the training set sizes are very different. I would like to set a Keras epoch to correspond to the smaller training set size but because of this issue (or something related to it) it appears that the training data for the task with the larger training set size resets and repeats. This effectively truncates the size of the large training set to be ~same as the small one. Is there a work-around other than setting the epoch to correspond to the large training set size?", "The shuffle order of a dataset instance is deterministic.\r\n\r\nIn order for different epochs to exhibit different shuffle order, they need to use different dataset instances:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ntf.enable_eager_execution()\r\n\r\ndef make_dataset():\r\n  return tf.data.Dataset.range(5).shuffle(5)\r\n\r\nelems = [item.numpy() for item in make_dataset()]\r\nprint(\"First epoch: \", elems)\r\nelems = [item.numpy() for item in make_dataset()]\r\nprint(\"Second epoch: \", elems)\r\n```\r\n\r\nBeing able to provide deterministic reshuffling across different iterations of the same dataset instances requires maintaining the state outside of the tf.data graph and is currently not supported.", "@jsimsa When I add seed=0 to shuffle() in the sample code given by @malmaud, the results don't appear to change: the random order is the same between epochs. Also it appears that 0 provides a deterministic order similar to other values.", "@rob-rowe mea culpa, as it turns out the argument only controls the op-level seed and the graph-level seed will still end up being non-zero. I have updated my original answer.", "@malmaud , you can mention seed inside shuffle to produce shuffled data between epochs. Please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/7285e92602be3455e8f60ca104c2b0f6/26179.ipynb) with the working code in Tensorflow 2.5.", "Closing this issue since it is fixed, feel free to re-open the issue for any concern.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26179\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26179\">No</a>\n"]}, {"number": 26178, "title": "Output_dir inconsitancy between \"model_to_estimator\" and \"export_savedmodel\"", "body": "**Problem explanation**\r\nSince I want to train my neural networks into the Google ML Cloud, I am trying to convert a Keras model to a TensorFlow (TF) estimator. \r\n\r\nA tutorial explaining how to do this can be found on [Training Keras with GPUs & Serving Predictions with Cloud ML Engine (Google Cloud AI Huddle)](https://www.youtube.com/watch?v=4pC97HRhK9E). The Jupyter notebook that accompanies this tutorial can be found on [kaggle.com](https://www.kaggle.com/yufengg/emnist-gpu-keras-to-tf). \r\n\r\nUnfortunately, while following this tutorial, I ran into some problems. \r\n\r\nWhen trying to convert a Keras model into a TF estimator (using the _model_to_estimator_ function while supplying the _output_dir = <USER_DIR>_ argument) and following saving this estimator (using the _export_savedmodel_ module_ function), I received the following error:\r\n\r\n`ValueError: Couldn't find trained model at ./estimator_model.`\r\n\r\nA solution to overcome this problem has been given on the following [stackoverflow post](https://stackoverflow.com/questions/54615708/exporting-a-keras-model-as-a-tf-estimator-trained-model-cannot-be-found). However I am reporting the issue here in case it was not yet solved.\r\n\r\n**System information**\r\n- Tested on Kaggle kernel and Windows 10 Pro\r\n- TF installed from source: v1.12.0\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: 9.0\r\n- GPU model and memory:  NVidia K80 GPUs, 13 GB RAM\r\n\r\n**Describe the current behaviour**\r\nCurrently because _model_to_estimator_ module saves the trained model in a \"Keras\" subfolder under the user specified _output_dir = '<USERDIR>'_ while the _export_savedmodel_ module uses the _model.output_dir_ parameter and thus the user specified parent folder to look for the trained model I get the following error:\r\n\r\n`ValueError: Couldn't find trained model at ./estimator_model.`\r\n\r\n**Describe the expected behaviour**\r\nI expected the _export_savedmodel_ module to successfully find the trained model in the _<USERDIR>_ instead of the _<USERDIR>/keras_ folder and save the TF model.\r\n\r\n**Current workaround**\r\nCurrently, I need to move the model files from the _<USERDIR>/keras_ folder to the _<USERDIR>_ to successfully save the model.\r\n\r\n**Code to reproduce the issue**\r\nThe issue can be reproduced by using the code provided by \r\n\r\nCreate a Keras model and save it:\r\n\r\n```\r\nimport keras\r\nmodel = keras.Sequential()\r\nmodel.add(keras.layers.Dense(units=1,\r\n                                activation='sigmoid',\r\n                                input_shape=(10, )))\r\nmodel.compile(loss='binary_crossentropy', optimizer='sgd')\r\nmodel.save('./model.h5')\r\n```\r\n\r\nNext, convert the model to an estimator with tf.keras.estimator.model_to_estimator, add an input receiver function and export it in the Savedmodel format with estimator.export_savedmodel:\r\n\r\n```\r\n# Convert keras model to TF estimator\r\ntf_files_path = './tf'\r\nestimator =\\\r\n    tf.keras.estimator.model_to_estimator(keras_model=model,\r\n                                          model_dir=tf_files_path)\r\ndef serving_input_receiver_fn():\r\n    return tf.estimator.export.build_raw_serving_input_receiver_fn(\r\n        {model.input_names[0]: tf.placeholder(tf.float32, shape=[None, 10])})\r\n\r\n# Export the estimator\r\nexport_path = './export'\r\nestimator.export_savedmodel(\r\n    export_path,\r\n    serving_input_receiver_fn=serving_input_receiver_fn())\r\n\r\n```\r\n\r\n**Error Log**\r\n```\r\n\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-25-fa76b1124e44> in <module>()\r\n----> 1 export_path = estimator_model.export_savedmodel('./export', serving_input_receiver_fn=serving_input_receiver_fn)\r\n      2 export_path\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py in export_savedmodel(self, export_dir_base, serving_input_receiver_fn, assets_extra, as_text, checkpoint_path, strip_default_attrs)\r\n    661         checkpoint_path=checkpoint_path,\r\n    662         strip_default_attrs=strip_default_attrs,\r\n--> 663         mode=model_fn_lib.ModeKeys.PREDICT)\r\n    664 \r\n    665   def export_saved_model(\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py in _export_saved_model_for_mode(self, export_dir_base, input_receiver_fn, assets_extra, as_text, checkpoint_path, strip_default_attrs, mode)\r\n    787         as_text=as_text,\r\n    788         checkpoint_path=checkpoint_path,\r\n--> 789         strip_default_attrs=strip_default_attrs)\r\n    790 \r\n    791   def _export_all_saved_models(\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py in _export_all_saved_models(self, export_dir_base, input_receiver_fn_map, assets_extra, as_text, checkpoint_path, strip_default_attrs)\r\n    876             self._model_dir)\r\n    877       if not checkpoint_path:\r\n--> 878         raise ValueError(\"Couldn't find trained model at %s.\" % self._model_dir)\r\n    879 \r\n    880       export_dir = export_helpers.get_timestamped_export_dir(export_dir_base)\r\n\r\nValueError: Couldn't find trained model at ./estimator_model.\r\n```", "comments": ["@rickstaa Is it possible for you to test whether this bug persists with latest version of TF? Thanks!", "@jvishnuvardhan Thanks for your response. I tested it in the newest TF build and the error still seems to be there.\r\n\r\n**OUTPUT LOG**\r\n```\r\n(base) [15:13:50] ricks@ricks-HP-ZBook:~/OneDrive/Education/Other/Deep Learning/Other/Keras2tf $ source /home/ricks/miniconda3/bin/activate(base) [15:13:50] ricks@ricks-HP-ZBook:~/OneDrive/Education/Other/Deep Learning/Other/Keras2tf $ conda activate datascience_tf_new\r\n(datascience_tf_new) [15:13:51] ricks@ricks-HP-ZBook:~/OneDrive/Education/Other/Deep Learning/Other/Keras2tf $ /home/ricks/miniconda3/envs/datascience_tf_new/bin/python \"/home/ricks/OneDrive/Education/Other/Deep Learning/Other/Keras2tf/keras_cloud_conversion.py\"\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0301 15:14:05.699281 140146440931136 deprecation.py:506] From /home/ricks/miniconda3/envs/datascience_tf_new/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nModel: \"sequential\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nconv2d (Conv2D)              (None, 12, 12, 12)        312       \r\n_________________________________________________________________\r\ndropout (Dropout)            (None, 12, 12, 12)        0         \r\n_________________________________________________________________\r\nconv2d_1 (Conv2D)            (None, 5, 5, 18)          1962      \r\n_________________________________________________________________\r\ndropout_1 (Dropout)          (None, 5, 5, 18)          0         \r\n_________________________________________________________________\r\nconv2d_2 (Conv2D)            (None, 4, 4, 24)          1752      \r\n_________________________________________________________________\r\nflatten (Flatten)            (None, 384)               0         _________________________________________________________________\r\ndense (Dense)                (None, 150)               57750     \r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 47)                7097      \r\n=================================================================\r\nTotal params: 68,873\r\nTrainable params: 68,873\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n[None, 12, 12, 12]\r\n[None, 12, 12, 12]\r\n[None, 5, 5, 18]\r\n[None, 5, 5, 18]\r\n[None, 4, 4, 24]\r\n[None, 384]\r\n[None, 150]\r\n[None, 47]\r\n(112800, 28, 28, 1)\r\n2019-03-01 15:14:14.039015: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-03-01 15:14:14.044111: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\r\n2019-03-01 15:14:14.128575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1009] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-03-01 15:14:14.185376: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x5584828ecca0 executing computations on platform CUDA. Devices:\r\n2019-03-01 15:14:14.185400: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): Quadro M1000M, Compute Capability 5.0\r\n2019-03-01 15:14:14.206354: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2592000000 Hz\r\n2019-03-01 15:14:14.206920: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x558457b11fa0 executing computations on platform Host. Devices:\r\n2019-03-01 15:14:14.206975: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-03-01 15:14:14.207346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1467] Found device 0 with properties: \r\nname: Quadro M1000M major: 5 minor: 0 memoryClockRate(GHz): 1.0715\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 3,95GiB freeMemory: 3,39GiB\r\n2019-03-01 15:14:14.207361: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1546] Adding visible gpu devices: 0\r\n2019-03-01 15:14:14.207437: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2019-03-01 15:14:14.208699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1015] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-03-01 15:14:14.208712: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1021]      0 \r\n2019-03-01 15:14:14.208718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] 0:   N \r\n2019-03-01 15:14:14.209027: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1149] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3168 MB memory) -> physical GPU (device: 0, name: Quadro M1000M, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\n^C      \r\n(datascience_tf_new) [15:15:31] ricks@ricks-HP-ZBook:~/OneDrive/Education/Other/Deep Learning/Other/Keras2tf $ /home/ricks/miniconda3/envs/datascience_tf_new/bin/python \"/home/ricks/OneDrive/Education/Other/Deep Learning/Other/Keras2tf/keras_cloud_conversion.py\"\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0301 15:24:53.037093 140332255565632 deprecation.py:506] From /home/ricks/miniconda3/envs/datascience_tf_new/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nModel: \"sequential\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nconv2d (Conv2D)              (None, 12, 12, 12)        312       \r\n_________________________________________________________________\r\ndropout (Dropout)            (None, 12, 12, 12)        0         \r\n_________________________________________________________________\r\nconv2d_1 (Conv2D)            (None, 5, 5, 18)          1962      \r\n_________________________________________________________________\r\ndropout_1 (Dropout)          (None, 5, 5, 18)          0         \r\n_________________________________________________________________\r\nconv2d_2 (Conv2D)            (None, 4, 4, 24)          1752      \r\n_________________________________________________________________\r\nflatten (Flatten)            (None, 384)               0         \r\n_________________________________________________________________\r\ndense (Dense)                (None, 150)               57750     \r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 47)                7097      \r\n=================================================================\r\nTotal params: 68,873\r\nTrainable params: 68,873\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n[None, 12, 12, 12]\r\n[None, 12, 12, 12]\r\n[None, 5, 5, 18]\r\n[None, 5, 5, 18]\r\n[None, 4, 4, 24]\r\n[None, 384]\r\n[None, 150]\r\n[None, 47]\r\n(112800, 28, 28, 1)\r\n2019-03-01 15:25:03.184462: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-03-01 15:25:03.192966: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\r\n2019-03-01 15:25:03.260354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1009] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-03-01 15:25:03.261681: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x560aedec8ff0 executing computations on platform CUDA. Devices:\r\n2019-03-01 15:25:03.261702: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): Quadro M1000M, Compute Capability 5.0\r\n2019-03-01 15:25:03.282273: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2592000000 Hz\r\n2019-03-01 15:25:03.282672: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x560aedec5dc0 executing computations on platform Host. Devices:\r\n2019-03-01 15:25:03.282691: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-03-01 15:25:03.287303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1467] Found device 0 with properties: \r\nname: Quadro M1000M major: 5 minor: 0 memoryClockRate(GHz): 1.0715\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 3,95GiB freeMemory: 3,39GiB\r\n2019-03-01 15:25:03.287336: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1546] Adding visible gpu devices: 0\r\n2019-03-01 15:25:03.287445: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2019-03-01 15:25:03.288709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1015] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-03-01 15:25:03.288739: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1021]      0 \r\n2019-03-01 15:25:03.288747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] 0:   N \r\n2019-03-01 15:25:03.288993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1149] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3173 MB memory) -> physical GPU (device: 0, name: Quadro M1000M, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\nEpoch 1/5\r\n2019-03-01 15:25:04.878610: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n2019-03-01 15:25:04.995056: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n500/500 [==============================] - 5s 11ms/step - loss: 2.5097 - acc: 0.3254 - val_loss: 1.4320 - val_acc: 0.6009\r\nEpoch 2/5\r\n500/500 [==============================] - 4s 9ms/step - loss: 1.4519 - acc: 0.5789 - val_loss: 1.0212 - val_acc: 0.6947\r\nEpoch 3/5\r\n500/500 [==============================] - 5s 9ms/step - loss: 1.1432 - acc: 0.6476 - val_loss: 0.8468 - val_acc: 0.7394\r\nEpoch 4/5\r\n500/500 [==============================] - 5s 9ms/step - loss: 1.0161 - acc: 0.6908 - val_loss: 0.7605 - val_acc: 0.7589\r\nEpoch 5/5\r\n500/500 [==============================] - 4s 8ms/step - loss: 0.9246 - acc: 0.7104 - val_loss: 0.6986 - val_acc: 0.7811\r\nPrediction:  9 , Char:  9\r\nLabel:  16\r\nPrediction:  14 , Char:  E\r\nLabel:  14\r\nPrediction:  26 , Char:  Q\r\nLabel:  26\r\nPrediction:  21 , Char:  L\r\nLabel:  12\r\nPrediction:  37 , Char:  b\r\nLabel:  37\r\nPrediction:  9 , Char:  9\r\nLabel:  16\r\nPrediction:  9 , Char:  9\r\nLabel:  16\r\nPrediction:  2 , Char:  2\r\nLabel:  2\r\nPrediction:  17 , Char:  H\r\nLabel:  20\r\nPrediction:  26 , Char:  Q\r\nLabel:  26\r\nconv2d_input\r\nTraceback (most recent call last):\r\n  File \"/home/ricks/OneDrive/Education/Other/Deep Learning/Other/Keras2tf/keras_cloud_conversion.py\", line 190, in <module>\r\n    export_path = estimator_model.export_savedmodel('./export', serving_input_receiver_fn=serving_input_receiver_fn)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1645, in export_savedmodel\r\n    experimental_mode=model_fn_lib.ModeKeys.PREDICT)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 723, in export_saved_model\r\n    checkpoint_path=checkpoint_path)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 801, in experimental_export_all_saved_models\r\n    raise ValueError(\"Couldn't find trained model at %s.\" % self._model_dir)\r\nValueError: Couldn't find trained model at ./model_estimator.\r\n(datascience_tf_new) [15:25:41] ricks@ricks-HP-ZBook:~/OneDrive/Education/Other/Deep Learning/Other/Keras2tf $ python --versionPython 3.6.8 :: Anaconda, Inc.(datascience_tf_new) [15:26:08] ricks@ricks-HP-ZBook:~/OneDrive/Education/Other/Deep Learning/Other/Keras2tf $ pip list | grep tensorflow\r\ntensorflow           1.13.1              \r\ntensorflow-estimator 1.13.0              \r\ntensorflow-gpu       1.13.1              \r\n```", "@rickstaa Thanks for checking the bug with latest version of TF. Also, thanks for the workaround you provided in stackoverflow. Thanks!", "Summarizing what the stackoverflow content covers:\r\nIt looks like what's happening here is that `model_to_estimator` is writing the model to a sub-folder called keras. So when you choose \"./model_estimator\" as your model folder, it actually puts the files in `\"./model_estimator/keras\"`. But it's not reflecting that in the model_dir attribute, which remains as `\"./model_estimator\"`. \r\n\r\nNot sure what the right thing to do here is -- the nested \"keras\" folder behavior was not present in the past, so it appears that this is desired behavior. The move in that case would be ensure that the model_dir value is appropriately populated.\r\n\r\nThe workaround I've been (shamefully) using is setting the underlying private variable directly, rather than moving the files around: \r\n`estimator_model._model_dir = './model_estimator/keras'`\r\nNote that this approach also technically answers #25772 , though it's not even close to a \"good\" approach...", "it appears the 'keras' subdirectory is being added [here](https://github.com/tensorflow/estimator/blob/master/tensorflow_estimator/python/estimator/keras.py#L313), during \"warm start\". I'm not quite sure of its purpose, but I suspect the intent here was originally for the conversions to be for \"untrained\" keras models, and for training of the converted estimators to be done using `estimator_model.train(...)` , which would yield checkpoint files in the appropriate top-level folder. \r\n\r\nHowever, the use case outlined here is such that folks are converting and then immediately exporting. In this case, nothing shows up in the top-level folder, and we're forced to resort to somewhat absurdist means of rectifying the situation.", "> Summarizing what the stackoverflow content covers:\r\n> It looks like what's happening here is that `model_to_estimator` is writing the model to a sub-folder called keras. So when you choose \"./model_estimator\" as your model folder, it actually puts the files in `\"./model_estimator/keras\"`. But it's not reflecting that in the model_dir attribute, which remains as `\"./model_estimator\"`.\r\n> \r\n> Not sure what the right thing to do here is -- the nested \"keras\" folder behavior was not present in the past, so it appears that this is desired behavior. The move in that case would be ensure that the model_dir value is appropriately populated.\r\n> \r\n> The workaround I've been (shamefully) using is setting the underlying private variable directly, rather than moving the files around:\r\n> `estimator_model._model_dir = './model_estimator/keras'`\r\n> Note that this approach also technically answers #25772 , though it's not even close to a \"good\" approach...\r\n\r\nThanks for the monkey patch suggestion. Ugly but saving me a lot of trouble.\r\nThis issue is most definitely needed to be solved. It is a crucial step in the process of serving a keras model using tf.serve for example", "If you train the estimator at all, it should create the appropriate model_dir, if I understand correctly, and therefore save out. If you are not training the estimator at all, is the purpose of conversion merely to save the TF SavedModel? If so, I would recommend exporting the savedmodel directly from Keras: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/experimental/export_saved_model\r\n\r\nNote that we are also working on making Keras models save to TF SavedModel by default; see the RFC at: https://github.com/tensorflow/community/pull/102\r\n\r\nIf I am misinterpreting, and there is some reason that you specifically need to pass through Estimator without training the estimator, let me know.", "Agreed that this method _would_ work; this issue was filed before r2.0 existed, so at the time using the old tf calls was the only way to export an _already trained_ Keras model. \r\nSince the new `export_saved_model` is only available in TF 2.0's experimental, it will not be accessible in many cases (non-1.xx users, non-experimental code teams/companies). Question: in the RFC, I notice that `export_saved_model` is not mentioned. It appears that the use case is covered by `model.save()`, is my understanding accurate?\r\n\r\nIn terms of reasons for conversion Keras to Estimator and immediately exporting, rather than using `tf.train()`, I believe the primary reason is due to the simpler nature (or legacy code) of the `keras.fit()` api. For users who are familiar with Keras, it is much more natural to train using Keras APIs since that's what all their other calls have been using, rather than suddenly switch over to using TF APIs for just the training portion of things.\r\n\r\nAdditionally, if one were to convert a `trained` keras model into a tf model, this is how they would do it, as there's not guarantee that there is necessarily any additional training (data) that is needed to be passed through the model. To that end, a utility for coverting a keras model to a TF model may be useful (several community-made examples exist currently). Though this may be obviated by the move to the TF SavedModel for Keras models (though I see that in the RFC, h5 format continues to be supported for backward compatibility reasons, suggesting that the legacy of this format will cause many to continue using it for some time during the migration period)", "To clarify, the code is available now as experimental, and will remain so for 1.x. So if you are in 1.x AND don't use experimental, you will have to stick with a workaround, but 2.x users or 1.x+experimental-okay users can save directly. WRT the RFC, the experimental export_saved_model functionality will be moved into model.save in 2.0, such that there is no need for the separate method/experimental label.\r\n\r\nWRT to not training with Estimator, you are right-- the community has spoken, and it prefers Keras. Hopefully, with the direct option in model.save in 2.0 (and the experimental API for 1.x), you will not need to pass through estimator at all.\r\n\r\nA utility for moving from h5 to TF SavedModel-- you should be able to load in the h5 to tf.keras, and then export directly as TF SavedModel. eg:\r\n\r\n```\r\nmodel = tf.keras.models.load_model('path_to_model.h5')\r\n\r\n# In 1.x\r\ntf.keras.experimental.export_saved_model(model, 'path/to/saved/model')\r\n\r\n# After the RFC is accepted/implemented in 2.x\r\nmodel.save('path/to/saved/model')\r\n\r\n```\r\n\r\nThere are various args around compilation and optimizers that you may want to explore, but I believe that should just work. Is that what you had in mind?", "And thanks for clarifying that the 'experimental' module is available in 1.x as well.\r\n\r\nRe: your example, this is pretty much what I imagined it would look like, given the new APIs available. \r\n\r\nThanks for writing this up!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26178\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26178\">No</a>\n", "> If you train the estimator at all, it should create the appropriate model_dir, if I understand correctly, and therefore save out. If you are not training the estimator at all, is the purpose of conversion merely to save the TF SavedModel? If so, I would recommend exporting the savedmodel directly from Keras: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/experimental/export_saved_model\r\n> \r\n> Note that we are also working on making Keras models save to TF SavedModel by default; see the RFC at: [tensorflow/community#102](https://github.com/tensorflow/community/pull/102)\r\n> \r\n> If I am misinterpreting, and there is some reason that you specifically need to pass through Estimator without training the estimator, let me know.\r\n\r\nThe reason to pass a keras model through an Estimator without training is the .export_saved_model() of the estimator:\r\nhttps://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#export_saved_model\r\nThis is the only production-ready way to merge a preprocessing transformation graph (using the serving_input_receiver_fn) into model graph inside the estimator. (in order to be used as a servable under tf.serving production server) This is a must to any keras production code that have any meaningful runtime preprocessing (all of them)\r\nif you know any other way let me know. \r\n", "This is coming soon for keras: https://github.com/tensorflow/community/pull/102/files#diff-79c7f63112904922da09631e4a9a902bR96 , and is already available using the lower-level tf.saved_model API: https://github.com/tensorflow/community/blob/master/rfcs/20181116-saved-model.md#specifying-signatures (You would just do what is done to the Net tf.Module there to your Keras Model, and it should work as expected for inference).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26178\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26178\">No</a>\n"]}, {"number": 26177, "title": "TensorFlow master build failing on Ubuntu18.04 x86", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: master as on today \r\n- Python version:  2.7.x\r\n- Bazel version (if compiling from source): 0.19.0 ( installed using bazel-0.19.0-installer-linux-x86_64.sh)\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n\r\n\r\n**Describe the problem**\r\nTensorflow build fails with an error:\r\n\r\nIn file included from external/hwloc/include/private/private.h:29:0,\r\n                 from external/hwloc/hwloc/traversal.c:11:\r\nexternal/hwloc/include/private/misc.h:491:10: fatal error: xlocale.h: No such file or directory\r\n #include \"xlocale.h\"\r\n          ^~~~~~~~~~~\r\ncompilation terminated.\r\n\r\n**Steps to reproduce:**\r\n git clone https://github.com/tensorflow/tensorflow/\r\ncd tensorflow\r\n./configure\r\n\r\n./configure\r\nExtracting Bazel installation...\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.19.0 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python]:\r\n\r\n\r\nFound possible Python library paths:\r\n  /usr/local/lib/python2.7/dist-packages\r\n  /usr/lib/python2.7/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]: N\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: N\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: N\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: N\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to download a fresh release of clang? (Experimental) [y/N]: N\r\nClang will not be downloaded.\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: N\r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare\r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=gdr            # Build with GDR support.\r\n        --config=verbs          # Build with libverbs support.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=noignite       # Disable Apache Ignite support.\r\n        --config=nokafka        # Disable Apache Kafka support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n\r\n bazel --host_jvm_args=\"-Xms512m\" --host_jvm_args=\"-Xmx1024m\"  build  -c opt  //tensorflow/toockage:build_pip_package \r\n\r\n", "comments": ["Looks like glibc version is upgraded on Ubuntu 18.04\r\nldd --version\r\nldd (Ubuntu GLIBC 2.27-3ubuntu1) 2.27\r\n\r\nAs mentioned [here](https://sourceware.org/ml/libc-alpha/2017-08/msg00010.html , https://sourceware.org/glibc/wiki/Release/2.26#Removal_of_.27xlocale.h.27) ,  The nonstandard header <xlocale.h> has been removed.\r\n\r\nThis comes through third_party code hwloc (https://github.com/tensorflow/tensorflow/tree/master/third_party/hwloc)\r\n\r\nAny workaround for this? I am not sure about functionality impact  if we disable including xlocale.h.", "Or should downgrade gcc ( build from source or installing particular version) ?", "A hack I've seen elsewhere is:\r\n\r\n`ln -s /usr/include/locale.h /usr/include/xlocale.h`\r\n\r\nReally the build should be using locale.h and not xlocale.h", "@wdirons ya..This hack worked. \r\n\r\n I was also looking for gcc version which might mentioned by TensorFlow community. \r\n\r\n\r\n", "Btw looks like community has made changes around xlocale.h through this [commit](https://github.com/tensorflow/tensorflow/commit/d624b1d75a640c50be281d8b921d62dbd6149f2a).\r\nWill try building TensorFlow master again . ", "@Nayana-ibm Is this still an issue? Did you get a chance to build TF again? Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "I think we can also work around the problem by unsetting `HAVE_XLOCALE_H`\r\nhowever, I cannot even find where we are setting it in the first place :/", "Looks like https://github.com/tensorflow/tensorflow/pull/26156 fixed this."]}, {"number": 26176, "title": "[Tensorflow 2]: Bring back tf.layers?", "body": "In the proposed TensorFlow 2.0 API, `tf.layers` has been removed in favor of `tf.keras.layers`. Would it be possible to bring it back as a simple alias for `tf.keras.layers`? \r\n\r\nFor reference, we already have:\r\n- `tf.losses` == `tf.keras.losses`\r\n- `tf.optimizers` == `tf.keras.optimizers`\r\n\r\nas aliases. I personally think that `tf.keras.layers` are as-used as optimizers and loss functions, and that `tf.layers` would be a small ergonomic improvement over `tf.keras.layers`.", "comments": ["Hi @alanhdu, \r\n\r\nI think this is the plan. It's just not quite implemented yet.\r\n\r\nCheck again in a couple of weeks.", "I am closing this as it was resolved. Thanks!", "Hi @MarkDaoust! I've been playing around with the latest TF2 builds, and I noticed that `tf.layers` has not been exposed as an alias. Is there a tracking issue or something I could subscribe to know when the change happens if it's still planned?"]}, {"number": 26175, "title": "MultiboxTracker making detections in groups of 9 - 13", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 18.04.1 LTS\r\n- TensorFlow version: 1.12.0\r\n- Python version: 2.7.15\r\n\r\n**Describe the current behavior**\r\nIt seems that the drawDebug method in the MultiBoxTracker.java file is making more detentions than it should be. \r\n\r\nI modified the drawDebug method such that it returns an ArayList of confidence values for each detection as well as booleans which are a check as to whether the detection is correct. Here is the code for that: \r\n```\r\n\r\npublic synchronized ArrayList<Tuple> drawDebug(final Canvas canvas, final String groundTruthClass) {\r\n    Log.d(\"myInfoTag\",\"IN DRAW DEBUG\");\r\n    final Paint boxPaint = new Paint();\r\n    ArrayList<Tuple> conVals = new ArrayList<Tuple>();\r\n    boxPaint.setAlpha(10);\r\n    boxPaint.setStyle(Style.STROKE);\r\n    boxPaint.setStrokeWidth(DETECTION_BOX_LINE_THICKNESS);\r\n    boxPaint.setPathEffect(new DashPathEffect(new float[] {DASH_LENGTH, DASH_GAP}, 0));\r\n\r\n    for (final Triplet<String, Float, RectF> detection : screenRects) {\r\n        final RectF rect = detection.getThird();\r\n        final String className = detection.getFirst();\r\n        final Float confidence = detection.getSecond();\r\n        boolean correct = false;\r\n\r\n        if (confidence >= 0.5) {\r\n            if (className.equals(groundTruthClass)) { // put parameter\r\n                correct = true;\r\n            }\r\n            Tuple conPair = new Tuple<Float, Boolean>(confidence, correct);\r\n            if (!(conVals.contains(conPair))) {\r\n                conVals.add(conPair);\r\n            }\r\n        }\r\n\r\n        final String labelString =\r\n              !TextUtils.isEmpty(className)\r\n                      ? String.format(\"%s %.0f%%\", className, confidence*100)\r\n                      : String.format(\"%.0f%%\", confidence);\r\n        if (className.equals(CLASS_1_NAME)) {\r\n            DISPLAY_THRESHOLD = CLASS_1_THRESHOLD;\r\n            boxPaint.setColor(CLASS_1_COLOR);\r\n        } else if (className.equals(CLASS_2_NAME)) {\r\n            DISPLAY_THRESHOLD = CLASS_2_THRESHOLD;\r\n            boxPaint.setColor(CLASS_2_COLOR);\r\n        } else if (className.equals(CLASS_3_NAME)) {\r\n            DISPLAY_THRESHOLD = CLASS_3_THRESHOLD;\r\n            boxPaint.setColor(CLASS_3_COLOR);\r\n        } else if (className.equals(CLASS_4_NAME)) {\r\n            DISPLAY_THRESHOLD = CLASS_4_THRESHOLD;\r\n            boxPaint.setColor(CLASS_4_COLOR);\r\n        } else if (className.equals(CLASS_5_NAME)) {\r\n            DISPLAY_THRESHOLD = CLASS_5_THRESHOLD;\r\n            boxPaint.setColor(CLASS_5_COLOR);\r\n        } else if (className.equals(CLASS_6_NAME)) {\r\n            DISPLAY_THRESHOLD = CLASS_6_THRESHOLD;\r\n            boxPaint.setColor(CLASS_6_COLOR);\r\n        } else if (className.equals(CLASS_7_NAME)) {\r\n            DISPLAY_THRESHOLD = CLASS_7_THRESHOLD;\r\n            boxPaint.setColor(CLASS_7_COLOR);\r\n        }\r\n        if (confidence>= DETECTION_THRESHOLD && confidence < DISPLAY_THRESHOLD) {\r\n            canvas.drawRect(rect, boxPaint);\r\n            borderedText.drawText(canvas, rect.left, rect.top, labelString);\r\n        }\r\n\r\n    }\r\n\r\n    if (objectTracker == null) {\r\n      return conVals;\r\n    }\r\n\r\n```\r\nIn DetectorActivity.java, I have all of the values of conVals printed to the screen. However, the same detection prints to the screen anywhere between 9 and 13 times. It seems that there might be an issue with threading, or the drawDebug method is making far too many detections.\r\n\r\nHere's the DetectorActivity.java code that takes all the confidences in the arrayList returned by drawDebug, groups them into confidence intervals, and prints them to the screen. I don't believe this is where the issue is, but I've included it just in case. You'll notice that I'm simply just making a count of how many predictions fall in certain confidence ranges and printing those to the screen. There is also a for loop that's denoted by the comment \"look here\" which is where the detections are individually printed to the screen as the app makes detections. This is how I noticed that the detections were being made in groups of 9 - 13, as opposed to being just single detections. \r\n\r\n    trackingOverlay.addCallback(\r\n        new DrawCallback() {\r\n          @Override\r\n          public void drawCallback(final Canvas canvas) {\r\n            tracker.draw(canvas);\r\n            if (isDebug()){\r\n              ArrayList<Tuple> conPairs = tracker.drawDebug(canvas, groundTruthClass); // need to add ground truth label to parameter list\r\n              if (record) {\r\n                for (Tuple<Float, Boolean> conPair : conPairs) {\r\n                  Float conVal = conPair.getFirst();\r\n                  Boolean conCorrect = conPair.getSecond();\r\n                  if (conVal != 0) {\r\n                    if (calArray.size() < 2) {\r\n                      calArray.add(conVal);\r\n                      if ((conVal > 0.0) && (conVal <= 0.1)) {\r\n                        interval0 += 1;\r\n                        cinterval0 += (conCorrect == true) ? 1 : 0;\r\n                      } else if ((conVal > 0.1) && (conVal <= 0.2)) {\r\n                        interval1 += 1;\r\n                        cinterval1 += (conCorrect == true) ? 1 : 0;\r\n                      } else if ((conVal > 0.2) && (conVal <= 0.3)) {\r\n                        interval2 += 1;\r\n                        cinterval2 += (conCorrect == true) ? 1 : 0;\r\n                      } else if ((conVal > 0.3) && (conVal <= 0.4)) {\r\n                        interval3 += 1;\r\n                        cinterval3 += (conCorrect == true) ? 1 : 0;\r\n                      } else if ((conVal > 0.4) && (conVal <= 0.5)) {\r\n                        interval4 += 1;\r\n                        cinterval4 += (conCorrect == true) ? 1 : 0;\r\n                      } else if ((conVal > 0.5) && (conVal <= 0.6)) {\r\n                        interval5 += 1;\r\n                        cinterval5 += (conCorrect == true) ? 1 : 0;\r\n                      } else if ((conVal > 0.6) && (conVal <= 0.7)) {\r\n                        interval6 += 1;\r\n                        cinterval6 += (conCorrect == true) ? 1 : 0;\r\n                      } else if ((conVal > 0.7) && (conVal <= 0.8)) {\r\n                        interval7 += 1;\r\n                        cinterval7 += (conCorrect == true) ? 1 : 0;\r\n                      } else if ((conVal > 0.8) && (conVal <= 0.9)) {\r\n                        interval8 += 1;\r\n                        cinterval8 += (conCorrect == true) ? 1 : 0;\r\n                      } else if ((conVal > 0.9) && (conVal <= 1.0)) {\r\n                        interval9 += 1;\r\n                        cinterval9 += (conCorrect == true) ? 1 : 0;\r\n                      }\r\n                      // } else if (!(((calArray.get(calArray.size() - 1)).equals(conVal))) && !((calArray.get(calArray.size() - 2)).equals(conVal))) {\r\n                    } else {\r\n                      calArray.add(conVal);\r\n                      if ((conVal > 0.0) && (conVal <= 0.1)) {\r\n                        interval0 += 1;\r\n                        cinterval0 += (conCorrect == true) ? 1 : 0;\r\n                      } else if ((conVal > 0.1) && (conVal <= 0.2)) {\r\n                        interval1 += 1;\r\n                        cinterval1 += (conCorrect == true) ? 1 : 0;\r\n                      } else if ((conVal > 0.2) && (conVal <= 0.3)) {\r\n                        interval2 += 1;\r\n                        cinterval2 += (conCorrect == true) ? 1 : 0;\r\n                      } else if ((conVal > 0.3) && (conVal <= 0.4)) {\r\n                        interval3 += 1;\r\n                        cinterval3 += (conCorrect == true) ? 1 : 0;\r\n                      } else if ((conVal > 0.4) && (conVal <= 0.5)) {\r\n                        interval4 += 1;\r\n                        cinterval4 += (conCorrect == true) ? 1 : 0;\r\n                      } else if ((conVal > 0.5) && (conVal <= 0.6)) {\r\n                        interval5 += 1;\r\n                        cinterval5 += (conCorrect == true) ? 1 : 0;\r\n                      } else if ((conVal > 0.6) && (conVal <= 0.7)) {\r\n                        interval6 += 1;\r\n                        cinterval6 += (conCorrect == true) ? 1 : 0;\r\n                      } else if ((conVal > 0.7) && (conVal <= 0.8)) {\r\n                        interval7 += 1;\r\n                        cinterval7 += (conCorrect == true) ? 1 : 0;\r\n                      } else if ((conVal > 0.8) && (conVal <= 0.9)) {\r\n                        interval8 += 1;\r\n                        cinterval8 += (conCorrect == true) ? 1 : 0;\r\n                      } else if ((conVal > 0.9) && (conVal <= 1.0)) {\r\n                        interval9 += 1;\r\n                        cinterval9 += (conCorrect == true) ? 1 : 0;\r\n                      }\r\n                    }\r\n\r\n                    per0 = (float) cinterval0/interval0;\r\n                    per1 = (float) cinterval1/interval1;\r\n                    per2 = (float) cinterval2/interval2;\r\n                    per3 = (float) cinterval3/interval3;\r\n                    per4 = (float) cinterval4/interval4;\r\n                    per5 = (float) cinterval5/interval5;\r\n                    per6 = (float) cinterval6/interval6;\r\n                    per7 = (float) cinterval7/interval7;\r\n                    per8 = (float) cinterval8/interval8;\r\n                    per9 = (float) cinterval9/interval9;\r\n\r\n                  }\r\n                }\r\n              }\r\n            }\r\n          }\r\n        });\r\n\r\n    addCallback(\r\n        new DrawCallback() {\r\n          @Override\r\n          public void drawCallback(final Canvas canvas) {\r\n            if (!isDebug()) {\r\n              return;\r\n            }\r\n            final Bitmap copy = cropCopyBitmap;\r\n            if (copy == null) {\r\n              return;\r\n            }\r\n            // Color 'tint' for volume down stats overlay\r\n            final int backgroundColor = Color.argb(0, 0, 0, 0);\r\n            canvas.drawColor(backgroundColor);\r\n\r\n            //final Matrix matrix = new Matrix();\r\n            //final float scaleFactor = 2;\r\n            //matrix.postScale(scaleFactor, scaleFactor);\r\n            //matrix.postTranslate(\r\n            //    canvas.getWidth() - copy.getWidth() * scaleFactor,\r\n            //    canvas.getHeight() - copy.getHeight() * scaleFactor);\r\n            //canvas.drawBitmap(copy, matrix, new Paint());\r\n\r\n            final Vector<String> lines = new Vector<String>();\r\n            lines.add(\"\");\r\n\r\n            if (record) {\r\n              lines.add(\"RECORDING ON\");\r\n            } else {\r\n              lines.add(\"RECORDING OFF\");\r\n            }\r\n\r\n            for (Float confidence: calArray) {\r\n              lines.add(String.valueOf(confidence));\r\n            }\r\n\r\n            lines.add(\"0 to 10: \" + String.valueOf(interval0) + \" \" + String.valueOf(cinterval0) + \" \" + Float.toString(per0));\r\n            lines.add(\"10 to 20: \" + String.valueOf(interval1) + \" \" + String.valueOf(cinterval1) + \" \" + Float.toString(per1));\r\n            lines.add(\"20 to 30: \" + String.valueOf(interval2) + \" \" + String.valueOf(cinterval2) + \" \" + Float.toString(per2));\r\n            lines.add(\"30 to 40: \" + String.valueOf(interval3) + \" \" + String.valueOf(cinterval3) + \" \" + Float.toString(per3));\r\n            lines.add(\"40 to 50: \" + String.valueOf(interval4) + \" \" + String.valueOf(cinterval4) + \" \" + Float.toString(per4));\r\n            lines.add(\"50 to 60: \" + String.valueOf(interval5) + \" \" + String.valueOf(cinterval5) + \" \" + Float.toString(per5));\r\n            lines.add(\"60 to 70: \" + String.valueOf(interval6) + \" \" + String.valueOf(cinterval6) + \" \" + Float.toString(per6));\r\n            lines.add(\"70 to 80: \" + String.valueOf(interval7) + \" \" + String.valueOf(cinterval7) + \" \" + Float.toString(per7));\r\n            lines.add(\"80 to 90: \" + String.valueOf(interval8) + \" \" + String.valueOf(cinterval8) + \" \" + Float.toString(per8));\r\n            lines.add(\"90 to 100: \" + String.valueOf(interval9) + \" \" + String.valueOf(cinterval9) + \" \" + Float.toString(per9));\r\n\r\n\r\n            borderedText.drawLines(canvas, 10, canvas.getHeight() - 10, lines);\r\n          }\r\n        });", "comments": ["@singhcpt Could you check whether the error persists with the newer version of TF? Thanks!", "There are two MultiBoxTracker.java files, are you using the TensorFlow or TensorFlow Lite Android example? If you share the full path of the file, that might help, thanks!", "> There are two MultiBoxTracker.java files, are you using the TensorFlow or TensorFlow Lite Android example? If you share the full path of the file, that might help, thanks!\r\n\r\nWe're using the TensorFlow example, not TensorFlow Lite - this is the file path: /home/plantvillage/tf_demo/src/org/tensorflow/demo/tracking/MultiBoxTracker.java\r\n", "> @singhcpt Could you check whether the error persists with the newer version of TF? Thanks\r\n\r\nWhere would I check the TF version? Thanks!", "Unfortunately I haven't had a chance to dive deeper into this one, it's been a while, and the mobile version of TensorFlow is deprecated in favor of TensorFlow Lite, so closing as won't fix.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26175\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26175\">No</a>\n"]}, {"number": 26174, "title": "TFLite Mfcc op has inconsistent requirements with standard Mfcc op", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, here's the code to reproduce: https://gist.github.com/reuben/57bee91669a5bd2717c32cf406ca951d\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.14\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.13.0-rc2-5-g6612da8951 1.13.1\r\n- **Python version**: 3.6.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: curl https://gist.githubusercontent.com/reuben/57bee91669a5bd2717c32cf406ca951d/raw/6b81d28d00ff3ec73ab1bcc6a698366bbe3dcb51/test_tflite_mfcc.py | python\r\n\r\n### Describe the problem\r\nThe Mfcc op in tensorflow.contrib.framework.python.ops.audio_ops (a.k.a. contrib_audio) enforces the shape of the sample rate parameter to be rank 0. The TFLite Mfcc op enforces the sample rate parameter to be rank 1. Converting a model with an Mfcc op in it results in a TFLite model that fails in the preparation step.\r\n\r\n### Source code / logs\r\nIf you try to pass a sample rate of rank 1 to the contrib_audio op:\r\n\r\n```\r\npython3 test_tflite.py\r\n2019-02-27 11:50:41.627337: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nTraceback (most recent call last):\r\n  File \"/Users/rmorais/.local/share/virtualenvs/DeepSpeech-HmF6CP0D/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1659, in _create_c_op\r\n    c_op = c_api.TF_FinishOperation(op_desc)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Shape must be rank 0 but is rank 1 for 'Mfcc' (op: 'Mfcc') with input shapes: [1,1,257], [1].\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"test_tflite.py\", line 13, in <module>\r\n    mfccs = contrib_audio.mfcc(spectrogram, [16000], dct_coefficient_count=13)\r\n  File \"/Users/rmorais/.local/share/virtualenvs/DeepSpeech-HmF6CP0D/lib/python3.6/site-packages/tensorflow/python/ops/gen_audio_ops.py\", line 454, in mfcc\r\n    dct_coefficient_count=dct_coefficient_count, name=name)\r\n  File \"/Users/rmorais/.local/share/virtualenvs/DeepSpeech-HmF6CP0D/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/Users/rmorais/.local/share/virtualenvs/DeepSpeech-HmF6CP0D/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/Users/rmorais/.local/share/virtualenvs/DeepSpeech-HmF6CP0D/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\r\n    op_def=op_def)\r\n  File \"/Users/rmorais/.local/share/virtualenvs/DeepSpeech-HmF6CP0D/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1823, in __init__\r\n    control_input_ops)\r\n  File \"/Users/rmorais/.local/share/virtualenvs/DeepSpeech-HmF6CP0D/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1662, in _create_c_op\r\n    raise ValueError(str(e))\r\nValueError: Shape must be rank 0 but is rank 1 for 'Mfcc' (op: 'Mfcc') with input shapes: [1,1,257], [1].\r\n```\r\n\r\nIf you pass a sample rate of rank 0, and then try to convert and use the TFLite model:\r\n\r\n```\r\npython3 test_tflite.py\r\n2019-02-27 11:50:27.082848: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nsession run works\r\ntoco_from_protos /var/folders/k1/7sbxsmm52n59_fpj0v65fmk40000gn/T/tmp_vtx0s5o /var/folders/k1/7sbxsmm52n59_fpj0v65fmk40000gn/T/tmp7v9bbo3t /var/folders/k1/7sbxsmm52n59_fpj0v65fmk40000gn/T/tmpilazyufw /var/folders/k1/7sbxsmm52n59_fpj0v65fmk40000gn/T/tmp9e1513n8\r\nTraceback (most recent call last):\r\n  File \"test_tflite.py\", line 30, in <module>\r\n    interpreter.allocate_tensors()\r\n  File \"/Users/rmorais/.local/share/virtualenvs/DeepSpeech-HmF6CP0D/lib/python3.6/site-packages/tensorflow/lite/python/interpreter.py\", line 73, in allocate_tensors\r\n    return self._interpreter.AllocateTensors()\r\n  File \"/Users/rmorais/.local/share/virtualenvs/DeepSpeech-HmF6CP0D/lib/python3.6/site-packages/tensorflow/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 106, in AllocateTensors\r\n    return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)\r\nRuntimeError: tensorflow/lite/kernels/mfcc.cc:75 NumDimensions(inputRate) != 1 (0 != 1)Node number 2 (Mfcc) failed to prepare.\r\n```\r\n\r\nAnd here's the source of the testing script just in case:\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport sys\r\nimport tempfile\r\n\r\nfrom tensorflow.contrib.framework.python.ops import audio_ops as contrib_audio\r\n\r\n\r\nwith tf.Session() as sess:\r\n    input_ph = tf.placeholder(tf.float32, [512])\r\n    samples = tf.reshape(input_ph, [512, 1])\r\n    spectrogram = contrib_audio.audio_spectrogram(samples, window_size=512, stride=320, magnitude_squared=True)\r\n    mfccs = contrib_audio.mfcc(spectrogram, 16000, dct_coefficient_count=13)\r\n\r\n    sess.run([mfccs], feed_dict={input_ph: np.random.random([512])})\r\n    print('session run works')\r\n\r\n    converter = tf.lite.TFLiteConverter(sess.graph_def, input_tensors=[input_ph], output_tensors=[mfccs])\r\n    converter.allow_custom_ops = True\r\n    tflite_model = converter.convert()\r\n\r\nwith tempfile.NamedTemporaryFile(delete=False) as fout:\r\n    temp_name = fout.name\r\n    fout.write(tflite_model)\r\n    fout.flush()\r\n\r\ntry:\r\n    # Load TFLite model and allocate tensors.\r\n    interpreter = tf.lite.Interpreter(model_path=temp_name)\r\n    interpreter.allocate_tensors()\r\n    print('tflite model prepare works')\r\n\r\n    # Get input and output tensors.\r\n    input_details = interpreter.get_input_details()\r\n    output_details = interpreter.get_output_details()\r\n\r\n    # Test model on random input data.\r\n    for inp in input_details:\r\n        input_data = np.array(np.random.random_sample(inp['shape']), dtype=np.float32)\r\n        interpreter.set_tensor(inp['index'], input_data)\r\n\r\n    interpreter.invoke()\r\n    output_data = interpreter.get_tensor(output_details[0]['index'])\r\n    print(output_data)\r\nfinally:\r\n    try:\r\n        os.remove(temp_name)\r\n    except:\r\n        pass\r\n```", "comments": ["@andrewharp pinging since you wrote the TFLite Mfcc op. Is there another way of converting the op to TFLite that I'm missing? As far as I can tell, because of the different behavior w.r.t. the sample rate parameter, there's no way to convert a TF graph to a TFLite model with an Mfcc op in it.", "FWIW, I've changed the test here to check if there's 0 dimensions instead of 1 and things have been working fine: https://github.com/tensorflow/tensorflow/blob/e6d074140d851e0ef52dbe4d382903b9100ba0bb/tensorflow/lite/kernels/mfcc.cc#L75\r\n\r\n```diff\r\ndiff --git a/tensorflow/lite/kernels/mfcc.cc b/tensorflow/lite/kernels/mfcc.cc\r\nindex f5b0212728..ff4a6d2e2c 100644\r\n--- a/tensorflow/lite/kernels/mfcc.cc\r\n+++ b/tensorflow/lite/kernels/mfcc.cc\r\n@@ -72,7 +72,7 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\r\n   TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\r\n\r\n   TF_LITE_ENSURE_EQ(context, NumDimensions(inputWav), 3);\r\n-  TF_LITE_ENSURE_EQ(context, NumDimensions(inputRate), 1);\r\n+  TF_LITE_ENSURE_EQ(context, NumDimensions(inputRate), 0);\r\n\r\n   TF_LITE_ENSURE_EQ(context, output->type, kTfLiteFloat32);\r\n   TF_LITE_ENSURE_EQ(context, inputWav->type, output->type);\r\n```", "> FWIW, I've changed the test here to check if there's 0 dimensions instead of 1 and things have been working fine:\r\n> \r\n> [tensorflow/tensorflow/lite/kernels/mfcc.cc](https://github.com/tensorflow/tensorflow/blob/e6d074140d851e0ef52dbe4d382903b9100ba0bb/tensorflow/lite/kernels/mfcc.cc#L75)\r\n> \r\n> Line 75 in [e6d0741](/tensorflow/tensorflow/commit/e6d074140d851e0ef52dbe4d382903b9100ba0bb)\r\n> \r\n>  TF_LITE_ENSURE_EQ(context, NumDimensions(inputRate), 1); \r\n> ```diff\r\n> diff --git a/tensorflow/lite/kernels/mfcc.cc b/tensorflow/lite/kernels/mfcc.cc\r\n> index f5b0212728..ff4a6d2e2c 100644\r\n> --- a/tensorflow/lite/kernels/mfcc.cc\r\n> +++ b/tensorflow/lite/kernels/mfcc.cc\r\n> @@ -72,7 +72,7 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\r\n>    TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\r\n> \r\n>    TF_LITE_ENSURE_EQ(context, NumDimensions(inputWav), 3);\r\n> -  TF_LITE_ENSURE_EQ(context, NumDimensions(inputRate), 1);\r\n> +  TF_LITE_ENSURE_EQ(context, NumDimensions(inputRate), 0);\r\n> \r\n>    TF_LITE_ENSURE_EQ(context, output->type, kTfLiteFloat32);\r\n>    TF_LITE_ENSURE_EQ(context, inputWav->type, output->type);\r\n> ```\r\n\r\nWhen the code can be merged to the master branch?", "@rryan ping. Can you let me know if this patch makes sense? I'll can make a PR.", "We tried to convert the model to tflite (from the speech recognition sample) with this script:\r\n\r\nhttps://github.com/aselle/tensorflow/blob/7e71aa528111cd73cabc9fefdbb68422e35c16ce/tensorflow/examples/speech_commands/conv_only.py\r\n\r\nBy the way, it would be nice to have the python code of the converter used for the android tflite version!)\r\n\r\nIt works perfectly for audio sample of 1 second but it fails for 2 seconds with the same kind of error as the initial post:\r\n\r\nNumDimensions(inputRate) != 1 ( 0 != 1)Node number 1 (Mfcc) failed to prepare.\r\n\r\nIs this fix the solution? \r\n(we've changed the shape of the input from 16000,1 to 32000,1 and 32000,2 but we get the error above)\r\n", "@reuben, I can confirm your patch fixed the issue. I was able to rebuild and convert `audio_ops.mfcc()` to TFLite! I am still looking into how to correctly use `sample_rate` input. Currently, it is hard-coded in the `audio_ops.mfcc()` call. I suppose we can declare it as a placeholder and feed it to sess.run() but I haven't had success with that while converting it to TFLite yet. Will be glad to hear if you have tried anything there. \r\n\r\nI am also curious to see where is tensorflow headed with MFCCs. There's some discussion on this thread https://github.com/tensorflow/tensorflow/issues/11339#issuecomment-345741527 about retiring `audio_ops` and making `tf.signal` compatible to convert to TFLite. Nevertheless, this patch definitely seems worth merging. Or it will be nice if someone can shed some light on why is the expected dimensions of `inputRate=1`  so we can feed it the right shape of inputs.\r\n\r\n\r\n", "> FWIW, I've changed the test here to check if there's 0 dimensions instead of 1 and things have been working fine:\r\n> \r\n> https://github.com/tensorflow/tensorflow/blob/e6d074140d851e0ef52dbe4d382903b9100ba0bb/tensorflow/lite/kernels/mfcc.cc#L75\r\n> \r\n> ```diff\r\n> diff --git a/tensorflow/lite/kernels/mfcc.cc b/tensorflow/lite/kernels/mfcc.cc\r\n> index f5b0212728..ff4a6d2e2c 100644\r\n> --- a/tensorflow/lite/kernels/mfcc.cc\r\n> +++ b/tensorflow/lite/kernels/mfcc.cc\r\n> @@ -72,7 +72,7 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\r\n>    TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\r\n> \r\n>    TF_LITE_ENSURE_EQ(context, NumDimensions(inputWav), 3);\r\n> -  TF_LITE_ENSURE_EQ(context, NumDimensions(inputRate), 1);\r\n> +  TF_LITE_ENSURE_EQ(context, NumDimensions(inputRate), 0);\r\n> \r\n>    TF_LITE_ENSURE_EQ(context, output->type, kTfLiteFloat32);\r\n>    TF_LITE_ENSURE_EQ(context, inputWav->type, output->type);\r\n> ```\r\n\r\nHello,\r\nDid you find a solution to this problem? My app crashes giving the same error. It isn't able to load the model. I tried the same solution but the error remained.", "Yes, that patch is the solution :)", "We've been using it in our packages ever since I ran into this issue without problems.", "I tried that patch. Replaced the 1 with 0. It didn't seem to work. Am I missing something?", "> I tried that patch. Replaced the 1 with 0. It didn't seem to work. Am I missing something?\r\n\r\nTensorflow Lite Interpreter is compiled code. Hope you are rebuilding Tensorflow from source (https://www.tensorflow.org/install/source) after making the code change.", "I guess that's what I was missing. So I have to start from \"Build the pip package\" onwards?", "> > I tried that patch. Replaced the 1 with 0. It didn't seem to work. Am I missing something?\r\n> \r\n> Tensorflow Lite Interpreter is compiled code. Hope you are rebuilding Tensorflow from source (https://www.tensorflow.org/install/source) after making the code change.\r\n\r\nI changed 1 to 0 and rebuilt it. It didn't work. Where do I make the following change??\r\n--- a/tensorflow/lite/kernels/mfcc.cc\r\n+++ b/tensorflow/lite/kernels/mfcc.cc\r\nI don't find this line in the code. Maybe this is the reason why the error still exists?\r\n\r\n", "This patch does not affect training, it only changes the TFLite interpreter, so whatever you're using to load a trained .tflite file, that's what you need to modify and recompile. If you're using the Python interpreter API, then yeah, you need to rebuild and reinstall the pip package.", "I'm trying to use an pretrained tflite just to see how the app works well. I did the rebuilding but to no effect. The model doesn't load but keeps giving the same error", "May I know what command did you execute for compilation after making that change. I don't seem to be getting the solution.", " @jdduke Looks like d5cc8288d939b522982738370facd456a0a643ba fixed this?", "Ah yes, thanks for flagging, should be fixed (by d5cc828). ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26174\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26174\">No</a>\n"]}, {"number": 26173, "title": "Fixed deprecated access of collections members.", "body": "Changed the access of the abstract base classes to use collections.abc.\r\nThis prevents warnings in Python 3.7, and runtime errors in Python 3.8+.", "comments": ["Closes https://github.com/tensorflow/tensorflow/issues/26095", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26173) for more info**.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26173) for more info**.\n\n<!-- ok -->", "If there are problems importing `collections.abc` on older Python, it would be helpful to either push this through (https://github.com/benjaminp/six/pull/241) or else use this:\r\n```\r\ntry:\r\n    import collections.abc as collections_abc # only works on python 3.3+\r\nexcept ImportError:\r\n    import collections as collections_abc\r\n```", "@NeilGirdhar can you change the pull request to do your proposed workaround for older python versions? TensorFlow still has to support python 2.7 and 3.5", "@alextp can we rely on six if they push through this pull request? https://github.com/benjaminp/six/pull/241\r\n\r\n3.5 is no problem.  collections.abc exists there.", "Yes, relying on six would be ideal.\n\nWe can also put this in\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/util/compat.py\nand import from there\n\nOn Mon, Mar 4, 2019 at 12:06 PM Neil <notifications@github.com> wrote:\n\n> @alextp <https://github.com/alextp> can we rely on six if they push\n> through this pull request? benjaminp/six#241\n> <https://github.com/benjaminp/six/pull/241>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/26173#issuecomment-469400701>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxb4uSavXm7V2iYroJq5jW8jkr19Cks5vTXzVgaJpZM4bUt-U>\n> .\n>\n\n\n-- \n - Alex\n", "@alextp Thanks.  So, two options: push this through now with a stub in compat or wait until six pushes through their changelist and use that.  Which do you prefer?", "Let's use a stub in compat.", "Any updates?", "@NeilGirdhar gentle ping ", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!", "@alextp @rthadur Sorry for the delay, I've been so busy.  I will do it now.  Would you mind reopening?", "> @alextp @rthadur Sorry for the delay, I've been so busy. I will do it now. Would you mind reopening?\r\n\r\n@NeilGirdhar  reopened , thank you for your contribution.", "@NeilGirdhar can you please fix build failures :https://source.cloud.google.com/results/invocations/31a31f4a-15ce-480e-b3d4-4d92916eea0c/log", "@rthadur\r\n\r\nHow do I run the test locally?\r\n\r\nAlso, tensorflow is missing `__init__.py` in many folders.  You should have those even if they are empty files to declare that those folders are packages.", "please follow this [link](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/README.md) to run the tests ", "@rthadur Is there anything I can do to make the automatic build start?  ", "> @rthadur Is there anything I can do to make the automatic build start?\r\n\r\ni have started the builds , thank you", "I don't understand these build errors.  It seems to be totally unrelated to my change.  Any ideas?", "@rthadur Could these errors have something to do with how the tests were run?", "@NeilGirdhar i believe you need to update the test cases which are associated with the files.", "@rthadur I don't see that?  I see errors like \"Broken by missing target @com_google_absl//absl/base:base\"  ?  I don't see how this relates...", "@alextp any thoughts why this tests are failing ?", "@rthadur Looked like a tool failure, retrying", "> @rthadur Looked like a tool failure, retrying\r\n\r\n@alextp thank you, seems like most of the test failures are gone,  can you please approve again. "]}, {"number": 26171, "title": "RuntimeError: Graph is finalized and cannot be modified.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:n/a\r\n- TensorFlow installed from (source or binary):n/a\r\n- TensorFlow version (use command below):1.12.0\r\n- Python version:2.7.x\r\n- Bazel version (if compiling from source):n/a\r\n- GCC/Compiler version (if compiling from source):(Ubuntu 5.4.0-6ubuntu1~16.04.11)5.4.0\r\n- CUDA/cuDNN version:n/a\r\n- GPU model and memory:n/a\r\n\r\n**Describe the problem**\r\nI use tfrecords file to train a stacked autoencoder. I want to train the encoding layer for 1000 steps. I tried to create batches from features and labels and use them to train my network.  However, when I run my code I get an error (RuntimeError: Graph is finalized and cannot be modified.) indicating that the problem comes from the following function: \r\n\r\n     def train_layer(output_layer, layer_loss,optimizer):\r\n     \"\"\"Train each encoding layer for 1000 steps\"\"\"\r\n     layer_name = output_layer.name.split('/')[0]\r\n     print('Pretraining {}'.format(layer_name))\r\n     num_steps = 1000\r\n      step=1\r\n     features, labels=train_input_fn()\r\n     input_l = tf.reshape(features, [-1, FLAGS.image_rows, FLAGS.image_cols, 1])\r\n      while step <= num_steps:\r\n     instance_batch, label_batch = tf.train.shuffle_batch([input_l], batch_size=5, capacity=200, min_after_dequeue=100)\r\n    _out_layer, _layer_loss,_ = sess.run([output_layer, layer_loss, optimizer],\r\n     feed_dict ={features:instance_batch,labels:label_batch})\r\n     #print(_layer_loss)\r\n     step += 1\r\n     print('layer finished')`\r\n\r\n", "comments": ["I have a tfrecords file from which I am looking to create batches of data. I want to train the encoding layer of my model many steps while calling the batches. For this purpose I used the below code to create batches and get the next batch:\r\n\r\n\r\n    \r\n\r\n     `def write_and_encode(data_list, tfrecord_filename):\r\n      writer = tf.python_io.TFRecordWriter(tfrecord_filename)\r\n      for label, data_matrix in data_list:\r\n        example = tf.train.Example(features=tf.train.Features(\r\n            feature={\r\n                \"label\": tf.train.Feature(int64_list=tf.train.Int64List(value=[label])),\r\n                \"data_raw\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[data_matrix.tostring()]))\r\n            }\r\n        ))\r\n        writer.write(example.SerializeToString())\r\n    writer.close()\r\n    def read_and_decode(tfrecord_filename):\r\n    reader = tf.TFRecordReader()\r\n     filename_queue = tf.train.string_input_producer([tfrecord_filename],)\r\n    _, serialized_example = reader.read(filename_queue)\r\n    feature = tf.parse_single_example(serialized_example,\r\n                                      features={\r\n                                          \"label\": tf.FixedLenFeature([], tf.int64),\r\n                                          \"data_raw\": tf.FixedLenFeature([], tf.string)\r\n                                      })\r\n    data = tf.decode_raw(feature[\"data_raw\"], tf.float64)\r\n    data = tf.reshape(data, [FLAGS.image_rows, FLAGS.image_cols])\r\n    return data, feature[\"label\"]`\r\n    def train_input_fn():\r\n    tfrecord_file = \"../resources/train_tfrecord\"  \r\n    dataset = tf.data.TFRecordDataset(tfrecord_file)\r\n    dataset = dataset.map(parser)\r\n    train_dataset = dataset.repeat(FLAGS.num_epochs).batch(FLAGS.batch_size)\r\n    train_iterator = train_dataset.make_one_shot_iterator()\r\n    features, labels = train_iterator.get_next()\r\n    return features, labels\r\n\r\n    def parser(record_line):\r\n\r\n    features = {\r\n        \"label\": tf.FixedLenFeature([], tf.int64),\r\n        \"data_raw\": tf.FixedLenFeature([], tf.string)\r\n    }\r\n    parsed = tf.parse_single_example(record_line, features=features)\r\n    label = tf.cast(parsed[\"label\"], tf.int32) - 1  \r\n    data = tf.decode_raw(parsed[\"data_raw\"], tf.float64)\r\n    data = tf.reshape(data, [FLAGS.image_rows, FLAGS.image_cols])\r\n    data = tf.cast(data, tf.float32)\r\n    return data, label`\r\n\r\nTo train the encoding layer of an autoencoder, I perfrom as follows:\r\n\r\n     `def train_layer(output_layer, layer_loss,optimizer):\r\n    \"\"\"Train each encoding layer for 1000 steps\"\"\"\r\n    layer_name = output_layer.name.split('/')[0]\r\n    print('Pretraining {}'.format(layer_name))\r\n    num_steps = 1000\r\n    step=1\r\n    features, labels=train_input_fn()\r\n    input_l = tf.reshape(features, [-1, FLAGS.image_rows, FLAGS.image_cols, 1])\r\n    while step <= num_steps:\r\n\r\n         instance_batch, label_batch = tf.train.shuffle_batch([input_l], batch_size=5, capacity=200, min_after_dequeue=100)\r\n\r\n    _out_layer, _layer_loss,_ = sess.run([output_layer, layer_loss, optimizer],\r\n      feed_dict ={features:instance_batch,labels:label_batch})\r\n    #print(_layer_loss)\r\n    step += 1\r\n    print('layer finished')`\r\n\r\n  \r\nFor the configuration of the Monitored Training Session, I implement it as follows: \r\n\r\n     `\"\"\"Use a MonitoredTrainingSession for running the computations.  It makes running on distributed    systems possible, handles checkpoints, saving summaries, and restoring from crashes easy.\"\"\"\r\n\r\n       #create hooks to pass to the session.  These can be used for adding additional calculations, loggin, etc.\r\n       #This hook simply tells the session how many steps to run\r\n       hooks=[tf.train.StopAtStepHook(last_step=10000)]\r\n       #This command collects all summary ops that have been added to the graph and prepares them to   run in the next session\r\n      tf.summary.merge_all()\r\n      logs_dir = 'logs'\r\n      with tf.train.MonitoredTrainingSession(hooks=hooks,  checkpoint_dir=logs_dir,save_summaries_steps=100) as sess:\r\n\r\n    start_time = time.time()\r\n\r\n    \"\"\"First train each layer one at a time, freezing weights from previous layers.\r\n    This was accomplished by declaring which variables to update when each layer optimizer was defined.\"\"\"\r\n        for layer_dict in model_layers:\r\n             output_layer = layer_dict['output_layer']\r\n              layer_loss = layer_dict['layer_loss']\r\n              optimizer = layer_dict['optimizer']\r\n               train_layer( output_layer, layer_loss, optimizer)\r\n      #Now train the whole network for classification allowing all weights to change.\r\n        while not sess.should_stop():\r\n                  _y, _cross_entropy, _net_op, _accuracy = sess.run([y, cross_entropy, net_op, accuracy], feed_dict={x:instance_batch,y_labels:label_batch})\r\n       print(_accuracy)\r\n       print('Training complete\\n')`\r\n\r\nWhen I run my code, I get an error:\r\n\r\n> File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2919, in _check_not_finalized\r\n    raise RuntimeError(\"Graph is finalized and cannot be modified.\")\r\nRuntimeError: Graph is finalized and cannot be modified.\r\n\r\nThe source of the problem is the Train_layer:\r\n\r\n> Pretraining layer_0\r\n> Traceback (most recent call last):\r\n>   File \"aut.py\", line 222, in <module>\r\n>     train_layer( output_layer, layer_loss, optimizer)\r\n>   File \"aut.py\", line 111, in train_layer\r\n>     features, labels=train_input_fn()\r\n>   File \"aut.py\", line 67, in train_input_fn\r\n>     dataset = tf.data.TFRecordDataset(tfrecord_file)\r\n\r\n", "@amelroua \r\nPlease fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=00-bug-performance-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Along with the template, please provide as many details as possible to find the root cause of the issue. Thanks!\r\n", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!", "This problem still exists.\r\nI need some help.\r\n\r\nError when entering a number greater than 3 for parameter n_classes in tf.estimator.DNNClassifier\r\n\r\n```\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-83-00f0f6fad3ff> in <module>\r\n      1 tf.Graph().finalize()\r\n      2 ## \u624b\u98063:Estimator\u3092\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5316\r\n----> 3 dnn_classifier = tf.estimator.DNNClassifier(\r\n      4     feature_columns=[image_feature_column],\r\n      5     hidden_units=[32, 16],\r\n\r\n~/opt/anaconda3/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py in __init__(self, hidden_units, feature_columns, model_dir, n_classes, weight_column, label_vocabulary, optimizer, activation_fn, dropout, config, warm_start_from, loss_reduction, batch_norm)\r\n    739       batch_norm: Whether to use batch normalization after each hidden layer.\r\n    740     \"\"\"\r\n--> 741     head = head_utils.binary_or_multi_class_head(\r\n    742         n_classes,\r\n    743         weight_column=weight_column,\r\n\r\n~/opt/anaconda3/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/head/head_utils.py in binary_or_multi_class_head(n_classes, weight_column, label_vocabulary, loss_reduction)\r\n     59         loss_reduction=loss_reduction)\r\n     60   else:\r\n---> 61     head = multi_class_head.MultiClassHead(\r\n     62         n_classes,\r\n     63         weight_column=weight_column,\r\n\r\n~/opt/anaconda3/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/head/multi_class_head.py in __init__(self, n_classes, weight_column, label_vocabulary, loss_reduction, loss_fn, name)\r\n    156     if loss_fn:\r\n    157       base_head.validate_loss_fn_args(loss_fn)\r\n--> 158     self._n_classes = base_head.validate_n_classes(n_classes)\r\n    159     self._weight_column = weight_column\r\n    160     self._label_vocabulary = label_vocabulary\r\n\r\n~/opt/anaconda3/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/head/base_head.py in validate_n_classes(n_classes)\r\n    696     raise ValueError('n_classes must be > 2: %s.' % n_classes)\r\n    697 \r\n--> 698   n_classes_as_tensor = ops.convert_to_tensor(n_classes)\r\n    699   assert_n_classes = tf.compat.v1.debugging.assert_greater(\r\n    700       n_classes_as_tensor, 2, message='n_classes must be greater than 2')\r\n\r\n~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py in wrapped(*args, **kwargs)\r\n    161         with Trace(trace_name, **trace_kwargs):\r\n    162           return func(*args, **kwargs)\r\n--> 163       return func(*args, **kwargs)\r\n    164 \r\n    165     return wrapped\r\n\r\n~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\r\n   1538 \r\n   1539     if ret is None:\r\n-> 1540       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n   1541 \r\n   1542     if ret is NotImplemented:\r\n\r\n~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/tensor_conversion_registry.py in _default_conversion_function(***failed resolving arguments***)\r\n     50 def _default_conversion_function(value, dtype, name, as_ref):\r\n     51   del as_ref  # Unused.\r\n---> 52   return constant_op.constant(value, dtype, name=name)\r\n     53 \r\n     54 \r\n\r\n~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name)\r\n    262     ValueError: if called on a symbolic tensor.\r\n    263   \"\"\"\r\n--> 264   return _constant_impl(value, dtype, shape, name, verify_shape=False,\r\n    265                         allow_broadcast=True)\r\n    266 \r\n\r\n~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)\r\n    284   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)\r\n    285   attrs = {\"value\": tensor_value, \"dtype\": dtype_value}\r\n--> 286   const_tensor = g._create_op_internal(  # pylint: disable=protected-access\r\n    287       \"Const\", [], [dtype_value.type], attrs=attrs, name=name).outputs[0]\r\n    288 \r\n\r\n~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py in _create_op_internal(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\r\n   3509       An `Operation` object.\r\n   3510     \"\"\"\r\n-> 3511     self._check_not_finalized()\r\n   3512     if name is None:\r\n   3513       name = op_type\r\n\r\n~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py in _check_not_finalized(self)\r\n   3099     \"\"\"\r\n   3100     if self._finalized:\r\n-> 3101       raise RuntimeError(\"Graph is finalized and cannot be modified.\")\r\n   3102 \r\n   3103   def _add_op(self, op, op_name):\r\n\r\nRuntimeError: Graph is finalized and cannot be modified.\r\n```", "version is 2.4.1", "@sinano1107 Can you please open a new issue with a simple standalone code to reproduce the issue? Thanks!"]}, {"number": 26170, "title": "Fix typo in nmt_with_attention.ipynb", "body": "weigths -> weights", "comments": ["Changes have been merged , closing this PR"]}, {"number": 26169, "title": "[INTEL MKL] Add 2D Concat Feature with MKL-DNN", "body": "This PR adds a feature for concatenating 2D tensors with MKL-DNN. Such concatenation is used in Wide and Deep models. ", "comments": ["Temporarily closing. Will reopen after some more changes.", "@penpornk did you have chance to look into this PR?"]}, {"number": 26167, "title": "CRF functions in TensorFlow 2.0", "body": "Hello,\r\n\r\nIt seems like CRF (tensorflow.contrib.crf) is moving to tensorflow/probability in TensorFlow 2.0. \r\n(https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md)\r\n\r\nbut, I couldn't find them on https://github.com/tensorflow/probability.\r\n \r\nWhere can I find CRF related features in TensorFlow 2.0?\r\n\r\n\r\n ", "comments": ["@invencode It does not look like `tf.contrib.crf` has been migrated to either the core TensorFlow 2.0 API or to TensorFlow Probability. If your code base is dependent on this symbol and you are interested in migrating it to TF 2.0, we could take a look at including it in the [addons](http://www.github.com/tensorflow/addons) repo.\r\n\r\nAre you interested in helping migrate `tf.contrib.crf` to `tensorflow/addons`?\r\n\r\ncc: @martinwicke @seanpmorgan ", "Hi @invencode currently addons is planning to move CRF https://github.com/tensorflow/addons/issues/22 but its unlikely to make the first release. The move requires adapting the code to use [Keras RNN Cell](https://github.com/tensorflow/community/blob/master/rfcs/20180920-unify-rnn-interface.md) in TF2.\r\n\r\nAddons is always looking for more contributors and maintainers, so if you're interested as @dynamicwebpaige suggested please reach out in the issue.", "@dynamicwebpaige @seanpmorgan, could I have try on this part?", "@a6802739 Any progress?", "We've ported CRF to TF2.0: https://github.com/tensorflow/addons/commit/47e687725eb4d65e45498e67a9026177fda684dd\r\n\r\nThis issue can be closed.", "Thank you, @Squadrick "]}, {"number": 26166, "title": "Tensorflow run with C++ API cause XAVIER crash", "body": "Tensoflow version:  '1.12.0-rc2'\r\nDevice:  Xavier\r\nNV POWER Mode: MAXN\r\nCUDA: 10.0.117\r\nCUDNN : 7.3.1\r\n\r\n##############KERNEL CODE#####################\r\nvoid semanticSegPro::getImgData(uchar* data)\r\n{\r\n\r\n    auto input_tensor_mapped = input_tensor.tensor<uchar, 4>();\r\n\r\n    for (int y = 0; y < height; ++y)\r\n    {\r\n        const uchar* source_row = data + (y * width * 3);\r\n        for (int x = 0; x < width; ++x)\r\n        {\r\n            const uchar* source_pixel = source_row + (x * 3);\r\n            for (int c = 0; c < 3; ++c)\r\n            {\r\n                const uchar* source_value = source_pixel + c;\r\n                input_tensor_mapped(0, y, x, c) = *source_value;\r\n            }\r\n        }\r\n    }\r\n\r\n    QString s=QDateTime::currentDateTime().toString(\"yyyy-MM-dd hh:mm:ss.zzz\");\r\n    qDebug()<<\"BF = \"<<s;\r\n\r\n    **Status status_run =\r\n            session->Run({{inputTensorName.toStdString(), input_tensor}},\r\n            {outputTensorName.toStdString()}, {}, &outputs);**\r\n\r\n    if (!status_run.ok()) {\r\n        qDebug()<<\"ERROR: RUN failed...\"<<QString::fromStdString(status_run.ToString());\r\n    }\r\n\r\n    s=QDateTime::currentDateTime().toString(\"yyyy-MM-dd hh:mm:ss.zzz\");\r\n    qDebug()<<\"AFT = \"<<s;\r\n\r\n    output_tensor = outputs[0];\r\n    auto tmap = output_tensor.tensor<int64, 3>();\r\n\r\n    for(int i = 0; i < height; i++)\r\n    {\r\n        for(int j =0; j< width; j++)\r\n        {\r\n            outData[i*width + j] = tmap(0,i,j);\r\n        }\r\n    }\r\n\r\n    emit segData((uchar*)outData);\r\n}\r\n##############KERNEL CODE#####################\r\n\r\nRUN HERE : \r\n    Status status_run =\r\n            session->Run({{inputTensorName.toStdString(), input_tensor}},\r\n            {outputTensorName.toStdString()}, {}, &outputs);\r\nThe program abnormal termination or crash.\r\n\r\nI used the same program with PC(GTX 1070), it runs well. \r\nI also used python API with Xavier platform, it runs well also.\r\nWhy the program crash with Xavier and C++ API???\r\n", "comments": ["@timoonboru Could you provide error log and any other details you wish to share to find root-cause of the issue? Could you try the latest version of TF to check whether the bug persists with recent version? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!"]}, {"number": 26165, "title": "Where to place 'tf.contrib.quantize.create_training_graph'  during multi-gpu Quantization-aware training?", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):**Yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):**Ubuntu 16.04**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:No\r\n- TensorFlow installed from (source or binary):**By pip**\r\n- TensorFlow version (use command below):**1.11.0**\r\n- Python version:**2.7**\r\n- Bazel version (if compiling from source):No\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nI follow \"quantization aware training\" tutorial in tensorflow to retrain my already trained floating point  model. And I wrote a muli-gpu version to do the stuff. Actually, I have written two snippets of training code, the first code snippet is :\r\n```python\r\nwith tf.variable_scope(tf.get_variable_scope()):\r\n    for i in xrange(len(GPU_NUM_ID)):\r\n       with tf.device('/gpu:%d' % GPU_NUM_ID[i]):\r\n              with tf.name_scope('%s_%d' % ('cnn_mg', i)) as scope:\r\n                     images, labels = ut._load_batch_filename()  \r\n                            logits, out_data = inference(images, reuse=tf.AUTO_REUSE,  num_classes=LABEL_NUM)\r\n                            loss ,accuracy_sep = _tower_loss_depth(scope, logits, labels)\r\n                            tf.get_variable_scope().reuse_variables()\r\n                            grads = optimizer.compute_gradients(loss_total_sep)\r\n                            tower_grads.append(grads)   \r\n\r\ntf.contrib.quantize.create_training_graph(quant_delay=DELAY_STEP)\r\n```\r\nThe second snippet of the training code is:\r\n```python\r\nwith tf.variable_scope(tf.get_variable_scope()):\r\n\r\n    for i in xrange(len(GPU_NUM_ID)):\r\n       with tf.device('/gpu:%d' % GPU_NUM_ID[i]):\r\n              with tf.name_scope('%s_%d' % ('cnn_mg', i)) as scope:\r\n                     images, labels = ut._load_batch_filename()  \r\n                            logits, out_data = inference(images, reuse=tf.AUTO_REUSE,  num_classes=LABEL_NUM)\r\n                            with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE): \r\n                                   tf.contrib.quantize.create_training_graph(quant_delay=DELAY_STEP)\r\n                            loss ,accuracy_sep = _tower_loss_depth(scope, logits, labels)\r\n                            tf.get_variable_scope().reuse_variables()\r\n                            grads = optimizer.compute_gradients(loss_total_sep)\r\n                            tower_grads.append(grads)\r\n```\r\n\r\n**Describe the expected behavior**\r\nWhat I have found is: The second code snippet if 1.5 times faster than the first  code snippet  with the same training setting.I want to know why this happens and  I want know which code snippet is the tensorflow recommended code snippet. \r\nHowever, When I update my tensorflow version to 1.12.0, the second version(faster) could cause some error(\"tensorflow Graph is invalid, contains a cycle\"). Dose this mean I have to slow down my multi-gpu Quantization-aware training in the future version of tensorflow?\r\n\r\n", "comments": ["Have you solved this problem? \r\n\r\nDo you know where to insert the code when using tf.Estimator?", "@Zhengtq We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.6 which is latest stable version of TF and let us know if the issue still persists in newer versions. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26165\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26165\">No</a>\n"]}, {"number": 26164, "title": "Memory Fault (Coredumped) java", "body": "Hi!\r\n\r\nWhen i try to run this Hello World TF example, when the program is ending, appears this message \"Memory Fault(Coredumped)\"\r\n\r\nMachine:\r\n-java 1.7.64\r\n-SO: Red Hat 7.3\r\nCores: 4\r\nRAM: 8 gb\r\n\r\nExample Code:\r\n\r\n`\r\nimport org.tensorflow.Graph;\r\nimport org.tensorflow.Session;\r\nimport org.tensorflow.Tensor;\r\nimport org.tensorflow.TensorFlow;\r\n\r\npublic class HelloTF {\r\n\t\r\n  public static void main(String[] args) throws Exception {\r\n    try (Graph g = new Graph()) {\r\n      final String value = \"Hello from \" + TensorFlow.version();\r\n\r\n      // Construct the computation graph with a single operation, a constant\r\n      // named \"MyConst\" with a value \"value\".\r\n      try (Tensor t = Tensor.create(value.getBytes(\"UTF-8\"))) {\r\n        // The Java API doesn't yet include convenience functions for adding operations.\r\n        g.opBuilder(\"Const\", \"MyConst\").setAttr(\"dtype\", t.dataType()).setAttr(\"value\", t).build();\r\n      }\r\n      catch(Exception e) {\r\n    \t  System.out.println(\"Try 1\");\r\n    \t  e.printStackTrace();\r\n      }\r\n\r\n      // Execute the \"MyConst\" operation in a Session.\r\n      try (Session s = new Session(g);\r\n           Tensor output = s.runner().fetch(\"MyConst\").run().get(0)) {\r\n        System.out.println(new String(output.bytesValue(), \"UTF-8\"));\r\n        s.close();\r\n      }\r\n      catch(Exception e) {\r\n    \t  System.out.println(\"Try 2\");\r\n    \t  e.printStackTrace();\r\n\r\n      }\r\n    }\r\n    catch(Exception e) {\r\n  \t  System.out.println(\"Try 3\");\r\n  \t  e.printStackTrace();\r\n    }\r\n    System.out.println(\"END\");\r\n  }\r\n\r\n}`\r\n\r\n\r\n\r\nPOM:\r\n`<project>\r\n       \r\n     <modelVersion>4.0.0</modelVersion>\r\n     <groupId>org.myorg</groupId>\r\n     <artifactId>hellotf</artifactId>\r\n     <version>1.0.0</version>\r\n    \r\n     <properties>\r\n       <exec.mainClass>HelloTF</exec.mainClass>\r\n       <maven.compiler.source>1.7</maven.compiler.source>\r\n       <maven.compiler.target>1.7</maven.compiler.target>  \r\n     </properties>\r\n    \r\n     <dependencies>\r\n       \r\n       <dependency>\r\n         <groupId>org.tensorflow</groupId>\r\n         <artifactId>tensorflow</artifactId>\r\n         <version>1.7.0</version>\r\n       </dependency>\r\n\r\n    </dependencies>\r\n\r\n</project>`\r\n\r\nAny Help?\r\n\r\nthanks!", "comments": ["@Arkariian This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!"]}, {"number": 26163, "title": "Update UniqueV2 SetShapeFN.", "body": "Fix UniqueV2 shape inference bug.", "comments": ["```\r\n# tensor 'x' is [[1, 0, 0],\r\n#                [1, 0, 0],\r\n#                [2, 0, 0]]\r\ny, idx = unique(x, axis=1)\r\ny ==> [[1, 0],\r\n       [1, 0],\r\n       [2, 0]]\r\nidx ==> [0, 1, 1]\r\n```\r\nThe dimensions of y should be [-1, -1], which follows the rank of x. In the previous version, the dimension of y is set to [-1] by hard code. This patch tries to fix this difference.\r\n", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}]