[{"number": 2322, "title": "InvalidArgumentError: Cannot assign a device to node when distributed running", "body": "I tried running [this script](https://github.com/tensorflow/models/tree/master/inception) on two machine, start script as follows, Note that each machine has 4 GPUs. \n\n```\n# machine 10.10.12.28\n~/models/inception/bazel-bin/inception/imagenet_distributed_train \\\n--batch_size=32 \\\n--data_dir=/data1/imagenet1k \\\n--job_name='worker' \\\n--task_id=0 \\\n--gpu_device_id=0 \\\n--ps_hosts='10.10.102.28:2220,10.10.102.29:2220' \\\n--worker_hosts='10.10.102.28:2221,10.10.102.28:2222,10.10.102.28:2223,10.10.102.29:2224,10.10.102.29:2221,10.10.102.29:2222,10.10.102.29:2223,10.10.102.29:2224' &\n\n~/models/inception/bazel-bin/inception/imagenet_distributed_train \\\n--batch_size=32 \\\n--data_dir=/data1/imagenet1k \\\n--job_name='worker' \\\n--task_id=1 \\\n--gpu_device_id=1 \\\n--ps_hosts='10.10.102.28:2220,10.10.102.29:2220' \\\n--worker_hosts='10.10.102.28:2221,10.10.102.28:2222,10.10.102.28:2223,10.10.102.29:2224,10.10.102.29:2221,10.10.102.29:2222,10.10.102.29:2223,10.10.102.29:2224' &\n\n~/models/inception/bazel-bin/inception/imagenet_distributed_train \\\n--batch_size=32 \\\n--data_dir=/data1/imagenet1k \\\n--job_name='worker' \\\n--task_id=2 \\\n--gpu_device_id=2 \\\n--ps_hosts='10.10.102.28:2220,10.10.102.29:2220' \\\n--worker_hosts='10.10.102.28:2221,10.10.102.28:2222,10.10.102.28:2223,10.10.102.29:2224,10.10.102.29:2221,10.10.102.29:2222,10.10.102.29:2223,10.10.102.29:2224' &\n\n~/models/inception/bazel-bin/inception/imagenet_distributed_train \\\n--batch_size=32 \\\n--data_dir=/data1/imagenet1k \\\n--job_name='worker' \\\n--task_id=3 \\\n--gpu_device_id=3 \\\n--ps_hosts='10.10.102.28:2220,10.10.102.29:2220' \\\n--worker_hosts='10.10.102.28:2221,10.10.102.28:2222,10.10.102.28:2223,10.10.102.29:2224,10.10.102.29:2221,10.10.102.29:2222,10.10.102.29:2223,10.10.102.29:2224' &\n\nCUDA_VISIBLE_DEVICES='' ~/models/inception/bazel-bin/inception/imagenet_distributed_train \\\n--job_name='ps' \\\n-task_id=0 \\\n--ps_hosts='10.10.102.28:2220,10.10.102.29:2220' \\\n--worker_hosts='10.10.102.28:2221,10.10.102.28:2222,10.10.102.28:2223,10.10.102.29:2224,10.10.102.29:2221,10.10.102.29:2222,10.10.102.29:2223,10.10.102.29:2224' &\n\n#machine 10.10.12.29\n~/models/inception/bazel-bin/inception/imagenet_distributed_train \\\n--batch_size=32 \\\n--data_dir=/data1/imagenet1k \\\n--job_name='worker' \\\n--task_id=4 \\\n--gpu_device_id=0 \\\n--ps_hosts='10.10.102.28:2220,10.10.102.29:2220' \\\n--worker_hosts='10.10.102.28:2221,10.10.102.28:2222,10.10.102.28:2223,10.10.102.29:2224,10.10.102.29:2221,10.10.102.29:2222,10.10.102.29:2223,10.10.102.29:2224' &\n\n~/models/inception/bazel-bin/inception/imagenet_distributed_train \\\n--batch_size=32 \\\n--data_dir=/data1/imagenet1k \\\n--job_name='worker' \\\n--task_id=5 \\\n--gpu_device_id=1 \\\n--ps_hosts='10.10.102.28:2220,10.10.102.29:2220' \\\n--worker_hosts='10.10.102.28:2221,10.10.102.28:2222,10.10.102.28:2223,10.10.102.29:2224,10.10.102.29:2221,10.10.102.29:2222,10.10.102.29:2223,10.10.102.29:2224' &\n\n~/models/inception/bazel-bin/inception/imagenet_distributed_train \\\n--batch_size=32 \\\n--data_dir=/data1/imagenet1k \\\n--job_name='worker' \\\n--task_id=6 \\\n--gpu_device_id=2 \\\n--ps_hosts='10.10.102.28:2220,10.10.102.29:2220' \\\n--worker_hosts='10.10.102.28:2221,10.10.102.28:2222,10.10.102.28:2223,10.10.102.29:2224,10.10.102.29:2221,10.10.102.29:2222,10.10.102.29:2223,10.10.102.29:2224' &\n\n~/models/inception/bazel-bin/inception/imagenet_distributed_train \\\n--batch_size=32 \\\n--data_dir=/data1/imagenet1k \\\n--job_name='worker' \\\n--task_id=7 \\\n--gpu_device_id=3 \\\n--ps_hosts='10.10.102.28:2220,10.10.102.29:2220' \\\n--worker_hosts='10.10.102.28:2221,10.10.102.28:2222,10.10.102.28:2223,10.10.102.29:2224,10.10.102.29:2221,10.10.102.29:2222,10.10.102.29:2223,10.10.102.29:2224' &\n\nCUDA_VISIBLE_DEVICES='' ~/models/inception/bazel-bin/inception/imagenet_distributed_train \\\n--job_name='ps' \\\n-task_id=1 \\\n--ps_hosts='10.10.102.28:2220,10.10.102.29:2220' \\\n--worker_hosts='10.10.102.28:2221,10.10.102.28:2222,10.10.102.28:2223,10.10.102.29:2224,10.10.102.29:2221,10.10.102.29:2222,10.10.102.29:2223,10.10.102.29:2224' &\n```\n\nerror log as follows: \n\n```\nTraceback (most recent call last):\n  File \"/home/zhufengda/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/__main__/inception/imagenet_distributed_train.py\", line 65, in <module>\n    tf.app.run()\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"/home/zhufengda/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/__main__/inception/imagenet_distributed_train.py\", line 61, in main\n    inception_distributed_train.train(server.target, dataset, cluster_spec)\n  File \"/home/zhufengda/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/__main__/inception/inception_distributed_train.py\", line 264, in train\n    sess = sv.prepare_or_wait_for_session(target, config=sess_config)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 681, in prepare_or_wait_for_session\n    max_wait_secs=max_wait_secs)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py\", line 282, in wait_for_session\n    sess.run([self._local_init_op])\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 355, in run\n    run_metadata_ptr)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 606, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 695, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 715, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'save/restore_slice_837/shape_and_slice': Could not satisfy explicit device specification '/job:ps/task:1/device:CPU:0' because no devices matching that specification are registered in this process; available devices: /job:ps/replica:0/task:0/cpu:0, /job:worker/replica:0/task:0/cpu:0, /job:worker/replica:0/task:0/gpu:0, /job:worker/replica:0/task:0/gpu:1, /job:worker/replica:0/task:0/gpu:2, /job:worker/replica:0/task:0/gpu:3, /job:worker/replica:0/task:1/cpu:0, /job:worker/replica:0/task:1/gpu:0, /job:worker/replica:0/task:1/gpu:1, /job:worker/replica:0/task:1/gpu:2, /job:worker/replica:0/task:1/gpu:3, /job:worker/replica:0/task:2/cpu:0, /job:worker/replica:0/task:2/gpu:0, /job:worker/replica:0/task:2/gpu:1, /job:worker/replica:0/task:2/gpu:2, /job:worker/replica:0/task:2/gpu:3, /job:worker/replica:0/task:4/cpu:0, /job:worker/replica:0/task:4/gpu:0, /job:worker/replica:0/task:4/gpu:1, /job:worker/replica:0/task:4/gpu:2, /job:worker/replica:0/task:4/gpu:3, /job:worker/replica:0/task:5/cpu:0, /job:worker/replica:0/task:5/gpu:0, /job:worker/replica:0/task:5/gpu:1, /job:worker/replica:0/task:5/gpu:2, /job:worker/replica:0/task:5/gpu:3, /job:worker/replica:0/task:6/cpu:0, /job:worker/replica:0/task:6/gpu:0, /job:worker/replica:0/task:6/gpu:1, /job:worker/replica:0/task:6/gpu:2, /job:worker/replica:0/task:6/gpu:3, /job:worker/replica:0/task:7/cpu:0, /job:worker/replica:0/task:7/gpu:0, /job:worker/replica:0/task:7/gpu:1, /job:worker/replica:0/task:7/gpu:2, /job:worker/replica:0/task:7/gpu:3\n     [[Node: save/restore_slice_837/shape_and_slice = Const[dtype=DT_STRING, value=Tensor<type: string shape: [] values: >, _device=\"/job:ps/task:1/device:CPU:0\"]()]]\nCaused by op u'save/restore_slice_837/shape_and_slice', defined at:\n  File \"/home/zhufengda/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/__main__/inception/imagenet_distributed_train.py\", line 65, in <module>\n    tf.app.run()\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"/home/zhufengda/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/__main__/inception/imagenet_distributed_train.py\", line 61, in main\n    inception_distributed_train.train(server.target, dataset, cluster_spec)\n  File \"/home/zhufengda/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/__main__/inception/inception_distributed_train.py\", line 237, in train\n    saver = tf.train.Saver()\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 845, in __init__\n    restore_sequentially=restore_sequentially)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 515, in build\n    filename_tensor, vars_to_save, restore_sequentially, reshape)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 271, in _AddRestoreOps\n    values = self.restore_op(filename_tensor, vs, preferred_shard)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 186, in restore_op\n    preferred_shard=preferred_shard)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/io_ops.py\", line 201, in _restore_slice\n    preferred_shard, name=name)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 271, in _restore_slice\n    preferred_shard=preferred_shard, name=name)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 444, in apply_op\n    as_ref=input_arg.is_ref)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 566, in convert_to_tensor\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/constant_op.py\", line 179, in _constant_tensor_conversion_function\n    return constant(v, dtype=dtype, name=name)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/constant_op.py\", line 166, in constant\n    attrs={\"value\": tensor_value, \"dtype\": dtype_value}, name=name).outputs[0]\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2177, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1161, in __init__\n    self._traceback = _extract_stack()\n```\n", "comments": ["Is this issue related to [this Stack Overflow question](http://stackoverflow.com/q/37177992/3574081)? The command lines and error messages are similar but not identical. I posted an answer on Stack Overflow.\n", "Looking at your command lines, it's a little suspicious that the PS tasks (mentioned in the error message) have the arg specified as `-task_id=1` (and not `--task_id=1`). Is it possible that your PS task 1 (i.e. on `10.10.102.29:2220`) thinks that it is actually task 0, because this flag isn't being parsed?\n", "Sorry for my typo, this issue can be closed. After further exploration, I have some idea of improvements and we can discuss at [here](https://github.com/tensorflow/models/issues/72)\n"]}, {"number": 2321, "title": "Confusion with QueueRunner doc", "body": "When reading the threading_and_queues how_to and the doc of QueueRunner the phrase \"repeatedly run an enqueue op\" confused me.\nI think these minor changes would save others a few minutes of confusion.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Hi there - just so we can make the comment as clear as possible, what was it about the remark in threading_and_queues.md that confused you?\n", "If you are not accustomed to the queue class it is not obvious that repeatedly runing an enqueue op will block.\nMy initial idea is that it would overflow the queue.\n", "Ping for @mrry, is the existing comment okay?\n", "closing due to inactivity, please comment and we'll reopen!\n"]}, {"number": 2320, "title": "examples/udacity/1_notmnist.ipynb image normalization is not equal to Udacity video", "body": "Hi,\nThis is just a minor thing, but it might be confusing for people who follow the Udacity course. The video about normalization of the input (https://www.youtube.com/watch?v=0mxNQA95mYE) describes the normalization as: pixel_value - 128 / 128. However the code seems to compute: pixel_value - 128 / 255 (since pixel_depth = 255). This changes the range of the data from -1 to 1 into -0.5 to 0.5 and might be confusing for people who blindly copy the function and expect it to work like in the video.\n", "comments": ["Sorry for posting it here, but I could not find another location to post issues like this or contact someone about the course.\n", "That's true. Maybe what this discrepancy conveys is that it doesn't matter :) Happy to take a PR if you think it should be changed, but please make sure the results don't change in unexpected ways as a result.\n", "Closing pending PR since this is not a mandatory fix.\n", "You are right, it does not really matter for the tutorial. I was just surprised to see it when I reused the code. I thought it might be worth a comment in the code or the course. But I'm new to this, so do with it as you see fit. (thanks for the videos by the way, they were really useful)\n"]}, {"number": 2319, "title": "tf.constant does not raise an error if too few values are provided", "body": "It is difficult to see the current behavior (repeating the final value) being useful for anyone:\n\n```\n>>> tf.constant(np.arange(5), shape=(10,)).eval()\narray([0, 1, 2, 3, 4, 4, 4, 4, 4, 4])\n```\n\nIn contrast, there is a sensible error message if too many values are provided:\n\n```\n>>> tf.constant(np.arange(5), shape=(3,)).eval()\nValueError: Too many elements provided. Needed at most 3, but received 5\n```\n\nI am running the internal version of TensorFlow.\n", "comments": ["This is intentional behavior. Many users write e.g. `tf.constant(4.0, shape=[100, 100])` to generate a tensor filled with a given value. Supporting this for non-scalars is more questionable, but might be required for backwards compatibility. \n", "Should I raise a separate issue for non-scalars then? The behavior is surprising and in my opinion quite likely to result in bugs. It's true that it's documented, though.\n"]}, {"number": 2318, "title": "tf.constant should support pandas Series and DataFrame as input", "body": "Currently, it does not.\n\nI would suggest implementing this by checking for a `__array__` method, which is the [standard API](http://docs.scipy.org/doc/numpy/reference/arrays.classes.html#numpy.class.__array__) used for indicating that an object is coercible into NumPy arrays, and is implemented by nearly every library that implements \"array like\" objects in the Python ecosystem. This includes pandas, as well as many other widely used libraries, including xarray and dask.array.\n", "comments": ["This should be easy to add, after my fix to #2328 goes in.\n\n@rdadolf: Was there any reason for your thumbs down? If this change is easy to make, is there any reason not to do it?\n", "So this caught my eye because of the pandas tag, but I suppose if I am to be consistent, it should apply to #2328 as well.\n\nMy feeling is that this kind of everything-should-map-to-everything coercion is harmful to a programming environment, mostly because it undermines the type system. Type errors turn into logical errors, which are far harder to debug. This is the reason I avoid pandas (sorry, @shoyer!), and I'd rather it not be in TensorFlow either.\n\nFrom your comments in the other thread and the existence of this, it seems likely I'm in the minority here, so the feature should probably move ahead. Still, I thought I'd weigh in.\n", "> My feeling is that this kind of everything-should-map-to-everything coercion is harmful to a programming environment, mostly because it undermines the type system. Type errors turn into logical errors, which are far harder to debug.\n\nI am in total agreement with you, but I think \"numpy array likes\" are a well defined duck type -- they're anything that implements the `__array__` method.\n\nImportantly, I do not propose to test this by seeing if coercing something to an array with `np.array` works. NumPy is far too willing to convert anything into 0d object arrays, though I think most NumPy devs agree that this is a [bad idea](https://github.com/numpy/numpy/issues/6070).\n\n> This is the reason I avoid pandas (sorry, @shoyer!), and I'd rather it not be in TensorFlow either.\n\nFunny you should mention this. Recently, most of my contributions to pandas seem to be fighting a [losing battle for type safety](https://github.com/pydata/pandas/pull/12482#issuecomment-218641824).\n\nOn a related note, whatever choice we make here for coercing arrays in `tf.constant` should also hold for values put into `feed_dict` for `Session.run`.\n", "> I am in total agreement with you, but I think \"numpy array likes\" are a well defined duck type -- they're anything that implements the `__array__` method.\n> \n> Importantly, I do not propose to test this by seeing if coercing something to an array with np.array works. NumPy is far too willing to convert anything into 0d object arrays, though I think most NumPy devs agree that this is a bad idea.\n\nThis seems convincing. It avoids ambiguity by explicitly foisting the decisions on the  `__array__` method author, which is a plus.\n\nI probably should have directed my emoji-flavored vitriol elsewhere. Or perhaps, you know, actually taken the time to write a coherent response.\n\n> On a related note, whatever choice we make here for coercing arrays in tf.constant should also hold for values put into feed_dict for Session.run.\n\nThis doesn't seem compatible with session's [existing method](https://github.com/tensorflow/tensorflow/blob/4a4f2461533847dde239851ecebe5056088a828c/tensorflow/python/client/session.py#L223), which is apparently distinct (?) from the one used for [normal conversion](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/ops.py#L690), which is also incompatible. Both use direct type comparisons, not duck-typing checks. It seems like both locations would need to change. Maybe @mrry has already dealt with this.\n", "We'd be happy to accept a PR for `__array__` support.\n", "I added a PR #8638 to support `__array__`. Please take a look and let me know if there are any issues."]}, {"number": 2317, "title": "Install the latest version whl of tensorflow from Jenkins daily build system, but can not find a part of the directory\u3002", "body": "hello, i have a question:\n\nOperating System:\nubuntu 14.04\n\nquestion\uff1a\nInstall the latest version whl of tensorflow from Jenkins daily build system, but can not find a part of the directory\u3002\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n\nfirst download the latest stable package:\n\nhttp://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_CONTAINER_TYPE=CPU,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=cpu-slave/lastStableBuild/\n1. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n\n> > > import tensorflow; print(tensorflow.**version**)\n> > > 0.8.0\n### Steps to reproduce\n1.  pip uninstall tensorflow\n2.  pip install  --upgrade  tensorflow-0.8.0-cp27-none-linux_x86_64.whl  \n### What have you tried?\n1. in  the  path anaconda2\\envs\\tensorflow\\lib\\python2.7\\site-packages\\tensorflow\\models\\image dictionary\uff0ccan not find the alexnet and imagenet dictionary, but can find them from the github path https://github.com/tensorflow/tensorflow/tree/master/tensorflow/models/image.\n\nhow can i update to get alexnet and imagenet dictionary?\n", "comments": ["I am fixing this for the future. For now, you can simply clone the tensorflow repository (the r0.8 branch if you want to be consistent with your binaries), and copy the `alexnet` and `imagenet` directories from the cloned directory into the `anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/models` directory.\n"]}, {"number": 2316, "title": "Try to build label_image:main.cc on Android", "body": "1.If I build  tensorflow_jni.cc with the BUILD file in label_image,I added android_binary.it ask me to compile with -std=c++.because the ndk r10e running with GCC4.9.I donot know how to set -std=c++11.\n2.If I build it with .mk.and copy the .h files in bazel-genfiles/tensorflow/cc/ops,like array_ops.h etc,  to tensorflow folder,setted include like this\n`#include \"tensorflow/cc/ops/array_ops.h\"\n# include \"tensorflow/cc/ops/const_op.h\"\n# include \"tensorflow/cc/ops/data_flow_ops.h\"\n# include \"tensorflow/cc/ops/image_ops.h\"`...\n\nit give me:\n`jni/./tensorflow_jni.cc:132: error: undefined reference to 'tensorflow::ops::Cast(tensorflow::NodeBuilder::NodeOut, tensorflow::DataType, tensorflow::GraphDefBuilder::Options const&)'`\nfor every tensorflow::ops:: method.it seems,ndk have found the method ,but not compile?\n3.Did any guies build label_image:main.cc on Android?\nthe android demo on tensorflow does not working will with my own graph,retrained by image_retraining .\n", "comments": ["Sorry you're hitting problems! The best way to tackle this is to update the Android Demo to handle retrained models, so I will work on that.\n", "thx\uff0cI am waiting for the new demo now.\n"]}, {"number": 2315, "title": "tf.nn.softmax outputs negative values (equal to tf.nn.log_softmax)!", "body": "### Environment info\n\nOperating System: Ubuntu 14.04.03\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nls -l /usr/local/cuda/lib_/libcud_\n-rw-r--r-- 1 root root    322936 Apr 25 11:05 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root        16 Apr 25 11:05 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root        19 Apr 25 11:05 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root    383336 Apr 25 11:05 /usr/local/cuda/lib64/libcudart.so.7.5.18\n-rw-r--r-- 1 root root    720192 Apr 25 11:05 /usr/local/cuda/lib64/libcudart_static.a\nlrwxrwxrwx 1 3319 users       13 Feb  9 09:48 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.4\nlrwxrwxrwx 1 3319 users       17 Feb  9 09:48 /usr/local/cuda/lib64/libcudnn.so.4 -> libcudnn.so.4.0.7\n-rwxrwxr-x 1 3319 users 61453024 Feb  8 14:12 /usr/local/cuda/lib64/libcudnn.so.4.0.7\n-rw-rw-r-- 1 3319 users 62025862 Feb  8 14:12 /usr/local/cuda/lib64/libcudnn_static.a\n-rw-r--r-- 1 root root    189170 Apr 25 11:05 /usr/local/cuda/lib/libcudadevrt.a\nlrwxrwxrwx 1 root root        16 Apr 25 11:05 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root        19 Apr 25 11:05 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root    311596 Apr 25 11:05 /usr/local/cuda/lib/libcudart.so.7.5.18\n-rw-r--r-- 1 root root    558020 Apr 25 11:05 /usr/local/cuda/lib/libcudart_static.a\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n\nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl\n1. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n\n0.8.0rc0\n\nIf installed from sources, provide the commit hash:\n### Steps to reproduce\n\nIt appears that tf.nn.softmax is output'ing negative values. After a lot of debugging, i added the following tf.Print line right after the call to tf.nn.softmax(). The output does indeed have negative values.. Strangely enough, the values match the output of tf.nn.log_softmax()!\n\nhttps://github.com/sdlg/nlc/blob/04af660b23218c026983785f339afeea19cd4e25/nlc_model.py#L152\n\nThis issue could not be reproduced with simple test cases. tf.nn.softmax() behaves well in simple test cases.\n### What have you tried?\n\n1.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n\nI tensorflow/core/kernels/logging_ops.cc:79] Checking for negative[-29.343697]\nI tensorflow/core/kernels/logging_ops.cc:79] Checking for negative[-26.83346]\nI tensorflow/core/kernels/logging_ops.cc:79] Checking for negative[-29.535496]\nI tensorflow/core/kernels/logging_ops.cc:79] Checking for negative[-26.830235]\nI tensorflow/core/kernels/logging_ops.cc:79] Checking for negative[-26.42857]\n", "comments": ["Can you isolate an input to `tf.nn.softmax` that produces the problematic values?  It would be nice to have a simple test case that exhibits the problem.  Does adding a `tf.Print` to the inputs give you a reproducible bad input?\n", "I have been trying, but on simple graphs things seem to work fine. Whether I add tf.Print or not, my application always witnesses negative values (i.e behavior like tf.nn.log_softmax). It is very consistent in the erroneous behavior.\n", "If it's consistent, it seems like it's a deterministic problem in softmax.  If that's the case, you should be able to get a small reproducible instance via something like:\n\n```\n... lots of complicated graph construction code ...\nlogits = ...\nprobs = tf.nn.softmax(logits)\n... more complicated graph construction code\n\n# Instead of\nnp_output = sess.run(output)\n# do\nnp_output, np_probs, np_logits = sess.run([output, probs, logits])\n\n# np_logits should now be a numpy array giving the reproducible bad input\nsimple_prob = tf.nn.softmax(np_logits).eval()\nassert np.all(simple_prob == np_probs)\n```\n", "Hmm, I found _a_ bug, and it's certainly consistent with your excellent observation about `log_softmax`, but I'm not sure how you're triggering it.  Basically `tf.nn.softmax` and `tf.nn.log_softmax` check the op name to see if it starts with `\"Log\"`, but they should be checking the op type.\n", "`\"Logistic\"`!! :)\n", "Fix coming up.\n", "wow, no wonder i couldn't reproduce it in simple test cases, and it was consistent in the program!\n"]}, {"number": 2314, "title": "SparseApply* operators for the GPU", "body": "At the moment, it appears that only GradientDescentOptimizer supports running on the GPU when there is a SparseTensor update. This is particularly relevant for any RNNs which train their embeddings, including the word2vec example (yes w2v isn't an RNN ;)\n\nSo far the common workaround everyone seems to use is to force the embedding variables to be on the CPU, but there can be _substantial_ speed improvements by allowing them to be stored on the GPU. For one, there is no need to transfer the vectors of embeddings to/from the GPU, and instead one can just transfer the embedding indexes, and then the gradients also don't need to be transferred backwards. In one test I've run where I implemented a version on the GPU, the difference in one epoch was roughly 1100s vs 300s. Basically anyone who doesn't freeze their embeddings can substantially benefit from this.\n\nThis is related to #1310 and #464. In #1310, there is a bug where variables are placed on the GPU, even though an op that appears later isn't available on the GPU. My understanding is a fix for this is under development, but it will only make sure variables are placed on devices which can perform the necessary ops. It will not actually include SparseApply's. This feature request would resolve the OP's reported bug in #1310, but not solve the variable-operator placement issue.\n\nIn #464, the issue is that no one has implemented any `SparseApplyRMSProp`, along with another bug that has since been resolved. The issue used to contain all optimizers except GDO, but since then SparseApply ops have been added for AdaGrad, yet they're currently only implemented for the CPU, not the GPU. This feature request is a rough superset of #464.\n\nOne thing that is presently unclear to me, is whether these really need to be implemented as C++ operators, with all the associated book keeping. Currently that is how the CPU version of the SparseApply*'s are updated (see [example](https://github.com/tensorflow/tensorflow/blob/5681406e2874a02835d34be579810a93ad74a473/tensorflow/core/kernels/training_ops.cc#L1101)), except for [GradientDescent](https://github.com/tensorflow/tensorflow/blob/5681406e2874a02835d34be579810a93ad74a473/tensorflow/python/training/gradient_descent.py#L54). Its `_sparse_apply` is implemented in python and uses `scatter_sub`.\n\nI already have a Python implementation of `_sparse_apply` for `MomentumOptimizer`, which uses `tf.gather`, `tf.scatter_update`, and `tf.scatter_sub`. It passes unit tests, and performs on both CPU and the GPU, though it does take about a 1% performance hit compared to the current CPU C++ `SparseApplyMomentum`. Would this be of interest, or is there a motivation behind implementing this in C++?\n", "comments": ["Apologies, this seems to have fallen through the cracks (@mrry).  We'd be very interested in GPU versions of these ops, but it's unfortunate if we have to sacrifice CPU performance to get them.\n\n@vrv: Can you think of a clean way to use a fused C++ op on CPU but a combination of separate ops on GPU?  I'd be tempted to just accept the fix, but people will probably yell at me if I let a performance regression through.\n", "IIRC, the reason for implementing all of these as C++ ops is due to race conditions between read/compute/update.  If you implement these as separate ops, the time between computing the various compositions of the update rule is much higher than if it is implemented in a single op.  We found this caused problems for multi-replica models due to racy updates, which is why I think most of these ops are fused.  So I'd be a bit concerned about adding the composite op version without sufficient testing.\n\nAnd no, I can't think of an easy way right now to do this, since we don't have a good rewrite infrastructure that's device dependent.\n", "Yeah I didn't think about race conditions on multiple machines; that would be a problem. I also spent a few days trying to implement the operator in C++, but my C++ is relatively weak, I don't know CUDA at all. So that wasn't fruitful.\n\nA week later I was talking to a colleague about this and I noticed that my patch probably also computes updates incorrectly if the same embedding is updated multiple times. Rather than adding all the gradients together, then updating the momentum, and then updating each var exactly once, it would repeatedly update momentum and the variable. As far as I can tell, the current CPU SparseApplyMomentum operator probably has the same bug, but I haven't tested anything; it's just thought experiments.\n\nI also haven't looked at this in a few weeks, so I'd have to rebase the patch and make sure it's still fresh anyway. I'll do that as soon as I get a chance, sometime this week.\n", "@stephenroller: Cool, I'll leave this open as contributions welcome, but based on Vijay's comments it would have to be a unified op to reduce races.  The GPU scatter and gather kernels are reasonably clean, so it wouldn't be too hard to mimic them.\n", "@stephenroller \r\nSorry for resurrecting an old thread. I was just wondering if there are any updates on the SparseApply* issue you identified. ", "Unfortunately my own research ended up taking me in a different direction, so I never got around to this, and probably won't in the immediate future. It would certainly still be welcome on my end though, if someone ever implements this.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Does that mean that there are no plans to implement SparseApply operators for the GPU (or that they've already been added)?  Or is there another issue tracking that feature? I'd just been following this one waiting for updates.", "@drasmuss It's been contributions welcome since November, and the person who said they might do it went a different direction.  We'd be happy to accept pull requests, but we're closing issues where no activity is on the horizon.  Would you like to submit a PR?", "As far as I can tell, this may have actually been addressed by\nhttps://github.com/tensorflow/tensorflow/commit/67443722b26c3585d860d44e7069d997300a7187\nOn Fri, Jun 16, 2017 at 16:07 Geoffrey Irving <notifications@github.com>\nwrote:\n\n> @drasmuss <https://github.com/drasmuss> It's been contributions welcome\n> since November, and the person who said they might do it went a different\n> direction. We'd be happy to accept pull requests, but we're closing issues\n> where no activity is on the horizon. Would you like to submit a PR?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/2314#issuecomment-309133171>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAB8mPyuILCt5QbJoPYz37CVJdLbZZ_Wks5sEu5YgaJpZM4Ibd3q>\n> .\n>\n", "Finally had a chance to check this, and just for anyone else coming across this thread, it looks like SparseApply* ops are still not implemented for the GPU (as of TF 1.14/2.0 beta) E.g., see https://github.com/tensorflow/tensorflow/blob/c174697c09737e543e1919ca5cd5cfc373a96012/tensorflow/core/kernels/training_ops.cc#L3646\r\n\r\n6744372 added support for SparseApply ops with ResourceVariables, but that's still CPU-only."]}, {"number": 2313, "title": "Batched gather", "body": "This is related to #2170, which describes how to use `tf.gather` to index on the first dimension of a tensor. What I'd like to do now is batch this operation:\n\n```\nparams = tf.constant([[1, 2], [3, 4], [5. 6]])\nindices = tf.constant([0, 1, 0])\ngathered = tf.batch_gather(params, indices)\n# gathered = [1, 4, 5]\n```\n", "comments": ["You might be able to do this using `tf.gather_nd()`, but this will ultimately be addressed by the support for enhanced slicing.\n\nClosing as a duplicate of #206.\n"]}, {"number": 2312, "title": "'module' object has no attribute 'atrous_conv2d'", "body": "### Environment info\n\nOperating System: ubuntu 14.04\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n```\n-rw-r--r-- 1 root root 322936 Aug 15  2015 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root     19 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 383336 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18\n-rw-r--r-- 1 root root 720192 Aug 15  2015 /usr/local/cuda/lib64/libcudart_static.a\n```\n### Tensorflow version\n\nlateset one\n### What have you tried?\n\n1.\n\n```\nimport tensorflow as tf\ntf.nn.atrous_conv2d\n```\n", "comments": ["Are you using the 0.8 release or a recent nightly build? `tf.nn.atrous_conv2d()` is only available in the nightly builds.\n", "@mrry   I downloaded the nightly build whl file and installed it by:\n\n```\nsudo pip install tensorflow-0.8.0-cp27-none-linux_x86_64.whl\n```\n\nbut I still could not find \n\n```\ntf.nn.atrous_conv2d()\n```\n"]}, {"number": 2311, "title": "Calling variable.assign() too many times crashes on memory allocation.", "body": "**Background**: I'm working on a set of networks that only share some layers, so I have a parameter server that sends new weights for the different clients to use.  These clients accept the new weights and bias for the layers they are using and assign the values to the TF.Variables via `sess.run(self.w1.assign(new_weights))`.  However, when I start it up and let it run, it crashes saying \n\n```\nW tensorflow/core/common_runtime/bfc_allocator.cc:271] Ran out of memory trying to allocate 16B.  See logs for memory state.\n```\n\n(Sometimes it's allocating 16B, other times its 3.9KiB)\n\nTo give you an idea of the size of the weights, I have three layers of: \nLayer 1(W,b): `(2, 1000), (1000, )`\nLayer 2(W,b): `(1000, 1000), (1000, )`\nLayer 3(W,b): `(1000, 4), (4, )`\nI'm running on a Titan X with 12G memory.\n\nWith `per_process_gpu_memory_fraction = 0.01`, the program dies at ~190 assign commands.\nWith `per_process_gpu_memory_fraction = 0.02`, the program dies at ~384 assign commands.\nWith `per_process_gpu_memory_fraction = 0.03`, the program dies at ~780 assign commands.\nWith `per_process_gpu_memory_fraction = 0.04`, the program dies at ~784 assign commands.\nWith `per_process_gpu_memory_fraction = 0.05`, the program dies at ~1582 assign commands.\nWith `per_process_gpu_memory_fraction = 0.06`, the program dies at ~1586 assign commands.\n\nI've tried to set `allow_growth=True`, and `deferred_deletion_bytes=1` in the session's GPUOptions after reading issue #1578, but that didn't get me much further.  (I have no idea what `deferred_deletion_bytes` does...)  Looking at the numbers just above (GPU%vsAssignmentCommands), it seems to be fairly linear, so it seems to me that the assign operation takes some of the GPU ram and it's never freed up.  **Is there any sense of GC on the GPU memory allocated durring the `var.assign()` op?**\n\nIt seems that I could delete and create a new session, but that sounds expensive to me, and I'd have to maintain the weights outside of session to be able to restore them correctly.  The second idea I had would to use placeholders and ship the weights in every time with the feed_dict, but again, that seems less that ideal and I think it would struggle in the optimizer on knowing what to optimize if they are just placeholders.\n\nLet me know if you would like any other logs or reports from me.  I figure this is the first time someone has tried to use assign operations like this, so I want to be helpful in fixing it if it's a bug.\n\nThanks\n### Environment info\n\n**Operating System:** Ubuntu 16.04\n**Installed version of CUDA and cuDNN:**\n/usr/local/cuda/lib/libcudart.so -> libcudart.so.7.0\n/usr/local/cuda/lib/libcudart.so.7.0 -> libcudart.so.7.0.28\n/usr/local/cuda/lib/libcudart.so.7.0.28\n/usr/local/cuda/lib/libcudart_static.a\n**Built from source.  Commit hash:**  35cd6a30112abd4302a8b3a17dda277899f1ed40\n", "comments": ["Here's a quick script that should break when you run it (if it helps...).  Mine dies on iteration 31.\n[crash_tf_assign_op.py.txt](https://github.com/tensorflow/tensorflow/files/262026/crash_tf_assign_op.py.txt)\n", "The assign op is not consuming memory, but the problem is caused by the fact that each instance of `new_weights` is converted to a constant op, and added to the graph. Each constant op owns a buffer containing the value that it produces, and a constant op on the GPU device will allocate that buffer in GPU memory.\n\nThe fix is to rewrite your program somewhat. Instead of doing:\n\n``` python\nfor i in range(3000):\n    print \"Assigning i:{}\".format(i)\n    sess.run(w1.assign(new_value_array))\n```\n\n... you should declare the assign op and a placeholder before the loop, and feed different values to the placeholder in each iteration:\n\n``` python\nassign_placeholder = tf.placeholder(tf.float32, shape=[1000, 1000])\nassign_op = w1.assign(assign_placeholder)\n\nfor i in range(3000):\n    print \"Assigning i:{}\".format(i)\n    sess.run(assign_op, feed_dict={assign_placeholder: new_value_array})\n```\n", "That totally makes sense now.  I never would have guessed to do that though.  Thanks so much.\n", "Indeed - it's a difficult error to disallow, because there are many totally valid patterns that involve adding nodes to the graph. One tip is to try calling `tf.get_default_graph().finalize()` before your training loop, so that an error will be thrown if you accidentally add a node. (However, we can't do that automatically - e.g. on the first `run()` call - because it would break a huge number of people :( ...)\n", "Thanks @mrry.  Everything is running great again.\n", "Glad to hear it!\n"]}, {"number": 2310, "title": "Bug: tf.app.run() does not allow to import ipdb", "body": "When using tensorflow with `tf.app.run()` one cannot `import ipdb`. I have tested several Tensorflow Versions and did run Tensorflow in several environments, all of them seems to be affected by this Bug.\n\nThe following script provides a minimal Example reproducing the error:\n\n```\n#!/usr/bin/env python\nimport ipdb\nimport tensorflow as tf\n\n\ndef main(_):\n    \"\"\"Run main function.\"\"\"\n    print(\"Hello World.\")\n\n\nif __name__ == '__main__':\n    tf.app.run()\n```\n\nExecuting the  script yields the following error message:\n\n```\nTraceback (most recent call last):\n  File \"tensorapp.py\", line 14, in <module>\n    tf.app.run()\n  File \"/home/marvin/.virtualenvs/tensorvision/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 29, in run\n    main = main or sys.modules['__main__'].main\nAttributeError: 'module' object has no attribute 'main'\n\nIf you suspect this is an IPython bug, please report it at:\n    https://github.com/ipython/ipython/issues\nor send an email to the mailing list at ipython-dev@scipy.org\n\nYou can print a more detailed traceback right now with \"%tb\", or use \"%debug\"\nto interactively debug it.\n\nExtra-detailed tracebacks for bug-reporting purposes can be enabled via:\n    %config Application.verbose_crash=True\n```\n### Environment info\n\nOperating System: Linux (varies)\n\nEffected Tensorflow Versions: 0.7.0; 0.7.1; 0.8.0 \n1. Which pip package you installed. https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\": 0.8.0\n### Steps to reproduce\n1. Run provided script\n", "comments": ["Which version of `ipdb` are you using? It looks like an issue regarding `sys.modules['__main__']` was recently closed: https://github.com/gotcha/ipdb/issues/85.\n", "Version 0.10.0. So the cited patch is already applied.\n\n```\npip freeze | grep ipdb\nipdb==0.10.0\npip freeze | grep ipython\nipython==4.2.0\n```\n", "OK, it looks like `ipdb` does something strange to `sys.modules`:\n\n``` python\nimport sys\nprint sys.modules['__main__']\nimport ipdb\nprint sys.modules['__main__']\n```\n\n...yields the following output:\n\n```\n$ python script.py\n<module '__main__' from 'script.py'>\n<module '__main__' (built-in)>]\n```\n\nI'm not sure if it's feasible to fix this. Two alternatives present themselves:\n1. Call `tf.app.run(main=main)`.\n2. Place the `import ipdb` statement in the `main()` function itself.\n", "@mrry: Feel free to close if you still think fixing it is infeasible.\n", "Closing as infeasible. One of the two [workarounds](https://github.com/tensorflow/tensorflow/issues/2310#issuecomment-218927231) should suffice.\n"]}, {"number": 2309, "title": "Slice all but last element of tensor", "body": "I'm trying to slice all but the last element of a tensor, equivalent to python's `a[:-1]`. However, I understand that tensorflow instead uses a start and size rather than start and end for slicing. My current workaround is to do something like this:\n\n```\na = tf.constant([1, 2, 3, 4])\n# b = [1, 2, 3]\nb = tf.reverse(tf.reverse(a, [True])[1:], [True])\n```\n\nThis is obviously a bit awkward, and potentially inefficient. Is there a better way to do this?\n", "comments": ["I've figured out that I can get the right size even if the shape is `[None]` by using `tf.shape` to get the shape and then subtracting 1 (or whatever desired offset from the end).\n", "As much as I have tested, [tf.strided_slice](https://www.tensorflow.org/api_docs/python/tf/strided_slice) supports negative _end_ indexing, i.e. if you specify -N in any dimension for _end_ parameter, you will slice all but last N elements. This works, but is not documented (even thou negative strides are documented)\r\n\r\nFor example:\r\n```python\r\na = tf.constant([1, 2, 3, 4, 5])\r\nb = tf.strided_slice(test, [0], [-1], [1])\r\nc = tf.strided_slice(test, [0], [-2], [1])\r\n\r\n# shape of b will be (4,)\r\n# shape of c will be (3,)\r\n\r\nd = tf.constant([[1, 2, 3, 4, 5], [1, 2, 3, 4, 5]])\r\ne = tf.strided_slice(test, [0, 0], [2, -2], [1, 1])\r\n\r\n# shape of e will be (2, 3) \r\n```\r\n\r\nAlso, TensorFlow now supports slicing exactly as in numpy, so the flowing would be equivalent to the above examples (and will actually call tf.strided_slice in background as stated in documentation):\r\n\r\n```python\r\na = tf.constant([1, 2, 3, 4, 5])\r\nb = a[:-1]\r\nc = a[:-2]\r\nd = tf.constant([[1, 2, 3, 4, 5], [1, 2, 3, 4, 5]])\r\ne = d[:2, :-2]\r\n```\r\n\r\nEverything works with placeholders with None dimensions also. Similar behavior is not supported by [tf.slice](https://www.tensorflow.org/api_docs/python/tf/slice)."]}, {"number": 2308, "title": "Theano-like scan non-sequences", "body": "It would be nice to add some functionality that allows passing non-sequences to `scan` function (introduced in 0.8.0). [Theano implements this feature](http://deeplearning.net/software/theano/library/scan.html) and it is very useful.\n\nThe suggested usage could be (pseudocode):\n\n``` python\nscan(fn=lambda state, c1, c2, el: state+c1+c2+el,\n     elems=[10,20,30],\n     initializer=[0],\n     non_sequences=[1, 2])\n```\n\nThe first argument could be the last output (state), then all `non_sequences` could be passed and, finally, the current input `el`. The pseudocode above would produce the following operations:\n\n```\n0 +1+2+10 => 13\n13+1+2+20 => 26\n26+1+2+30 => 59\n```\n\nUnfortunatelly I haven't found any hack that would achieve the same behavior (especially when `non_sequences` contain objects other than tensors, such as `GruCells`). Maybe there is some, in that case I would really appreciate some hint :)\n\nI believe that this, together with #2294, would help developers to write their scans equally effectively as they do in theno.\n", "comments": ["Have you tried the following?\n\n```\nc1 = tf.constant(1)\nc2 = tf.constant(2)\nscan(fn=lambda state, el: state+c1+c2+el,\n     elems=[10,20,30],\n     initializer=[0])\n```\n", "@yuanbyu thanks for prompt reply. Well, yes, in such simple case this works.\n\nHowever, imagine that `c1` is `GruCell`. It is forbidden to access such computational nodes that are not passed directly by scan from the inner function (afaik).\n\n@vojtsek could give more description of the problem with scan and using the variable that are not passed by it.\n", "I don't quite understand your `GruCell` example. Could you provide a concrete example?\n", "@yuanbyu Hi, the actual part of code looks like this:\n\n`state_Tbh = scan(fn=process_turn,\n                     elems=tf.transpose(embedded_input_bTte, perm=[1, 0, 2, 3]),\n                     initializer=init_state_bh)`\n\nHere we scan through a sequence of states and inputs. The problem is, that we want to use `GRUCell` inside the `process_turn` function. We don't want to initialize it in each turn. If we pass it from the outer scope like you suggested with the constants, it does not compile.\n", "Is it a python compile error or a TensorFlow graph build error?\n", "@yuanbyu it is tensorflow error that occurs on graph compilation\n", "Would it be possible to create a simple example that could reproduce the problem? It looks like a bug.\n", "@yuanbyu While I was creating the reproducing example I realised, that we have made different kind of error in our code. We just mistakenly thought that this was a problem. My simple example uses the mechanism described above and works just fine, so there is actually no issue to be solved. Sorry for that and thanks for your help.\n", "So just to sum up, TensorFlow offers more comfortable way of passing non-sequences than Theano. Even though I still miss the feature described in #2294, I am happy that this is supported :)\n", "Sorry for reopening this. @petrbel I am looking for the same function. I am implementing a GRULayer from Theano that have a scan function. It's a little bit late but how did you implemented the `non_sequences` part from the scan function? ", "@vojtsek do you have the code for @gaceladri ? I can't find it...", "Hi @petrbel, \r\n\r\nThanks for your response, I have just seen this paper which could be some approximation \r\n\r\n```\r\ndef scan(fn, elems, init):\r\n  elem_ta = TensorArray(dtype=elems.dtype).unstack(elems)\r\n  result_ta = TensorArray(dtype=init.dtype)\r\n  n = elem_ta.size()\r\n  def pred(i, a, ta):\r\n    return i < n\r\n  def body(i, a, ta):\r\n    a_out = fn(a, elem_ta.read(i))\r\n    ta = ta.write(i, a_out)\r\n    return (i + 1, a_out, ta)\r\n    _, _, r = while_loop(pred, body, (0, init, result_ta))\r\n  return r.stack()\r\n```\r\n\r\nFrom this paper \r\n\"Dynamic Control Flow in Large-Scale Machine Learning\"\r\nhttps://arxiv.org/abs/1805.01772\r\nRegards."]}, {"number": 2307, "title": "Tensorflow serving with Retrained Inception graph", "body": "Hello!\n\nAre there any plans to release a tutorial similar to the one to use a checkpointed Inception graph with Tensorflow Serving, but with a Retrained Inception graph as created with the [transfer learning tutorial](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/how_tos/image_retraining/index.md)?\n", "comments": ["In theory it should be possible to reuse the TensorFlow Serving tutorial and just change the name of the output layer to \"final_result\", as shown in the label_image example:\nhttps://www.tensorflow.org/versions/master/how_tos/image_retraining/index.html#using-the-retrained-model\n\nUnfortunately the TensorFlow Serving code creates its own model from scratch, and then loads in the checkpoints to fill in the variables. This means that it's not obvious how to convert a plain GraphDef with the weights as constant ops into their format. I'm passing this over to @jharmsen to see if he can offer any advice?\n", "@jharmsen Do you have any further information on this topic? I am unfortunately still at a standstill.\n", "I am interested in serving a retrained-inception graph as well. Is there a way to 'serve' the .pb retrained file that is an output of retrain.py and is used in label_image? Or a way to serve label_image pointing at a .pb file?\n", "I'm stuck at this point too. Did anyone find a solution?\n", "@nfiedel can you help with this?\n", "@fangweili and I were discussing offline. Now that [SessionBundle](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/session_bundle) is in contrib, the next natural step is to update the inception model / retrain.py to directly use the SessionBundle Exporter. Contributions welcome though we may get to it over the coming weeks.\n", "Trying to do the same.\nWould be awesome to see an update to inception / retrain.py!\n", "Also trying to do the same... I think it's safe to say that there are many others who haven't chimed in but would love for this to be added, or documented steps so that we can implement it ourselves.  Are we at least on the right path to use export_meta_graph and Saver.save() to convert the retrained .pb and label files into the metagraph def and checkpoint files needed for TF serving?\n", "@nfiedel is there update to this topic?\n", "Might not totally be related to the original question but what worked for me as a workaround is to fully train the inception model from scratch and then export it to serving. Of course, that involves the extra resources needed for the full training run but might be a suitable way for some seeking to serve their own classification model.", "Hello guys, i am stuck at this point. I don't have the resources to train from scratch the inception model, although is something that i want to do soon. Any update on this topic is welcome. Thx", "Sorry we don't have the cycles to support that. It's a known problem. It might be worth asking the community on stackoverflow.", "Automatically closing due to recent activity.\r\nAs we moved models out of core TensorFlow repository, this issue may be a better fit for models or serving repositories.", "@petewarden Any walkthrough for the tensorflow serving on custom retrained Inception v3.", "Please watch this video tutorial. It might help you.\r\nhttps://www.youtube.com/watch?v=9l2lkQtinx4&t=14s\r\n"]}, {"number": 2306, "title": "Building from source in Ubuntu 16.04 LTS amd64", "body": "I have installed the `nvidia-cuda-toolkit` from Ubuntu Multiverse package and I wish to compile TensorFlow from source. I realize that I still need CuDNN to be installed the traditional way because its not (yet) packaged by Ubuntu.\n\nThe `configure` scripts has some issues which hinders me to configure it for Compute Capability 3.0.\n### Environment info\n\nOperating System: Ubuntu 16.04 LTS (amd64)\n\nTensorFlow version hash as reported by `git log -1 --oneline`:\n`5681406 Add polygamma and zeta function to tensorflow (#1834)`\n\nInstalled version of CUDA and cuDNN: \n\n```\n# find /usr/lib -name libcud\\*\n/usr/lib/i386-linux-gnu/libcuda.so.1\n/usr/lib/i386-linux-gnu/libcuda.so.361.42\n/usr/lib/i386-linux-gnu/libcuda.so\n/usr/lib/x86_64-linux-gnu/libcuda.so.1\n/usr/lib/x86_64-linux-gnu/libcudart.so.7.5.18\n/usr/lib/x86_64-linux-gnu/libcuda.so.361.42\n/usr/lib/x86_64-linux-gnu/libcudart.so\n/usr/lib/x86_64-linux-gnu/libcudart.so.7.5\n/usr/lib/x86_64-linux-gnu/libcudadevrt.a\n/usr/lib/x86_64-linux-gnu/stubs/libcuda.so\n/usr/lib/x86_64-linux-gnu/libcuda.so\n/usr/lib/x86_64-linux-gnu/libcudart_static.a\n```\n\n```\n# find /usr/local/cuda\n/usr/local/cuda\n/usr/local/cuda/lib64\n/usr/local/cuda/lib64/libcudnn_static.a\n/usr/local/cuda/lib64/libcudnn.so\n/usr/local/cuda/lib64/libcudnn.so.4\n/usr/local/cuda/lib64/libcudnn.so.4.0.7\n/usr/local/cuda/include\n/usr/local/cuda/include/cudnn.h\n```\n### Steps to reproduce\n1. `git checkout https://github.com/tensorflow/tensorflow`\n2. `git checkout 5681406`\n3. `./configure`\n", "comments": ["You should configure with `TF_UNOFFICIAL_SETTINGS=1` in order to allow compute capability 3.0.\n", "I used `grep` to search through the source for that string and I could not find any trace of it being used by the configuration script. I will however try that and report back to make sure I did not miss anything.\n", "@hholst80 You're right, `git grep` didn't turn up anything for me either. This worked for me though on my GTX 660.\n\nI got the solution from https://github.com/tensorflow/tensorflow/issues/25#issuecomment-156234658. Note that the variable is actually `TF_UNOFFICIAL_SETTING` (no \"s\") - sorry!\n", "I made some changes to build TensorFlow 0.8.0 on my workstation which is running Ubuntu 16.04 LTS amd64. The changes can be found here https://git.frostbite.com/hholst/ea-tensorflow\n", "@hholst80 How did you escape the configure:\n\n```\nPlease specify the location where CUDA 7.5 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\nInvalid path to CUDA  toolkit. /usr/local/cuda/lib64/libcudart.so cannot be found\n```\n\nwhen asked for `libcudart.so`?\n\nCause `nvidia-cuda-toolkit` package is installed in `/usr/lib/x86_64-linux-gnu/libcudart.so`\n", "@adamcavendish apparently just use \"/usr\" instead of  \"/usr/local/cuda\" for cuda path.\nHelped me a lot for caffe. Found it on Google Forums. Sorry but cant find the reference anymore.\n", "@prafiles Sorry, I can't make it work. The `configure` file asks me to specify a CUDA path, and it'll look for `THE_CUDA_PATH/lib64/libcudart.so` while the `libcudart.so` is here at `/usr/lib/x86_64-linux-gnu/libcudart.so`, therefore it failed.\n", "@adamcavendish \nSadly I too spent my last day fixing these bugs. Two things came to light. Install CUDA SDK from the .run file. Use the provided driver. And finally use gcc-5 only along with INLINE changes for NVCC.\n\nInstalled Caffe, Theano and Tensorflow as per this. There is a lot of mixture of information available over the internet about this. However removing CUDA SDK from apt and switching to run won't be that hard i guess. (Your mileage may vary)\n\nInstalling CUDA reference:\nhttps://www.pugetsystems.com/labs/articles/NVIDIA-CUDA-with-Ubuntu-16-04-beta-on-a-laptop-if-you-just-cannot-wait-775/\n\nModifying NVCC flags:\nNVCCFLAGS += -D_FORCE_INLINES -ccbin=$(CXX) -Xcompiler -fPIC $(COMMON_FLAGS).\nReference: https://github.com/BVLC/caffe/issues/4046\nCMake method didn't work for me. \n\nI'll try to help you more, however I am not at my CUDA workstation right now. \n", "@prafiles Thx a lot. There's not many choices for us.\n", "Sorry, I did not see that the corperate git repo was firewalled. I copy up the changes to github instead. \n", "https://github.com/hholst80/tensorflow is a snapshot of the customizations I made to build TensorFlow v0.8.0 on Ubuntu 16.04 LTS.\n", "@hholst80 First of all, thanks for sharing your solution. Secondly, I have tried it on Ubuntu 16.04 64 bit, and I manage to run \"./configure\" with no problem, but I get an error when attempting the next step on the tensorflow instructions. Specifically, when I run \"bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\", I get the following:\n`bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\nERROR: /home/alberto/Temporary/tensorflow/tensorflow/tools/pip_package/BUILD:8:1: error loading package 'tensorflow/core': Extension file not found. Unable to load package for '//google/protobuf:protobuf.bzl': BUILD file not found on package path and referenced by '//tensorflow/tools/pip_package:other_headers_gather'.\nERROR: Loading failed; build aborted.\nINFO: Elapsed time: 0.105s\n`\nI am a n00b for this, could you help me understand the error?\n", "@albertotonda I have not maintained that version or looked at it since this. I can try and get some time to return to this later, but time and talent is as always a limiting factor. I think that any solution used for this should rely on the official Cuda repository provided by NVIDIA:\n\n```\n$ cat /etc/apt/sources.list.d/cuda.list \ndeb http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64 /\n```\n", "Dear Henrik,\n\nactually, I managed to solve the issue and/or it was addressed in one of\nthe later releases (I cannot remember, exactly); point is, it's now working.\n\nThanks anyway and best regards.\n\nOn Mon, Nov 14, 2016 at 1:33 PM, Henrik Holst notifications@github.com\nwrote:\n\n> @albertotonda https://github.com/albertotonda I have not maintained\n> that version or looked at it since this. I can try and get some time to\n> return to this later, but time and talent is as always a limiting factor. I\n> think that any solution used for this should rely on the official Cuda\n> repository provided by NVIDIA:\n> \n> $ cat /etc/apt/sources.list.d/cuda.list\n> deb http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64 /\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2306#issuecomment-260323666,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AGb6kRqG6BwQj9g87YnxMLOdnCaxDrdzks5q-FUzgaJpZM4IbO5B\n> .\n\n## \n\nAlberto Tonda\n", "Looks like this has been resolved. Closing this issue."]}, {"number": 2305, "title": "FR: Create/Support an automated hyperparameter selector like TPOT", "body": "Not sure if you've seen TPOT (http://rhiever.github.io/tpot/, introductory blog post at http://www.randalolson.com/2016/05/08/tpot-a-python-tool-for-automating-data-science/) but it would be awesome if you collaborated with them to add something like this to TensorFlow.\n", "comments": ["Closing because this issue is too broad and keeping it open is unlikely to be helpful. "]}, {"number": 2304, "title": "Tensorflow retrain.py random distortions", "body": "### Environment info\n\nOperating System: Mac OS X with Tensorflow 0.8 installed.\n\nI am trying to run retrain.py with my custom training set, but whenever I try passing in the argument `--random_crop 5` when running retrain.py the program errors out and says:\n\n> tensorflow.python.framework.errors.InvalidArgumentError: assertion failed: [Need value.shape >= size, got ] [314 314 1] [299 299 3]\n>    [[Node: random_crop/Assert = [Assert T=[DT_STRING, DT_INT32, DT_INT32], summarize=3, _device=\"/job:localhost/replica:0/task:0/cpu:0\"(random_crop/All, random_crop/Assert/data_0, random_crop/Shape, random_crop/size)]]\n> Caused by op u'random_crop/Assert'\n\nThe command I am using is:\n\n`bazel-bin/tensorflow/examples/image_retraining/retrain --image_dir /path/to/my/training_images --how_many_training_steps 5000 --random_crop 5`\n\nIs this an issue with the size of the images that I am using for my training set?\n", "comments": ["Sorry you're hitting problems. Are you able to reproduce the same error with the standard flowers set of photos? If you can, that will help me track down what's going wrong, thanks!\n", "Thanks for taking the time to respond and look into it! I was not able to reproduce the same error with the standard flowers set of photos. In fact, I was stumped for a while as I noticed all the images within the flowers set are all .jpg's, but my photos are all also .jpg's. When looking at the metadata, I noticed for all JPEGs that are EXIF standard instead of JFIF standard, Tensorflow will break when trying the distortions.  However, when removing all the photos with EXIF standard, the program runs successfully.\n", "Thanks for investigating! Can you upload one of the EXIF files that's causing the error and send me a link? I'll try to reproduce the problem here.\n", "Hey guys, \nI ran into the same problem and have a fix ready. You seem to have problems with grayscale pictures that have one channel only.\nThe fix is available in this PR\nhttps://github.com/tensorflow/tensorflow/pull/2349#issuecomment-219077419 \nand you can find the fix in the marked line:\nhttps://github.com/tensorflow/tensorflow/pull/2349/files#diff-e27def9997dcd569e1f2412fd6f3ae76R619\n(I load every image as a 3 channel image)\n", "Hey, my pull request was accepted. Please check if your problem remains. \n", "Closing this as presumed fix, since it's been a while. Please reopen or let me know if it's still an issue.\n"]}, {"number": 2303, "title": "Error parsing bazel version (bazel 0.2.2b)", "body": "TensorFlow version: v0.8.0\nBazel version: 0.2.2b (installed via deb package)\n\nCommand: `bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer --verbose_failures`\nError message:\n\n```\nERROR: /path_to_my_workspace/tensorflow/WORKSPACE:21:1: Traceback (most recent call last):\n    File \"/path_to_my_workspace/tensorflow/WORKSPACE\", line 21\n        check_version(\"0.1.4\")\n    File \"/path_to_my_workspace/tensorflow/tensorflow/tensorflow.bzl\", line 22, in check_version\n        _parse_bazel_version(native.bazel_version)\n    File \"/path_to_my_workspace/tensorflow/tensorflow/tensorflow.bzl\", line 15, in _parse_bazel_version\n        int(number)\ninvalid literal for int(): \"2b\".\nERROR: Error evaluating WORKSPACE file.\nERROR: no such package 'external': Package 'external' contains errors.\n```\n\nThe error seems to be `_parse_bazel_version` not expecting a non decimal character.\n", "comments": ["I cannot believe there's another person suffering with same problem, just 30 minutes ago!\n", "It seems to be fixed in current master. \nhttps://github.com/tensorflow/tensorflow/commit/3be665fb87ff12c90766afc7ceed63b3c7c69317\n", "Have you tried bazel 0.2.2?\n", "Yes. Bazel 0.2.2 worked perfectly.\n", "Are you synced to master?  Or are you on the 0.8 branch?  This is fixed at HEAD, so there should be no reason to downgrade to Bazel 0.2.2 at HEAD on master.\n"]}, {"number": 2302, "title": "Fixed Tensorboard minimap not being drawn on Safari (OS X, iOS)", "body": "Fixed Tensorboard minimap not being drawn on Safari (OS X, iOS)\nTested on Chrome (OS X), Safari (OS X, iOS)\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n", "This patch also fixes \"Download PNG\" on Safari.\n", "@dsmilkov: look ok?  We'll merge if so.\n", "Thanks for trying to fix this!\n\nI tested it and the minimap doesn't work on Firefox now. Can you get a solution that works on Chrome/Firefox/MacOS? Thanks!\n", "@qbx2 I did some testing and having the old way as a fallback makes it work on Firefox/Safari/Chrome. Can you change your commit to:\n\n```\nimage.src = 'data:image/svg+xml;charset=utf-8,' + svgXml.replace(/\\n/g,'');\nimage.onerror = () => {\n  let blob = new Blob([svgXml], {type: 'image/svg+xml;charset=utf-8'});\n  image.src = URL.createObjectURL(blob);\n};\n```\n", "@dsmilkov I committed as you suggested. Test please. Thank you.\n", "Thanks! One last question: any reason why in the new commit, you removed the \"replace\" as in svgXml.replace(/\\n/g,''); Is that not needed? Thanks!\n", "I used \"replace\" for debugging because URL must not contain any new-line characters. After checking if it would work if i remove \"replace\", I removed it for clearer code because both worked well.\n", "Great - was just curious! @vrv Good to merge!\n", "Url-encoding svgXml data would be better!\n", "there are conflicts unfortunately -- can you rebase so we can test and merge?\n", "We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@vrv test please.\n", "@tensorflow-jenkins test this please\n"]}, {"number": 2301, "title": "Contact for legal issues", "body": "Good night\n\nCurrently my university wants to use Tensorflow for a software project. The lawyers of my university want a letter which says TensorFlow creators allow my university to use his library. Any idea who must I contact for this issue? Thanks a lot.\n", "comments": ["I'm not a contributer or lawyer, but:\nhttps://github.com/tensorflow/tensorflow/blob/master/LICENSE\n\nMaybe this can be the \"letter\"?\n\nIt's an apache license, which means you can basically use it (also commercially), you can redistribute it, modify it and are not required to publish your changes or your source code.\n\nhttps://tldrlegal.com/license/apache-license-2.0-(apache-2.0)\n", "TensorFlow is an Apache licensed open source project, so there is no need for a specific permission grant (nor do we provide such).\n"]}, {"number": 2300, "title": "what is the format of model when using save", "body": "Can it possbile to give the doc about the format of model when using save function?\n\nHow can I define the self-model,just only using fwrite ,and read-model using fread?\n", "comments": ["I'm not sure if it answers your question, but here is some documentation on the GraphDef fileformat we use to save models:\nhttps://www.tensorflow.org/versions/master/how_tos/tool_developers/index.html\n"]}, {"number": 2299, "title": "Branch 121885139", "body": "Merge Google changes into open-source.\n", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "@tensorflow-jenkins test this please\n", "@tensorflow-jenkins test this please\n", "@tensorflow-jenkins test this please\n", "jenkins seems to be complaining about the change in the google/protobuf submodule hash. @petewarden , PTAL\n\nhttp://ci.tensorflow.org/job/tensorflow-pull-requests-multijob/824/console\n\n> FATAL: Command \"git submodule update --init --recursive\" returned status code 1:\n> stdout: \n> stderr: Cloning into 'google/protobuf'...\n> fatal: reference is not a tree: f4c8a518c9f3123d144bafcac844cbba2a63d0f1\n> Unable to checkout 'f4c8a518c9f3123d144bafcac844cbba2a63d0f1' in submodule path 'google/protobuf'\n", "ping for @petewarden \n", "I'm investigating what's going wrong with our internal change porting process, and I'll update once I know more.\n"]}, {"number": 2298, "title": "opt.compute_gradients() returns values different from the weight difference of opt.apply_gradients()", "body": "This isn't a bug, bug I wasn't sure where to ask for help on this one, so I'm sorry to just throw in another issue here on github.  I've tried searching, but haven't found answers to my questions yet.\n\nMy question is what is the most efficient way to get the delta of my weights when training?  I assume it's related to gradients, but the optimizer (I think) does a lot more than just apply the raw gradients returned from `opt.compute_gradients()`.\n\n**Where I'm at**: I've got the operators hooked up as follows (thanks to this [SO question](http://stackoverflow.com/questions/34687761/efficiently-grab-gradients-from-tensorflow)):\n\n<pre>\nself.cost = `the rest of the network`\nself.rmsprop = tf.train.RMSPropOptimizer(lr,rms_decay,0.0,rms_eps)\nself.comp_grads = self.rmsprop.compute_gradients(self.cost)\nself.grad_placeholder = [(tf.placeholder(\"float\", shape=grad[1].get_shape(), name=\"grad_placeholder\"), grad[1]) for grad in self.comp_grads]\nself.apply_grads = self.rmsprop.apply_gradients(self.grad_placeholder)\n</pre>\n\n\nNow, to feed in information, I run the following:\n\n<pre>\nfeed_dict = `training variables`\nstart_weights = self.sess.run([self.w1,self.b1, etc])\ngrad_vals = self.sess.run([grad[0] for grad in self.comp_grads], feed_dict=feed_dict)\n\nfeed_dict2 = `feed_dict plus gradient values added to self.grad_placeholder`\nself.sess.run(self.apply_grads, feed_dict=feed_dict2) # Updates the weights\nend_weights = self.sess.run([self.w1,self.b1, etc])\ndelta_weights = [end_weights[i]-start_weights[i] for i in range(len(start_weights))]\n# Note: delta_weights != grad_vals...\n</pre>\n\n\nThe command of `run(self.apply_grads)` will update the network weights, but when I compute the differences in the starting and ending weights (`run(self.w1)`), those numbers are different than what is stored in `grad_vals[0]`.  I figure this is because the RMSPropOptimizer does more to the raw gradients, but I'm not sure what, or where to find out what it does.  I could go to the paper and write it myself, but I don't want to hardcode things in like that where TF already does it somewhere else.  I'll admit I'm still new to TF, but I tried to figure out what happens by looking at the source, but it's still confusing for me.\n\nSo back to the question: Is there a way to get the delta's for the weights in a more efficient way than calculating the difference?  Am I stuck running `self.w1.eval(sess)` multiple times to get the weights and calculate the difference?  Is there something that I'm missing with the `tf.RMSPropOptimizer` (or any other Optimizer for that matter) function that I haven't seen or heard about\n\nThanks!  And sorry again for adding to the issues list.\n", "comments": ["Tensorflow has the code calculates the difference in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/training_ops.cc#L143 , so look at it.\nAdditionally, I see that there are no python interfaces for it now.\n", "Thanks for the pointer @qbx2, I'll look there and come back if I have any problems still.\n"]}, {"number": 2297, "title": "Fix botched merge in conv_grad_ops.cc.", "body": "We returned in the wrong location, so that we ended up doing twice the\nwork: the optimal gemm and then the suboptimal conv (for 1x1 filter).\n\nNo tests failed because the result was the same.\n", "comments": ["LGTM\n"]}, {"number": 2296, "title": "Issue with retrain.py, \"could not convert string to float\" when creating bottlenecks", "body": "### Environment info\n\nOperating System:\nUbuntu 14.04 \nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\nCUDA 7.5, cuDNN v2 (6.5)\n`lrwxrwxrwx 1 root root 19 May  9 09:11 /usr/local/cuda -> /usr/local/cuda-7.5`\n\nInstalled the Nightly pip package from April 12th, with GPU support\nTensorflow version: 0.7.1\n\nIf installed from sources, provide the commit hash:\n### Steps to reproduce\n1. Running the retrain with a very large image directory (around 600k files) causes the training to fail around 3/4 the way through, during the bottleneck creation process. It always seems to fail while within the same label folder, but it does not appear that any file names within that label folder are corrupted or named incorrectly.\n### What have you tried?\n1. Changing the number of training steps\n2. Checking to see if any files are less than 30k (theoretically, very small files are likely to be corrupted jpeg data)\n### Logs or other output that would be helpful\n\nFor 610,000 files, the process can't seem to get through more than 415k. Am I simply just using too many files? Or am I missing a very, very subtle naming convention issue?\n## Full stack trace:\n\n`File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"retrain_reshape_error.py\", line 695, in main\n    cache_bottlenecks(sess, image_lists, FLAGS.image_dir, FLAGS.bottleneck_dir, jpeg_data_tensor, bottleneck_tensor)\n  File \"retrain_reshape_error.py\", line 400, in cache_bottlenecks\n    image_dir, category, bottleneck_dir, jpeg_data_tensor, bottleneck_tensor)\n  File \"retrain_reshape_error.py\", line 372, in get_or_create_bottleneck\n    bottleneck_values = [float(x) for x in bottleneck_string.split(',')] \nValueErrror: could not convert string to float`\n\nI should also note that I have been using this training process for the last few months with absolutely no failure.\n\nThanks!\n\nOren\n", "comments": ["I don't think you are using too many files, however using large amounts of data makes it more likely to expose underlying bugs. The error indicates that \"bottleneck_string\" contains some non-numeric characters. Not familiar with that part of code, maybe @petewarden  has an idea how non-digits could've snuck into `bottleneck_string`?\n", "@yaroslavvb Yup, I had the same idea. Corrupted JPEG files would theoretically be missing the JPEG \"tag\" at beginning of file, right? I tried looking for that tag specifically but the process took prohibitively long on my machine.\n", "Yes, it probably could be missing. @dave-andersen had fun dealing with various forms of corruption when trying to reprocess imagenet dataset. Some \".jpg\" files in the set were actually \".png\" files in disguise\n", "@yaroslavvb the one issue is that I've created all of the files through an automated process, so it doesn't quite make sense that just one would be corrupted.\n", "@yaroslavvb Do you think, though, that there would be a hard-cap limit for number of images trained using this transfer learning approach? Eventually I would like to build my own CNN (or train Inception from scratch) but I'll likely have to wait for the new GTX1080 to be able to do that with any speed\n", "There shouldn't be a theoretical cap, but there could be practical caps. In my own experiments I saw overfitting happening when using <1M images. However, when increasing to 20M, I didn't see overfitting after hundreds of millions of steps, so I've been using it as personal rule of thumb for desired number of training images. I'm expecting GTX1080 to be 2x faster than TitanX so it's not a great panacea. It should go beyond 2x speedup for float16, but probably will require adjustments to training procedure. \n", "@yaroslavvb Interesting. Another question\u2014albeit a stupid one\u2014should the number of training steps reflect the number of images 1-1? As in, 600k images, 600k training steps?\n", "If you had something like 1B images, then you could train for one epoch\nsince another epoch would be 2x longer but a only small improvement in accuracy\nBut for 600k you probably will need several passes to get convergence (a\nfew dozen epochs at batch size 32 maybe?). I don't remember the details off\nthe top of my head, but ImageNet papers should talk about how many epochs\nthey used\n\nOn Mon, May 9, 2016 at 2:06 PM, oren weingrod notifications@github.com\nwrote:\n\n> @yaroslavvb https://github.com/yaroslavvb Interesting. Another\n> question\u2014albeit a stupid one\u2014should the number of training steps reflect\n> the number of images 1-1? As in, 600k images, 600k training steps?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2296#issuecomment-217989795\n", "@yaroslavvb I'll take a look there. Thanks a lot!\n", "No problem, closing this for now since not sure what can be done on our side, but feel free to re-open if you are able to get more debugging info (ie, the contents of `bottleneck_string` for the failing example)\n", "Attempting to grab that data now.\n", "I have a helper program I used to validate all of the imagenet files, but it's a little too Google-specific to be useful more broadly.  The heart of it, though, was just:\n\n... do something to read in your JPEG into a string called image_buffer ...\n\n  features = tf.parse_single_example(example_serialized, feature_map)\n  image_buffer = features['image']\n  filename = features['filename']\n\n  image_saved = tf.Variable('')\n  save_image = image_saved.assign(image_buffer)\n  decode_saved_image = tf.image.decode_jpeg(image_saved)\n\n  init = tf.initialize_all_variables()\n  with tf.control_dependencies([decode_saved_image]):\n    force_decode_no_copy = tf.no_op()\n\n  with tf.Session() as sess:\n    sess.run(init)\n    tf.start_queue_runners()\n\n```\ntry:\n  while True:\n    key, fn, _ = sess.run([exid, filename, save_image])\n    try:\n      _ = sess.run(force_decode_no_copy)\n    except:\n      print \"Error decoding \", key, fn\n\nexcept tf.errors.OutOfRangeError:\n  print \"OK\"\n```\n\nYou just need a wrapper mechanism to run a bunch of parallel threads for this and handle the input depending on how you're getting it all read in.\n", "@yaroslavvb new information! I tried catching the error and continuing after skipping the image file in question.\n\n`Creating bottleneck at ~/tmp/bottleneck/832764-001/3-output00000727.jpg.txt\nE tensorflow/stream_executor/cuda/cuda_driver.cc:932] failed to allocate 190.44M (199688192 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY`\n\nSo, looks like it was a memory issue causing the ValueError. How can I free up more GPU memory during the process? I have ~4GB.\n\nShould note this was after creating ~400k bottleneck caches, and with no other processes using GPU memory on the machine.\n\nEDIT: After leaving the crashed and out of memory Tensorflow process running, I saw this new error:\n\nEDIT 2: Well, that actually was a memory error. Clearing the memory and restarting seemed to fix, for now.\n", "@oweingrod: What's the status of this?  It seems to have fallen through the cracks.  Cc @petewarden.\n", "Since there's been no update on this for a while, closing for now. Please reopen if the issue is still occurring.\n", "Hello, I am having the same issue. I am using a docker container to train a dataset of almost 101000 images. Tensorflow starts creating bottlenecks of the images and after creating 15500 bottleneck files crashes with the following error:\r\n\r\n```\r\n15500 bottleneck files created.\r\nTraceback (most recent call last):\r\n  File \"tensorflow/examples/image_retraining/retrain.py\", line 1012, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 43, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"tensorflow/examples/image_retraining/retrain.py\", line 782, in main\r\n    jpeg_data_tensor, bottleneck_tensor)\r\n  File \"tensorflow/examples/image_retraining/retrain.py\", line 440, in cache_bottlenecks\r\n    jpeg_data_tensor, bottleneck_tensor)\r\n  File \"tensorflow/examples/image_retraining/retrain.py\", line 405, in get_or_create_bottleneck\r\n    bottleneck_values = [float(x) for x in bottleneck_string.split(',')]\r\nValueError: could not convert string to float:\r\n```\r\n\r\nI am not really sure that it is a memory issue. Even if so, how should I try and clean it? I am using Windows 10 64-bit with Docker 1.13", "So I placed some checks in the retrain.py file and its working fine now. From What I understand this is not because of a faulty image file but a faulty bottleneck file. As soon as the error was caught by my checks, the script started creating new bottleneck files instead of checking the cached ones. \r\n\r\nIn other words, if an incomplete bottleneck file exists in the cache then this error arises  (this file can be produced because of sudden system shutdown).\r\n\r\nPlease add some checks to the code. I'll paste my solution below:\r\n\r\n```\r\n#retrain.py\r\n...\r\ndef get_or_create_bottleneck(sess, image_lists, label_name, index, image_dir,\r\n                             category, bottleneck_dir, jpeg_data_tensor,\r\n                             bottleneck_tensor):\r\n...\r\n  with open(bottleneck_path, 'r') as bottleneck_file:\r\n    bottleneck_string = bottleneck_file.read()\r\n  \r\n  #avoiding non float values\r\n    try:\r\n      bottleneck_values = [float(x) for x in bottleneck_string.split(',')]\r\n    except:\r\n      print(\"Invalid float found, sending None instead\")\r\n      return None\r\n\r\n  return bottleneck_values\r\n\r\n...\r\ndef cache_bottlenecks(sess, image_lists, image_dir, bottleneck_dir,\r\n                      jpeg_data_tensor, bottleneck_tensor):\r\n...\r\n      for index, unused_base_name in enumerate(category_list):\r\n        bNeck = get_or_create_bottleneck(sess, image_lists, label_name, index,\r\n                                 image_dir, category, bottleneck_dir,\r\n                                 jpeg_data_tensor, bottleneck_tensor)\r\n        if bNeck is not None:\r\n          how_many_bottlenecks += 1\r\n          if how_many_bottlenecks % 100 == 0:\r\n            print(str(how_many_bottlenecks) + ' bottleneck files created.')\r\n\r\n...\r\ndef get_random_cached_bottlenecks(sess, image_lists, how_many, category,\r\n                                  bottleneck_dir, image_dir, jpeg_data_tensor,\r\n                                  bottleneck_tensor):\r\n...\r\n  for unused_i in range(how_many):\r\n    label_index = random.randrange(class_count)\r\n    label_name = list(image_lists.keys())[label_index]\r\n    image_index = random.randrange(MAX_NUM_IMAGES_PER_CLASS + 1)\r\n    bottleneck = get_or_create_bottleneck(sess, image_lists, label_name,\r\n                                          image_index, image_dir, category,\r\n                                          bottleneck_dir, jpeg_data_tensor,\r\n                                          bottleneck_tensor)\r\n    if bottleneck is not None:\r\n      ground_truth = np.zeros(class_count, dtype=np.float32)\r\n      ground_truth[label_index] = 1.0\r\n      bottlenecks.append(bottleneck)\r\n      ground_truths.append(ground_truth)\r\n  return bottlenecks, ground_truths\r\n\r\n...\r\ndef get_random_distorted_bottlenecks(\r\n    sess, image_lists, how_many, category, image_dir, input_jpeg_tensor,\r\n    distorted_image, resized_input_tensor, bottleneck_tensor):\r\n...\r\n    bottleneck = run_bottleneck_on_image(sess, distorted_image_data,\r\n                                         resized_input_tensor,\r\n                                         bottleneck_tensor)\r\n    if bottleneck is not None:\r\n      ground_truth = np.zeros(class_count, dtype=np.float32)\r\n      ground_truth[label_index] = 1.0\r\n      bottlenecks.append(bottleneck)\r\n      ground_truths.append(ground_truth)\r\n  return bottlenecks, ground_truths"]}, {"number": 2295, "title": "pow() gradients create +inf at 0.0", "body": "Hi,\n\nI had a pipeline where I was first clamping to [0..1] and then doing x^2.4 (converting to/from gamma RGB to linear RGB):\n\n```\ny = tf.minimum(tf.maximum(y, 0.0), 1.0)\ngamma = tf.constant(1.0/2.4, tf.float32)\ny = tf.pow(y, gamma)\n```\n\nHowever, this instantly created inf values (and NaNs in the next step), seemingly due to the use of log() to compute the gradients. The gradient of pow() in 0.0 should be pretty well defined as long as the exponent is nonzero; at the very least, it is not infinity.\n\nAs a workaround, clamping to 1e-5 instead of 0.0 caused the NaNs to disappear.\n", "comments": ["Just out of curiosity, what is the gradient value at y = 1e-5 then?\n", "I have no idea, I never printed them out. But the graph trained just fine.\n", "The fast way to solve this is an add an extra op that does `x * log y` and does the right thing for `x = y = 0`.  The slow way is to do that in Python via `tf.select(tf.equal(x, 0), 0, x * tf.log(y))`.  Unfortunate that simplistic code doesn't work out of the box because `tf.select` doesn't broadcast.\n\nThe simple way to solve it is to use `x * tf.log(y + y.dtype.epsilon)` in the gradient.  Probably we should just do that.\n\n@sesse: I'm happy to review if you want to make the change, but won't be able to work on this soon.\n"]}, {"number": 2294, "title": "Feature Request: Let tf.scan accept/return tuples of Tensors in addition to just Tensors", "body": "Right now, `tf.scan` splits `Tensors` along dimension 0, and to process multiple state/input `Tensors` with `tf.scan`, we need to concatenate/split. For example, to create an LSTM block using `tf.scan`, we might have something like\n\n```\ndef lstm_block(c_prev_and_m_prev, x):\n    c_prev, m_prev = tf.split(1, 2, c_prev_and_m_prev)\n    # Compute new c, m\n    c_and_m = tf.concat(1, [c, m])\n    return c_and_m\n```\n\nThis has been asked about by others in https://github.com/tensorflow/tensorflow/issues/1725 and https://github.com/rdipietro/jupyter-notebooks/issues/2\n\nAlso, if we hope to process multiple `Tensors` with significantly different shapes, then the current solution will become even less concise, as we'd probably have to do something like flatten, maintain shape info, unflatten, etc.\n\nI can probably make this change if it's something that'd be accepted.\n", "comments": ["You are welcome to contribute.  We will accept it if it passes our review process. \n", "+ebrevdo@. One way to do it is to extend TensorArray a bit so we could convert between a (python) list of tensors and TensorArray.  It would also be nice to change session slightly so we could return a list of tensors for a single output. \n", "Is anybody working on this?\nOr is there a workaround to do this feature, in case that tensors have different types?\nI've been stuck implementing Viterbi-like decoding algorithm due to the fact that I can't simultaneously save argmax and max value...\n", "@jihunchoi I don't know that anyone is working on it currently.\n", "We are working on more generalized nesting in general; then I'll pick this up in a couple of weeks once we have that in.\n", "Ping :).\n\n@jihunchoi for right now I just recommend using `tf.while_loop`. The accompanying `TensorArray` stuff is a bit messy, but you can create as many `TensorArrays` as you need, even with different-shaped `Tensors` inside, bypassing the problems with the higher-level `tf.scan`.\n", "We just pushed support for this request. Can you confirm you see it at head?\nOn Jul 6, 2016 6:56 AM, \"Robert DiPietro\" notifications@github.com wrote:\n\n> Ping :).\n> \n> @jihunchoi https://github.com/jihunchoi for right now I just recommend\n> using tf.while_loop. The accompanying TensorArray stuff is a bit messy,\n> but you can create as many TensorArrays as you need, even with\n> different-shaped Tensors inside, bypassing the problems with the\n> higher-level tf.scan.\n> \n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2294#issuecomment-230778559,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/ABtim_sJ5I9eKySIJe-jidq9o5UiachNks5qS7QbgaJpZM4IaWN3\n> .\n", "That's great \u2013 thanks.\n\nI'm not seeing the change yet though ... changes must have been made to `functional_ops.py`, yes?\n", "Closing due to inactivity. Feel free to open a new github issue if the problem still persists in recent versions."]}, {"number": 2293, "title": "Documentation update: virtualenv should not be run from ~", "body": "Hi,\n\nThe documentation suggests that you make a virtualenv essentially by \u201ccd ~; mkdir tensorflow; virtualenv ~/tensorflow\u201d. Unfortunately, for some reason I cannot comprehend, this makes virtualenv try to scan every HTML file (possibly also others) in ~; in my case, this hit a 22 MB file I had lying around, causing virtualenv to use 5+ minutes (I eventually aborted it) and 4.5 GB of RAM without really saying why.\n\nA better workaround is \u201ccd ~; mkdir tensorflow; cd tensorflow; virtualenv .\u201d, since seemingly the scanning is of the current directory. The documentation should probably be updated.\n", "comments": ["Is it possible that you had . in your $PATH or $path (in csh)? That's not advised for security reasons, and it might confuse virtualenv which is using your system versions of software (which is probably found by using $PATH).\n", "Automatically closing due to lack of recent activity. Please reopen when further information becomes available.\n"]}]