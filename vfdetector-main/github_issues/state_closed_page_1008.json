[{"number": 23116, "title": "1.12.0-rc2 cherry-pick request: Check for the presence of a Worker machine when reassigning hooks in \u2026", "body": "\u2026distributed training jobs.\r\n\r\nPiperOrigin-RevId: 217407558", "comments": []}, {"number": 23115, "title": "Cannot build Tensorflow android example with Android Studio", "body": "When I build the project in android studio it goes through with just 1 warning asking me to not specify the JDK version in the manifest file. I don't think this is the cause of the error, and for now I have left it as is.\r\n\r\nWhen I run the project on a virtual device (tried pixel 2 with Android Pie, and Nexus 5X with Android 7), both give the same error stack. Here it is below. Help would be GREATLY appreciated. I have tried compiling the example on both mac and linux, and both seem to fail. \r\n\r\n```\r\nExecuting tasks: [:assembleDebug]\r\n\r\n:buildInfoDebugLoader\r\n:checkDebugClasspath UP-TO-DATE\r\n:preBuild UP-TO-DATE\r\n:preDebugBuild UP-TO-DATE\r\n:compileDebugAidl NO-SOURCE\r\n:compileDebugRenderscript UP-TO-DATE\r\n:checkDebugManifest UP-TO-DATE\r\n:generateDebugBuildConfig UP-TO-DATE\r\n:prepareLintJar UP-TO-DATE\r\n:mainApkListPersistenceDebug UP-TO-DATE\r\n:generateDebugResValues UP-TO-DATE\r\n:generateDebugResources UP-TO-DATE\r\n:mergeDebugResources UP-TO-DATE\r\n:createDebugCompatibleScreenManifests UP-TO-DATE\r\n:processDebugManifest\r\n:splitsDiscoveryTaskDebug UP-TO-DATE\r\n:processDebugResources\r\n:generateDebugSources\r\n:javaPreCompileDebug\r\n/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/SpeechActivity.java:54: error: package org.tensorflow.contrib.android does not exist\r\nimport org.tensorflow.contrib.android.TensorFlowInferenceInterface;\r\n                                     ^\r\n/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/SpeechActivity.java:95: error: cannot find symbol\r\n  private TensorFlowInferenceInterface inferenceInterface;\r\n          ^\r\n  symbol:   class TensorFlowInferenceInterface\r\n  location: class SpeechActivity\r\n/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/StylizeActivity.java:57: error: package org.tensorflow.contrib.android does not exist\r\nimport org.tensorflow.contrib.android.TensorFlowInferenceInterface;\r\n                                     ^\r\n/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/StylizeActivity.java:117: error: cannot find symbol\r\n  private TensorFlowInferenceInterface inferenceInterface;\r\n          ^\r\n  symbol:   class TensorFlowInferenceInterface\r\n  location: class StylizeActivity\r\n/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowImageClassifier.java:30: error: cannot find symbol\r\nimport org.tensorflow.Operation;\r\n                     ^\r\n  symbol:   class Operation\r\n  location: package org.tensorflow\r\n/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowImageClassifier.java:31: error: package org.tensorflow.contrib.android does not exist\r\nimport org.tensorflow.contrib.android.TensorFlowInferenceInterface;\r\n                                     ^\r\n/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowImageClassifier.java:57: error: cannot find symbol\r\n  private TensorFlowInferenceInterface inferenceInterface;\r\n          ^\r\n  symbol:   class TensorFlowInferenceInterface\r\n  location: class TensorFlowImageClassifier\r\n/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowMultiBoxDetector.java:32: error: cannot find symbol\r\nimport org.tensorflow.Graph;\r\n                     ^\r\n  symbol:   class Graph\r\n  location: package org.tensorflow\r\n/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowMultiBoxDetector.java:33: error: cannot find symbol\r\nimport org.tensorflow.Operation;\r\n                     ^\r\n  symbol:   class Operation\r\n  location: package org.tensorflow\r\n/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowMultiBoxDetector.java:34: error: package org.tensorflow.contrib.android does not exist\r\nimport org.tensorflow.contrib.android.TensorFlowInferenceInterface;\r\n                                     ^\r\n/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowMultiBoxDetector.java:63: error: cannot find symbol\r\n  private TensorFlowInferenceInterface inferenceInterface;\r\n          ^\r\n  symbol:   class TensorFlowInferenceInterface\r\n  location: class TensorFlowMultiBoxDetector\r\n/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowObjectDetectionAPIModel.java:31: error: cannot find symbol\r\nimport org.tensorflow.Graph;\r\n                     ^\r\n  symbol:   class Graph\r\n  location: package org.tensorflow\r\n/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowObjectDetectionAPIModel.java:32: error: cannot find symbol\r\nimport org.tensorflow.Operation;\r\n                     ^\r\n  symbol:   class Operation\r\n  location: package org.tensorflow\r\n/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowObjectDetectionAPIModel.java:33: error: package org.tensorflow.contrib.android does not exist\r\nimport org.tensorflow.contrib.android.TensorFlowInferenceInterface;\r\n                                     ^\r\n/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowObjectDetectionAPIModel.java:62: error: cannot find symbol\r\n  private TensorFlowInferenceInterface inferenceInterface;\r\n          ^\r\n  symbol:   class TensorFlowInferenceInterface\r\n  location: class TensorFlowObjectDetectionAPIModel\r\n/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowYoloDetector.java:26: error: package org.tensorflow.contrib.android does not exist\r\nimport org.tensorflow.contrib.android.TensorFlowInferenceInterface;\r\n                                     ^\r\n/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowYoloDetector.java:87: error: cannot find symbol\r\n  private TensorFlowInferenceInterface inferenceInterface;\r\n          ^\r\n  symbol:   class TensorFlowInferenceInterface\r\n  location: class TensorFlowYoloDetector\r\n/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/SpeechActivity.java:152: error: cannot find symbol\r\n    inferenceInterface = new TensorFlowInferenceInterface(getAssets(), MODEL_FILENAME);\r\n                             ^\r\n  symbol:   class TensorFlowInferenceInterface\r\n  location: class SpeechActivity\r\n/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/StylizeActivity.java:365: error: cannot find symbol\r\n    inferenceInterface = new TensorFlowInferenceInterface(getAssets(), MODEL_FILE);\r\n                             ^\r\n  symbol:   class TensorFlowInferenceInterface\r\n  location: class StylizeActivity\r\n/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowImageClassifier.java:103: error: cannot find symbol\r\n    c.inferenceInterface = new TensorFlowInferenceInterface(assetManager, modelFilename);\r\n                               ^\r\n  symbol:   class TensorFlowInferenceInterface\r\n  location: class TensorFlowImageClassifier\r\n/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowImageClassifier.java:106: error: cannot find symbol\r\n    final Operation operation = c.inferenceInterface.graphOperation(outputName);\r\n          ^\r\n  symbol:   class Operation\r\n  location: class TensorFlowImageClassifier\r\n/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowMultiBoxDetector.java:90: error: cannot find symbol\r\n    d.inferenceInterface = new TensorFlowInferenceInterface(assetManager, modelFilename);\r\n                               ^\r\n  symbol:   class TensorFlowInferenceInterface\r\n  location: class TensorFlowMultiBoxDetector\r\n/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowMultiBoxDetector.java:92: error: cannot find symbol\r\n    final Graph g = d.inferenceInterface.graph();\r\n          ^\r\n  symbol:   class Graph\r\n  location: class TensorFlowMultiBoxDetector\r\n/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowMultiBoxDetector.java:99: error: cannot find symbol\r\n    final Operation inputOp = g.operation(inputName);\r\n          ^\r\n  symbol:   class Operation\r\n  location: class TensorFlowMultiBoxDetector\r\n/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowMultiBoxDetector.java:108: error: cannot find symbol\r\n    final Operation outputOp = g.operation(outputScoresName);\r\n          ^\r\n  symbol:   class Operation\r\n  location: class TensorFlowMultiBoxDetector\r\n/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowObjectDetectionAPIModel.java:91: error: cannot find symbol\r\n    d.inferenceInterface = new TensorFlowInferenceInterface(assetManager, modelFilename);\r\n                               ^\r\n  symbol:   class TensorFlowInferenceInterface\r\n  location: class TensorFlowObjectDetectionAPIModel\r\n/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowObjectDetectionAPIModel.java:93: error: cannot find symbol\r\n    final Graph g = d.inferenceInterface.graph();\r\n          ^\r\n  symbol:   class Graph\r\n  location: class TensorFlowObjectDetectionAPIModel\r\n/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowObjectDetectionAPIModel.java:100: error: cannot find symbol\r\n    final Operation inputOp = g.operation(d.inputName);\r\n          ^\r\n  symbol:   class Operation\r\n  location: class TensorFlowObjectDetectionAPIModel\r\n/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowObjectDetectionAPIModel.java:107: error: cannot find symbol\r\n    final Operation outputOp1 = g.operation(\"detection_scores\");\r\n          ^\r\n  symbol:   class Operation\r\n  location: class TensorFlowObjectDetectionAPIModel\r\n/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowObjectDetectionAPIModel.java:111: error: cannot find symbol\r\n    final Operation outputOp2 = g.operation(\"detection_boxes\");\r\n          ^\r\n  symbol:   class Operation\r\n  location: class TensorFlowObjectDetectionAPIModel\r\n/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowObjectDetectionAPIModel.java:115: error: cannot find symbol\r\n    final Operation outputOp3 = g.operation(\"detection_classes\");\r\n          ^\r\n  symbol:   class Operation\r\n  location: class TensorFlowObjectDetectionAPIModel\r\n/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowYoloDetector.java:107: error: cannot find symbol\r\n    d.inferenceInterface = new TensorFlowInferenceInterface(assetManager, modelFilename);\r\n                               ^\r\n  symbol:   class TensorFlowInferenceInterface\r\n  location: class TensorFlowYoloDetector\r\nNote: Some input files use or override a deprecated API.\r\nNote: Recompile with -Xlint:deprecation for details.\r\n32 errors\r\n:compileDebugJavaWithJavac FAILED\r\n:buildInfoGeneratorDebug\r\n\r\nFAILURE: Build failed with an exception.\r\n\r\n* What went wrong:\r\nExecution failed for task ':compileDebugJavaWithJavac'.\r\n> Compilation failed; see the compiler error output for details.\r\n\r\n* Try:\r\nRun with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights.\r\n\r\n* Get more help at https://help.gradle.org\r\n\r\nBUILD FAILED in 1s\r\n17 actionable tasks: 6 executed, 11 up-to-date\r\n\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Solved the error. Frankly, I just had to follow the tensorflow tutorial for Android more closely.", "Solved the error. Frankly, I just had to follow the tensorflow tutorial for Android more closely.", "@Spandan-Madan I meet the same error as you,can you tell me the resolve,because i can't find the tutorial for android,thanks so much ", "@shenyingying and anyone else having the same issue, in the build.gradle file change the following:\r\n`def nativeBuildSystem = 'bazel'`\r\nto \r\n`def nativeBuildSystem = 'none'`\r\n\r\nThis solves the **_\"package org.tensorflow.contrib.android does not exist\"_** error \r\n\r\nBut for more info, read through the [instructions](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/README.md) carefully\r\n\r\nHope it helps!"]}, {"number": 23114, "title": "Can tf.sparse_reduce_sum infer shape while keepdims=True?", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.11\r\n- Are you willing to contribute it (Yes/No): \r\nNo. Sorry, I'm not familiar with the TF codebase.\r\n\r\n**Describe the feature and the current behavior/state.**\r\ntf.sparse_reduce_sum should be able to infer the shape if the input has known shape.\r\n```pythonIn [1]: import tensorflow as tf\r\n\r\nIn [2]: x = tf.SparseTensor(indices=[[0,0],[1,1]], values=[1,2], dense_shape=[2,2])\r\n\r\nIn [3]: tf.sparse_reduce_sum(x)\r\nOut[3]: <tf.Tensor 'SparseReduceSum:0' shape=<unknown> dtype=int32>\r\n\r\nIn [4]: tf.sparse_reduce_sum(x, axis=1, keepdims=True)\r\nOut[4]: <tf.Tensor 'SparseReduceSum_1:0' shape=<unknown> dtype=int32>\r\n```\r\n\r\n\r\n**Will this change the current api? How?** \r\nNo.\r\n**Who will benefit with this feature?**\r\nUsers who use TensorFlow to do graph learning\r\n**Any Other info.**\r\n\r\nHi, please check the following information, thanks!\r\n\r\nHave I written custom code\r\nN/A\r\n\r\nOS Platform and Distribution\r\nWindows 10\r\n\r\nTensorFlow installed from\r\nconda -c anaconda tensorflow-gpu\r\n\r\nBazel version\r\nN/A\r\n\r\nCUDA/cuDNN version\r\n9.0/7.1.4\r\n\r\nGPU model and memory\r\nQuadro M520 2GB memory\r\n\r\nExact command to reproduce\r\ndescribe in the original post\r\n\r\nMobile device\r\nN/A", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Hi, please check the following information, thanks!\r\n\r\nHave I written custom code\r\nN/A\r\n\r\nOS Platform and Distribution\r\nWindows 10\r\n\r\nTensorFlow installed from\r\nconda -c anaconda tensorflow-gpu\r\n\r\nBazel version\r\nN/A\r\n\r\nCUDA/cuDNN version\r\n9.0/7.1.4\r\n\r\nGPU model and memory\r\nQuadro M520 2GB memory\r\n\r\nExact command to reproduce\r\ndescribe in the original post\r\n\r\nMobile device\r\nN/A", "Added a PR #23271 for the fix.", "Thanks for the PR @yongtang "]}, {"number": 23113, "title": "testing tflite quantized model on an input image", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): tf_nightly\r\n- Python version:  3.5.2\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source):  5.4.0\r\n- CUDA/cuDNN version: 9.0/7.1\r\n- GPU model and memory: GeFORCE 1080 Ti and 12 GB\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\n NA\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\r\n`import numpy as np\r\nimport tensorflow as tf\r\nimport pathlib\r\nimport cv2\r\nfrom keras.preprocessing import image\r\nfrom keras.applications.resnet50 import preprocess_input, decode_predictions\r\nimg_path = 'elephant.jpg'\r\nimg = image.load_img(img_path, target_size=(299, 299))\r\n\r\nx = image.img_to_array(img)\r\ninput_data = np.expand_dims(x, axis=0)\r\n\r\n#print(input_data.type)\r\n#x = preprocess_input(x)\r\n\r\narchive_path = tf.keras.utils.get_file(\"resnet_v2_101.tgz\", \"https://storage.googleapis.com/download.tensorflow.org/models/tflite_11_05_08/resnet_v2_101.tgz\", extract=True)\r\narchive_path = pathlib.Path(archive_path)\r\narchive_dir = str(archive_path.parent)\r\nprint(archive_dir)\r\n\r\n\r\ngraph_def_file = pathlib.Path(archive_path).parent/\"resnet_v2_101_299_frozen.pb\"\r\ninput_arrays = [\"input\"] \r\noutput_arrays = [\"output\"]\r\n\r\n\r\nconverter = tf.contrib.lite.TFLiteConverter.from_frozen_graph(\r\n  str(graph_def_file), input_arrays, output_arrays, input_shapes={\"input\":[1,299,299,3]})\r\n\r\nconverter.post_training_quantize = False\r\nresnet_tflite_file = graph_def_file.parent/\"resnet_v2_101.tflite\"\r\nresnet_tflite_file.write_bytes(converter.convert())\r\n\r\n\r\nconverter.post_training_quantize = True\r\nresnet_tflite_file = graph_def_file.parent/\"resnet_v2_101_quantized.tflite\"\r\nresnet_tflite_file.write_bytes(converter.convert())\r\n\r\n\r\n#!ls -lh {archive_dir}/*.tflite\r\n\r\n# Load TFLite model and allocate tensors.\r\ninterpreter = tf.contrib.lite.Interpreter(model_path=\"/home/sumeet/.keras/datasets/resnet_v2_101_quantized.tflite\")\r\ninterpreter.allocate_tensors()\r\n\r\n# Get input and output tensors.\r\ninput_index= interpreter.get_input_details()\r\nprint(input_index)\r\n#output_details = interpreter.get_output_details()\r\noutput_index = interpreter.get_output_details()\r\nprint(output_index)\r\n\r\n# Test model on random input data.\r\ninput_shape = input_index[0]['shape']\r\n#input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\r\n#input_data = np.array(X(input_shape), dtype=np.float32)\r\ninterpreter.set_tensor(input_index[0]['index'],input_data)\r\n\r\ninterpreter.invoke()\r\npredictions = interpreter.get_tensor(output_index[0]['index'])\r\npredictions[0]['name']\r\nprint(predictions[0,0])\r\n#print('Predicted:', decode_predictions(predictions, top=3)[0])`\r\n\r\n\r\n\r\nHow to make predictions on a custom input image like the code given below\r\nfrom keras.applications.resnet50 import ResNet50\r\nfrom keras.preprocessing import image\r\nfrom keras.applications.resnet50 import preprocess_input, decode_predictions\r\nimport numpy as np\r\n\r\nmodel = ResNet50(weights='imagenet')\r\n\r\nimg_path = 'elephant.jpg'\r\nimg = image.load_img(img_path, target_size=(224, 224))\r\nx = image.img_to_array(img)\r\nx = np.expand_dims(x, axis=0)\r\nx = preprocess_input(x)\r\n\r\npreds = model.predict(x)\r\nprint(preds.shape)\r\n# decode the results into a list of tuples (class, description, probability)\r\n# (one such list for each sample in the batch)\r\nprint('Predicted:', decode_predictions(preds, top=3)[0])\r\n# Predicted: [(u'n02504013', u'Indian_elephant', 0.82658225), (u'n01871265', u'tusker', 0.1122357), (u'n02504458', u'African_elephant', 0.061040461)\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nExact command to reproduce", "Can you please provide following information? Thanks\r\nDescribe the current behavior.\r\nDescribe the expected behavior.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Hi, if you are looking for some object detection and can be converted to .tflite file. I think this is great.https://coral.withgoogle.com/models/. google released this model for its new hardware device.  But I am still trying to convert my own model. I'd like to know after converting to tflite model, how to keep your tflite output still float32 type."]}, {"number": 23112, "title": "ImportError: /home/pi/.virtualenvs/keras_tf/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so: cannot open shared object file: No such file or directory", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Raspbian\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 0.11\r\n- Python version: 3.5.3\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): 6.3.0 20170516\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: Nvidia GTX 1050, 4 gb\r\n\r\n\r\n\r\n**Describe the problem**\r\nI am new learner in deep learning and do not know much about it. I was trying to install tensorflow in Raspbian following a blog [https://www.pyimagesearch.com/2016/11/14/installing-keras-with-tensorflow-backend/](url). After installing the tensorflow i got an issue while importing tensorflow, which i am not able to solve. I searched lots of blog and asked many for help but i did not get solution.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n$ mkvirtualenv keras_tf -p python3\r\n$ workon keras_tf\r\n$ pip install --upgrade tensorflow\r\n$ python\r\n>>> import tensorflow\r\n--\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nHere is the error which i am getting\r\n\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/pi/.virtualenvs/keras_tf/lib/python3.5/site-packages/tensorflow/__init__.py\", line 23, in <module>\r\n    from tensorflow.python import *\r\n  File \"/home/pi/.virtualenvs/keras_tf/lib/python3.5/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/pi/.virtualenvs/keras_tf/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"/home/pi/.virtualenvs/keras_tf/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\r\n  File \"/home/pi/.virtualenvs/keras_tf/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/pi/.virtualenvs/keras_tf/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: /home/pi/.virtualenvs/keras_tf/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so: cannot open shared object file: No such file or directory", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nExact command to reproduce", "@gauravssb1  - This could be due to the compiler version. Please use GCC 4.8.\r\n\r\nAlso see if it helps. #18482", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 23111, "title": "Update version", "body": "\r\nUpdate TF 1.12 version to 1.12-rc2", "comments": []}, {"number": 23110, "title": "Fix issue 22298", "body": " add storing variable created in variable scope in _VariableStore class", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "@DELTA37 could you sign the CLA?", "Ok, i will sign\r\n", "I have signed", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "@DELTA37 could you make sure you signed the CLA?", "What is this change trying to fix? Can you add a unit test for it?", "Closing this PR due to lack of activity ."]}, {"number": 23109, "title": "[Redo of #23090] Clean up binary element-wise assertions", "body": "**The working branch off of which PR #23090 was based was destroyed in a tragic git accident. This PR is a redo of the same changes.**\r\n\r\n**The reviewers on the original PR were @ymodak and @iganichev. @rohan100jain was also following the original PR.**\r\n\r\n**Original description follows:**\r\n\r\n\r\nTensorFlow 1.5 added two pieces of useful functionality to the `assert_equals` op. In eager mode, `assert_equals` now prints a more useful error message that pinpoints which elements of the input tensors differ (see [commit 361c55899cb524ca078c65eabdd3d79bfc10c8f9](https://github.com/tensorflow/tensorflow/commit/361c55899cb524ca078c65eabdd3d79bfc10c8f9)). In graph mode, `assert_equals` now evaluates the assertion at graph construction time when both inputs can be evaluated statically (see [commit cfbeafe11d9b86f8685c1c0f97d285885b5a5f1f](https://github.com/tensorflow/tensorflow/commit/cfbeafe11d9b86f8685c1c0f97d285885b5a5f1f)).\r\n\r\nThis PR ports this additional functionality to the other binary element-wise assertion ops `assert_none_equal`, `assert_less`, `assert_less_equal`, `assert_greater`, and `assert_greater_equal`. \r\n\r\n**Before:**\r\n```\r\nIn [1]: import numpy as np \r\n   ...: import tensorflow as tf \r\n   ...: tf.enable_eager_execution() \r\n   ...: zeros = np.zeros(1000) \r\n   ...: mostly_ones = np.full(1000, 1.) \r\n   ...: mostly_ones[567] = 0. \r\n   ...: tf.assert_none_equal(zeros, mostly_ones, summarize=3)            \r\n       \r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-1-23dba1a27a31> in <module>\r\n      5 mostly_ones = np.full(1000, 1.)\r\n      6 mostly_ones[567] = 0.\r\n----> 7 tf.assert_none_equal(zeros, mostly_ones, summarize=3)\r\n[...stack trace continues...]\r\n\r\nInvalidArgumentError: Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b''\r\nb'Condition x != y did not hold for every single element:'\r\nb'x (shape=(1000,) dtype=float64) = '\r\n0.0, 0.0, 0.0, ...\r\nb'y (shape=(1000,) dtype=float64) = '\r\n1.0, 1.0, 1.0, ...\r\n```\r\n**After:**\r\n```\r\nIn [1]: import numpy as np \r\n   ...: import tensorflow as tf \r\n   ...: tf.enable_eager_execution() \r\n   ...: zeros = np.zeros(1000) \r\n   ...: mostly_ones = np.full(1000, 1.) \r\n   ...: mostly_ones[567] = 0. \r\n   ...: tf.assert_none_equal(zeros, mostly_ones, summarize=3)                \r\n                                       \r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-1-1c86ae0b9399> in <module>\r\n      5 mostly_ones = np.full(1000, 1.)\r\n      6 mostly_ones[567] = 0.\r\n----> 7 tf.assert_none_equal(zeros, mostly_ones)\r\n[...stack trace continues...]\r\n\r\nInvalidArgumentError: Condition x != y did not hold.\r\nIndices of first 1 different values:\r\n[[567]]\r\nCorresponding x values:\r\n[0.]\r\nCorresponding y values:\r\n[0.]\r\nFirst 3 elements of x:\r\n[0. 0. 0.]\r\nFirst 3 elements of y:\r\n[1. 1. 1.]\r\n```\r\n\r\nThe ops `assert_negative`, `assert_non_negative`, `assert_positive`, and `assert_non_positive` also get some of the new functionality, as they are based on `assert_less` and `assert_less_equal`.\r\n\r\n**Before:**\r\n```\r\nIn [1]: import tensorflow as tf \r\n   ...: tf.assert_non_negative(-1.)                                             \r\nOut[1]: <tf.Operation 'assert_non_negative/assert_less_equal/Assert/AssertGuard/Merge' type=Merge>\r\n```\r\n**After:**\r\n```\r\nIn [1]: import tensorflow as tf \r\n   ...: tf.assert_non_negative(-1.)                               \r\n                                            \r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-1-c2ddbad603d6> in <module>\r\n      1 import tensorflow as tf\r\n----> 2 tf.assert_non_negative(-1.)\r\n[...stack trace continues...]\r\n\r\nInvalidArgumentError: \r\nCondition x >= 0 did not hold element-wise:\r\nx (assert_non_negative/x:0) = \r\n-1.0\r\n```\r\n\r\nI also removed some unnecessary newlines from the error messages and fixed a glitch in the handling of the `message` parameter when the `data` parameter is used.\r\n**Before:**\r\n```\r\nIn [1]: import tensorflow as tf \r\n   ...: tf.assert_equal(1., 0., data=[3.], message=\"My error message\")          \r\n\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-1-ef2b26ad5e73> in <module>\r\n      1 import tensorflow as tf\r\n----> 2 tf.assert_equal(1., 0., data=[3.], message=\"My error message\")\r\n                                    [snip!]\r\nInvalidArgumentError: 3.0\r\n```\r\n**After:**\r\n```\r\nIn [1]: import tensorflow as tf \r\n   ...: tf.assert_equal(1., 0., data=[3.], message=\"My error message\")                  \r\n                      \r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-1-ef2b26ad5e73> in <module>\r\n      1 import tensorflow as tf\r\n----> 2 tf.assert_equal(1., 0., data=[3.], message=\"My error message\")\r\n[...stack trace continues...]\r\n\r\nInvalidArgumentError: My error message\r\n3.0\r\n```\r\n\r\nIn the process, I replaced a bunch of near-duplicate code and documentation across the `assert_*` functions with a single function (`_binary_assert()` in `check_ops.py`) and common blocks of documentation (`_binary_assert_doc()` and `_unary_assert_doc()` in `check_ops.py`). `check_ops.py` is now about 125 lines shorter.\r\n\r\nI added some new regression tests to cover static assertion checks in graph mode and modified some existing tests to account for the new functionality.\r\n\r\nI built a local copy of the documentation for the `tf.debugging` package and reviewed all the resulting Markdown files.\r\n", "comments": ["Review comments from previous PR:\r\n\r\n**1**\r\nReviewer: iganichev \r\nLocation: tensorflow/python/ops/check_ops.py line 202\r\nComment: 'eq' should be 'test' here.\r\nStatus: Resolved by current changes\r\n\r\n**2**\r\nReviewer: iganichev \r\nLocation: tensorflow/python/ops/check_ops.py line 181 (now line **312**)\r\nComment: nit: Since you are returning above, no need for the \"else\" and extra indentation.\r\nStatus: **Not yet resolved; will fix shortly**\r\n\r\n**3**\r\nReviewer: iganichev \r\nLocation: tensorflow/python/ops/check_ops.py line 228\r\nComment: Define outside the function as something like _pretty_print to avoid redefinition overhead on every call\r\nStatus: Resolved by current changes\r\n\r\n**4**\r\nReviewer: iganichev \r\nLocation: tensorflow/python/ops/check_ops.py line 217\r\nComment: does not seem necessary\r\nStatus: Resolved by current changes\r\n\r\n**5**\r\nReviewer: iganichev \r\nLocation: tensorflow/python/ops/check_ops_test.py line 516\r\nComment: assert_none_equal -> assert_less\r\nStatus: **Not yet resolved; will fix shortly**\r\n\r\n**6**\r\nReviewer: iganichev \r\nLocation: tensorflow/python/ops/check_ops.py line 1363\r\nComment: -1 is non_positive. If this test passes, assert_non_positive has a bug.\r\nStatus: Resolved by current changes", "Pushed some additional changes to address open review comments from @iganichev. \r\n\r\nThanks for the review, and sorry for creating additional work for you with the commit structure of the previous PR.", "@frreiss Can you please make changes to address test failures to proceed with the PR? Please let me know if you need any information. Thanks!", "@frreiss I looked at a few failures. They either expect a wrong string message in the error of different exception type (because we did not use static checking in all the asserts). They should be fairly straightforward to fix.", "Looking into the test failures now.", "Pushed updates that fix the failing regression tests on my test machines.", "Can you fix the test failures?", "Hmm, there seem to be some CI test failures on build servers that I don't have access to. Can someone please post a list of tests that are failing so I can run the tests on my own machines?", " //tensorflow/python/kernel_tests:check_ops_test is failing with error \r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/build/work/f2a67cb4535ca2fd3f7a4553877de059/google3/runfiles/google3/third_party/py/absl/third_party/unittest3_backport/case.py\", line 37, in testPartExecutor\r\n    yield\r\n  File \"/build/work/f2a67cb4535ca2fd3f7a4553877de059/google3/runfiles/google3/third_party/py/absl/third_party/unittest3_backport/case.py\", line 162, in run\r\n    testMethod()\r\n  File \"/build/work/f2a67cb4535ca2fd3f7a4553877de059/google3/runfiles/google3/third_party/tensorflow/python/kernel_tests/check_ops_test.py\", line 320, in test_error_message_eager\r\n    t, t, message=\"This is the error message.\", summarize=10)\r\n  File \"/usr/grte/v4/k8-linux/lib/python2.7/unittest/case.py\", line 129, in __exit__\r\n    (expected_regexp.pattern, str(exc_value)))\r\nAssertionError: \"\\[0\\. 1\\. 2\\. 3\\. 4\\. 5\\.\\]\" does not match \"This is the error message.\r\nCondition x != y did not hold.\r\nIndices of first 6 different values:\r\n[[0 0]\r\n [0 1]\r\n [0 2]\r\n [1 0]\r\n [1 1]\r\n [1 2]]\r\nCorresponding x values:\r\n[ 0.  1.  2.  3.  4.  5.]\r\nCorresponding y values:\r\n[ 0.  1.  2.  3.  4.  5.]\r\nFirst 6 elements of x:\r\n[ 0.  1.  2.  3.  4.  5.]\r\nFirst 6 elements of y:\r\n[ 0.  1.  2.  3.  4.  5.]\"\r\n```", "(only on GPUs)", "Thanks for the pointers. I'm currently working through some issues with my GPU machine access. Should be able to resolve the failing regression test tomorrow or Monday.", "Ok, now I'm able to run the test with a GPU. Unfortunately, both `//tensorflow/python/kernel_tests:check_ops_test` and `//tensorflow/python/kernel_tests:check_ops_test_gpu` are passing on my test machine.\r\n\r\nLooking a bit more closely at the error message above, it appears that the GPU CI machine adds some leading whitespace when it prints out floating point tensors. I've added some slack to the regexes in `check_ops_test.py` that should hopefully resolve the issue. Could you please rerun the \"GPU Python3\" CI build?", "@frreiss Can you please resolve the conflicts? Thanks!", "Resolved the merge conflicts. I'm currently rerunning regression tests on my build machine.", "@alextp @iganichev Can you please review this PR? Thanks!", "@frreiss can you please check build failures", "Looking into build failures. I'll need to merge in changes from master first.", "@frreiss can you please resolve conflicts", "More conflicts resolved.", "The failed builds seem to be due to a glitch in the CI system. Would you mind rerunning them?", "Failed windows builds were caused by this error:\r\n```\r\n.\\tensorflow/lite/profiling/profile_buffer.h(18): fatal error C1083: Cannot open include file: 'sys/time.h': No such file or directory\r\n```\r\nwhich doesn't seem to have anything to do with this diff. It appears that there was a problem with `.\\tensorflow/lite/profiling/profile_buffer.h` for a few days, but the problem has now been fixed in the master branch.\r\n\r\nWould you mind running those windows builds one more time?", "This time around, the non-GPU Windows build failed with a linker error:\r\n```\r\npywrap_tensorflow_internal.obj : error LNK2019: unresolved external symbol \"int __cdecl tf_cxx11_abi_flag(void)\" (?tf_cxx11_abi_flag@@YAHXZ) referenced in function __cxx11_abi_flag___swigconstant\r\npywrap_tensorflow_internal.obj : error LNK2019: unresolved external symbol \"int __cdecl tf_monolithic_build(void)\" (?tf_monolithic_build@@YAHXZ) referenced in function __monolithic_build___swigconstant\r\nbazel-out/x64_windows-opt/bin/tensorflow/python/_pywrap_tensorflow_internal.so : fatal error LNK1120: 2 unresolved externals\r\n```\r\n...and the GPU build failed with a compiler error:\r\n```\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc(46): fatal error C1083: Cannot open include file: 'third_party/gpus/cudnn/cudnn.h': No such file or directory\r\n```\r\n...and the MacOS build just stopped in mid-stream with no errors printed to the log.\r\n\r\nI don't think any of these issues are due to the changes in this pull request.", "@frreiss there were some lint errors from internal checks , can you please address those, thank you", "My latest commit should resolve all the linter warnings. The regression tests are working on my test machine.", "Pushed some commits that should resolve the linter warnings. I'm running a bit blind here, though, as none of the linters on my machine produce any of these warnings.", "Can someone look at the log from that failing copybara job and let me know what went wrong? I don't have access to that CI server.", "tensorflow/contrib/neural_link:utils_test is failing, with the following message\r\n\r\n```\r\n======================================================================\r\nERROR: testApplyFeatureMaskWithInvalidMaskNegative (__main__.DecayOverTimeTest)\r\ntestApplyFeatureMaskWithInvalidMaskNegative (__main__.DecayOverTimeTest)\r\nTest the apply_feature_mask function with mask value < 0.\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/build/work/59fe95cabf7ec270c279aa86384651aff604/google3/runfiles/google3/third_party/py/absl/third_party/unittest3_backport/case.py\", line 37, in testPartExecutor\r\n    yield\r\n  File \"/build/work/59fe95cabf7ec270c279aa86384651aff604/google3/runfiles/google3/third_party/py/absl/third_party/unittest3_backport/case.py\", line 162, in run\r\n    testMethod()\r\n  File \"/build/work/59fe95cabf7ec270c279aa86384651aff604/google3/runfiles/google3/third_party/tensorflow/contrib/neural_link/utils_test.py\", line 261, in testApplyFeatureMaskWithInvalidMaskNegative\r\n    constant(features), constant(mask))\r\n  File \"/build/work/59fe95cabf7ec270c279aa86384651aff604/google3/runfiles/google3/third_party/tensorflow/contrib/neural_link/utils.py\", line 226, in apply_feature_mask\r\n    check_ops.assert_non_negative(feature_mask),\r\n  File \"/build/work/59fe95cabf7ec270c279aa86384651aff604/google3/runfiles/google3/third_party/tensorflow/python/ops/check_ops.py\", line 560, in assert_non_negative\r\n    return assert_less_equal(zero, x, data=data, summarize=summarize)\r\n  File \"/build/work/59fe95cabf7ec270c279aa86384651aff604/google3/runfiles/google3/third_party/tensorflow/python/ops/check_ops.py\", line 922, in assert_less_equal\r\n    name)\r\n  File \"/build/work/59fe95cabf7ec270c279aa86384651aff604/google3/runfiles/google3/third_party/tensorflow/python/ops/check_ops.py\", line 371, in _binary_assert\r\n    _assert_static(condition_static, data)\r\n  File \"/build/work/59fe95cabf7ec270c279aa86384651aff604/google3/runfiles/google3/third_party/tensorflow/python/ops/check_ops.py\", line 86, in _assert_static\r\n    message='\\n'.join(data_static))\r\nInvalidArgumentError: \r\nCondition x >= 0 did not hold element-wise:\r\nx (Const_1:0) = \r\n[-1.  1.]\r\n\r\n======================================================================\r\nERROR: testApplyFeatureMaskWithInvalidMaskTooLarge (__main__.DecayOverTimeTest)\r\ntestApplyFeatureMaskWithInvalidMaskTooLarge (__main__.DecayOverTimeTest)\r\nTest the apply_feature_mask function with mask value > 1.\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/build/work/59fe95cabf7ec270c279aa86384651aff604/google3/runfiles/google3/third_party/py/absl/third_party/unittest3_backport/case.py\", line 37, in testPartExecutor\r\n    yield\r\n  File \"/build/work/59fe95cabf7ec270c279aa86384651aff604/google3/runfiles/google3/third_party/py/absl/third_party/unittest3_backport/case.py\", line 162, in run\r\n    testMethod()\r\n  File \"/build/work/59fe95cabf7ec270c279aa86384651aff604/google3/runfiles/google3/third_party/tensorflow/contrib/neural_link/utils_test.py\", line 271, in testApplyFeatureMaskWithInvalidMaskTooLarge\r\n    constant(features), constant(mask))\r\n  File \"/build/work/59fe95cabf7ec270c279aa86384651aff604/google3/runfiles/google3/third_party/tensorflow/contrib/neural_link/utils.py\", line 228, in apply_feature_mask\r\n    feature_mask, constant_op.constant(1, dtype=feature_mask.dtype))\r\n  File \"/build/work/59fe95cabf7ec270c279aa86384651aff604/google3/runfiles/google3/third_party/tensorflow/python/ops/check_ops.py\", line 922, in assert_less_equal\r\n    name)\r\n  File \"/build/work/59fe95cabf7ec270c279aa86384651aff604/google3/runfiles/google3/third_party/tensorflow/python/ops/check_ops.py\", line 371, in _binary_assert\r\n    _assert_static(condition_static, data)\r\n  File \"/build/work/59fe95cabf7ec270c279aa86384651aff604/google3/runfiles/google3/third_party/tensorflow/python/ops/check_ops.py\", line 86, in _assert_static\r\n    message='\\n'.join(data_static))\r\nInvalidArgumentError: Condition x <= y did not hold element-wise:\r\nx (Const_1:0) = \r\n[ 1.  2.]\r\ny (Const_2:0) = \r\n1.0\r\n\r\n----------------------------------------------------------------------\r\n```", "I'm fine with disabling this test / reverting assertion changes which affect it", "I don't have the source code that's being exercised there. But from the stack trace, it looks like the test case `testApplyFeatureMaskWithInvalidMaskNegative` in `contrib/neural_link/utils_test.py`, deliberately passes an invalid value to the `apply_feature_mask()` function. The test case expects this invalid value to generate an exception later on when running the graph, but after the changes in this PR, the invalid value generates an exception immediately. The same thing happens with `testApplyFeatureMaskWithInvalidMaskTooLarge` in the same source file.\r\n\r\nIf this diagnosis is correct, you can fix the failing tests by wrapping the call to `apply_feature_mask` in a `with self.assertRaisesRegexp(errors.InvalidArgumentError)` context manager and removing the part of the test case that calls `Session.run()`. I made a similar change at line 44 of  `tensorflow/contrib/distributions/python/kernel_tests/deterministic_test.py` in this PR.\r\n\r\nOr you can just disable the test cases `testApplyFeatureMaskWithInvalidMaskNegative` and `testApplyFeatureMaskWithInvalidMaskTooLarge` in `tensorflow/contrib/neural_link/utils_test.py`\r\n\r\n", "@frreiss  here is the internal error \r\n\r\n`Traceback (most recent call last):\r\n  File \"/py/absl/third_party/unittest3_backport/case.py\", line 37, in testPartExecutor\r\n    yield\r\n  File \"py/absl/third_party/unittest3_backport/case.py\", line 162, in run\r\n    testMethod()\r\n  File \"tensorflow/contrib/neural_link/utils_test.py\", line 261, in testApplyFeatureMaskWithInvalidMaskNegative\r\n    constant(features), constant(mask))\r\n  File \"/tensorflow/contrib/neural_link/utils.py\", line 226, in apply_feature_mask\r\n    check_ops.assert_non_negative(feature_mask),\r\n  File \"tensorflow/python/ops/check_ops.py\", line 560, in assert_non_negative\r\n    return assert_less_equal(zero, x, data=data, summarize=summarize)\r\n  File \"tensorflow/python/ops/check_ops.py\", line 922, in assert_less_equal\r\n    name)\r\n  File \"tensorflow/python/ops/check_ops.py\", line 371, in _binary_assert\r\n    _assert_static(condition_static, data)\r\n  File \"ensorflow/python/ops/check_ops.py\", line 86, in _assert_static\r\n    message='\\n'.join(data_static))\r\nInvalidArgumentError: \r\nCondition x >= 0 did not hold element-wise:\r\nx (Const_1:0) = \r\n[-1.  1.]`", "> @frreiss here is the internal error\r\n> \r\n> `[. . .]`\r\n\r\nThanks for that info, @rthadur. That traceback seems to be the same one that @alextp posted above. Looking at the stack trace again, I'm pretty confident in my assessment from yesterday: This PR adds a static check to `assert_less_equal`, and the test case triggers that static check at graph construction time instead of triggering the old dynamic check at graph execution time. \r\n\r\nIf you wrap the statement at line 260 of `tensorflow/contrib/neural_link/utils_test.py` in a `with self.assertRaisesRegexp(errors.InvalidArgumentError, \"Condition x >= 0\"):` context manager and delete the parts of `testApplyFeatureMaskWithInvalidMaskNegative()` that come after line 261, the test should pass. \r\n", "@frreiss Sorry for delay. I am fixing the internal errors now.", "@frreiss. I submitted the change but it broke quite a few more tests in various projects I did not anticipate and they are not as straightforward as before - when we are trying to get a constant value from a tensor \"x_static = tensor_util.constant_value(x)\", the new tensor x_static becomes unfeedable - users cannot change it in feed_dict. Some tests try to change the value after going through some assert op.\r\n\r\nI am rolling it back and will try to resubmit after fixing all the issues.", "Thanks, @iganichev, I appreciate the help in shepherding this change."]}, {"number": 23108, "title": "I get error when I want to load my Model", "body": "I am building a deep CNN and I get my graph files by running my code on a GPU on cluster. The training procedure work perfect. Then I move my graph files to my laptop to build this network on my laptop which has CPU. But when I try to load the model I receive the following error:\r\n\r\nTraceback (most recent call last): File \"dev_test.py\", line 28, in new_saver = tf.train.import_meta_graph('./3/Model_Arch3/Deep_CNN_Color_Arch8.ckpt-178000.meta') File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1810, in import_meta_graph **kwargs) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/meta_graph.py\", line 660, in import_scoped_meta_graph producer_op_list=producer_op_list) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py\", line 285, in import_graph_def raise ValueError('No op named %s in defined operations.' % node.op) ValueError: No op named ParseSingleExample in defined operations.\r\n\r\nThe strange thing is that when I train this network on my laptop using my CPU, then I can load the model without any problem!\r\n\r\nAlso I can build the graph and load the model successfully on the GPU cluster where I trained it! \r\n\r\nI tried the dir(tf.contrib) solution that was suggested in other question, but it did not work for me.\r\n\r\nI really appreciate if someone helps me", "comments": ["Maybe the problem is from the verison of tf?", "@blueeda is correct. Make sure you have the same tf version between CPU and GPU, you can provide more details by filling out the issue template. "]}, {"number": 23107, "title": "Number of elements was larger than representable by 32-bit output type", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux release 7.5.1804 (Core)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 1.11.0,  v1.11.0-0-gc19e29306c\r\n- Python version: Python 3.7.0 (default, Jun 28 2018, 13:15:42)\r\n- Bazel version (if compiling from source): Build label: 0.18.0- (@non-git)\r\n- GCC/Compiler version (if compiling from source): gcc version 4.8.5 20150623 (Red Hat 4.8.5-28) (GCC)\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nI'm training a 3D U-Net model with a tensor size of 128x128x128x1 in order to develop models for brain MRIs. It works for a batch size of 8 and less. When I move to a batch size of 16, the size is larger than the int32 index can handle. I have sufficient RAM (384 GB) to allocate the tensors because I can run 2 such processes on the machine without a problem. The error is:\r\n\r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Number of elements was larger than representable by 32-bit output type\r\n            [[{{node training/Adam/gradients/batch_normalization_2/moments/mean_grad/Prod}} = Size[T=DT_FLOAT, _class=[\"loc:@training/Adam/gradients/batch_normalization_2/moments/mean_grad/truediv\"], out_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](conv1b/add, ^training/Adam/gradients/batch_normalization_2/moments/mean_grad/Shape)]]\r\n```\r\n**Describe the expected behavior**\r\nIt should be able to handle larger tensors and batch sizes.\r\n\r\n**Exact command to reproduce**\r\n`python benchmark_model.py --dim_length 128 --bz 16`\r\n\r\n**Code to reproduce the issue**\r\nI have test code here:  https://github.com/NervanaSystems/topologies/tree/master/3D_UNet\r\n\r\n`python benchmark_model.py --dim_length 128 --bz 8` will work\r\n`python benchmark_model.py --dim_length 128 --bz 16` will give error\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```...which was originally created as op 'batch_normalization_2/moments/mean', defined at:\r\n  File \"benchmark_model.py\", line 108, in <module>\r\n    print_summary=args.print_model)\r\n  File \"/home/bduser/topologies/3D_UNet/model.py\", line 96, in define_model\r\n    conv1 = K.layers.BatchNormalization()(conv1)\r\n  File \"/home/bduser/anaconda3/envs/tf/lib/python3.6/site-packages/keras/engine/base_layer.py\", line 457, in __call__\r\n    output = self.call(inputs, **kwargs)\r\n  File \"/home/bduser/anaconda3/envs/tf/lib/python3.6/site-packages/keras/layers/normalization.py\", line 185, in call\r\n    epsilon=self.epsilon)\r\n  File \"/home/bduser/anaconda3/envs/tf/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 1869, in normalize_batch_in_training\r\n    epsilon=epsilon)\r\n  File \"/home/bduser/anaconda3/envs/tf/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 1750, in _regular_normalize_batch_in_training\r\n    None, None, False)\r\n  File \"/home/bduser/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py\", line 690, in moments\r\n    mean = math_ops.reduce_mean(y, axes, keepdims=True, name=\"mean\")\r\n  File \"/home/bduser/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/bduser/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 1492, in reduce_mean\r\n    name=name))\r\n  File \"/home/bduser/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 4778, in mean\r\n    name=name)\r\n  File \"/home/bduser/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/bduser/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/bduser/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3272, in create_op\r\n    op_def=op_def)\r\n  File \"/home/bduser/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1768, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): Number of elements was larger than representable by 32-bit output type\r\n\t [[{{node gradients/batch_normalization_2/moments/mean_grad/Prod}} = Size[T=DT_FLOAT, out_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](conv1b/add, ^gradients/batch_normalization_2/batchnorm/mul_1_grad/Shape)]]\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nExact command to reproduce", "Updated. Thanks.", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!\r\n", "At the very least it is a feature request.  In short I am saying that there's a hard limit on the input tensor size because the code is indexing using INT32. So for large tensor sizes (as is seen in medical imaging), TensorFlow can't handle the entire image at once (even though there may be enough RAM).", "I encountered the very same issue working with large tensors."]}, {"number": 23106, "title": "Update relnotes with Ignite information", "body": "", "comments": ["@dmitrievanthony FYI", "Thanks, @martinwicke!"]}, {"number": 23105, "title": "[XLA] AlgebraicSimplifier pass - handle And and Or", "body": "Add handling of logical And and Or, where:\r\n* (a && True) => a\r\n* (True && a) => a\r\n* (a && False) => False\r\n* (False && a) => False\r\n* (a || True) => True\r\n* (True || a) => True\r\n* (a || False) => a\r\n* (False || a) => a\r\n\r\nAdded tests for these as well.", "comments": ["Thank you for the review, all done now"]}, {"number": 23104, "title": "Bazel will not compile under Windows when the user folder has spaces", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: master\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: no\r\n- Bazel version (if compiling from source): 0.18.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 7\r\n- GPU model and memory: GTX 1080 Ti 11Gb\r\n- Have I written custom code: no\r\n- Exact command to reproduce: bazel build\r\n\r\nThe initial state build files usually call a _wrap_bash_cmd function, which does not handle spaces in the paths. I changed line 71 of repo.bzl from:\r\n`[\"patch\", \"-p1\", \"-d\", ctx.path(\".\"), \"-i\", ctx.path(patch_file)]`\r\nto\r\n`[\"patch\", \"-p1\", \"-d\", '\"' + str(ctx.path(\".\")) + '\"', \"-i\", ctx.path(patch_file)]`\r\n\r\nand it helped, but there are a lot of other files where this also happens and the build keeps failing.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nExact command to reproduce", "@waltermaldonado  - Hi, can you please provide the information asked by the tensorflowbutler.", "I already did it in an edit. The referred problem happens when the user folder eg \"C:\\Users\\Walter Maldonado\" has spaces on it, generating several errors in bazel build.", "Is this still an issue? If this is still an issue, workaround is create folder without spaces and install. thanks!", "One potential issue is the maximum path length check.\r\nYou can try disabling path length limit by following here:\r\nhttps://www.howtogeek.com/266621/how-to-make-windows-10-accept-file-paths-over-260-characters/", "@waltermaldonado please confirm if the above comments help and your issue is resolved.", "WE also found one other issue in our scripts, and fixed that. I will assume this issue fixed for now. Please ping if you see it again.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23104\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23104\">No</a>\n"]}, {"number": 23103, "title": "Add uint8 support for Pad with GPU", "body": "\r\nThis fix tries to address some of the issue raised in #17823,\r\nwhere Pad with GPU does not support uint8. This fix adds the uint8 support.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@yongtang Can you please approve this PR? Thanks!", "@ymodak I think the PR has been approved by @wujingyue.", "@ymodak Any update to this PR, or is there anything else I need to do?", "@yongtang Given how simple this implementation looks - do you think it would be the same to extend uint8 functionality to strided slice to close off #17823?"]}, {"number": 23102, "title": "compile from source fails", "body": "**System information**\r\n- Ubuntu 18.04\r\n- From source / r1.12\r\n- TensorFlow version:\r\n- Python 3.6.6\r\n- Inside virtualenv\r\n- Bazel 0.18.0\r\n- gcc 6.4.0\r\n- Cuda 9 / Cudnn 7\r\n\r\n**The problem**\r\nBuilding from source, I end up with: \r\n\r\n```\r\nERROR: /home/dev/tensorflow/tensorflow/core/BUILD:319:1: undeclared inclusion(s) in rule '//tensorflow/core:platform_base':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/core/platform/env_time.cc':\r\n  '/usr/lib/gcc/x86_64-linux-gnu/6/include/stdint.h'\r\n  '/usr/lib/gcc/x86_64-linux-gnu/6/include/stddef.h'\r\n  '/usr/lib/gcc/x86_64-linux-gnu/6/include/stdarg.h'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 0.460s, Critical Path: 0.26s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\n```\r\nThis is when I run: \r\n\r\n`bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nAlso note that if I repeat the command several times, I don't always get the same error message. After I do `bazel clean` I get: \r\n\r\n```\r\nERROR: /home/.cache/bazel/_bazel_nnnnn/75c0d842b4eca8fbdb48dc37e31275de/external/protobuf_archive/BUILD:659:1: C++ compilation of rule '@protobuf_archive//:python/google/protobuf/pyext/_message.so' failed (Exit 1)\r\nIn file included from external/protobuf_archive/python/google/protobuf/pyext/map_container.cc:33:0:\r\nexternal/protobuf_archive/python/google/protobuf/pyext/map_container.h:34:20: fatal error: Python.h: No such file or directory\r\n #include <Python.h>\r\n                    ^\r\ncompilation terminated.\r\n\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@C0NTRIBUTE Have you tried installing TensorFlow from binary?"]}, {"number": 23101, "title": "Possible bug in the design of `tf.keras.layers.Conv`", "body": "**System information**\r\n- TensorFlow version (you are using): 1.10\r\n- Are you willing to contribute it (Yes/No): No (Don't know tf's code base well enough.)\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrent behaviour of `tf.layers.Conv2D` is that\r\n```\r\nimport tensorflow as tf\r\n\r\nexample = tf.zeros([1, 32, 32, 1])\r\nexample2 = tf.zeros([1, 41, 41, 1])\r\n\r\n# Using a dilated convolution layer\r\nconvolution_op = tf.layers.Conv2D(4, 3, padding=\"SAME\", dilation_rate=4)\r\n\r\nprint(convolution_op( example ))\r\nprint(convolution_op( example2 ))\r\n```\r\ncauses the following exception\r\n```\r\nValueError: Dimension size must be evenly divisible by 4 but is 49 for 'conv2d_2/SpaceToBatchND_1' (op: 'SpaceToBatchND') with input shapes: [1,41,41,1], [2], [2,2] and with computed input tensors: input[1] = <4 4>, input[2] = <[4 4][4 4]>.\r\n```\r\nThe reason for this is that in the `build` function for `tf.keras.layers.Conv` (https://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/python/keras/layers/convolutional.py) the convolution operation is created - however, the *dilated* convolution operation also contains padding dependent on the size of the first input to `call`.\r\nThere's no reason for this, as the padding operation (i.e. the call to `tf.nn.conv2d`) could simply be different for each of the two `call`s.\r\nNote that with the functional layer API it is still possible to do the reuse of the layers:\r\n```\r\nconv1 = tf.layers.conv2d(example, 4, 3, padding=\"SAME\", dilation_rate=4, name=\"test\")\r\nconv2 = tf.layers.conv2d(example2, 4, 3, padding=\"SAME\", dilation_rate=4, name=\"test\", reuse=True)\r\n```\r\nbecause this causes two separate layer creations. I am proposing that the behaviour should be similar to this.\r\n\r\nThe consequence of this is that at the moment, to my knowledge, there is no good way of doing weight reuse of dilated convolution weights using `tf.keras.layers`. I think that the above is possibly a bug in the implementation - the convolution operation should maybe be create in the `call` function of `tf.keras.layers.Conv`?\r\n\r\n\r\n\r\n\r\n**Will this change the current api? How?**\r\nThis should not raise an error:\r\n```\r\nimport tensorflow as tf\r\n\r\nexample = tf.zeros([1, 32, 32, 1])\r\nexample2 = tf.zeros([1, 41, 41, 1])\r\n\r\nconvolution_op = tf.layers.Conv2D(4, 3, padding=\"SAME\", dilation_rate=4)\r\n\r\nprint(convolution_op( example ))\r\nprint(convolution_op( example2 ))\r\n\r\n```\r\n**Who will benefit with this feature?**\r\nEveryone, as this would be a more consistent behavior of the API, and would allow things that right now are not possible.\r\n\r\n**Any Other info.**\r\nMore information is also in https://github.com/tensorflow/tensorflow/issues/23019 - I had raised it but it was closed without a relevant reply.\r\n\r\nHave I written custom code \r\nas above.\r\nOS Platform and Distribution\r\nN/A\r\nTensorFlow installed from\r\npip3\r\nBazel version\r\nN/A\r\nCUDA/cuDNN version\r\nN/A\r\nGPU model and memory\r\nN/A\r\nExact command to reproduce\r\nas above\r\nMobile device\r\nN/A\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Updated.", "I'm not sure I understand your question clearly, however I think `convolution_op(example)` should be invoked only once.", "I also ran into this problem (using TF 1.14.0 and 2.0-compatible operations). I would be curious if this behavior is considered intentional by developers.\r\n\r\nFor others who land on this issue, I've pasted an example workaround for tf.keras.layers.Conv1D:\r\n\r\n```python\r\nimport functools\r\nimport tensorflow._api.v1.compat.v2 as tf\r\n\r\n\r\nclass Conv1D(tf.keras.layers.Conv1D):\r\n    \"\"\"Prevent keras from caching the input shape when building.\r\n    \"\"\"\r\n    # pylint: disable=no-name-in-module\r\n    from tensorflow.python.keras.utils import conv_utils\r\n    # pylint: enable=no-name-in-module\r\n\r\n    def build(self, input_shape):\r\n        \"\"\"Use the functional convolution operation instead of an instance \r\n        of the internal TF Convolution class, which stores and reuses the input shape.\r\n        \"\"\"\r\n        super().build(input_shape)\r\n\r\n        if self.padding == 'causal':\r\n            op_padding = 'valid'\r\n        else:\r\n            op_padding = self.padding\r\n        if not isinstance(op_padding, (list, tuple)):\r\n            op_padding = op_padding.upper()\r\n\r\n        # pylint: disable=attribute-defined-outside-init\r\n        self._convolution_op = functools.partial(\r\n            tf.nn.convolution,\r\n            strides=self.strides,\r\n            padding=op_padding,\r\n            data_format=self.conv_utils.convert_data_format(\r\n                self.data_format, self.rank + 2\r\n            ),\r\n            dilations=self.dilation_rate\r\n        )\r\n        # pylint: enable=attribute-defined-outside-init\r\n```", "This is fixed with TF 2.2 \r\n`convolution_op = tf.keras.layers.Conv2D(4, 3, padding=\"SAME\", dilation_rate=4)`\r\nThanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23101\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23101\">No</a>\n"]}, {"number": 23100, "title": "Upgrade to protobuf 3.6.1  ", "body": "An incompatible change in Bazel: https://github.com/bazelbuild/bazel/issues/6384 breaks Tensorflow on Windows: https://buildkite.com/bazel/bazel-with-downstream-projects-bazel/builds/498#3e15d20b-30a5-4071-8c3f-ce7425ce5f35. The breakage comes from using protobuf 3.6.0.\r\n\r\nProtobuf 3.6.1 is compatible with the change.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Closing as PR is merged."]}, {"number": 23099, "title": "Running rename_protobuf.sh causes sed: can't read s%#include \\([<\"]\\)google/protobuf/%#include \\1google/protobuf3/%: No such file or directory", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: https://github.com/tensorflow/tensorflow/commit/93bc2e2072e0daccbcff7a90d397b704a9e8f778/\r\n- Python version: python3\r\n- Installed using virtualenv? pip? conda?: \r\n- Bazel version (if compiling from source): 0.16.1\r\n- GCC/Compiler version (if compiling from source): 5.4.0\r\n- CUDA/cuDNN version: 10.0 / 7.1.4\r\n- GPU model and memory: \r\n\r\n\r\n\r\n**Describe the problem**\r\nhttps://stackoverflow.com/questions/52886112/running-rename-protobuf-sh\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nsudo bash tensorflow/contrib/makefile/download_dependencies.sh\r\nsudo bash tensorflow/contrib/makefile/rename_protobuf.sh\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\r\nThe error message:\r\nsed: can't read s%#include \\([<\"]\\)google/protobuf/%#include \\1google/protobuf3/%: No such file or directory\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nExact command to reproduce", "@benoitsteiner Can you PTAL? Thanks!", "Closing this issue due to staleness. Please switch to latest version of TF and try again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=23099\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=23099\">No</a>\n", "The same issue too. Why there are so many \"No such file or directory\" here?"]}, {"number": 23098, "title": "TFRecordDataset entries shuffle between examples", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Mojave 10.14 (18A391)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.11.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n- Have I written custom code: N/A\r\n- Exact command to reproduce: Check Code to reproduce the issue\r\n\r\n**Describe the problem**\r\nWhen I load several examples into a TFRecordDataset and parse the serialised examples using `parse_single_example`, the entry values of the examples are interchanged randomly. For example, if I save two examples `{\"a\": [1, 2, 3], \"b\": [10, 20, 30]}` and `{\"a\": [3, 4, 5], \"b\": [30, 40, 50]}` into a `tfrecord` and then load and parse it using `TFRecordDataset` and `parse_single_example`, the loaded examples will become `{\"a\": [3, 4, 5], \"b\": [10, 20, 30]}` and `{\"a\": [1, 2, 3], \"b\": [30, 40, 50]}`, while it will be correct if I use `Example.ParseFromString` to parse the evaluated string.\r\n\r\n**Code to reproduce the issue**\r\nCode to write the tfrecord\r\n```\r\nimport tensorflow as tf\r\n\r\ntest = tf.train.Features(feature={\r\n    'a': tf.train.Feature(float_list=tf.train.FloatList(value=[1,2,3])),\r\n    'b': tf.train.Feature(float_list=tf.train.FloatList(value=[10,20,30]))\r\n})\r\ntest_2 = tf.train.Features(feature={\r\n    'a': tf.train.Feature(float_list=tf.train.FloatList(value=[3,4,5])),\r\n    'b': tf.train.Feature(float_list=tf.train.FloatList(value=[30,40,50]))\r\n})\r\n\r\nexample = tf.train.Example(features=test)\r\nexample_2 = tf.train.Example(features=test_2)\r\n\r\nwith tf.python_io.TFRecordWriter('test.tfrecord') as writer:\r\n    writer.write(example.SerializeToString())\r\n    writer.write(example_2.SerializeToString())\r\n```\r\nCode to load the tfrecord. Example parsed via `ParseFromString` is printed first, and then the one parsed via `parse_single_example`\r\n```\r\ndataset = tf.data.TFRecordDataset('test.tfrecord')\r\ndataset = dataset.repeat()\r\nexamples = dataset.make_one_shot_iterator()\r\nwith tf.Session() as sess:\r\n    for _ in range(2):\r\n        serialized_example = examples.get_next()\r\n        correct_example = tf.train.Example()\r\n        correct_example.ParseFromString(serialized_example.eval())\r\n        print(correct_example)\r\n        features = {\r\n            \"a\": tf.FixedLenFeature([3], tf.float32),\r\n            \"b\": tf.FixedLenFeature([3], tf.float32),\r\n        }\r\n        example = tf.parse_single_example(serialized=serialized_example, features=features)\r\n        for name, tensor in example.items():\r\n            print('{}: {}'.format(name, tensor.eval()))\r\n```\r\n\r\n**Other info / logs**\r\nThe log I've got from the second snippet\r\n```\r\nfeatures {\r\n  feature {\r\n    key: \"a\"\r\n    value {\r\n      float_list {\r\n        value: 1.0\r\n        value: 2.0\r\n        value: 3.0\r\n      }\r\n    }\r\n  }\r\n  feature {\r\n    key: \"b\"\r\n    value {\r\n      float_list {\r\n        value: 10.0\r\n        value: 20.0\r\n        value: 30.0\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\na: [3. 4. 5.]\r\nb: [10. 20. 30.]\r\nfeatures {\r\n  feature {\r\n    key: \"a\"\r\n    value {\r\n      float_list {\r\n        value: 3.0\r\n        value: 4.0\r\n        value: 5.0\r\n      }\r\n    }\r\n  }\r\n  feature {\r\n    key: \"b\"\r\n    value {\r\n      float_list {\r\n        value: 30.0\r\n        value: 40.0\r\n        value: 50.0\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\na: [1. 2. 3.]\r\nb: [30. 40. 50.]\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nExact command to reproduce", "@yipsang @harshini-gadige I think this is not a bug in TensorFlow. Instead, there are some issues in the example code. In `serialized_example = examples.get_next()`, `serialized_example` is an iterator, but both `correct_example` and `example` will call `get_next` in one loop. That is why the output is not in the right order. For reading part, the below code can return the right output:\r\n```\r\n    dataset = tf.data.TFRecordDataset(filename)\r\n    examples = dataset.make_one_shot_iterator()\r\n    with tf.Session() as sess:\r\n        for _ in range(2):\r\n            serialized_example = examples.get_next()\r\n            # correct_example = tf.train.Example()\r\n            # correct_example.ParseFromString(serialized_example.eval())\r\n            # print(correct_example)\r\n            features = {\r\n                \"a\": tf.FixedLenFeature([3], tf.float32),\r\n                \"b\": tf.FixedLenFeature([3], tf.float32),\r\n            }\r\n            example = tf.parse_single_example(serialized=serialized_example, features=features)\r\n\r\n            keys = []\r\n            values = []\r\n            for name, tensor in example.items():\r\n                keys.append(name)\r\n                values.append(tensor)\r\n\r\n            real_values = sess.run(values)\r\n            for i in range(len(keys)):\r\n                print('{}: {}'.format(keys[i], real_values[i]))\r\n```  ", "@shivaniag  -  Hi, could you please look into this ?", "Thanks @feihugis for diagnosing the problem! The iterator is working as intended, and the problem comes from comparing the values returned from different `Session.run()` calls.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=23098\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=23098\">No</a>\n"]}, {"number": 23097, "title": "add shape info for convenience", "body": "", "comments": ["@zh794390558 Can you please resolve the conflicts? Thanks!", "@ymodak sure, but I can not see the conflict.", "Closing this PR since the ```tensorflow/contrib/lite/optional_debug_tools.cc``` has been moved to  ```tensorflow/lite/optional_debug_tools.cc``` .  Please resubmit  new PR addressing this change. Thanks!"]}, {"number": 23096, "title": "[Source Installation] Unable to install TF due multiple errors. Info as extensive as possible", "body": "**System information**\r\n- **OS Platform and Distribution** (e.g., Linux Ubuntu 16.04): Mac OS High Sierra 10.13.6 (17G65)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- **TensorFlow installed from** (source or binary): Source\r\n- **TensorFlow version**: r1.10 / r1.11\r\n- **Python version**: 2.7.10\r\n- **Installed using virtualenv? pip? conda?**: Installed inside a virtualenv using pip\r\n- **Bazel version** (if compiling from source): \r\nBuild label: 0.16.1\r\nBuild target: bazel-out/darwin-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Mon Aug 13 13:42:50 2018 (1534167770)\r\nBuild timestamp: 1534167770\r\nBuild timestamp as int: 1534167770\r\n\r\n- **GCC/Compiler version** (if compiling from source):\r\n\u279c  Tensorflow-SDK git:(r1.10) gcc --version\r\nConfigured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/usr/include/c++/4.2.1\r\nApple LLVM version 10.0.0 (clang-1000.11.45.2)\r\nTarget: x86_64-apple-darwin17.7.0\r\nThread model: posix\r\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\r\n- CUDA/cuDNN version: Not using Cuda but CUDA v9.0.176 and Cuddn v7.0\r\n- GPU model and memory: Unsure.\r\n\r\n### Describe the problem\r\n**Attempt one** \r\nI had Bazel v  0.18.0-homebrew installed and I was trying to compile Tensorflow r1.10 from source. \r\nI followed the following steps : \r\n* ./configure (followed everything written [here](https://www.tensorflow.org/install/source))\r\n* Everytime I executed `bazel test -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/contrib/lite/...` it gave me the following error so I just decided to skip it and execute the build command.\r\n\r\n```\r\nERROR: /Users/daksh_s/Development/Sources/ARCore/Resources/TFObjectDetection/Tensorflow-SDK/tensorflow/python/eager/BUILD:10:1: C++ compilation of rule '//tensorflow/python/eager:pywrap_tfe_lib' failed (Exit 1)\r\nIn file included from tensorflow/python/eager/pywrap_tfe_src.cc:18:\r\nIn file included from ./tensorflow/python/eager/pywrap_tfe.h:22:\r\nIn file included from ./tensorflow/core/lib/core/status.h:23:\r\nIn file included from bazel-out/host/genfiles/tensorflow/core/lib/core/error_codes.pb.h:9:\r\nIn file included from external/protobuf_archive/src/google/protobuf/stubs/common.h:39:\r\nIn file included from /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/iostream:38:\r\nIn file included from /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/ios:216:\r\nfatal error: too many errors emitted, stopping now [-ferror-limit=]\r\n```\r\n* On building, I'm able to compile about 3k files when it eventually fails stating `fatal error: too many errors emitted, stopping now [-ferror-limit=]`\r\n\r\n```\r\nERROR: /Users/daksh_s/Development/Sources/ARCore/Resources/TFObjectDetection/Tensorflow-SDK/tensorflow/core/kernels/BUILD:145:1: error while parsing .d file: /private/var/tmp/_bazel_daksh_s/b40f90ee3cec22f597caca95f90fbe9c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/core/kernels/_objs/concat_lib_gpu/concat_lib_gpu_impl.cu.pic.d (No such file or directory)\r\nnvcc fatal   : Unsupported gpu architecture 'compute_70'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```\r\nAt this point, I stopped using CUDA and built for CPU only. Also, I downgraded by Bazel from 0.18 to 0.16.1. The following are my observations : \r\n* Even though I give \"N\" as the answer to majority of the questions in `./configure` command, the following is generated in `.bazelrc`:\r\n\r\n```\r\nbuild --action_env PYTHON_BIN_PATH=\"/Users/daksh_s/Development/Sources/ARCore/Resources/TFObjectDetection/bin/python\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/Users/daksh_s/Development/Sources/ARCore/Resources/TFObjectDetection/lib/python2.7/site-packages\"\r\nbuild --python_path=\"/Users/daksh_s/Development/Sources/ARCore/Resources/TFObjectDetection/bin/python\"\r\nbuild:gcp --define with_gcp_support=true\r\nbuild --define with_hdfs_support=true\r\nbuild:aws --define with_aws_support=true\r\nbuild:kafka --define with_kafka_support=true\r\nbuild:xla --define with_xla_support=true\r\nbuild:gdr --define with_gdr_support=true\r\nbuild:verbs --define with_verbs_support=true\r\nbuild --action_env TF_NEED_OPENCL_SYCL=\"0\"\r\nbuild --action_env TF_NEED_CUDA=\"0\"\r\nbuild --action_env TF_DOWNLOAD_CLANG=\"1\"\r\nbuild --config=download_clang\r\ntest --config=download_clang\r\nbuild --define grpc_no_ares=true\r\nbuild:opt --copt=-march=native\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\nbuild --strip=always\r\n```\r\nSince I did not want Kafka/aws... etc, I manually changed them all to false and re-executed.\r\n* The bulild fails 100% of the time if `TF_DOWNLOAD_CLANG=0` citing the following. Apparently, its not optional. \r\n```\r\nERROR: no such package '@local_config_download_clang//': cc_download_clang_toolchain rule //external:local_config_download_clang must create a directory\r\n```\r\n* If I let TF download it by keeping `TF_DOWNLOAD_CLANG=1`, I get the following build error : \r\n```\r\nfatal error: too many errors emitted, stopping now [-ferror-limit=]\r\n20 errors generated.\r\nError in child process '/usr/bin/xcrun'. 1\r\nERROR: /Users/daksh_s/Development/Sources/ARCore/Resources/TFObjectDetection/Tensorflow-SDK/tensorflow/python/eager/BUILD:10:1: output 'tensorflow/python/eager/_objs/pywrap_tfe_lib/pywrap_tfe_src.o' was not created\r\nERROR: /Users/daksh_s/Development/Sources/ARCore/Resources/TFObjectDetection/Tensorflow-SDK/tensorflow/python/eager/BUILD:10:1: not all outputs were created or valid\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```\r\n\r\n**Any other info / logs**\r\nYou may download the terminal saved output from [here](https://1drv.ms/u/s!AlU4ab7adLFlgfZNsL1IHuby1vknnA)", "comments": ["I upgraded my Bazel version to 0.17 but a similar error forced the build to fail. You may view the log of this failure [here](https://1drv.ms/u/s!AlU4ab7adLFlgfZPutjUgmOucBR4Ig).  `pywrap_tfe_lib` seems to be causing a lot of ruckus. Can I leave it out completely?  \r\n\r\nIn a nutshell, its this : \r\n```\r\nTensorflow-SDK/tensorflow/python/eager/BUILD:10:1: C++ compilation of rule '//tensorflow/python/eager:pywrap_tfe_lib' failed (Exit 1): wrapped_clang failed: error executing command\r\n(cd /private/var/tmp/_bazel_daksh_s/b40f90ee3cec22f597caca95f90fbe9c/execroot/org_tensorflow && \\\r\nexec env - \\.......\r\n```", "@dakshsrivastava  -  Hi, from the above post I understand that you are using Bazel versions 0.16 and 0.17 for CPU(correct me if I'm wrong). Bazel 0.15.0 is the tested version for Mac OS and request you to try with Bazel 0.15. Also for future reference, please use [this link ](https://www.tensorflow.org/install/source#tested_build_configurations)to know the compatible versions for Tensorflow for macOS. Thank you !", "Hey @harshini-gadige. Thanks for the link and info. I downgraded bazel to v0.15 but still, no luck. Final error : \r\n```\r\nERROR: tensorflow/python/eager/BUILD:10:1: C++ compilation of rule '//tensorflow/python/eager:pywrap_tfe_lib' failed (Exit 1): wrapped_clang failed: error executing command (cd /private/var/tmp/_bazel_daksh_s/b40f90ee3cec22f597caca95f90fbe9c/execroot/org_tensorflow && \\\r\n  exec env ..........\r\n```\r\n\r\nYou may view the log [here](https://1drv.ms/u/s!AlU4ab7adLFlgfZQwA5ER8-LFTiSnw).", "@gunan - Hi, could you please take a look at this.", "Here is one of the actual error messages:\r\n```\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/__locale:781:5: error: expected expression\r\n    return use_facet<ctype<_CharT> >(__loc).is(ctype_base::upper, __c);\r\n    ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/__locale:782:2: error: expected ';' at end of declaration\r\n}\r\n ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/__locale:787:21: error: too many arguments provided to function-like macro invocation\r\n```\r\n\r\nLooks to be emitted by the Xcode libraries. I will try to reproduce the problem.", "On my machine, TF built just fine with bazel 0.18\r\nCould you try running the following:\r\n```\r\nbazel clean --expunge_async\r\nyes \"\" | ./configure\r\nbazel build --config=opt tensorflow/tools/pip_package:build_pip_package\r\n```", "@dakshsrivastava  Any update ?", "> Here is one of the actual error messages:\r\n> \r\n> ```\r\n> /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/__locale:781:5: error: expected expression\r\n>     return use_facet<ctype<_CharT> >(__loc).is(ctype_base::upper, __c);\r\n>     ^\r\n> /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/__locale:782:2: error: expected ';' at end of declaration\r\n> }\r\n>  ^\r\n> /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/__locale:787:21: error: too many arguments provided to function-like macro invocation\r\n> ```\r\n> Looks to be emitted by the Xcode libraries. I will try to reproduce the problem.\r\n\r\n@gunan I'm working with r1.12 and MacOS Mojave, building with Bazel 0.15.0, and such error occurs. Seems something is wrong with eager module. Could you help to check or is there a way to skip building eager module? I tried to build r1.6 without eager and succeed. ", "I saw the same error when building tensorflow-serving r1.12 on Mojave using Bazel 0.18.0.\r\n\r\n```\r\nERROR: /private/var/tmp/_bazel_ygong/54239953e47919fb5d8e76b539992026/external/org_tensorflow/tensorflow/python/eager/BUILD:10:1: C++ compilation of rule '@org_tensorflow//tensorflow/python/eager:pywrap_tfe_lib' failed (Exit 1)\r\nIn file included from external/org_tensorflow/tensorflow/python/eager/pywrap_tfe_src.cc:18:\r\nIn file included from external/org_tensorflow/tensorflow/python/eager/pywrap_tfe.h:22:\r\nIn file included from external/org_tensorflow/tensorflow/core/lib/core/status.h:23:\r\nIn file included from bazel-out/host/genfiles/external/org_tensorflow/tensorflow/core/lib/core/error_codes.pb.h:9:\r\nIn file included from external/protobuf_archive/src/google/protobuf/stubs/common.h:39:\r\nIn file included from /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/iostream:38:\r\nIn file included from /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/ios:216:\r\n```", "I think this may be due to trying to compile with GPU support.\r\nI am not sure how well CUDA is supported on macos.\r\nSince that is ambigious, TF does not officially support GPUs on macos.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=23096\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=23096\">No</a>\n", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 23095, "title": "fix keras relu layer bug (wrong arguments type)", "body": "In `tf.keras.layers.ReLU` function, the config of arguments(max_value, negative_slope, threshold) become dict but float is the need while `tf.keras.models.load_model` or `tf.contrib.lite.TocoConverter.from_keras_model_file`\r\n\r\n**network structure**\r\n```python3\r\ninputs = Input(shape=(32, 32, 3))\r\nx = Conv2D(32, 3, strides=1, padding='same')(inputs)\r\nx = ReLU()(x)\r\nx = Conv2D(64, 3, strides=2, padding='valid')(x)\r\nx = ReLU()(x)\r\nx = Conv2D(64, 3, strides=2, padding='valid')(x)\r\nx = ReLU()(x)\r\n...\r\n```\r\n**save model** \r\n`tf.keras.models.save_model(model, 'model.h5')`\r\n\r\n**load model** (error occurred)\r\n`tf.keras.models.load_model('model.h5')`\r\n\r\n**error message**\r\nTypeError: '<' not supported between instances of 'dict' and 'float' ", "comments": ["So, either the documentation is wrong or this needs to be fixed at the caller site. (`max_value` is said to be a float).", "This is already fixed on keras master.\r\n\r\nJust need to add the following when we set max_value:\r\n```\r\nif max_value is not None:\r\n    max_value = K.cast_to_floatx(max_value)\r\n```", "Hi, @drpngx. It needs to be fixed at the caller site.\r\nThe bug was due to `tf.keras.models.load_model`. In my opion, the minimal change is as my commit. But I might be wrong, I wish you could give me some advice and suggestions.", "This has already been fixed in a different place. This argument is float and passing anything else is an error.", "Hi, @fchollet, thank you for your reply.\r\nI\u2019m not sure I get your meaning. The error caused is due to `tf.keras.model.load_model`, not the argument I passed to the function.\r\n\r\nMy tensorflow version is 1.11.0, python version is 3.6.7.\r\n\r\nI attach the code which would cause error message below, could you help me to check that is there something wrong with my environment or my usage.  Thank you.\r\n\r\n```python3\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\nprint(tf.__version__)\r\n\r\nmodel = keras.models.Sequential([\r\n    keras.layers.Dense(512, input_shape=(784, )),\r\n    keras.layers.ReLU(),\r\n    keras.layers.Dense(200),\r\n    keras.layers.ReLU()\r\n])\r\n\r\nmodel.compile(optimizer=keras.optimizers.Adam(), loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\r\nmodel.summary()\r\n\r\nmodel.save('./model.h5')\r\n\r\nmodel2 = keras.models.load_model('./model.h5')\r\nmodel2.summary()\r\n```", "Excuse me, @fchollet, could you help me with this problem?", "@drpngx, could you help me?", "@Kirayue Did you try against the `master` branch?", "@drpngx, I did not. I just `pip3 install tensorflow`  and ran the code above. I did not check out to any other branch. "]}, {"number": 23094, "title": "fix keras ReLU layer bug", "body": "In `tf.keras.layers.ReLU` function, the config of arguments(max_value, negative_slope, threshold) become dict but float is the need while `tf.keras.models.load_model` or `tf.contrib.lite.TocoConverter.from_keras_model_file`\r\n\r\n**network structure**\r\n```python3\r\ninputs = Input(shape=(32, 32, 3))\r\nx = Conv2D(32, 3, strides=1, padding='same')(inputs)\r\nx = ReLU()(x)\r\nx = Conv2D(64, 3, strides=2, padding='valid')(x)\r\nx = ReLU()(x)\r\nx = Conv2D(64, 3, strides=2, padding='valid')(x)\r\nx = ReLU()(x)\r\n...\r\n```\r\n**save model** \r\n`tf.keras.models.save_model(model, 'model.h5')`\r\n\r\n**load model** (error occurred)\r\n`tf.keras.models.load_model('model.h5')`\r\n\r\n**error message**\r\nTypeError: '<' not supported between instances of 'dict' and 'float' \r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "I signed it!"]}, {"number": 23093, "title": "add print interpreter api", "body": "", "comments": ["@jdduke Can you please take a look at this PR? Thanks!", "These files are moved out of contrib folder , closing this PR"]}, {"number": 23092, "title": "add interpreter debug func", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "Has some problem, so closed"]}, {"number": 23091, "title": "latest cpu version tensorflow report error when run slim model", "body": "**System information**\r\n- use tensorlow master branch commit 0918aa74153664dbc61604af3cf0d66eb334c7ce build the code, build command is : bazel build -c opt --copt=-L/home/guizili/tool/gcc6.3/lib64 //tensorflow/tools/pip_package:build_pip_package\r\n- OS Platform and Distribution (CentOS Linux release 7.4.1708 (Core)):\r\n- Python version 2.7.5:\r\n- Bazel version (0.15.0):\r\n- GCC/Compiler version (6.3.0):\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\nrun train command in tensorlfow model: https://github.com/tensorflow/models\r\ncommand is:\r\ntrain_dir=\"./train_dir\"\r\n    rm -rf $train_dir\r\n    mkdir -p $train_dir\r\n    python research/slim/train_image_classifier.py \\\r\n    --dataset_name=imagenet \\\r\n    --train_dir=$train_dir \\\r\n    --dataset_dir=/lustre/dataset/tensorflow/imagenet \\\r\n    --dataset_split_name=train \\\r\n    --model_name=inception_v3 \\\r\n    --batch_size=64 \\\r\n    --max_number_of_steps=220 \\\r\n    --clone_on_cpu=True\r\n\r\nerror log:\r\n2018-10-19 09:29:57.155023: E tensorflow/core/framework/node_def_util.cc:110] Error in the node: {{node InceptionV3/InceptionV3/Mixed_7c/Branch_1/Conv2d_0b_1x3/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](InceptionV3/InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/Relu, InceptionV3/Mixed_7c/Branch_1/Conv2d_0b_1x3/weights/read)\r\n2018-10-19 09:29:57.179106: E tensorflow/core/framework/node_def_util.cc:110] Error in the node: {{node InceptionV3/InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](InceptionV3/InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/Relu, InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/weights/read)\r\n2018-10-19 09:29:57.204720: E tensorflow/core/framework/node_def_util.cc:110] Error in the node: {{node InceptionV3/InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](InceptionV3/InceptionV3/Mixed_7b/concat, InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/weights/read)\r\n2018-10-19 09:29:57.228912: E tensorflow/core/framework/node_def_util.cc:110] Error in the node: {{node InceptionV3/InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](InceptionV3/InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/Relu, InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/weights/read)\r\n\r\nattach the log while run mobilenet.\r\n[log.txt](https://github.com/tensorflow/tensorflow/files/2494158/log.txt)\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nTensorFlow installed from\nTensorFlow version\nExact command to reproduce\nMobile device", "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NA\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below):commit 0918aa74153664dbc61604af3cf0d66eb334c7ce\r\n-Exact command to reproduce:\r\ntrain_dir=\"./train_dir\"\r\nrm -rf $train_dir\r\nmkdir -p $train_dir\r\npython research/slim/train_image_classifier.py \r\n--dataset_name=imagenet \r\n--train_dir=$train_dir \r\n--dataset_dir=/lustre/dataset/tensorflow/imagenet \r\n--dataset_split_name=train \r\n--model_name=inception_v3 \r\n--batch_size=64 \r\n--max_number_of_steps=220 \r\n--clone_on_cpu=True", "Any updates on this? I'm having a similar problem...", "There has been quite a few changes made since this issue is opened, you can retry with the latest version here https://github.com/tensorflow/models/tree/master/research/slim#Training and close this issue if your issue is resolved. Thanks!", "> There has been quite a few changes made since this issue is opened, you can retry with the latest version here https://github.com/tensorflow/models/tree/master/research/slim#Training and close this issue if your issue is resolved. Thanks!\n\nThanks and close it as it too old."]}, {"number": 23090, "title": "[aborted] Clean up binary element-wise assertions", "body": "**Edit: Accidentally messed up my branch. Recreating my changes in a new branch and resubmitting this PR.**\r\n\r\nTensorFlow 1.5 added two pieces of useful functionality to the `assert_equals` op. In eager mode, `assert_equals` now prints a more useful error message that pinpoints which elements of the input tensors differ (see [commit 361c55899cb524ca078c65eabdd3d79bfc10c8f9](https://github.com/tensorflow/tensorflow/commit/361c55899cb524ca078c65eabdd3d79bfc10c8f9)). In graph mode, `assert_equals` now evaluates the assertion at graph construction time when both inputs can be evaluated statically (see [commit cfbeafe11d9b86f8685c1c0f97d285885b5a5f1f](https://github.com/tensorflow/tensorflow/commit/cfbeafe11d9b86f8685c1c0f97d285885b5a5f1f)).\r\n\r\nThis PR ports this additional functionality to the other binary element-wise assertion ops `assert_none_equal`, `assert_less`, `assert_less_equal`, `assert_greater`, and `assert_greater_equal`. \r\n\r\n**Before:**\r\n```\r\nIn [1]: import numpy as np \r\n   ...: import tensorflow as tf \r\n   ...: tf.enable_eager_execution() \r\n   ...: zeros = np.zeros(1000) \r\n   ...: mostly_ones = np.full(1000, 1.) \r\n   ...: mostly_ones[567] = 0. \r\n   ...: tf.assert_none_equal(zeros, mostly_ones, summarize=3)            \r\n       \r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-1-23dba1a27a31> in <module>\r\n      5 mostly_ones = np.full(1000, 1.)\r\n      6 mostly_ones[567] = 0.\r\n----> 7 tf.assert_none_equal(zeros, mostly_ones, summarize=3)\r\n[...stack trace continues...]\r\n\r\nInvalidArgumentError: Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b''\r\nb'Condition x != y did not hold for every single element:'\r\nb'x (shape=(1000,) dtype=float64) = '\r\n0.0, 0.0, 0.0, ...\r\nb'y (shape=(1000,) dtype=float64) = '\r\n1.0, 1.0, 1.0, ...\r\n```\r\n**After:**\r\n```\r\nIn [1]: import numpy as np \r\n   ...: import tensorflow as tf \r\n   ...: tf.enable_eager_execution() \r\n   ...: zeros = np.zeros(1000) \r\n   ...: mostly_ones = np.full(1000, 1.) \r\n   ...: mostly_ones[567] = 0. \r\n   ...: tf.assert_none_equal(zeros, mostly_ones, summarize=3)                \r\n                                       \r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-1-1c86ae0b9399> in <module>\r\n      5 mostly_ones = np.full(1000, 1.)\r\n      6 mostly_ones[567] = 0.\r\n----> 7 tf.assert_none_equal(zeros, mostly_ones)\r\n[...stack trace continues...]\r\n\r\nInvalidArgumentError: Condition x != y did not hold.\r\nIndices of first 1 different values:\r\n[[567]]\r\nCorresponding x values:\r\n[0.]\r\nCorresponding y values:\r\n[0.]\r\nFirst 3 elements of x:\r\n[0. 0. 0.]\r\nFirst 3 elements of y:\r\n[1. 1. 1.]\r\n```\r\n\r\nThe ops `assert_negative`, `assert_non_negative`, `assert_positive`, and `assert_non_positive` also get some of the new functionality, as they are based on `assert_less` and `assert_less_equal`.\r\n\r\n**Before:**\r\n```\r\nIn [1]: import tensorflow as tf \r\n   ...: tf.assert_non_negative(-1.)                                             \r\nOut[1]: <tf.Operation 'assert_non_negative/assert_less_equal/Assert/AssertGuard/Merge' type=Merge>\r\n```\r\n**After:**\r\n```\r\nIn [1]: import tensorflow as tf \r\n   ...: tf.assert_non_negative(-1.)                               \r\n                                            \r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-1-c2ddbad603d6> in <module>\r\n      1 import tensorflow as tf\r\n----> 2 tf.assert_non_negative(-1.)\r\n[...stack trace continues...]\r\n\r\nInvalidArgumentError: \r\nCondition x >= 0 did not hold element-wise:\r\nx (assert_non_negative/x:0) = \r\n-1.0\r\n```\r\n\r\nI also removed some unnecessary newlines from the error messages and fixed a glitch in the handling of the `message` parameter when the `data` parameter is used.\r\n**Before:**\r\n```\r\nIn [1]: import tensorflow as tf \r\n   ...: tf.assert_equal(1., 0., data=[3.], message=\"My error message\")          \r\n\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-1-ef2b26ad5e73> in <module>\r\n      1 import tensorflow as tf\r\n----> 2 tf.assert_equal(1., 0., data=[3.], message=\"My error message\")\r\n                                    [snip!]\r\nInvalidArgumentError: 3.0\r\n```\r\n**After:**\r\n```\r\nIn [1]: import tensorflow as tf \r\n   ...: tf.assert_equal(1., 0., data=[3.], message=\"My error message\")                  \r\n                      \r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-1-ef2b26ad5e73> in <module>\r\n      1 import tensorflow as tf\r\n----> 2 tf.assert_equal(1., 0., data=[3.], message=\"My error message\")\r\n[...stack trace continues...]\r\n\r\nInvalidArgumentError: My error message\r\n3.0\r\n```\r\n\r\nIn the process, I replaced a bunch of near-duplicate code and documentation across the `assert_*` functions with a single function (`_binary_assert()` in `check_ops.py`) and common blocks of documentation (`_binary_assert_doc()` and `_unary_assert_doc()` in `check_ops.py`). `check_ops.py` is now about 125 lines shorter.\r\n\r\nI added some new regression tests to cover static assertion checks in graph mode and modified some existing tests to account for the new functionality.\r\n\r\nI built a local copy of the documentation for the `tf.debugging` package and reviewed all the resulting Markdown files.\r\n", "comments": ["Hmm, something bad happened when I did a manual rebase. Recreating the PR on a new branch."]}, {"number": 23089, "title": "Make shared S3 file system lib linkable", "body": "My (and I think most) linkers expect shared object files to start with `lib`, so using the target `s3_file_system.so` resulted in an error along the lines of `error: cannot find -ls3_file_system`, because this target creates a file named `s3_file_system.so` as opposed to `libs3_file_system.so`.\r\n\r\nJust prefixing the name with `lib` would have caused the output to collide with the output of `s3_file_system`, so I also appended `_shared` before `.so`.\r\n\r\nAdding this line: `cc_binary(name = \"test\", srcs = [\":s3_file_system.so\"])` in the same `BUILD` file is enough to demonstrate the linker error for me, although it also complains about an undefined reference to `main`. Adding a trivial main file (`int main() { return 0; }`) gets rid of this but preserves the linker error for a more minimal (in some ways) reproduction", "comments": ["@yongtang Can you please take a look at this PR? Thanks!", "@palmerlao I tried with the patch and the test still works, so I think the PR is fine as it does not impact the working function. Though I am not very clear about:\r\n\r\n> Just prefixing the name with lib would have caused the output to collide with the output of s3_file_system, so I also appended _shared before .so.\r\n\r\nI don't see the name will collide on Linux, maybe you could clarify a little more?", "@yongtang I tried it again and I guess you're right - I don't need to append `_shared` to the target name. I'm not very sure why I needed it before, perhaps it was an idiosyncrasy of my workplace's Bazel setup.", "Hi @yongtang , if you take a look at line 238 here: https://source.cloud.google.com/results/invocations/c5801843-d4f1-4be9-81e8-33a0da1700d9/log\r\n\r\nit displays the error I previously ran into, where the shared object file `libs3_file_system.so` is generated by two different targets, which apparently is an error (I guess whether or not it appears is closely related to the exact bazel invocation).\r\n\r\nI can try reproducing it locally later with the command it lists, and then adding back the `_shared` suffix. Does that sound reasonable?", "@palmerlao I think it makes sense. Can you strip out the second commit so that only the first commit `Make shared S3 lib linkable` remains?", "Sorry for letting this sit. Do you prefer rebase/force pushes or revert commits?", "@palmerlao I think you can just strip the second commit (so that only the first commit remains), then do a force push.", "Sorry, I seem to have bungled the force push that was described two comments ago.\r\n\r\nI believe this is still a valid PR. Happy to rebase if that is conventional in this repo.", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on."]}, {"number": 23088, "title": "Trying to install tensorflow in windows 10 and got error when trying to import", "body": "\r\n(tfp3.6) C:\\WINDOWS\\system32>python\r\nPython 3.6.6 |Anaconda, Inc.| (default, Jun 28 2018, 11:27:44) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Dell\\Anaconda3\\envs\\tfp3.6\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Dell\\Anaconda3\\envs\\tfp3.6\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Dell\\Anaconda3\\envs\\tfp3.6\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Dell\\Anaconda3\\envs\\tfp3.6\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Dell\\Anaconda3\\envs\\tfp3.6\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Dell\\Anaconda3\\envs\\tfp3.6\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Dell\\Anaconda3\\envs\\tfp3.6\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Dell\\Anaconda3\\envs\\tfp3.6\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Dell\\Anaconda3\\envs\\tfp3.6\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Dell\\Anaconda3\\envs\\tfp3.6\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Dell\\Anaconda3\\envs\\tfp3.6\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Dell\\Anaconda3\\envs\\tfp3.6\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Dell\\Anaconda3\\envs\\tfp3.6\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["I'm having the same issue. I tried different versions of tensorflow and installed directly the wheel version but nothing works. Did you find a solution ?", "Please provide **System information** : \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n", "Dear @ymodak ,\r\nThanks for looking into this issue! I experience the same error. Hope this helps.\r\n* OS: Windows 10\r\n* Mobile device: No\r\n* TensorFlow installed from (source or binary): pip install tensorflow-gpu\r\n* TensorFlow version: tensorflow-gpu==1.11.0\r\n* Python version: Python 3.6.6\r\n* Installed using virtualenv? pip? conda?: Within a virtualenv installed using pip. No conda.\r\n* Bazel version (if compiling from source): not from source\r\n* GCC/Compiler version (if compiling from source): not from source\r\n* CUDA/cuDNN version: cuda 10.0.130_411.31 / cuDNN v7.3.1.20\r\n* GPU model and memory: Quadro M1000M with 2048MiB\r\n\r\n(rasa-env) <home_folder>>python\r\nPython 3.6.6 (v3.6.6:4cf1f54eb7, Jun 27 2018, 03:37:03) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"<virtualenv_folder>\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"<virtualenv_folder>\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"<virtualenv_folder>\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python36_64\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python36_64\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"<virtualenv_folder>\\lib\\site-packages\\tensorflow\\__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"<virtualenv_folder>\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"<virtualenv_folder>\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"<virtualenv_folder>\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"<virtualenv_folder>\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"<virtualenv_folder>\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python36_64\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python36_64\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>>", "For my issue it seems like CUDA 10 is not yet supported by tensorflow. See ymodak's answer here: https://github.com/tensorflow/tensorflow/issues/21741#issuecomment-427508153", "@AmnaRamzan Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 23087, "title": "How to interpret feature weights from bucketized and crossed columns from a trained model in TensorFlow", "body": "```\r\nHave I written custom code: NA\r\nOS Platform and Distribution: Mac\r\nTensorFlow installed from: pip\r\nTensorFlow version: 1.11\r\nBazel version: NA\r\nCUDA/cuDNN version: NA \r\nGPU model and memory: NA\r\nExact command to reproduce: NA \r\nMobile device: NA\r\n```\r\n\r\n\r\nI'm currently running into a problem where I'm not able to fetch features' weight from a linear model in TensorFlow. I know how to do it when the feature columns only consist of ``numeric_column`` or ``categorical_columns_with_vocabulary_list``, unless ``bucketized_column`` or ``crossed_column`` are added additionally, especially because of **hash bins** introduced in ``crossed_column``.\r\n\r\nThe problem looks like this:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.feature_column as fc\r\n\r\nx1 = [1,2,3,4,5,6,7,8]\r\nx2 = [10,20,30,40,50]\r\n\r\nx1_fc = fc.bucketized_column('x1', bins=[3,6])\r\nx2_fc = fc.bucketized_column('x2', bins=[20,40])\r\nx1_x_x2_fc = fc.indicator_column(fc.crossed_column([x1_fc, x2_fc], 7))\r\nfeature_columns = [x1_fc, x2_fc, x1_x_x2_fc]\r\n\r\n# constuct the linear model\r\nmodel = tf.estimator.LinearClassifier(feature_columns)\r\n```\r\n\r\nThe question is that when model is trained and ready to do inferences, is there any way to inspect the predictions by looking into the model's each weight of those features (both bucketized and crossed features)? I mean the weights' values can be extracted without an issue, but I struggle to interpret those values associated with my original inputs.\r\n\r\nBesides, for my experiences, when I only use ``numeric_column`` in the model, I do notice that the features are ordered alphabetically in the weights layer, which is somewhat weird to me.\r\n\r\n[This answer][1] does not seem very natural, and can be hard to implement when feature number grows or the **hash bins** gets large.\r\n\r\n\r\n  [1]: https://stackoverflow.com/questions/47282229/tensorflow-inspecting-hash-buckets-for-categorical-and-feature-columns", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, along with minimal repro example. Thanks!\r\n"]}]