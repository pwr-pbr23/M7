[{"number": 55296, "title": "Upstream Gradient of variables in tf.custom_gradient is (unexpectedly and seemingly generally) zero", "body": "**System information**\r\nTF 2.8, via Google Colab (so none of the other information should be relevant here?)\r\n\r\n**Describe the current behavior**\r\nUsing tf.custom_gradient the upstream of a variable currently seems to be zero generally. Exemplary code:\r\n\r\n```\r\ndef call_model_with_weights(model, x, weights):\r\n  vars = [var.assign(val) for var, val in zip(model.trainable_weights, weights)]\r\n  y = model(x)\r\n  len_vars = len(weights)\r\n  len_y = len(y)\r\n\r\n  #stitching together the gradient\r\n  @tf.custom_gradient\r\n  def copy_gradient(var, val, y):\r\n    def grad(*upstream):\r\n      var_up = upstream[:len_vars]\r\n      y_up = upstream[len_vars:]\r\n      return var_up, var_up, y_up\r\n\r\n    return (var, y), grad\r\n\r\n  _, _y = copy_gradient(model.trainable_weights, weights, y) #this does not change when changing 'model.trainable_weights' to 'vars'\r\n  return _y\r\n\r\nmodel=some_model\r\nx=some_input\r\nweights = some_list_of_model_weights_with_appropriate_shapes #in the simplest case just model.trainable_weights\r\nwith tf.GradientTape() as tape:\r\n  tape.watch(weights)\r\n  y = call_model_with_weights(model, x, weights)\r\ngradient=tape.gradient(y, weights) #this gradient is zero (not even NaN as in not connected, but just appropriate shapes of zeros)\r\n```\r\n\r\n(background: Trying to implement an equivalent to PyTorch [higher](https://github.com/facebookresearch/higher) library, to enable meta-learning research also on TF, where it currently strongly lacks behind)\r\n\r\nI have come upon this while trying to implement a wrapper that calls a function as a function of weights. I observe that generally, the upstream gradient of any variable seems to be zero.\r\n\r\nI'm reporting this as a bug, because the behavior is (1) unexpected, (2) undocumented (from what I could find at least), and (3) the expected behavior would generally be more useful from what I understand. \r\n\r\nA full colab notebook reproducing the problem can be found here: https://colab.research.google.com/drive/1kSQhgtxaIhwwpzp0VObZQbqGC0dmxeNc?usp=sharing\r\n\r\nI hope this is not just some other stupid coding mistake I made somewhere, but I've been trying to eliminate any other possible reasons for this behavior I could come up with.\r\n\r\n\r\n**Describe the expected behavior**\r\nThe upstream gradient of a variable in tf.custom_gradient should be the actual upstream gradient, not a vector of zeros of the same shape\r\n\r\n- Do you want to contribute a PR? (yes/no): Since I'm not sure how thiis could be fixed, no; If I get pointed in the correct direction I will happily do that! \r\n- Briefly describe your candidate solution(if contributing): See above\r\n\r\n**Standalone code to reproduce the issue**\r\nLink again: https://colab.research.google.com/drive/1kSQhgtxaIhwwpzp0VObZQbqGC0dmxeNc?usp=sharing\r\n\r\n**Other info / logs** \r\nNone", "comments": ["@sachinprasadhs  I was able to reproduce the issues on colab using TF v[2.8.0](https://colab.research.google.com/gist/sushreebarsa/b2b115903e074e955da48c03125a59a1/copyfor-github-of-call_model_as_function_of_weights.ipynb#scrollTo=lM94qa2YPzKO) and [tf-nightly](https://colab.research.google.com/gist/sushreebarsa/9a0a49e29822bedc9fd467d7d0fde224/copyfor-github-of-call_model_as_function_of_weights.ipynb#scrollTo=lM94qa2YPzKO) ,please find the attached gists. Thanks!"]}, {"number": 55291, "title": "Missing information on headers providing symbols", "body": "While scrolling through the API documentation I was missing the information on which headers (for C/C++) the TF (and TFLite) APIs want me to include. As I'm using [IWYU](https://github.com/include-what-you-use/include-what-you-use) it would be nice if the documentation could provide clear guidance on which headers of the API are regarded as internal implementation details and which headers are meant for use by foreign code.\r\n\r\nQuestions that should be answered by the documentation are (e.g.):\r\n* Is there a central (set of) header(s) to include? If so which?\r\n* Are there certain headers meant for inclusion to group several related includes?\r\n* What headers are not typically meant for use by foreign code?", "comments": ["Hi @BenBE ! Are you looking for the header files in these links? [C header files](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/c) , [Lite header files.](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite). Thank you!", "Hi @mohantym.\r\n\r\nI'm looking at the TFLite C++ API right now for the project I'm working on. So let's look for example at [the TFLite C++ API](https://www.tensorflow.org/lite/api_docs/cc): When looking at the various symbols/types/functions there is a rough description of what things are doing. If I then follow up to look e.g. at the `tflite::InterpreterBuilder` at [here](https://www.tensorflow.org/lite/api_docs/cc/class/tflite/interpreter-builder) I basically see:\r\n\r\n> **tflite::InterpreterBuilder** `#include <interpreter_builder.h>`\r\n> \r\n> Build an interpreter capable of interpreting model.\r\n\r\n![image](https://user-images.githubusercontent.com/111196/159364082-d857fb7d-c6d1-42d4-92fc-914ba3cffdcc.png)\r\n\r\nWhile the header file here seems to be included in this case, it's quite easy to miss, and a better layout would probably be:\r\n\r\n> **tflite::InterpreterBuilder**\r\n> `#include <interpreter_builder.h>`\r\n> \r\n> Build an interpreter capable of interpreting model.\r\n\r\nor \r\n\r\n> `#include <interpreter_builder.h>`\r\n> **tflite::InterpreterBuilder**\r\n> \r\n> Build an interpreter capable of interpreting model.\r\n\r\nI.e. force the header line to start on a fresh line. A site that tackles this quite well is e.g. [cpp-reference](https://en.cppreference.com/w/cpp/utility).\r\n\r\nWhere the header specs get confusing is with [`tflite::OpResolver`](https://www.tensorflow.org/lite/api_docs/cc/class/tflite/op-resolver#classtflite_1_1_op_resolver) which describes two types on that page: `tflite::OpResolver` and `tflite::TfLiteDelegatePtrVector`. For the latter it's not evident that the include from above still holds. Also `tflite::TfLiteDelegatePtrVector` is not listed in the overall overview page (like probably others too).\r\n\r\nSo while for the top-level classes there seems to be a hint, it's not exactly intuitive or readily visible. For some other types (aliases?) it's not obvious whether they just inherit the header from the page they are documented/mentioned on.\r\n\r\n---\r\n\r\nThe second part of my issue boils down to a question of whether `#include \"tensorflow/lite/kernels/register.h\"` is intended public API or if another header should be used instead. In particular which headers are intended public API (I guess `#include \"tensorflow/lite/interpreter_builder.h\"` is) and which headers should be treated as private implementation details? Unfortunately given both the code and the docs I couldn't quite figure out which of the various headers (in the directories you linked above) should be used as public API and which ones you should avoid (like things in `\"internal/*\"` and `\"kernels/*\"` probably? It would be nice if the documentation could be extended to include some information on this. Or if a central header (like `opencv2/opencv.hpp`) could be added, that lists all header files belonging to the public part of the API.\r\n\r\nI hope this answer sheds some more light on the information that would be nice to see added to/clarified in the docs."]}, {"number": 55290, "title": "tf.nn.ctc_loss documentation misleads about GPU support, leading to 50x performance difference", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/nn/ctc_loss\r\n\r\n## Description of issue (what needs changing):\r\n\r\n[The documentation for `tf.nn.ctc_loss`](https://github.com/tensorflow/tensorflow/blob/v2.8.0/tensorflow/python/ops/ctc_ops.py#L790) states:\r\n> On TPU and GPU: Only dense padded labels are supported.\r\n> On CPU: Caller may use SparseTensor or dense padded labels but calling with a SparseTensor will be significantly faster.\r\n\r\nHowever, reading through [the code for the CTC loss implementation](https://github.com/tensorflow/tensorflow/blob/v2.8.0/tensorflow/python/ops/ctc_ops.py#L921-L957) shows that SparseTensors _are_ supported on GPU, and in practice, the SparseTensor code path uses CUDNN and is about 50x faster than the non-sparse code path.\r\n\r\nI have tested this on GPU, but not on TPU, where I expect the documentation may be accurate (as the CUDNN code path doesn't apply to TPU). \r\n\r\n### Submit a pull request?\r\n\r\nWould be happy to, after confirmation that this is indeed an issue.\r\n\r\ncc @kaixih, @f90", "comments": []}, {"number": 55282, "title": "TFX Starter Tutorial Typo", "body": "Hello,\r\n\r\nthere is a typo in the TFX [starter tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple#run_the_pipeline).\r\n\r\nLocalDagRunner provides fast iterations for **developemnt** and debugging. TFX also supports other orchestrators including Kubeflow Pipelines and Apache Airflow which are suitable for production use cases.", "comments": ["Hi @LukasS91 !  Could you please post this on [TFX ](https://github.com/tensorflow/tfx)github repo? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "@LukasS91, \r\n\r\nThanks for reporting. Created [PR](https://github.com/tensorflow/tfx/pull/4773) to fix this issue. Thanks!\r\n"]}, {"number": 55273, "title": "EIGEN_DEVICE_FUNC annotation cleanup", "body": "Fix some compiler warnings, and reduce chance of future bugs being introduced where device functions call host-only functions.", "comments": ["@nluehr Can you please resolve conflicts? Thank you!"]}, {"number": 55271, "title": "Update literal_comparison.h - Returns a comparison function", "body": "Returns a comparison function using the provided key function on each value", "comments": ["@r4nt Can you please review this PR ? Thank you!"]}, {"number": 55268, "title": "Build TensorFlow Lite for iOS on macOS Mojave", "body": "Hello,\r\nI'm trying to build TensorFlow Lite 2.8.0 for iOS on an Intel MacBook Pro with macOS Monterey. \r\n\r\nI'm following [this link](https://www.tensorflow.org/lite/guide/build_ios?hl=en)\r\n\r\nbut whan I execute the command:\r\n\r\n`bazel build --config=ios_fat -c opt //tensorflow/lite/ios:TensorFlowLiteC_framework`\r\n\r\nI get this error:\r\n\r\n```\r\nERROR: .../tensorflow/tensorflow/lite/ios/BUILD:104:21: Bundling Preprocessed_TensorFlowLiteC_framework failed: (Exit 127): bundletool failed: error executing command \r\n  (cd /private/var/tmp/_bazel_gcutri/7cb05f4ce32b1b08677be1af22c0dd6b/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n  bazel-out/host/bin/external/build_bazel_rules_apple/tools/bundletool/bundletool bazel-out/applebin_ios-ios_armv7-opt-ST-36ad1e8d6baf/bin/tensorflow/lite/ios/Preprocessed_TensorFlowLiteC_framework-intermediates/bundletool_control.json)\r\nExecution platform: @local_execution_config_platform//:platform\r\nenv: python: No such file or directory\r\n```\r\n\r\nIt seems that python is not found when executing the script bundletool. I installed both python 3.9 and 2.7.\r\n\r\nPlease help", "comments": ["Hi, looks like this is a python location issue. When you ran [configure.py](https://github.com/tensorflow/tensorflow/blob/master/configure.py), did you specify the location of python? You should specify it, otherwise the default one is used.\r\n\r\nOne possible scenario is that python 3 is used, but it is finding `/usr/bin/python` folder. Find your python3 path with `whereis python3` and maybe create a symlink to it: `sudo ln -s /usr/bin/python3 /usr/bin/python`. Hope this could help!", "Hi yishuangP,\r\nthank you for your response. I have already tried to specify the location of python (trying different paths) but I don't think that's the problem since many python scripts are correctly executed before that problem occurs.\r\n\r\nI have also already tried to create the symbolic link but it didn't help.\r\n\r\nI tried to run the command `exec env - bazel-out/host/bin/external/build_bazel_rules_apple/tools/bundletool/bundletool` from command line and still got the problem but if I write `python bazel-out/host/bin/external/build_bazel_rules_apple/tools/bundletool/bundletool`, the script is executed (but I get a different error at runtime), so python is not found only when I use `exec env -`.\r\n\r\nHow can I solve this?", "This is weird. I wonder if this is because you have two versions of python installed. And I think this is a bazel build issue. How about `python3 bazel-out/host/bin/external/build_bazel_rules_apple/tools/bundletool/bundletool`? Does it execute? `exec env -` clears the environment variables before executing. Can you try removing the old version? Sorry I only have python 3 installed and I can execute the command fine.", "Yes, that command executes without `exec env -`.\r\n\r\nAnyhow, the problem is not related to the fact that I have python 2 installed because there was also before I installed it (actually, I installed python 2 to try to solve it).\r\n\r\nI think, maybe, the problem is related to the fact that I have a fresh installation of macOS Mojave (that does not come with python pre-installed anymore) and I installed python 3.9 from the official web site. Maybe this setup is different from an update to Mojave from a previous version.", " Hi is your problem resolved? Maybe the following commands could help \r\n ```\r\n # Setup Python\r\n  PYTHON_BIN_PATH=$(which python3)\r\n  export PYTHON_BIN_PATH\r\n  # Update /usr/bin/python3 for Bazel's host tool execution.\r\n  sudo rm -f /usr/bin/python3\r\n  sudo ln -s \"${PYTHON_BIN_PATH}\" /usr/bin/python3\r\n```"]}, {"number": 55267, "title": "[RNN] Max and min for dynamic tensors should be recorded during calibration: Failed for tensor arg1 Empty min/max for tensor arg1", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- TensorFlow installation (pip package or built from source): pip\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): 2.8.0\r\n\r\n### 2. Code\r\n\r\nProvide code to help us reproduce your issues using one of the following options:\r\n\r\n[Colab Link\r\n](https://colab.research.google.com/drive/1FMUCqyB48x88mEwhIJA1KM1SKA1SDWwb?usp=sharing\r\n)\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n# Load MNIST dataset\r\nmnist = tf.keras.datasets.mnist\r\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\r\n\r\n# Normalize the input image so that each pixel value is between 0 to 1.\r\ntrain_images = train_images / 255.0\r\ntest_images = test_images / 255.0\r\n\r\n# Define the model architecture\r\nmodel = tf.keras.Sequential([\r\n  tf.keras.layers.InputLayer(input_shape=(28, 28)),\r\n  tf.keras.layers.GRU(5,return_sequences=True),\r\n  tf.keras.layers.Flatten(),\r\n  tf.keras.layers.Dense(10)\r\n])\r\n\r\n# Train the digit classification model\r\nmodel.compile(optimizer='adam',\r\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n              metrics=['accuracy'])\r\nmodel.fit(\r\n  train_images,\r\n  train_labels,\r\n  epochs=1,\r\n  validation_data=(test_images, test_labels)\r\n)\r\n\r\n# Wrap it into function (like in https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/experimental_new_converter/Keras_LSTM_fusion_Codelab.ipynb )\r\nrun_model = tf.function(lambda x: model(x))\r\nBATCH_SIZE = 1\r\nconcrete_func = run_model.get_concrete_function(\r\n    tf.TensorSpec([BATCH_SIZE, model.input.shape[1], model.input.shape[2]], model.inputs[0].dtype))\r\n\r\nMODEL_DIR = \"keras_model\"\r\nmodel.save(MODEL_DIR, save_format=\"tf\", signatures=concrete_func)\r\n\r\n# Convert it using integer quantization with int16 activations\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]\r\nconverter.inference_input_type = tf.int16\r\nconverter.inference_output_type = tf.int16\r\n\r\nmnist_train, _ = tf.keras.datasets.mnist.load_data()\r\nimages = tf.cast(mnist_train[0], tf.float32) / 255.0\r\nmnist_ds = tf.data.Dataset.from_tensor_slices((images)).batch(1)\r\ndef representative_data_gen():\r\n  for input_value in mnist_ds.take(10):\r\n    # Model has only one input so each data point has one element.\r\n    yield [input_value]\r\nconverter.representative_dataset = representative_data_gen\r\n\r\ntflite_16x8_model = converter.convert()\r\n```\r\nOutput:\r\n`RuntimeError: Max and min for dynamic tensors should be recorded during calibration: Failed for tensor arg1\r\nEmpty min/max for tensor arg1`\r\n\r\nI'm trying to quantize this model using 8 bit for weights and 16 bit for activations. Code works perfectly if full 8 bit quantization (including activations) is used or if the GRU layer is removed. So the experimental 8x16 bit quantization seems to fail for GRUs.\r\n\r\n### 4. (optional) RNN conversion support\r\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n\r\n### 5. (optional) Any other info / logs\r\nflatbuffers downgraded to version 1.12, got error otherwise (same as in https://github.com/tensorflow/tensorflow/issues/51590#issuecomment-912614740 ). ", "comments": ["@schmiph2 ,\r\nCan you please take a look at this SO [link](https://stackoverflow.com/questions/64968250/how-to-solve-runtime-error-empty-min-max-for-tensor-cast-while-doing-post-train) and the [issue](https://github.com/tensorflow/tensorflow/issues/39718) with the similar error.It helps.Thanks!", "Unfortunately, these links didn't help me. The problem is really just with the [16x8 quantization](https://www.tensorflow.org/lite/performance/post_training_integer_quant_16x8) of the GRU layer. If I remove the GRU layer, I can use the 16x8 quantization. Furthermore, when I switch to 8x8 quantization, everything works fine (including the GRU layer).\r\n\r\nIf you want to test it yourself without the GRU layer, please comment out the line `tf.keras.layers.GRU(5,return_sequences=True)` in the [colab notebook](https://colab.research.google.com/drive/1FMUCqyB48x88mEwhIJA1KM1SKA1SDWwb?usp=sharing).", "Hi @sachinprasadhs ! Could you please look at this issue? It is replicating in  [2.8](https://colab.sandbox.google.com/gist/mohantym/0dc1a1da6c14d21d7106845e79ad3b00/gru_post_training_integer_quant_16x8.ipynb#scrollTo=BzYaZhHk8ZAR) and throwing different error in [2.7](https://colab.sandbox.google.com/gist/mohantym/fe9acca2be0a5de06d1ebcf63f3f148b/gru_post_training_integer_quant_16x8.ipynb#scrollTo=BzYaZhHk8ZAR), [nightly](https://colab.sandbox.google.com/gist/mohantym/87f59acdd3648a353692946e745c993d/gru_post_training_integer_quant_16x8.ipynb#scrollTo=hWSAjQWagIHl) ."]}, {"number": 55264, "title": "[tflite] replace non-ASCII single quote", "body": "Non-ASCII single quotes prevent the code from successful\r\ncompilation on windows 10 + vs 2019 + msys2", "comments": []}, {"number": 55260, "title": "List of TF ops that tf.vectorized_map supports", "body": "Where can I get this information? \r\n\r\nWhich tf operation is a vectorized operation? and which one is not?\r\n\r\n", "comments": ["@Suzan009 \r\nPlease refer this [link ](https://www.tensorflow.org/api_docs/python/tf/vectorized_map) and let us know if it helps?For any further queries please post this issue in TF discussion [forum](https://discuss.tensorflow.org/) as there is a larger community to help.Thanks!", "It seems that [is not available](https://github.com/keras-team/keras-cv/pull/146#issuecomment-1063495630) so it Is a valid feature request.\r\n\r\nAlso, If you are looking instead for XLA supported ops, Markdowns tables are not updated by years as you can find in the related ticket https://github.com/tensorflow/tensorflow/issues/14798#issuecomment-1047796247\r\n\r\n/cc @wangpengmit @rohan100jain \r\n", "The solution should be similar to https://github.com/tensorflow/tensorflow/issues/14798#issuecomment-1047796247, an auto-gen tool that dumps `tf.vectorized_map`'s internal registration table to a markdown file.", "> The solution should be similar to https://github.com/tensorflow/tensorflow/issues/14798#issuecomment-1047796247, an auto-gen tool that dumps `tf.vectorized_map`'s internal registration table to a markdown file.\n\nYes but we also need to find an internal orchestration owner cause orchestration It is not opensoruce/visibile to the community "]}, {"number": 55252, "title": "Add zero point check for kTfLiteInt16 and scale check for multiple operators", "body": "Hello, have added zero point checks and scale checks to the following operators\r\n\r\narg_min_max.cc\r\ngather.cc\r\ngather_nd.cc\r\nmul.cc\r\npack.cc\r\npad.cc\r\npooling.cc\r\nresize_bilinear.cc\r\nresize_nearest_neighbor.cc\r\nselect.cc\r\nslice.cc\r\ntranspose.cc\r\nunpack.cc", "comments": ["@smpurkis could you share the motivation of this change? Why are these runtime check necessary?", "@abattery Of course, there are two checks that have been added, a zero point check, such as in\r\n`tensorflow/lite/kernels/mul.cc`\r\n```\r\n  if (output->type == kTfLiteInt16) {\r\n    TF_LITE_ENSURE_EQ(context, input1->params.zero_point, 0);\r\n    TF_LITE_ENSURE_EQ(context, input2->params.zero_point, 0);\r\n    TF_LITE_ENSURE_EQ(context, output->params.zero_point, 0);\r\n  }\r\n```\r\nthis ensures that for `int16` the constraint that the zero point is 0 is met.\r\n\r\nThe second check (less commonly added) is a scale check that is applied to `uint8`, `int8`, `int16`, such as in \r\n`tensorflow/lite/kernels/resize_bilinear.cc`\r\n```\r\n  if (input->type == kTfLiteUInt8 || input->type == kTfLiteInt8 ||\r\n      input->type == kTfLiteInt16) {\r\n    TF_LITE_ENSURE_EQ(context, input->params.scale, output->params.scale);\r\n```\r\nthis ensures that the scaling between input and output is the same.\r\n\r\nThese checks already exist in the majority of operators. The ones I've added, seem to be missing from the operators I've added them to.\r\nFor example, `tensorflow/lite/kernels/add.cc` has the zero point check [here](https://github.com/tensorflow/tensorflow/blob/5bde0752f9a4f7fd1991463aa15dd388cc3c4644/tensorflow/lite/kernels/add.cc#L139). While `tensorflow/lite/kernels/mul.cc` doesn't.\r\n\r\nFurthermore, I found that a couple of the pooling tests, that test `int16` had the zero point being non-zero. Thus failing once I had added the zero point checks to `tensorflow/lite/kernels/pooling.cc`.", "@abattery  Can you please assist on above comments from @smpurkis. Thank you!"]}, {"number": 55249, "title": "Add mhlo.transpose splat constant folding", "body": "This PR is for splat constant folding.  \r\nShould we fold a nonsplat constant? (need to manipulate constant's value)", "comments": []}, {"number": 55246, "title": "Generalise `xla::Map` to functions over arbitrary shapes", "body": "**System information**\r\n- TensorFlow version (you are using): 2.8\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nCurrently `xla::Map` only allow functions from shape [] to []. I'd like to use functions with arbitrary shapes. For example, apply a function with shapes [2, 3] -> [5] to a tensor [100, 4, 2, 3] to get a [100, 4, 5].\r\n\r\nI've not yet thought about how it would work when mapping multiple input tensors at once.\r\n\r\n**Will this change the current api? How?**\r\n\r\nIt would extend `xla::Map`, either internally, or as an overload. It's possible additional information would need to be passed to a more general implementation of map, in which an overload may be preferable to maintain backwards compatibility.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nAnyone using XLA who'd like to apply a function over sections of a tensor. It would effectively be an alternative to (some portion of) broadcasting, allowing people to use functions that don't allow leading dimensions to tensors with leading dimensions.\r\n\r\nIt is particularly useful for me as I'm working with dependent types and adding leading dimensions is non-trivial (and verbose) in type signatures.\r\n\r\n**Any Other info.**\r\n\r\nIt is already possible to do this by indexing into the tensor and iteratively applying the function to the contents, then concatenating the results, but I expect this is significantly slower than could be achieved within XLA. I have considered using `xla::While` for this but I still expect the slicing and concatenation would still come with a significant performance cost.", "comments": []}, {"number": 55243, "title": "tf.random.poisson hangs on nan input", "body": "Running on colab.research.google.com (`v2.8.0-0-g3f878cff5b6` at time of writing).\r\n\r\nLooking at this code.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport math\r\n\r\nprint(tf.version.GIT_VERSION, tf.version.VERSION)\r\n\r\nfor lam in [3.2,-3.2,math.inf,-math.inf,math.nan]:\r\n  print(lam,tf.random.poisson((1,),tf.convert_to_tensor(lam).numpy()))\r\n\r\n```\r\n\r\n**Describe the current behavior**\r\n\r\n`tf.random.poisson` currently seems to handle nonsensical input as follows\r\n- Negative values --> return 0\r\n- +infinite values --> return infinity\r\n- -infinite values --> return 0\r\n- nan values --> freeze program completely (presumably running some rejection sampling algorithm forever??)\r\n\r\nAll of these seem okay except the last one.  \r\n\r\n**Describe the expected behavior**\r\n\r\nI have no strong opinions here.  I suppose my intuition would be \r\n- Negative values --> return nan\r\n- +infinite values --> return infinity\r\n- -infinite values --> return nan\r\n- nan values --> return nan\r\n\r\nBut there may be good reasons for other behavior.  \r\n\r\nAt minimum, I suppose something should be added to the documentation indicating what expected behavior is (or that behavior is undefined in some cases).  \r\n", "comments": ["@chunduriv ,\r\nI was able to reproduce the issue in tf v2.8, v2.7 and nightly.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/aa2046d9316bb90d5a6356601133b843/untitled249.ipynb)."]}, {"number": 55237, "title": "`tf.strided_slice()` option for `inclusive_end`", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): Latest (2.8)\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nI am maintainer of the [R interface to tensorflow](https://github.com/rstudio/tensorflow). The convention in R is for subsetting with `[` to be 1-based, and inclusive of the slice end. In most situations it is trivial to adapt the `[` tensor method in R to Python convention of 0-based indices and a half-open interval. Except for one case: when the slice end is also the first element of the container.\r\n\r\nFor example,\r\n```\r\nx = [1, 2, 3, 4, 5, 6]\r\n```\r\nAn R call like `x[3:1]` would return `[3, 2, 1]`. To translate this to Python, the call would be `x[2::-1]`. This translation is easy enough to do in eager mode, but it's difficult in graph mode if the user supplied the slice end as a tensor, because that would make the dtype of `end` a varient (An integer or `None`, depending on the tensor value at run time).\r\n\r\nWrapping the `end` in a `tf.experimental.Optional()` doesn't work, nor does `tf.strided_slice()` accept a tensor value for `end_mask`. This means that there is no way to faithfully translate an R call like `x[start:end]` if `end` is a tensor and `end` could potentially resolve to the first element of the tensor.\r\n\r\n\r\n**Will this change the current api? How?**\r\n\r\nThere are a couple of ways to resolve this, but the simplest I believe is to add an argument with default to `tf.strided_slice(..., inclusive_end=False)`. This would be no breaking change to existing code. From the R interface, we can then call `tf.strided_slice(..., inclusive_end = True)`.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nAll Tensorflow users from R, as well as interfaces to Tensorflow from other languages with 1-based and closed interval slice semantics (e.g., Julia).\r\n\r\n**Any Other info.**\r\n\r\nHaving an additional arg `tf.strided_slice(..., one_based = False)` would be icing on the cake :). Having `tf.strided_slice()` automatically infer when `stride` should be negative would be the cherries on top :)", "comments": ["Thanks for identifying the problem! This is a reasonable feature request. Unfortunately I'm not sure we'll have bandwidth for it in the near term.", "Thanks for responding. This would be a great quality-of-life improvement for R users, and I'd be willing to help how I can implement it. Would you be open to accepting a PR for this?", "Yeah I'm happy to accept a PR for this. Your contribution will be appreciated!"]}, {"number": 55234, "title": "MutexLock should not segfault given a non-mutex", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): N/A\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):2.8.0\r\n- Python version:3.7.12\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n\r\n**Describe the current behavior**\r\nGiven the following code:\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ntf.raw_ops.MutexLock(mutex=np.shape(a=4))\r\n```\r\nThis currently crashes the colab notebook. On a different machine, it leads to a segfault.\r\n\r\n**Describe the expected behavior**\r\n\r\nTensorflow should throw an exception rejecting the invalid argument.  Tensorflow should reject the `mutex` argument since it's not a mutex resource (`A Tensor of type resource`, based on https://www.tensorflow.org/api_docs/python/tf/raw_ops/MutexLock)\r\n\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nhttps://colab.research.google.com/drive/1cwMDIYX9pruA1eG22eYykAxlbhDayjT8?usp=sharing\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ntf.raw_ops.MutexLock(mutex=np.shape(a=4))\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@chunduriv I was able to replicate the issue on colab using TF v2.8.0 and tf-nightly, please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/1698f2822d4c658961418a0dd5b52f2d/55234.ipynb)  for reference.Thanks!"]}, {"number": 55231, "title": "Fix softmax bug for ragged tensors", "body": "The current implementation of softmax for ragged tensors is numerically unstable. In particular, the `exp()` function is computed on the raw logits. This causes an overflow when the logits are large positive. When the logits are large negative, this leads to an underflow and division by zero. The problem manifests through `nan` entries in the ragged tensor.\r\n\r\nIn this PR I (hopefully) fix the ragged tensor implementation and implement a corresponding test.\r\n\r\nFor reference of a numerically stable implementation, consider the [numpy implementation](https://github.com/maresb/tensorflow/blob/8e75354bae33d10bd43091534ed5e1c1062d85db/tensorflow/python/ops/nn_test.py#L97-L104) in the test suite.\r\n\r\n(Recall that the softmax function is invariant under adding an overall constant to the input logits. Before applying `exp()` it is advantageous to subtract the max so that one logit component is zero and the rest are negative. This ensures that the denominator is numerically finite.)", "comments": ["Oops, I seem to have messed up something. :frowning_face: \r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/nn_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1493, in decorated\r\n    f(self, *args, **kwargs)\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/nn_test_cpu.runfiles/org_tensorflow/tensorflow/python/ops/nn_test.py\", line 164, in testRagged\r\n    self.assertAllClose(y_tf, expected, eps)\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/nn_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1533, in decorated\r\n    return f(*args, **kwds)\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/nn_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 3108, in assertAllClose\r\n    self._assertAllCloseRecursive(a, b, rtol=rtol, atol=atol, msg=msg)\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/nn_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 3012, in _assertAllCloseRecursive\r\n    return self._assertRaggedClose(a, b, rtol, atol, msg)\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/nn_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 3596, in _assertRaggedClose\r\n    self._assertListCloseRecursive(a_list, b_list, rtol, atol, msg)\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/nn_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 3608, in _assertListCloseRecursive\r\n    self._assertListCloseRecursive(a[i], b[i], rtol, atol, msg,\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/nn_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 3606, in _assertListCloseRecursive\r\n    self.assertLen(a, len(b), \"Length differs for %s\" % path)\r\nAssertionError: [0.6652409434318542, 0.2447284758090973, 0.09003057330846786] has length of 3, expected 1. : Length differs for value[0]\r\n```", "Ok, it seems to be succeeding now. \ud83d\ude05 ", "@rohan100jain  Can you please review this PR ? Thank you!"]}, {"number": 55230, "title": "Incorrect calculation of 2nd derivative of a determinant of a matrix", "body": "\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS Monterey.  Also seen on Linux.\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.8.0-rc1-32-g3f878cff5b6 2.8.0 (also seen in 2.6.X)\r\n- Python version: 3.8.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A, but also seen in cuda11.4\r\n- GPU model and memory: CPU, but also seen in A100 40GB/80GB\r\n\r\n**Describe the current behavior**\r\n\r\nThe calculation of the second derivative of a determinant is incorrect. \r\n\r\nIt is a little challenging to describe the full issue in markdown.  I've created a standalone notebook to reproduce the issue.  I compare the TF gradients through a determinant against both finite differences and a custom (albeit slow) determinant implementation.  The TF gradients for the determinant of a matrix, let's call them G, agree with both finite differences and the custom op.  The second derivative, dG/dx, aka the Hessian of the determinant of a matrix, is badly incorrect.  Using autodifferentiation on the custom determinant operation twice, however, agrees well with finite differences methods.\r\n\r\nFull reproducer available in this notebook (standalone) here: https://github.com/Nuclear-Physics-with-Machine-Learning/MLQM/blob/spin/examples/2nd%20Derivative%20of%20Determinant%20of%20a%20matrix.ipynb\r\n\r\n**Describe the expected behavior**\r\n\r\nThe calculation of the second derivative of a determinant should be correct.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):  If we can track down the bug, and it's something i can fix, I could.  I haven't modified yet.\r\n- Briefly describe your candidate solution(if contributing):  I believe the operations registered for the backprop through a determinant of a matrix must be missing something, and this could be fixed.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nhttps://github.com/Nuclear-Physics-with-Machine-Learning/MLQM/blob/spin/examples/2nd%20Derivative%20of%20Determinant%20of%20a%20matrix.ipynb\r\n\r\n", "comments": ["@gadagashwini ,\r\nI was able to reproduce the issue in tf [v2.8](https://colab.research.google.com/gist/tilakrayal/0bd5fcb3ec4eebee43faf38bd81a5b13/55230.ipynb), [v2.7](https://colab.research.google.com/gist/tilakrayal/5683fc7edd39e62db6d23bf70d7e07a7/2-7-55230.ipynb) and [nightly](https://colab.research.google.com/gist/tilakrayal/cbe58602a2e27c83d33aa0282bd24376/nightly55230.ipynb).Please find the gist attached.", "Thanks for reproducing it.  I wasn't able to track down where this originates in Tensorflow.  I found the function that computes the derivative of det(A), but I don't know enough about tensorflow or the linalg package to try debugging this.  How can I help?", "Hi there, any thoughts on this?  Anything I can do to help?  "]}, {"number": 55226, "title": "Tracker for TensorFlow-DirectML in TF 2.9.0", "body": "cc: @PatriceVignola @wchao1115\r\n\r\nThis issue tracks pending PRs, issues, and possible cherry-picks necessary for TensorFlow-DirectML for TF 2.9.0 release. Please post a comment with new things to track and I will update this post to reflect the changes.\r\n\r\nNew PRs:\r\n* #51759 [Touches C API interface. Pending re-review]\r\n* https://github.com/tensorflow/tensorflow/pull/55544 \r\n* https://github.com/tensorflow/tensorflow/pull/55557\r\n* https://github.com/tensorflow/tensorflow/pull/55558\r\n* https://github.com/tensorflow/tensorflow/pull/55579\r\n* https://github.com/tensorflow/tensorflow/pull/55677\r\n* https://github.com/tensorflow/tensorflow/pull/55678\r\n\r\nPRs that need more investigation.\r\n* #55382 [A similar PR broke tests before so this can't go in until the issue is resolved.]\r\n* https://github.com/tensorflow/tensorflow/pull/55381 \r\n    * Reverted because it broke internal tests. \r\n\r\n\r\nPRs that made it into TF nightly / TF 2.10:\r\n* https://github.com/tensorflow/tensorflow/pull/55379\r\n    * Resubmitted in https://github.com/tensorflow/tensorflow/pull/55640\r\n\r\nPRs that made it into TF 2.9:\r\n* #55173 \r\n* #54468 \r\n* #54139\r\n* #51758\r\n* https://github.com/tensorflow/tensorflow/pull/55384 \r\n* https://github.com/tensorflow/tensorflow/pull/55385 \r\n* https://github.com/tensorflow/tensorflow/pull/55386 \r\n* https://github.com/tensorflow/tensorflow/pull/55387 \r\n* https://github.com/tensorflow/tensorflow/pull/55392\r\n* https://github.com/tensorflow/tensorflow/pull/55393\r\n* https://github.com/tensorflow/tensorflow/pull/54746\r\n* https://github.com/tensorflow/tensorflow/pull/55395\r\n\r\nClosed PR:\r\n* #55362", "comments": ["@penpornk How can pluggable devices implement the `Assign` op which uses the legacy variables? `TF_AssignVariable` already exists to implement `AssignVariableOp` which uses resources, but there doesn't seem to be an equivalent for the legacy variables. Should we add a `TF_AssignRefVariable` function?", "@PatriceVignola \r\n\r\n> @penpornk How can pluggable devices implement the `Assign` op which uses the legacy variables? `TF_AssignVariable` already exists to implement `AssignVariableOp` which uses resources, but there doesn't seem to be an equivalent for the legacy variables. Should we add a `TF_AssignRefVariable` function?\r\n\r\nLooping in @wangpengmit for more info on variables.", "Resource and ref vars are very different, so my first thought is that `TF_AssignRefVariable` is probably needed.", "I have a working prototype in a fork that I put in the kernels_experimental header. I can submit a PR if that's ok with everyone.\r\n\r\nAlso, my understanding is that ref vars are deprecated in TF2 and are replaced by resources at the python API level. Is that correct? But even though they are deprecated, some popular benchmarks like AI-Benchmark use a frozen TF1 model when running on TF2, which yield bad results for pluggable devices since they don't currently support them.", "> my understanding is that ref vars are deprecated in TF2 and are replaced by resources at the python API level. Is that correct? But even though they are deprecated, some popular benchmarks like AI-Benchmark use a frozen TF1 model when running on TF2.\r\n\r\nYes, ref vars are deprecated at the Python level. Existing TF1 models may still be using them, so TF2 internals still support them.", "> I have a working prototype in a fork that I put in the kernels_experimental header. I can submit a PR if that's ok with everyone.\r\n\r\n@PatriceVignola Oh, that's great! Please submit the PR and we can continue the discussion there (e.g., whether this needs to be an RFC, etc). Thank you very much! :)", "@penpornk I submitted a PR here: https://github.com/tensorflow/tensorflow/pull/55379", "New PRs to add to the list:\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/55381\r\nhttps://github.com/tensorflow/tensorflow/pull/55382\r\nhttps://github.com/tensorflow/tensorflow/pull/55384\r\nhttps://github.com/tensorflow/tensorflow/pull/55385\r\nhttps://github.com/tensorflow/tensorflow/pull/55386\r\nhttps://github.com/tensorflow/tensorflow/pull/55387", "3 new PRs:\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/55392\r\nhttps://github.com/tensorflow/tensorflow/pull/55393\r\nhttps://github.com/tensorflow/tensorflow/pull/55395", "@penpornk A simple PR that was opened a month ago. I think it flew under the radar.\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/54746", "New PR: https://github.com/tensorflow/tensorflow/pull/55544", "New PR: https://github.com/tensorflow/tensorflow/pull/55557", "New PR: https://github.com/tensorflow/tensorflow/pull/55558", "New PR: https://github.com/tensorflow/tensorflow/pull/55579", "@penpornk We noticed that support for `TF_VARIANT` is not really available for pluggable devices through the API since it's very hard (or even impossible) to read the content of a C++ object through ABIs. Even if we were to use the exact same headers as TensorFlow uses for all C++ objects, we would most likely need to use the exact same compiler.\r\n\r\nSince some models depend heavily on variant tensors which contain TensorList objects, we'd like to propose 2 new APIs that address this issue:\r\n\r\n```cpp\r\nTF_CAPI_EXPORT extern void TF_AddNVariant(\r\n    TF_OpKernelContext* ctx,\r\n    void (*binaryAddFunc)(TF_OpKernelContext* ctx, const TF_Tensor* a, const TF_Tensor* b, TF_Tensor* out),\r\n    TF_Status* status);\r\n\r\nTF_CAPI_EXPORT extern void TF_ZerosLikeVariant(\r\n    TF_OpKernelContext* ctx,\r\n    void (*zerosLikeFunc)(TF_OpKernelContext* ctx, const TF_Tensor* input, TF_Tensor* out),\r\n    TF_Status* status);\r\n```\r\n\r\nLike the `TF_AssignVariable` and `TF_AssignUpdateVariable` functions currently available in `kernels_experimental.h`, the goal of `TF_AddNVariant` and `TF_ZerosLikeVariant` would be to allow plugins to implement those 2 key operations by treating the Variant objects as a black box. The Variant objects would be unwrapped within TensorFlow core, which would then call the `binaryAddFunc` or `zerosLikeFunc` functions provided by the user, which are guaranteed to contain tensors of primitives (e.g. `TF_FLOAT`).\r\n\r\nIs this something that the TensorFlow team would like to see an RFC or PR for? For one of the RNN models that we track (Pixel-RNN), we see up to a 5x performance improvement by not having to do those operations on the CPU.", "> @penpornk We noticed that support for `TF_VARIANT` is not really available for pluggable devices through the API since it's very hard (or even impossible) to read the content of a C++ object through ABIs. Even if we were to use the exact same headers as TensorFlow uses for all C++ objects, we would most likely need to use the exact same compiler.\r\n\r\nYes, we currently don't have a generic way to support `TF_VARIANT`/`DT_VARIANT`. Even if you use the exact same header, compiler, and compiler flags, it's possible something else could still go wrong. The support so far has been on an op-by-op basis, e.g., [Kernel C API extension for Variable ops](https://github.com/tensorflow/community/pull/387), [TensorList support](https://github.com/tensorflow/tensorflow/pull/51822), and your recent PRs.\r\n\r\n> Since some models depend heavily on variant tensors which contain TensorList objects, we'd like to propose 2 new APIs that address this issue:\r\n> \r\n> ```c++\r\n> TF_CAPI_EXPORT extern void TF_AddNVariant(\r\n>     TF_OpKernelContext* ctx,\r\n>     void (*binaryAddFunc)(TF_OpKernelContext* ctx, const TF_Tensor* a, const TF_Tensor* b, TF_Tensor* out),\r\n>     TF_Status* status);\r\n> \r\n> TF_CAPI_EXPORT extern void TF_ZerosLikeVariant(\r\n>     TF_OpKernelContext* ctx,\r\n>     void (*zerosLikeFunc)(TF_OpKernelContext* ctx, const TF_Tensor* input, TF_Tensor* out),\r\n>     TF_Status* status);\r\n> ```\r\n> \r\n> Like the `TF_AssignVariable` and `TF_AssignUpdateVariable` functions currently available in `kernels_experimental.h`, the goal of `TF_AddNVariant` and `TF_ZerosLikeVariant` would be to allow plugins to implement those 2 key operations by treating the Variant objects as a black box. The Variant objects would be unwrapped within TensorFlow core, which would then call the `binaryAddFunc` or `zerosLikeFunc` functions provided by the user, which are guaranteed to contain tensors of primitives (e.g. `TF_FLOAT`).\r\n> \r\n> Is this something that the TensorFlow team would like to see an RFC or PR for? For one of the RNN models that we track (Pixel-RNN), we see up to a 5x performance improvement by not having to do those operations on the CPU.\r\n\r\nThank you for the suggestion! The team thinks this sounds reasonable. If you already have a prototype code for this, would you mind opening a PR? We can take it from there. (If there are some points that need more discussion, we can start an RFC.)\r\n", "Sure! I created a PR here: https://github.com/tensorflow/tensorflow/pull/55645", "2 new PRs:\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/55677\r\nhttps://github.com/tensorflow/tensorflow/pull/55678"]}, {"number": 55222, "title": "Fix invalid examples for `tf.RaggedTensor.__abs__`", "body": "This PR tries to address the issue raised in [#53828](https://github.com/tensorflow/tensorflow/issues/53828) where\r\ntf.tensor Examples was added to tf.RaggedTensor __abs__\r\nReplacing the tensor examples with tf.RaggedTensor examples\r\nThis PR fixes [#53828](https://github.com/tensorflow/tensorflow/issues/53828).", "comments": ["[tf.RaggedTensor](https://www.tensorflow.org/api_docs/python/tf/RaggedTensor) represents usage and examples for RaggedTensor. Since few methods of tf.RaggedTensor such as https://www.tensorflow.org/api_docs/python/tf/RaggedTensor#__abs__ has Dense Tensor examples. \r\n\r\nThis PR fixes issue https://github.com/tensorflow/tensorflow/issues/53828.", "The symbol you are changing is `tf.abs`, for dense tensors", "`tf.RaggedTensor` doc has symbol `__abs__`. Can you please take a look https://www.tensorflow.org/api_docs/python/tf/RaggedTensor#__abs__. Thanks!", "```python\r\n@tf_export(\"math.abs\", \"abs\")\r\n```\r\n\r\nThis means this docstring is attached to `tf.math.abs` and `tf.abs`", "Thank you !\r\nOnly ask here is symbols like `__abs__`, `__add__`, `__and__` ,`__bool__` and etc, are appearing under [tf.RaggedTensor](https://www.tensorflow.org/api_docs/python/tf/RaggedTensor) document. How do we justify the examples given there. Since examples has Dense tensor but document is explaining about the RaggedTensor ", "A RaggedTensor is a subclass of Tensor. If the method is not redefined, the parent class method is used."]}, {"number": 55220, "title": "Failed to use evaluation function from TF, TypeError: tf__loss() after loading a saved model", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 18.04**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): **from pip**\r\n- TensorFlow version (use command below): 2.8\r\n- Python version: 3.10.2\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.2\r\n- GPU model and memory:   A100-PCI 40GB *4\r\n\r\n**Describe the current behavior**\r\n```\r\n# Save and load using High-level API\r\nsiamese.save('test_model')\r\na = tf.keras.models.load_model(\"test_model\", custom_objects={'contrastive_loss': loss})\r\n\r\n## Evaluate the model\r\nresults = a.evaluate([x_test_1, x_test_2], labels_test)\r\nprint(\"test loss, test acc:\", results)\r\n```\r\nAfter saving my trained model (assigned as `siamese`), I loaded the saved model that I just had saved (assigned as `a`) . When I perform evaluate on the loaded model (load `a`). I face this error:\r\n\r\n`TypeError: tf__loss() takes from 0 to 1 positional arguments but 2 were given\r\n`\r\n**Describe the expected behavior**\r\nWhen I tried evaluating with `siamese` model, it worked fine. But if I saved it, loaded it I got the above error, \r\n\r\nHere is my loss function:\r\n```\r\n\r\ndef loss(margin=1):\r\n    def contrastive_loss(y_true, y_pred):\r\n        square_pred = tf.math.square(y_pred)\r\n        margin_square = tf.math.square(tf.math.maximum(margin - (y_pred), 0))\r\n        return tf.math.reduce_mean(\r\n            (1 - y_true) * square_pred + (y_true) * margin_square\r\n        )\r\n    return contrastive_loss\r\n```", "comments": ["@algoteam5 \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "@sushreebarsa \r\n1. Please download this folder named `test_load_model` \r\n`https://drive.google.com/drive/folders/1g7E92X2xwa7yqyIIPqe87tugz-OPy7LY?usp=sharing\r\n`\r\n\r\n2. and then build and run docker as follow\r\n`docker build -t harry/sm-tf-testload:latest . && docker run --name harry_tf_testload --rm --shm-size 8G -it --gpus all harry/sm-tf-testload:latest`\r\n\r\n3. Finally, please run `python testload.py` inside  docker container to reproduce the error\r\n", "@gadagashwini I was able to replicate the issue on colab using TF v2.8.0 and tf-nightly ,please find the gist[ here](https://colab.research.google.com/gist/sushreebarsa/320fde521ea662be6a31e0c9d98ebc5b/gist55220.ipynb) for reference.Thanks!", "@algoteam5,\r\nSeems like issue is with custom loss function. \r\nSince evaluate method takes x_test and y_test\r\n`results = a.evaluate([x_test_1, x_test_2], labels_test)`\r\nChange this to \r\n`results = a.evaluate([x_test_1,x_test_2])`\r\n", "@gadagashwini \r\nWithout the `labels_test`, your code could fix the error but there were no loss and accuracy values.\r\n```\r\n1779/1779 [==============================] - 2s 1ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00\r\n[0.0, 0.0]\r\n```", "@algoteam5, \r\n`results = a.evaluate(x, y)`.\r\n\r\n\r\nx | Input data. It could be:A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs).A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs).A dict mapping input names to the corresponding array/tensors, if the model has named inputs.A\u00a0tf.data\u00a0dataset. Should return a tuple of either\u00a0(inputs, targets)\u00a0or\u00a0(inputs, targets, sample_weights).\r\n-- | --\r\n\r\n\r\n\r\ny | Target data. Like the input data\u00a0x, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with\u00a0x\u00a0(you cannot have Numpy inputs and tensor targets, or inversely). If\u00a0x\u00a0is a dataset, generator or\u00a0keras.utils.Sequence\u00a0instance,\u00a0y\u00a0should not be specified (since targets will be obtained from the iterator/dataset).\r\n-- | --\r\n\r\n", "I dont get this. Have you tried it yourself? `results = a.evaluate(x, y)` will produce the error as in the title. Tensorflow team should work on this problem to resovle it."]}, {"number": 55217, "title": "`tf.experimental.numpy.stack` should check out-of-bound `axis` ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.7.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nx = tf.random.uniform([5])\r\nprint(tf.experimental.numpy.stack(x, axis=-2)) # Outputs a shape [5] tensor\r\n```\r\n\r\n**Describe the current behavior**\r\nIn the above example, the input is a rank-1 tensor, so `axis` cannot be -2. `tf.experimental.numpy.stack` did not check the argument `axis`.\r\n\r\nExpect it to throw an error like `tf.stack`:\r\n```\r\n\r\nimport tensorflow as tf\r\nx = tf.random.uniform([5])\r\ntf.stack(x, axis=-1) # Pass\r\ntf.stack(x, axis=-2) # ValueError: Argument `axis` = -2 not in range [-1, 1)\r\n```\r\n\r\n", "comments": []}, {"number": 55214, "title": "list_ops.empty_tensor_list() crashed when element_shape is a two dimensional tensor", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.6.0, 2.7.0\r\n- Python version: 3.7.11\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nWhen running test cases for list_ops.empty_tensor_list(), the program crashed for core dump if I used 2-d tensor in the attribute of element_shape. The normal case is using 1-d tensor like [1], but getting crashed when using [[1]].\r\n\r\n**Describe the expected behavior**\r\nSince users might misuse some parameters, the program would show ValueError related to incorrect shape information.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```\r\nfrom tensorflow.python.framework import constant_op\r\nfrom tensorflow.python.framework import dtypes\r\nfrom tensorflow.python.ops import list_ops\r\nfrom tensorflow.python.platform import test\r\n\r\nclass TensorsTest(test.TestCase):\r\n\r\n    def test_is_tensor_array(self):\r\n        # this case will pass if using [1] in element_shape\r\n        return list_ops.empty_tensor_list(element_shape=constant_op.constant([[1]], dtype=dtypes.int32),\r\n                                          element_dtype=dtypes.int32) \r\n\r\nif (__name__ == '__main__'):\r\n    test.main()\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\nHere is a snapshot of the error log.\r\n```\r\n[ RUN      ] TensorsTest.test_is_tensor_array\r\n2022-03-14 04:44:22.682042: F tensorflow/core/framework/tensor_shape.cc:46] Check failed: NDIMS == dims() (1 vs. 2)Asking for tensor of 1 dimensions from a tensor of 2 dimensions\r\nFatal Python error: Aborted\r\n\r\nCurrent thread 0x00007ff0c7b02180 (most recent call first):\r\n  File \"/root/anaconda3/envs/tf2.5.0/lib/python3.7/site-packages/tensorflow/python/ops/gen_list_ops.py\", line 49 in empty_tensor_list\r\n  File \"/root/anaconda3/envs/tf2.5.0/lib/python3.7/site-packages/tensorflow/python/ops/list_ops.py\", line 62 in empty_tensor_list\r\n  File \"test_bug/bug2.py\", line 11 in test_is_tensor_array\r\n  File \"/root/anaconda3/envs/tf2.5.0/lib/python3.7/unittest/case.py\", line 628 in run\r\n  File \"/root/anaconda3/envs/tf2.5.0/lib/python3.7/unittest/case.py\", line 676 in __call__\r\n  File \"/root/anaconda3/envs/tf2.5.0/lib/python3.7/unittest/suite.py\", line 122 in run\r\n......\r\n\r\n*** End stack trace ***\r\nAborted (core dumped)\r\n```\r\n", "comments": ["I could able to reproduce the issue on [Tf v2.8](https://colab.sandbox.google.com/gist/sachinprasadhs/e428986a67c7911d9a9362580b339325/-55214.ipynb). Thanks!", "@gadagashwini Could you open the access of the colab?", "@JXRiver , I updated the link with Github Gist."]}, {"number": 55210, "title": "Fix invalid input for `tf.gather_nd` with `batch_dims`", "body": "This PR tries to address the issue raised in #55203 where\r\ninvalid batch_dim (bool) was passed to  tf.gather_nd\r\nwith error output returned silently.\r\nThe reason was that `int()` was applied incorrectly, which always cast\r\nany value to integer.\r\nThis PR fixes #55203.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Hi @yongtang Can you please check @mihaimaruseac's comments and keep us posted ? Thank you!"]}, {"number": 55207, "title": "Is tf2xla  used even when jit_compile=False", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.8\r\n- Python version: 3.9.10\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n**Describe the current behavior**\r\nWith TF_CPP_MAX_VLOG_LEVEL=3 ,  still see lines like the following  in the log.\r\n\r\n\r\n> 10048:2022-03-11 09:50:05.273639: I tensorflow/compiler/tf2xla/xla_op_registry.cc:328] XLA op registration: device: XLA_GPU_JIT op: XlaWhile\r\n10049:2022-03-11 09:50:05.273651: I tensorflow/compiler/tf2xla/xla_op_registry.cc:328] XLA op registration: device: XLA_CPU_JIT op: XlaWhile\r\n10050:2022-03-11 09:50:05.273660: I tensorflow/compiler/tf2xla/xla_op_registry.cc:328] XLA op registration: device: XLA_GPU_JIT op: XlaIf\r\n10051:2022-03-11 09:50:05.273670: I tensorflow/compiler/tf2xla/xla_op_registry.cc:328] XLA op registration: device: XLA_CPU_JIT op: XlaIf\r\n10052:2022-03-11 09:50:05.273680: I tensorflow/compiler/tf2xla/xla_op_registry.cc:328] XLA op registration: device: XLA_GPU_JIT op: StatelessIf\r\n10053:2022-03-11 09:50:05.273689: I tensorflow/compiler/tf2xla/xla_op_registry.cc:328] XLA op registration: device: XLA_CPU_JIT op: StatelessIf\r\n10054:2022-03-11 09:50:05.273698: I tensorflow/compiler/tf2xla/xla_op_registry.cc:328] XLA op registration: device: XLA_GPU_JIT op: If\r\n10055:2022-03-11 09:50:05.273707: I tensorflow/compiler/tf2xla/xla_op_registry.cc:328] XLA op registration: device: XLA_CPU_JIT op: If\r\n10056:2022-03-11 09:50:05.273718: I tensorflow/compiler/tf2xla/xla_op_registry.cc:328] XLA op registration: device: XLA_GPU_JIT op: StatelessCase\r\n10057:2022-03-11 09:50:05.273727: I tensorflow/compiler/tf2xla/xla_op_registry.cc:328] XLA op registration: device: XLA_CPU_JIT op: StatelessCase\r\n.\r\n.\r\n.\r\n10973:2022-03-11 09:50:05.281066: I tensorflow/compiler/tf2xla/xla_op_registry.cc:414] For operation Reshape required constants are: shape\r\n10974:2022-03-11 09:50:05.281070: I tensorflow/compiler/tf2xla/const_analysis.cc:261] marking consts for must-be-const inputs of model/flatten/Reshape\r\n10975:2022-03-11 09:50:05.281074: I tensorflow/compiler/tf2xla/const_analysis.cc:222] marking consts for must-be-const node model/flatten/Const\r\n10978:2022-03-11 09:50:05.281228: I tensorflow/compiler/tf2xla/xla_op_registry.cc:153] tf_xla_cpu_global_jit = 0\r\n10979:2022-03-11 09:50:05.281237: I tensorflow/compiler/tf2xla/xla_op_registry.cc:51] LaunchOpHasKernelForDevice kernel_class_name: XlaLocalLaunchOp\r\n10980:2022-03-11 09:50:05.281247: I tensorflow/compiler/tf2xla/xla_op_registry.cc:51] LaunchOpHasKernelForDevice kernel_class_name: XlaLocalLaunchOp\r\n11170:2022-03-11 09:50:05.282582: I tensorflow/compiler/tf2xla/xla_op_registry.cc:414] For operation Sum required constants are: reduction_indices\r\n11171:2022-03-11 09:50:05.282588: I tensorflow/compiler/tf2xla/const_analysis.cc:261] marking consts for must-be-const inputs of sparse_categorical_crossentropy/weighted_loss/Sum\r\n11172:2022-03-11 09:50:05.282605: I tensorflow/compiler/tf2xla/xla_op_registry.cc:414] For operation Reshape required constants are: shape\r\n11173:2022-03-11 09:50:05.282611: I tensorflow/compiler/tf2xla/const_analysis.cc:261] marking consts for must-be-const inputs of model/flatten/Reshape\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\ntf2xla should not show up in the log. \r\n\r\n", "comments": ["@whatdhack, \r\n\r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here.\r\n\r\nMeanwhile, can you refer this [comment](https://github.com/tensorflow/tensorflow/issues/54925#issuecomment-1059028169) and let us know if it helps? Thanks!", "Hi, I think it is coming from the following.  Looks like this is not guarded by any check for whether XLA is asked for or not. \r\n  \r\nhttps://github.com/tensorflow/tensorflow/blob/3f878cff5b698b82eea85db2b60d65a2e320850e/tensorflow/compiler/jit/jit_compilation_pass_registration.cc#L35\r\n\r\nWhat I am trying to answer if this pollutes non-XLA executions with XLA  kernels. \r\n\r\n\r\n", "@whatdhack,\r\nOn colab I dont see any XLA related logs if I make `jit_compile=False `. \r\nTake a look at [gist](https://colab.research.google.com/drive/1OpxbEb9csWAIlzNCFX8HG-xCt7CqFu8e?usp=sharing). Thanks!", "@gadagashwini , thanks for following up.  I made a slight change to your code as follows. \r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n@tf.function(jit_compile=False)\r\ndef func(a, b):\r\n    for i in tf.range(10):\r\n        c = a + b\r\n        a = b\r\n        b = c \r\n    return c\r\nconcrete_fn = func.get_concrete_function(\r\n    tf.TensorSpec(shape=[3], dtype=tf.float32, name='a'),\r\n    tf.TensorSpec(shape=[3], dtype=tf.float32, name='b')\r\n    )\r\na = tf.random.normal([3])\r\nb = tf.random.normal([3])\r\n\r\nc = concrete_fn(a,b)\r\n\r\n```\r\nRun the above code as follows and you will see many calls to tf2xla and xla.\r\n\r\nTF_CPP_MAX_VLOG_LEVEL=3 python ./xla_gist.py\r\n\r\n\r\n\r\n\r\n", "@whatdhack, \r\nI tried with given code on colab, i didn\u2019t see XLA logs. \r\nPlease take a look at [gist](https://colab.research.google.com/drive/1go4KJvraudH35S1EWH0GmNsjIuMqhLYR?usp=sharing). Thanks! ", "Did you set TF_CPP_MAX_VLOG_LEVEL=3 ? Can you add your log please also ? ", "@whatdhack,\r\nI tried on Colab. Added the code with log \r\n```\r\nTF_CPP_MAX_VLOG_LEVEL=3\r\nimport tensorflow as tf\r\n\r\n\r\n@tf.function(jit_compile=False)\r\ndef func(a, b):\r\n    for i in tf.range(10):\r\n        c = a + b\r\n        a = b\r\n        b = c \r\n    return c\r\nconcrete_fn = func.get_concrete_function(\r\n    tf.TensorSpec(shape=[3], dtype=tf.float32, name='a'),\r\n    tf.TensorSpec(shape=[3], dtype=tf.float32, name='b')\r\n    )\r\na = tf.random.normal([3])\r\nb = tf.random.normal([3])\r\n\r\nc = concrete_fn(a,b)\r\nc\r\n```\r\n`<tf.Tensor: shape=(3,), dtype=float32, numpy=array([-189.39798 ,   98.73976 ,   34.542355], dtype=float32)>`", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "@gadagashwini , thanks. I do not see the log, only see the output. Looks like there are issues with seeing logs in colab per - https://github.com/tensorflow/tensorflow/issues/31870  .  You will need to use a cloud or on-premise instance to see the log. ", "When the `jit_compile` is set to False XLA won't be used to perform operations.\r\nI tried to reproduce the error using your code in colab by running the code as python script. \r\nI was able to see some logs, but it did not have any trace of usage of XLA. \r\n[Here](https://colab.sandbox.google.com/gist/sachinprasadhs/ad87e5a5065d3a46afc78ebde6f39d91/55207.ipynb) is the attached gist for reference. Thanks!", "Thanks for enabling the log in colab.  The following are the 4 references to xla I  could see in the log, \r\n\r\n```\r\n2022-04-18 19:39:22.726818: W tensorflow/core/util/dump_graph.cc:134] Failed to dump encapsulate_xla_computations_before because dump location is not  specified through either TF_DUMP_GRAPH_PREFIX environment variable or function argument.\r\n2022-04-18 19:39:22.727244: W tensorflow/core/util/dump_graph.cc:134] Failed to dump encapsulate_xla_computations_halfway because dump location is not  specified through either TF_DUMP_GRAPH_PREFIX environment variable or function argument.\r\n2022-04-18 19:39:22.727439: W tensorflow/core/util/dump_graph.cc:134] Failed to dump encapsulate_xla_computations_after because dump location is not  specified through either TF_DUMP_GRAPH_PREFIX environment variable or function argument.\r\n\r\n\r\n2022-04-18 19:39:22.753706: W tensorflow/core/util/dump_graph.cc:134] Failed to dump build_xla_ops because dump location is not  specified through either TF_DUMP_GRAPH_PREFIX environment variable or function argument.\r\n```\r\n", "It is mentioned as failed to dump `encapsulate_xla_computations` for before, halfway and after in the warning log, it means it is not making use of XLA. \r\nYou can ignore these warnings."]}, {"number": 55205, "title": "QAT model conversion adds extra RESHAPE layers compared to PTQ", "body": "Hello,\r\n\r\nWhen converting a QAT FullyConnected model to TFLite, the Reshape layers before and after the FullyConnected layer remain. This is due to the fact that there are Quant/Dequant stat nodes before and after the Reshapes therefore the patterns do not match.\r\nThe solution to this is to run  the CreateOptimizePass after the Quantization passes.\r\nAdded test to verify this.\r\n\r\nCode to reproduce: \r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_model_optimization as tfmot\r\n\r\ninp = tf.keras.Input(shape=(8, 4), batch_size=1)\r\nout = tf.keras.layers.Dense(16)(inp)\r\nmodel = tf.keras.Model(inp,out)\r\n\r\ndef representative_dataset():\r\n    for _ in range(100):\r\n        yield [tf.random.uniform(shape=(1, 8, 4))]\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_dataset\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.int8\r\nconverter.inference_output_type = tf.int8\r\ntflite_model = converter.convert()\r\nwith open('./dense_ptq.tflite', 'wb') as f:\r\n    f.write(tflite_model)\r\n\r\ntf.keras.backend.clear_session()\r\n\r\ninp = tf.keras.Input(shape=(8, 4), batch_size=1)\r\nout = tfmot.quantization.keras.quantize_annotate_layer(tf.keras.layers.Dense(16))(inp)\r\nmodel = tfmot.quantization.keras.quantize_apply(tf.keras.Model(inp,out))\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.inference_input_type = tf.int8\r\nconverter.inference_output_type = tf.int8\r\ntflite_model = converter.convert()\r\nwith open('./dense_qat.tflite', 'wb') as f:\r\n    f.write(tflite_model)\r\n```\r\nModel produced: \r\n![image-2021-12-08-09-57-17-159](https://user-images.githubusercontent.com/44364573/157916054-36d8404f-1e91-4f4d-8606-02db189b586d.png)", "comments": ["@SaoirseARM Can you please resolve conflicts? Thank you!", "> @SaoirseARM Can you please resolve conflicts? Thank you!\r\n\r\nThank you. I have resolved the conflict.\r\nBest regards,\r\nSaoirse", "@SaoirseARM Can you please address PyLint errors? Thank you!", "> @SaoirseARM Can you please address PyLint errors? Thank you!\r\n\r\nThanks, I have now fixed those PyLint errrors.\r\nBest regards, \r\nSaoirse"]}, {"number": 55203, "title": "`tf.gather_nd` and `tf.gather` have inconsistent type check for `batch_dims`", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.8.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nparams = tf.random.uniform([3, 1, 12, 64], dtype=tf.float32)\r\nindices = tf.random.uniform([35, 2], minval=0, maxval=1, dtype=tf.int64)\r\nbatch_dims = False\r\ntf.gather_nd(params, indices, batch_dims=batch_dims) # Pass\r\ntf.gather(params, indices, batch_dims=batch_dims) # InvalidArgumentError\r\n```\r\nDetailed error message:\r\n```\r\nInvalidArgumentError: Value for attr 'Taxis' of bool is not in the list of allowed values: int32, int64\r\n\t; NodeDef: {{node GatherV2}}; Op<name=GatherV2; signature=params:Tparams, indices:Tindices, axis:Taxis -> output:Tparams; attr=batch_dims:int,default=0; attr=Tparams:type; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=Taxis:type,allowed=[DT_INT32, DT_INT64]> [Op:GatherV2]\r\n```\r\n\r\n**Describe the current behavior**\r\nIn the above code, `batch_dims` is a `bool`, not a `int`. `tf.gather` complains about this type mismatch and throws `InvalidArgumentError`. However, `tf.gather_nd` would do implicit conversion and convert `False` to `0`. There is an inconsistency in the type checking.\r\n\r\n**Describe the expected behavior**\r\nEither allow implicit `bool`-`int` conversion in all cases, or throw an Error in all cases.", "comments": ["Added a PR #55210 for the fix."]}, {"number": 55201, "title": "Encounter Abort(core dump) in ClipOpsTest when changing shapes", "body": "**System information**\r\n- Ubuntu 20.04\r\n- python 3.7.11\r\n- In TF 2.5.0, 2.6.0, 2.7.0, 2.8.0\r\n\r\nWhen I run test cases in TensorFlow, it occurred to me that ClipOpsTest would crash when I use a huge attribute in shape.\r\n\r\n\r\n**Describe the current behavior**\r\nThe code crashed and provided 'Abort (core dump)' message.\r\n\r\n**Describe the expected behavior**\r\nThe program found shape-related problems and returned ValueError or MemoryError.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing): \r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nfrom tensorflow.python.framework import ops\r\nfrom tensorflow.python.ops import clip_ops\r\nfrom tensorflow.python.platform import test\r\nfrom tensorflow.python.framework import constant_op\r\n\r\nclass ClipOpsTest(test.TestCase):\r\n\r\n    def _testClipIndexedSlicesByNorm(self, values, indices, shape, max_norm, axes):\r\n        values = constant_op.constant(values)\r\n        indices = constant_op.constant(indices)\r\n        shape = constant_op.constant(shape)\r\n        indexed_slices = ops.IndexedSlices(values, indices, shape)\r\n        clipped = clip_ops.clip_by_norm(indexed_slices, max_norm, axes)\r\n        clipped = ops.convert_to_tensor(clipped)\r\n        \r\n    def testClipIndexedSlicesByNorm_Failed(self):\r\n        values = [[[(- 3.0), 0.0, 0.0], [4.0, 0.0, 0.0]], [[0.0, 2.0, 0.0], [0.0, 0.0, (- 1.0)]]]\r\n        indices = [2, 6]\r\n        # shape = [9223372036854775807, 1, 9223372036854775807]\r\n     \r\n        shape = [9223372036854775807, 2, 3] \r\n        self._testClipIndexedSlicesByNorm(values, indices, shape, 4.0, None) # crashed\r\n\r\n\r\n    def testClipIndexedSlicesByNorm_Pass(self):\r\n        values = [[[(- 3.0), 0.0, 0.0], [4.0, 0.0, 0.0]], [[0.0, 2.0, 0.0], [0.0, 0.0, (- 1.0)]]]\r\n        indices = [2, 6]\r\n\r\n        shape = [10, 2, 3] \r\n        self._testClipIndexedSlicesByNorm(values, indices, shape, 4.0, None) # passed\r\n\r\nif (__name__ == '__main__'):\r\n    test.main()\r\n```\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n\r\n\r\n[ RUN      ] ClipOpsTest.testClipIndexedSlicesByNorm\r\n**2022-03-12 05:39:14.085939: F tensorflow/core/framework/tensor_shape.cc:404] Check failed: 0 <= new_num_elements (0 vs. -2)\r\nFatal Python error: Aborted**\r\nCurrent thread 0x00007fb591724180 (most recent call first):\r\n  File \"/root/anaconda3/envs/tf2.8.0/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 12118 in unsorted_segment_sum\r\n  File \"/root/anaconda3/envs/tf2.8.0/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py\", line 448 in _indexed_slices_to_tensor\r\n  File \"/root/anaconda3/envs/tf2.8.0/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1695 in convert_to_tensor\r\n  File \"/root/anaconda3/envs/tf2.8.0/lib/python3.7/site-packages/tensorflow/python/profiler/trace.py\", line 183 in wrapped\r\n  File \"test_bug.py\", line 16 in _testClipIndexedSlicesByNorm\r\n  File \"test_bug.py\", line 29 in testClipIndexedSlicesByNorm\r\n  File \"/root/anaconda3/envs/tf2.8.0/lib/python3.7/unittest/case.py\", line 628 in run\r\n  File \"/root/anaconda3/envs/tf2.8.0/lib/python3.7/unittest/case.py\", line 676 in __call__\r\n  File \"/root/anaconda3/envs/tf2.8.0/lib/python3.7/unittest/suite.py\", line 122 in run\r\n  File \"/root/anaconda3/envs/tf2.8.0/lib/python3.7/unittest/suite.py\", line 84 in __call__\r\n  File \"/root/anaconda3/envs/tf2.8.0/lib/python3.7/unittest/suite.py\", line 122 in run\r\n  File \"/root/anaconda3/envs/tf2.8.0/lib/python3.7/unittest/suite.py\", line 84 in __call__\r\n  File \"/root/anaconda3/envs/tf2.8.0/lib/python3.7/unittest/runner.py\", line 176 in run\r\n  File \"/root/anaconda3/envs/tf2.8.0/lib/python3.7/site-packages/absl/testing/_pretty_print_reporter.py\", line 86 in run\r\n  File \"/root/anaconda3/envs/tf2.8.0/lib/python3.7/unittest/main.py\", line 271 in runTests\r\n  File \"/root/anaconda3/envs/tf2.8.0/lib/python3.7/unittest/main.py\", line 101 in __init__\r\n  File \"/root/anaconda3/envs/tf2.8.0/lib/python3.7/site-packages/absl/testing/absltest.py\", line 2537 in _run_and_get_tests_result\r\n  File \"/root/anaconda3/envs/tf2.8.0/lib/python3.7/site-packages/absl/testing/absltest.py\", line 2569 in run_tests\r\n  File \"/root/anaconda3/envs/tf2.8.0/lib/python3.7/site-packages/absl/testing/absltest.py\", line 2156 in _run_in_app\r\n  File \"/root/anaconda3/envs/tf2.8.0/lib/python3.7/site-packages/absl/testing/absltest.py\", line 2049 in main\r\n  File \"/root/anaconda3/envs/tf2.8.0/lib/python3.7/site-packages/tensorflow/python/platform/googletest.py\", line 51 in g_main\r\n  File \"/root/anaconda3/envs/tf2.8.0/lib/python3.7/site-packages/absl/app.py\", line 258 in _run_main\r\n  File \"/root/anaconda3/envs/tf2.8.0/lib/python3.7/site-packages/absl/app.py\", line 312 in run\r\n  File \"/root/anaconda3/envs/tf2.8.0/lib/python3.7/site-packages/tensorflow/python/platform/googletest.py\", line 60 in main_wrapper\r\n  File \"/root/anaconda3/envs/tf2.8.0/lib/python3.7/site-packages/tensorflow/python/platform/benchmark.py\", line 503 in benchmarks_main\r\n  File \"/root/anaconda3/envs/tf2.8.0/lib/python3.7/site-packages/tensorflow/python/platform/googletest.py\", line 62 in main\r\n  File \"/root/anaconda3/envs/tf2.8.0/lib/python3.7/site-packages/tensorflow/python/platform/test.py\", line 56 in main\r\n  File \"test_bug.py\", line 32 in <module>\r\n\r\n\r\n", "comments": ["I could able to reproduce the issue with [tf v2.8](https://colab.research.google.com/drive/140dUONEvZAbFS2TsiuTlMQN1nQs-5cXb?usp=sharing). Thanks!", "Is there a real use-case?\r\n\r\nWe're likely running into an integer overflow here.  Your total number of elements for that shape exceeds the int64 max.", "@cantonios \r\nIn most cases this program runs well if the shape is smaller. Do you mean the functionality of this API is related to int64 max?", "@DLFrameworkBug yes, the total number of elements in the tensor must be less than `int64` max.  Here when trying to compute the new number of elements to add, it overflows to negative.  We just don't support such large shapes.", "Maybe it's better to contain a condition check for overflow shape?", "You can submit a PR for that if you like.  This is not unique to clip ops - all tensors are restricted by this limitation."]}, {"number": 55197, "title": "supported Image formats (.PNG) for custom object detection training", "body": "Hey guys,\r\n\r\nAfter reading through the topics a bit, it was mentioned that certain image formats are not supported, but that wrappers have been written by individual users.\r\n\r\nThese were then added to the code in the tensorflow/core/..... etc. folder structure.\r\n\r\nHow does TensorFlow behave with an individual training with .PNG images? Would I also have to use one of these wrappers and if so, it is not exactly clear to me where this has to be inserted.\r\n\r\nThank you very much, I am grateful for any tips.\r\n", "comments": ["Hi @Petros626 ! Can you please share a standalone code with PNG dataset to confirm  the above use case?", "Hey @mohantym \r\n\r\nthank you for your quick response!\r\n\r\nI don't have any code for it myself, but I have read in various topics that different picture format decoders have been gradually added.\r\n\r\nI have to relate the question to my application. I want to know if API's Object Detection supports a format other than .jpg, because in the Python scripts to create the TFRecord files .jpg is used.", "Apologies for the delayed response.\r\nYou can use .PNG format in your custom object detection training.\r\nFor preprocessing pipeline you can read the .PNG file using [tf.io.read_file](https://www.tensorflow.org/api_docs/python/tf/io/read_file).\r\nAlso, refer [this](https://www.tensorflow.org/guide/data#parsing_tfexample_protocol_buffer_messages) example where PNG file is used in `tf.data` pipeline. Thanks!", "Hello,\r\n\r\ni think you can use this if you use TensorFlow via an IDE, but the Object Detection API has ready made scripts to provide training on the computer (hard coded)", "You can refer [this](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/using_your_own_dataset.md) document from the models repo for object detection where it mentions about the use of Jpeg or PNG as your inout data.", "Yes I've read this one, but I'm using this script to generate my TFRecord files (sorry wasn't from the Object Detection API). \r\nIt uses jpg Images. (https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/blob/master/generate_tfrecord.py). So I think I must change line 62, 66, 67 and 80 ? ", "Yes, basically you need to change the pipeline to adapt PNG input files instead of JPG files."]}, {"number": 55194, "title": "XLA support for ImageProjectiveTransformV3", "body": "**System information**\r\n- TensorFlow version (you are using):\r\nmaster\r\n- Are you willing to contribute it (Yes/No):\r\nI don't know, only if I have a clear contribution path\r\n\r\n**Describe the feature and the current behavior/state.**\r\nXLA support for `ImageProjectiveTransformV3` \r\n**Will this change the current api? How?**\r\nNo\r\n**Who will benefit with this feature?**\r\nPerformance on image preprocessing\r\n**Any Other info.**\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n@tf.function(jit_compile=True)\r\ndef transform():\r\n    inp = np.arange(15).reshape((1, 5, 3, 1)).astype(np.float32)\r\n    transform_matrix = np.asarray([[1., 0., 0., 0., 1., -1., 0., 0.]])\r\n    output_shape = tf.shape(inp)[1:3]\r\n    tf.raw_ops.ImageProjectiveTransformV3(\r\n        images=inp,\r\n        output_shape=output_shape,\r\n        fill_value=0.0,\r\n        transforms=transform_matrix,\r\n        fill_mode=\"constant\",\r\n        interpolation=\"bilinear\")\r\n\r\ntransform()\r\n```\r\n\r\n```\r\n on XLA_CPU_JIT: ImageProjectiveTransformV3 (No registered 'ImageProjectiveTransformV3' OpKernel for XLA_CPU_JIT devices compatible with node {{node ImageProjectiveTransformV3}}){{node ImageProjectiveTransformV3}}\r\n```\r\n\r\nExtra:\r\nPlease also note that the CPU/GPU TF2XLA supported ops tables are probably outdated (2018):\r\nhttps://github.com/tensorflow/tensorflow/issues/14798#issuecomment-1047796247", "comments": ["/cc @wangpengmit", "@sachinprasadhs ,\r\nI was able to reproduce the issue in tf v2.8, v2.7 and nightly.Please find the gist [here](https://colab.research.google.com/gist/tilakrayal/51715e77586dd8ed5b886e12ad983713/55194.ipynb).", "Assigning to @smit-hinsu for triage.", "We don't have a plan to support ImageProjectiveTransform at the moment so for now the best option will be to put this op outside of the XLA cluster.", "Ok but It Is quite recurrent in image augmentation  layers", "Also, I don't know if you could expose some steps for a contributor to add support to this op trought a PR.", "See also https://github.com/keras-team/keras/issues/16219#issuecomment-1071827074"]}]