[{"number": 29753, "title": "Dataset Optimization does not complete with a large number of Datasets", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.6.7\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: Geforce RTX 2080 ti\r\n\r\n**Describe the current behavior**\r\nI am trying to use tf.data.experimental.choose_from_datasets the create specifically constructed batches from a large number of datasets, ~7300. Each of the datasets I want to sample from is a TFRecord Dataset. It fails with a DeadlineExceededError (stack trace below).\r\n\r\nThe model is implemented as an estimator. I attempted to adjust the operation timeout but that does not seem to have any effect.\r\n\r\nA few observations:\r\n\r\n1. I know I may have pushed this a bit far. If I should actually build either a completely custom dataset or a custom operator that could take a stream of examples and build the batches with some sort of internal buffer let me know.\r\n1. It is much slower (~5x) to create the datasets in Graph Mode as opposed to eager mode.\r\n1. Eager mode has no problem yielding examples if I just loop over dataset and print them out.\r\n1. The graph that is created by the estimator is extremely large (graph.pbtxt of 2.5GB)\r\n\r\n\r\n**Describe the expected behavior**\r\nIt would be nice if this dataset could execute in graph mode.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\ndef build_triplet_dataset(root, num_classes, classes_per_batch, num_examples, num_frames, repeat=None, num_gpus=1) -> tf.data.Dataset:\r\n    cores = multiprocessing.cpu_count()\r\n\r\n    def _generator():\r\n        while True:\r\n            result = []\r\n            for _ in range(classes_per_batch):\r\n                x = random.randint(0, num_classes)\r\n                result.append([x] * num_examples)\r\n            yield list(itertools.chain(*result))\r\n\r\n    def _per_class_dataset(path) -> tf.data.Dataset:\r\n        dataset = tf.data.TFRecordDataset(path, compression_type='GZIP')\r\n        dataset = dataset.apply(tf.data.experimental.shuffle_and_repeat(num_gpus * num_examples, repeat))\r\n        dataset = dataset.map(lambda x: _parse_example(x, num_frames), num_parallel_calls=cores)\r\n        dataset = dataset.apply(tf.data.experimental.unbatch())\r\n        dataset = dataset.shuffle(num_gpus*num_examples)\r\n        return dataset\r\n\r\n    files = glob.glob(os.path.join(root, \"*.tfrecord.gz\"))\r\n    print('Loading Datasets')\r\n    datasets = list(tqdm(map(lambda x: _per_class_dataset(x), files)))\r\n\r\n    choice_dataset = tf.data.Dataset.from_generator(lambda: _generator(), (tf.int64),\r\n                                                    output_shapes=(tf.TensorShape([num_examples * classes_per_batch])))\r\n    choice_dataset = choice_dataset.apply(tf.data.experimental.unbatch())\r\n\r\n    dataset = tf.data.experimental.choose_from_datasets(datasets, choice_dataset)\r\n    dataset = dataset.batch(classes_per_batch * num_examples)\r\n    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\r\n    return dataset\r\n```\r\n\r\n**Other info / logs**\r\nCaused by op 'OptimizeDataset', defined at:\r\n  File \"sonia/models/my_model/train.py\", line 14, in <module>\r\n    ds.train_and_evaluate(model_params, hyperparameters, [], [])\r\n  File \"/home/steve/source/core/python/sonia/models/deep_speaker/model2.py\", line 453, in train_and_evaluate\r\n    steps=None))\r\n  File \"/home/steve/venv/sonia/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\", line 471, in train_and_evaluate\r\n    return executor.run()\r\n  File \"/home/steve/venv/sonia/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\", line 611, in run\r\n    return self.run_local()\r\n  File \"/home/steve/venv/sonia/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\", line 712, in run_local\r\n    saving_listeners=saving_listeners)\r\n  File \"/home/steve/venv/sonia/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 358, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/home/steve/venv/sonia/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1122, in _train_model\r\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n  File \"/home/steve/venv/sonia/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1185, in _train_model_distributed\r\n    self._config._train_distribute, input_fn, hooks, saving_listeners)\r\n  File \"/home/steve/venv/sonia/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1206, in _actual_train_model_distributed\r\n    input_fn, model_fn_lib.ModeKeys.TRAIN, strategy)\r\n  File \"/home/steve/venv/sonia/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 985, in _get_iterator_from_input_fn\r\n    iterator = result.make_initializable_iterator()\r\n  File \"/home/steve/venv/sonia/lib/python3.6/site-packages/tensorflow/python/distribute/values.py\", line 1193, in make_initializable_iterator\r\n    self._dataset, self._devices)\r\n  File \"/home/steve/venv/sonia/lib/python3.6/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py\", line 180, in __init__\r\n    self._dataset._as_variant_tensor(),  # pylint: disable=protected-access\r\n  File \"/home/steve/venv/sonia/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 2993, in _as_variant_tensor\r\n    self._input_dataset._as_variant_tensor(),  # pylint: disable=protected-access\r\n  File \"/home/steve/venv/sonia/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 3013, in _as_variant_tensor\r\n    **flat_structure(self))\r\n  File \"/home/steve/venv/sonia/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 3036, in optimize_dataset\r\n    output_shapes=output_shapes, name=name)\r\n  File \"/home/steve/venv/sonia/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/steve/venv/sonia/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/steve/venv/sonia/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\r\n    op_def=op_def)\r\n  File \"/home/steve/venv/sonia/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nDeadlineExceededError (see above for traceback): meta_optimizer exceeded deadline.\r\n         [[node OptimizeDataset (defined at /home/steve/venv/sonia/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py:985) ]]\r\n         [[node MultiDeviceIteratorInit (defined at /home/steve/venv/sonia/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py:985) ]]\r\n", "comments": ["One brief update. I disabled optimization by setting options.experimental_optimization.apply_default_optimizations = False which leads to\r\n\r\nINFO:tensorflow:Initialize strategy                                                                                                                                                                                \r\n2019-06-15 13:38:17.149375: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\r\nterminate called after throwing an instance of 'std::system_error'                                                                                                                                                 \r\n  what():  Resource temporarily unavailable                                                                                                                                                                        \r\nAborted", "Please provide us complete code to reproduce the issue.Thanks", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 29752, "title": "[XLA][ROCm]Provide kernel compilation support for ROCm", "body": "Provide support for compilation of kernels for AMDGPU target ", "comments": ["@jlebar, Thanks for the feedback. Agree,  it will help to be able to compile for both AMD/Nvidia.  Will need some time to revise and get back. ", "@jlebar Thanks for the feedback. We will try merge `nvptx_compiler` and `amdgpu_compiler` into one to largely reuse the logic and revise the PR. We might use target triple to enable / disable certain platform-specific HLO optimization passes.\r\n\r\nOn the other hand, for logic to drive LLVM backend (`nvptx_backend_lib` and `amdgpu_backend_lib`) because they've diverged too greatly we'll likely keep them separate. Does this sound okay with you?", "> We might use target triple to enable / disable certain platform-specific HLO optimization passes.\r\n\r\nSounds good.  We're already enabling/disabling some passes depending on whether you're compiling for Volta, this is just the natural extension of that.\r\n\r\n> On the other hand, for logic to drive LLVM backend (nvptx_backend_lib and amdgpu_backend_lib) because they've diverged too greatly we'll likely keep them separate. Does this sound okay with you?\r\n\r\nThe way you put it sounds like you took nvptx_backend_lib, copy-pasted it to amdgpu_backend_lib, and then modified it extensively.  Without looking very closely at the code, I will guess that this likely leaves us with significant duplicated logic, e.g. for setting up the LLVM pass pipeline.  Duplicated logic creates a maintenance burden for us, and we're not going to accept that in XLA.\r\n\r\nThere will likely need to be some engineering work done to figure out what's the right place to cut things and what are the right platform-independent pieces to pull out into their own abstractions.  This will be harder than copy/pasting it and then mutating the code, and when you send us the PR we may even disagree on the right places to put those cuts.  That's software engineering, I guess.", "Forfeiting this PR. Will submit a new one."]}, {"number": 29751, "title": "KeyError: 'ParallelInterleaveDataset' in Tf >= 1.13 (mkl/gpu)", "body": "OS Platform and Distribution - Linux Ubuntu 16.04)\r\nTensorFlow installed from Anaconda dist 3.6.\r\nTensorFlow version 1.13-mkl / 1.13-gpu\r\nPython version: Python 3.6.6 :: Anaconda, Inc.\r\n\r\n*Describe the current behavior**\r\nThere is an import error on `tf.train.import_meta_graph()` while importing one of the models from the model zoo / coral_ready model. The import works fine on 1.12 but not >= 1.13 ver\r\n**Describe the expected behavior**\r\nThe model should be otherwise loaded with the placeholder and graph\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nThe model was taken from here - [Model Link] (http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03.tar.gz)\r\n[Model Website](https://coral.withgoogle.com/models/)\r\n\r\n**Other info / logs**\r\n```\r\nTraceback (most recent call last):\r\n  File \"ssd_test.py\", line 6, in <module>\r\n    model = Importers.ImportMeta(dir_path=model_dir, quantizer='LINEAR')\r\n  File \"/home/local/SRI/e32640/Documents/aiquantizer/Importers.py\", line 89, in __init__\r\n    self.import_graph()\r\n  File \"/home/local/SRI/e32640/Documents/aiquantizer/Importers.py\", line 118, in import_graph\r\n    clear_devices=True)\r\n  File \"/home/local/SRI/e32640/anaconda3/envs/tf13-gpu/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1435, in import_meta_graph\r\n    meta_graph_or_file, clear_devices, import_scope, **kwargs)[0]\r\n  File \"/home/local/SRI/e32640/anaconda3/envs/tf13-gpu/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1457, in _import_meta_graph_with_return_elements\r\n    **kwargs))\r\n  File \"/home/local/SRI/e32640/anaconda3/envs/tf13-gpu/lib/python3.6/site-packages/tensorflow/python/framework/meta_graph.py\", line 806, in import_scoped_meta_graph_with_return_elements\r\n    return_elements=return_elements)\r\n  File \"/home/local/SRI/e32640/anaconda3/envs/tf13-gpu/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/local/SRI/e32640/anaconda3/envs/tf13-gpu/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\", line 399, in import_graph_def\r\n    _RemoveDefaultAttrs(op_dict, producer_op_list, graph_def)\r\n  File \"/home/local/SRI/e32640/anaconda3/envs/tf13-gpu/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\", line 159, in _RemoveDefaultAttrs\r\n    op_def = op_dict[node.op]\r\nKeyError: 'ParallelInterleaveDataset'\r\n\r\n```\r\n\r\nUPDATE [1]:\r\nThe following code snippet is used for importing the model downloaded from the model zoo\r\n\r\n```\r\nmodel_dir = 'ssd_mobilenet_v2/'\r\nmeta = glob.glob(model_dir + \"*.meta\")[0]\r\nckpt = meta.replace('.meta', '').strip()\r\n\r\ngraph = tf.Graph()\r\nwith graph.as_default():\r\n    with tf.Session() as sess:\r\n        reader = tf.train.import_meta_graph(meta, clear_devices=True)\r\n        reader.restore(sess, ckpt)\r\n        writer = tf.summary.FileWriter(logdir=model_dir, graph=tf.get_default_graph())  # write to event\r\n        writer.flush()\r\n        vari = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\r\n        for var in vari:\r\n            print(var.name, \"\\n\")\r\n```", "comments": ["Will it be possible for a small code snippet that can reproduce the issue. It will really help us to proceed faster. Thanks!", "@achandraa  Have updated the issue with a snippet. The simple importer is what I have used. \r\n\r\nI had a similar issue earlier and it is still open [#27752](https://github.com/tensorflow/tensorflow/issues/27752#issue-432019580)", "Jiri, did we remove this op? If so I'm guessing it was just experimental?", "The op was renamed to `ExperimentalParallelInterleaveDataset`.", "Do we have a solution for this? I'm trying to reload the .meta file for a model that I downloaded from  this colab notebook https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/asr_transformer.ipynb#scrollTo=Qz5u2O5LvShm. The following code\r\n```\r\nsess = tf.Session()\r\nnew_saver = tf.train.import_meta_graph('t2t_checkpoints/model.ckpt-230000.meta')\r\n\r\n```\r\n\r\nthrows this error\r\n```\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-8-05d93930a4a2> in <module>\r\n      1 sess = tf.Session()\r\n----> 2 new_saver = tf.train.import_meta_graph('t2t_checkpoints/model.ckpt-230000.meta')\r\n\r\n~/transformer/lib/python3.5/site-packages/tensorflow/python/training/saver.py in import_meta_graph(meta_graph_or_file, clear_devices, import_scope, **kwargs)\r\n   1447   return _import_meta_graph_with_return_elements(meta_graph_or_file,\r\n   1448                                                  clear_devices, import_scope,\r\n-> 1449                                                  **kwargs)[0]\r\n   1450 \r\n   1451 \r\n\r\n~/transformer/lib/python3.5/site-packages/tensorflow/python/training/saver.py in _import_meta_graph_with_return_elements(meta_graph_or_file, clear_devices, import_scope, return_elements, **kwargs)\r\n   1471           import_scope=import_scope,\r\n   1472           return_elements=return_elements,\r\n-> 1473           **kwargs))\r\n   1474 \r\n   1475   saver = _create_saver_from_imported_meta_graph(meta_graph_def, import_scope,\r\n\r\n~/transformer/lib/python3.5/site-packages/tensorflow/python/framework/meta_graph.py in import_scoped_meta_graph_with_return_elements(meta_graph_or_file, clear_devices, graph, import_scope, input_map, unbound_inputs_col_name, restore_collections_predicate, return_elements)\r\n    855         input_map=input_map,\r\n    856         producer_op_list=producer_op_list,\r\n--> 857         return_elements=return_elements)\r\n    858 \r\n    859     # TensorFlow versions before 1.9 (not inclusive) exported SavedModels\r\n\r\n~/transformer/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)\r\n    505                 'in a future version' if date is None else ('after %s' % date),\r\n    506                 instructions)\r\n--> 507       return func(*args, **kwargs)\r\n    508 \r\n    509     doc = _add_deprecated_arg_notice_to_docstring(\r\n\r\n~/transformer/lib/python3.5/site-packages/tensorflow/python/framework/importer.py in import_graph_def(graph_def, input_map, return_elements, name, op_dict, producer_op_list)\r\n    398   if producer_op_list is not None:\r\n    399     # TODO(skyewm): make a copy of graph_def so we're not mutating the argument?\r\n--> 400     _RemoveDefaultAttrs(op_dict, producer_op_list, graph_def)\r\n    401 \r\n    402   graph = ops.get_default_graph()\r\n\r\n~/transformer/lib/python3.5/site-packages/tensorflow/python/framework/importer.py in _RemoveDefaultAttrs(op_dict, producer_op_list, graph_def)\r\n    158     # Remove any default attr values that aren't in op_def.\r\n    159     if node.op in producer_op_dict:\r\n--> 160       op_def = op_dict[node.op]\r\n    161       producer_op_def = producer_op_dict[node.op]\r\n    162       # We make a copy of node.attr to iterate through since we may modify\r\n\r\nKeyError: 'ParallelInterleaveDataset'\r\n```", "The solution for this is to update the serialized checkpoint, replacing occurrences of `ParallelInterleaveDataset` with `ExperimentalParallelInterleaveDataset`.", "@jsimsa Thank you for your reply! What about `ParallelInterleaveDatasetV2` instances? Leaving them unchanged causes the same error. Changing them to `ExperimentalParallelInterleaveDatasetV2` cases `DecodeError: Error parsing message`", "All experimental tf.data ops renamed as part of https://github.com/tensorflow/tensorflow/commit/bf70c447d68070e81c1dbc2ebf0b6b0d8ae2c597 need to be updated accordingly.", "Notably, `ParallelInterleaveDatasetV2` should not be renamed. I suspect that when you replaced `ParallelInterleaveDataset` to `ExperimentalParallelInterleaveDataset` you also accidentally renamed `ParallelInterleaveDatasetV2` to `ExperimentalParallelInterleaveDatasetV2`.", "Any updates on this keyerror ? \r\n\r\nThere is also another KeyError called `FixedLengthDatasetV2`.", "Do you mean `FixedLengthRecordDatasetV2`? That operation has not been renamed, so I suspect that is a different issue. Please create a separate issue with details on how to reproduce the error.", "Hello @jsimsa. Thanks a lot for your help. What's the best way to update the serialized checkpoint? I tried replacing occurrences (as you suggested) using vim, but now I have a corrupted checkpoint file. Also, do I have to update `ParallelInterleaveDataset` to `ExperimentalParallelInterleaveDataset` in Tensorflow files such as `dataset_ops.py`, `gen_dataset_ops.py`, `interleave_ops.py`, and `readers.py`.", "I think the best way would be to write a simple program that reads the checkpoint file, parses the proto the file contains (I believe this would be [metagraph](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/meta_graph.proto), then walks the data structure to update all occurrences of the renamed transformations, and saves the updated proto to a new file.\r\n\r\nIf you do not mind sharing your checkpoint file, I can write the utility for you and share it via my github.", "@jsimsa That would be great. I really couldn't find a way to manipulate metagraph to remove those nodes and replace with a simple placeholder. \r\nTF suggests to use `graph_transform` tools for which you need to bazel build it.\r\n\r\nHow can I share the meta+ckpt files with you ? ", "How big is it? Could you use Google Drive or Dropbox and share a link to it?", "> How big is it? Could you use Google Drive or Dropbox and share a link to it?\r\n\r\nThe link is in the issue description. ", "Is using tf-nightly an option for you? Instead of providing a tool for fixing the checkpoint, I am leaning towards fixing TensorFlow to be able to read the checkpoint. However, this change would only be available in the nightly build.", "@jsimsa, I think rather than fixing in nightly-build, its better to update the model-zoo models stripped off these experimental nodes. \r\n\r\nyes nightly-build does help, but hope it gels well with other tf operations. It would be really great if you could provide a snippet on how you strip off such nodes. I never really go-to use the graph-transform tools that level since we need to bazel build it", "@jsimsa Any leads on the error ? \r\n", "@prateethvnayak it is not possible to fix the model-zoo model in a way that would work with all versions of Tensorflow.\r\n\r\nNote that the tf-nightly build of TensorFlow should now correctly handle the model thanks to https://github.com/tensorflow/tensorflow/commit/de9c460b06b437a44ebb3d795d9ff3cc20a8c4f1. Could you please verify that the error is indeed no longer present? Thank you.", "I'm not familiar with this code, so unassigning myself.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29751\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29751\">No</a>\n", "In tensorflow 1.12,  also has the problem.\r\nHow could I give a workaround.\r\nThanks!", "Is the proposed find/replace solution above still recommended?  \r\nI'm running into this problem when attempting to write the imported meta graph from this object detection api model: http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03.tar.gz\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nsess = tf.Session()\r\ntf.train.import_meta_graph(\"/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03/model.ckpt.meta\")\r\ntf.summary.FileWriter(\"__tb\", sess.graph)\r\n```\r\n\r\nMy tf version is 1.14.0\r\n\r\nNOTE: I'm writing this file to view the output ops in tensorboard so that I can then use tf 1.14.0's *freeze_graph()* function to then save a frozen graph of the model and perform inference with it.", "Yes, the best know solution is to update the serialized checkpoint, replacing occurrences of ParallelInterleaveDataset with ExperimentalParallelInterleaveDataset.", "@jsimsa Thanks for the information.  Do you know of a resource explaining how to do this?  I noticed earlier in this thread, that some were having trouble doing the replacement.", "@jdcast @jsimsa I had the same issue, but somehow fixed it by the following code on TF 1.14.0.\r\nI hope this would be helpful to you.\r\n```py\r\nimport os\r\n\r\nfrom google.protobuf import text_format\r\nimport tensorflow as tf\r\n\r\n\r\ndef read_meta_graph_file(filename):\r\n    # https://github.com/tensorflow/tensorflow/blob/87989f69597d6b2d60de8f112e1e3cea23be7298/tensorflow/python/framework/meta_graph.py#L670  # NOQA\r\n    meta_graph_def = tf.MetaGraphDef()\r\n    if not tf.gfile.Exists(filename):\r\n        raise IOError(\"File %s does not exist.\" % filename)\r\n    file_content = tf.gfile.GFile(filename, \"rb\").read()\r\n    try:\r\n        meta_graph_def.ParseFromString(file_content)\r\n        return meta_graph_def\r\n    except Exception:\r\n        pass\r\n\r\n    try:\r\n        text_format.Merge(file_content.decode(\"utf-8\"), meta_graph_def)\r\n    except text_format.ParseError as e:\r\n        raise IOError(\"Cannot parse file %s: %s.\" % (filename, str(e)))\r\n\r\n    return meta_graph_def\r\n\r\n\r\n# https://raw.githubusercontent.com/tensorflow/tensorflow/87989f69597d6b2d60de8f112e1e3cea23be7298/tensorflow/core/ops/compat/ops_history.v1.pbtxt  # NOQA\r\n# EDIT HERE\r\n_CONVERSION = {\r\n    'ParallelInterleaveDataset': 'ExperimentalParallelInterleaveDataset',\r\n    'MapAndBatchDatasetV2': 'ExperimentalMapAndBatchDataset',\r\n}\r\n\r\n\r\ndef _rename_op(s):\r\n    for k, v in _CONVERSION.items():\r\n        if k in s:\r\n            return s.replace(k, v)\r\n    return s\r\n\r\n\r\ndef convert(input_file, output_file):\r\n    meta_graph_def = read_meta_graph_file(input_file)\r\n    for node in meta_graph_def.graph_def.node:\r\n        node.name = _rename_op(node.name)\r\n        node.op = _rename_op(node.op)\r\n        node.input[:] = [_rename_op(s) for s in node.input]\r\n    tf.io.write_graph(meta_graph_def,\r\n                      os.path.dirname(output_file),\r\n                      os.path.basename(output_file),\r\n                      as_text=False)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    import argparse\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"--meta_file\", required=True)\r\n    parser.add_argument(\"--output_file\", required=True)\r\n    parser.add_argument(\"--check\", action=\"store_true\")\r\n    args = parser.parse_args()\r\n    convert(args.meta_file, args.output_file)\r\n    if args.check:\r\n        _saver = tf.train.import_meta_graph(args.output_file)\r\n```\r\n", "> In tensorflow 1.12, also has the problem.\r\n> How could I give a workaround.\r\n> Thanks!\r\n\r\nHave you solve the problem? I have the same problem with you."]}, {"number": 29750, "title": "No module named 'tensorflow._api'", "body": "**System information**\r\n- Microsoft Windows 10 Home\r\n- Version 10.0.17.134 Build 17134\r\n- TensorFlow installed from: Not sure, installed using python environments within visual studio\r\n- TensorFlow version: 1.13.1\r\n- Python version: 3.7 (64-bit)\r\n- Installed using pip\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: Asus - GeForce GTX 1060 6GB Dual Video Card\r\n\r\n\r\n\r\nNew user to python and this website. So please bear with me and I'll do my best. \r\n\r\nI am attempting to run a simple code here: https://www.tensorflow.org/tutorials/load_data/images\r\nin order to eventually upload my own set of images for a neural net to process. \r\n\r\nI'm using visual studio as well, and know little about how that could affect things. \r\n\r\nHere is my exact code: \r\n\r\n```\r\nfrom __future__ import absolute_import, division, print_function\r\n\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\ntf.VERSION\r\n\r\nAUTOTUNE = tf.data.experimental.AUTOTUNE\r\n```\r\nAnd here is the full error provided:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"c:\\program files (x86)\\microsoft visual studio\\2019\\community\\common7\\ide\\extensions\\microsoft\\python\\core\\ptvsd_launcher.py\", line 119, in <module>\r\n    vspd.debug(filename, port_num, debug_id, debug_options, run_as)\r\n  File \"c:\\program files (x86)\\microsoft visual studio\\2019\\community\\common7\\ide\\extensions\\microsoft\\python\\core\\Packages\\ptvsd\\debugger.py\", line 41, in debug\r\n    run(address, filename, *args, **kwargs)\r\n  File \"c:\\program files (x86)\\microsoft visual studio\\2019\\community\\common7\\ide\\extensions\\microsoft\\python\\core\\Packages\\ptvsd\\_local.py\", line 80, in run_file\r\n    run(argv, addr, **kwargs)\r\n  File \"c:\\program files (x86)\\microsoft visual studio\\2019\\community\\common7\\ide\\extensions\\microsoft\\python\\core\\Packages\\ptvsd\\_local.py\", line 140, in _run\r\n    _pydevd.main()\r\n  File \"c:\\program files (x86)\\microsoft visual studio\\2019\\community\\common7\\ide\\extensions\\microsoft\\python\\core\\Packages\\ptvsd\\_vendored\\pydevd\\pydevd.py\", line 2329, in main\r\n    globals = debugger.run(setup['file'], None, None, is_module)\r\n  File \"c:\\program files (x86)\\microsoft visual studio\\2019\\community\\common7\\ide\\extensions\\microsoft\\python\\core\\Packages\\ptvsd\\_vendored\\pydevd\\pydevd.py\", line 1664, in run\r\n    return self._exec(is_module, entry_point_fn, module_name, file, globals, locals)\r\n  File \"c:\\program files (x86)\\microsoft visual studio\\2019\\community\\common7\\ide\\extensions\\microsoft\\python\\core\\Packages\\ptvsd\\_vendored\\pydevd\\pydevd.py\", line 1671, in _exec\r\n    pydev_imports.execfile(file, globals, locals)  # execute the script\r\n  File \"c:\\program files (x86)\\microsoft visual studio\\2019\\community\\common7\\ide\\extensions\\microsoft\\python\\core\\Packages\\ptvsd\\_vendored\\pydevd\\_pydev_imps\\_pydev_execfile.py\", line 25, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"C:\\Users\\Andrew\\Desktop\\Visual Studio\\Python\\Scoliosis NN Thesis\\ScNN_v1.1.py\", line 3, in <module>\r\n    import tensorflow as tf\r\n  File \"D:\\Apps\\Python\\lib\\site-packages\\tensorflow\\__init__.py\", line 26, in <module>\r\n    from tensorflow._api.v1 import app\r\n**ModuleNotFoundError: No module named 'tensorflow._api'**\r\nPress any key to continue . . .\r\n```\r\nIf there is any other information I can provide, please let me know. \r\n\r\n\r\n\r\n\r\n", "comments": ["Hello,\r\nfor me it looks like something went wrong installing tensorflow.\r\n1) There are problems in Visual Studio installing tensorflow since - as far as I know - uses virtual environments to install its packages. Especially with the gpu version.\r\n\r\nIt doesnt look like a driver issue to me so something must have gone wrong there. I would recommend trying to installing it locally on your real machine and not in an virtual environment. That will probably make it work just follow the tensorflow documentation. https://www.tensorflow.org/install/gpu\r\n\r\nBest,\r\nLeon", "> Hello,\r\n> for me it looks like something went wrong installing tensorflow.\r\n> \r\n> 1. There are problems in Visual Studio installing tensorflow since - as far as I know - uses virtual environments to install its packages. Especially with the gpu version.\r\n> \r\n> It doesnt look like a driver issue to me so something must have gone wrong there. I would recommend trying to installing it locally on your real machine and not in an virtual environment. That will probably make it work just follow the tensorflow documentation. https://www.tensorflow.org/install/gpu\r\n> \r\n> Best,\r\n> Leon\r\n\r\nThank you Leon, I'll give that a try. ", "> > Hello,\r\n> > for me it looks like something went wrong installing tensorflow.\r\n> > \r\n> > 1. There are problems in Visual Studio installing tensorflow since - as far as I know - uses virtual environments to install its packages. Especially with the gpu version.\r\n> > \r\n> > It doesnt look like a driver issue to me so something must have gone wrong there. I would recommend trying to installing it locally on your real machine and not in an virtual environment. That will probably make it work just follow the tensorflow documentation. https://www.tensorflow.org/install/gpu\r\n> > Best,\r\n> > Leon\r\n> \r\n> Thank you Leon, I'll give that a try.\r\n\r\nWhen I was running it on my machine. It was all right and working. If it was the solution for you, please close the issue. And always happy to help :)\r\n\r\nBest,\r\nLeon", "> > > Hello,\r\n> > > for me it looks like something went wrong installing tensorflow.\r\n> > > \r\n> > > 1. There are problems in Visual Studio installing tensorflow since - as far as I know - uses virtual environments to install its packages. Especially with the gpu version.\r\n> > > \r\n> > > It doesnt look like a driver issue to me so something must have gone wrong there. I would recommend trying to installing it locally on your real machine and not in an virtual environment. That will probably make it work just follow the tensorflow documentation. https://www.tensorflow.org/install/gpu\r\n> > > Best,\r\n> > > Leon\r\n> > \r\n> > \r\n> > Thank you Leon, I'll give that a try.\r\n> \r\n> When I was running it on my machine. It was all right and working. If it was the solution for you, please close the issue. And always happy to help :)\r\n> \r\n> Best,\r\n> Leon\r\n\r\nAs of the time of writing, I ran into another issue when attempting to install the latest CUDA, cuDNN, visual studio IDE, and python. I am attempting to use 9.0 versions of CUDA and cuDNN to see if that works.\r\n", "Just to confirm, is this using a nightly version of TF or a released one?\r\n\r\nCan you try\r\n\r\n```python\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\n```", "> Just to confirm, is this using a nightly version of TF or a released one?\r\n> \r\n> Can you try\r\n> \r\n> ```python\r\n> import tensorflow as tf\r\n> print(tf.__version__)\r\n> ```\r\n\r\nI have trouble just running the import tensorflow command. However, you may be on to something. I believe I at one point used a nightly version, which may be causing the issue (as I said, I'm new at this and am very much still learning). Hopefully a clean install will fix the issue. ", "Can you try\r\n\r\n```bash\r\npip uninstall tensorflow\r\npip uninstall tensorflow_estimator\r\npip install tensorflow tensorflow_estimator\r\n```\r\n\r\nJust to make sure. If the `tensorflow_estimator` uninstall fails (with not found) just ignore that error", "Can also check the installed version by looking at the file names in `D:\\Apps\\Python\\lib\\site-packages`. You'll see a `tensorflow` folder and a `tensorflow-<version>` or a `tf-nightly-<version>` one", "Leon: Thank you for your comment. I believe I have it working now with version 10 of both CUDA and cuDNN installed independently of visual code. Running import tensorflow as tf now works.\r\n\r\nMihaimaruseac: I've since already uninstalled and reinstalled everything, so I believe any nightly versions should also be gone. \r\n", "> Can you try\r\n> \r\n> ```shell\r\n> pip uninstall tensorflow\r\n> pip uninstall tensorflow_estimator\r\n> pip install tensorflow tensorflow_estimator\r\n> ```\r\n> \r\n> Just to make sure. If the `tensorflow_estimator` uninstall fails (with not found) just ignore that error\r\n\r\nthanks helped me  :p", "Locking to prevent \"it works\" and \"thanks\" messages from drowning up the solution"]}, {"number": 29749, "title": "Run build files through buildifier", "body": "Somehow some BUILD files that don't pass recent versions of `buildifier` have snuck into the master branch. I'm not sure how that happened. Anyhow, this PR adjusts a few BUILD files so that they pass `buildifier` version 0.26.", "comments": []}, {"number": 29748, "title": "Improve interface for warmstarting non-TRAINABLE variables in warm_starting_util", "body": "**System information**\r\n- TensorFlow version (you are using): 1.13.1\r\n- Are you willing to contribute it (Yes/No): Yes (if need be)\r\n\r\n**Describe the feature and the current behavior/state.**\r\nWhen warmstarting a pre-trained estimator, a common workflow is to warmstart a small number of non-TRAINABLE variables in addition to the TRAINABLE_VARIABLES. In particular, when warmstarting a Keras estimator with BatchNorm layers, it's necessary to manually warmstart the moving_mean and moving_variance variables, since these are non-trainable (see https://github.com/tensorflow/tensorflow/issues/19903, https://github.com/tensorflow/tensorflow/issues/17950, https://github.com/keras-team/keras/pull/9965). Failure to properly restore these variables can break warmstarting for pre-trained Keras models like InceptionV3 (see https://github.com/keras-team/keras/issues/9214).\r\n\r\nThe current solution is to create `tf.estimator.WarmStartSettings()` object with `vars_to_warm_start=['.*']`. As per the [TF WarmStartSettings docs ](https://www.tensorflow.org/api_docs/python/tf/estimator/WarmStartSettings), passing in a list will warmstart all variables (including non-TRAINABLE), which allows users to warmstart vars from GLOBAL_VARIABLES. However, as [noted](https://github.com/tensorflow/tensorflow/issues/19903#issuecomment-470009739) by some users, this solution is undesirable because it will warmstart all GLOBAL_VARIABLES (including global_step, accumulators, and other state that we generally don't want to restore). \r\n\r\nIdeally, the warmstart API should allow users to cherry-pick additional non-TRAINABLE variables to warmstart alongside TRAINABLE variables. Unfortunately, it's generally not possible to use the regex mechanism in `vars_to_warm_start` for this purpose, since there's no distinction between trainable and non-trainable variables in the default TF variable naming scheme. Furthermore, in most cases, the `WarmStartSettings` object is defined before graph creation, so it's not possible for the user to access `TRAINABLE_VARIABLES` or `GLOBAL_VARIABLES` at Estimator creation time. \r\n\r\n**Proposed solution**\r\nExtend the `tf.train.warm_start()` / `tf.estimator.WarmStartSettings()` API to allow the user to separately define TRAINABLE and non-TRAINABLE variables to warmstart. I can see a couple ways to accomplish this:\r\n\r\n1. Create separate arguments `trainable_vars_to_warmstart` and `nontrainable_vars_to_warmstart`, where non-TRAINABLE variables are defined as `set(GLOBAL_VARIABLES) - set(TRAINABLE_VARIABLES)`.\r\n2. Extend the `vars_to_warmstart` arg to accept a dict of `{\"trainable\": \".*\", \"nontrainable\": \"\\moving_\\\"}`, which in this case would warmstart all TRAINABLE_VARIABLES plus the BatchNorm moving mean and variance.\r\n\r\n**Will this change the current api? How?**\r\nDepending on the solution, the args for `tf.train.warm_start()` / `tf.estimator.WarmStartSettings()` may change. Something like Option 2 above would probably be able to preserve backwards compatibility reasonably well.\r\n\r\n**Who will benefit with this feature?**\r\nUsers who want to warmstart a pre-trained Keras model that contains BatchNormalization layers or other stateful model components not found in TRAINABLE_VARIABLES.\r\n\r\n**Any Other info.**\r\nThis [blog post](http://blog.datumbox.com/the-batch-normalization-layer-of-keras-is-broken/) gives useful background on partially-overlapping issues related to BatchNorm in Keras.", "comments": ["/cc @eddie-zhou @bhack Any thoughts? It'd be super useful for us to have this in TF 1.15 (if that is in fact a thing). Otherwise, do you have any suggestions for how to deal with this issue? Our current solution involves manually adding the batch_norm moving mean and variance variables to the TRAINABLE_VARIABLES collection prior to creating the Estimator, which is very hacky and comes with its own issues.", "@gabegrand,\r\nSorry for the delayed response. The documentation of [Estimators](https://www.tensorflow.org/guide/estimator) state: \r\n\r\n> Warning: Estimators are not recommended for new code. Estimators run v1.Session-style code which is more difficult to write correctly, and can behave unexpectedly, especially when combined with TF 2 code. Estimators do fall under our compatibility guarantees, but will receive no fixes other than security vulnerabilities. See the migration guide for details.\r\n\r\nCan you please let us know if this **`Feature`** is still relevant? Thanks!", "Thanks for following up @rmothukuru. That\u2019s right, this feature is no longer relevant, unless you have a time machine that goes back to pre-pandemic times. (If only I\u2019d know I\u2019d have bigger things to worry about than warmstarting APIs for TF\u2026)"]}, {"number": 29747, "title": " 'TensorSliceDataset' object has no attribute {'make_initializable_iterator', 'output_shapes', 'make initializable_iterator'...}", "body": "Short description\r\nTensorSliceDataset is lacking a lot of attributes to make is usable ('make_initializable_iterator', 'output_shapes', 'make initializable_iterator'...).\r\n\r\nEnvironment information\r\n\r\nOperating System: 18.04\r\nPython version: 3.6\r\ntensorflow-datasets/tfds-nightly version: tfds-nightly\r\ntensorflow/tensorflow-gpu/tf-nightly/tf-nightly-gpu version: tf-nightly-2.0-preview\r\nReproduction instructions\r\n\r\nrandt = tf.random.uniform([2000, 80, 5])\r\ndataset = tf.data.Dataset.from_tensor_slices(randt)\r\ndataset.batch(20)\r\niter = dataset.make_one_shot_iterator()\r\n\r\nLink to logs\r\nAttributeError: 'TensorSliceDataset' object has no attribute 'make_one_shot_iterator'", "comments": ["@jackshi0912 was this resolved? I've been encountering this frequently in Jupyter Notebooks as I'm shifting from 1.14 to 2.0.0-alpha. ", "Yes, this method is no longer supported in 2.0\r\nYou should consider justing using a for loop to iterate through the dataset. Pretty much all methods that returns a graph node is gone.", "Thank you. I'm using an example COLAB notebook from TF 1.14 and trying to migrate it to 2.0. I'm not new to coding, but am to TF. \r\n\r\n`print('type: ', tf.data.get_output_types(path_ds))`. I know the method is deprecated, but having types / shapes of even one image printed would still be useful for this project. Any thoughts?\r\n\r\nSorry, I'm updating as I navigate the docs. I'm building a dataset of my image paths using from_tensor_slices, and I do want to confirm that output shapes and types match throughout the dataset. Could you provide an example of what this would look like? I'm unsure of how to retrieve shapes / types of a slice, even within the for loop. "]}, {"number": 29746, "title": "Inference Time TensorFlow C++ API vs Python API", "body": "Hi,\r\nIn the inference mode, Is C++ API faster than Python API? In the some issues, I see the C++ slower than python, why? Is this right?\r\n[Issue 1](https://github.com/tensorflow/tensorflow/issues/22852 )\r\n[Issue 2](https://github.com/tensorflow/tensorflow/issues/15552)", "comments": ["@PythonImageDeveloper It looks, This is not bug/performance or build/install issue. Please post this kind of support questions at [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!\r\n", "@gadagashwini , Here is best where that people can answer the question. Please don't interfere with the others.\r\n", "@PythonImageDeveloper Based on this comment, https://github.com/tensorflow/tensorflow/issues/15552#issuecomment-396573076 it need some warm-up. Thanks! If you have further question, could you create a standalone code using recent TF version and share a gist. Thanks!", "This is a stale issue. Please check the issue with latest TensorFlow. If the issue still persists in the newer version of TF, please feel free to reopen it by providing details about the issue and a standalone code to reproduce the issue. Thanks!"]}, {"number": 29745, "title": "To be deleted", "body": "To be deleted, moved to the right section ", "comments": []}, {"number": 29744, "title": "ImportError: cannot import name 'label_map_util' in pycharm", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@indotechh Please fill the above template because it is really difficult to help without that information. Thanks!\r\n\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 29743, "title": "TFRecordDataset iterator not usuable in tf.keras .fit function (steps_per_epoch)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): pip3 (binary)\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.6.7\r\n\r\n**Describe the current behavior**\r\nWhen using tf.Dataset (TFRecordDataset) API with new tf.keras API, I am passing the data iterator made from the dataset, however, before the first epoch finished, I got an `When using data tensors as input to a model, you should specify the `steps_per_epoch` argument.` exception, even though I've set this attribute in the fit method.\r\n\r\n```\r\n2019-06-13 14:22:25.393398: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 1995445000 Hz\r\n2019-06-13 14:22:25.393681: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2f7d120 executing computations on platform Host. Devices:\r\n2019-06-13 14:22:25.393708: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\nEpoch 1/2\r\n19/20 [===========================>..] - ETA: 0s - loss: 1.1921e-07 - acc: 1.0000Traceback (most recent call last):\r\n  File \"TestServe.py\", line 62, in <module>\r\n    ts.train()\r\n  File \"TestServe.py\", line 56, in train\r\n    epochs=2, verbose=1, callbacks=callbacks, steps_per_epoch=20) #The steps_per_epoch is typically samples_per_epoch / batch_size\r\n  File \"/home/josef/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 880, in fit\r\n    validation_steps=validation_steps)\r\n  File \"/home/josef/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\", line 364, in model_iteration\r\n    validation_in_fit=True)\r\n  File \"/home/josef/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\", line 202, in model_iteration\r\n    steps_per_epoch)\r\n  File \"/home/josef/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\", line 76, in _get_num_samples_or_steps\r\n    'steps_per_epoch')\r\n  File \"/home/josef/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py\", line 230, in check_num_samples\r\n    if check_steps_argument(ins, steps, steps_name):\r\n  File \"/home/josef/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py\", line 960, in check_steps_argument\r\n    input_type=input_type_str, steps_name=steps_name))\r\nValueError: When using data tensors as input to a model, you should specify the `steps_per_epoch` argument.\r\n```\r\n\r\n**Describe the expected behavior**\r\nI would expect the epochs to to through smoothly, I actually suspect the validation part, because it is failing exactly before it.\r\n\r\n**Code to reproduce the issue**\r\nHere is my failing part:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom typing import Union, List\r\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\r\nfrom tensorflow.keras import layers\r\nfrom tftools import TFTools\r\n\r\n\r\nclass TestServe():\r\n    def __init__(self, tfrecords: Union[List[tf.train.Example], tf.train.Example], batch_size: int = 10, input_shape: tuple = (64, 23)) -> None:\r\n        self.tfrecords = tfrecords\r\n        self.batch_size = batch_size\r\n        self.input_shape = input_shape\r\n\r\n    def get_model(self):\r\n        ins = layers.Input(shape=(64, 23))\r\n\r\n        l = layers.Reshape((*self.input_shape, 1))(ins)\r\n        l = layers.Conv2D(8, (30, 23), padding='same', activation='relu')(l)\r\n        l = layers.MaxPool2D((4, 5), strides=(4, 5))(l)\r\n        l = layers.Conv2D(16, (3, 3), padding='same', activation='relu')(l)\r\n        l = layers.Conv2D(32, (3, 3), padding='same', activation='relu')(l)\r\n        l = layers.MaxPool2D((2, 2), strides=(2, 2))(l)\r\n        l = layers.Flatten()(l)\r\n\r\n        out = layers.Dense(1, activation='softmax')(l)\r\n        return tf.keras.models.Model(ins, out)\r\n\r\n    def train(self):\r\n\r\n        # Create Dataset\r\n        dataset = TFTools.create_dataset(self.tfrecords)\r\n        dataset = dataset.repeat(6).batch(self.batch_size)\r\n\r\n        val_iterator = dataset.take(300).make_one_shot_iterator()\r\n        train_iterator = dataset.skip(300).make_one_shot_iterator()\r\n\r\n        model = self.get_model()\r\n        model.summary()\r\n        model.compile(optimizer='rmsprop',\r\n                      loss='binary_crossentropy', metrics=['accuracy'])\r\n        model.fit(train_iterator, validation_data=val_iterator,\r\n                  epochs=10, verbose=1, steps_per_epoch=20)\r\n\r\n    def predict(self, X: np.array) -> np.array:\r\n        pass\r\n\r\nts = TestServe(['./ok.tfrecord', './nok.tfrecord'])\r\nts.train()\r\n```\r\n\r\n**Other info / logs**\r\nI have ~ 1500 samples in those two tfrecord files, repeating it 6 times is 9.5k samples, so I dont think my generator ran out of samples.\r\n\r\nI've even created [StackOverflow](https://stackoverflow.com/questions/56580538/tf-keras-api-with-tf-dataset-problem-steps-per-epoch-argument-problem) question as I am not entirely sure if this is a bug, but looks like it, since the tf.keras si relatively new, I cannot find much help elsewhere.", "comments": ["I think I got it. The error message should be changed, because it is telling me to provide `steps_per_epoch` but I was actually missing `validation_steps` argument, later when looking into the TF source I found this.\r\n\r\n```\r\nvalidation_steps: Only relevant if `validation_data` is provided and\r\n    is a dataset or dataset iterator. Total number of steps (batches of\r\n    samples) to draw before stopping when performing validation\r\n    at the end of every epoch.\r\n```\r\nSo if I have 1000 samples with batch size of 10, I need to set 50 steps_per_epoch and 50 validation steps. however I have to repeat my dataset by number of epochs.\r\n\r\n`number of samples / batchsize = steps_per_epoch + validation_steps`\r\n\r\nBut atleast I got it to train\r\n```\r\n2019-06-14 10:44:06.813409: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 1995425000 Hz\r\n2019-06-14 10:44:06.813795: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x1f41520 executing computations on platform Host. Devices:\r\n2019-06-14 10:44:06.813839: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\nEpoch 1/5\r\n49/50 [============================>.] - ETA: 0s - loss: 8.1339 - acc: 0.4898\r\nEpoch 00001: val_loss improved from inf to 7.65234, saving model to cnn.hdf\r\n50/50 [==============================] - 9s 174ms/step - loss: 8.2900 - acc: 0.4800 - val_loss: 7.6523 - val_acc: 0.5200\r\nEpoch 2/5\r\n```", "@josefkorbel Good to know that it resolved. We will close the issue now. Thanks!", "@gadagashwini  I am using the tf.data.Dataset API and trying to convert Estimator to Keras fit function. I got this error because I use validation_steps=None (wait until the TFRecordDataset runs out of samples, which was ok with Estimator). Is there a way or I really need to know the number of samples?", "@iteal, Please open new issue and provide the all the information asked in the Template. Thanks!", "@gadagashwini  if the data is very huge, we need to use tf.data dataset. Does this still require `validation_steps` and `steps_per_epoch` ? If yes, this defeat the purpose of using tf.data dataset, as it is impractical to obtain total number of samples in the training dataset."]}, {"number": 29742, "title": "Update release notes for 2.0.0-beta1", "body": "", "comments": ["I see. Sounds good.\n"]}, {"number": 29741, "title": "Update version to 2.0.0-beta1.", "body": "", "comments": []}, {"number": 29740, "title": "Refine permuation const device placement for LayoutOptimizer.", "body": "In layout optimizer of Grappler, two permutation const nodes would be\r\nadded into the graph by default. The nodes should be placed to a default\r\ndevice assigned by a Virtual Placer, which was constructed from the\r\ncluster infomation. However, the Virtual Placer could not distinguish\r\nremote devices from local ones in distributed runtime, which causes that\r\na remote device might be the default device. Thus unnecessary dependency\r\nbetween workers would be implicitly built, unexpected crashed would\r\noccur in asynchronous distributed graph running.\r\n\r\nThis fix only refines the behaviour of LayoutOptimizer. Permutation\r\nconst nodes would be lazy created when necessary and would be colocated\r\nwith the connected nodes if the device was already done. Each device\r\nwould keep one pair and at most one pair of permutation consts, so that\r\nthese consts would not bring cross-device send-recv in ideal cases.", "comments": ["Here is a real case found in our production jobs.\r\n\r\nWorkers are running the asynchronous training independently. But after worker 3 successfully finished, other workers reported errors for failed receiving operation and exited because that the permutation const was placed on worker 3 by layout optimizer. That's why we try to fix this issue in this PR.\r\n![image](https://user-images.githubusercontent.com/10669111/59427011-39ae8080-8e0c-11e9-93b5-9df375a29ca4.png)\r\n  ", "Looks good. Let's get this merged.", "> Looks good. Let's get this merged.\r\n\r\nThanks for approving the commit. I saw some UTs was broken in CI build, some of them were caused by invalid node name. I'll check and fix it.", "@rmlarsen I've fixed a bug caused by empty device name in the code and also stripped the first '_' char for the perm const node. Could you take a check again? Thx.", "@lowintelligence thanks!"]}, {"number": 29739, "title": "Undefined symbol: DeleteGpuDelegate(_TfLiteDelegate*)", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\r\n\r\nI had built tensorflowlite.a for ios follow offcial website, build successful!\r\nbut not working for gpu, below is error msg with xcode:\r\n\r\nUndefined symbol: DeleteGpuDelegate(_TfLiteDelegate*)\r\n![image](https://user-images.githubusercontent.com/17869361/59423729-4e3b4a80-8e05-11e9-9844-dfa2fe2f785b.png)\r\n\r\nOffcial website building instruction do not enable gpu feature, how to do this?\r\n\r\nBelow is website about how to build tensorflowlite for ios\r\nhttps://www.tensorflow.org/lite/guide/build_ios \r\n\r\n\r\n\r\n", "comments": ["Please have a look on #28468 and let us know if that resolves the issue. Also can you please provide us the code snippet and information about the TensorFlow version you are using. Thanks!", "@achandraa \r\nthanks!\r\n\r\nI came cross a strange issues only occurred on HuaWei Phone. \r\nFor phone infomation with this image.\r\n![image](https://user-images.githubusercontent.com/17869361/59587579-26572a00-9118-11e9-8b20-67a14a5c8f08.png)\r\n\r\nAt first time run inference no crash, It is always crash at second time. below is crash info.\r\n![image](https://user-images.githubusercontent.com/17869361/59587879-e3498680-9118-11e9-8fe6-bf97e2e2301b.png)\r\n\r\nBelow is processed crash info with ndk-stack, but unable to locate in tensorflow source as build tflite from source with no debug symbols and i do not know how to build with debug symbol\r\n![image](https://user-images.githubusercontent.com/17869361/59588620-9a92cd00-911a-11e9-9fb9-ba68baa06f52.png)\r\n\r\nI have tried many methods for building tflite with debug symbol but not successed\r\nFor example\r\n1\u3001 bazel build -c dbg --strip=never --compilation_mode=dbg --per_file_copt=//tensorflow/lite/.*\\.cc@-g,-O0  //tensorflow/lite:libtensorflowLite.so --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cxxopt=\"-std=c++11\"\r\nwith -c dbg --strip=never  --compilation_mode=dbg\r\n\r\n![image](https://user-images.githubusercontent.com/17869361/59588889-345a7a00-911b-11e9-9424-c3cfcdd87d99.png)\r\nCrash occurred at 62 line\r\nIt crashed at second time only on HuaWei Phone. Other phones and ios has no crash.\r\n\r\nps:This issue finally crashed at 277 line with below image\r\n![image](https://user-images.githubusercontent.com/17869361/59751454-217aad80-92b3-11e9-8a98-155da7e58c92.png)\r\n\r\nI guess the bias_data address is unavaible\r\n![image](https://user-images.githubusercontent.com/17869361/59751423-10ca3780-92b3-11e9-971c-99ec5e074624.png)", "@weinixuehao \r\n\r\nSorry for the late response.  I've been out of town.\r\n\r\nI need a little bit of clarification.  This bug started with linking error \"undefined symbol of DelegateGpuDelegate\" but now, you're experiencing a crash that is not related to the GPU delegate, right?", "@impjdi Right!\r\nThis is two issues", "@weinixuehao \r\nI meet the same crash problem on zuk z2 and mi6. My code is almost same as yours. \r\nIt crashed at the DepthWiseConv.\r\nHave you fixed the problem? Thank you!", "@jianchong-chen \r\nI used tflite java version to deal with the problem because having no time to consider it.  \r\nif you still want to use c++ api which building tflite yourself maybe is a good start\r\nsuch as building it with xcode or android studio directly which has a benefits to allow you debugging  this source and to watch some variable when them vary", "@weinixuehao @jianchong-chen  @impjdi  hello,I met a similar problem.The app crush when running at the invoke-code.But I use **libtensorflowtflite_jni.so in official-prebuilt AAR**.I now have another issue,how do you init your tflite model? And how do you detect the image?Do you put your interpreter and input/output tensor in global variable at Native C++ file?I doubt this maybe a Android  error but I don't know how to solve it.Looking forward to your reply\r\n\r\n```logcat\r\n2020-07-30 20:45:48.645 1079-1079/? I//system/bin/tombstoned: received crash request for pid 20694\r\n2020-07-30 20:45:48.647 20711-20711/? I/crash_dump64: performing dump of process 20553 (target tid = 20694)\r\n2020-07-30 20:45:48.649 20553-20553/com.longjajaja.video D/JavaCameraView: Preview Frame received. Frame size: 3110400\r\n2020-07-30 20:45:48.658 20711-20711/? A/DEBUG: *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***\r\n2020-07-30 20:45:48.658 20711-20711/? A/DEBUG: Build fingerprint: 'xiaomi/whyred/whyred:9/PKQ1.180904.001/V11.0.2.0.PEICNXM:user/release-keys'\r\n2020-07-30 20:45:48.658 20711-20711/? A/DEBUG: Revision: '0'\r\n2020-07-30 20:45:48.658 20711-20711/? A/DEBUG: ABI: 'arm64'\r\n2020-07-30 20:45:48.658 20711-20711/? A/DEBUG: pid: 20553, tid: 20694, name: Thread-4  >>> com.longjajaja.video <<<\r\n2020-07-30 20:45:48.658 20711-20711/? A/DEBUG: signal 7 (SIGBUS), code 1 (BUS_ADRALN), fault addr 0xfffff4c7fffff5ae\r\n2020-07-30 20:45:48.658 20711-20711/? A/DEBUG:     x0  00000073443fd470  x1  0000000000000002  x2  0000000000000004  x3  000000734766d5a0\r\n2020-07-30 20:45:48.658 20711-20711/? A/DEBUG:     x4  000000734766d89c  x5  0000007343c0d000  x6  0000000000000000  x7  000000000000005c\r\n2020-07-30 20:45:48.658 20711-20711/? A/DEBUG:     x8  fffff4c7fffff5ae  x9  5a3d69e4024727e5  x10 0000000000000180  x11 0000000000000300\r\n2020-07-30 20:45:48.658 20711-20711/? A/DEBUG:     x12 0000000000000180  x13 0000000000000000  x14 0000007343c0d000  x15 0000007343c5a400\r\n2020-07-30 20:45:48.658 20711-20711/? A/DEBUG:     x16 000000734766d840  x17 0000000000000000  x18 0000000000000034  x19 0000007343c5f480\r\n2020-07-30 20:45:48.658 20711-20711/? A/DEBUG:     x20 0000000000000001  x21 0000000000000000  x22 0000000000000001  x23 0000000000000000\r\n2020-07-30 20:45:48.658 20711-20711/? A/DEBUG:     x24 0000000000000001  x25 0000007343c5f400  x26 0000000000000002  x27 00000073443fd5d0\r\n2020-07-30 20:45:48.659 20711-20711/? A/DEBUG:     x28 0000000000000002  x29 00000073443fd650\r\n2020-07-30 20:45:48.659 20711-20711/? A/DEBUG:     sp  00000073443fd5b0  lr  0000007348ddb428  pc  0000007348ddb11c\r\n2020-07-30 20:45:48.683 20711-20711/? A/DEBUG: backtrace:\r\n2020-07-30 20:45:48.683 20711-20711/? A/DEBUG:     #00 pc 000000000015611c  /data/app/com.longjajaja.video-B3aMXwD_IYNGKqDP80TGng==/lib/arm64/libtensorflowlite_jni.so\r\n2020-07-30 20:45:48.683 20711-20711/? A/DEBUG:     #01 pc 00000000001570ac  /data/app/com.longjajaja.video-B3aMXwD_IYNGKqDP80TGng==/lib/arm64/libtensorflowlite_jni.so\r\n2020-07-30 20:45:48.683 20711-20711/? A/DEBUG:     #02 pc 0000000000155bb8  /data/app/com.longjajaja.video-B3aMXwD_IYNGKqDP80TGng==/lib/arm64/libtensorflowlite_jni.so\r\n2020-07-30 20:45:48.683 20711-20711/? A/DEBUG:     #03 pc 0000000000062594  /data/app/com.longjajaja.video-B3aMXwD_IYNGKqDP80TGng==/lib/arm64/libtensorflowlite_jni.so\r\n2020-07-30 20:45:48.683 20711-20711/? A/DEBUG:     #04 pc 000000000006121c  /data/app/com.longjajaja.video-B3aMXwD_IYNGKqDP80TGng==/lib/arm64/libtensorflowlite_jni.so\r\n2020-07-30 20:45:48.683 20711-20711/? A/DEBUG:     #05 pc 000000000006a2e0  /data/app/com.longjajaja.video-B3aMXwD_IYNGKqDP80TGng==/lib/arm64/libtensorflowlite_jni.so\r\n2020-07-30 20:45:48.683 20711-20711/? A/DEBUG:     #06 pc 000000000005ced8  /data/app/com.longjajaja.video-B3aMXwD_IYNGKqDP80TGng==/lib/arm64/libtensorflowlite_jni.so\r\n2020-07-30 20:45:48.683 20711-20711/? A/DEBUG:     #07 pc 000000000015b370  /data/app/com.longjajaja.video-B3aMXwD_IYNGKqDP80TGng==/lib/arm64/libtensorflowlite_jni.so\r\n2020-07-30 20:45:48.683 20711-20711/? A/DEBUG:     #08 pc 000000000015e938  /data/app/com.longjajaja.video-B3aMXwD_IYNGKqDP80TGng==/lib/arm64/libtensorflowlite_jni.so\r\n2020-07-30 20:45:48.683 20711-20711/? A/DEBUG:     #09 pc 0000000000007648  /data/app/com.longjajaja.video-B3aMXwD_IYNGKqDP80TGng==/lib/arm64/libnative-gw.so (Java_com_longjajaja_video_utils_nativeutils_NativeLoad_TfLiteDetect+448)\r\n2020-07-30 20:45:48.683 20711-20711/? A/DEBUG:     #10 pc 000000000001a288  /data/app/com.longjajaja.video-B3aMXwD_IYNGKqDP80TGng==/oat/arm64/base.odex (offset 0x17000) (com.longjajaja.video.utils.nativeutils.NativeLoad.TfLiteDetect+152)\r\n2020-07-30 20:45:48.683 20711-20711/? A/DEBUG:     #11 pc 000000000055764c  /system/lib64/libart.so (art_quick_invoke_static_stub+604)\r\n2020-07-30 20:45:48.683 20711-20711/? A/DEBUG:     #12 pc 00000000000cfce8  /system/lib64/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+232)\r\n2020-07-30 20:45:48.683 20711-20711/? A/DEBUG:     #13 pc 0000000000280338  /system/lib64/libart.so (art::interpreter::ArtInterpreterToCompiledCodeBridge(art::Thread*, art::ArtMethod*, art::ShadowFrame*, unsigned short, art::JValue*)+344)\r\n2020-07-30 20:45:48.683 20711-20711/? A/DEBUG:     #14 pc 000000000027a34c  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+968)\r\n2020-07-30 20:45:48.683 20711-20711/? A/DEBUG:     #15 pc 00000000005279cc  /system/lib64/libart.so (MterpInvokeStatic+204)\r\n2020-07-30 20:45:48.683 20711-20711/? A/DEBUG:     #16 pc 0000000000549b14  /system/lib64/libart.so (ExecuteMterpImpl+14612)\r\n2020-07-30 20:45:48.683 20711-20711/? A/DEBUG:     #17 pc 00000000004d0a0a  /data/app/com.longjajaja.video-B3aMXwD_IYNGKqDP80TGng==/oat/arm64/base.vdex (com.longjajaja.video.activity.CameraOpenCVActivity$2.run+8)\r\n2020-07-30 20:45:48.683 20711-20711/? A/DEBUG:     #18 pc 0000000000254050  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3375396565+488)\r\n2020-07-30 20:45:48.683 20711-20711/? A/DEBUG:     #19 pc 0000000000259b44  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)\r\n2020-07-30 20:45:48.684 20711-20711/? A/DEBUG:     #20 pc 000000000027a330  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+940)\r\n2020-07-30 20:45:48.684 20711-20711/? A/DEBUG:     #21 pc 0000000000527444  /system/lib64/libart.so (MterpInvokeInterface+1392)\r\n2020-07-30 20:45:48.684 20711-20711/? A/DEBUG:     #22 pc 0000000000549b94  /system/lib64/libart.so (ExecuteMterpImpl+14740)\r\n2020-07-30 20:45:48.684 20711-20711/? A/DEBUG:     #23 pc 00000000000dff8c  /system/framework/boot-core-oj.vdex (java.lang.Thread.run+12)\r\n2020-07-30 20:45:48.684 20711-20711/? A/DEBUG:     #24 pc 0000000000254050  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3375396565+488)\r\n2020-07-30 20:45:48.684 20711-20711/? A/DEBUG:     #25 pc 0000000000516d7c  /system/lib64/libart.so (artQuickToInterpreterBridge+1020)\r\n2020-07-30 20:45:48.684 20711-20711/? A/DEBUG:     #26 pc 00000000005604fc  /system/lib64/libart.so (art_quick_to_interpreter_bridge+92)\r\n2020-07-30 20:45:48.684 20711-20711/? A/DEBUG:     #27 pc 0000000000557388  /system/lib64/libart.so (art_quick_invoke_stub+584)\r\n2020-07-30 20:45:48.684 20711-20711/? A/DEBUG:     #28 pc 00000000000cfcc8  /system/lib64/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+200)\r\n2020-07-30 20:45:48.684 20711-20711/? A/DEBUG:     #29 pc 000000000045dd7c  /system/lib64/libart.so (art::(anonymous namespace)::InvokeWithArgArray(art::ScopedObjectAccessAlreadyRunnable const&, art::ArtMethod*, art::(anonymous namespace)::ArgArray*, art::JValue*, char const*)+104)\r\n2020-07-30 20:45:48.684 20711-20711/? A/DEBUG:     #30 pc 000000000045ee38  /system/lib64/libart.so (art::InvokeVirtualOrInterfaceWithJValues(art::ScopedObjectAccessAlreadyRunnable const&, _jobject*, _jmethodID*, jvalue*)+424)\r\n2020-07-30 20:45:48.684 20711-20711/? A/DEBUG:     #31 pc 0000000000489d28  /system/lib64/libart.so (art::Thread::CreateCallback(void*)+1120)\r\n2020-07-30 20:45:48.684 20711-20711/? A/DEBUG:     #32 pc 0000000000090328  /system/lib64/libc.so (__pthread_start(void*)+36)\r\n2020-07-30 20:45:48.684 20711-20711/? A/DEBUG:     #33 pc 0000000000023a28  /system/lib64/libc.so (__start_thread+68)\r\n2020-07-30 20:45:48.700 20553-20553/com.longjajaja.video D/JavaCameraView: Preview Frame received. Frame size: 3110400\r\n```", "@weinixuehao Could you please let us know if this issue still persists ? If it is resolved then please feel free to move this issue to close status ? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29739\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29739\">No</a>\n"]}, {"number": 29738, "title": "custom op: undefined symbol: _ZN10tensorflow8str_util9LowercaseEN4absl11string_viewE", "body": "\r\n**System information**\r\ntensorflow/tensorflow   latest-py3                     4ddde1227df8       \r\ntensorflow/tensorflow   devel-py3                      b558e7fc2018 \r\n\r\n**Describe the current behavior**\r\ncompile ops.so under latest-py3 , then using the ops.so with tf c++ api under devel-py3.\r\n\r\nthe error is below:\r\n```\r\nops.so: undefined symbol: _ZN10tensorflow8str_util9LowercaseEN4absl11string_viewE\r\n```\r\n\r\n**Describe the expected behavior**\r\nI think ops.so will work in both docker image.\r\n\r\n\r\n**Other info / logs**\r\nrefs:\r\nhttps://github.com/tensorflow/io/issues/161\r\nhttps://github.com/tensorflow/io/issues/173\r\n", "comments": ["@zh794390558 Please provide details about what platform you are using (operating system, architecture). Make sure you also include the exact command if possible to produce the output included in your test case.\r\n\r\n", "@gadagashwini All test is on Docker Images provided by Tensorflow.\r\n```\r\ntensorflow/tensorflow latest-py3 4ddde1227df8\r\ntensorflow/tensorflow devel-py3 b558e7fc2018\r\n```\r\n\r\ncustom_op.so compile under latest-py3, using under devel-py3.\r\n\r\nI think this is the compile problem. latest-py3 using pip install tensorflow, which maybe compile not under the latest-py3 image. \r\n", "@zh794390558, the `latest-` Docker images aren't designed to build custom ops. Can you use the `tensorflow/tensorflow:custom-op` image instead?", "@angersson  I need using `latest-` docker images to run experiment, which need using custom ops.  And I think all `tf` image are based on same image. I also may using the `latest-` image to train model on GPU devices, so I think it must support build custom ops under any `tf` images.\r\n\r\nIf I can not compile custom ops in docker images, how can I compile it under physical machine.", "@yifeif I'm not sure if the `latest` images can work for this -- `custom-op` is the way to go, right?", "Also, @zh794390558, you'll need to use `devel` instead of `latest-devel` -- `latest-devel` is much older (I'm working on getting them synced up, but `devel` is the canonical image).", "`devel` has pip package installed? And it has source code? I need a docker images which has `bazel`, `tensorflow_src`,  `tensorflow pip package`, and can compile `custom ops`.", "This problem not solved now for 1.14-py3 image.", "Can you try one of the latest `custom-ops` images? See here: https://hub.docker.com/r/tensorflow/tensorflow/tags/", "what's the difference of `custom-ops` and `latest-py3`?", "custom-ops using g++4.8, but python 3.4.3, our repo must using python>=3.6.\r\n\r\n`custom-ops` using ubuntu14.04, but `latest-py3` using ubuntu18.04.", "Hi @zh794390558, the custom-op images should contains the same tool chain as how our pip packages are built with, which is not necessary the case for latest-py3 etc docker images. Could you take a look at https://github.com/tensorflow/custom-op ?", "@yifeif So this the problem, I just want to using one released image to training model with python3, which can also compile and run with custom-ops.  `custom-op` not satisfy my needs.  Which one should I use\uff1f\r\n\r\nMore info:\r\nlatest-py3 which version is 1.13 can work well , but latest-py3 which version is 1.14 not work will.", "@zh794390558, this image has python3: \r\n\r\n```\r\n$ docker run -it tensorflow/tensorflow:custom-op-ubuntu16 /bin/bash\r\n$ python3 -V\r\nPython 3.6.8\r\n```\r\n\r\nIf that doesn't work for you, you may need to put together your own image.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29738\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29738\">No</a>\n"]}, {"number": 29737, "title": "tacotron pb model conversion to tflite", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu 18.04\r\n- TensorFlow installed from (source or binary):\r\nSource 1.13.1\r\n- TensorFlow version (or github SHA if from source):\r\n1.13.1\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n# Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, DIV, EXPAND_DIMS, FILL, FLOOR, FULLY_CONNECTED, GATHER, GREATER_EQUAL, LESS, LOGICAL_AND, LOGICAL_NOT, LOGICAL_OR, LOGISTIC, MAXIMUM, MINIMUM, MUL, PACK, RANGE, REDUCE_ANY, REDUCE_MAX, RESHAPE, SELECT, SHAPE, SOFTMAX, SPLIT, SQUEEZE, STRIDED_SLICE, SUM, TANH, TILE, TRANSPOSE. Here is a list of operators for which you will need custom implementations: All, BatchMatMul, Enter, Exit, LoopCond, Merge, RandomUniform, ReverseSequence, Round, Switch, TensorArrayGatherV3, TensorArrayReadV3, TensorArrayScatterV3, TensorArraySizeV3, TensorArrayV3, TensorArrayWriteV3.\r\n\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Please take a look at https://www.tensorflow.org/lite/guide/faq#models_operations to know more.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 29736, "title": "Object Detection API: Unable to train a quantization aware Faster RCNN + Resnet50 object detector", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian Linux 9 (m26 Deep Learning Image) running on Google Cloud\r\n- TensorFlow installed from (source or binary): binary (using the one provided with the Deep Learning image 'tensorflow_gpu-1.13.1+nv-cp35-cp35m-linux_x86_64.whl')\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.5.3\r\n- CUDA/cuDNN version: CUDA 10.0 / cuDNN 7.4.1\r\n- GPU model and memory: NVIDIA Tesla V100\r\n\r\n**Describe the current behavior**\r\n\r\nWhen I try to fine tune a Faster RCNN + Resnet50 object detector I get a runtime error.\r\n\r\nStack trace:\r\n`Traceback (most recent call last):\r\n  File \"models/research/object_detection/model_main.py\", line 109, in <module>\r\n    tf.app.run()\r\n  File \"/home/klemen/ml/models/venv/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"models/research/object_detection/model_main.py\", line 105, in main\r\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])\r\n  File \"/home/klemen/ml/models/venv/lib/python3.5/site-packages/tensorflow_estimator/python/estimator/training.py\", line 471, in train_and_evaluate\r\n    return executor.run()\r\n  File \"/home/klemen/ml/models/venv/lib/python3.5/site-packages/tensorflow_estimator/python/estimator/training.py\", line 611, in run\r\n    return self.run_local()\r\n  File \"/home/klemen/ml/models/venv/lib/python3.5/site-packages/tensorflow_estimator/python/estimator/training.py\", line 712, in run_local\r\n    saving_listeners=saving_listeners)\r\n  File \"/home/klemen/ml/models/venv/lib/python3.5/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 358, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/home/klemen/ml/models/venv/lib/python3.5/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1124, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"/home/klemen/ml/models/venv/lib/python3.5/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1154, in _train_model_default\r\n    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\r\n  File \"/home/klemen/ml/models/venv/lib/python3.5/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1112, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"/home/klemen/ml/models/research/object_detection/model_lib.py\", line 369, in model_fn\r\n    graph_rewriter_fn()\r\n  File \"/home/klemen/ml//models/research/object_detection/builders/graph_rewriter_builder.py\", line 37, in graph_rewrite_fn\r\n    quant_delay=graph_rewriter_config.quantization.delay\r\n  File \"/home/klemen/ml//models/venv/lib/python3.5/site-packages/tensorflow/contrib/quantize/python/quantize_graph.py\", line 205, in experimental_create_training_graph\r\n    scope=scope)\r\n  File \"/home/klemen/ml/models/venv/lib/python3.5/site-packages/tensorflow/contrib/quantize/python/quantize_graph.py\", line 73, in _create_graph\r\n    is_training=is_training)\r\n  File \"/home/klemen/ml/models/venv/lib/python3.5/site-packages/tensorflow/contrib/quantize/python/fold_batch_norms.py\", line 53, in FoldBatchNorms\r\n    graph, is_training, freeze_batch_norm_delay=freeze_batch_norm_delay)\r\n  File \"/home/klemen/ml/models/venv/lib/python3.5/site-packages/tensorflow/contrib/quantize/python/fold_batch_norms.py\", line 98, in _FoldFusedBatchNorms\r\n    freeze_batch_norm_delay=freeze_batch_norm_delay))\r\n  File \"/home/klemen/ml/models/venv/lib/python3.5/site-packages/tensorflow/contrib/quantize/python/fold_batch_norms.py\", line 384, in _ComputeBatchNormCorrections\r\n    match.moving_variance_tensor + match.batch_epsilon)\r\nTypeError: unsupported operand type(s) for +: 'NoneType' and 'float'`\r\n\r\nConfig used:\r\n`model {\r\n  faster_rcnn {\r\n    num_classes: 1\r\n    image_resizer {\r\n      fixed_shape_resizer {\r\n        height: 600\r\n        width: 600\r\n      }\r\n    }\r\n    feature_extractor {\r\n      type: \"faster_rcnn_resnet50\"\r\n      first_stage_features_stride: 16\r\n    }\r\n    first_stage_anchor_generator {\r\n      grid_anchor_generator {\r\n        height_stride: 16\r\n        width_stride: 16\r\n        scales: 0.25\r\n        scales: 0.5\r\n        scales: 1.0\r\n        scales: 2.0                                                                                                                                                                                                                 \r\n        aspect_ratios: 0.5\r\n        aspect_ratios: 1.0\r\n        aspect_ratios: 2.0\r\n      }\r\n    }\r\n    first_stage_box_predictor_conv_hyperparams {\r\n      op: CONV\r\n      regularizer {\r\n        l2_regularizer {\r\n          weight: 0.0\r\n        }\r\n      }\r\n      initializer {\r\n        truncated_normal_initializer {\r\n          stddev: 0.00999999977648\r\n        }\r\n      }\r\n    }\r\n    first_stage_nms_score_threshold: 0.0\r\n    first_stage_nms_iou_threshold: 0.699999988079                                                                                                                                                                                    \r\n    first_stage_max_proposals: 20\r\n    first_stage_localization_loss_weight: 2.0\r\n    first_stage_objectness_loss_weight: 1.0\r\n    second_stage_batch_size: 20\r\n    initial_crop_size: 14\r\n    maxpool_kernel_size: 2\r\n    maxpool_stride: 2\r\n    second_stage_box_predictor {\r\n      mask_rcnn_box_predictor {\r\n        fc_hyperparams {\r\n          op: FC\r\n          regularizer {\r\n            l2_regularizer {\r\n              weight: 0.0\r\n            }\r\n          }\r\n          initializer {\r\n            variance_scaling_initializer {\r\n              factor: 1.0\r\n              uniform: true                                                                                                                                                                                                          \r\n              mode: FAN_AVG\r\n            }\r\n          }\r\n        }\r\n        use_dropout: false\r\n        dropout_keep_probability: 1.0\r\n      }\r\n    }\r\n    second_stage_post_processing {\r\n      batch_non_max_suppression {\r\n        score_threshold: 0.300000011921\r\n        iou_threshold: 0.600000023842\r\n        max_detections_per_class: 20\r\n        max_total_detections: 20\r\n      }\r\n      score_converter: SOFTMAX\r\n    }\r\n    second_stage_localization_loss_weight: 2.0\r\n    second_stage_classification_loss_weight: 1.0\r\n  }                                                                                                                                                                                                                                 \r\n}\r\ntrain_config {\r\n  batch_size: 16\r\n  data_augmentation_options {\r\n    random_horizontal_flip {\r\n    }\r\n  }\r\n  optimizer {\r\n    momentum_optimizer {\r\n      learning_rate {\r\n        manual_step_learning_rate {\r\n          initial_learning_rate: 0.000300000014249\r\n          schedule {\r\n            step: 0\r\n            learning_rate: 0.000300000014249\r\n          }\r\n          schedule {\r\n            step: 900000\r\n            learning_rate: 2.99999992421e-05\r\n          }                                                                                                                                                                                                                          \r\n          schedule {\r\n            step: 1200000\r\n            learning_rate: 3.00000010611e-06\r\n          }\r\n        }\r\n      }\r\n      momentum_optimizer_value: 0.899999976158\r\n    }\r\n    use_moving_average: false\r\n  }\r\n  gradient_clipping_by_norm: 10.0\r\n  fine_tune_checkpoint: \"/home/klemen/ml/resnet50/model.ckpt\"\r\n  fine_tune_checkpoint_type: \"detection\"\r\n  num_steps: 200000\r\n}\r\n\r\ntrain_input_reader: {\r\n  tf_record_input_reader {\r\n    input_path: \"/home/klemen/ml/widerface-to-tfrecord/output/train.tfrecord-?????-of-00010\"\r\n  }                                                                                                                                                                                                                                  \r\n  label_map_path: \"/home/klemen/ml/widerface-to-tfrecord/output/face_label_map.pbtxt\"\r\n}\r\n\r\neval_config: {\r\n  metrics_set: \"coco_detection_metrics\"\r\n  num_examples: 1000\r\n}\r\n\r\neval_input_reader: {\r\n  tf_record_input_reader {\r\n    input_path: \"/home/klemen/ml/widerface-to-tfrecord/output/val.tfrecord-?????-of-00010\"\r\n  }\r\n  label_map_path: \"/home/klemen/ml/widerface-to-tfrecord/output/face_label_map.pbtxt\"\r\n  shuffle: false\r\n  num_readers: 1\r\n}\r\n\r\ngraph_rewriter {\r\n  quantization {\r\n    delay: 100\r\n    activation_bits: 8\r\n    weight_bits: 8\r\n  }\r\n}\r\n`\r\n\r\n**Describe the expected behavior**\r\nIt's supposed to train the model and not throw the error.\r\n\r\n**Code to reproduce the issue**\r\nGet the Faster RCNN + ResNet50 pretrained model from `http://download.tensorflow.org/models/object_detection/faster_rcnn_resnet50_lowproposals_coco_2018_01_28.tar.gz` and run `models/research/object_detection/model_main.py` with the above config.\r\n", "comments": ["@pkulzc , do you perhaps know what might be wrong?", "@klemen-forstneric : Please help us to reproduce the issue since I was not able to find model_main.py file. Also will it be possible for you to provide us the code snippet that can replicate the issue. It will help us to proceed faster. Thanks!", "Quantization is only support for SSD models right now.", "@achandraa, I was refering to this [model_main.py](https://github.com/tensorflow/models/blob/master/research/object_detection/model_main.py). I wasn't using anything than that model_main.py and the above config with the pretrained [ResNet 50](http://download.tensorflow.org/models/object_detection/faster_rcnn_resnet50_lowproposals_coco_2018_01_28.tar.gz).\r\n\r\n@derekjchow, Okay I see. Is this something that's going to be fixed for TF 1 or should I wait for TF 2? Are you able to share any info what's going to happen to TF Object Detection API when TF 2 arrives?", "@klemen-forstneric We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.6 which is latest stable version of TF and let us know if the issue still persists in newer versions. we will get you the right help.Thanks!", "Hi @kumariko, I don't need help anymore. Thanks for the tip!", "@klemen-forstneric Thanks for the update. closing this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29736\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29736\">No</a>\n"]}, {"number": 29735, "title": " Cannot copy between a TensorFlowLite tensor with shape", "body": "In Android , the sample as it is ,working fine.\r\nBut with my model which is having only 2 labels getting crashed.\r\nBelow is my crash description.\r\n\r\norg.tensorflow.lite.examples.classification E/AndroidRuntime: FATAL EXCEPTION: inference\r\n    Process: org.tensorflow.lite.examples.classification, PID: 31666\r\n    java.lang.IllegalArgumentException: Cannot copy between a TensorFlowLite tensor with shape [1, 1001] and a Java object with shape [1, 2].\r\n        at org.tensorflow.lite.Tensor.throwIfShapeIsIncompatible(Tensor.java:282)\r\n        at org.tensorflow.lite.Tensor.throwIfDataIsIncompatible(Tensor.java:249)\r\n        at org.tensorflow.lite.Tensor.copyTo(Tensor.java:141)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:161)\r\n        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:275)\r\n        at org.tensorflow.lite.Interpreter.run(Interpreter.java:249)\r\n        at org.tensorflow.lite.examples.classification.tflite.ClassifierQuantizedMobileNet.runInference(ClassifierQuantizedMobileNet.java:94)\r\n        at org.tensorflow.lite.examples.classification.tflite.Classifier.recognizeImage(Classifier.java:257)\r\n        at org.tensorflow.lite.examples.classification.ClassifierActivity$1.run(ClassifierActivity.java:114)\r\n        at android.os.Handler.handleCallback(Handler.java:751)\r\n        at android.os.Handler.dispatchMessage(Handler.java:95)\r\n        at android.os.Looper.loop(Looper.java:154)\r\n        at android.os.HandlerThread.run(HandlerThread.java:61)\r\n\r\n\r\n", "comments": ["This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 29734, "title": "Autograph failed for function contains Non-ASCII character comment in TF 2.0.0-beta0", "body": "**System information**\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0-beta0\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: GeForce GTX 1080 8GB\r\n\r\n**Describe the current behavior**\r\n\r\nWhen using non-ASCII character in the function with @tf.function, autograph failed.\r\n\r\n> W0613 17:25:35.704982  5776 ag_logging.py:145] Entity <function train at 0x0000000003C96F28> could not be transformed and will be executed as-is. Please report \r\nthis to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: (unicode\r\n error) 'utf-8' codec can't decode byte 0x93 in position 1: invalid start byte (tmp9j9bce4r.py, line 6)\r\n\r\nTemporary file is created by Shift JIS (Japanese Windows default) character code.\r\n\r\n**Describe the expected behavior**\r\n\r\nNo warning and building a graph successfully.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\n@tf.function\r\ndef train():\r\n    ''' Japanese character \u65e5\u672c\u8a9e '''\r\n    pass\r\ntrain()\r\n```\r\n", "comments": ["@munemasa I tried reproducing the issue on Colab with Tf-2.0.0.beta0 but i did not receive any error. Let us know if that still an issue. Thanks!", "@gadagashwini I think it will be occurred on non utf-8 system.\r\nJapanese Windows is used CP932(Shift JIS) coding system as default.\r\n", "It appears that the temporary file is saved with the wrong encoding - will investigate.", "@munemasa https://github.com/tensorflow/tensorflow/commit/8125a69d70b5e7a36a62c58656dfe03c01518d1e should fix this at head. Please reopen this bug if that didn't work."]}, {"number": 29733, "title": "Does TF_SessionRun have a memory leak?", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, it is a C++ project using the C API\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): C API (no installation necessary, only linked)\r\n- TensorFlow version (use command below): 1.13\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: - (CPU only)\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nI have written a code performing inference on frozen graphs. Due to certain circumstances I must use the C API. The code does exactly what I expect. However as I must do the inference thousands of times during a simulation, at some point the simulation crashes as I run out of memory. So I started to uncomment my code and rejoin one part after the other. I found out that the memory leak only occurs if I uncomment TF_SessionRun. I was unable to figure out what it does to increase the required memory. Maybe it is also my fault but I do not see an error in memory management. Another remarkable thing is that before the inference I get the following warning 5 times:\r\n\r\n`W tensorflow/core/framework/allocator.cc:124] Allocation of 7420800000 exceeds 10% of system memory.`\r\n\r\n**Describe the expected behavior**\r\n\r\nBasically just as it is now, without increasing used memory every single time I do the inference.\r\n\r\n**Code to reproduce the issue**\r\nI will come up later with the code.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Please provide us the minimal code snippet that can replicate the issue. This will help us to proceed faster. Thanks!", "Hi, I have similar issue with `TF_SessionRun` on Windows 10 with binaries Tensorflow 1.14, I'm also using Visual Studio Community 2017 and its heap profiling tool. I'm doing semantic segmentation using U-Net and every time I run `TF_SessionRun` I have leak of size \r\n(input tensor dimension size) + (output tensor dimension size) + 159 bytes. \r\nSo for example when I predict one class 512x512 RGB image I have 512 * 512 * 3 + 512 * 512 * 1 + 159 = 1048735 bytes leaked. All this leaked memory remain after `TF_DeleteSession` (but I don't want to reopen session every time I want to predict, first prediction takes too much time). The code snippet:\r\n\r\n```c\r\n\tfloat * arrayOfFloats = (float*)malloc(512 * 512 * 3 * sizeof(float));\r\n\tfor (int i = 0; i < 512 * 512 * 3; i++) {\r\n\t\tarrayOfFloats[i] = 100;\r\n\t}\r\n\tconst int64_t dims[4] = { 1, 512, 512, 3 };\r\n\tsize_t nbytes = 512 * 512 * 3;\r\n\r\n\tTF_Output* input;\r\n\tTF_Tensor* input_value;\r\n\tTF_Output input_opout = { model->input.oper, 0 };\r\n\tinput = &input_opout;\r\n\tinput_value = TF_NewTensor(TF_FLOAT, dims, 4, (void*)arrayOfFloats, nbytes * sizeof(float), &Deallocator, NULL);\r\n\r\n\tint outputSize = 512 * 512;\r\n\r\n\tint64_t out_dims[] = { 1, 512, 512, 1 };\r\n\tTF_Output* output;\r\n\tTF_Output output_opout = { model->output.oper, 0 };\r\n\toutput = &output_opout;\r\n\tTF_Tensor* output_value = TF_AllocateTensor(TF_FLOAT, out_dims, 4, outputSize * sizeof(float));\r\n\r\n\tTF_SessionRun(model->session, NULL,\r\n\t\tinput, &input_value, 1,\r\n\t\toutput, &output_value, 1,\r\n\t\t/* No target operations to run */\r\n\t\tNULL, 0, NULL, model->status);\r\n\tif (!Okay(model->status)) return NULL;\r\n\tTF_DeleteTensor(input_value);\r\n\tTF_DeleteTensor(output_value);\r\n```\r\n\r\nSo when I run this code without `TF_SessionRun` I have no memory leaks. When I run this code with `TF_SessionRun` I get 3 new allocations that is not freed and has size described above. So when program continually predict it ran out of memory.\r\n\r\nI have tried two models of different realizations of U-Net but all of them have same leak. The leak remains in both GPU and CPU dlls. Everything besides the leak works well and prediction gives result.", "@VladislavAD Did you solve this problem? I have similar issue too. My TF version is 1.12.", "@StephenGreat I solved it by using [this repo by Neargye](https://github.com/Neargye/hello_tf_c_api). I don't find what caused the leak, but my problem was solved. You can try to compare your code with provided in repo, I suppose the leak is caused by something in graph initialization.", "@VladislavAD Thanks for sharing the repo. I found my problems. That I have forgot to use TF_DeleteTensor() to delete output tensors. So the memory kept growing.", "Hey everyone :)\r\n\r\nI would just like to ask a question regarding TF_SessionRun and output tensors. It is directly related to the problem you encountered.\r\n\r\nIt seems to me that you have to free the previously used and allocate new output TF_Tensor object(s) every time you call TF_SessionRun. That seems very memory fragmentation heavy plus the operating system has to spend time allocating a new chunk of memory with every TF_SessionRun call. Is this true ?\r\n\r\nI've recently started using the Tensorflow C library and may not be fully familiar with how it works.\r\n \r\nThanks :)\r\n", "@adaber In c_api.h, the comment about `TF_SessionRun()` shows something.\r\n[c_api.zip](https://github.com/tensorflow/tensorflow/files/3523715/c_api.zip)\r\n```\r\n// On success, the tensors corresponding to outputs[0,noutputs-1] are placed in\r\n// output_values[]. Ownership of the elements of output_values[] is transferred\r\n// to the caller, **which must eventually call TF_DeleteTensor on them.**\r\n```\r\nIn my experiment, I have to call TF_DeleteTensor to delete tenors after calling TF_SessionRun() to avoid the growing system memory.\r\n\r\n\r\n\r\n\r\n\r\n", "Hey Stephen,\r\n\r\nThanks for the response.\r\n\r\nI understand that part. You pass a pointer to a TF_Tensor pointer (output_values) which is used by TF_SessionRun to allocate/create TF_Tensor output objects. Then, you are responsible to delete/deallocate those objects.\r\n\r\nMy question was more about if we could reuse the same output TF_Tensor objects (output_values) in subsequent TF_SessionRun calls without having to deallocate and allocate new TF_Tensor objects every time we call TF_SessionRun ? We will, of course, delete them (once) in a destructor eventually.\r\n\r\nI hope someone from the Tensorflow's team can also comment on this :)\r\n\r\nThanks.", "> Hey Stephen,\r\n> \r\n> Thanks for the response.\r\n> \r\n> I understand that part. You pass a pointer to a TF_Tensor pointer (output_values) which is used by TF_SessionRun to allocate/create TF_Tensor output objects. Then, you are responsible to delete/deallocate those objects.\r\n> \r\n> My question was more about if we could reuse the same output TF_Tensor objects (output_values) in subsequent TF_SessionRun calls without having to deallocate and allocate new TF_Tensor objects every time we call TF_SessionRun ? We will, of course, delete them (once) in a destructor eventually.\r\n> \r\n> I hope someone from the Tensorflow's team can also comment on this :)\r\n> \r\n> Thanks.\r\n\r\n@adaber Same question, here is my code, wondering if this is the correct way to do so.\r\n\r\n```\r\nfor (int i = 0; i < 100; i++) { \r\n  tf_utils::RunSession(session, input_ops, input_tensors, out_ops, output_tensors);  \r\n  tf_utils::DeleteTensors(output_tensors);  \r\n}\r\n```\r\nwhere DeleteTensors is a wrapper for TF_DeleteTenor.\r\n\r\n\r\n\r\n", "Hi ZhuoranLyu,\r\n\r\nThe order of operations seems ok. The overall correctness depends on the implementation of RunSession and DeleteTensors, of course. Those are probably two custom functions defined in a namespace.", "> Hey Stephen,\r\n> \r\n> Thanks for the response.\r\n> \r\n> I understand that part. You pass a pointer to a TF_Tensor pointer (output_values) which is used by TF_SessionRun to allocate/create TF_Tensor output objects. Then, you are responsible to delete/deallocate those objects.\r\n> \r\n> My question was more about if we could reuse the same output TF_Tensor objects (output_values) in subsequent TF_SessionRun calls without having to deallocate and allocate new TF_Tensor objects every time we call TF_SessionRun ? We will, of course, delete them (once) in a destructor eventually.\r\n> \r\n> I hope someone from the Tensorflow's team can also comment on this :)\r\n> \r\n> Thanks.\r\n\r\n@adaber \r\nHi adaber, I came across the same problem with you. \r\nI tried in my program and print the address for \"output_values\". But it kept changing after each TF_SessionRun. \r\nSo I am wondering if we can use a certain trunk of memory to hold the outputs. Have you figure out a solution?\r\nCheers!\r\n", "@Weishuo93 \r\nHi. Did you find the solution? Having same issues here ", "Hello,\r\n\r\nI also noticed that for each `TF_SessionRun` call, memory for the output tensor(s) gets allocated. I solved my memory leak issue by calling `TF_DeleteTensor` on each output tensor for each execution. \r\n\r\nHowever, this solution is not ideal. I'm using the Tensorflow C API (2.6) for inference in the context of real-time audio processing, and `malloc` is not a real-time safe operation. \r\n\r\nI would like to have the possibility to specify to `TF_SessionRun` where it should write the output data, for example by giving it a pre-allocated tensor as additional argument, so that no memory would be allocated on a `TF_SessionRun` call.  \r\n\r\nIs there a way to do this with the current Tensorflow C API ? If not, I believe that such feature should be added to Tensorflow. That would be great if someone from the Tensorflow team could answer on this.\r\n\r\nThanks!"]}, {"number": 29732, "title": "saved_model_cli can't convert a severable model for Tensorflow:serving", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux RHEL 7.6\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):1.13.1\r\n- Python version: Python 2.7.16 :: Anaconda, Inc.\r\n- CUDA/cuDNN version: CUDA 10.1, cuDNN 7.6\r\n- GPU model and memory: Tesla K80, 12G\r\n\r\n**Describe the current behavior**\r\n\r\nUse the tool saved_model_cli to covert a severable model with TF-TRT:\r\n~~~\r\nsaved_model_cli convert \\\r\n--dir \"/path/to/mask_rcnn/saved_model\" \\\r\n--output_dir \"/path/to/trt-mask-rcnn\" \\\r\n--tag_set serve \\\r\ntensorrt --precision_mode FP32 --max_batch_size 32 --is_dynamic_op True\r\n~~~\r\n\r\n## While I got a frozen_graph model and the variables fold is empty, which is not severable!\r\n~~~\r\nvariables/\r\n    NULL\r\nsaved_model.pb\r\n~~~\r\n\r\n**Describe the expected behavior**\r\n\r\nThe following structure is expected. https://www.tensorflow.org/guide/saved_model#structure_of_a_savedmodel_directory\r\n\r\n~~~\r\nassets/\r\nassets.extra/\r\nvariables/\r\n    variables.data-?????-of-?????\r\n    variables.index\r\nsaved_model.pb|saved_model.pbtxt\r\n~~~\r\n\r\nMy final target is to [Optimizing TensorFlow Serving performance with NVIDIA TensorRT](https://medium.com/tensorflow/optimizing-tensorflow-serving-performance-with-nvidia-tensorrt-6d8a2347869a), and the model that I want to launch is a pre-trained mask-rcnn downloaded from [tensorflow_model_zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md)\r\n\r\n\r\nAny help would be grateful!", "comments": ["@Eloring Looks like you are able to save model and create .pb file but having problem with tensorflow serving. For better and faster support you can post this on [Tensorflow serving repo](https://github.com/tensorflow/serving/issues). Let us know if you are stuck. Thanks!", "@gadagashwini Thanks for your reply! I have solved the problem.\r\nAlthough the variables fold is empty, the converted model can be launched on tensorflow:serving.\r\n\r\nThe procedures are:\r\n1. prepare a pre-trained model (.ckpt and .config files)\r\n2. export to a saved_model (.pb and variables), which can be launched on tf:serving\r\n3. convert to another saved_model (only .pb) with TensorFlow-TensorRT \r\n\r\nMy questions is,  in the 2nd step, only saved_model.pb can not be launched, but in the 3rd step, it can.\r\nWhy?", "@Eloring This is better suited for TF_serving repo. Please post it [here](https://github.com/tensorflow/serving/issues). After posting there, please close the issue here. Thanks!", "ok, thank you so much!"]}, {"number": 29731, "title": "Error while building Android AAR target with select TensorFlow ops", "body": "**System information**\r\n- OS Platform and Distribution: macos 10.14.4\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: \r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version: 1.13.1\r\n- Python version: 3.5.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 0.25.2\r\n\r\n\r\n**Describe the problem**\r\nEncountered an error when building AAR with select TensorFlow ops:\r\n`bazel build --cxxopt='--std=c++11' -c opt             \\\r\n  --config=android_arm --config=monolithic          \\\r\n  //tensorflow/lite/java:tensorflow-lite-with-select-tf-ops`\r\n\r\nError shows: \r\n```\r\nERROR: /Users/darrenzhao/Documents/Git_repository/tensorflow/tensorflow/lite/java/BUILD:33:1: Executing genrule //tensorflow/lite/java:tensorflow-lite-with-select-tf-ops_binary_manifest_generator failed (Exit 127)\r\n: command not found\r\nTarget //tensorflow/lite/java:tensorflow-lite-with-select-tf-ops failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 2702.849s, Critical Path: 661.01s\r\nINFO: 1941 processes: 1941 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\nBesides, as new to TensorFlow Lite, I have trouble understanding what is 'TensorFlow Lite build environment' mentioned in the [doc](https://www.tensorflow.org/lite/guide/ops_select#android_aar). To me it means Android studio + bazel + tensorflow, is that correct?\r\n\r\n\r\n**Any other info / logs**\r\noutput of `--verbose_failures` :\r\n```\r\nERROR: /Users/darrenzhao/Documents/Git_repository/tensorflow/tensorflow/lite/java/BUILD:33:1: Executing genrule //tensorflow/lite/java:tensorflow-lite-with-select-tf-ops_binary_manifest_generator failed (Exit 127): bash failed: error executing command \r\n  (cd /private/var/tmp/_bazel_darrenzhao/c04f0a0d10b76f360494fa7712b36837/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    ANDROID_BUILD_TOOLS_VERSION=28.0.3 \\\r\n    ANDROID_NDK_API_LEVEL=18 \\\r\n    ANDROID_NDK_HOME=/Users/darrenzhao/library/Android/Sdk/ndk-bundle \\\r\n    ANDROID_SDK_API_LEVEL=28 \\\r\n    ANDROID_SDK_HOME=/Users/darrenzhao/library/Android/Sdk \\\r\n    PATH=/anaconda3/bin:/anaconda3/condabin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Users/darrenzhao/Library/Android/sdk/tools:/Users/darrenzhao/Library/Android/sdk/platform-tools:/Users/darrenzhao/bin \\\r\n    PYTHON_BIN_PATH=/anaconda3/bin/python \\\r\n    PYTHON_LIB_PATH=/anaconda3/lib/python3.7/site-packages \\\r\n    TF_CONFIGURE_IOS=0 \\\r\n    TF_DOWNLOAD_CLANG=0 \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n    TF_NEED_ROCM=0 \\\r\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; \r\ncat > bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/java/tensorflow-lite-with-select-tf-ops_generated_AndroidManifest.xml <<EOF\r\n<manifest\r\n  xmlns:android=\"http://schemas.android.com/apk/res/android\"\r\n  package=\"dummy.package.for.so\">\r\n  <uses-sdk android:minSdkVersion=\"999\"/>\r\n</manifest>\r\nEOF\r\n')\r\nExecution platform: @bazel_tools//platforms:host_platform\r\n: command not found\r\nTarget //tensorflow/lite/java:tensorflow-lite-with-select-tf-ops failed to build\r\nINFO: Elapsed time: 2.356s, Critical Path: 0.70s\r\nINFO: 4 processes: 4 worker.\r\n```\r\n\r\nAny help would be appreciated, thanks!", "comments": ["Did you run `./configure` from the root checkout directory, and specify options for the Android build? In particular, defining ANDROID_SDK_HOME and ANDROID_SDK_ROOT env variables will make setup simpler.", "@jdduke sorry for the late reply. Yes, I did. However I found some problem about my ndk version that bazel doesn't support ndk version >= 18, I will downgraded it and try to build later. \r\n", "@jdduke hi, I have downloaded the supported ndk(17c), set up the paths in configure and cloned the master branch of tensorflow again just in case the previous one was bad. But after I ran bazel build, I had error:\r\n```\r\nCONFIGURATION: bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/java/tensorflowlite_flex_processed_manifest/AndroidManifest.xml has no minSdkVersion. Using 1.\r\nINFO: From Validating Android resources for //tensorflow/lite/java:tensorflowlite_flex:\r\n6\u6708 24, 2019 9:26:33 \u4e0b\u5348 com.google.devtools.build.android.AndroidManifest parseFrom\r\n\u8b66\u544a: \r\nCONFIGURATION: bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/java/tensorflowlite_flex_processed_manifest/AndroidManifest.xml has no minSdkVersion. Using 1.\r\nERROR: /Users/darrenzhao/tensorflow/tensorflow/lite/java/BUILD:270:1: Linking of rule '//tensorflow/lite/java:libtensorflowlite_flex_jni.so' failed (Exit 1): clang failed: error executing command \r\n  (cd /private/var/tmp/_bazel_darrenzhao/f2ad9b9f8818ca4cb1b274cd7d820d38/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    ANDROID_BUILD_TOOLS_VERSION=28.0.3 \\\r\n    ANDROID_NDK_API_LEVEL=17 \\\r\n    ANDROID_NDK_HOME=/Users/darrenzhao/Documents/Android-ndk-17c/android-ndk-r17c \\\r\n    ANDROID_SDK_API_LEVEL=28 \\\r\n    ANDROID_SDK_HOME=/Users/darrenzhao/library/Android/Sdk \\\r\n    PATH=/anaconda3/bin:/anaconda3/condabin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Users/darrenzhao/Library/Android/sdk/tools:/Users/darrenzhao/Library/Android/sdk/platform-tools:/Users/darrenzhao/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/anaconda3/bin/python \\\r\n    PYTHON_LIB_PATH=/anaconda3/lib/python3.7/site-packages \\\r\n    TF_CONFIGURE_IOS=0 \\\r\n  external/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/clang -shared -o bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/java/libtensorflowlite_flex_jni.so -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/delegates/flex/libdelegate_only_runtime.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/delegates/flex/libkernel.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/delegates/flex/libdelegate_data.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/delegates/flex/libbuffer_map.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/delegates/flex/libutil.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/delegates/nnapi/java/src/main/native/libnative.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/java/src/main/native/libnative.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/java/src/main/native/libnative_framework_only.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/kernels/libbuiltin_ops.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/kernels/libbuiltin_op_kernels.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/kernels/libcpu_backend_support.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/kernels/libeigen_support.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/kernels/liblstm_eval.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/kernels/internal/libaudio_utils.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/kernels/internal/libkernel_utils.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/kernels/libcpu_backend_gemm.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/experimental/ruy/libruy.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/experimental/ruy/libkernel.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/experimental/ruy/libpack.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/libstring_util.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/kernels/internal/libtensor_utils.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/kernels/internal/libneon_tensor_utils.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/external/androidndk/libcpufeatures.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/kernels/libcpu_backend_context.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/experimental/ruy/libcontext.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/experimental/ruy/liballocator.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/experimental/ruy/libdetect_dotprod.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/experimental/ruy/libthread_pool.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/experimental/ruy/libblocking_counter.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/experimental/ruy/libtrace.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/experimental/ruy/libblock_map.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/experimental/ruy/libtune.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/external/flatbuffers/libflatbuffers.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/libframework.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/libarena_planner.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/libsimple_memory_arena.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/core/libversion_lib.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/core/api/libapi.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/delegates/nnapi/libnnapi_delegate.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/libminimal_logging.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/libutil.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/kernels/libkernel_util.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/kernels/internal/libquantization_util.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/nnapi/libnnapi_implementation.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/c/libc_api_internal.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/java/src/main/native/libinit_tensorflow.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/testing/libinit_tensorflow.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/core/libandroid_tensorflow_lib.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/core/kernels/libandroid_tensorflow_kernels.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/core/libandroid_tensorflow_lib_lite.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/core/libplatform_base.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/external/com_google_absl/absl/hash/libhash.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/external/com_google_absl/absl/hash/libcity.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/external/com_google_absl/absl/types/libbad_variant_access.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/external/com_google_absl/absl/container/libraw_hash_set.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/external/com_google_absl/absl/types/libbad_optional_access.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/external/com_google_absl/absl/container/libhashtablez_sampler.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/external/com_google_absl/absl/synchronization/libsynchronization.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/external/com_google_absl/absl/synchronization/libgraphcycles_internal.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/external/com_google_absl/absl/debugging/libstacktrace.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/external/com_google_absl/absl/debugging/libsymbolize.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/external/com_google_absl/absl/debugging/libdebugging_internal.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/external/com_google_absl/absl/debugging/libdemangle_internal.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/external/com_google_absl/absl/base/libmalloc_internal.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/external/com_google_absl/absl/time/libtime.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/external/com_google_absl/absl/time/internal/cctz/libtime_zone.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/external/com_google_absl/absl/time/internal/cctz/libcivil_time.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/external/com_google_absl/absl/strings/libstrings.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/external/com_google_absl/absl/strings/libinternal.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/external/com_google_absl/absl/base/libthrow_delegate.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/external/com_google_absl/absl/base/libbase.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/external/com_google_absl/absl/base/libdynamic_annotations.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/external/com_google_absl/absl/base/libspinlock_wait.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/external/com_google_absl/absl/numeric/libint128.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/core/libstats_calculator_portable.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/external/double_conversion/libdouble-conversion.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/external/farmhash_archive/libfarmhash.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/external/nsync/libnsync_cpp.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/core/libprotos_all_proto_cc_impl.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/tensorflow/core/liberror_codes_proto_cc_impl.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/external/fft2d/libfft2d.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/external/protobuf_archive/libprotobuf.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/external/protobuf_archive/libprotobuf_lite.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/android-armeabi-v7a-opt/bin/external/zlib_archive/libzlib.pic.a -Wl,-no-whole-archive external/androidndk/ndk/sources/cxx-stl/llvm-libc++/libs/armeabi-v7a/libandroid_support.a external/androidndk/ndk/sources/cxx-stl/llvm-libc++/libs/armeabi-v7a/libc++.a external/androidndk/ndk/sources/cxx-stl/llvm-libc++/libs/armeabi-v7a/libc++_static.a external/androidndk/ndk/sources/cxx-stl/llvm-libc++/libs/armeabi-v7a/libc++abi.a external/androidndk/ndk/sources/cxx-stl/llvm-libc++/libs/armeabi-v7a/libunwind.a -Wl,--gc-sections -Wl,--as-needed -latomic -s -Wl,--version-script,tensorflow/lite/java/src/main/native/version_script.lds -lm -ldl -ldl -llog -ldl -ldl -ldl -lz -pthread -pthread -pthread -framework Foundation -lm -lm -static-libgcc -target armv7-none-linux-androideabi -Wl,--fix-cortex-a8 -gcc-toolchain external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64 -no-canonical-prefixes -Lexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/libs/armeabi-v7a '--sysroot=external/androidndk/ndk/platforms/android-16/arch-arm')\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nexternal/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: error: cannot open Foundation: No such file or directory\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nTarget //tensorflow/lite/java:tensorflow-lite-with-select-tf-ops failed to build\r\nINFO: Elapsed time: 3888.896s, Critical Path: 1861.61s\r\nINFO: 2001 processes: 1990 local, 11 worker.\r\nFAILED: Build did NOT complete successfully\r\n```", "ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: error: cannot open Foundation: No such file or directory\r\nsome error!", "Likewise, I'm stuck getting the same error, building TF for Android on OSX.  I've tried on r1.14.0 and on current master.  Attempted with a number of different NDK versions and still getting the same issue.  I'm not sure what to try next.", "> Likewise, I'm stuck getting the same error, building TF for Android on OSX. I've tried on r1.14.0 and on current master. Attempted with a number of different NDK versions and still getting the same issue. I'm not sure what to try next.\r\n\r\n+1 stuck on this issue. Kindly help", "> ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: error: cannot open Foundation: No such file or directory\r\n> some error!\r\n\r\nI am stuck in the same position. Could anyone share some possible solutions?", "@yyoon would you mind taking a look to see if you can repro?", "Sure, I'll take a look, hopefully in a week.", "got the same problem, but the solution was to modify bazel-tensorflow/external/com_google_absl/absl/time/internal/cctz/Build.bazel file and remove \r\n\r\n```\r\n config_setting(\r\n \tname = \"osx\",\r\n \tconstraint_values = [\r\n \t\t\"@bazel_tools//platforms:osx\",\r\n \t],\r\n)\r\n \r\n config_setting(\r\n      name = \"ios\",\r\n      constraint_values = [\r\n          \"@bazel_tools//platforms:ios\",\r\n      ],\r\n)\r\n```\r\nand \r\n\r\n```\r\nlinkopts = select({\r\n      \":osx\": [\r\n          \"-framework Foundation\",\r\n      ],\r\n      \":ios\": [\r\n          \"-framework Foundation\",\r\n      ],\r\n      \"//conditions:default\": [],\r\n  }),\r\n```\r\nthen try to build it again", "> got the same problem, but the solution was to modify bazel-tensorflow/external/com_google_absl/absl/time/internal/cctz/Build.bazel file and remove\r\n> \r\n> ```\r\n>  config_setting(\r\n>  \tname = \"osx\",\r\n>  \tconstraint_values = [\r\n>  \t\t\"@bazel_tools//platforms:osx\",\r\n>  \t],\r\n> )\r\n>  \r\n>  config_setting(\r\n>       name = \"ios\",\r\n>       constraint_values = [\r\n>           \"@bazel_tools//platforms:ios\",\r\n>       ],\r\n> )\r\n> ```\r\n> \r\n> and\r\n> \r\n> ```\r\n> linkopts = select({\r\n>       \":osx\": [\r\n>           \"-framework Foundation\",\r\n>       ],\r\n>       \":ios\": [\r\n>           \"-framework Foundation\",\r\n>       ],\r\n>       \"//conditions:default\": [],\r\n>   }),\r\n> ```\r\n> \r\n> then try to build it again\r\n\r\nIt works for me! Thanks for the suggestion.", "04e169a should fix this. Let me know if you still have issues.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29731\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29731\">No</a>\n"]}, {"number": 29730, "title": "MirroredStrategy does not work with CudnnLSTM", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.13.1 v1.13.1-2-ga5c387b5ed\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source): 0.21.0\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: 1080 Ti 11GB\r\n\r\n**Describe the current behavior**\r\nUnable to train a model using MirroredStrategy with CudnnLSTM. Fails with the following error:\r\n\r\n```\r\nFailedPreconditionError (see above for traceback): Error while reading resource variable cudnn_lstm/opaque_kernel from Container: localhost. This could mean that the variable was uninitialized. Not found: Container localhost does not exist. (Could not find resource: localhost/cudnn_lstm/opaque_kernel)\r\n\t [[node Shape_2/Identity_1/ReadVariableOp (defined at /home/sharvil/.virtualenv/tensorflow/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py:1254) ]]\r\n```\r\n\r\n**Describe the expected behavior**\r\nCode should run.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\ndef gen():\r\n  yield 0\r\n\r\n\r\ndef model_fn(features, labels, mode):\r\n  shape = [1, 100, 10]\r\n  x = tf.random_normal(shape)\r\n  y = tf.zeros(shape)\r\n\r\n  cell = tf.contrib.cudnn_rnn.CudnnLSTM(1, shape[-1])\r\n  prediction, _ = cell(x)\r\n\r\n  loss = tf.reduce_sum(tf.squared_difference(prediction, y))\r\n  train_op = tf.train.AdamOptimizer().minimize(loss, tf.train.get_or_create_global_step())\r\n  return tf.estimator.EstimatorSpec(mode, prediction, loss, train_op)\r\n\r\nconfig = tf.estimator.RunConfig()\r\n# Training begins successfully with the following line commented out.\r\nconfig = config.replace(train_distribute=tf.distribute.MirroredStrategy())\r\nestimator = tf.estimator.Estimator(model_fn, '/tmp/tf-bug', config=config)\r\nestimator.train(input_fn=lambda: tf.data.Dataset.from_generator(gen, tf.int32))\r\n```", "comments": ["Have tried on Colab with TF 1.13.1 (CUDA 10.0) and was able to reproduce the issue.", "I tested with 1.14 in a colab and it seems to work. Please try with 1.14 and re-open if still an issue. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29730\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29730\">No</a>\n"]}, {"number": 29729, "title": "tf.keras.Model can't initialize variables given input shape in tf2.0 beta", "body": "When given `input_shape` of the input, the model built on `Sequential` will initialize all the variables at once. \r\n```python  \r\nmodel = Sequential()\r\nmodel.add(layers.Dense(32, input_shape=(500,)))\r\nmodel.add(layers.Dense(32))\r\nprint(len(model.weights)) #output 4\r\n```\r\n\r\n--------------------------------------\r\nWhen given `input_shape` of the input, the model built on `Model` will not initialize all the variables at once and only initializes them after doing some predictions.\r\nI can understand when no `input_shape` is given, the model cannot initialize all the variables, but why it doesn't when given `input_shape`?\r\n\r\n```python\r\nclass MyModel(tf.keras.Model):\r\n  def __init__(self, state_dim, action_dim):\r\n    super(MyModel, self).__init__()\r\n    self.state_dim = state_dim\r\n    self.action_dim = action_dim\r\n\r\n    self.fc1 = tf.keras.layers.Dense(100, input_shape=(2,))\r\n    self.fc2 = tf.keras.layers.Dense(1)\r\n\r\n  def call(self, inputs):\r\n    out = self.fc1(inputs)\r\n    out = self.fc2(out)\r\n    return out\r\n\r\nQ = MyModel(2, 2)\r\nprint(len(Q.weights)) # output 0\r\nstates = np.random.random((10, 2))\r\nQ(states)\r\nprint(len(Q.weights)) # output 4\r\n```\r\n", "comments": ["I am able to reproduce the issue on colab with Tf-2.0.0.brta0. ", "any progress here?", "any progress here? \r\nAgain.", "Subclass models in Keras are not guaranteed to have made all of their variables until the first time they have been called on inputs. This is because the model has no way to know how your layers are connected until you actually enter the `call` method, so it is insufficient to specify an input layer shape on one of your layers.\r\n\r\nIf your subclass model doesn't implement a custom `build` method, you should be able to call `model.build()` to force the variables to instantiate.\r\n\r\nWe are currently exploring ways of allowing subclass models that do implement custom `build` methods to still initialize all variables when their `build` method is called. (Models implement custom `build` methods when the layers can't all be constructed in the `__init__` method because the number of layers may depend on the input dimensions)\r\n\r\nAlternatively, you can also use the Keras functional API, because the variables will be created right away without needing to call `.build()`. https://www.tensorflow.org/beta/guide/keras/functional", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29729\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29729\">No</a>\n", "Thanks for replying. \r\nI do want to use limited number of  keras functional API to build my network, as these APIs are not as flexible as origin tf.xx APIs. So I will try using model.build() to initialize the variables. "]}, {"number": 29728, "title": "Fix: EagerFunc tensor conversion error", "body": "This is the fix to the issue [https://github.com/tensorflow/tensorflow/issues/29727](https://github.com/tensorflow/tensorflow/issues/29727). When the desired dtype is integer, let input argument be 0 (strict integer) instead of 0.0.", "comments": ["@kami93 Could you please check reviewer comments and keep us posted. Thanks!", "@alextp \r\nThe issue is regarding calculating gradients of operations that involves pipeline through the \"tfe.py_func\" as described in [https://github.com/tensorflow/tensorflow/issues/29727](https://github.com/tensorflow/tensorflow/issues/29727). When calculating such gradients, the member function \"_convert\" of the EagerFunc class defined in \"tensorflow/python/ops/script_ops.py\" tries to make zero constants with the same dtypes as the tensors given as input arguments to the tfe.py_func pipeline if the gradients with repect to them are None.\r\n```\r\nclass EagerFunc(object):\r\n  \"\"\"A wrapper for a function owned by an EagerPyFunc.\"\"\"\r\n\r\n  def __init__(self, func, Tout, is_grad_func):\r\n    \"\"\"Constructs an EagerFunc.\r\n    Args:\r\n      func: The function to wrap.\r\n      Tout: A list of datatypes for the output; an empty list if the output is\r\n        None.\r\n      is_grad_func: Whether this EagerFunc is the gradient of another\r\n        EagerPyFunc.\r\n    \"\"\"\r\n    self._func = func\r\n    self._out_dtypes = Tout\r\n    self._is_grad_func = is_grad_func\r\n\r\n    context.ensure_initialized()\r\n\r\n  def _convert(self, value, dtype):\r\n    \"\"\"Converts `value` to a tensor of type `dtype`, with error checking.\r\n    Args:\r\n      value: The tensor to convert.\r\n      dtype: The desired dtype.\r\n    Returns:\r\n      A tensor of type `dtype`, or a zeros tensor if value is None and\r\n      this function is in fact a grdient function.\r\n    Raises:\r\n      RuntimeError: if `value` is a variable.\r\n    \"\"\"\r\n\r\n    if isinstance(value, resource_variable_ops.ResourceVariable):\r\n      raise RuntimeError(\r\n          \"Attempting to return a variable from an eagerly executed py_func. \"\r\n          \"Only numeric data structures like Tensors or NumPy arrays should \"\r\n          \"be returned; to return the value of a variable, make sure to obtain \"\r\n          \"the Tensor backing it by calling `.read_value()` on the variable in \"\r\n          \"question: %s\" % value)\r\n    if value is None and self._is_grad_func:\r\n      # Gradient functions may legitimately return a list that contains\r\n      # both Tensors and Python Nones. Unfortuantely this breaks the\r\n      # OpKernel, so for now we replace None objects with zeros, which is\r\n      # mathematically correct but will prevent short-circuiting gradient\r\n      # computations.\r\n      #\r\n      # TODO(akshayka): Make it possible to return a list of both Tensors and\r\n      # Nones from an EagerPyFunc.\r\n      return constant_op.constant(0.0, dtype=dtype)\r\n    return ops.convert_to_tensor(value, dtype=dtype)\r\n```\r\nIn such case, the \"_convert\" function returns \"constant_op.constant(0.0, dtype=dtype)\" as shown in the above code block. With this code, if some of the dtypes happen to be \"int\" types, TypeError is raised because 0.0 is \"float\" while the dtype is \"int\".\r\n\r\nMy commit is just a simple fix to this which replaces 0.0 by 0 if the dtype is integer.", "@kami93 this makes sense. I just want a unit test because my experience has been that in TF any bugfix without a test protecting it will regress into a bug as we make many changes to the codebase. A simple test will do.", "@kami93 Did you get a chance to look on reviewer comments? Please let us know on the update. Thanks! ", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 29727, "title": "EagerFunc tensor conversion error - dtype mismatch", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 10.0 / 7.4.2\r\n- GPU model and memory: 1080Ti\r\n\r\n**Describe the current behavior**\r\n[GIST Link 1](https://gist.github.com/kami93/27da32514c4c9b085bf4fd20dcf6ef57)\r\n[GIST Link 2](https://gist.github.com/kami93/49355db2491ff7b4a03a22fbfbce09f7)\r\n\r\nI am trying to use tfe.pyfunc to combine eager block in the operation graph (See Link 1). There are four input arguments to the eager mode function of dtype tf.float32 and tf.int32. When I try to calculate the gradient of a tensor w.r.t some variables that the calculations involving them are happen in the eager block, the error in the Link 2 is raised (TypeError: Cannot convert provided value to EagerTensor. Provided value: 0.0 Requested dtype: int32).\r\n\r\n**Describe the expected behavior**\r\nThe gradient is calculated and returned after calling tf.Session.run,\r\n\r\n**Code to reproduce the issue**\r\nSee Link 1.\r\n\r\n**Other info / logs**\r\nSee Link 2.\r\nI posted a Pull Request for the fix of this issue. [https://github.com/tensorflow/tensorflow/pull/29728](https://github.com/tensorflow/tensorflow/pull/29728)", "comments": ["@kami93 Let us know is this still an issue. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29727\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29727\">No</a>\n"]}, {"number": 29726, "title": "Updated maximum layer documentation", "body": "Added an example and Raises as well as updated docstring and Arguments. This will take care of this issue #29297", "comments": []}, {"number": 29725, "title": "Updated tanh example", "body": "Updated tanh with example, Arguments and returns", "comments": []}, {"number": 29724, "title": "ok", "body": "bazel build --config=opt --config=mkl //tensorflow:libtensorflow_cc.so\r\nsuccessfully!\r\nbut, have no absl and eigen3\r\n", "comments": ["@cSingleboy Sorry, What did you mean of no absl and eigen3?", "Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}]