[{"number": 54172, "title": "Linking of rule '//tensorflow/python:_pywrap_tensorflow_internal.so' failed (Exit 1)", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX Monterey 12.1 on M1 Chip\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 1.5.4\r\n- Python version: 3.6.12\r\n- Installed using virtualenv? pip? conda?: Yes pipenv\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): Apple clang version 13.0.0 (clang-1300.0.29.30) aka 4.2.1\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n\r\n\r\n**Describe the problem**\r\nBuild fails with linker error - pointer not aligned at address\r\n`tensorflow/tensorflow/python/BUILD:4936:1: Linking of rule '//tensorflow/python:_pywrap_tensorflow_internal.so' failed (Exit 1)`\r\n\r\n```\r\nld: warning: cannot export hidden symbol unsigned int\r\nstd::__1::__sort5<std::__1::vector<tensorflow::tfprof::CodeNode*,\r\nstd::__1::allocator<tensorflow::tfprof::CodeNode*> >\r\ntensorflow::tfprof::TFMultiShow::SortNodes<tensorflow::tfprof::CodeNode>(std::__1::vector<tensorflow::tfprof::CodeNode*,\r\nstd::__1::allocator<tensorflow::tfprof::CodeNode*> > const&,\r\ntensorflow::tfprof::Options const&)::'lambda'\r\n(tensorflow::tfprof::CodeNode const*, tensorflow::tfprof::CodeNode\r\nconst*)&, tensorflow::tfprof::CodeNode**>\r\n(std::__1::vector<tensorflow::tfprof::CodeNode*,\r\nstd::__1::allocator<tensorflow::tfprof::CodeNode*> >\r\ntensorflow::tfprof::TFMultiShow::SortNodes<tensorflow::tfprof::CodeNode>(std::__1::vector<tensorflow::tfprof::CodeNode*,\r\nstd::__1::allocator<tensorflow::tfprof::CodeNode*> > const&,\r\ntensorflow::tfprof::Options const&)::'lambda'\r\n(tensorflow::tfprof::CodeNode const*, tensorflow::tfprof::CodeNode\r\nconst*)&, std::__1::vector<tensorflow::tfprof::CodeNode*,\r\nstd::__1::allocator<tensorflow::tfprof::CodeNode*> >\r\ntensorflow::tfprof::TFMultiShow::SortNodes<tensorflow::tfprof::CodeNode>(std::__1::vector<tensorflow::tfprof::CodeNode*,\r\nstd::__1::allocator<tensorflow::tfprof::CodeNode*> > const&,\r\ntensorflow::tfprof::Options const&)::'lambda'\r\n(tensorflow::tfprof::CodeNode const*, tensorflow::tfprof::CodeNode\r\nconst*)&, std::__1::vector<tensorflow::tfprof::CodeNode*,\r\nstd::__1::allocator<tensorflow::tfprof::CodeNode*> >\r\ntensorflow::tfprof::TFMultiShow::SortNodes<tensorflow::tfprof::CodeNode>(std::__1::vector<tensorflow::tfprof::CodeNode*,\r\nstd::__1::allocator<tensorflow::tfprof::CodeNode*> > const&,\r\ntensorflow::tfprof::Options const&)::'lambda'\r\n(tensorflow::tfprof::CodeNode const*, tensorflow::tfprof::CodeNode\r\nconst*)&, std::__1::vector<tensorflow::tfprof::CodeNode*,\r\nstd::__1::allocator<tensorflow::tfprof::CodeNode*> >\r\ntensorflow::tfprof::TFMultiShow::SortNodes<tensorflow::tfprof::CodeNode>(std::__1::vector<tensorflow::tfprof::CodeNode*,\r\nstd::__1::allocator<tensorflow::tfprof::CodeNode*> > const&,\r\ntensorflow::tfprof::Options const&)::'lambda'\r\n(tensorflow::tfprof::CodeNode const*, tensorflow::tfprof::CodeNode\r\nconst*)&, std::__1::vector<tensorflow::tfprof::CodeNode*,\r\nstd::__1::allocator<tensorflow::tfprof::CodeNode*> >\r\ntensorflow::tfprof::TFMultiShow::SortNodes<tensorflow::tfprof::CodeNode>(std::__1::vector<tensorflow::tfprof::CodeNode*,\r\nstd::__1::allocator<tensorflow::tfprof::CodeNode*> > const&,\r\ntensorflow::tfprof::Options const&)::'lambda'\r\n(tensorflow::tfprof::CodeNode const*, tensorflow::tfprof::CodeNode\r\nconst*)&, tensorflow::tfprof::CodeNode) from\r\nbazel-out/host/bin/tensorflow/core/profiler/internal/libtfprof_code.a\r\n(tfprof_code.o)\r\n```\r\nThe same error repeats for `tfprof_graph` and `tfprof_op`\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow && git checkout tags/v1.5.4\r\n./configure\r\nbazel build //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n**Please see attachment**\r\n[build_fail.txt](https://github.com/tensorflow/tensorflow/files/7960509/build_fail.txt)", "comments": ["@knkumar ,\r\nWe see that you are using tf version 1.5, 1.x is not actively supported, please update to latest stable v2.7 and let us know if you are facing same issue.Also Please refer to similar issues #44751,#47782 and for installation please take a look at this [link](https://github.com/apple/tensorflow_macos). It helps.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54172\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54172\">No</a>\n"]}, {"number": 54170, "title": "[TF-TRT] Add converter for tf.fill", "body": "This PR adds a TF-TRT converter for the [tf.fill operator.](https://www.tensorflow.org/api_docs/python/tf/fill)\r\nThe converter supports tensor and weights for both inputs (`dims` and `value`).\r\n\r\nIt adds an overload for `TRTNetworkBuilder::Constant ` in layer_utils.h to create a Constant layer from a value, a shape and a data type.\r\n\r\nIt also moves the `TFTRT_INTERNAL_ERROR_AT_NODE` and `TFTRT_RETURN_ERROR_IF_NULLPTR ` macros to the convert_nodes header.", "comments": ["Tagging @bixia1 for review.", "Looks good to me. Please also squash the commits if you change is only for address the comments.", "@Kh4L Can you please address Ubuntu Sanity errors? Thanks!", "@Kh4L Can you please resolve conflicts? Thanks!", "> @Kh4L Can you please resolve conflicts? Thanks!\r\n\r\nDone", "Some tests are failing because of error message mismatch, convert_nodes_test.cc:\r\nValue of: status\r\nExpected: has a status code that is equal to UNIMPLEMENTED, and has an error message that has substring \"Conversion for Fill is not implemented inimplicit batch mode\"\r\n  Actual: UNIMPLEMENTED: Op type Fill is not supported. (of type tensorflow::Status), whose error message is wrong\r\n", "> Some tests are failing because of error message mismatch, convert_nodes_test.cc: Value of: status Expected: has a status code that is equal to UNIMPLEMENTED, and has an error message that has substring \"Conversion for Fill is not implemented inimplicit batch mode\" Actual: UNIMPLEMENTED: Op type Fill is not supported. (of type tensorflow::Status), whose error message is wrong\r\n\r\nI didn't add the version check in the tests. I just pushed the fix."]}, {"number": 54169, "title": "[oneDNN] Add env variable to MKL allocator to control mem allocation.", "body": "This PR implements an env variable to request the MKL allocator to always use the system allocator. We recently. found a few usages where the lock overhead in BFC allocator was significant and switching back to default allocator helped performance.", "comments": ["We have found this patch allowed us to improve CPU utilization and reduce tailed latency considerably. ", "@rohan100jain checking if you had the chance to review the PR.", "@rohan100jain Thanks for reviewing the PR.\r\nThe lambda implementation in the PR is consistent with some of the newer env variable implementations. For example\r\nhttps://github.com/tensorflow/tensorflow/blob/0b6b491d21d6a4eb5fbab1cca565bc1e94ca9543/tensorflow/core/util/util.cc#L146\r\nThe implementation for the other variable in this file is pretty old.\r\nThe small size allocator is eventually using the system allocator and I felt including small_size_allocator in the name is too implementation specific. System allocator can be a generic option even if the MklCPUAllocator implementation changes in the future.", "@penpornk and @rohan100jain  I have made the changes requested. Please take a look.", "@Srini511 Can you please address Ubuntu Sanity errors? Thanks!", "> @Srini511 Can you please address Ubuntu Sanity errors? Thanks!\r\n\r\n@gbaned Since Srini is on vacation , I pushed the changes to address Ubuntu sanity errors", "@sachinmuradi Could you please help take a look at the [Windows error](https://source.cloud.google.com/results/invocations/c660e49d-7006-420f-88be-9f450ec8c666/targets/%2F%2Ftensorflow%2Ftools%2Fci_build%2Fbuilds:gen_win_out/log) as well? It has cyclic dependency error.\r\n\r\n```\r\nERROR: T:/src/github/tensorflow/tensorflow/core/common_runtime/BUILD:1779:16: in cc_library rule //tensorflow/core/common_runtime:core_cpu_impl: cycle in dependency graph:\r\n    //tensorflow/tools/pip_package:build_pip_package (9ac86ca5448b6ce48b067072aa0b958763639077f6985cad58583d5a07d0ae27)\r\n    //tensorflow/python/tpu:tpu (9ac86ca5448b6ce48b067072aa0b958763639077f6985cad58583d5a07d0ae27)\r\n    //tensorflow/python/tpu:tpu_estimator (9ac86ca5448b6ce48b067072aa0b958763639077f6985cad58583d5a07d0ae27)\r\n    //tensorflow/python:function (9ac86ca5448b6ce48b067072aa0b958763639077f6985cad58583d5a07d0ae27)\r\n    //tensorflow/python/framework:function (9ac86ca5448b6ce48b067072aa0b958763639077f6985cad58583d5a07d0ae27)\r\n    //tensorflow/python/framework:graph_to_function_def (9ac86ca5448b6ce48b067072aa0b958763639077f6985cad58583d5a07d0ae27)\r\n    //tensorflow/python/framework:op_def_registry (9ac86ca5448b6ce48b067072aa0b958763639077f6985cad58583d5a07d0ae27)\r\n    //tensorflow/python/framework:_op_def_registry (9ac86ca5448b6ce48b067072aa0b958763639077f6985cad58583d5a07d0ae27)\r\n    //tensorflow/python/framework:_op_def_registry.pyd (9ac86ca5448b6ce48b067072aa0b958763639077f6985cad58583d5a07d0ae27)\r\n    //tensorflow/python/framework:_op_def_registry_pyd_copy (9ac86ca5448b6ce48b067072aa0b958763639077f6985cad58583d5a07d0ae27)\r\n    //tensorflow/python/framework:_op_def_registry.so (9ac86ca5448b6ce48b067072aa0b958763639077f6985cad58583d5a07d0ae27)\r\n    //tensorflow/python/lib/core:pybind11_status (9ac86ca5448b6ce48b067072aa0b958763639077f6985cad58583d5a07d0ae27)\r\n    //tensorflow/python/lib/core:pybind11_status_headers (9ac86ca5448b6ce48b067072aa0b958763639077f6985cad58583d5a07d0ae27)\r\n    //tensorflow/core/common_runtime:core_cpu_headers_lib (9ac86ca5448b6ce48b067072aa0b958763639077f6985cad58583d5a07d0ae27)\r\n    //tensorflow/core/common_runtime:core_cpu_headers_lib_gather (9ac86ca5448b6ce48b067072aa0b958763639077f6985cad58583d5a07d0ae27)\r\n    //tensorflow/core/common_runtime:core_cpu_lib (9ac86ca5448b6ce48b067072aa0b958763639077f6985cad58583d5a07d0ae27)\r\n.-> //tensorflow/core/common_runtime:core_cpu_impl (9ac86ca5448b6ce48b067072aa0b958763639077f6985cad58583d5a07d0ae27)\r\n|   //tensorflow/core/common_runtime:threadpool_device_factory (9ac86ca5448b6ce48b067072aa0b958763639077f6985cad58583d5a07d0ae27)\r\n|   //tensorflow/core/common_runtime:threadpool_device (9ac86ca5448b6ce48b067072aa0b958763639077f6985cad58583d5a07d0ae27)\r\n|   //tensorflow/core/common_runtime:mkl_cpu_allocator (9ac86ca5448b6ce48b067072aa0b958763639077f6985cad58583d5a07d0ae27)\r\n|   //tensorflow/core/util:onednn_env_vars (9ac86ca5448b6ce48b067072aa0b958763639077f6985cad58583d5a07d0ae27)\r\n|   //tensorflow/core:core_cpu (9ac86ca5448b6ce48b067072aa0b958763639077f6985cad58583d5a07d0ae27)\r\n|   //tensorflow/core/common_runtime:core_cpu (9ac86ca5448b6ce48b067072aa0b958763639077f6985cad58583d5a07d0ae27)\r\n|   //tensorflow/core/common_runtime:core_cpu_internal (9ac86ca5448b6ce48b067072aa0b958763639077f6985cad58583d5a07d0ae27)\r\n`-- //tensorflow/core/common_runtime:core_cpu_impl (9ac86ca5448b6ce48b067072aa0b958763639077f6985cad58583d5a07d0ae27)\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted\r\n```"]}, {"number": 54168, "title": "Disable broken tests", "body": null, "comments": []}, {"number": 54167, "title": "Disable broken tests", "body": null, "comments": []}, {"number": 54166, "title": "Disable broken tests", "body": null, "comments": []}, {"number": 54165, "title": "Build did NOT complete successfully ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip install \r\n- TensorFlow version: 2.6.0\r\n- Python version:3.8.5\r\n- Installed using virtualenv? pip? conda?: conda,  maybe I do not use conda env\r\n- Bazel version (if compiling from source): 4.2.2\r\n- GCC/Compiler version (if compiling from source):9.3.0\r\n- CUDA/cuDNN version: 11.5\r\n- GPU model and memory: Quadro P4000   8G\r\n\r\n\r\n\r\n**Describe the problem**\r\nI couldn't build successfully as the instructions.   \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\nsudo proxychains bash apollo.sh build\r\n\r\n# OR\r\nsudo  bash apollo.sh build\r\n```\r\n![image](https://user-images.githubusercontent.com/30117686/151467138-48a0f20d-24ef-4f67-b501-37d9bebe64c7.png)\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n```\r\nroot@in-dev-docker:/apollo# sudo proxychains bash apollo.sh build\r\nProxyChains-3.1 (http://proxychains.sf.net)\r\n[INFO] Apollo Environment Settings:\r\n[INFO]     APOLLO_ROOT_DIR: /apollo\r\n[INFO]     APOLLO_CACHE_DIR: /apollo/.cache\r\n[INFO]     APOLLO_IN_DOCKER: true\r\n[INFO]     APOLLO_VERSION: predtr-2021-12-28-463fb82f9e\r\n[INFO]     DOCKER_IMG: \r\n[INFO]     APOLLO_ENV:  STAGE=dev USE_ESD_CAN=false\r\n[INFO]     USE_GPU: USE_GPU_HOST= USE_GPU_TARGET=1\r\n[ OK ] Running GPU build on x86_64 platform.\r\n[WARNING] ESD CAN library supplied by ESD Electronics doesn't exist.\r\n[WARNING] If you need ESD CAN, please refer to:\r\n[WARNING]   third_party/can_card_library/esd_can/README.md\r\n[INFO] Build Overview: \r\n[INFO]     USE_GPU: 1  [ 0 for CPU, 1 for GPU ]\r\n[INFO]     Bazel Options: --config=gpu\r\n[INFO]     Build Targets: //modules/... union //cyber/...\r\n[INFO]     Disabled:      except //modules/drivers/canbus/can_client/esd/...\r\nStarting local Bazel server and connecting to it...\r\nWARNING: ignoring LD_PRELOAD in environment.\r\n(00:29:33) INFO: Invocation ID: 40695d7b-c7dd-416c-b7ae-de72a1612dec\r\n(00:29:33) INFO: Current date is 2022-01-28\r\n(00:30:32) INFO: Repository build_bazel_rules_swift instantiated at:\r\n  /apollo/WORKSPACE:68:16: in <toplevel>\r\n  /apollo/.cache/bazel/540135163923dd7d5820f3ee4b306b32/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:38:29: in grpc_extra_deps\r\n  /apollo/.cache/bazel/540135163923dd7d5820f3ee4b306b32/external/build_bazel_rules_apple/apple/repositories.bzl:117:11: in apple_rules_dependencies\r\n  /apollo/.cache/bazel/540135163923dd7d5820f3ee4b306b32/external/build_bazel_rules_apple/apple/repositories.bzl:84:14: in _maybe\r\nRepository rule http_archive defined at:\r\n  /apollo/.cache/bazel/540135163923dd7d5820f3ee4b306b32/external/bazel_tools/tools/build_defs/repo/http.bzl:336:31: in <toplevel>\r\n(00:30:32) INFO: Repository rules_java instantiated at:\r\n  /apollo/WORKSPACE:68:16: in <toplevel>\r\n  /apollo/.cache/bazel/540135163923dd7d5820f3ee4b306b32/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:29:18: in grpc_extra_deps\r\n  /apollo/.cache/bazel/540135163923dd7d5820f3ee4b306b32/external/com_google_protobuf/protobuf_deps.bzl:44:21: in protobuf_deps\r\nRepository rule http_archive defined at:\r\n  /apollo/.cache/bazel/540135163923dd7d5820f3ee4b306b32/external/bazel_tools/tools/build_defs/repo/http.bzl:336:31: in <toplevel>\r\n(00:30:32) WARNING: Download from https://github.com/bazelbuild/rules_java/archive/981f06c3d2bd10225e85209904090eb7b5fb26bd.tar.gz failed: class java.io.IOException connect timed out\r\n(00:30:32) ERROR: An error occurred during the fetch of repository 'rules_java':\r\n   Traceback (most recent call last):\r\n\tFile \"/apollo/.cache/bazel/540135163923dd7d5820f3ee4b306b32/external/bazel_tools/tools/build_defs/repo/http.bzl\", line 111, column 45, in _http_archive_impl\r\n\t\tdownload_info = ctx.download_and_extract(\r\nError in download_and_extract: java.io.IOException: Error downloading [https://github.com/bazelbuild/rules_java/archive/981f06c3d2bd10225e85209904090eb7b5fb26bd.tar.gz] to /apollo/.cache/bazel/540135163923dd7d5820f3ee4b306b32/external/rules_java/temp14011213055655260428/981f06c3d2bd10225e85209904090eb7b5fb26bd.tar.gz: connect timed out\r\n(00:30:32) ERROR: While resolving toolchains for target //modules/v2x/v2x_proxy/os_interface:os_interface_cpplint: invalid registered toolchain '@bazel_tools//tools/jdk:all': while parsing '@bazel_tools//tools/jdk:all': no such package '@rules_java//java': java.io.IOException: Error downloading [https://github.com/bazelbuild/rules_java/archive/981f06c3d2bd10225e85209904090eb7b5fb26bd.tar.gz] to /apollo/.cache/bazel/540135163923dd7d5820f3ee4b306b32/external/rules_java/temp14011213055655260428/981f06c3d2bd10225e85209904090eb7b5fb26bd.tar.gz: connect timed out\r\n(00:30:32) ERROR: Analysis of target '//modules/v2x/v2x_proxy/os_interface:os_interface_cpplint' failed; build aborted: invalid registered toolchain '@bazel_tools//tools/jdk:all': while parsing '@bazel_tools//tools/jdk:all': no such package '@rules_java//java': java.io.IOException: Error downloading [https://github.com/bazelbuild/rules_java/archive/981f06c3d2bd10225e85209904090eb7b5fb26bd.tar.gz] to /apollo/.cache/bazel/540135163923dd7d5820f3ee4b306b32/external/rules_java/temp14011213055655260428/981f06c3d2bd10225e85209904090eb7b5fb26bd.tar.gz: connect timed out\r\n(00:30:32) INFO: Elapsed time: 59.517s\r\n(00:30:32) INFO: 0 processes.\r\n(00:30:32) FAILED: Build did NOT complete successfully (595 packages loaded, 6242 targets configured)\r\n    currently loading: @bazel_tools//tools/jdk ... (3 packages)\r\nroot@in-dev-docker:/apollo# \r\n```", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54165\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54165\">No</a>\n"]}, {"number": 54164, "title": "RuntimeError: Encountered unresolved custom op: UnboundedIndexRangeEncode. number 3 (UnboundedIndexRangeEncode) failed to prepare. Node number 108 (WHILE) failed to invoke.", "body": "**### 1. System information**\r\n\r\nLinux\r\nTF 2.7.0\r\n### 2. Code\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\nConverter:\r\n\r\n  import argparse\r\n  import io\r\n  import os\r\n  import sys\r\n  import urllib\r\n  from absl import app\r\n  from absl.flags import argparse_flags\r\n  import cv2\r\n  import numpy as np\r\n  import tensorflow.compat.v1 as tf\r\n  \r\n  #import tensorflow as tf\r\n  from tensorflow.python import pywrap_tensorflow\r\n  import tensorflow_compression as tfc  # pylint:disable=unused-import\r\n  \r\n  \r\n  \r\n  \r\n  with tf.Session() as sess:\r\n          saver = tf.train.import_meta_graph('/mnt/6t_hdd/Priyanka/hific-lo.metagraph')\r\n          #saver.restore(sess, latest_checkpoint_path)\r\n          inputs=None\r\n          outputs=None\r\n          signature='sender'\r\n          request = urllib.request.urlopen('file:///mnt/6t_hdd/Priyanka/hific-lo.metagraph') # replace it with your local path and model\r\n          try:\r\n                  string = request.read()\r\n          finally:\r\n                  request.close()\r\n  \r\n          metagraph = tf.compat.v1.MetaGraphDef()\r\n          loaded = metagraph.ParseFromString(string)\r\n  \r\n          wrapped_import = tf.compat.v1.wrap_function(lambda: tf.compat.v1.train.import_meta_graph('/mnt/6t_hdd/Priyanka/hific-lo.metagraph'), [])\r\n          graph = wrapped_import.graph\r\n          print(\"*************************************\")\r\n          inputs = metagraph.signature_def['sender'].inputs\r\n          print(inputs)\r\n          concrete_function =  metagraph.signature_def['sender']\r\n          print(\"$$$$$$$$$$\")\r\n          inputs = [graph.as_graph_element(inputs[k].name) for k in sorted(inputs)]\r\n          print(inputs)\r\n          outputs = metagraph.signature_def[signature].outputs\r\n          print(outputs)\r\n          outputs = [graph.as_graph_element(outputs[k].name) for k in sorted(outputs)]\r\n          print(\"*************************************\")\r\n          converter = tf.lite.TFLiteConverter.from_session(sess, inputs, outputs)\r\n          converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n          converter.allow_custom_ops = True\r\n          converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n          tflite_model = converter.convert()\r\n          open(\"/mnt/6t_hdd/Priyanka/converted_model_fp16.tflite\", \"wb\").write(tflite_model) # replace the path with your local path\r\n\r\n\r\nHere, i get the warning for custom ops as below - \r\nCustom ops: UnboundedIndexRangeDecode, UnboundedIndexRangeEncode\r\nDetails:\r\n        tf.UnboundedIndexRangeDecode(tensor<*x!tf_type.string>, tensor<?x?x320xi32>, tensor<320x48xi32>, tensor<320xi32>, tensor<320xi32>) -> (tensor<?x?x320xi32>) : {debug_level = 0 : i64, device = \"\", overflow_width = 4 : i64, precision = 16 : i64}\r\n        tf.UnboundedIndexRangeEncode(tensor<*xi32>, tensor<*xi32>, tensor<64x1481xi32>, tensor<64xi32>, tensor<64xi32>) -> (tensor<!tf_type.string>) : {debug_level = 0 : i64, device = \"\", overflow_width = 4 : i64, precision = 16 : i64}\r\n        tf.UnboundedIndexRangeEncode(tensor<*xi32>, tensor<?x?x320xi32>, tensor<320x48xi32>, tensor<320xi32>, tensor<320xi32>) -> (tensor<!tf_type.string>) : {debug_level = 0 : i64, device = \"\", overflow_width = 4 : i64, precision = 16 : i64}\r\nSee instructions: https://www.tensorflow.org/lite/guide/ops_custom\r\n\r\n\r\n\r\n\r\n\r\nInference:\r\n\r\n# Load the TFLite model and allocate tensors.\r\ninterpreter = tf.lite.Interpreter(model_path=\"./model.tflite\")\r\ninterpreter.allocate_tensors()\r\nprint(\"all ok\")\r\n\r\n# # Get input and output tensors.\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\n# # Test the model on random input data.\r\ninput_shape = input_details[0]['shape']\r\ninput_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\r\ninterpreter.set_tensor(input_details[0]['index'], input_data)\r\n\r\ninterpreter.invoke()\r\n\r\n# # The function `get_tensor()` returns a copy of the tensor data.\r\n# # Use `tensor()` in order to get a pointer to the tensor.\r\noutput_data = interpreter.get_tensor(output_details[0]['index'])\r\nprint(output_data)\r\n\r\n\r\n*The output from the converter invocation*\r\n\r\nTraceback (most recent call last):\r\n  File \"load_tflite.py\", line 31, in <module>\r\n    interpreter.invoke()\r\n  File \"/home/marlin/anaconda/envs/prune/lib/python3.7/site-packages/tensorflow/lite/python/interpreter.py\", line 923, in invoke\r\n    self._interpreter.Invoke()\r\nRuntimeError: Encountered unresolved custom op: UnboundedIndexRangeEncode.\r\nSee instructions: https://www.tensorflow.org/lite/guide/ops_customNode number 3 (UnboundedIndexRangeEncode) failed to prepare.\r\nNode number 108 (WHILE) failed to invoke.\r\n\r\n\r\nI'm wondering why the flex delegate couldn't prepare the UnboundedIndexRangeEncode function, even though I have the SELECT_TF_OPS flag enabled?\r\n\r\n", "comments": ["@prmudgal ,\r\nWhile executing the given code i was facing different error.Please find the gist [here](https://colab.research.google.com/gist/tilakrayal/ed5f93dbeace20ae3f146ca8f3819dc1/untitled203.ipynb).In order to expedite the trouble-shooting process, could you please provide the complete code and dataset to reproduce the issue reported here.", "@tilakrayal \r\nthe error is because of missing model. You can find the trained model [here ](https://storage.googleapis.com/tensorflow_compression/metagraphs/hific-lo.metagraph)\r\nDataset - [Coco2014](https://cocodataset.org/#home)\r\n\r\nAlthough the error is because of missing Custom op. Is it possible to add \"UnboundedIndexRangeEncode\" in custom op?", "@prmudgal ,\r\nThe code provided is fairly complex hence it would be difficult for us to pinpoint the issue. Could you please get the example down to the simplest possible repro? That will allow us to determine the source of the issue easily. Thanks!", "Code is updated. Please see the it in original comment. Download the model from [here ](https://storage.googleapis.com/tensorflow_compression/metagraphs/hific-lo.metagraph).", "I updated the code in colab, please see the gist [here](https://colab.research.google.com/gist/prmudgal/a8080494c3e554da70e55abc1acdc5e3/untitled203.ipynb)", "Is there any update on this issue?", "@prmudgal ,\r\nLooks like this is not a bug  related to Tensorflow end.Please take a look this issue [1](https://github.com/tensorflow/compression/issues/22) and [2](https://github.com/tensorflow/compression/issues/32) from the compression repo and try to post in respective repo from [here](https://github.com/tensorflow/compression/issues).It helps.Thanks", "@tilakrayal Thanks for the suggestion. Tensorflow compression suggestion is to include it as custom op in TFLite. Please see [here](https://github.com/tensorflow/compression/issues/119#issuecomment-1029331406). It will be helpful if TFLite team can support to include this as custom op.", "@sachinprasadhs ,\r\nI have tried in colab with TF version v2.5, v2.7 and nightly version and noticed that session is being crashed. Please, find the gist [here](https://colab.research.google.com/gist/tilakrayal/b88eb4cdbee6c90015cc76dc2415ee8f/54164.ipynb). Thanks!", "Is there any update on this?", "@sachinprasadhs following up on this issue.", "Hello @prmudgal The TF version of the op can in included in the TFLite model if it's in the [TFLite Allowlist](https://www.tensorflow.org/lite/guide/op_select_allowlist). Refer to [this FAQ](https://www.tensorflow.org/lite/guide/faq#why_are_some_operations_not_implemented_in_tensorflow_lite). If not, you would have to create your own custom op or you can [request an op via a Github issue](https://github.com/tensorflow/tensorflow/issues/new?assignees=&labels=comp%3Alite&template=40-tflite-op-request.md). Before you request a new op, check if there [existing issues](https://github.com/tensorflow/tensorflow/issues?q=label%3Acomp%3Alite+) with the same request and comment on it with your request details.\r\n\r\nWe will no be able to support this as a custom op, unless we have several users requesting the same op, due to limited bandwidth.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54164\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54164\">No</a>\n"]}, {"number": 54163, "title": "Why the parameter size of mobilenetv3 in 2.7.0 is different from nightly?", "body": "<em>\r\n</em>\r\nThe number of parameters of MobileNetv3Small(alpha=1) in tf=2.7.0 is 1.5M, while it is 0.9M in tf-nightly. \r\n\r\nThe same code was used to load the MobileNetV3Small model.\r\n\r\n`\r\nbase_model =keras.applications.MobileNetV3Small(input_shape=(384, 512, 3),include_top=False,weights='imagenet',)\r\nbase_model.summary()\r\n`\r\n\r\nI saw that some layers were deleted in tf-nightly. Why is that?\r\n\r\n![image](https://user-images.githubusercontent.com/41880345/151437092-63762b58-5bca-4ca7-9409-2146ed80631e.png)\r\n", "comments": ["Hi @molvcan ! \r\nCould you please post this on [TF forum](https://discuss.tensorflow.org/) as  this is not a bug or feature request. Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 54162, "title": "Bump the maximum threshold before erroring", "body": "PiperOrigin-RevId: 424653571\r\nChange-Id: Ic2d9f3a7db627d78cde80ad415105f3d53735b3b", "comments": []}, {"number": 54161, "title": "Bump the maximum threshold before erroring", "body": "PiperOrigin-RevId: 424653571\r\nChange-Id: Ic2d9f3a7db627d78cde80ad415105f3d53735b3b", "comments": []}, {"number": 54160, "title": "Bump the maximum threshold before erroring", "body": "PiperOrigin-RevId: 424653571\r\nChange-Id: Ic2d9f3a7db627d78cde80ad415105f3d53735b3b", "comments": []}, {"number": 54159, "title": "Bump the maximum threshold before erroring", "body": "PiperOrigin-RevId: 424653571\r\nChange-Id: Ic2d9f3a7db627d78cde80ad415105f3d53735b3b", "comments": []}, {"number": 54158, "title": "Revert \"Fix Null-pointer dereference in BuildXlaCompilationCache\"", "body": "Reverts tensorflow/tensorflow#54092", "comments": []}, {"number": 54156, "title": "MacOS build compilation error", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS High Sierra (10.13.6)\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.6\r\n- Python version: 3.9.10\r\n- Bazel version (if compiling from source): 4.2.2\r\n- GCC/Compiler version (if compiling from source): Apple LLVM version 10.0.0 (clang-1000.10.44.4)\r\n\r\n**Describe the problem**\r\n\r\nHi.\r\n\r\nThe problem i'm trying to solve is to build and install TF on early2009 mac mini with core2duo and 4gb ram inside.\r\n\r\nFirstly I tried to install TF through PIP (`pip install tensorflow/pip install tensorflow-cpu`). But after running `python -c 'import keras'` i got `illegal instruction 4` error.\r\n\r\nAs far as I understand the pip package is build with AVX support which not supported on my workstation. So i'm trying to create package manually from the 'r2.6' branch but stucked on some wired c++ compilation errors.\r\n\r\n\r\n\r\nThe build commands i'm trying:\r\n```\r\nbazel build --config libc++ //tensorflow/tool/...\r\nbazel build //tensorflow/tool/...\r\n```\r\n\r\n\r\nProduces such kind of output\r\n```\r\nStarting local Bazel server and connecting to it... \r\nINFO: Options provided by the client: \r\n  Inherited 'common' options: --isatty=1 --terminal_columns=109 \r\nINFO: Reading rc options for 'build' from /Users/macmini/Documents/extra_money/tensorflow/.bazelrc: \r\n  Inherited 'common' options: --experimental_repo_remote_exec \r\nINFO: Reading rc options for 'build' from /Users/macmini/Documents/extra_money/tensorflow/.bazelrc: \r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true \r\nINFO: Reading rc options for 'build' from /Users/macmini/Documents/extra_money/tensorflow/.tf_configure.bazelrc: \r\n  'build' options: --action_env PYTHON_BIN_PATH=/usr/local/opt/python@3.9/bin/python3.9 --action_env PYTHON_LIB_PATH=/usr/local/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages --python_path=/usr/local/opt/python@3.9/bin/python3.9 \r\nINFO: Found applicable config definition build:short_logs in file /Users/macmini/Documents/extra_money/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING \r\nINFO: Found applicable config definition build:v2 in file /Users/macmini/Documents/extra_money/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1 \r\nINFO: Found applicable config definition build:libc++ in file /Users/macmini/Documents/extra_money/tensorflow/.bazelrc: --action_env=CC --action_env=CXX --action_env=CXXFLAGS=-stdlib=libc++ --action_env=PATH --define force_libcpp=enabled --linkopt -fuse-ld=lld \r\nINFO: Found applicable config definition build:macos in file /Users/macmini/Documents/extra_money/tensorflow/.bazelrc: --apple_platform_type=macos --copt=-DGRPC_BAZEL_BUILD --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 \r\nDEBUG: /private/var/tmp/_bazel_macmini/09f55674c85a83ddba7431ff9a1fa9d3/external/tf_runtime/third_party/cuda/dependencies.bzl:51:10: The following command will download NVIDIA proprietary software. By using the software you agree to comply with the terms of the license agreement that accompanies the software. If you do not agree to the terms of the license agreement, do not use the software. \r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\" \r\nDEBUG: Repository io_bazel_rules_docker instantiated at: \r\n  /Users/macmini/Documents/extra_money/tensorflow/WORKSPACE:23:14: in <toplevel> \r\n  /Users/macmini/Documents/extra_money/tensorflow/tensorflow/workspace0.bzl:108:34: in workspace \r\n  /private/var/tmp/_bazel_macmini/09f55674c85a83ddba7431ff9a1fa9d3/external/bazel_toolchains/repositories/repositories.bzl:37:23: in repositories \r\nRepository rule git_repository defined at: \r\n  /private/var/tmp/_bazel_macmini/09f55674c85a83ddba7431ff9a1fa9d3/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel> \r\nINFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (428 packages loaded, 33200 targets configured). \r\nINFO: Found 1 target... \r\n[109 / 551] 2 actions running\r\n\r\n....\r\n```\r\n\r\nand after a couple of hours waiting i get  huge list of  errors like\r\n\r\n```\r\nIn file included from ./tensorflow/core/util/example_proto_helper.h:32: \r\n./tensorflow/core/util/sparse/sparse_tensor.h:181:10: error: no template named 'StatusOr' \r\n  static StatusOr<SparseTensor> Slice(const SparseTensor& tensor, \r\n         ^ \r\n\r\nor \r\n\r\n\r\n/tensorflow/core/util/sparse/sparse_tensor.h:599:7: error: no viable conversion from returned value of type '::tensorflow::Status' to function return type 'int'\r\nTF_RETURN_IF_ERROR( \r\n      ^~~~~~~ \r\n\r\n```\r\n\r\n\r\nAlso I've tried with actual 'master' branch and also got \r\n\r\n```\r\nERROR: /Users/macmini/Documents/extra_money/tensorflow/tensorflow/compiler/xla/BUILD:226:11: Compiling tensorflow/compiler/xla/util.cc failed: (Exit 1): cc_wrapper.sh failed: error executing command external/local_config_cc/cc_wrapper.sh -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 98 argument(s) skipped) \r\ntensorflow/compiler/xla/util.cc:142:7: error: no matching function for call to 'isnan' \r\n  if (std::isnan(value) && kPayloadBits > 0) { \r\n      ^~~~~~ \r\ntensorflow/compiler/xla/util.cc:153:3: note: in instantiation of function template specialization 'xla::RoundTripNanPayload<unsigned short, Eigen::bfloat16>' requested here \r\n  RoundTripNanPayload<uint16_t>(value, &result); \r\n  ^ \r\n/Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:505:1: note: candidate template ignored: requirement 'std::is_floating_point<bfloat16>::value' was not satisfied [with _A1 = Eigen::bfloat16] \r\nisnan(_A1 __lcpp_x) _NOEXCEPT \r\n^ \r\n/Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:513:1: note: candidate template ignored: requirement 'std::is_integral<bfloat16>::value' was not satisfied [with _A1 = Eigen::bfloat16] \r\nisnan(_A1) _NOEXCEPT \r\n^ \r\ntensorflow/compiler/xla/util.cc:142:7: error: no matching function for call to 'isnan' \r\n  if (std::isnan(value) && kPayloadBits > 0) { \r\n      ^~~~~~ \r\ntensorflow/compiler/xla/util.cc:159:3: note: in instantiation of function template specialization 'xla::RoundTripNanPayload<unsigned short, Eigen::half>' requested here \r\n  RoundTripNanPayload<uint16_t>(value, &result); \r\n  ^ \r\n/Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:505:1: note: candidate template ignored: requirement 'std::is_floating_point<half>::value' was not satisfied [with _A1 = Eigen::half] \r\nisnan(_A1 __lcpp_x) _NOEXCEPT \r\n^ \r\n/Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:513:1: note: candidate template ignored: requirement 'std::is_integral<half>::value' was not satisfied [with _A1 = Eigen::half] \r\nisnan(_A1) _NOEXCEPT \r\n^ \r\n2 errors generated. \r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build \r\nUse --verbose_failures to see the command lines of failed build steps. \r\nINFO: Elapsed time: 6493.890s, Critical Path: 218.62s \r\nINFO: 2739 processes: 7 internal, 2732 local. \r\nFAILED: Build did NOT complete successfully \r\n```\r\n\r\nHonestly I'm not good in c++ templates and I don't understand how to solve this compilation error. \r\nMaybe you could help me with this?", "comments": ["Also attaching whole 2.6 error\r\n[error2.6.log](https://github.com/tensorflow/tensorflow/files/7952732/error2.6.log)\r\nr", "Also, is there any reason try to use gcc instead of clang? ", "@i00lii ,\r\nEvery TensorFlow release is compatible with a certain version, for more information please take a look at the [tested build configurations](https://www.tensorflow.org/install/source#cpu_2).In this case, can you please try installing TensorFlow v2.6 with respective configurations and check if you are facing the same error. Thanks!", "@tilakrayal thanks for your response.\r\nI'll try to update llvm/clang up to required version (Clang from xcode 10.11) and then continue with tensorflow itself. ", "Also, here is minimal repo of compilation error.\r\n\r\n```\r\n#include <iostream>\r\n#include <cmath>\r\n#include \"Eigen/Core\"\r\n\r\ntemplate <typename FloatT>\r\nstatic bool Yoba(FloatT value) {\r\n    return std::isnan(value);\r\n}\r\n\r\nbool Yoba1(Eigen::half value) {\r\n    return Yoba(value);\r\n}\r\n\r\nint main() {\r\n    Eigen::half x = Eigen::half(0.1);\r\n    std::cout << Yoba1(x);\r\n    return 0;\r\n}\r\n```\r\n\r\nThis code compiles correctly on my modern macbook with `clang++ -Wall -std=c++11 main.cc -o main -I.../eigen/3.4.0_1/include/eigen3` command\r\n\r\nbut compilation failed on old mac with \r\n```\r\nmac-mini:Documents macmini$ clang++ -Wall -std=c++11 main.cc -o main -I/usr/local/Cellar/eigen/3.4.0_1/include/eigen3 \r\nIn file included from main.cc:3: \r\nIn file included from /usr/local/Cellar/eigen/3.4.0_1/include/eigen3/Eigen/Core:15: \r\n/usr/local/Cellar/eigen/3.4.0_1/include/eigen3/Eigen/src/Core/util/DisableStupidWarnings.h:48:38: warning:  \r\n      unknown warning group '-Wimplicit-int-float-conversion', ignored \r\n      [-Wunknown-warning-option] \r\n    #pragma clang diagnostic ignored \"-Wimplicit-int-float-conversion\" \r\n                                     ^ \r\nmain.cc:6:12: error: no matching function for call to 'isnan' \r\n    return std::isnan(value); \r\n           ^~~~~~ \r\nmain.cc:9:12: note: in instantiation of function template specialization \r\n      'Yoba<Eigen::half>' requested here \r\n    return Yoba(value); \r\n           ^ \r\n/Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:505:1: note:  \r\n      candidate template ignored: requirement 'std::is_floating_point<half>::value' \r\n      was not satisfied [with _A1 = Eigen::half] \r\nisnan(_A1 __lcpp_x) _NOEXCEPT \r\n^ \r\n/Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:513:1: note:  \r\n      candidate template ignored: requirement 'std::is_integral<half>::value' was \r\n      not satisfied [with _A1 = Eigen::half] \r\nisnan(_A1) _NOEXCEPT \r\n^ \r\n1 warning and 1 error generated.\r\n```\r\n\r\nMaybe you could know why this error occures?", "It's me again. \r\n\r\nThe build process started after I updated llvm and clang up to latest version", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54156\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54156\">No</a>\n", "Here is the result package \r\n\r\nTF 2.6.2 macos 10.13 CPU-only no AVX\r\nhttps://drive.google.com/file/d/1Wndu6YEJIc7N__dERUaGjNm6wri6YhVs/view?usp=sharing"]}, {"number": 54154, "title": "tensorflow-cpu 2.7.0 missing from conda-forge", "body": "`tensorflow-cpu` 2.7.0 is [present on PyPI](https://pypi.org/project/tensorflow-cpu/) as of now but [missing from conda-forge](https://anaconda.org/conda-forge/tensorflow-cpu), where latest is 2.6.2.", "comments": ["The Tensorflow releases made on `PyPI` is maintained by Tensorflow but the Tensorflow releases made on `conda-forge` is a community build releases. \r\nUsually these community builds are done by `TensorFlow Special Interest Groups (SIGs)` and these [community builds ](https://github.com/tensorflow/build#community-supported-tensorflow-builds)are not supported by Tensorflow.\r\n\r\nYou can see the overview and details about the Community supported Tensorflow builds and releases [here](https://github.com/tensorflow/community/blob/master/sigs/build/community-builds.md). Thanks!", "@sachinprasadhs Thanks for clarifying, looks like that happens for conda-forge at https://github.com/conda-forge/tensorflow-feedstock.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54154\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54154\">No</a>\n"]}, {"number": 54153, "title": "Fix multiple declaration of same variable due to bad cherrypick", "body": null, "comments": []}, {"number": 54152, "title": "Fix multiple declaration of same variable due to bad cherrypick", "body": null, "comments": []}, {"number": 54151, "title": "Add missing `statusor` include", "body": null, "comments": []}, {"number": 54150, "title": "Add missing `statusor` include", "body": null, "comments": []}, {"number": 54149, "title": "Check for type inference error on node construction.", "body": "PiperOrigin-RevId: 409415804\r\nChange-Id: Ieb6e020906b96f522bf8e2fa103715ddbbdc434a", "comments": []}, {"number": 54148, "title": "Check for type inference error on node construction.", "body": "PiperOrigin-RevId: 409415804\r\nChange-Id: Ieb6e020906b96f522bf8e2fa103715ddbbdc434a", "comments": []}, {"number": 54147, "title": "tf.math.angle not working in tflite concrete function", "body": "Hi,\r\nWhen i try to use tf.math.angle function inside concrete function and save as tflite model it gives an error like below:\r\n\r\nSome ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select\r\n\r\ntf version 2.7 \r\n\r\nCan you help me with this?\r\n\r\nThanks", "comments": ["@zeynepgulhanuslu ,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code and dataset to reproduce the issue reported here.", "Complete code is like below.\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nprint(tf.__version__)\r\n\r\n\r\nclass MyFeatExtractor(tf.Module):\r\n    def __init__(self,\r\n                 frame_length=512,  # in samples, assumes 25ms of 16khz audio\r\n                 frame_shift=128):\r\n        super().__init__()\r\n        self.frame_length = frame_length\r\n        self.frame_shift = frame_shift\r\n        self.fft_length = int(2 ** np.ceil(np.log2(self.frame_length)))\r\n        self.window_type = \"povey\"\r\n        if self.window_type == \"povey\":\r\n            self.window = tf.reshape(\r\n                tf.pow(tf.signal.hann_window(self.frame_length, periodic=False),\r\n                       0.85), (1, 1, -1))\r\n        else:\r\n            raise NotImplementedError  # TODO: Implement others\r\n\r\n    @tf.function\r\n    def __call__(self, pcm):\r\n        return self.calculate(pcm)\r\n\r\n    def calculate(self, pcm):\r\n        # Gather frames\r\n        framed_signals = tf.signal.frame(pcm, self.frame_length,\r\n                                         self.frame_shift, pad_end=False)\r\n        # Remove DC offset\r\n        means = tf.expand_dims(\r\n            tf.reduce_mean(framed_signals, axis=-1, keepdims=False, name=None),\r\n            -1)\r\n        framed_signals -= means\r\n\r\n        # Apply window\r\n        framed_signals *= self.window\r\n        # Apply FFT\r\n        framed_signals = tf.pad(framed_signals,\r\n                                [[0, 0], [0, 0],\r\n                                 [0, self.fft_length - self.frame_length]])\r\n        fft_result = tf.signal.rfft(framed_signals, [self.fft_length])\r\n        fft_real_part = tf.math.real(fft_result)\r\n        fft_imag_part = tf.math.imag(fft_result)\r\n        fft_abs = tf.math.abs(fft_result)\r\n        fft_phase = tf.math.angle(fft_result)\r\n        return fft_abs, fft_phase\r\n\r\n\r\nif __name__ == '__main__':\r\n    feat_type = \"custom_feat\"\r\n    feat_ex = MyFeatExtractor()\r\n\r\n    concrete_func = feat_ex.__call__.get_concrete_function(\r\n        pcm=tf.TensorSpec(shape=[1, None], dtype=tf.float32))\r\n\r\n    # Convert the model\r\n    converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\n    #converter.experimental_new_converter = True\r\n    converter.allow_custom_ops = True\r\n\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n\r\n    converter.experimental_enable_mlir_converter = True\r\n    tflite_model = converter.convert()\r\n\r\n    # Save the model.\r\n    with open(f'tf_{feat_type}_calculator.tflite', 'wb') as f:\r\n        f.write(tflite_model)\r\n```\r\nI just want to create a model that generates rfft angle and abs value. Not using any dataset.\r\n\r\nThanks", "@zeynepgulhanuslu ,\r\nI was able to execute the mentioned code without any error.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/334e1187362654deec81d8f49cf1c8f0/untitled202.ipynb).Please confirm or provide the error log which you are facing the issue.", "Thank you for your reply. I try to run again and it works :) Sorry for the trouble. It just gives a warning like below:\r\n```\r\nTFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):\r\nFlex ops: FlexAngle\r\nDetails:\r\n\ttf.Angle(tensor<?x?x257xcomplex<f32>>) -> (tensor<?x?x257xf32>) : {device = \"\"}```\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54147\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54147\">No</a>\n"]}, {"number": 54145, "title": "[MHLO]: add ReduceWindow and Clamp support in hlo-legalize-to-lhlo", "body": "It made HloToLhloReduceOpConverter a template, to support both Reduce and ReduceWindow.\r\nIt also added Clamp.", "comments": []}, {"number": 54144, "title": "Removed Extra lines and Rearranged the triple ( \"\"\"   \"\"\" ) comment", "body": null, "comments": ["@Rohitw3code  Can you please sign CLA. Thanks!", "@gbaned  can you help me i want to contribute in tf , don't know where to start", "I have signed in CLA", "This PR doesn't provide any value for the code base, so I would like to just leave the existing code as is. \r\n\r\nOn a side note, you would like to contribute to TF and provide some actually value, please check the contribution welcome list and pick any task from there. \r\n\r\nhttps://github.com/tensorflow/tensorflow/issues?q=is%3Aissue+is%3Aopen+contribution+welcome+label%3A%22stat%3Acontributions+welcome%22", "@qlzh727 thanks for your suggestion"]}, {"number": 54143, "title": "Warning Tensorflow: \"Invalid GPU Compute Capability\"", "body": "\r\nHi...i have  this warning Tensorflow: \r\n\r\n\"Invalid GPU Compute Capability\"\r\n\r\nAtt. \r\nJcnew07", "comments": ["Hi @jcnew07 ! Minimum Gpu compute capability is 3.5 . Attaching relevant [thread](https://www.tensorflow.org/install/gpu#hardware_requirements) for reference. Please fill the [template ](https://github.com/tensorflow/tensorflow/issues/new/choose)for further assistance.Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54143\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54143\">No</a>\n"]}, {"number": 54142, "title": "[PluggableDevice] Add int32 `DEVICE_DEFAULT` registration for Range", "body": null, "comments": ["Hi,\r\n\r\nCould someone review this PR? This is something that we need for the release of our pluggable device plugin.\r\n\r\nThank you!"]}, {"number": 54141, "title": "Add some tolerance to linear_operator_circulant_test so it will pass \u2026", "body": "\u2026on AARCH4\r\nThis test failure was being masked by disabling FMA instructions but as they will be enabled again according to https://github.com/tensorflow/tensorflow/issues/52544 this test will need to be fixed as well.", "comments": ["Tagging in @penpornk and @everton1984 "]}, {"number": 54140, "title": "kernel_tests/linalg:self_adjoint_eig_op_test.py ", "body": "Updating delta variable according to the following PR: https://github.com/tensorflow/tensorflow/issues/52544\r\n\r\nThis is addressing a bug when running the self_adjoint_eig_op_test bazel test on aarch64 devices.\r\nReducing the delta variable from .1 to .08 resolves this. This bug is confirmed to be present on master branch and resolved by this change on master.\r\n\r\nSigned-off-by: Theodore Grey <theodore.grey@gmail.com>", "comments": ["@penpornk tagging you on this PR as you requested. Thanks.", "@penpornk I cannot reproduce the Ubuntu failures. My tests on x86 including Ubuntu 20.04 all pass. Is there a way to re-run that test?", "@K1504296 Can you please resolve conflicts? Thank you!", "The original issue #52544 has been fixed by https://github.com/tensorflow/tensorflow/commit/f2debbd864374477fa45e892173bf1d0ddfb5a47 so I think this PR should just be dropped now. @K1504296 I think you can just close this PR now."]}, {"number": 54139, "title": "[PluggableDevice] Make `candidate_input_indices` constant in `TF_ForwardInputOrAllocateOutput`", "body": null, "comments": ["I'm sorry I get to this PR so late. Is this required for TF-DirectML for TF 2.9? Asking because we are having an API freeze starting tomorrow."]}]