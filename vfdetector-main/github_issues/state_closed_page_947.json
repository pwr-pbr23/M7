[{"number": 25023, "title": "TF device_test additional test cases covers", "body": "1- merge_device api test case", "comments": ["@penpornk \r\n\r\nEager mode execution test check added to validate merge_device test case. Thanks"]}, {"number": 25022, "title": "Bazel Build Tensorflow error", "body": "\r\nroot@gpu-B85M-D3H:/home/gpu/Downloads/tensorflow# bazel build --config=opt --config=cuda --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow/tools/pip_package:build_pip_package\r\nWARNING: Processed legacy workspace file /home/gpu/Downloads/tensorflow/tools/bazel.rc. This file will not be processed in the next release of Bazel. Please read https://github.com/bazelbuild/bazel/issues/6319 for further information, including how to upgrade.\r\nStarting local Bazel server and connecting to it...\r\nERROR: /home/gpu/Downloads/tensorflow/tensorflow/tools/pip_package/BUILD:34:1: Illegal ambiguous match on configurable attribute \"deps\" in //tensorflow/tools/pip_package:included_headers_gather:\r\n@local_config_cuda//cuda:using_nvcc\r\n@local_config_cuda//cuda:using_clang\r\nMultiple matches are not allowed unless one is unambiguously more specialized.\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: \r\n\r\n/home/gpu/Downloads/tensorflow/tensorflow/tools/pip_package/BUILD:34:1: Illegal ambiguous match on configurable attribute \"deps\" in //tensorflow/tools/pip_package:included_headers_gather:\r\n@local_config_cuda//cuda:using_nvcc\r\n@local_config_cuda//cuda:using_clang\r\nMultiple matches are not allowed unless one is unambiguously more specialized.\r\nINFO: Elapsed time: 4.158s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (198 packages loaded)\r\n    currently loading: tensorflow/core/kernels ... (2 packages)\r\n\r\n\r\n\r\nkindly help me to get sloved ", "comments": ["@ajit2678 Thank you for your post. We noticed you have not filled out the fields in the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Along with the template, please provide as many details as possible to find the root cause of the issue. Thanks!", "ERROR: /root/.cache/bazel/_bazel_root/750c4c3f7edf43cdaa78f904ca850347/external/local_config_cuda/cuda/BUILD:1249:1: declared output 'external/local_config_cuda/cuda/cuda/lib/libcudnn.so.7' is a dangling symbolic link\r\nERROR: /root/.cache/bazel/_bazel_root/750c4c3f7edf43cdaa78f904ca850347/external/local_config_cuda/cuda/BUILD:1249:1: not all outputs were created or valid\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 13.777s, Critical Path: 9.38s\r\nINFO: 3 processes: 3 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n\r\nNow This error i am getting\r\n\r\n\u2022\tCUDA 10.0\r\n\u2022\tCUDNN 7.3\r\n\u2022\tNCCL 2.3\r\n", "@ajit2678 Could you please fill the template as mentioned in my earlier response. Details in the template will help us finding the root-cause and solving the issue. Waiting for your system information [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Thanks!", "**System information**\r\n- OS Platform and Distribution (Linux Ubuntu 16.04):- Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:-NO\r\n- TensorFlow installed from (source or binary):- from Git hub source\r\n- TensorFlow version:-1.8\r\n- Python version:-3.5\r\n- Installed using virtualenv? pip? conda?:- conda\r\n- Bazel version (if compiling from source): - 1.8\r\n- GCC/Compiler version (if compiling from source): \r\n- CUDA/cuDNN version: -10.0/7.3 & nccl 2.3\r\n- GPU model and memory: Geforce GT 610\r\n", "@ajit2678 Could you follow the instructions [here](https://github.com/rnreich/ubuntu-tensorflow-gpu-all-versions) and let us know how it works. I recently used those steps and successfully installed tensorflow-gpu. Thanks!", "I have solved the Problem ", "@ajit2678 Thanks! Have a nice day", "I have similar issue when build graph transform. How did you solve yours?", "set clang as cuda compiler to No\r\nDo you want to use clang as CUDA compiler? [y/N]: N \r\nin ./configure  resolve for me"]}, {"number": 25021, "title": "[TFLite, Quantization, Performance] float32 nodes faster than uint8.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.12.0, b'v1.12.0-6341-g8a5d48a'\r\n- Python version: 3.5\r\n- Bazel version (if compiling from source): 0.19.2\r\n- GCC/Compiler version (if compiling from source): c++ (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0\r\n- CUDA/cuDNN version: No\r\n- GPU model and memory: No\r\n\r\n**Describe the current behavior**\r\nI converted my TF model to two TFLite models: float32 and uint8. Then I compared them and nodes DEPTHWISE_CONV2D and SOFTMAX with float32 were faster than with uint8.\r\n\r\n**Describe the expected behavior**\r\nI thought, that at least conv2d with uint8 will faster than with float32.\r\n\r\n**Code to reproduce the issue**\r\nMy code:\r\n```\r\nimport tensorflow as tf\r\n\r\nfor fl_quant in (0, 1):\r\n    X = tf.placeholder(tf.float32, [100, 10, 10, 1], 'X')\r\n\r\n    W1 = tf.Variable(tf.random_normal([3, 3, 1, 32]), name='W1')\r\n    B1 = tf.Variable(tf.random_normal([32]), name='B1')\r\n\r\n    W2 = tf.Variable(tf.random_normal([3200, 10]), name='W2')\r\n    B2 = tf.Variable(tf.random_normal([10]), name='B2')\r\n\r\n    XW1 = tf.nn.conv2d(X, W1, [1, 1, 1, 1], 'SAME')\r\n    XWB1 = tf.nn.relu6(tf.nn.bias_add(XW1, B1))\r\n    XWB1 = tf.reshape(XWB1, [100, 3200])\r\n    XWB2 = tf.nn.bias_add(tf.matmul(XWB1, W2), B2)\r\n    Result = tf.nn.softmax(XWB2)\r\n\r\n    if fl_quant:\r\n        tf.contrib.quantize.create_eval_graph(tf.get_default_graph())\r\n\r\n    init = tf.global_variables_initializer()\r\n\r\n    with tf.Session() as sess:\r\n        sess.run(init)\r\n        converter_dot = tf.lite.TFLiteConverter.from_session(sess, [X], [Result])\r\n        converter_tflite = tf.lite.TFLiteConverter.from_session(sess, [X], [Result])\r\n    tf.reset_default_graph()\r\n    if fl_quant:\r\n        converter_dot.inference_type = tf.lite.constants.QUANTIZED_UINT8\r\n        converter_dot.inference_input_type = tf.lite.constants.QUANTIZED_UINT8\r\n        converter_dot.quantized_input_stats = {'X': (127, 127)}\r\n        converter_dot.default_ranges_stats = (0, 255)\r\n\r\n        converter_tflite.inference_type = tf.lite.constants.QUANTIZED_UINT8\r\n        converter_tflite.inference_input_type = tf.lite.constants.QUANTIZED_UINT8\r\n        converter_tflite.quantized_input_stats = {'X': (127, 127)}\r\n        converter_tflite.default_ranges_stats = (0, 255)\r\n    converter_dot.output_format = tf.lite.constants.GRAPHVIZ_DOT\r\n    model_dot = converter_dot.convert()\r\n    model_tflite = converter_tflite.convert()\r\n    if fl_quant:\r\n        open(\"quant_model.dot\", \"wb\").write(model_dot)\r\n        open(\"quant_model.tflite\", \"wb\").write(model_tflite)\r\n    else:\r\n        open(\"model.dot\", \"wb\").write(model_dot)\r\n        open(\"model.tflite\", \"wb\").write(model_tflite)\r\n```\r\n\r\n**Other info / logs**\r\nBenchmark for float32:\r\n```\r\nMin num runs: [50]\r\nMin runs duration (seconds): [1]\r\nInter-run delay (seconds): [-1]\r\nNum threads: [1]\r\nBenchmark name: []\r\nOutput prefix: []\r\nMin warmup runs: [1]\r\nMin warmup runs duration (seconds): [0.5]\r\nGraph: [model.tflite]\r\nInput layers: []\r\nInput shapes: []\r\nUse nnapi : [0]\r\nLoaded model model.tflite\r\nresolved reporter\r\nInitialized session in 0.378ms\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds\r\ncount=302 first=5983 curr=1592 min=1586 max=5983 avg=1652.99 std=360\r\n\r\nRunning benchmark for at least 50 iterations and at least 1 seconds\r\ncount=626 first=1675 curr=1587 min=1586 max=1684 avg=1592.71 std=14\r\n\r\n============================== Run Order ==============================\r\n                     [node type]                  [start]         [first]        [avg ms]            [%]          [cdf%]          [mem KB]      [times called]  [Name]\r\n               DEPTHWISE_CONV_2D                    0.000           0.802           0.719        45.131%         45.131%             0.000              1       [Relu6]\r\n                 FULLY_CONNECTED                    0.719           0.848           0.849        53.325%         98.456%             0.000              1       [BiasAdd_1]\r\n                         SOFTMAX                    1.568           0.025           0.025         1.544%        100.000%             0.000              1       [Softmax]\r\n\r\n============================== Top by Computation Time ==============================\r\n                     [node type]                  [start]         [first]        [avg ms]            [%]          [cdf%]          [mem KB]      [times called]  [Name]\r\n                 FULLY_CONNECTED                    0.719           0.848           0.849        53.325%         53.325%             0.000              1       [BiasAdd_1]\r\n               DEPTHWISE_CONV_2D                    0.000           0.802           0.719        45.131%         98.456%             0.000              1       [Relu6]\r\n                         SOFTMAX                    1.568           0.025           0.025         1.544%        100.000%             0.000              1       [Softmax]\r\n\r\nNumber of nodes executed: 3\r\n============================== Summary by node type ==============================\r\n                     [Node type]          [count]         [avg ms]          [avg %]         [cdf %]       [mem KB]      [times called]\r\n                 FULLY_CONNECTED                1            0.849          53.363%         53.363%          0.000              1\r\n               DEPTHWISE_CONV_2D                1            0.718          45.129%         98.492%          0.000              1\r\n                         SOFTMAX                1            0.024           1.508%        100.000%          0.000              1\r\n\r\nTimings (microseconds): count=626 first=1675 curr=1587 min=1585 max=1684 avg=1592.5 std=14\r\nMemory (bytes): count=0\r\n3 nodes observed\r\n\r\n\r\nAverage inference timings in us: Warmup: 1652.99, Init: 378, no stats: 1592.71\r\n```\r\n\r\nBenchmark for uint8:\r\n```\r\nMin num runs: [50]\r\nMin runs duration (seconds): [1]\r\nInter-run delay (seconds): [-1]\r\nNum threads: [1]\r\nBenchmark name: []\r\nOutput prefix: []\r\nMin warmup runs: [1]\r\nMin warmup runs duration (seconds): [0.5]\r\nGraph: [quant_model.tflite]\r\nInput layers: []\r\nInput shapes: []\r\nUse nnapi : [0]\r\nLoaded model quant_model.tflite\r\nresolved reporter\r\nInitialized session in 0.311ms\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds\r\ncount=159 first=9037 curr=3022 min=3020 max=9037 avg=3152.67 std=598\r\n\r\nRunning benchmark for at least 50 iterations and at least 1 seconds\r\ncount=316 first=3222 curr=3194 min=3020 max=3254 avg=3162.37 std=67\r\n\r\n============================== Run Order ==============================\r\n                     [node type]                  [start]         [first]        [avg ms]            [%]          [cdf%]          [mem KB]      [times called]  [Name]\r\n               DEPTHWISE_CONV_2D                    0.000           2.884           2.829        89.470%         89.470%             0.000              1       [Relu6]\r\n                 FULLY_CONNECTED                    2.829           0.288           0.283         8.960%         98.430%             0.000              1       [BiasAdd_1]\r\n                         SOFTMAX                    3.113           0.050           0.050         1.570%        100.000%             0.000              1       [Softmax]\r\n\r\n============================== Top by Computation Time ==============================\r\n                     [node type]                  [start]         [first]        [avg ms]            [%]          [cdf%]          [mem KB]      [times called]  [Name]\r\n               DEPTHWISE_CONV_2D                    0.000           2.884           2.829        89.470%         89.470%             0.000              1       [Relu6]\r\n                 FULLY_CONNECTED                    2.829           0.288           0.283         8.960%         98.430%             0.000              1       [BiasAdd_1]\r\n                         SOFTMAX                    3.113           0.050           0.050         1.570%        100.000%             0.000              1       [Softmax]\r\n\r\nNumber of nodes executed: 3\r\n============================== Summary by node type ==============================\r\n                     [Node type]          [count]         [avg ms]          [avg %]         [cdf %]       [mem KB]      [times called]\r\n               DEPTHWISE_CONV_2D                1            2.829          89.497%         89.497%          0.000              1\r\n                 FULLY_CONNECTED                1            0.283           8.953%         98.450%          0.000              1\r\n                         SOFTMAX                1            0.049           1.550%        100.000%          0.000              1\r\n\r\nTimings (microseconds): count=316 first=3222 curr=3194 min=3019 max=3254 avg=3162.23 std=67\r\nMemory (bytes): count=0\r\n3 nodes observed\r\n\r\n\r\nAverage inference timings in us: Warmup: 3152.67, Init: 311, no stats: 3162.37\r\n```\r\n\r\n", "comments": ["If you're measuring latency on an Intel cpu, then I don't think uint8 ops would be optimized, which would explain why your floating model is running faster. Try measuring on an ARM core.", "@Vooblin This is not due to TF Lite but due to hardware CPU/GPU configuration as mentioned by @ruffles1. This is well know issue which is similar to [1](https://github.com/tensorflow/tensorflow/issues/1300), [2](https://github.com/tensorflow/tensorflow/issues/15585), and [3](https://github.com/tensorflow/tensorflow/issues/5592). All these links have many more groups mentioning similar issues. In future, please post these kind of support questions in Stackoverflow as the community will get benefited more. Thanks!", "@ruffles1 @jvishnuvardhan Thanks a lot for your responses! It was very helpful!"]}, {"number": 25020, "title": "installation error", "body": "~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59   from tensorflow.python.pywrap_tensorflow_internal import __version__\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in <module>\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow_internal = swig_import_helper()\r\n     29     del swig_import_helper\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     23             try:\r\n---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n     25             finally:\r\n\r\n~\\Anaconda3\\lib\\imp.py in load_module(name, file, filename, details)\r\n    242         else:\r\n--> 243             return load_dynamic(name, filename, file)\r\n    244     elif type_ == PKG_DIRECTORY:\r\n\r\n~\\Anaconda3\\lib\\imp.py in load_dynamic(name, path, file)\r\n    342             name=name, loader=loader, origin=path)\r\n--> 343         return _load(spec)\r\n    344 \r\n\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-7-822a3379d161> in <module>\r\n----> 1 import tensorflow as tf\r\n      2 import pandas as pd\r\n      3 import tflearn\r\n      4 from tflearn.layers.conv import conv_3d, max_pool_3d\r\n      5 from tflearn.layers.core import input_data, dropout, fully_connected\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py in <module>\r\n     26 \r\n     27 # pylint: disable=g-bad-import-order\r\n---> 28 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n     29 \r\n     30 from tensorflow._api.v1 import app\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>\r\n     47 import numpy as np\r\n     48 \r\n---> 49 from tensorflow.python import pywrap_tensorflow\r\n     50 \r\n     51 # Protocol buffers\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     72 for some common reasons and solutions.  Include the entire stack trace\r\n     73 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 74   raise ImportError(msg)\r\n     75 \r\n     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long", "comments": ["@Sakthi20111997 Thank you for your post. We noticed you have not filled out the fields in the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Along with the template, please provide as many details as possible to find the root cause of the issue. Thanks!", "@Sakthi20111997 Could you follow the instructions [here](https://github.com/rnreich/ubuntu-tensorflow-gpu-all-versions) and let us know how it works. I recently used those steps and successfully installed tensorflow-gpu. If you had already resolved the issue, please close the issue or let us know. Thanks!", "I think it was resolved. I am closing the issue. Please open new ticket if you see similar issue again. Thanks!"]}, {"number": 25019, "title": "Update label_image.cc", "body": "fix incorrect printing of output tensor type in tflite label_image example", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "Sorry to say but we cannot proceed until CLA reflects YES. Could you please open a new PR(which may resolve the CLA issue) and try. Closing this."]}, {"number": 25018, "title": "error while loading shared libraries: libprotobuf.so.17: cannot open shared object file: No such file or directory", "body": "techv@techversant-pc:~/Desktop/django-deepspeech-server/tensorflow$ tensorflow/contrib/makefile/gen/bin/benchmark  --graph=$HOME/graphs/inception/tensorflow_inception_graph.pb\r\ntensorflow/contrib/makefile/gen/bin/benchmark: error while loading shared libraries: libprotobuf.so.17: cannot open shared object file: No such file or directory\r\n", "comments": ["It looks like you haven't used a template to create this issue. Please resubmit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation.\r\n", "Hi, same error. Have filed an [issue](https://github.com/tensorflow/tensorflow/issues/25576)"]}, {"number": 25017, "title": "[XLA/AOT] Add check for --help command line option.", "body": "Here is a duplicate pull request, I was accidentally deleting the tf repo for this pull request https://github.com/tensorflow/tensorflow/pull/23877, and not able to update the patch, regarding to review.\r\ncc @hgadig @jlebar ", "comments": ["@jlebar  Thanks for approving. Could you please approve again as I do not see approval mark(green) yet.\r\n"]}, {"number": 25016, "title": "import tensorflow in python3 -> TypeError: __new__() got an unexpected keyword argument 'serialized_options'", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nI have the following issue when importing tensorflow in python3 since some days ago:\r\n```\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/__init__.py\", line 59, in <module>\r\n    from tensorflow.core.framework.graph_pb2 import *\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/core/framework/graph_pb2.py\", line 15, in <module>\r\n    from tensorflow.core.framework import node_def_pb2 as tensorflow_dot_core_dot_framework_dot_node__def__pb2\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/core/framework/node_def_pb2.py\", line 15, in <module>\r\n    from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/core/framework/attr_value_pb2.py\", line 15, in <module>\r\n    from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/core/framework/tensor_pb2.py\", line 15, in <module>\r\n    from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/core/framework/resource_handle_pb2.py\", line 22, in <module>\r\n    serialized_pb=_b('\\n/tensorflow/core/framework/resource_handle.proto\\x12\\ntensorflow\\\"r\\n\\x13ResourceHandleProto\\x12\\x0e\\n\\x06\\x64\\x65vice\\x18\\x01 \\x01(\\t\\x12\\x11\\n\\tcontainer\\x18\\x02 \\x01(\\t\\x12\\x0c\\n\\x04name\\x18\\x03 \\x01(\\t\\x12\\x11\\n\\thash_code\\x18\\x04 \\x01(\\x04\\x12\\x17\\n\\x0fmaybe_type_name\\x18\\x05 \\x01(\\tBn\\n\\x18org.tensorflow.frameworkB\\x0eResourceHandleP\\x01Z=github.com/tensorflow/tensorflow/tensorflow/go/core/framework\\xf8\\x01\\x01\\x62\\x06proto3')\r\nTypeError: __new__() got an unexpected keyword argument 'serialized_options'\r\n\r\n```\r\n\r\n**Describe the expected behavior**\r\n```\r\n>>> import tensorflow as tf\r\n>>> \r\n```\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO\r\n- OS Platform and Distribution: Ubuntu 18.04 (4.15.0-43-generic x86_64 GNU/Linux)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NO \r\n- TensorFlow installed from (source or binary): NO (using pip3)\r\n- TensorFlow version (use command below): 1.12.0 (command below is not working as tf doesn't load)\r\n- Python version: Python 3.6.7\r\n- Bazel version (if compiling from source): NO\r\n- GCC/Compiler version (if compiling from source): NO\r\n- CUDA/cuDNN version: NO CUDA\r\n- GPU model and memory: Intel HD Graphics 5500:\r\n```\r\n$ sudo lshw -C display\r\n  *-display                 \r\n       description: VGA compatible controller\r\n       product: HD Graphics 5500\r\n       vendor: Intel Corporation\r\n       physical id: 2\r\n       bus info: pci@0000:00:02.0\r\n       version: 09\r\n       width: 64 bits\r\n       clock: 33MHz\r\n       capabilities: msi pm vga_controller bus_master cap_list rom\r\n       configuration: driver=i915 latency=0\r\n       resources: irq:44 memory:f0000000-f0ffffff memory:e0000000-efffffff ioport:3000(size=64) memory:c0000-dffff\r\n\r\n```\r\n\r\n\r\n<em>You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\":</em>\r\n\r\nThis doesn't work as tf doesn't load.\r\n\r\n\r\n**Code to reproduce the issue**\r\n`>>> import tensorflow as tf`\r\n\r\n**Other info / logs**\r\n<em>Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.</em>\r\n\r\n```\r\n$ pip3 list | grep tensor\r\ntensorboard                   1.12.0     \r\ntensorflow                    1.12.0 \r\n```\r\nEverything is up-to-date for tensorflow:\r\n```\r\n$ sudo -H pip3 install --upgrade tensorflow\r\nRequirement already up-to-date: tensorflow in /usr/local/lib/python3.6/dist-packages (1.12.0)\r\nRequirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.16.1)\r\nRequirement already satisfied, skipping upgrade: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.6.1)\r\nRequirement already satisfied, skipping upgrade: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\r\nRequirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\r\nRequirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.6)\r\nRequirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.6.1)\r\nRequirement already satisfied, skipping upgrade: tensorboard<1.13.0,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\r\nRequirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.7.1)\r\nRequirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.4)\r\nRequirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\r\nRequirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.5)\r\nRequirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorflow) (0.30.0)\r\nRequirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow) (2.8.0)\r\nRequirement already satisfied, skipping upgrade: setuptools in /usr/lib/python3/dist-packages (from protobuf>=3.6.1->tensorflow) (39.0.1)\r\nRequirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow) (3.0.1)\r\nRequirement already satisfied, skipping upgrade: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow) (0.14.1)\r\n\r\n```", "comments": ["@swiss-knight You mentioned tensorflow was working earlier. Did you update anything recently? Please give us more details to find the root cause of the issue? Thanks!", "Nothing more than the OS regular updates.\r\n\r\nBy diving more into it, i think some protobuf stuffs were required as a dependency from one of these and that they have \"kind of\" taken the priority over the protobuf I have manually installed using pip3.\r\n\r\nWhat's wrong seems that:\r\n1. \r\n```\r\n$ protoc --version\r\nlibprotoc 3.0.0\r\n```\r\n2. In python3 now:\r\n```\r\n>>> from google import protobuf\r\n>>> protobuf.__version__\r\n'3.0.0'\r\n>>> protobuf.__file__\r\n'/usr/lib/python3/dist-packages/google/protobuf/__init__.py'\r\n```\r\n\r\n3. But I guess in python3 it was 3.6.x before because of:\r\n```\r\n$ pip3 list | grep proto\r\nprotobuf                      3.6.1 \r\n```\r\n\r\nand:\r\n\r\n```\r\n$ apt-cache policy libprotobuf10 \r\nlibprotobuf10:\r\n  Installed: 3.0.0-9.1ubuntu1\r\n  Candidate: 3.0.0-9.1ubuntu1\r\n  Version table:\r\n *** 3.0.0-9.1ubuntu1 500\r\n        500 http://ch.archive.ubuntu.com/ubuntu bionic/main amd64 Packages\r\n        100 /var/lib/dpkg/status\r\n```\r\n\r\nAnyway, protobuf 3.6.1 (not 3.0.0!) seems installed properly in \"/usr/local/lib/\": \r\n```\r\n$ find /usr/local/lib/python3.6/dist-packages -iname \"*protobuf*\"\r\n/usr/local/lib/python3.6/dist-packages/torch/share/cmake/Caffe2/public/protobuf.cmake\r\n/usr/local/lib/python3.6/dist-packages/protobuf-3.6.1.dist-info\r\n/usr/local/lib/python3.6/dist-packages/protobuf-3.6.1-py3.6-nspkg.pth\r\n/usr/local/lib/python3.6/dist-packages/google/protobuf\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/protobuf\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/include/google/protobuf\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/include/external/protobuf_archive\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/include/external/protobuf_archive/python/google/protobuf\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/include/external/protobuf_archive/src/google/protobuf\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/platform/protobuf_internal.h\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/platform/protobuf.h\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/platform/protobuf_compiler.h\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/platform/default/protobuf.h\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/platform/default/protobuf_compiler.h\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/protobuf\r\n```\r\nThis \"/usr/local/lib/\" package should override the \"/usr/lib/\" one (if it exists) when loading protobuf in python3, but there is nothing in the system wide \"/usr/lib\" place:\r\n```\r\n$ find /usr/lib/python3.6/ -iname \"*proto*\"\r\n$\r\n```\r\n\r\nSo I guess it's coming from elsewhere, probably linked with the 'libprotobuf10' Ubuntu package that was installed from the official repositories. \r\n\r\nHope it helps, but it's only a guess from my side, I may be wrong somewhere in my understanding of the situation.", "@swiss-knight Could you follow the instructions [here](https://github.com/rnreich/ubuntu-tensorflow-gpu-all-versions) and let us know how it works. I recently used those steps and successfully installed tensorflow-gpu. If you had already resolved the issue, please close the issue or let us know. Thanks!", "I tried first to force-reinstall it from the PyPi repository with pip3 and it seems OK now ( but protobuf itself seems always stuck to version 3.0.0 from \"/usr/lib\" ).\r\n\r\nHere are the reinstalled packages that were updated during the `pip3 --upgrade --force-reinstall tensorflow` process:\r\n\r\n```\r\n$ sudo -H pip3 install --upgrade --force-reinstall tensorflow\r\nCollecting tensorflow\r\n  Using cached https://files.pythonhosted.org/packages/22/cc/ca70b78087015d21c5f3f93694107f34ebccb3be9624385a911d4b52ecef/tensorflow-1.12.0-cp36-cp36m-manylinux1_x86_64.whl\r\nCollecting protobuf>=3.6.1 (from tensorflow)\r\n  Using cached https://files.pythonhosted.org/packages/c2/f9/28787754923612ca9bfdffc588daa05580ed70698add063a5629d1a4209d/protobuf-3.6.1-cp36-cp36m-manylinux1_x86_64.whl\r\nCollecting six>=1.10.0 (from tensorflow)\r\n  Using cached https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\r\nCollecting numpy>=1.13.3 (from tensorflow)\r\n  Using cached https://files.pythonhosted.org/packages/7b/74/54c5f9bb9bd4dae27a61ec1b39076a39d359b3fb7ba15da79ef23858a9d8/numpy-1.16.0-cp36-cp36m-manylinux1_x86_64.whl\r\nCollecting astor>=0.6.0 (from tensorflow)\r\n  Using cached https://files.pythonhosted.org/packages/35/6b/11530768cac581a12952a2aad00e1526b89d242d0b9f59534ef6e6a1752f/astor-0.7.1-py2.py3-none-any.whl\r\nCollecting tensorboard<1.13.0,>=1.12.0 (from tensorflow)\r\n  Using cached https://files.pythonhosted.org/packages/07/53/8d32ce9471c18f8d99028b7cef2e5b39ea8765bd7ef250ca05b490880971/tensorboard-1.12.2-py3-none-any.whl\r\nCollecting absl-py>=0.1.6 (from tensorflow)\r\nCollecting termcolor>=1.1.0 (from tensorflow)\r\nCollecting wheel>=0.26 (from tensorflow)\r\n  Using cached https://files.pythonhosted.org/packages/ff/47/1dfa4795e24fd6f93d5d58602dd716c3f101cfd5a77cd9acbe519b44a0a9/wheel-0.32.3-py2.py3-none-any.whl\r\nCollecting gast>=0.2.0 (from tensorflow)\r\nCollecting grpcio>=1.8.6 (from tensorflow)\r\n  Using cached https://files.pythonhosted.org/packages/51/a3/489ce6a67047263e9b0da8784b2925c4f89b688a6e33073c5bb6c4c2867f/grpcio-1.18.0-cp36-cp36m-manylinux1_x86_64.whl\r\nCollecting keras-applications>=1.0.6 (from tensorflow)\r\n  Using cached https://files.pythonhosted.org/packages/3f/c4/2ff40221029f7098d58f8d7fb99b97e8100f3293f9856f0fb5834bef100b/Keras_Applications-1.0.6-py2.py3-none-any.whl\r\nCollecting keras-preprocessing>=1.0.5 (from tensorflow)\r\n  Using cached https://files.pythonhosted.org/packages/fc/94/74e0fa783d3fc07e41715973435dd051ca89c550881b3454233c39c73e69/Keras_Preprocessing-1.0.5-py2.py3-none-any.whl\r\nCollecting setuptools (from protobuf>=3.6.1->tensorflow)\r\n  Using cached https://files.pythonhosted.org/packages/bf/ae/a23db1762646069742cc21393833577d3fa438eecaa59d11fb04fa57fcd5/setuptools-40.7.1-py2.py3-none-any.whl\r\nCollecting werkzeug>=0.11.10 (from tensorboard<1.13.0,>=1.12.0->tensorflow)\r\n  Using cached https://files.pythonhosted.org/packages/20/c4/12e3e56473e52375aa29c4764e70d1b8f3efa6682bef8d0aae04fe335243/Werkzeug-0.14.1-py2.py3-none-any.whl\r\nCollecting markdown>=2.6.8 (from tensorboard<1.13.0,>=1.12.0->tensorflow)\r\n  Using cached https://files.pythonhosted.org/packages/7a/6b/5600647404ba15545ec37d2f7f58844d690baf2f81f3a60b862e48f29287/Markdown-3.0.1-py2.py3-none-any.whl\r\nCollecting h5py (from keras-applications>=1.0.6->tensorflow)\r\n  Using cached https://files.pythonhosted.org/packages/30/99/d7d4fbf2d02bb30fb76179911a250074b55b852d34e98dd452a9f394ac06/h5py-2.9.0-cp36-cp36m-manylinux1_x86_64.whl\r\nInstalling collected packages: setuptools, six, protobuf, numpy, astor, wheel, werkzeug, grpcio, markdown, tensorboard, absl-py, termcolor, gast, h5py, keras-applications, keras-preprocessing, tensorflow\r\n  Found existing installation: setuptools 40.7.1\r\n    Uninstalling setuptools-40.7.1:\r\n      Successfully uninstalled setuptools-40.7.1\r\n  Found existing installation: six 1.12.0\r\n    Uninstalling six-1.12.0:\r\n      Successfully uninstalled six-1.12.0\r\n  Found existing installation: protobuf 3.6.1\r\n    Uninstalling protobuf-3.6.1:\r\n      Successfully uninstalled protobuf-3.6.1\r\n  Found existing installation: numpy 1.16.0\r\n    Uninstalling numpy-1.16.0:\r\n      Successfully uninstalled numpy-1.16.0\r\n  Found existing installation: astor 0.7.1\r\n    Uninstalling astor-0.7.1:\r\n      Successfully uninstalled astor-0.7.1\r\n  Found existing installation: wheel 0.32.3\r\n    Uninstalling wheel-0.32.3:\r\n      Successfully uninstalled wheel-0.32.3\r\n  Found existing installation: Werkzeug 0.14.1\r\n    Uninstalling Werkzeug-0.14.1:\r\n      Successfully uninstalled Werkzeug-0.14.1\r\n  Found existing installation: grpcio 1.18.0\r\n    Uninstalling grpcio-1.18.0:\r\n      Successfully uninstalled grpcio-1.18.0\r\n  Found existing installation: Markdown 3.0.1\r\n    Uninstalling Markdown-3.0.1:\r\n      Successfully uninstalled Markdown-3.0.1\r\n  Found existing installation: tensorboard 1.12.2\r\n    Uninstalling tensorboard-1.12.2:\r\n      Successfully uninstalled tensorboard-1.12.2\r\n  Found existing installation: absl-py 0.7.0\r\n    Uninstalling absl-py-0.7.0:\r\n      Successfully uninstalled absl-py-0.7.0\r\n  Found existing installation: termcolor 1.1.0\r\n    Uninstalling termcolor-1.1.0:\r\n      Successfully uninstalled termcolor-1.1.0\r\n  Found existing installation: gast 0.2.2\r\n    Uninstalling gast-0.2.2:\r\n      Successfully uninstalled gast-0.2.2\r\n  Found existing installation: h5py 2.9.0\r\n    Uninstalling h5py-2.9.0:\r\n      Successfully uninstalled h5py-2.9.0\r\n  Found existing installation: Keras-Applications 1.0.6\r\n    Uninstalling Keras-Applications-1.0.6:\r\n      Successfully uninstalled Keras-Applications-1.0.6\r\n  Found existing installation: Keras-Preprocessing 1.0.5\r\n    Uninstalling Keras-Preprocessing-1.0.5:\r\n      Successfully uninstalled Keras-Preprocessing-1.0.5\r\n  Found existing installation: tensorflow 1.12.0\r\n    Uninstalling tensorflow-1.12.0:\r\n      Successfully uninstalled tensorflow-1.12.0\r\nSuccessfully installed absl-py-0.7.0 astor-0.7.1 gast-0.2.2 grpcio-1.18.0 h5py-2.9.0 keras-applications-1.0.6 keras-preprocessing-1.0.5 markdown-3.0.1 numpy-1.16.0 protobuf-3.6.1 setuptools-40.7.1 six-1.12.0 tensorboard-1.12.2 tensorflow-1.12.0 termcolor-1.1.0 werkzeug-0.14.1 wheel-0.32.3\r\n```\r\n\r\n"]}, {"number": 25015, "title": "TF utils_test additional test cases covers", "body": "1- constant_value api test case\r\n2- get_reachable_from_inputs api test case", "comments": ["@fchollet \r\n\r\nPlease help to review the test case", "@Dayananda-V could you please fix build errors", "@rthadur \r\n\r\nthose build eeror is not induced from my code and those test are not required on my changes. Please check and update me if require any support.", "Compilation error fix changes is commit using new PR #27735, closing the old one."]}, {"number": 25014, "title": "Fix several more r1.13 related test failures.", "body": "", "comments": []}, {"number": 25013, "title": "Remove enum34 from bazel BUILD file for pip package", "body": "Attempt to fix the error:\r\n```\r\ntensorflow/tools/pip_package/BUILD:240:1: //tensorflow/tools/pip_package:build_pip_package: missing input file '@enum34_archive//:LICENSE'\r\n```\r\nAs it's happening here:\r\nhttps://source.cloud.google.com/results/invocations/1b52e710-ee08-4aeb-986f-5dffaeb2fc26/log", "comments": ["Note: This change has already been merged, when they rolled back the entire commit that caused the build break in: https://github.com/tensorflow/tensorflow/commit/28401a50f1839f44c76f7b202c8766ecb61fc569"]}, {"number": 25012, "title": "TF Build Fails: missing input file '@enum34_archive//:LICENSE'", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Cent OS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: master branch\r\n- Python version: 2.7\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): 0.19.2\r\n- GCC/Compiler version (if compiling from source): GCC 6.3\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n\r\n**Describe the problem**\r\nTensorflow build fails with this error:\r\nERROR: missing input file '@enum34_archive//:LICENSE'\r\nERROR: /ec/fm/disks/nrvlab_300G_work01/mabuzain/pr/private-tensorflow/tensorflow/tools/pip_package/BUILD:240:1: //tensorflow/tools/pip_package:build_pip_package: missing input file '@enum34_archive//:LICENSE'\r\n", "comments": [" @gunan Can you please take a look at this? Thanks!", "This is fixed at head.\r\nWe rolled back the breaking change, and @revan has fixed the initial issue later."]}, {"number": 25011, "title": "[GPU][ROCm][CUDA] StreamExecutor logic for ROCm / CUDA platform (PR 20709 / 22669 / 24156 continued)", "body": "New PR to continue the efforts started by @deven-amd in #20709 / #22669 / #24156.\r\n\r\nThis PR aims to refactor StreamExecutor GPU interfaces so it can be shared among CUDA and ROCm. The PR would be the first part of a series of PRs.\r\n\r\nBased on @timshen91 's inputs, I've refactored logic in #214156 so :\r\n\r\n- only contains changes in stream_executor/....\r\n- does not remove any stream_executor/cuda/*.h, so that things outside of stream_executor don't break. All the types and functions in the namespace cuda now alias to namespace gpu counterparts. For example, namespace cuda { using CUDADriver = gpu::GpuDriver; }.\r\n- all stream_executor/gpu/BUILD targets should be only visible to //third_party/tensorflow/stream_executor:__subpackages__.\r\n- target stream_executor/gpu:X should be only used by stream_executor/cuda:cuda_X or stream_executor/rocm:rocm_X, not cuda_Y. For example, cuda:cuda_platform should depend on cuda:cuda_driver, not gpu:gpu_driver.\r\n", "comments": ["@timshen91 I've finished addressing your comments in #24156 and revise the PR here. The most noticeable change is that no operators/XLA logic are changed, plus all CUDA SE headers are retained. All known good tests are now passing. Logs in those failing test targets suggest those failures are irrelevant to this PR. Would you mind take another look at this PR? Thanks.", "@timshen91 a gentle ping", "@timshen91 Could you help advise how to move this PR forward? Just a heads up I'm working on getting ROCm components dynamically loaded like their CUDA counterparts and will likely come up with another PR later this week or early next week. But without getting this PR landed it's hard for us to submit new ones for StreamExecutor on ROCm.", "Sorry for the delay. I'm looking at this PR today.", "The patch looks roughly as I intended with one or two modifications. I'll try to integrate them.", "@whchung, I'm trying to integrate. In the mean time, can you fix the commit by adding back cuda_helpers.h and the build target \"cuda_helpers\"?", "ok let me restore the target", "About:\r\n  target stream_executor/gpu:X should be only used by stream_executor/cuda:cuda_X or stream_executor/rocm:rocm_X, not cuda_Y. For example, cuda:cuda_platform should depend on cuda:cuda_driver, not gpu:gpu_driver.\r\n\r\nCan you also grep cuda/ for `#include.*gpu/gpu_`, and replace the ones with `cuda/cuda_*`? I'm doing the same thing locally so that I don't have to wait on your changes.", "One more thing: can you apply the same kind of changes I did in commit bc4a279?", "@timshen91 I reviewed my latest commit and found some anomalies. For some reason `cuda_helpers.h` wasn't correctly included in the earlier commit and has now been fixed. build targets in CUDA StreamExecutor path have also been guarded with `if_cuda_is_configured`.", "@timshen91 a gentle ping again. I've resolved the conflicts with master.", "@timshen91 ping?", "+ @deven-amd @dagamayank", "@timshen91 It seems the PR was merged by you after some changes on your side. I'll close the PR. Thanks!", "Hi @whchung , I integrated an earlier version of this PR plus some modifications. Sorry I didn't sync to the latest version of this PR, since I had to do modifications and the rebasing on this PR will make this even longer.\r\n\r\nIf it doesn't get reverted in a day or two, we can work on the rest.\r\n\r\nI want to highlight one design decision:\r\n* High-level runtime libraries (dnn, fft, blas, rng) should be public. Other than that,\r\n* keep `gpu/*` private `cuda/*` / `rocm/*` private to `//tensorflow/stream_executor/...`, as they are just implementation details. Most of the primitives are exposed through high level portable API like \"Stream / StreamExecutor\" anyway.\r\n\r\nI hope this sounds reasonable. Please feel free to post your opinions.", "@timshen91 Thanks. Let me digest the commit you just made first. Also I'd like to understand a bit on the recent changes in StreamExecutor and the general direction we are moving to:\r\n\r\n- StreamExecutor now loads CUDA libraries dynamically, instead of dynamically linked with CUDA libraries: I think this part is rather clear to me and I've been working on getting a PR for that now.\r\n- On the other hand, other parts of TensorFlow (`grappler`, `core/common_runtime`, implementations of certain TensorFlow operators) still have static dependency to CUDA/HIP runtime APIs. Would they also migrate toward dynamically loaded scheme?\r\n", "@timshen91 also I'd like to understand better about what do you mean by:\r\n\r\n- keep `gpu/*` private `cuda/*` / `rocm/*` private to `//tensorflow/stream_executor/...`\r\n\r\nIn TensorFlow we can see logic using `stream_executor::cuda` right now, they are not completely abstracted by interfaces within `stream_executor_pimpl.h`. Moving forward, do you anticipate everything be hidden by StreamExecutor interfaces defined in `stream_executor_pimpl.h`? Or would we expose some details via `stream_executor::gpu` namespace?\r\n\r\n", "> @timshen91 Thanks. Let me digest the commit you just made first. Also I'd like to understand a bit on the recent changes in StreamExecutor and the general direction we are moving to:\r\n> \r\n> * StreamExecutor now loads CUDA libraries dynamically, instead of dynamically linked with CUDA libraries: I think this part is rather clear to me and I've been working on getting a PR for that now.\r\n\r\nYes. The goal is to be able to build \"core tensorflow\" + \"CPU and GPU stream_executor\" without depending on cuda libraries like cudnn at build time. The built binary should also work on a non-GPU machine.\r\n\r\n> * On the other hand, other parts of TensorFlow (`grappler`, `core/common_runtime`, implementations of certain TensorFlow operators) still have static dependency to CUDA/HIP runtime APIs. Would they also migrate toward dynamically loaded scheme?\r\n\r\nMy understanding is that either the build-time dependencies will be removed, or excluded from \"core tensorflow\". For example, TF ops won't be part of of \"core tensorflow\", so it's fine to have build-time CUDA dependencies in them.", "> @timshen91 also I'd like to understand better about what do you mean by:\r\n> \r\n> * keep `gpu/*` private `cuda/*` / `rocm/*` private to `//tensorflow/stream_executor/...`\r\n\r\nMy bad, I meant \"keep ... private as much as possible\", and of course excluding the dnn/blas libraries.\r\n\r\n> \r\n> In TensorFlow we can see logic using `stream_executor::cuda` right now, they are not completely abstracted by interfaces within `stream_executor_pimpl.h`. Moving forward, do you anticipate everything be hidden by StreamExecutor interfaces defined in `stream_executor_pimpl.h`? Or would we expose some details via `stream_executor::gpu` namespace?\r\n\r\nLet me take a step back. It's actually an open topic and I'd like your opinions. The constraints we have are:\r\n* For portable code, we already have a portable layer `stream_executor_pimpl.h`, so we don't want to have a second portable layer. That will be a source of confusion and inconsistency.\r\n* For non-portable code, by definition it is insufficient to solely use `gpu/...`. In that case, the user should seek one of the backends `cuda/...` or `rocm/...`.\r\n* In general we try to minimize the non-portable code.\r\n\r\nThus I tentativelly draw the conclusion that targets in `gpu/...` shouldn't be public outside of `stream_executor`. This has nothing to do with `namespace gpu`. The uses of `namespace gpu` probably should go as originally planned."]}, {"number": 25010, "title": "Location of Policy Gradient Algorithm in python/grappler for device placement", "body": "I have been going over this paper: [Device Placement Optimization with Reinforcement Learning](https://arxiv.org/abs/1706.04972) and I have been reviewing the corresponding code within [python/grappler](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/grappler). It seems that the starting point for this code is [graph_placer.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/grappler/graph_placer.py), which sets up the RL model for device placement.\r\n\r\nI want to see what happens when I adjust the reinforcement model optimization policies. According to the paper, it is currently using a generic policy gradient algorithm. I'm having a hard time location the mechanism of this algorithm in grappler. Can someone point me towards the location (line number and file) of this code and help explain to me how it works?", "comments": ["@ashewx Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from questions. Github is mainly for addressing bugs in installation and performance. Thanks!", "I am closing the issue but please post this kind of support questions at Stackoverflow. Thanks!"]}, {"number": 25009, "title": "[ROCm][CUDA] StreamExecutor logic for ROCm / CUDA platform (PR 20709 / 22669 / 24156 continued)", "body": "New PR to continue the efforts started by @deven-amd in #20709 / #22669 / #24156.\r\n\r\nThis PR aims to refactor StreamExecutor GPU interfaces so it can be shared among CUDA and ROCm.\r\n\r\nThe PR is still work in progress, based on @timshen91 's inputs it need to be refactored so:\r\n\r\nIt will be much easier for you to split this commit to multiple PR. The first PR:\r\n\r\n- (done) only contains changes in `stream_executor/....`\r\n- does not remove any `stream_executor/cuda/*.h`, so that things outside of stream_executor don't break. All the types and functions in the namespace cuda now alias to namespace gpu counterparts. For example, `namespace cuda { using CUDADriver = gpu::GpuDriver; }`.\r\n- all `stream_executor/gpu/BUILD` targets should be only visible to `//third_party/tensorflow/stream_executor:__subpackages__`.\r\n- target `stream_executor/gpu:X` should be only used by `stream_executor/cuda:cuda_X` or `stream_executor/rocm:rocm_X`, not `cuda_Y`. For example, `cuda:cuda_platform` should depend on `cuda:cuda_driver`, not `gpu:gpu_driver`.\r\nWe can worry about the second PR once the first is checked in.\r\n\r\nThe general idea is to make redundancy during transition (e.g. keep namespace cuda, but alias to gpu), and change as little as possible. I tried to ingrate the whole thing in order to \"get this over with\" but that simply didn't work. It's much better to integrate them incrementally.", "comments": ["submitted with the wrong branch. will start a new one. sorry for the confusions."]}, {"number": 25008, "title": "add tensor[tensor > a] syntax for issue #24133", "body": "Add tensor[tensor > a] syntax for issue #24133.\r\nHi @alextp, could you take a look my changes?\r\n ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "@musikisomorphie you signed it but the commit shows as being authored by someone else; is that the case?", "@alextp, I guess it is because I used to keep my email private and the system uses 12932355+musikisomorphie@users.noreply.github.com instead. Now I changed it. \r\nDoes it help to resolve the problem or should I reopen a PR?", "Can you say \"I signed it\" again?", "I signed it!", "Also maybe if you rebase after responding to my comments the googlebot will be happier?", "I signed it", "I think you still need to sign the CLA with all emails you used to commit, or rebase this PR so it only has commits from the email github recognizes.", "Hi @alextp, I uploaded my changes on array_ops_test.py. Somehow it triggered cla error again.\r\nI feel so weird coz my email address was already set on my git commits by the following command\r\ngit config --global user.email \"my email address\".\r\n\r\nAlso, I run those unit tests on my local computer and they seem to be fine.\r\n\r\n\r\n", "I signed it", "@gunan do you know why the cla bot is refusing to recognize this?", "I think a squashing rebase should fix this", "> I think a squashing rebase should fix this\r\n\r\nI got some issues when using rebase command. And I undo the rebase mistake, now the codes suppose to look like what I want them to. Maybe I will stop trying to rebase for now.  ", "> > I think a squashing rebase should fix this\r\n> \r\n> I got some issues when using rebase command. And I undo the rebase mistake, now the codes suppose to look like what I want them to. Maybe I will stop trying to rebase for now.  I am sorry for the cla problem again.\r\n\r\n", "I think some commits do not have their emails set correctly.\r\nIf you look at the commits, github does not seem to recognize the authors of all users, and that is why I think the CLA bot is failing.", "@musikisomorphie you have 6 commits, the last 3 commits are configured correctly with your email but the other 3 commits does not have email configured correctly, instead they are `<jwu@jwus-MacBook-Pro.local>`. GitHub could not link the incorrect email address You have to fix the commits email. One easy way is to invoke the following command:\r\n```\r\ngit rebase -i HEAD~6 -x \"git commit --amend --author 'Yong Name <Your True Email>' --no-edit\"\r\n```\r\n\r\nReplace `Your Name` and `<Your True Email>` accordingly in the command.\r\n\r\nJust save when rebase prompt, and that should fix the issue.", "Hi @yongtang , thank you for your tips. I tried it and checked by git log:\r\ncommit 4ae951e935e9d53e2b6d71d88014bc5bd9a24d4b (HEAD -> boolean_mask_numpy)\r\nAuthor: musikisomorphie <wujiqing9@gmail.com>\r\nDate:   Fri Jan 18 20:49:58 2019 +0100\r\n\r\n    add unit tests for two types: numpy array and tensor for tensor[tensor > a] syntax\r\n\r\ncommit 378e4af71b11806433856f5b52a5447d92c55346\r\nAuthor: musikisomorphie <wujiqing9@gmail.com>\r\nDate:   Thu Jan 17 21:56:51 2019 +0100\r\n\r\n    remove some comments\r\n\r\ncommit 986329bebc9ee6d0a509a900b45028394c088770\r\nAuthor: musikisomorphie <wujiqing9@gmail.com>\r\nDate:   Thu Jan 17 21:44:30 2019 +0100\r\n\r\n    remove some comments\r\n\r\ncommit da8b5ce8fd22887d29d953271cde6d9da800a3db\r\nAuthor: musikisomorphie <wujiqing9@gmail.com>\r\nDate:   Thu Jan 17 20:17:42 2019 +0100\r\n\r\n    achieve tensor[... > a] syntax\r\n\r\ncommit 3a1765b8690c46ad45f5ce8586175d3a3b74254c\r\nAuthor: musikisomorphie <wujiqing9@gmail.com>\r\nDate:   Wed Jan 16 07:51:23 2019 -0800\r\n\r\n    Use Ophint to support unidirectional sequence rnn (static)\r\n    \r\n    PiperOrigin-RevId: 229556771\r\n\r\ncommit 3d8b29dd010d7c6342ae986c4cec87694060bdf2\r\nAuthor: musikisomorphie <wujiqing9@gmail.com>\r\nDate:   Wed Jan 16 07:42:12 2019 -0800\r\n\r\nIs it correct?\r\n", "Your local git repo is out of sync with your git repo on GitHub, as:\r\n```\r\nUse Ophint to support unidirectional sequence rnn (static)\r\n\r\nPiperOrigin-RevId: 229556771\r\n```\r\nis a commit from others.\r\n\r\nYou may have to do the following (from a new directory):\r\n```\r\n$ git clone https://github.com/musikisomorphie/tensorflow.git\r\n$ cd tensorflow\r\n$ git checkout boolean_mask_numpy\r\n$ git rebase -i HEAD~6 -x \"git commit --amend --author 'Yong Name <Your True Email>' --no-edit\"\r\n$ git push -f origin boolean_mask_numpy\r\n```", "CLAs look good, thanks!\n\n<!-- ok -->", "@yongtang, thank you. I saw some changes occurred in GitHub. It did work. ", "Thanks @musikisomorphie. One additional thing you may want to do is to strip out the last two commits. While it is not bad to keep the commits for conversation history, the last two commits from your PR cancels out each other so it really does not add anything.\r\n\r\nYou could do (from your local branch of `boolean_mask_numpy`):\r\n```\r\n$ git reset HEAD~2\r\n$ git push -f origin boolean_mask_numpy\r\n```\r\n\r\nAfter that, your PR will be much cleaner.", "> Thanks @musikisomorphie. One additional thing you may want to do is to strip out the last two commits. While it is not bad to keep the commits for conversation history, the last two commits from your PR cancels out each other so it really does not add anything.\r\n> \r\n> You could do (from your local branch of `boolean_mask_numpy`):\r\n> \r\n> ```\r\n> $ git reset HEAD~2\r\n> $ git push -f origin boolean_mask_numpy\r\n> ```\r\n> After that, your PR will be much cleaner.\r\n\r\nThank you again, @yongtang. I changed it based on your suggestions. \r\n", "Hi @alextp, is my PR check going well? I saw two tests Ubuntu CC and Ubuntu Sanity remaining to be checked, just let me know if there is anything I should do.", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "I think you need to rebase this PR over tf's master", "CLAs look good, thanks!\n\n<!-- ok -->", "> I think you need to rebase this PR over tf's master\r\n\r\nI rebased my PR, please let me know if there is anything I should fix.", "Hi guys @alextp, @shoyer, after giving some thoughts about all our previous discussion, I decided to go back to my previous code, which involves two extra if conditions determining whether the input in the [....] is a scalar. The reason is, though boolean_mask take a scalar numpy array/ tensor as a ValueError, such input can be meaningful to the original index slicing operation.\r\n\r\nTo examine this, I also add the corresponding test in array_ops_test.py, more specifically,\r\n\r\n ```\r\n     scalar = np.array(0)\r\n      # Test tensor type mask\r\n      ...\r\n      _ = checker[ops.convert_to_tensor(scalar)]\r\n\r\n       # Test numpy array type mask\r\n      ...\r\n      _ = checker1[scalar]\r\n```\r\nI run those tests myself, they all passed. So I would suggest that we can stick to my current version of code. ", "So, what needs to work is:\n\ntensor[boolean_tensor], tensor[other_tensor], tensor[boolean_array],\ntensor[array], and the preexisting tests.\n\nCan we have simple unit tests which just assert that these expressions\ncompute the right thing?\n\nOn Wed, Jan 30, 2019 at 11:59 AM musikisomorphie <notifications@github.com>\nwrote:\n\n> Hi guys @alextp <https://github.com/alextp>, @shoyer\n> <https://github.com/shoyer>, after giving some thoughts about all our\n> previous discussion, I decided to go back to my previous code, which\n> involves two extra if conditions determining whether the input in the\n> [....] is a scalar. The reason is, though boolean_mask take a scalar numpy\n> array/ tensor as a ValueError, such input can be meaningful to the original\n> index slicing operation.\n>\n> To examine this, I also add the corresponding test in array_ops_test.py,\n> more specifically,\n>\n>     scalar = np.array(0)\n>      # Test tensor type mask\n>      ...\n>      _ = checker[ops.convert_to_tensor(scalar)]\n>\n>       # Test numpy array type mask\n>      ...\n>      _ = checker1[scalar]\n>\n> I run those tests myself, they all passed. So I would suggest that we can\n> stick to my current version of code.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/25008#issuecomment-459085973>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxdbwj70u3RzZaVXQIqYuyvMXjGGfks5vIfm5gaJpZM4aGlYz>\n> .\n>\n\n\n-- \n - Alex\n", "> So, what needs to work is: tensor[boolean_tensor], tensor[other_tensor], tensor[boolean_array], tensor[array], and the preexisting tests. Can we have simple unit tests which just assert that these expressions compute the right thing?\r\n> [\u2026](#)\r\n> On Wed, Jan 30, 2019 at 11:59 AM musikisomorphie ***@***.***> wrote: Hi guys @alextp <https://github.com/alextp>, @shoyer <https://github.com/shoyer>, after giving some thoughts about all our previous discussion, I decided to go back to my previous code, which involves two extra if conditions determining whether the input in the [....] is a scalar. The reason is, though boolean_mask take a scalar numpy array/ tensor as a ValueError, such input can be meaningful to the original index slicing operation. To examine this, I also add the corresponding test in array_ops_test.py, more specifically, scalar = np.array(0) # Test tensor type mask ... _ = checker[ops.convert_to_tensor(scalar)] # Test numpy array type mask ... _ = checker1[scalar] I run those tests myself, they all passed. So I would suggest that we can stick to my current version of code. \u2014 You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <[#25008 (comment)](https://github.com/tensorflow/tensorflow/pull/25008#issuecomment-459085973)>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AAATxdbwj70u3RzZaVXQIqYuyvMXjGGfks5vIfm5gaJpZM4aGlYz> .\r\n> -- - Alex\r\n\r\n@Alex Sure, it can be done. For these two cases tensor[other_tensor], tensor[other_array], as long as other_tensor and other_array are not scalars,  the computation of tensor[other_tensor], tensor[other_array] will be identical to boolean_mask().  ", "@alex I just discovered the inconsistency between boolean_mask() and its corresponding numpy operation.  For example, the following test code \r\n```\r\n    foo_tf = tf.constant([[1, 2, 3, 4], [4, 5, 6, 7], [7, 8, 9, 10]])\r\n    foo_np = np.array([[1, 2, 3, 4], [4, 5, 6, 7], [7, 8, 9, 10]])\r\n    foo_mask = np.array([0.1, 1.1, 2.1], dtype=float)\r\n    print(tf.boolean_mask(foo_tf, foo_mask).eval())\r\n    print(foo_np[foo_mask])\r\n```\r\nwill output\r\n```\r\n[[ 1  2  3  4]\r\n [ 4  5  6  7]\r\n [ 7  8  9 10]]\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 46, in <module>\r\n    print(foo_np[foo_mask])\r\nIndexError: arrays used as indices must be of integer (or boolean) type\r\n```\r\n\r\nIs this a bug in boolean_mask() or is this result expected?\r\nIf it is expected, then I can keep writing test code. Otherwise, we need to first fix the bug existed in boolean_mask(). \r\n\r\n\r\n\r\n", "Maybe boolean_mask is casting the floats to booleans, and hence we should\nonly dispatch to boolean_mask if there's either a python bool, numpy bool\nndarray, or bool tensor?\n\nOn Wed, Jan 30, 2019 at 12:44 PM musikisomorphie <notifications@github.com>\nwrote:\n\n> @alex <https://github.com/alex> I just discovered the inconsistency\n> between boolean_mask() and its corresponding numpy operation. For example,\n> the following test code\n>\n>     foo_tf = tf.constant([[1, 2, 3, 4], [4, 5, 6, 7], [7, 8, 9, 10]])\n>     foo_np = np.array([[1, 2, 3, 4], [4, 5, 6, 7], [7, 8, 9, 10]])\n>     foo_mask = np.array([0.1, 1.1, 2.1], dtype=float)\n>     print(tf.boolean_mask(foo_tf, foo_mask).eval())\n>     print(foo_np[foo_mask])\n>\n> will output\n>\n> [[ 1  2  3  4]\n>  [ 4  5  6  7]\n>  [ 7  8  9 10]]\n> Traceback (most recent call last):\n>   File \"test.py\", line 46, in <module>\n>     print(foo_np[foo_mask])\n> IndexError: arrays used as indices must be of integer (or boolean) type\n>\n> Is this a bug in boolean_mask() or is this result expected?\n> If it is expected, then I can keep writing test code. Otherwise, we need\n> to first fix the bug existed in boolean_mask().\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/25008#issuecomment-459100507>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxTpUN1NXg1IXlFnCewiEpN-9x9b6ks5vIgQTgaJpZM4aGlYz>\n> .\n>\n\n\n-- \n - Alex\n", "> Maybe boolean_mask is casting the floats to booleans, and hence we should only dispatch to boolean_mask if there's either a python bool, numpy bool ndarray, or bool tensor?\r\n> [\u2026](#)\r\n> On Wed, Jan 30, 2019 at 12:44 PM musikisomorphie ***@***.***> wrote: @alex <https://github.com/alex> I just discovered the inconsistency between boolean_mask() and its corresponding numpy operation. For example, the following test code foo_tf = tf.constant([[1, 2, 3, 4], [4, 5, 6, 7], [7, 8, 9, 10]]) foo_np = np.array([[1, 2, 3, 4], [4, 5, 6, 7], [7, 8, 9, 10]]) foo_mask = np.array([0.1, 1.1, 2.1], dtype=float) print(tf.boolean_mask(foo_tf, foo_mask).eval()) print(foo_np[foo_mask]) will output [[ 1 2 3 4] [ 4 5 6 7] [ 7 8 9 10]] Traceback (most recent call last): File \"test.py\", line 46, in <module> print(foo_np[foo_mask]) IndexError: arrays used as indices must be of integer (or boolean) type Is this a bug in boolean_mask() or is this result expected? If it is expected, then I can keep writing test code. Otherwise, we need to first fix the bug existed in boolean_mask(). \u2014 You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <[#25008 (comment)](https://github.com/tensorflow/tensorflow/pull/25008#issuecomment-459100507)>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AAATxTpUN1NXg1IXlFnCewiEpN-9x9b6ks5vIgQTgaJpZM4aGlYz> .\r\n> -- - Alex\r\n\r\nThat will indeed solve the x[x>a] problem, but then the boolean_mask() remains \r\n\r\n> Maybe boolean_mask is casting the floats to booleans, and hence we should only dispatch to boolean_mask if there's either a python bool, numpy bool ndarray, or bool tensor?\r\n> [\u2026](#)\r\n> On Wed, Jan 30, 2019 at 12:44 PM musikisomorphie ***@***.***> wrote: @alex <https://github.com/alex> I just discovered the inconsistency between boolean_mask() and its corresponding numpy operation. For example, the following test code foo_tf = tf.constant([[1, 2, 3, 4], [4, 5, 6, 7], [7, 8, 9, 10]]) foo_np = np.array([[1, 2, 3, 4], [4, 5, 6, 7], [7, 8, 9, 10]]) foo_mask = np.array([0.1, 1.1, 2.1], dtype=float) print(tf.boolean_mask(foo_tf, foo_mask).eval()) print(foo_np[foo_mask]) will output [[ 1 2 3 4] [ 4 5 6 7] [ 7 8 9 10]] Traceback (most recent call last): File \"test.py\", line 46, in <module> print(foo_np[foo_mask]) IndexError: arrays used as indices must be of integer (or boolean) type Is this a bug in boolean_mask() or is this result expected? If it is expected, then I can keep writing test code. Otherwise, we need to first fix the bug existed in boolean_mask(). \u2014 You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <[#25008 (comment)](https://github.com/tensorflow/tensorflow/pull/25008#issuecomment-459100507)>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AAATxTpUN1NXg1IXlFnCewiEpN-9x9b6ks5vIgQTgaJpZM4aGlYz> .\r\n> -- - Alex\r\n\r\nI think this may cause confusion due to the inconsistency between x[...] and boolean,\r\n\r\n> Maybe boolean_mask is casting the floats to booleans, and hence we should only dispatch to boolean_mask if there's either a python bool, numpy bool ndarray, or bool tensor?\r\n> [\u2026](#)\r\n> On Wed, Jan 30, 2019 at 12:44 PM musikisomorphie ***@***.***> wrote: @alex <https://github.com/alex> I just discovered the inconsistency between boolean_mask() and its corresponding numpy operation. For example, the following test code foo_tf = tf.constant([[1, 2, 3, 4], [4, 5, 6, 7], [7, 8, 9, 10]]) foo_np = np.array([[1, 2, 3, 4], [4, 5, 6, 7], [7, 8, 9, 10]]) foo_mask = np.array([0.1, 1.1, 2.1], dtype=float) print(tf.boolean_mask(foo_tf, foo_mask).eval()) print(foo_np[foo_mask]) will output [[ 1 2 3 4] [ 4 5 6 7] [ 7 8 9 10]] Traceback (most recent call last): File \"test.py\", line 46, in <module> print(foo_np[foo_mask]) IndexError: arrays used as indices must be of integer (or boolean) type Is this a bug in boolean_mask() or is this result expected? If it is expected, then I can keep writing test code. Otherwise, we need to first fix the bug existed in boolean_mask(). \u2014 You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <[#25008 (comment)](https://github.com/tensorflow/tensorflow/pull/25008#issuecomment-459100507)>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AAATxTpUN1NXg1IXlFnCewiEpN-9x9b6ks5vIgQTgaJpZM4aGlYz> .\r\n> -- - Alex\r\n\r\nI think what you suggested make sense. In this case, do you think  boolean_mask() should also run the type check? Because otherwise such inconsistency between x[...] and boolean_mask can be confusing, besides casting non boolean type to boolean (always cast to True) does not make too much sense to me. It is quite late in my place, feel free to leave your comments. I will keep working on it tomorrow.", "I would avoid changing boolean_mask now because it has boolean in the name\nwhich can be understood to imply the cast.\n\nOn Wed, Jan 30, 2019 at 1:21 PM musikisomorphie <notifications@github.com>\nwrote:\n\n> Maybe boolean_mask is casting the floats to booleans, and hence we should\n> only dispatch to boolean_mask if there's either a python bool, numpy bool\n> ndarray, or bool tensor?\n> \u2026 <#m_4729633210475361670_>\n> On Wed, Jan 30, 2019 at 12:44 PM musikisomorphie ***@***.***> wrote: @alex\n> <https://github.com/alex> https://github.com/alex I just discovered the\n> inconsistency between boolean_mask() and its corresponding numpy operation.\n> For example, the following test code foo_tf = tf.constant([[1, 2, 3, 4],\n> [4, 5, 6, 7], [7, 8, 9, 10]]) foo_np = np.array([[1, 2, 3, 4], [4, 5, 6,\n> 7], [7, 8, 9, 10]]) foo_mask = np.array([0.1, 1.1, 2.1], dtype=float)\n> print(tf.boolean_mask(foo_tf, foo_mask).eval()) print(foo_np[foo_mask])\n> will output [[ 1 2 3 4] [ 4 5 6 7] [ 7 8 9 10]] Traceback (most recent call\n> last): File \"test.py\", line 46, in print(foo_np[foo_mask]) IndexError:\n> arrays used as indices must be of integer (or boolean) type Is this a bug\n> in boolean_mask() or is this result expected? If it is expected, then I can\n> keep writing test code. Otherwise, we need to first fix the bug existed in\n> boolean_mask(). \u2014 You are receiving this because you were mentioned. Reply\n> to this email directly, view it on GitHub <#25008 (comment)\n> <https://github.com/tensorflow/tensorflow/pull/25008#issuecomment-459100507>>,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAATxTpUN1NXg1IXlFnCewiEpN-9x9b6ks5vIgQTgaJpZM4aGlYz\n> .\n> -- - Alex\n>\n> That will indeed solve the x[x>a] problem, but then the boolean_mask()\n> remains\n>\n> Maybe boolean_mask is casting the floats to booleans, and hence we should\n> only dispatch to boolean_mask if there's either a python bool, numpy bool\n> ndarray, or bool tensor?\n> \u2026 <#m_4729633210475361670_>\n> On Wed, Jan 30, 2019 at 12:44 PM musikisomorphie ***@***.***> wrote: @alex\n> <https://github.com/alex> https://github.com/alex I just discovered the\n> inconsistency between boolean_mask() and its corresponding numpy operation.\n> For example, the following test code foo_tf = tf.constant([[1, 2, 3, 4],\n> [4, 5, 6, 7], [7, 8, 9, 10]]) foo_np = np.array([[1, 2, 3, 4], [4, 5, 6,\n> 7], [7, 8, 9, 10]]) foo_mask = np.array([0.1, 1.1, 2.1], dtype=float)\n> print(tf.boolean_mask(foo_tf, foo_mask).eval()) print(foo_np[foo_mask])\n> will output [[ 1 2 3 4] [ 4 5 6 7] [ 7 8 9 10]] Traceback (most recent call\n> last): File \"test.py\", line 46, in print(foo_np[foo_mask]) IndexError:\n> arrays used as indices must be of integer (or boolean) type Is this a bug\n> in boolean_mask() or is this result expected? If it is expected, then I can\n> keep writing test code. Otherwise, we need to first fix the bug existed in\n> boolean_mask(). \u2014 You are receiving this because you were mentioned. Reply\n> to this email directly, view it on GitHub <#25008 (comment)\n> <https://github.com/tensorflow/tensorflow/pull/25008#issuecomment-459100507>>,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAATxTpUN1NXg1IXlFnCewiEpN-9x9b6ks5vIgQTgaJpZM4aGlYz\n> .\n> -- - Alex\n>\n> I think this may cause confusion due to the inconsistency between x[...]\n> and boolean,\n>\n> Maybe boolean_mask is casting the floats to booleans, and hence we should\n> only dispatch to boolean_mask if there's either a python bool, numpy bool\n> ndarray, or bool tensor?\n> \u2026 <#m_4729633210475361670_>\n> On Wed, Jan 30, 2019 at 12:44 PM musikisomorphie ***@***.***> wrote: @alex\n> <https://github.com/alex> https://github.com/alex I just discovered the\n> inconsistency between boolean_mask() and its corresponding numpy operation.\n> For example, the following test code foo_tf = tf.constant([[1, 2, 3, 4],\n> [4, 5, 6, 7], [7, 8, 9, 10]]) foo_np = np.array([[1, 2, 3, 4], [4, 5, 6,\n> 7], [7, 8, 9, 10]]) foo_mask = np.array([0.1, 1.1, 2.1], dtype=float)\n> print(tf.boolean_mask(foo_tf, foo_mask).eval()) print(foo_np[foo_mask])\n> will output [[ 1 2 3 4] [ 4 5 6 7] [ 7 8 9 10]] Traceback (most recent call\n> last): File \"test.py\", line 46, in print(foo_np[foo_mask]) IndexError:\n> arrays used as indices must be of integer (or boolean) type Is this a bug\n> in boolean_mask() or is this result expected? If it is expected, then I can\n> keep writing test code. Otherwise, we need to first fix the bug existed in\n> boolean_mask(). \u2014 You are receiving this because you were mentioned. Reply\n> to this email directly, view it on GitHub <#25008 (comment)\n> <https://github.com/tensorflow/tensorflow/pull/25008#issuecomment-459100507>>,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAATxTpUN1NXg1IXlFnCewiEpN-9x9b6ks5vIgQTgaJpZM4aGlYz\n> .\n> -- - Alex\n>\n> I think what you suggested make sense. In this case, do you think\n> boolean_mask() should also run the type check? Because otherwise such\n> inconsistency between x[...] and boolean_mask can be confusing, besides\n> casting non boolean type to boolean (always cast to True) does not make too\n> much sense to me. It is quite late in my place, feel free to leave your\n> comments. I will keep working on it tomorrow.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/25008#issuecomment-459110791>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxZJCbG1yl0o0XMavTBw-lPEB9nCaks5vIgzvgaJpZM4aGlYz>\n> .\n>\n\n\n-- \n - Alex\n", "Hi @alextp, following our previous discussion, I dispatch only bool (array tensor etc.) types to boolean_mask and add more related unit tests.  ", "Hi guys @alextp  @shoyer, sorry for the delay. I made some new changes. \r\n\r\n```\r\n  if isinstance(slice_spec, bool) or \\\r\n          (isinstance(slice_spec, ops.Tensor) and slice_spec.dtype == dtypes.bool) or \\\r\n          (isinstance(slice_spec, np.ndarray) and slice_spec.dtype == bool) or \\\r\n          (isinstance(slice_spec, (list, tuple)) and np.asarray(slice_spec).dtype == bool):\r\n    return boolean_mask(tensor=tensor, mask=slice_spec)\r\n```\r\nThe first three conditions are clear, as @alextp's suggested. The last one is to check whether elements of a list or tuple are all bool, if so we run boolean_mask(). \r\n\r\nAlso, np.asarray(slice_spec) doesn't report errors or affect those NumPy-style slicing syntax during the multiple tests I run myself  and unit tests.\r\n\r\n", "Hi @alextp, @shoyer, I fixed the non-whitelisted pylint errors and one unit test error.\r\nCould you guys review my code again?", "Hi @alextp, I saw the internal CI build failure and checked the logs, I didn't find a direct connection between those failures with my code.  What should I do next?", "I'll retry the internal stuff.\n\nOn Tue, Feb 5, 2019 at 11:52 AM musikisomorphie <notifications@github.com>\nwrote:\n\n> Hi @alextp <https://github.com/alextp>, I saw the internal CI build\n> failure and checked the logs, I didn't find a direct connection between\n> those failures with my code. What should I do next?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/25008#issuecomment-460777828>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxXfOzSxlesvKOKAzAQII4K1mLuZbks5vKeDsgaJpZM4aGlYz>\n> .\n>\n\n\n-- \n - Alex\n", "> I'll retry the internal stuff.\r\n> [\u2026](#)\r\n> On Tue, Feb 5, 2019 at 11:52 AM musikisomorphie ***@***.***> wrote: Hi @alextp <https://github.com/alextp>, I saw the internal CI build failure and checked the logs, I didn't find a direct connection between those failures with my code. What should I do next? \u2014 You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <[#25008 (comment)](https://github.com/tensorflow/tensorflow/pull/25008#issuecomment-460777828)>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AAATxXfOzSxlesvKOKAzAQII4K1mLuZbks5vKeDsgaJpZM4aGlYz> .\r\n> -- - Alex\r\n\r\nThanks", "After double checking the error report, I saw some errors related to my change, I am going to keep working on it. ", "Hi @alextp, could you run the required tests again? I removed the np.asarray() condition, because it will cause some errors of raggedtensor reported by the required tests last time. ", "Hi @alextp,  the two internal failures related to my change are fixed this time. now there is only one internal CI build failed, I didn't see it has anything to do with my changes. Still just let me know if there is anything I should do. ", "Those failures are unrelated to your change.", "> Those failures are unrelated to your change.\r\n\r\nOkay, good to know. Thanks ", "@alextp, it's been a while since the last time I updated my PR, is my PR going to be merged?  ", "@rthadur what's the state of the internal push for this pull request?", "> @rthadur what's the state of the internal push for this pull request?\r\n\r\n@alextp it has been deleted , let me rerun ,thanks ", "> > @rthadur what's the state of the internal push for this pull request?\r\n> \r\n> @alextp it has been deleted , let me rerun ,thanks\r\n\r\n@rthadur @alextp , I saw the import/copybara error which did not happen before, is there anything  I should do ?", "> > > @rthadur what's the state of the internal push for this pull request?\r\n> > \r\n> > \r\n> > @alextp it has been deleted , let me rerun ,thanks\r\n> \r\n> @rthadur @alextp , I saw the import/copybara error which did not happen before, is there anything I should do ?\r\n\r\nThank you @musikisomorphie , i believe the changes are pushed internally , so we have to close this PR.", "No, it has not been merged internally; I don't see this code.\n\nOn Wed, Feb 20, 2019 at 12:18 PM rthadur <notifications@github.com> wrote:\n\n> @rthadur <https://github.com/rthadur> what's the state of the internal\n> push for this pull request?\n>\n> @alextp <https://github.com/alextp> it has been deleted , let me rerun\n> ,thanks\n>\n> @rthadur <https://github.com/rthadur> @alextp <https://github.com/alextp>\n> , I saw the import/copybara error which did not happen before, is there\n> anything I should do ?\n>\n> Thank you @musikisomorphie <https://github.com/musikisomorphie> , i\n> believe the changes are pushed internally , so we have to close this PR.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/25008#issuecomment-465736587>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxWbUnE22o_0fwdKp1RZhxBp7ISzYks5vPa2TgaJpZM4aGlYz>\n> .\n>\n\n\n-- \n - Alex\n", "Hi @rthadur, how is the progress of this PR?"]}, {"number": 25007, "title": "FAILED: Build did NOT complete successfully", "body": "**System information**\r\n- OS Platform and Distribution: Ubuntu 16.04.5 LTS\r\n- TensorFlow installed from: Docker image `tensorflow/tensorflow:nightly-devel-gpu`\r\n- TensorFlow version: `master` branch\r\n- Python version: 2 and 3\r\n- Installed using: Docker\r\n\r\n\r\n**Describe the problem**\r\n\r\nI want to submit pull requests and am unable to run the TensorFlow unit tests. I will be happy to add directions to the documentation once I solve the problem.\r\n\r\nI set up an Azure virtual machine with Tesla M60 GPUs (the most recent that my academic subscription allows) with the image `Data Science Virtual Machine` on a `Standard NV6 (6 vcpus, 56 GB memory)`. I ssh into it, run these commands, and get the error in the title.\r\n\r\n```bash\r\nsudo docker pull tensorflow/tensorflow:nightly-devel-gpu\r\nsudo nvidia-docker run -it abbaea1b533c\r\n```\r\n\r\nThen, inside the Docker container:\r\n\r\n```bash\r\nexport LD_LIBRARY_PATH=\"${LD_LIBRARY_PATH}:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH\"\r\nexport flags=\"--config=opt --config=cuda -k\"\r\ncd /tensorflow/\r\nbazel test ${flags} //tensorflow/python/...\r\n```\r\n\r\nI attach the whole console output, which ends with\r\n\r\n```\r\nFAILED: Build did NOT complete successfully\r\n```\r\n[failed_build_log.txt](https://github.com/tensorflow/tensorflow/files/2770095/failed_build_log.txt)\r\n\r\n\r\nHow can I set up a machine that runs TensorFlow's unit tests?", "comments": ["@angersson for issues regarding our docker containers.", "I don't see enough information here to know whether or not the Docker setup is to blame. Try building with CPU only first (and the suggested docker flags) to see if that works for you.", "Same issue as #24619 and I've posted a solution there.", "After some experimentation, I found that choosing another image for the Azure virtual machine fixes this issue. I used `NVIDIA GPU Cloud Image` instead of `Data Science Virtual Machine`, and now the build proceeds until it runs out of space (a `Standard_NV24` gives 32 GiB of disk and Docker lists 17 GiB for the size and 20 GiB for the virtual size). That is another issue and I will close this one.\r\n\r\n@ppwwyyxx : although I did not try your solution, I believe it is related. The first error in my log file has a warning after it that mentions `libcuda`:\r\n\r\n```\r\nERROR: /tensorflow/tensorflow/python/BUILD:1741:1: Couldn't build file tensorflow/python/gen_linalg_ops_py_wrappers_cc: Linking of rule '//tensorflow/python:gen_linalg_ops_py_wrappers_cc' failed (Exit 1)\r\n/usr/bin/ld: warning: libcuda.so.1, needed by bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so, not found (try using -rpath or -rpath-link)\r\n```", "A virtual machine with an additional 1 TB disk is able to build:\r\n\r\n```\r\nExecuted 1045 out of 1045 tests: 511 tests pass and 534 fail locally.\r\nINFO: Build completed, 534 tests FAILED, 24783 total actions\r\n```\r\n\r\nI will look into those failures and that is a separate issue."]}, {"number": 25006, "title": "install TensorFlow without using GPU", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS Mojave 10.14\r\n- TensorFlow installed from (source or binary): \r\n`pip install https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.10.1-py3-none-any.whl`\r\n- TensorFlow version: 1.10.1\r\n- Python version: 3.6.3\r\n- Installed using virtualenv? pip? conda?: pip (and I also tried pip3)\r\n- Bazel version (if compiling from source): 0.21.0(added this information in 1/20)\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: \r\n- GPU model and memory:\r\n\r\n**Describe the problem**\r\n\r\nI want to use TensorFlow in my laptop, and I would like to use TensorFlow without using GPU.\r\n\r\nWhen I started Python on the command line, the following error occurred.\r\nI have found a number of responses to similar errors, but they are all considered to be using GPUs because they are using CUDA.\r\nIn addition, although I got information that it operates by invalidating SIP by #19720, it did not solve my problem because I don't have directory like named 'cuda'.\r\n\r\nAlso, because of eGPU 's circumstances, mojave got information that GPEN version of TensorFlow will not work. I would like to ask about this as well.\r\n\r\n```\r\nPython 3.6.3 |Anaconda custom (64-bit)| (default, Oct  6 2017, 12:04:38) \r\n[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import numpy as np\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"/Users/yuichikato/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/Users/yuichikato/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/Users/yuichikato/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/Users/yuichikato/anaconda3/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/Users/yuichikato/anaconda3/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: dlopen(/Users/yuichikato/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 10): Library not loaded: @rpath/libcublas.8.0.dylib\r\n  Referenced from: /Users/yuichikato/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n  Reason: image not found\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/yuichikato/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/Users/yuichikato/anaconda3/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 51, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/Users/yuichikato/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/Users/yuichikato/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/Users/yuichikato/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/Users/yuichikato/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/Users/yuichikato/anaconda3/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/Users/yuichikato/anaconda3/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: dlopen(/Users/yuichikato/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 10): Library not loaded: @rpath/libcublas.8.0.dylib\r\n  Referenced from: /Users/yuichikato/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n  Reason: image not found\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n```", "comments": ["@katoyu Couple of questions. What was the bazel version, what were the commands you used to install tensorflow? Please check tested [build configurations](https://www.tensorflow.org/install/source). Please provide as many details as possible to find the root cause of the issue. Thanks!", "@jvishnuvardhan \r\nThanks for your reply.\r\nAlthough this is a Japanese site, I referred to this page. https://qiita.com/tom_ato/items/bbdf4574b3ecb0048fa1\r\nAnd I'm sorry but I confirmed that I did not have Bazel in my environment. I will install Bazel and install TensorFlow again.", "I tried to build Bazel with seeing [build configurations](https://www.tensorflow.org/install/source), but I failed it.\r\n```\r\n$ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\nINFO: Invocation ID: d653837f-cc0b-497a-a45d-bcabfdf22e7e\r\nERROR: /private/var/tmp/_bazel_yuichikato/2f56da6d0e205256e4b3591a707923ab/external/local_config_cc/BUILD:58:5: in apple_cc_toolchain rule @local_config_cc//:cc-compiler-armeabi-v7a: Xcode version must be specified to use an Apple CROSSTOOL. If your Xcode version has changed recently, try: \"bazel clean --expunge\" to re-run Xcode configuration\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis of target '@local_config_cc//:cc-compiler-armeabi-v7a' failed; build aborted\r\nINFO: Elapsed time: 25.019s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (164 packages loaded, 3136 targets\\\r\n configured)\r\n    currently loading: tensorflow/contrib/resampler\r\n    Fetching @grpc; fetching\r\n    Fetching @gif_archive; fetching\r\n$ bazel clean --expunge\r\nINFO: Invocation ID: 54ca3adf-f3f3-4fbf-96e0-c51e90344dee\r\nINFO: Starting clean.\r\n$ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\nStarting local Bazel server and connecting to it...\r\nINFO: Invocation ID: 1ef449d8-976c-427f-8ffc-66e48c45b98b\r\nERROR: /private/var/tmp/_bazel_yuichikato/2f56da6d0e205256e4b3591a707923ab/external/local_config_cc/BUILD:58:5: in apple_cc_toolchain rule @local_config_cc//:cc-compiler-armeabi-v7a: Xcode version must be specified to use an Apple CROSSTOOL. If your Xcode version has changed recently, try: \"bazel clean --expunge\" to re-run Xcode configuration\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis of target '@local_config_cc//:cc-compiler-armeabi-v7a' failed; build aborted\r\nINFO: Elapsed time: 17.207s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (179 packages loaded, 3054 targets\\\r\n configured)\r\n    Fetching @com_google_absl; fetching 5s\r\n```\r\nI wrote the full text on the link below.\r\nhttps://github.com/katoyu/about-install-tensorflow/blob/master/bazel-error", "@katoyu \r\nTry installing Tensorflow by using ```pip install tensorflow==1.10.1```. \r\nIt should work just fine.", "@AndreiMaga \r\nThank you for your comments, and I maybe successed to use tensorflow owing to your advice.\r\n```\r\n$ cd\r\n$ python\r\nPython 3.6.3 (default, Jan 21 2019, 18:20:23) \r\n[GCC 4.2.1 Compatible Apple LLVM 10.0.0 (clang-1000.10.44.4)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n/Users/yuichikato/.pyenv/versions/3.6.3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\r\n  return f(*args, **kwds)\r\n>>> hello = tf.constant('Hello, TensorFlow!')\r\n>>> sess = tf.Session()\r\n2019-01-21 21:28:04.749299: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n>>> print(sess.run(hello))\r\nb'Hello, TensorFlow!'\r\n>>> \r\n```\r\nI would like to ask again with confirmation, but can I ignore this Runtime Warning sentence?\r\nCan you also erase it?", "@katoyu \r\n```I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA``` \r\nThis is a warning \"Tensorflow was not compiled to use your cpu's low level api\". Basically it tells you that tensorflow can comunicate better with your cpu with  the ```AVX2``` and ```FMA``` flags. These flags are set when tensorflow is compiled.\r\n\r\nIt should work fine like this but \"faster\" with these flags set.\r\nYou cannot erase that unfortunately.", "@AndreiMaga \r\nOkay, it is not a big issue.\r\nI'm grateful for a polite reply.", "@katoyu Could you try this [solution](https://stackoverflow.com/questions/35911252/disable-tensorflow-debugging-information) to disable warnings. @AndreiMaga Thanks for your support.", "I think it was resolved. I am closing the issue. Please open new ticket if you see similar issue again. Thanks!\r\n"]}, {"number": 25005, "title": "Update", "body": "Created in SparseApply if needed.", "comments": ["I think we don't need that variable sorry for the inconvinence ", "No worries and have a great week!"]}, {"number": 25004, "title": "Cherry pick additional test failures.", "body": "", "comments": []}, {"number": 25003, "title": "Unable to figure out CTS tests for Android NNAPI for  SOFTMAX inputs/outputs ", "body": "Have been using Android P for our internal Product Ramp up  and looked into following code where I am unable to figure out , how below  softmax input  and output could match like what the mathematical formula here ?...any one could help me to understand or any link for documentations around it ?\r\n http://androidxref.com/9.0.0_r3/xref/frameworks/ml/nn/runtime/test/generated/examples/softmax_float_1.example.cpp \r\n\r\n\r\n**Describe the documentation issue**\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n", "comments": ["@avaish1 Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support. Github is mainly for addressing bugs in installation and performance. Thanks!", "thanx..would post on stackoverflow...", "@avaish1 Thank you!"]}, {"number": 25002, "title": "Tensorflow segmentation fault with RTX 2080Ti CUDA 9.0", "body": "<em>Recently, I try to run my previous programming with our new multiple GPUs servers with RTX 2080 Ti. I do not make any about my code which can run successfully on Cuda 9.0 with Tesla V100. I am not sure what the problem is and it seems that there is a problem with Cuda support. When I run the optimizer of Tensorflow, I get the error of segmentation fault. Self-attention GAN is one example that I cannot run successfully. This link is the source code. https://github.com/taki0112/Self-Attention-GAN-Tensorflow</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): Conda\r\n- TensorFlow version (use command below): 1.10.1\r\n- Python version:Python 3.6.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): GCC 7.2.0\r\n- CUDA/cuDNN version: Cuda compilation tools, release 9.0, V9.0.176\r\n- GPU model and memory: RTX 2080Ti\r\n\r\n**Describe the current behavior**\r\nTensorflow can build graph successfully, but when it comes to flow the data into model and update parameters,  it always gives the segmentation fault error. The error is as the following.\r\n\r\ngenerator/attention/h_conv/bias:0 (float32_ref 256) [256, bytes: 1024]\r\ngenerator/attention/gamma:0 (float32_ref 1) [1, bytes: 4]\r\ngenerator/up_conv_2/kernel:0 (float32_ref 3x3x256x128) [294912, bytes: 1179648]\r\ngenerator/up_conv_2/bias:0 (float32_ref 128) [128, bytes: 512]\r\ngenerator/batch_norm_2/beta:0 (float32_ref 128) [128, bytes: 512]\r\ngenerator/batch_norm_2/gamma:0 (float32_ref 128) [128, bytes: 512]\r\ngenerator/up_conv_3/kernel:0 (float32_ref 3x3x128x64) [73728, bytes: 294912]\r\ngenerator/up_conv_3/bias:0 (float32_ref 64) [64, bytes: 256]\r\ngenerator/batch_norm_3/beta:0 (float32_ref 64) [64, bytes: 256]\r\ngenerator/batch_norm_3/gamma:0 (float32_ref 64) [64, bytes: 256]\r\ngenerator/G_conv_logit/kernel:0 (float32_ref 3x3x64x3) [1728, bytes: 6912]\r\ngenerator/G_conv_logit/bias:0 (float32_ref 3) [3, bytes: 12]\r\nTotal size of variables: 19748741\r\nTotal bytes of variables: 78994964\r\n [*] Reading checkpoints...\r\n [*] Failed to find a checkpoint\r\n [!] Load failed...\r\nSegmentation fault (core dumped)\r\n\r\n\r\n**Describe the expected behavior**\r\nThe program should run successfully.\r\n\r\n**Code to reproduce the issue**\r\n Self-attention GAN is one example that I cannot run successfully. This link is the source code. https://github.com/taki0112/Self-Attention-GAN-Tensorflow. Maybe this code is a little bit complex, but it can show you where the problem is.\r\n\r\n**Other info / logs**\r\nIt seems that there is a version mismatching between the CUDA version and the RTX support, but I am not sure.", "comments": ["@crownk1997 I believe you need cuda10 to be able to use Turing generation cards.", "Please have a look at this tutorial, I am currently facing the same problem. https://www.pytorials.com/how-to-install-tensorflow-gpu-with-cuda-10-0-for-python-on-ubuntu/"]}, {"number": 25001, "title": "close", "body": "", "comments": ["@ebrevdo Can you PTAL? Thanks", "If you call `tf.nn.{dynamic_,}rnn` with `sequence_length` vector provided and use padded inputs, it will behave the same as if you used pytorch's pack_padded_sequence.  Specifically, the final state will properly contain the state of the final step for each batch, and the output values in a batch row `b` corresponding to time points after `sequence_length[b]` will be zeroed out.", "but in tensorflow 2.0, there is no `tf.nn.{dynamic_,}rnn`"]}, {"number": 25000, "title": "\"no such package '@com_google_protobuf//'\" while running \"bazel test\" (r1.13)", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.1 LTS\r\n- TensorFlow version: 1.13\r\n- Python version: 3.6.7\r\n- Installed using virtualenv? pip? conda?: created virtualenv for build process\r\n- Bazel version (if compiling from source): 0.21.0\r\n- GCC/Compiler version (if compiling from source): 7.3.0\r\n- CUDA/cuDNN version: 10.0.130 / 7.4.2.24\r\n- GPU model and memory: Tesla P40 (inside virtual machine via Nvidia Virtual GPU), 8GB memory\r\n\r\n\r\n\r\n**Describe the problem**\r\nDuring the build process described on the tensorflow website for r1.13, the command `bazel test -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/lite/...` fails due to not finding the package `'@com_google_protobuf//'`. The full error message is:\r\n```\r\nERROR: /home/cgv/tensorflow/tensorflow/core/BUILD:255:1: every rule of type proto_library implicitly depends upon the target '@com_google_protobuf//:protoc', but this target could not be found because of: no such package '@com_google_protobuf//': The native http_archive rule is deprecated. load(\"@bazel_tools//tools/build_defs/repo:http.bzl\", \"http_archive\") for a drop-in replacement.\r\nUse --incompatible_remove_native_http_archive=false to temporarily continue using the native rule.\r\nERROR: Analysis of target '//tensorflow/core:example_protos' failed; build aborted: Analysis failed\r\n```\r\nIf I use the switch `--incompatible_remove_native_http_archive=false` as the error message suggests, I receive different errors:\r\n```\r\nERROR: /home/cgv/.cache/bazel/_bazel_cgv/ad496aba49fed364b29fcfdc32e3267a/external/com_google_protobuf/BUILD:597:1: Traceback (most recent call last):\r\n\tFile \"/home/cgv/.cache/bazel/_bazel_cgv/ad496aba49fed364b29fcfdc32e3267a/external/com_google_protobuf/BUILD\", line 597\r\n\t\tinternal_gen_well_known_protos_java(srcs = WELL_KNOWN_PROTOS)\r\n\tFile \"/home/cgv/.cache/bazel/_bazel_cgv/ad496aba49fed364b29fcfdc32e3267a/external/com_google_protobuf/protobuf.bzl\", line 266, in internal_gen_well_known_protos_java\r\n\t\tLabel((\"%s//protobuf_java\" % REPOSITOR...))\r\n\tFile \"/home/cgv/.cache/bazel/_bazel_cgv/ad496aba49fed364b29fcfdc32e3267a/external/com_google_protobuf/protobuf.bzl\", line 266, in Label\r\n\t\tREPOSITORY_NAME\r\nThe value 'REPOSITORY_NAME' has been removed in favor of 'repository_name()', please use the latter (https://docs.bazel.build/versions/master/skylark/lib/native.html#repository_name). You can temporarily allow the old name by using --incompatible_package_name_is_a_function=false\r\nERROR: /home/cgv/.cache/bazel/_bazel_cgv/ad496aba49fed364b29fcfdc32e3267a/external/com_google_protobuf/BUILD:380:1: Target '@com_google_protobuf//:android' contains an error and its package is in error and referenced by '@com_google_protobuf//:protoc'\r\nERROR: /home/cgv/.cache/bazel/_bazel_cgv/ad496aba49fed364b29fcfdc32e3267a/external/com_google_protobuf/BUILD:380:1: Target '@com_google_protobuf//:windows' contains an error and its package is in error and referenced by '@com_google_protobuf//:protoc'\r\nERROR: /home/cgv/.cache/bazel/_bazel_cgv/ad496aba49fed364b29fcfdc32e3267a/external/com_google_protobuf/BUILD:380:1: Target '@com_google_protobuf//:windows_msvc' contains an error and its package is in error and referenced by '@com_google_protobuf//:protoc'\r\nERROR: /home/cgv/tensorflow/tensorflow/core/BUILD:255:1: every rule of type proto_library implicitly depends upon the target '@com_google_protobuf//:protoc', but this target could not be found because of: Target '@com_google_protobuf//:protoc' contains an error and its package is in error\r\nERROR: Analysis of target '//tensorflow/core:example_protos' failed; build aborted: Analysis failed\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n1. Install `python3-dev` and other dependencies directly from Ubuntu via apt\r\n2. Install `bazel` from repository https://storage.googleapis.com/bazel-apt via apt\r\n3. Clone the r1.13 branch of TensorFlow via `git clone https://github.com/tensorflow/tensorflow.git --branch r1.13 --single-branch`\r\n4. Create a virtual environment with `python3 -m venv tensorflow-build` and activate it\r\n5. Install the remaining dependencies via pip as stated on https://www.tensorflow.org/install/source\r\n`pip install -U --user pip six numpy wheel mock`\r\n`pip install -U --user keras_applications==1.0.6 --no-deps`\r\n`pip install -U --user keras_preprocessing==1.0.5 --no-deps`\r\n6. run `./configure` in the cloned repo (full output in [configure.log](https://github.com/tensorflow/tensorflow/files/2769428/configure.log))\r\n7. run `bazel test -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/lite/...`\r\n\r\nThen after a few seconds of loading packages and analyzing (and a few deprecation warnings), the described error occurs.\r\n\r\nPlease let me know if I can provide any further information. Thanks!", "comments": ["Looks like bazel 0.21.0 is not fully working with the protobuf repo we have.\r\nI think @case540 ran into this before @meteorcloudy from bazel team to help route this problem.", "Yes, this is due to an incompatible change in 0.21.0, https://github.com/bazelbuild/bazel/issues/6570\r\n\r\nYou can [upgrade the protobuf version](https://github.com/tensorflow/tensorflow/commit/b0d7d8a477d3041e2d0ebd0cb1d35e4a7fa09663) or use an old version of Bazel (eg, 0.20.0)", "The protobuf version apparently was already set to 3.6.1.2 (I cloned branch r1.13). I have now downgraded to bazel 0.20.0, which seems to have fixed the issue that I had. Unfortunately, I get a different error now:\r\n```\r\nERROR: /home/cgv/tensorflow/tensorflow/contrib/resampler/BUILD:103:1: in deps attribute of py_test rule //tensorflow/contrib/resampler:resampler_ops_xla_test_gpu: '//tensorflow/compiler/tf2xla/kernels:resampler_ops' does not have mandatory providers: 'py'. Since this rule was created by the macro 'tf_xla_py_test', the error might have been caused by the macro implementation in /home/cgv/tensorflow/tensorflow/compiler/tests/build_defs.bzl:94:20\r\nERROR: Analysis of target '//tensorflow/contrib/resampler:resampler_ops_xla_test_gpu' failed; build aborted: Analysis of target '//tensorflow/contrib/resampler:resampler_ops_xla_test_gpu' failed; build aborted\r\n```\r\nI can open a new issue for this if you prefer that.", "I debugged a bit on this, looks like you also have to cherry-pick https://github.com/tensorflow/tensorflow/commit/8c22259497e9914c37a7adcd33aebaf754473a02 into r1.13, then the build should work with Bazel 0.21.0", "It's quite annoying the error message didn't report where `com_google_protobuf` is defined. In ./tensorflow/workspace.bzl it defines `com_google_protobuf` without using native `http_archive`, but the actual external repo comes from `rules_closure` transitively. That's why https://github.com/tensorflow/tensorflow/commit/8c22259497e9914c37a7adcd33aebaf754473a02 is needed. \r\n\r\nIs it possible to print where the external repo definition is in the error message? @aehlig", "I can confirm that cherry-picking 8c22259497e9914c37a7adcd33aebaf754473a02 fixes the protobuf issue with bazel 0.21.0. Thank you!", "@huljar \r\nClosing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!", "@jvishnuvardhan I have same issue as @huljar.\r\n\r\n> ERROR: /home/wmcnally/tensorflow/tensorflow/contrib/resampler/BUILD:103:1: in deps attribute of py_test rule //tensorflow/contrib/resampler:resampler_ops_xla_test_gpu: '//tensorflow/compiler/tf2xla/kernels:resampler_ops' does not have mandatory providers: 'py'. Since this rule was created by the macro 'tf_xla_py_test', the error might have been caused by the macro implementation in /home/wmcnally/tensorflow/tensorflow/compiler/tests/build_defs.bzl:96:20\r\nERROR: Analysis of target '//tensorflow/contrib/resampler:resampler_ops_xla_test_gpu' failed; build aborted\r\n\r\nI tried cherry-picking 8c22259 but it did not work (I'm very new to git so maybe I'm doing something wrong). \r\n", "Same issue as @wmcnally here. \r\n\r\n`ERROR: /tensorflow/tensorflow/contrib/resampler/BUILD:103:1: in deps attribute of py_test rule //tensorflow/contrib/resampler:resampler_ops_xla_test_gpu: '//tensorflow/compiler/tf2xla/kernels:resampler_ops' does not have mandatory providers: 'py'. Since this rule was created by the macro 'tf_xla_py_test', the error might have been caused by the macro implementation in /tensorflow/tensorflow/compiler/tests/build_defs.bzl:96:20\r\nERROR: Analysis of target '//tensorflow/contrib/resampler:resampler_ops_xla_test_gpu' failed; build aborted: Analysis of target '//tensorflow/contrib/resampler:resampler_ops_xla_test_gpu' failed; build aborted`\r\n\r\n[8c22259](https://github.com/tensorflow/tensorflow/commit/8c22259497e9914c37a7adcd33aebaf754473a02) seems to be integrated in WORKPLACE as of today, but I got the same error as above running bazel 0.21. Also failed with bazel 0.22 and 0.20.", "@Habardeen, same problem here. There is a separate issue to track this problem; I just commented there, see #25195.", "I see the same issue. ", "I solved it by removing the resampler from the BUILD file. That fixed the issue. Seems to be an issue in a BUILD file present in the resampler lib.", "Same issue with bazel 0.21 and 0.22", "This is ridiculous. I downgraded Bazel to 0.20 because of this issue, now it's giving me this error: \r\nhttps://github.com/grpc/grpc-web/issues/510 which is telling me to go upgrade bazel\r\nThis is the most widely used Machine Learnign API, CAN you please properly support C++ developers instead of catering to python. Why do we need to use python to build a C++ library that was coded in C++? Is it difficult to create a CMake project?"]}, {"number": 24999, "title": "chief_training_hooks for TPUEstimatorSpec", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.12\r\n- Are you willing to contribute it (Yes/No): Yes, very much!\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently, `training_chief_hooks` is not part of the [TPUEstimatorSpec API](https://www.tensorflow.org/api_docs/python/tf/contrib/tpu/TPUEstimatorSpec).\r\nHowever I saw that they are indeed used, when wrapping the user-supplied `model_fn`:\r\nhttps://github.com/tensorflow/tensorflow/blob/9331096d56002c7fcb8bae411684b8b78fc196c4/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py#L2755-L2764\r\nFrom looking at the code, I feel it would be a doable task to extend it in this direction.\r\nIn my use-case, I would like to provide an additional, custom `CheckpointSaverHook`.\r\n\r\n**Will this change the current api? How?**\r\nyes, extend it:\r\n``` python\r\nTPUEstimatorSpec(\r\n    mode,\r\n    predictions=None,\r\n    loss=None,\r\n    train_op=None,\r\n    eval_metrics=None,\r\n    export_outputs=None,\r\n    scaffold_fn=None,\r\n    host_call=None,\r\n    # new:\r\n    training_chief_hooks=None,\r\n    # end\r\n    training_hooks=None,\r\n    evaluation_hooks=None,\r\n    prediction_hooks=None\r\n)\r\n```\r\n\r\n**Who will benefit with this feature?**\r\nEveryone using TPUs and being a bit frustrated that the API is more restrictive than for more general-purpose accelerators (e.g. GPUs).\r\n\r\n**Any Other info.**\r\nnope", "comments": ["TPUEstimator uses in-graph replication with a single coordinator.\r\n\r\nAll hooks are executed on the coordinator node (your VM), so there's no distinction between chief hooks and non-chief hooks.\r\n\r\nFeel free to re-open if there's a subtlety I missed here.", "yes, I guess you are right. The reason behind this issue was the following:\r\nI wanted to create a `CheckpointSaverHook` which uses a custom `Saver`. The `Estimator` API would check if a `CheckpointSaverHook` has already been created and supplied, and only create a default `CheckpointSaverHook` if no one was found. In the `TPUEstimator` API, the `CheckpointSaverHook` is created always.\r\n\r\nBut eventually I found out that the problem lies somewhere in my custom `Saver` and not the way the `TPUEstimator` deals with the hooks."]}, {"number": 24998, "title": "TPUEstimatorSpec supports hooks or not?", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: 1.12\r\n- Doc Links:\r\n  - https://www.tensorflow.org/guide/using_tpu\r\n  - https://cloud.google.com/ml-engine/docs/tensorflow/using-tpus\r\n  - https://www.tensorflow.org/api_docs/python/tf/contrib/tpu/TPUEstimatorSpec\r\n\r\n\r\n**Describe the documentation issue**\r\n> The tf.train.SessionRunHook are unsupported, so these fields are omitted\r\n\r\nThe first link from above claims that no hooks are supported whatsover.\r\n> TPUEstimator handles many of the details of running on TPU devices, such as replicating inputs and models for each core, and returning to host periodically to run hooks.\r\n\r\nThe second link from above claims that they are indeed supported.\r\nThe documentation of the `TPUEstimatorSpec` itself does not mention anything about that.\r\n\r\nWhich of the two is now the case? Which hooks are supported (`training_hooks`, `evaluation_hooks`, `prediction_hooks`)?\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\nYes absolutely! If you clarify the current level of support, I gladly update the documentation!", "comments": ["@patzm, \r\nThe documentation of [TPU EstimatorSpec](https://www.tensorflow.org/api_docs/python/tf/compat/v1/estimator/tpu/TPUEstimatorSpec) has **`training_hooks`**, **`evaluation_hooks`** and **`prediction_hooks`** as arguments suggesting they are supported. \r\n\r\nCan you please confirm if we can close this issue, as the documentation is correct now? Thanks!"]}, {"number": 24997, "title": "Chief exit without waiting other workers when distributed training with ParameterServerStrategy", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):centos 7\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): compile using tags/v1.10.0\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): 0.15.0\r\n- CUDA/cuDNN version: cuda 9.1/cuDNN 7\r\n\r\n**Describe the current behavior**\r\nI'm using `tf.Estimator.train_and_evaluate` API and trying to make it work in a distributed environment. According to the doc, the configuration should be simple: set correct TF_CONFIG and pass the DistributeStrategy. \r\n\r\nI have 1chief, 2worker, 1ps, 1evaluator, the training starts with no error, everything seems right, but the training does not stop correctly. Based on my observation, when the training data is not balanced distributed among workers and chief, more specific, when the chief has fewer data. it will exit without waiting other workers. I draw a picture to show the problem:\r\n\r\n![xnip2019-01-21_18-33-16](https://user-images.githubusercontent.com/1506580/51469063-6275a100-1dab-11e9-94ef-5731357f5905.jpg)\r\n\r\n**Describe the expected behavior**\r\n\r\nbecause only the chief does the checkpoint, I think the chief must wait others finishing, otherwise the model is partially trained and evaluator will not stop.\r\n\r\n**Code to reproduce the issue**\r\n```\r\ndef create_estimator_and_specs(run_config):\r\n    model_params = tf.contrib.training.HParams(\r\n        batch_size=FLAGS.batch_size,\r\n        learning_rate=FLAGS.learning_rate)\r\n\r\n    estimator = tf.estimator.Estimator(\r\n        model_fn=model_fn,\r\n        config=run_config,\r\n        params=model_params\r\n    )\r\n\r\n    train_spec = tf.estimator.TrainSpec(\r\n        input_fn=generate_tfrecord_dataset(\r\n            mode=tf.estimator.ModeKeys.TRAIN,\r\n            directory=\"inputs\",\r\n            batch_size=FLAGS.batch_size,\r\n            train_size=FLAGS.train_size,\r\n            epoch=FLAGS.epoch),\r\n        max_steps=FLAGS.max_steps\r\n    )\r\n\r\n    eval_spec = tf.estimator.EvalSpec(\r\n        input_fn=generate_tfrecord_dataset(\r\n            mode=tf.estimator.ModeKeys.EVAL,\r\n            directory=\"inputs\",\r\n            batch_size=FLAGS.batch_size,\r\n            train_size=FLAGS.eval_size,\r\n            epoch=FLAGS.epoch),\r\n        throttle_secs=FLAGS.checkpoint_secs\r\n    )\r\n\r\n    return estimator, train_spec, eval_spec\r\n\r\n\r\ntrain_distribute = ParameterServerStrategy(num_gpus_per_worker=FLAGS.gpu_per_worker)\r\neval_distribute = MirroredStrategy(num_gpus_per_worker=FLAGS.gpu_per_worker,\r\n                                   cross_tower_ops=cross_tower_ops_lib.AllReduceCrossTowerOps())\r\nrun_config = tf.estimator.RunConfig(\r\n    model_dir=FLAGS.model_dir,\r\n    save_checkpoints_secs=FLAGS.checkpoint_secs,\r\n    save_summary_steps=FLAGS.summary_steps,\r\n    keep_checkpoint_max=FLAGS.max_checkpoints,\r\n    train_distribute=train_distribute,\r\n    eval_distribute=eval_distribute)\r\n\r\nestimator, train_spec, eval_spec = create_estimator_and_specs(run_config=run_config)\r\ntf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n```\r\n\r\nTF_CONFIG:\r\n```\r\nchief node:\r\n {u'cluster': {\r\n   u'ps': [u'ps:12615'], \r\n   u'chief': [u'chief:20396'], \r\n   u'worker': [u'worker1:18339', u'worker2:11609'], \r\n   u'evaluator': [u'evaluator:24352']\r\n   }, \r\n   u'task': {u'index': 0, u'type': u'chief'}\r\n }\r\n\r\n woker 0:\r\n {u'cluster': {\r\n   u'ps': [u'ps:12615'], \r\n   u'chief': [u'chief:20396'], \r\n   u'worker': [u'worker1:18339', u'worker2:11609'], \r\n   u'evaluator': [u'evaluator:24352']\r\n   }, \r\n   u'task': {u'index': 0, u'type': u'worker'}\r\n }\r\n\r\n worker 1:\r\n {u'cluster': {\r\n   u'ps': [u'ps:12615'], \r\n   u'chief': [u'chief:20396'], \r\n   u'worker': [u'worker1:18339', u'worker2:11609'],\r\n   u'evaluator': [u'evaluator:24352']\r\n   }, \r\n   u'task': {u'index': 1, u'type': u'worker'}\r\n }\r\n\r\nevaluator:\r\n{u'cluster': {\r\n  u'ps': [u'ps:12615'], \r\n  u'chief': [u'chief:20396'], \r\n  u'worker': [u'worker1:18339', u'worker2:11609'],\r\n  u'evaluator': [u'evaluator:24352']\r\n  }, \r\n  u'task': {u'index': 0, u'type': u'evaluator'}\r\n}\r\n```\r\n\r\n\r\n**Other info / logs**\r\nlog screenshot\r\n![image](https://user-images.githubusercontent.com/1506580/51469382-3575be00-1dac-11e9-9dc7-9e183d3da095.png)\r\n\r\nI did post [a stackoverflow question](https://stackoverflow.com/questions/54252279/estimator-api-training-data-is-unbalance-split-among-workers-chief-may-finish) but no response so far. Please help.", "comments": ["@yuefengz sorry to trouble you, can you help?", "If you make chief wait for other workers, you'll partially lose the benefit of async training which is fault-tolerance of workers. Could you just give chief more data to train?", "@yuefengz Thanks for your reply. I am confused about the stop condition also, please correct me.\r\n\r\nIn my case(data splited among workers), I noticed chief and workers are printing steps while training. In chief node,  it printed as global_step, in worker node, it printed as step.\r\n\r\n1. Are these steps referring to the same thing? I mean, do these steps all get from param server, or just local step maintain by themselves?\r\n2. In the `train_and_eval` api document, it says the only stop condition is `trainSpec.max_step` arrives.  So, which step is used to compare against max_step. the `global_step` or `woker_local_step`?\r\n\r\nif there is a global_step and tensorflow use global_step against max_step to check stop condition, the chief won't quit earlier, and user can simply set `max_step` to `total_record_cnt/batch`. but based on my observation, none of the workers can reach `total_record_cnt/batch` then quit, because it not has enough data. So I am wondering that, each worker including chief are using local step for stop checking. Am I right? And we are using one `max_step` among unbalanced workers, it is not only early quit chief issue may happen, but also unutilized data\uff1f", "problem reproduced in tf version 1.10.0,but no problem with 1.13.0, maybe it is a bug of tf version 1.10.0", "Please re-open if the issue persists in newer versions of tensorflow.", "@wangdejian @isaprykin \r\nThis problem still exists in tf 2.5 (chief exits before worker finishes)\r\n\r\nYou can verify this using the two files pasted at the end. \r\n1. Run `bash train.sh`\r\n2. Run `ps aux | grep chief_exit`, and you will see processes for chief, worker, etc\r\n3. Monitor the ~/chief_log and ~/worker_log, when ~/chief_log stops updating, Run `ps aux | grep chief_exit` again, and you will see the chief process is gone, while the worker process is still there.\r\n\r\nNote: The worker trains on twice the amount of data of the chief, and that's why chief finishes first\r\n\r\nmodel_chief_exit.py\r\n```python\r\n# -*- coding: utf-8 -*-\r\nimport os, json\r\nimport tensorflow.compat.v1 as tf\r\ntf.disable_eager_execution()\r\nfrom tensorflow.feature_column import categorical_column_with_hash_bucket, embedding_column\r\nimport argparse\r\n\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument(\"-dr\", \"--distributed_role\", help=\"specify destributed role, chief, evaluator, worker or ps\", type=str, default=\"chief\")\r\nargs = parser.parse_args()\r\n\r\nos.environ[\"TF_CONFIG\"] = json.dumps({\r\n    \"cluster\": {\r\n        \"chief\": [\"localhost:15000\"],\r\n        \"worker\": [\"localhost:15001\"],\r\n        \"ps\": [\"localhost:15002\"],\r\n        \"evaluator\": [\"localhost:15003\"]\r\n    },\r\n    \"task\": {\"type\": args.distributed_role, \"index\": 0}\r\n})\r\n\r\ndef input_fn():\r\n    if args.distributed_role == 'worker':\r\n        ds = tf.data.Dataset.range(10000000)\r\n    else:\r\n        ds = tf.data.Dataset.range(5000000)\r\n    ds = ds.map(lambda x: ({'video_id':x}, x % 2)).batch(1024)\r\n    return ds\r\n\r\nif __name__ == '__main__':\r\n    cat_column = categorical_column_with_hash_bucket('video_id', hash_bucket_size=400000, dtype=tf.int64)\r\n    emb_column = embedding_column(cat_column, dimension=64, combiner='sum', use_safe_embedding_lookup=False)\r\n\r\n    strategy = tf.distribute.experimental.ParameterServerStrategy()\r\n    config = tf.estimator.RunConfig(\r\n        save_checkpoints_steps=10000,\r\n        train_distribute=strategy,\r\n        eval_distribute=None\r\n        )\r\n    cls = tf.estimator.DNNClassifier(hidden_units=[512,256], feature_columns=[emb_column], model_dir=\"/tmp/model2/\", config=config)\r\n\r\n    train_spec = tf.estimator.TrainSpec(input_fn=input_fn)\r\n    eval_spec = tf.estimator.EvalSpec(input_fn=input_fn)\r\n    tf.estimator.train_and_evaluate(\r\n        cls,\r\n        train_spec,\r\n        eval_spec\r\n    )\r\n\r\n```\r\n\r\ntrain.sh\r\n```bash\r\nnohup python model_chief_exit.py -dr ps &> ~/ps_log &\r\nnohup python model_chief_exit.py -dr worker &> ~/worker_log &\r\nnohup python model_chief_exit.py -dr chief &> ~/chief_log &\r\nnohup python model_chief_exit.py -dr evaluator &> ~/eval_log &\r\n```"]}, {"number": 24996, "title": "[XLA] Data transfer - don't dump the whole literal into VLOG(1)", "body": "I think this is a bit excessive for VLOG(1), especially with big graphs, and should go lower.", "comments": []}, {"number": 24995, "title": "Tensorflow does not build in a python3", "body": "**Ubuntu 18.04LTS** \r\n**TensorFlow installed from (source or binary):**\r\nSource\r\n**TensorFlow version (use command below):**\r\n1.12.0\r\n**Python version:**\r\n3.6.7\r\n**Bazel version (if compiling from source):**\r\n0.19.0\r\n\r\n**Source code / logs**\r\n\r\n> ERROR: /home/sonfire/tensorflow/tensorflow/core/BUILD:2534:1: Executing genrule //tensorflow/core:version_info_gen failed (Exit 127)\r\n> /usr/bin/env: 'python': No such file or directory\r\n> Target //tensorflow/tools/pip_package:build_pip_package failed to build\r\n> Use --verbose_failures to see the command lines of failed build steps.\r\n> INFO: Elapsed time: 595.106s, Critical Path: 14.90s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]\r\n> INFO: 438 processes: 438 local.\r\n> FAILED: Build did NOT complete successfully\r\n> \r\n\r\nI don`t used env", "comments": ["make sure python path is in $PATH\r\n\r\nand if you only have python3, link python3 to python by this command\r\n`ln -s your_python_path /usr/local/bin/python`", "\r\nI managed everything in env.\r\nAnd it seems to be compiled, maybe"]}, {"number": 24994, "title": "Add details for running tests from nightly build images. Closes #24993.", "body": "The current documentation guidelines throw an error after running the\r\ncommands in the tensorflow nightly build container. Following this\r\nMagenta issue:\r\n\r\nhttps://github.com/tensorflow/magenta/issues/1232\r\n\r\nrunning the test requires changing directory. Rather than changing the\r\nWORKDIR in the image, which would probably affect many uses and users,\r\nthis change adds the direction of changing directory after mentioning\r\nthe nightly build image.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "I am curious how adding two lines in the documentation causes all internal CI builds to fail.For example, the Invocation Log for `GPU Python3` ends with\r\n\r\n```\r\nFAILED: Build did NOT complete successfully\r\nExecuted 0 out of 278 tests: 278 fail to build.\r\n```\r\n\r\nI have planned pull requests for code, in which case I will be unable to tell if the checks are failing because of an internal error or because of my changes. What can I do differently to understand the source of the failed checks?"]}]