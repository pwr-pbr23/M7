[{"number": 54435, "title": "Module structure not recognized by Visual Studio Code Linter", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 11 10.0.22000\r\n- TensorFlow installed from (source or binary): through pip?\r\n- TensorFlow version (use command below): 2.8.0\r\n- Python version: 3.10.2\r\n\r\nI am very perplexed by tensorflow's module structure.  I recently started using tensorflow in Visual Studio Code and immediately ran into a problem where imports from tensorflow.keras cannot be resolved by Pylance.  For example, the \"layers\" module is not recognized from the line `from tensorflow.keras import layers`.  Interestingly enough, the code runs fine despite this error, but the lack of support from the linter makes writing code very difficult.  (For reference, I am using the \"Probabilistic Bayesian Neural Networks\" example script.)\r\n\r\nMy attempts to fix this issue led to a number of other discoveries which still have me confused:\r\n\r\n- Using `from tensorflow import keras` is recognized by the linter, but the linter still can't offer any helpful predictions off of the keras module.  In this instance, replacing references to `layers` with `keras.layers` still work, despite no indication from the linter that they should.  \r\n- Importing keras directly (2.8.0) instead of through tensorflow is recognized, and `keras.layers` is still valid, but hints from linting are still not present, and other parts of the code will break unexpectedly.  For example, a call to `keras.optimizers.RMSprop`, is invalid even though `keras.optimizers` is recognized and both are recognized if keras is imported through tensor flow.\r\n- Importing keras through tensorflow.python behaves similarly to directly importing keras.\r\n- Even though `keras.layers` is not recognized by the linter after importing just keras, the linter does recognize `from keras import layers` and offers completions for layers after the fact.\r\n- `from tensorflow import keras` was recognized in tensorflow 2.7.0.  This issue only started when I updated.\r\n\r\nAdmittedly, my understanding of packaging and linting in Python is somewhat limited, but I've never had issues like this with any other package I've worked with.  Am I missing something obvious here?  Is this a known issue?  If this kind of structure is intentional, what is the rationale behind it?  Are there known workarounds/resources that I could use to better understand this issue?\r\n\r\nThanks in advance for the help.\r\n", "comments": ["Check:\r\nhttps://github.com/microsoft/pylance-release/issues/1941#issuecomment-1022601065\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/54104", "@bmorledge-hampton19 ,\r\nThe issue will move to closed status once the PR is merged.", "Thanks for the links. They are very helpful. My apologies for rehashing an issue that is clearly already recognized.", "Hope the pr gets merged soon that linter error it's driving me nuts", "> Thanks for the links. They are very helpful. My apologies for rehashing an issue that is clearly already recognized.\n\nDon't apologize, the issue has been around since TF 2.0 release and nothing was ever done about it, it is unacceptable that every new release breaks PyLance.\nI am not saying that this is either TF or PyLance's fault, but that the larger part of the community probably using VSCode, the support for it should be taken more seriously IMO.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54435\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54435\">No</a>\n"]}, {"number": 54434, "title": "[INTEL oneDNN] Added BF16 support for Inv and InvGrad", "body": "This PR will add BF16 support for Inv Op and InvGrad Op", "comments": ["@rohan100jain Any update? ", "Adding bf16 support will help any models using Inv and InvGrad instead of Reciprocal, mainly old models. Some of the older models use Inv and InvGrad", "Unfortunately I am not the one who can accept this change.\r\n\r\n@rohan100jain, can you please decide whether this change is ok? Or find someone else to approve?", "@rohan100jain Any update on this PR? ", "Any update @rohan100jain ?"]}, {"number": 54433, "title": "Use tensor create function for cudnn frontend APIs", "body": "This PR uses a unified function to take care of tensor creation for cudnn frontend APIs.\r\n\r\nAlso, cleanup some used functions for the cudnn frontend.\r\n\r\n\r\ncc. @nluehr ", "comments": ["Rebased. One question about the uid array: I saw in the PR https://github.com/tensorflow/tensorflow/pull/53843, my changes of kUnfusedConvUid and kFusedConvUid were removed when merging. I created them in the hope of removing the potential of inconsistency of tensor marking and op graph creation. Without them, we need to use the {'x', 'w', 'y'} or {'x', 'w', 'z', 'b', 'y'} in multiple places, including this PR https://github.com/tensorflow/tensorflow/pull/53938. Also, in this PR, we need to make sure those characters are copied correctly when creating tensors.", "There were code review comments on the import of that PR that I ended up addressing myself.  The main point was that it's much less human-readable to refer to tensors as `kUnfusedConvUid[3]`, and since the graph construction code still ends up using opaque integers to refer to tensor IDs, the constants don't really give much additional protection, just trading somewhat-intuitive char names for meaningless integer names -- without the constants, there's a risk of permuting the tensors when constructing the runners, but with them, there's an equal risk of permuting the tensors by using the wrong indices in graph construction.  The risk of a bug here is negligible anyway, since a name mismatch would be caught by any single use of the affected op in any test, so human readability took precedence.", "Yes, that makes sense. Thanks for the clarification. @awpr ", "I have rebased the changes. PTAL. @awpr ", "Just rebased to resolve a reported conflict."]}, {"number": 54432, "title": "Add appropriate dtype check for `tf.boolean_mask`'s mask", "body": "This PR tries to address the issue raised in #54412 where\r\nmask's dtype was checked in tf.boolean_mask and an invalid\r\nresult has been returned instead.\r\n\r\nThis PR fixes #54412.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 54431, "title": "[INTEL oneDNN] Added bf16 support for Inv and InvGrad", "body": null, "comments": []}, {"number": 54429, "title": "Add appropriate error check for nbins in `tf.histogram_fixed_width_bins`", "body": "This PR tries to address the issue raised in #54415 where\r\nnbins was not checked for tf.histogram_fixed_width_bins\r\nand an incorrect result was returned when nbins < 0.\r\n\r\nThis PR fixes #54415.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 54428, "title": "[oneDNN] Fuse Softplus + Tanh + Mul to Mish", "body": "Add support for fusing Softplus + Tanh + Mul to MISH op", "comments": []}, {"number": 54427, "title": "Implementation of the converter for Tile operation", "body": "- ConvertTile operator is implemented so that both parameters (tensor and multiplier) can be passed as tensors and weights. A unit test with 6 tests and all combinations of passed parameters is provided.\r\n- New macro `#define CHECK_INPUT_SIZE (size, exp_size, node_def)` unifies the testing of the number of parameters passed. It is used in 8 different places.\r\n- The helper function template void `AdjustVectorByDims(...)` makes writing tests much easier (especially tests with parameters that can be passed as tensors and weights). For example, today we can use some parameters like tensors with predefined dimensions, but with no real numbers. An attempt to pass the same parameters with the same values as weights leads to the crash. We see a similar crash when the vector providing the values assigned to the tensor is not empty, but its length does not match the number of values defined by the dimensions. `AdjustVectorByDims` fixes these two problems and is called by default in the situations described, but these calls can also be blocked when such failures are intentional.\r\n", "comments": ["This is re-created from https://github.com/tensorflow/tensorflow/pull/53718", "@bixia1 : I'm done with your last request. I think this PR is ready to merge.", "@bixia1 : I just fixed the Ubuntu Sanity check error. ", "@bixia1 : Finally, all checks were successful, and it looks like the merge can be done.", "Seems auto-merge is not happening but the changes are merged into master now, so we can close this. Thank you for the PR.\r\n"]}, {"number": 54426, "title": "Add complex support for tf.math.atan", "body": "Fixes https://github.com/tensorflow/tensorflow/issues/54411", "comments": ["Seems auto-merge is not happening but the changes are now committed."]}, {"number": 54424, "title": "Implementation of the converter for Tile operation", "body": "- ConvertTile operator is implemented so that both parameters (tensor and multiplier) can be passed as tensors and weights. A unit test with 6 tests and all combinations of passed parameters is provided.\r\n- New macro `#define CHECK_INPUT_SIZE (size, exp_size, node_def)` unifies the testing of the number of parameters passed. It is used in 8 different places.\r\n- The helper function template void `AdjustVectorByDims(...)` makes writing tests much easier (especially tests with parameters that can be passed as tensors and weights). For example, today we can use some parameters like tensors with predefined dimensions, but with no real numbers. An attempt to pass the same parameters with the same values as weights leads to the crash. We see a similar crash when the vector providing the values assigned to the tensor is not empty, but its length does not match the number of values defined by the dimensions. `AdjustVectorByDims` fixes these two problems and is called by default in the situations described, but these calls can also be blocked when such failures are intentional.\r\n\r\n", "comments": ["This is moved to https://github.com/tensorflow/tensorflow/pull/53718.", "@drivanov I don't understand what has happened, but it looks like I closed the PR by accident and I couldn't figure out a way to reopen it. Can you please help?", "@bixia1: According to [this](https://github.com/gitbucket/gitbucket/issues/378) it should be possible when **the author** of PR accidentally closes it. Please, take a look at the last group of comments from this link. If you WILL NOT find the **Reopen PR** button, I will create a new PR.", "@bixia1 : Please, ignore my previous message. I just created a new [PR#54427](https://github.com/tensorflow/tensorflow/pull/54427)"]}, {"number": 54423, "title": "tensorflow-gpu doesn't pack CUDA and cuDNN anymore through conda-forge", "body": "**System information**\r\n- OS Platform and Distribution : Windows 10\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version: 2.3.0\r\n- Python version: 3.8.12\r\n- Installed using virtualenv? pip? conda?: conda\r\n- CUDA/cuDNN version: 10.1/7.6\r\n- GPU model and memory: NVIDIA Quadro P400, 2Gb\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nA few months ago, I installed TensorFlow on a machine through conda-forge. After running into some issues installing it at the system level, I found the simplest solution was to install the tensorflow-gpu package in a conda environment. \r\nThat meta-package came in with cudatoolkit and cudnn, and it worked out of the box.\r\n\r\nHowever, as I tried to replicate that install on another machine, I found that the list of packages provided when trying to install tensorflow-gpu through conda-forge did not contain cudatoolkit or cudnn. \r\n\r\nI also tried again on the machine where I had a successful install, in another environment, and the issue persisted.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n`conda install tensorflow-gpu==2.3.0 -c conda-forge`\r\n\r\nconda outputs the following : \r\n\r\n```\r\nThe following NEW packages will be INSTALLED:\r\n\r\n  _tflow_select      pkgs/main/win-64::_tflow_select-2.3.0-gpu\r\n  absl-py            conda-forge/noarch::absl-py-1.0.0-pyhd8ed1ab_0\r\n  aiohttp            conda-forge/win-64::aiohttp-3.8.1-py38h294d835_0\r\n  aiosignal          conda-forge/noarch::aiosignal-1.2.0-pyhd8ed1ab_0\r\n  astor              conda-forge/noarch::astor-0.8.1-pyh9f0ad1d_0\r\n  astunparse         conda-forge/noarch::astunparse-1.6.3-pyhd8ed1ab_0\r\n  async-timeout      conda-forge/noarch::async-timeout-4.0.2-pyhd8ed1ab_0\r\n  attrs              conda-forge/noarch::attrs-21.4.0-pyhd8ed1ab_0\r\n  blinker            conda-forge/noarch::blinker-1.4-py_1\r\n  brotlipy           conda-forge/win-64::brotlipy-0.7.0-py38h294d835_1003\r\n  cachetools         conda-forge/noarch::cachetools-5.0.0-pyhd8ed1ab_0\r\n  cffi               conda-forge/win-64::cffi-1.15.0-py38hd8c33c5_0\r\n  charset-normalizer conda-forge/noarch::charset-normalizer-2.0.12-pyhd8ed1ab_0\r\n  click              conda-forge/win-64::click-8.0.3-py38haa244fe_1\r\n  colorama           conda-forge/noarch::colorama-0.4.4-pyh9f0ad1d_0\r\n  cryptography       conda-forge/win-64::cryptography-36.0.1-py38hb7941b4_0\r\n  frozenlist         conda-forge/win-64::frozenlist-1.3.0-py38h294d835_0\r\n  gast               conda-forge/noarch::gast-0.4.0-pyh9f0ad1d_0\r\n  google-auth        conda-forge/noarch::google-auth-2.6.0-pyh6c4a22f_1\r\n  google-auth-oauth~ conda-forge/noarch::google-auth-oauthlib-0.4.6-pyhd8ed1ab_0\r\n  google-pasta       conda-forge/noarch::google-pasta-0.2.0-pyh8c360ce_0\r\n  grpcio             conda-forge/win-64::grpcio-1.43.0-py38he5377a8_0\r\n  h5py               conda-forge/win-64::h5py-2.10.0-nompi_py38he6c2248_106\r\n  hdf5               conda-forge/win-64::hdf5-1.10.6-nompi_h5268f04_1114\r\n  idna               conda-forge/noarch::idna-3.3-pyhd8ed1ab_0\r\n  importlib-metadata conda-forge/win-64::importlib-metadata-4.11.1-py38haa244fe_0\r\n  intel-openmp       conda-forge/win-64::intel-openmp-2022.0.0-h57928b3_3663\r\n  keras-applications conda-forge/noarch::keras-applications-1.0.8-py_1\r\n  keras-preprocessi~ conda-forge/noarch::keras-preprocessing-1.1.2-pyhd8ed1ab_0\r\n  krb5               conda-forge/win-64::krb5-1.19.2-h20d022d_3\r\n  libblas            conda-forge/win-64::libblas-3.9.0-13_win64_mkl\r\n  libcblas           conda-forge/win-64::libcblas-3.9.0-13_win64_mkl\r\n  libcurl            conda-forge/win-64::libcurl-7.81.0-h789b8ee_0\r\n  liblapack          conda-forge/win-64::liblapack-3.9.0-13_win64_mkl\r\n  libprotobuf        conda-forge/win-64::libprotobuf-3.19.4-h7755175_0\r\n  libssh2            conda-forge/win-64::libssh2-1.10.0-h680486a_2\r\n  libzlib            conda-forge/win-64::libzlib-1.2.11-h8ffe710_1013\r\n  m2w64-gcc-libgfor~ conda-forge/win-64::m2w64-gcc-libgfortran-5.3.0-6\r\n  m2w64-gcc-libs     conda-forge/win-64::m2w64-gcc-libs-5.3.0-7\r\n  m2w64-gcc-libs-co~ conda-forge/win-64::m2w64-gcc-libs-core-5.3.0-7\r\n  m2w64-gmp          conda-forge/win-64::m2w64-gmp-6.1.0-2\r\n  m2w64-libwinpthre~ conda-forge/win-64::m2w64-libwinpthread-git-5.0.0.4634.697f757-2\r\n  markdown           conda-forge/noarch::markdown-3.3.6-pyhd8ed1ab_0\r\n  mkl                conda-forge/win-64::mkl-2022.0.0-h0e2418a_796\r\n  msys2-conda-epoch  conda-forge/win-64::msys2-conda-epoch-20160418-1\r\n  multidict          conda-forge/win-64::multidict-6.0.2-py38h294d835_0\r\n  numpy              conda-forge/win-64::numpy-1.22.2-py38hcf66579_0\r\n  oauthlib           conda-forge/noarch::oauthlib-3.2.0-pyhd8ed1ab_0\r\n  opt_einsum         conda-forge/noarch::opt_einsum-3.3.0-pyhd8ed1ab_1\r\n  protobuf           conda-forge/win-64::protobuf-3.19.4-py38h885f38d_0\r\n  pyasn1             conda-forge/noarch::pyasn1-0.4.8-py_0\r\n  pyasn1-modules     conda-forge/noarch::pyasn1-modules-0.2.7-py_0\r\n  pycparser          conda-forge/noarch::pycparser-2.21-pyhd8ed1ab_0\r\n  pyjwt              conda-forge/noarch::pyjwt-2.3.0-pyhd8ed1ab_1\r\n  pyopenssl          conda-forge/noarch::pyopenssl-22.0.0-pyhd8ed1ab_0\r\n  pyreadline         conda-forge/win-64::pyreadline-2.1-py38haa244fe_1005\r\n  pysocks            conda-forge/win-64::pysocks-1.7.1-py38haa244fe_4\r\n  python_abi         conda-forge/win-64::python_abi-3.8-2_cp38\r\n  pyu2f              conda-forge/noarch::pyu2f-0.1.5-pyhd8ed1ab_0\r\n  requests           conda-forge/noarch::requests-2.27.1-pyhd8ed1ab_0\r\n  requests-oauthlib  conda-forge/noarch::requests-oauthlib-1.3.1-pyhd8ed1ab_0\r\n  rsa                conda-forge/noarch::rsa-4.8-pyhd8ed1ab_0\r\n  scipy              conda-forge/win-64::scipy-1.8.0-py38ha1292f7_1\r\n  six                conda-forge/noarch::six-1.16.0-pyh6c4a22f_0\r\n  tbb                conda-forge/win-64::tbb-2021.5.0-h2d74725_0\r\n  tensorboard        conda-forge/noarch::tensorboard-2.8.0-pyhd8ed1ab_1\r\n  tensorboard-data-~ conda-forge/win-64::tensorboard-data-server-0.6.0-py38haa244fe_1\r\n  tensorboard-plugi~ conda-forge/noarch::tensorboard-plugin-wit-1.8.1-pyhd8ed1ab_0\r\n  tensorflow         pkgs/main/win-64::tensorflow-2.3.0-mkl_py38h8557ec7_0\r\n  tensorflow-base    pkgs/main/win-64::tensorflow-base-2.3.0-eigen_py38h75a453f_0\r\n  tensorflow-estima~ conda-forge/noarch::tensorflow-estimator-2.5.0-pyh81a9013_1\r\n  tensorflow-gpu     pkgs/main/win-64::tensorflow-gpu-2.3.0-he13fc11_0\r\n  termcolor          conda-forge/noarch::termcolor-1.1.0-py_2\r\n  tk                 conda-forge/win-64::tk-8.6.12-h8ffe710_0\r\n  typing-extensions  conda-forge/noarch::typing-extensions-4.1.1-hd8ed1ab_0\r\n  typing_extensions  conda-forge/noarch::typing_extensions-4.1.1-pyha770c72_0\r\n  urllib3            conda-forge/noarch::urllib3-1.26.8-pyhd8ed1ab_1\r\n  werkzeug           conda-forge/noarch::werkzeug-2.0.3-pyhd8ed1ab_1\r\n  win_inet_pton      conda-forge/win-64::win_inet_pton-1.1.0-py38haa244fe_3\r\n  wrapt              conda-forge/win-64::wrapt-1.13.3-py38h294d835_1\r\n  yarl               conda-forge/win-64::yarl-1.7.2-py38h294d835_1\r\n  zipp               conda-forge/noarch::zipp-3.7.0-pyhd8ed1ab_1\r\n  zlib               conda-forge/win-64::zlib-1.2.11-h8ffe710_1013\r\n```\r\n\r\nThen, running those lines : \r\n\r\n```\r\nimport tensorflow as tf\r\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\r\n```\r\n\r\nOutputs the following : \r\n\r\n```\r\n2022-02-17 18:50:41.041229: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\r\n2022-02-17 18:50:41.046722: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n2022-02-17 18:50:46.483787: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\r\n2022-02-17 18:50:46.490003: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cublas64_11.dll'; dlerror: cublas64_11.dll not found      \r\n2022-02-17 18:50:46.496485: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cublasLt64_11.dll'; dlerror: cublasLt64_11.dll not found  \r\n2022-02-17 18:50:46.506046: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cusolver64_11.dll'; dlerror: cusolver64_11.dll not found\r\n2022-02-17 18:50:46.512642: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cusparse64_11.dll'; dlerror: cusparse64_11.dll not found  \r\n2022-02-17 18:50:46.519233: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found\r\n2022-02-17 18:50:46.524611: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.       \r\nSkipping registering GPU devices...\r\nNum GPUs Available:  0\r\n```\r\n", "comments": ["Looks like you have to be careful you match the correct Nvidia drivers, could be an issue? https://www.tensorflow.org/install/gpu", "@BptGrm ! Could you please check with above [comment](https://github.com/tensorflow/tensorflow/issues/54423#issuecomment-1043549196) ?", "> Looks like you have to be careful you match the correct Nvidia drivers, could be an issue? https://www.tensorflow.org/install/gpu\r\n\r\nThe drivers I have installed can support CUDA 11.2. The TensorFlow install I'm looking for is compatible with CUDA 10.1.\r\nIs there a problem with retrocompatibility that would require downgrading my GPU drivers ?\r\n\r\nAlso, as I stated before, even on the machine that already has a functional TF install, and thus presumably correct drivers, conda-forge doesn't provide cudatoolkit or cudnn either.", "Please try with \"conda install -c anaconda cudatoolkit \" command . If that does not resolve the issue please check with instructions from [here ](https://www.tensorflow.org/install/pip) with TF 2.8 after activating Conda environment?", "The GPU still isn't recognized, and cuDNN still isn't installed either through conda with cudatoolkit or pip with tensorflow.", "@BptGrm,\r\nFollow these steps to install Tensorflow-gpu version with CUDA and cuDNN using Conda package \r\n\r\n```\r\n#Create virtual environment \r\nconda create --name tf_gpu\r\n#Activate environment \r\nconda activate tf_gpu\r\n#Install Python \r\nconda install python==3.9\r\n#Install CUDA\r\nconda install -c anaconda cudatoolkit=11.2\r\n#Install cuDNN\r\nconda install cudnn=8.1\r\n#Install Tensorflow-gpu =~2.8\r\npip install tensorflow==2.8\r\n```", "I have been able to install TF like this previously. \r\nMy issue was that those packages used to be wrapped in a single metapackage that, if I remember correctly, would install the correct verions of everything needed. \r\nIt was, in my opinion, an easier and safer way to install TF, and I was wondering why it isn't available anymore.", "@BptGrm, It used be earlier. When all the compatible packages release happen at the same time, in that case we no need to explicitly mention any of the versions. Binary installer takes the latest stable version. Since when the CUDA and cuDNN and Tensorflow compatibility is required, we need to mention each package version explicitly. Thanks!  ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54423\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54423\">No</a>\n"]}, {"number": 54421, "title": "[oneDNN] BN perf fix from oneDNN - Not For Merge", "body": null, "comments": []}, {"number": 54420, "title": "tflite inference on single image confidence scores.", "body": "I am fairly new to tensorflow and I have a tflite model which needs inference on a single image (ie no datasets).  When I run the inference on the quantised model, I out output confidence scores in the range of [0,255] as uint8.\r\n\r\nHowever, when I do the inference on batch I get scores in the [0,1] float range.\r\n\r\nI am a bit puzzled why this is. For example, if you see this post: https://thinkmobile.dev/testing-tensorflow-lite-image-classification-model/ you can see that the model results are in the 0-1 range, but, if you do it over a single image like so: https://towardsdev.com/custom-image-classification-model-using-tensorflow-lite-model-maker-68ee4514cd45 the results are in [0-255] int range.\r\n\r\nBit perplexed as to why this is.", "comments": ["@abaqusstudent ,\r\nWe see that the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced] or if possible share a colab gist with the issue reported.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 54419, "title": "Check is redudant, use `state_->code`", "body": "Since `state_` must not be `nullptr` in this branch. We can use `state_->code` directly instead of `code()` which checks `state_` again", "comments": ["@rohan100jain Can you please review this PR ? Thank you!"]}, {"number": 54418, "title": "cmake with DTFLITE_KERNEL_TEST=on in ubuntu reports errors", "body": "Platform: Ubuntu 20.04\r\nSteps to reproduce:\r\n1. git clone https://github.com/tensorflow/tensorflow\r\n2. mkdir tflite_build & cd tflite_build\r\n3. cmake ../tensorflow/tensorflow/lite -DCMAKE_BUILD_TYPE=Debug -DTFLITE_KERNEL_TEST=on -DTFLITE_ENABLE_GPU=ON\r\nBelow errors are printed:\r\n```\r\nCMake Error at kernels/CMakeLists.txt:145 (add_executable):\r\nCannot find source file:\r\n\r\n\r\n\r\nrandom_uniform_test.cc\r\n\r\n\r\n\r\nTried extensions .c .C .c++ .cc .cpp .cxx .cu .m .M .mm .h .hh .h++ .hm\r\n.hpp .hxx .in .txx\r\nCall Stack (most recent call first):\r\nkernels/CMakeLists.txt:312 (add_kernel_test)\r\n\r\n\r\n\r\n\r\nCMake Error at kernels/CMakeLists.txt:145 (add_executable):\r\nCannot find source file:\r\n\r\n\r\n\r\nrandom_standard_normal_test.cc\r\n\r\n\r\n\r\nTried extensions .c .C .c++ .cc .cpp .cxx .cu .m .M .mm .h .hh .h++ .hm\r\n.hpp .hxx .in .txx\r\nCall Stack (most recent call first):\r\nkernels/CMakeLists.txt:312 (add_kernel_test)\r\n\r\n\r\n\r\n\r\nCMake Error at kernels/CMakeLists.txt:145 (add_executable):\r\nNo SOURCES given to target: random_uniform_test\r\nCall Stack (most recent call first):\r\nkernels/CMakeLists.txt:312 (add_kernel_test)\r\n\r\n\r\n\r\n\r\nCMake Error at kernels/CMakeLists.txt:145 (add_executable):\r\nNo SOURCES given to target: random_standard_normal_test\r\nCall Stack (most recent call first):\r\nkernels/CMakeLists.txt:312 (add_kernel_test)\r\n\r\n\r\n\r\n\r\nCMake Generate step failed. Build files cannot be regenerated correctly.\r\n```\r\nIf I do below changes, it works:\r\n```\r\ndiff --git a/tensorflow/lite/kernels/CMakeLists.txt b/tensorflow/lite/kernels/CMakeLists.txt\r\nindex daa390fa93c..67ccb896896 100644\r\n--- a/tensorflow/lite/kernels/CMakeLists.txt\r\n+++ b/tensorflow/lite/kernels/CMakeLists.txt\r\n@@ -252,8 +252,6 @@ set(TEST_WITH_EXTERNAL_MAIN_LIST\r\n   pow_test.cc\r\n   quant_basic_lstm_test.cc\r\n   quantize_test.cc\r\n-  random_standard_normal_test.cc\r\n-  random_uniform_test.cc\r\n   range_test.cc\r\n   rank_test.cc\r\n   reduce_test.cc\r\n@@ -323,4 +321,4 @@ if(${CMAKE_CROSSCOMPILING})\r\n     ${CMAKE_CURRENT_BINARY_DIR}/run-tests.cmake\r\n     COPYONLY\r\n   )\r\n```\r\nHowever, in the next build step `cmake --build . -j8`. I will meet many build failures.\r\n\r\nAnd I am a newbie for tflite gpu. I want to be familiar with tflite gpu gl backend. So could you help me how to run the kernel tests on ubuntu with gpu-gl backend? Which target should I build? ", "comments": ["Hi @qjia7 ! Can you try the instruction in this [thread](https://stackoverflow.com/a/43401929/11530462)?", "> Hi @qjia7 ! Can you try the instruction in this [thread](https://stackoverflow.com/a/43401929/11530462)?\r\n\r\nThese two files `random_standard_normal_test.cc` and `random_uniform_test.cc` even don't exist in current source code. I don't think it can work.", "Hi @gadagashwini ! Could you please look at this issue ? It is replicating in [Colab](https://colab.sandbox.google.com/gist/mohantym/f80a3e1eeb6766a6b611082833a1462e/github_54418.ipynb#scrollTo=HNZm6QrgS3bf) environment too.", "@mohantym @gadagashwini  Is there way to run `benchmark_model` with gpu gl backend on linux? I tried below commands:\r\n```\r\ncmake ../tensorflow/tensorflow/lite -DCMAKE_BUILD_TYPE=Debug -DTFLITE_ENABLE_GPU=ON\r\ncmake --build . -j8 -t benchmark_model\r\n./tools/benchmark/benchmark_model --graph=../mobilenet_v2_1.0_224_1_metadata_1.tflite --use_gpu=true --gpu_backend=gl\r\n```\r\nHowever below error is reported:\r\n```\r\nSTARTING!\r\nDuplicate flags: num_threads\r\nLog parameter values verbosely: [0]\r\nGraph: [../mobilenet_v2_1.0_224_1_metadata_1.tflite]\r\nUse gpu: [1]\r\nLoaded model ../mobilenet_v2_1.0_224_1_metadata_1.tflite\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite delegate for GPU.\r\nGPU delegate created.\r\nINFO: Replacing 66 node(s) with delegate (TfLiteGpuDelegateV2) node, yielding 1 partitions.\r\nERROR: Can not open OpenCL library on this device - libOpenCL.so: cannot open shared object file: No such file or directory\r\nERROR: Falling back to OpenGL\r\nERROR: TfLiteGpuDelegate Init: OpenGL-based API disabled\r\nINFO: Created 0 GPU delegate kernels.\r\nERROR: TfLiteGpuDelegate Prepare: delegate is not initialized\r\nERROR: Node number 66 (TfLiteGpuDelegateV2) failed to prepare.\r\nERROR: Restored original execution plan after delegate application failure.\r\nFailed to apply GPU delegate.\r\nBenchmarking failed.\r\n```\r\nI checked that `OpenGL-based API disabled` error is because that `-DCL_DELEGATE_NO_GL` is set. But if I manually remove `\"-DCL_DELEGATE_NO_GL\"` in `tensorflow/lite/CMakeLists.txt b/tensorflow/lite/CMakeLists.txt` build and run again. Below error is printed:\r\n```\r\nuse_gpu=true --gpu_backend=gl\r\nSTARTING!\r\nDuplicate flags: num_threads\r\nUnconsumed cmdline flags: --gpu_backend=gl\r\nLog parameter values verbosely: [0]\r\nGraph: [../mobilenet_v2_1.0_224_1_metadata_1.tflite]\r\nUse gpu: [1]\r\nLoaded model ../mobilenet_v2_1.0_224_1_metadata_1.tflite\r\nINFO: Initialized TensorFlow Lite runtime.\r\nThe GPU delegate compile options are only supported on Android or iOS platforms or when the tool was built with -DCL_DELEGATE_NO_GL.\r\nGPU acceleration is unsupported on this platform.\r\nINFO: Applying 1 TensorFlow Lite delegate(s) lazily.\r\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\r\n```\r\nIt seems that I can't run `benchmark_model` with gpu gl backend on linux. Is that true? Could you help guide me how to test tflite gpu gl backend on linux? Which target should I build and run? Thank you very much.\r\n", "As per the commit here https://github.com/tensorflow/tensorflow/commit/9a5eb944b9562c97a8189a9c893919e10597292d `random_uniform.cc` has been moved to `random_uniform_custom.cc` and `random_uniform_test.cc` has been moved to `random_uniform_custom_test.cc`, please make these changes and build again. Thanks!", "@sachinprasadhs With this fix, in the next step `cmake --build . -j8`, I meet below error:\r\n```\r\n[ 83%] Linking CXX executable elementwise_test\r\n/usr/bin/ld: ../libtensorflow-lite.a(op_signature.cc.o): in function `tflite::GetOpSignature(tflite::OperatorCode const*, tflite::Operator const*, tflite::SubGraph const*, tflite::Model const*)':\r\n/home/wp/workspace/jiajia/tensorflow/tensorflow/lite/tools/versioning/op_signature.cc:113: multiple definition of `tflite::GetOpSignature(tflite::OperatorCode const*, tflite::Operator const*, tflite::SubGraph const*, tflite::Model const*)'; libtensorflow-lite-test-base.a(op_signature.cc.o):/home/wp/workspace/jiajia/tensorflow/tensorflow/lite/tools/versioning/op_signature.cc:113: first defined here\r\n/usr/bin/ld: ../libtensorflow-lite.a(op_signature.cc.o): in function `tflite::GetOpSignature(TfLiteContext const*, TfLiteNode const*, TfLiteRegistration const*)':\r\n/home/wp/workspace/jiajia/tensorflow/tensorflow/lite/tools/versioning/op_signature.cc:233: multiple definition of `tflite::GetOpSignature(TfLiteContext const*, TfLiteNode const*, TfLiteRegistration const*)'; libtensorflow-lite-test-base.a(op_signature.cc.o):/home/wp/workspace/jiajia/tensorflow/tensorflow/lite/tools/versioning/op_signature.cc:233: first defined here\r\n/usr/bin/ld: ../libtensorflow-lite.a(external_delegate.cc.o): in function `TfLiteExternalDelegateOptionsInsert(TfLiteExternalDelegateOptions*, char const*, char const*)':\r\n/home/wp/workspace/jiajia/tensorflow/tensorflow/lite/delegates/external/external_delegate.cc:183: multiple definition of `TfLiteExternalDelegateOptionsInsert(TfLiteExternalDelegateOptions*, char const*, char const*)'; libtensorflow-lite-test-base.a(external_delegate.cc.o):/home/wp/workspace/jiajia/tensorflow/tensorflow/lite/delegates/external/external_delegate.cc:183: first defined here\r\n/usr/bin/ld: ../libtensorflow-lite.a(external_delegate.cc.o): in function `TfLiteExternalDelegateOptionsDefault':\r\n/home/wp/workspace/jiajia/tensorflow/tensorflow/lite/delegates/external/external_delegate.cc:194: multiple definition of `TfLiteExternalDelegateOptionsDefault'; libtensorflow-lite-test-base.a(external_delegate.cc.o):/home/wp/workspace/jiajia/tensorflow/tensorflow/lite/delegates/external/external_delegate.cc:194: first defined here\r\n/usr/bin/ld: ../libtensorflow-lite.a(external_delegate.cc.o): in function `TfLiteExternalDelegateCreate':\r\n/home/wp/workspace/jiajia/tensorflow/tensorflow/lite/delegates/external/external_delegate.cc:206: multiple definition of `TfLiteExternalDelegateCreate'; libtensorflow-lite-test-base.a(external_delegate.cc.o):/home/wp/workspace/jiajia/tensorflow/tensorflow/lite/delegates/external/external_delegate.cc:206: first defined here\r\n/usr/bin/ld: ../libtensorflow-lite.a(external_delegate.cc.o): in function `TfLiteExternalDelegateDelete':\r\n/home/wp/workspace/jiajia/tensorflow/tensorflow/lite/delegates/external/external_delegate.cc:215: multiple definition of `TfLiteExternalDelegateDelete'; libtensorflow-lite-test-base.a(external_delegate.cc.o):/home/wp/workspace/jiajia/tensorflow/tensorflow/lite/delegates/external/external_delegate.cc:215: first defined here\r\ncollect2: error: ld returned 1 exit status\r\nmake[2]: *** [kernels/CMakeFiles/while_test.dir/build.make:172: kernels/while_test] Error 1\r\nmake[1]: *** [CMakeFiles/Makefile2:6888: kernels/CMakeFiles/while_test.dir/all] Error 2\r\nmake[1]: *** Waiting for unfinished jobs....", "Hi, did you make  changes in `CMakesLists.txt` from `random_standard_normal_test.cc`\r\n  `random_uniform_test.cc` to `random_ops_test.cc`, since in future the existing tests for random ops will be handled in `random_ops_test`.", "> Hi, did you make changes in `CMakesLists.txt` from `random_standard_normal_test.cc` `random_uniform_test.cc` to `random_ops_test.cc`, since in future the existing tests for random ops will be handled in `random_ops_test`.\r\n\r\nYes, I did the changes. However, I still meet above build errors.\r\nThe steps to reproduce:\r\n1. Do below changes:\r\n```\r\n--- a/tensorflow/lite/kernels/CMakeLists.txt\r\n+++ b/tensorflow/lite/kernels/CMakeLists.txt\r\n@@ -252,8 +252,7 @@ set(TEST_WITH_EXTERNAL_MAIN_LIST\r\n   pow_test.cc\r\n   quant_basic_lstm_test.cc\r\n   quantize_test.cc\r\n-  random_standard_normal_test.cc\r\n-  random_uniform_test.cc\r\n+  random_ops_test.cc\r\n   range_test.cc\r\n   rank_test.cc\r\n   reduce_test.cc\r\n@@ -323,4 +322,4 @@ if(${CMAKE_CROSSCOMPILING})\r\n     ${CMAKE_CURRENT_BINARY_DIR}/run-tests.cmake\r\n     COPYONLY\r\n   )\r\n-endif()\r\n```\r\n2. cmake ../tensorflow/tensorflow/lite -DCMAKE_BUILD_TYPE=Debug -DTFLITE_KERNEL_TEST=on -DTFLITE_ENABLE_GPU=ON\r\n3. cmake --build . -j8 -t conv_test\r\nThen, below errors are shown:\r\n```\r\n[100%] Linking CXX executable conv_test\r\n/usr/bin/ld: ../libtensorflow-lite.a(op_signature.cc.o): in function `tflite::GetOpSignature(tflite::OperatorCode const*, tflite::Operator const*, tflite::SubGraph const*, tflite::Model const*)':\r\n/home/wp/workspace/jiajia/tensorflow/tensorflow/lite/tools/versioning/op_signature.cc:113: multiple definition of `tflite::GetOpSignature(tflite::OperatorCode const*, tflite::Operator const*, tflite::SubGraph const*, tflite::Model const*)'; libtensorflow-lite-test-base.a(op_signature.cc.o):/home/wp/workspace/jiajia/tensorflow/tensorflow/lite/tools/versioning/op_signature.cc:113: first defined here\r\n/usr/bin/ld: ../libtensorflow-lite.a(op_signature.cc.o): in function `tflite::GetOpSignature(TfLiteContext const*, TfLiteNode const*, TfLiteRegistration const*)':\r\n/home/wp/workspace/jiajia/tensorflow/tensorflow/lite/tools/versioning/op_signature.cc:233: multiple definition of `tflite::GetOpSignature(TfLiteContext const*, TfLiteNode const*, TfLiteRegistration const*)'; libtensorflow-lite-test-base.a(op_signature.cc.o):/home/wp/workspace/jiajia/tensorflow/tensorflow/lite/tools/versioning/op_signature.cc:233: first defined here\r\n/usr/bin/ld: ../libtensorflow-lite.a(external_delegate.cc.o): in function `TfLiteExternalDelegateOptionsInsert(TfLiteExternalDelegateOptions*, char const*, char const*)':\r\n/home/wp/workspace/jiajia/tensorflow/tensorflow/lite/delegates/external/external_delegate.cc:183: multiple definition of `TfLiteExternalDelegateOptionsInsert(TfLiteExternalDelegateOptions*, char const*, char const*)'; libtensorflow-lite-test-base.a(external_delegate.cc.o):/home/wp/workspace/jiajia/tensorflow/tensorflow/lite/delegates/external/external_delegate.cc:183: first defined here\r\n/usr/bin/ld: ../libtensorflow-lite.a(external_delegate.cc.o): in function `TfLiteExternalDelegateOptionsDefault':\r\n/home/wp/workspace/jiajia/tensorflow/tensorflow/lite/delegates/external/external_delegate.cc:194: multiple definition of `TfLiteExternalDelegateOptionsDefault'; libtensorflow-lite-test-base.a(external_delegate.cc.o):/home/wp/workspace/jiajia/tensorflow/tensorflow/lite/delegates/external/external_delegate.cc:194: first defined here\r\n/usr/bin/ld: ../libtensorflow-lite.a(external_delegate.cc.o): in function `TfLiteExternalDelegateCreate':\r\n/home/wp/workspace/jiajia/tensorflow/tensorflow/lite/delegates/external/external_delegate.cc:206: multiple definition of `TfLiteExternalDelegateCreate'; libtensorflow-lite-test-base.a(external_delegate.cc.o):/home/wp/workspace/jiajia/tensorflow/tensorflow/lite/delegates/external/external_delegate.cc:206: first defined here\r\n/usr/bin/ld: ../libtensorflow-lite.a(external_delegate.cc.o): in function `TfLiteExternalDelegateDelete':\r\n/home/wp/workspace/jiajia/tensorflow/tensorflow/lite/delegates/external/external_delegate.cc:215: multiple definition of `TfLiteExternalDelegateDelete'; libtensorflow-lite-test-base.a(external_delegate.cc.o):/home/wp/workspace/jiajia/tensorflow/tensorflow/lite/delegates/external/external_delegate.cc:215: first defined here\r\ncollect2: error: ld returned 1 exit status\r\nmake[3]: *** [kernels/CMakeFiles/conv_test.dir/build.make:172: kernels/conv_test] Error 1\r\nmake[2]: *** [CMakeFiles/Makefile2:11521: kernels/CMakeFiles/conv_test.dir/all] Error 2\r\nmake[1]: *** [CMakeFiles/Makefile2:11528: kernels/CMakeFiles/conv_test.dir/rule] Error 2\r\nmake: *** [Makefile:2472: conv_test] Error 2", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54418\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54418\">No</a>\n", "Could you please try build against the master branch and report if you face any error. Thanks!", "\r\n\r\n\r\n> Could you please try build against the master branch and report if you face any error. Thanks!\r\n\r\nI still meet below errors with the master branch. \r\nSteps to reproduce:\r\n1. cmake ../tensorflow/tensorflow/lite -DCMAKE_BUILD_TYPE=Debug -DTFLITE_KERNEL_TEST=on -DTFLITE_ENABLE_GPU=ON\r\n2. cmake --build . -j8 -t conv_test\r\n```\r\n[100%] Building CXX object kernels/CMakeFiles/conv_test.dir/conv_test.cc.o\r\n[100%] Linking CXX executable conv_test\r\n/usr/bin/ld: ../libtensorflow-lite.a(op_signature.cc.o): in function `tflite::GetOpSignature(tflite::OperatorCode const*, tflite::Operator const*, tflite::SubGraph const*, tflite::Model const*)':\r\n/home/wp/workspace/jiajia/tensorflow/tensorflow/lite/tools/versioning/op_signature.cc:113: multiple definition of `tflite::GetOpSignature(tflite::OperatorCode const*, tflite::Operator const*, tflite::SubGraph const*, tflite::Model const*)'; libtensorflow-lite-test-base.a(op_signature.cc.o):/home/wp/workspace/jiajia/tensorflow/tensorflow/lite/tools/versioning/op_signature.cc:113: first defined here\r\n/usr/bin/ld: ../libtensorflow-lite.a(op_signature.cc.o): in function `tflite::GetOpSignature(TfLiteContext const*, TfLiteNode const*, TfLiteRegistration const*)':\r\n/home/wp/workspace/jiajia/tensorflow/tensorflow/lite/tools/versioning/op_signature.cc:233: multiple definition of `tflite::GetOpSignature(TfLiteContext const*, TfLiteNode const*, TfLiteRegistration const*)'; libtensorflow-lite-test-base.a(op_signature.cc.o):/home/wp/workspace/jiajia/tensorflow/tensorflow/lite/tools/versioning/op_signature.cc:233: first defined here\r\n/usr/bin/ld: ../libtensorflow-lite.a(external_delegate.cc.o): in function `TfLiteExternalDelegateOptionsInsert':\r\n/home/wp/workspace/jiajia/tensorflow/tensorflow/lite/delegates/external/external_delegate.cc:183: multiple definition of `TfLiteExternalDelegateOptionsInsert'; libtensorflow-lite-test-base.a(external_delegate.cc.o):/home/wp/workspace/jiajia/tensorflow/tensorflow/lite/delegates/external/external_delegate.cc:183: first defined here\r\n/usr/bin/ld: ../libtensorflow-lite.a(external_delegate.cc.o): in function `TfLiteExternalDelegateOptionsDefault':\r\n/home/wp/workspace/jiajia/tensorflow/tensorflow/lite/delegates/external/external_delegate.cc:194: multiple definition of `TfLiteExternalDelegateOptionsDefault'; libtensorflow-lite-test-base.a(external_delegate.cc.o):/home/wp/workspace/jiajia/tensorflow/tensorflow/lite/delegates/external/external_delegate.cc:194: first defined here\r\n/usr/bin/ld: ../libtensorflow-lite.a(external_delegate.cc.o): in function `TfLiteExternalDelegateCreate':\r\n/home/wp/workspace/jiajia/tensorflow/tensorflow/lite/delegates/external/external_delegate.cc:206: multiple definition of `TfLiteExternalDelegateCreate'; libtensorflow-lite-test-base.a(external_delegate.cc.o):/home/wp/workspace/jiajia/tensorflow/tensorflow/lite/delegates/external/external_delegate.cc:206: first defined here\r\n/usr/bin/ld: ../libtensorflow-lite.a(external_delegate.cc.o): in function `TfLiteExternalDelegateDelete':\r\n/home/wp/workspace/jiajia/tensorflow/tensorflow/lite/delegates/external/external_delegate.cc:215: multiple definition of `TfLiteExternalDelegateDelete'; libtensorflow-lite-test-base.a(external_delegate.cc.o):/home/wp/workspace/jiajia/tensorflow/tensorflow/lite/delegates/external/external_delegate.cc:215: first defined here\r\ncollect2: error: ld returned 1 exit status\r\nmake[3]: *** [kernels/CMakeFiles/conv_test.dir/build.make:172: kernels/conv_test] Error 1\r\nmake[2]: *** [CMakeFiles/Makefile2:11351: kernels/CMakeFiles/conv_test.dir/all] Error 2\r\nmake[1]: *** [CMakeFiles/Makefile2:11358: kernels/CMakeFiles/conv_test.dir/rule] Error 2\r\nmake: *** [Makefile:2394: conv_test] Error 2\r\n```", "@qjia7 If the error has changed, could you create a new Github issue to track the details? @sachinprasadhs or @mohantym  is it possible to reproduce this in a new gist?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54418\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54418\">No</a>\n"]}, {"number": 54416, "title": "Tensorflow not detecting GPU RTX 2070", "body": "I am trying to get tensorflow to detect my RTX 2070. I am using Ubuntu with the nvidia-510 drivers. Pytorch is detecting the GPU but tensorflow is not detecting it. I tried reinstalling my drivers and trying the nightly version. I also followed the GPU instructions on the website and tried to install it wtih conda.\r\n\r\nI am getting those errors when starting Tensorflow:\r\n\r\n```\r\n2022-02-16 22:23:08.039986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:987] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-02-16 22:23:08.042291: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\r\n2022-02-16 22:23:08.042658: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n```\r\n\r\nThis is my nvidia-smi output:\r\n![image](https://user-images.githubusercontent.com/68305035/154398647-3fdee35a-5758-40f4-8724-d68b5efd1075.png)\r\n\r\nI am using this code to detect the GPU:\r\n```\r\nimport tensorflow as tf\r\ntf.config.list_physical_devices()\r\n```\r\n\r\nThis is what my usr/local looks like:\r\n![image](https://user-images.githubusercontent.com/68305035/154398885-eb990fdf-1327-4251-9754-08e977f48418.png)\r\n\r\n\r\n\r\n", "comments": ["I installed it with the instructions on the website and I also installed it with the conda instructions [here](https://docs.anaconda.com/anaconda/user-guide/tasks/tensorflow/). echo $LD_LIBRARY_PATH is blank and echo $PATH gives me this:\r\n```\r\n/home/aneesh/miniconda3/bin:/home/aneesh/miniconda3/condabin:/home/aneesh/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\r\n```", "@Cheeseboy8020 ,\r\nCan you please take a look at this [issue](https://github.com/tensorflow/tensorflow/issues/52988) with the similar error.It helps.\r\nIn order to expedite the trouble-shooting process, could you please provide the following information\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary):\r\nTensorFlow version:\r\nPython version:\r\nInstalled using virtualenv? pip? conda?:\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\n \r\nand the exact sequence of commands / steps that you executed before running into the problem\r\n\r\n", "OS: Ubuntu 21.10\r\nTensorFlow installed from: Binary\r\nTensorflow Version: 2.8.0 Release & 2.9.0 Nightly\r\nPython version: 3.10.0\r\nInstalled using: pip & conda\r\nCUDA: Installed 11.0 but nvidia-smi shows 11.6\r\nGPU: RTX 2070 8GB", "The important error message is:\r\n`2022-02-16 22:23:08.042291: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory`\r\nThese libraries are located in your cuda folder, and currently, the PATH and LD_LIBRARY_PATH don't include it.\r\nAs mentioned in the issue linked by [tilakrayal](https://github.com/tilakrayal), try to add \r\n/usr/local/cuda-11.0/lib64 (or /usr/local/cuda/lib64)\r\nto your LD_LIBRARY_PATH. If you add the cuda folder (without the version number), it has the benefit that the link still holds even if you switch cuda versions, because the cuda folder just links to the cuda-11.0 folder right now.\r\nI added it in the bashrc \r\n```\r\nexport PATH=\"$PATH:/usr/local/cuda/bin\"\r\nexport LD_LIBRARY_PATH=/usr/local/cuda/lib64\r\n```\r\n(I also have\r\n`export LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:/usr/local/cuda/extras/CUPTI/lib64\"`\r\nin there, but I'm not sure anymore _why_ exactly. But I did not place it there randomly. But as long as it runs for you without that, I wouldn't add it.)\r\n\r\n[This link](\r\nhttps://forums.developer.nvidia.com/t/how-to-set-environment-variable-ld-library-path-linux-ld-library-path/3275/3) also mentioned to add the path with a file in /etc/ld.so.conf.d, I didn't try it but there seem to be multiple ways.\r\n\r\nAs for nvidia-smi showing another cuda version, from my understanding, it shows the highest possible cuda version for this driver. As long as it is not smaller than your current cuda version, there should be no problems.", "> I installed it with the instructions on the website and I also installed it with the conda instructions [here](https://docs.anaconda.com/anaconda/user-guide/tasks/tensorflow/). echo $LD_LIBRARY_PATH is blank and echo $PATH gives me this:\r\n> \r\n> ```\r\n> /home/aneesh/miniconda3/bin:/home/aneesh/miniconda3/condabin:/home/aneesh/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\r\n> ```\r\n\r\nDue to a recent change, you will need to expose the cuda libs via LD_LIBRARY_PATH and put the appropriate compilers (e.g. ptxas) on PATH if needed (though this latter part doesn't necessarily affect the gpu discovery per se). ", "I fixed by trying all the solutions that were provided. That didn't work so I deleted all my nvidia drivers and cuda drivers. Then I installed the cuda driver first with the runfile and I didn't include the nvidia driver in the install. Then I installed the nvidia driver. I used cuda 11.6 and nvidia driver 510. After that it worked.", "Still not working?\r\n\r\nCould you please try to install all from conda-forge to see if that helps? Print `conda info` and `conda list` please if that doesn't work. Also try something like the below command:\r\n\r\n`mamba create -n tf -c conda-forge tensorflow==2.7.0=cuda112*`", "I forgot to mention that after the driver reinstall the GPU was detected.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54416\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54416\">No</a>\n"]}, {"number": 54415, "title": "`tf.histogram_fixed_width_bins` lack checking for `nbins`", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.7.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nnbins = -16\r\nvalue_range = [0.0, 5.0]\r\nnew_values = [-1.0, 0.0, 1.5, 2.0, 5.0, 15]\r\nindices = tf.histogram_fixed_width_bins(new_values, value_range, nbins=nbins)\r\nindices.numpy()\r\n```\r\nOutputs:\r\n```\r\narray([0, 0, 0, 0, 0, 0], dtype=int32)\r\n```\r\n\r\n**Describe the current behavior**\r\n`tf.histogram_fixed_width_bins` has an argument `nbins` which should be a **positive** integer. However, it does not perform any validity checking and can accept a **negative** value like `-16`.  `tf.histogram_fixed_width` (another API with similar functionality) can detect this error and raise an `InvalidArgumentError`:\r\n```\r\nimport tensorflow as tf\r\nnbins = -16\r\nvalue_range = [0.0, 5.0]\r\nnew_values = [-1.0, 0.0, 1.5, 2.0, 5.0, 15]\r\nindices = tf.histogram_fixed_width(new_values, value_range, nbins=nbins)\r\nindices.numpy()\r\n# InvalidArgumentError: nbins should be a positive number, but got '-16' [Op:HistogramFixedWidth]\r\n```\r\n\r\n\r\n**Describe the expected behavior**\r\n`tf.histogram_fixed_width_bins` should have better input checking.\r\n", "comments": ["Hi @chunduriv ! Could you please look at this issue? It is replicating  in TF 2.7 and 2.8. Attaching gist in [2.8 ](https://colab.sandbox.google.com/gist/mohantym/422bcc9269550d072870afe4f0b3c578/github_54415_2-8.ipynb#scrollTo=7AJOJUOiT-Gu) and [2.7](https://colab.sandbox.google.com/gist/mohantym/c3e643aa8b2ac6ec0ae83b37b25c5465/github_54415_2-8.ipynb#scrollTo=Fhee8nopUdjy)  for reference. Thanks!", "Added a PR #54429 for the fix.", "@ArrowIntoTheSky, The issue will move to closed status once the https://github.com/tensorflow/tensorflow/pull/54429 is merged. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54415\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54415\">No</a>\n"]}, {"number": 54413, "title": "`tf.compat.as_bytes` does not check the encoding string", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.7.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nbytes_or_text = \"hello\"\r\nencoding = \"valid\"\r\nt1 = tf.compat.as_text(bytes_or_text, encoding=encoding)\r\nprint(t1) # hello\r\nt2 = tf.compat.as_bytes(bytes_or_text,encoding=encoding)\r\n# LookupError: unknown encoding: valid\r\n```\r\n\r\n**Describe the current behavior**\r\n`\"valid\"` is not valid value for `encoding`, as we can see that `tf.compat.as_bytes` would throw an `LoopupError`. However, `tf.compat.as_text` does not perform any validity checking and can accept it and even give an output.\r\n\r\n\r\n**Describe the expected behavior**\r\n`tf.compat.as_text` should check the validity of `encoding`.\r\n", "comments": ["@ArrowIntoTheSky ,\r\nPlease find the difference between tf.compat.as_text and tf.compat.as_bytes.\r\ntf.compat.as_text:Converts any string-like python input types to unicode.\r\ntf.compat.as_bytes:Converts bytearray, bytes, or unicode python input types to bytes", "@tilakrayal \r\nYes, they are different APIs. I just use `tf.compat.as_bytes` to show the correct error handling of a **wrong** encoding string. It is obvious that `encoding` cannot be `valid` or `hi` as in the following example:\r\n```\r\nimport tensorflow as tf\r\nbytes_or_text = \"hello\"\r\nencoding = \"hi\"\r\nt1 = tf.compat.as_text(bytes_or_text, encoding=encoding) # This pass! But it should not.\r\n```", "@gadagashwini ,\r\nI was able to reproduce the issue in tf v2.7, v2.8 and nightly.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/00dfd63ad2db6914065b7d00893cfc41/54413.ipynb).", "Added a PR #54503 for the fix.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54413\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54413\">No</a>\n"]}, {"number": 54412, "title": "`tf.boolean_mask` lack checking for bool arguments", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.7.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\ntensor = [0,1,2,3]\r\nmask = tf.random.uniform([4], dtype=tf.float64)\r\ntf.boolean_mask(tensor, mask) \r\n# Outputs: <tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 1, 2, 3], dtype=int32)>\r\n```\r\n\r\n**Describe the current behavior**\r\n`tf.boolean_mask` has an argument `mask` which should be a `bool` tensor. However, it does not perform any validity checking and can accept a `float64` value. \r\n\r\n\r\n**Describe the expected behavior**\r\n`tf.boolean_mask` should check the dtype of input tensor `mask`.\r\n\r\nFor example, `tf.math.reduce_any` would check the first argument and throw an `InvalidArgumentError` for non-boolean inputs.\r\n```\r\nimport tensorflow as tf\r\ninput_tensor = tf.random.uniform([4], dtype=tf.float64)\r\ntf.math.reduce_any(input_tensor) # InvalidArgumentError: cannot compute Any as input #0(zero-based) was expected to be a bool tensor but is a double tensor [Op:Any]\r\n```\r\n", "comments": ["Added a PR #54412 for the fix.", "@ArrowIntoTheSky, The issue will move to closed status once the https://github.com/tensorflow/tensorflow/pull/54432 is merged. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54412\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54412\">No</a>\n"]}, {"number": 54411, "title": "`tf.math.atan` lack support for `complex64`", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.7.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nx = tf.complex(tf.random.uniform([8, 8], dtype=tf.float32),tf.random.uniform([8, 8], dtype=tf.float32))\r\nprint(x.dtype) # <dtype: 'complex64'>\r\ntf.math.atan(x)\r\n```\r\n\r\n**Describe the current behavior**\r\n`tf.math.atan` cannot accept a tensor of type `complex64`. However, according to the [document](https://www.tensorflow.org/api_docs/python/tf/math/atan?hl=en) it should support `complex64` and `complex128`.\r\nFor the above code snippet, the error message is:\r\n```\r\nNotFoundError: Could not find device for node: {{node Atan}} = Atan[T=DT_COMPLEX64]\r\nAll kernels registered for op Atan:\r\n  device='GPU'; T in [DT_DOUBLE]\r\n  device='GPU'; T in [DT_FLOAT]\r\n  device='GPU'; T in [DT_HALF]\r\n  device='GPU'; T in [DT_BFLOAT16]\r\n  device='CPU'; T in [DT_DOUBLE]\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='CPU'; T in [DT_BFLOAT16]\r\n  device='CPU'; T in [DT_HALF]\r\n [Op:Atan]\r\n```\r\n\r\n", "comments": ["Related PR has been merged here are the [changes](https://github.com/tensorflow/tensorflow/commit/64af5cd59743d8d595ec4d3654e25ffb11e6ef0d) . Thank you for raising the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54411\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54411\">No</a>\n"]}, {"number": 54409, "title": "Include c++17 in nvcc allowed std options.", "body": "Support for c++17 was added in nvcc 11.0.167+:\r\nhttps://gist.github.com/ax3l/9489132#device-side-c-standard-support\r\n\r\nValid --std values documented here:\r\nhttps://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#options-for-altering-compiler-linker-behavior-std", "comments": ["I think #54329 is a better fix", "Closing in favor of #54329"]}, {"number": 54408, "title": "Add c++17 to supported nvcc allowed std options.", "body": "Enable Tensorflow with CUDA and c++17.\r\n\r\nhttps://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#options-for-altering-compiler-linker-behavior-std", "comments": []}, {"number": 54407, "title": "bazel build tensorflow with CUDA and TennsorRT on windows 10 failed", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): master\r\n- TensorFlow version: 2.9.0\r\n- Python version: 3.8.10\r\n- Installed using virtualenv? pip? conda?: conda create env -n myenvs\r\n- Bazel version (if compiling from source): bazel 5.0.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 11.3 / cuDNN 8\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI want to build TensorFlow C++ API on Windows 10 with CUDA and TensorRT support.\r\nI downloaded the master branch (https://github.com/tensorflow/tensorflow/ - date: 16.02.22)\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nThe configuration:\r\n\r\npython ./configure.py\r\nYou have bazel 5.0.0 installed.\r\nPlease specify the location of python. [Default is C:\\DevTools\\Python\\Python_3.8.10\\python.exe]:\r\n\r\n\r\nFound possible Python library paths:\r\n  C:\\DevTools\\Python\\Python_3.8.10\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [C:\\DevTools\\Python\\Python_3.8.10\\lib\\site-packages]\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]:\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: y\r\nTensorRT support will be enabled for TensorFlow.\r\n\r\nWARNING: TensorRT support on Windows is experimental\r\n\r\nFound CUDA 11.3 in:\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.3/lib/x64\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.3/include\r\nFound cuDNN 8 in:\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.3/lib/x64\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.3/include\r\nFound TensorRT 8 in:\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.3/lib/x64\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.3/include\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as \"x.y\" or \"compute_xy\" to include both virtual and binary GPU code, or as \"sm_xy\" to only include the binary code.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 7.5\r\n\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\n\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:\r\nEigen strong inline overridden.\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]:\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=mkl_aarch64    # Build with oneDNN and Compute Library for the Arm Architecture (ACL).\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v1             # Build with TensorFlow 1 API instead of TF 2 API.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\n\r\nThen the baze build command:\r\nbazel build --config=opt tensorflow:tensorflow.dll\r\n\r\nThis is the error:\r\n\r\nStarting local Bazel server and connecting to it...\r\nWARNING: Option 'java_toolchain' is deprecated\r\nWARNING: Option 'host_java_toolchain' is deprecated\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=237\r\nINFO: Reading rc options for 'build' from c:\\devtools\\tensorflow_master\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=C:/DevTools/Python/Python_3.8.10/python.exe\r\nINFO: Reading rc options for 'build' from c:\\devtools\\tensorflow_master\\.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=//tensorflow/tools/toolchains/java:tf_java_toolchain --host_java_toolchain=//tensorflow/tools/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library\r\nINFO: Reading rc options for 'build' from c:\\devtools\\tensorflow_master\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=C:/DevTools/Python/Python_3.8.10/python.exe --action_env PYTHON_LIB_PATH=C:/DevTools/Python/Python_3.8.10/lib/site-packages --python_path=C:/DevTools/Python/Python_3.8.10/python.exe --config=tensorrt --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.3 --action_env TF_CUDA_COMPUTE_CAPABILITIES=7.5 --config=cuda --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --define=override_eigen_strong_inline=true\r\nINFO: Reading rc options for 'build' from c:\\devtools\\tensorflow_master\\.bazelrc:\r\n  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils\r\nINFO: Found applicable config definition build:short_logs in file c:\\devtools\\tensorflow_master\\.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file c:\\devtools\\tensorflow_master\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:tensorrt in file c:\\devtools\\tensorflow_master\\.bazelrc: --repo_env TF_NEED_TENSORRT=1\r\nINFO: Found applicable config definition build:cuda in file c:\\devtools\\tensorflow_master\\.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda\r\nINFO: Found applicable config definition build:opt in file c:\\devtools\\tensorflow_master\\.tf_configure.bazelrc: --copt=/arch:AVX --host_copt=/arch:AVX\r\nINFO: Found applicable config definition build:windows in file c:\\devtools\\tensorflow_master\\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --features=compiler_param_file --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:monolithic in file c:\\devtools\\tensorflow_master\\.bazelrc: --define framework_shared_object=false\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/0a77ba77a0e7f58932a038381d997c231947c77f.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found\r\nINFO: Repository local_config_cuda instantiated at:\r\n  C:/devtools/tensorflow_master/WORKSPACE:15:14: in <toplevel>\r\n  C:/devtools/tensorflow_master/tensorflow/workspace2.bzl:868:19: in workspace\r\n  C:/devtools/tensorflow_master/tensorflow/workspace2.bzl:96:19: in _tf_toolchains\r\nRepository rule cuda_configure defined at:\r\n  C:/devtools/tensorflow_master/third_party/gpus/cuda_configure.bzl:1448:33: in <toplevel>\r\nERROR: An error occurred during the fetch of repository 'local_config_cuda':\r\n   Traceback (most recent call last):\r\n        File \"C:/devtools/tensorflow_master/third_party/gpus/cuda_configure.bzl\", line 1401, column 38, in _cuda_autoconf_impl\r\n                _create_local_cuda_repository(repository_ctx)\r\n        File \"C:/devtools/tensorflow_master/third_party/gpus/cuda_configure.bzl\", line 1239, column 56, in _create_local_cuda_repository\r\n                host_compiler_includes + _cuda_include_path(\r\n        File \"C:/devtools/tensorflow_master/third_party/gpus/cuda_configure.bzl\", line 364, column 32, in _cuda_include_path\r\n                inc_entries.append(realpath(repository_ctx, cuda_config.cuda_toolkit_path + \"/include\"))\r\n        File \"C:/devtools/tensorflow_master/third_party/remote_config/common.bzl\", line 290, column 19, in realpath\r\n                return execute(repository_ctx, [bash_bin, \"-c\", \"realpath \\\"%s\\\"\" % path]).stdout.strip()\r\n        File \"C:/devtools/tensorflow_master/third_party/remote_config/common.bzl\", line 230, column 13, in execute\r\n                fail(\r\nError in fail: Repository command failed\r\n/usr/bin/bash: line 1: realpath: command not found\r\nERROR: C:/devtools/tensorflow_master/WORKSPACE:15:14: fetching cuda_configure rule //external:local_config_cuda: Traceback (most recent call last):\r\n        File \"C:/devtools/tensorflow_master/third_party/gpus/cuda_configure.bzl\", line 1401, column 38, in _cuda_autoconf_impl\r\n                _create_local_cuda_repository(repository_ctx)\r\n        File \"C:/devtools/tensorflow_master/third_party/gpus/cuda_configure.bzl\", line 1239, column 56, in _create_local_cuda_repository\r\n                host_compiler_includes + _cuda_include_path(\r\n        File \"C:/devtools/tensorflow_master/third_party/gpus/cuda_configure.bzl\", line 364, column 32, in _cuda_include_path\r\n                inc_entries.append(realpath(repository_ctx, cuda_config.cuda_toolkit_path + \"/include\"))\r\n        File \"C:/devtools/tensorflow_master/third_party/remote_config/common.bzl\", line 290, column 19, in realpath\r\n                return execute(repository_ctx, [bash_bin, \"-c\", \"realpath \\\"%s\\\"\" % path]).stdout.strip()\r\n        File \"C:/devtools/tensorflow_master/third_party/remote_config/common.bzl\", line 230, column 13, in execute\r\n                fail(\r\nError in fail: Repository command failed\r\n/usr/bin/bash: line 1: realpath: command not found\r\nINFO: Found applicable config definition build:cuda in file c:\\devtools\\tensorflow_master\\.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda\r\nWARNING: Option 'java_toolchain' is deprecated\r\nWARNING: Option 'host_java_toolchain' is deprecated\r\nERROR: @local_config_cuda//:enable_cuda :: Error loading option @local_config_cuda//:enable_cuda: Repository command failed\r\n/usr/bin/bash: line 1: realpath: command not found\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@OpDaSo Could you please have a look at the [link](https://github.com/tensorflow/docs/blob/master/site/en/install/source.md#gpu) ,and [link1 ](https://www.tensorflow.org/install/source_windows) try with the latest TF version(2.8.0) with compatible configurations ?Please let us know if it helps?Thanks!", "@sushreebarsa With TF version 2.8.0 it is possible to build tensorflow. But with TF version 2.8.0 there is no possibiliy to choose TensorRT support.", "I think you forgot to put MSYS2 to your `%PATH%`. Check if this command prints a path to `realpath.exe`:\r\n```cmd\r\nfor /F %a in (\"realpath.exe\") do @echo.%~f$PATH:a\r\n```\r\nIf it prints an empty string, your `%PATH%` is not configured correctly", "yes, this was it. thanks!", "Unfortunately, I receive another error when building the dll:\r\n\r\nbazel build --config=opt tensorflow:tensorflow.dll\r\n\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=237\r\nINFO: Reading rc options for 'build' from c:\\devtools\\tensorflow_v2.8.0\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=C:/DevTools/Python/Python_3.8.10/python.exe\r\nINFO: Reading rc options for 'build' from c:\\devtools\\tensorflow_v2.8.0\\.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library\r\nINFO: Reading rc options for 'build' from c:\\devtools\\tensorflow_v2.8.0\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=C:/DevTools/Python/Python_3.8.10/python.exe --action_env PYTHON_LIB_PATH=C:/DevTools/Python/Python_3.8.10/lib/site-packages --python_path=C:/DevTools/Python/Python_3.8.10/python.exe --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.3 --action_env TF_CUDA_COMPUTE_CAPABILITIES=7.5 --config=cuda --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --define=override_eigen_strong_inline=true\r\nINFO: Reading rc options for 'build' from c:\\devtools\\tensorflow_v2.8.0\\.bazelrc:\r\n  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils\r\nINFO: Found applicable config definition build:short_logs in file c:\\devtools\\tensorflow_v2.8.0\\.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file c:\\devtools\\tensorflow_v2.8.0\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:cuda in file c:\\devtools\\tensorflow_v2.8.0\\.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda\r\nINFO: Found applicable config definition build:opt in file c:\\devtools\\tensorflow_v2.8.0\\.tf_configure.bazelrc: --copt=/arch:AVX --host_copt=/arch:AVX\r\nINFO: Found applicable config definition build:windows in file c:\\devtools\\tensorflow_v2.8.0\\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --features=compiler_param_file --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:monolithic in file c:\\devtools\\tensorflow_v2.8.0\\.bazelrc: --define framework_shared_object=false\r\nINFO: Analyzed target //tensorflow:tensorflow.dll (0 packages loaded, 0 targets configured).\r\nINFO: Found 1 target...\r\nERROR: C:/devtools/tensorflow_v2.8.0/tensorflow/cc/BUILD:344:11: Compiling tensorflow/cc/framework/grad_op_registry.cc failed: (Exit 1): python.exe failed: error executing command\r\n  cd C:/users/xxx/_bazel_xxx/r6ypy3np/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.3\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\ATLMFC\\include;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\winrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\cppwinrt\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\ATLMFC\\lib\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\lib\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.19041.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.19041.0\\um\\x64\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\\\Extensions\\Microsoft\\IntelliCode\\CLI;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\bin\\HostX64\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\MSBuild\\Current\\bin\\Roslyn;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Team Tools\\Performance Tools\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\Tools\\devinit;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.19041.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\\\MSBuild\\Current\\Bin;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\Tools\\;;C:\\WINDOWS\\system32;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/DevTools/Python/Python_3.8.10/python.exe\r\n    SET PYTHON_LIB_PATH=C:/DevTools/Python/Python_3.8.10/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TEMP=C:\\Users\\XXX\\AppData\\Local\\Temp\r\n    SET TF2_BEHAVIOR=1\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=7.5\r\n    SET TMP=C:\\Users\\XXX\\AppData\\Local\\Temp\r\n  C:/DevTools/Python/Python_3.8.10/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/bin /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/gif /Ibazel-out/x64_windows-opt/bin/external/gif /Iexternal/libjpeg_turbo /Ibazel-out/x64_windows-opt/bin/external/libjpeg_turbo /Iexternal/com_google_protobuf /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-opt/bin/external/highwayhash /Iexternal/zlib /Ibazel-out/x64_windows-opt/bin/external/zlib /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/local_config_cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda /Iexternal/local_config_rocm /Ibazel-out/x64_windows-opt/bin/external/local_config_rocm /Iexternal/local_config_tensorrt /Ibazel-out/x64_windows-opt/bin/external/local_config_tensorrt /Iexternal/cudnn_frontend_archive /Ibazel-out/x64_windows-opt/bin/external/cudnn_frontend_archive /Iexternal/llvm-project /Ibazel-out/x64_windows-opt/bin/external/llvm-project /Iexternal/llvm_terminfo /Ibazel-out/x64_windows-opt/bin/external/llvm_terminfo /Iexternal/llvm_zlib /Ibazel-out/x64_windows-opt/bin/external/llvm_zlib /Iexternal/curl /Ibazel-out/x64_windows-opt/bin/external/curl /Iexternal/boringssl /Ibazel-out/x64_windows-opt/bin/external/boringssl /Iexternal/jsoncpp_git /Ibazel-out/x64_windows-opt/bin/external/jsoncpp_git /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cudnn_header /Ibazel-out/x64_windows-opt/bin/external/cudnn_frontend_archive/_virtual_includes/cudnn_frontend /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinAttributeInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinAttributesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinDialectIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinLocationAttributesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinTypeInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinTypesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/CallOpInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/CastOpInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/InferTypeOpInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/OpAsmInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/RegionKindInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/SideEffectInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/SubElementInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/SymbolInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/TensorEncodingIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ControlFlowInterfacesIncGen /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Ithird_party/eigen3/mkl_include /Ibazel-out/x64_windows-opt/bin/third_party/eigen3/mkl_include /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/gif /Ibazel-out/x64_windows-opt/bin/external/gif /Iexternal/gif/windows /Ibazel-out/x64_windows-opt/bin/external/gif/windows /Iexternal/com_google_protobuf/src /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf/src /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive/src /Iexternal/zlib /Ibazel-out/x64_windows-opt/bin/external/zlib /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include /Iexternal/local_config_rocm/rocm /Ibazel-out/x64_windows-opt/bin/external/local_config_rocm/rocm /Iexternal/local_config_rocm/rocm/rocm/include /Ibazel-out/x64_windows-opt/bin/external/local_config_rocm/rocm/rocm/include /Iexternal/local_config_rocm/rocm/rocm/include/rocrand /Ibazel-out/x64_windows-opt/bin/external/local_config_rocm/rocm/rocm/include/rocrand /Iexternal/local_config_rocm/rocm/rocm/include/roctracer /Ibazel-out/x64_windows-opt/bin/external/local_config_rocm/rocm/rocm/include/roctracer /Iexternal/llvm-project/llvm/include /Ibazel-out/x64_windows-opt/bin/external/llvm-project/llvm/include /Iexternal/llvm-project/mlir/include /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/include /Iexternal/curl/include /Ibazel-out/x64_windows-opt/bin/external/curl/include /Iexternal/boringssl/src/include /Ibazel-out/x64_windows-opt/bin/external/boringssl/src/include /Iexternal/jsoncpp_git/include /Ibazel-out/x64_windows-opt/bin/external/jsoncpp_git/include /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_CRT_NONSTDC_NO_DEPRECATE /D_CRT_NONSTDC_NO_WARNINGS /D_SCL_SECURE_NO_DEPRECATE /D_SCL_SECURE_NO_WARNINGS /DUNICODE /D_UNICODE /DLTDL_SHLIB_EXT=\".dll\" /DLLVM_PLUGIN_EXT=\".dll\" /DLLVM_NATIVE_ARCH=\"X86\" /DLLVM_NATIVE_ASMPARSER=LLVMInitializeX86AsmParser /DLLVM_NATIVE_ASMPRINTER=LLVMInitializeX86AsmPrinter /DLLVM_NATIVE_DISASSEMBLER=LLVMInitializeX86Disassembler /DLLVM_NATIVE_TARGET=LLVMInitializeX86Target /DLLVM_NATIVE_TARGETINFO=LLVMInitializeX86TargetInfo /DLLVM_NATIVE_TARGETMC=LLVMInitializeX86TargetMC /DLLVM_NATIVE_TARGETMCA=LLVMInitializeX86TargetMCA /DLLVM_HOST_TRIPLE=\"x86_64-pc-win32\" /DLLVM_DEFAULT_TARGET_TRIPLE=\"x86_64-pc-win32\" /D__STDC_LIMIT_MACROS /D__STDC_CONSTANT_MACROS /D__STDC_FORMAT_MACROS /DCURL_STATICLIB /DTF_USE_SNAPPY /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /showIncludes /MD /O2 /DNDEBUG /W0 /D_USE_MATH_DEFINES -DWIN32_LEAN_AND_MEAN -DNOGDI /experimental:preprocessor /d2ReducedOptimizeHugeFunctions /arch:AVX /std:c++14 /Fobazel-out/x64_windows-opt/bin/tensorflow/cc/_objs/grad_op_registry/grad_op_registry.obj /c tensorflow/cc/framework/grad_op_registry.cc\r\nExecution platform: @local_execution_config_platform//:platform\r\nTraceback (most recent call last):\r\n  File \"external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py\", line 217, in <module>\r\n    sys.exit(main())\r\n  File \"external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py\", line 214, in main\r\n    return subprocess.call([CPU_COMPILER] + cpu_compiler_flags)\r\n  File \"C:\\DevTools\\Python\\Python_3.8.10\\lib\\subprocess.py\", line 340, in call\r\n    with Popen(*popenargs, **kwargs) as p:\r\n  File \"C:\\DevTools\\Python\\Python_3.8.10\\lib\\subprocess.py\", line 858, in __init__\r\n    self._execute_child(args, executable, preexec_fn, close_fds,\r\n  File \"C:\\DevTools\\Python\\Python_3.8.10\\lib\\subprocess.py\", line 1311, in _execute_child\r\n    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\r\nFileNotFoundError: [WinError 2] Das System kann die angegebene Datei nicht finden\r\nTarget //tensorflow:tensorflow.dll failed to build\r\nINFO: Elapsed time: 1.760s, Critical Path: 0.24s\r\nINFO: 34 processes: 34 internal.\r\nFAILED: Build did NOT complete successfully", "@OpDaSo,\r\nLooks like error is coming from MSVC. Make sure you have installed \r\n1. [Visual C++ Build Tools 2019](https://www.tensorflow.org/install/source_windows#install_visual_c_build_tools_2019).\r\nThanks!", "Thanks! This is the solution!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54407\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54407\">No</a>\n"]}, {"number": 54406, "title": "Typo in https://blog.tensorflow.org/2022/02/boost-your-models-accuracy.html?linkId=8033010", "body": "...layer with a on top...", "comments": ["@ToonTalk ,\r\nWe see that the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]", "I only read the blog post and noticed a typo. Why fill in a template for that?", "@ToonTalk ,\r\nPlease feel free to submit a PR for the requested change or share the link where requested change is to be made", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "The typo is in https://blog.tensorflow.org/2022/02/boost-your-models-accuracy.html?linkId=8033010\r\n\r\n...layer with a on top...", "@ToonTalk,\r\n\r\nThis issue is fixed now. Please refer to the updated [blog post](https://blog.tensorflow.org/2022/02/boost-your-models-accuracy.html). Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 54405, "title": "The model size after Lite conversion is much larger than the original Tensorflow model", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installation (pip package or built from source): pip install\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): 2.8.0\r\n\r\n### 2. Code\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_text as text\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(\r\n    saved_model_dir='chatter_engine',\r\n    signature_keys=['serving_default']\r\n)  # path to the SavedModel directory\r\nprint('after load')\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\nconverter.target_spec.supported_types = [tf.float16]\r\n\r\nprint('before conversion')\r\ntflite_model = converter.convert()\r\nprint('after conversion')\r\n\r\n# Save the model.\r\nwith open('model.tflite', 'wb') as f:\r\n    f.write(tflite_model)\r\nprint('after write')\r\n\r\n```\r\n\r\n```\r\n- Tensorflow model : https://drive.google.com/drive/folders/1WrNca1DQ9xpbH00JAVQKE8PsxFzB60qj?usp=sharing\r\n- Tensorflow lite model: https://drive.google.com/file/d/1nqpkM3ShOTit37BHZ1A5DYEDCjaaac6J/view?usp=sharing\r\n```\r\n\r\n### 3. Failure after conversion\r\nIf the conversion is successful, but the generated model is wrong, then state what is wrong: the converted model size is more than 400 MB where the original model is only 4 MB", "comments": ["Hi @chunduriv! Could you please look at this issue? It is replicating in 2.7 ,[2.8 ](https://colab.sandbox.google.com/gist/mohantym/0e54856c2c952ab3d5c9763384fc6131/github_54405.ipynb#scrollTo=iQvBGhKm-p7v)and nightly. Size of  tflite file came 400 mb after conversion as mentioned in template.", "I am also having the same issue.\r\nI have a custom TF model that is about 16Mb\r\nSizes after conversion are\r\n\r\nTF Lite Dynamic Range file size is 73.5 Mb\r\nTF Lite Int with float fallback file size is 267.7 Mb\r\nTF lite Full integer quantization file size is 271.2 Mb\r\n\r\nHere is a colab that showcases the problem\r\nTensorFlow 2.8\r\nhttps://colab.research.google.com/drive/1uYZOrPJWbSGfhf-DYEgMtCVmy5xLBE5G?usp=sharing\r\nTensorFlow 2.7.1\r\nhttps://colab.research.google.com/drive/15kzXMSF2olfzNskYN99WKq7DarUy-3tj?usp=sharing\r\n\r\nNote that integer quantization fails in 2.7.1 and for all model generated they have an incorrect input size of 1x1x1x3 but I will open a separate issue for that. ", "Please see my reply on similar question [here](https://github.com/tensorflow/tensorflow/issues/54659)", "Closing please reopen if you have questions.\r\n\r\nThanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54405\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54405\">No</a>\n"]}, {"number": 54404, "title": "Help! Didnt change anything in code and was working fine but today tensorflow gives me a bug.", "body": "Hi guys, till yesterday i was running without a problem my GAN in colab pro + and today i stopped it and tried to begin it again and i am taking an error  \"NotImplementedError: Cannot convert a symbolic Tensor (frame/Size:0) to a numpy array.\" I literally didnt change a single thing and i am doing this process the last month without a problem. Does anyone know what is happening?\r\n", "comments": ["@RitoRitoni \r\nIn order to expedite the trouble-shooting process here,Could you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose),\r\nThanks!", "Hi, I have the same issue since Tuesday. Have you solved it? or do you know what has happened?\r\nThanks!", "@RitoRitoni \r\nIn order to reproduce the issue reported here, could you please  fill the [template](https://github.com/tensorflow/tensorflow/issues/new/choose)  and provide the complete code  , tensorflow version you are using? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54404\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54404\">No</a>\n"]}, {"number": 54403, "title": "high GPU memory usage with tf.data.Dataset.from_tensor_slices", "body": "System information\r\n- OS Platform and Distribution: Linux 5.13.0-28-generic\r\n- TensorFlow installed from: source\r\n- TensorFlow version: 2.8.0\r\n- Python version: 3.8.10\r\n- Installed using: conda\r\n- CUDA version: 11.5\r\n- GPU model and memory: NVIDIA GeForce RTX 3090 24 GB\r\n- CPU model: Intel(R) Core(TM) i9-10980XE CPU @ 3.00GHz\r\n\r\nWhen I run the following code on Jupyter Notebook, it uses 22651 MB GPU memory when viewed with `nvtop`:\r\n```\r\nimport tensorflow as tf\r\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\r\n```\r\n\r\nIt also prints the following message:\r\n```\r\n2022-02-16 11:55:26.100430: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2022-02-16 11:55:26.576039: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22232 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:17:00.0, compute capability: 8.6\r\n```\r\n\r\nI found other issues with similar messages, but they did not have the same memory issue. \r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54403\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54403\">No</a>\n", "I made a typo by passing lowercase 'gpu' instead of 'GPU' as the device_type, but it went unnoticed as there was no error. Fixing that solved the issue.\r\n```\r\ngpus = tf.config.experimental.list_physical_devices(device_type='gpu')\r\nfor gpu in gpus:\r\n    tf.config.experimental.set_memory_growth(gpu, True)\r\n```\r\n\r\n"]}, {"number": 54402, "title": "[TF-TRT] Add TF-TRT converter for ZerosLike and OnesLike", "body": "This PR adds a TF-TRT converter for the [tf.zeros_like](https://www.tensorflow.org/api_docs/python/tf/zeros_like) and [tf.ones_like](https://www.tensorflow.org/api_docs/python/tf/ones_like) operators (and the equivalent ops `ZerosLike` and `OnesLike` ).\r\nThe converter supports tensor and weights for the input. It supports both explicit batch mode and dynamic shape mode.\r\n\r\nThe converter uses helper functions introduced in https://github.com/tensorflow/tensorflow/pull/54170#issue-1117203815 , so it can be merged only after the fill converter has been merged.\r\n\r\nIt refactors the tf.fill converter to reuse the main TRT network building logic in the new converters.\r\n", "comments": ["@bixia1 @tfeher as discussed with Tamas offline, I moved `AddFill` to the layer_utils and put the 2 converters into separate files. The PR is ready for final review"]}, {"number": 54401, "title": "Error: cannot import name 'LayerNormalization' from 'tensorflow.python.keras.layers.normalization' ", "body": "How to fix the following error?\r\n\r\n`Error: cannot import name 'LayerNormalization' from 'tensorflow.python.keras.layers.normalization' (/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/normalization/__init__.py)`\r\n\r\nThanks!\r\n", "comments": ["Hi @OliveS9 ! Could you please fill the template which will  help us expedite the issue? You can try following options though.\r\n1. use tf.keras instead of keras \r\n2. use tf.keras.layers.LayerNormalization  \r\n2. uninstall and reinstall latest tensorflow version in a new environment.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "@mohantym Thank you for your reply, but how to use \"tf.keras\"? Thanks.", "Just replace this command. \r\n`import keras `\r\nwith\r\n`from tensorflow import keras ` \r\n\r\nCan you please update the steps too in the template?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54401\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54401\">No</a>\n"]}, {"number": 54400, "title": "ValueError: Could not find matching function to call loaded from the SavedModel. Got:", "body": "from bert.tokenization import FullTokenizer\r\nimport pandas as pd\r\nimport tensorflow_hub as hub\r\n\r\nbert_path = \"https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/4\"\r\n\r\nsess = tf.Session()\r\n\r\ndef create_tokenizer_from_hub_module():\r\n    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\r\n    bert_module =  hub.load(bert_path)\r\n    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\r\n    vocab_file, do_lower_case = sess.run(\r\n        [\r\n            tokenization_info[\"vocab_file\"],\r\n            tokenization_info[\"do_lower_case\"],\r\n        ]\r\n    )\r\n\r\n    return FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\r\n  \r\ntokenizer = create_tokenizer_from_hub_module()\r\n)\r\n\r\n but in this line : tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True) \r\nI am getting  error: ValueError: Could not find matching function to call loaded from the SavedModel. Got:\r\n\r\n\r\n  Positional arguments (2 total):\r\n    * False\r\n    * None\r\n  Keyword arguments: {'do_lower_case': False, 'as_dict': True, 'signature': 'tokenization_info'}\r\n\r\nExpected these arguments to match one of the following 4 option(s):\r\n\r\nOption 1:\r\n  Positional arguments (3 total):\r\n    * {u'input_word_ids': TensorSpec(shape=(?, ?), dtype=tf.int32, name=u'input_word_ids'), u'input_mask': TensorSpec(shape=(?, ?), dtype=tf.int32, name=u'input_mask'), u'input_type_ids': TensorSpec(shape=(?, ?), dtype=tf.int32, name=u'input_type_ids')}\r\n    * False\r\n    * None\r\n  Keyword arguments: {}\r\n\r\nOption 2:\r\n  Positional arguments (3 total):\r\n    * {u'input_word_ids': TensorSpec(shape=(?, ?), dtype=tf.int32, name=u'inputs/input_word_ids'), u'input_mask': TensorSpec(shape=(?, ?), dtype=tf.int32, name=u'inputs/input_mask'), u'input_type_ids': TensorSpec(shape=(?, ?), dtype=tf.int32, name=u'inputs/input_type_ids')}\r\n    * False\r\n    * None\r\n  Keyword arguments: {}\r\n\r\nOption 3:\r\n  Positional arguments (3 total):\r\n    * {u'input_word_ids': TensorSpec(shape=(?, ?), dtype=tf.int32, name=u'inputs/input_word_ids'), u'input_mask': TensorSpec(shape=(?, ?), dtype=tf.int32, name=u'inputs/input_mask'), u'input_type_ids': TensorSpec(shape=(?, ?), dtype=tf.int32, name=u'inputs/input_type_ids')}\r\n    * True\r\n    * None\r\n  Keyword arguments: {}\r\n\r\nOption 4:\r\n  Positional arguments (3 total):\r\n    * {u'input_word_ids': TensorSpec(shape=(?, ?), dtype=tf.int32, name=u'input_word_ids'), u'input_mask': TensorSpec(shape=(?, ?), dtype=tf.int32, name=u'input_mask'), u'input_type_ids': TensorSpec(shape=(?, ?), dtype=tf.int32, name=u'input_type_ids')}\r\n    * True\r\n    * None\r\n  Keyword arguments: {}\r\n\r\n", "comments": ["@kusumlata123 \r\nIn order to reproduce the issue reported here, could you please provide the complete code and the dataset , tensorflow version you are using. Session. run is deprecated api for TF v2.x please make sure you are using TF v2.4 or later. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}]