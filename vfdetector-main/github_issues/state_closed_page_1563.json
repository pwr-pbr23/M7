[{"number": 6025, "title": "For which inputs are gradients computed?", "body": "What is a good way to determine which inputs of a tensorflow op are backpropagated through?\r\nFor example, consider the [crop_and_resize](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard2/tf.image.crop_and_resize.md) op. The description sounds rather straightforward - \r\n>\"Extracts crops from the input image tensor and bilinearly resizes them\". \r\n\r\nHowever, if you dig through the code, this function pops up [_CropAndResizeGrad](https://github.com/tensorflow/tensorflow/blob/33e88f6d7b69f796d4fad452394237448f62b976/tensorflow/python/ops/image_grad.py#L77), which states \r\n\r\n> \"We back-propagate to the image only when the input image tensor has floating point dtype but we always back-propagate to the input boxes tensor.\",\r\n\r\nwhich indicates that the input bounding box coordinates will get gradients.\r\n\r\nIt might just be me, but I have a hard time telling which input tensor variables will get gradients and which will not. Is there a standard way? Should this be part of the documentation? \r\n\r\nThanks.", "comments": ["It should be part of the documentation. Ideally, every op that does *not* have gradients for some of its inputs should call this out. Adding those to the docs when you see them would be a service to us all.", "While in an ideal world, each op that doesn't have gradients for some of it's inputs would be enough, it's tough to verify.  Shouldn't this be the sort of thing that is documented by default for all functions as a requirement for merge.  Rather than leaving this as something that is documented when there isn't a gradient for some input which is easy to overlook at code review.  An alternative is requiring that each argument in the Arg: portion of the documentation explicitly states whether gradients are passed.  This would be easy to review and also serve to highlight which functions haven't been fixed yet.", "Automatically closing due to lack of recent activity. We hope that you were able to resolve it on your own. However, since this is a support issue rather than a bug or feature request, you will probably get more information by posting it on StackOverflow."]}, {"number": 6024, "title": "where is Tensorflow directory on Windows ?", "body": "I have installed and verified Tensorflow installation on Windows, that's great. But how I am supposed to to stuff like `pyhthon3 cifar10_train.py`? ran that command got error no such file or directory. Also run the whole code in cifar10.py and got error there is no module named tensorflow.modles.\r\n\r\n`ImportError was unhandled by user code\r\nMessage: No module named 'tensorflow.models'`\r\n", "comments": ["The location of the TensorFlow directory on Windows will depend on your Python distribution. You should be able to find it by doing the following:\r\n\r\n```\r\nC:\\> python\r\n>>>import tensorflow as tf\r\n>>>print(tf.__file__ )\r\n```\r\n\r\nHowever, the TensorFlow directory doesn't currently include the `tensorflow.models` (see #5953), so even if you download the `cifar10_train.py` code it won't quite work. I've sent #6029 to fix this, and it should appear in the final release.\r\n\r\n(I'll close this issue for now, but please watch #5953 for an update on when the `tensorflow.models` will be easily usable on Windows.)", "Tensorflow on Windows does not have CMake in contrib folder. @mrry ", "@codeloverr I'm not sure where you're looking: it's certainly in the git repository (since that's how we build it). The CMake directory isn't included in the PIP package because it doesn't contain anything useful for Python users who've downloaded the built package.", "I downloaded the zip file from github, extracted and added those files I needed to the directory tensorflow was expecting them to be. In my case c:/users/my_username/Anaconda\r\n3/Lib/"]}, {"number": 6023, "title": "Update nightly links to 0.12.0rc0", "body": "", "comments": ["doc change, merging."]}, {"number": 6022, "title": "Negative axis support for argmin and argmax", "body": "I'd like `tf.argmax(value, axis=-1)` to work.  A lot of other ops support negative axes (`tf.concat`, `tf.stack`, etc.), but not `argmin` and `argmax`.", "comments": ["Sorry if I reopen this, but I was desperately looking for an explanation of negative axis concatenation. rf.reshape does explain the special case for -1, but not tf.concat.\r\nMay someone be so kind to explain what does that mean?\r\nThanks", "`axis=-1` means the last axis, in the same way that `list[-1]` in Python means the last entry.  See also any numpy function, such as https://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.sum.html.", "Great, that confirmed the idea I got by experimenting. Thanks", "Thanks", "resolve my confusion about `axis=-1` .thanks"]}, {"number": 6021, "title": "Linker error when building custom op", "body": "I was not getting this error a couple of weeks ago, but now when I repull it/update the repository I am getting an error when running configure.\r\n\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\ngit pull\r\ngit status\r\n    `On branch master\r\nYour branch is up-to-date with 'origin/master'.\r\nUntracked files:\r\n  (use \"git add <file>...\" to include in what will be committed)\r\n\r\n\ttensorflow/core/platform/default/build_config.bzl-e\r\n\ttensorflow/models/image/mnist/data/\r\n\ttensorflow/user_ops/zero_out.cc\r\n\tzero_out.cc\r\n\tzero_out_2.so\r\n\r\nnothing added to commit but untracked files present (use \"git add\" to track)`\r\n\r\nThen I run ./configure:\r\n\r\nError:\r\n`ERROR: /Users/johnpeurifoy/Documents/Skewl/GeneralUnitaryRNN/tensorflow/tensorflow/tensorflow.bzl:660:21: syntax error at '=': expected expression.\r\nERROR: /Users/johnpeurifoy/Documents/Skewl/GeneralUnitaryRNN/tensorflow/tensorflow/tensorflow.bzl:735:1: nested functions are not allowed. Move the function to top-level.\r\nERROR: /Users/johnpeurifoy/Documents/Skewl/GeneralUnitaryRNN/tensorflow/tensorflow/tensorflow.bzl:763:1: nested functions are not allowed. Move the function to top-level.\r\nERROR: /Users/johnpeurifoy/Documents/Skewl/GeneralUnitaryRNN/tensorflow/tensorflow/tensorflow.bzl:797:1: nested functions are not allowed. Move the function to top-level.\r\nERROR: /Users/johnpeurifoy/Documents/Skewl/GeneralUnitaryRNN/tensorflow/tensorflow/tensorflow.bzl:800:1: nested functions are not allowed. Move the function to top-level.\r\nERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Extension 'tensorflow/tensorflow.bzl' has errors.\r\n`\r\n\r\nOverall, I am trying to implement a new operator using tensorflow. I am able to easily import tensorflow, but when I try to build the new operator (following the tutorial on https://www.tensorflow.org/versions/r0.12/how_tos/adding_an_op/index.html), i get:\r\n\r\n`  \"tensorflow::Tensor::CheckTypeAndIsAligned(tensorflow::DataType) const\", referenced from:\r\n      ZeroOutOp::Compute(tensorflow::OpKernelContext*) in zero_out-a7cd0e.o\r\n  \"typeinfo for tensorflow::OpKernel\", referenced from:\r\n      typeinfo for ZeroOutOp in zero_out-a7cd0e.o\r\nld: symbol(s) not found for architecture x86_64\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)`\r\n\r\nThoughts? I have tried checking out the other branches as well and trying them, but no luck.\r\n\r\nMy pip freeze looks like:\r\n\r\n`alabaster==0.7.9\r\nanaconda-clean==1.0\r\nanaconda-client==1.5.1\r\nanaconda-navigator==1.3.1\r\nappnope==0.1.0\r\nappscript==1.0.1\r\nargcomplete==1.0.0\r\nastroid==1.4.7\r\nastropy==1.2.1\r\nBabel==2.3.4\r\nbackports-abc==0.4\r\nbackports.shutil-get-terminal-size==1.0.0\r\nbackports.ssl-match-hostname==3.4.0.2\r\nbeautifulsoup4==4.5.1\r\nbitarray==0.8.1\r\nblaze==0.10.1\r\nbokeh==0.12.2\r\nboto==2.42.0\r\nBottleneck==1.1.0\r\ncdecimal==2.3\r\ncffi==1.7.0\r\nchest==0.2.3\r\nclick==6.6\r\ncloudpickle==0.2.1\r\nclyent==1.2.2\r\ncolorama==0.3.7\r\nconda==4.2.12\r\nconda-build==2.0.2\r\nconfigobj==5.0.6\r\nconfigparser==3.5.0\r\ncontextlib2==0.5.3\r\ncryptography==1.5\r\ncycler==0.10.0\r\nCython==0.24.1\r\ncytoolz==0.8.0\r\ndask==0.11.0\r\ndatashape==0.5.2\r\ndecorator==4.0.10\r\ndill==0.2.5\r\ndocutils==0.12\r\ndynd==0.7.3.dev1\r\nenum34==1.1.6\r\net-xmlfile==1.0.1\r\nfastcache==1.0.2\r\nfilelock==2.0.6\r\nFlask==0.11.1\r\nFlask-Cors==2.1.2\r\nfuncsigs==1.0.2\r\nfunctools32==3.2.3.post2\r\nfutures==3.0.5\r\ngevent==1.1.2\r\ngreenlet==0.4.10\r\ngrin==1.2.1\r\nh5py==2.6.0\r\nHeapDict==1.0.0\r\nidna==2.1\r\nimagesize==0.7.1\r\nipaddress==1.0.16\r\nipykernel==4.5.0\r\nipython==5.1.0\r\nipython-genutils==0.1.0\r\nipywidgets==5.2.2\r\nitsdangerous==0.24\r\njdcal==1.2\r\njedi==0.9.0\r\nJinja2==2.8\r\njsonschema==2.5.1\r\njupyter==1.0.0\r\njupyter-client==4.4.0\r\njupyter-console==5.0.0\r\njupyter-core==4.2.0\r\nKeras==1.1.1\r\nlazy-object-proxy==1.2.1\r\nllvmlite==0.13.0\r\nlocket==0.2.0\r\nlxml==3.6.4\r\nMarkupSafe==0.23\r\nmatplotlib==1.5.3\r\nmistune==0.7.3\r\nmock==2.0.0\r\nmpmath==0.19\r\nmultipledispatch==0.4.8\r\nnb-anacondacloud==1.2.0\r\nnb-conda==2.0.0\r\nnb-conda-kernels==2.0.0\r\nnbconvert==4.2.0\r\nnbformat==4.1.0\r\nnbpresent==3.0.2\r\nnetworkx==1.11\r\nnltk==3.2.1\r\nnose==1.3.7\r\nnotebook==4.2.3\r\nnumba==0.28.1+0.gfe99fbc.dirty\r\nnumexpr==2.6.1\r\nnumpy==1.11.1\r\nodo==0.5.0\r\nopenpyxl==2.3.2\r\npandas==0.18.1\r\npartd==0.3.6\r\npath.py==0.0.0\r\npathlib2==2.1.0\r\npatsy==0.4.1\r\npbr==1.10.0\r\npep8==1.7.0\r\npexpect==4.0.1\r\npickleshare==0.7.4\r\nPillow==3.3.1\r\npkginfo==1.3.2\r\nply==3.9\r\nprompt-toolkit==1.0.3\r\nprotobuf==3.0.0\r\npsutil==4.3.1\r\nptyprocess==0.5.1\r\npy==1.4.31\r\npyasn1==0.1.9\r\nPyAudio==0.2.7\r\npycosat==0.6.1\r\npycparser==2.14\r\npycrypto==2.6.1\r\npycurl==7.43.0\r\npyflakes==1.3.0\r\nPygments==2.1.3\r\npylint==1.5.4\r\npyOpenSSL==16.0.0\r\npyparsing==2.1.4\r\npytest==2.9.2\r\npython-dateutil==2.5.3\r\npytz==2016.6.1\r\nPyYAML==3.12\r\npyzmq==15.4.0\r\nQtAwesome==0.3.3\r\nqtconsole==4.2.1\r\nQtPy==1.1.2\r\nredis==2.10.5\r\nrequests==2.11.1\r\nrope==0.9.4\r\nruamel-yaml===-VERSION\r\nscikit-image==0.12.3\r\nscikit-learn==0.17.1\r\nscipy==0.18.1\r\nsimplegeneric==0.8.1\r\nsingledispatch==3.4.0.3\r\nsix==1.10.0\r\nsnowballstemmer==1.2.1\r\nsockjs-tornado==1.0.3\r\nSphinx==1.4.6\r\nspyder==3.0.0\r\nSQLAlchemy==1.0.13\r\nstatsmodels==0.6.1\r\nsympy==1.0\r\ntables==3.2.3.1\r\ntensorflow==0.11.0rc2\r\nterminado==0.6\r\nTheano==0.8.2\r\ntoolz==0.8.0\r\ntornado==4.4.1\r\ntraitlets==4.3.0\r\nunicodecsv==0.14.1\r\nwcwidth==0.1.7\r\nWerkzeug==0.11.11\r\nwidgetsnbextension==1.2.6\r\nwrapt==1.10.6\r\nxlrd==1.0.0\r\nXlsxWriter==0.9.3\r\nxlwings==0.10.0\r\nxlwt==1.1.2`\r\n\r\n\r\n\r\n", "comments": ["@iguanaus What platform are you trying this on?", "Macbook running os x el captain, 10.11.3\n\nSent from my iPhone\n\n> On Dec 5, 2016, at 11:59 AM, Manjunath Kudlur <notifications@github.com> wrote:\n> \n> @iguanaus What platform are you trying this on?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n> \n", "Upgrade to the latest bazel version. If you are using homebrew:\r\n\r\n```bash\r\nbrew update\r\nbrew upgrade bazel\r\n```", "@iguanaus Have you tried it with the latest version of bazel?", "Yes this does not resolve it.\n\nSent from my iPhone\n\n> On Dec 8, 2016, at 2:04 PM, Manjunath Kudlur <notifications@github.com> wrote:\n> \n> @iguanaus Have you tried it with the latest version of bazel?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n> \n", "@iguanaus What version of bazel are you using? I had the same issue and updating bazel to `0.4.1` solved it.", "with bazel 0.4.2, we now have multiple mirrors for our external dependencies.\r\nCould you try bazel 0.4.2?", "Closing the issue  due to inactivity."]}, {"number": 6020, "title": "[Windows/CMake] Cherry-pick TensorBoard support into r0.12", "body": "[CMake] Add TensorBoard dependencies to PIP package.\r\n\r\nApplies PR #5844 to the release branch.", "comments": []}, {"number": 6019, "title": "Computer freeze when feeding a large numpy array as input in MNIST tutorial", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nNone\r\n\r\n### Environment info\r\nintel i5, 8gb ram\r\nOperating System:\r\nUbuntu 14.04.5:\r\nLinux 4.4.0-45-generic #66~14.04.1-Ubuntu SMP Wed Oct 19 15:05:38 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\nInstalled version of CUDA and cuDNN: \r\nNo cuda or cuDNN, running on CPU\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\nTF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.11.0-cp34-cp34m-linux_x86_64.whl\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n0.11.0rc2\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nUse the mnist tutorial, replace\r\n    `train_accuracy = accuracy.eval(feed_dict={\r\n        x:batch[0], y_: batch[1], keep_prob: 1.0})`\r\nwith \r\n`train_accuracy = accuracy.eval(feed_dict={ x:mnist.train.images, y_: mnist.train.labels, keep_prob: 1.0})`\r\n\r\nSo that the accuracy is evaluated over the entire training set. I've reproduced this using another dataset, and the problem goes away when using a smaller number of examples.\r\n(pastebin for entire file with this modification: http://pastebin.com/THNqB4ws)\r\n\r\n### What other attempted solutions have you tried?\r\nNone, there's no error message and the computer hangs the first time it executes `accuracy.eval`\r\n\r\n", "comments": ["The documentation (https://www.tensorflow.org/versions/r0.12/tutorials/mnist/beginners/index.html) says that mnist.test.images is 10k images, while mnist.train.images is 55k images.  By feeding the entire training set at once, it seems likely that you're causing too much memory to be allocated, and that may be your problem.  8GB is not a lot of RAM these days.   In linux you can use ulimit to bound the memory permitted to a process.", "@poxvoculi Thanks, I guessed as much, but just before the freeze I still see 3gbs free of ram; maybe a big chunk gets allocated just before the freeze and causes it. Also, I thought having no swap would cause an out-of-memory error, my bad, so maybe that could be it.  I'll check tomorrow from the computer at work if limiting the memory for the process helps to avoid the freeze.  \r\n\r\nNonetheless, the entire training set is 160mb if in float32. The output of the convolutional layers are 1.3gb and 650mb respectively. 214mb for the first fully connected and just 2 megs for the output, for a total of ~2.5gb total. The memory required for the weights is negligible. It seems weird the activations of the intermediate layers would take that much space. ", "Indeed limiting the memory of the process avoids the OOM. TF is trying to use about 11gbs of memory to evaluate the entire training set. While this can be workaround quite easily (evaluating the accuracy in smaller batches), isn't 11gbs a bit overmuch? Is it keeping the activations of every layer even when not doing a train_step.run()? If the activations are not saved for the backward pass, no more than 1.3gb+160mb should be used by the forward pass. Is there any way to avoid storing the activations?", "Further testing with a really simple network: conv(4) - pool(2x2) - fc(10) (that's 4 filters for the convolutional layer) full code available in http://pastebin.com/Zh14pS3C.\r\nI removed the training, accuracy evaluation, and most layers from the net, while also reducing the filters for the convolutional layer and the relu. For the whole training dataset this still requires **6.5gb** of memory. There's a 1gb decrease if I evaluate with 10000 less training examples, 2gb for 20000, and so on. Is this normal?", "What happens if you run with tcmalloc? There's been some memory leaks\nreported earlier which disappear when you do tcmalloc (as specified here\n<http://goog-perftools.sourceforge.net/doc/tcmalloc.html>)\nAlso, you can see which ops are allocating memory by doing something like\nthis\n\nrun_metadata = tf.RunMetadata()\nsess.run([C.op],\n             options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE),\n             run_metadata=run_metadata)\nprint run_metadata\n\nOr more visually in Chrome Trace Viewer (see\nhttps://github.com/tensorflow/tensorflow/issues/1824#issuecomment-225754659)\n\ntrace = timeline.Timeline(step_stats=run_metadata.step_stats)\nwith open('timeline.ctf.json', 'w') as trace_file:\n  trace_file.write(trace.generate_chrome_trace_format(show_memory=True))\n\n\nOn Fri, Dec 2, 2016 at 10:56 AM, Pepe Mandioca <notifications@github.com>\nwrote:\n\n> Further testing with a really simple network: conv(4) - pool(2x2) - fc(10)\n> (that's 4 filters for the convolutional layer) full code available in\n> http://pastebin.com/Zh14pS3C.\n> I removed the training, accuracy evaluation, and most layers from the net,\n> while also reducing the filters for the convolutional layer and the relu.\n> For the whole training dataset this still requires *6.5gb* of memory.\n> There's a roughly 1gb decrease if I evaluate with 10000 less training\n> examples, 2gb for 20000, and so on. Is this normal?\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6019#issuecomment-264533170>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AABaHPtgmRHA9TlLuX2xxFdyyzookUQOks5rEGnkgaJpZM4LBp_t>\n> .\n>\n", "@yaroslavvb Running with tcmalloc decreased the memory usage slightly but not entirely.\r\n\r\nI tried the tracer but it seems there's only cpu use info.\r\n\r\nHere's the output from `print(run_metadata)`, http://pastebin.com/tXwB7Lfn when testing with **4000** training examples (so that the run() completes).\r\n\r\nThe relevant? bits from the memory subsection:\r\nThe Conv2D allocates `total_bytes: 25088000` (~25mb) (28*28*4000*4*2) \r\nThe Add allocates `total_bytes: 25088000` (~25mb) (28*28*4000*4*2)\r\nThe MaxPool allocates `total_bytes: 6272000` (~6mb) (14*14*4000*4*2)\r\nThe MatMul allocates `total_bytes: 160000` (~160k) (10*4000*4)\r\nThe Add_1 allocates `total_bytes: 160000` (~160k) (10*4000*4)\r\nThe SoftmaxCrossEntropyWithLogits allocates `total_bytes: 192000` (~192k) (10*4000*4+4000*8)\r\n\r\nThere are also a lot of  \"Shape\", \"Slice\", most of which allocate just 4, 8, 16 or 200 bytes. Other nodes of the sort allocate bytes 15680, 15680, 12544000, 6272000, 160000, 160000, but all of these add up to at most 20mb. So if with 4000 samples the \"use\" is less than 100mb, with 55000 it would be about 1350mb or 1.35gb. \r\n\r\nNote: Are the 'total bytes' and 'requested_bytes' added up? if so the conv, add and maxpool layers occupy twice as much. \r\n", "Uncompressed MNIST dataset is about 50MB, and you are feeding the whole dataset at each step for 20k steps. This [line](https://github.com/tensorflow/tensorflow/blob/d42facc3cc9611f0c9722c81551a7404a0bd3f6b/tensorflow/python/client/tf_session_helper.cc#L445) in tf_session_helper.cc does a copy of numpy data into TensorFlow\r\n\r\n`std::memcpy(data, PyArray_DATA(array), size)`\r\n\r\nSo during the run of this program you are trying to copy 1TB worth of numpy arrays into TensorFlow, how much memory does your machine have?", "Well, if you check the MWE you'll see that I removed the for loop and I'm doing a single eval, so I guess that theory is irrelevant for this issue.\r\n\r\n Regardless, you are telling me that each time I feed a numpy array to a graph memory gets allocated and never released? ", "I'm not able to reproduce freezing after single eval. When I ran your example from http://pastebin.com/THNqB4ws it runs for >100 iterations, at which point I killed it. I've seen cases of a machine freezing to a point of requiring reboot when TensorFlow allocated too much memory.\r\n\r\nIf I put a pause after 100 iterations in your script, the memory usage returns to normal, so I guess that memory gets garbage collected eventually.\r\n", "Here's a version of your script with some extra info printed: http://pastebin.com/Sfhn3sAP\r\nWhen I run it, I see this:\r\n\r\n```\r\nIteration 0 time 81.24 3.31\r\ntcmalloc: large alloc 5017829376 bytes == 0xc3be0000 @  0x7f5cb6236c4c 0x7f5cb6255040 0x7f5c50ac253f 0x7f5c517604f8 0x7f5c5176cd7d 0x7f5c5177ba14 0x7f5c5177dd23 0x7f5c51d2a49e 0x7f5c51d1dca0 0x7f5c51efb668 0x7f5c51efae42 0x7f5cb49bf870 0x7f5cb5b1e184 0x7f5cb4f3637d (nil)\r\ntest accuracy 0.1052\r\nEIteration 0 time 1790.80 10.95\r\nIteration 10 time 41.54 10.95\r\nIteration 20 time 41.43 10.95\r\nIteration 30 time 40.35 10.95\r\nIteration 40 time 41.02 10.95\r\nIteration 50 time 39.61 10.95\r\nIteration 60 time 42.82 10.95\r\nIteration 70 time 41.60 10.95\r\nIteration 80 time 39.79 10.95\r\nIteration 90 time 41.15 10.95\r\nIteration 100 time 42.20 10.95\r\ntcmalloc: large alloc 5017829376 bytes == 0x6825e000 @  0x7f5cb6236c4c 0x7f5cb6255040 0x7f5c50ac253f 0x7f5c517604f8 0x7f5c5176cd7d 0x7f5c5177ba14 0x7f5c5177dd23 0x7f5c51d2a49e 0x7f5c51d1dca0 0x7f5c51efb668 0x7f5c51efae42 0x7f5cb49bf870 0x7f5cb5b1e184 0x7f5cb4f3637d (nil)\r\ntest accuracy 0.7846\r\nEIteration 100 time 1668.47 10.95\r\nIteration 110 time 43.09 10.95\r\nIteration 120 time 47.44 10.95\r\nIteration 130 time 49.22 10.95\r\nIteration 140 time 45.06 10.95\r\nIteration 150 time 45.78 10.95\r\nIteration 160 time 44.96 10.95\r\nIteration 170 time 45.14 10.95\r\nIteration 180 time 45.68 10.95\r\nIteration 190 time 40.74 10.95\r\nIteration 200 time 41.44 10.95\r\ntcmalloc: large alloc 5017829376 bytes == 0xc3bde000 @  0x7f5cb6236c4c 0x7f5cb6255040 0x7f5c50ac253f 0x7f5c517604f8 0x7f5c5176cd7d 0x7f5c5177ba14 0x7f5c5177dd23 0x7f5c51d2a49e 0x7f5c51d1dca0 0x7f5c51efb668 0x7f5c51efae42 0x7f5cb49bf870 0x7f5cb5b1e184 0x7f5cb4f3637d (nil)\r\ntest accuracy 0.8924\r\nEIteration 200 time 1707.17 10.95\r\nIteration 210 time 40.57 10.95\r\nIteration 220 time 44.48 10.95\r\nIteration 230 time 42.95 10.95\r\nIteration 240 time 40.86 10.95\r\nIteration 250 time 42.96 10.95\r\nIteration 260 time 42.18 10.95\r\nIteration 270 time 40.89 10.95\r\nIteration 280 time 42.90 10.95\r\nIteration 290 time 40.68 10.95\r\nIteration 300 time 41.88 10.95\r\ntcmalloc: large alloc 5017829376 bytes == 0xc3bde000 @  0x7f5cb6236c4c 0x7f5cb6255040 0x7f5c50ac253f 0x7f5c517604f8 0x7f5c5176cd7d 0x7f5c5177ba14 0x7f5c5177dd23 0x7f5c51d2a49e 0x7f5c51d1dca0 0x7f5c51efb668 0x7f5c51efae42 0x7f5cb49bf870 0x7f5cb5b1e184 0x7f5cb4f3637d (nil)\r\ntest accuracy 0.9203\r\nEIteration 300 time 1684.26 10.95\r\nIteration 310 time 51.82 10.95\r\nIteration 320 time 41.72 10.95\r\nIteration 330 time 40.68 10.95\r\nIteration 340 time 43.42 10.95\r\nIteration 350 time 48.30 10.95\r\nIteration 360 time 45.63 10.95\r\nIteration 370 time 43.82 10.95\r\nIteration 380 time 41.13 10.95\r\nIteration 390 time 41.07 10.95\r\nIteration 400 time 40.19 10.95\r\ntcmalloc: large alloc 5017829376 bytes == 0xc3bde000 @  0x7f5cb6236c4c 0x7f5cb6255040 0x7f5c50ac253f 0x7f5c517604f8 0x7f5c5176cd7d 0x7f5c5177ba14 0x7f5c5177dd23 0x7f5c51d2a49e 0x7f5c51d1dca0 0x7f5c51efb668 0x7f5c51efae42 0x7f5cb49bf870 0x7f5cb5b1e184 0x7f5cb4f3637d (nil)\r\n```\r\n\r\nSo it seems to be executing normally without slowdown. ", "I took a [closer look](https://github.com/yaroslavvb/notebooks/blob/master/mnist-memory.ipynb) at your memory usage and the main source of memory usage is the activation in your first conv layer.\r\n\r\nYour convolution has 32 filters, so activations for first conv layer takes 4x28x28x32 = 100k per example. This is followed by broadcasting add which makes it 200k total. To run this over whole dataset you need 60k x 200k = 12GB of RAM\r\n\r\n", "Thanks Yarsolav, that may be it. I thought the forward pass would be smarter and discard intermediate computations or do them in-place when not needed for the backward pass, but thinking about it a bit more that may be hard to do with a framework as general as tf.", "Actually if you look at [memory timeline](https://github.com/yaroslavvb/notebooks/blob/master/mnist-memory.ipynb) you'll see that activations for `h_conv1` are discarded as soon as `h_conv1+b_conv1` completes. This is because it's eval pass, so they are not needed for derivatives. If `add` could be done in place this would essentially lower the peak usage from 12GB to 6GB, and that's perhaps what the upcoming XLA framework could do. However, it would be trickier to lower usage for the backward pass as well since `h_conv1` activations are needed until much later -- you would need an implementation of fused `conv + add` op, and a corresponding gradient.", "Yes, a fused conv+add could store the activation A(b_conv1), and do A(b_conv1)-b_conv1 in the backward pass (again, in-place) to obtain A(h_conv1) again! That would surely help for big models even with small batch sizes. \r\n\r\nAnyhow, in this case I guess the sane thing to do is to use a tf input queue that would automatically batch stuff or calculate the full training set accuracy by feeding small batches which is easy since it's a reduction op.", "I have the same problem. I cannot load the numpy array into memory (it's not mnist) it freezes. I think I have to split my numpy array into some smaller arrays and use a generator to load every one of them and feed to my model but I think it's too slow. Is there any way to handle this problem? ", "It is the best solution for OOM, the activations occupied the great majority of GPU memory, rather than images of test dataset.  But too much batch size will make activations become huge, and get the OOM."]}, {"number": 6018, "title": "Unecessary messages in queue handling", "body": "Suppose you want to have one or multiple threads to preload/preprocess images and push them into an image queue that is going to be consumed by your main process, running on the GPU. A simple way of doing so would be:\r\n\r\n```\r\ndef load_image(session, image_list, enqueue_op, image_data_placeholder, coord):\r\n  index = 0\r\n  while not coord.should_stop():\r\n    idata = image_list[index]\r\n    session.run(enqueue_op, feed_dict={image_data_placeholder: idata})\r\n    index = (index + 1) % len(image_list)\r\n```\r\n\r\nEventually, your optimization is finished and you want all threads to end, but if you have threads feeding your queue, they are most likely blocked at the enqueue_op, waiting for an image to be consumed, so they can finally insert theirs in the queue.\r\n\r\nSince closing the queue doesn't unblock them (surprisingly), your most evident choice is to use the timeout option to limit how long the enqueue_op can block your thread, with something like this:\r\n\r\n```\r\ndef load_image(session, image_list, enqueue_op, image_data_placeholder, coord):\r\n  index = 0\r\n  while not coord.should_stop():\r\n    idata = image_list[index]\r\n    try:\r\n      session.run(enqueue_op, feed_dict={image_data_placeholder: idata}, options=tf.RunOptions(timeout_in_ms=1000))\r\n      index = (index + 1) % len(name_list)\r\n    except:\r\n      pass\r\n```\r\n\r\nAnd it does work: the thread will continue trying to push the same image until either it succeeds (and moves on to the next one) or coordinator says it should stop everything and finish.\r\n\r\nThe problem is that the timeout not only throws an exception saying that the time has been exceeded, but TF also \"manually\" prints a warning message. Bad news, you have 3 options: (1) manually edit TryAttemptLocked in queue_base.cc to remove the message from your tensorflow copy, (2) accept that this pesky message is going to flood your logs with useless information or (3) force TF_CPP_MIN_LOG_LEVEL to be at level 2, suppressing all information messages and warnings that could be important, only to get rid of this one message. There is also a 4th option: open an issue here in hope there is a better way of handling this, or that some dev changes this behavior \u2014 the exception should be enough for any situation, the forced message doesn't seem to do any good. :-)\r\n\r\nDisclaimer: I know it's far from optimal to pass a list of features (image_list) to be enqueued. This is just for the sake of simplicity in this example.", "comments": ["I think the log message here is intentional, especially for running in distributed settings where the log message might be printed on a parameter server and you want to diagnose a worker failure.\r\n\r\nHere's a 5th option: we could figure out how to make your code work without using timeouts.\r\n\r\nLet's say your queue is called `queue`. I think all that's necessary is to define your [`queue.close()`](https://www.tensorflow.org/versions/r0.12/api_docs/python/io_ops.html#QueueBase.close) op as:\r\n\r\n```python\r\nclose_op = queue.close(cancel_pending_enqueues=True)\r\n```\r\n\r\nDoes that solve the problem?", "Yes, it gracefully solves the problem \u2014 I didn't know this parameter existed. But unfortunately the solution was included in option 4 (\"... there is a better way of handling this ...\"). :-P\r\n\r\nThank you @mrry for the clarification about the log and for the solution. Closing the issue."]}, {"number": 6017, "title": "Initialise DNNClassifier with normalizer_fn ", "body": "For Contrib added option to pass normalizer_fn to DNNClassifier. This should automatically attach the normaliser to every layer in the deep neural network. Something like:\r\n     m = tf.contrib.learn.DNNClassifier(model_dir         = model_dir,\r\n                                                                feature_columns   = deep_columns,\r\n                                                                dropout           = DROPOUT,\r\n                                                                normalizer_fn     = tf.contrib.layers.batch_norm,\r\n                                                                normalizer_params = normalizer_params...)\r\n  \r\nModified dnn.py", "comments": ["Can one of the admins verify this patch?", "Hi All,\r\nJust thought this might be a useful feature to have in the contrib/learn functionality. It looks like the batch_norm method in /layers can be attached straight forwardly to the layers in the dnn, along with all the batch_parameters that need to be passed along. The only pitfalls I can see are there need to be some clear warning about argument scoping, and making sure the is_training variable is set properly depending on whether the model is being fit or evaluated. ", "@Montmorency sorry this fell through the cracks, could you pull rebase and push again?", "Closing for now. Feel free to re-open once you have made changes to the code."]}, {"number": 6016, "title": "fix deprecation date", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for the change. Can you revert the markdown changes, those are autogenerated from the source file.\r\n@gunan do we want to merge this into r0.12? if so do we also need to get it separately into master?", "Thanks for bringing this to my attention.\r\nI agree with all your comments @danmane \r\nMarkdown changes should be automated, so should be rolled back,\r\nYes, we want this in 0.12\r\nAnd yes, we want this also separately merged into master. My preference is to merge it to master before 0.12, but I am not picky about the order.", "Thanks for comments. I reverted markdown changes.", "Jenkins, test this please.", "Given that this is such a trivial change, I feel OK just merging it in."]}, {"number": 6015, "title": "initialization of local variables missing in example: fully_connected_preloaded.py", "body": "change introduced in this commit: \r\nhttps://github.com/tensorflow/tensorflow/commit/cbd3cacfb73bbea912b9d01c2540187684f751a7\r\nso, not sure if the other files have to be changed as well\r\n\r\n`init_op = tf.global_variables_initializer()`\r\n->\r\n`init_op = tf.group(tf.global_variables_initializer(),\r\n                       tf.local_variables_initializer())`\r\n\r\nwithout the change the loop is exited immediately and this error occus:\r\n\r\n> tensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value input/input_producer/input_producer/fraction_of_32_full/limit_epochs/epochs\r\n\t [[Node: input/input_producer/input_producer/fraction_of_32_full/limit_epochs/CountUpTo = CountUpTo[T=DT_INT64, _class=[\"loc:@input/input_producer/input_producer/fraction_of_32_full/limit_epochs/epochs\"], limit=2, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input/input_producer/input_producer/fraction_of_32_full/limit_epochs/epochs)]]\r\n  ", "comments": ["Ping @ilblackdragon", "This is fixed."]}, {"number": 6014, "title": "[loss_op]Denote the returned value is already mean, it happens quite a few tim\u2026", "body": "\u2026es that we often add tf.reduce_mean after that since tf.nn.sparse... returns loss without averaging it.", "comments": ["Can one of the admins verify this patch?", "Can you make this change to python/ops/losses/loss.py as well? And for both the sparse and non-sparse versions?", "@martinwicke Since I didn't manage to find python/ops/losses/losses.py in the patch for this pull request, so I created a new patch with a new request #6359 for this additional change. ", "Ok, I'll close this one then.", "Oh wait, I see. Can you add the content of this PR to the other one? It'll be cleaner that way."]}, {"number": 6013, "title": "Fix incorrect version string in gcs_test/Dockerfile", "body": "Also avoid hard-coding version string in the Dockerfile.\r\nThe pip wheel is now specified as an argument to gcs_smoke.sh.", "comments": ["Passed experimental build (viewing requires jenkins login):\r\nhttp://ci.tensorflow.org/view/Experimental/job/experimental-cais-gcs-smoke/1/console"]}, {"number": 6012, "title": "Editing Tensorboard graphs", "body": "Thanks for the powerful package, equipped with an awesome tensorboard like visualization environment.\r\n\r\nThere is just one issue. Graphs generated under Tensorboard occupy too much space than necessary, which makes it difficult to understand what is going on, thanks to consequent relative reduction in font size for all the blocks. It would be really helpful if user could move the nodes around, such that rearranged view is printer friendly, and also user friendly. Please implement the feature soonest you can, as it will help newbies like me (and anyone in general, since this package too is quite new relatively) to ramp up quickly, and also for debugging the code. Thank you.\r\n\r\nVedhas", "comments": ["This would be a really involved change, since we passively rely on dagre for layout of the graph. If we took a naive approach to adding draggable nodes, it would reset every time the user collapsed or expanded a node. \r\n\r\nIt may be there are simpler approaches for solving your problem, e.g. filtering out certain subsections of the graph so that it is easier to display and understand. In any case, I am going to mark this issue \"Contributions Welcome\" in case someone else wants to take it on, but adding manual graph layout is not on our roadmap.", "I notice that there is option to download the graph as a png image. How about making another downloadable format available? Perhaps SVG, which people can edit easily on inkscape/gimp like tools for own documentation...", "I closed this issue (because TensorBoard is moving into its own repo outside of tensorflow/tensorflow), and started another issue to track @vedhas's proposal of having the graph explorer provide SVGs: tensorflow/tensorboard#66"]}, {"number": 6011, "title": "Is it possible to implement spatial pyramid pooling layer with tensorflow?", "body": "I would like to implement the spatial pyramid pooling layer as introduced [in this paper.](https://arxiv.org/pdf/1406.4729v4.pdf)\r\nAs the paper setting, the keypoint is to define variant kernel size and stride size of max_pooling layer, which is:\r\n```\r\nkernel_size = ceil(a/n)\r\nstride_size = floor(a/n)\r\n```\r\nwhere `a` is the input tensor spatial size, and `n` is the pyramid level, i.e. spatial bins of the pooling output.\r\n\r\nI try to implement this layer with tensorflow, the code is:\r\n\r\n```\r\ndef spp_layer(input_, name = 'SPP_layer'):\r\n    '''4 level spp layer \r\n    spatial bins: [6_6, 3_3, 2_2, 1_1] '''\r\n\r\n    shape = input_.get_shape().as_list()\r\n\r\n    with tf.variable_scope(name):\r\n\r\n        spp_6_6_pool = tf.nn.max_pool(input_, ksize=[1, np.ceil(shape[1]/6).astype(np.int32), np.ceil(shape[1]/6).astype(np.int32), 1], \r\n                                      strides=[1, shape[1]//6, shape[2]//6, 1], \r\n                                      padding='SAME')\r\n        print('SPP layer level 6:', spp_6_6_pool.get_shape().as_list())\r\n\r\n        spp_3_3_pool = tf.nn.max_pool(input_, ksize=[1, np.ceil(shape[1]/3).astype(np.int32), np.ceil(shape[2]/3).astype(np.int32), 1], \r\n                                      strides=[1, shape[1]//3, shape[2]//3, 1], \r\n                                      padding='SAME')\r\n        print('SPP layer level 3:', spp_3_3_pool.get_shape().as_list())\r\n\r\n        spp_2_2_pool = tf.nn.max_pool(input_, ksize=[1, np.ceil(shape[1]/2).astype(np.int32), np.ceil(shape[2]/2).astype(np.int32), 1], \r\n                                      strides=[1, shape[1]//2, shape[2]//2, 1], \r\n                                      padding='SAME')\r\n        print('SPP layer level 2:', spp_2_2_pool.get_shape().as_list())\r\n\r\n        spp_1_1_pool = tf.nn.max_pool(input_, ksize=[1, np.ceil(shape[1]/1).astype(np.int32), np.ceil(shape[2]/1).astype(np.int32), 1], \r\n                                      strides=[1, shape[1]//1, shape[2]//1, 1], \r\n                                      padding='SAME')\r\n        print('SPP layer level 1:', spp_1_1_pool.get_shape().as_list())\r\n\r\n\r\n        spp_6_6_pool_flat = tf.reshape(spp_6_6_pool, [shape[0], -1])\r\n        spp_3_3_pool_flat = tf.reshape(spp_3_3_pool, [shape[0], -1])\r\n        spp_2_2_pool_flat = tf.reshape(spp_2_2_pool, [shape[0], -1])\r\n        spp_1_1_pool_flat = tf.reshape(spp_1_1_pool, [shape[0], -1])\r\n\r\n        spp_pool = tf.concat(1, [spp_6_6_pool_flat, spp_3_3_pool_flat, spp_2_2_pool_flat, spp_1_1_pool_flat])\r\n\r\n    return spp_pool\r\n```\r\n\r\nBut it cannot gurantee the same length pooling output, when the input sizes are different.\r\n\r\nDoes tensorflow support the fixed length spatial pyramid pooling layer?\r\n\r\nI also post the question on the [stackoverflow](http://stackoverflow.com/questions/40913794/tensorflow-how-to-implement-the-fixed-length-spatial-pyramid-pooling-layer).\r\n\r\n\r\n", "comments": ["I change the padding from 'SAME' to 'VALID' and it is also not invariant to the input data shape", "This seems very similar to Atrous Convolution ([Paper]( http://arxiv.org/abs/1606.00915)) for which keras has a Tensorflow  implementation ([https://github.com/fchollet/keras/blob/master/keras/backend/tensorflow_backend.py#L1739](https://github.com/fchollet/keras/blob/master/keras/backend/tensorflow_backend.py#L1739))", "@karlTUM, you can try the code below:\r\n\r\n```\r\ndef spp_layer2(input_, levels=[2, 1], name = 'SPP_layer'):\r\n    '''Multiple Level SPP layer.\r\n       Works for levels=[1, 2, 3, 6].'''\r\n    shape = input_.get_shape().as_list()\r\n    with tf.variable_scope(name):\r\n        pool_outputs = []\r\n        for l in levels:\r\n            pool = tf.nn.max_pool(input_, ksize=[1, np.ceil(shape[1] * 1. / l).astype(np.int32), np.ceil(shape[2] * 1. / l).astype(np.int32), 1], \r\n                                      strides=[1, np.floor(shape[1] * 1. / l + 1).astype(np.int32), np.floor(shape[2] * 1. / l + 1), 1], \r\n                                      padding='SAME')\r\n            print \"Pool Level {:}: shape {:}\".format(l, pool.get_shape().as_list())\r\n            pool_outputs.append(tf.reshape(pool, [shape[0], -1]))\r\n        spp_pool = tf.concat(1, pool_outputs)\r\n    return spp_pool\r\n```\r\n\r\nI haven't extensively tested it, but it outputs a vector of the same length (in case of `levels=[1, 2, 3, 6]`): `50=[1x1 + 2x2 + 3x3 + 6x6]x(channels)` for every input with each spatial dimension more (or equal) than `36=6x6`.", "@DrSleep  Thank you.", "I can't get it to work on python3. Any recommendations?\r\n\r\nOutput\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-24-41b88a9b0081> in <module>()\r\n     12 rate = 0.001\r\n     13 \r\n---> 14 logits = TrafficNet(x)\r\n     15 cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits, one_hot_y)\r\n     16 loss_operation = tf.reduce_mean(cross_entropy)\r\n\r\n<ipython-input-23-673412c4ac1b> in TrafficNet(x)\r\n     71         return spp_pool\r\n     72 \r\n---> 73     pool3 = spp_layer2(conv13)\r\n     74 \r\n     75     from IPython.core.debugger import Tracer\r\n\r\n<ipython-input-23-673412c4ac1b> in spp_layer2(input_, levels, name)\r\n     67                                       strides=[1, np.floor(shape[1] * 1. / l + 1).astype(np.int32), np.floor(shape[2] * 1. / l + 1), 1],\r\n     68                                       padding='SAME')\r\n---> 69                 pool_outputs.append(tf.reshape(pool, [shape[0], -1]))\r\n     70             spp_pool = tf.concat(1, pool_outputs)\r\n     71         return spp_pool\r\n\r\n/Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py in reshape(tensor, shape, name)\r\n   2446   \"\"\"\r\n   2447   result = _op_def_lib.apply_op(\"Reshape\", tensor=tensor, shape=shape,\r\n-> 2448                                 name=name)\r\n   2449   return result\r\n   2450 \r\n\r\n/Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py in apply_op(self, op_type_name, name, **keywords)\r\n    491           except TypeError as err:\r\n    492             if dtype is None:\r\n--> 493               raise err\r\n    494             else:\r\n    495               raise TypeError(\r\n\r\n/Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py in apply_op(self, op_type_name, name, **keywords)\r\n    488                 dtype=dtype,\r\n    489                 as_ref=input_arg.is_ref,\r\n--> 490                 preferred_dtype=default_dtype)\r\n    491           except TypeError as err:\r\n    492             if dtype is None:\r\n\r\n/Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype)\r\n    667 \r\n    668         if ret is None:\r\n--> 669           ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n    670 \r\n    671         if ret is NotImplemented:\r\n\r\n/Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)\r\n    174                                          as_ref=False):\r\n    175   _ = as_ref\r\n--> 176   return constant(v, dtype=dtype, name=name)\r\n    177 \r\n    178 \r\n\r\n/Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name, verify_shape)\r\n    163   tensor_value = attr_value_pb2.AttrValue()\r\n    164   tensor_value.tensor.CopyFrom(\r\n--> 165       tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n    166   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)\r\n    167   const_tensor = g.create_op(\r\n\r\n/Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape)\r\n    439   if numpy_dtype == dtypes.string and not isinstance(values, np.ndarray):\r\n    440     proto_values = _FlattenToStrings(values)\r\n--> 441     tensor_proto.string_val.extend([compat.as_bytes(x) for x in proto_values])\r\n    442     return tensor_proto\r\n    443 \r\n\r\n/Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py in <listcomp>(.0)\r\n    439   if numpy_dtype == dtypes.string and not isinstance(values, np.ndarray):\r\n    440     proto_values = _FlattenToStrings(values)\r\n--> 441     tensor_proto.string_val.extend([compat.as_bytes(x) for x in proto_values])\r\n    442     return tensor_proto\r\n    443 \r\n\r\n/Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/site-packages/tensorflow/python/util/compat.py in as_bytes(bytes_or_text, encoding)\r\n     63   else:\r\n     64     raise TypeError('Expected binary or unicode string, got %r' %\r\n---> 65                     (bytes_or_text,))\r\n     66 \r\n     67 \r\n\r\nTypeError: Expected binary or unicode string, got None\r\n```", "It is not a problem with python3. \r\nYou have this error because the batch size of your input is not defined (it is None), and it is referred to in the reshape operation: `pool_outputs.append(tf.reshape(pool, [shape[0], -1]))`.\r\nYou can fix it in multiple ways: for example, rather than keeping `-1` to find out the hidden size, you can infer the batch size instead and run `pool_outputs.append(tf.reshape(pool, [-1, l * l * shape(3)]))`. Another way is to just use `tf.shape(input_)[0]`, which is a tensor.", "It seems that the solution above only works if the input size is known beforehand (since the size of the pooling window is static). In order to work with any input sizes, the argument to the pooling operation should be a symbolic variable (so that the size of the pooling window is calculated at execution time). However, this doesn't seem to be supported yet by the max-pooling operation in tensorflow.\r\n\r\nFor reference, here is the implementation of SPP on lasagne: https://github.com/Lasagne/Lasagne/blob/master/lasagne/layers/dnn.py#L510-L596. It uses a pooling implementation from theano.sandbox.cuda.dnn that supports symbolic variables as arguments", "Is anyone aware of work towards making TF's pooling layer accept a tensor with size defined at runtime?", "@javiribera -  FYI, I implemented a version of SPP in Theano+lasagne that does not use the standard pooling operation, but instead manually slices the input region. Take a look in the function \"pool_2d_nxn_regions\" here:\r\nhttps://github.com/Lasagne/Lasagne/pull/799/commits/c01e3d922a5712ca4c54617a15a794c23746ac8c#diff-d0bc475ce6f46dfdf62cea86f4a2b911\r\n\r\nI think that this can be implemented in tensorflow. It requires: slicing a tensor using another tensor as indices (which is supported, at least in version 1.0), symbolical versions of floor and ceil (tf.floor, tf.ceil), and casting to integer (tf.cast).", "@luizgh How do you deal with back-propagation in your new pooling layer? If I use slice operations in one pooling layer, can tensorflow train it using auto diff?", "Hi @Evan-Gao,\r\n\r\nTheano will backpropagate only through the part of the input tensor that was selected in the slice operation. I ran a quick experiment and Tensorflow does the same:\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ninputs = tf.placeholder(dtype=tf.float32, shape=(2,3,3))\r\nwhich_channel = tf.placeholder(dtype=tf.int32)\r\nsliced = inputs[which_channel]\r\ncost = tf.norm(sliced)\r\ngrad = tf.gradients(cost, inputs)\r\n\r\nsess = tf.Session()\r\nprint(sess.run(grad, feed_dict={inputs: np.ones((2,3,3)), which_channel:0}))\r\nprint(sess.run(grad, feed_dict={inputs: np.ones((2,3,3)), which_channel:1}))\r\n```\r\n\r\nThis code uses a symbolic variable (\"which_channel\") to slice the input, and considers the cost as the norm of the sliced tensor. We can see that backpropagation will only affect the part of the input that was selected in the slice operation: when we select the channel 0, it shows a gradient different than 0 for the first channel, with zeros in the gradient for the second channel, and vice versa when we select which_channel=1\r\n\r\n", "@luizgh thanks for all these comments!  I'm just wading into this conversation now, but wanted to point out that there was a layer implemented for Keras [here](https://github.com/yhenon/keras-spp/blob/master/SpatialPyramidPooling.py).  I'm not sure, but it looks like it implements pooling directly by indexing into the input and using tf.reduce_max() under the hood.  \r\n\r\nAny comments?  ", "@andrewgiessel Indeed, it seems that they are manually slicing the tensors (as I did for implementing it in lasagne). It looks, however, that they do the indexing in a different way than the original paper - for instance the pooling regions seem to be non-overlapping (that's not necessarily a problem). If you want to implement it as in the original paper, you can port the code from this pool request (for theano+lasagne) - it should be straightforward as Tensorflow seem to support all the necessary operations.\r\n\r\nhttps://github.com/Lasagne/Lasagne/pull/799/\r\n", "@luizgh @andrewgiessel I'm new to keras. It seems the spp pooling layer and RoI pooling layer in that keras implementation just support forward operation. Can it be used to train a new network via back propagation? \r\nI have seen some other code adding a new layer by implementing forward and backward separately (with cpu and gpu version). But I am searching for a easier way to implement a custom pooling layer.", "I gave it a shot and tried to port the Lasagne implementation by @luizgh to TensorFlow:\r\n\r\n```python\r\ndef max_pool_2d_nxn_regions(inputs, output_size: int, mode: str):\r\n    \"\"\"\r\n    Performs a pooling operation that results in a fixed size:\r\n    output_size x output_size.\r\n    \r\n    Used by spatial_pyramid_pool. Refer to appendix A in [1].\r\n    \r\n    Args:\r\n        inputs: A 4D Tensor (B, H, W, C)\r\n        output_size: The output size of the pooling operation.\r\n        mode: The pooling mode {max, avg}\r\n        \r\n    Returns:\r\n        A list of tensors, for each output bin.\r\n        The list contains output_size * output_size elements, where\r\n        each elment is a Tensor (N, C).\r\n        \r\n    References:\r\n        [1] He, Kaiming et al (2015):\r\n            Spatial Pyramid Pooling in Deep Convolutional Networks\r\n            for Visual Recognition.\r\n            https://arxiv.org/pdf/1406.4729.pdf.\r\n            \r\n    Ported from: https://github.com/luizgh/Lasagne/commit/c01e3d922a5712ca4c54617a15a794c23746ac8c\r\n    \"\"\"\r\n    inputs_shape = tf.shape(inputs)\r\n    h = tf.cast(tf.gather(inputs_shape, 1), tf.int32)\r\n    w = tf.cast(tf.gather(inputs_shape, 2), tf.int32)\r\n    \r\n    if mode == 'max':\r\n        pooling_op = tf.reduce_max\r\n    elif mode == 'avg':\r\n        pooling_op = tf.reduce_mean\r\n    else:\r\n        msg = \"Mode must be either 'max' or 'avg'. Got '{0}'\"\r\n        raise ValueError(msg.format(mode))\r\n        \r\n    result = []\r\n    n = output_size\r\n    for row in range(output_size):\r\n        for col in range(output_size):\r\n            # start_h = floor(row / n * h)\r\n            start_h = tf.cast(tf.floor(tf.mul(tf.divide(row, n), tf.cast(h, tf.float32))), tf.int32)\r\n            # end_h = ceil((row + 1) / n * h)\r\n            end_h = tf.cast(tf.ceil(tf.mul(tf.divide((row + 1), n), tf.cast(h, tf.float32))), tf.int32)\r\n            # start_w = floor(col / n * w)\r\n            start_w = tf.cast(tf.floor(tf.mul(tf.divide(col, n), tf.cast(w, tf.float32))), tf.int32)\r\n            # end_w = ceil((col + 1) / n * w)\r\n            end_w = tf.cast(tf.ceil(tf.mul(tf.divide((col + 1), n), tf.cast(w, tf.float32))), tf.int32)\r\n            pooling_region = inputs[:, start_h:end_h, start_w:end_w, :]\r\n            pool_result = pooling_op(pooling_region, axis=(1, 2))\r\n            result.append(pool_result)\r\n    return result\r\n\r\ndef spatial_pyramid_pool(inputs, dimensions=[2,1], mode='max', implementation='kaiming'):\r\n    \"\"\"\r\n    Performs spatial pyramid pooling (SPP) over the input.\r\n    It will turn a 2D input of arbitrary size into an output of fixed\r\n    dimenson.\r\n    Hence, the convlutional part of a DNN can be connected to a dense part\r\n    with a fixed number of nodes even if the dimensions of the input\r\n    image are unknown.\r\n    \r\n    The pooling is performed over :math:`l` pooling levels.\r\n    Each pooling level :math:`i` will create :math:`M_i` output features.\r\n    :math:`M_i` is given by :math:`n_i * n_i`, with :math:`n_i` as the number\r\n    of pooling operations per dimension level :math:`i`.\r\n    \r\n    The length of the parameter dimensions is the level of the spatial pyramid.\r\n    \r\n    Args:\r\n        inputs: A 4D Tensor (B, H, W, C).\r\n        dimensions: The list of :math:`n_i`'s that define the output dimension\r\n        of each pooling level :math:`i`. The length of dimensions is the level of\r\n        the spatial pyramid.\r\n        mode: Pooling mode 'max' or 'avg'.\r\n        implementation: The implementation to use, either 'kaiming' or 'fast'.\r\n        kamming is the original implementation from the paper, and supports variable\r\n        sizes of input vectors, which fast does not support.\r\n    \r\n    Returns:\r\n        A fixed length vector representing the inputs.\r\n    \r\n    Notes:\r\n        SPP should be inserted between the convolutional part of a DNN and it's\r\n        dense part. Convolutions can be used for arbitrary input dimensions, but\r\n        the size of their output will depend on their input dimensions.\r\n        Connecting the output of the convolutional to the dense part then\r\n        usually demands us to fix the dimensons of the network's input.\r\n        The spatial pyramid pooling layer, however, allows us to leave \r\n        the network input dimensions arbitrary. \r\n        The advantage over a global pooling layer is the added robustness \r\n        against object deformations due to the pooling on different scales.\r\n        \r\n    References:\r\n        [1] He, Kaiming et al (2015):\r\n            Spatial Pyramid Pooling in Deep Convolutional Networks\r\n            for Visual Recognition.\r\n            https://arxiv.org/pdf/1406.4729.pdf.\r\n            \r\n    Ported from: https://github.com/luizgh/Lasagne/commit/c01e3d922a5712ca4c54617a15a794c23746ac8c\r\n    \"\"\"\r\n    pool_list = []\r\n    if implementation == 'kaiming':\r\n        for pool_dim in dimensions:\r\n            pool_list += max_pool_2d_nxn_regions(inputs, pool_dim, mode)\r\n    else:\r\n        shape = inputs.get_shape().as_list()\r\n        for d in dimensions:\r\n            h = shape[1]\r\n            w = shape[2]\r\n            ph = np.ceil(h * 1.0 / d).astype(np.int32)\r\n            pw = np.ceil(w * 1.0 / d).astype(np.int32)\r\n            sh = np.floor(h * 1.0 / d + 1).astype(np.int32)\r\n            sw = np.floor(w * 1.0 / d + 1).astype(np.int32)\r\n            pool_result = tf.nn.max_pool(inputs,\r\n                                         ksize=[1, ph, pw, 1], \r\n                                         strides=[1, sh, sw, 1],\r\n                                         padding='SAME')\r\n            pool_list.append(tf.reshape(pool_result, [tf.shape(inputs)[0], -1]))\r\n    return tf.concat(1, pool_list)\r\n```", "mark", "\u4f60\u597d\uff0c\u76ee\u524d\u6211\u7528tf.py_func\u5b9e\u73b0\u7684\uff0c\u5229\u7528tf\u81ea\u52a8\u6c42\u5bfc\u7684\u7279\u6027\uff0c\u6ca1\u6709\u5199\u76f8\u5e94\u7684backward\u51fd\u6570\u3002\r\n\r\nFrom: thewintersun <notifications@github.com<mailto:notifications@github.com>>\r\nReply-To: tensorflow/tensorflow <reply@reply.github.com<mailto:reply@reply.github.com>>\r\nDate: Monday, 10 April 2017 at 1:57 PM\r\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com<mailto:tensorflow@noreply.github.com>>\r\nCc: Yifan Gao <Evan-Gao@outlook.com<mailto:Evan-Gao@outlook.com>>, Mention <mention@noreply.github.com<mailto:mention@noreply.github.com>>\r\nSubject: Re: [tensorflow/tensorflow] Is it possible to implement spatial pyramid pooling layer with tensorflow? (#6011)\r\n\r\n\r\n\u697c\u4e3b\uff0c\u4f60\u8fd9\u4e2a\u95ee\u9898\uff0c\u540e\u6765\u7528tensorflow\u641e\u5b9a\u4e86\u5417\uff1f\r\n\r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/6011#issuecomment-292856399>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AWDOt69pNAoYLPOFJRRxP-PxcHUHkkjAks5rucTmgaJpZM4LBgLy>.\r\n", "why SPP net cannot update  parameters below SPP  layer during training\uff1f", "Made the PR for SPP implementation. https://github.com/tensorflow/tensorflow/pull/12852", "@RikHeijdens  Thx", "I tested RikHeijdens' code in tf1.6. `tf.mul` should be changed as `tf.multiply`.  And The function`spatial_pyramid_pool` should return  `tf.concat(pool_list, 1)` instead.", "  Hello, have you realize that with different size of input  images even though output number is same, how you unify the input layer? My point is during training, if I set sess.graph.finalize(), when switch to new image training I will get error Graph is finalized and cannot be modified. How can we keep Graph consistent during training different size images.  ", "> @karlTUM, you can try the code below:\r\n> \r\n> ```\r\n> def spp_layer2(input_, levels=[2, 1], name = 'SPP_layer'):\r\n>     '''Multiple Level SPP layer.\r\n>        Works for levels=[1, 2, 3, 6].'''\r\n>     shape = input_.get_shape().as_list()\r\n>     with tf.variable_scope(name):\r\n>         pool_outputs = []\r\n>         for l in levels:\r\n>             pool = tf.nn.max_pool(input_, ksize=[1, np.ceil(shape[1] * 1. / l).astype(np.int32), np.ceil(shape[2] * 1. / l).astype(np.int32), 1], \r\n>                                       strides=[1, np.floor(shape[1] * 1. / l + 1).astype(np.int32), np.floor(shape[2] * 1. / l + 1), 1], \r\n>                                       padding='SAME')\r\n>             print \"Pool Level {:}: shape {:}\".format(l, pool.get_shape().as_list())\r\n>             pool_outputs.append(tf.reshape(pool, [shape[0], -1]))\r\n>         spp_pool = tf.concat(1, pool_outputs)\r\n>     return spp_pool\r\n> ```\r\n> \r\n> I haven't extensively tested it, but it outputs a vector of the same length (in case of `levels=[1, 2, 3, 6]`): `50=[1x1 + 2x2 + 3x3 + 6x6]x(channels)` for every input with each spatial dimension more (or equal) than `36=6x6`.\r\n\r\nLets say input is like a placeholder of unknown shape eg. input_= tf.placholder(tf.float32, shape=(batch_size, None, 3,1)) \r\nso shape[1]/n will not work. If you have any idea please share.", "@RikHeijdens How can i use it instead of Maxpooling?"]}, {"number": 6010, "title": "Public access to current absolute name scope", "body": "I can get it via `tf.get_default_graph()._name_stack`. But I think there is no public API to get that information. Some public API would be nice.", "comments": ["I have no objection to this feature request, but the correct resolution probably depends on the resolution to #6007 (i.e. could we use **variable** scopes instead?). Assigning to @lukaszkaiser, in the interests of message consistency.", "I do not see any compelling use case for this. As we're trying to migrate the public API away from name_scope and towards variable_scope, I'd be against adding it without a compelling need."]}, {"number": 6009, "title": "Use correct variable scope for weights in rnn_cell._linear.", "body": "If a scope is passed in it should be used for the variable scope\r\nrather than always falling back to the default scope.\r\n\r\nThis change in behaviour was a consequence of recent cleanup in\r\n92da8abf. The commit may alter the name of \"weights\" variable in saved\r\ncheckpoints but is necessary if two `rnn_cell._linear` layers exist in\r\nthe same parent variable scope.", "comments": ["Can one of the admins verify this patch?", "@ebrevdo - I guess you might be interested in this one since I don't think you intended to change the behaviour here (it breaks some uses of `_linear`, even though that may not be part of the public API atm).", "It is hidden for a reason; you should not be using it.  Use tf.contrib.layers.linear instead.  The scope argument will be removed soon."]}, {"number": 6008, "title": "Unintuitive error messages when using functions that require a axis parameter", "body": "Multiple tensorflow functions (I tested tf.argmax,tf.argmin), that require axis parameters to be provided, show very unintuitive error messages if the axis parameter is omitted.\r\n\r\nIt would probably be more convenient to either throw exceptions earlier with more intuitive error descriptions ('Axis parameter must be passed/must not be None') or to define the axis parameters as a required positional parameters. \r\n\r\n### Environment info\r\nOperating System:\r\nOS X 10.11.6, running with cpu backend\r\n\r\n1. A link to the pip package you installed:\r\nhttps://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.12.0rc0-py3-none-any.whl\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.:\r\n0.12.0-rc0\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n```\r\nimport tensorflow as tf\r\nt =tf.get_variable('test', shape=[100])\r\nmaxt = tf.argmax(t)\r\n```\r\n\r\n### Logs or other output that would be helpful\r\nException:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/maexlich/.virtualenvs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 490, in apply_op\r\n    preferred_dtype=default_dtype)\r\n  File \"/Users/maexlich/.virtualenvs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 669, in convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/Users/maexlich/.virtualenvs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py\", line 176, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"/Users/maexlich/.virtualenvs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py\", line 165, in constant\r\n    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n  File \"/Users/maexlich/.virtualenvs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py\", line 360, in make_tensor_proto\r\n    raise ValueError(\"None values not supported.\")\r\nValueError: None values not supported.\r\nDuring handling of the above exception, another exception occurred:\r\nTraceback (most recent call last):\r\n  File \"/Users/maexlich/.virtualenvs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-6-8fd26b27f1ef>\", line 1, in <module>\r\n    max = tf.argmax(t)\r\n  File \"/Users/maexlich/.virtualenvs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py\", line 249, in argmax\r\n    return gen_math_ops.arg_max(input, axis, name)\r\n  File \"/Users/maexlich/.virtualenvs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 168, in arg_max\r\n    name=name)\r\n  File \"/Users/maexlich/.virtualenvs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 503, in apply_op\r\n    as_ref=input_arg.is_ref).dtype.name\r\n  File \"/Users/maexlich/.virtualenvs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 669, in convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/Users/maexlich/.virtualenvs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py\", line 176, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"/Users/maexlich/.virtualenvs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py\", line 165, in constant\r\n    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n  File \"/Users/maexlich/.virtualenvs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py\", line 360, in make_tensor_proto\r\n    raise ValueError(\"None values not supported.\")\r\nValueError: None values not supported.\r\n```", "comments": ["@aselle This looks like a bug to me, but you know that code better now, wdyt?", "Thanks for the issue @maexlich. Unfortunately, the current state of affairs is part of the process of renaming the arguments to be \"axis\". To support the old argument during deprecation, we have to make axis have a default of None, which is invalid. This code is going to go away, and axis will go back to being required. That will be happening soon, but if it is delayed, I will see if I can do a quick step of throwing exceptions when axis is None in these cases.", "Automatically closing due to lack of recent activity. We hope that you were able to resolve it on your own. However, since this is a support issue rather than a bug or feature request, you will probably get more information by posting it on StackOverflow."]}, {"number": 6007, "title": "tf.name_scope with reuse", "body": "I want to reuse a name scope which was created earlier with `tf.name_scope`. Normally, name_scope will create a new unique name in the current namespace. If it ends with \"/\", it will ignore the current namespace and use it as an absolute name scope but then it doesn't make it unique. I want a way that it just uses the name as I provide it in the current namespace.\r\n\r\nCurrent code:\r\n```\r\n  @contextlib.contextmanager\r\n  def name_scope(self, name):\r\n      ...\r\n      old_stack = self._name_stack\r\n      if not name:  # Both for name=None and name=\"\" we re-set to empty scope.\r\n        new_stack = None\r\n      elif name and name[-1] == \"/\":\r\n        new_stack = name[:-1]\r\n      else:\r\n        new_stack = self.unique_name(name)\r\n      self._name_stack = new_stack\r\n      yield \"\" if new_stack is None else new_stack + \"/\"\r\n```\r\n\r\nI suggest something like:\r\n\r\n```\r\n  @contextlib.contextmanager\r\n  def name_scope(self, name, reuse=False):\r\n    if reuse:\r\n      new_stack = self._name_stack + \"/\" + name\r\n    else:\r\n      ...\r\n```\r\n", "comments": ["What would be the motivation for this feature? At the very least, its seems to conflict with the way `reuse` is meant in variable scopes, and that could be confusing for users.", "I thought that `reuse` for variable scope has a very similar meaning ? (or even the same?)\r\n\r\nThe motivation is this:\r\n\r\n```\r\nwith tf.name_scope(\"foo\"):\r\n   # sth\r\n\r\n# ...\r\n\r\nwith tf.name_scope(\"foo\", reuse=True):  # don't create \"Foo_1\" but use \"Foo\" name scope\r\n  # sth\r\n```\r\n\r\n", "Wouldn't the following work just as well?\r\n\r\n```python\r\n\r\nwith tf.name_scope(\"foo\") as scope:\r\n   # sth\r\n\r\n# ...\r\n\r\nwith tf.name_scope(scope):\r\n  # sth\r\n```", "@mrry Yes but than I need keep track about all the scopes which is not what I want. In my specific case, in one place of the code, I create all the variables with a deeper scope hierarchy and in another separated place, I define my computation graph which uses these vars, and I want it to be in the same scopes.", "I have no idea if that's something that we'd like to support. I'll defer to @lukaszkaiser, who added the equivalent feature in `tf.variable_scope()` and has been simplifying the existing scopes over the last few months.", "We were considering this long time ago as the first version of variable_scope. Then the reuse checks came and so on -- it basically evlolved into variable_scope. Currently, we recommend everyone to use variable_scope and not use name_scope except for internal code and libraries. Wouldn't just using variable_scope solve your problem?", "I just checked the code of variable_scope, and it seems as if even with reuse=True, it would not reuse the name_scope. The reuse option doesn't seem to have an effect on the name_scope. It's basically this code, if I use `tf.variable_scope(\"foo\", reuse=True)`:\r\n\r\n```\r\n  with g.as_default():\r\n      name_scope = name_or_scope\r\n      with ops.name_scope(name_scope) as cur_name_scope:\r\n          with _pure_variable_scope(\r\n              name_or_scope,\r\n              reuse=reuse,\r\n          ...\r\n```\r\nSo, as it is right now, even variable_scope would not reuse the name scope, but I want to reuse the name scope. Is that even the intended behavior of variable_scope or a bug?\r\n", "The idea of reusing a name_scope is a bit strange, because every op in the graph has to have a unique name anyway in the end. So you're only pushing back where the suffix will be added, it will go at the end od the op-name instead of the scope name. You're aware of that, right? I just want to make sure that that's really the goal you want to achieve.\r\n\r\nIf that's the goal, then you can do this with variable_scope and its .original_name_scope property.\r\n\r\n```\r\n# First time just open variable_scope.\r\nwith tf.variable_scope(\"test\") as scope:\r\n  # do things, name_scope = \"test\"\r\n  ...\r\n# Next time open name_scope with scope.original_name_scope.\r\nwith tf.name_scope(scope.original_name_scope):\r\n   # we're back in the name_scope as before\r\n```\r\n\r\nDoes that help?", "Yes, that's my goal. Your suggestion is basically what @mrry suggested but this solution is not optimal because I would need to keep the `scope` variable around for all scopes where I want to do this, which is for almost all scopes. This is because I create all the variables for my model at one place, with a specific name scope hierarchy, and then at some later time, I create the computation graph, and I want it to use the same name scope hierarchy.", "I see, but your use case is very specific and very strange. I think what you want deep down might be an object-oriented layer API like in Keras, and it is being done in tf.layers and will be part of contrib. In its technical aspect it relies on storing the scope as part of an object, as discussed above. In all cases, I think we can close this issue.", "Hi,\r\n\r\nAs a matter of fact, it appears that I had the same use case than albertz \r\n- variables created and stored in one place,\r\n- various operations defined later and if possible in the same scope than the variables.\r\n\r\nI ended up independently to the same conclusion that the scope object had to be stored in the same place than the variables.\r\n\r\nIt looks like it's not the right way to work with tensorflow variables and so I wonder if you could point me to some guidelines/good practices to structure models. [I'm currently following the practices described here: https://danijar.com/structuring-your-tensorflow-models/, but it doesn't work so well as mentioned above when you want to use the same scope for your operations).\r\n\r\nThx!\r\n", "I think there are 2 good ways to structure TF models.\r\n\r\n* The functional / scoping way. Everything is a function, create variables inside (don't worry about creating them beforehand). I use this a lot for research as it makes it super-easy to just put a new layer here and there. It is the basis of [tensorflow-slim](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim) and there are many examples in the [slim models library](https://github.com/tensorflow/models/tree/master/slim), e.g., take a look at [AlexNet](https://github.com/tensorflow/models/blob/master/slim/nets/alexnet.py).\r\n\r\n* The OO way. Every layer is an object, you need to define composition models. We're still working on putting it in core TF (see tf.layers implementation, a lot is there already). But, in essence, it's the same ideas as [Keras](https://github.com/fchollet/keras) and its conventions.\r\n\r\nHope that helps!", "Thx, it definitely  helps. I appreciate!", "I have another use-case for the having multiple variables have the same name. (I might be completely misunderstanding something, any help is appreciated)\r\n\r\nI want to track some variables in TensorBoard. I am saving the validation loss and training loss (and other things) in separate directories, such that TensorBoard thinks they are from separate runs. The only problem with this approach is that duplicate names get a '`_1`' appended to the name, so they don't show up in the same plot.", "@Faur Your issue is more related to #6150 than this one.", "What about the following use case:\r\nI am observing the gradients in my model with summary operations, but there are two separate parts of the codes where gradients are calculated. However, I would like the summaries to go into the same name scope. ", "Note that I have implemented this function myself [here in `reuse_name_scope`](https://github.com/rwth-i6/returnn/blob/master/TFUtil.py#L491). This is a bit ugly right now because I wanted it to support both the variable scope and the name scope.", "I found out that at least in the current version (1.2), if you call `tf.name_scope` with a string ending in `/` it will _not_ add a suffix even if it already exists. Although it is not documented, I hope the developers let it be like that; for me, it seems fairly clear that saying `tf.name_scope(\"my_scope/\")` means that I explicitly want to use the scope `my_scope` exactly.", "A string ending with a slash will be interpret as an absolute name also,\nnot relative.\n\nThis does not work however for variable_scope.\n\n\nAm 12.07.2017 19:13 schrieb \"Javier Dehesa\" <notifications@github.com>:\n\n> I found out that at least in the current version (1.2), if you call\n> tf.name_scope with a string ending in / it will *not* add a suffix even\n> if it already exists. Although it is not documented, I hope the developers\n> let it be like that; for me, it seems fairly clear that saying\n> tf.name_scope(\"my_scope/\") means that I explicitly want to use the scope\n> my_scope exactly.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6007#issuecomment-314835518>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AADm_NetQBtPgqKV_rni4eFqdWz0qryOks5sNP63gaJpZM4LBMne>\n> .\n>\n", "@albertz Thanks for the info. I guess the complete snippet to reuse an existing name scope would then be:\r\n\r\n```\r\ncurrent_name_scope = tf.get_default_graph().get_name_scope()\r\nif len(current_name_scope) > 0:\r\n    current_name_scope += \"/\"\r\nwith tf.name_scope(\"{}my_scope/\".format(current_name_scope)):\r\n    # ...\r\n```\r\n\r\nAlthough as you said `tf.variable_scope` / `tf.get_variable` work differently. In fact, if you do:\r\n\r\n```\r\nwith tf.name_scope(\"a\"):  # or tf.variable_scope(\"a\")\r\n    pass\r\n\r\nwith tf.variable_scope(\"a\"):\r\n    v = tf.get_variable(\"v\", 0)\r\n    v2 = v * 2\r\n\r\nprint(v.op.name, v2.op.name)\r\n```\r\n\r\nYou get `a/v a_1/mul`, which I think is rather confusing (I'd expect that everything I create within a scope actually ends up in the same scope).", "@javidcf In case you are interested, I implemented `reuse_name_scope` [here](https://github.com/rwth-i6/returnn/blob/master/TFUtil.py), which tries to handle both the name scope and the variable scope. It's a bit confusing anyway, as the `tf.variable_scope` will implicitly also set the name scope but not the other way around. As @lukaszkaiser maybe implied, name scope might go away at some point, or they will be both merged together. I don't really see why there is the distinction at all.\r\n", "I think the distinction makes sense as soon as you replicate a part of a network, but feed it different inputs. For example, if you want to re-input adversarial examples back into your network to train it to classify those correctly too, you want to reuse the variable scope of the original model, but put all your operations into a separate scope (at least if you want to inspect your graph at some point and want it to be structured)", "@albertz, @javidcf I was following your discussion here, as I've been also interested in re-opening name scopes. Here's a bit of warning though (~~without having studied your implementations~~). When you add `/` to your name scope names, TF will interpret the name scope as absolute path, ignoring any parental name-scopes that you are currently in. This can be quite suprising. Here's a short example that illustrates the issue.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nwith tf.Graph().as_default() as g:\r\n   with g.name_scope('parent'):\r\n       with g.name_scope('nested'):\r\n           x = tf.placeholder(dtype=tf.float32, name='x')\r\n           print(x.name)\r\n       with g.name_scope('nested'):\r\n           y = tf.placeholder(dtype=tf.float32, name='y')\r\n           print(y.name)\r\n       with g.name_scope('nested/'):\r\n           z = tf.placeholder(dtype=tf.float32, name='z')\r\n           print(z.name)\r\n   \r\n# Gives\r\n# parent/nested/x:0\r\n# parent/nested_1/y:0\r\n# nested/z:0\r\n```\r\nAlthough no suffix was added to `nested` name scope, it lost the parental relationship to `parent`. Especially in complex architectures, you never know which name-scopes have been opened outside of your function (e.g to instance your network twice under different names).\r\n\r\n**Update** Ok, seems like your code accounts for that by prepending the current name scope"]}, {"number": 6006, "title": "split will move leading or trailing whitespace when sep is whitespace", "body": "split will move leading or trailing whitespace when sep is whitespace, so strip() is useless\r\n\r\nIf sep is not specified or is None, a different splitting algorithm is applied: runs of consecutive whitespace are regarded as a single separator, and the result will contain no empty strings at the start or end if the string has leading or trailing whitespace. Consequently, splitting an empty string or a string consisting of just whitespace with a None separator returns [].\r\n\r\nsee https://docs.python.org/3/library/stdtypes.html?highlight=split#str.split", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins Test this, please.", "@biolee can you fix the conflicts?", "AFAICS this file is not in tensorflow/tensorflow repository anymore.\r\nPlease create a new PR to tensorflow/models repository for this change."]}, {"number": 6005, "title": "Tensorboard can't show item on windows", "body": "I have install the tensorflow by pip on windows 10 64bit\r\n\r\ntensorflow have no Problem.\r\nbut,Tensorboard can't run.\r\n\r\nthis is Log\r\n---------------------------------------------------------------------------------------------------------\r\nD:\\Python35\\Lib\\site-packages\\tensorflow\\tensorboard>d:\\Python35\\python.exe tensorboard.py --logdir=D:\\tensorBoard --debug\r\nINFO:tensorflow:TensorBoard is in debug mode.\r\nINFO:tensorflow:Starting TensorBoard in directory D:\\Python35\\Lib\\site-packages\\tensorflow\\tensorboard\r\nINFO:tensorflow:TensorBoard path_to_run is: {'D:\\\\tensorBoard': 'D'}\r\nINFO:tensorflow:Event Multiplexer initializing.\r\nINFO:tensorflow:Event Multiplexer done initializing\r\nINFO:tensorflow:TensorBoard reload process beginning\r\nINFO:tensorflow:Starting AddRunsFromDirectory: D:\\tensorBoard\r\nINFO:tensorflow:Adding events from directory D:\\tensorBoard\r\nINFO:tensorflow:Constructing EventAccumulator for D:\\tensorBoard\r\nINFO:tensorflow:Done with AddRunsFromDirectory: D:\\tensorBoard\r\nINFO:tensorflow:TensorBoard reload process: Reload the whole Multiplexer\r\nINFO:tensorflow:Beginning EventMultiplexer.Reload()\r\nDEBUG:tensorflow:Opening a record reader pointing at D:\\tensorBoard\\events.out.tfevents.1480577231.SHPC052\r\nDEBUG:tensorflow:No more events in D:\\tensorBoard\\events.out.tfevents.1480577231.SHPC052\r\nINFO:tensorflow:No path found after D:\\tensorBoard\\events.out.tfevents.1480577231.SHPC052\r\nINFO:tensorflow:Finished with EventMultiplexer.Reload()\r\nINFO:tensorflow:TensorBoard done reloading. Load took 0.008 secs\r\nINFO:tensorflow:TensorBoard is tag: b'39'\r\nStarting TensorBoard b'39' on port 6006\r\n(You can navigate to http://10.237.101.186:6006)\r\nINFO:tensorflow:path ../external\\webcomponentsjs/webcomponents-lite.min.js not found, sending 404\r\nINFO:tensorflow:returning 404 to 127.0.0.1 for /webcomponentsjs/webcomponents-lite.min.js\r\nINFO:tensorflow:path ../external\\plottable/plottable.css not found, sending 404\r\nINFO:tensorflow:returning 404 to 127.0.0.1 for /plottable/plottable.css\r\nINFO:tensorflow:TensorBoard reload process beginning\r\nINFO:tensorflow:Starting AddRunsFromDirectory: D:\\tensorBoard\r\nINFO:tensorflow:Adding events from directory D:\\tensorBoard\r\nINFO:tensorflow:Done with AddRunsFromDirectory: D:\\tensorBoard\r\nINFO:tensorflow:TensorBoard reload process: Reload the whole Multiplexer\r\nINFO:tensorflow:Beginning EventMultiplexer.Reload()\r\nDEBUG:tensorflow:No more events in D:\\tensorBoard\\events.out.tfevents.1480577231.SHPC052\r\nINFO:tensorflow:No path found after D:\\tensorBoard\\events.out.tfevents.1480577231.SHPC052\r\nINFO:tensorflow:Finished with EventMultiplexer.Reload()\r\nINFO:tensorflow:TensorBoard done reloading. Load took 0.009 secs\r\n\r\n---------------------------------------------------------------------------------------\r\nthis is like 404 problem.\r\n\r\nINFO:tensorflow:path ../external\\webcomponentsjs/webcomponents-lite.min.js not found, sending 404\r\nINFO:tensorflow:returning 404 to 127.0.0.1 for /webcomponentsjs/webcomponents-lite.min.js\r\nINFO:tensorflow:path ../external\\plottable/plottable.css not found, sending 404\r\nINFO:tensorflow:returning 404 to 127.0.0.1 for /plottable/plottable.css\r\n\r\nCan anyone help me?\r\nThank you very much.\r\n\r\n\r\n\r\n\r\n", "comments": ["[#5844](https://github.com/tensorflow/tensorflow/pull/5844) can help for you.", "Closing this as a duplicate of #5983. We've merged #5844 and will be adding it to the final release for 0.12.\r\n\r\nIf you have any questions, please follow up on #5983!"]}, {"number": 6004, "title": "Fix typo in executor.h", "body": "", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please."]}, {"number": 6003, "title": "C/Other Languages: Support serialization and deserialization of Tensors", "body": "In C++ and Python, Tensors can be serialized into a `TensorProto` using functions like [`Tensor::AsProtoTensorContents`](https://github.com/tensorflow/tensorflow/blob/f7ec99516ce0e0937e0b865e90aa02c748cd36c6/tensorflow/core/framework/tensor.h#L205)  and [`make_tensor_proto`](https://www.tensorflow.org/versions/r0.12/api_docs/python/contrib.util.html#make_tensor_proto)\r\n\r\nThis is particularly useful when communicating tensors across process boundaries such as [PredictRequest](https://github.com/tensorflow/serving/blob/8c89d72259a7c15f0a9711a67eecf284d9460cca/tensorflow_serving/apis/predict.proto#L22) RPCs to a [TensorFlow Model Server](https://tensorflow.github.io/serving/) \r\n\r\nFiling this to track any changes to the [C API](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api.h) and language bindings.\r\n\r\nInterested parties: @asimshankar @jhseu @nfiedel @josh11b ", "comments": ["@asimshankar, saw \"Go: API functions to serialize/deserialize Tensors\" was added with #6137.  Are we expecting further update on this issue?", "Nobody is actively working on this, this was more of a tracking bug. I think it's fine to close this for now (and re-open if and when the need is more urgent). The \"need\" would be to save/load tensors directly (as opposed to using things like the Restore operations or a SavedModel loader).", "@asimshankar, could we reopen this? Tensor se/de operations in C would be extremely useful for multi-language interoperability. ", "@sbsends : Happy to reopen it, but would love to hear what specifically you're interested in. Could you elaborate on what you're trying to do and the problems being run into?", "@asimshankar, we are interested in passing TensorProto's directly between languages. E.g. generating a packed TensorProto in Python and sending them over the wire to a program running in Golang. This is possible using Pythons implementation of [make_proto](https://www.tensorflow.org/api_docs/python/tf/make_tensor_proto) and then building a custom handler in Golang to parse the TensorProto's contents/type/dim's. The tensor contents can be read using [ReadTensor](https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go#ReadTensor), but this only supports numeric data types. IMHO, it would be significantly cleaner (and less error prone) to handle ser/de in a standardized way across all client libraries using the C API.\r\n\r\nEdit: grammar fix", "@asimshankar, would you consider giving feedback on our stab at building a standard for transporting tensors across applications/languages? The goal is to support many frameworks and act as an intermediary between application specific protocols. What can we do to increase the likelihood of gaining TensorFlow support?\r\n\r\nhttps://github.com/tensortask/ttp", "@asimshankar Would it we possible to reopen this? In our case, we would like to serialize/deserialize tensors to communicate with Tensorflow Serving from Go. We can not use the Go library as it does not handle serialization of string tensors.", "Seconding the above comment -- I ran into this issue trying to use to Go API `tf.ReadTensor()` only to discover that `DT_STRING` is not supported.\r\n\r\nIn the meantime is there an easy workaround for parsing string `TensorProto` into Go `tf.Tensor`?"]}, {"number": 6002, "title": "Updating CMake Readme", "body": "Adding 0.12r updates and pip installation instructions #5942.\r\n", "comments": ["Can one of the admins verify this patch?", "Should I close this PR based [here](https://github.com/tensorflow/tensorflow/issues/5942#issuecomment-265613813)?", "@mrry So sorry Derek, I meant the CMake docs one as I made a separate PR earlier for \"common problems\", #6167, which was merged already. I should've rolled back those last commits, very sorry. But as you suggested those changes I will update accordingly. ", "No problem! If you'd like to rebase on top of your changes and apply the suggestions, that'd be great!", "We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->", "Oops, I think something went wrong with the rebase and now it's pulling in a 60k-line diff! I'm not sure how to back out from this, but maybe it'd be easiest to open a new PR against master?", "@mrry Oh no! 60k lines? That's messed up, I just dropped those two commits and reworded the last. I am really sorry. You can close this PR and I will open a new one. Apologies for the confusion.", "Don't worry about it! I think the pull request interface is easily confused :). Looking forward to your new PR!"]}, {"number": 6001, "title": "How to enable the support of cuda compute capability 3.0 GPU in windows", "body": "I just setup gpu supported tensorflow on my windows 10 PC following https://developers.googleblog.com/2016/11/tensorflow-0-12-adds-support-for-windows.html\r\n\r\nBut right now I found I cannot use my GPU as it is a GeForece GTX760 and only has 3.0 CUDA compute capability\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nThere are several discussion for linux and MAC tensorflow, as they can build from source so they can just change the list of the computer capability. right now I cannot build from source so I am wondering if there are any other way to change the setup?\r\n\r\n### Environment info\r\nOperating System:\r\nWindows 10 64bit\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\nCUDA 8.0\r\ncuDNN 5.1\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n", "comments": ["Also running into this issue on Windows 10 with a Quadro K1000M.", "For binary size reasons, we don't provide pre-built binaries that include code for CC 3.0 devices, so you'll need to build from source. You can follow the steps for the [CMake build](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/cmake) with modifications to `CMakeLists.txt` on [this line](https://github.com/tensorflow/tensorflow/blob/a7b4f636d50b6fcb9cd1f7f138cc50b720fc86dd/tensorflow/contrib/cmake/CMakeLists.txt#L156) and [this line](https://github.com/tensorflow/tensorflow/blob/a7b4f636d50b6fcb9cd1f7f138cc50b720fc86dd/tensorflow/contrib/cmake/CMakeLists.txt#L166) to set a compute capability of `3.0` (instead of `3.5` and `5.2`). ", "very funny. No config support, no pre-built support. WTF?"]}, {"number": 6000, "title": "distributed seq2seq used SyncReplicasOptimizerV2 question", "body": "Now I want to change seq2seq to distributed mode used SyncReplicasOptimizerV2. I've been trying for a few days\uff0cbut still can't work out. this is my code,one file is seq2seq_train.py,on file is seq2seq_model.py\r\nI try to run in 1 ps and 2 worker\r\n\r\nthe error is:\r\nTraceback (most recent call last):\r\n  File \"/home/test/replicas_seq2seq_v4/seq2seq_train.py\", line 188, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 43, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"/home/test/replicas_seq2seq_v4/seq2seq_train.py\", line 82, in main\r\n    task_index = FLAGS.task_index ,replicas_to_aggregate=num_workers, forward_only=False)\r\n  File \"/home/test/replicas_seq2seq_v4/seq2seq_model.py\", line 221, in __init__\r\n    self.session = supervisor.prepare_or_wait_for_session(self.server.target, config=sess_config)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 720, in prepare_or_wait_for_session\r\n    init_feed_dict=self._init_feed_dict, init_fn=self._init_fn)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py\", line 251, in prepare_session\r\n    self._local_init_op, msg))\r\nRuntimeError: Init operations did not make model ready.  Init op: init, init fn: None, local_init_op: name: \"sync_replicas_3/group_deps\"\r\nop: \"NoOp\"\r\ninput: \"^sync_replicas_3/group_deps/NoOp\"\r\ninput: \"^sync_replicas_3/group_deps/NoOp_1\"\r\ninput: \"^sync_replicas_3/group_deps/NoOp_2\"\r\ndevice: \"/job:worker/task:0\"\r\n, error: Variables not initialized: sync_rep_local_step, sync_rep_local_step_1, sync_rep_local_step_2\r\n\r\nps:\r\n python seq2seq_train.py --ps_hosts=127.0.0.1:2240 --worker_hosts=127.0.0.1:2230,127.0.0.1:2242 --job_name=ps --task_index=0 --data_dir=data1 --train_dir=result  --batch_size=128 --size=1024 --num_layers=2 --steps_per_checkpoint=5 --en_vocab_size=40000 --fr_vocab_size=40000 --learning_rate=0.5 --learning_rate_decay_factor=0.95 --max_gradient_norm=2.0 --max_train_data_size=50000\r\n\r\nworker1:\r\n python seq2seq_train.py --ps_hosts=127.0.0.1:2240 --worker_hosts=127.0.0.1:2230,127.0.0.1:2242 --job_name=worker --task_index=0 --data_dir=data1 --train_dir=result  --batch_size=128 --size=1024 --num_layers=2 --steps_per_checkpoint=5 --en_vocab_size=40000 --fr_vocab_size=40000 --learning_rate=0.5 --learning_rate_decay_factor=0.95 --max_gradient_norm=2.0 --max_train_data_size=50000\r\n\r\nworker2:\r\n python seq2seq_train.py --ps_hosts=127.0.0.1:2240 --worker_hosts=127.0.0.1:2230,127.0.0.1:2242 --job_name=worker --task_index=1 --data_dir=data1 --train_dir=result  --batch_size=128 --size=1024 --num_layers=2 --steps_per_checkpoint=5 --en_vocab_size=40000 --fr_vocab_size=40000 --learning_rate=0.5 --learning_rate_decay_factor=0.95 --max_gradient_norm=2.0 --max_train_data_size=50000\r\n\r\n\r\n\r\nseq2seq_train.py\r\n\r\n```python\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport tempfile\r\nimport math\r\nimport os\r\nimport random\r\nimport sys\r\nimport time\r\nimport json\r\n\r\nimport numpy as np\r\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.models.rnn.translate import data_utils\r\nimport seq2seq_model\r\n\r\ntf.app.flags.DEFINE_string(\"ps_hosts\", \"\",\r\n                           \"Comma-separated list of hostname:port pairs\")\r\ntf.app.flags.DEFINE_string(\"worker_hosts\", \"\",\r\n                           \"Comma-separated list of hostname:port pairs\")\r\n\r\ntf.app.flags.DEFINE_string(\"job_name\", \"\", \"One of 'ps', 'worker'\")\r\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\r\n\r\ntf.app.flags.DEFINE_float(\"learning_rate\", 0.5, \"Learning rate.\")\r\ntf.app.flags.DEFINE_float(\"learning_rate_decay_factor\", 0.99,\r\n                          \"Learning rate decays by this much.\")\r\ntf.app.flags.DEFINE_float(\"max_gradient_norm\", 5.0,\r\n                          \"Clip gradients to this norm.\")\r\ntf.app.flags.DEFINE_integer(\"batch_size\", 64,\r\n                            \"Batch size to use during training.\")\r\ntf.app.flags.DEFINE_integer(\"size\", 1024, \"Size of each model layer.\")\r\ntf.app.flags.DEFINE_integer(\"num_layers\", 3, \"Number of layers in the model.\")\r\ntf.app.flags.DEFINE_integer(\"en_vocab_size\", 40000, \"English vocabulary size.\")\r\ntf.app.flags.DEFINE_integer(\"fr_vocab_size\", 40000, \"French vocabulary size.\")\r\ntf.app.flags.DEFINE_string(\"data_dir\", \"/tmp/data\", \"Data directory\")\r\ntf.app.flags.DEFINE_string(\"train_dir\", \"/tmp\", \"Training directory.\")\r\ntf.app.flags.DEFINE_integer(\"max_train_data_size\", 0,\r\n                            \"Limit on the size of training data (0: no limit).\")\r\ntf.app.flags.DEFINE_integer(\"steps_per_checkpoint\", 200,\r\n                            \"How many training steps to do per checkpoint.\")\r\ntf.app.flags.DEFINE_boolean(\"decode\", False,\r\n                            \"Set to True for interactive decoding.\")\r\ntf.app.flags.DEFINE_boolean(\"self_test\", False,\r\n                            \"Run a self-test if this is set to True.\")\r\n\r\nFLAGS = tf.app.flags.FLAGS\r\n\r\n_buckets = [(5, 10), (10, 15), (20, 25), (40, 50)]\r\n\r\ndef main(_):\r\n  if FLAGS.job_name is None or FLAGS.job_name == \"\":\r\n    raise ValueError(\"Must specify an explicit `job_name`\")\r\n  if FLAGS.task_index is None or FLAGS.task_index ==\"\":\r\n    raise ValueError(\"Must specify an explicit `task_index`\")\r\n\r\n\r\n  ps_hosts = FLAGS.ps_hosts.split(\",\")\r\n  worker_hosts = FLAGS.worker_hosts.split(\",\")\r\n\r\n  cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\r\n\r\n  server = tf.train.Server(cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index)\r\n  num_workers = len(worker_hosts)\r\n\r\n  server = tf.train.Server(cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index)\r\n  if FLAGS.job_name == \"ps\":\r\n    server.join()\r\n    return\r\n  is_chief = (FLAGS.task_index == 0)\r\n  model = seq2seq_model.Seq2SeqModel(server, FLAGS.en_vocab_size, FLAGS.fr_vocab_size, _buckets,\r\n      FLAGS.size, FLAGS.num_layers, FLAGS.max_gradient_norm, FLAGS.batch_size,\r\n      FLAGS.learning_rate, FLAGS.learning_rate_decay_factor,num_workers = num_workers, \r\n      task_index = FLAGS.task_index ,replicas_to_aggregate=num_workers, forward_only=False)\r\n  step_time, loss = 0.0, 0.0\r\n  current_step = 0\r\n  previous_losses = []\r\n\r\n  print(\"query -> title:\")\r\n  en_train = FLAGS.data_dir + \"/giga-fren.release2.ids40000.en\"\r\n  fr_train = FLAGS.data_dir + \"/giga-fren.release2.ids40000.fr\"\r\n  en_dev = FLAGS.data_dir + \"/newstest2013.ids40000.en\"\r\n  fr_dev = FLAGS.data_dir + \"/newstest2013.ids40000.fr\"\r\n\r\n  print (\"Reading development and training data (limit: %d).\" % FLAGS.max_train_data_size)\r\n  dev_set = read_data(en_dev, fr_dev)\r\n  train_set = read_train_data(en_train, fr_train, num_workers, FLAGS.max_train_data_size)\r\n  train_bucket_sizes = [len(train_set[b]) for b in xrange(len(_buckets))]\r\n  train_total_size = float(sum(train_bucket_sizes))\r\n\r\n  train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size for i in xrange(len(train_bucket_sizes))]\r\n  print(\"finish to load data\")\r\n  sess = model.session\r\n  while True:\r\n    random_number_01 = np.random.random_sample()\r\n    bucket_id = min([i for i in xrange(len(train_buckets_scale)) if train_buckets_scale[i] > random_number_01])\r\n\r\n    start_time = time.time()\r\n    encoder_inputs, decoder_inputs, target_weights = model.get_batch(train_set, bucket_id)\r\n    _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, False)\r\n    step_time += (time.time() - start_time) / FLAGS.steps_per_checkpoint\r\n    step_time_now = (time.time() - start_time)\r\n    loss += step_loss / FLAGS.steps_per_checkpoint\r\n    step_loss_now = step_loss\r\n    current_step += 1\r\n    print(\"step: %d\" % current_step);\r\n\r\n    if (True):\r\n      perplexity = math.exp(step_loss) if step_loss < 300 else float('inf')\r\n      print (\"global step %d learning rate %.4f step-time %.2f perplexity \"\r\n             \"%.2f\" % (model.global_step.eval(sess), model.learning_rate.eval(sess), step_time_now, perplexity))\r\n\r\n      previous_losses.append(step_loss)\r\n\r\n      checkpoint_path = os.path.join(train_dir, \"translate.ckpt\")\r\n      model.saver.save(sess, checkpoint_path, global_step=model.global_step)\r\n      if (current_step %  FLAGS.steps_per_checkpoint == 0):\r\n        for bucket_id in xrange(len(_buckets)):\r\n          if len(dev_set[bucket_id]) == 0:\r\n            print(\"  eval: empty bucket %d\" % (bucket_id))\r\n            continue\r\n          encoder_inputs, decoder_inputs, target_weights = model.get_batch(dev_set, bucket_id)\r\n          _, eval_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, True)\r\n          eval_ppx = math.exp(eval_loss) if eval_loss < 300 else float('inf')\r\n          print(\"  eval: bucket %d perplexity %.2f\" % (bucket_id, eval_ppx))\r\n        step_time, loss = 0.0, 0.0\r\n      sys.stdout.flush()\r\n\r\n\r\ndef read_train_data(source_path, target_path, num_workers, max_size=None):\r\n  data_set = [[] for _ in _buckets]\r\n  with tf.gfile.GFile(source_path, mode=\"r\") as source_file:\r\n    with tf.gfile.GFile(target_path, mode=\"r\") as target_file:\r\n      source, target = source_file.readline(), target_file.readline()\r\n      counter = 0\r\n      while source and target and (not max_size or counter < max_size):\r\n        counter += 1\r\n        if counter % 1000000 == 0:\r\n          print(\"  reading data line %d\" % counter)\r\n          sys.stdout.flush()\r\n        source_ids = [int(x) for x in source.split()]\r\n        target_ids = [int(x) for x in target.split()]\r\n        target_ids.append(data_utils.EOS_ID)\r\n        if (counter % num_workers == 0):\r\n          for bucket_id, (source_size, target_size) in enumerate(_buckets):\r\n            if len(source_ids) < source_size and len(target_ids) < target_size:\r\n              data_set[bucket_id].append([source_ids, target_ids])\r\n              break\r\n        source, target = source_file.readline(), target_file.readline()\r\n  return data_set\r\n\r\ndef read_data(source_path, target_path, max_size=None):\r\n  data_set = [[] for _ in _buckets]\r\n  with tf.gfile.GFile(source_path, mode=\"r\") as source_file:\r\n    with tf.gfile.GFile(target_path, mode=\"r\") as target_file:\r\n      source, target = source_file.readline(), target_file.readline()\r\n      counter = 0\r\n      while source and target and (not max_size or counter < max_size):\r\n        counter += 1\r\n        if counter % 100000 == 0:\r\n          print(\"  reading data line %d\" % counter)\r\n          sys.stdout.flush()\r\n        source_ids = [int(x) for x in source.split()]\r\n        target_ids = [int(x) for x in target.split()]\r\n        target_ids.append(data_utils.EOS_ID)\r\n        for bucket_id, (source_size, target_size) in enumerate(_buckets):\r\n          if len(source_ids) < source_size and len(target_ids) < target_size:\r\n            data_set[bucket_id].append([source_ids, target_ids])\r\n            break\r\n        source, target = source_file.readline(), target_file.readline()\r\n  return data_set\r\n\r\nif __name__ == \"__main__\":\r\n      tf.app.run()\r\n```\r\n\r\nseq2seq_model.py\r\n\r\n\r\n\r\n```python\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport random\r\n\r\nimport numpy as np\r\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.models.rnn.translate import data_utils\r\n\r\n\r\nclass Seq2SeqModel(object):\r\n\r\n  def __init__(self,\r\n               server,\r\n               source_vocab_size,\r\n               target_vocab_size,\r\n               buckets,\r\n               size,\r\n               num_layers,\r\n               max_gradient_norm,\r\n               batch_size,\r\n               learning_rate,\r\n               learning_rate_decay_factor,\r\n               num_workers=2, \r\n               task_index=0,\r\n               replicas_to_aggregate=2,\r\n               use_lstm=False,\r\n               num_samples=512,\r\n               forward_only=False,\r\n               dtype=tf.float32):\r\n\r\n    self.graph = tf.Graph()\r\n    self.server = server\r\n    self.source_vocab_size = source_vocab_size\r\n    self.target_vocab_size = target_vocab_size\r\n    self.buckets = buckets\r\n    self.batch_size = batch_size\r\n    self.num_workers = num_workers \r\n    self.task_index = task_index\r\n    self.is_chief = (task_index == 0)\r\n    self.num_workers = num_workers\r\n    self.replicas_to_aggregate = replicas_to_aggregate\r\n\r\n    with self.graph.as_default():\r\n      with tf.device(\"/job:ps/task:0\"): \r\n      #with tf.device(\"/cpu:0\"): \r\n        self.global_step = tf.Variable(0, trainable=False)\r\n        self.learning_rate = tf.Variable(\r\n            float(learning_rate), trainable=False, dtype=dtype)\r\n\r\n      output_projection = None\r\n      softmax_loss_function = None\r\n      if num_samples > 0 and num_samples < self.target_vocab_size:\r\n        with tf.device(\"/job:ps/task:0\"): \r\n          w_t = tf.get_variable(\"proj_w\", [self.target_vocab_size, size], dtype=dtype)\r\n          w = tf.transpose(w_t)\r\n          b = tf.get_variable(\"proj_b\", [self.target_vocab_size], dtype=dtype)\r\n        output_projection = (w, b)\r\n  \r\n        def sampled_loss(inputs, labels):\r\n          with tf.device(\"/job:worker/task:\"+str(self.task_index)):\r\n            labels = tf.reshape(labels, [-1, 1])\r\n            local_w_t = tf.cast(w_t, tf.float32)\r\n            local_b = tf.cast(b, tf.float32)\r\n            local_inputs = tf.cast(inputs, tf.float32)\r\n            return tf.cast(\r\n                tf.nn.sampled_softmax_loss(local_w_t, local_b, local_inputs, labels,\r\n                                           num_samples, self.target_vocab_size),\r\n                dtype)\r\n        softmax_loss_function = sampled_loss\r\n  \r\n      with tf.device(\"/job:worker/task:\"+str(self.task_index)):\r\n        single_cell = tf.nn.rnn_cell.GRUCell(size)\r\n        if use_lstm:\r\n          single_cell = tf.nn.rnn_cell.BasicLSTMCell(size)\r\n        cell = single_cell\r\n        if num_layers > 1:\r\n          cell = tf.nn.rnn_cell.MultiRNNCell([single_cell] * num_layers)\r\n  \r\n      def seq2seq_f(encoder_inputs, decoder_inputs, do_decode):\r\n        with tf.device(\"/job:worker/task:\"+str(self.task_index)):\r\n          return tf.nn.seq2seq.embedding_attention_seq2seq(\r\n              encoder_inputs,\r\n              decoder_inputs,\r\n              cell,\r\n              num_encoder_symbols=source_vocab_size,\r\n              num_decoder_symbols=target_vocab_size,\r\n              embedding_size=size,\r\n              output_projection=output_projection,\r\n              feed_previous=do_decode,\r\n              dtype=dtype)\r\n\r\n      with tf.device(\"/job:worker/task:\"+str(self.task_index)):\r\n        self.encoder_inputs = []\r\n        self.decoder_inputs = []\r\n        self.target_weights = []\r\n        for i in xrange(buckets[-1][0]):  # Last bucket is the biggest one.\r\n          self.encoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\r\n                                                    name=\"encoder{0}\".format(i)))\r\n        for i in xrange(buckets[-1][1] + 1):\r\n          self.decoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\r\n                                                    name=\"decoder{0}\".format(i)))\r\n          self.target_weights.append(tf.placeholder(dtype, shape=[None],\r\n                                                    name=\"weight{0}\".format(i)))\r\n  \r\n        targets = [self.decoder_inputs[i + 1]\r\n                   for i in xrange(len(self.decoder_inputs) - 1)]\r\n        if forward_only:\r\n          self.outputs, self.losses = tf.nn.seq2seq.model_with_buckets(\r\n              self.encoder_inputs, self.decoder_inputs, targets,\r\n              self.target_weights, buckets, lambda x, y: seq2seq_f(x, y, True),\r\n              softmax_loss_function=softmax_loss_function)\r\n          if output_projection is not None:\r\n            for b in xrange(len(buckets)):\r\n              self.outputs[b] = [\r\n                  tf.matmul(output, output_projection[0]) + output_projection[1]\r\n                  for output in self.outputs[b]\r\n              ]\r\n        else:\r\n          self.outputs, self.losses = tf.nn.seq2seq.model_with_buckets(\r\n              self.encoder_inputs, self.decoder_inputs, targets,\r\n              self.target_weights, buckets,\r\n              lambda x, y: seq2seq_f(x, y, False),\r\n              softmax_loss_function=softmax_loss_function)\r\n  \r\n      with tf.device(\"/job:worker/task:\"+str(self.task_index)):\r\n        params = tf.trainable_variables()\r\n        self.gradient_norms = []\r\n        self.updates = []\r\n        sgd_opt = tf.train.GradientDescentOptimizer(2.0)\r\n        sync_rep_opt = tf.train.SyncReplicasOptimizerV2(\r\n            sgd_opt, replicas_to_aggregate=replicas_to_aggregate,\r\n            total_num_replicas=num_workers)\r\n        for b in xrange(len(buckets)):\r\n          gradients = tf.gradients(self.losses[b], params)\r\n          clipped_gradients, norm = tf.clip_by_global_norm(gradients,\r\n                                                           max_gradient_norm)\r\n          self.gradient_norms.append(norm)\r\n          self.updates.append(sync_rep_opt.apply_gradients(\r\n              zip(clipped_gradients, params), global_step=self.global_step))\r\n\r\n        init_op = tf.global_variables_initializer()\r\n        local_init_op = sync_rep_opt.local_step_init_op\r\n        if self.is_chief:\r\n          local_init_op = sync_rep_opt.chief_init_op\r\n        ready_for_local_init_op = sync_rep_opt.ready_for_local_init_op\r\n\r\n        chief_queue_runner = sync_rep_opt.get_chief_queue_runner()\r\n        sync_init_op = sync_rep_opt.get_init_tokens_op(num_workers)\r\n\r\n    supervisor = tf.train.Supervisor(\r\n        graph=self.graph,\r\n        is_chief=self.is_chief,\r\n        recovery_wait_secs=1,\r\n        init_op=init_op,\r\n        local_init_op=local_init_op,\r\n        ready_for_local_init_op=ready_for_local_init_op)\r\n\r\n    sess_config = tf.ConfigProto(\r\n        allow_soft_placement=True,\r\n        log_device_placement=True,\r\n        device_filters=[\"/job:ps\", \"/job:worker/task:%d\" % self.task_index])\r\n\r\n\r\n    self.session = supervisor.prepare_or_wait_for_session(self.server.target, config=sess_config)\r\n\r\n    if self.is_chief:\r\n      self.session.run(sync_init_op)\r\n      supervisor.StartQueueRunners(self.session, [chief_queue_runner])\r\n\r\n    if self.is_chief:\r\n      self.session.run(sync_init_op)\r\n      supervisor.StartQueueRunners(self.session, [chief_queue_runner])\r\n\r\n\r\n  def step(self, session, encoder_inputs, decoder_inputs, target_weights,\r\n           bucket_id, forward_only):\r\n\r\n    encoder_size, decoder_size = self.buckets[bucket_id]\r\n    if len(encoder_inputs) != encoder_size:\r\n      raise ValueError(\"Encoder length must be equal to the one in bucket,\"\r\n                       \" %d != %d.\" % (len(encoder_inputs), encoder_size))\r\n    if len(decoder_inputs) != decoder_size:\r\n      raise ValueError(\"Decoder length must be equal to the one in bucket,\"\r\n                       \" %d != %d.\" % (len(decoder_inputs), decoder_size))\r\n    if len(target_weights) != decoder_size:\r\n      raise ValueError(\"Weights length must be equal to the one in bucket,\"\r\n                       \" %d != %d.\" % (len(target_weights), decoder_size))\r\n\r\n\r\n    input_feed = {}\r\n    for l in xrange(encoder_size):\r\n      input_feed[self.encoder_inputs[l].name] = encoder_inputs[l]\r\n    for l in xrange(decoder_size):\r\n      input_feed[self.decoder_inputs[l].name] = decoder_inputs[l]\r\n      input_feed[self.target_weights[l].name] = target_weights[l]\r\n\r\n\r\n    last_target = self.decoder_inputs[decoder_size].name\r\n    input_feed[last_target] = np.zeros([self.batch_size], dtype=np.int32)\r\n\r\n\r\n    if not forward_only:\r\n      output_feed = [self.updates[bucket_id],  # Update Op that does SGD.\r\n                     self.gradient_norms[bucket_id],  # Gradient norm.\r\n                     self.losses[bucket_id]]  # Loss for this batch.\r\n    else:\r\n      output_feed = [self.losses[bucket_id]]  # Loss for this batch.\r\n      for l in xrange(decoder_size):  # Output logits.\r\n        output_feed.append(self.outputs[bucket_id][l])\r\n\r\n    outputs = session.run(output_feed, input_feed)\r\n    if not forward_only:\r\n      return outputs[1], outputs[2], None  # Gradient norm, loss, no outputs.\r\n    else:\r\n      return None, outputs[0], outputs[1:]  # No gradient norm, loss, outputs.\r\n\r\n  def get_batch(self, data, bucket_id):\r\n\r\n    encoder_size, decoder_size = self.buckets[bucket_id]\r\n    encoder_inputs, decoder_inputs = [], []\r\n\r\n\r\n    for _ in xrange(self.batch_size):\r\n      encoder_input, decoder_input = random.choice(data[bucket_id])\r\n\r\n\r\n      encoder_pad = [data_utils.PAD_ID] * (encoder_size - len(encoder_input))\r\n      encoder_inputs.append(list(reversed(encoder_input + encoder_pad)))\r\n\r\n\r\n      decoder_pad_size = decoder_size - len(decoder_input) - 1\r\n      decoder_inputs.append([data_utils.GO_ID] + decoder_input +\r\n                            [data_utils.PAD_ID] * decoder_pad_size)\r\n\r\n\r\n    batch_encoder_inputs, batch_decoder_inputs, batch_weights = [], [], []\r\n\r\n\r\n    for length_idx in xrange(encoder_size):\r\n      batch_encoder_inputs.append(\r\n          np.array([encoder_inputs[batch_idx][length_idx]\r\n                    for batch_idx in xrange(self.batch_size)], dtype=np.int32))\r\n\r\n\r\n    for length_idx in xrange(decoder_size):\r\n      batch_decoder_inputs.append(\r\n          np.array([decoder_inputs[batch_idx][length_idx]\r\n                    for batch_idx in xrange(self.batch_size)], dtype=np.int32))\r\n\r\n\r\n      batch_weight = np.ones(self.batch_size, dtype=np.float32)\r\n      for batch_idx in xrange(self.batch_size):\r\n        if length_idx < decoder_size - 1:\r\n          target = decoder_inputs[batch_idx][length_idx + 1]\r\n        if length_idx == decoder_size - 1 or target == data_utils.PAD_ID:\r\n          batch_weight[batch_idx] = 0.0\r\n      batch_weights.append(batch_weight)\r\n    return batch_encoder_inputs, batch_decoder_inputs, batch_weights\r\n```\r\n", "comments": ["@DjangoPeng  \r\nHave you solved this problem", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Closing as obsolete."]}, {"number": 5999, "title": "distributed seq2seq used SyncReplicasOptimizerV2 question", "body": "Now I want to change seq2seq to distributed mode used SyncReplicasOptimizerV2. I've been trying for a few days\uff0cbut still can't work out. this is my code,one file is seq2seq_train.py,on file is seq2seq_model.py\r\nI try to run in 1 ps and 2 worker\r\n\r\nthe error is:\r\nTraceback (most recent call last):\r\n  File \"/home/test/replicas_seq2seq_v4/seq2seq_train.py\", line 188, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 43, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"/home/test/replicas_seq2seq_v4/seq2seq_train.py\", line 82, in main\r\n    task_index = FLAGS.task_index ,replicas_to_aggregate=num_workers, forward_only=False)\r\n  File \"/home/test/replicas_seq2seq_v4/seq2seq_model.py\", line 221, in __init__\r\n    self.session = supervisor.prepare_or_wait_for_session(self.server.target, config=sess_config)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 720, in prepare_or_wait_for_session\r\n    init_feed_dict=self._init_feed_dict, init_fn=self._init_fn)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py\", line 251, in prepare_session\r\n    self._local_init_op, msg))\r\nRuntimeError: Init operations did not make model ready.  Init op: init, init fn: None, local_init_op: name: \"sync_replicas_3/group_deps\"\r\nop: \"NoOp\"\r\ninput: \"^sync_replicas_3/group_deps/NoOp\"\r\ninput: \"^sync_replicas_3/group_deps/NoOp_1\"\r\ninput: \"^sync_replicas_3/group_deps/NoOp_2\"\r\ndevice: \"/job:worker/task:0\"\r\n, error: Variables not initialized: sync_rep_local_step, sync_rep_local_step_1, sync_rep_local_step_2\r\n\r\nps:\r\n python seq2seq_train.py --ps_hosts=127.0.0.1:2240 --worker_hosts=127.0.0.1:2230,127.0.0.1:2242 --job_name=ps --task_index=0 --data_dir=data1 --train_dir=result  --batch_size=128 --size=1024 --num_layers=2 --steps_per_checkpoint=5 --en_vocab_size=40000 --fr_vocab_size=40000 --learning_rate=0.5 --learning_rate_decay_factor=0.95 --max_gradient_norm=2.0 --max_train_data_size=50000\r\n\r\nworker1:\r\n python seq2seq_train.py --ps_hosts=127.0.0.1:2240 --worker_hosts=127.0.0.1:2230,127.0.0.1:2242 --job_name=worker --task_index=0 --data_dir=data1 --train_dir=result  --batch_size=128 --size=1024 --num_layers=2 --steps_per_checkpoint=5 --en_vocab_size=40000 --fr_vocab_size=40000 --learning_rate=0.5 --learning_rate_decay_factor=0.95 --max_gradient_norm=2.0 --\r\n\r\nworker2:\r\n python seq2seq_train.py --ps_hosts=127.0.0.1:2240 --worker_hosts=127.0.0.1:2230,127.0.0.1:2242 --job_name=worker --task_index=1 --data_dir=data1 --train_dir=result  --batch_size=128 --size=1024 --num_layers=2 --steps_per_checkpoint=5 --en_vocab_size=40000 --fr_vocab_size=40000 --learning_rate=0.5 --learning_rate_decay_factor=0.95 --max_gradient_norm=2.0 --\r\n\r\n\r\n\r\nseq2seq_train.py\r\n\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport tempfile\r\nimport math\r\nimport os\r\nimport random\r\nimport sys\r\nimport time\r\nimport json\r\n\r\nimport numpy as np\r\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.models.rnn.translate import data_utils\r\n#from tensorflow.models.rnn.translate import seq2seq_model\r\nimport seq2seq_model\r\n\r\n# Flags for defining the tf.train.ClusterSpec\r\ntf.app.flags.DEFINE_string(\"ps_hosts\", \"\",\r\n                           \"Comma-separated list of hostname:port pairs\")\r\ntf.app.flags.DEFINE_string(\"worker_hosts\", \"\",\r\n                           \"Comma-separated list of hostname:port pairs\")\r\n\r\n# Flags for defining the tf.train.Server\r\ntf.app.flags.DEFINE_string(\"job_name\", \"\", \"One of 'ps', 'worker'\")\r\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\r\n\r\ntf.app.flags.DEFINE_float(\"learning_rate\", 0.5, \"Learning rate.\")\r\ntf.app.flags.DEFINE_float(\"learning_rate_decay_factor\", 0.99,\r\n                          \"Learning rate decays by this much.\")\r\ntf.app.flags.DEFINE_float(\"max_gradient_norm\", 5.0,\r\n                          \"Clip gradients to this norm.\")\r\ntf.app.flags.DEFINE_integer(\"batch_size\", 64,\r\n                            \"Batch size to use during training.\")\r\ntf.app.flags.DEFINE_integer(\"size\", 1024, \"Size of each model layer.\")\r\ntf.app.flags.DEFINE_integer(\"num_layers\", 3, \"Number of layers in the model.\")\r\ntf.app.flags.DEFINE_integer(\"en_vocab_size\", 40000, \"English vocabulary size.\")\r\ntf.app.flags.DEFINE_integer(\"fr_vocab_size\", 40000, \"French vocabulary size.\")\r\ntf.app.flags.DEFINE_string(\"data_dir\", \"/tmp/data\", \"Data directory\")\r\ntf.app.flags.DEFINE_string(\"train_dir\", \"/tmp\", \"Training directory.\")\r\ntf.app.flags.DEFINE_integer(\"max_train_data_size\", 0,\r\n                            \"Limit on the size of training data (0: no limit).\")\r\ntf.app.flags.DEFINE_integer(\"steps_per_checkpoint\", 200,\r\n                            \"How many training steps to do per checkpoint.\")\r\ntf.app.flags.DEFINE_boolean(\"decode\", False,\r\n                            \"Set to True for interactive decoding.\")\r\ntf.app.flags.DEFINE_boolean(\"self_test\", False,\r\n                            \"Run a self-test if this is set to True.\")\r\n\r\nFLAGS = tf.app.flags.FLAGS\r\n\r\n_buckets = [(5, 10), (10, 15), (20, 25), (40, 50)]\r\n\r\ndef main(_):\r\n  if FLAGS.job_name is None or FLAGS.job_name == \"\":\r\n    raise ValueError(\"Must specify an explicit `job_name`\")\r\n  if FLAGS.task_index is None or FLAGS.task_index ==\"\":\r\n    raise ValueError(\"Must specify an explicit `task_index`\")\r\n\r\n\r\n  ps_hosts = FLAGS.ps_hosts.split(\",\")\r\n  worker_hosts = FLAGS.worker_hosts.split(\",\")\r\n\r\n  # Create a cluster from the parameter server and worker hosts.\r\n  cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\r\n\r\n  # Create and start a server for the local task.\r\n  server = tf.train.Server(cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index)\r\n  num_workers = len(worker_hosts)\r\n\r\n  server = tf.train.Server(cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index)\r\n  if FLAGS.job_name == \"ps\":\r\n    server.join()\r\n    return\r\n  is_chief = (FLAGS.task_index == 0)\r\n  model = seq2seq_model.Seq2SeqModel(server, FLAGS.en_vocab_size, FLAGS.fr_vocab_size, _buckets,\r\n      FLAGS.size, FLAGS.num_layers, FLAGS.max_gradient_norm, FLAGS.batch_size,\r\n      FLAGS.learning_rate, FLAGS.learning_rate_decay_factor,num_workers = num_workers, \r\n      task_index = FLAGS.task_index ,replicas_to_aggregate=num_workers, forward_only=False)\r\n  step_time, loss = 0.0, 0.0\r\n  current_step = 0\r\n  previous_losses = []\r\n\r\n  print(\"query -> title:\")\r\n  en_train = FLAGS.data_dir + \"/giga-fren.release2.ids40000.en\"\r\n  fr_train = FLAGS.data_dir + \"/giga-fren.release2.ids40000.fr\"\r\n  en_dev = FLAGS.data_dir + \"/newstest2013.ids40000.en\"\r\n  fr_dev = FLAGS.data_dir + \"/newstest2013.ids40000.fr\"\r\n\r\n  # Read data into buckets and compute their sizes.\r\n  print (\"Reading development and training data (limit: %d).\" % FLAGS.max_train_data_size)\r\n  dev_set = read_data(en_dev, fr_dev)\r\n  train_set = read_train_data(en_train, fr_train, num_workers, FLAGS.max_train_data_size)\r\n  train_bucket_sizes = [len(train_set[b]) for b in xrange(len(_buckets))]\r\n  train_total_size = float(sum(train_bucket_sizes))\r\n\r\n  train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size for i in xrange(len(train_bucket_sizes))]\r\n  print(\"finish to load data\")\r\n  sess = model.session\r\n  while True:\r\n    random_number_01 = np.random.random_sample()\r\n    bucket_id = min([i for i in xrange(len(train_buckets_scale)) if train_buckets_scale[i] > random_number_01])\r\n\r\n    # Get a batch and make a step.\r\n    start_time = time.time()\r\n    encoder_inputs, decoder_inputs, target_weights = model.get_batch(train_set, bucket_id)\r\n    _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, False)\r\n    step_time += (time.time() - start_time) / FLAGS.steps_per_checkpoint\r\n    step_time_now = (time.time() - start_time)\r\n    loss += step_loss / FLAGS.steps_per_checkpoint\r\n    step_loss_now = step_loss\r\n    current_step += 1\r\n    print(\"step: %d\" % current_step);\r\n\r\n    if (True):\r\n      perplexity = math.exp(step_loss) if step_loss < 300 else float('inf')\r\n      print (\"global step %d learning rate %.4f step-time %.2f perplexity \"\r\n             \"%.2f\" % (model.global_step.eval(sess), model.learning_rate.eval(sess), step_time_now, perplexity))\r\n      previous_losses.append(step_loss)\r\n\r\n      checkpoint_path = os.path.join(train_dir, \"translate.ckpt\")\r\n      model.saver.save(sess, checkpoint_path, global_step=model.global_step)\r\n      if (current_step %  FLAGS.steps_per_checkpoint == 0):\r\n        for bucket_id in xrange(len(_buckets)):\r\n          if len(dev_set[bucket_id]) == 0:\r\n            print(\"  eval: empty bucket %d\" % (bucket_id))\r\n            continue\r\n          encoder_inputs, decoder_inputs, target_weights = model.get_batch(dev_set, bucket_id)\r\n          _, eval_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, True)\r\n          eval_ppx = math.exp(eval_loss) if eval_loss < 300 else float('inf')\r\n          print(\"  eval: bucket %d perplexity %.2f\" % (bucket_id, eval_ppx))\r\n        step_time, loss = 0.0, 0.0\r\n      sys.stdout.flush()\r\n\r\n\r\ndef read_train_data(source_path, target_path, num_workers, max_size=None):\r\n  data_set = [[] for _ in _buckets]\r\n  with tf.gfile.GFile(source_path, mode=\"r\") as source_file:\r\n    with tf.gfile.GFile(target_path, mode=\"r\") as target_file:\r\n      source, target = source_file.readline(), target_file.readline()\r\n      counter = 0\r\n      while source and target and (not max_size or counter < max_size):\r\n        counter += 1\r\n        if counter % 1000000 == 0:\r\n          print(\"  reading data line %d\" % counter)\r\n          sys.stdout.flush()\r\n        source_ids = [int(x) for x in source.split()]\r\n        target_ids = [int(x) for x in target.split()]\r\n        target_ids.append(data_utils.EOS_ID)\r\n        # if (counter % num_workers == 0):\r\n        if (True):\r\n          for bucket_id, (source_size, target_size) in enumerate(_buckets):\r\n            if len(source_ids) < source_size and len(target_ids) < target_size:\r\n              data_set[bucket_id].append([source_ids, target_ids])\r\n              break\r\n        source, target = source_file.readline(), target_file.readline()\r\n  return data_set\r\n\r\ndef read_data(source_path, target_path, max_size=None):\r\n  data_set = [[] for _ in _buckets]\r\n  with tf.gfile.GFile(source_path, mode=\"r\") as source_file:\r\n    with tf.gfile.GFile(target_path, mode=\"r\") as target_file:\r\n      source, target = source_file.readline(), target_file.readline()\r\n      counter = 0\r\n      while source and target and (not max_size or counter < max_size):\r\n        counter += 1\r\n        if counter % 100000 == 0:\r\n          print(\"  reading data line %d\" % counter)\r\n          sys.stdout.flush()\r\n        source_ids = [int(x) for x in source.split()]\r\n        target_ids = [int(x) for x in target.split()]\r\n        target_ids.append(data_utils.EOS_ID)\r\n        for bucket_id, (source_size, target_size) in enumerate(_buckets):\r\n          if len(source_ids) < source_size and len(target_ids) < target_size:\r\n            data_set[bucket_id].append([source_ids, target_ids])\r\n            break\r\n        source, target = source_file.readline(), target_file.readline()\r\n  return data_set\r\n\r\nif __name__ == \"__main__\":\r\n      tf.app.run()\r\n\r\nseq2seq_model.py\r\n\r\n# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n# \r\n\r\n\"\"\"Sequence-to-sequence model with an attention mechanism.\"\"\"\r\n\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport random\r\n\r\nimport numpy as np\r\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.models.rnn.translate import data_utils\r\n\r\n\r\nclass Seq2SeqModel(object):\r\n\r\n  def __init__(self,\r\n               server,\r\n               source_vocab_size,\r\n               target_vocab_size,\r\n               buckets,\r\n               size,\r\n               num_layers,\r\n               max_gradient_norm,\r\n               batch_size,\r\n               learning_rate,\r\n               learning_rate_decay_factor,\r\n               num_workers=2, \r\n               task_index=0,\r\n               replicas_to_aggregate=2,\r\n               use_lstm=False,\r\n               num_samples=512,\r\n               forward_only=False,\r\n               dtype=tf.float32):\r\n\r\n    self.graph = tf.Graph()\r\n    self.server = server\r\n    self.source_vocab_size = source_vocab_size\r\n    self.target_vocab_size = target_vocab_size\r\n    self.buckets = buckets\r\n    self.batch_size = batch_size\r\n    self.num_workers = num_workers \r\n    self.task_index = task_index\r\n    self.is_chief = (task_index == 0)\r\n    self.num_workers = num_workers\r\n    self.replicas_to_aggregate = replicas_to_aggregate\r\n\r\n    with self.graph.as_default():\r\n      with tf.device(\"/job:ps/task:0\"): \r\n        self.global_step = tf.Variable(0, trainable=False)\r\n        self.learning_rate = tf.Variable(\r\n            float(learning_rate), trainable=False, dtype=dtype)\r\n  \r\n      output_projection = None\r\n      softmax_loss_function = None\r\n      # Sampled softmax only makes sense if we sample less than vocabulary size.\r\n      if num_samples > 0 and num_samples < self.target_vocab_size:\r\n        with tf.device(\"/job:ps/task:0\"): \r\n          w_t = tf.get_variable(\"proj_w\", [self.target_vocab_size, size], dtype=dtype)\r\n          w = tf.transpose(w_t)\r\n          b = tf.get_variable(\"proj_b\", [self.target_vocab_size], dtype=dtype)\r\n        output_projection = (w, b)\r\n  \r\n        def sampled_loss(inputs, labels):\r\n          with tf.device(\"/job:worker/task:\"+str(self.task_index)):\r\n            labels = tf.reshape(labels, [-1, 1])\r\n            # We need to compute the sampled_softmax_loss using 32bit floats to\r\n            # avoid numerical instabilities.\r\n            local_w_t = tf.cast(w_t, tf.float32)\r\n            local_b = tf.cast(b, tf.float32)\r\n            local_inputs = tf.cast(inputs, tf.float32)\r\n            return tf.cast(\r\n                tf.nn.sampled_softmax_loss(local_w_t, local_b, local_inputs, labels,\r\n                                           num_samples, self.target_vocab_size),\r\n                dtype)\r\n        softmax_loss_function = sampled_loss\r\n  \r\n      # Create the internal multi-layer cell for our RNN.\r\n      with tf.device(\"/job:worker/task:\"+str(self.task_index)):\r\n        single_cell = tf.nn.rnn_cell.GRUCell(size)\r\n        if use_lstm:\r\n          single_cell = tf.nn.rnn_cell.BasicLSTMCell(size)\r\n        cell = single_cell\r\n        if num_layers > 1:\r\n          cell = tf.nn.rnn_cell.MultiRNNCell([single_cell] * num_layers)\r\n  \r\n      # The seq2seq function: we use embedding for the input and attention.\r\n      def seq2seq_f(encoder_inputs, decoder_inputs, do_decode):\r\n        with tf.device(\"/job:worker/task:\"+str(self.task_index)):\r\n          return tf.nn.seq2seq.embedding_attention_seq2seq(\r\n              encoder_inputs,\r\n              decoder_inputs,\r\n              cell,\r\n              num_encoder_symbols=source_vocab_size,\r\n              num_decoder_symbols=target_vocab_size,\r\n              embedding_size=size,\r\n              output_projection=output_projection,\r\n              feed_previous=do_decode,\r\n              dtype=dtype)\r\n\r\n      # Feeds for inputs.\r\n      with tf.device(\"/job:worker/task:\"+str(self.task_index)):\r\n        self.encoder_inputs = []\r\n        self.decoder_inputs = []\r\n        self.target_weights = []\r\n        for i in xrange(buckets[-1][0]):  # Last bucket is the biggest one.\r\n          self.encoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\r\n                                                    name=\"encoder{0}\".format(i)))\r\n        for i in xrange(buckets[-1][1] + 1):\r\n          self.decoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\r\n                                                    name=\"decoder{0}\".format(i)))\r\n          self.target_weights.append(tf.placeholder(dtype, shape=[None],\r\n                                                    name=\"weight{0}\".format(i)))\r\n  \r\n        # Our targets are decoder inputs shifted by one.\r\n        targets = [self.decoder_inputs[i + 1]\r\n                   for i in xrange(len(self.decoder_inputs) - 1)]\r\n        if forward_only:\r\n          self.outputs, self.losses = tf.nn.seq2seq.model_with_buckets(\r\n              self.encoder_inputs, self.decoder_inputs, targets,\r\n              self.target_weights, buckets, lambda x, y: seq2seq_f(x, y, True),\r\n              softmax_loss_function=softmax_loss_function)\r\n          # If we use output projection, we need to project outputs for decoding.\r\n          if output_projection is not None:\r\n            for b in xrange(len(buckets)):\r\n              self.outputs[b] = [\r\n                  tf.matmul(output, output_projection[0]) + output_projection[1]\r\n                  for output in self.outputs[b]\r\n              ]\r\n        else:\r\n          self.outputs, self.losses = tf.nn.seq2seq.model_with_buckets(\r\n              self.encoder_inputs, self.decoder_inputs, targets,\r\n              self.target_weights, buckets,\r\n              lambda x, y: seq2seq_f(x, y, False),\r\n              softmax_loss_function=softmax_loss_function)\r\n  \r\n        # Gradients and SGD update operation for training the model.\r\n      with tf.device(\"/job:worker/task:\"+str(self.task_index)):\r\n        params = tf.trainable_variables()\r\n        self.gradient_norms = []\r\n        self.updates = []\r\n      #sync_rep_opt = tf.train.AdamOptimizer(self.learning_rate)\r\n        sgd_opt = tf.train.GradientDescentOptimizer(2.0)\r\n        sync_rep_opt = tf.train.SyncReplicasOptimizerV2(\r\n            sgd_opt, replicas_to_aggregate=replicas_to_aggregate,\r\n            total_num_replicas=num_workers)\r\n        for b in xrange(len(buckets)):\r\n          gradients = tf.gradients(self.losses[b], params)\r\n          clipped_gradients, norm = tf.clip_by_global_norm(gradients,\r\n                                                           max_gradient_norm)\r\n          self.gradient_norms.append(norm)\r\n          self.updates.append(sync_rep_opt.apply_gradients(\r\n              zip(clipped_gradients, params), global_step=self.global_step))\r\n\r\n        init_op = tf.global_variables_initializer()\r\n        local_init_op = sync_rep_opt.local_step_init_op\r\n        if self.is_chief:\r\n          local_init_op = sync_rep_opt.chief_init_op\r\n        ready_for_local_init_op = sync_rep_opt.ready_for_local_init_op\r\n\r\n        # Chief_queue_runner\r\n        chief_queue_runner = sync_rep_opt.get_chief_queue_runner()\r\n        sync_init_op = sync_rep_opt.get_init_tokens_op(num_workers)\r\n\r\n    # Creates session for chief.\r\n    supervisor = tf.train.Supervisor(\r\n        graph=self.graph,\r\n        is_chief=self.is_chief,\r\n        recovery_wait_secs=1,\r\n        init_op=init_op,\r\n        local_init_op=local_init_op,\r\n        ready_for_local_init_op=ready_for_local_init_op)\r\n\r\n    sess_config = tf.ConfigProto(\r\n        allow_soft_placement=True,\r\n        log_device_placement=True,\r\n        device_filters=[\"/job:ps\", \"/job:worker/task:%d\" % self.task_index])\r\n\r\n\r\n    self.session = supervisor.prepare_or_wait_for_session(self.server.target, config=sess_config)\r\n\r\n    # Chief should execute the sync_init_op and start the chief queue runner.\r\n    if self.is_chief:\r\n      self.session.run(sync_init_op)\r\n      supervisor.StartQueueRunners(self.session, [chief_queue_runner])\r\n\r\n    if self.is_chief:\r\n      self.session.run(sync_init_op)\r\n      supervisor.StartQueueRunners(self.session, [chief_queue_runner])\r\n\r\n\r\n  def step(self, session, encoder_inputs, decoder_inputs, target_weights,\r\n           bucket_id, forward_only):\r\n    \"\"\"Run a step of the model feeding the given inputs.\r\n\r\n    Args:\r\n      session: tensorflow session to use.\r\n      encoder_inputs: list of numpy int vectors to feed as encoder inputs.\r\n      decoder_inputs: list of numpy int vectors to feed as decoder inputs.\r\n      target_weights: list of numpy float vectors to feed as target weights.\r\n      bucket_id: which bucket of the model to use.\r\n      forward_only: whether to do the backward step or only forward.\r\n\r\n    Returns:\r\n      A triple consisting of gradient norm (or None if we did not do backward),\r\n      average perplexity, and the outputs.\r\n\r\n    Raises:\r\n      ValueError: if length of encoder_inputs, decoder_inputs, or\r\n        target_weights disagrees with bucket size for the specified bucket_id.\r\n    \"\"\"\r\n    # Check if the sizes match.\r\n    encoder_size, decoder_size = self.buckets[bucket_id]\r\n    if len(encoder_inputs) != encoder_size:\r\n      raise ValueError(\"Encoder length must be equal to the one in bucket,\"\r\n                       \" %d != %d.\" % (len(encoder_inputs), encoder_size))\r\n    if len(decoder_inputs) != decoder_size:\r\n      raise ValueError(\"Decoder length must be equal to the one in bucket,\"\r\n                       \" %d != %d.\" % (len(decoder_inputs), decoder_size))\r\n    if len(target_weights) != decoder_size:\r\n      raise ValueError(\"Weights length must be equal to the one in bucket,\"\r\n                       \" %d != %d.\" % (len(target_weights), decoder_size))\r\n\r\n    # Input feed: encoder inputs, decoder inputs, target_weights, as provided.\r\n    input_feed = {}\r\n    for l in xrange(encoder_size):\r\n      input_feed[self.encoder_inputs[l].name] = encoder_inputs[l]\r\n    for l in xrange(decoder_size):\r\n      input_feed[self.decoder_inputs[l].name] = decoder_inputs[l]\r\n      input_feed[self.target_weights[l].name] = target_weights[l]\r\n\r\n    # Since our targets are decoder inputs shifted by one, we need one more.\r\n    last_target = self.decoder_inputs[decoder_size].name\r\n    input_feed[last_target] = np.zeros([self.batch_size], dtype=np.int32)\r\n\r\n    # Output feed: depends on whether we do a backward step or not.\r\n    if not forward_only:\r\n      output_feed = [self.updates[bucket_id],  # Update Op that does SGD.\r\n                     self.gradient_norms[bucket_id],  # Gradient norm.\r\n                     self.losses[bucket_id]]  # Loss for this batch.\r\n    else:\r\n      output_feed = [self.losses[bucket_id]]  # Loss for this batch.\r\n      for l in xrange(decoder_size):  # Output logits.\r\n        output_feed.append(self.outputs[bucket_id][l])\r\n\r\n    outputs = session.run(output_feed, input_feed)\r\n    if not forward_only:\r\n      return outputs[1], outputs[2], None  # Gradient norm, loss, no outputs.\r\n    else:\r\n      return None, outputs[0], outputs[1:]  # No gradient norm, loss, outputs.\r\n\r\n  def get_batch(self, data, bucket_id):\r\n    \"\"\"Get a random batch of data from the specified bucket, prepare for step.\r\n\r\n    To feed data in step(..) it must be a list of batch-major vectors, while\r\n    data here contains single length-major cases. So the main logic of this\r\n    function is to re-index data cases to be in the proper format for feeding.\r\n\r\n    Args:\r\n      data: a tuple of size len(self.buckets) in which each element contains\r\n        lists of pairs of input and output data that we use to create a batch.\r\n      bucket_id: integer, which bucket to get the batch for.\r\n\r\n    Returns:\r\n      The triple (encoder_inputs, decoder_inputs, target_weights) for\r\n      the constructed batch that has the proper format to call step(...) later.\r\n    \"\"\"\r\n    encoder_size, decoder_size = self.buckets[bucket_id]\r\n    encoder_inputs, decoder_inputs = [], []\r\n\r\n    # Get a random batch of encoder and decoder inputs from data,\r\n    # pad them if needed, reverse encoder inputs and add GO to decoder.\r\n    for _ in xrange(self.batch_size):\r\n      encoder_input, decoder_input = random.choice(data[bucket_id])\r\n\r\n      # Encoder inputs are padded and then reversed.\r\n      encoder_pad = [data_utils.PAD_ID] * (encoder_size - len(encoder_input))\r\n      encoder_inputs.append(list(reversed(encoder_input + encoder_pad)))\r\n\r\n      # Decoder inputs get an extra \"GO\" symbol, and are padded then.\r\n      decoder_pad_size = decoder_size - len(decoder_input) - 1\r\n      decoder_inputs.append([data_utils.GO_ID] + decoder_input +\r\n                            [data_utils.PAD_ID] * decoder_pad_size)\r\n\r\n    # Now we create batch-major vectors from the data selected above.\r\n    batch_encoder_inputs, batch_decoder_inputs, batch_weights = [], [], []\r\n\r\n    # Batch encoder inputs are just re-indexed encoder_inputs.\r\n    for length_idx in xrange(encoder_size):\r\n      batch_encoder_inputs.append(\r\n          np.array([encoder_inputs[batch_idx][length_idx]\r\n                    for batch_idx in xrange(self.batch_size)], dtype=np.int32))\r\n\r\n    # Batch decoder inputs are re-indexed decoder_inputs, we create weights.\r\n    for length_idx in xrange(decoder_size):\r\n      batch_decoder_inputs.append(\r\n          np.array([decoder_inputs[batch_idx][length_idx]\r\n                    for batch_idx in xrange(self.batch_size)], dtype=np.int32))\r\n\r\n      # Create target_weights to be 0 for targets that are padding.\r\n      batch_weight = np.ones(self.batch_size, dtype=np.float32)\r\n      for batch_idx in xrange(self.batch_size):\r\n        # We set weight to 0 if the corresponding target is a PAD symbol.\r\n        # The corresponding target is decoder_input shifted by 1 forward.\r\n        if length_idx < decoder_size - 1:\r\n          target = decoder_inputs[batch_idx][length_idx + 1]\r\n        if length_idx == decoder_size - 1 or target == data_utils.PAD_ID:\r\n          batch_weight[batch_idx] = 0.0\r\n      batch_weights.append(batch_weight)\r\n    return batch_encoder_inputs, batch_decoder_inputs, batch_weights\r\n\r\n", "comments": []}, {"number": 5998, "title": "Branch 140671682", "body": "Manually fixed conflicts in:\r\nRELEASE.md\r\ntensorflow/contrib/session_bundle/session_bundle.cc\r\ntensorflow/contrib/slim/python/slim/evaluation.py\r\ntensorflow/core/kernels/BUILD\r\ntensorflow/python/training/summary_io.py", "comments": ["Is this pushing PR ready? If so, I can approve and merge it."]}, {"number": 5997, "title": "Please update the optimized protobuf binary library recommended on tensorflow.org", "body": "The following page [1] describes how to use the optimized protobuf binary to allow for large protos. Recently the protobuf library removed the limit on the proto size once and for all [2]. \r\n\r\nThis should fix some of the issues still being reported on #582. \r\n\r\nCould you please update the binary to the latest version and the documentation if applicable?\r\n\r\nThanks\r\n\r\n[1] https://www.tensorflow.org/versions/r0.12/get_started/os_setup.html#protobuf-library-related-issues\r\n[2] https://github.com/google/protobuf/commit/5a76e633ea9b5adb215e93fdc11e1c0c08b3fc74#diff-922cc541c2d97d0ca70fce2b001de379R640", "comments": ["I am not sure if I fully understand the request.\r\nProtobuf library released still uses slow python implementation of protobuf encoder/decoder.\r\nSo our instructions are still needed for people who want to use faster protobuf library, with C++ implementation compiled on their system.\r\n\r\nAs for the notice there about 64MB protobuf limits, I think our builds did not have those limits for a while.\r\nBut that error still can be seen for people who might have an old version of protobuf in their system, so message is still helpful, IMO.\r\n\r\nDoes the above explanation help?", "Spoke with @elmer-garduno offline.\r\nLooks like we somehow have the 64 MB limit on the library we have up.\r\n@jhseu agreed to help out with this as I am working on something else.", "@jhseu recently updated the packages pointed by tensorflow.org.\r\nClosing this issue."]}, {"number": 5996, "title": "Add toolchain deps for SWIG compilation", "body": "", "comments": ["Jenkins, test this please.\r\nand do not flake this time, grrrr", "@martinwicke test failure is a known issue.\r\nWould you like to merge this?"]}]