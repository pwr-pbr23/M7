[{"number": 41494, "title": "Session->Run (c++) not working if launched in a thread", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version (use command below):1.15.2\r\n- Python version:3.8\r\n- Bazel version (if compiling from source):0.26.1\r\n- GCC/Compiler version (if compiling from source):5.5\r\n- CUDA/cuDNN version:10.1/7.5\r\n- GPU model and memory:Jetson Xavier\r\n\r\n\r\n**Describe the current behavior**\r\nI'm working on creating an object detection script using Tensorflow in C++. For this, I've used https://github.com/lysukhin/tensorflow-object-detection-cpp as a start and after some changes to make it faster, I'm able to run it at a very good speed on the Jetson Xavier. But today, I would like to separate the capture and inference task to let them run by themself as threads. The problem is that my \"session ->run\" instruction is not giving me any information about what is going on. The only thing that I know is that my session->run is not working properly because it should output a tensorflow::Status and I'm getting nothing (neither an error). Since there is no error, It is pretty hard for me to find why my run instruction is not doing anything.\r\n\r\n**Describe the expected behavior**\r\nDescribed above.\r\n\r\nThanks for the futur help ! \r\n", "comments": ["@Kmarconi \r\n\r\nI think this is more related to models repo.Please, raise an issue in models repo by filling issue template from [here](https://github.com/tensorflow/models/issues/new/choose).Also, please share related code so it helps us in localizing the issue faster.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41494\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41494\">No</a>\n"]}, {"number": 41493, "title": "tf.parallel_stack() doesn't support eager execution", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\nBug example:\r\n```python\r\nx = tf.constant([1, 4])\r\ny = tf.constant([2, 5])\r\nz = tf.constant([3, 6])\r\ntf.parallel_stack([x, y, z])\r\n```\r\nWork example\r\n```python\r\nx = tf.constant([1, 4])\r\ny = tf.constant([2, 5])\r\nz = tf.constant([3, 6])\r\nwith tf.compat.v1.Session() as sess:\r\n    print(sess.run(tf.parallel_stack([x, y, z])))\r\n```\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@fsx950223,\r\nI was able to reproduce the issue reported here, but the 'working example' too seems to throw an error. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/07cc20c8528e0f6209800732ec5aa297/41493.ipynb).\r\n\r\nIf possible could you please provide a working example too? Thanks!\r\n", "```python\r\n@tf.function\r\ndef test():\r\n  x = tf.constant([1, 4])\r\n  y = tf.constant([2, 5])\r\n  z = tf.constant([3, 6])\r\n  return tf.parallel_stack([x, y, z])\r\ntest()\r\n```", "I have a similar issue but with \"Moving average\".\r\n\r\n`ValueError: Moving average not supported in eager mode.`", "Was able to reproduce the issue with [TF v2.2](https://colab.research.google.com/gist/amahendrakar/cb1060f15cfb1bd38cdec20ad1e53ec9/41493-2-2.ipynb), [TF v2.3.0rc2](https://colab.research.google.com/gist/amahendrakar/8da30a0e2e728a189dcf709baf807e09/41493-2-3.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/401e1a9ff76255026c667b19e5e42ac3/41493-tf-nightly.ipynb). Please find the attached gist. Thanks!", "https://github.com/tensorflow/tensorflow/commit/868256f05a2abd55b43746c7b3c8b6fb9053668e should have fixed the issue. Parallel Stack has no eager implementation and we now raise an error. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41493\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41493\">No</a>\n", "Please open a separate issue for moving average and we'll address it."]}, {"number": 41492, "title": "Cannot download celeb_a dataset from tensorflow_datasets :(", "body": "Running this\r\n```python\r\n(train_data, test_data), info = tfds.load(name = 'celeb_a', split = ['train', 'test'], as_supervised = True, shuffle_files = True, with_info = True)\r\n```\r\nGives this\r\n```\r\nNonMatchingChecksumError: Artifact https://drive.google.com/uc?export=download&id=0B7EVK8r0v71pZjFTYXZWM3FlRnM, downloaded to /root/tensorflow_datasets/downloads/ucexport_download_id_0B7EVK8r0v71pZjFTYXZWM3FlDDaXUAQO8EGH_a7VqGNLRtW52mva1LzDrb-V723OQN8.tmp.4ec0de7ede1541dca88a21190e298882/uc, has wrong checksum.\r\n```\r\n\r\nAs far as I know, this issue is there because the dataset is on the google drive.", "comments": ["Please check now by rerunning the command, Checksum error resolved, I think it was unaccessible because too many people were viewing the original GDrive Link.", "Nope, still getting the same error!", "I guess this issue can be resolved easily if the dataset is stored somewhere else, other than google drive :(\r\n", "You can download from [source](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) if you're getting the same problem.", "Oh yeah sure I can, but the problem is if I do that, there is no way (as far as I know) I can load this folder as `tf.data.Dataset`.\r\n\r\nTensorflow does not allow to create a **full custom dataset** by sub classing `tf.data.Dataset`... (like in Pytorch), though we can read the images from a folder but it is required that the images are in their respective folders whose names are the names of the classes...\r\n\r\nBut CelebA is a multi class dataset, each image belongs to more than single class.\r\n\r\nSo reading such a folder and creating dataset from it in tensorflow, is just impossible (as far as I know, I tried my best by reading all the docs and tutorials, etc...), that is why I had to rely on tensorflow_dataset to provide a pre defined dataset, which would be an instance of `tf.data.Dataset`, so that I can apply `.batch`, `.map`, `.cache` etc...\r\n\r\nHave a look at [this](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html), here as you can see we can subclass the `torch.utils.data.Dataset`, and then can easily load literally any kind of data with all sorts of complexity in terms of reading it.\r\n\r\nIf there's any way that is possible in tensorflow then please do tell me, it would be my dream come true.\r\n\r\n(I'm in love with the ecosystem of the tensorflow and do want to completely switch all of my workflow to it, but this hassle and inconvenience of reading complex dataset from folders always makes me switch back to Pytorch)\r\n", "There is way to add a adatset, check it out [here](https://www.tensorflow.org/datasets/add_dataset).", "Is there any other way of creating a custom dataset other than this? Cuz this looks a whole lot of unnecessary work to do for  reading a simple dataset. ", "Hi there, I think there is a way and I've managed to do what I wanted. Here's my code:-\r\n\r\n```python\r\nimage_paths = sorted(glob.glob(os.path.join('dataset', 'img_align_celeba', 'img_align_celeba', '*.jpg')))\r\n\r\ndf = pd.read_csv('dataset/list_attr_celeba.csv')\r\ndf.replace(to_replace = -1, value = 0, inplace = True)\r\nlabels = df.iloc[:, 1:].values\r\n\r\nprint(image_paths[:2])\r\nprint(labels[:2])\r\n# prints\r\n'''\r\n['dataset/img_align_celeba/img_align_celeba/000001.jpg', 'dataset/img_align_celeba/img_align_celeba/000002.jpg']\r\n[[0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 1 0 1 0\r\n  1 0 0 1]\r\n [0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0\r\n  0 0 0 1]]\r\n'''\r\n\r\n# here's the tensorflow part...\r\n@tf.function\r\ndef read_image(image_path):\r\n    image = tf.io.read_file(image_path)\r\n    image = tf.image.decode_image(image, channels = 3, dtype = tf.float32)\r\n    return image\r\n\r\n@tf.function\r\ndef normalize(image):\r\n    image = (image - tf.reduce_min(image))/(tf.reduce_max(image) - tf.reduce_min(image))\r\n    image = (2 * image) - 1\r\n    return image\r\n\r\n@tf.function\r\ndef augment(image):\r\n    image = tf.image.random_crop(image, (178, 178, 3))\r\n    image = tf.image.resize(image, (256, 256))\r\n    image = tf.image.random_flip_left_right(image)\r\n    image = tf.image.random_saturation(image, 0.5, 2.0)\r\n    image = tf.image.random_brightness(image, 0.5)\r\n    return image\r\n\r\n@tf.function\r\ndef preprocess(image_path, label):\r\n    image = read_image(image_path)\r\n    image = augment(image)\r\n    image = normalize(image)\r\n    return image, label\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\r\ndataset = dataset.map(preprocess, num_parallel_calls = tf.data.experimental.AUTOTUNE)\r\ndataset = dataset.shuffle(buffer_size = 1024)\r\ndataset = dataset.batch(batch_size = 128)\r\ndataset = dataset.prefetch(buffer_size = tf.data.experimental.AUTOTUNE)\r\n\r\nfor x, y in dataset:\r\n    break\r\n\r\nprint(x.shape, y.shape)\r\n# prints\r\n# (128, 256, 256, 3) (128, 40)\r\n```\r\n\r\nWith this code, everything is working just fine for now. Also, do share your opinion for reading the data based on the above code, like performance and memory wise, and some modifications that I should make if any. And if everything is okay, then please add this example to the official tensorflow documentation, it'd very helpful for others as well.\r\n\r\nNow one thing I'd still want to achieve, in the the `augment` function, I can't get the dimensions of the image, like if I run \r\n\r\n```python\r\nprint(min(image.shape[:-1])) # :-1 to ignore the channels\r\n```\r\ninside this `augment` function to get the minimum of height and width, then it gives me `<unknown>`. I want to replace this line \r\n\r\n\"`image = tf.image.random_crop(image, (178, 178, 3))`\" with this\r\n\r\n```python\r\nmin_dim = min(image.shape[:-1])\r\nimage = tf.image.random_crop(image, (min_dim, min_dim, 3))\r\n```\r\nI tried removed the `@tf.function` decorator, but still not working.\r\n\r\nSo is a there a way I can get this last part done?\r\n\r\nThanks for your time.", "@braindotai \r\nI ran the code shared please find the [gist here](https://colab.research.google.com/gist/Saduf2019/ce3d5838f99e76424bf4653261ea507c/untitled284.ipynb).\r\n Please share all dependencies for us to replicate the issue faced or if possible share a colab gist with the issue reported.\r\n", "Run below cells first:-\r\n```python\r\n!pip install -q kaggle\r\nfrom google.colab import files\r\nfiles.upload()\r\n```\r\nThis will ask you to upload the kaggle api json file, which you can get from your kaggle account section.\r\nAfter uploading that, run:-\r\n\r\n```\r\n!mkdir -p ~/.kaggle\r\n!cp kaggle.json ~/.kaggle/\r\n!chmod 600 /root/.kaggle/kaggle.json\r\n```\r\n```\r\n!kaggle datasets download jessicali9530/celeba-dataset\r\n!unzip -d dataset celeba-dataset.zip\r\n```\r\nAfter after this, you can run the rest", "I further looked into other api's, and found that I should decode the image using `tf.image.decode_jpeg` instead of `tf.image.decode_image`. And now I'm successfully being able to random crop the image based on its shape.\r\n\r\nLoading celeb_a from tensorflow_datasets is still giving error though. :("]}, {"number": 41491, "title": "Failed to load the native TensorFlow runtime", "body": "Getting Below error while initializing tensor flow in python:\r\n\r\n**Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common reasons and solutions. Include the entire stack trace above this error message when asking for help.**\r\n\r\nAlso the console part start flickering as in this error message is displaying for an infinite loop", "comments": ["@digvijayv \r\n\r\nRequest you to fill [issue template.](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?.\r\n\r\nCan you please share error log.\r\n\r\nPlease, refer similar issues #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41491\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41491\">No</a>\n"]}, {"number": 41490, "title": "Building fails at //tensorflow/python/keras/api:keras_python_api_gen_compat_v1", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian \"sid\"\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): NA\r\n- TensorFlow version: r2.3\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?: NA\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): 8.4.0\r\n- CUDA/cuDNN version: 10.1/7.6.5\r\n- GPU model and memory: NA\r\nI should mention that this box has a CPU without AVX support (which is why I need to build from source).\r\n\r\nThe build fails almost at the end with this stack trace\r\n\r\n```ERROR: /home/hans/src/tensorflow.bak/tensorflow/python/keras/api/BUILD:123:1: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v1 failed (Exit 1)\r\nTraceback (most recent call last):\r\n  File \"/home/hans/.cache/bazel/_bazel_hans/2d6565a8de40a1199bd197af102140f1/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: /home/hans/.cache/bazel/_bazel_hans/2d6565a8de40a1199bd197af102140f1/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZTIN6icu_678ByteSinkE\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/hans/.cache/bazel/_bazel_hans/2d6565a8de40a1199bd197af102140f1/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 26, in <module>\r\n    from tensorflow.python.tools.api.generator import doc_srcs\r\n  File \"/home/hans/.cache/bazel/_bazel_hans/2d6565a8de40a1199bd197af102140f1/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 40, in <module>\r\n    from tensorflow.python.eager import context\r\n  File \"/home/hans/.cache/bazel/_bazel_hans/2d6565a8de40a1199bd197af102140f1/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/eager/context.py\", line 35, in <module>\r\n    from tensorflow.python import pywrap_tfe\r\n  File \"/home/hans/.cache/bazel/_bazel_hans/2d6565a8de40a1199bd197af102140f1/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tfe.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/hans/.cache/bazel/_bazel_hans/2d6565a8de40a1199bd197af102140f1/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 83, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/hans/.cache/bazel/_bazel_hans/2d6565a8de40a1199bd197af102140f1/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: /home/hans/.cache/bazel/_bazel_hans/2d6565a8de40a1199bd197af102140f1/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZTIN6icu_678ByteSinkE\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /home/hans/src/tensorflow.bak/tensorflow/tools/pip_package/BUILD:66:1 Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v1 failed (Exit 1)\r\nINFO: Elapsed time: 32684.441s, Critical Path: 205.34s\r\nINFO: 23720 processes: 23720 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\nThe command was a straight \r\n`bazel build //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nThe content of `tf_configure.bazel`\r\n\r\n```build --action_env PYTHON_BIN_PATH=\"/usr/bin/python3\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/usr/lib/python3/dist-packages\"\r\nbuild --python_path=\"/usr/bin/python3\"\r\nbuild --config=xla\r\nbuild --action_env CUDA_TOOLKIT_PATH=\"/usr/local/cuda\"\r\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"6.1\"\r\nbuild --action_env GCC_HOST_COMPILER_PATH=\"/usr/bin/x86_64-linux-gnu-gcc-8\"\r\nbuild --config=cuda\r\nbuild:opt --copt=-march=native\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\ntest --flaky_test_attempts=3\r\ntest --test_size_filters=small,medium\r\ntest --test_env=LD_LIBRARY_PATH\r\ntest:v1 --test_tag_filters=-benchmark-test,-no_oss,-no_gpu,-oss_serial\r\ntest:v1 --build_tag_filters=-benchmark-test,-no_oss,-no_gpu\r\ntest:v2 --test_tag_filters=-benchmark-test,-no_oss,-no_gpu,-oss_serial,-v1only\r\ntest:v2 --build_tag_filters=-benchmark-test,-no_oss,-no_gpu,-v1only\r\nbuild --action_env TF_CONFIGURE_IOS=\"0\"```", "comments": ["@hans-ekbrand,\r\nCould you please check if you have installed the TensorFlow pip package dependencies as mentioned in [this guide](https://www.tensorflow.org/install/source#install_python_and_the_tensorflow_package_dependencies).\r\n\r\n```\r\npip install -U --user pip six 'numpy<1.19.0' wheel setuptools mock 'future>=0.17.1'\r\npip install -U --user keras_applications --no-deps\r\npip install -U --user keras_preprocessing --no-deps\r\n```\r\n\r\nPlease take a look at this similar issue [#36903](https://github.com/tensorflow/tensorflow/issues/36903) for reference. Thanks!", "Thanks for helping me out, @amahendrakar. Here are the installed modules in this virtual environment:\r\n\r\n```hans@gpu-master:~$ source ~/.virtualenvs/tf_dev/bin/activate\r\n(tf_dev) hans@gpu-master:~$ pip list\r\nPackage             Version\r\n------------------- -------\r\nfuture              0.18.2\r\nKeras-Applications  1.0.8\r\nKeras-Preprocessing 1.1.2\r\nmock                4.0.2\r\nnumpy               1.18.5\r\npip                 20.1.1\r\npkg-resources       0.0.0\r\nsetuptools          49.2.0\r\nsix                 1.15.0\r\nwheel               0.34.2\r\n```", "Can one replicate the failing step manually? I was thinking about using `strace` to identify the files that the failing step opens. The error, \"unknown symbol\", sounds like a version mismatch, which in turn could be identified if we knew the files involved.", "```\r\nImportError: .../_pywrap_tensorflow_internal.so: undefined symbol: _ZTIN6icu_678ByteSinkE\r\n```\r\n\r\nThis means there is a linker error. [Demangling](https://demangler.com/) says that `typeinfo for icu_67::ByteSink` is not found. This does not look like a TF error, though, since TF does not directly depend on ICU. Most likely your system has an incompatible version of that library.", "This is the version of icu I seem to have installed, which version should I have?\r\n```\r\ndpkg -l | grep \"icu\" | grep ^ii\r\nii  icu-devtools                               67.1-2                          amd64        Development utilities for International Components for Unicode\r\nii  libharfbuzz-icu0:amd64                     2.6.7-1                         amd64        OpenType text shaping engine ICU backend\r\nii  libicu-dev:amd64                           67.1-2                          amd64        Development files for International Components for Unicode\r\nii  libicu67:amd64                             67.1-2                          amd64        International Components for Unicode\r\n```", "@hans-ekbrand Were you able to figure out the cause of this issue?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41490\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41490\">No</a>\n"]}, {"number": 41489, "title": "tf.data.Dataset.get_files produces infinite loop", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows Server 2016\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.14.1\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): MS Visual Studio 2017\r\n- CUDA/cuDNN version: 10.1/7\r\n- GPU model and memory: GeForce GTX 1080 Ti (11 Go)\r\n\r\n**Describe the current behavior**\r\nWhen using tf.data.Dataset to produce a dataset from a set of files (jpg images), it seems that it is unable to find any file or to build the dataset correctly. By looping through the dataset and printing each element, the output is an infinite list of \"Tensor(\"IteratorGetNext_N:0\", shape=(), dtype=string)\" where N is the number of iterations. The problem occurs either by using directly Dataset.list_files(\"/path/to/*.jpg\") of by using glob independently and giving the result to Dataset.from_tensor_slices. The problem also occurs when iterating through a small number of elements by using Dataset.take().\r\nI know I am supposed to upgrade my TensorFlow version, but I have for now to stick to the 1.14 version because all the software we distribute in my company is linked to this particular version.\r\n\r\n**Describe the expected behavior**\r\nIt is supposed to produce a dataset of approximately 1000 jpg images.\r\n\r\n**Standalone code to reproduce the issue**\r\n`import tensorflow as tf`\r\n`test_dataset = tf.data.Dataset.list_files(\"D:/test/*.jpg\")`\r\n`for element in test_dataset:`\r\n`\tprint(element)`\r\n\r\n**Other info / logs**\r\nAttached is a zip file containing 3 jpg images in a \"test\" directory to reproduce the issue. It works with any file anyway.\r\n[test.zip](https://github.com/tensorflow/tensorflow/files/4936851/test.zip)\r\n", "comments": ["Just for the record, I found that the infinite loop can also be produced with a simple call to from_tensor_slices:\r\n\r\nimport tensorflow as tf\r\n\r\ntest_dataset = tf.data.Dataset.from_tensor_slices([1,2,3])\r\nfor element in test_dataset.enumerate():\r\n\tprint(element)\r\n\r\nUsing enumerate(test_dataset) or test_dataset.enumerate() as suggested in [this post](https://github.com/tensorflow/tensorflow/issues/30802) did not solve the issue. The print infinitely produces something like this:\r\n\r\n(8149, <tf.Tensor 'IteratorGetNext_8149:0' shape=() dtype=int32>)", "@Rayndell \r\nI ran the above code please find the [gist here](https://colab.research.google.com/gist/Saduf2019/027fe9057feedb3bed3f16e317d19aeb/untitled282.ipynb) and let us know if it confirms your issue or if possible share a colab gist with the issue reported.\r\nAlso is there any particular reason for using older version of tf when newer versions are available,", "No. In my case the code produces, as stated above, an infinite loop. I reproduced the problem with the following gist:\r\n\r\nhttps://colab.research.google.com/drive/1hirnPgD2gYec31ZeIVIpbZWruA_D1W-N?usp=sharing\r\n\r\nAs I mentioned in my first post, I have to stick to version 1.14 because all scripts, programs and libraries we produced and distributed in my company is based on this particular version, and I do not want to risk any compatibility problems.", "Ok, I found a solution. By testing with TF 1.15 (https://colab.research.google.com/drive/11_NXDLw3NU8nNI3HyXq50GVDR0KUIQoR?usp=sharing) I found that my application was not in \"eager\" mode and then nothing was evaluated.\r\n\r\nFixed by adding the following instruction at the beginning of the program:\r\n\r\n`tf.compat.v1.enable_eager_execution()`", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41489\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41489\">No</a>\n"]}, {"number": 41488, "title": "XLA NMS bug", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\nhttps://colab.research.google.com/drive/1IyYxIvXTPdxgMI1Q4F99WEftzXqvv91X?usp=sharing\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n```\r\nInvalidArgumentError: Function invoked by the following node is not compilable: {{node __inference_test_32}} = __inference_test_32[_XlaMustCompile=true, config_proto=\"\\n\\007\\n\\003CPU\\020\\001\\n\\007\\n\\003GPU\\020\\0002\\002J\\0008\\001\", executor_type=\"\"](dummy_input, dummy_input, dummy_input, dummy_input, dummy_input, dummy_input).\r\nUncompilable nodes:\r\nNonMaxSuppressionV5: unsupported op: No registered 'NonMaxSuppressionV5' OpKernel for XLA_CPU_JIT devices compatible with node {{node NonMaxSuppressionV5}}\r\n\tStacktrace:\r\n\t\tNode: __inference_test_32, function: \r\n\t\tNode: NonMaxSuppressionV5, function: __inference_test_32\r\n [Op:__inference_test_32]\r\n```\r\n**Describe the expected behavior**\r\nTf1.15 doesn't have the bug\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@fsx950223 \r\n\r\nI have tried in colab with TF 2.2, 2.3rc1 and nightly versions and was able to reproduce the issue. However if we run in eager mode by commenting(`@tf.function(experimental_compile=True)`) i am not seeing any issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/381c642621b517dd33ccf472f9a4a5b9/untitled146.ipynb).Thanks!", "> @fsx950223\r\n> \r\n> I have tried in colab with TF 2.2, 2.3rc1 and nightly versions and was able to reproduce the issue. However if we run in eager mode by commenting(`@tf.function(experimental_compile=True)`) i am not seeing any issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/381c642621b517dd33ccf472f9a4a5b9/untitled146.ipynb).Thanks!\r\n\r\nYes it's a bug of xla kernel.", "@fsx950223 Just a question. Are you trying to run the code in TF1.x or TF2.x? You mentioned that expected behavior is the code doesn't throw error when using TF1.15 as shown below.\r\n\r\n> Describe the expected behavior\r\n> Tf1.15 doesn't have the bug\r\n> \r\n\r\nI didn't see any error with TF1.15 or TF1.15.3. Please take a look at the [gist](https://colab.research.google.com/gist/jvishnuvardhan/5755b83e13777b13100b8818a53b23c8/untitled146.ipynb). Thanks!\r\n", "I expect Tf2.2 could work same as Tf1.15", "Was able to replicate the issue with TF 2.5,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/4ba055a487d09ce88f9148760f824dda/untitled317.ipynb) ..Thanks!", "Fixed in tf2.6", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41488\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41488\">No</a>\n"]}, {"number": 41487, "title": "[TFLite] Add int16x8 support for RESIZE_NEAREST_NEIGHBOR operator", "body": "Hi,\r\n\r\nThis PR adds int16x8 support for the RESIZE_NEAREST_NEIGHBOR operator in TensorFlow Lite.\r\n\r\nThibaut", "comments": ["Apologies for the delay, @talumbau can you help review?"]}, {"number": 41486, "title": "error: the following arguments are required: -p/--prototxt, -m/--model", "body": "![python_error](https://user-images.githubusercontent.com/39181530/87752493-20051700-c81e-11ea-828f-fa42efab335e.png)\r\n\r\nSir i am new to this field and would like to resolve this issue as soon as possible i am using as it is i have not done any modification in code and i will be interested to find out solution  if any sir, kindly help ,  I am using preloaded video and using on spyder IDE for processing so kindly guide me about this error. thank you.", "comments": []}, {"number": 41485, "title": "FL16 model run on GPU", "body": "**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- Tensorflow version (commit SHA if source): 1.15\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):Android 9 api28 ,Mali-T864 GPU\r\n\r\n**Describe the problem**\r\nWe tried to run a post-quantized (to float16) model on a robot with GPU delegate according to https://www.tensorflow.org/lite/performance/gpu and https://medium.com/tensorflow/tensorflow-model-optimization-toolkit-float16-quantization-halves-model-size-cc113c75a2fa\r\n but it fails to run on GPU even after we graph transformed non-GPU supported operators in it. The logs is attached. Interesting thing is if we do not quantize it to fl16, all operators of the model can successfully run on GPU. Netron shows there are lots of 'dequantize' operators added to the graph after we use tflite converter to quantize the model to fl16. So what should we do to let the quantized fl16 model run on GPU entirely?\r\n\r\nOne more question is we found a parameter SetAllowFp16PrecisionForFp32 in tflite c++. What is the difference between 1).set this to true and use a fl32 model. 2). set this to true and use fl16 model. 3). set this to false and use fl32 model. 4) set this to false and use fl16 model?\r\n\r\nMany thanks.\r\n\r\nModel is uploaded in:\r\nhttps://drive.google.com/drive/folders/18B4Wx4BEPxfptsTmIEZySwILLZNXbE2v?usp=sharing\r\nInputs are image of size 193*321*3\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite delegate for GPU.\r\nERROR: Next operations are not supported by GPU delegate:\r\nCONV_2D: Expected 1 input tensor(s), but node has 3 runtime input(s).\r\nDEPTHWISE_CONV_2D: Expected 1 input tensor(s), but node has 3 runtime input(s).\r\nDEQUANTIZE: Operation is not supported.\r\nFirst 0 operations will run on the GPU, and the remaining 198 on the CPU.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["@rxiang040 \r\nI am unable to access the code shared on google drive.", "> @rxiang040\r\n> I am unable to access the code shared on google drive.\r\n\r\nSorry, please use this link:\r\nhttps://drive.google.com/drive/folders/18B4Wx4BEPxfptsTmIEZySwILLZNXbE2v?usp=sharing\r\n\r\nIt's not the code file, but tflite models", "Is it because TFlite V1 does not support dequantize op? And I visualize my quantized model by Netro, basically, every operation is has input 'dequantize', and that's the reason none operation can be performed on GPU. So what is this 'dequantize' for? I learned it aims to convert fl16 to fl32, but if I want to run on GPU then they are all useless?", "@rxiang040 Can you please share a standalone code to reproduce the issue? Specifically, model conversion code with a saved model or model development code. Did you try  `tf-nightly` for conversion? Thanks!", "> @rxiang040 Can you please share a standalone code to reproduce the issue? Specifically, model conversion code with a saved model or model development code. Did you try `tf-nightly` for conversion? Thanks!\r\n\r\nHi thanks for your reply. Here's our conversion code:\r\n\r\nimport tensorflow as tf\r\nimport pathlib\r\n\r\nroot_path = \"../bazel_export/\"\r\ngraph_name = \"bazel_optimized_graph.pb\"\r\n\r\ntflite_path = \"../tflite/\"\r\ntflite_name = \"test1.tflite\"\r\n\r\ngraph_def_file = root_path + graph_name\r\n\r\ninput_arrays = [\"sub_2\"]\r\noutput_arrays = [\"ResizeBilinear_2\"]\r\n\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays, input_shapes={\"sub_2\" : [1, 193, 321, 3]})\r\n\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_types = [tf.float16]\r\n\r\ntflite_model = converter.convert()\r\nopen(tflite_path + tflite_name, \"wb\").write(tflite_model)\r\n\r\ntflite_path2 = pathlib.Path(tflite_path)\r\ntflite_path2.mkdir(exist_ok=True, parents=True)\r\ntflite_file = tflite_path2/tflite_name\r\nprint(tflite_file.write_bytes(tflite_model))\r\n\r\nAnd for model development, we use the code suggested in : https://medium.com/tensorflow/tensorflow-model-optimization-toolkit-float16-quantization-halves-model-size-cc113c75a2fa\r\nAs\r\n//Prepare GPU delegate.\r\nconst TfLiteGpuDelegateOptions options = {\r\n  .metadata = NULL,\r\n  .compile_options = {\r\n    .precision_loss_allowed = 1,  // FP16\r\n    .preferred_gl_object_type = TFLITE_GL_OBJECT_TYPE_FASTEST,\r\n    .dynamic_batch_enabled = 0,   // Not fully functional yet\r\n  },\r\n};\r\n\r\n\r\nSo we use tf 1.15.0, but not tf-nightly. Should we use tf-nightly instead of pip installed tf?\r\n\r\n\r\n", "Hi Sachin, can you help take a look?", "@rxiang040 You are using an older version of the GPU delegate, which isn't maintained anymore. Could you try with [TfLiteGpuDelegateOptionsV2](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/delegate.h#L103)? Its in a different header. Look at the [Delegate page](https://www.tensorflow.org/lite/performance/gpu_advanced?source=post_page---------------------------#android_cc) for other languages.\r\n\r\nOut of curiosity, have you tried using the [8-bit quantization support](https://www.tensorflow.org/lite/performance/gpu_advanced?source=post_page---------------------------#running_quantized_models_experimental) w/ GPU? There might be a slight precision dip, but the models turn out 50% smaller compared to fp16.", "> @rxiang040 You are using an older version of the GPU delegate, which isn't maintained anymore. Could you try with [TfLiteGpuDelegateOptionsV2](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/delegate.h#L103)? Its in a different header. Look at the [Delegate page](https://www.tensorflow.org/lite/performance/gpu_advanced?source=post_page---------------------------#android_cc) for other languages.\r\n> \r\n> Out of curiosity, have you tried using the [8-bit quantization support](https://www.tensorflow.org/lite/performance/gpu_advanced?source=post_page---------------------------#running_quantized_models_experimental) w/ GPU? There might be a slight precision dip, but the models turn out 50% smaller compared to fp16.\r\n\r\nThanks so much for your advice. We will try TfLiteGpuDelegateOptionsV2. As for the 8-bit quantization support w/ GPU, can GPU perform 8-bit computations? I thought only 32-bit and 16-bit model can use GPU delegate. ", "The GPU delegate now supports 8-bit quantized models (see link in the reply above), and the latency is on-par with corresponding floating-point models.", "@rxiang040 Over to you, feel free to close this with an update once things work on your side :-)", "@srjoglekar246 Thanks very much for your advice. Now it can run on GPU with tflite2.3. But still we have one problem. I have attached our model structure. At the last layer of our model, it is a ResizeBilinear layer. We found that this operation is much efficient if we can run it with CPU. So we modified `tensorflow/lite/delegates/utils.cc` at line 219 by adding the following code:\r\n\r\n`  if (node_id == 197) {`\r\n      `std::string msg = \"197 Bilinear upsamping is not run on GPU but CPU\";`\r\n      `unsupported_details = &msg;`\r\n      `return false;`\r\n ` }`\r\n\r\n197 is the node_id of ResizeBilinear layer. However when we run our program on the bot, the log writes:\r\n\r\n`ERROR: Following operations are not supported by GPU delegate:`\r\n`DEQUANTIZE: `\r\n`RESIZE_BILINEAR: `\r\n`197 operations will run on the GPU, and the remaining 1 operations will run on the CPU.`\r\n\r\nIdeally, RESIZE_BILINEAR should be the only one op that will not run on GPU, but somehow, DEQUANTIZE shows up here. More strangely, even here, it reports two names of ops, but in the next sentence it says \"the remaining 1 operations will run on the CPU\". So do you know what's happening here?\r\n\r\nAlso I tested FL32 model and FL16M model, they almost have no inference time difference. So why should we use fl16 quantization here? (any advantage of fl16 comparing to fl32?)\r\n\r\nThanks!\r\n\r\n![model_halfed_V2_fl16 tflite](https://user-images.githubusercontent.com/67887251/92472643-5a7ea500-f20c-11ea-81e4-428656d4538e.png)\r\n\r\n", "@rxiang040 The LOG statements are sometimes buggy, especially when the graph structure is a bit complex (as is the case for FP16 graph). Its on our radar to improve this :-).\r\n\r\nDoes the model provide a speedup on the GPU? If yes, I wouldn't worry too much about a node (or two) not being run on GPU, since the other nodes in your visualization seem heavier.\r\n\r\nAs for FP16 vs FP32, this shouldn't matter on GPU (except FP16 might be a bit faster). The only reason to gofor FP16 is to get a 50% smaller model. 8-bit quantization will give you a model 75% smaller, but with a bit of an accuracy hit.", "The model is faster on GPU. We actually tested the inference time of our model (fl32) with the last layer run on GPU or CPU. I would say the difference is huge. If the whole model is on GPU, the inference time is 195 ms/frame, and if the last layer on CPU and others on GPU, the inference time is 162 ms/frame.\r\n\r\nSo for int8 model, will it run faster on GPU? ", "How did you test the whole model being on GPU?\r\n\r\nint8 models will provide the same latency benefits as fp32, but will be 4x smaller in size. They will also avoid weird DEQUANTIZE issues that fp16 faces.", "We have a RK3399 board with Andoird system and we use c++ for deployment. Basically the time is measured from inputting an image to obtaining the final segmentation result. E.g with total 77 ops, test1: time of (76 GPU operation + 1 CPU operation), 162 ms/frame; test2: time of 77 ops on GPU, 195 ms/frame", "I see. Can you try using [dynamic range quantization](https://www.tensorflow.org/lite/performance/post_training_quant#optimizing_an_existing_model)? It will give you a smaller model, and may not have the DEQUANTIZE issue.\r\n\r\nIn general, the debug statements can be a little weird, so I wouldn't worry if the speedup is good.", "Sure, we will try that. So if it's on an RK3399 with GPU using C++ implementation, what would be the fastest setup to run a tflite in general? Like, what the options and quantiztations should be without considering RAM usage.", "If size isn't an issue, go with FP32 model & GPU delegate, as long as OpenCL is supported. You could also try 4-threaded CPU execution.", "Cool! Thank:) I will close this issue. Thanks for help:)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41485\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41485\">No</a>\n", "@rxiang040 Hi, Did you figure out how to run float16 tflite model with GPU delegate? I still have a problem with DEQUANTIZE. It shows:\r\n```\r\nInternal error: Failed to apply delegate: Following operations are not supported by GPU delegate:\r\n    DEQUANTIZE: \r\n    92 operations will run on the GPU, and the remaining 1 operations will run on the CPU.\r\n```", "@pl-ang Thats fine, that operation doesn't work on the GPU, but the model is still accelerated.", "> @pl-ang Thats fine, that operation doesn't work on the GPU, but the model is still accelerated.\r\n\r\nThanks!"]}, {"number": 41484, "title": "I'm having issue when trying to convert keras model to json", "body": "I found an error where it could not find my keras model (input path) and the output path, I think it's because of the space character between \"My Drive\", since I was using google drive to load and save the model. And I think you can't change the name \"My Drive\". \r\nSo Instead of using google drive, I was using the local google colab's drive. So then I uploaded the keras model and convert it and save it in the google colab's drive and downloaded it. \r\n\r\nSo If it's a bug, I hope in the future it will be fixed. Thank you.\r\n\r\n**Environment**\r\n- Google Colab\r\n\r\n**System information**\r\n- TensorFlow 2.2\r\n- tensorflowjs ver2.0.1.post1\r\n\r\n**Code**\r\nfrom google.colab import drive\r\ndrive.mount('/content/gdrive')\r\n\r\n!tensorflowjs_converter --input_format=keras '/content/gdrive/My Drive/model/model.h5' '/content/gdrive/My Drive/json'\r\n\r\n**Error**\r\nusage: TensorFlow.js model converters. [-h]\r\n                                       [--input_format {keras,tf_frozen_model,keras_saved_model,tf_hub,tfjs_layers_model,tf_saved_model}]\r\n                                       [--output_format {keras_saved_model,tfjs_graph_model,tfjs_layers_model,keras}]\r\n                                       [--signature_name SIGNATURE_NAME]\r\n                                       [--saved_model_tags SAVED_MODEL_TAGS]\r\n                                       [--quantize_float16 [QUANTIZE_FLOAT16]]\r\n                                       [--quantize_uint8 [QUANTIZE_UINT8]]\r\n                                       [--quantize_uint16 [QUANTIZE_UINT16]]\r\n                                       [--quantization_bytes {1,2}]\r\n                                       [--split_weights_by_layer] [--version]\r\n                                       [--skip_op_check]\r\n                                       [--strip_debug_ops STRIP_DEBUG_OPS]\r\n                                       [--weight_shard_size_bytes WEIGHT_SHARD_SIZE_BYTES]\r\n                                       [--output_node_names OUTPUT_NODE_NAMES]\r\n                                       [--control_flow_v2 CONTROL_FLOW_V2]\r\n                                       [input_path] [output_path]\r\nTensorFlow.js model converters.: error: unrecognized arguments: /content/gdrive/My Drive/json\r\n", "comments": ["@lexms \r\n\r\nThis issue is more suitable for TFjsrepo. Please post it on TFjs repo from [here.](https://github.com/tensorflow/tfjs/issues) Thanks!", "Right, thank you! I've posted [here](https://github.com/tensorflow/tfjs/issues/3618)"]}, {"number": 41483, "title": "Raise an error when some but not all values passed to the first layer\u2026", "body": "Raise an error when some but not all values passed to the first call arg of a custom layer are symbolic. This setting can cause functional models to be constructed incorrectly.\r\n\r\n.This means the setting described in GitHub Issue #40638 will raise an error with an actionable message instead of silently missing weights.\r\n\r\nSupport for this functionality will be added when we enable the KerasTensors refactoring.\r\n\r\n", "comments": []}, {"number": 41482, "title": "Access denied when loading frozen inference graph", "body": "------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: No\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 Home\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**: No\r\n-   **TensorFlow installed from (source or binary)**: Source\r\n-   **TensorFlow version (use command below)**: 1.14.0\r\n-   **Python version**: 3.6.7\r\n-   **Bazel version (if compiling from source)**: Not using Bazel\r\n-   **GCC/Compiler version (if compiling from source)**: Not using GCC\r\n-   **CUDA/cuDNN version**: CUDA/cuDNN: 10.0\r\n-   **GPU model and memory**: NVIDIA GeForce GTX 1080, 12G\r\n-   **Exact command to reproduce**:\r\nimage_data = tf.gfile.FastGFile('ssd_mobilenet_v1_coco_2017_11_17/output_inference_graph_v1.pb', 'rb').read() \r\n\r\n**Error:**\r\n`Traceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\rzimm\\anaconda3\\envs\\rapidrzr\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\", line 122, in read\r\n    self._preread_check()\r\n  File \"C:\\Users\\rzimm\\anaconda3\\envs\\rapidrzr\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\", line 84, in _preread_check\r\n    compat.as_bytes(self.__name), 1024 * 512)\r\ntensorflow.python.framework.errors_impl.UnknownError: NewRandomAccessFile failed to Create/Open: ssd_mobilenet_v1_coco_2017_11_17\\output_inference_graph_v1.pb : Access is denied.\r\n; Input/output error\r\n>>> image_data = tf.gfile.FastGFile('ssd_mobilenet_v1_coco_2017_11_17/output_inference_graph_v1.pb', 'rb').read()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\rzimm\\anaconda3\\envs\\rapidrzr\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\", line 122, in read\r\n    self._preread_check()\r\n  File \"C:\\Users\\rzimm\\anaconda3\\envs\\rapidrzr\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\", line 84, in _preread_check\r\n    compat.as_bytes(self.__name), 1024 * 512)\r\ntensorflow.python.framework.errors_impl.UnknownError: NewRandomAccessFile failed to Create/Open: ssd_mobilenet_v1_coco_2017_11_17/output_inference_graph_v1.pb : Access is denied.`\r\n\r\n### Describe the problem\r\nI tried using the Object Detection Tutorial Notebook to classify some images, but instead of using the downloaded model, I substituted by another model I had trained. When trying to substitute, it returns an error, not allowing me to use the model I had trained with Tensorflow.\r\n\r\n### Source code / logs\r\n\r\n", "comments": ["@rzimmerdev,\r\nI was able to load the default model without any issues. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/3f79b58ce5ccd6299bba4ead37f49190/41482.ipynb).\r\n\r\nIn order to reproduce the issue, could you please share the code and the model you've built. Thanks!", "Yes, this is the model I froze and am now trying to use to predict the labels:\r\n[Frozen model](https://drive.google.com/drive/folders/1xnwN6kYJtWo4JHSEpWUfH0KHlx-YyP5u?usp=sharing)\r\n\r\n(I used the following **command to freeze the model**, as suggested in [the tutorial](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html)):\r\n\r\n`python export_inference_graph.py --input_type image_tensor --pipeline_config_path training/ssd_inception_v2_coco.config --trained_checkpoint_prefix training/model.ckpt-13302 --output_directory trained-inference-graphs/output_inference_graph_v1.pb`\r\n\r\n\r\nThe **command used to try and load the model** was:\r\n`image_data = tf.gfile.FastGFile('ssd_mobilenet_v1_coco_2017_11_17/output_inference_graph_v1.pb', 'rb').read()`\r\n\r\n_Before loading the model, I moved the .pb file to another folder, but I don't happen to think this should lead to any changes._\r\n\r\n@amahendrakar , thank you for your fast response!", "I am so sorry for my mistake, as I have solved the issue myself. \r\nThe problem was that I was freezing my model in a folder ending with .pb, and not a file with the .pb extension, as I thought that the default frozen model for the tutorial was also a folder with the .pb extension. \r\n\r\n**Redirecting my path to the .pb file inside my previous folder with the .pb ending changed the problem**, and I no longer received the Access Denied Error", "@rzimmerdev  Solved my problem after a full day of troubleshooting !! Thanks !! "]}, {"number": 41481, "title": "Fix tf32 for real.", "body": "I thought I fixed this in https://github.com/tensorflow/tensorflow/pull/40957, but when I fixed the build error on Windows, I inadvertently caused tf32 to stop working again.", "comments": ["Ugh I still fail Linux CPU and MacOS CPU. I tested Windows CPU and Linux GPU and assumed that would be enough.\r\n\r\nI will take a look tomorrow.", "Are the failures related? Might be broken at HEAD. I'll force a rerun. ", "So just to clarify, you're moving the dependency to if_static, but what if the user accesses the API from open source? What is the expected behavior?", "The expected behavior is the API is always accessible and there is one copy of the global variable. In the static case, it will directly be linked in. In the dynamic case, it will be linked in libtensorflow_framework but not any pywrap_tensorflow_internal or any other shared libraries.\r\n\r\nI think I fixed the issue, but we will see :)"]}, {"number": 41480, "title": "Server terminated abruptly (error code: 14, error message: 'Socket closed'", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):   Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):  source\r\n- TensorFlow version:  2.4.0\r\n- Python version: 3.5\r\n- Installed using virtualenv? pip? conda?:  docker\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 9.0\r\n- GPU model and memory:  Tesla P100 16G\r\n\r\nHere is my Dockerfile :\r\n\r\nFROM nvidia/cuda:9.0-cudnn7-devel-ubuntu16.04\r\n\r\nRUN apt-get update\r\n\r\nRUN apt-get install -y wget \\\r\n                vim \\\r\n                cmake\r\n\r\n\r\nRUN apt-get -y install \\\r\n    python3 \\\r\n    python3-pip \\\r\n    python3-setuptools\r\n\r\nHere is my configure:\r\nYou have bazel 3.1.0 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python3]:\r\n\r\n\r\nFound possible Python library paths:\r\n  /usr/lib/python3/dist-packages\r\n  /usr/local/lib/python3.5/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/lib/python3/dist-packages]\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: n\r\nNo TensorRT support will be enabled for TensorFlow.\r\n\r\nFound CUDA 9.0 in:\r\n    /usr/local/cuda-9.0/targets/x86_64-linux/lib\r\n    /usr/local/cuda-9.0/targets/x86_64-linux/include\r\nFound cuDNN 7 in:\r\n    /usr/lib/x86_64-linux-gnu\r\n    /usr/include\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as \"x.y\" or \"compute_xy\" to include both virtual and binary GPU code, or as \"sm_xy\" to only include the binary code.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]:\r\n\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]: n\r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]:\r\n\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]:\r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v2             # Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n\r\n\r\nHere is my compile command:\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\nand then I get the error:\r\n   ^\r\n[15,648 / 26,555] 16 actions running\r\n    Compiling tensorflow/compiler/xla/service/hlo_instructions.cc [for host]; 102s local\r\n    Compiling tensorflow/compiler/xla/service/hlo_computation.cc [for host]; 102s local\r\n    Compiling tensorflow/compiler/xla/service/hlo_instruction.cc [for host]; 101s local\r\n    Compiling tensorflow/compiler/xla/service/hlo_parser.cc [for host]; 97s local\r\n    Compiling tensorflow/compiler/xla/service/hlo_domain_map.cc [for host]; 96s local\r\n    Compiling tensorflow/compiler/xla/service/hlo_query.cc [for host]; 93s local\r\n    Compiling tensorflow/compiler/xla/service/channel_tracker.cc [for host]; 93s local\r\n    Compiling tensorflow/compiler/xla/service/collective_ops_utils.cc [for host]; 92s local ...\r\n\r\nServer terminated abruptly (error code: 14, error message: 'Socket closed', log file: '/root/.cache/bazel/_bazel_root/7c482caa6d82184234d890a1494c53ec/server/jvm.out')\r\n\r\nI search this error in issue, but I not found the answer. How can I  fix this bug?\r\n\r\nBy the way, I can compile success in win10 system, use the same tensorflow source, \r\nand the wheel named \"tensorflow-2.4.0-cp36-cp36m-win_amd64.whl\"\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@Julius-ZCJ \r\nPlease refer to [this link](https://github.com/tensorflow/models/issues/3647) with same error, this does not seem like a TF issue.\r\n\r\nSimilar error:\r\n#35431  [this link explains a directory issue](https://stackoverflow.com/questions/50658110/bazel-server-terminated-abruptly-error-code-14-error-message-log-file-c) [link1](https://github.com/tensorflow/tensorflow/issues/36093#issuecomment-580854910)", "@Saduf2019 \r\nYes, I have see these issues before I raise this issue, but not found fix this error method, if you can reproduce this error, can you tell me how to fix this bug.", "@Julius-ZCJ As mentioned [here](https://github.com/tensorflow/models/issues/3647#issuecomment-610666720), you are running out of memory while building from source. Please take a look at it and let me know if it helps. Thanks!", "@gowthamkpr \r\nOK, I will try again, I remember I still have memory when this error raise. I will run again to check it. Thank you", "@Julius-ZCJ Any updates on this issue?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41480\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41480\">No</a>\n", "Still get this issue got Tensorflow r2.3 on Cent OS 8, 16 cores, 16 GB RAM, CPU only build with bazel 3.7.0 and python 3.6.8 Intel(R) Xeon(R) CPU E5-2687W v4 @ 3.00GHz\r\nbazel build --config=opt --local_ram_resources=12000 --local_cpu_resources=14 -j 14 --remote_retries=100 --remote_max_connections=20 --remote_download_minimal --verbose_failures //tensorflow/tools/pip_package:build_pip_package\r\n\r\nAm using the following compiler flags to get a portable build without AVX and AVX2 flags:\r\n-O2 -g -pipe -Wall -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -Wp,-D_GLIBCXX_ASSERTIONS -fexceptions -fstack-protector-strong -grecord-gcc-switches -m64 -mtune=generic -fasynchronous-unwind-tables -fstack-clash-protection -fcf-protection -Wno-sign-compare", "@gowthamkpr Maybe helpful to reopen this issue.", "I have also encountered this problem. Generally, insufficient memory space will lead to this error. You can use \u3010-- local_ ram_ Resources = 40960\u3011 (40Gb) to limit the memory size\u3002"]}, {"number": 41479, "title": "Add auto_mixed_precision_mkl to run-once optimizer list", "body": "Add the MKL AutoMixedPrecision to run once optimizer list.", "comments": ["@rmlarsen - just a gentle ping on reviewing this PR. thanks", "@rmlarsen - have you got a chance to check this PR? thanks."]}, {"number": 41478, "title": "tf.ResizeNearestNeighbor not support convert to tensorflow lite ,i dont know how to slove it", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\nException: <unknown>:0: error: loc(fused[\"llyolo/up_sampling2d/resize/ResizeNearestNeighbor@__inference__wrapped_model_15290\", \"StatefulPartitionedCall/llyolo/up_sampling2d/resize/ResizeNearestNeighbor\"]): 'tf.ResizeNearestNeighbor' op is neither a custom op nor a flex op\r\n<unknown>:0: error: loc(fused[\"llyolo/up_sampling2d_1/resize/ResizeNearestNeighbor@__inference__wrapped_model_15290\", \"StatefulPartitionedCall/llyolo/up_sampling2d_1/resize/ResizeNearestNeighbor\"]): 'tf.ResizeNearestNeighbor' op is neither a custom op nor a flex op\r\n<unknown>:0: error: loc(fused[\"llyolo/up_sampling2d_2/resize/ResizeNearestNeighbor@__inference__wrapped_model_15290\", \"StatefulPartitionedCall/llyolo/up_sampling2d_2/resize/ResizeNearestNeighbor\"]): 'tf.ResizeNearestNeighbor' op is neither a custom op nor a flex op\r\n<unknown>:0: error: loc(fused[\"llyolo/up_sampling2d_3/resize/ResizeNearestNeighbor@__inference__wrapped_model_15290\", \"StatefulPartitionedCall/llyolo/up_sampling2d_3/resize/ResizeNearestNeighbor\"]): 'tf.ResizeNearestNeighbor' op is neither a custom op nor a flex op\r\n<unknown>:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag): ResizeNearestNeighbor,ResizeNearestNeighbor,ResizeNearestNeighbor,ResizeNearestNeighbor.\r\n", "comments": ["@Stevenzzz1996 \r\n\r\nRequest you to fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\nRequest you to provide simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41478\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41478\">No</a>\n"]}, {"number": 41477, "title": "S3 writable and appendable", "body": "@mihaimaruseac \r\nThis PR adds `tf_writable_file`, `NewWritableFile` and `NewAppendableFile`.", "comments": ["Internal import failed. Is there any way that I can access the result or run some tests locally to make sure my PR pass the internal import ?", "It failed again :(", "I'll import manually, sorry for the delay."]}, {"number": 41476, "title": "[Intel MKL] Enabling BatchNorm test in AutoMPMkl", "body": "This PR makes 2 fixes: one in BatchNormGrad and another one in ReLU.\r\n\r\nBatchNormGrad fix ensures that both the inputs of BatchNormGrad are\r\nin same layout. As per DNNL doc, lack of it would lead to sub-optimal\r\nperformance in BatchNormGrad.\r\n\r\nReLU fix ensures that if input of ReLU gets reordered into a blocked\r\nlayout from native layout, then output of ReLU carries correct\r\nmeta tensor.\r\n\r\nLast change enables 1 disabled unit test in AutoMixedPrecisionMkl.", "comments": ["@reedwm I was told this is related to the fix. Would you mind reviewing this?", "Talked to @reedwm offline. He said the mixed precision file looks good. \r\nI'll review the rest of the PR soon. Sorry for my delay!", "@nhasabni  Can you please resolve conflicts? Thanks!", "@gbaned Fixed it.", "@nhasabni  Can you please check @penpornk's comments and keep us posted ? Thanks!", "@nhasabni  Any update on this PR? Please. Thanks!", "@penpornk, @gbaned - thanks for reviewing on this PR. Sorry for the long delayed response to the comments from our team. I just pushed a new commit, and also responded to the review comments in the comment section. Please let me know if any additional action needed. Thanks"]}, {"number": 41475, "title": "[INTEL MKL] Adding OneDNN+MPI+Horovod partials and dockerfiles", "body": "This PR adds [OneDNN](https://github.com/oneapi-src/oneDNN) and `MPI+Horovod` partials and Dockerfiles for 3 most recent versions of `Ubuntu` that are still supported for a while.\r\n\r\nTo build these docker images, simply run:\r\n```\r\nalias asm_images=\"docker run --rm -v $(pwd):/tf -v /var/run/docker.sock:/var/run/docker.sock tf-tools python3 assembler.py \"\r\n\r\nTF_VERSION=2.2.0 && asm_images --release onednn --repository intel/intel-optimized-tensorflow --arg BAZEL_VERSION=2.0.0 --arg TF_BRANCH=v${TF_VERSION} --arg TF_PACKAGE_VERSION=${TF_VERSION} --arg _TAG_PREFIX=${TF_VERSION}-ubuntu --build_images --only_tags_matching '.*onednn-mpi-horovod.*'\r\n```\r\nOnce the builds are complete you should have the following images:\r\n\r\n```\r\nintel/intel-optimized-tensorflow:2.2.0-ubuntu-16.04-onednn-mpi-horovod\r\nintel/intel-optimized-tensorflow:2.2.0-ubuntu-16.04-onednn-mpi-horovod\r\nintel/intel-optimized-tensorflow:2.2.0-ubuntu-18.04-onednn-mpi-horovod\r\nintel/intel-optimized-tensorflow:2.2.0-ubuntu-18.04-onednn-mpi-horovod\r\nintel/intel-optimized-tensorflow:2.2.0-ubuntu-16.04-onednn-mpi-horovod-jupyter\r\nintel/intel-optimized-tensorflow:2.2.0-ubuntu-20.04-onednn-mpi-horovod\r\nintel/intel-optimized-tensorflow:2.2.0-ubuntu-20.04-onednn-mpi-horovod\r\nintel/intel-optimized-tensorflow:2.2.0-ubuntu-18.04-onednn-mpi-horovod-jupyter\r\nintel/intel-optimized-tensorflow:2.2.0-ubuntu-20.04-onednn-mpi-horovod-jupyter\r\n```", "comments": ["@googlebot \r\n/assign @angerson ", "Hi @angerson \r\nHere is the second set of `OneDNN` partials. Would you please review when you get a chance?\r\n\r\nThanks."]}, {"number": 41474, "title": "CycleGAN tranining pipeline in Documentation failing silently with ipython kernel dying", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 10.1 /  7.6.4.38-1+cuda10.1\r\n- GPU model and memory: GTX1080Ti / 12GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nWhen training a custom img2img cycleGAN on 4600+ images in each domain (9300+ images in total), the ipython kernel of the jupyter notebook dies silently some time into training.\r\n**Describe the expected behavior**\r\nTraining proceeds normally.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\nDue the data problem, it cannot really be reproduced elsewhere.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nThe one log I find fishy:\r\n\r\n```\r\n The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the\r\n partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to \r\n`dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\r\n```\r\nThis is the code I have:\r\n```\r\ntrain_A = train_A.map(\r\n    preprocess_image_train, num_parallel_calls=AUTOTUNE).cache().shuffle(\r\n    BUFFER_SIZE).batch(1)\r\n```\r\nI think I should change it to:\r\n```\r\ntrain_A = train_A.map(\r\n    preprocess_image_train, num_parallel_calls=AUTOTUNE).batch(1).cache().shuffle(\r\n    BUFFER_SIZE)\r\n```\r\nIs that correct?\r\n\r\nThank you!", "comments": ["@lzyang2000,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here or share the link to the documentation you're following. Thanks!", "Here is the link to the documentation that I am following:\r\n[https://www.tensorflow.org/tutorials/generative/cyclegan](https://www.tensorflow.org/tutorials/generative/cyclegan)\r\nIt is the same as my code save for the difference in the amount of data loaded.\r\n\r\nThank you!", "@lzyang2000,\r\nI was able to run the code without any issues, please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/96cff7fa952064730857a281591c0568/41474.ipynb). \r\n\r\nCould you try running the code in a virtual environment and check if you are facing the same issue. Thanks!", "Sorry for the late response, yes the original tutorial code works, it's when I try to load a custom dataset with more images (5000+) then it fails.\r\n\r\nThe problem is I do not have the resources to scale up the memory of the server that I am working on, so I cannot confirm whether or not it's a memory issue.\r\n\r\nHere is the code that I used to load the images:\r\n`train_rgb = tf.data.Dataset.list_files(\"/tf/gan/cycleGAN/images/\"+dataset_name+\"/trainA/*.jpg\")`\r\n`train_rgb = train_rgb.map(\r\n    preprocess_image_train, num_parallel_calls=AUTOTUNE).cache().shuffle(\r\n    BUFFER_SIZE).batch(1)`\r\n\r\nwhere `preprocess_image_train` is the same as in the original code.", "@lzyang2000 Can you please share a standalone code to reproduce the issue? You don't need to share your private data as I can use available public data to reproduce the issue. Thanks!", "Thanks, here is the code:\r\n```\r\nimport tensorflow as tf\r\ndataset_name=\"a\"\r\ntrain_rgb = tf.data.Dataset.list_files(\"/tf/gan/cycleGAN/images/\"+dataset_name+\"/trainA/*.jpg\")  # doctest: +SKIP\r\ntrain_thm = tf.data.Dataset.list_files(\"/tf/gan/cycleGAN/images/\"+dataset_name+\"/trainB/*.jpg\")  # doctest: +SKIP\r\ntest_rgb = tf.data.Dataset.list_files(\"/tf/gan/cycleGAN/images/\"+dataset_name+\"/testA/*.jpg\")  # doctest: +SKIP\r\ntest_thm = tf.data.Dataset.list_files(\"/tf/gan/cycleGAN/images/\"+dataset_name+\"/testB/*.jpg\")  # doctest: +SKIP\r\nBUFFER_SIZE = 1000\r\nBATCH_SIZE = 1\r\nIMG_WIDTH = 1024\r\nIMG_HEIGHT = 1024\r\ndef random_crop(image):\r\n  cropped_image = tf.image.random_crop(\r\n      image, size=[IMG_HEIGHT, IMG_WIDTH, 3])\r\n\r\n  return cropped_image\r\n# normalizing the images to [-1, 1]\r\ndef normalize(image):\r\n  image = tf.cast(image, tf.float32)\r\n  image = (image / 127.5) - 1\r\n  return image\r\ndef random_jitter(image):\r\n  # resizing to 286 x 286 x 3\r\n  image = tf.image.resize(image, [1144, 1144],\r\n                          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\r\n\r\n  # randomly cropping to 256 x 256 x 3\r\n  image = random_crop(image)\r\n\r\n  # random mirroring\r\n  image = tf.image.random_flip_left_right(image)\r\n\r\n  return image\r\ndef process_path(file_path):\r\n  # load the raw data from the file as a string\r\n  img = tf.io.read_file(file_path)\r\n  img = decode_img(img)\r\n  return img\r\n\r\ndef decode_img(img):\r\n  # convert the compressed string to a 3D uint8 tensor\r\n  img = tf.image.decode_jpeg(img, channels=3)\r\n  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\r\n # img = tf.image.convert_image_dtype(img, tf.float32)\r\n  # resize the image to the desired size.\r\n  return img\r\n\r\ndef preprocess_image_train(image,label=None):\r\n  image = process_path(image)\r\n  image = random_jitter(image)\r\n  image = normalize(image)\r\n  return image\r\n\r\ndef preprocess_image_test(image,label=None):\r\n  image = process_path(image)\r\n  image = normalize(image)\r\n  return image\r\n\r\n############# The place where TensorFlow logs the errors #############\r\ntrain_rgb = train_rgb.map(\r\n    preprocess_image_train, num_parallel_calls=AUTOTUNE).cache().shuffle(\r\n    BUFFER_SIZE).batch(1)\r\n\r\ntrain_thm = train_thm.map(\r\n    preprocess_image_train, num_parallel_calls=AUTOTUNE).cache().shuffle(\r\n    BUFFER_SIZE).batch(1)\r\n\r\ntest_rgb = test_rgb.map(\r\n    preprocess_image_test, num_parallel_calls=AUTOTUNE).cache().shuffle(\r\n    BUFFER_SIZE).batch(1)\r\n\r\ntest_thm = test_thm.map(\r\n    preprocess_image_test, num_parallel_calls=AUTOTUNE).cache().shuffle(\r\n    BUFFER_SIZE).batch(1)\r\n######################################################################\r\n```", "Hello, just wanting to see if there are any updates?\r\n\r\nThank you!", "@lzyang2000 Sorry for late response. Can you please share a full standalone code? Can you try to use any public datasets such as `mnist` and make the code code a standalone?  Without standalone code, it is difficult to find root-cause of the error you are facing. thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41473, "title": "Created using Colaboratory", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41473) for more info**.\n\n<!-- need_sender_cla -->", "Check out this pull request on&nbsp; <a href=\"https://app.reviewnb.com/tensorflow/tensorflow/pull/41473\"><img align=\"absmiddle\"  alt=\"ReviewNB\" height=\"28\" class=\"BotMessageButtonImage\" src=\"https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png\"/></a> \n\n Review Jupyter notebook visual diffs & provide feedback on notebooks. \n\n---\n\n <i>Powered by <a href='https://www.reviewnb.com'>ReviewNB</a></i>", "@slarrauri  Thank you for your contribution. Can you please sign CLA? Thanks!", "@slarrauri Can you please sign CLA. Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 41472, "title": "Add palette-based png support for tf.image.decode_png", "body": "This PR tries to address the issue raised in #28256 where\r\ntf.image.decode_png process palette-based png image\r\nincorrectly.\r\n\r\nThe issue was an redundant call png_set_rgb_to_gray in\r\ntensorflow/core/lib/png/png_io.cc. This PR fixes the issue.\r\n\r\nThis PR fixes #28256.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 41471, "title": "Add distribution_util to benchmarks.", "body": "Add distribution_util and support `multi_worker_mirrored`, `one_device`, `mirrored` and `off`. ", "comments": ["After[ this PR](https://github.com/tensorflow/tensorflow/pull/41456) merged into repo, I will resolve the conflicts in BUILD file.", "Can you run these checks again? @gbaned. Some errors didn't cause by my PR."]}, {"number": 41470, "title": "Model Maker Outputing Single File, no labels.txt", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/lite/tutorials/model_maker_image_classification\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe tutorial for Tensorflow Model Maker says that on export, there will be a model.tflite file and then a labels.txt file. However, when I export the model using the instructions, it only outputs a single model.tflite. The console output says that it is stored in a temp directory (which appears to be subsequently deleted), and that the labels are merged into the model.tflite file. Would I still be able to use this on mobile or is there any way I can extract labels.txt?\r\n\r\n### Clear description\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["Hello!\r\n\r\nI managed to extract the `labels.txt` file by the following method.\r\n- Let's say that you have a `dataset` folder where you train your images. Inside the `dataset` folder, there's bound to be subfolders consisting of images (e.g. inside the `dataset` folder, there is also `dandellions`, `tulips`, `lilies`, `roses`, et cetera).\r\n- The thing is that TensorFlow trains your data in a descending order. If we take above assumption, then TensorFlow will train your data starting from `dandellions`, `lilies`, `roses`, and then finally the `tulips`.\r\n- What I infer from this is that you can recreate your own `labels.txt` file by simply listing the names of each folder. How do we do that?\r\n\r\nWe simply use this following Python snippet (I run this on Google Colab's Jupyter Notebook):\r\n```\r\ndef recreate_labels():\r\n  # 1) We use this in order to ignore any hidden files that might be here.\r\n  # 'Datasets' is the name of the folder where we store our training data. The 'listdir' is used to fetch all the folder names.\r\n  labels = [folder for folder in os.listdir('datasets') if not folder.startswith('.')]\r\n  \r\n  # 2) Then, we output the contents of each folder name to a file.\r\n  with open('labels.txt', 'w') as file:\r\n    for label in labels:\r\n      file.write(label)\r\n      file.write('\\n')\r\n\r\nrecreate_labels()   \r\n```\r\n\r\nThis should be the workaround for now. After this is done, you could test it using various methods, one of them is [Python's TensorFlow Lite Image Classification Tester](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/python). If you are using this, then make sure to change the filename where it takes the `labels` file (line 94).\r\n\r\nAlternatively, you could use the [TensorFlow's Make Image Classifier](https://github.com/tensorflow/hub/tree/master/tensorflow_hub/tools/make_image_classifier). This tool also outputs the image classifier model that you are creating to a `.tflite` model, complete with `labels.txt`.\r\n\r\nThank you and please don't hesitate to comment if you have anymore questions!"]}, {"number": 41469, "title": "[Intel MKL] Fixing bfloat16 build failure in MklRelu", "body": "", "comments": []}, {"number": 41468, "title": "tf.esitmator.Estimator extremely slow with tf.data.Dataset", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Pip\r\n- TensorFlow version (use command below): 1.15.2\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: 2080 ti\r\n\r\n**Describe the current behavior**\r\nI have constructed a dataset which can read raw waveform and extract acoustic feature. Then I send it to tf.estimator.train_and_evaluate. But I find the GPU utils is 0 most of the time. It will go to peak periodically (but the period is very long, about half an hour). Based on my observation, I find the data reading is always running, and the reading and extraction take about 3 seconds per audios. I use 60 parallel workers , and set the batch size as 4.\r\n\r\n**Describe the expected behavior**\r\nBased on my understanding, the dataset read and process batch by batch. So it should feed the data to GPU after it finish processing the batch. In my case, since I use 60 parallel workers and set batch size as 4, it should take only couple of seconds to read the data, and then send the batch to GPU. But now I see data loading is always running, while GPU util is 0 most of time.\r\n\r\n**Standalone code to reproduce the issue**\r\nThe system is pretty complicated (it is an open source tool called spleeter: https://github.com/deezer/spleeter). I can give some key codes for showing the points that may have problems:\r\n\r\n1. The training code is\r\n```\r\ntf.estimator.train_and_evaluate(\r\n        estimator,\r\n        train_spec,\r\n        evaluation_spec)\r\n    ModelProvider.writeProbe(params['model_dir'])\r\n```\r\n\r\n2. The estimator is\r\n```\r\n\r\n    estimator = tf.estimator.Estimator(\r\n        model_fn=model_fn,\r\n        model_dir=params['model_dir'],\r\n        params=params,\r\n        config=tf.estimator.RunConfig(\r\n            save_checkpoints_steps=params['save_checkpoints_steps'],\r\n            tf_random_seed=params['random_seed'],\r\n            save_summary_steps=params['save_summary_steps'],\r\n            session_config=session_config,\r\n            log_step_count_steps=10,\r\n            keep_checkpoint_max=2))\r\n```\r\n\r\n3. The train_spec code:\r\n```\r\ndef _create_train_spec(params, audio_adapter, audio_path):\r\n    \"\"\" Creates train spec.\r\n\r\n    :param params: TF params to build spec from.\r\n    :returns: Built train spec.\r\n    \"\"\"\r\n    input_fn = partial(get_training_dataset, params, audio_adapter, audio_path)\r\n    train_spec = tf.estimator.TrainSpec(\r\n        input_fn=input_fn,\r\n        max_steps=params['train_max_steps'])\r\n    return train_spec\r\n```\r\n\r\n4. The `get_training_dataset` code:\r\n```\r\ndef get_training_dataset(audio_params, audio_adapter, audio_path):\r\n    \"\"\" Builds training dataset.\r\n\r\n    :param audio_params: Audio parameters.\r\n    :param audio_adapter: Adapter to load audio from.\r\n    :param audio_path: Path of directory containing audio.\r\n    :returns: Built dataset.\r\n    \"\"\"\r\n    builder = DatasetBuilder(\r\n        audio_params,\r\n        audio_adapter,\r\n        audio_path,\r\n        chunk_duration=audio_params.get('chunk_duration', 20.0),\r\n        random_seed=audio_params.get('random_seed', 0))\r\n    return builder.build(\r\n        audio_params.get('train_csv'),\r\n        cache_directory=audio_params.get('training_cache'),\r\n        batch_size=audio_params.get('batch_size'),\r\n        n_chunks_per_song=audio_params.get('n_chunks_per_song', 2),\r\n        random_data_augmentation=False,\r\n        convert_to_uint=True,\r\n        wait_for_cache=False)\r\n```\r\n\r\n5. The dataset builder code:\r\n```\r\n    def build(\r\n            self, csv_path,\r\n            batch_size=8, shuffle=True, convert_to_uint=True,\r\n            random_data_augmentation=False, random_time_crop=True,\r\n            infinite_generator=True, cache_directory=None,\r\n            wait_for_cache=False, num_parallel_calls=60, n_chunks_per_song=2,):\r\n        \"\"\"\r\n        TO BE DOCUMENTED.\r\n        \"\"\"\r\n        dataset = dataset_from_csv(csv_path)\r\n        dataset = self.compute_segments(dataset, n_chunks_per_song)\r\n        # Shuffle data\r\n        if shuffle:\r\n            dataset = dataset.shuffle(\r\n                buffer_size=200000,\r\n                seed=self._random_seed,\r\n                # useless since it is cached :\r\n                reshuffle_each_iteration=False)\r\n        # Expand audio path.\r\n        dataset = dataset.map(self.expand_path, num_parallel_calls=AUTOTUNE)\r\n        # Load waveform, compute spectrogram, and filtering error,\r\n        # K bins frequencies, and waveform.\r\n        N = num_parallel_calls\r\n        for instrument in self.instruments:\r\n            dataset = (\r\n                dataset\r\n                .map(instrument.load_waveform, num_parallel_calls=N)\r\n                .filter(self.filter_error)\r\n                .map(instrument.compute_spectrogram, num_parallel_calls=N)\r\n                .map(instrument.filter_frequencies,num_parallel_calls=AUTOTUNE))\r\n        dataset = dataset.map(self.filter_waveform, num_parallel_calls=AUTOTUNE)\r\n        # Convert to uint before caching in order to save space.\r\n        if convert_to_uint:\r\n            for instrument in self.instruments:\r\n                dataset = dataset.map(instrument.convert_to_uint, num_parallel_calls=AUTOTUNE)\r\n        dataset = self.cache(dataset, None, wait_for_cache)\r\n        # Check for INFINITY (should not happen)\r\n        for instrument in self.instruments:\r\n            dataset = dataset.filter(instrument.filter_infinity)\r\n        # Repeat indefinitly\r\n        if infinite_generator:\r\n            dataset = dataset.repeat(count=-1)\r\n        # Ensure same size for vocals and mix spectrograms.\r\n        # NOTE: could be done before caching ?\r\n        dataset = dataset.map(self.harmonize_spectrogram, num_parallel_calls=AUTOTUNE)\r\n        # Filter out too short segment.\r\n        # NOTE: could be done before caching ?\r\n        dataset = dataset.filter(self.filter_short_segments)\r\n        # Random time crop of 11.88s\r\n        if random_time_crop:\r\n            dataset = dataset.map(self.random_time_crop, num_parallel_calls=N)\r\n        else:\r\n            # frame_duration = 11.88/T\r\n            # take central segment (for validation)\r\n            for instrument in self.instruments:\r\n                dataset = dataset.map(instrument.time_crop, num_parallel_calls=AUTOTUNE)\r\n        # Post cache shuffling. Done where the data are the lightest:\r\n        # after croping but before converting back to float.\r\n        if shuffle:\r\n            dataset = dataset.shuffle(\r\n                buffer_size=256, seed=self._random_seed,\r\n                reshuffle_each_iteration=False)\r\n        # Convert back to float32\r\n        if convert_to_uint:\r\n            for instrument in self.instruments:\r\n                dataset = dataset.map(\r\n                    instrument.convert_to_float32, num_parallel_calls=N)\r\n        M = 8  # Parallel call post caching.\r\n        # Must be applied with the same factor on mix and vocals.\r\n        if random_data_augmentation:\r\n            dataset = (\r\n                dataset\r\n                .map(self.random_time_stretch, num_parallel_calls=M)\r\n                .map(self.random_pitch_shift, num_parallel_calls=M))\r\n        # Filter by shape (remove badly shaped tensors).\r\n        for instrument in self.instruments:\r\n            dataset = (\r\n                dataset\r\n                .filter(instrument.filter_shape)\r\n                .map(instrument.reshape_spectrogram, num_parallel_calls=AUTOTUNE))\r\n        # Select features and annotation.\r\n        dataset = dataset.map(self.map_features, num_parallel_calls=AUTOTUNE)\r\n        # Make batch (done after selection to avoid\r\n        # error due to unprocessed instrument spectrogram batching).\r\n        dataset = dataset.batch(batch_size)\r\n        return dataset.prefetch(AUTOTUNE)\r\n```\r\n\r\nThank you very much!", "comments": ["@sun-peach \r\n\r\nRequest you to share colab link or complete code snippet with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "Thank you, but the setting up is pretty complicated. And this code has been reported slow under different environments. So I assume the problem is in the code. Could anyone tell me why the estimator does not start to train the model even I see the dataset has iterated over all the data item?\r\n\r\nThank you once again.", "@sun-peach Its very hard for us to let you know the root cause of this issue without reproducing it. This is similar to this issue [here](https://stackoverflow.com/questions/53080032/tensorflow-1-11-estimator-dataset-training-speed-low-and-irregular). Please go through the comment ad let me know if it helps. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41466, "title": "Error", "body": "Got this error.\r\n\r\nImportError                               Traceback (most recent call last)\r\n/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59 \r\n\r\n/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py in <module>\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow_internal = swig_import_helper()\r\n     29     del swig_import_helper\r\n\r\n/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py in swig_import_helper()\r\n     23             try:\r\n---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n     25             finally:\r\n\r\n/opt/anaconda3/lib/python3.7/imp.py in load_module(name, file, filename, details)\r\n    241         else:\r\n--> 242             return load_dynamic(name, filename, file)\r\n    243     elif type_ == PKG_DIRECTORY:\r\n\r\n/opt/anaconda3/lib/python3.7/imp.py in load_dynamic(name, path, file)\r\n    341             name=name, loader=loader, origin=path)\r\n--> 342         return _load(spec)\r\n    343 \r\n\r\nImportError: dlopen(/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _SecKeyCopyExternalRepresentation\r\n  Referenced from: /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.2.dylib\r\n  Expected in: /System/Library/Frameworks/Security.framework/Versions/A/Security\r\n in /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.2.dylib\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-6-dc0837bd74eb> in <module>\r\n----> 1 from batchglm.api.models.tf1.glm_nb import Simulator\r\n      2 \r\n      3 sim = Simulator(num_observations=200, num_features=100)\r\n      4 sim.generate_sample_description(num_batches=0, num_conditions=2)\r\n      5 sim.generate_params(\r\n\r\n/opt/anaconda3/lib/python3.7/site-packages/batchglm/api/models/tf1/__init__.py in <module>\r\n----> 1 from . import glm_beta\r\n      2 from . import glm_nb\r\n      3 from . import glm_norm\r\n\r\n/opt/anaconda3/lib/python3.7/site-packages/batchglm/api/models/tf1/glm_beta.py in <module>\r\n      1 from batchglm.models.glm_beta import InputDataGLM, Model, Simulator\r\n----> 2 from batchglm.train.tf1.glm_beta import Estimator\r\n\r\n/opt/anaconda3/lib/python3.7/site-packages/batchglm/train/tf1/glm_beta/__init__.py in <module>\r\n----> 1 from .estimator import Estimator\r\n      2 from .estimator_graph import EstimatorGraph\r\n      3 from .model import BasicModelGraph, ModelVars, ProcessModel\r\n      4 from .hessians import Hessians\r\n      5 from .fim import FIM\r\n\r\n/opt/anaconda3/lib/python3.7/site-packages/batchglm/train/tf1/glm_beta/estimator.py in <module>\r\n      3 \r\n      4 import numpy as np\r\n----> 5 import tensorflow as tf\r\n      6 \r\n      7 from .external import TFEstimatorGLM, InputDataGLM, Model\r\n\r\n/opt/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py in <module>\r\n     39 import sys as _sys\r\n     40 \r\n---> 41 from tensorflow.python.tools import module_util as _module_util\r\n     42 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader\r\n     43 \r\n\r\n/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/__init__.py in <module>\r\n     48 import numpy as np\r\n     49 \r\n---> 50 from tensorflow.python import pywrap_tensorflow\r\n     51 \r\n     52 # Protocol buffers\r\n\r\n/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>\r\n     67 for some common reasons and solutions.  Include the entire stack trace\r\n     68 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 69   raise ImportError(msg)\r\n     70 \r\n     71 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/opt/anaconda3/lib/python3.7/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/opt/anaconda3/lib/python3.7/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: dlopen(/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _SecKeyCopyExternalRepresentation\r\n  Referenced from: /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.2.dylib\r\n  Expected in: /System/Library/Frameworks/Security.framework/Versions/A/Security\r\n in /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.2.dylib\r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n   * For TF-GPU - See point 1\n   * For TF-CPU - See point 2\n-----------------------------------------------------------------------------------------------\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\nMake sure you are using compatible TF and CUDA versions. Please refer following TF version and CUDA version compatibility table.\n| TF  | CUDA |\n| :-------------: | :-------------: |\n| 2.1.0 - 2.2.0  | 10.1 |\n| 1.13.1 - 2.0  | 10.0  |\n| 1.5.0 - 1.12.0 | 9.0 |\n\n  * If you have above configuration and using _**Windows**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n    * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n  * If you have above configuration and using _**Ubuntu/Linux**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n    * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n  * If error still persists then, apparently your CPU model does not support AVX instruction sets.\n    * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n   * Try Google Colab to use TensorFlow.\n      * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed latest stable TF version. Also you can use ```pip install```  to install any other preferred TF version.\n      * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n      * All you need is a good internet connection and you are all set.\n   * Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*\n", "Closing this issue as user has deleted account."]}, {"number": 41465, "title": "Updating labels during online augmentations - ImageGenerator + imgaug", "body": "Hi, \r\n\r\nI have been using imgaug with ImageGenerator's preprocessing_function for augmentations while training. This works well for non-geometric augmentations. \r\n\r\nMy labels include location, orientation, etc, and they need to change when I apply geometric transformations like flip/rotate. The preprocessing_function takes in just one argument: \"image\", so I do not have the label information at the time of augmentation, in this callback function.\r\n\r\nIs there a way to do this? Other that pre-augmenting the images.\r\n\r\n(I am on TensorFlow 2.2, Windows 10)\r\n\r\n", "comments": ["@ectg \r\n\r\nRequest you to share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "This isn't an issue per se, but a question on whether such a feature exists. Here is what I do for online augmentations right now:\r\n\r\n`datagen = ImageDataGenerator(preprocessing_function=additional_augmentation)`\r\n\r\nThe function additional_augmentation is user defined, with the stipulation that it takes in an image and nothing else. This function gets called for every image while training, and inside this function, I use the imgaug library to transform the image. But since I don't have access to the image label here, I cannot change it in response to geometric operations like flip. Hence I am sticking to non-geometric augmentations in the example below:\r\n\r\n`from imgaug import augmenters as iaa`\r\n\r\n```\r\naug = iaa.OneOf([iaa.ElasticTransformation(alpha=(0.1, 1.5), sigma=0.025),\r\n                    iaa.Multiply((1, 1.5), per_channel=0.2),\r\n                    iaa.ContrastNormalization((0.75, 2.5), per_channel=0.5),\r\n                    iaa.GaussianBlur(sigma=(3, 10.0))\r\n                    ])\r\n\r\n```\r\n\r\n```\r\ndef additional_augmentation(image):\r\naug.augment_image(image)\r\nreturn image\r\n```\r\n\r\nThe traingenerator is then declared as:\r\n```\r\ntrain_generator = datagen.flow_from_dataframe(dataframe=df_train,\r\n                                                directory=train_dir,\r\n                                                x_col='image',\r\n                                                y_col=['x', 'y', 'dvx', 'dvy'],\r\n                                                subset='training',\r\n                                                batch_size=batch_size,\r\n                                                shuffle=True,\r\n                                                class_mode=\"raw\", \r\n                                                target_size=(img_rows,img_cols))\r\n```\r\n\r\n", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\nThanks!\r\n"]}, {"number": 41464, "title": "Update \"master\" to \"dispatch\"/\"dispatcher\" in tf.data service terminology", "body": "Dispatcher is more descriptive and follows the guidance in https://developers.google.com/style/word-list#master\r\n\r\nPiperOrigin-RevId: 321613785\r\nChange-Id: Iaa576d35f0581e21278101f8b31201ba737a6865", "comments": []}]