[{"number": 20109, "title": "NotFoundError when using python to call funtion in .so file", "body": "Hi,\r\n   when I using TF1.5(compiled with ubuntu 14.04 cuda8.0 cudnn5.1 gcc4.8,blaze),I met error like this:\r\n  _hough_voting_gpu_module = tf.load_op_library(filename)\r\n  File \"/home/sky/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/load_library.py\", line 56, in load_op_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename, status)\r\n  File \"/home/sky/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.py\", line 473, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: <exception str() failed>\r\nin other test examples,errors are\r\n _hough_voting_gpu.so: undefined symbol: _ZN2cv6String8allocateEm\r\nand this function does exist in opencv(stl.cpp),Is the problem of ubuntu version?or something else?Thank you!", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "\r\nOS Platform and Distribution:Ubuntu14.04\r\nTensorFlow installed from:download from github\r\nTensorFlow version:Tensoflow1.5\r\nBazel version:0.12.0-dist\r\nCUDA/cuDNN version:cuda8.0,cudnn5.1\r\nGPU model and memory:GTX1060,6G\r\nExact command to reproduce:\r\n\r\n1.source code\r\nthe whole code is long,and I paste it in https://blog.csdn.net/u010364639/article/details/80923385\r\nyou can make it and compile it rightly.\r\n2.use \"python test.py\" to run the test.py file \r\n\r\nerror this like:\r\n_hough_voting_gpu_module = tf.load_op_library(filename)\r\nFile \"/home/sky/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/load_library.py\", line 56, in load_op_library\r\nlib_handle = py_tf.TF_LoadLibrary(library_filename, status)\r\nFile \"/home/sky/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.py\", line 473, in exit\r\nc_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: <exception str() failed>\r\nin other test examples,errors are\r\n_hough_voting_gpu.so: undefined symbol: _ZN2cv6String8allocateEm\r\nand this function does exist in opencv(stl.cpp),I tried many opencv version,and believe the error didn't come from opencv.Can you give me solution? Thank you!\r\n", "Nagging Assignee @ispirmustafa: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ispirmustafa: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hi @michaelisard , unfortunately I'm not relevant with this part of the code. @martinwicke could you please find us a relevant developer? thanks!", "Please check your linker settings. You may not be including libstdc++ properly. \r\n\r\nI don't think this is an issue with TensorFlow (apart from the fact that compiling custom ops should be simpler)."]}, {"number": 20108, "title": "Update glorot_uniform_initializer to match other Initializers", "body": "This fix tries to address the issue raised in #19910 where the signature of glorot_uniform_initializer does not match other Initializers (e.g., random_uniform_initializer).\r\n\r\nThis fix update the glorot_uniform_initializer so that it matches the signature in a similiar way as other\r\nInitializers.\r\n\r\nThis fix fixes #19910.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Nudge @fchollet for review/approval.", "Sorry for the late reply. Please resolve the merge conflict, and fix the api compatibility test failure. See  https://source.cloud.google.com/results/invocations/4de5073a-8d47-4424-a7fa-bef0a215d19c/targets/%2F%2Ftensorflow%2Ftools%2Fapi%2Ftests:api_compatibility_test/log", "@qlzh727 Thanks for the review. The PR has been updated and all tests passes now. Please take a look and let me know if there are any issues.", "Sure. Will cc @martinwicke on the change.", "@tensorflow/api-owners This is an almost-noop in terms of API, I think it's fine.", "Nagging Assignee @qlzh727: It has been 16 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 20107, "title": "Update tb-nightly dep to >= 1.10.0a0, < 1.11.0a0", "body": "Synchronize tf-nightly dep on current tb-nightly: https://pypi.org/project/tb-nightly/1.10.0a20180618/", "comments": []}, {"number": 20106, "title": "Negative Axis Support for tf.manip.roll and other ops", "body": "tf.manip.roll does not support a negative axis argument. Some ops have had support for negative axes added (e.g. see #6022 regarding argmin), so perhaps tf.manip.roll should be modified, or maybe it should warn the user that the axis argument must not be negative. \r\n\r\nIn general, is there a guideline for knowing when a negative axis is supported by a TensorFlow op? \r\n\r\n### System information\r\nI am using TensorFlow 1.8 installed from binary, Python 3.6, and macOS 10.13.4.\r\n\r\n### The problem\r\n\r\nI expected tf.manip.roll to behave the same as numpy.roll when given a negative axis argument. As shown in the following code, this is not the case.\r\n\r\n```python\r\nx = tf.placeholder(tf.float32, [None, 5])\r\ny = tf.manip.roll(x,1,1)\r\nz = tf.manip.roll(x,1,-1)\r\n\r\nwith tf.Session() as sess:\r\n    print(sess.run([y,z], feed_dict={x:[range(5)]}))\r\n```\r\n\r\n[array([[4., 0., 1., 2., 3.]], dtype=float32), array([[0., 1., 2., 3., 4.]], dtype=float32)]\r\n\r\n```python\r\nx= np.array([range(5)])\r\ny = np.roll(x,1, axis=1)\r\nz = np.roll(x,1, axis=-1)\r\n\r\nprint(y, z)\r\n```\r\n[[4 0 1 2 3]] [[4 0 1 2 3]]\r\n\r\nThank you!", "comments": ["@bbartoldson I think PR #18409 added the negative axis support for tf.manip.roll. It will be available in the next version I believe.", "Awesome. I tried searching the open issues for this, but I didn't think about checking pull requests. Thanks!"]}, {"number": 20105, "title": "Add Apache Ignite dataset", "body": "This is a proposal to add IgniteDataset so that it is possible to read data from Apache Ignite. Ignite is a memory-centric distributed database, caching, and processing platform for transactional, analytical, and streaming workloads, delivering in-memory speeds at petabyte scale.\r\n\r\nThis proposal is a part of a more global initiative to implement so called \"TensorFlow on Apache Ignite\". You can read more about it in TensorFlow ecosystem issues (https://github.com/tensorflow/ecosystem/issues/81), Apache Jira (https://issues.apache.org/jira/browse/IGNITE-8335) and in correspondent design document (https://docs.google.com/document/d/1jROIahK1rc7bSgOvhJhfpMqIGvht_IE8zn5NAt6x8ks/edit?usp=sharing).\r\n\r\nThis pull request is a first step of the integration and it allows to read data from Apache Ignite Cache that with integer as keys and strings as values. The integration is based on Apache Ignite C++ Fat Client (https://apacheignite-cpp.readme.io/docs) that made the configuration a bit tricky (the upcoming Apache Ignite release includes C++ Thin Client that significantly simplifies configuration and improves performance).\r\n\r\nTests have also been added. They use docker to hide configuration complexity, so that the implemented functionality can be tested quite simply by manual run.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "@artemmalykh, can you sign the CLA if you haven't sign it before? Thanks.", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Adding Martin and Derek for review, feel free to add more people since this is non-trivial PR.", "@mrry @martinwicke Gentle ping.", "@mrry Do you have a minute to check my changes? :)", "Looks like there are some test failures... can you please take a look?", "@mrry Fixed everything. About tests: there was a compilation error in commit https://github.com/tensorflow/tensorflow/pull/20105/commits/16670918fad6960c8debd89bdd61b7c0226f785a , it was also fixed, project builds again. ", "@artemmalykh could you fix the pylint and buildifier errors? Then I'll run the tests again.", "@martinwicke Please, rerun tests", "@mrry Thanks for rerunning tests. It is strange that tests are still failing, locally I used docker image of tf to run sanity checks on my branch and it has shown no errors from pylint. I'll fix everything according to last error log, but still there is the question: is runing tensorflow/contrib/ignite/python/kernel_tests/ignite_test.py should be equivalent to 'Ubuntu Sanity'? Also, what should i do with 'redifining builtin' for reduce in python2 check? In python3 it is not build-in in contrast to python2. ", "Ubuntu sanity runs a number of tests unrelated to your ignite_test, including pylint for py2 and py3. Any code in TF must work with both py2 and py3, hence the redefining builtin error.", "@martinwicke Sorry, wrong string from clipboard, I meant not ignite_test.py, of course, but tensorflow/tools/ci_build/ci_sanity.sh. From invocation details seems that it should be equivalent , but when I run it locally, it gave me different result. Also, I see that build of ee72b9c failed, but I cannot see link for details of 'Ubuntu Sanity' so I cannot see what's wrong (but link can be seen near almost every other entry in checks list (except for cla which is understandable)) .", "Can someone rerun the tests please?", "Guys, hello. Do I understand correctly that all required tests passed and PR is ready to be merged?", "It looks like the build is still failing, with errors like the following:\r\n\r\n```\r\nIn file included from external/ignite/modules/platforms/cpp/core/src/impl/transactions/transactions_impl.cpp:18:\r\nIn file included from external/ignite/modules/platforms/cpp/core/include/ignite/impl/transactions/transactions_impl.h:26:\r\nIn file included from external/ignite/modules/platforms/cpp/core/include/ignite/impl/interop/interop_target.h:22:\r\nIn file included from external/ignite/modules/platforms/cpp/core/include/ignite/impl/operations.h:28:\r\nexternal/ignite/modules/platforms/cpp/binary/include/ignite/impl/binary/binary_reader_impl.h:999:58: error: variable has incomplete type 'ignite::binary::BinaryReader'\r\n                            ignite::binary::BinaryReader reader(&readerImpl);\r\n                                                         ^\r\nexternal/ignite/modules/platforms/cpp/binary/include/ignite/impl/binary/binary_type_impl.h:30:15: note: forward declaration of 'ignite::binary::BinaryReader'\r\n        class BinaryReader;\r\n              ^\r\nIn file included from external/ignite/modules/platforms/cpp/core/src/impl/transactions/transactions_impl.cpp:18:\r\nIn file included from external/ignite/modules/platforms/cpp/core/include/ignite/impl/transactions/transactions_impl.h:26:\r\nIn file included from external/ignite/modules/platforms/cpp/core/include/ignite/impl/interop/interop_target.h:22:\r\nIn file included from external/ignite/modules/platforms/cpp/core/include/ignite/impl/operations.h:29:\r\nexternal/ignite/modules/platforms/cpp/binary/include/ignite/impl/binary/binary_writer_impl.h:719:54: error: variable has incomplete type 'ignite::binary::BinaryWriter'\r\n                        ignite::binary::BinaryWriter writer(&writerImpl);\r\n                                                     ^\r\nexternal/ignite/modules/platforms/cpp/binary/include/ignite/impl/binary/binary_type_impl.h:31:15: note: forward declaration of 'ignite::binary::BinaryWriter'\r\n        class BinaryWriter;\r\n              ^\r\n3 warnings and 2 errors generated.\r\n```", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "Time flies... During my attempts to fix all compilation issues our team has prepared fully new IgniteDataset based on Apache Ignite Binary Client Protocol. It's more lightweight, doesn't require Java on client-side and supports a lot of cute features (https://github.com/tensorflow/tensorflow/pull/21853).\r\n\r\nAs result of these I think that it makes sense to close this pull request. Let me say a big thank you to everyone who reviewed this request, I really apreciate your effort which was very helpful to build this new solution.", "Can [Rapids](https://rapids.ai/index.html) which depends on Apache Arrow support tensorflow? Currently, only PyTorch and Chainer might be supported.", "@SpikingNeurons That might make a good feature request/contribution to the new [community-managed tensorflow/io repository](https://github.com/tensorflow/io)."]}, {"number": 20104, "title": "Changing the colab link to the right one", "body": "Changing the colab link to the right one so it doesn't crash", "comments": []}, {"number": 20103, "title": "tensorflow/go: add tests for zero length arrays passed to C", "body": "This fixes some nil panics on zero length arrays passed to C in Consumers and Attr introduced in #19953  and #19915 as well as tests for them.\r\n\r\nIt also preserves the property that an unknown attribute name throws an error instead of just returning the empty value in all cases.\r\n\r\nCC @asimshankar ", "comments": ["I just updated this to do the range checks up front in most places. It's not possible in all places since total_size can still be zero if list_size > 0.", "If someone removes/changes the _ function it'll break the test & build so no worries there right?", "@d4l3k : Yeah, so the change will be detected by the broken build/test. It's just sub-optimal to have tests that depend on internal details, but since this is isolated to a test, we can go ahead and perhaps fix it up later. Thanks very much for the contribution!"]}, {"number": 20102, "title": "cpu and gpu Dockerfiles for ppc64le", "body": "Adding Dockerfile.cpu.ppc64le and Dockerfile.gpu.ppc64le to enable the ability\r\nto do builds using docker on ppc64le. Also enables the ability to run\r\nci_sanity.sh (from ci_build.sh) on ppc64le.\r\n\r\nModified ci_build.sh and ci_parameterized_build.sh to accept container types\r\nthat start with cpu or gpu.\r\n\r\nAdded install_bazel_from_source.sh and install_buildifier_from_source.sh install\r\nscripts to avoid installing x86 versions of the binaries. These scripts could be\r\nused by other platforms in the future.", "comments": []}, {"number": 20101, "title": "Add missing Eager relnotes for TensorFlow 1.9.", "body": "", "comments": []}, {"number": 20100, "title": "R1.9", "body": "test", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "The release of TensorFlow is managed by TF team member, and this PR seems to be a test, judging from it commit message."]}, {"number": 20099, "title": "QR factorization unstable on GPU", "body": "### Describe the problem\r\nPerforming QR factorization of relatively large matrices on the GPU is numerically unstable. I am unsure if it's a tensorflow issue, or an upstrea cuSOLVE issue, as I don't have the time to write a custom cuSOLVE C call to test. The issue is neither present on the CPU, nor in numpy.\r\n\r\nSteps to reproduce:\r\n\r\n```\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\n\r\nwith tf.device('/gpu:0'):\r\n    q, r = tf.qr(tf.random_normal((2048, 1024))\r\n    print(tf.norm(q, axis=0))\r\n    print(tf.reduce_sum(tf.cast(tf.abs(tf.norm(q, axis=0) - 1) > 1, tf.float32)))\r\n```\r\n\r\nRun this multiple times and you get *wildly* different results, sometimes all norms are almost 1.0, sometimes the last entries have \"slightly\" larger norms 1-10, sometimes the last norms are huge (example: 3.1662303e+11), and sometimes they are NaN.\r\n\r\nRunning in float64 does not solve the issue.\r\n\r\nRunning on the CPU does solve the issue.\r\n\r\n### System information\r\n\r\nTensorflow version as per `print(tf.GIT_VERSION, tf.VERSION)`:  `v1.7.0-3-g024aecf414 1.7.0` \r\n\r\nWhat follows is the output of `tf_env_collect.sh`:\r\n\r\nNote 1: I cannot upgrade to latest TF to check it there right now.\r\nNote 2: Multiple cuda versions are shown below, but only 9.0 is the first one in all `*PATH` variables.\r\n\r\n```\r\n== cat /etc/issue ===============================================\r\nLinux grimbergen 4.4.0-116-generic #140~14.04.1-Ubuntu SMP Fri Feb 16 09:25:20 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"14.04.5 LTS, Trusty Tahr\"\r\nVERSION_ID=\"14.04\"\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 4.8.4-2ubuntu1~14.04.4) 4.8.4\r\nCopyright (C) 2013 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux grimbergen 4.4.0-116-generic #140~14.04.1-Ubuntu SMP Fri Feb 16 09:25:20 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy              1.14.2      \r\nprotobuf           3.5.2.post1 \r\ntensorflow-gpu     1.7.0       \r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.7.0\r\ntf.GIT_VERSION = v1.7.0-3-g024aecf414\r\ntf.COMPILER_VERSION = v1.7.0-3-g024aecf414\r\n/home/beyer/.pyenv/versions/tensorflow-latest/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /home/beyer/inst/cudnn7/lib64:/usr/local/cuda-9.0/extras/CUPTI/lib64/:/usr/local/cuda-9.0/lib64:/home/beyer/inst/cudnn6/lib64:/usr/local/cuda-8.0/extras/CUPTI/lib64/:/usr/local/cuda-8.0/lib64:/home/beyer/inst/lib64:/home/beyer/inst/lib:\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nMon Jun 18 18:14:33 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 384.111                Driver Version: 384.111                   |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  TITAN X (Pascal)    Off  | 00000000:01:00.0 Off |                  N/A |\r\n| 30%   50C    P8    22W / 250W |  11591MiB / 12187MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce GTX 108...  Off  | 00000000:02:00.0 Off |                  N/A |\r\n| 44%   39C    P2    60W / 250W |  10625MiB / 11172MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      4713      C   ...3.6.2/envs/tensorflow-latest/bin/python 11581MiB |\r\n|    1      4713      C   ...3.6.2/envs/tensorflow-latest/bin/python 10615MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-8.0/lib64/libcudart_static.a\r\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0.44\r\n/usr/local/cuda-9.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-9.0/lib64/libcudart_static.a\r\n/usr/local/cuda-9.0/lib64/libcudart.so.9.0.176\r\n/usr/local/cuda-7.5/lib/libcudart_static.a\r\n/usr/local/cuda-7.5/lib/libcudart.so.7.5.18\r\n/usr/local/cuda-7.5/doc/man/man7/libcudart.7\r\n/usr/local/cuda-7.5/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-7.5/lib64/libcudart_static.a\r\n/usr/local/cuda-7.5/lib64/libcudart.so.7.5.18\r\n\r\n", "comments": ["@chsigg @zheng-xq - Mind taking a look?", "Just adding that I found this out when orthogonal_initializer for a large matrix sometimes blew up my runs from the start and sometimes didn't.", "@rmlarsen in case you have some ideas.", "Nagging Assignee @chsigg: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "There was/is a bug in cuSOLVE, so the gpu kernel now calls cpu one (to avoid collocation problems with variable initialization that uses qr).\r\nTake a look at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/qr_op_float.cc. if interested.", "Hi @lucasb-eyer, somewhat funny that we stumble upon the same problems. :) (Actually via @vieting.)", "Haha hey @albertz ! Small world indeed, hope to also bump into you again at conferences when things go back to normal =)"]}, {"number": 20097, "title": "r1.9 cherry-pick request: Fix keras guide (docs only)", "body": "r1.9 currently only contains a rough draft of the keras guide. It was rewritten by this commit. We'd prefer to publish this one.\r\n\r\nOriginal Commit message:\r\n\r\nCopy edits to Keras guide, formatting, moving some things around.\r\n\r\nMake the right TOC nav more useful.\r\n\r\nPiperOrigin-RevId: 199863216", "comments": []}, {"number": 20096, "title": "ModuleNotFoundError: No module named 'tensorflow.keras'", "body": "\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary (CPU linux wheel)\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: 3.6.5\r\n\r\n### Describe the problem\r\n\r\nUsing Tensorflow 1.8.0, running:\r\n```python\r\nfrom tensorflow.keras.utils import Progbar\r\n```\r\n\r\nraises an error:\r\n```\r\nModuleNotFoundError: No module named 'tensorflow.keras'\r\n```\r\n\r\nOf course, `from tensorflow import keras` works fine.\r\n\r\nThis is a minor nit since there's an obvious workaround, but IMO this is pretty unintuitive behavior for how modules work in Python. I'm not sure what kind of sorcery is going on to end up with this result :laughing:.", "comments": ["```python\r\nfrom tensorflow.python.keras.utils import Progbar\r\n```", "As @brge17 pointed out, the package organization is a bit nuanced. You could use aliases instead as well:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nProgbar = tf.keras.utils.Progbar\r\n```\r\n\r\nClosing this since we are unlikely to change package organization in the near future.\r\n", "> ```python\r\n> from tensorflow.python.keras.utils import Progbar\r\n> ```\r\n\r\nI had the same problem but your solution was useful.\r\nthanks"]}, {"number": 20095, "title": "Verification invalid eager env_ or  env_->rendezvous_mgr.", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "Seems that the CLA status is not updated. Can u check again and make sure you have signed it? Thanks.", "CLAs look good, thanks!\n\n<!-- ok -->", "Just a side note, what's the error scenario when you hit the nullptr in this case. Also please update the test case. ", "@qlzh727   updated the test case.  ", "updated files.@akshaym", "ping @xxxx001 to resolve merge conflicts and ping @akshaym for the final approval. ", "modify . @qlzh727 ", "modify @akshaym, It is mainly caused by user's habit of calling function. so  i  only  modify   Status EagerServiceImpl::CreateContext func ,  All the others have been removed.", "Nudge @akshaym for review/approval. "]}, {"number": 20094, "title": "Wrong Results for tf.matmul(X,Y) with X.shape[0]>2^21", "body": "\r\n\r\n------------------------\r\n\r\n### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Win 10, 64bit\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.8.0 (same issue with 1.5.x)\r\n- **Python version**: 3.5.3\r\n- **GPU model and memory**: GTX 1080 \r\n\r\n\r\n### Describe the problem\r\nWhen using `tf.matmul(X,Y)` with `X.shape==[n,2]` ,`Y.shape==[2,2]` **AND** `n>2^21 #=2097152` only the first `[0:2^21 , : ]` results are correct\r\n\r\n- Does only occur on GPU\r\n- Does not occur if Y==tf.eye(2)\r\n- it is possible to circumvent the problem by splitting the operation into chunks of `n<=2**21`\r\n\r\n-> i guess a solution would be to automaticaly split the operation or warn the user \r\n\r\n### Source code\r\n`import tensorflow as tf`\r\n\r\n`X=tf.ones([10+2**21,2],dtype=tf.float32)`\r\n`Y=tf.constant([[1,0],[0.01,1]],dtype=tf.float32)`\r\n`mul=tf.matmul(X,Y)`\r\n\r\n`sess=tf.Session()`\r\n`sess.run(tf.global_variables_initializer())`\r\n`result=sess.run(mul)`\r\n`sess.close()`\r\n\r\n`print(\"These are ok: \" + str(result[2**21-2:2**21,:]))`\r\n\r\n`print(\"These are wrong: \" + str(result[2**21:2**21+2,:]))`\r\n\r\n\r\n### Output Log\r\n\r\n_2018-06-18 14:49:53.627009: I T:\\src\\github\\tensorflow\\tensorflow\\core\\platform\\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2018-06-18 14:49:53.868050: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1356] Found device 0 with properties: \r\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7715\r\npciBusID: 0000:03:00.0\r\ntotalMemory: 8.00GiB freeMemory: 6.61GiB\r\n2018-06-18 14:49:53.868514: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1435] Adding visible gpu devices: 0\r\n2018-06-18 14:49:54.450846: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-06-18 14:49:54.451158: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:929]      0 \r\n2018-06-18 14:49:54.451346: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:942] 0:   N \r\n2018-06-18 14:49:54.451641: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6384 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:03:00.0, compute capability: 6.1)_\r\n\r\nThese are ok: [[1.01 1.  ]\r\n [1.01 1.  ]]\r\nThese are wrong: [[0. 0.]\r\n [0. 0.]]\r\n", "comments": ["Example for the workaround is to use this function (might not be perfect, but it works):\r\n```\r\nimport tensorflow as tf\r\n```\r\n```\r\ndef large_koord_tensor_mul(tensor1, tensor2):\r\n    steps = int(int(tensor1.shape[0]) / 2097152)\r\n    if steps == 0:\r\n        output = tf.matmul(tensor1, tensor2)\r\n    else:\r\n        output = tf.matmul(tensor1[0:2097152, :], tensor2)\r\n        for i in range(1, steps):\r\n            addon_output = tf.matmul(tensor1[i * 2097152:(i + 1) * 2097152, :], tensor2)\r\n            output = tf.concat([output, addon_output], 0)\r\n        addon_output = tf.matmul(tensor1[steps * 2097152:, :], tensor2)\r\n        output = tf.concat([output, addon_output], 0)\r\n    return output\r\n```\r\n\r\n\r\n\r\n\r\n```\r\nX=tf.ones([10+2**21,2],dtype=tf.float32)\r\nY=tf.constant([[1,0],[0.01,1]],dtype=tf.float32)\r\n\r\n\r\nmul=large_koord_tensor_mul(X,Y)\r\n\r\nsess=tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\n\r\nresult=sess.run(mul)\r\nsess.close()\r\n\r\nprint(\"These are ok:\" + str(result[2**21-2:2**21,:]))\r\n\r\nprint(\"These are NOT wrong\" + str(result[2**21:2**21+2,:]))\r\n```\r\n\r\n\r\n### Result\r\n\r\n\r\nThese are ok:[[1.01 1.  ]\r\n [1.01 1.  ]]\r\nThese are NOT wrong[[1.01 1.  ]\r\n [1.01 1.  ]]", "@zheng-xq @chsigg : I believe something like this was recently addressed?", "Nagging Assignee @chsigg: It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This has been fixed on Linux, tested with tensorflow/tensorflow:1.11.0-devel-gpu-py3 docker.\r\n\r\nThe issue has been reported on Windows, which I have no easy means to test. If you still experience the issue, please let us know."]}, {"number": 20093, "title": "Feature request: add Tensorflow.js", "body": "Thanks for this tutorial, found it super-useful and helped me understand a lot. It would be great to see a version that uses Tensorflow.js / for Node, and how this works differently.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@nkreeger would know about such documentation in the pipeline.", "Hi @stopsatgreen which tutorial are you referencing?\r\n\r\nWe have a variety of docs up on http://js.tensorflow.org and our examples repo has some node-specific demos as well: https://github.com/tensorflow/tfjs-examples", "@nkreeger Sorry, it was *Tensorflow for Poets*. I went through the Python demo, which was great, would love to see the same for JS.", "Glad you found it helpful. There are no plans to port that specific tutorial to TensorFlow.js, but there are others available here: https://js.tensorflow.org/tutorials/\r\nBut will keep *TensorFlow for Poets* in mind for the future. Thanks."]}, {"number": 20092, "title": "Tensorflow Website XSRF Token missing or incorrect", "body": "The error occurs when I click develop button in \"www.tensorflow.org/programmers_guide/variables\" or in everywhere. My default lang is set to English(but it does not matter and has influence only on the bottom menu). But when I'm trying to change the language to Chinese I get the following error message:\r\n\r\n- XSRF Token missing or incorrect\r\n\r\n![image](https://user-images.githubusercontent.com/19337606/41533342-02b6581e-732d-11e8-95a9-afbe28a40dbe.png)\r\n\r\n![image](https://user-images.githubusercontent.com/19337606/41533370-1cda29dc-732d-11e8-9912-2eb9583feb71.png)\r\n\r\nWindows 10\r\nVersion 67.0.3396.87 (Official Build) (64-bit)\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@wolffg - mind taking a look?", "There are XSRF issues with switching languages if you are signed in with multiple accounts.  Is that the case here?", "Nagging Assignee @wolffg: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Given there's been no followup in two weeks, I'm closing this.  We are aware of a bug with XSRF and multi-login, and I assume this is was what happened here.  The site eng team is working on it.\r\n\r\nIf it wasn't please reopen or refile.  Thanks!"]}, {"number": 20091, "title": "NaN appearing on tf.gradients calculation with tf.where and division by zero on the false branch", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes, script is below\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux 4.15.0-23-generic #25-Ubuntu SMP Wed May 23 18:02:16 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"18.04 LTS (Bionic Beaver)\"\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\nv1.8.0-0-g93bc2e2072 1.8.0\r\n- **Python version**: \r\nPython 3.6.5\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **GCC/Compiler version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: n/a, using CPU version\r\n- **GPU model and memory**: n/a using CPU\r\n- **Exact command to reproduce**: just run \"python3 script.py\"\r\n\r\n### Describe the problem\r\nWhen using the tf.where function where a division by zero exists in one of the two where branches, you get a NaN gradient even if the division by zero was on the where branch which was not executed.\r\n\r\nThis seems similar to https://github.com/tensorflow/tensorflow/issues/2540 but the workarounds suggested there (e.g. using tf.boolean_mask) did not work.\r\n\r\n### Source code / logs\r\n\r\n```\r\nimport tensorflow as tf\r\nsess = tf.Session()\r\nW1 = tf.Variable([2.0])\r\nW2 = tf.Variable([0.0])\r\noutput=tf.where(W1>4, W1/W2, tf.zeros_like(W1))  # gives correct answer (zero) since W1>4 is false\r\ngradient=tf.gradients(output, W2)[0] # should be zero, but it gives NaN\r\nsess.run(tf.global_variables_initializer())\r\nprint(sess.run([output, gradient]))\r\n```\r\n\r\n# Program output:\r\n#[array([0.], dtype=float32), array([nan], dtype=float32)]\r\n", "comments": ["I agree that the issue is duplication of #2540, do you try the workaround below suggested by @anishathalye ? \r\n\r\n```python\r\nx = tf.placeholder(tf.float32)\r\n# y = tf.where(x > 0, 0., tf.exp(x))\r\n\r\n# trick: we're not using the result of safe_exp when x > 0, so we can\r\n# substitute a safe value for x in that case\r\n# it doesn't really matter what we put in here, as long as the backward pass\r\n# returns some finite value\r\nsafe_exp = tf.exp(tf.where(x > 0, 1.0, x))\r\ny = tf.where(x > 0, 0., safe_exp)\r\n```\r\n\r\nI think it should solve your problem. \r\n\r\nIn fact, I also propose to implement a new op #15706 to fix the issue totally, unfortunately, google tensorflow don't reply to it.", "Thanks.  Yes it did solve my problem.  \r\n\r\nimport tensorflow as tf\r\nsess = tf.Session()\r\n\r\nW1 = tf.Variable([2.0])\r\nW2 = tf.Variable([0.0])\r\n\r\nsafe_W2 = tf.where(W1>4, W2, [1.0])\r\noutput = tf.where(W1 >4, W1/safe_W2, tf.zeros_like(W1))\r\ngradient=tf.gradients(output, W2)[0] \r\nsess.run(tf.global_variables_initializer())\r\n\r\nprint(sess.run([output, gradient])) # prints 0,0, i.e. correct answers now", "Nagging Assignee @skye: It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 20090, "title": "Error reported to Coordinator: /dbfs/FileStore/tables/TFOS/trainData/checkpoint.tmpb97208e81436466b80430829067e21d8; Input/output error", "body": "018-06-12 14:59:28,653 INFO (MainThread-2183) global step 75: loss = 1.4016 (7.176 sec/step)\r\n2018-06-12 14:59:35,636 INFO (MainThread-2183) global step 76: loss = 1.3862 (6.978 sec/step)\r\n2018-06-12 14:59:43,038 INFO (MainThread-2183) global step 77: loss = 3.2210 (7.398 sec/step)\r\n2018-06-12 14:59:50,353 INFO (MainThread-2183) global step 78: loss = 1.3114 (7.311 sec/step)\r\n2018-06-12 14:59:57,775 INFO (MainThread-2183) global step 79: loss = 1.3847 (7.417 sec/step)\r\n2018-06-12 15:00:05,181 INFO (MainThread-2183) global step 80: loss = 1.3856 (7.402 sec/step)\r\n2018-06-12 15:00:12,513 INFO (MainThread-2183) global step 81: loss = 1.2985 (7.327 sec/step)\r\n2018-06-12 15:00:20,160 INFO (MainThread-2183) global step 82: loss = 1.2625 (7.643 sec/step)\r\n2018-06-12 15:00:27,564 INFO (MainThread-2183) global step 83: loss = 1.2674 (7.400 sec/step)\r\n2018-06-12 15:00:32,553 INFO (Thread-4-2183) Error reported to Coordinator: /dbfs/FileStore/tables/TFOS/trainData/checkpoint.tmpb97208e81436466b80430829067e21d8; Input/output error\r\nTraceback (most recent call last):\r\n  File \"/databricks/python/lib/python3.5/site-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\r\n    yield\r\n  File \"/databricks/python/lib/python3.5/site-packages/tensorflow/python/training/coordinator.py\", line 495, in run\r\n    self.run_loop()\r\n  File \"/databricks/python/lib/python3.5/site-packages/tensorflow/python/training/supervisor.py\", line 1104, in run_loop\r\n    global_step=self._sv.global_step)\r\n  File \"/databricks/python/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1686, in save\r\n    save_relative_paths=self._save_relative_paths)\r\n  File \"/databricks/python/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1041, in _update_checkpoint_state\r\n    text_format.MessageToString(ckpt))\r\n  File \"/databricks/python/lib/python3.5/site-packages/tensorflow/python/lib/io/file_io.py\", line 431, in atomic_write_string_to_file\r\n    rename(temp_pathname, filename, overwrite)\r\n  File \"/databricks/python/lib/python3.5/site-packages/tensorflow/python/lib/io/file_io.py\", line 410, in rename\r\n    compat.as_bytes(oldname), compat.as_bytes(newname), overwrite, status)\r\n  File \"/databricks/python/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 516, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.UnknownError: /dbfs/FileStore/tables/TFOS/trainData/checkpoint.tmpb97208e81436466b80430829067e21d8; Input/output error\r\n2018-06-12 15:00:34,910 INFO (MainThread-2183) global step 84: loss = 2.6778 (7.342 sec/step)\r\n2018-06-12 15:00:34,913 INFO (MainThread-2183) Finished training! Saving model to disk.\r\n18/06/12 15:02:02 ERROR Executor: Exception in task 1.0 in stage 0.0 (TID 1)\r\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/databricks/python/lib/python3.5/site-packages/tensorflow/python/training/supervisor.py\", line 990, in managed_session\r\n    yield sess\r\n  File \"/databricks/python/lib/python3.5/site-packages/tensorflow/contrib/slim/python/slim/learning.py\", line 780, in train\r\n    sv.saver.save(sess, sv.save_path, global_step=sv.global_step)\r\n  File \"/databricks/python/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1686, in save\r\n    save_relative_paths=self._save_relative_paths)\r\n  File \"/databricks/python/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1041, in _update_checkpoint_state\r\n    text_format.MessageToString(ckpt))\r\n  File \"/databricks/python/lib/python3.5/site-packages/tensorflow/python/lib/io/file_io.py\", line 431, in atomic_write_string_to_file\r\n    rename(temp_pathname, filename, overwrite)\r\n  File \"/databricks/python/lib/python3.5/site-packages/tensorflow/python/lib/io/file_io.py\", line 410, in rename\r\n    compat.as_bytes(oldname), compat.as_bytes(newname), overwrite, status)\r\n  File \"/databricks/python/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 516, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.UnknownError: /dbfs/FileStore/tables/TFOS/trainData/checkpoint.tmp51f40c3652e54c3b93209b402acfc49a; Input/output error\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "**Have I written custom code --> No just clone it from git (tensorflow/models repository)\r\nTensorFlow installed from --> using azure dataBricks clusters (tensorflow pre-installed with latest version)\r\nTensorFlow version->1-.8\r\nCUDA/cuDNN version -->pre-installed in azure dataBricks clusters\r\nGPU model and memory ->\r\nExact command to reproduce**  ->train.py (and passing all the necessary argument)\r\n\r\n-->We are storing all the checkpoint in **dbfs** .\r\nQ) is tensorflow is having any issues while saving its checkpoint in dbfs?\r\n\r\nSome pointers  i)when we ran for only 50 steps of training, it ran successfully and finished the training without any error \r\nii)After testing with 50 odd steps when we increase the number of steps from 50 to 20k it get errored out between 80 to 90 steps (error is mentioned above)\r\n\r\n", "I don't know much about `dbfs`, but from the error message and stacktrace it seems like the program is having trouble writing the checkpoint. Is it possible that a quota or file limit is being hit? If you delete the older checkpoints, can you make more progress?", "I have tried deleting the older checkpoints but did't work.\r\n", "Sorry for the delay in responding to this. It's pretty hard to see what the problem could be without more information. Your rename is failing with an i/o error. Probably that's because e.g. there is a name clash, or as @asimshankar mentioned maybe you don't have space/quota, or...\r\n\r\n@ispirmustafa do you know anything about dbfs?", "unfortunately i don't know.", "Un-assigning myself so that it will be triaged.", "@vishal-kr-yadav could you check if transport endpoint is not connected? Probably start with test run on checkpoint to make sure all nodes are alive.", "@wt-huang yes i checked with this one also, it seems that the issue is not on the tensorflow side. As we are writing the checkpoints to dbfs, its throwing the error. We checked with other storage system and it worked fine.\r\nIf anyone of you could simply run a tensorflow object detection model and check whether you are able to write the checkpoints in dbfs or not.", "Nagging Assignee @wt-huang: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@vishal-kr-yadav Good to know that the issue is not related to TensorFlow. Is this issue still blocking you? I was able to write the checkpoints in dbfs. ", "I tried saving checkpoints on dbfs about a month ago, and I wasn't able to, its interesting that you're able to do this @wt-huang , do you know if there is a public example of this somewhere?", "@wt-huang Is it possible for us to view the notebook you used to write checkpoints to dbfs? ", "@dnwarnock As mentioned above, this issue is not TensorFlow related. There were tweaks and fiddling I had to go through to get it to work which may not be optimal. FWIW the notebook I used is one of the image detection examples.", "Closing, feel free to reopen if any additional questions."]}, {"number": 20089, "title": "Keras predict_on_batch() won't work on the first call", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: 3.6.5 |Anaconda custom (64-bit)| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)]\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: see source code\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nCalling `model.predict_on_batch(x)` the first time results in the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\user\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2963, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-2-ae1592c2ea28>\", line 22, in <module>\r\n    model.predict_on_batch(x)\r\n  File \"C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\engine\\training.py\", line 1459, in predict_on_batch\r\n    return self(inputs)  # pylint: disable=not-callable\r\n  File \"C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\engine\\base_layer.py\", line 314, in __call__\r\n    output = super(Layer, self).__call__(inputs, *args, **kwargs)\r\n  File \"C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 630, in __call__\r\n    in_deferred_mode = isinstance(input_list[0], _DeferredTensor)\r\nIndexError: list index out of range\r\n```\r\nHowever, executing the same line a second time gives the expected results.\r\n\r\n### Source code / logs\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\n\r\ntfe.enable_eager_execution()\r\n\r\nclass MyModel(tf.keras.Model):\r\n    def __init__(self):\r\n        super(MyModel, self).__init__()\r\n        self.dense1 = tf.keras.layers.Dense(10)\r\n        self.dense2 = tf.keras.layers.Dense(2, activation='softmax')\r\n\r\n    def call(self, inputs, training=None, mask=None):\r\n        results = self.dense1(inputs)\r\n        results = self.dense2(results)\r\n\r\n        return results\r\n\r\n\r\nx = tf.random_uniform((100, 10))\r\n\r\nmodel = MyModel()\r\nmodel.predict_on_batch(x)\r\n```\r\n", "comments": ["Why do you want to use predict_on_batch?\r\nDoing `model(x)` will work.", "(BTW, this has been fixed and the fix will be available in the 1.9 release. In the mean time, as @Dref360 suggested, you could use `model(x)` in 1.8)"]}, {"number": 20088, "title": "Tensorflow 1.8.0, GPU Build fails with error C2872: 'GPUDevice': ambiguous symbol", "body": "### Describe the problem\r\nTrying to compile Tensorflow 1.8.0 with GPU support using CUDA 9.0 and CUDNN 7.1 on Windows. Compilation fails.\r\n\r\n### Source code / logs\r\n```\r\nERROR: C:/l/tensorflow-gpu-base_1529265306395/work/tensorflow/contrib/fused_conv/BUILD:82:1: C++ compilation of rule '//tensorflow/contrib/fused_conv:python/ops/_fused_conv2d_bias_activation_op.so' failed (Exit 2): msvc_cl.bat failed: error executing command\r\n  cd C:/t/_bazel_nwani/0uz4ee68/execroot/org_tensorflow\r\n  SET CUDA_COMPUTE_CAPABILITIE=None\r\n    SET CUDA_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0\r\n    SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0\r\n    SET CUDNN_INSTALL_PATH=C:/Users/nwani/Downloads/cudnn-9.0-windows10-x64-v7.1/cuda\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\INCLUDE;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.10240.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\include\\um;C:\\Program Files (x86)\\Windows Kits\\8.1\\include\\\\shared;C:\\Program Files (x86)\\Windows Kits\\8.1\\include\\\\um;C:\\Program Files (x86)\\Windows Kits\\8.1\\include\\\\winrt;\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\LIB\\amd64;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\LIB\\amd64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.10240.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\lib\\um\\x64;C:\\Program Files (x86)\\Windows Kits\\8.1\\lib\\winv6.3\\um\\x64;\r\n    SET NO_WHOLE_ARCHIVE_OPTION=1\r\n    SET PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0/bin;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\amd64;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\Tools;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Team Tools\\Performance Tools\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Windows Kits\\8.1\\bin\\x64;C:\\Program Files (x86)\\Windows Kits\\8.1\\bin\\x86;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;;C:\\WINDOWS\\system32\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/l/tensorflow-gpu-base_1529265306395/_h_env/python.exe\r\n    SET PYTHON_LIB_PATH=C:/l/tensorflow-gpu-base_1529265306395/_h_env/Lib/site-packages\r\n    SET TEMP=C:\\Users\\nwani\\AppData\\Local\\Temp\r\n    SET TF_CUDA_CLANG=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=3.0,3.5,5.2,6.0,6.1,7.0\r\n    SET TF_CUDA_VERSION=9.0\r\n    SET TF_CUDNN_VERSION=7\r\n    SET TF_NEED_CUDA=1\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TMP=C:\\Users\\nwani\\AppData\\Local\\Temp\r\n  external/local_config_cc/wrapper/bin/msvc_cl.bat /c tensorflow/contrib/fused_conv/kernels/fused_conv2d_bias_activation_op.cc /Fobazel-out/x64_windows-py3-opt/bin/tensorflow/contrib/fused_conv/_objs/python/ops/_fused_conv2d_bias_activation_op.so/tensorflow/contrib/fused_conv/kernels/fused_conv2d_bias_activation_op.o /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 -Xcompilation-mode=opt -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -w /I. /Ibazel-out/x64_windows-py3-opt/genfiles /Iexternal/protobuf_archive /Ibazel-out/x64_windows-py3-opt/genfiles/external/protobuf_archive /Iexternal/bazel_tools /Ibazel-out/x64_windows-py3-opt/genfiles/external/bazel_tools /Iexternal/eigen_archive /Ibazel-out/x64_windows-py3-opt/genfiles/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-py3-opt/genfiles/external/local_config_sycl /Iexternal/local_config_cuda /Ibazel-out/x64_windows-py3-opt/genfiles/external/local_config_cuda /Iexternal/protobuf_archive/src /Ibazel-out/x64_windows-py3-opt/genfiles/external/protobuf_archive/src /Iexternal/eigen_archive /Ibazel-out/x64_windows-py3-opt/genfiles/external/eigen_archive /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-py3-opt/genfiles/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-py3-opt/genfiles/external/local_config_cuda/cuda/cuda/include /Iexternal/local_config_cuda/cuda/cuda/include/crt /Ibazel-out/x64_windows-py3-opt/genfiles/external/local_config_cuda/cuda/cuda/include/crt /DEIGEN_MPL2_ONLY /showIncludes /MD /O2 /DNDEBUG -DGOOGLE_CUDA=1 -DTENSORFLOW_MONOLITHIC_BUILD /D__VERSION__=\"MSVC\" /DPLATFORM_WINDOWS /DEIGEN_HAS_C99_MATH /DTENSORFLOW_USE_EIGEN_THREADPOOL /DEIGEN_AVOID_STL_ARRAY /Iexternal/gemmlowp /wd4018 /U_HAS_EXCEPTIONS /D_HAS_EXCEPTIONS=1 /EHsc /DNOGDI /UTF_COMPILE_LIBRARY\r\nC:\\t\\_bazel_nwani\\0uz4ee68\\execroot\\org_tensorflow\\tensorflow\\contrib\\fused_conv\\kernels\\fused_conv2d_bias_activation_op.cc(305): error C2872: 'GPUDevice': ambiguous symbol\r\n.\\tensorflow/core/kernels/conv_ops_gpu.h(181): note: could be 'Eigen::GpuDevice tensorflow::GPUDevice'\r\nC:\\t\\_bazel_nwani\\0uz4ee68\\execroot\\org_tensorflow\\tensorflow\\contrib\\fused_conv\\kernels\\fused_conv2d_bias_activation_op.cc(46): note: or       'tensorflow::`anonymous-namespace'::GPUDevice'\r\nC:\\t\\_bazel_nwani\\0uz4ee68\\execroot\\org_tensorflow\\tensorflow\\contrib\\fused_conv\\kernels\\fused_conv2d_bias_activation_op.cc(631): error C2872: 'GPUDevice': ambiguous symbol\r\n.\\tensorflow/core/kernels/conv_ops_gpu.h(181): note: could be 'Eigen::GpuDevice tensorflow::GPUDevice'\r\nC:\\t\\_bazel_nwani\\0uz4ee68\\execroot\\org_tensorflow\\tensorflow\\contrib\\fused_conv\\kernels\\fused_conv2d_bias_activation_op.cc(46): note: or       'tensorflow::`anonymous-namespace'::GPUDevice'\r\nC:\\t\\_bazel_nwani\\0uz4ee68\\execroot\\org_tensorflow\\tensorflow\\contrib\\fused_conv\\kernels\\fused_conv2d_bias_activation_op.cc(632): error C2872: 'GPUDevice': ambiguous symbol\r\n.\\tensorflow/core/kernels/conv_ops_gpu.h(181): note: could be 'Eigen::GpuDevice tensorflow::GPUDevice'\r\nC:\\t\\_bazel_nwani\\0uz4ee68\\execroot\\org_tensorflow\\tensorflow\\contrib\\fused_conv\\kernels\\fused_conv2d_bias_activation_op.cc(46): note: or       'tensorflow::`anonymous-namespace'::GPUDevice'\r\nC:\\t\\_bazel_nwani\\0uz4ee68\\execroot\\org_tensorflow\\tensorflow\\contrib\\fused_conv\\kernels\\fused_conv2d_bias_activation_op.cc(645): error C2872: 'GPUDevice': ambiguous symbol\r\n.\\tensorflow/core/kernels/conv_ops_gpu.h(181): note: could be 'Eigen::GpuDevice tensorflow::GPUDevice'\r\nC:\\t\\_bazel_nwani\\0uz4ee68\\execroot\\org_tensorflow\\tensorflow\\contrib\\fused_conv\\kernels\\fused_conv2d_bias_activation_op.cc(46): note: or       'tensorflow::`anonymous-namespace'::GPUDevice'\r\nC:\\t\\_bazel_nwani\\0uz4ee68\\execroot\\org_tensorflow\\tensorflow\\contrib\\fused_conv\\kernels\\fused_conv2d_bias_activation_op.cc(654): error C2872: 'GPUDevice': ambiguous symbol\r\n.\\tensorflow/core/kernels/conv_ops_gpu.h(181): note: could be 'Eigen::GpuDevice tensorflow::GPUDevice'\r\nC:\\t\\_bazel_nwani\\0uz4ee68\\execroot\\org_tensorflow\\tensorflow\\contrib\\fused_conv\\kernels\\fused_conv2d_bias_activation_op.cc(46): note: or       'tensorflow::`anonymous-namespace'::GPUDevice'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 1314.928s, Critical Path: 343.16s\r\nFAILED: Build did NOT complete successfully\r\n+ exit 1\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code: No\r\nOS Platform and Distribution: Windows 10 (compiling myself)\r\nTensorFlow installed from: Compiling myself, bazel, on Windows, v1.8.0\r\nTensorFlow version: v1.8.0\r\nBazel version: 1.14.0\r\nCUDA/cuDNN version: 9.0/7.1\r\nGPU model and memory: Too lazy to grab that, and also, not relevant ;-)\r\nExact command to reproduce: Follow the stuff in https://github.com/tensorflow/tensorflow/tree/v1.8.0/tensorflow/tools/ci_build/windows/gpu", "Closing as there is a newer issue that seems to point to some solutions."]}, {"number": 20087, "title": "Python double free or corruption", "body": "Hi, \r\n\r\nI try tensorflow-gpu 1.9.0rc1, but got the following error. I use ubuntu 16.04 (fresh install last night), numpy=1.13.3 (as required by 1.9.0rc1). Anyone tell me what the reason for this error? Thanks in advance.\r\n\r\nTuan \r\n\r\n`*** Error in `python': double free or corruption (out): 0x00007ff8980021b0 ***\r\n*** Error in `python': double free or corruption (out): 0x00007ff898001d20 ***\r\n======= Backtrace: =========\r\n======= Backtrace: =========\r\n/lib/x86_64-linux-gnu/libc.so.6(+0x777e5)[0x7ff9781137e5]\r\n/lib/x86_64-linux-gnu/libc.so.6(+0x8037a)[0x7ff97811c37a]\r\n/lib/x86_64-linux-gnu/libc.so.6(cfree+0x4c)[0x7ff97812053c]\r\n/home/anhnt/tf-190/venv/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN10tensorflow8EventMgr10PollEventsEbPNS_3gtl13InlinedVectorINS0_5InUseELi4EEE+0x6fb)[0x7ff943fcb14b]\r\n/home/anhnt/tf-190/venv/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN10tensorflow7GPUUtil18CopyGPUTensorToCPUEPNS_6DeviceEPKNS_13DeviceContextEPKNS_6TensorEPS6_St8functionIFvRKNS_6StatusEEE+0x3ff)[0x7ff93e95241f]\r\n/home/anhnt/tf-190/venv/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN10tensorflow16GPUDeviceContext21CopyDeviceTensorToCPUEPKNS_6TensorENS_11StringPieceEPNS_6DeviceEPS1_St8functionIFvRKNS_6StatusEEE+0x82)[0x7ff93e953a72]\r\n/home/anhnt/tf-190/venv/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so(+0x5f59ef)[0x7ff93e97e9ef]\r\n/home/anhnt/tf-190/venv/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN10tensorflow10CopyTensor6ViaDMAENS_11StringPieceEPNS_13DeviceContextES3_PNS_6DeviceES5_NS_19AllocatorAttributesES6_PKNS_6TensorEPS7_St8functionIFvRKNS_6StatusEEE+0x67a)[0x7ff93e97f53a]\r\n/home/anhnt/tf-190/venv/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN10tensorflow22IntraProcessRendezvous18SameWorkerRecvDoneERKNS_10Rendezvous9ParsedKeyERKNS1_4ArgsES7_RKNS_6TensorEPS8_St8functionIFvRKNS_6StatusEEE+0x4cc)[0x7ff93e9c7cac]\r\n/home/anhnt/tf-190/venv/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so(+0x63f2bd)[0x7ff93e9c82bd]\r\n/home/anhnt/tf-190/venv/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN10tensorflow19LocalRendezvousImpl9RecvAsyncERKNS_10Rendezvous9ParsedKeyERKNS1_4ArgsESt8functionIFvRKNS_6StatusES7_S7_RKNS_6TensorEbEE+0x961)[0x7ff93e7ebaf1]\r\n/home/anhnt/tf-190/venv/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN10tensorflow22IntraProcessRendezvous9RecvAsyncERKNS_10Rendezvous9ParsedKeyERKNS1_4ArgsESt8functionIFvRKNS_6StatusES7_S7_RKNS_6TensorEbEE+0x473)[0x7ff93e9c8903]\r\n/lib/x86_64-linux-gnu/libc.so.6(+0x777e5)[0x7ff9781137e5]\r\n/lib/x86_64-linux-gnu/libc.so.6(+0x8037a)[0x7ff97811c37a]\r\n/lib/x86_64-linux-gnu/libc.so.6(cfree+0x4c)[0x7ff97812053c]\r\n/home/anhnt/tf-190/venv/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so(+0x6036b9)[0x7ff93e98c6b9]\r\n/home/anhnt/tf-190/venv/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so(+0x606ba8)[0x7ff93e98fba8]\r\n/home/anhnt/tf-190/venv/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so(+0x60c8c3)[0x7ff93e9958c3]\r\n/home/anhnt/tf-190/venv/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so(+0x60ca2a)[0x7ff93e995a2a]\r\n/home/anhnt/tf-190/venv/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN5Eigen26NonBlockingThreadPoolTemplIN10tensorflow6thread16EigenEnvironmentEE10WorkerLoopEi+0x21a)[0x7ff93e9f3fba]\r\n/home/anhnt/tf-190/venv/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZNSt17_Function_handlerIFvvEZN10tensorflow6thread16EigenEnvironment12CreateThreadESt8functionIS0_EEUlvE_E9_M_invokeERKSt9_Any_data+0x32)[0x7ff93e9f3062]\r\n/usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0xb8c80)[0x7ff921537c80]\r\n/lib/x86_64-linux-gnu/libpthread.so.0(+0x76ba)[0x7ff97846d6ba]\r\n/lib/x86_64-linux-gnu/libc.so.6(clone+0x6d)[0x7ff9781a341d]`", "comments": ["Hi again,\r\n\r\nThis is the full stack trace.\r\n\r\nTuan\r\n[error-1.9.0rc1.txt](https://github.com/tensorflow/tensorflow/files/2109750/error-1.9.0rc1.txt)\r\n", "@anhnt1 - thanks for the report. Is it possible for you to create a [reproducible example](https://stackoverflow.com/help/mcve)?\r\n\r\nThanks", "@asimshankar - I got the error twice yesterday. But the work-around is here https://github.com/tensorflow/tensorflow/issues/6968, compile numpy from source.\r\nBut I still got segfault, another issue :)\r\nTuan\r\n", "@anhnt1 : Not sure I followed - do you have some code to share that can reproduce the problem?", "@asimshankar \r\nHi, \r\nThe code is attached, but data is quite big. I do not know how I can share it with you.\r\nTuan\r\n\r\n", "[bug.txt](https://github.com/tensorflow/tensorflow/files/2113459/bug.txt)\r\n", "@asimshankar Change the bug.txt to bug.py. I will zip data files later.\r\nTuan", "@asimshankar \r\nData files (zip version) is still too big (125M). Not sure how I can share it with you.\r\nTuan \r\n", "@asimshankar ", "@asimshankar \r\n[train-policy.txt](https://github.com/tensorflow/tensorflow/files/2113508/train-policy.txt)\r\n[test-policy.txt](https://github.com/tensorflow/tensorflow/files/2113509/test-policy.txt)\r\n\r\nkar ", "@anhnt1 : I was hoping for a much smaller, [minimal, verifiable, complete example](https://stackoverflow.com/help/mcve). Is there something you could do to isolate the issue further? \r\n", "@asimshankar : I uploaded a subset of my data. It should be enough to reproduce the error."]}, {"number": 20085, "title": "Codelab: Tensorlab for Poets not working due to incomplete documentation", "body": "on Tensor flow for poets step 3: \r\nhttps://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#3\r\n\r\nIt appears to be missing some vital steps\r\n\r\nIt says: \r\n```Start your retraining with one big command (note the --summaries_dir option, sending training progress reports to the directory that tensorboard is monitoring) :\r\n\r\npython -m scripts.retrain \\\r\n  --bottleneck_dir=tf_files/bottlenecks \\\r\n  --how_many_training_steps=500 \\\r\n  --model_dir=tf_files/models/ \\\r\n  --summaries_dir=tf_files/training_summaries/\"${ARCHITECTURE}\" \\\r\n  --output_graph=tf_files/retrained_graph.pb \\\r\n  --output_labels=tf_files/retrained_labels.txt \\\r\n  --architecture=\"${ARCHITECTURE}\" \\\r\n  --image_dir=tf_files/flower_photos```\r\nThis fails with the following errror: \r\n```\r\n/Users/grantkemp/tensorflow/bin/python: Error while finding module specification for 'scripts.retrain' (ModuleNotFoundError: No module named 'scripts')\r\n```\r\n\r\n**Solution  ( It should say!)** \r\n\r\nIn order to continue - you need to cd ..\r\n till you get to the folder with scripts in it before continuing\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have you tried the solution printed in the error trace you quote above?\r\nSolution \r\nIn order to continue - you nee to `cd ..` till you get to the folder with scripts in it before continuing", "Hi @karmel  yes it did work for me - which is why I recommended that someone update the code lab to make sure others aren't confused. ", "@lamberta - is the TensorFlow for Poets codelab something we produce? Perhaps a clarifying clause might be helpful to indicate to readers that they should be in the directory containing the script.", "Nagging Assignees @lamberta, @MarkDaoust: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @lamberta, @MarkDaoust: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I think this is fixed. In the current version there are no steps suggesting that the user should cd out of the repo root. ", "> I think this is fixed. In the current version there are no steps suggesting that the user should cd out of the repo root.\r\n\r\nNo steps suggesting the user should cd out of the repo is the reason this is NOT fixed."]}, {"number": 20084, "title": "Neural Machine Translation with Attention", "body": "Added Neural Machine Translation with Attention notebook", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it", "CLAs look good, thanks!\n\n<!-- ok -->", "@alextp "]}, {"number": 20083, "title": "Adding NMT with Attention notebook", "body": "Adding Neural Machine Translation with Attention notebook", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->"]}, {"number": 20082, "title": "Fix typo", "body": "fix typo\r\n\r\n```within in``` -> ```whitin```", "comments": []}, {"number": 20081, "title": "ghfcxzcgvhbjkojhghjuhbgbhgb       Zxcvbn./ ", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 20080, "title": "TF1.9rc missing headers", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.9.0-rc0-35-g17d6639b55 1.9.0-rc1\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\n```\r\nHOROVOD_WITH_TENSORFLOW=1 HOROVOD_CUDA_HOME=/xx/cuda/9.0/ HOROVOD_NCCL_HOME=/xx/NCCL/2.2.13/ HOROVOD_GPU_ALLREDUCE=NCCL pip install --no-cache-dir horovod --user -U\r\n```\r\n\r\nLogs:\r\n```\r\n In file included from /xx/.local/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/dnn.h:33:0,\r\n                     from /xx/.local/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/stream.h:30,\r\n                     from horovod/tensorflow/mpi_ops.cc:29:\r\n    /xx/.local/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor.h:21:46: fatal error: tensorflow/compiler/xla/statusor.h: No such file or di\r\nrectory\r\n    compilation terminated.\r\n```\r\n\r\nLooks like some necessary headers are missing in this release.", "comments": ["duplicate of #18841"]}, {"number": 20079, "title": "fix a mistake in the doc str of class GradientTape", "body": "There is a mistake in the original help (or doc) string under class GradientTape in version 1.8.0,  The expected value for two of the example output is interchanged, which could lead to user misunderstanding if not fixed. ", "comments": []}, {"number": 20078, "title": "Fixes the bug in model.save", "body": "Fixes the bug where if you get the h5py error, and install it in Jupyter, you still can't use save because keras never tries to reimport.", "comments": ["Good call, maybe the try block should be kept where it was and just\nduplicated inside save.\n\nOn Sat, Jun 16, 2018, 2:24 PM Shanqing Cai <notifications@github.com> wrote:\n\n> *@caisq* commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/python/keras/engine/saving.py\n> <https://github.com/tensorflow/tensorflow/pull/20078#discussion_r195909118>\n> :\n>\n> > @@ -73,8 +66,11 @@ def save_model(model, filepath, overwrite=True, include_optimizer=True):\n>    Raises:\n>        ImportError: if h5py is not available.\n>    \"\"\"\n> -\n> -  if h5py is None:\n> +  # pylint: disable=g-import-not-at-top\n> +  try:\n> +    import h5py\n> +    HDF5_OBJECT_HEADER_LIMIT = 64512\n>\n> HDF5_OBJECT_HEADER_LIMIT used to be a module-wide global variable. Now it\n> is private to this function. Won't this cause problems for functions that\n> use it?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/20078#pullrequestreview-129369649>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AA67DrayjjNlMjtlohUCfpl0RVsMd6g2ks5t9U1qgaJpZM4Uqj-m>\n> .\n>\n", "@aylusltd Or maybe just move `HDF5_OBJECT_HEADER_LIMIT` out from this function to the module level.", "@caisq thanks for that, sorry for the sloppy commit. Should have seen that `HDF5_OBJECT_HEADER_LIMIT` was used in `save_attributes_to_hdf5_group`. Just wasn't paying attention. ", "LGTM. Let us know what you think, @fchollet ", "Nagging Reviewer @fchollet: It has been 15 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer%(reviewers): You have been added as a reviewer to this pull request. Please add your review or reassign. It has been  30ays with no activity and the `awaiting review` label has been applied.", "Nagging Reviewer @fchollet: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 45 days with no activity and the `awaiting review` label has been applied.", "Gentle ping @fchollet . Let us know whether you think the approach here is okay.", "> Fixes the bug where if you get the h5py error, and install it in Jupyter, you still can't use save because keras never tries to reimport.\r\n\r\nDoesn't sound like a bug, you just need to `reload(keras)`. The current approach (try/except import at the top) seems correct to me and I don't think we should do a dynamic import every time we need h5py like is done in this PR."]}]