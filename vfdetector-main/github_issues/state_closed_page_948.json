[{"number": 24993, "title": "ERROR: The 'test' command is only supported from within a workspace.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution: Ubuntu 16.04.5 LTS\r\n- TensorFlow installed from: Docker image `tensorflow/tensorflow:nightly-devel-gpu`\r\n- TensorFlow version: `master` branch\r\n- Python version: 2 and 3\r\n- Installed using: Docker\r\n\r\n\r\n**Describe the problem**\r\n\r\nI set up an Azure virtual machine with `Data Science Virtual Machine` on a `Standard NV6 (6 vcpus, 56 GB memory)`. I ssh into it, run these commands, and get the error in the title.\r\n\r\n```\r\nmiguelmorin@ds-docker-gpu:~$ sudo docker pull tensorflow/tensorflow:nightly-devel-gpu\r\n...\r\nmiguelmorin@ds-docker-gpu:~$ sudo docker images\r\nREPOSITORY              TAG                 IMAGE ID            CREATED             SIZE\r\ntensorflow/tensorflow   nightly-devel-gpu   abbaea1b533c        6 weeks ago         3.82GB\r\nmiguelmorin@ds-docker-gpu:~$ sudo docker run -it abbaea1b533c\r\nroot@fa1e7cf2c349:~# export LD_LIBRARY_PATH=\"${LD_LIBRARY_PATH}:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH\"\r\nroot@fa1e7cf2c349:~# export flags=\"--config=opt --config=cuda -k\"\r\nroot@fa1e7cf2c349:~# bazel test ${flags} //tensorflow/python/...\r\nExtracting Bazel installation...\r\nERROR: The 'test' command is only supported from within a workspace.\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\n```\r\n\r\nIt also fails running the container with `nvidia-docker` instead of `docker`:\r\n\r\n```\r\nmiguelmorin@ds-docker-gpu:~$ sudo nvidia-docker run -it abbaea1b533c\r\nroot@148ea4778aab:~# export LD_LIBRARY_PATH=\"${LD_LIBRARY_PATH}:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH\"\r\nroot@148ea4778aab:~# \r\nroot@148ea4778aab:~# export flags=\"--config=opt --config=cuda -k\"\r\nroot@148ea4778aab:~# bazel test ${flags} //tensorflow/python/...\r\nExtracting Bazel installation...\r\nERROR: The 'test' command is only supported from within a workspace.\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\n```", "comments": ["You need to change to the correct directory. As [the TF Docker Hub](https://hub.docker.com/r/tensorflow/tensorflow/) explains, TensorFlow's source code is in /tensorflow_src."]}, {"number": 24992, "title": "TF Keras sequence_test additional test cases covers", "body": "1- remove_long_seq api test case added", "comments": ["@pavithrasv  \r\n\r\nPlease help to review the test case", "@pavithrasv  Could you PTAL and approve or suggest changes(if required)."]}, {"number": 24991, "title": "Non-utilisation of available cores on multi-core Intel CPU for Intel Optimized TF build", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):     Yes (custom code)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary( Intel Optimized TF)\r\n- TensorFlow version (use command below): 1.12\r\n- Python version: 3.6.5\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A(no GPU) but Intel i5 7200U @ 2.50GHz * 4  with 7 GB memory and 64-bit OS\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nThe load does not get shared by all the available cores(mostly one core at a time), as a result the performance is poor.\r\n\r\n**Describe the expected behavior**\r\nExpect the load to be shared by all the available cores and the performance to scale up with increasing number of cores.\r\n\r\n**Code to reproduce the issue**\r\nFollow this link and run the scripts as mentioned: https://github.com/mystic123/tensorflow-yolo-v3https://github.com/mystic123/tensorflow-yolo-v3\r\n\r\n**Other info / logs**\r\nI have used  KMP, OMP and ConfigProto settings and nothing seems to improve the performance.\r\n", "comments": ["I don't think this is distribution strategy related, maybe someone from performance team could take a look? @azaks2 perhaps? ", "Pinging @TensorFlow-MKL ", "@psgithubuser I cannot access the github repo you've linked for the code. Are the inter/intra op settings the ones you set in ConfigProto or something else? Also, did you see the CPU perf section here: https://www.tensorflow.org/guide/performance/overview     ?\r\n", "oh, i see the link is messed up. The repo is [here](https://github.com/mystic123/tensorflow-yolo-v3). If you've set up your runtime settings according to the tensorflow performance page, please give me more info on your env (for ex, \"conda list\" if you ware using conda) and installation routine. ", "@Nathan Greeneltch\nThank you for replying. Yes, I have set the runtime settings according to\nthe tensorflow performance page. Please find the packages list(using conda\nlist) attached. I installed the Intel-Optimization for Tensorflow using:\nconda install tensorflow\n\nRegards,\nParth Sinha\n\nOn Fri, Jan 25, 2019 at 4:00 AM Nathan Greeneltch <notifications@github.com>\nwrote:\n\n> oh, i see the link is messed up. The repo is here\n> <https://github.com/mystic123/tensorflow-yolo-v3>. If you've set up your\n> runtime settings according to the tensorflow performance page, please give\n> me more info on your env (for ex, \"conda list\" if you ware using conda) and\n> installation routine.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/24991#issuecomment-457382101>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AsXXHoGBRp8bJbmZRxh4uvihHdpKnLXeks5vGjP1gaJpZM4aE-wP>\n> .\n>\n\n# packages in environment at /home/ignitarium/anaconda3/envs/testtf:\n#\nabsl-py                   0.6.1                    py36_0  \nastor                     0.7.1                    py36_0  \nblas                      1.0                         mkl  \nbzip2                     1.0.6                h14c3975_5  \nc-ares                    1.15.0               h7b6447c_1  \ncairo                     1.14.12              h8948797_3  \ncython                    0.29.2           py36he6710b0_0  \nffmpeg                    4.0                  hcdf2ecd_0  \nfontconfig                2.13.0               h9420a91_0  \nfreeglut                  3.0.0                hf484d3e_5  \nfreetype                  2.9.1                h8a8886c_1  \ngast                      0.2.0                    py36_0  \nglib                      2.56.2               hd408876_0  \ngraphite2                 1.3.13               h23475e2_0  \ngrpcio                    1.14.1           py36h9ba97e2_0  \nh5py                      2.8.0            py36h61e79e4_3  \nharfbuzz                  1.8.8                hffaf4a1_0  \nhdf5                      1.8.20               hba1933b_1  \nicu                       58.2                 h9c2bf20_1  \nintel-openmp              2019.1                intel_144    intel\nintelpython               2019.1                        0    intel\njasper                    2.0.14               h07fcdf6_1  \njpeg                      9b                   h024ee3a_2  \nkeras-applications        1.0.6                    py36_0  \nkeras-preprocessing       1.0.5                    py36_0  \nlibedit                   3.1.20170329         h6b74fdf_2  \nlibffi                    3.2.1                hd88cf55_4  \nlibgcc-ng                 8.2.0                hdf63c60_1  \nlibgfortran-ng            7.3.0                hdf63c60_0  \nlibglu                    9.0.0                hf484d3e_1  \nlibopencv                 3.4.2                h765d7f9_1  \nlibopus                   1.3                  h7b6447c_0  \nlibpng                    1.6.35               hbc83047_0  \nlibprotobuf               3.6.1                hd408876_0  \nlibstdcxx-ng              8.2.0                hdf63c60_1  \nlibtiff                   4.0.9                he85c1e1_1  \nlibuuid                   1.0.3                h1bed415_2  \nlibvpx                    1.7.0                h439df22_0  \nlibxcb                    1.13                 h1bed415_1  \nlibxml2                   2.9.8                h26e45fe_1  \nmarkdown                  3.0.1                    py36_0  \nmkl                       2019.1                intel_144    intel\nncurses                   6.1                  he6710b0_1  \nnumpy                     1.14.2           py36hdbf6ddf_0  \nolefile                   0.46                     py36_0  \nopencv                    3.4.2            py36h40b0b35_1  \nopenssl                   1.0.2p                        0    intel\npcre                      8.42                 h439df22_0  \npillow                    5.3.0            py36h34e0f95_0  \npip                       9.0.3                    py36_1    intel\npixman                    0.34.0               hceecf20_3  \nprotobuf                  3.6.1            py36he6710b0_0  \npy-opencv                 3.4.2            py36h765d7f9_1  \npython                    3.6.5                hc3d631a_2  \nreadline                  7.0                  h7b6447c_5  \nscipy                     1.1.0            py36h7c811a0_2  \nsetuptools                27.2.0                   py36_0  \nsix                       1.11.0                   py36_3    intel\nsqlite                    3.26.0               h7b6447c_0  \ntbb                       2019.3                  intel_0    intel\ntensorboard               1.12.2           py36he6710b0_0  \ntensorflow                1.12.0                   py36_0    intel\ntermcolor                 1.1.0                    py36_1  \ntk                        8.6.8                hbc83047_0  \nwerkzeug                  0.14.1                   py36_0    intel\nwheel                     0.31.0                   py36_3    intel\nxz                        5.2.4                h14c3975_4  \nzlib                      1.2.11                        5    intel\n", "@psgithubuser we've seen similar issues on test code using dummy date when the size of the data was very small. MKL never launches the extra threads because the problem size is too small. If this is how you are running, try making the data size larger. Otherwise, please provide me more information on the type of data you are passing. (COCO? if so, how many images). At that point I can try to reproduce. also provide me your settings for OMP_NUM_THREADS, intra/inter_op_threads, and KMP blockime/affinity so I can try the exact same settings.", "Thank you for your suggestion Nathan. I was passing one 4k image and the performance was poor so did not increase the workload. I will try it with an array of images or video frames. We tried many settings but I am mainly using this setting: OMP_NUM_THREADS=4, intra_op=4, inter_op=8, KMP blocktime =1, KMP affinity=verbose", "Is there any more followup needed on this issue? "]}, {"number": 24990, "title": "How to generate .pbtxt with .cc REGISTER_OP file", "body": "I want to understand exactly  how user_ops generated,  and I found .cc .h op file \r\ncan generated by .pbtxt ,but I'm not konw how to generate .pbtxt with .cc \r\nREGISTER_OP file .", "comments": ["Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support. Github is mainly for addressing bugs in installation and performance. Thanks!", "I am closing the issue but please post this kind of support questions at Stackoverflow. Thanks!"]}, {"number": 24989, "title": "TF Keras image_test additional test cases covers", "body": "1- array_to_img invlaid usage test case\r\n2- img_transforms api test case", "comments": ["@fchollet \r\n\r\nPlease help to review the test case", "Can one of the admins verify this patch?", "@Dayananda-V  can you please check build failures ?", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 24988, "title": "how to use tensorflow read a image from remote hdfs cluster", "body": "I need to read the image dataset directly from HDFS and use the tensorflow related API for preprocessing, but according to the official website's tensorflow and HDFS API, I did not get a specific answer. So can explain, for example: how to use the tensorflow related API to read a picture from HDFS and open it with other libraries.", "comments": ["@lighTQ Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support. Github is mainly for addressing bugs in installation and performance. Thanks!", "I am closing the issue. Please post support related questions in StackOverflow. We encourage users to submit an issue/feature request here. Thanks!"]}, {"number": 24987, "title": "Running in google colab and got the errror:  'tensorflow.python.framework.ops.EagerTensor' has no len()", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: '1.12.0'\r\n- Doc Link: https://github.com/tensorflow/docs/blob/master/site/en/tutorials/eager/custom_training_walkthrough.ipynb\r\n\r\n**Describe the documentation issue**\r\nBy running on google CoLab this notebook ( https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/eager/custom_training_walkthrough.ipynb#scrollTo=iDuG94H-C122 ), I have received the following error\r\n\r\n![image](https://user-images.githubusercontent.com/19335547/51309762-17445100-1a80-11e9-8199-195b3d460e2f.png)\r\n\r\nIf one converts the tensors to numpy, it works normally.\r\n\r\n```\r\nplt.scatter(features['petal_length'].numpy(),\r\n            features['sepal_length'].numpy(),\r\n            c=labels.numpy(),\r\n            cmap='viridis')\r\n\r\nplt.xlabel(\"Petal length\")\r\nplt.ylabel(\"Sepal length\");\r\n```\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n", "comments": ["Merged to master Tensorflow/docs, pull request number 301\r\n\r\n\"Merge pull request # 301 from testingcompa:master\""]}, {"number": 24986, "title": "TFLite GPU Delegate error Initializing", "body": "Dear Tensorflow developers,\r\n\r\nAfter trying out the gpu-delegate demo on Android, I reimplemented in my app to try it out on my own model.\r\n\r\nMy model is the stripped ssd as the official tflite file did.\r\nAt the interpreter initialization stage, the gpu delegate seems not working.\r\nOn logcat I saw : `Failed to apply delegate: GpuDelegate Prepare: Node is already a consumer of the valueNode number 77 (GpuDelegate) failed to prepare.`\r\n\r\nBut it worked fine on the official mobilenet-ssd model.\r\n\r\nWhat does this error message suppose to mean?\r\nOr is there anything I need to be careful on TFLite converting?\r\nAnything would be helpful.\r\nThanks!", "comments": ["Thanks for trying out the GPU delegate.\r\n\r\nThe GPU delegate re-interprets the TFLite model in the form that is suitable for GPU and does some additional check.  During this check, it must have determined that a consumer-producer pair is present twice.  Probably you have a use case we haven't seen before.  Would it be possible for us to take a look at your model?", "@impjdi \r\nThanks for your reply!\r\nI'll need to have my superior's consent to show you the model.\r\n\r\nBut I have few more questions...\r\n1. In GPU delegate initiation process, what did the interpreter actually do?\r\n    Is there another shader program running behind the app?\r\n    My app's pipeline have its own shader code, changing into proper initialized tflite code will get it hanged.\r\n2. If so, is there any possible solution to avoid these GL conflicts?\r\n\r\nThanks in advance!", "@lcycoding \r\nIf network weights etc. are of your concern, you can also send me the network architecture pre-training, if that makes the decision easier :)\r\n\r\n1. This is an internal that may change, but roughly, the following happens:\r\n- When you call NewGpuDelegate(), a very simple bookkeeping object is created.\r\n- Then when you call interpreter->ModifyGraphWithDelegate, the GPU backend inspects your TFLite graph def and tells the TFLite framework which ops it can handle and which it can't.  Based on this, TFLite CPU and GPU communicate which parts should be handled by the CPU and which parts by the GPU.\r\n- Then, the GPU backend reshapes the subgraph that it is supposed to handle to a more GPU-friendly form with some optimizations.  After this restructuring, we decide which shader programs need to be prepared and compiled.  When all of these are compiled, the GPU backend is ready.\r\n- Of course, I understand that you are using the Java interface (based on the log you pasted), but these C++ functions are called through JNI.\r\n\r\n2. It is okay for you to run other shader code.  What you need to be aware is to keep the GL context consistent.  I will help you if you run into other GL related conflicts, but let's resolve the network issue first :)", "@impjdi \r\nHere I come again.\r\n\r\nWe decided to solve our pipeline's issue first since our model might still be changing.\r\n\r\nSo here's the condition now...\r\nOur pipeline captures frame with GL compute shader and do lots of preprocessing.\r\nIn origin pipeline, we pulled out the bitmap from GPU and transform into bytebuffer then do the inference things with quantized tflite model.\r\n\r\nLast week, I changed the inference part with GPU delegate mode with properly initialized model ( here I have tried official mobilenet ssd and mobilenet classifier on gpu tutorial page)\r\nThe initialize part works great, doesn't come up with any error code.\r\nBut right after the program run into `interpreter.runMultipleInputsOutputs` part, my thread stucked...\r\nI looked into Trace these days, it just stucked at `Tensor<init>` method.\r\n\r\nNote: Here I didn't use any input/output buffer techniques in advanced GPU tutorial.\r\n\r\nDoes gpu-experimental have any debug mode to figure out why my inference thread stucked?\r\n\r\nReally appreciated for your help. \ud83d\udc4d ", "@lcycoding \r\n\r\nSorry for the late reply.  Somehow didn't get the email from github directly. :(\r\n\r\n> We decided to solve our pipeline's issue first since our model might still be changing.\r\n\r\nSounds good to me.  Please let us know if you run into the issue again.  It could be a toco issue or GPU backend issue, so anything that will let us reproduce the issue would be great.\r\n\r\n> But right after the program run into interpreter.runMultipleInputsOutputs part, my thread stucked...\r\n\r\nI have to admit I'm not super familiar with TFLite's Java interfaces, but looks like `interpreter.runForMultipleInputsOutputs` does a lot of things.  For starters, can you comment out the input / output tensor `ByteBuffer` thing for now, i.e. leave input tensors uninitialized, and just call `interpreter.run` instead of `interpreter.runForMultipleInputsOutputs`?  If that works, you know the GPU backend is working fine, but it's somewhere in the method that is waiting for additional action to happen.", "@impjdi \r\nThanks for your response! \ud83d\udcaf \r\n\r\nI just tried the native inference way. Here I use mobilenet v1 1.0 224 model.\r\n`interpreter.run(img, new float[1][1001]);`\r\n\r\nThe thread hanged again :(\r\n\r\nIs there anything I can do after this test?\r\n\r\n", "@lcycoding \r\nA couple of things to check:\r\n1. Is the model the float model or the quantized model?  GPU backend only supports float.\r\n2. Is the input tensor in float or in uint8?  In other words, if you have the input tensor of size [1, 224, 224, 3], are you feeding 1x224x224x3 bytes or 1x224x224x3xsizeof(float) bytes?", "@impjdi \r\nHi, I've check the mentioned items.\r\n\r\n1. The model is float model.\r\n2. Input tensor is float32 with 224x224x3x4 bytes. I've tried changing into 224x224x3, it immediately popped up android runtime error.", "@lcycoding \r\n\r\nThanks for checking!\r\n\r\n1. This was expected, because we would have blown up if non-float model.\r\n2. I was hoping that it would be this issue, but looks like that's not the case.  Let me get back to you tomorrow probably with an example for sanity check.\r\n\r\nOut of curiosity, what phone are you using?", "@impjdi \r\nThanks for your promptly reply!\r\n\r\nI've been testing these codes on Mi A2 and Redmi note3.\r\n\r\nI think I'll try other compatible phones out today, and also iOS devices \ud83d\udc4d \r\n\r\nSee you tomorrow, have a good day!", "@lcycoding \r\n\r\nAccording to a quick web search, Mi A2 and Redmi Note 3, they seem to have Adreno 512 and Adreno 510, respectively.  And according to these:\r\n\r\nhttps://www.notebookcheck.net/Qualcomm-Adreno-510.169537.0.html\r\nhttps://www.notebookcheck.net/Qualcomm-Adreno-512-GPU.299840.0.html\r\n\r\nit looks like they both support OpenGL ES 3.1.  So theoretically, they should work.  We don't have any Adreno 510 & 512 devices around, but I'll to find something close and report back.", "Hi @impjdi,\r\nAbout the hanging issue, I still have no clue where I can do the test.\r\nIt just hanged in my app's pipeline, any other demo apps works fine :(\r\n\r\nBack to the initialization issue, I noticed that we might have a special use case in network.\r\nSorry, I still can't let you access our current model.\r\nBut please check the following structure code.\r\n```\r\ndef resize(x):\r\n  output_shape = [dim * multiple for dim, multiple in zip(x.shape,\r\n                                                          [1, 2, 2, 1])]\r\n  output = tf.concat([x, x], axis=3)     # input1 same as input2\r\n  output = tf.concat([output, output], axis=2)     # input1 same as input2\r\n  output = tf.reshape(\r\n      output, (tf.shape(x) * [1, 2, 2, 1] if not x.shape.is_fully_defined() else\r\n               output_shape))\r\n  output.set_shape(output_shape)\r\n  return output\r\n\r\nnet = conv2d(feature_map_x, z, 1)\r\nnet = resize(net)\r\nnet = net + feature_map_y\r\nnet_egl = resize(net)\r\nnet_egl = net_egl + feature_map_z\r\n```\r\nBasically replace resize_bilinear op to tf.concat steps since tflite doesn't support resize_bilinear\r\nThis is a work around we made few months ago. \r\nThe network works in normal (non-gpu delegate) mode and quantized mode.\r\nI would say this might be an issue cause we ran into trouble when converting this graph into coreML model. We made another work around for coreML model though.\r\nBut I'm not fully understand the process in conversion steps to GPU-friendly model, so I can do nothing :(\r\n\r\n\r\nIf the above information is useful, please let me know \ud83d\udc4d  \r\n", "@lcycoding \r\n\r\nre: hanging: Yeah, I've been thinking what would be a barebones binary / source that will allow you to run.  It's not easy to send a binary over from my side... I might have some instructions soon.\r\n\r\nre: initialization: I perfectly understand that you cannot disclose your network.  No worries.  The code looks alright to me.  Can you specify the shape / dimensions of everything?  TFLite and TFLite GPU both support resize bilinear.", "@impjdi \r\nre: initialization:\r\nThe whole network's input shape is simple [1,256,256,3]\r\nThe shape of these resize blocks are just like making [1, 32, 32, 128] -> [1, 32, 32, 256] -> [1, 32, 64, 256] then reshape into [1, 64, 64, 128]\r\nIs there any information I could provide?\r\n\r\niOS debug console\r\n```\r\nWARNING: 25 cannot be handled by this delegate.  Only the first 78 ops will run on the GPU, and the remaining 1 on the CPU.\r\nNode is already a consumer of the value\r\nNode number 79 (MetalGpuDelegate) failed to prepare.\r\n\r\nNode number 79 (MetalGpuDelegate) failed to prepare.\r\n```\r\n\r\nReally appreciated for your help!", "@impjdi \r\nAfter we resolved the initialization issue, I tried the same way to cut our model into pieces to test if there are certain Ops getting hanged.\r\n\r\nHere are some results and assumptions.\r\n`Conv2D`, `DepthwiseConv2D` are the most important computing node in deep learning models, and also the place hanging issue happen.\r\nIf I cut the model into only `tf.tile` or `tf.concat` part, the pipeline wouldn't get stuck. \r\nSo I guess it would only hang on Conv-like operation on GPU.\r\n\r\nNote: I've also tested on more devices, such as LG K10 and Shield TV. All of them were dead on this too :(", "@lcycoding \r\n\r\nThanks for further bisecting it to find out the issue.  From what you describe, I suspect (and I might be completely wrong here) that we're trying to use a bigger workgroup size.  Unfortunately, the Java APIs do not print thing happening in the lower level.  Let me try something...", "To all developers wants to migrate gpu delegate into your app:\r\n1. Make sure single operation does not have repeated input source.\r\n2. Make sure your init thread and inference thread are the same :P\r\n\r\nThanks to @impjdi for excellent answers!", "Here are some results and assumptions.\r\nConv2D, DepthwiseConv2D are the most important computing node in deep learning models, and also the place hanging issue happen.\r\n\r\nAlso have this problem for this command's @lcycoding could you share you solution please?", "@aleksandrovych \r\nThe hanging issue happens because of the inconsistency of the GL Thread.\r\nJust make the initialize process and inference process working on the same thread will solve the problem.", "thanks) @lcycoding ", "Hi @lcycoding ! Is there any tutorial or example code on how to initialize and infer in the same thread? Thank you in advance", "@tgpsantos \r\nNormal code structure will run on the same thread, just try it on your own.\r\nOr you could check the sample code in tensorflow/lite folder", "I tried to apply it to my own model following  I did try it by following the [tutorial provided by tensorflow](https://www.tensorflow.org/install/gpu) and I've got the following error.\r\n\r\n`java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: TfLiteGpuDelegate Invoke: Delegate should run on the same thread where it was initialized.Node number 237 (TfLiteGpuDelegate) failed to invoke.\r\n`\r\n\r\nIn [this guide](https://www.youtube.com/watch?v=Xkhgre8r5G0) it's also not mentioned how to work with the threads. I am new to Android and I never worked with threads, when I searched for this problem I actually ran into this issue and saw you mentioning the init and running thread being the same. I was hoping this could actually be an \"easy\" implementation ", "@tgpsantos \r\nEven with this whole package of demo app, it still run into this error?\r\nIf the project below does not show error, try to implement it in your way.\r\nTo work on threads, that's kind like java issue, not tensorflow-related.\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo", "@lcycoding @impjdi Why is repeated input source not allowed? Is there any good reason to not support it?", "@yxchng I forgot the details, but it's possible that it's not allowed, due to OpenGL restrictions.", "@impjdi Ok. Another question. In this post, you said resize_bilinear is supported. I just tested. It gives the correct output when align_corners=false but gives blank output when align_corners=true. Is this behavior expected?", "@yxchng Well, you can find out what's going on by reading the code.  `//tf/lite/delegates/gpu/gl/kernels/upsampling_bilinear.cc` apparently does not take `align_corners` into consideration which is a bug.  I would file a separate issue rather than piggybacking on this thread.", "> To all developers wants to migrate gpu delegate into your app:\r\n> \r\n> 1. Make sure single operation does not have repeated input source.\r\n> 2. Make sure your init thread and inference thread are the same :P\r\n> \r\n> Thanks to @impjdi for excellent answers!\r\n\r\n@lcycoding : Can you explain, what you mean by repeated input source? I think I have used my input source `r1` which is an **input layer**, at two places, and whilst the code runs great on the python interpreter, it fails on android saying ` Unexpected failure when preparing tensor allocations: Next operations are not supported by GPU delegate:` giving a list of unsupported operations such as `L2_NORMALIZATION, MEAN, CONV2D` ,etc.", "@sirius0503 \r\nHi there, the error message is not the same as the condition I mentioned here, I think you might have used some incompatible nodes in your graph.\r\nYou might want to check the whole graph with this tutorial https://www.tensorflow.org/lite/performance/gpu_advanced#supported_ops\r\n\r\nNote that gpu delegate is more complicated than the normal graph working on python, if the implementation is not done yet, you'll not be able to run the graph in proper mode. ", "@lcycoding :  My full error log is as follows: \r\n```\r\njava.lang.IllegalStateException: Internal error: Unexpected failure when preparing tensor allocations: Next operations are not supported by GPU delegate:\r\n2020-02-25 15:15:57.160 22863-22863/com.valuepitch.intruderdetector W/System.err: CONV_2D: Expected 1 input tensor(s), but node has 2 runtime input(s).\r\n2020-02-25 15:15:57.161 22863-22863/com.valuepitch.intruderdetector W/System.err: CONV_2D: Expected 1 input tensor(s), but node has 3 runtime input(s).\r\n2020-02-25 15:15:57.161 22863-22863/com.valuepitch.intruderdetector W/System.err: L2_NORMALIZATION: Operation is not supported.\r\n2020-02-25 15:15:57.161 22863-22863/com.valuepitch.intruderdetector W/System.err: MEAN: Expected 1 input tensor(s), but node has 2 runtime input(s).\r\n2020-02-25 15:15:57.161 22863-22863/com.valuepitch.intruderdetector W/System.err: SUM: Operation is not supported.\r\n2020-02-25 15:15:57.162 22863-22863/com.valuepitch.intruderdetector W/System.err: TOPK_V2: Operation is not supported.\r\n2020-02-25 15:15:57.162 22863-22863/com.valuepitch.intruderdetector W/System.err: First 0 operations will run on the GPU, and the remaining 186 on th\r\n```\r\nas I see in the link to supported ops for tensorflow gpu, CONV2D is supported, so the other **L2_Normalization,  MEAN, SUM and TOPK_V2 ** are not supported by tensorflow gpu delegate? Also, is **SUM, L2_Normalization, and TOPK_V2** not supported in tflite in general?\r\n\r\nAlso in front of `CONV2D` it's written `v1`, what does that mean? Do I have to use **tensorflow 1.x** and not **tensorflow 2.x**", "@sirius0503 \r\nThe reason why CONV2D is not supported is because it has more than 1 input tensor.\r\nOther tensors are simply not supported yet.", "2021 now , tflite gpu delegate support sum op?"]}, {"number": 24985, "title": "how to flatten a tensor in block-wise", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version : 1.12\r\n- Are you willing to contribute it : Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nA is a tensor with shape `[n,h,w,block_size*block_size]`. I hope to flatten it into `[n,h*block_size,w*block_size,1]`. In Numpy, this can be implemented by:\r\n```python\r\nimport numpy as np\r\nA = np.ones((1,2,2,4))\r\nA_flat= np.zeros((1,4,4,1))\r\nblock_size = 2\r\nfor n in range(A.shape[0]):\r\n    for h in range(A.shape[1]):\r\n        for w in range(A.shape[2]):\r\n            A_flat[n:n+1, h*block_size:(h+1)*block_size, w*block_size:(w+1)*block_size, 0:1] \\\r\n                = A[n:n+1, h:h+1, w:w+1, :].reshape(1, block_size, block_size, 1)\r\n```\r\nHow can i do it using tensorflow if these functions exist?\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n**Who will benefit with this feature?**\r\nSome researchers of image processing\r\n**Any Other info.**\r\n", "comments": ["`tf.nn.depth_to_space` does what you want.", "@ppwwyyxx Thank you!"]}, {"number": 24984, "title": "Unable to install tensorflow 2.0 nightly for python 3.4", "body": "OS version: Ubuntu 14.04\r\nWhen I run I get the below output\r\n pip install tf-nightly-2.0-preview\r\nCollecting tf-nightly-2.0-preview\r\n  Could not find a version that satisfies the requirement tf-nightly-2.0-preview (from versions: )\r\nNo matching distribution found for tf-nightly-2.0-preview\r\n", "comments": ["you can use python 3.6\r\n\r\n3.4 is outdated and deprecated version", "@dibya001  Did you try pip install tf-nightly? Thanks!", "@dibya001 Could you try the [solution](https://github.com/tensorflow/tensorflow/issues/24886) provided here. Please let us know how it progresses. Thanks!", "I think it was resolved. I am closing the issue. Please open new ticket if you see similar issue again. Thanks!"]}, {"number": 24983, "title": "TF Keras vis_utils_test introducing test cases", "body": "Test case introducing for vis_utils.py file under keras package.", "comments": ["@tanzhenyu \r\n\r\nPlease help to review the test case", "Nagging Reviewer @tanzhenyu: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "@Dayananda-V please check failed tests", "This is been failing unit test -- can you investigate?", "@tanzhenyu \r\n\r\nfixed compilation problem and please review once again, TIA!......"]}, {"number": 24982, "title": "Enable reference code of kernels(internal) in TF-Lite", "body": "Hi,\r\nI want to benchmark the model based on reference code and Neon optimized code for kernels.So that I  want to enable the reference code implementation while executing the Tensorflow Lite model.\r\n\r\nIs there is any way to run reference code ?\r\nI tried to make changes in build file but there is some compilation issues.\r\nThanks and Regards,\r\nAmar kumar\r\n", "comments": ["@babuya7 \r\nCould you please try on latest stable version of tf and let us know if this is still an issue.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24982\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24982\">No</a>\n"]}, {"number": 24981, "title": "Error in tf.nn.softmax_cross_entropy_with_logits_v2 due to tensor shape change", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Cent OS 6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): installed from source\r\n- TensorFlow version (use command below):  tensorflow-1.8.0\r\n- Python version: python 3.0\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: cuda 9.0\r\n- GPU model and memory: Tesla V100 (16GB)\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nError in tf.nn.softmax_cross_entropy_with_logits_v2\r\nBelow is outcome of before and after the optimization.\r\n\r\ndimension of final output for vgg is [?, 3]\r\ndimension of final output for yolo is [?, 1, 1, 3]\r\n\r\n`softmax = tf.nn.softmax_cross_entropy_with_logits_v2(logits=vgg_pred, labels=y, dim=-1)`\r\nprint(softmax)\r\n\r\nrun 1\r\nvgg output\r\n[ 1110.82873535     0.          1372.21044922  1051.81994629  5117.62011719]\r\nafter optimization\r\n[    0.          8890.65722656     0.             0.          6899.49121094]\r\nyolo output\r\n[[[       0. ]] [[ 8094592. ]] [[       0. ]] [[ 8089163.5]] [[ 8069554.5]]]\r\nafter optimization\r\n**[[[ 1.09861231]] [[ 1.09861231]] [[ 1.09861231]] [[ 1.09861231]] [[ 1.09861231]]]**\r\n\r\nrun2\r\nvgg output\r\n[    0.             0.          2072.43017578     0.          2563.55004883]\r\nafter optimization\r\n[  8293.29980469  11375.015625        0.          12365.3359375   1054.58935547]\r\nyolo output\r\n[[[ 8573584.]] [[       0.]] [[       0.]] [[ 8399384.]] [[ 9033760.]]]\r\nafter optimization\r\n**[[[ 1.09861231]] [[ 1.09861231]] [[ 1.09861231]] [[ 1.09861231]] [[ 1.09861231]]]**\r\n\r\nrun3\r\nvgg output\r\n[    0.          2648.55541992     0.          2143.43066406     0.        ] \r\nafter optimization\r\n[ 12912.1875          0.          13529.96679688      0.          12147.125     ]\r\nyolo output\r\n[[[ 7737216.]] [[ 9160056.]] [[ 7647175.]] [[ 7012569.]] [[ 8301956.]]]\r\nafter optimization\r\n**[[[ 1.09861231]] [[ 1.09861231]] [[ 1.09861231]] [[ 1.09861231]] [[ 1.09861231]]]**\r\n\r\nrun4\r\nvgg output\r\n[ 1804.08349609     0.             0.             0.          2595.27880859]\r\nafter optimization\r\n[     0.          11371.57226562  10606.33691406  11048.16992188      0.        ]\r\nyolo output\r\n[[[ 8434419. ]] [[ 7721689.5]] [[       0. ]] [[ 8754676. ]] [[ 6599649.5]]]\r\nafter optimization\r\n**[[[ 1.09861231]] [[ 1.09861231]] [[ 1.09861231]] [[ 1.09861231]] [[ 1.09861231]]]**\r\n\r\n**Describe the expected behavior**\r\nI believe softmax outcome should be different in each run. It doesn't happen when I use tf.nn.softmax_cross_entropy_with_logits, which is to be removed in future version.\r\n\r\n**Other info / logs**\r\nArchitecture of vgg and yolo is bit different. Both are manually editted by me. Training vgg gives good enough result; however, training yolo gives poor outcome. Particularly, error is not properly calculated, so the network is not trained at all.", "comments": ["@shkdidrlf Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support. Github is mainly for addressing bugs in installation and performance. If you think it is related to a bug in the TF, then please provide a code to reproduce the bug so that we can find root cause of the issue. Thanks!", "Thanks for your message.\r\nI found out that this error actually happened since relu was erroneously used in output layer.\r\nThat's why errors were not properly backpropagated."]}, {"number": 24980, "title": "MKL-backed tensorflow throws errors on softmax on empty tensors; GPU and Eigen versions do not", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary from anaconda\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nCalling `tf.nn.softmax` on a tensor that has a dimension 0 throws an exception at runtime, when Tensorflow is backed by MKL. The same op returns an empty array of suitable size (i.e., the same size as the input tensor) under GPU and Eigen.\r\n\r\nNote that this happens no matter which axis the softmax is taken across (the 0-length dim or a nonzero-length one).\r\n\r\nStacktrace:\r\n```\r\npython test.py \r\n2019-01-16 17:55:50.289518: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2019-01-16 17:55:50.294890: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n2019-01-16 17:55:50.303432: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at mkl_softmax_op.cc:167 : Aborted: Operation received an exception:Status: 3, message: could not initialize a memory descriptor, in file tensorflow/core/kernels/mkl_softmax_op.cc:164\r\nTraceback (most recent call last):\r\n  File \"/home/matt/anaconda3/envs/learn/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1334, in _do_call\r\n    return fn(*args)\r\n  File \"/home/matt/anaconda3/envs/learn/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1319, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/home/matt/anaconda3/envs/learn/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1407, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.AbortedError: Operation received an exception:Status: 3, message: could not initialize a memory descriptor, in file tensorflow/core/kernels/mkl_softmax_op.cc:164\r\n\t [[{{node Softmax}} = _MklSoftmax[T=DT_FLOAT, _kernel=\"MklOp\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_Placeholder_0_0, DMT/_0)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 9, in <module>\r\n    print(sess.run(softmax, {tensor: x}))\r\n  File \"/home/matt/anaconda3/envs/learn/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 929, in run\r\n    run_metadata_ptr)\r\n  File \"/home/matt/anaconda3/envs/learn/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1152, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/matt/anaconda3/envs/learn/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\r\n    run_metadata)\r\n  File \"/home/matt/anaconda3/envs/learn/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.AbortedError: Operation received an exception:Status: 3, message: could not initialize a memory descriptor, in file tensorflow/core/kernels/mkl_softmax_op.cc:164\r\n\t [[node Softmax (defined at test.py:5)  = _MklSoftmax[T=DT_FLOAT, _kernel=\"MklOp\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_Placeholder_0_0, DMT/_0)]]\r\n\r\nCaused by op 'Softmax', defined at:\r\n  File \"test.py\", line 5, in <module>\r\n    softmax = tf.nn.softmax(tensor)\r\n  File \"/home/matt/anaconda3/envs/learn/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/matt/anaconda3/envs/learn/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1722, in softmax\r\n    return _softmax(logits, gen_nn_ops.softmax, axis, name)\r\n  File \"/home/matt/anaconda3/envs/learn/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1673, in _softmax\r\n    return compute_op(logits, name=name)\r\n  File \"/home/matt/anaconda3/envs/learn/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 7138, in softmax\r\n    \"Softmax\", logits=logits, name=name)\r\n  File \"/home/matt/anaconda3/envs/learn/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/matt/anaconda3/envs/learn/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/matt/anaconda3/envs/learn/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3274, in create_op\r\n    op_def=op_def)\r\n  File \"/home/matt/anaconda3/envs/learn/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1770, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nAbortedError (see above for traceback): Operation received an exception:Status: 3, message: could not initialize a memory descriptor, in file tensorflow/core/kernels/mkl_softmax_op.cc:164\r\n\t [[node Softmax (defined at test.py:5)  = _MklSoftmax[T=DT_FLOAT, _kernel=\"MklOp\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_Placeholder_0_0, DMT/_0)]]\r\n```\r\n\r\n**Describe the expected behavior**\r\n`tf.nn.softmax` on an empty tensor returns an empty tensor on all configurations.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ntensor = tf.placeholder(tf.float32, [10, None])\r\nsoftmax = tf.nn.softmax(tensor)\r\nx = np.random.randn(10, 0)\r\n\r\nwith tf.Session() as sess:\r\n    print(sess.run(softmax, {tensor: x}))\r\n```\r\n**Other info / logs**\r\nThis may be related to https://github.com/tensorflow/tensorflow/issues/23145 which has a similar error message but seems to be caused by a different situation.\r\n", "comments": ["@azaks2 who should look at this?", "I have the same issue with Tensorflow softmax.\r\nI use macOS 10.14, python 3.6 with Conda installed Tensorflow.\r\n\r\n\"\r\n  File \"/Users/lvhw/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\r\n    raise type(e)(node_def, op, message)\r\n\r\nAbortedError: Operation received an exception:Status: 3, message: could not initialize a memory descriptor, in file tensorflow/core/kernels/mkl_softmax_op.cc:164\r\n\t [[node Softmax (defined at /Users/lvhw/Desktop/CRF-image-segmentation-master/CRFCNNImageSegmentation.py:171)  = _MklSoftmax[T=DT_FLOAT, _kernel=\"MklOp\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](conv2d_transpose, conv2d_transpose:1)]]\r\n\r\nCaused by op 'Softmax', defined at:\r\n  File \"/Users/lvhw/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/Users/lvhw/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/Users/lvhw/anaconda3/lib/python3.6/site-packages/spyder_kernels/console/__main__.py\", line 11, in <module>\r\n    start.main()\r\n\"", "@agramesh1  - Can you take a look at this?", "@tatianashp  thanks.  Pinging @TensorFlow-MKL ", "Would you please upgrade your tensorflow to the latest version (ver 1.13.1)? It seems to be fixed in this release.\r\n\r\n$ pip install --upgrade intel-tensorflow\r\n$ python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\nb'v1.13.1-0-g6612da8' 1.13.1\r\n$ python test.py\r\n2019-03-19 16:15:55.541077: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n2019-03-19 16:15:55.574388: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2500000000 Hz\r\n2019-03-19 16:15:55.585260: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2f945c0 executing computations on platform Host. Devices:\r\n2019-03-19 16:15:55.585295: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-03-19 16:15:55.595114: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n2019-03-19 16:15:55.630654: E tensorflow/core/common_runtime/bfc_allocator.cc:373] tried to deallocate nullptr\r\n2019-03-19 16:15:55.630703: E tensorflow/core/common_runtime/bfc_allocator.cc:373] tried to deallocate nullptr\r\n[]\r\n\r\ntest.py is the reproducible test case.\r\n\r\nPlease let me know if this does not work on your side.", "please close this issue if you have this solution working ", "workaround suggested. Closing this issue. Please reopen this issue if the solution provided doesn't solve the issue", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=24980\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=24980\">No</a>\n"]}, {"number": 24979, "title": "Use safe variance epsilon for float16 layer_norm", "body": "- The original epsilon value (1e-12) is too small for float16 and can\r\n  cause NaNs in the output when the variance is small.\r\n- This commit also adds float16 and float32 cases to LayerNormTest and\r\n  improves the numerical robustness of the test logic.\r\n\r\nI believe this is one of (but not all of) the reasons that NaNs are observed during fp16 training of the BERT model as noted here: https://github.com/google-research/bert/pull/255#issuecomment-450591507", "comments": ["> **import/copybara ** \u2014 An error happened while migrating the change\r\nRequired\r\n\r\n@rthadur any idea what this means / how to retrigger?"]}, {"number": 24978, "title": "[XLA:GPU] Make sm_35 the min supported LLVM proc", "body": "- XLA nvptx backend does not support sm_30, and TensorFlow also only\r\n  supports sm_35 onward.\r\n\r\nThis is a follow-up from https://github.com/tensorflow/tensorflow/pull/24350#issuecomment-447164505", "comments": []}, {"number": 24977, "title": "\"Upgrade to Tensorflow 2.0\" documentation lists wrong TF version", "body": "**System information**\r\n- TensorFlow version: 1.12\r\n- Doc Link: https://github.com/tensorflow/docs/blob/master/site/en/r2/guide/upgrade.md\r\n\r\n\r\n**Describe the documentation issue**\r\nThe document says that `Note: tf_upgrade_v2 is installed automatically by pip install for TensorFlow 1.12 and later.` but looking at the setup.py files this console command is not installed by 1.12, but will be installed in 1.13\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\nNo\r\n", "comments": ["This has been fixed. "]}, {"number": 24976, "title": "Linking to both tensorflow and protobuf causes segmentation fault during static initializers", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 4.18.10-1rodete2-amd64 (Debian-derived)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): nightly Jan 15, 2018 (protobuf built from HEAD Jan 15)\r\n- Python version: N/A\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): gcc 7.3.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nAborts on SIGSEGV\r\n\r\n**Describe the expected behavior**\r\nExits cleanly\r\n\r\n**Details**\r\nI want to create an application that calls the C API but also can parse protocol buffers on its own behalf. For that want to link dynamically to tensorflow and statically to protobuf. When I do this, it seems like protobuf may be tricking libtensorflow.so into thinking that it has run some static initializers that it in fact has not run (on the static variables needed by its own internal copy of protobuf).\r\n\r\nThe segfault is only on Linux. Linking the same way on Windows works fine.\r\n\r\nI have varied libtensorflow and protobuf versions, and it seems to happen with all of them. It also happens whether I choose static or dynamic linking for my binary's copy of protobuf.\r\n\r\nI also tried building my own liba.so that itself statically links protobuf and then a binary that linked dynamically to \"a\" and statically to protobuf. This worked, which is pointing away from this being a purely protobuf issue.\r\n\r\n**Code to reproduce the issue**\r\n\r\n* bash\r\n```\r\nc++ -o main \\\r\n  -L$TF_DIR/lib -I$TF_DIR/include \\\r\n  -L$PROTO_DIR/lib -I$PROTO_DIR/include \\\r\n  main.cc -l tensorflow -l protobuf\r\n\r\nLD_LIBRARY_PATH=$TF_DIR/lib:$PROTO_DIR/lib ./main\r\n```\r\n\r\nRemoving -lprotobuf from the above command will get rid of the segfault.\r\n\r\n* main.cc\r\n```\r\nint main(int argc, char** argv) {}\r\n```\r\n\r\n**Other info / logs**\r\n\r\nProgram received signal SIGSEGV, Segmentation fault.\r\n0x00007fffed8f20b8 in tensorflow::kernel_factory::OpKernelRegistrar::InitInternal(tensorflow::KernelDef const*, absl::string_view, std::un\r\nique_ptr<tensorflow::kernel_factory::OpKernelFactory, std::default_delete<tensorflow::kernel_factory::OpKernelFactory> >) ()\r\n   from /usr/local/google/home/mattharvey/no_backup/libtensorflow/lib/libtensorflow_framework.so\r\n(gdb) bt\r\n#0  0x00007fffed8f20b8 in tensorflow::kernel_factory::OpKernelRegistrar::InitInternal(tensorflow::KernelDef const*, absl::string_view, std\r\n::unique_ptr<tensorflow::kernel_factory::OpKernelFactory, std::default_delete<tensorflow::kernel_factory::OpKernelFactory> >) ()\r\n   from /usr/local/google/home/mattharvey/no_backup/libtensorflow/lib/libtensorflow_framework.so\r\n#1  0x00007fffed88336a in tensorflow::kernel_factory::OpKernelRegistrar::OpKernelRegistrar(tensorflow::KernelDef const*, absl::string_view\r\n, tensorflow::OpKernel* (*)(tensorflow::OpKernelConstruction*)) ()\r\n   from /usr/local/google/home/mattharvey/no_backup/libtensorflow/lib/libtensorflow_framework.so\r\n#2  0x00007fffed85f806 in _GLOBAL__sub_I_dataset.cc ()\r\n   from /usr/local/google/home/mattharvey/no_backup/libtensorflow/lib/libtensorflow_framework.so\r\n#3  0x00007ffff7de88aa in call_init (l=<optimized out>, argc=argc@entry=1, argv=argv@entry=0x7fffffffdc68, env=env@entry=0x7fffffffdc78)\r\n    at dl-init.c:72\r\n#4  0x00007ffff7de89bb in call_init (env=0x7fffffffdc78, argv=0x7fffffffdc68, argc=1, l=<optimized out>) at dl-init.c:30\r\n#5  _dl_init (main_map=0x7ffff7ffe170, argc=1, argv=0x7fffffffdc68, env=0x7fffffffdc78) at dl-init.c:120\r\n#6  0x00007ffff7dd9c5a in _dl_start_user () from /lib64/ld-linux-x86-64.so.2\r\n#7  0x0000000000000001 in ?? ()\r\n#8  0x00007fffffffdf2e in ?? ()\r\n#9  0x0000000000000000 in ?? ()\r\n\r\n   0x00007fffed8f20a0 <+80>:    mov    0x50(%r15),%rax\r\n   0x00007fffed8f20a4 <+84>:    lea    -0xa0(%rbp),%rbx\r\n   0x00007fffed8f20ab <+91>:    mov    %rbx,%rdi\r\n   0x00007fffed8f20ae <+94>:    mov    (%rax),%r8\r\n   0x00007fffed8f20b1 <+97>:    mov    0x48(%r15),%rax\r\n   0x00007fffed8f20b5 <+101>:   mov    (%rax),%rsi\r\n=> 0x00007fffed8f20b8 <+104>:   mov    -0x18(%r8),%r9\r\n\r\nHow did -0x18(%r8) get illegal?\r\n\r\n(gdb) info register r8\r\nr8             0x0                 0\r\n\r\n-0x18 is certainly illegal. Where did it come from? 0x50(%r15) if we trace through the above.\r\n\r\n(gdb) info register r15\r\nr15            0x555555768d10      93824994413840\r\n\r\n(gdb) x/2 0x555555768d60\r\n0x555555768d60: 0xee2c0bc0      0x00007fff\r\n\r\n(gdb) x/2 0x00007fffee2c0bc0\r\n0x7fffee2c0bc0 <google::protobuf::internal::fixed_address_empty_string>:        0x00000000      0x00000000\r\n\r\n... the 0x0 that ended up in r8.\r\n\r\nZoom out to find lots of stuff uninitialized:\r\n\r\n(gdb) x/64x 0x7fffee4ddb00\r\n0x7fffee4ddb00 <google::protobuf::_DoubleValue_default_instance_>:      0x00000000      0x00000000      0x00000000      0x00000000\r\n0x7fffee4ddb10 <google::protobuf::_DoubleValue_default_instance_+16>:   0x00000000      0x00000000      0x00000000      0x00000000\r\n0x7fffee4ddb20 <_ZStL8__ioinit>:        0x00000000      0x00000000      0x00000000      0x00000000\r\n0x7fffee4ddb30 <_ZStL8__ioinit>:        0x00000000      0x00000000      0x00000000      0x00000000\r\n0x7fffee4ddb40 <google::protobuf::internal::RepeatedPrimitiveDefaults::default_instance()::instance>:   0x00000000      0x00000000      0x00000000        0x00000000\r\n0x7fffee4ddb50 <guard variable for google::protobuf::internal::RepeatedStringTypeTraits::GetDefaultRepeatedField()::instance>:  0x000000000x00000000      0x00000000      0x00000000\r\n0x7fffee4ddb60 <guard variable for google::protobuf::internal::(anonymous namespace)::Register(google::protobuf::MessageLite const*, int, google::protobuf::internal::ExtensionInfo)::local_static_registry>:     0x00000000      0x00000000      0x00000000      0x00000000\r\n0x7fffee4ddb70 <_ZStL8__ioinit>:        0x00000000      0x00000000      0x00000000      0x00000000\r\n0x7fffee4ddb80 <google::protobuf::internal::InitSCCImpl(google::protobuf::internal::SCCInfoBase*)::mu>: 0x00000000      0x00000000      0x00000000        0x00000000\r\n0x7fffee4ddb90 <google::protobuf::internal::InitSCCImpl(google::protobuf::internal::SCCInfoBase*)::mu+16>:      0x00000000      0x000000000x00000000      0x00000000\r\n0x7fffee4ddba0 <google::protobuf::internal::InitSCCImpl(google::protobuf::internal::SCCInfoBase*)::mu+32>:      0x00000000      0x000000000x00000000      0x00000000\r\n0x7fffee4ddbb0 <guard variable for google::protobuf::internal::InitSCCImpl(google::protobuf::internal::SCCInfoBase*)::runner>:  0x000000000x00000000      0x00000000      0x00000000\r\n0x7fffee4ddbc0 <google::protobuf::internal::fixed_address_empty_string>:        0x00000000      0x00000000      0x00000000      0x00000000\r\n0x7fffee4ddbd0 <google::protobuf::internal::implicit_weak_message_default_instance>:    0x00000000      0x00000000      0x00000000      0x00000000\r\n0x7fffee4ddbe0 <google::protobuf::internal::implicit_weak_message_default_instance+16>: 0x00000000      0x00000000      0x00000000      0x00000000\r\n0x7fffee4ddbf0 <google::protobuf::ShutdownProtobufLibrary()::is_shutdown>:      0x00000000      0x00000000      0x00000000      0x00000000\r\n", "comments": ["I found a temporary workaround for myself, but it should still be possible to do this from released binaries without the need to rebuild.\r\n\r\n\r\n\r\nLocal opt build works from r1.12 at a6d8ffae097d0132989ae4688d224121ec6d8f35\r\n\r\n```\r\nbazel build -c opt --copt=-mavx --define=grpc_no_ares=true //tensorflow/tools/lib_package:libtensorflow\r\n\r\ntar zxvf ../tensorflow/bazel-bin/tensorflow/tools/lib_package/libtensorflow.tar.gz\r\n```\r\n\r\nHowever I get the segfault from\r\n\r\nhttps://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-linux-x86_64-1.12.0.tar.gz\r\n\r\nwith protobuf built locally from\r\n\r\nhttps://github.com/protocolbuffers/protobuf/releases/download/v3.6.0/protobuf-all-3.6.0.tar.gz\r\n\r\nand also from\r\n\r\nhttps://storage.googleapis.com/tensorflow-nightly/github/tensorflow/lib_package/libtensorflow-cpu-linux-x86_64.tar.gz   # Wed Jan 16 22:33:29 PST 2019\r\n\r\nwith protobuf built locally from head (3.6.1) around the same time.", "I just hit this with the `libtegra_tensorflow.so` that Nvidia provides on their Xavier board's rootfs. Is building from scratch really the only workaround?", "I also stumbled upon this problem.\r\n\r\n- On Ubuntu 18.04.1, using GCC 7.3.0\r\n- Using libtensorflow-cpu-linux-x86_64-1.12.0.tar.gz from the same link @matth79 mentioned above.\r\n- Using protobuf 3.6.1 built locally.\r\n\r\nThe problem is easy to replicate:\r\n\r\n> #include \"iostream\"\r\n> #include \"tensorflow/c/c_api.h\"\r\n> \r\n> // Enable this line to include protobuf\r\n> //#include \"google/protobuf/message.h\"\r\n> \r\n> // Main program entry\r\n> int main(int argc, char* argv[])\r\n> {\r\n> \tstd::cout << \"Tensorflow version: \" << TF_Version();\r\n> \treturn 0;\r\n> }\r\n\r\nLink with -ltensorflow and it works fine. Uncomment the line to include protobuf and link with both  -ltensorflow and -lprotobuf and observe the segmentation fault on initialization.\r\n", "@gunan @allenlavoie can either of you comment?", "This has been over a month ago and we're still having issues with this. An update or fix would be very much appreciated!", "We are also having issues with this problem on NVidia's Xavier and would appreciate and update/fix.  If there are no plans to fix the bug, we will try to build Tensorflow with the hints from matth79.", "Sounds like it must be a symbol conflict. And since it's the same library, it's not a case where we can just rename one of the symbols to avoid the conflict. The workarounds sound like (1) only load the second copy of protobuf in a .so that does not use TensorFlow, and you can use both that .so and TensorFlow's .so from your main program, (2) instead of linking normally, dlopen() TensorFlow with RTLD_DEEPBIND set so TensorFlow prefers its own symbols.\r\n\r\nI'm not sure what TensorFlow can do. Putting something in the global symbol table which conflicts with TensorFlow's protobuf usage isn't something we can easily work around. Unless someone has a suggestion?", "Hello. I get the same problem , the info like this:\r\n0x00007fffddef3058 in tensorflow::kernel_factory::OpKernelRegistrar::InitInternal(tensorflow::KernelDef const*, absl::string_view, std::unique_ptr<tensorflow::kernel_factory::OpKernelFactory, std::default_delete<tensorflow::kernel_factory::OpKernelFactory> >) ()\r\n   from /usr/local/lib/python3.5/dist-packages/tensorflow/python/../libtensorflow_framework.so\r\n\r\n\r\nI am using C++ to call python's tensorflow. The protobuf library is called in our own environment, when we call python's \u201cimport tensorflow as tf\u201d in C++ in our own environment. The above problem will occur. When the\u201c import tensorflow as tf \u201cis deleted, the problem will disappear. Do you know the reason?\r\n\r\nI think that the protobuf of my environment conflicts with the protobuf of tensorflow.\r\n\r\ncan you help me . thanks\r\n", "This is indeed a problem with protobuf; there's not much TF itself can do as @allenlavoie mentioned. We dealt with this by running TF operations in a separate process that talks over a UNIX socket, but @allenlavoie's solutions should work too.", "I hope the readers have learned a valuable lesson about using static initializers in this way from this thread.", "I also have this issue. Reproduced with `libtensorflow-gpu-linux-x86_64-1.15.0.tar.gz`.", "While I do not want to close this issue, as @allenlavoie wrote in https://github.com/tensorflow/tensorflow/issues/24976#issuecomment-471760777 , I am not sure what we can do.\r\nTF is working on the slow path to hide all protobuf symbols from its API surface. Even then static initializers will be executed twice. I am not sure what will happen, as I am not sure how protobuf uses them.\r\n\r\nSo, unfortunately I can only offer https://github.com/tensorflow/tensorflow/issues/24976#issuecomment-471760777 , and we should close this as \"Infeasible\".", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24976\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24976\">No</a>\n", "> Hello. I get the same problem , the info like this:\r\n> 0x00007fffddef3058 in tensorflow::kernel_factory::OpKernelRegistrar::InitInternal(tensorflow::KernelDef const*, absl::string_view, std::unique_ptr<tensorflow::kernel_factory::OpKernelFactory, std::default_deletetensorflow::kernel_factory::OpKernelFactory >) ()\r\n> from /usr/local/lib/python3.5/dist-packages/tensorflow/python/../libtensorflow_framework.so\r\n> \r\n> I am using C++ to call python's tensorflow. The protobuf library is called in our own environment, when we call python's \u201cimport tensorflow as tf\u201d in C++ in our own environment. The above problem will occur. When the\u201c import tensorflow as tf \u201cis deleted, the problem will disappear. Do you know the reason?\r\n> \r\n> I think that the protobuf of my environment conflicts with the protobuf of tensorflow.\r\n> \r\n> can you help me . thanks\r\n\r\nI ran into core dump issue when call import tensorflow using C++ Python API. \r\n\r\n`\r\nThread 1 \"tf\" received signal SIGSEGV, Segmentation fault.\r\ngoogle::protobuf::internal::AddDescriptors(google::protobuf::internal::DescriptorTable const*) ()\r\n   from /usr/local/lib/python3.6/dist-packages/google/protobuf/pyext/_message.cpython-36m-x86_64-linux-gnu.so\r\n`\r\n\r\nFinally, I installed python protobuf that matches with Tensorflow's protobuf version, 3.7.1. It magically works. I don't know how to check the protobuf version inside tensorflow library libtensorflow_framework.so or _pywrap_tensorflow_internal.so.\r\n\r\nSince Tensorflow 1.14 requires protobuf >= 3.6.1, so I installed 3.6.1 first and then my program throws an error said \r\n`\r\n[libprotobuf FATAL external/protobuf_archive/src/google/protobuf/stubs/common.cc:86] This program was compiled against version 3.6.1 of the Protocol Buffer runtime library, which is not compatible with the installed version (3.7.1).  Contact the program author for an update.  If you compiled the program yourself, make sure that your headers are from the same version of Protocol Buffers as your link-time library.  (Version verification failed in \"google/protobuf/descriptor.pb.cc\".)`\r\n\r\nHowever, if I install python protobuf to 3.11.3, I got segfault.\r\n\r\nSo once I upgrade protobuf into 3.7.1, it works."]}, {"number": 24975, "title": "Cherry pick fix to tflite tests.", "body": "", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n<!-- need_author_consent -->"]}, {"number": 24974, "title": "GetTempFilename is not implemented", "body": "**System information**\r\n- Windows 10:\r\n- TensorFlow installed from (source or binary):\r\n- master branch:\r\n- python 3.6:\r\n\r\n**tensorflow\\core\\lib\\io\\path.cc(290) : error C4716: 'tensorflow::io::GetTempFilename': must return a value**\r\n\r\n**bazel build --config=opt -c fastbuild //tensorflow:libtensorflow.so**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n![image](https://user-images.githubusercontent.com/1705364/51275349-4e8b0300-1997-11e9-9991-5a86eb694e02.png)\r\n\r\n", "comments": ["@Oceania2018 Please provide as many details as possible to find the root cause of the issue. Are you getting the error during installation or during running of some code. It would be great if you can provide a small code to reproduce the error. Thanks!", "@jvishnuvardhan When I build TensorFlow on Windows in bazel `bazel build -c opt -c fastbuild //tensorflow:libtensorflow.so`, the `GetTempFilename` in `path.cc` file throw error.", "It works now.\r\n![image](https://user-images.githubusercontent.com/1705364/51429904-72d62200-1bd9-11e9-8c85-901ff6199b19.png)\r\n", "@Oceania2018 Thanks you very much for providing solution also. ", "> It works now.\r\n> ![image](https://user-images.githubusercontent.com/1705364/51429904-72d62200-1bd9-11e9-8c85-901ff6199b19.png)\r\n\r\nThis is not a proper solution. The original problem still remains when building with\r\n\r\n`bazel build -c opt -c fastbuild //tensorflow:libtensorflow.so\r\n`\r\nIt would be great if the function is implemented for Windows platform.\r\n\r\n```\r\nstring GetTempFilename(const string& extension) {\r\n#if defined(PLATFORM_WINDOWS) || defined(__ANDROID__)\r\n  LOG(FATAL) << \"GetTempFilename is not implemented in this platform.\";\r\n#else\r\n...\r\n```\r\n\r\nin  the GetTempFilename in path.cc", "I had the same problem, building with this command:\r\n\r\n> bazel build -c dbg //tensorflow:tensorflow_cc\r\n\r\nTF 1.14\r\nWindows 10 x64", "if I want to run tensorflow.dll via **Debug Mode** in Visual Studio, which builds option should be set as belown ???\r\n\r\n- --compilation_mode=dbg\r\n- --compilation_mode=opt\r\n- --compilation_mode=fastbuild", "@AndreyPlotkinOr\r\n> I had the same problem, building with this command:\r\n> \r\n> > bazel build -c dbg //tensorflow:tensorflow_cc\r\n> \r\n> TF 1.14\r\n> Windows 10 x64\r\n\r\nusing the command below to build  tensorlfow.dll of Release for windows\r\n> bazel build  //tensorflow:tensorflow_cc\r\n\r\nhowever, debug still could not be resolved.\r\n", "I tried to build a debug version now and got the same problem.\r\nWindows 10, TF 1.15\r\n\r\nbazel build -c dbg --strip=never --compilation_mode=dbg --config=opt --config=cuda --copt=-nvcc_options=disable-warnings --define=no_tensorflow_py_deps=true -s --explain=explain.txt --verbose_explanations --subcommands=pretty_print //tensorflow:tensorflow_cc\r\n\r\nerror C4716: 'tensorflow::io::GetTempFilename': must return a value\r\n\r\n", "@Oceania2018 Please reopen the issue, it's not solved.\r\nI need a TF in debug mode with full debug symbols, and it's the only way to compile it.", "```cpp\r\n#if defined(PLATFORM_WINDOWS)\r\n  for (const char* dir : std::vector<const char*>(\r\n           {getenv(\"TEST_TMPDIR\"), getenv(\"TMPDIR\"), getenv(\"TMP\"), getenv(\"TEMP\"), \"/tmp\"})) {\r\n    if (!dir || !dir[0]) {\r\n      continue;\r\n    }\r\n    struct stat statbuf;\r\n    if (!stat(dir, &statbuf) && S_ISDIR(statbuf.st_mode)) {\r\n      string tmp_filepath;\r\n      tmp_filepath = io::JoinPath(\r\n          dir,\r\n          strings::StrCat(\"tmp_file_tensorflow_\", UniqueId(), \"_XXXXXX\", extension));\r\n\t  //TODO Switch to secured method _mktemp_s\r\n\t  //int sizeInChars = strnlen(names[i], 9) + 1; \r\n\t  //int err = _mktemp_s( names[i], sizeInChars );\r\n      char* fd = _mktemp(&tmp_filepath[0]);\r\n      if (fd == nullptr) {\r\n        LOG(FATAL) << \"Failed to create temp file.\";\r\n      } else {\r\n        return tmp_filepath;\r\n      }\r\n    }\r\n  }\r\n  LOG(FATAL) << \"No temp directory found.\";  \r\n```", "Will take a while until I can get to this, but it's something to be handled as part of tensorflow/community#101", "To get rid of this C4716 error, I added `--copt=/w34716` to the bazel build command line, e.g.\r\n`bazel build --copt=/w34716 --experimental_shortened_obj_file_path //moco:moco`\r\nThis makes the MSVC compiler happy.\r\nInspired by: https://stackoverflow.com/a/11386226/12338026\r\n", "@Oceania2018,\r\n\r\nWe are checking to see if you still need help on this issue. We recommend that you upgrade to `2.6` which is latest stable version of TF and let us know if the issue still persists in newer versions. You can use this [tested build configurations](https://www.tensorflow.org/install/source_windows#tested_build_configurations) as a reference for configuring the build environment. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24974\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24974\">No</a>\n"]}, {"number": 24973, "title": "Add tf_cc_test to build file to fix sanity check.", "body": "", "comments": []}, {"number": 24972, "title": "tflite 0.0.0-gpu-experimental still long interference time", "body": "I tested tflite 0.0.0-gpu-experimental on Android.\r\nComparing to version 1.12.0, the speed up is quite poor.\r\nAverage of total interference time on the devices I tested:\r\n\r\ndevice | Android v | tflite 1.12.0 time | tflite gpu exp time\r\n--------- | ------------ | --------------------- | ------------------------\r\nHuawei p10 lite (was-lx1) | 7.0, API 24 | ~651ms | ~605ms\r\nLGE LG-M200 | 7.0, API 24 | ~1207ms | ~1390ms\r\nSony Xperia M5 (e5603) | 6.0, API 23 | ~690ms | ~609ms\r\nXiaomi Redmi 3 | 5.1.1, API 22 | ~942ms | ~800ms\r\nAlcatel TCL 5033D | 8.1.0, API 27 | ~1128ms |  ~1032ms\r\n\r\nwhereas for the same model on CoreML (iOS platform) it takes much less on an old iPhone.\r\n", "comments": ["Thanks for trying the delegate out.\r\n\r\nCould you elaborate on what model and what ops you are using in the model. It could be that you are using ops that are not accelerated on the GPU, and since the time didn't change almost at all, I would suspect that to be the case. Also, make sure you are actually enabling the delegate? The demo app automatically enabled the delegate when you switch AARs, but on your own app you will need to enable it explicitly. i.e. see\r\n\"Trying the GPU Delegate on your own model\"\r\nin https://www.tensorflow.org/lite/performance/gpu \r\n\r\nAlso, have you tried tflite on iOS? That will give you a apple to apple (pun intended) hardware comparison.\r\n\r\nYou can look print out the logs and look for the message\r\n          \"WARNING: op code #%d cannot be handled by this delegate.  Only the \"\r\n          \"first %d ops will run on the GPU, and the remaining %d on the CPU.\",\r\n", "I'm using mostly conv2d (Darknet-like bottom as in YOLO9000). Indeed I did not enable the delegate in the code.\r\n> \"Trying the GPU Delegate on your own model\"\r\n> in https://www.tensorflow.org/lite/performance/gpu\r\n\r\nAfter adding `GpuDelegate` it seems to \"work\".\r\nBy \"work\" I mean I get an exception about  \r\n```Caused by: java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: WARNING: op code #55 cannot be handled by this delegate.  Only the first 2 ops will run on the GPU, and the remaining 48 on the CPU.GpuDelegate Prepare: depth_multiplier 16 != weights output channels 1Node number 50 (GpuDelegate) failed to prepare.```, \r\nbut this is no longer the case for this ticket, so I'm closing this one.\r\nThank you!\r\n\r\n"]}, {"number": 24971, "title": "TensorFlow unit tests crash or take a long time on Azure Standard A1", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution: Ubuntu 16.04 LTS\r\n- TensorFlow installed from: source\r\n- TensorFlow version: `master` branch\r\n- Python version: 2 and 3\r\n- Installed using: I cloned the repo, `cd`-ed into it, and ran the unit tests with Docker: `tensorflow/tools/ci_build/ci_build.sh CPU bazel test //tensorflow/...`, as mentioned at the end of the [contributing guidelines](https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md)\r\n- Bazel version (if compiling from source): 0.20.0\r\n- GCC/Compiler version (if compiling from source): latest (from Docker image)\r\n- CUDA/cuDNN version: I believe this is not installed\r\n- GPU model and memory: no GPU listed for this virtual machine in [Microsoft Docs](https://docs.microsoft.com/en-us/azure/virtual-machines/windows/sizes-previous-gen)\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nHello, I am a new contributor here trying to run the unit tests before submitting pull requests and confirming that they do not break the tests. I'll be happy to add the solution to the documentation when we find one.\r\n\r\nI am running the unit tests of TensorFlow according to the [contributing guidelines](https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md). I set up a `Docker on Ubuntu Server` virtual machine (`Standard A1 (1 Core, 1.75 GiB memory)`) on Azure, and ran these commands:\r\n\r\n    git clone https://github.com/tensorflow/tensorflow.git\r\n    cd tensorflow/\r\n    tensorflow/tools/ci_build/ci_build.sh CPU bazel test //tensorflow/...\r\n\r\nMy first attempt crashed with these lines at the end:\r\n\r\n    Analyzing: 7629 targets (561 packages loaded, 37218 targets configured)\r\n    java.lang.OutOfMemoryError: GC overhead limit exceeded\r\n    Dumping heap to /home/mmorin/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_mmorin/eab0d61a99b6696edb3d2aff87b585e8/java_pid25131.hprof ...\r\n    Heap dump file created [617886369 bytes in 9.053 secs]\r\n    Internal error thrown during build. Printing stack trace: java.lang.OutOfMemoryError: GC overhead limit exceeded\r\n    \tat com.google.devtools.build.lib.analysis.configuredtargets.RuleConfiguredTarget.<init>(RuleConfiguredTarget.java:115)\r\n    \tat com.google.devtools.build.lib.analysis.configuredtargets.RuleConfiguredTarget.<init>(RuleConfiguredTarget.java:142)\r\n    \tat com.google.devtools.build.lib.analysis.RuleConfiguredTargetBuilder.build(RuleConfiguredTargetBuilder.java:174)\r\n    \tat com.google.devtools.build.lib.rules.python.PyBinary.create(PyBinary.java:53)\r\n    \tat com.google.devtools.build.lib.rules.python.PyBinary.create(PyBinary.java:36)\r\n    \tat com.google.devtools.build.lib.analysis.ConfiguredTargetFactory.createRule(ConfiguredTargetFactory.java:323)\r\n    \tat com.google.devtools.build.lib.analysis.ConfiguredTargetFactory.createConfiguredTarget(ConfiguredTargetFactory.java:207)\r\n    \tat com.google.devtools.build.lib.skyframe.SkyframeBuildView.createConfiguredTarget(SkyframeBuildView.java:636)\r\n    \tat com.google.devtools.build.lib.skyframe.ConfiguredTargetFunction.createConfiguredTarget(ConfiguredTargetFunction.java:783)\r\n    \tat com.google.devtools.build.lib.skyframe.ConfiguredTargetFunction.compute(ConfiguredTargetFunction.java:326)\r\n    \tat com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:422)\r\n    \tat com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:368)\r\n    \tat java.base/java.util.concurrent.ForkJoinTask$AdaptedRunnableAction.exec(Unknown Source)\r\n    \tat java.base/java.util.concurrent.ForkJoinTask.doExec(Unknown Source)\r\n    \tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.localPopAndExec(Unknown Source)\r\n    \tat java.base/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)\r\n    \tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)\r\n    \r\n    INFO: Elapsed time: 873.635s\r\n    INFO: 0 processes.\r\n    FAILED: Build did NOT complete successfully (561 packages loaded, 37254 targets configured)\r\n    Internal error thrown during build. Printing stack trace: java.lang.OutOfMemoryError: GC overhead limit exceeded\r\n    \tat com.google.devtools.build.lib.analysis.configuredtargets.RuleConfiguredTarget.<init>(RuleConfiguredTarget.java:115)\r\n    \tat com.google.devtools.build.lib.analysis.configuredtargets.RuleConfiguredTarget.<init>(RuleConfiguredTarget.java:142)\r\n    \tat com.google.devtools.build.lib.analysis.RuleConfiguredTargetBuilder.build(RuleConfiguredTargetBuilder.java:174)\r\n    \tat com.google.devtools.build.lib.rules.python.PyBinary.create(PyBinary.java:53)\r\n    \tat com.google.devtools.build.lib.rules.python.PyBinary.create(PyBinary.java:36)\r\n    \tat com.google.devtools.build.lib.analysis.ConfiguredTargetFactory.createRule(ConfiguredTargetFactory.java:323)\r\n    \tat com.google.devtools.build.lib.analysis.ConfiguredTargetFactory.createConfiguredTarget(ConfiguredTargetFactory.java:207)\r\n    \tat com.google.devtools.build.lib.skyframe.SkyframeBuildView.createConfiguredTarget(SkyframeBuildView.java:636)\r\n    \tat com.google.devtools.build.lib.skyframe.ConfiguredTargetFunction.createConfiguredTarget(ConfiguredTargetFunction.java:783)\r\n    \tat com.google.devtools.build.lib.skyframe.ConfiguredTargetFunction.compute(ConfiguredTargetFunction.java:326)\r\n    \tat com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:422)\r\n    \tat com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:368)\r\n    \tat java.base/java.util.concurrent.ForkJoinTask$AdaptedRunnableAction.exec(Unknown Source)\r\n    \tat java.base/java.util.concurrent.ForkJoinTask.doExec(Unknown Source)\r\n    \tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.localPopAndExec(Unknown Source)\r\n    \tat java.base/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)\r\n    \tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)\r\n    java.lang.OutOfMemoryError: GC overhead limit exceeded\r\n    \tat com.google.devtools.build.lib.analysis.configuredtargets.RuleConfiguredTarget.<init>(RuleConfiguredTarget.java:115)\r\n    \tat com.google.devtools.build.lib.analysis.configuredtargets.RuleConfiguredTarget.<init>(RuleConfiguredTarget.java:142)\r\n    \tat com.google.devtools.build.lib.analysis.RuleConfiguredTargetBuilder.build(RuleConfiguredTargetBuilder.java:174)\r\n    \tat com.google.devtools.build.lib.rules.python.PyBinary.create(PyBinary.java:53)\r\n    \tat com.google.devtools.build.lib.rules.python.PyBinary.create(PyBinary.java:36)\r\n    \tat com.google.devtools.build.lib.analysis.ConfiguredTargetFactory.createRule(ConfiguredTargetFactory.java:323)\r\n    \tat com.google.devtools.build.lib.analysis.ConfiguredTargetFactory.createConfiguredTarget(ConfiguredTargetFactory.java:207)\r\n    \tat com.google.devtools.build.lib.skyframe.SkyframeBuildView.createConfiguredTarget(SkyframeBuildView.java:636)\r\n    \tat com.google.devtools.build.lib.skyframe.ConfiguredTargetFunction.createConfiguredTarget(ConfiguredTargetFunction.java:783)\r\n    \tat com.google.devtools.build.lib.skyframe.ConfiguredTargetFunction.compute(ConfiguredTargetFunction.java:326)\r\n    \tat com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:422)\r\n    \tat com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:368)\r\n    \tat java.base/java.util.concurrent.ForkJoinTask$AdaptedRunnableAction.exec(Unknown Source)\r\n    \tat java.base/java.util.concurrent.ForkJoinTask.doExec(Unknown Source)\r\n    \tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.localPopAndExec(Unknown Source)\r\n    \tat java.base/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)\r\n    \tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)\r\n    GC overhead limit exceeded\r\n    \r\n    ERROR: bazel ran out of memory and crashed.\r\n    FAILED: Build did NOT complete successfully (561 packages loaded, 37254 targets configured)\r\n\r\nMy second attempt reached more targets configured, but seems stuck or very slow after running for 4 hours as it is now incrementing targets one by one:\r\n\r\n```\r\nAnalyzing: 7629 targets (561 packages loaded, 35347 targets configured)\r\nAnalyzing: 7629 targets (561 packages loaded, 35347 targets configured)\r\nAnalyzing: 7629 targets (561 packages loaded, 35348 targets configured)\r\nAnalyzing: 7629 targets (561 packages loaded, 35349 targets configured)\r\nAnalyzing: 7629 targets (561 packages loaded, 35351 targets configured)\r\nAnalyzing: 7629 targets (561 packages loaded, 35354 targets configured)\r\nAnalyzing: 7629 targets (561 packages loaded, 35358 targets configured)\r\nAnalyzing: 7629 targets (561 packages loaded, 35361 targets configured)\r\nAnalyzing: 7629 targets (561 packages loaded, 35365 targets configured)\r\n```\r\n\r\nThe Docker container was at the first of these lines after 1 hour, so the others took 3 hours.\r\n\r\nHow long should TensorFlow unit tests take to run? Has anyone succeeded in running them on Azure, and if so, which image and machine did you use?\r\n\r\n**Any other info / logs**\r\nI attach the two log for the crash and for the long time running:\r\n\r\n[TensorFlow unit test crash log output.txt](https://github.com/tensorflow/tensorflow/files/2765390/TensorFlow.unit.test.crash.log.output.txt)\r\n[long time log.txt](https://github.com/tensorflow/tensorflow/files/2765391/long.time.log.txt)\r\n", "comments": ["1.75 GB is very likely not enough memory to compile and test TensorFlow, especially not in a reasonable amount of time. Try using an Azure instance with a lot more memory, or possibly limit Bazel's memory usage by changing the [Bazel Build Options noted on this page](https://www.tensorflow.org/install/source#linux).\r\n\r\nIf you still have trouble trying to handle low-memory environments, this issue would be much more applicable as a Stack Overflow question, where your question is more likely to help other developers who run into similar problems."]}, {"number": 24970, "title": "Fix softmax_cross_entropy issue", "body": "This fix is from #24397/#24736 and all credit goes to @felix-schneider (\ud83d\udc4d )\r\n\r\nThis fix fixes #24397. This fix fixes #24736.\r\n\r\n/cc @felix-schneider", "comments": []}, {"number": 24969, "title": "Inconsistencies between tf.contrib.layer.fully_connected, tf.layers.dense, tf.contrib.slim.fully_connected, tf.keras.layers.Dense", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Mojave (10.14.2 (18C54))\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): `pip install tensorflow`\r\n- TensorFlow version (use command below): v1.12.0-rc2-3-ga6d8ffae09 1.12.0\r\n- Python version: Python 3.5.6 :: Anaconda, Inc.\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\nI am trying to implement policy gradient for a contextual bandit problem (https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-1-5-contextual-bandits-bff01d1aad9c).\r\n\r\nI am defining a model in tensorflow to solve this problem using a single **fully-connected** layer.\r\n\r\nI am trying out different APIs from tensorflow, but want to avoid using the `contrib` package since it is not tensorflow-supported. I am interested in using the `keras` API since I am already familiar with the functional interface, and it is now implemented as `tf.keras`. However, I can only seem to get results to work when using `tf.contrib.slim.fully_connected`, or `tf.contrib.layers.fully_connected` (the former calls the latter).\r\n\r\n\r\nThe following two snippets work correctly (`one_hot_encoded_state_input` and `num_actions` both adhere to the expected tensor shapes for the layers).\r\n\r\n```python\r\nimport tensorflow.contrib.slim as slim\r\naction_probability_distribution = slim.fully_connected(\r\n    one_hot_encoded_state_input, \\\r\n    num_actions, \\     \r\n    biases_initializer=None, \\\r\n    activation_fn=tf.nn.sigmoid, \\\r\n    weights_initializer=tf.ones_initializer())\r\n```\r\n\r\nand \r\n\r\n```python\r\nfrom tensorflow.contrib.layers import fully_connected\r\naction_probability_distribution = fully_connected(\r\n    one_hot_encoded_state_input,\r\n    num_actions,\\\r\n    biases_initializer=None, \\\r\n    activation_fn=tf.nn.sigmoid, \\\r\n    weights_initializer=tf.ones_initializer())\r\n```\r\n\r\nOn the other hand, neither of the following work:\r\n\r\n```python\r\naction_probability_distribution = tf.layers.dense(\r\n    one_hot_encoded_state_input, \\\r\n    num_actions, \\\r\n    activation=tf.nn.sigmoid, \\\r\n    bias_initializer=None, \\\r\n    kernel_initializer=tf.ones_initializer())\r\n```\r\n\r\nnor\r\n\r\n```python\r\naction_probability_distribution = tf.keras.layers.Dense(\r\n    num_actions, \\\r\n    activation='sigmoid', \\\r\n    bias_initializer=None, \\\r\n    kernel_initializer = 'Ones')(one_hot_encoded_state_input)\r\n```\r\n\r\nThe last two cases use tensorflow's high level APIs `layers` and `keras`. Ideally, I would like to know if **I am incorrectly implementing the first two cases using the last two cases**, and if the only issue I am having is that **the latter two are not equivalent to the former two**.\r\n\r\nFor completeness, here is the entire code needed to run this (Note: python 3.5.6 and tensorflow 1.12.0 were used).\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\ntf.reset_default_graph()\r\n\r\nnum_states = 3\r\nnum_actions = 4\r\nlearning_rate = 1e-3\r\n\r\nstate_input = tf.placeholder(shape=(None,),dtype=tf.int32, name='state_input')\r\none_hot_encoded_state_input = tf.one_hot(state_input, num_states)\r\n\r\n# DOESN'T WORK\r\naction_probability_distribution = tf.keras.layers.Dense(num_actions, activation='sigmoid', bias_initializer=None, kernel_initializer = 'Ones')(one_hot_encoded_state_input)\r\n\r\n# WORKS\r\n# import tensorflow.contrib.slim as slim\r\n# action_probability_distribution = slim.fully_connected(one_hot_encoded_state_input,num_actions,\\\r\n#     biases_initializer=None,activation_fn=tf.nn.sigmoid,weights_initializer=tf.ones_initializer())\r\n\r\n# WORKS\r\n# from tensorflow.contrib.layers import fully_connected\r\n# action_probability_distribution = fully_connected(one_hot_encoded_state_input,num_actions,\\\r\n#     biases_initializer=None,activation_fn=tf.nn.sigmoid,weights_initializer=tf.ones_initializer())\r\n\r\n# DOESN'T WORK\r\n# action_probability_distribution = tf.layers.dense(one_hot_encoded_state_input,num_actions, activation=tf.nn.sigmoid, bias_initializer=None, kernel_initializer=tf.ones_initializer())\r\n\r\naction_probability_distribution = tf.squeeze(action_probability_distribution)\r\naction_chosen = tf.argmax(action_probability_distribution)\r\n\r\nreward_input = tf.placeholder(shape=(None,), dtype=tf.float32, name='reward_input')\r\naction_input = tf.placeholder(shape=(None,), dtype=tf.int32, name='action_input')\r\nresponsible_weight = tf.slice(action_probability_distribution, action_input, [1])\r\nloss = -(tf.log(responsible_weight)*reward_input)\r\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\r\nupdate = optimizer.minimize(loss)\r\n\r\n\r\nbandits = np.array([[0.2,0,-0.0,-5],\r\n                    [0.1,-5,1,0.25],\r\n                    [-5,5,5,5]])\r\n\r\nassert bandits.shape == (num_states, num_actions)\r\n\r\ndef get_reward(state, action): # the lower the value of bandits[state][action], the higher the likelihood of reward\r\n    if np.random.randn() > bandits[state][action]:\r\n        return 1\r\n    return -1\r\n\r\nmax_episodes = 10000\r\nepsilon = 0.1\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    rewards = np.zeros(num_states)\r\n    for episode in range(max_episodes):\r\n        state = np.random.randint(0,num_states)\r\n        action = sess.run(action_chosen, feed_dict={state_input:[state]})\r\n        if np.random.rand(1) < epsilon:\r\n            action = np.random.randint(0, num_actions)\r\n\r\n        reward = get_reward(state, action)\r\n        sess.run([update, action_probability_distribution, loss], feed_dict = {reward_input: [reward], action_input: [action], state_input: [state]})\r\n        \r\n        rewards[state] += reward\r\n        \r\n        if episode%500 == 0:\r\n            print(rewards)\r\n```\r\n\r\nWhen using the chunks commented `# THIS WORKS`, the agent learns and maximizes reward across all three states. On the other hand, those commented `# THIS DOESN'T WORK# don't learn and typically converge extremely quickly to choosing one action.\r\n\r\n\r\n\r\nQuestion asked on stackoverflow: https://stackoverflow.com/questions/54221778/inconsistencies-between-tf-contrib-layer-fully-connected-tf-layers-dense-tf-co\r\n\r\nI will link here if it is answered.", "comments": ["Wow, quite the long-winded post. It turns out it comes down to bias initialization differences. `tf.contrib` (and by extension`tf.contrib.slim`) do **not** use bias if `bias_initializer` is set to `None`, whereas `tf.layers` and `tf.keras.layers` **do**. "]}, {"number": 24968, "title": "tf.data.Dataset.from_tensor_slices().batch() broken when batch size equals dataset size", "body": "**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution: OS X 10.13.6 \r\n- Mobile device if the issue happens on mobile device: X\r\n- TensorFlow installed from: Binary\r\n- TensorFlow version: 1.12.0\r\n- Python version: 3.6.7\r\n- Bazel version:x\r\n- GCC/Compiler version: x\r\n- CUDA/cuDNN version: x\r\n- GPU model and memory: x\r\n\r\n**Describe the current behavior**\r\nWhen using tf.data.Dataset.from_tensor_slices(Data).batch(batchsize) using a batchsize which equals to the number of samples,  I get an out of range error:\r\n```\r\nOutOfRangeError: End of sequence\r\n\t [[{{node IteratorGetNext}} = IteratorGetNext[output_shapes=[[?,2], [?,1]], output_types=[DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](IteratorV2)]]\r\n```\r\nI think this due to the iterator wanting to construct a second batch, but there's nothing left due to the size of the first batch. In other words, it's impossible right now (I think) to use the tf.data pipeline without minibatching.\r\n\r\n**Describe the expected behavior**\r\nNo error; the iterator produces a single batch.\r\n\r\n**Code to reproduce the issue**\r\n```\r\ndataset = tf.data.Dataset.from_tensor_slices(np.arange(10)).batch(10)\r\niterator = tf.data.Iterator.from_structure(dataset.output_types, dataset.output_shapes)\r\nx = iterator.get_next()\r\n\r\nuse_test_set = iterator.make_initializer(dataset)\r\nwith tf.Session() as sess:\r\n    sess.run(use_test_set)\r\n    print(sess.run(x)) #This runs\r\n    print(sess.run(x)) #This line gives an error\r\n```\r\n**Other info / logs**\r\nNone.\r\n", "comments": ["Do you mean that `OutOfRangeError` is raised when you invoke `get_next()` twice?\r\nIf yes,  the result is expected because all data have been used up. If you use other high level modules, like tf.keras or tf.estimator, those modules always catch the Exception and then exit. You can also catch it by yourself if necessary:\r\n```python\r\n  while True:\r\n    try:\r\n      sess.run(next_element)\r\n    except tf.errors.OutOfRangeError:\r\n      break\r\n```", "Thanks for your answer! I am confused with the data API: I was expecting the iterator to return again the first batch. (I just checked the documentation and saw that this can be done by .repeat())\r\n\r\n Your answer also implies that the number of training epochs can not be bigger than dataset_size/batch_size, but this is not what I see happening when training my models (in my case, adam optimizer). Do you have any idea why this would not throw an error?", "@GJBoth  It would be great if you can provide a small code to reproduce the error. Please provide as many details as possible to find the root cause of the issue. Thanks!\r\n", "So what I tried doing was having a dataset for my data using the API, combined with a feed_dict for a mask I apply, like this:\r\n\r\n```\r\ntrain_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(BATCH_SIZE)\r\niterator = tf.data.Iterator.from_structure(train_dataset.output_types, train_dataset.output_shapes)\r\nX_tf, y_tf = iterator.get_next()\r\n\r\nuse_train_set = iterator.make_initializer(train_dataset)\r\n\r\n# Defining the mask\r\nsparsity_mask = tf.placeholder(tf.int32, shape=[16, 1]) # 16 is the total number of terms   \r\n```\r\n\r\nNow, when I look at what data the iterator returns:\r\n```\r\nfor it in np.arange(nIter):\r\n  \r\n    sess.run(use_train_set)\r\n    print(sess.run(X_tf, feed_dict=mask_dict)[0,:])\r\n```\r\n\r\nthey're all the same, meaning that the iterator resets when used in combination with a feed_dict. Is that expected behaviour?", "@GJBoth : Your example code (re)initializes the iterator inside the loop. Initializing the dataset once before the loop should yield the desired behavior.\r\n\r\n```python\r\nsess.run(use_train_set)\r\nfor it in np.arange(nIter):\r\n    print(sess.run(X_tf, feed_dict=mask_dict)[0,:])\r\n```", "Closing this because @suphoff has figured out the problem (thanks!) and there doesn't seem to be a bug in the underlying library."]}, {"number": 24967, "title": "[INTEL MKL] Fix for failure in //tensorflow/core/kernels:eigen_spatial_convolutions_test", "body": "Added missing definition to fix compilation errors when using --config=mkl  + Fixes for clang format errors.", "comments": []}, {"number": 24966, "title": "What input pre-processing is required for TensorFlow Lite pre-trained models", "body": "TensorFlow Lite framework has some pre-trained models here https://www.tensorflow.org/lite/models\r\n\r\nHow do I know what pre-processing to input of the network is needed? (scale, mean values)\r\nI'm talking about .pb file with FakeQuant* operation.\r\nI need this to understand where those `min` and `max` parameters of FakeQuant* operation trained with pre-processing or not \r\n\r\nThank you for your answer!", "comments": ["lol, nice username :)\r\n\r\nThe tensorflow model (.pb) with FakeQuant ops requires the same preprocessing as the normal float model. So for the image models that is normal inception_preprocessing in the range [0.0f, 1.0f]: https://github.com/tensorflow/models/blob/master/research/slim/preprocessing/inception_preprocessing.py#L256\r\n\r\nSome extra info:\r\n\r\nThe actually quantized uint8 tflite model expects uint8 inputs from [0, 255]. Tensorflow Lite requires information on how that maps to the float [0.0, 1.0] values. These are the std_values and mean_values flags. They can be set using the following.\r\n\r\nmean_value = the uint8 value that corresponds to floating point 0.0f. In this case the value is 0.\r\nstd_value = The number of uint8 values between 0.0 to 1.0. AKA integer range / float range.\r\n\r\nHope that helps!"]}, {"number": 24965, "title": "Added Randomized Leaky ReLU operator to Keras", "body": "Added advance activation function Randomized Leaky ReLU to keras layer", "comments": ["@tanzhenyu pls review the changes", "@tanzhenyu @hgadig  , can you pls have a look, i have updated everything as per your review comments", "> @tanzhenyu @hgadig , can you pls have a look, i have updated everything as per your review comments\r\n\r\nThanks. But can you please rebase as I can see there are branch conflicts. Rebase would solve this.", "@hgadig & @tanzhenyu , i have rebased, pylint and tested the code, kindly approve and merge", "> @hgadig & @tanzhenyu , i have rebased, pylint and tested the code, kindly approve and merge\r\n\r\n@tanzhenyu  Could you please take a look and approve this.", "@tanzhenyu & @hgadig , i have fixed all the issues pointed out, again did the pylint, (even spell check this time) and tested the code", "Also I think it makes more sense to place it in tensorflow addons, instead of tf.keras.\r\nPlease see: https://github.com/tensorflow/addons\r\n\r\n(In general we only maintain that in core once the api and implementation is near finalized)", "@tanzhenyu , this is advance activation function which is implmented by many frameworks including pytorch, you can refer to it and see that this activation function is put along with other activation functions, i feel this folder might be the correct place for the same, let me know what you feel.", "Nagging Reviewer @tanzhenyu: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 29 days with no activity and the `awaiting review` label has been applied.", "Same comment as before, we could put them in add-on.", "@tanzhenyu, thanks for the review, as suggested i am closing the PR.\r\n\r\nRegards \r\nAmit "]}, {"number": 24964, "title": "Change target name of tensorflow.dll in cmake installation", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.8\r\n- Python version: 3.6.3\r\n- Installed using virtualenv? pip? conda?: cmake\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): Visual Studio 2015\r\n- CUDA/cuDNN version: 9.0/7.0\r\n- GPU model and memory: GeForce GTX 1080 Ti (11 GB)\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI am trying to build two different versions of TensorFlow 1.8 (one with GPU support and one without). Even if it is deprecated, I use the cmake installation since I had a lot of troubles with bazel. And the cmake installation allows to directly build a shared library as well as a tensorflo.dll file, which suits our need for integration in our applications in Windows.\r\n\r\nHowever, the two versions (with or without GPU) output a dll with the same name (tensorflow.dll), and we will need to have the two different dlls in our applications. I was searching for a way to change the target name (tensorfflow_cpu.dll and tensorflow_gpu.dll for instance), is it possible? I already tried to change the target name in the Visual Studio projet (i.e. tensorflow), but the .lib seems to always reference tensorflow.dll.\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24964\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24964\">No</a>\n", "Closing out this issue because it hasn't been updated in the last year.  Please reopen if this is still relevant."]}]