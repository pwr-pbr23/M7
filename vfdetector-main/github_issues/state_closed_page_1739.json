[{"number": 727, "title": "ci_build: improve user handling inside the container", "body": "- create user home with .bazelrc (this is needed for slaves)\n- create and use user based on uid instead of user name\n- add bazel-ci_build-cache to .gitignore\n- add optional extra ci build parameters (this is good for hacky jobs like git bisect for bazel)\n", "comments": []}, {"number": 726, "title": "Segmentation fault for cifar10_train.py and minist/convolutional.py using 0.6.0 on ec2", "body": "I installed tensorflow but running either the cifar10 or the mnist convolution result in a segmentation fault. It looks like a out of memory error, but the prints say they are `Allocating 3.66GiB bytes` and the GRID K520 has 3.95GiB free. Also both scripts have been run by other people successfully on ec2 instances.\n\nFor the installation I used erikbern's  [install-tensorflow.sh](https://gist.github.com/erikbern/78ba519b97b440e10640), but used the cuda 7.0 deb installer and installed Tensorflow 0.6.0 instead of 0.5.0\n\nWhat additional data would be helpful or needed?\n\nTensorflow version: 0.6.0\n\n``` bash\nludwig@rnd-development-02:/mnt/tmp$ python -c'import tensorflow; print tensorflow.__version__'\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcublas.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcudnn.so.6.5 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcufft.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcurand.so.7.0 locally\n0.6.0\n```\n\nRunning cifar10_train.py\n\n``` bash\nludwig@rnd-development-02:/mnt/tmp/tensorflow/tensorflow/models/image/cifar10$ python cifar10_train.py\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcublas.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcudnn.so.6.5 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcufft.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcurand.so.7.0 locally\nFilling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:909] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:103] Found device 0 with properties:\nname: GRID K520\nmajor: 3 minor: 0 memoryClockRate (GHz) 0.797\npciBusID 0000:00:03.0\nTotal memory: 4.00GiB\nFree memory: 3.95GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:127] DMA: 0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:137] 0:   Y\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:706] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GRID K520, pci bus id: 0000:00:03.0)\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:43] Allocating 3.66GiB bytes.\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] GPU 0 memory begins at 0x7022a0000 extends to 0x7ec588000\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 1.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 2.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 4.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 8.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 16.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 32.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 64.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 128.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 256.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 512.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 1.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 2.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 4.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 8.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 16.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 32.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 64.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 128.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 256.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 512.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 1.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 2.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 4.00GiB\nF tensorflow/stream_executor/cuda/cuda_driver.cc:299] failed to query current context: CUDA_ERROR_DEINITIALIZED\nF tensorflow/stream_executor/cuda/cuda_driver.cc:408] Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(prior_context_) (0 vs. 4)\nAborted (core dumped)\n```\n\nRunning convolutional.py\n\n``` bash\nludwig@rnd-development-02:/mnt/tmp/tensorflow/tensorflow/models/image/cifar10$ python tensorflow/models/image/mnist/convolutional.py python: can't open file 'tensorflow/models/image/mnist/convolutional.py': [Errno 2] No such file or directory\nludwig@rnd-development-02:/mnt/tmp/tensorflow/tensorflow/models/image/cifar10$ cd ../../\nludwig@rnd-development-02:/mnt/tmp/tensorflow/tensorflow/models$ cd ../../\nludwig@rnd-development-02:/mnt/tmp/tensorflow$ python tensorflow/models/image/mnist/convolutional.py\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcublas.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcudnn.so.6.5 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcufft.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcurand.so.7.0 locally\nSuccessfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\nSuccessfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\nSuccessfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\nSuccessfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:909] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:103] Found device 0 with properties:\nname: GRID K520\nmajor: 3 minor: 0 memoryClockRate (GHz) 0.797\npciBusID 0000:00:03.0\nTotal memory: 4.00GiB\nFree memory: 3.95GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:127] DMA: 0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:137] 0:   Y\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:706] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GRID K520, pci bus id: 0000:00:03.0)\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:43] Allocating 3.66GiB bytes.\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] GPU 0 memory begins at 0x7022a0000 extends to 0x7ec588000\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 1.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 2.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 4.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 8.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 16.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 32.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 64.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 128.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 256.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 512.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 1.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 2.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 4.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 8.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 16.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 32.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 64.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 128.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 256.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 512.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 1.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 2.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 4.00GiB\nSegmentation fault (core dumped)\n```\n", "comments": ["Oh, just noticed a very similar (maybe) dupe error in #713, which I overlooked because I searched for `segmentation fault` and not `CUDA_ERROR_DEINITIALIZED`. So this is probably a dupe then.\n", "Yeah, likely a dupe of #713, we can re-open if we find out there's another issue here.\n"]}, {"number": 725, "title": "Packages in ~/tensorflow/tensorflow/models/image", "body": "If we take a look at perhaps ~/tensorflow/tensorflow/models/image/cifar10/cifar10_train.py, there's a line\n\nfrom tensorflow.models.image.cifar10 import cifar10\n\nThis works perfectly, and since there's a **init**.py in each directory, it can be seen as somewhat a package. When I try to create my own projects in this directory, it does not work. For example:\n\n~/tensorflow/tensorflow/models/image/my_project/, such that\n\nmy_project/\n    **init**.py\n    code1.py\n    code2.py\n\nWhen I try to do the following in code1.py, it does not work:\n\nfrom tensorflow.models.image.my_project import code2\n\n*Another interesting to note, is that deleting all **init**.py in the various directories still allow tensorflow.models.image.cifar10 import cifar10 to work. There's something more to this that I'm not getting. Does anyone have this problem too?\n\n**P.S. The init has two underscores in front and behind, the markdown removes it.\n", "comments": ["Maybe the tensorflow dir you are talking about is not the same as the one tensorflow is actually using. I also think this kind of question is better suited to stackoverflow: https://stackoverflow.com/questions/tagged/tensorflow\n", "It's the same dir I believe. Alright I'll close this and put the question in stackoverflow.\n"]}, {"number": 723, "title": "failed to query current context: CUDA_ERROR_DEINITIALIZED", "body": "mlds-ws2@mldsws2-ThinkStation-P500:~/tensorflow/tensorflow/models/image/mnist$ python convolutional.py \nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcublas.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcudnn.so.6.5 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcufft.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcurand.so.7.0 locally\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:103] Found device 0 with properties: \nname: GeForce GTX TITAN X\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\npciBusID 0000:03:00.0\nTotal memory: 12.00GiB\nFree memory: 11.67GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:127] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:137] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:705] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:03:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Allocating 11.09GiB bytes.\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:52] GPU 0 memory begins at 0xb06c80000 extends to 0xdcc64a99a\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 1.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 2.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 4.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 8.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 16.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 32.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 64.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 128.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 256.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 512.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 1.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 2.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 4.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 8.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 16.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 32.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 64.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 128.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 256.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 512.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 1.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 2.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 4.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 8.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 16.00GiB\nF tensorflow/stream_executor/cuda/cuda_driver.cc:408] Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(prior_context_) (0 vs. 4)\nF tensorflow/stream_executor/cuda/cuda_driver.cc:299] failed to query current context: CUDA_ERROR_DEINITIALIZED\nAborted (core dumped)\n\n---\n\nAfter installing tensorflow, I executed mnist example. \nWhat is this error?;;; this error is similar to #713.\nHow do I fix this problem? \nPlease help me...\n", "comments": ["Dupe of #713\n"]}, {"number": 722, "title": "Problems training the cnn model (Python 2.7.6rc1, Red Hat 4.4.7-16, 64bit) ", "body": "I'm getting the following error when trying to train a cnn model:\n\n```\n_pywrap_tensorflow.so: undefined symbol: cudnnCreate\n```\n\nFull traceback:\n\n```\n[root@localhost cnn_test]# python test_train.py\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcublas.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:93] Couldn't open CUDA library libcudnn.so.6.5. LD_LIBRARY_PATH: /usr/local/cuda-7.0/lib64\nI tensorflow/stream_executor/cuda/cuda_dnn.cc:1382] Unable to load cuDNN DSO\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcufft.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcurand.so.7.0 locally\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:127] DMA: 0 1 2 3 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:137] 0:   Y Y Y Y \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:137] 1:   Y Y Y Y \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:137] 2:   Y Y Y Y \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:137] 3:   Y Y Y Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:702] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K20c, pci bus id: 0000:02:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:702] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla K20c, pci bus id: 0000:04:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:702] Creating TensorFlow device (/gpu:2) -> (device: 2, name: Tesla K20c, pci bus id: 0000:83:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:702] Creating TensorFlow device (/gpu:3) -> (device: 3, name: Tesla K20c, pci bus id: 0000:84:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Allocating 4.31GiB bytes.\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:52] GPU 0 memory begins at 0x130fb80000 extends to 0x1423e4d000\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 1.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 2.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 4.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 8.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 16.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 32.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 64.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 128.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 256.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 512.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 1.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 2.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 4.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 8.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 16.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 32.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 64.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 128.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 256.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 512.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 1.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 2.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 4.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 8.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Allocating 4.31GiB bytes.\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:52] GPU 1 memory begins at 0x1423e60000 extends to 0x15380e5000\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 1.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 2.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 4.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 8.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 16.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 32.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 64.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 128.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 256.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 512.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 1.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 2.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 4.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 8.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 16.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 32.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 64.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 128.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 256.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 512.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 1.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 2.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 4.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 8.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Allocating 4.31GiB bytes.\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:52] GPU 2 memory begins at 0x1538100000 extends to 0x164c33f000\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 1.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 2.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 4.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 8.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 16.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 32.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 64.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 128.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 256.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 512.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 1.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 2.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 4.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 8.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 16.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 32.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 64.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 128.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 256.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 512.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 1.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 2.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 4.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 8.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Allocating 4.31GiB bytes.\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:52] GPU 3 memory begins at 0x164c340000 extends to 0x176053d000\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 1.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 2.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 4.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 8.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 16.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 32.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 64.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 128.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 256.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 512.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 1.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 2.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 4.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 8.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 16.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 32.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 64.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 128.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 256.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 512.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 1.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 2.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 4.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 8.00GiB\nI tensorflow/core/common_runtime/direct_session.cc:58] Direct session inter op parallelism threads: 32\nF tensorflow/stream_executor/cuda/cuda_dnn.cc:207] could not find cudnnCreate in cudnn DSO; dlerror: /usr/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so: undefined symbol: cudnnCreate\nAborted (core dumped)\n```\n\n@FreakTheMighty  ,  I saw that you met the same problem. Can you tell me how to solve this problem,  appreciate your help very much\n", "comments": ["@BreathL I'm having trouble finding the issue where I posted something similar. If you can find it, it might help jog my memory. I did get things up and running, and I'm pretty sure any issues I had were the result of not reading the install directions carefully the first time.\n", "@FreakTheMighty  thank you\nthis problem has been solved. I reviewed the install directions, and found that the CUDNN 6.5 V2 is not properly installed. You can install like [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#optional-install-cuda-gpus-on-linux)\n", "Hi guys, I have the following error while running TensorFlow Demo code.\r\n\"undefined symbol: cudnnCreate\r\nAborted (core dumped)\"\r\n\r\nI then reinstalled cuDNN v5 and even tried upgrade it to v5.1, Still couldn't fix it. Any ideas?\r\nI current have CUDA 8.0 with cuDNN 5.1, Tesla k80", "are you running the code from command line? try to run from command line. \r\nadd the following in your system include path.\r\n\r\nexport PATH=/usr/local/cuda/bin:$PATH\r\nexport LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:/usr/local/cuda/lib64\"\r\nexport CUDA_HOME=/usr/local/cuda", "Same here. Using tensorflow 0.12 and master (self-compiled from git and the release .whl from the tensorflow installation guide), I get this error when trying to use inceptionv3 after putting it through [optimize_for_inference.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/optimize_for_inference.py).\r\n\r\nIt can be worked-around, however, by using [0.12rc1](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3.5,label=gpu-linux/) from the nightly builds which solved the problem for now. ", "I found that solution. I changed cudnn version 6.0 to 5.1.", "+1 on the 5.1 vs 6.0.  Wasted a lot of time figuring that out.", "going back to 5.1 was also the solution for me. ", "as @yunjunglab said above, use cudnn version 5 solve the problem."]}, {"number": 721, "title": "Tensorflow API doc image is wrong for tf.scatter_*", "body": "In `tf.scatter_{add,sub,update}`, the image has `indices = [0, 2, 5]`, but the arrow suggest it's actually `[2, 0, 5]`.  We can't help fix it though since the images in the dos are not part of git.\n\nhttps://www.tensorflow.org/versions/master/api_docs/python/state_ops.html#scatter_update\n", "comments": ["I believe this was fixed, @colah can you confirm and close?\n", "@colah: Should this be closed?\n", "Posting for @colah: Fixed, so closing.\n"]}, {"number": 720, "title": "Make tensorflow and tensorflow-devel deb packages", "body": "Will you make it possible to install and update it by distributions package systems (deb & rpm)? You can make an official repository.\nWill you make it possible to install a Tensorflow-devel package, which gives acces to C++ headers to make programs, which link to Tensorflow?\n", "comments": ["You'd be able to dynamically link Tensorflow, while building it,  to libraries available by the distribution.\n", "We want to provide at least apt and homebrew packages. If someone would like to help out, we'd be thrilled. \n", "Also, C++ applications'd be able to use the latest version without having to recompile.\n", "Isn't that only true if we update the binary packages? Which realistically,\nwe'd only do for releases.\n\nOn Thu, Jan 7, 2016 at 4:07 PM Sherif89 notifications@github.com wrote:\n\n> Also, C++ applications'd be able to use the latest version without having\n> to recompile.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/720#issuecomment-169845846\n> .\n", "Yes, correct me, if wrong. If you dynamically link to a C++ library and the library is updated, you'll use the latest version of this library, which's Tensorflow's in this case.\n", "+1 for a yum repo containing this.\n", "Debian [ITP #804612](https://bugs.debian.org/804612) blocked by Bazel [ITP #782654](https://bugs.debian.org/782654)\n", "/cc @CDLuminate\n", "@bhack  You're right.\n\nThings like splitting `libtensorflow` and `libtensorflow-devel` apart are done by the distributions instead of the upstream.\n\nIn order to get package `Tensorflow` into Debian's official archive, currently we need first to get `bazel` into the archive. However, if another build system is provided (Tensorflow #380 ), tensorflow gets rid of the blocker `bazel`.\n\nAs I'm holding Debian's ITP for tensorflow, I'm waiting for either an alternative build system, or the bazel being packaged and uploaded.\n", "Would it be necessary to add another package for Python API?\nYou can also add a doc package.\n", "In debian Bazel it is blocked by protobuf >= 3.0.0 [ITP #795841](https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=795841).\n", "What about Arch, OpenSUSE or Fedora?\n", "See [Arch](https://aur.archlinux.org/packages/?O=0&SeB=nd&K=tensorflow&outdated=&SB=n&SO=a&PP=50&do_Search=Go)\n", "> See Arch\n\nCan it be added to a repository to be installable by pacman? Why does it use Python 2 not 3?\n", "You can install these arch AUR packages with `yaourt`, see arch wiki for detail.\n\nArch packages (`pkgbuild`) always require less working hour on packaging, so you may find it very fast that arch community adopts new packages.\n\nSo please just be patient about the distro's progress, many contributors are working on it under  a non-profit circumstance. \n", "I'm very patient. I just hope it's part of its roadmap to make it:\n1. Easy to install & update like any package.\n2. Easy to link with C++ programs and the latest Python version.\n", "I'm sure that Debian will have at least `CUDA7.5+` and `Caffe` officially in this year (before or within Q2) but not sure for `Tensorflow`. And then Ubuntu devs will pull these Debian packages into Ubuntu devel. As for other distros I'm afraid I can provide no info.\n\n`1` must be in the roadmap. \n`2` is a bit complicated ....\n", "Dirty hacked packages come out very fast.\n", "What's necessary to package Tensorflow? What about defining steps?\nWould #1309 make it closer?\n", "#1309 would solve the bazel issue for Debian.\n\nOn Mon, Mar 7, 2016 at 10:00 PM Sherif89 notifications@github.com wrote:\n\n> What's necessary to package Tensorflow?\n> Would #1309 https://github.com/tensorflow/tensorflow/pull/1309 make it\n> closer?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/720#issuecomment-193620324\n> .\n", "OK, it would be great. When #1309 is merged, I can unblock tensorflow and then continue.\n", "When #1309 is fixed, I'll try out the cmake build on CentOS 7.  CentOS 7 doesn't have JDK 1.8 or bazel in its yum repo so using bazel to roll the rpm would be cumbersome.  Supposedly bazel works with 1.7 even though it is deprecated, but it wouldn't work for me.  \n\nRolling rpms should be much more straightforward after we can build with cmake.\n", "@ricochet2200 any update? i just need a spec file that can build tensor-flow. \ncentos6 , amd-64\n", "@iahmad-khan I haven't been following this bug closely.  Can you build with cmake now?  \n", "Just the core for now -- but it may still be interesting to make a tensorflow-core package from that. \n", "You can put it in the open build service. It supports many distributions (Ubuntu, Fedora, OpenSUSE,...).\n", "@martinwicke could you comment?", "It will become more interesting as we fix the problems with our linkage / symbol visibility. But for now, I'm reluctant to maintain more packages.\r\n\r\nI would still welcome contributions here. ", "Getting Tensorflow uploaded to the official debian repository is currently blocked because Bazel doesn't follow the Debian policy manual (https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=782654#45). But you could still provide deb packages even if they aren't accepted to upstream Debian.\r\n\r\nIf you are able to provide source packages for Tensorflow and all of its dependencies, then you could upload the packages to a PPA. Despite the bazel dependency, it sounds like this might be possible according to https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=782654#65.\r\n\r\n* DIGITS ([scripts](https://github.com/NVIDIA/DIGITS/tree/83254ed7a0/packaging/deb/templates), [build](https://github.com/NVIDIA/DIGITS/blob/83254ed7a09/packaging/deb/Dockerfile#L30-L32), [upload](https://github.com/NVIDIA/DIGITS/blob/83254ed7a09/scripts/travis/ppa-upload.sh#L25))\r\n* Ethereum ([scripts](https://github.com/ethereum/go-ethereum/tree/c008176f9f/build), [build+upload](https://github.com/ethereum/go-ethereum/blob/c008176f9f/build/ci.go#L456))\r\n\r\nOr, if you can't provide source packages for tensorflow or one of it's dependencies (e.g. if you're working on a CUDA build), then you could still build your own and host it on your own servers (like you currently do for Python wheels).\r\n\r\n* nvidia-docker ([scripts](https://github.com/NVIDIA/nvidia-docker/tree/v1.0.1/build/deb), [build](https://github.com/NVIDIA/nvidia-docker/blob/v1.0.1/Dockerfile.deb#L32-L37))", "@lukeyeager The most straightforward way to make a tensorflow deb package is to repack the official whl package as a deb package, in which the Bazel dependency is skipped.\r\n\r\nTensorflow packaging is in my todolist but with a low priority, since pip / conda already provided a quite convenient way to install. Currently some Debian developer is also interested in packaging tensorflow, and he is currently trying to solve the Bazel ITP.", "@CDLuminate: the TF package would be simple, but I realized today I would have to re-package a bunch of its dependencies, too. It looked like I was going to have to make new packages for `protobuf`, `weakref`, `bleach`, `html5lib`, `markdown`, and `werkzeug` (and possibly some of the sub-dependencies of those packages) in order to satisfy all the dependencies on 16.04. Yuck.", "Most of the js deps will go away (tensorboard is moving to a separate pip\npackage) so it could be somewhat simpler.\n", "@lukeyeager New Debian stable had just been released. Developers will upload new versions of their packages soon. AFAIK, for instance, the protobuf 3.3.1 packaging is already available but still not uploaded. Some new packages still stay in experimental.", "Issue: [Proposal: Making the cmake build distribution-friendly](https://github.com/tensorflow/tensorflow/issues/13061)", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "@cdluminate  I see you already started doing [some work](https://salsa.debian.org/science-team/tensorflow) on the packaging, is it already packageable? I tried running  dpkg-deb --build tensorflow but still get errors. An updated Readme would help.", "@mehditlili Well, without error log I cannot tell you anything useful.\r\n\r\nsalsa/science-team/tensorflow\r\n---\r\nThis repo holds the CPU version. For detail about progress please lookup README.Debian .\r\n\r\nI can build the package from the git repo you referred on at least two architectures (amd64,s390x). The latest result is that the package was successfully compiled on amd64 and x390x. Some other architectures failed to build due to OOM killer during parallel `ld` so I don't know if the package compiles on them.\r\n\r\nhttps://launchpad.net/~lumin0/+archive/ubuntu/ppa/+build/15309497\r\nhttps://launchpad.net/~lumin0/+archive/ubuntu/ppa/+build/15309502\r\n\r\nBesides, my targeting distribution is Debian experimental. However, it's impossible to write any documentation for user at this stage because this package is a really experimental WIP and there are still many pending choices.", "@cdluminate I would be happy to sponsor it in Debian if you need!\r\n", "> OOM killer during parallel ld\r\n\r\nSometimes, adding _-gsplit-dwarf_ in the CXXFLAGS fixes the linking issue. ", "@sylvestre Thank you for your attention but I'm already a DD :-) . I posted a pre-RFS to the mentors list because when the package is ready, I still want someone else to help me check if I'm doing anything stupid.", "@sylvestre I reuploaded the source to PPA with the `-gsplit-dwarf` flag and let's see what the difference will be. Thanks for the hint!", "@cdluminate I am trying to convert it into a  Ubuntu 16.04 package. Did you encounter any dependency problem when building this package, was that a reason why it was only done for Ubuntu 18?", "@mehditlili I'm moving to a new build system written from scratch with python plus ninja. And currently the debian directory is a bit messy. Please don't try to debug this messy WIP until I updated debian/rules for the new python+ninja build system.\r\n\r\nAs for dependency, yes indeed many system-shipped packages in Ubuntu 16.04 is too old to build tensorflow. If you encounter a build failure, try to embed a copy of newer source code to the package and use it. From my PPA you see only package for Ubuntu 18 is built because I don't care about old releases but only **Debian unstable/experimental**.", "@mehditlili My build system is nearly in shape. Is it working for you?\r\n\r\nhttps://lists.debian.org/debian-mentors/2018/09/msg00044.html", "@sylvestre I forgot to CC you in the latest mail.\r\n\r\nPreliminary binary packages available here:\r\n\r\nhttp://debomatic-amd64.debian.net/debomatic/experimental/pool/tensorflow_1.10.1+dfsg-A1u27/tensorflow_1.10.1+dfsg-A1u27_amd64.changes\r\n\r\nreadme: https://lists.debian.org/debian-mentors/2018/09/msg00052.html", "Closing the issue as per the solution provided above. Can reopen the issue if it still exists.", "Yes this issue has been resolved. The first upload of TF is currently waiting in Debian's upload queue.", "> Yes this issue has been resolved. The first upload of TF is currently waiting in Debian's upload queue.\r\n\r\nGreat !!", "> Closing the issue as per the solution provided above. Can reopen the issue if it still exists.\r\n\r\n@Harshini-Gadige looks like you can reopen.\r\nEspecially since I am completely failing to get it running on aarch64 which means only a binary package would solve the problem.\r\nAs a result, I can t run spleeter from Deezer.", "Any progress for Ubuntu 20.04?", "Is there any light in the tunnel?", "@Dilshat  I'm also waiting for. Here is the latest information about packages in Debian (Removed from experimental in 2019-08-27):\r\nhttps://tracker.debian.org/pkg/tensorflow\r\nhttps://salsa.debian.org/deeplearning-team/tensorflow/\r\n", "https://tracker.debian.org/news/1301497/accepted-tensorflow-231-1-source-amd64-into-experimental-experimental/\r\n\r\n2.3.1 re-entered experimental thanks to Wookey@debian.org"]}, {"number": 719, "title": "failed to enqueue async memcpy from device to host with latest code", "body": "This problem shows up after I did a git pull today to the last commit of Jan 6th. <63bddb>\nRoll back the the last commit of Jan 5th solves the issue\n\nfailed to enqueue async memcpy from device to host\ncheck failed: CUDA_SUCCESS == dynlod::cuCtxSetCurrent(prior_context_) (0 vs. 4)\n", "comments": ["+1. Similar to #713.\n", "Let's take discussion to that \n"]}, {"number": 718, "title": "Tensorflow not showing summaries anymore", "body": "On current master, the tensorboard will start as intended but only show the 'No scalar summaries found' error message. In the log, there are two errors:\n\n```\nWARNING:tensorflow:IOError [Errno 2] No such file or directory: '/usr/local/lib/python2.7/dist-packages/external/paper-tabs/paper-tabs.html' on path /usr/local/lib/python2.7/dist-packages/external/paper-t\nabs/paper-tabs.html\n\n... some GET\n\nWARNING:tensorflow:IOError [Errno 2] No such file or directory: '/usr/local/lib/python2.7/dist-packages/tensorflow/tensorboard/runs' on path /usr/local/lib/python2.7/dist-packages/tensorflow/tensorboard/runs\n```\n", "comments": ["+1\n", "Seems to be fixed by 5d824f1. However, I'm still having a lot of interface issues: the links in the top right don't work, and \"Split on underscores\" does nothing.\n", "Summaries are shown again, but the navigation is still broken. In the log there is still this error:\n\n```\nWARNING:tensorflow:IOError [Errno 2] No such file or directory: '/usr/local/lib/python2.7/dist-packages/external/paper-tabs/paper-tabs.html' on path /usr/local/lib/python2.7/dist-packages/external/paper-tabs/paper-tabs.html\n```\n\nAnd clicking on e. g. `images` does redirect to `http://localhost:6006/#null`. Manually entering `http://localhost:6006/#images` works.\n", "Thanks for the report. The issue is that we took a dependency on paper-tabs but didn't update the root.workspace rule to download paper-tabs (there was a bug in the synchronization script). It's fixed now and should make its way to master shortly.\n"]}, {"number": 717, "title": "Bazel stuck during build tensorflow.", "body": "mlds-ws2@mldsws2-ThinkStation-P500:~/tensorflow$ bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\n.......\nINFO: Loading package: \n\n---\n\nAnd stop. After 1~2 hours, doesn't work anything.\nI have 3 computer. \n1 is server. 2 CPU and TITAN X 3EA. -> This machine does work well.\nBut 2, 3 are desktop. 1 CPU and TITAN X or GTX 980 1EA -> Stuck while build tensorflow.\nI did format and reinstall ubuntu and necessary program, 3 times. Results are same.\nPlease help me.\n", "comments": ["I've had some strange java-related issues with Ubuntu that caused similar behavior. Doing a complete shutdown (not a restart) and then rebooting after waiting 10-15 seconds seems to work for me (no idea why).\n", "possible duplicate of: https://github.com/tensorflow/tensorflow/issues/628\n\ndid u try the fix in that thread? remove tensorboard from the BUILD\n", "Thnks skearnes. I solved this problem through cold starting.\n", "can confirm... weirdiest bug, cold start fixes problem but reboot does not....\n", "@lberki, @davidzchen, @damienmg in case they know the cause of these hangs, or at least are aware of them.\n", "Can we have the full log of such a problem? Maybe with -s?\nAlso `bazel info` and more information about the system (e.g. number of execution unit, amount of RAM, system version) would be useful.\n", "Closing since this affects an older version of bazel. If it persists, please file a bug with bazel including debug information.\n", "I hit bazel hangs on Ubuntu that looked similar. In my case, there were a bunch of \"linux-sandbox\" processes that were in uninterruptible sleep ('D' state shown in ps). Looking at stack traces, the linux-sandbox processes were hung in the kernel in the NFS client.\r\n\r\nI had a stale NFS mount pointed at a server that was no longer live. I've had the system in this state for weeks, but it didn't impact anything I was doing. I brought up an NFS server on the IP address where NFS mount was pointed to, and this allowed me to unmount. After that, bazel started running without issues.\r\n\r\nI didn't investigate further after resolving my issue, but I suspect that as a part of creating the sandbox, user namespace creation enumerates the mounts. If the system has a hung mount, this will hang bazel. This was surprising to me (the user) because the hung mount had no relation to the directory where I was running bazel."]}, {"number": 716, "title": "TensorBoard maybe timeout when show imagenet inception-3 event", "body": "when i run imagenet example ; in tensorflow/models/image/imagenet/classify_image.py i add\n-    w=tf.python.training.summary_io.SummaryWriter( sess.graph_def, FLAGS.model_dir, 'graph.pbtxt')  \n-    tf.train.SummaryWriter.flush(w)\n-    tf.train.SummaryWriter.close(w)\n\noutput a file: events.out.tfevents.1452154974.ip-172-30-.-.\nwhen i open tensorboard , in tab  graph ,then i check top,tensorboard use cpu 100%; but wait a while ,tensorboard cpu use is down; tensorboard log a http 200:\"GET /graph?run=%2Fopt%2Fpractice%2Ftf%2Fimagenet%2Flog%2F HTTP/1.1\" 200 ;\n\nmaybe event file so big cause http timeout?? \n92M Jan  7 09:01 log/events.out.tfevents.1452154974.ip-172-30\n\nany one can help?\n", "comments": ["i will check cifar-10 example for what missing?\n", "@zdx3578 so the graph page displays nothing?\n", "@danmane: Do you know if this is still an issue? \n", "This looks like a duplicate of #1287 , which is fixed as of latest master. @zdx3578 if the fix for #1287 doesn't work for you, feel free to re-open and we'll dig into it a bit more - if you do re-open it, can you please upload an events file for which you aren't able to visualize the graph? That will make it a lot easier to dig into.\n"]}, {"number": 715, "title": "Android Demo build Error", "body": "Get an error when building the Android demo on Ubuntu 14.04.\n\ntp@tp-Inspiron-3421:~/tensorflow$ /home/tp/bin/bazel build tensorflow/examples/android:tensorflow_demo --verbose_failures\nINFO: Found 1 target...\nINFO: From Compiling google/protobuf/src/google/protobuf/util/internal/utility.cc:\ngoogle/protobuf/src/google/protobuf/util/internal/utility.cc:50:19: warning: 'const google::protobuf::StringPiece google::protobuf::util::converter::{anonymous}::SkipWhiteSpace(google::protobuf::StringPiece)' defined but not used [-Wunused-function]\n const StringPiece SkipWhiteSpace(StringPiece str) {\n                   ^\nINFO: From Compiling google/protobuf/src/google/protobuf/util/internal/field_mask_utility.cc:\ngoogle/protobuf/src/google/protobuf/util/internal/field_mask_utility.cc:47:14: warning: 'google::protobuf::util::Status google::protobuf::util::converter::{anonymous}::CreatePublicError(google::protobuf::util::error::Code, const string&)' defined but not used [-Wunused-function]\n util::Status CreatePublicError(util::error::Code code,\n              ^\nINFO: From Compiling external/re2/re2/stringpiece.cc:\nexternal/re2/re2/stringpiece.cc:6:23: fatal error: util/util.h: No such file or directory\n #include \"util/util.h\"\n                       ^\ncompilation terminated.\nERROR: /home/tp/.cache/bazel/_bazel_tp/52515bfa99a2d82b1037ec6d5029c6c1/external/re2/BUILD:7:1: C++ compilation of rule '@re2//:stringpiece' failed: arm-linux-androideabi-gcc failed: error executing command \n  (cd /home/tp/.cache/bazel/_bazel_tp/52515bfa99a2d82b1037ec6d5029c6c1/tensorflow && \\\n  exec env - \\\n    PATH=/home/tp/bin:/home/tp/android-ndk-r10e:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/java/jdk1.8.0_65/bin \\\n  external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-gcc -fstack-protector-strong -fpic -ffunction-sections -funwind-tables -no-canonical-prefixes '-march=armv7-a' '-mfpu=vfpv3-d16' '-mfloat-abi=softfp' -iquote external/re2 -iquote bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/genfiles/external/re2 -iquote external/bazel_tools -iquote bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/genfiles/external/bazel_tools -isystem external/re2 -isystem bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/genfiles/external/re2 -isystem external/bazel_tools/tools/cpp/gcc3 '--sysroot=external/androidndk/ndk/platforms/android-21/arch-arm' -isystem external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9/include -isystem external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9/include-fixed -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/backward '-frandom-seed=bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/bin/external/re2/_objs/stringpiece/external/re2/re2/stringpiece.o' -MD -MF bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/bin/external/re2/_objs/stringpiece/external/re2/re2/stringpiece.d -c external/re2/re2/stringpiece.cc -o bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/bin/external/re2/_objs/stringpiece/external/re2/re2/stringpiece.o): arm-linux-androideabi-gcc failed: error executing command \n  (cd /home/tp/.cache/bazel/_bazel_tp/52515bfa99a2d82b1037ec6d5029c6c1/tensorflow && \\\n  exec env - \\\n    PATH=/home/tp/bin:/home/tp/android-ndk-r10e:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/java/jdk1.8.0_65/bin \\\n  external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-gcc -fstack-protector-strong -fpic -ffunction-sections -funwind-tables -no-canonical-prefixes '-march=armv7-a' '-mfpu=vfpv3-d16' '-mfloat-abi=softfp' -iquote external/re2 -iquote bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/genfiles/external/re2 -iquote external/bazel_tools -iquote bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/genfiles/external/bazel_tools -isystem external/re2 -isystem bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/genfiles/external/re2 -isystem external/bazel_tools/tools/cpp/gcc3 '--sysroot=external/androidndk/ndk/platforms/android-21/arch-arm' -isystem external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9/include -isystem external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9/include-fixed -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/backward '-frandom-seed=bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/bin/external/re2/_objs/stringpiece/external/re2/re2/stringpiece.o' -MD -MF bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/bin/external/re2/_objs/stringpiece/external/re2/re2/stringpiece.d -c external/re2/re2/stringpiece.cc -o bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/bin/external/re2/_objs/stringpiece/external/re2/re2/stringpiece.o).\nTarget //tensorflow/examples/android:tensorflow_demo failed to build\nINFO: Elapsed time: 92.115s, Critical Path: 88.64s.\n", "comments": ["What is the content of your bazel-tensorflow/external/re2/util directory? util.h should be located in there. If it is there, then it seems there must be some sort of path issue.\n\nWhat version of Bazel are you using?\n", "Closing due to lack of activity.  Feel free to reopen if the problem persists.\n", "I facing the same issue on Ubuntu 14.04.  My Workspace setting for Android SDK and NDK are  given below:\n\nandroid_sdk_repository(\nname = \"androidsdk\",\napi_level = 23,\nbuild_tools_version = \"23.0.1\",\npath = \"/home/kuntal/Android/Sdk\"\n)\n\nandroid_ndk_repository (\nname = \"androidndk\",\npath = \"/home/kuntal/knowledge/IDE/android/android-ndk-r10e/\",\napi_level = 21\n)\n\n Also i have the proper version in the SDK/NDk repo. NDK version (r10e). \n\nNow im trying to build with bazel using the following command:\n\nsudo bazel build tensorflow_demo -c opt --copt=-mfpu=neon\n\nAnd i even tried with sudo bazel build tensorflow_demo \n\nbut i'm getting the following error:\n\nINFO: Found 1 target...\nERROR: /home/kuntal/knowledge/codebase/PRACTICE/BIG-DATA/TensorFlow/tensorflow/tensorflow/examples/android/BUILD:65:1: Processing resources failed: resources_processor failed: error executing command bazel-out/host/bin/external/bazel_tools/tools/android/resources_processor --buildToolsVersion 23.0.1 --aapt bazel-out/host/bin/external/androidsdk/aapt_binary --annotationJar ... (remaining 13 argument(s) skipped).\nsrc/main/tools/namespace-sandbox.c:558: mount(opt->mount_sources[i], full_sandbox_path, NULL, MS_REC | MS_BIND | MS_RDONLY, NULL): Permission denied\nTarget //tensorflow/examples/android:tensorflow_demo failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 1.950s, Critical Path: 0.14s\n\nPlease suggest how to fix these error?\n", "This seems like the important bit.  It's unlikely to be an issue in tensorflow:\n\n```\nsrc/main/tools/namespace-sandbox.c:558: mount(opt->mount_sources[i], full_sandbox_path, NULL, MS_REC | MS_BIND | MS_RDONLY, NULL): Permission denied\n```\n", "I'm not finding any clue how to proceed for this issue?? and where to dig in??\n", "@Kuntal-G To clarify, you received the re2 error prior to trying sudo, and only get the permission denied error when using sudo?\n", "When i'm running this command without sudo\n\n> bazel build tensorflow_demo -c opt --copt=-mfpu=neon\n\ngetting the following error\n\n*_Error: *_mkdir('/home/kuntal/.cache/bazel/_bazel_kuntal'): Permission denied\n\nBut with sudo im getting the following error mentioned above with  lots of warning before that:\n\nWARNING: /home/kuntal/knowledge/codebase/PRACTICE/BIG-DATA/TensorFlow/tensorflow/tensorflow/core/BUILD:649:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib: please do not import '//tensorflow/core/kernels:where_op.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\nWARNING: /home/kuntal/knowledge/codebase/PRACTICE/BIG-DATA/TensorFlow/tensorflow/tensorflow/core/BUILD:649:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib: please do not import '//tensorflow/core/kernels:xent_op.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\nINFO: Found 1 target...\n**ERROR: ****/home/kuntal/.cache/bazel/_bazel_root/5da6062df6ce7596334ec4b5c79b36c3/external/androidsdk/BUILD:150:2: Executing genrule @androidsdk//:zipalign_runner failed: bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped).\nsrc/main/tools/namespace-sandbox.c:482: mount(opt->sandbox_root, opt->sandbox_root, NULL, MS_BIND | MS_NOSUID, NULL): Permission denied\nTarget //tensorflow/examples/android:tensorflow_demo failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 1.530s, Critical Path: 0.41s\n", "I'm facing the similar issue at thread #4832\n"]}, {"number": 714, "title": "when do we support cudnn4?", "body": "cudnn4 natively support batch normalization, when do we support cudnn4?\nActually, theano acts very fast, it support cudnn4 only several days after it comes out.\n", "comments": ["@fayeshine, we will officially switch to Cudnn R4 when it is officially released. For now, it is a release candidate.\n", "After commit [22ebf0a](https://github.com/tensorflow/tensorflow/commit/22ebf0a94fd42af2d78b7964e836c92673ddfa31), you can build TensorFlow with Cudnn R3 and R4, after the following changes to your Cudnn library. \n\nFor Cudnn R3:\n$ ln -s libcudnn.so.7.0 libcudnn.so.6.5\n\nFor Cudnn R4:\n$ ln -s libcudnn.so.4.0.4 libcudnn.so.6.5\n\nThis is currently still an unofficial feature. TensorFlow will officially switch to R4 soon after the Cudnn R4 final version is released.\n\nThe performance with R4 is better than R2. There are still a number of memory and performance improvements we are working on. Please stay tuned for more changes in this area. \n\nThanks. \n-XQ\n", "@zheng-xq ,thank you! hope to see the official cudnn4 comes out soon! I'm really interested to see how theano and tensowflow acts on cudnn4. I believe tensowflow will compile much faster and hope it can also run the training faster than theano. \n", "@zheng-xq However, when do we start to support cuda 7.5 or upcoming 8.0?\n", "@zheng-xq , cudnn v4 now is officially released\n", "We are working on upgrading to Cudnn R4, and test it on all platforms. We'll keep this thread posted. \n", "@zheng-xq , and what about cuda 7.5?\n", "@fayeshine I think cuda 7.5 is also experimentally supported right now. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#using-a-different-cuda-sdk-and-cudnn-versions\n"]}, {"number": 713, "title": "failed to query current context: CUDA_ERROR_DEINITIALIZED", "body": "This is a regression problem, code was working fine 2 wks ago, did a git sync to the head and now I have this log message:\n\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcublas.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcudnn.so.6.5 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcufft.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcurand.so.7.0 locally\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:103] Found device 0 with properties: \nname: GeForce GTX TITAN X\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\npciBusID 0000:02:00.0\nTotal memory: 12.00GiB\nFree memory: 11.87GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:127] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:137] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:705] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Allocating 11.27GiB bytes.\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:52] GPU 0 memory begins at 0x2306c80000 extends to 0x25d8436a67\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 1.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 2.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 4.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 8.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 16.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 32.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 64.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 128.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 256.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 512.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 1.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 2.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 4.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 8.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 16.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 32.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 64.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 128.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 256.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 512.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 1.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 2.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 4.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 8.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 16.00GiB\ncreate_encoder graph time 0.569116\ncreate_decoder graph time 2.629546\ncreate_loss graph time 0.960969\ncreate_optimizer graph time 34.311540\nF tensorflow/stream_executor/cuda/cuda_driver.cc:299] failed to query current context: CUDA_ERROR_DEINITIALIZED\nF tensorflow/stream_executor/cuda/cuda_driver.cc:299] failed to query current context: CUDA_ERROR_DEINITIALIZED\nF tensorflow/stream_executor/cuda/cuda_driver.cc:299] failed to query current context: CUDA_ERROR_DEINITIALIZED\nF tensorflow/stream_executor/cuda/cuda_driver.cc:408] Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(prior_context_) (0 vs. 4)\nAborted (core dumped)\n\nNote: (the create_*) is my code creating the graph.\nThe python code it crashes on is:\nsess.run(tf.initialize_all_variables())\n", "comments": ["here's the stack trace from gdb:\n\n#2  0x00007fff52e11ad4 in tensorflow::internal::LogMessageFatal::~LogMessageFatal() () from /data-local/wchan/tensorflow_env/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#3  0x00007fff52de0ff3 in perftools::gputools::cuda::(anonymous namespace)::CurrentContext() () from /data-local/wchan/tensorflow_env/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#4  0x00007fff52de130c in perftools::gputools::cuda::ScopedActivateContext::ScopedActivateContext(CUctx_st_, perftools::gputools::cuda::MultiOpActivation) ()\n   from /data-local/wchan/tensorflow_env/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#5  0x00007fff52da145a in perftools::gputools::cuda::ScopedActivateExecutorContext::ScopedActivateExecutorContext(perftools::gputools::cuda::CUDAExecutor_, perftools::gputools::cuda::MultiOpActivation) ()\n   from /data-local/wchan/tensorflow_env/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#6  0x00007fff52c2e3e7 in tensorflow::BaseGPUDevice::Compute(tensorflow::OpKernel_, tensorflow::OpKernelContext_) () from /data-local/wchan/tensorflow_env/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#7  0x00007fff52c4eac8 in tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) ()\n   from /data-local/wchan/tensorflow_env/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#8  0x00007fff52c43d20 in std::_Function_handler<void (), std::_Bind<std::_Mem_fn<void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long)> (tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long)> >::_M_invoke(std::_Any_data const&) ()\n   from /data-local/wchan/tensorflow_env/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#9  0x00007fff52dff357 in tensorflow::thread::ThreadPool::WorkerLoop() () from /data-local/wchan/tensorflow_env/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#10 0x00007ffff436ea40 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\n#11 0x00007ffff7bc4182 in start_thread (arg=0x7ffef87f8700) at pthread_create.c:312\n#12 0x00007ffff78f147d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111\n", "+1\n", "I've got the same issue\n", "I have a suspicion that our mega-commit 1c579361cd1e088dd5e05a394b1561a73e3667ba might have introduced a change that broke this (there are a bunch of changes that could be responsible, including changes to the stream executor and updating to a new version of Eigen, among other things).\n\nIt would be great if someone could confirm if it works at 295625035eefc7801cfe831bd1ba851faacb7579 but fails at the commit right after.\n", "@vrv 2956250 works for me; 1c57936 does not.\n", "Cool, thanks for verifying.  If you have time:\n\nRevert the WORKSPACE file and eigen.BUILD changes (to use an earlier version of Eigen) and see if the problem goes away.\n\nI'll dig into potential other issues in that commit.\n", "Checking out WORKSPACE and eigen.BUILD from 2956250 does not fix the problem.\n", "Ok thanks, that helps.\n", "@zheng-xq, who independently was trying to debug this.\n", "I reverted to 2956250 and it seems to work, but the execution speed is cut in half\n", "Yup, that commit improved speed for some models, @zheng-xq is looking into what broke it.\n", "I can confirm that with eigen-a0661a2bb165, an Eigen kernel fails to\nlaunch. But with the older eigen-ce5a455b34c0, this problem goes away.\n\nMy reproducing command line:\n\nbazel run -c opt --config=cuda\n//tensorflow/models/image/alexnet:alexnet_benchmark\n\nPassing to @benoitsteiner\n\nOn Thu, Jan 7, 2016 at 11:18 AM, Martin Wicke notifications@github.com\nwrote:\n\n> Assigned #713 https://github.com/tensorflow/tensorflow/issues/713 to\n> @zheng-xq https://github.com/zheng-xq.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/713#event-508621546.\n", "@zheng-xq: what eigen kernel failed, so we can figure out which kernel was the problem?\n", "The kernel that was causing problem is:\n\nEigenMetaKernel_Vectorizable<Eigen::TensorEvaluator<Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float,\n1, 1, int>, 16>,\nEigen::TensorCwiseUnaryOp<Eigen::internal::scalar_right<float, float,\nEigen::internal::scalar_product_op<float, float>, true>,\nEigen::TensorMap<Eigen::Tensor<float const, 1, 1, int>, 16> const> const>\nconst, Eigen::GpuDevice>, int>\n\nIt came from cwise_mul:\n\ntensorflow::BinaryOp<Eigen::GpuDevice, tensorflow::functor::mul<float> >\n\nOn Thu, Jan 7, 2016 at 6:53 PM, Vijay Vasudevan notifications@github.com\nwrote:\n\n> @zheng-xq https://github.com/zheng-xq: what eigen kernel failed, so we\n> can figure out which kernel was the problem?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/713#issuecomment-169877615\n> .\n", "When running \"bazel-bin/tensorflow/models/image/alexnet/alexnet_benchmark --benchmarks=all\" on my machine, I get the following error:\nexternal/re2/re2/regexp.cc:136: Bad reference count 65535\nF tensorflow/stream_executor/cuda/cuda_driver.cc:408] Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(prior_context_) (0 vs. 4)\n", "Okay, we rolled back the change to switch to the newer version of Eigen, which apparently had some issues.  https://github.com/tensorflow/tensorflow/commit/22267addd64b00f9457605cdce3d82276a478780\n\nSo things should be now be working.\n\nHowever, this uses an older version of Eigen that has some speed problems.\n\n@benoitsteiner has upstreamed a newer fix to Eigen which should bring back the speed improvements _and_ not crash things, so look for that soon.\n\nClosing this for now, let us know if you still see problems.\n", "FWIW, I spent some time independently debugging this issue as well.   What I found was that the preprocessor magic in Eigen was removing nvcc's ability to know what templated kernels to instantiate for the device code.  The fix for the non-convolutional kernels was to modify the `eigen/unsupported/CXX11/src/tensor/TensorDeviceCuda.h` file and modify the LAUNCH_CUDA_KERNEL macro for the device code to force instantiation of the kernel.\n\nThe full patch and details are in the Bitbucket pull request:\n\nhttps://bitbucket.org/eigen/eigen/pull-requests/152/alternative-way-of-forcing-instantiation/diff\n\n```\n#ifndef __CUDA_ARCH__\n#define LAUNCH_CUDA_KERNEL(kernel, gridsize, blocksize, sharedmem, device, ...)            \\\n    (kernel) <<< (gridsize), (blocksize), (sharedmem), (device).stream() >>> (__VA_ARGS__); \\\n    assert(cudaGetLastError() == cudaSuccess);\n#else\n#define LAUNCH_CUDA_KERNEL(kernel, ...)  \\\n    { static const auto __attribute__((__unused__)) __makeTheKernelInstantiate = &(kernel); } \\\n   eigen_assert(false && \"Cannot launch a kernel from another kernel\");\n#endif\n```\n"]}, {"number": 712, "title": "CSV file loading example using pack", "body": "[The example code to read CSV files](https://www.tensorflow.org/versions/master/how_tos/reading_data/index.html#csv-files) raised the exception:\n\n> tensorflow.python.framework.errors.InvalidArgumentError: ConcatOp : Expected concatenating dimensions in the range [0, 0), but got 0\n\nThe CSV files contents were:\n\n``` csv\n5,4,3,2,1\n```\n\nUpdated the documentation to use [tf.pack](https://www.tensorflow.org/versions/master/api_docs/python/array_ops.html#pack) for the same reasons mentioned in [this stack overflow answer](http://stackoverflow.com/a/33686790/1589147).\n\nThis change might be irrelevant due to any updates on #278.\n", "comments": ["Can one of the admins verify this patch?\n"]}, {"number": 711, "title": "Delete most of the local copy of eigen that is shipped with tensorflow", "body": "TensorFlow now pulls the eigen code directly from the eigen upstream repository. There is no need to keep a local copy anymore.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "Can you squash the two commits?\nOn Wed, Jan 6, 2016 at 17:54 Benoit Steiner notifications@github.com\nwrote:\n\n> TensorFlow now pulls the eigen code directly from the eigen upstream\n> \n> ## repository. There is no need to keep a local copy anymore.\n> \n> You can view, comment on, or merge this pull request online at:\n> \n>   https://github.com/tensorflow/tensorflow/pull/711\n> Commit Summary\n> - Pruned our local copy of Eigen of code that we don't use in\n>   TensorFlow\n> - Pruned our local copy of Eigen of more code that TensorFlow doesn't\n>   use\n> \n> File Changes\n> - _D_ third_party/eigen3/Eigen/src/CholmodSupport/CholmodSupport.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-0 (607)\n> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/Block.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-1 (126)\n> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/Cwise.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-2 (192)\n> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/CwiseOperators.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-3 (298)\n> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/Geometry/AlignedBox.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-4 (159)\n> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/Geometry/All.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-5 (115)\n> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/Geometry/AngleAxis.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-6 (228)\n> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/Geometry/Hyperplane.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-7 (254)\n> - _D_\n>   third_party/eigen3/Eigen/src/Eigen2Support/Geometry/ParametrizedLine.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-8 (141)\n> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/Geometry/Quaternion.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-9 (495)\n> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/Geometry/Rotation2D.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-10\n>   (145)\n> - _D_\n>   third_party/eigen3/Eigen/src/Eigen2Support/Geometry/RotationBase.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-11\n>   (123)\n> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/Geometry/Scaling.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-12\n>   (167)\n> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/Geometry/Transform.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-13\n>   (786)\n> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/Geometry/Translation.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-14\n>   (184)\n> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/LU.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-15\n>   (120)\n> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/Lazy.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-16 (71)\n> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/LeastSquares.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-17\n>   (170)\n> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/Macros.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-18 (20)\n> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/MathFunctions.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-19 (57)\n> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/Memory.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-20 (45)\n> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/Meta.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-21 (75)\n> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/Minor.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-22\n>   (117)\n> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/QR.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-23 (67)\n> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/SVD.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-24\n>   (637)\n> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/TriangularSolver.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-25 (42)\n> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/VectorBlock.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-26 (94)\n> - _D_ third_party/eigen3/Eigen/src/Geometry/AlignedBox.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-27\n>   (379)\n> - _D_ third_party/eigen3/Eigen/src/Geometry/AngleAxis.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-28\n>   (233)\n> - _D_ third_party/eigen3/Eigen/src/Geometry/EulerAngles.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-29\n>   (104)\n> - _D_ third_party/eigen3/Eigen/src/Geometry/Homogeneous.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-30\n>   (307)\n> - _D_ third_party/eigen3/Eigen/src/Geometry/Hyperplane.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-31\n>   (270)\n> - _D_ third_party/eigen3/Eigen/src/Geometry/OrthoMethods.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-32\n>   (221)\n> - _D_ third_party/eigen3/Eigen/src/Geometry/ParametrizedLine.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-33\n>   (195)\n> - _D_ third_party/eigen3/Eigen/src/Geometry/Quaternion.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-34\n>   (778)\n> - _D_ third_party/eigen3/Eigen/src/Geometry/Rotation2D.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-35\n>   (157)\n> - _D_ third_party/eigen3/Eigen/src/Geometry/RotationBase.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-36\n>   (206)\n> - _D_ third_party/eigen3/Eigen/src/Geometry/Scaling.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-37\n>   (166)\n> - _D_ third_party/eigen3/Eigen/src/Geometry/Transform.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-38 (0)\n> - _D_ third_party/eigen3/Eigen/src/Geometry/Translation.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-39 (0)\n> - _D_ third_party/eigen3/Eigen/src/Geometry/Umeyama.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-40 (0)\n> - _D_ third_party/eigen3/Eigen/src/Geometry/arch/Geometry_SSE.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-41 (0)\n> - _D_ third_party/eigen3/Eigen/src/Householder/BlockHouseholder.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-42 (0)\n> - _D_ third_party/eigen3/Eigen/src/Householder/Householder.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-43 (0)\n> - _D_ third_party/eigen3/Eigen/src/Householder/HouseholderSequence.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-44 (0)\n> - _D_\n>   third_party/eigen3/Eigen/src/IterativeLinearSolvers/BasicPreconditioners.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-45 (0)\n> - _D_ third_party/eigen3/Eigen/src/IterativeLinearSolvers/BiCGSTAB.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-46 (0)\n> - _D_\n>   third_party/eigen3/Eigen/src/IterativeLinearSolvers/ConjugateGradient.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-47 (0)\n> - _D_\n>   third_party/eigen3/Eigen/src/IterativeLinearSolvers/IncompleteLUT.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-48 (0)\n> - _D_\n>   third_party/eigen3/Eigen/src/IterativeLinearSolvers/IterativeSolverBase.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-49 (0)\n> - _D_ third_party/eigen3/Eigen/src/Jacobi/Jacobi.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-50 (0)\n> - _D_ third_party/eigen3/Eigen/src/MetisSupport/MetisSupport.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-51 (0)\n> - _D_ third_party/eigen3/Eigen/src/OrderingMethods/Eigen_Colamd.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-52 (0)\n> - _D_ third_party/eigen3/Eigen/src/OrderingMethods/Ordering.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-53 (0)\n> - _D_ third_party/eigen3/Eigen/src/PaStiXSupport/PaStiXSupport.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-54 (0)\n> - _D_ third_party/eigen3/Eigen/src/PardisoSupport/PardisoSupport.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-55 (0)\n> - _D_ third_party/eigen3/Eigen/src/QR/ColPivHouseholderQR.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-56 (0)\n> - _D_ third_party/eigen3/Eigen/src/QR/ColPivHouseholderQR_MKL.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-57 (0)\n> - _D_ third_party/eigen3/Eigen/src/QR/FullPivHouseholderQR.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-58 (0)\n> - _D_ third_party/eigen3/Eigen/src/QR/HouseholderQR.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-59 (0)\n> - _D_ third_party/eigen3/Eigen/src/QR/HouseholderQR_MKL.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-60 (0)\n> - _D_ third_party/eigen3/Eigen/src/SPQRSupport/SuiteSparseQRSupport.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-61 (0)\n> - _D_ third_party/eigen3/Eigen/src/SVD/JacobiSVD.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-62 (0)\n> - _D_ third_party/eigen3/Eigen/src/SVD/JacobiSVD_MKL.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-63 (0)\n> - _D_ third_party/eigen3/Eigen/src/SVD/UpperBidiagonalization.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-64 (0)\n> - _D_ third_party/eigen3/Eigen/src/SparseCore/AmbiVector.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-65 (0)\n> - _D_ third_party/eigen3/Eigen/src/SparseCore/CompressedStorage.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-66 (0)\n> - _D_\n>   third_party/eigen3/Eigen/src/SparseCore/ConservativeSparseSparseProduct.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-67 (0)\n> - _D_ third_party/eigen3/Eigen/src/SparseCore/MappedSparseMatrix.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-68 (0)\n> - _D_ third_party/eigen3/Eigen/src/SparseCore/SparseBlock.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-69 (0)\n> - _D_ third_party/eigen3/Eigen/src/SparseCore/SparseColEtree.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-70 (0)\n> - _D_ third_party/eigen3/Eigen/src/SparseCore/SparseCwiseBinaryOp.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-71 (0)\n> - _D_ third_party/eigen3/Eigen/src/SparseCore/SparseCwiseUnaryOp.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-72 (0)\n> - _D_ third_party/eigen3/Eigen/src/SparseCore/SparseDenseProduct.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-73 (0)\n> - _D_ third_party/eigen3/Eigen/src/SparseCore/SparseDiagonalProduct.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-74 (0)\n> - _D_ third_party/eigen3/Eigen/src/SparseCore/SparseDot.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-75 (0)\n> - _D_ third_party/eigen3/Eigen/src/SparseCore/SparseFuzzy.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-76 (0)\n> - _D_ third_party/eigen3/Eigen/src/SparseCore/SparseMatrix.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-77 (0)\n> - _D_ third_party/eigen3/Eigen/src/SparseCore/SparseMatrixBase.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-78 (0)\n> - _D_ third_party/eigen3/Eigen/src/SparseCore/SparsePermutation.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-79 (0)\n> - _D_ third_party/eigen3/Eigen/src/SparseCore/SparseProduct.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-80 (0)\n> - _D_ third_party/eigen3/Eigen/src/SparseCore/SparseRedux.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-81 (0)\n> - _D_ third_party/eigen3/Eigen/src/SparseCore/SparseSelfAdjointView.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-82 (0)\n> - _D_\n>   third_party/eigen3/Eigen/src/SparseCore/SparseSparseProductWithPruning.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-83 (0)\n> - _D_ third_party/eigen3/Eigen/src/SparseCore/SparseTranspose.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-84 (0)\n> - _D_ third_party/eigen3/Eigen/src/SparseCore/SparseTriangularView.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-85 (0)\n> - _D_ third_party/eigen3/Eigen/src/SparseCore/SparseUtil.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-86 (0)\n> - _D_ third_party/eigen3/Eigen/src/SparseCore/SparseVector.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-87 (0)\n> - _D_ third_party/eigen3/Eigen/src/SparseCore/SparseView.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-88 (0)\n> - _D_ third_party/eigen3/Eigen/src/SparseCore/TriangularSolver.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-89 (0)\n> - _D_ third_party/eigen3/Eigen/src/SparseLU/SparseLU.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-90 (0)\n> - _D_ third_party/eigen3/Eigen/src/SparseLU/SparseLUImpl.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-91 (0)\n> - _D_ third_party/eigen3/Eigen/src/SparseLU/SparseLU_Memory.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-92 (0)\n> - _D_ third_party/eigen3/Eigen/src/SparseLU/SparseLU_Structs.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-93 (0)\n> - _D_ third_party/eigen3/Eigen/src/SparseLU/SparseLU_SupernodalMatrix.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-94 (0)\n> - _D_ third_party/eigen3/Eigen/src/SparseLU/SparseLU_Utils.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-95 (0)\n> - _D_ third_party/eigen3/Eigen/src/SparseLU/SparseLU_column_bmod.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-96 (0)\n> - _D_ third_party/eigen3/Eigen/src/SparseLU/SparseLU_column_dfs.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-97 (0)\n> - _D_ third_party/eigen3/Eigen/src/SparseLU/SparseLU_copy_to_ucol.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-98 (0)\n> - _D_ third_party/eigen3/Eigen/src/SparseLU/SparseLU_gemm_kernel.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-99 (0)\n> - _D_ third_party/eigen3/Eigen/src/SparseLU/SparseLU_heap_relax_snode.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-100 (0)\n> - _D_ third_party/eigen3/Eigen/src/SparseLU/SparseLU_kernel_bmod.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-101 (0)\n> - _D_ third_party/eigen3/Eigen/src/SparseLU/SparseLU_panel_bmod.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-102 (0)\n> - _D_ third_party/eigen3/Eigen/src/SparseLU/SparseLU_panel_dfs.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-103 (0)\n> - _D_ third_party/eigen3/Eigen/src/SparseLU/SparseLU_pivotL.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-104 (0)\n> - _D_ third_party/eigen3/Eigen/src/SparseLU/SparseLU_pruneL.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-105 (0)\n> - _D_ third_party/eigen3/Eigen/src/SparseLU/SparseLU_relax_snode.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-106 (0)\n> - _D_ third_party/eigen3/Eigen/src/SparseQR/SparseQR.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-107 (0)\n> - _D_ third_party/eigen3/Eigen/src/StlSupport/StdDeque.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-108 (0)\n> - _D_ third_party/eigen3/Eigen/src/StlSupport/StdList.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-109 (0)\n> - _D_ third_party/eigen3/Eigen/src/StlSupport/StdVector.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-110 (0)\n> - _D_ third_party/eigen3/Eigen/src/StlSupport/details.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-111 (0)\n> - _D_ third_party/eigen3/Eigen/src/SuperLUSupport/SuperLUSupport.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-112 (0)\n> - _D_ third_party/eigen3/Eigen/src/UmfPackSupport/UmfPackSupport.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-113 (0)\n> - _D_ third_party/eigen3/unsupported/Eigen/FFT\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-114 (0)\n> - _D_ third_party/eigen3/unsupported/Eigen/KroneckerProduct\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-115 (0)\n> - _D_ third_party/eigen3/unsupported/Eigen/src/FFT/CMakeLists.txt\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-116 (0)\n> - _D_ third_party/eigen3/unsupported/Eigen/src/FFT/ei_fftw_impl.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-117 (0)\n> - _D_ third_party/eigen3/unsupported/Eigen/src/FFT/ei_kissfft_impl.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-118 (0)\n> - _D_\n>   third_party/eigen3/unsupported/Eigen/src/KroneckerProduct/CMakeLists.txt\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-119 (0)\n> - _D_\n>   third_party/eigen3/unsupported/Eigen/src/KroneckerProduct/KroneckerTensorProduct.h\n>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-120 (0)\n> \n> Patch Links:\n> - https://github.com/tensorflow/tensorflow/pull/711.patch\n> - https://github.com/tensorflow/tensorflow/pull/711.diff\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/711.\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->\n", "Can you rebase and squash again (see [rebaseandsqua.sh](rebaseandsqua.sh))? The merge made things very ugly.\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "Can you rebase to head?\n"]}, {"number": 710, "title": "Fix documentation typos in ops.", "body": "Since this PR does not include the fixes on generated docs,\nthe corresponding md files need to be regenerated after merging this PR.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks!\n", "Thank you!\n"]}, {"number": 709, "title": "Add round to nearest", "body": "We have `tf.floor` and `tf.ceil`, but we don't have round to nearest.  I am conflicted on whether it should be called `rint` or `round`.  It would also be annoying if it depended on the processor flags.\n", "comments": ["I like that the `tf` interface is fairly `numpy`-like. It would be nice to keep it that way. Thus in my opinion I would use `tf.rint` as it is [in numpy](http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.rint.html). ;)\n", "Sounds reasonable to me.\n", "I believe this is fixed -- `tf.round` seems to work. It's different from the patch I had prepared, and I think different from @chemelnucfin's PR, but somehow someone wrote it.\n"]}, {"number": 708, "title": "ConcatOp error text change", "body": "I think this message was meant to be ended with a `]` but I am uncertain if that notation is clear.\n\nBefore this change, the script:\n\n``` python\nimport tensorflow as tf\n\nwith tf.Session() as sess:\n    sess.run(tf.concat(0, [1, 2, 3]))\n```\n\nThrows the following exception:\n\n> InvalidArgumentError: ConcatOp : Expected concatenating dimensions in the range [0, 0**)**, but got 0\n\nAfter this change the output is:\n\n> InvalidArgumentError: ConcatOp : Expected concatenating dimensions in the range [0, 0**]***, but got 0\n", "comments": ["Can one of the admins verify this patch?\n", "No, it was correct before. [0,0) means excluding 0, [0,0] means including 0, which would also mean that 0 would be valid and the error message would be confusing indeed.\n", "Ah, that makes sense. Thank you\n"]}, {"number": 707, "title": "Mistakes in docs of tf.range", "body": "The `limit` argument doesn't have a default value of `None`, and the `start` argument does have a default value of `0`. As specified, the header part of the doc reads: `tf.range(start, limit=None, delta=1, name='range')`, which is misleading and incorrect for something like `tf.range(5)`.\n", "comments": ["Do you want to send us a fix to the docs?\n", "Sure, although I'm not currently set up so it'll probably take a little bit of time.\n", "This is an odd one -- we're mimicking python's range behavior, which is hard to properly encode in python because overloading doesn't exist. The docs for tf.range are pretty clear what it does. \n\nThe fact that the signature itself is misleading is unfortunate, but somewhat out of our control (the limit parameter does have a default of None, and start does not have a default value of 0, python doesn't have a way of expressing the signature tf.range(start=0 if limit is None else no default, limit=None) in the signature alone).\n\nI will close this for now. By all means, if you have a better way of expressing the semantics, let us know.\n", "i see your point. This is pretty confusing though. Are the signatures autogenerated? If not one possibility is to write it out to mirror the semantics, even though it won't be correct python code. I.e. `tf.range(start=0, limit, delta=1, name='range')`, and then just spell it out in the docs that that is the behavior that's effectively achieved, even though it's actually implemented differently in python.\n\nAlternatively maybe say something explicitly in the docs about this caveat? My confusion arose from scanning the signature of the function and thinking it made no sense. Obviously when I read the text the semantics became clearer, but a note explaining the caveat with the signature would be helpful.\n", "The signatures in the docs are taken directly from the code by the doc\ngenerator.\nOn Thu, Jan 7, 2016 at 06:32 Mohammed AlQuraishi notifications@github.com\nwrote:\n\n> i see your point. This is pretty confusing though. Are the signatures\n> autogenerated? If not one possibility is to write it out to mirror the\n> semantics, even though it won't be correct python code. I.e. tf.range(start=0,\n> limit, delta=1, name='range'), and then just spell it out in the docs\n> that that is the behavior that's effectively achieved, even though it's\n> actually implemented differently in python.\n> \n> Alternatively maybe say something explicitly in the docs about this\n> caveat? My confusion arose from scanning the signature of the function and\n> thinking it made no sense. Obviously when I read the text the semantics\n> became clearer, but a note explaining the caveat with the signature would\n> be helpful.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/707#issuecomment-169679915\n> .\n", "While not everybody reads all the way to the \"Args\" section that explains it, most users will read the first paragraphs up to the example. It should therefore be sufficient to more precisely state the behaviour in the sentence starting \"Like the Python builtin\": Instead of wrongly saying that start defaults to 0, say what really happens, similarly to what you say in the \"Args\" section for \"limit\".\r\n\r\nI am sorry I have to express my suggestion carefully to not include any phrase that could be part of a solution. This is because I cannot sign Google's contributor licence agreement at this time and from my past experience providing concrete solutions without the CLA is counterproductive. Presumably, developers with CLA fear that any solution they come up with might be too similar to my solutions and therefore decide do not fix the respective issues at all."]}, {"number": 706, "title": "swig command not found", "body": "I try to build the GPU version on uBuntu 64 machine with command, \n\n```\nbazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\n```\n\nI installed swig and bazel successfully, but it still reports that swig not found.\n\n```\nsrc/main/tools/namespace-sandbox.c:476: arg: tensorflow/python/tensorflow.i\nbazel-out/host/bin/tensorflow/swig: line 17: swig: command not found\nERROR: /userdata01/Users/wenhuchen/tensorflow/tensorflow/tensorflow.bzl:298:3: SWIGing tensorflow/python/tensorflow.i failed: Error during execution of spawn: Process exited with status 127: Error during execution of spawn: Process exited with status 127.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\n```\n\nCan anyone help me with that?\n", "comments": ["swig version is 3.0.8\n", "Where is your copy of `swig` installed (i.e. what result do you get when you run `which swig`)?\n\nAs a short-term fix, you can modify [`swig.sh`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/swig/swig.sh) to invoke SWIG by its absolute path.\n", "I installed it locally, I specify the absolute path in the swig.sh and got the same error\n\n```\n /home/wenhuchen/.local/bin/swig: No such file or directory\nERROR: /userdata01/Users/wenhuchen/tensorflow/tensorflow/tensorflow.bzl:298:3: SWIGing tensorflow/python/tensorflow.i failed: Error during execution of spawn: Process exited with status 127: Error during execution of spawn: Process exited with status 127.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\n```\n\nI'm sure swig is usable, and my swig.sh looks like below\n\n```\n /home/wenhuchen/.local/bin/swig \"$@\"\n```\n", "Just to confirm, if you run `/home/wenhuchen/.local/bin/swig` from the command line, does that work without error?\n", "try modifying [tensorflow.bzl here ](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorflow.bzl#L261) by adding an extra parameter:\n\n```\nuse_default_shell_env=True,\n```\n\nand building with: \n\n```\n--genrule_strategy=standalone --spawn_strategy=standalone\n```\n\nthanks to [bazel support](https://groups.google.com/forum/#!msg/bazel-discuss/FrHiyndAtik/HAbZBGGXFQAJ)\n", "Fixed the problem. Thanks for help\n", "@wenhuchen How did you fix the problem. Just editing the `swig.sh` file?\n", "Thank you for this @psycharo - I had the same issue when building (using newer version, so different line numbers):\n\n`bazel-out/host/bin/external/org_tensorflow/tensorflow/swig: line 25: swig: command not found`\n\nThought it might be helpful to point out that the same solution (for r0.10) is to insert\n`use_default_shell_env=True,`\n\nat\nhttps://github.com/tensorflow/tensorflow/blob/r0.10/tensorflow/tensorflow.bzl#L483\n"]}, {"number": 705, "title": "Fail to build from source", "body": "Hi!\n\nI am unable to build TensorFlow from source. I am using a custom installation of Python 3.5.1 on Centos7. Since the system  only has Cuda 7.5 / cudnn 7.0 installed, I changed all references in the source code to that (grepping for \"libcu\" in the source code).\n\n```\n$ bazel build --config=cuda --verbose_failures --spawn_strategy=standalone -c opt //tensorflow/tools/pip_package:build_pip_package\nWARNING: Output base '/system/user/unterthi/.cache/bazel/_bazel_unterthi/85e103d92fd5b6e762399323367e173a' is on NFS. This may lead to surprising failures and undetermined behavior.\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\n____Loading...\n____Loading complete.  Analyzing...\n____Found 1 target...\n____Building...\n ____From Compiling tensorflow/stream_executor/cuda/cuda_dnn.cc:\ntensorflow/stream_executor/cuda/cuda_dnn.cc:147:39: error: '::cudnnAddTensor_v3' has not been declared\n     typedef std::add_pointer<decltype(::__name)>::type FuncPointerT;       \\\n                                       ^\ntensorflow/stream_executor/cuda/cuda_dnn.cc:215:3: note: in expansion of macro 'PERFTOOLS_GPUTOOLS_CUDNN_WRAP'\n   __macro(cudnnAddTensor_v3)                                  \\\n   ^\ntensorflow/stream_executor/cuda/cuda_dnn.cc:221:1: note: in expansion of macro 'CUDNN_DNN_ROUTINE_EACH_R3'\n CUDNN_DNN_ROUTINE_EACH_R3(PERFTOOLS_GPUTOOLS_CUDNN_WRAP)\n ^\ntensorflow/stream_executor/cuda/cuda_dnn.cc:147:39: error: '::cudnnAddTensor_v3' has not been declared\n     typedef std::add_pointer<decltype(::__name)>::type FuncPointerT;       \\\n                                       ^\ntensorflow/stream_executor/cuda/cuda_dnn.cc:215:3: note: in expansion of macro 'PERFTOOLS_GPUTOOLS_CUDNN_WRAP'\n   __macro(cudnnAddTensor_v3)                                  \\\n   ^\ntensorflow/stream_executor/cuda/cuda_dnn.cc:221:1: note: in expansion of macro 'CUDNN_DNN_ROUTINE_EACH_R3'\n CUDNN_DNN_ROUTINE_EACH_R3(PERFTOOLS_GPUTOOLS_CUDNN_WRAP)\n ^\ntensorflow/stream_executor/cuda/cuda_dnn.cc:147:48: error: template argument 1 is invalid\n     typedef std::add_pointer<decltype(::__name)>::type FuncPointerT;       \\\n                                                ^\ntensorflow/stream_executor/cuda/cuda_dnn.cc:215:3: note: in expansion of macro 'PERFTOOLS_GPUTOOLS_CUDNN_WRAP'\n   __macro(cudnnAddTensor_v3)                                  \\\n   ^\ntensorflow/stream_executor/cuda/cuda_dnn.cc:221:1: note: in expansion of macro 'CUDNN_DNN_ROUTINE_EACH_R3'\n CUDNN_DNN_ROUTINE_EACH_R3(PERFTOOLS_GPUTOOLS_CUDNN_WRAP)\n ^\ntensorflow/stream_executor/cuda/cuda_dnn.cc:147:51: error: expected ';' at end of member declaration\n     typedef std::add_pointer<decltype(::__name)>::type FuncPointerT;       \\\n                                                   ^\ntensorflow/stream_executor/cuda/cuda_dnn.cc:215:3: note: in expansion of macro 'PERFTOOLS_GPUTOOLS_CUDNN_WRAP'\n   __macro(cudnnAddTensor_v3)                                  \\\n   ^\ntensorflow/stream_executor/cuda/cuda_dnn.cc:221:1: note: in expansion of macro 'CUDNN_DNN_ROUTINE_EACH_R3'\n CUDNN_DNN_ROUTINE_EACH_R3(PERFTOOLS_GPUTOOLS_CUDNN_WRAP)\n ^\ntensorflow/stream_executor/cuda/cuda_dnn.cc:147:56: error: 'FuncPointerT' does not name a type\n     typedef std::add_pointer<decltype(::__name)>::type FuncPointerT;       \\\n                                                        ^\ntensorflow/stream_executor/cuda/cuda_dnn.cc:215:3: note: in expansion of macro 'PERFTOOLS_GPUTOOLS_CUDNN_WRAP'\n   __macro(cudnnAddTensor_v3)                                  \\\n   ^\ntensorflow/stream_executor/cuda/cuda_dnn.cc:221:1: note: in expansion of macro 'CUDNN_DNN_ROUTINE_EACH_R3'\n CUDNN_DNN_ROUTINE_EACH_R3(PERFTOOLS_GPUTOOLS_CUDNN_WRAP)\n ^\ntensorflow/stream_executor/cuda/cuda_dnn.cc:148:12: error: 'FuncPointerT' does not name a type\n     static FuncPointerT DynLoad() {                                        \\\n            ^\ntensorflow/stream_executor/cuda/cuda_dnn.cc:215:3: note: in expansion of macro 'PERFTOOLS_GPUTOOLS_CUDNN_WRAP'\n   __macro(cudnnAddTensor_v3)                                  \\\n   ^\ntensorflow/stream_executor/cuda/cuda_dnn.cc:221:1: note: in expansion of macro 'CUDNN_DNN_ROUTINE_EACH_R3'\n CUDNN_DNN_ROUTINE_EACH_R3(PERFTOOLS_GPUTOOLS_CUDNN_WRAP)\n ^\ntensorflow/stream_executor/cuda/cuda_dnn.cc: In member function 'void perftools::gputools::cuda::dynload::DynLoadShim__cudnnAddTensor_v3::CallWrapper(perftools::gputools::cuda::CUDAExecutor*, tensorflow::Notification*, cudnnStatus_t*, const Args& ...)':\ntensorflow/stream_executor/cuda/cuda_dnn.cc:160:25: error: there are no arguments to 'DynLoad' that depend on a template parameter, so a declaration of 'DynLoad' must be available [-fpermissive]\n       *retval = DynLoad()(args...);                                        \\\n                         ^\ntensorflow/stream_executor/cuda/cuda_dnn.cc:215:3: note: in expansion of macro 'PERFTOOLS_GPUTOOLS_CUDNN_WRAP'\n   __macro(cudnnAddTensor_v3)                                  \\\n   ^\ntensorflow/stream_executor/cuda/cuda_dnn.cc:221:1: note: in expansion of macro 'CUDNN_DNN_ROUTINE_EACH_R3'\n CUDNN_DNN_ROUTINE_EACH_R3(PERFTOOLS_GPUTOOLS_CUDNN_WRAP)\n ^\ntensorflow/stream_executor/cuda/cuda_dnn.cc:160:25: note: (if you use '-fpermissive', G++ will accept your code, but allowing the use of an undeclared name is deprecated)\n       *retval = DynLoad()(args...);                                        \\\n                         ^\ntensorflow/stream_executor/cuda/cuda_dnn.cc:215:3: note: in expansion of macro 'PERFTOOLS_GPUTOOLS_CUDNN_WRAP'\n   __macro(cudnnAddTensor_v3)                                  \\\n   ^\ntensorflow/stream_executor/cuda/cuda_dnn.cc:221:1: note: in expansion of macro 'CUDNN_DNN_ROUTINE_EACH_R3'\n CUDNN_DNN_ROUTINE_EACH_R3(PERFTOOLS_GPUTOOLS_CUDNN_WRAP)\n ^\ntensorflow/stream_executor/cuda/cuda_dnn.cc: In instantiation of 'void perftools::gputools::cuda::dynload::DynLoadShim__cudnnAddTensor_v3::CallWrapper(perftools::gputools::cuda::CUDAExecutor*, tensorflow::Notification*, cudnnStatus_t*, const Args& ...) [with Args = {cudnnContext*, const float*, cudnnTensorStruct*, const void*, const float*, cudnnTensorStruct*, void*}]':\ntensorflow/stream_executor/cuda/cuda_dnn.cc:221:1:   required from 'cudnnStatus_t perftools::gputools::cuda::dynload::DynLoadShim__cudnnAddTensor_v3::operator()(perftools::gputools::cuda::CUDAExecutor*, Args ...) [with Args = {cudnnContext*, const float*, cudnnTensorStruct*, const void*, const float*, cudnnTensorStruct*, void*}]'\ntensorflow/stream_executor/cuda/cuda_dnn.cc:1081:30:   required from here\ntensorflow/stream_executor/cuda/cuda_dnn.cc:160:25: error: 'DynLoad' was not declared in this scope\n       *retval = DynLoad()(args...);                                        \\\n                         ^\ntensorflow/stream_executor/cuda/cuda_dnn.cc:215:3: note: in expansion of macro 'PERFTOOLS_GPUTOOLS_CUDNN_WRAP'\n   __macro(cudnnAddTensor_v3)                                  \\\n   ^\ntensorflow/stream_executor/cuda/cuda_dnn.cc:221:1: note: in expansion of macro 'CUDNN_DNN_ROUTINE_EACH_R3'\n CUDNN_DNN_ROUTINE_EACH_R3(PERFTOOLS_GPUTOOLS_CUDNN_WRAP)\n ^\ntensorflow/stream_executor/cuda/cuda_dnn.cc: In member function 'virtual bool perftools::gputools::cuda::CudnnSupport::DoNormalize(perftools::gputools::Stream*, const perftools::gputools::dnn::NormalizeDescriptor&, const perftools::gputools::DeviceMemory<float>&, perftools::gputools::DeviceMemory<float>*)':\ntensorflow/stream_executor/cuda/cuda_dnn.cc:1226:1: warning: control reaches end of non-void function [-Wreturn-type]\n }\n ^\ntensorflow/stream_executor/cuda/cuda_dnn.cc: In member function 'virtual bool perftools::gputools::cuda::CudnnSupport::DoElementwiseOperate(perftools::gputools::Stream*, perftools::gputools::dnn::ElementwiseOperation, tensorflow::gtl::ArraySlice<perftools::gputools::dnn::BatchDescriptor>, tensorflow::gtl::ArraySlice<const perftools::gputools::DeviceMemory<float>*>, const perftools::gputools::dnn::BatchDescriptor&, perftools::gputools::DeviceMemory<float>*)':\ntensorflow/stream_executor/cuda/cuda_dnn.cc:1287:1: warning: control reaches end of non-void function [-Wreturn-type]\n }\n ^\ntensorflow/stream_executor/cuda/cuda_dnn.cc: In member function 'virtual bool perftools::gputools::cuda::CudnnSupport::DoXYPad(perftools::gputools::Stream*, const perftools::gputools::dnn::BatchDescriptor&, const perftools::gputools::DeviceMemory<float>&, tensorflow::int64, tensorflow::int64, tensorflow::int64, tensorflow::int64, perftools::gputools::DeviceMemory<float>*)':\ntensorflow/stream_executor/cuda/cuda_dnn.cc:1294:1: warning: control reaches end of non-void function [-Wreturn-type]\n }\n ^\ntensorflow/stream_executor/cuda/cuda_dnn.cc: In member function 'virtual bool perftools::gputools::cuda::CudnnSupport::DoXYSlice(perftools::gputools::Stream*, const perftools::gputools::dnn::BatchDescriptor&, const perftools::gputools::DeviceMemory<float>&, tensorflow::int64, tensorflow::int64, tensorflow::int64, tensorflow::int64, perftools::gputools::DeviceMemory<float>*)':\ntensorflow/stream_executor/cuda/cuda_dnn.cc:1301:1: warning: control reaches end of non-void function [-Wreturn-type]\n }\n ^\nERROR: /system/apps/biosoft/python-351/tmp/tensorflow/tensorflow/stream_executor/BUILD:5:1: C++ compilation of rule '//tensorflow/stream_executor:stream_executor' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command \n  (cd /system/user/unterthi/.cache/bazel/_bazel_unterthi/85e103d92fd5b6e762399323367e173a/tensorflow && \\\n  exec env - \\\n    INTERCEPT_LOCALLY_EXECUTABLE=1 \\\n    PATH=/system/apps/biosoft/jdk1.8.0_40/bin:/system/apps/biosoft/bazel-0.1.1/bazel-bin/src:/usr/local/cuda/bin:/system/apps/biosoft/R-3.2.0/bin:/system/apps/biosoft/caffe_py351/distribute/bin:/system/apps/biosoft/lmdb-0.9.17/bin:/system/apps/biosoft/protobuf-3.0.0-alpha-3.1_py351/bin:/system/apps/biosoft/boost_1_59_0_py351/bin:/system/apps/biosoft/python-351/bin:/usr/local/cuda/bin:/system/apps/biosoft/R-3.2.0/bin:/system/apps/biosoft/caffe_py351/distribute/bin:/system/apps/biosoft/lmdb-0.9.17/bin:/system/apps/biosoft/protobuf-3.0.0-alpha-3.1_py351/bin:/system/apps/biosoft/boost_1_59_0_py351/bin:/system/apps/biosoft/python-351/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/system/user/unterthi/bin:/system/user/unterthi/bin \\\n  third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -iquote . -iquote bazel-out/local_linux-py3-opt/genfiles -isystem google/protobuf/src -isystem bazel-out/local_linux-py3-opt/genfiles/google/protobuf/src -isystem tools/cpp/gcc3 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-py3-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-py3-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local_linux-py3-opt/genfiles/external/re2 -isystem third_party/gpus/cuda -isystem bazel-out/local_linux-py3-opt/genfiles/third_party/gpus/cuda -isystem third_party/gpus/cuda/include -isystem bazel-out/local_linux-py3-opt/genfiles/third_party/gpus/cuda/include -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' '-frandom-seed=bazel-out/local_linux-py3-opt/bin/tensorflow/stream_executor/_objs/stream_executor/tensorflow/stream_executor/cuda/cuda_dnn.pic.o' -MD -MF bazel-out/local_linux-py3-opt/bin/tensorflow/stream_executor/_objs/stream_executor/tensorflow/stream_executor/cuda/cuda_dnn.pic.d -fPIC -c tensorflow/stream_executor/cuda/cuda_dnn.cc -o bazel-out/local_linux-py3-opt/bin/tensorflow/stream_executor/_objs/stream_executor/tensorflow/stream_executor/cuda/cuda_dnn.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1: crosstool_wrapper_driver_is_not_gcc failed: error executing command \n  (cd /system/user/unterthi/.cache/bazel/_bazel_unterthi/85e103d92fd5b6e762399323367e173a/tensorflow && \\\n  exec env - \\\n    INTERCEPT_LOCALLY_EXECUTABLE=1 \\\n    PATH=/system/apps/biosoft/jdk1.8.0_40/bin:/system/apps/biosoft/bazel-0.1.1/bazel-bin/src:/usr/local/cuda/bin:/system/apps/biosoft/R-3.2.0/bin:/system/apps/biosoft/caffe_py351/distribute/bin:/system/apps/biosoft/lmdb-0.9.17/bin:/system/apps/biosoft/protobuf-3.0.0-alpha-3.1_py351/bin:/system/apps/biosoft/boost_1_59_0_py351/bin:/system/apps/biosoft/python-351/bin:/usr/local/cuda/bin:/system/apps/biosoft/R-3.2.0/bin:/system/apps/biosoft/caffe_py351/distribute/bin:/system/apps/biosoft/lmdb-0.9.17/bin:/system/apps/biosoft/protobuf-3.0.0-alpha-3.1_py351/bin:/system/apps/biosoft/boost_1_59_0_py351/bin:/system/apps/biosoft/python-351/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/system/user/unterthi/bin:/system/user/unterthi/bin \\\n  third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -iquote . -iquote bazel-out/local_linux-py3-opt/genfiles -isystem google/protobuf/src -isystem bazel-out/local_linux-py3-opt/genfiles/google/protobuf/src -isystem tools/cpp/gcc3 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-py3-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-py3-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local_linux-py3-opt/genfiles/external/re2 -isystem third_party/gpus/cuda -isystem bazel-out/local_linux-py3-opt/genfiles/third_party/gpus/cuda -isystem third_party/gpus/cuda/include -isystem bazel-out/local_linux-py3-opt/genfiles/third_party/gpus/cuda/include -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' '-frandom-seed=bazel-out/local_linux-py3-opt/bin/tensorflow/stream_executor/_objs/stream_executor/tensorflow/stream_executor/cuda/cuda_dnn.pic.o' -MD -MF bazel-out/local_linux-py3-opt/bin/tensorflow/stream_executor/_objs/stream_executor/tensorflow/stream_executor/cuda/cuda_dnn.pic.d -fPIC -c tensorflow/stream_executor/cuda/cuda_dnn.cc -o bazel-out/local_linux-py3-opt/bin/tensorflow/stream_executor/_objs/stream_executor/tensorflow/stream_executor/cuda/cuda_dnn.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\n____Elapsed time: 11.581s, Critical Path: 9.50s\n```\n\nAny help would be appreciated\n", "comments": ["I just noticed this seems to have been my own fault, I had installed cudnn incorrectly.\n", "@untom What was the specific solution?\n", "to install a more recent version of cudnn ;)\n"]}, {"number": 704, "title": "ci_build updates", "body": "A few ci_build changes:\n- add the `--rm` for `docker run` to decrease the number of docker images on a machine and avoid problems we have run into\n- workaround problem newer bazel have with android ndk\n- cleanups\n", "comments": []}, {"number": 703, "title": "Test the TensorFlow installation location using inspect", "body": "While testing an installation I executed `python -c 'import site; print(\"\\n\".join(site.getsitepackages()))'` from within a virtualenv environment. This raised the same error mentioned in #392.\n\nAccording to pypa/virtualenv#355 this is an issue with their custom site implementation and may not be updated soon.\n\nI updated the command in the documentation to output the directory where TensorFlow is installed using [inspect](https://docs.python.org/2/library/inspect.html).\n\n``` python\nimport os\nimport inspect\nimport tensorflow\n\nprint(os.path.dirname(inspect.getfile(tensorflow)))\n```\n", "comments": ["Can one of the admins verify this patch?\n", "Can one of the admins verify this patch?\n", "Can one of the admins verify this patch?\n", "Can you squash your commits?\n"]}, {"number": 702, "title": "Test the TensorFlow installation location using inspect", "body": "While testing an installation I executed `python -c 'import site; print(\"\\n\".join(site.getsitepackages()))'` from within a virtualenv environment. This raised the same error mentioned in #392.\n\nAccording to pypa/virtualenv#355 this is an issue with their custom site implementation and may not be updated soon.\n\nI updated the command in the documentation to output the directory where TensorFlow is installed using [inspect](https://docs.python.org/2/library/inspect.html).\n\n``` python\nimport os\nimport inspect\nimport tensorflow\n\nprint(os.path.dirname(inspect.getfile(tensorflow)))\n```\n\nI hope this is useful.\n", "comments": ["Can one of the admins verify this patch?\n", "I've improperly squashed a commit on this branch and now it doesn't properly reflect the history of this change (missing revert). Opening a new pull request which includes all changes properly.\n"]}, {"number": 701, "title": "Provide a string_concat op", "body": "Per http://stackoverflow.com/questions/34247374/merge-string-tensors-in-tensorflow\n\nProvide an op that will concatenate two strings into a single string.\n", "comments": ["Also, an op that splits strings.\n", "I've just submitted string concatenation upstream. Should be available in the next push.\n", "Fixed by https://github.com/tensorflow/tensorflow/commit/7116cc8b39c432f64e6e82b5708d2773f3fec159\n"]}, {"number": 700, "title": "Potential Memory Leak with Session Closing", "body": "Bug: When repeatedly creating and closing sessions on a GPU or CPU (on 0.6.0 release and Jan 5 dev version), memory appears to leak as the python process continually grows in memory usage. Additionally, the time to execute continues to grow. \n\nExpected behaviour: Once a session is closed, resources allocated should be freed or overwritten.\n\nThe following code illustrates the issue with a minimal graph created:\n\nfor i in range(0,10000000):\n    t0 = time.clock()\n\n```\nsess = tf.Session()\n\na = tf.placeholder(\"int16\",name = 'a')\ny=tf.identity(a,name='y')\n\nsess.run(y,feed_dict={a:3})\nsess.close()\n\nprint time.clock() - t0\n```\n\nThe same issue happens even if I use a constant or variable instead of placeholder.\n\nThanks!\n", "comments": ["It looks like you're adding more and more ops to a graph, which is not freed when the session is closed. If that is in fact the case (you can check by making a graph and using it `with graph.as_default()`, and destroying it), then this is intended behavior -- the graph should survive a session.\n", "Martin's answer looks spot on here. The following code should not leak, and if it does let us know and we'll reopen the issue:\n\n``` python\nfor i in range(0,10000000):\n  t0 = time.clock()\n\n  with tf.Graph().as_default():\n    sess = tf.Session()\n\n    a = tf.placeholder(tf.int16, name='a')\n    y = tf.identity(a, name='y')\n\n    sess.run(y, feed_dict={a: 3})\n    sess.close()\n\nprint time.clock() - t0\n```\n", "You are correct. It was a misunderstanding on my part of what's being added to the graph.\n\nThanks!\n"]}, {"number": 699, "title": "Change name of sigmoid_cross_entropy_with_logits to log_loss", "body": "Hi folks. This name sigmoid_cross_entropy_with_logits struck me as strange. In addition to being a mouthful, we're calling it cross_entropy when it really is just binary cross_entropy calculated separately on every node. \n\nThe more intuitive name I would give to this function would be log_loss or mean_log_loss (if normalized). I think this would be both easier to write, and easier to understand. \n", "comments": ["The name may not be the best one, but we probably want to avoid saying \"loss\" in the function (since it's use it not necessarily restricted to being used a loss).\n", "Good point. Similarly, it's not restricted to being used with sigmoid units. Maybe better to call it binary_cross_entropy?\n", "Sounds good to me. @sherrym, I'll assign you for summary judgment. \n", "Following your points on naming, maybe a better name for `nn.l2_loss` is `nn.l2_norm`.\n", "@zackchase , sigmoid_cross_entropy_with_logits actually is restricted to being used with sigmoid units, in the sense that it only gives you the cross entropy if you are using such units. It is the op for binary cross entropy when the binary probability is produced by applying the sigmoid function to one argument. You could imagine other ways of getting binary probabilities, like thresholding the argument to the interval [0,1], or taking softplus(x)/(1+softplus(x)) instead of sigmoid, etc.\n", "Closing since changing the name is too little benefit for the large amount of required work.\n"]}, {"number": 698, "title": "Deleted the TF copy of the tensor code", "body": "Since  we're now pulling the tensor code directly from the eigen repository we don't need to keep a local copy anymore.\n", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it\n\nOn Tue, Jan 5, 2016 at 12:50 PM, googlebot notifications@github.com wrote:\n\n> Thanks for your pull request. It looks like this may be your first\n> contribution to a Google open source project. Before we can look at your\n> pull request, you'll need to sign a Contributor License Agreement (CLA).\n> \n> [image: :memo:] _Please visit https://cla.developers.google.com/\n> https://cla.developers.google.com/ to sign._\n> \n> Once you've signed, please reply here (e.g. I signed it!) and we'll\n> \n> ## verify. Thanks.\n> - If you've already signed a CLA, it's possible we don't have your\n>   GitHub username or you're using a different email address. Check your\n>   existing CLA data https://cla.developers.google.com/clas and verify\n>   that your email is set on your git commits\n>   https://help.github.com/articles/setting-your-email-in-git/.\n> - If you signed the CLA as a corporation, please let us know the\n>   company's name.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/698#issuecomment-169129411\n> .\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Can one of the admins verify this patch?\n", "Jenkins, test this please\n", "Can one of the admins verify this patch?\n", "martin: you need a period after \"please\" :)\n\n@tensorflow-jenkins: test this please.\n", "I need to change that regex, please.\n\nOn Tue, Jan 5, 2016 at 9:50 PM Vijay Vasudevan notifications@github.com\nwrote:\n\n> martin: you need a period after \"please\" :)\n> \n> @tensorflow-jenkins https://github.com/tensorflow-jenkins: test this\n> please.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/698#issuecomment-169235658\n> .\n", "Jenkins, retest this please.\n", "retest this please.\n"]}, {"number": 697, "title": "Check if logdir already exists before trying to create it", "body": "Currently, running something like this:\n\n`tf.train.write_graph(sess.graph_def, 'models/', 'model.pbtxt', as_text=True)`\n\nWill fail the second time its run because it tries to create a folder that now already exists. This patch fixes that behaviour by checking if the directory exists before attempting to create it.\n", "comments": ["Is this different than #693 ?  That one seems to handle a few more cases\n", "Nope, I guess not. I did a search for `write_graph` and it didn't show up.\n", "No worries, it was just sent a few hours ago :)\n"]}]