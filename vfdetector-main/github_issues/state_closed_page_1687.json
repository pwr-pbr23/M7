[{"number": 2292, "title": "Bug on specifying GPU to tutorial example minist", "body": "I tried to specify GPU ID to run the tutorial example [mnist](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/fully_connected_feed.py). I change the code to:\n\n```\nwith tf.device('/gpu:3\u2018):\n    # Generate placeholders for the images and labels.\n    images_placeholder, labels_placeholder = placeholder_inputs(\n        FLAGS.batch_size)\n    # Build a Graph that computes predictions from the inference model.\n    logits = mnist.inference(images_placeholder,\n                                FLAGS.hidden1,\n                                FLAGS.hidden2)\n    # Add to the Graph the Ops for loss calculation.\n    loss = mnist.loss(logits, labels_placeholder)\n\n    # Add to the Graph the Ops that calculate and apply gradients.\n    train_op = mnist.training(loss, FLAGS.learning_rate)\n```\n\nThen it reports error when running:\n\ntensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'global_step': Could not satisfy explicit device specification '/device:GPU:3' because no supported kernel for GPU devices is available\n     [[Node: global_step = Variable[container=\"\", dtype=DT_INT32, shape=[], shared_name=\"\", _device=\"/device:GPU:3\"]()]]\nCaused by op u'global_step', defined at:\n  File \"fully_connected_feed.py\", line 232, in <module>\n    tf.app.run()\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"fully_connected_feed.py\", line 228, in main\n    run_training()\n  File \"fully_connected_feed.py\", line 150, in run_training\n    train_op = mnist.training(loss, FLAGS.learning_rate)\n  File \"/search/guangliang/package/tensorflow/tensorflow/examples/tutorials/mnist/mnist.py\", line 125, in training\n    global_step = tf.Variable(0, name='global_step', trainable=False)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 209, in **init**\n    dtype=dtype)\n...\n\nThen I fix the line 125 in \"mnist.py\" with the following code:\n\n  with tf.device('/cpu:0'):\n    global_step = tf.Variable(0, name='global_step', trainable=False)\n\nThen it reports the following error on rerunning:\n\ntensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'gradients/xentropy_mean_grad/Prod': Could not satisfy explicit device specification '/device:GPU:3' because no supported kernel for GPU devices is available\n     [[Node: gradients/xentropy_mean_grad/Prod = Prod[T=DT_INT32, keep_dims=false, _device=\"/device:GPU:3\"](gradients/xentropy_mean_grad/Shape_2, gradients/xentropy_mean_grad/range_1)]]\nCaused by op u'gradients/xentropy_mean_grad/Prod', defined at:\n  File \"fully_connected_feed.py\", line 232, in <module>\n    tf.app.run()\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"fully_connected_feed.py\", line 228, in main\n    run_training()\n  File \"fully_connected_feed.py\", line 150, in run_training\n    train_op = mnist.training(loss, FLAGS.learning_rate)\n  File \"/search/guangliang/package/tensorflow/tensorflow/examples/tutorials/mnist/mnist.py\", line 129, in training\n    train_op = optimizer.minimize(loss, global_step=global_step)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 190, in minimize\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n...\n\nWould you please help on this?\nThanks a lot in advance!\n", "comments": ["I just follow mrry's suggestion [here](https://github.com/tensorflow/tensorflow/issues/2285), adding \"allow_soft_placement=True\" as follows:\n\nconfig = tf.ConfigProto(allow_soft_placement = True)\nsess = tf.Session(config = config)\n\nThen it works.\n\nI reviewed the [Using GPUs](https://www.tensorflow.org/versions/r0.8/how_tos/using_gpu/index.html) in tutorial. It mentions adding \"allow_soft_placement\" under the error \"Could not satisfy explicit device specification '/gpu:X' \". But it not mentions it could also solve the error \"no supported kernel for GPU devices is available\". Maybe it's better to add this in tutorial text in order to avoid confusing future users.\n", "Have you notice that even if there is no error occurs, but the `/gpu:3` device is not used ?\n\nI have a problem described [here](https://github.com/tensorflow/tensorflow/issues/2322) that I cannot make use of the GPUs on the second machine. If I use like `tf.device(\"/gpu:5\")`, the error like `InvalidArgumentError: Cannot assign a device to node...` occurs. But if I set `allow_soft_placement` to `True`, then all tasks will be running on the 4 gpus on machine A.\n", "GPU3 is really under use if \"allow_soft_placement = True\" is added. \nIt seems multi-GPU-tower style can't assign your work to another machine, it could only parallel work to  multiple GPUs inner machine. If you want to parallel it in a multi-node GPU cluster, you should try [Distributed Tensorflow](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/how_tos/distributed/index.md)\n", "Yes, you are right. `/gpu:%d` is for local devices. \n", "As @smartcat2010 mentioned, the tutorial is to illustrate the use of allow_soft_placement.\n\nClosing this as it's a not a bug.\n", "I want to notice, that **after doing** `tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)` it **does actually choose the device you specify** (`gpu_n`) without \"no supported kernel for GPU devices is available\" error.\n", "Why is this the case?\n\nI'm happy that this also solved my problem, but I'm a bit confused.\n\nAccording to the doc, the `allow_soft_placement=True` is a flag used to find substitute devices if the device specified is unavailable. In this case, we specified a different device that **is** available. We shouldn't need this flag.\n", "after setting `allow_soft_placement=True' I get\r\n```\r\nsite-packages/tensorflow/python/framework/test_util.py\", line 248, in prepare_config\r\n    config.allow_soft_placement = False\r\nAttributeError: 'NoneType' object has no attribute 'allow_soft_placement'\r\n```", "I was not getting this issue with TensorFlow 1.1, but after an upgrade to 1.4 I keep getting this issue (running the exact same file).\r\n\r\nIf i use `allow_soft_placement=True` I get a new error:\r\n\r\n```\r\nInvalidArgumentError: AttrValue must not have reference type value of float_ref\r\n\t for attr 'tensor_type'\r\n\t; NodeDef: Conv/weights/Adam_1/_515 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_3091_Conv/weights/Adam_1\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^optimizer/beta1_power/read/_281, ^optimizer/beta2_power/read/_283, ^optimizer/learning_rate/mul_2/_285, ^optimizer/Adam/beta1/_287, ^optimizer/Adam/beta2/_289, ^optimizer/Adam/epsilon/_291, ^optimizer/gradients/AddN_40/_517); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>\r\n\t [[Node: Conv/weights/Adam_1/_515 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_3091_Conv/weights/Adam_1\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^optimizer/beta1_power/read/_281, ^optimizer/beta2_power/read/_283, ^optimizer/learning_rate/mul_2/_285, ^optimizer/Adam/beta1/_287, ^optimizer/Adam/beta2/_289, ^optimizer/Adam/epsilon/_291, ^optimizer/gradients/AddN_40/_517)]]\r\n```", "Im getting this issue on TensorFlow 1.5.", "I had this problem on Tensorflow-gpu 1.8 and Tensorflow-gpu 1.5 on GPU clusters but I didn't get this issue after installing Tensorflow-gpu 1.0.1. So my problem was solved. \r\nOfcourse, I had the code below for all tests. \r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allocator_type = 'BFC'\r\nconfig.gpu_options.allow_growth = True\r\nwith tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) as sess:"]}, {"number": 2291, "title": "Add -lm (Was: undefined reference to symbol 'ceil@@GLIBC_2.2.5)", "body": "### Environment info\n\nOperating System:\n\nepel-release-6-8.noarch\nredhat-release-server-6Server-6.7.0.3.el6.x86_64\n\nInstalled version of CUDA and cuDNN: \n\nNone\n\nIf installed from sources, provide the commit hash:\n\nf8eb1d70a7ea7dc2cd5e1eddde389395f88a6be9\n### Steps to reproduce\n1. bazel clean\n2. ./configure\n3. bazel build -c opt //tensorflow/tools/pip_package:build_pip_package\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n\nERROR: /home/ebice/tensorflow/google/protobuf/BUILD:272:1: Linking of rule '//google/protobuf:protoc' failed: gcc failed: error executing command /opt/rh/devtoolset-2/root/usr/bin/gcc -o bazel-out/host/bin/google/protobuf/protoc -no-canonical-prefixes -B/opt/rh/devtoolset-2/root/usr/bin -pass-exit-codes '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' ... (remaining 11 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\n/opt/rh/devtoolset-2/root/usr/bin/ld: /opt/rh/devtoolset-2/root/usr/lib/gcc/x86_64-redhat-linux/4.8.2/libstdc++_nonshared.a(hashtable_c++0x44.o): undefined reference to symbol 'ceil@@GLIBC_2.2.5'\n/opt/rh/devtoolset-2/root/usr/bin/ld: note: 'ceil@@GLIBC_2.2.5' is defined in DSO /lib64/libm.so.6 so try adding it to the linker command line\n/lib64/libm.so.6: could not read symbols: Invalid operation\ncollect2: error: ld returned 1 exit status\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\n", "comments": ["Edited google/protobuf/BUILD\n\nchanging \n\nLINK_OPTS = [\"-lpthread\"]\n\nto\n\nLINK_OPTS = [\"-lpthread\", \"-lm\"]\n\nand the build completes OK.\n", "@martinwicke: Do you know of a reason we can't add `-lm` in all cases?  Would it bloat the binary? \n", "@petewarden would be most sensitive to bloat. I'm always in favor of -lm.\n\nOn Tue, Jun 7, 2016 at 5:51 PM Geoffrey Irving notifications@github.com\nwrote:\n\n> @martinwicke https://github.com/martinwicke: Do you know of a reason we\n> can't add -lm in all cases? Would it bloat the binary?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2291#issuecomment-224457136,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AAjO_baHZU43Sz4HYMkJsqTYQ2Bq-X_5ks5qJhH9gaJpZM4IaJBS\n> .\n", "@edi-bice: Let us know if you want to send us a PR with `-lm`!\n", "the overhead is high with that one - you guys go ahead if you don't mind\n", "@edi-bice \nWhere is `google/protobuf/BUILD`? I am having this issue but I can not find this file to edit like you mentioned. I'm totally stuck now.\n", "For those who have a similar issue. I had to modify the content of the file `bazel-tensorflow/external/protobuf/BUILD`, re-ran bazel build command  and it helped to resolve this issue in my case.\n\nBefore:\n\n```\nLINK_OPTS = select({\n    \":android\": [],\n    \"//conditions:default\": [\"-lpthread\"],\n})\n```\n\nNew:\n\n```\nLINK_OPTS = select({\n    \":android\": [],\n    \"//conditions:default\": [\"-lpthread\",\"-lrt\",\"-lm\"],\n})\n```\n", "I think it's easiest to simply append `-lm` on everything, by hacking third_party/gpus/crosstool/CROSSTOOL. Check https://github.com/leelabcnbc/DevOps/blob/master/Docker/tensorflow/0.10.0/centos6/py35/gpu/CROSSTOOL.tpl#L59, and my answer on #1426. Interestingly, this problem only appear when compiling with GPU...\n\nIf my understanding about GCC is correct, `-lm` redundantly won't increase size of binary, since if no code from `-lm` is needed, it will not get copied.\n", "I [had](https://github.com/tensorflow/tensorflow/issues/110#issuecomment-265431453) to combine suggestions from @zym1010 and @xuanchien to build tensorflow 0.12rc0 on CentOS6.5. I've used [CROSSTOOL.tpl#L59](https://github.com/leelabcnbc/DevOps/blob/master/Docker/tensorflow/0.10.0/centos6/py35/gpu/CROSSTOOL.tpl#L59), as a place to add necessary linker flags, but I had to add `-lrt` flag.", "@i3v Thanks for feedback! My methods still works for 0.11, but I'm not sure of 0.12, as the latest stable Bazel (which somehow I assumed is needed to compile TF 0.12) won't compile on CentOS.", "\"won't compile on CentOS\" is not true. Most likely is your environment var settings are not correct.\r\n\r\nPlease read my doc for building the latest github master branch on CentOS 6.7 with gcc 4.9\r\nhttp://biophysics.med.jhmi.edu/~yliu120/tensorflow.html", "@zym1010 I might be wrong, that `-lrt` is required. The thing is - I found a suggestion to add `-lrt` before I found a suggestion to add `-lm`. And after first tests it looked like `-lrt` is enough. I was not hitting any errors. However, shortly after that, I've noticed your suggestion to add `-lm`, so I've added it as well :) .\r\n\r\nI'm not aware of any real cases, where both flags are needed. Neither I know any cases that would indicate that one flag is better than the other.\r\n\r\nConcerning Bazel - I was able to build TF 0.12 with Bazel 0.4.0 so I doubt that most recent version is absolutely necessary. Also, the only issue I ran into, when trying to compile it [was](https://github.com/bazelbuild/bazel/issues/2177) associated with the number of threads Bazel bootstrapping script creates.", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you."]}, {"number": 2290, "title": "why does sess.run()so slow on android?", "body": "I used retrain.py to retrain inception_v3 with my own images.It works will on pc with label_image.cc.It costs about 2-4s on android demo(just change the graph.pb) once sess.run().In original demo,it costs only 400ms?whats wase,the result always get wrong.I have no idea,why it works so slow?why the accuracy so low on android device?\n", "comments": ["Sorry you're hitting problems! There are various changes you need to make to successfully run the Inception v3 model in the Android demo, including altering the mean and std values that the input images are scaled by. We don't have an example of this yet, but we'll track it in this bug.\n", "trying to build label_image:main.cc on Android...\n"]}, {"number": 2289, "title": "Broadcasting ops on numpy arrays creates tons of ops", "body": "Hi,\n\nI had a little toy project where I took in some data from numpy:\n\n```\nx_data = np.zeros([18145, 3], np.float32)\nb_init = np.zeros([1, 3], np.float32)\nb_init[0][0] = 16.0\nb_init[0][1] = 128.0\nb_init[0][2] = 128.0\nW_init = np.zeros([3, 3], np.float32)\n\n[fill in x_data here]\n\nW = tf.Variable(W_init)\nb = tf.Variable(b_init)\ny = tf.matmul(x_data - b, W)\n```\n\nThis took a _lot_ of time and memory to set up (several minutes), and it was nowhere obvious why.\n\nIt eventually turned out that the broadcasting on the subtraction is what's causing the issue; it creates one op per line in x_data. I needed to write\n\n```\ny = tf.matmul(tf.constant(x_data) - b, W)\n```\n\nto make it fast.\n\nShouldn't this be the default behavior?\n", "comments": ["Perhaps we need the `__array_priority__` hack in `tf.Variable`? We already use it in `tf.Tensor` [here](https://github.com/tensorflow/tensorflow/blob/2296dd8060ce77c71fc820c77442835f050399dd/tensorflow/python/framework/ops.py#L407). Operator overloading is a mess....\n", "Sounds like that would fix it, since making \"b\" a `tf.constant` instead of `Variable` makes it work\n"]}, {"number": 2288, "title": "NaN checker does not check outputs of optimizers", "body": "Hi,\n\nI noticed this while debugging fp16 support for the Adam optimizer, so I thought I'd write it down here before it was forgotten.\n\nI had problems with NaNs in my pipeline, but couldn't really figure out where they came from. So I called tf.add_check_numerics_ops(), but it turns out that if the Adam optimizer creates a NaN (in my case, because the default epsilon of 1e-8 was too small for fp16), it is not actually checked.\n\nInstead, you get the error on a later Read op (when the NaN is attempted used), which can be very confusing, especially since there's a lot of Read ops and it's not always easy to figure out which one the error message is about.\n", "comments": ["Are you calling `add_check_numerics_ops` after calling `AdamOptimizer`? In `add_check_numerics` I see that it adds check numerics ops to all ops present in the graph\n", "Yes, I called it after AdamOptimizer. I believe the optimizer is special because nothing reads from its outputs (before the next iteration), and that's why it falls through the cracks.\n", "Closing due to inactivity. Feel free to open a new github issue if the problem still persists in recent versions."]}, {"number": 2287, "title": "update local repo", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I'm so sorry for this misoperation. Please ignore this request.\n", "new test "]}, {"number": 2286, "title": "Why resize_image_with_crop_or_pad require fully defined image?", "body": "I would like to ask, why `tensorflow.image.resize_image_with_crop_or_pad` require fully defined image? Wouldn't it be better to have some function that is more general and that can handle dynamically created tensor?\n\nI am currently playing with this function and I would like to use it for not fully defined Tensors.\n\nIs there some plan to implement some more complex `crop_tensor` function that would work with dynamic tensors? And that would possibly be able to handle https://github.com/tensorflow/tensorflow/issues/2284 \n", "comments": ["I'm going to merge this thread into the extremely related #1029.  We'd be happy to accept PRs for either one.\n"]}, {"number": 2285, "title": "the bug of using multiple GPUs, related to tf.Variable pinned to CPU", "body": "### Environment info\n\nOperating System: Ubuntu 14.04\n\nInstalled version of CUDA and cuDNN: 7.5 and 4.0.7\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from sources, provide the commit hash: 4a4f2461533847dde239851ecebe5056088a828c\n### Steps to reproduce\n\nRun the following code\n\n``` python\nimport tensorflow as tf\n\ndef main():\n    a = tf.Variable(1)\n    init_a = tf.initialize_all_variables()\n    with tf.Session() as sess:\n        sess.run(init_a)\n\n    with tf.device(\"/gpu:0\"):\n        b = tf.constant(2)\n        init_b = tf.initialize_all_variables()\n    with tf.Session() as sess:\n        sess.run(init_b)\n\n    with tf.device(\"/cpu:0\"):\n        c = tf.Variable(2)\n        init_c = tf.initialize_all_variables()\n    with tf.Session() as sess:\n        sess.run(init_c)\n\n    with tf.device(\"/gpu:0\"):\n        d = tf.Variable(2)\n        init_d = tf.initialize_all_variables()\n    with tf.Session() as sess:\n        sess.run(init_d)\n\nif __name__ == '__main__':\n    main()\n```\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n\n```\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX TITAN X\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.266\npciBusID 0000:05:00.0\nTotal memory: 12.00GiB\nFree memory: 11.02GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 1 with properties: \nname: GeForce GTX 980\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.2785\npciBusID 0000:09:00.0\nTotal memory: 4.00GiB\nFree memory: 3.91GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 0 to device ordinal 1\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 1 to device ordinal 0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 1 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y N \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 1:   N Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:756] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:05:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:756] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX 980, pci bus id: 0000:09:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:756] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:05:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:756] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX 980, pci bus id: 0000:09:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:756] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:05:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:756] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX 980, pci bus id: 0000:09:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:756] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:05:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:756] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX 980, pci bus id: 0000:09:00.0)\nTraceback (most recent call last):\n  File \"test_multi_gpu.py\", line 30, in <module>\n    main()\n  File \"test_multi_gpu.py\", line 26, in main\n    sess.run(init_d)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 332, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 572, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 652, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 672, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'Variable_2': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available\n     [[Node: Variable_2 = Variable[container=\"\", dtype=DT_INT32, shape=[], shared_name=\"\", _device=\"/device:GPU:0\"]()]]\nCaused by op u'Variable_2', defined at:\n  File \"test_multi_gpu.py\", line 30, in <module>\n    main()\n  File \"test_multi_gpu.py\", line 23, in main\n    d = tf.Variable(2)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 211, in __init__\n    dtype=dtype)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 292, in _init_from_args\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/state_ops.py\", line 139, in variable_op\n    container=container, shared_name=shared_name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_state_ops.py\", line 351, in _variable\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py\", line 693, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2177, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1161, in __init__\n    self._traceback = _extract_stack()\n```\n\nI also noticed that the documentation for [Using GPUs](https://www.tensorflow.org/versions/r0.8/how_tos/using_gpu/index.html) doesn't mentioned about tf.Variable, it only involves the tf.constant and tf.matmul.\n\n**OK, I found the documentation from [Convolutional Neural Networks]**(https://www.tensorflow.org/versions/r0.8/tutorials/deep_cnn/index.html),\nquotes:\n\n```\nAll variables are pinned to the CPU and accessed via tf.get_variable() in order to share them in a multi-GPU version. See how-to on Sharing Variables.\n```\n\nI want ask that since tf.Variables is pinned to CPU by tensorflow, could we fix this error? Do we need to looking very carefully to exclude the tf.Variable declaration outside the `with tf.device('/gpu:xx')` scope, or use netsted `with tf.device(None)` to handle it?\n", "comments": ["So, there are some ops that are not valid for tf.device(), such as tf.nn.local_response_normalization(),\nSee the code below:\n\n``` python\n    with tf.device(\"/gpu:0\"):\n        d = tf.placeholder(\"float\", shape=[100, 100, 100, 10])\n        with tf.device(None):\n            lrn1 = tf.nn.local_response_normalization(d, depth_radius=5, bias=1.0, alpha=1e-4, beta=0.75)\n        lrn2 = tf.nn.local_response_normalization(d, depth_radius=5, bias=1.0, alpha=1e-4, beta=0.75)\n        init_d = tf.initialize_all_variables()\n    with tf.Session() as sess:\n        sess.run(init_d)\n        r = np.random.randn(100, 100, 100, 10)\n        sess.run(lrn1, feed_dict={d: r}) #Run ok\n        sess.run(lrn2, feed_dict={d: r}) # Error\n```\n\nThe output is below:\n\n```\nTraceback (most recent call last):\n  File \"test_multi_gpu.py\", line 44, in <module>\n    main()\n  File \"test_multi_gpu.py\", line 40, in main\n    sess.run(lrn2, feed_dict={d: r})\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 332, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 572, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 652, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 672, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'LRN_1': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available\n     [[Node: LRN_1 = LRN[alpha=0.0001, beta=0.75, bias=1, depth_radius=5, _device=\"/device:GPU:0\"](Placeholder)]]\nCaused by op u'LRN_1', defined at:\n  File \"test_multi_gpu.py\", line 44, in <module>\n    main()\n  File \"test_multi_gpu.py\", line 34, in main\n    lrn2 = tf.nn.local_response_normalization(d, depth_radius=5, bias=1.0, alpha=1e-4, beta=0.75)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 737, in lrn\n    bias=bias, alpha=alpha, beta=beta, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py\", line 693, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2177, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1161, in __init__\n    self._traceback = _extract_stack()\n```\n\n**The reason** of this error might be clear enough I think. There're some internal tf.Variable in the `tf.nn.local_response_normalization` which we couldn't use outside code to remain the computation node to specified gpu while excluding all the internal variables.\n\nFor now, I think tensorflow should do either of two things below:\n1. Make tf.Variable not influenced by the tf.device(). (This might be preferred.)\n2. List the ops out which needs to use `tf.device(None)` to help user finish their code, right?\n", "The high-level problem should be fixed by @vrv's ongoing work to improve device placement. (Making `tf.Variable` ignore `tf.device()` will not work, because many of our users, especially in distributed settings, use this to configure parameter servers.) In the short term,  try using soft placement in your session constructor:\n\n``` python\nconfig = tf.ConfigProto(allow_soft_placement=True)\nwith tf.Session(config=config) as sess:\n    # ...\n```\n", "Thanks for your suggestion, it seems using `allow_soft_placement=True` will fix the issue. As stated in #2292 , it's better to improve the corresponding document for user to know this.\n"]}, {"number": 2284, "title": "tensorflow `resize_image_with_crop_or_pad` for whole batch", "body": "Currently there exists `tensorflow.image.resize_image_with_crop_or_pad()` that works well, but for me it would be very beneficial to have also something that would be able to perform crop on whole batch of images or even conv2d or conv2d_transpose outputs.\n\nIn other words it should work on Tensor of shape `[batch, height, width, channels]`.\n\nSo far I have found this mentioned here http://stackoverflow.com/questions/33944683/tensorflow-map-operation-for-tensor\n\nAnd for now I'd probably go with the `tf.while_loop`, but general built in function in tensorflow would be helpful.\n", "comments": ["If you have a batch of images in a `[batch, height, width, channels]`, can't you just use `tf.slice()` to crop the whole batch?\n", "Closing for now, since slice does seem to be a good solution.  @ziky90: Please comment if slice doesn't fit your use case. \n", "@mrry How would you use slice to accomplish this?", "@cancan101 Assuming the crop region should be at the same location in each image, you would just take `images[:, y_start:y_end, x_start:x_end, :]` where `x_start`, `x_end`, `y_start`, and `y_end` define the corners of the crop region.", "okay, but then need to copy over logic like:\r\n```python\r\n  width_diff = target_width - width\r\n  offset_crop_width = max_(-width_diff // 2, 0)\r\n  offset_pad_width = max_(width_diff // 2, 0)\r\n\r\n  height_diff = target_height - height\r\n  offset_crop_height = max_(-height_diff // 2, 0)\r\n  offset_pad_height = max_(height_diff // 2, 0)\r\n...\r\n```\r\nto get those values.", "Sure, but you don't need to use a `tf.while_loop()`, which was suggested in the original issue.\r\n\r\nI don't feel strongly either way adding such a function in `image_ops.py`... feel free to open a new issue with the proposed interface if you'd like!"]}, {"number": 2283, "title": "IndexedSlices problem on porting ptb_word_lm.py to multi-GPU-towers", "body": "I try to port RNN model in [ptb_word_lm.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/ptb_word_lm.py) to multi-GPU cards. I follow the multi-tower style in [cifar10_multi_gpu_train.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/cifar10/cifar10_multi_gpu_train.py) \nHowever, I found the \"grads\" returned by `tf.clip_by_global_norm(tf.gradients(cost, tvars), config.max_grad_norm)` is not list of Tensor type. It is a list of type `tensorflow.python.framework.ops.IndexedSlices`.  Now I need to sum & average the lists of \"grads\" returned by multiple GPU towers into one list of IndexedSlices or Tensor, in order to pass it into `self._train_op = optimizer.apply_gradients(zip(grads, tvars))`\n I've tried the `tf.convert_to_tensor` to conver IndexedSlices to Tensor, but it failed with the following errors:\n\n  File \"ptb_word_lm.py\", line 328, in <module>\n    tf.app.run()\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"ptb_word_lm.py\", line 305, in main\n    m = PTBModel(is_training=True, config=config)\n  File \"ptb_word_lm.py\", line 152, in __init__\n    grads_0_tensor = tf.convert_to_tensor(grads[0])\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 566, in convert_to_tensor\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py\", line 77, in _IndexedSlicesToTensor\n    % str(value))\nValueError: Tensor conversion requested for IndexedSlices without dense_shape: IndexedSlices(indices=Tensor(\"model/gradients/concat_1:0\", shape=(400,), dtype=int32), values=Tensor(\"model/clip_by_global_norm/model/clip_by_global_norm/_0:0\", shape=(?, 200), dtype=float32))\n\nIs it a bug?\nHow could I reduce_mean these IndexedSlices(reduce_mean seems only accept \"Tensor\" as input)? Or is there any existing code that parallelize RNN in multi-GPU-towers style?\n\nThanks a lot in advance!\n", "comments": ["@smartcat2010: Could you try to isolate where the problematic `IndexedSlices` object without a dense shape is coming from?  It's likely that whoever is creating that object needs to attach a shape at creation time. \n", "Hi, girving:\nThe problematic IndexedSlices comes from \"tf.gradients(cost, tvars)\". Because if I remove the wrapper \"tf.clip_by_global_norm\" and leave \"grads, _ = tf.gradients(cost, tvars)\" alone, the same error shows again.\n", "@smartcat2010: We need more information than that it comes from somewhere inside `tf.gradients`, which contains something like half of TensorFlow.  The best way would to be either find the specific line that constructs the `IndexedSlices` without specifying a `dense_shape`, or to produce a minimized test case.  Here are some ways to go about the former:\n1. Add an assert in the constructor of `IndexedSlices` that `dense_shape` is specified.  If you're lucky, this will break on the problematic line.\n2. In the constructor of `IndexedSlices`, print `id(self)` if `dense_shape` is not specified, and compare it to the `id` of the problematic `IndexedSlices` you're getting out.\n", "Closing for now.  @smartcat2010: Please comment if you have more information and I'll reopen. \n", "I have the same problem, my code is as following. Please help me.\n\n```\ntower_outputs = []\ntower_loss = []\ntower_grad_norms = []\ntower_clipped_grads = []\n\nfor i in xrange(1, 2):\n  with tf.device('/gpu:%d' % i):\n    # Training outputs and losses.\n    if forward_only:\n      outputs, losses = tf.nn.seq2seq.model_with_buckets(\n          encoder_inputs_split[i], decoder_inputs_split[i], targets_split[i],\n          target_weights_split[i], buckets, lambda x, y: seq2seq_f(x, y, True),\n          softmax_loss_function=softmax_loss_function)\n      # If we use output projection, we need to project outputs for decoding.\n      if output_projection is not None:\n        for b in xrange(len(buckets)):\n          outputs[b] = [\n              tf.matmul(output, output_projection[0]) + output_projection[1]\n              for output in outputs[b]\n          ]\n    else:\n      outputs, losses = tf.nn.seq2seq.model_with_buckets(\n          encoder_inputs_split[i], decoder_inputs_split[i], targets_split[i],\n          target_weights_split[i], buckets,\n          lambda x, y: seq2seq_f(x, y, False),\n          softmax_loss_function=softmax_loss_function)\n\n    tower_outputs.append(outputs)\n    tower_loss.append(losses)\n\n    # Gradients and SGD update operation for training the model.\n    params = tf.trainable_variables()\n    print(params)\n    tmp_norms = []\n    tmp_clip_grad = []\n    if not forward_only:\n      for b in xrange(len(buckets)):\n        gradients = tf.gradients(losses[b], params)\n        clipped_gradients, norm = tf.clip_by_global_norm(gradients, max_gradient_norm)\n        tmp_norms.append(norm)\n        tmp_clip_grad.append(clipped_gradients)\n        tower_grad_norms.append(tmp_norms)\n        tower_clipped_grads.append(tmp_clip_grad)\n     print('gpu:%d' % i)\n     tf.get_variable_scope().reuse_variables()\n\nself.outputs = tf.concat(1, tower_outputs)\nself.losses = tf.reduce_mean(tower_loss, 0)\n\nif not forward_only:\n  self.gradient_norms = tf.reduce_mean(tower_grad_norms, 0)\n  gradients = self.average_gradients(tower_clipped_grads)\n  self.updates = []\n  for b in xrange(len(buckets)):\n    clipped_gradients = gradients[b]\n    self.updates.append(opt.apply_gradients(\n        zip(clipped_gradients, params), global_step=self.global_step))\n\nself.saver = tf.train.Saver(tf.all_variables())\n\ndef average_gradients(self, tower_grads):\n  average_grads = []\n  for tower_grad in zip(*tower_grads):\n    # Average over the 'tower' dimension.\n    grad = tf.reduce_mean(tower_grad, 1)\n\n    # Keep in mind that the Variables are redundant because they are shared across towers. So .. we will just return the first tower's pointer to the Variable.\n    average_grads.append(grad)\n  return average_grads\n```\n", "@lijiajia Can you (1) include your error message and (2) isolate which IndexedSlices is responsible as I requested of @smartcat2010?\n", "The error message is as following. The problematic IndexedSlices comes from \"gradients = tf.gradients(losses[b], params)\". \n\nThe calculated tf.gradients returns:\n[Tensor, IndexedSlices, IndexedSlices, Tensor, Tensor, Tensor, IndexedSlices, Tensor, Tensor]\n\nTraceback (most recent call last):\n  File \"train.py\", line 277, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"train.py\", line 274, in main\n    train()\n  File \"train.py\", line 168, in train\n    model = create_model(sess, False)\n  File \"train.py\", line 142, in create_model\n    forward_only=forward_only, gpus=FLAGS.gpus)\n  File \"/root/dialog_sms/seq2seq_model.py\", line 200, in **init**\n    gradients = self.average_gradients(tower_clipped_grads)\n  File \"/root/dialog_sms/seq2seq_model.py\", line 213, in average_gradients\n    grad = tf.reduce_mean(tower_grad, 1)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py\", line 1089, in reduce_mean\n    keep_dims, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 1345, in _mean\n    keep_dims=keep_dims, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 454, in apply_op\n    as_ref=input_arg.is_ref)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 621, in convert_to_tensor\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 630, in _autopacking_conversion_function\n    return _autopacking_helper(v, inferred_dtype, name or \"packed\")\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 576, in _autopacking_helper\n    converted_elem = _autopacking_helper(elem, dtype, str(i))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 592, in _autopacking_helper\n    constant_op.constant(elem, dtype=dtype, name=str(i)))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py\", line 163, in constant\n    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py\", line 353, in make_tensor_proto\n    _AssertCompatible(values, dtype)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py\", line 290, in _AssertCompatible\n    (dtype.name, repr(mismatch), type(mismatch).__name__))\nTypeError: Expected float32, got <tensorflow.python.framework.ops.IndexedSlices object at 0x7f2ab403bfd0> of type 'IndexedSlices' instead.\n", "Copying my comment above for convenience:\n\n@lijiajia We need more information than that it comes from somewhere inside `tf.gradients`, which contains something like half of TensorFlow. The best way would to be either find the specific line that constructs the `IndexedSlices` without specifying a `dense_shape`, or to produce a minimized test case. Here are some ways to go about the former:\n1. Add an assert in the constructor of `IndexedSlices` that `dense_shape` is specified. If you're lucky, this will break on the problematic line.\n2. In the constructor of `IndexedSlices`, `print id(self)` if `dense_shape` is not specified, and compare it to the id of the problematic `IndexedSlices` you're getting out.\n", "Closing due to inactivity. Please re-open if you have more info.\n", "I ran into the same problem and the indexedslices are coming from the keras **Embedding** Layer based on my own model. Right now I'm minimizing them individually. Is there a way to reduce and average these for multi-gpu gradient descent."]}, {"number": 2282, "title": "Fix distributed example code document", "body": "Current distributed document of example code is invalid. Simply fix it.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@tensorflow-jenkins test this please\n", "Merged. Thanks, @itsarbit \n"]}, {"number": 2281, "title": "Weird partial_run segment fault bug", "body": "### Environment info\n\nOperating System: Ubuntu 14.04\n\nInstalled version of CUDA and cuDNN: 7.5 and 4.0.7\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n```\n/usr/local/cuda/lib64/libcudadevrt.a    /usr/local/cuda/lib64/libcudart.so.7.5.18  /usr/local/cuda/lib64/libcudnn.so.4\n/usr/local/cuda/lib64/libcudart.so      /usr/local/cuda/lib64/libcudart_static.a   /usr/local/cuda/lib64/libcudnn.so.4.0.7\n/usr/local/cuda/lib64/libcudart.so.7.5  /usr/local/cuda/lib64/libcudnn.so          /usr/local/cuda/lib64/libcudnn_static.a\n```\n\nIf installed from sources, provide the commit hash: 4a4f2461533847dde239851ecebe5056088a828c\n### Steps to reproduce\n\n``` python\nimport tensorflow as tf\nimport numpy as np\n\n# @profile\ndef main():\n    a = tf.Variable(tf.random_normal(shape=[1000, 1000]))\n    b = a * 2\n    c = b * 3\n    sess = tf.Session()\n    sess.run(tf.initialize_all_variables())\n\n\n    sess.run(c)\n\n    # Uncomment this code region, and segment fault will be gone.\n    # for _ in xrange(1000):\n    #     h = sess.partial_run_setup([b, c], [])\n    #     res = sess.partial_run(h, [b, c])\n\n    for _ in xrange(1000):\n        d = sess.run(b)\n    for _ in xrange(1000):\n        e = sess.run(c)\n    for _ in xrange(1000):\n        f = sess.run([b, c])\n    for _ in xrange(1000):\n        h = sess.partial_run_setup([b, c], [])\n        res = sess.partial_run(h, [b, c])\n\nif __name__ == '__main__':\n    main()\n```\n### What have you tried?\n1. Uncomment the commented region will make segment fault disappear.\n", "comments": ["Thanks for reporting.  It will be fixed in the next pull.\n"]}, {"number": 2280, "title": "Can we optimize redundant computation?", "body": "I just find that for now, tensorflow haven't optimize the computation and leads to very slow performance. This issue I think might be very severe while compute the gradients to all trainable variables of a DNN network via **tf.gradients()**. To simplify the issue demonstration, I wrote a toy code snippets and run a small experiment.\n\n``` python\nimport tensorflow as tf\nimport numpy as np\n\n@profile\ndef main():\n    a = tf.Variable(tf.random_normal(shape=[1000, 1000]))\n    b = a * 2\n    c = b * 3\n    sess = tf.Session()\n    sess.run(tf.initialize_all_variables())\n\n    for _ in xrange(1000):\n        d = sess.run(b)\n    for _ in xrange(1000):\n        e = sess.run(c)\n    for _ in xrange(1000):\n        f = sess.run([b, c])\n\nif __name__ == '__main__':\n    main()\n```\n\nAnd here's the profile result using line_profiler.\n\n```\nTimer unit: 1e-06 s\n\nTotal time: 7.54528 s\nFile: test_tf_time.py\nFunction: main at line 4\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n     4                                           @profile\n     5                                           def main():\n     6         1         9650   9650.0      0.1      a = tf.Variable(tf.random_normal(shape=[1000, 1000]))\n     7         1         1611   1611.0      0.0      b = a * 2\n     8         1         1501   1501.0      0.0      c = b * 3\n     9         1       375872 375872.0      5.0      sess = tf.Session()\n    10         1       232079 232079.0      3.1      sess.run(tf.initialize_all_variables())\n    11                                           \n    12      1001         1663      1.7      0.0      for _ in xrange(1000):\n    13      1000      1321475   1321.5     17.5          d = sess.run(b)\n    14      1001         2179      2.2      0.0      for _ in xrange(1000):\n    15      1000      1570674   1570.7     20.8          e = sess.run(c)\n    16      1001         2835      2.8      0.0      for _ in xrange(1000):\n    17      1000      4025739   4025.7     53.4          f = sess.run([b, c])\n```\n\nYou see what I mean? If the graph is like below:\n\n```\na-------b--------c\n     *         *\n     |          |\n    2         3\n```\n\nAnd if calculating `b` and `c` will consume comparable 20% time, then when I want to get [b, c], it should be optimized to consume around 20% time, right? Since I already have result of \"b\" in the middle of the procedure if I want to get result of \"c\".\n\nLet's think about the gradient computing as I stated in the head of this issue. You have, say 10 layers DNN.\nYou want to get the gradients to all the 10 \"W\" variables, then will the computation being very unoptimized? Since you treat the 10 gradients separately, you will compute gradients to the 10th layer first, and get the result, and compute gradients to the 9th layer while not using the result of 10th layer, and so on.\n", "comments": ["Seems tensorflow has optimized when the previous `a` is a placeholder. Here's another version of toy snippet.\n\n```\nTimer unit: 1e-06 s\n\nTotal time: 28.7196 s\nFile: test_tf_time.py\nFunction: main at line 4\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n     4                                           @profile\n     5                                           def main():\n     6                                               # a = tf.Variable(tf.random_normal(shape=[1000, 1000]))\n     7         1         2280   2280.0      0.0      a = tf.placeholder(\"float\", shape=[1000, 1000])\n     8         1        54021  54021.0      0.2      mat = np.random.randn(1000, 1000)\n     9         1         2338   2338.0      0.0      b = a * 2\n    10         1         1809   1809.0      0.0      c = b * 3\n    11         1       359919 359919.0      1.3      sess = tf.Session()\n    12         1         9476   9476.0      0.0      sess.run(tf.initialize_all_variables())\n    13                                           \n    14      1001         3013      3.0      0.0      for _ in xrange(1000):\n    15      1000       462844    462.8      1.6          h = sess.partial_run_setup([b, c], [a])\n    16      1000      7600600   7600.6     26.5          res = sess.partial_run(h, [b, c], feed_dict={a: mat})\n    17      1001         3028      3.0      0.0      for _ in xrange(1000):\n    18      1000      5680910   5680.9     19.8          d = sess.run(b, feed_dict={a: mat})\n    19      1001         2960      3.0      0.0      for _ in xrange(1000):\n    20      1000      6443068   6443.1     22.4          e = sess.run(c, feed_dict={a: mat})\n    21      1001         2853      2.9      0.0      for _ in xrange(1000):\n    22      1000      8090507   8090.5     28.2          f = sess.run([b, c], feed_dict={a: mat})\n```\n\nBut using `partial_run` while `a` is a Variable is useless.\n\n```\nTimer unit: 1e-06 s\n\nTotal time: 10.8776 s\nFile: test_tf_time.py\nFunction: main at line 4\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n     4                                           @profile\n     5                                           def main():\n     6         1         9836   9836.0      0.1      a = tf.Variable(tf.random_normal(shape=[1000, 1000]))\n     7                                               # a = tf.placeholder(\"float\", shape=[1000, 1000])\n     8                                               # mat = np.random.randn(1000, 1000)\n     9         1         1608   1608.0      0.0      b = a * 2\n    10         1         1525   1525.0      0.0      c = b * 3\n    11         1       427775 427775.0      3.9      sess = tf.Session()\n    12         1       181565 181565.0      1.7      sess.run(tf.initialize_all_variables())\n    13                                           \n    14      1001         2385      2.4      0.0      for _ in xrange(1000):\n    15      1000       196307    196.3      1.8          h = sess.partial_run_setup([b, c], [])\n    16      1000      3327579   3327.6     30.6          res = sess.partial_run(h, [b, c])\n    17                                                   # res = sess.partial_run(h, [b, c], feed_dict={a: mat})\n    18      1001         2048      2.0      0.0      for _ in xrange(1000):\n    19                                                   # d = sess.run(b, feed_dict={a: mat})\n    20      1000      1428858   1428.9     13.1          d = sess.run(b)\n    21      1001         2058      2.1      0.0      for _ in xrange(1000):\n    22                                                   # e = sess.run(c, feed_dict={a: mat})\n    23      1000      1449288   1449.3     13.3          e = sess.run(c)\n    24      1001         2769      2.8      0.0      for _ in xrange(1000):\n    25                                                   # f = sess.run([b, c], feed_dict={a: mat})\n    26      1000      3843996   3844.0     35.3          f = sess.run([b, c])\n```\n\nI know transfer content from RAM to gpu memory is very time consuming, is this fact related?\nI mean, when `a` is a placeholder, since the time of placeholder transfer dominates, so getting `[b, c]` with or without `partial_run` is time comparable to getting `b` or `c`?\n", "`partial_run` should enable these kinds of computation reuse. Can you check your device placement to make sure you are not spending your time waiting for CPU->GPU memory transfers?\n\nAlso, is this issue fixed if you consolidate and fetch all your outputs in a single `run`?\n\nYou can enable device placement logging by creating your session with `config = tf.ConfigProto(log_device_placement=True)`.\n\nAlso more generally, you can see more details about CPU-GPU transfers by using following snippets (not sure if `partial_run` looks at run_options though)\n\n```\nrun_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\nrun_metadata = tf.RunMetadata()\nsess.run(..., options=run_options, run_metadata=run_metadata)\nrun_summarize(run_metadata)\n\ndef run_summarize(run_metadata):\n  global run_metadata\n  print \"***\"\n  for device in run_metadata.step_stats.dev_stats:\n    print device.device\n    for node_stats in device.node_stats:\n      print \"   \", node_stats.node_name\n\n```\n", "Here are some more investigation.\n\n```\nTimer unit: 1e-06 s\n\nTotal time: 12.3761 s\nFile: test_tf_time.py\nFunction: main at line 4\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n     4                                           @profile\n     5                                           def main():\n     6         1          515    515.0      0.0      with tf.device(\"/gpu:1\"):\n     7         1            3      3.0      0.0          s = 500\n     8                                                   # a = tf.Variable(tf.random_normal(shape=[s, s]))\n     9         1         2327   2327.0      0.0          a = tf.placeholder(\"float\", shape=[s, s])\n    10         1        15789  15789.0      0.1          np_a = np.random.randn(s, s)\n    11         1         2305   2305.0      0.0          b = tf.matmul(a, a)\n    12         1         1878   1878.0      0.0          c = tf.matmul(b, b)\n    13         1          567    567.0      0.0          init = tf.initialize_all_variables()\n    14         1       264673 264673.0      2.1      sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True))\n    15         1         9307   9307.0      0.1      sess.run(init)\n    16                                           \n    17                                           \n    18         1       140504 140504.0      1.1      sess.run(c, feed_dict={a: np_a})\n    19                                           \n    20      1001         1911      1.9      0.0      for _ in xrange(1000):\n    21      1000       240162    240.2      1.9          h = sess.partial_run_setup([b, c], [a])\n    22      1000      2517772   2517.8     20.3          res = sess.partial_run(h, [b, c], feed_dict={a: np_a})\n    23                                           \n    24      1001         2108      2.1      0.0      for _ in xrange(1000):\n    25      1000      1971059   1971.1     15.9          d = sess.run(b, feed_dict={a: np_a})\n    26      1001         1938      1.9      0.0      for _ in xrange(1000):\n    27      1000      2251912   2251.9     18.2          e = sess.run(c, feed_dict={a: np_a})\n    28      1001         2102      2.1      0.0      for _ in xrange(1000):\n    29      1000      2627560   2627.6     21.2          f = sess.run([b, c], feed_dict={a: np_a})\n    30      1001         2021      2.0      0.0      for _ in xrange(1000):\n    31      1000      2319638   2319.6     18.7          f = sess.run([c, c], feed_dict={a: np_a})\n```\n\nchange `a` from **placeholder** to **Variable**. Then total running time decrease to **half** of above.\n\n```\nTimer unit: 1e-06 s\n\nTotal time: 5.92345 s\nFile: test_tf_time.py\nFunction: main at line 4\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n     4                                           @profile\n     5                                           def main():\n     6         1          539    539.0      0.0      with tf.device(\"/gpu:1\"):\n     7         1            1      1.0      0.0          s = 500\n     8         1        16648  16648.0      0.3          a = tf.Variable(tf.random_normal(shape=[s, s]))\n     9                                                   # a = tf.placeholder(\"float\", shape=[s, s])\n    10                                                   # np_a = np.random.randn(s, s)\n    11         1         1856   1856.0      0.0          b = tf.matmul(a, a)\n    12         1         1801   1801.0      0.0          c = tf.matmul(b, b)\n    13         1          823    823.0      0.0          init = tf.initialize_all_variables()\n    14         1       315201 315201.0      5.3      sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True))\n    15         1       161516 161516.0      2.7      sess.run(init)\n    16                                           \n    17                                           \n    18                                               # sess.run(c, feed_dict={a: np_a})\n    19         1       126629 126629.0      2.1      sess.run(c)\n    20                                           \n    21      1001         1401      1.4      0.0      for _ in xrange(1000):\n    22                                                   # h = sess.partial_run_setup([b, c], [a])\n    23                                                   # res = sess.partial_run(h, [b, c], feed_dict={a: np_a})\n    24      1000       194673    194.7      3.3          h = sess.partial_run_setup([b, c], [])\n    25      1000      1121128   1121.1     18.9          res = sess.partial_run(h, [b, c])\n    26                                           \n    27      1001         1130      1.1      0.0      for _ in xrange(1000):\n    28                                                   # d = sess.run(b, feed_dict={a: np_a})\n    29      1000       733446    733.4     12.4          d = sess.run(b)\n    30      1001         1003      1.0      0.0      for _ in xrange(1000):\n    31                                                   # e = sess.run(c, feed_dict={a: np_a})\n    32      1000       854119    854.1     14.4          e = sess.run(c)\n    33      1001         1283      1.3      0.0      for _ in xrange(1000):\n    34                                                   # f = sess.run([b, c], feed_dict={a: np_a})\n    35      1000      1240861   1240.9     20.9          f = sess.run([b, c])\n    36      1001         1387      1.4      0.0      for _ in xrange(1000):\n    37                                                   # f = sess.run([c, c], feed_dict={a: np_a})\n    38      1000      1148003   1148.0     19.4          f = sess.run([c, c])\n```\n\nUsing these toy snippets isn't my intension. I intended to find out where is the slow component of tensorflow framework.\nActually, I can confirm that current `tensorflow` runs kind of very slow compared to `Torch` based on my practical experiments running.\n\nI propose this issue because I'm facing practical performance problem with tensorflow. I'm using tensorflow to re-implement a project which is implemented with `Torch`.\nFor now the result and code seems run properly, and the experiments result are similar to that project, the issue is the computation time.\nWith **same** network architecture, **same** batchsize, \"Torch\" project's single mini-batch forward and backward plus some extra computation takes only, say around **1** second, while my tensorflow version implementation takes around **3** seconds to finish a min-batch (actually, in torch they just backward with gradient Input, while I compute gradients with gradient Input, then apply gradients via tensorflow since it has no backward API), and almost 80+% time is consuming by tensorflow **API** according to `line_profiler`.\nSo, there seems no place of code to diagnose except \"tensorflow API\" (the key component I use is feedforward, compute gradients and apply gradients).\n\nIt's really hard for me to wait 3 days experiments with tensorflow compared to one day with `Torch`.\n", "@mrry: Looks like this fell through the cracks.  Do you know what's wrong?\n\n@myme5261314: I'd guess that the reason `[b, c]` is slower than just `c` is that all you're timing is the overhead of converting things back to numpy.  I don't think it's related to your original motivating problem.\n", "The redundant computation _is_ eliminated: `b.op` only executes once when you call `sess.run([b, c])`. Running this locally, it seems that almost all of the overhead in this microbenchmark is due to the additional copying of another 4MB tensor back to Python. To check this, you can see that `sess.run([b.op, c])` (which explicitly runs both ops but only copies back the output of `c`) takes the same time as `sess.run(c)`.\n\nThe moral of the story is that you shouldn't copy tensors back to Python if you don't need them. Some of the immediate computation work is trying to make this more efficient, by returning handles to computed tensors back to Python rather than the entire values.\n"]}, {"number": 2279, "title": "[tf.learn] Remove accidentally repeated code", "body": "Tried to fix #2198 but I guess I'll fire a separate PR when I get a chance\n", "comments": ["Can one of the admins verify this patch?\n", "Test this please, Jenkins.\n", "@petewarden @vrv Could any of you run the Jenkins again? I just found a huge portion of repeated code - maybe from conflicts not being resolved correctly during internal branch merge. Also cc @ilblackdragon \n", "@ilblackdragon Good to know. I'll take a look at graph_actions later when I get a chance. I've reverted the changes. Let's just keep this PR to be removing those repeated lines (tests passed since the repeated code are inside a function block). Could you take a look and merge this first or people will have some issues? \n", "@tensorflow-jenkins test this please.\n"]}, {"number": 2278, "title": "OS X segfault on import", "body": "OS X 10.11.2, with CUDA:\n\n```\n/Developer/NVIDIA/CUDA-7.5/lib/libcudadevrt.a\n/Developer/NVIDIA/CUDA-7.5/lib/libcudart.7.5.dylib\n/Developer/NVIDIA/CUDA-7.5/lib/libcudart.dylib -> libcudart.7.5.dylib\n/Developer/NVIDIA/CUDA-7.5/lib/libcudart_static.a\n/Developer/NVIDIA/CUDA-7.5/lib/libcudnn.5.dylib\n/Developer/NVIDIA/CUDA-7.5/lib/libcudnn.dylib -> libcudnn.5.dylib\n/Developer/NVIDIA/CUDA-7.5/lib/libcudnn_static.a\n```\n\nTensorflow built according to https://medium.com/@fabmilo/how-to-compile-tensorflow-with-cuda-support-on-osx-fd27108e27e1#.v8ibv617m, main difference being that CUDA toolkit was installed from NVidia installer instead of via `brew cask install cuda`, and using homebrew Python 3.5 instead of Anaconda Python.\n\nIn other words:\n\n1) Install CUDA toolkit.\n2) Download cudnn-7.5-osx-x64-v5.0-rc.tgz and move files to /Developer/NVIDIA/CUDA-7.5/{include,lib}\n3) Install bazel 0.2.1 via brew.\n4) Create Python 3.5 virtualenv, install numpy 1.11 into it so tensorflow can build against it(?).\n5) Clone tensorflow repo.\n6) Build with:\n\n```\nPYTHON_BIN_PATH=\"/Users/pikeas/.virtualenvs/hnn/bin/python\" CUDA_TOOLKIT_PATH=\"/Developer/NVIDIA/CUDA-7.5\" CUDNN_INSTALL_PATH=\"/Developer/NVIDIA/CUDA-7.5\" TF_UNOFFICIAL_SETTING=1 TF_NEED_CUDA=1 TF_CUDA_COMPUTE_CAPABILITIES=\"3.0\" TF_CUDNN_VERSION=\"5\" TF_CUDA_VERSION=\"7.5\" TF_CUDA_VERSION_TOOLKIT=7.5 ./configure\nbazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\nbazel-bin/tensorflow/tools/pip_package/build_pip_package\n```\n\n7) `export DYLD_LIBRARY_PATH=/Developer/NVIDIA/CUDA-7.5/lib`\n8) Install built tensorflow-0.8.0-py3-none-any.whl into virtualenv.\n9) `import tensorflow` fails with:\n\n```\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.7.5.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.5.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.7.5.dylib locally\n[1]    78583 segmentation fault  python\n```\n\nI've tried removing scipy per the recent similar Linux issue, which didn't help.\n", "comments": ["I created a test file which contains just `import tensorflow`. Then:\n\n```\n$ gdb -ex r --args python test.py\nwarning: `/Users/travis/build/MacPython/numpy-wheels/numpy/build/temp.macosx-10.6-intel-3.5/Users/travis/build/MacPython/numpy-wheels/numpy/numpy/_build_utils/src/apple_sgemv_fix.o': can't open to read symbols: No such file or directory.\n...\nDozens of lines like this for different numpy files.\nMy username is not Travis and I'm not running OS X 10.6, so I have no idea where these references are from.\n...\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.7.5.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.5.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.7.5.dylib locally\n\nProgram received signal SIGSEGV, Segmentation fault.\n0x00007fff8f874152 in strlen () from /usr/lib/system/libsystem_c.dylib\n```\n", "From looking at the warnings, it does seem like there's a numpy installation problem. Are you able to run a simple test program that imports numpy?\n", "Yes, numpy installed correctly - standard pip install into the virtualenv later used by `PYTHON_BIN_PATH`. It can be imported and used without issue.\n", "Could you show me a system call trace using dtruss?\n", "It's 2200 lines of output, so I've created a gist: https://gist.github.com/pikeas/293556206511cda72b94bcf154c35ddc\n\nIt's my first use of dtrace, so let me know if this output is what you need. I invoked it by doing:\n\n```\nsudo -i\nexport DYLD_LIBRARY_PATH=/Developer/NVIDIA/CUDA-7.5/lib\ndtruss /Users/pikeas/.virtualenvs/hnn/bin/python foo.py > dtruss.out 2>&1\n```\n\nfoo.py only imports tensorflow.\n", "ImportError: dlopen(/Users/pikeas/.virtualenvs/hnn/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so, 10): Library not loaded: @rpath/libcudart.7.5.dylib\n\nWould you check if the file exists there, and try to put it?\n", "@qbx2 Hm, I'm not sure what you mean? `_pywrap_tensorflow.so` exists in that location, and if you're referring to `@rpath/libcudart.7.5.dylib`, the file is in /Developer/NVIDIA/CUDA-7.5/lib, which is where my CUDA was installed (default location from NVIDIA installer). That's why I set DYLD_LIBRARY_PATH.\n", "Would you try dtruss using following method:\n1) Launch python from any shell, and find its pid. (For me, it's 5136)\n2) `sudo dtruss -p 5136`\n3) `import tensorflow` in python interactive shell\n4) Check dtruss output\n", "@qbx2 I really appreciate the help! Here's the dtruss output for `import tensorflow` run in a separate REPL - https://gist.github.com/pikeas/ea3cfca6ee49b190319f1f2da65e57c3\n\nI think the original dtruss output may have been from a shell containing a tensorflow directory, which would have caused the output in the first Gist. Sorry for the confusion.\n", "Yes, it seems to be a numpy.random problem. Could you check if numpy.random works?\n", "Yep, works:\n\n```\n$ /Users/pikeas/.virtualenvs/hnn/bin/python -c 'from numpy.random import random; print(random(5))'\n[ 0.87424813  0.06297989  0.89478638  0.2749335   0.93622187]\n```\n", "I think the gist is truncated?\n", "@qbx2 I agree that the gist looks truncated, but I've just re-run and got the same result.\n\nIt looks like some dtruss runs stop early at the numpy.random output, but some runs reach a more useful trace:\n\n```\nwrite_nocancel(0x2, \"I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.7.5.dylib locally\\nNone\\n\\n        Returns\\n        -------\\n        result : MaskedArray\\n            The imaginary part of the masked array.\\n\\n        See Also\\n        ---\", 0x6C)       = 108 0\ngetattrlist(\"/Users\\0\", 0x7FFF93BD8DC4, 0x7FFF51E1F530)      = 0 0\ngetattrlist(\"/Users/pikeas\\0\", 0x7FFF93BD8DC4, 0x7FFF51E1F530)       = 0 0\ngetattrlist(\"/Users/pikeas/.virtualenvs\\0\", 0x7FFF93BD8DC4, 0x7FFF51E1F530)      = 0 0\ngetattrlist(\"/Users/pikeas/.virtualenvs/hnn\\0\", 0x7FFF93BD8DC4, 0x7FFF51E1F530)      = 0 0\ngetattrlist(\"/Users/pikeas/.virtualenvs/hnn/bin\\0\", 0x7FFF93BD8DC4, 0x7FFF51E1F530)      = 0 0\ngetattrlist(\"/Users/pikeas/.virtualenvs/hnn/bin/python\\0\", 0x7FFF93BD8DC4, 0x7FFF51E1F530)       = 0 0\nreadlink(\"/Users/pikeas/.virtualenvs/hnn/bin/python\\0\", 0x7FFF51E1E930, 0x3FF)       = 9 0\ngetattrlist(\"/Users/pikeas/.virtualenvs/hnn/bin/python3.5\\0\", 0x7FFF93BD8DC4, 0x7FFF51E1F530)        = 0 0\ngetattrlist(\"/Users\\0\", 0x7FFF93BD8DC4, 0x7FFF51E1FAE0)      = 0 0\ngetattrlist(\"/Users/pikeas\\0\", 0x7FFF93BD8DC4, 0x7FFF51E1FAE0)       = 0 0\ngetattrlist(\"/Users/pikeas/.virtualenvs\\0\", 0x7FFF93BD8DC4, 0x7FFF51E1FAE0)      = 0 0\ngetattrlist(\"/Users/pikeas/.virtualenvs/hnn\\0\", 0x7FFF93BD8DC4, 0x7FFF51E1FAE0)      = 0 0\ngetattrlist(\"/Users/pikeas/.virtualenvs/hnn/bin\\0\", 0x7FFF93BD8DC4, 0x7FFF51E1FAE0)      = 0 0\ngetattrlist(\"/Users/pikeas/.virtualenvs/hnn/bin/driver\\0\", 0x7FFF93BD8DC4, 0x7FFF51E1FAE0)       = -1 Err#2\nstat64(\"/Developer/NVIDIA/CUDA-7.5/lib/libcuda.dylib\\0\", 0x7FFF51E1F998, 0x7FFF51E1FAE0)         = -1 Err#2\nstat64(\"$ORIGIN/../../_solib_darwin/_U_S_Sthird_Uparty_Sgpus_Scuda_Ccudart___Uthird_Uparty_Sgpus_Scuda_Slib/libcuda.dylib\\0\", 0x7FFF51E1F548, 0x7FFF51E1FAE0)        = -1 Err#2\nstat64(\"third_party/gpus/cuda/lib/libcuda.dylib\\0\", 0x7FFF51E1F598, 0x7FFF51E1FAE0)      = -1 Err#2\nstat64(\"third_party/gpus/cuda/extras/CUPTI/lib/libcuda.dylib\\0\", 0x7FFF51E1F588, 0x7FFF51E1FAE0)         = -1 Err#2\nstat64(\"libcuda.dylib\\0\", 0x7FFF51E1F5C8, 0x7FFF51E1FAE0)        = -1 Err#2\nstat64(\"/Users/pikeas/lib/libcuda.dylib\\0\", 0x7FFF51E1F9A8, 0x7FFF51E1FAE0)      = -1 Err#2\nstat64(\"/usr/local/lib/libcuda.dylib\\0\", 0x7FFF51E1F9A8, 0x7FFF51E1FAE0)         = -1 Err#2\nstat64(\"/usr/lib/libcuda.dylib\\0\", 0x7FFF51E1F9B8, 0x7FFF51E1FAE0)       = -1 Err#2\n```\n", "Solved!\n\nGist problem - dtruss sometimes truncated its output. When I re-ran, I got a slightly longer trace that mentioned libcuda.dylib. This file is _not_ in /Developer/NVIDIA/CUDA-7.5/lib, but it is in /usr/local/cuda/lib.\n\nIn other words, the solution is adding to my dylib export: `export DYLD_LIBRARY_PATH=\"/Developer/NVIDIA/CUDA-7.5/lib:/usr/local/cuda/lib\"`\n\nPlease note that I used stock everything: CUDA from NVIDIA, Python from homebrew, numpy from pip, tensorflow from source. As far as I can tell, anyone building under Mac OS X El Capitan, and very likely Yosemite/Mavericks as well, will experience the same problem.\n\nI strongly urge the project to create and maintain until OS X build instructions.\n", "In the setup tutorial ( https://www.tensorflow.org/versions/r0.8/get_started/os_setup.html ),\nIt says that install CUDA and CuDNN in /usr/local/cuda.\nI think you've missed it.\n", "I experienced a very similar issue with prebuilt TensorFlow 0.10 binary and CUDA installed according to instructions.\n\nIt turns out TensorFlow wants to import `libcuda.1.dylib`, not the `libcuda.dylib` that NVIDIA's CUDA installer installed. Manually creating a new symbolic link from `libcuda.dylib` to `libcuda.1.dylib` in /usr/local/cuda/lib fixed the issue for me.\n", "It also worked for me running on a MacBook Pro (Retina, 15-inch, Late 2013) + Tensorflow r0.11 + CUDA 8.0 + cuDNN v8 + Anaconda3\n\nI just used the command:\n`sudo ln -s /usr/local/cuda/lib/libcuda.dylib /usr/local/cuda/lib/libcuda.1.dylib`\n\nand now it is working no more Segmentation Fault:11.\n", "Same here. Linking the libcuda.dylib to libcuda.1.dylib fixed the problem. Maybe this step should be added to the documentation as it seems to happen with CUDA 8.0 reproducible\n", "If this is the naming convention from NVIDIA, maybe a change in the source related to `stream_executor/dso_loader.cc` is needed where it's getting tripped up."]}, {"number": 2277, "title": "Udacity assignment 4 run time error", "body": "Hi, Thanks for the tutorials. I was able to run everything until now. I'm trying to run assignment 4 and in last step in the following part, I get exception which is related to \"session\" and its graph! I haven't change anything, I'm just running the code:\n\n``` python\nnum_steps = 1001\n\nwith tf.Session(graph=graph) as session:\n  tf.initialize_all_variables().run()\n  print('Initialized')\n  for step in range(num_steps):\n    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n    batch_labels = train_labels[offset:(offset + batch_size), :]\n    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n    _, l, predictions = session.run(\n      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n    if (step % 50 == 0):\n      print('Minibatch loss at step %d: %f' % (step, l))\n      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n      print('Validation accuracy: %.1f%%' % accuracy(\n        valid_prediction.eval(), valid_labels))\n  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n```\n\nand the error messages are:\n\n```\nInitialized\nMinibatch loss at step 0: 3.362968\nMinibatch accuracy: 6.2%\n\nInternalErrorTraceback (most recent call last)\n<ipython-input-23-c56b394d9d7c> in <module>()\n     15       print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n     16       print('Validation accuracy: %.1f%%' % accuracy(\n---> 17         valid_prediction.eval(), valid_labels))\n     18   print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc in eval(self, feed_dict, session)\n    500 \n    501     \"\"\"\n--> 502     return _eval_using_default_session(self, feed_dict, self.graph, session)\n    503 \n    504 \n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc in _eval_using_default_session(tensors, feed_dict, graph, session)\n   3346                        \"the tensor's graph is different from the session's \"\n   3347                        \"graph.\")\n-> 3348   return session.run(tensors, feed_dict)\n   3349 \n   3350 \n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict, options, run_metadata)\n    341     try:\n    342       result = self._run(None, fetches, feed_dict, options_ptr,\n--> 343                          run_metadata_ptr)\n    344       if run_metadata:\n    345         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in _run(self, handle, fetches, feed_dict, options, run_metadata)\n    565     try:\n    566       results = self._do_run(handle, target_list, unique_fetches,\n--> 567                              feed_dict_string, options, run_metadata)\n    568     finally:\n    569       # The movers are no longer used. Delete them.\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\n    638     if handle is None:\n    639       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n--> 640                            target_list, options, run_metadata)\n    641     else:\n    642       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in _do_call(self, fn, *args)\n    660       # pylint: disable=protected-access\n    661       raise errors._make_specific_exception(node_def, op, error_message,\n--> 662                                             e.code)\n    663       # pylint: enable=protected-access\n    664 \n\nInternalError: Dst tensor is not initialized.\n     [[Node: Const = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [10000,28,28,1] values: -0.45294118 0.029411765 0.40588236...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\nCaused by op u'Const', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 596, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 442, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 160, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 433, in _handle_events\n    self._handle_recv()\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 465, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 407, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 391, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 199, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2723, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2825, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2885, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-18-aedb1ef46e04>\", line 14, in <module>\n    tf_valid_dataset = tf.constant(valid_dataset)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/constant_op.py\", line 166, in constant\n    attrs={\"value\": tensor_value, \"dtype\": dtype_value}, name=name).outputs[0]\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2154, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1154, in __init__\n    self._traceback = _extract_stack()\n```\n\nIs there anything I have to do ? I can see the exception comes from `.eval()` function. I guess @vincentvanhoucke might be right person to ping. \n\nThanks\n", "comments": ["I will attempt to reproduce on Monday. However, this specific error is typically TensorFlow's oh-so-cryptic way of saying it's run out of memory. Can that be the case here? Can you also tell me what tf.**version** this is?\n", "Thanks, here are the information you asked for:\n\n```\nubuntu@ip:~$ free -m\n             total       used       free     shared    buffers     cached\nMem:         15039       5314       9725         66         36       1109\n-/+ buffers/cache:       4169      10870\nSwap:            0          0          0\nubuntu@ip:~$ pip show tensorflow\n---\nMetadata-Version: 2.0\nName: tensorflow\nVersion: 0.8.0\n\n```\n", "I can't reproduce this at the head of the tree. It seems to strongly point at a memory issue.\nIf you reduce the size of the test and validation sets (which are kept in memory), does it start working?\n", "I have just received a very similar error on Assignment 3, after copying over an existing and working model as a base from Assignment 2. I restarted my colab kernel and things worked fine afterwards.\n", "I'm seeing this error on assignment 4 when running on the GPU  (GTX 1070). If I switch back to CPU it completes successfully. \r\n\r\nI was using iPython notebook. I tried restarting the notebook server and then the code worked successfully with the GPU. I think the multi-layer network I was running for assignment 3 used up to much GPU memory and there wasn't enough left over for assignment 4. Restarting the server must have freed the memory."]}, {"number": 2276, "title": "Add parameter to specify inner activations for rnn cells", "body": "Allow to specify activation function of inner states for GRU, BasicLSTM and LSTM cells. The paramter defaults to `tanh` so that existing code does not need to be changed. This is equivalent to the `activation` parameter for GRU and the `inner_activation` parameter for LSTM in Keras and the `activation` parameter for LSTM and GRU in Theanets.\n", "comments": ["Can one of the admins verify this patch?\n", "Test this please, Jenkins.\n", "We have some changes incoming to the signature of the various RNN Cells.  This change has a few lines conflicting with it, so will hold off till Wednesday or so to review.\n", "@ebrevdo Any update on this?\n", "Yes - please rebase, resolve any conflicts, and ping when ready.\n", "@ebrevdo Ping\n", "Jenkins, test this please.\n", "Jenkins, test this please.\n", "Thanks!\n"]}, {"number": 2275, "title": "Added comments to clarify how/why the embeddings are being minimized.", "body": "Reason for commit: For a student who has been going through the udacity examples, this particular use of the minimize function is a bit confusing as its difference between the previous examples is subtle. To the student it appears that only the softmax weights are being modified.\n", "comments": ["Can one of the admins verify this patch?\n", "Test this please, Jenkins.\n"]}, {"number": 2274, "title": "[tf.learn] Support HDF5 in tf.learn", "body": "See issue #2089\ncc: @ilblackdragon \n", "comments": ["Can one of the admins verify this patch?\n", "@ilblackdragon Not so much special handling is needed. I have added an example and a test case. I think this is ready to merge. \n", "Test this please, Jenkins.\n", "I overlooked something. Will fix it soon. @petewarden \n", "@petewarden Could you trigger test again (should be fixed)? Sorry I wasn't able to test it locally for some reason. Errors like this happen recently even after I re-clone the whole repo. It worked fine before.\n\n```\nexec ${PAGER:-/usr/bin/less} \"$0\" || exit 1\n-----------------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"tensorflow/contrib/learn/test_base\", line 110, in <module>\n    Main()\n  File \"tensorflow/contrib/learn/test_base\", line 93, in Main\n    program = python_program = FindPythonBinary()\n  File \"tensorflow/contrib/learn/test_base\", line 25, in FindPythonBinary\n    'Bazel does not support execution of Python interpreters via labels yet')\nAssertionError: Bazel does not support execution of Python interpreters via labels yet\n```\n", "Test this please, Jenkins.\n", "Tests passed. Ping @petewarden \n", "Thanks @terrytangyuan, I've reviewed the changes and they look good, merging! I appreciate your work on this.\n"]}, {"number": 2273, "title": "Markdown links using `[This link](http://example.net/)` syntax", "body": "Noticed these spots are using a syntax similar to markdown links but weren't rendering in the markdown under `g3doc`.\n\nThere may be a valid reason these links are different, they do cross new line boundaries but there are other links which do as well and use `[This link](http://example.net/)` syntax.\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n", "Test this please, Jenkins.\n"]}, {"number": 2272, "title": "Renamed `vocabulary_size` to `character_set_size`.  \"Vocabulary\" mean\u2026", "body": "\u2026s \"a set of words\" not a \"set of characters\" and thus the variable name was misleading.\n", "comments": ["Can one of the admins verify this patch?\n", "Test this please, Jenkins.\n", "vocabulary is a very generic term in this context, just like alphabet for coding theory. I don't think we'd want to deviate from standard nomenclature for character level models, but @vincentvanhoucke should decide.\n", "Yes, 'vocabulary' in the context of NLP defines the set of entities that are being predicted. They're often words but can be sub-word units or characters. This is very standard practice, so I'll close this PR.\n"]}, {"number": 2271, "title": "Sudden slowdowns due to the TLB shootdowns", "body": "### Environment info\n\nOperating System: Arch Linux\n\nInstalled version of CUDA and cuDNN: CUDA 7.5, CUDNN v4\nIf installed from binary pip package, provide:\n1. Which pip package you installed: https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0-cp34-cp34m-linux_x86_64.whl\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\": 0.8.0\n### Steps to reproduce\n\nI am experiencing slowdowns due to some problem with memory caching. My system has 32GB of RAM. I am training a CNN on one large image dataset. During training at some point almost 100% of memory is cached which is expected when working with a large dataset. But then soon after TF starts to experience slowdowns (3-5x slower execution per batch). I used netdata and observed that the slowdowns are accompanied by a sudden huge spikes in TLB shootdowns. I found a way how to fix a problem from outside, the slowdowns disappear for quite some time if I just clear all the cached memory like this:\n\n```\n$ free && sync && echo 3 > /proc/sys/vm/drop_caches && free\n              total        used        free      shared  buff/cache   available\nMem:       32825492     3454324      209124      638072    29162044    28463116\n              total        used        free      shared  buff/cache   available\nMem:       32825492     3455692    28603760      637636      766040    28476376\n\n```\n\nThen later when memory is fully cached again they reappear. First row is the state at which the slowdown is happening. You can see that most memory is cached and I am actually using only around 10% of RAM.\nDoes anyone have an idea what could cause these TLB shootdowns or what can I do to find out more about what is going on?\n", "comments": ["Facing the same issue here, dropping caches via sysctl fixes it temporarily.\n\nThis is with current tensorflow@9078909131d5e91e8bd1878eac54136885656043 on a ec2 g2.2xlarge running Xenial. \n", "@rmlarsen: Any guesses as to what's going on? \n", "This is the graph that shows a drop in page faults after I clear all cached memory.\nI don't have an idea why would the number of memory page faults decrease after the cache is cleared.\n\n![Image](http://i.imgur.com/RaivaXq.png)\n\nI forgot to mention that I am using tf.train.batch for data fetching, defined like this:\n\n```\ndef _read_and_decode(filename_queue):\n  reader = tf.TFRecordReader()\n  _, serialized_example = reader.read(filename_queue)\n  features = tf.parse_single_example(\n      serialized_example,\n      features={\n          'height': tf.FixedLenFeature([], tf.int64),\n          'width': tf.FixedLenFeature([], tf.int64),\n          'depth': tf.FixedLenFeature([], tf.int64),\n          'num_labels': tf.FixedLenFeature([], tf.int64),\n          'img_name': tf.FixedLenFeature([], tf.string),\n          'rgb': tf.FixedLenFeature([], tf.string),\n          'label_weights': tf.FixedLenFeature([], tf.string),\n          'labels': tf.FixedLenFeature([], tf.string),\n      })\n\n  labels = tf.to_int32(tf.decode_raw(features['labels'], tf.int8, name='decode_labels'))\n  image = tf.decode_raw(features['rgb'], tf.float32, name='decode_image')\n  weights = tf.decode_raw(features['label_weights'], tf.float32, name='decode_weights')\n  num_labels = features['num_labels']\n  img_name = features['img_name']\n\n  image = tf.reshape(image, shape=[FLAGS.img_height, FLAGS.img_width, FLAGS.img_depth])\n  num_pixels = FLAGS.img_height * FLAGS.img_width\n  labels = tf.reshape(labels, shape=[num_pixels])\n  weights = tf.reshape(weights, shape=[num_pixels])\n\n  return image, labels, weights, num_labels, img_name\n\ndef inputs(dataset, shuffle=True, num_epochs=None):\n  with tf.name_scope('input'), tf.device('/cpu:0'):\n    filename_queue = tf.train.string_input_producer(dataset.get_filenames(),\n        num_epochs=num_epochs, shuffle=shuffle, capacity=dataset.num_examples())\n\n    image, labels, weights, num_labels, img_name = _read_and_decode(filename_queue)\n\n    image, labels, weights, num_labels, img_name = tf.train.batch(\n        [image, labels, weights, num_labels, img_name], batch_size=FLAGS.batch_size, num_threads=2,\n        capacity=64)\n    return image, labels, weights, num_labels, img_name\n```\n", "Is it TLB misses or page faults? Page cache should cause page faults.\nWhat process does trigger the faults?\nYou can check it with `perf record -a -g -e faults`, then ctrl+C after 10 seconds, then `perf report`.\nDoes the tf program read any files? Is it possible that some other process constantly reads something from the disk?\n", "@ivankreso Did you make any progress on investigating this further?\n", "@rmlarsen No, I am currently running experiments on a smaller dataset which fits into memory. I will have free time on my machine over the weekend or during next week and then I can try to reproduce the problem again on the newest TF version. I will then post any new observations and the report from perf record logs here.\n@dvyukov Thanks for good suggestion!\n", "I build the newest TF version from master and the problem doesn't appear anymore.\nClosing the issue. If it reappears i will post the new info here. \n", "> I build the newest TF version from master and the problem doesn't appear anymore.\r\n> Closing the issue. If it reappears i will post the new info here.\r\n\r\nI came across the same problem as you, do you know how to solve it now. I use tf 1.9.0"]}, {"number": 2270, "title": "Sequence2Sequence decode does not work", "body": "The rnn sample, \"translate\" or \"sequence2sequence\" has some issues during \"decode\". After I successfully trained, it always gives the following error:\n\npython translate.py --data_dir tensorflow/tensorflow/models/rnn/translate/data --train_dir tensorflow/tensorflow/models/rnn/translate/train-data --size=256 --num_layers=2 --steps_per_checkpoint=50\n\ntensorflow.python.framework.errors.NotFoundError: Tensor name \"embedding_attention_seq2seq/RNN/MultiRNNCell/Cell2/GRUCell/Gates/Linear/Bias\" not found in checkpoint files tensorflow/tensorflow/models/rnn/translate/train-data/translate.ckpt-750\n         [[Node: save/restore_slice_13 = RestoreSlice[dt=DT_FLOAT, preferred_shard=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/restore_slice_13/tensor_name, save/restore_slice_13/shape_and_slice)]]\nCaused by op u'save/restore_slice_13', defined at:\n\nI tried with many checkpoints stored, but I get the same error. This is with latest tensorflow-0.8.\n\nPlease see if I am missing something.\n", "comments": ["Sorry, it was my mistake or rather the README was the issue. I had to give the same command line with \"--size=256 --num_layers=2\" during --decode too. Otherwise, it is looking for Cell3 which is non existent (and looks like default is that). \n"]}, {"number": 2269, "title": "How to resize one tensor to (e.g., 1.5 * its original shape)?", "body": "The tensor shape is not fixed, and can change with different input.\n", "comments": ["Hi there. I'm going to close this issue because, as it says in the issue template:\n\n> GitHub issues are for bugs / installation problems / feature requests.  \n> For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\n> To make bugs and feature requests more easy to find and organize, we close issues that are deemed\n> out of scope for GitHub Issues and point people to StackOverflow.\n\nYou might find the [`tf.tile()`](https://www.tensorflow.org/versions/r0.8/api_docs/python/array_ops.html#tile) and [`tf.pad()`](https://www.tensorflow.org/versions/r0.8/api_docs/python/array_ops.html#pad) ops useful, however. If you have further questions, please ask the question on Stack Overflow instead.\n"]}, {"number": 2268, "title": "Add x permission to integration_tests.sh", "body": "Accidentally removed x permission in commit\nce575f174b37f08dea85c1d11e2d2cdb7bda9d6e\n\nThis lead to a few recent nightly build failures in the PIP builds.\n", "comments": ["@tensorflow-jenkins test this please\n"]}, {"number": 2267, "title": "fully_connected_preloaded.py fails with Error on Python 2.7.6", "body": "### Environment info\n\nOperating System: Linux xxxxx-Lenovo-Ideapad-V570 3.19.0-58-generic #64~14.04.1-Ubuntu SMP Fri Mar 18 19:05:43 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\n\nInstalled version of CUDA and cuDNN: n/a\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n   n/a - used install.sh\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n   0.6.0\n\nIf installed from sources, provide the commit hash:\n0d350f9324ae08dcbe0c14f0dbfde169da260a37\n### Steps to reproduce\n1. python tensorflow/examples/how_tos/reading_data/fully_connected_preloaded.py \n   2.\n   3.\n### What have you tried?\n1. cd tensorflow/examples/how_tos/reading_data\n   python fully_connected_preloaded.py\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\nExtracting /tmp/data/train-images-idx3-ubyte.gz\nExtracting /tmp/data/train-labels-idx1-ubyte.gz\nExtracting /tmp/data/t10k-images-idx3-ubyte.gz\nExtracting /tmp/data/t10k-labels-idx1-ubyte.gz\nI tensorflow/core/common_runtime/local_device.cc:40] Local device intra op parallelism threads: 4\nI tensorflow/core/common_runtime/direct_session.cc:58] Direct session inter op parallelism threads: 4\nTraceback (most recent call last):\n  File \"tensorflow/examples/how_tos/reading_data/fully_connected_preloaded.py\", line 157, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/default/_app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"tensorflow/examples/how_tos/reading_data/fully_connected_preloaded.py\", line 153, in main\n    run_training()\n  File \"tensorflow/examples/how_tos/reading_data/fully_connected_preloaded.py\", line 106, in run_training\n    summary_writer = tf.train.SummaryWriter(FLAGS.train_dir, sess.graph)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/summary_io.py\", line 103, in __init__\n    self.add_graph(graph_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/summary_io.py\", line 151, in add_graph\n    event = event_pb2.Event(wall_time=time.time(), graph_def=graph_def)\n  File \"/usr/local/lib/python2.7/dist-packages/google/protobuf/internal/python_message.py\", line 455, in init\n    _ReraiseTypeErrorWithFieldName(message_descriptor.name, field_name)\n  File \"/usr/local/lib/python2.7/dist-packages/google/protobuf/internal/python_message.py\", line 386, in _ReraiseTypeErrorWithFieldName\n    raise type(exc)(exc, sys.exc_info()[2])\nTypeError: (TypeError('Parameter to MergeFrom() must be instance of same class: expected GraphDef got Graph. for field Event.graph_def',), <traceback object at 0x7fce35658b00>)\n", "comments": ["This example has been updated to use features only available in TensorFlow 0.8. I'd strongly recommend upgrading to the newest version. Otherwise, you should check out the [version of this example from the 0.6.0 branch](https://github.com/tensorflow/tensorflow/blob/0.6.0/tensorflow/examples/how_tos/reading_data/fully_connected_preloaded.py).\n"]}, {"number": 2266, "title": "CentOS 6.7 Compilation Issue at HEAD", "body": "CentOS 6.7\nCuda 7.0\ncuDNN 6.5\n\nI previously got TensorFlow 7.x to compile with an older version of Bazel (1.x), but this fails with HEAD, and it also fails if I use Bazel 0.2.0 as suggested in other issues.\n\nBazel 0.2.0 builds fine.\n\nKeeping with what worked before, I run\n\n```\nexport EXTRA_BAZEL_ARGS='-s --verbose_failures --ignore_unsupported_sandboxing --genrule_strategy=standalone --spawn_strategy=standalone --jobs 8'\n\nbazel build -c opt --config=cuda --linkopt '-lrt' --copt=\"-DGPR_BACKWARDS_COMPATIBILITY_MODE\" --conlyopt=\"-std=c99\" //tensorflow/tools/pip_package:build_pip_package\n```\n\nHere is the error I now get. Any ideas? Please note that python2.7 is in my path, and /usr/bin/env python2.7 opens an interpreter with no problem.\n\nThanks.\n\n```\nERROR: /home-4/rdipiet2@jhu.edu/install/tensorflow/google/protobuf/BUILD:520:1: Linking of rule '//google/protobuf:internal/_api_implementation.so' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command \n  (cd /home-4/rdipiet2@jhu.edu/.cache/bazel/_bazel_rdipiet2@jhu.edu/549db212089e33b4d213773753834e47/tensorflow && \\\n  exec env - \\\n  third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -shared -o bazel-out/local_linux-opt/bin/google/protobuf/internal/_api_implementation.so -Wl,-whole-archive bazel-out/local_linux-opt/bin/google/protobuf/_objs/internal/_api_implementation.so/google/protobuf/python/google/protobuf/internal/api_implementation.pic.o -Wl,-no-whole-archive -Wl,-R/cm/shared/apps/gcc/4.8.2/lib64 -lstdc++ -B/usr/bin/ -Wl,-z,relro,-z,now -no-canonical-prefixes -pass-exit-codes '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -Wl,--gc-sections -lrt): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 127: crosstool_wrapper_driver_is_not_gcc failed: error executing command \n  (cd /home-4/rdipiet2@jhu.edu/.cache/bazel/_bazel_rdipiet2@jhu.edu/549db212089e33b4d213773753834e47/tensorflow && \\\n  exec env - \\\n  third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -shared -o bazel-out/local_linux-opt/bin/google/protobuf/internal/_api_implementation.so -Wl,-whole-archive bazel-out/local_linux-opt/bin/google/protobuf/_objs/internal/_api_implementation.so/google/protobuf/python/google/protobuf/internal/api_implementation.pic.o -Wl,-no-whole-archive -Wl,-R/cm/shared/apps/gcc/4.8.2/lib64 -lstdc++ -B/usr/bin/ -Wl,-z,relro,-z,now -no-canonical-prefixes -pass-exit-codes '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -Wl,--gc-sections -lrt): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 127.\n/usr/bin/env: python2.7: No such file or directory\n```\n", "comments": ["Could you pass the absolute path to your python, such as? \n\n/usr/bin/python\n", "Thanks for the quick response. It's `/cm/shared/apps/anaconda/2.7.10/bin/python`.\n", "Any ideas about why this wouldn't be picked up?\n", "The error message is from the first line here: \n\n#!/usr/bin/env python2.7\nhttps://github.com/tensorflow/tensorflow/blob/master/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc\n\nCould you confirm that you are running bash? And if you start from your bash shell, if you run the displayed command line from \"bazel\", you would pass the first line? \n\nIf so, it could be possible that the new bazel is running a more restricted bash and however you put python2.7 on env didn't get picked up. If that is indeed the case, it would be better solved if you file a bazel issue directly. \n\nhttps://github.com/bazelbuild/bazel\n", "I think you're right. I'll file a bazel issue along with other lost dependencies after I get it to successfully compile.\n"]}, {"number": 2265, "title": "Parameter keep_prob now dropout", "body": "Seems as parameter keep_prob is now called dropout. Because of the comment above I'm pretty sure droupout is meant.\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins : test this please\n", "Merged. Thanks, @codingyourlife \n"]}, {"number": 2264, "title": "TensorFlow 1st Test: \u201ccould not open file to read NUMA node\u201d - what's wrong?", "body": "I went to StackOverflow with this and was pointed back to Github. ;-)\nsee [http://stackoverflow.com/questions/37067297]\n### Environment info\n\nOperating System: Gentoo Linux on Lenovo P50\n\nInstalled version of CUDA and cuDNN: \nI installed dev-util/nvidia-cuda-toolkit package, version 7.5.18-r2\n\n```\n# ll /opt/cuda/lib/libcud*\n-rw-r--r-- 1 root root 189082 May  6 10:42 /opt/cuda/lib/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Sep 19  2015 /opt/cuda/lib/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root     19 Sep 19  2015 /opt/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 311596 Sep 19  2015 /opt/cuda/lib/libcudart.so.7.5.18\n-rw-r--r-- 1 root root 557240 May  6 10:42 /opt/cuda/lib/libcudart_static.a\n```\n\nPlus I installed cuDNN 5 downloaded from Nvidia\n\n```\n# ll libcud*\nlrwxrwxrwx 1 rj rj       13 Mar 22 08:44 libcudnn.so -> libcudnn.so.5\nlrwxrwxrwx 1 rj rj       17 Mar 22 08:44 libcudnn.so.5 -> libcudnn.so.5.0.4\n-rwxrwxr-x 1 rj rj 59823168 Mar 22 02:37 libcudnn.so.5.0.4\n-rw-rw-r-- 1 rj rj 58734618 Mar 22 02:37 libcudnn_static.a\n```\n\nIf installed from binary pip package, provide:\n\nWhich pip package you installed.\n\n```\n# pip3 -V\npip 8.1.1 from /usr/lib64/python3.4/site-packages (python 3.4)\n```\n\nThe output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n\n```\n# python -c \"import tensorflow; print(tensorflow.__version__)\"\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n0.8.0\n```\n### Steps to reproduce\n1. import tensorflow as tf\n2. hello = tf.constant('Hello, TensorFlow!')\n3. sess = tf.Session()\n### What have you tried?\n1. stackoverflow ;-)\n", "comments": ["For reference, here's the error output from StackOverflow:\n\n```\nE tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: Quadro M2000M\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.137\npciBusID 0000:01:00.0\nTotal memory: 4.00GiB\nFree memory: 3.96GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0)\nF tensorflow/core/common_runtime/gpu/process_state.cc:155] Check failed: numa_node >= 0 (-1 vs. 0)\nAborted\n```\n\nAssigning to @poxvoculi \n", "For reference, cuda_gpu_executor [here](https://github.com/tensorflow/tensorflow/blob/7b4d733593842361d066d7e33a03a07da5dca465/tensorflow/stream_executor/cuda/cuda_gpu_executor.cc#L905) should've convereted all -1 to 0 (I've gotten -1 Numa node on machines with 1 or 2 nodes, but it turns into 0 in the end\n", "Thanks @yaroslavvb, looks like it's failing to get the numa node on one of the other logic paths through TryToReadNumaNode.  @petamem: Fix coming.  Meanwhile, it's probably safe for you to hack common_runtime/gpu/gpu_device to set the num_node parameter to 0, where it is used.  This value is only important if you have multiple GPUs on different PCI buses.  If that's your case, then we need to fix the stream executor to find the numa node correctly.  \n", "If there's anything I can do to help searching for the culprit (enabling some debug switches, using other (combinations of) versions), let me know. The hardware I tried it is a Lenovo P50 with a Nvidia Quadro M2000M and also, there is an Intel GPU P530. lscpi says:\n\n...\n00:02.0 VGA compatible controller: Intel Corporation HD Graphics P530 (rev 06)\n...\n01:00.0 VGA compatible controller: NVIDIA Corporation GM107GLM [Quadro M2000M](rev a2)\n...\n\nMaybe this is a \"multiple GPU on different PCI busses\" situation. I had expected the Intel GPU would have been ignored.\n", "Ok. In order to test the hypothesis that I could mitigate the error by disabling the Intel P530 GPU, I disabled it in the BIOS. lspci henceforth does not list it, so for the OS it should look like there is only one GPU - the M2000M - present.\n\nUnfortunately, the error remains the same (GPU just uses slightly more memory because the frame buffer):\n\n```\n>>> sess = tf.Session()\nE tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: Quadro M2000M\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.137\npciBusID 0000:01:00.0\nTotal memory: 4.00GiB\nFree memory: 3.68GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0)\nF tensorflow/core/common_runtime/gpu/process_state.cc:155] Check failed: numa_node >= 0 (-1 vs. 0)\n```\n\nWhich leads me to the conclusion, that there must be some own OS-independent GPU detection routine present. Not sure if this is a good thing.\n\nAny ETA for the fix? I could try hacking common_runtime/gpu/gpu_device as suggested, but if there's soon a fix, that'd be probably moot.\n", "Unfortunately, the fix is not in the latest (pending) merge up from Google: https://github.com/tensorflow/tensorflow/pull/2299\nIt should be in the next one.  The testing process makes the code pushes slow.\n\nIf you'd like to help debug the underlying problem, look at tensorflow/stream_executor/cuda/cuda_gpu_executor.cc\nIn there is a function TryToReadNumaNode() which examines the /sys/ directory to try to find the socket/bus affinity of a GPU device.  Evidently this doesn't work on your system.  It would be nice to know why: is the data not present, or stored somewhere else?\n\nIf you just want to get on with your other task, override the kUnknownNumaNode value (-1) being returned with 0, and the rest of the system should work ok...assuming that TF is actually accessing the correct GPU.  (Notice that for OS/X that's exactly what happens in TryToReadNumaNode().)\n", "Hm. I'm afraid the path template \"/sys/bus/pci/devices/%s/numa_node\" cannot be found on my system as /sys/bus/pci/devices looks like that:\n\n```\n# ll /sys/bus/pci/devices/0000\\:*\nlrwxrwxrwx 1 root root 0 May 11 15:06 /sys/bus/pci/devices/0000:00:00.0 -> ../../../devices/pci0000:00/0000:00:00.0\nlrwxrwxrwx 1 root root 0 May 11 15:06 /sys/bus/pci/devices/0000:00:01.0 -> ../../../devices/pci0000:00/0000:00:01.0\nlrwxrwxrwx 1 root root 0 May 11 15:06 /sys/bus/pci/devices/0000:00:14.0 -> ../../../devices/pci0000:00/0000:00:14.0\nlrwxrwxrwx 1 root root 0 May 11 15:06 /sys/bus/pci/devices/0000:00:14.2 -> ../../../devices/pci0000:00/0000:00:14.2\nlrwxrwxrwx 1 root root 0 May 11 15:06 /sys/bus/pci/devices/0000:00:16.0 -> ../../../devices/pci0000:00/0000:00:16.0\nlrwxrwxrwx 1 root root 0 May 11 15:06 /sys/bus/pci/devices/0000:00:16.3 -> ../../../devices/pci0000:00/0000:00:16.3\nlrwxrwxrwx 1 root root 0 May 11 15:06 /sys/bus/pci/devices/0000:00:17.0 -> ../../../devices/pci0000:00/0000:00:17.0\nlrwxrwxrwx 1 root root 0 May 11 15:06 /sys/bus/pci/devices/0000:00:1c.0 -> ../../../devices/pci0000:00/0000:00:1c.0\nlrwxrwxrwx 1 root root 0 May 11 15:06 /sys/bus/pci/devices/0000:00:1c.2 -> ../../../devices/pci0000:00/0000:00:1c.2\nlrwxrwxrwx 1 root root 0 May 11 15:06 /sys/bus/pci/devices/0000:00:1c.4 -> ../../../devices/pci0000:00/0000:00:1c.4\nlrwxrwxrwx 1 root root 0 May 11 15:06 /sys/bus/pci/devices/0000:00:1d.0 -> ../../../devices/pci0000:00/0000:00:1d.0\nlrwxrwxrwx 1 root root 0 May 11 15:06 /sys/bus/pci/devices/0000:00:1d.4 -> ../../../devices/pci0000:00/0000:00:1d.4\nlrwxrwxrwx 1 root root 0 May 11 15:06 /sys/bus/pci/devices/0000:00:1f.0 -> ../../../devices/pci0000:00/0000:00:1f.0\nlrwxrwxrwx 1 root root 0 May 11 15:06 /sys/bus/pci/devices/0000:00:1f.2 -> ../../../devices/pci0000:00/0000:00:1f.2\nlrwxrwxrwx 1 root root 0 May 11 15:06 /sys/bus/pci/devices/0000:00:1f.3 -> ../../../devices/pci0000:00/0000:00:1f.3\nlrwxrwxrwx 1 root root 0 May 11 15:06 /sys/bus/pci/devices/0000:00:1f.4 -> ../../../devices/pci0000:00/0000:00:1f.4\nlrwxrwxrwx 1 root root 0 May 11 15:06 /sys/bus/pci/devices/0000:00:1f.6 -> ../../../devices/pci0000:00/0000:00:1f.6\nlrwxrwxrwx 1 root root 0 May 11 15:06 /sys/bus/pci/devices/0000:01:00.0 -> ../../../devices/pci0000:00/0000:00:01.0/0000:01:00.0\nlrwxrwxrwx 1 root root 0 May 11 15:06 /sys/bus/pci/devices/0000:01:00.1 -> ../../../devices/pci0000:00/0000:00:01.0/0000:01:00.1\nlrwxrwxrwx 1 root root 0 May 11 15:06 /sys/bus/pci/devices/0000:04:00.0 -> ../../../devices/pci0000:00/0000:00:1c.2/0000:04:00.0\nlrwxrwxrwx 1 root root 0 May 11 15:06 /sys/bus/pci/devices/0000:3e:00.0 -> ../../../devices/pci0000:00/0000:00:1d.0/0000:3e:00.0\nlrwxrwxrwx 1 root root 0 May 11 15:06 /sys/bus/pci/devices/0000:3f:00.0 -> ../../../devices/pci0000:00/0000:00:1d.4/0000:3f:00.0\n\nls /sys/bus/pci/devices/*/numa_node\n```\n\n   no such file or directory\n\nWhich it does - of course, because the kernel has no NUMA support. :-)\nSo I compiled a new 4.5.3 kernel with NUMA support and ... \n\n```\n>>> sess = tf.Session()\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:900] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: Quadro M2000M\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.137\npciBusID 0000:01:00.0\nTotal memory: 4.00GiB\nFree memory: 3.47GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0)\n```\n\nSo it seems my problem was simply due to the fact, that the kernel had no NUMA support.\nI will now switch back to Hybrid mode (activate the Intel GPU) and test if that works with NUMA enabled.\n\nedit: it does. So the \"fix\" could be some warning sign somewhere \"NUMA support is a must\".\n", "Thanks, glad you found a solution.  I'll look into the error messages.\n", "Officially you need a GPU with compute level >= 3.5, but see\nhttps://github.com/tensorflow/tensorflow/issues/25\n\nOn Thu, Aug 18, 2016 at 9:30 AM, abhijayghildyal notifications@github.com\nwrote:\n\n> When I ran\n> \n> bazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu\n> \n> it gave me the following error:\n> \n> I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\n> I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\n> I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\n> I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\n> I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\n> I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n> I tensorflow/core/common_runtime/gpu/gpu_init.cc:118] Found device 0 with properties:\n> name: GeForce GT 640\n> major: 3 minor: 0 memoryClockRate (GHz) 0.9015\n> pciBusID 0000:05:00.0\n> Total memory: 2.00GiB\n> Free memory: 1.98GiB\n> I tensorflow/core/common_runtime/gpu/gpu_init.cc:138] DMA: 0\n> I tensorflow/core/common_runtime/gpu/gpu_init.cc:148] 0:   Y\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:843] Ignoring gpu device (device: 0, name: GeForce GT 640, pci bus id: 0000:05:00.0) with Cuda compute capability 3.0. The minimum required Cuda capability is 3.5.\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:843] Ignoring gpu device (device: 0, name: GeForce GT 640, pci bus id: 0000:05:00.0) with Cuda compute capability 3.0. The minimum required Cuda capability is 3.5.\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:843] Ignoring gpu device (device: 0, name: GeForce GT 640, pci bus id: 0000:05:00.0) with Cuda compute capability 3.0. The minimum required Cuda capability is 3.5.\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:843] Ignoring gpu device (device: 0, name: GeForce GT 640, pci bus id: 0000:05:00.0) with Cuda compute capability 3.0. The minimum required Cuda capability is 3.5.\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:843] Ignoring gpu device (device: 0, name: GeForce GT 640, pci bus id: 0000:05:00.0) with Cuda compute capability 3.0. The minimum required Cuda capability is 3.5.\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:843] Ignoring gpu device (device: 0, name: GeForce GT 640, pci bus id: 0000:05:00.0) with Cuda compute capability 3.0. The minimum required Cuda capability is 3.5.\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:843] Ignoring gpu device (device: 0, name: GeForce GT 640, pci bus id: 0000:05:00.0) with Cuda compute capability 3.0. The minimum required Cuda capability is 3.5.\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:843] Ignoring gpu device (device: 0, name: GeForce GT 640, pci bus id: 0000:05:00.0) with Cuda compute capability 3.0. The minimum required Cuda capability is 3.5.\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:843] Ignoring gpu device (device: 0, name: GeForce GT 640, pci bus id: 0000:05:00.0) with Cuda compute capability 3.0. The minimum required Cuda capability is 3.5.\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:843] Ignoring gpu device (device: 0, name: GeForce GT 640, pci bus id: 0000:05:00.0) with Cuda compute capability 3.0. The minimum required Cuda capability is 3.5.\n> F tensorflow/cc/tutorials/example_trainer.cc:128] Check failed: ::tensorflow::Status::OK() == (session->Run({{\"x\", x}}, {\"y:0\", \"y_normalized:0\"}, {}, &outputs)) (OK vs. Invalid argument: Cannot assign a device to node 'Cast': Could not satisfy explicit device specification '/gpu:0' because no devices matching that specification are registered in this process; available devices: /job:localhost/replica:0/task:0/cpu:0\n>      [[Node: Cast = Cast[DstT=DT_FLOAT, SrcT=DT_INT32, _device=\"/gpu:0\"](Const)]])\n> F tensorflow/cc/tutorials/example_trainer.cc:128] Check failed: ::tensorflow::Status::OK() == (session->Run({{\"x\", x}}, {\"y:0\", \"y_normalized:0\"}, {}, &outputs)) (OK vs. Invalid argument: Cannot assign a device to node 'Cast': Could not satisfy explicit device specification '/gpu:0' because no devices matching that specification are registered in this process; available devices: /job:localhost/replica:0/task:0/cpu:0\n>      [[Node: Cast = Cast[DstT=DT_FLOAT, SrcT=DT_INT32, _device=\"/gpu:0\"](Const)]])\n> F tensorflow/cc/tutorials/example_trainer.cc:128] Check failed: ::tensorflow::Status::OK() == (session->Run({{\"x\", x}}, {\"y:0\", \"y_normalized:0\"}, {}, &outputs)) (OK vs. Invalid argument: Cannot assign a device to node 'Cast': Could not satisfy explicit device specification '/gpu:0' because no devices matching that specification are registered in this process; available devices: /job:localhost/replica:0/task:0/cpu:0\n>      [[Node: Cast = Cast[DstT=DT_FLOAT, SrcT=DT_INT32, _device=\"/gpu:0\"](Const)]])\n> F tensorflow/cc/tutorials/example_trainer.cc:128] Check failed: ::tensorflow::Status::OK() == (session->Run({{\"x\", x}}, {\"y:0\", \"y_normalized:0\"}, {}, &outputs)) (OK vs. Invalid argument: Cannot assign a device to node 'Cast': Could not satisfy explicit device specification '/gpu:0' because no devices matching that specification are registered in this process; available devices: /job:localhost/replica:0/task:0/cpu:0\n>      [[Node: Cast = Cast[DstT=DT_FLOAT, SrcT=DT_INT32, _device=\"/gpu:0\"](Const)]])\n> Aborted (core dumped)\n> \n> Will I need a different gpu to run this?\n> \n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2264#issuecomment-240779046,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AO818Z6uyOczcrz6c1FBY8kKGzgKe4cVks5qhIiXgaJpZM4IZbcS\n> .\n", "@petamem Hi, I am trying to fix a similar issue here. Could you help me figure out how to \"a new 4.5.3 kernel with NUMA support\"? (T_T)\r\n\r\nI am runing tensorflow on Jetson TX1 and **sess=tf.Session()** gives me **NUMA** error on both python 2.7 and python 3.5.\r\n```\r\n>>> import tensorflow as tf\r\n>>> sess = tf.Session()\r\n2017-12-30 01:13:41.421430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:857] ARM64 does not support NUMA - returning NUMA node zero\r\n2017-12-30 01:13:41.421551: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: \r\nname: NVIDIA Tegra X1\r\nmajor: 5 minor: 3 memoryClockRate (GHz) 0.9984\r\npciBusID 0000:00:00.0\r\nTotal memory: 3.89GiB\r\nFree memory: 365.80MiB\r\n2017-12-30 01:13:41.421603: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 \r\n2017-12-30 01:13:41.421631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y \r\n2017-12-30 01:13:41.421671: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: NVIDIA Tegra X1, pci bus id: 0000:00:00.0)\r\n```\r\nI followed build on this repo: [https://github.com/jetsonhacks/installTensorFlowTX1](https://github.com/jetsonhacks/installTensorFlowTX1).", "@petamem Could you please share with us how to do this? Thank you.\r\n- `I will now switch back to Hybrid mode (activate the Intel GPU) and test if that works with NUMA enabled.`", "@petamem - I'm afraid I don't quite understand.  Suffering the same problem with a Titan GTX/Tensorflow 1.14 (Compute capability 3.5) - I'd compiled this myself rather painstakingly.  How did you do:\r\n\r\n> So I compiled a new 4.5.3 kernel with NUMA support and ...\r\n4.5.3 kernel of what, and how?  Thank you!", "tensorflow 1.9 will return 0, do not fail", "Hi all,\r\nI am stuck in the same error. I am working with tensorflow. If you found any solution, kindly share. Thanks\r\n\r\n`successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero`"]}, {"number": 2263, "title": "Add complex128 support", "body": "Added the complex128 support, based on the guidance from @girving and others in PR #2244. All tests (in CPU) including backwards_compatibility_test are passed! I couldn't test the GPU version.\n\nThe complex128 support is not added for _FFT_ ops with input: complex64 and output: complex64.\n", "comments": ["Can one of the admins verify this patch?\n", "Please squash into one commit.  It looks like a lot of the intermediate commits don't even compile, much less work, which makes them harder to review.\n", "@girving Thanks! All commits are squashed, and lines more than 80 columns are fixed.\nFour lines (81/82 columns) in tensorflow/core/kernels/cwise_ops_common.h are not fixed for better readability.\n\nAll my local 457 tests pass.\n", "Please wrap the 81/82 column lines.  It's a hard limit, except for Python import statements.\n", "Looks good, thanks!  I'll fire off the tests once you fix the `complex6128` bit and the > 80 column lines.\n", "Done!\n", "Woot!  Jenkins, test this please.\n", "Jenkins, test this please.\n", "Sorry that I could not run GPU tests locally. But I guess now it should pass GPU Tests.\n", "Is there any test excercising the new type complex128's operations?\n", "@zffchen78, I don't think we have tests for complex128 for now, but we will have some very soon. \n", "The code base is fast changing. The only way to ensure it works for everyone is to maintain a high standard of the code quality. One important aspect is to make sure testing is done well. In your case, can you add proper test cases covering the operation this PR touches? It should not be too complicated. Most relevant tests should be in python/kernel_tests/cwise_ops_test.py. You can grep test cases for complex64, most of which is just matter of extending the type list w/ the new type.\n", "@zffchen78, thanks! I agree, and I'll work on it after this PR.\n", "@girving Do you have more comments on this PR? I'd be happy to address your comments.\n", "@hunkim: Apologies, I should have clarified.  I agree with @zffchen78 that you should add tests, and was waiting on those.  It should be as simple as searching for `complex64` in `cwise_ops_test.py` and making sure `complex128` does the same thing.\n", "@girving Sounds good. I'm on it and will update soon.\n", "Woot! We have a new PR #2417.\n"]}]