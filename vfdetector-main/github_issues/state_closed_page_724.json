[{"number": 31855, "title": "Building TensorFlow 1.14 with local GRPC via TF_SYSTEM_LIBS", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Debian 9\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 1.14\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): gcc 6.3.0\r\n- CUDA/cuDNN version: 10.1, 7.5.1\r\n- GPU model and memory: N/A\r\n\r\n**Describe the problem**\r\n\r\nI'm trying to build TF 1.14.0 from source using the setup described above. We have a patched version of GRPC that I'm trying to build against, so I'm trying to use TF_SYSTEM_LIBS=\"grpc\". I've copied the GRPC binaries and headers into a folder called \"prefix\" and set the PREFIX variable before calling configure. Here's the resulting .tf_configure.bazelrc:\r\n\r\n```\r\nbuild --action_env PYTHON_BIN_PATH=\"/home/adamson/tsgit/tf-114-gpu/ext/public/google/tensorflow/1/14/0/python/build_bin/python3.6\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/home/adamson/tsgit/tf-114-gpu/ext/public/google/tensorflow/1/14/0/python/build/tensorflow/python3.6/tensorflow/build/lib/python3.6\"\r\nbuild --python_path=\"/home/adamson/tsgit/tf-114-gpu/ext/public/google/tensorflow/1/14/0/python/build_bin/python3.6\"\r\nbuild:xla --define with_xla_support=true\r\nbuild --action_env TF_NEED_OPENCL_SYCL=\"0\"\r\nbuild --action_env TF_NEED_ROCM=\"0\"\r\nbuild --action_env TF_NEED_CUDA=\"1\"\r\nbuild --action_env TF_NEED_TENSORRT=\"0\"\r\nbuild --action_env TF_CUDA_VERSION=\"10.1\"\r\nbuild --action_env TF_CUDNN_VERSION=\"7.5\"\r\nbuild --action_env TF_NCCL_VERSION=\"2.4\"\r\nbuild --action_env TF_CUDA_PATHS=\"/home/adamson/tsgit/tf-114-gpu/ext/public/google/tensorflow/1/14/0/python/build/tensorflow/python3.6/tensorflow/build/cuda,/home/adamson/tsgit/tf-114-gpu/ext/public/google/tensorflow/1/14/0/python/build/tensorflow/python3.6/tensorflow/build/cudnn,/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/nvidia/nccl/2/4/2/dist\"\r\nbuild --action_env CUDA_TOOLKIT_PATH=\"/home/adamson/tsgit/tf-114-gpu/ext/public/google/tensorflow/1/14/0/python/build/tensorflow/python3.6/tensorflow/build/cuda\"\r\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"6.0\"\r\nbuild --action_env LD_LIBRARY_PATH=\"/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/nvidia/cuda/10/1/local/cuda/lib64/stubs:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/bazel/skylib/lib:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/jq/dist/lib:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/oniguruma/5/9/6/dist/lib:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/hdf5/1/8/19/c/lib:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/vendor/intel/mkl/2019/2/187/dist/lib:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/nvidia/nccl/2/4/2/dist/lib/x86_64-linux-gnu:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/nvidia/cudnn/7/5/1/dist/lib/x86_64-linux-gnu:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/nvidia/cuda/10/1/dist/lib64:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/cares/1/15/0/dist/lib:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/openjdk/11/0/4/dist/lib:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/google/protobuf/3/6/1/c/dist/lib:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/libffi/3/2/1/dist/lib64/:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/libuuid/1/0/3/dist/lib:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/zlib/1/2/8/dist/lib:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/xz/5/2/dist/lib:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/gnu/libs/gcc/x/dist/lib:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/gnu/gcc/5/4/0/dist/exported_libs/:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/gnu/gdb/7/8/c/lib:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/bz2/1/0/6/c/lib:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/sqlite/3/15/1/dist/lib:/opt/openssl/1.0/lib:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/gdbm/1/11/c/lib:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/db/4/7/25/c/lib\"\r\nbuild --action_env TF_CUDA_CLANG=\"0\"\r\nbuild --action_env GCC_HOST_COMPILER_PATH=\"/usr/bin/gcc\"\r\nbuild --config=cuda\r\nbuild:opt --copt=-march=sandybridge\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\nbuild --action_env TF_SYSTEM_LIBS=\"grpc\"\r\nbuild --define=PREFIX=/home/adamson/tsgit/tf-114-gpu/ext/public/google/tensorflow/1/14/0/python/build/tensorflow/python3.6/tensorflow/prefix\r\nbuild --define=LIBDIR=/home/adamson/tsgit/tf-114-gpu/ext/public/google/tensorflow/1/14/0/python/build/tensorflow/python3.6/tensorflow/prefix/lib\r\nbuild --define=INCLUDEDIR=/home/adamson/tsgit/tf-114-gpu/ext/public/google/tensorflow/1/14/0/python/build/tensorflow/python3.6/tensorflow/prefix/include\r\nbuild:v2 --define=tf_api_version=2\r\ntest --flaky_test_attempts=3\r\ntest --test_size_filters=small,medium\r\ntest --test_tag_filters=-benchmark-test,-no_oss,-oss_serial\r\ntest --build_tag_filters=-benchmark-test,-no_oss\r\ntest --test_tag_filters=-no_gpu\r\ntest --build_tag_filters=-no_gpu\r\ntest --test_env=LD_LIBRARY_PATH\r\nbuild --action_env TF_CONFIGURE_IOS=\"0\"\r\n```\r\n\r\nHere are the contents of prefix:\r\n```\r\n/home/adamson/tsgit/tf-114-gpu/ext/public/google/tensorflow/1/14/0/python/build/tensorflow/python3.6/tensorflow/prefix\r\n\u251c\u2500\u2500 include\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 grpc -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/include/grpc\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 grpc++ -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/include/grpc++\r\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 grpcpp -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/include/grpcpp\r\n\u2514\u2500\u2500 lib\r\n    \u251c\u2500\u2500 libaddress_sorting.so -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libaddress_sorting.so\r\n    \u251c\u2500\u2500 libaddress_sorting.so.7 -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libaddress_sorting.so.7\r\n    \u251c\u2500\u2500 libaddress_sorting.so.7.0.0 -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libaddress_sorting.so.7.0.0\r\n    \u251c\u2500\u2500 libgpr.a -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgpr.a\r\n    \u251c\u2500\u2500 libgpr.so -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgpr.so\r\n    \u251c\u2500\u2500 libgpr.so.7 -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgpr.so.7\r\n    \u251c\u2500\u2500 libgpr.so.7.0.0 -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgpr.so.7.0.0\r\n    \u251c\u2500\u2500 libgrpc.a -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc.a\r\n    \u251c\u2500\u2500 libgrpc++.a -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc++.a\r\n    \u251c\u2500\u2500 libgrpc_cronet.a -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc_cronet.a\r\n    \u251c\u2500\u2500 libgrpc++_cronet.a -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc++_cronet.a\r\n    \u251c\u2500\u2500 libgrpc_cronet.so -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc_cronet.so\r\n    \u251c\u2500\u2500 libgrpc++_cronet.so -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc++_cronet.so\r\n    \u251c\u2500\u2500 libgrpc++_cronet.so.1 -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc++_cronet.so.1\r\n    \u251c\u2500\u2500 libgrpc++_cronet.so.1.18.0 -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc++_cronet.so.1.18.0\r\n    \u251c\u2500\u2500 libgrpc_cronet.so.7 -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc_cronet.so.7\r\n    \u251c\u2500\u2500 libgrpc_cronet.so.7.0.0 -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc_cronet.so.7.0.0\r\n    \u251c\u2500\u2500 libgrpc++_error_details.a -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc++_error_details.a\r\n    \u251c\u2500\u2500 libgrpc++_error_details.so -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc++_error_details.so\r\n    \u251c\u2500\u2500 libgrpc++_error_details.so.1 -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc++_error_details.so.1\r\n    \u251c\u2500\u2500 libgrpc++_error_details.so.1.18.0 -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc++_error_details.so.1.18.0\r\n    \u251c\u2500\u2500 libgrpcpp_channelz.a -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpcpp_channelz.a\r\n    \u251c\u2500\u2500 libgrpcpp_channelz.so -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpcpp_channelz.so\r\n    \u251c\u2500\u2500 libgrpcpp_channelz.so.1 -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpcpp_channelz.so.1\r\n    \u251c\u2500\u2500 libgrpcpp_channelz.so.1.18.0 -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpcpp_channelz.so.1.18.0\r\n    \u251c\u2500\u2500 libgrpc++_reflection.a -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc++_reflection.a\r\n    \u251c\u2500\u2500 libgrpc++_reflection.so -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc++_reflection.so\r\n    \u251c\u2500\u2500 libgrpc++_reflection.so.1 -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc++_reflection.so.1\r\n    \u251c\u2500\u2500 libgrpc++_reflection.so.1.18.0 -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc++_reflection.so.1.18.0\r\n    \u251c\u2500\u2500 libgrpc.so -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc.so\r\n    \u251c\u2500\u2500 libgrpc++.so -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc++.so\r\n    \u251c\u2500\u2500 libgrpc++.so.1 -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc++.so.1\r\n    \u251c\u2500\u2500 libgrpc++.so.1.18.0 -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc++.so.1.18.0\r\n    \u251c\u2500\u2500 libgrpc.so.7 -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc.so.7\r\n    \u251c\u2500\u2500 libgrpc.so.7.0.0 -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc.so.7.0.0\r\n    \u251c\u2500\u2500 libgrpc_unsecure.a -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc_unsecure.a\r\n    \u251c\u2500\u2500 libgrpc++_unsecure.a -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc++_unsecure.a\r\n    \u251c\u2500\u2500 libgrpc_unsecure.so -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc_unsecure.so\r\n    \u251c\u2500\u2500 libgrpc++_unsecure.so -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc++_unsecure.so\r\n    \u251c\u2500\u2500 libgrpc++_unsecure.so.1 -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc++_unsecure.so.1\r\n    \u251c\u2500\u2500 libgrpc++_unsecure.so.1.18.0 -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc++_unsecure.so.1.18.0\r\n    \u251c\u2500\u2500 libgrpc_unsecure.so.7 -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc_unsecure.so.7\r\n    \u251c\u2500\u2500 libgrpc_unsecure.so.7.0.0 -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc_unsecure.so.7.0.0\r\n    \u2514\u2500\u2500 pkgconfig -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/pkgconfig\r\n```\r\nand /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/include/grpcpp contains impl/codegen/async_generic_service.h.\r\n\r\nAt build time, bazel complains that it can't find grpcpp/impl/codegen/async_generic_service.h:\r\n\r\n```\r\nERROR: /home/adamson/tsgit/tf-114-gpu/ext/public/google/tensorflow/1/14/0/python/build/tensorflow/python3.6/tensorflow/tensorflow/core/debug/BUILD:49:1: C++ compilation of rule '//tensorflow/core/debug:debug_service_proto_cc' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd /home/adamson/tsgit/tf-114-gpu/ext/public/google/tensorflow/1/14/0/python/build/tensorflow/python3.6/bazel_cache/python3.6/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/bin:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/gnu/gcc/5/4/0/dist/libexec/gcc/x86_64-unknown-linux-gnu/5.4.0:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/nvidia/cuda/10/1/dist/bin:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/google/bazelversions/0/24/1/bin:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/google/protobuf/x/dist/bin:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/gnu/gcc/x/bin:/bin:/usr/bin \\\r\n    PWD=/proc/self/cwd \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/host/bin/tensorflow/core/debug/_objs/debug_service_proto_cc/debug_service.grpc.pb.pic.d '-frandom-seed=bazel-out/host/bin/tensorflow/core/debug/_objs/debug_service_proto_cc/debug_service.grpc.pb.pic.o' -iquote . -iquote bazel-out/host/genfiles -iquote bazel-out/host/bin -iquote external/protobuf_archive -iquote bazel-out/host/genfiles/external/protobuf_archive -iquote bazel-out/host/bin/external/protobuf_archive -iquote external/grpc -iquote bazel-out/host/genfiles/external/grpc -iquote bazel-out/host/bin/external/grpc -isystem external/protobuf_archive/src -isystem bazel-out/host/genfiles/external/protobuf_archive/src -isystem bazel-out/host/bin/external/protobuf_archive/src '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -g0 '-march=native' -g0 -Wno-unknown-warning-option -Wno-unused-but-set-variable -Wno-sign-compare -c bazel-out/host/genfiles/tensorflow/core/debug/debug_service.grpc.pb.cc -o bazel-out/host/bin/tensorflow/core/debug/_objs/debug_service_proto_cc/debug_service.grpc.pb.pic.o)\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nIn file included from bazel-out/host/genfiles/tensorflow/core/debug/debug_service.grpc.pb.cc:6:0:\r\nbazel-out/host/genfiles/tensorflow/core/debug/debug_service.grpc.pb.h:26:55: fatal error: grpcpp/impl/codegen/async_generic_service.h: No such file or directory\r\n #include <grpcpp/impl/codegen/async_generic_service.h>\r\n                                                       ^\r\n```\r\n\r\nThe provided prefix directory isn't showing up in the include options based to gcc (but -iquote external/grpc is). What do I need to do to get the build to look in the prefix directory for the grpc headers?", "comments": ["@perfinion Can you please take a look? Looks similar to #31020 Thanks!", "I figured it out. The grpc systemlibs BUILD file wasn't properly including the headers. I'll post a patch shortly.", "Closing this issue since it's resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31855\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31855\">No</a>\n"]}, {"number": 31854, "title": "added Layer.add_callback", "body": "This is in reference to #30809\r\n\r\n### API Changes:\r\n- added `Layer.add_callback(callback: k.callbacks.Callback) -> None`\r\n- added `Layer.callbacks -> List[k.callbacks.Callback]` returns callbacks added to this layer and all children layers.\r\n\r\n### Under-the-hood Changes\r\n- `model.fit` concatenates the `callbacks` kw param with all children callbacks.\r\n\r\nThat's it \ud83d\ude0a", "comments": ["@rchao Can you please take a look on this PR? Thanks!", "@beasteers Could you please address the reviewer comments and resolve conflicts? Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 31853, "title": "Cannot load saved model with tf.keras.models.load_model", "body": "**System information**\r\n- Wrote custom code for simple model\r\n- Ubuntu 16.04.6 LTS\r\n- Tensorflow 2.0.0-beta1 installed from binary using pip\r\n- Python 3.7, no CUDA or GPU\r\n\r\nLoading a saved model results in error: \r\n__init__() got an unexpected keyword argument 'reduction'\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nmodel = tf.keras.Model(inputs=[input_x], outputs=[logits])\r\nloss = tf.keras.losses.MeanSquaredError()\r\nmodel.compile(optimizer=tf.keras.optimizers.Adam(),  # Optimizer\r\n              # Loss function to minimize; Emphasize not getting false positives with pos_weight\r\n              loss=loss, # tf.nn.weighted_cross_entropy_with_logits(logits,labels,pos_weight=1) # tf.keras.losses.MeanSquaredError()\r\n              # tf.keras.losses.mean_squared_error\r\n              # List of metrics to monitor\r\n              metrics=[tf.keras.losses.MeanSquaredError()])\r\ncheckpointer = tf.keras.callbacks.ModelCheckpoint(session_name + '_backup.h5', save_best_only=True, monitor = 'acc', verbose = 0)\r\nearly_stopper = tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=3, verbose=1,min_delta=0.005)\r\nhistory = model.fit(data_train, roi_zoom_train,\r\n                    batch_size=batch_size,\r\n                    epochs = 1,\r\n                    # We pass some validation for\r\n                    # monitoring validation loss and metrics\r\n                    # at the end of each epoch\r\n                    validation_data=(data_val, roi_zoom_val),callbacks=[checkpointer,early_stopper]) #\r\nmodel.save(session_name + '.h5')\r\nmodel = tf.keras.models.load_model(session_name + '.h5')\r\n```\r\n\r\n**Other info / logs**\r\nTraceback (most recent call last):\r\n  File \"/home/m047659/Programs/pycharm/helpers/pydev/_pydevd_bundle/pydevd_exec2.py\", line 3, in Exec\r\n    exec(exp, global_vars, local_vars)\r\n  File \"<input>\", line 1, in <module>\r\n  File \"/home/m047659/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py\", line 137, in load_model\r\n    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)\r\n  File \"/home/m047659/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py\", line 178, in load_model_from_hdf5\r\n    training_config, custom_objects))\r\n  File \"/home/m047659/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\", line 458, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/home/m047659/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 320, in compile\r\n    self._cache_output_metric_attributes(metrics, weighted_metrics)\r\n  File \"/home/m047659/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 1735, in _cache_output_metric_attributes\r\n    metrics, self.output_names, output_shapes, self.loss_functions)\r\n  File \"/home/m047659/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_utils.py\", line 755, in collect_per_output_metric_info\r\n    metric_name = get_metric_name(metric, is_weighted)\r\n  File \"/home/m047659/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_utils.py\", line 929, in get_metric_name\r\n    metric = metrics_module.get(metric)\r\n  File \"/home/m047659/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/metrics.py\", line 2833, in get\r\n    return deserialize(identifier)\r\n  File \"/home/m047659/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/metrics.py\", line 2827, in deserialize\r\n    printable_module_name='metric function')\r\n  File \"/home/m047659/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py\", line 194, in deserialize_keras_object\r\n    return cls.from_config(cls_config)\r\n  File \"/home/m047659/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 451, in from_config\r\n    return cls(**config)\r\nTypeError: __init__() got an unexpected keyword argument 'reduction'\r\n", "comments": ["Looks like the code is incomplete.Can you please provide full code snippet to reproduce it on our environment.Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31853\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31853\">No</a>\n"]}, {"number": 31852, "title": "Tensorflow2.0 distributed training gives error :- A non-DistributedValues value 8 cannot be reduced with the given reduce op ReduceOp.SUM.", "body": "<em> **Getting the error when i am running distributed training**  as described in this tutorial (https://colab.research.google.com/github/tensorflow/docs/blob/r2.0rc/site/en/r2/guide/distribute_strategy.ipynb#scrollTo=IlYVC0goepdk)</em>\r\n\r\n**System information**\r\n- OS Platform: - Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.0-beta\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.01\r\n- GPU model and memory: p100, 16GB\r\n\r\n```\r\n@tf.function\r\ndef train_step(dist_inputs):\r\n    def step_fn(inputs):\r\n        features, labels = inputs\r\n\r\n        with tf.GradientTape() as tape:\r\n            logits = model(features)\r\n            #print(logits)\r\n            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\r\n                        logits=logits, labels=labels)\r\n            loss = tf.reduce_sum(cross_entropy) * (1.0 / global_batch_size)\r\n        grads = tape.gradient(loss, model.trainable_variables)\r\n        optimizer.apply_gradients(list(zip(grads, model.trainable_variables)))\r\n        return cross_entropy\r\n\r\n    per_example_losses = mirrored_strategy.experimental_run_v2(\r\n            step_fn, args=(dist_inputs,))\r\n    mean_loss = mirrored_strategy.reduce(\r\n            tf.compat.v2.distribute.ReduceOp.MEAN, per_example_losses, axis=0)\r\n    return mean_loss\r\n```\r\nI am getting this error while running abobe distributed code.\r\n`ValueError: A non-DistributedValues value 8 cannot be reduced with the given reduce op ReduceOp.SUM.\r\n\r\n**But it works when I use tf.distribute.ReduceOp.SUM instead of tf.distribute.ReduceOp.MEAN** \r\n`", "comments": ["@akanyaani I tried executing the colab with ReduceOp.SUM and ReduceOp.MEAN but i didn't get any error. Please can you try once and let us know if issue still persists. Thanks!", "Hi @gadagashwini \r\n\r\nIt works on colab because you are not running on multiple GPU, \r\nReduceOp.MEAN is working when I am running on a single GPU, but not working when I am running on multiple GPU.", "@akanyaani Thanks for the update. ", "FWIW, I'm running into the same problem (on 2.0.0-rc0), but changing it to `ReduceOp.SUM` does not help.", "Hi, @reuben You can still train your model on distributed GPU, I am able to train using ReduceOp.SUM only but after that, you have to divide by global batch size you will get the same value which ReduceOp.mean would give.\r\n\r\n```\r\nreduce_sum_out = self.mirrored_strategy.reduce(\r\n            tf.distribute.ReduceOp.SUM, per_example_losses, axis=0)\r\n\r\nloss  = reduce_sum_out / global_batch_size\r\n```\r\nBut I am not 100 percent sure that weather I am doing it correctly on not.\r\n", "@akanyaani I get the same error even if I use ReduceOp.SUM. ", "Hi, @reuben can you share the stack trace because I am able to train on multiple GPU using above code. But that is not a permanent solution.", "I can't get it easily because I've deleted the virtualenv I was using for experimenting with TF2.0.", "I can still reproduce this with TF 2.0 stable: https://colab.research.google.com/drive/1HiablUdEyJECtClfyMr7JtV3ftIbta_M (this is just a copy of https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/distributed_training.ipynb but with code to add two virtual GPUs).", "Hi, @reuben Yes this bug is still there.", "I also have this bug. Anyone gets proper solution or fixes yet?", "Hi, @samuelmat19  You can still train your model on distributed GPU, I am able to train using ReduceOp.SUM only but after that, you have to divide by global batch size you will get the same value which ReduceOp.mean would give.\r\n\r\n```\r\nreduce_sum_out = self.mirrored_strategy.reduce(\r\n            tf.distribute.ReduceOp.SUM, per_example_losses, axis=0)\r\n\r\nloss  = reduce_sum_out / global_batch_size\r\n```", "I also hit this bug and switching to `ReduceOp.SUM` can get around it. Hope someone can fix this! ", "I wonder if this is due to ReduceOp.MEAN trying to reduce the lengths of the cross-replica batches, but those aren't stored distributed in eager mode for some reason?\r\n\r\nI noticed that the value it's failing with is `A non-DistributedValues value 50 cannot be reduced...`, and my batches are size 50.", "it seems that MEAN and SUM do not work like tf.reduce_mean and tf.reduce_sum\r\nfor example,my step_fn return result with shape [batch,512,512],if i ues SUM and set axis = 0,the SUM operation just return [512,512],so i think everyone should notice that.\r\nso for MEAN,i think it also differnt from tf.reduce_mean", "I have the same issue.\r\n \r\n TF 2.0.0;  ananconda python 3.73 ;linux ubuntu LTS 16.04;  GPU: 1080Ti 11G * 2\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.python.client import device_lib\r\n\r\ndef get_available_gpus():\r\n    local_device_protos = device_lib.list_local_devices()\r\n    return [x.name for x in local_device_protos if x.device_type == 'GPU']\r\nget_available_gpus()\r\n```\r\n\r\noutput:  ['/device:GPU:0', '/device:GPU:1']\r\n\r\n```\r\nglobal_batch_size=20\r\nmirrored_strategy = tf.distribute.MirroredStrategy()\r\nwith mirrored_strategy.scope():\r\n  model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(1,))])\r\n  optimizer = tf.keras.optimizers.SGD()\r\ndataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat(1000).batch(\r\n    global_batch_size)\r\ndist_dataset = mirrored_strategy.experimental_distribute_dataset(dataset)\r\n\r\n@tf.function\r\ndef train_step(dist_inputs):\r\n  def step_fn(inputs):\r\n    features, labels = inputs\r\n\r\n    with tf.GradientTape() as tape:\r\n      # training=True is only needed if there are layers with different\r\n      # behavior during training versus inference (e.g. Dropout).\r\n      logits = model(features, training=True)\r\n      cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\r\n          logits=logits, labels=labels)\r\n      loss = tf.reduce_sum(cross_entropy) * (1.0 / global_batch_size)\r\n      #print(\"loss\", loss)\r\n\r\n    grads = tape.gradient(loss, model.trainable_variables)\r\n    optimizer.apply_gradients(list(zip(grads, model.trainable_variables)))\r\n    return cross_entropy\r\n\r\n  per_example_losses = mirrored_strategy.experimental_run_v2(\r\n      step_fn, args=(dist_inputs,))\r\n  \r\n  mean_loss = mirrored_strategy.reduce(\r\n      tf.distribute.ReduceOp.MEAN, per_example_losses, axis=0)\r\n  #print(\"mean_loss\", mean_loss)\r\n  return mean_loss\r\n\r\nwith mirrored_strategy.scope():\r\n  for inputs in dist_dataset:\r\n    print(train_step(inputs))\r\n\r\n```\r\n\r\noutput:\r\n\r\n```\r\nINFO:tensorflow:batch_all_reduce: 2 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-7-3472ab066f79> in <module>\r\n      1 with mirrored_strategy.scope():\r\n      2   for inputs in dist_dataset:\r\n----> 3     print(train_step(inputs))\r\n\r\n~/data1/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    455 \r\n    456     tracing_count = self._get_tracing_count()\r\n--> 457     result = self._call(*args, **kwds)\r\n    458     if tracing_count == self._get_tracing_count():\r\n    459       self._call_counter.called_without_tracing()\r\n\r\n~/data1/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    501       # This is the first call of __call__, so we have to initialize.\r\n    502       initializer_map = object_identity.ObjectIdentityDictionary()\r\n--> 503       self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n    504     finally:\r\n    505       # At this point we know that the initialization is complete (or less\r\n\r\n~/data1/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n    406     self._concrete_stateful_fn = (\r\n    407         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n--> 408             *args, **kwds))\r\n    409 \r\n    410     def invalid_creator_scope(*unused_args, **unused_kwds):\r\n\r\n~/data1/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n   1846     if self.input_signature:\r\n   1847       args, kwargs = None, None\r\n-> 1848     graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n   1849     return graph_function\r\n   1850 \r\n\r\n~/data1/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   2148         graph_function = self._function_cache.primary.get(cache_key, None)\r\n   2149         if graph_function is None:\r\n-> 2150           graph_function = self._create_graph_function(args, kwargs)\r\n   2151           self._function_cache.primary[cache_key] = graph_function\r\n   2152         return graph_function, args, kwargs\r\n\r\n~/data1/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   2039             arg_names=arg_names,\r\n   2040             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 2041             capture_by_value=self._capture_by_value),\r\n   2042         self._function_attributes,\r\n   2043         # Tell the ConcreteFunction to clean up its graph once it goes out of\r\n\r\n~/data1/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    913                                           converted_func)\r\n    914 \r\n--> 915       func_outputs = python_func(*func_args, **func_kwargs)\r\n    916 \r\n    917       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n~/data1/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n    356         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n    357         # the function a weak reference to itself to avoid a reference cycle.\r\n--> 358         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    359     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n    360 \r\n\r\n~/data1/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n    903           except Exception as e:  # pylint:disable=broad-except\r\n    904             if hasattr(e, \"ag_error_metadata\"):\r\n--> 905               raise e.ag_error_metadata.to_exception(e)\r\n    906             else:\r\n    907               raise\r\n\r\nValueError: in converted code:\r\n\r\n    <ipython-input-6-5b7f112175aa>:22 train_step  *\r\n        mean_loss = mirrored_strategy.reduce(\r\n    /home/ana/data1/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py:852 reduce\r\n        denom = self._extended._reduce(reduce_util.ReduceOp.SUM, denom)  # pylint: disable=protected-access\r\n    /home/ana/data1/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py:1436 _reduce\r\n        device_util.current() or \"/device:CPU:0\"))[0]\r\n    /home/ana/data1/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py:701 _reduce_to\r\n        reduce_op, self._device_map, value, destinations)\r\n    /home/ana/data1/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/cross_device_ops.py:102 reduce_non_distributed_value\r\n        \"the given reduce op %s.\" % (value, reduce_op))\r\n\r\n    ValueError: A non-DistributedValues value 10 cannot be reduced with the given reduce op ReduceOp.SUM.\r\n```\r\n", "I have the same error", "I have the same problem ! who can deal with it ?", "@CheungZeeCn @akanyaani\r\n\r\nThe above code run perfectly in `3`-GPU configuration \r\nThe code fails when the `global_batch_size` is perfectly dividable by no.of GPUs.\r\n\r\nFor example, if the no. of GPUs is 4. with global_batch_size 20. Each replica will have batch size of 5 each. This will fail with error  ` ValueError: A non-DistributedValues value 5 cannot be reduced with the given reduce op ReduceOp.SUM.`\r\n\r\nIf 3-GPU batch size across replicas is 7, 7, 6. which will train perfectly\r\n<img width=\"1012\" alt=\"Screenshot 2020-04-24 at 8 41 30 PM\" src=\"https://user-images.githubusercontent.com/17096858/80227920-fcda2980-866b-11ea-83a6-416140a5a835.png\">\r\n\r\nScreenshot of code running in 3 and 4 GPU configuration\r\n`3-GPU`\r\n![screencapture-localhost-8080-notebooks-testing-multigpu-ipynb-2020-04-24-20_44_13](https://user-images.githubusercontent.com/17096858/80228761-129c1e80-866d-11ea-9d8d-279a72909453.png)\r\n`4-GPU`\r\n![screencapture-localhost-8080-notebooks-testing-multigpu-ipynb-2020-04-24-20_45_53](https://user-images.githubusercontent.com/17096858/80228875-3d867280-866d-11ea-95d2-0d4f67159237.png)\r\n", "@akanyaani  It looks like you are using an older Version of Tensorflow. Many bugs have been fixed in the latest version. Could you please execute your code using Latest Version  2.5 and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31852\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31852\">No</a>\n"]}, {"number": 31851, "title": "When subclassing, keras converts SparseTensors to Tensors", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\nSystem information\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu 18.04\r\n\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNo\r\n\r\nTensorFlow installed from (source or binary):\r\nBinary\r\n\r\nTensorFlow version (use command below):\r\n1.14.0\r\n\r\n**Describe the current behavior**\r\n\r\nkeras.Model.fit converts SparseTensors to tensors. \r\n\r\n**Describe the expected behavior**\r\n\r\nkeras.Model.fit passes SparseTensors as is.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow.compat.v2 as tf\r\n\r\ndef dummy_parse_fn(iterable):\r\n  features = {}\r\n  # the input is always constant\r\n  features['sparse_tensor'] = tf.SparseTensor(\r\n    indices=tf.constant([[0,0],[1,1]], dtype=tf.int64),\r\n    values=tf.constant([1.0, 1.0], dtype=tf.float32),\r\n    dense_shape=tf.constant([2, 2], dtype=tf.int64))\r\n  labels = tf.constant([1.0, 1.0], dtype=tf.float32)\r\n  return features, labels\r\n\r\n\r\ndef get_dummy_dataset():\r\n  iterable =  np.random.random((128, 1)).astype(np.float32)\r\n  return (\r\n    tf.data.Dataset\r\n    .from_tensor_slices(iterable)\r\n    .map(dummy_parse_fn)\r\n    .take(1024)\r\n  )\r\n\r\n\r\nclass SparseModel(tf.keras.Model):\r\n  def call(self, features):\r\n    sparse_tensor = features['sparse_tensor']\r\n    assert isinstance(sparse_tensor, tf.sparse.SparseTensor), type(sparse_tensor)\r\n    return sparse_tensor.values\r\n\r\n\r\nif __name__ == \"__main__\":\r\n\r\n  print(tf.__version__)\r\n\r\n  model = SparseModel()\r\n  optimizer = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9, nesterov=True)\r\n  loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\r\n\r\n  model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\r\n  model.fit(get_dummy_dataset(), epochs=2)\r\n```\r\n", "comments": ["Closing this because `tf-nightly` is failing with a more explicit error:\r\n\r\n```\r\n1.15.0-dev20190821\r\nW0821 10:51:25.285699 139658675762816 training_utils.py:1545] Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.\r\n2019-08-21 10:51:25.289208: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-08-21 10:51:25.307253: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2112500000 Hz\r\n2019-08-21 10:51:25.308770: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c6cbd07e50 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2019-08-21 10:51:25.308820: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nTraceback (most recent call last):\r\n  File \"/tmp/sparse_model_v2.py\", line 41, in <module>\r\n    model.fit(get_dummy_dataset(), epochs=2)\r\n  File \"/home/pyalamanchili/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 727, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/home/pyalamanchili/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\", line 643, in fit\r\n    shuffle=shuffle)\r\n  File \"/home/pyalamanchili/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 2418, in _standardize_user_data\r\n    all_inputs, y_input, dict_inputs = self._build_model_with_inputs(x, y)\r\n  File \"/home/pyalamanchili/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 2607, in _build_model_with_inputs\r\n    (input_tensor,))\r\nValueError: All SparseTensor and RaggedTensor inputs must be explicitly declared using a keras.Input() with sparse=True or ragged=True. We found an undeclared input SparseTensor(indices=Tensor(\"DeserializeSparse:0\", shape=(?, 2), dtype=int64), values=Tensor(\"DeserializeSparse:1\", shape=(?,), dtype=float32), dense_shape=Tensor(\"DeserializeSparse:2\", shape=(2,), dtype=int64)). For Sequential models, please add a keras.Input() as your first Layer. For subclassed models, please call self._set_inputs() on your input set, which you can create using keras.Input() for each input to your model.\r\n```\r\n\r\nHowever the suggested alternative doesnt work either as seen here: https://github.com/tensorflow/tensorflow/issues/31819", "For posterity here is a link to a working sample in tf-nightly if someone else is looking: \r\nhttps://gist.github.com/pavanky/3e1635edb322c81436d66ac9d9bdf69f"]}, {"number": 31850, "title": "Using Tensorflow 2.0 loss functions that require inputs gives \"`tf.Tensor` as a Python `bool` is not allowed\"  error", "body": "** System information **\r\n\r\n- Wrote custom code for simple model\r\n- Ubuntu 16.04.6 LTS\r\n- Tensorflow 2.0.0-beta1 installed from binary using pip\r\n- Python 3.7, no CUDA or GPU\r\n- Loading a saved model results in error:\r\ninit() got an unexpected keyword argument 'reduction'\r\n\r\n** Code to reproduce the issue **\r\n\r\nRelevant architecture: \r\n\r\n```\r\nlabels = tf.keras.Input(batch_shape=(batch_size, xsze, ysze), name='labels')\r\nlogits = tf.keras.layers.Conv2D(1, [1, 1], activation='linear', name='output_x')(drop11)\r\nmodel = tf.keras.Model(inputs=[input_x], outputs=[logits]\r\nmodel.compile(optimizer=tf.keras.optimizers.Adam(),  \r\n              loss=loss, \r\n              metrics=[tf.keras.losses.MeanSquaredError()])\r\n```\r\n\r\nIf I use a keras loss class that doesn't require explicit input arguments, it works fine: \r\n`loss = tf.keras.losses.MeanSquaredError() `\r\n\r\nBut I get the error with any loss that requires inputs: \r\n`loss = tf.keras.losses.sparse_categorical_crossentropy(labels,logits)`\r\n\r\nIncluding custom functions which don't seem to be mixing numpy and tensor operations, nor do they check for if not None: \r\n\r\n```\r\ndef dice_loss_per_image(labels, logits):\r\n     smooth = tf.constant(1e-17,dtype=float)\r\n\r\n    numerator = tf.constant(2, dtype=float) * tf.reduce_sum(labels * logits, axis=(1, 2))\r\n    denominator = tf.reduce_sum(labels + logits, axis=(1, 2))\r\n\r\n    loss = tf.constant(1.0,dtype=float) - (numerator + smooth)/(denominator + smooth)\r\n    return loss\r\n```\r\n\r\n** Error traceback ** \r\n\r\nTraceback (most recent call last):\r\n  File \"/home/m047659/Programs/pycharm/helpers/pydev/_pydevd_bundle/pydevd_exec2.py\", line 3, in Exec\r\n    exec(exp, global_vars, local_vars)\r\n  File \"<input>\", line 1, in <module>\r\n  File \"/home/m047659/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py\", line 137, in load_model\r\n    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)\r\n  File \"/home/m047659/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py\", line 178, in load_model_from_hdf5\r\n    training_config, custom_objects))\r\n  File \"/home/m047659/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\", line 458, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/home/m047659/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 320, in compile\r\n    self._cache_output_metric_attributes(metrics, weighted_metrics)\r\n  File \"/home/m047659/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 1735, in _cache_output_metric_attributes\r\n    metrics, self.output_names, output_shapes, self.loss_functions)\r\n  File \"/home/m047659/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_utils.py\", line 755, in collect_per_output_metric_info\r\n    metric_name = get_metric_name(metric, is_weighted)\r\n  File \"/home/m047659/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_utils.py\", line 929, in get_metric_name\r\n    metric = metrics_module.get(metric)\r\n  File \"/home/m047659/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/metrics.py\", line 2833, in get\r\n    return deserialize(identifier)\r\n  File \"/home/m047659/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/metrics.py\", line 2827, in deserialize\r\n    printable_module_name='metric function')\r\n  File \"/home/m047659/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py\", line 194, in deserialize_keras_object\r\n    return cls.from_config(cls_config)\r\n  File \"/home/m047659/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 451, in from_config\r\n    return cls(**config)\r\nTypeError: __init__() got an unexpected keyword argument 'reduction' \r\n\r\nAny way to fix this? \r\nThank you. ", "comments": ["Looks like the code is incomplete.Can you please provide full code snippet to reproduce it on our environment.Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31850\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31850\">No</a>\n", "For anyone running into this, particularly when looking to run inference on a pre-trained `tf.keras.Model` object, simply load the model with the flag `compile=False`.\r\n\r\n```python\r\nmodel = tf.keras.models.load_model('model_file.hdf5', compile=False)\r\n```\r\n\r\nYou will still be able to run inference using the `predict` functionality.\r\n\r\n```python\r\nresult = model.predict(np.zeros((1, 299, 299, 3)))\r\nprint(result)\r\n```\r\n```python\r\narray([[ 6.98463172e-02, -2.29704636e-03, -2.56902035e-02,\r\n         4.15272117e-02, -4.94105788e-03, -1.22769624e-02,\r\n         4.48811650e-02, -3.03351078e-02, -9.19307582e-03,\r\n        -3.35764536e-03, -1.34163469e-01, -2.70350054e-02,\r\n         ...\r\n         4.74378504e-02,  1.43903289e-02, -6.42125458e-02,\r\n        -1.12845495e-01,  5.82809448e-02, -8.94622803e-02,\r\n        -2.12892611e-02]], dtype=float32)\r\n```", "@ghunkins \r\n\r\nRequest you to open the new issue by filling the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).Thanks!", "@ravikyram No issue, just adding a solution for any other lonely wanderers who have the same question / issue.", "@ghunkins Thanks a lot! Your suggestion works. Any intuition behind what is actually going on? This issue only seems to affect saved models. That is a model which is created and tested immediately after training does not have this issue."]}, {"number": 31849, "title": "[ROCm] add Clang 10-based header files into bazel scripts.", "body": "bazel script changes to cope with upcoming hcc toolchain from ROCm 2.8+ which would be based on Clang 10.", "comments": []}, {"number": 31848, "title": "Eager execution prevents using while loops on Keras symbolic tensors", "body": "**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution: Linux Mint 19.1\r\n- TensorFlow installed from: binary (using pip)\r\n- TensorFlow version: 2.0.0-beta1 & 2.0.0-dev10290731 (tried on both)\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: 10.0 / 7.5\r\n- GPU model and memory: Nvidia Quadro P1000 - 4 GB GDDR5\r\n\r\n**Describe the current behavior**\r\n\r\nWhen Eager execution is enabled, `tf.while_loop` uses a backend implementation suited for Eager tensors only, which practically disallows the use of while loops with Keras symbolic tensors. Specifically, the line `while cond(*loop_vars):` ([here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/control_flow_ops.py#L2713) as of this date) is only valid when `loop_vars` is a list of `EagerTensor` instances, thus enabling the `while` check on the `numpy` attribute of the results.\r\n\r\nNow, my issue is that I actually need to implement a function that works on Keras symbolic tensors and uses a `tf.while_loop`, which seemingly proves impossible (apart from disabling Eager execution, which is a workaround I am comfortable to use in the long term, but i does not feel like an actual solution).\r\n\r\nIntuitively, I would think a way to fix the issue would be to follow an alternative route within `tf.while_loop`'s source code when using symbolic tensors (_e.g._ that used when Eager is disabled), and keeping the Eager-suited one otherwise. I tried a nasty fix which consisted in adding `and all(isinstance(tensor, ops.EagerTensor) for tensor in loop_vars)` to the `executing_eagerly = context.executing_eagerly()` line at the beginning of `while_loop`'s body in the source code, but this results in raising `AttributeError: Tensor.name is meaningless when eager execution is enabled.` within the loop constructor (only with symbolic tensors - when using Eager ones, everything goes fine). I would be happy to dig deeper and contribute a fix if I can find one, but first it would be nice to know whether a solution already exist (I might just be missing something, perhaps from the keras backend submodule).\r\n\r\n**Describe the expected behavior**\r\n\r\nI would like to be able to run symbolic tensors through a while loop without disabling Eager execution (basically, I would like Eager execution not to take away practical functionalities which are very useful in designing models without having to put up small hacks of the framework, which are bound to decrease readability and stability).\r\n\r\n**Code to reproduce the issue**\r\n\r\nBase code defining the function I want to implement and two minimalist tests (in practice my symbolic tensors are not mere inputs, but the issue is strictly similar):\r\n```python\r\nimport tensorflow as tf\r\n\r\ndef pred_in_top_k(y_true, y_pred, k=5):\r\n    \"\"\"Check whether targets are in top K predictions, for batched samples.\r\n\r\n    Extension of `tf.keras.metrics.sparse_top_k_categorical_accuracy`\r\n    to batched sequences of targets and predictions.\r\n\r\n    y_true : true labels; tf.int32 or tf.int64 Tensor of shape\r\n             (batch_len, max_seq_len)\r\n    y_pred : predicted probabilities; tf.float32 Tensor of shape\r\n             (batch_len, max_seq_len, n_labels)\r\n    \"\"\"\r\n    # Define the loop's body.\r\n    def body(i, matches):\r\n        \"\"\"Compute matches for a given sample in the batch.\"\"\"\r\n        matching = tf.nn.in_top_k(y_true[i], y_pred[i], k=k)\r\n        matching = tf.expand_dims(tf.cast(matching, tf.float32), 0)\r\n        updated = tf.concat([matches[:i], matching, matches[i + 1:]], axis=0)\r\n        updated.set_shape(matches.shape)\r\n        return i + 1, updated\r\n    # Define the loop's stopping condition.\r\n    def cond(i, _):\r\n        \"\"\"Stop when the entire batch has been processed.\"\"\"\r\n        return tf.less(i, tf.shape(y_true)[0])\r\n    # Run the loop and return the results.\r\n    loop_vars = [tf.constant(0), tf.zeros_like(y_true, dtype=tf.float32)]\r\n    _, matches = tf.while_loop(cond, body, loop_vars)\r\n    return matches\r\n\r\n\r\ndef test_random_tensors():\r\n    \"\"\"\"Run pred_in_top_k on random tensors.\"\"\"\r\n    y_true = tf.random.uniform(shape=(4, 10), maxval=20, dtype=tf.int64)\r\n    y_pred = tf.nn.softmax(tf.random.normal(shape=(4, 10, 20))) \r\n    return pred_in_top_k(y_true, y_pred)\r\n\r\n\r\ndef test_symbolic_tensors():\r\n    \"\"\"Run pred_in_top_k on symbolic tensors.\"\"\"\r\n    y_true = tf.keras.Input((None,), dtype=tf.int64)\r\n    y_pred = tf.keras.Input((None, 20), dtype=tf.float32)\r\n    return pred_in_top_k(y_true, y_pred)\r\n```\r\n\r\n`test_random_tensors()` works both with and without having executed `tf.compat.v1.disable_eager_execution()` first.\r\n\r\n`test_symbolic_tensors()` fails when Eager is left enabled, with the following error messages depending on the tensorflow 2.0 installation used:\r\n\r\n2.0b1:\r\n```python\r\nTypeError: Using a `tf.Tensor` as a Python `bool` is not allowed. Use `if t is not None:` instead of `if t:` to test if a tensor is defined, and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the value of a tensor.\r\n```\r\n\r\n2.0 nightly:\r\n```python\r\nOperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.\r\n```\r\n\r\nNote that decoration with `@tf.function` does not solve the issue, as functions wrapped this way do not accept Keras symbolic tensors as inputs.", "comments": ["**Additional note**\r\n\r\nIn the specific case I used as en example, there is a smarter way to proceed which does not require using a while loop (one simply needs to flatten the batches, use the `tf.nn.in_top_k` function and reshape the outputs).\r\n\r\nThat being said, I still feel like the issue may be relevant as this kind of workaround is not always possible - I therefore insist that my issue is a general one, and not really attached to the example use case.", "@pandrey-fr ,\r\nCan you please provide the gist of colab for 2 scenario mentioned ?Thanks!", "@oanush \r\n\r\n1st case: run the code I provided, plus:\r\n```python\r\ntest_random_tensors()    # works\r\ntest_symbolic_tensors()  # crashes\r\n```\r\n\r\n2nd case (you need to restart Python): run the code I provided, plus:\r\n```python\r\ntf.compat.v1.disable_eager_execution()\r\ntest_random_tensors()    # works\r\ntest_symbolic_tensors()  # works\r\n```", "Issue replicating with TF version-2.0beta,please find the [gist](https://colab.sandbox.google.com/gist/oanush/3c1fa81e49e4597ce75fa8a8fde529d1/31848.ipynb) of colab.Thanks", "@pandrey-fr The following works in both the cases. Please take a look at [the gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/64f61393de99906f7134650142bec277/tf31848.ipynb). Thanks!\r\n\r\n```\r\ndef test_symbolic_tensors():\r\n    \"\"\"Run pred_in_top_k on symbolic tensors.\"\"\"\r\n    y_true = tf.keras.Input((None,), dtype=tf.int64)\r\n    y_pred = tf.keras.Input((None, 20), dtype=tf.float32)\r\n    return tf.keras.layers.Lambda(lambda y:pred_in_top_k(*y))([y_true, y_pred])\r\n```", "@jvishnuvardhan Wow, that is great! I must say I do not quite get yet when exactly it is relevant to use Lambda layers and when it is not, but at least I will know to try it when I encounter similar issues... Thank you :-)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31848\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31848\">No</a>\n"]}, {"number": 31847, "title": "Update lite/experimental/micro to Ambiq Apollo3 SDK 2.2.0", "body": "Pulling in the latest SDK from Ambiq to support the Apollo3 microcontroller used in the lite/experimental/micro examples 'micro_speech' and 'micro_vision'\r\n\r\nThis is regular maintenance that will keep future development efforts relevant. ", "comments": ["I tried this code but could not manage to flash the code to the Edge v2 device. \r\nIt fails at uart_wired_update.py\r\n\r\nSteps:\r\n\r\ngmake -f tensorflow/lite/experimental/micro/tools/make/Makefile TARGET=sparkfun_edge2 micro_speech_bin\r\n\r\npython3 tensorflow/lite/experimental/micro/tools/make/downloads/AmbiqSuite-Rel2.2.0/tools/apollo3_scripts/create_cust_image_blob.py --bin tensorflow/lite/experimental/micro/tools/make/gen/sparkfun_edge2_cortex-m4/bin/micro_speech.bin --load-address 0xC000 --magic-num 0xCB -o main_nonsecure_ota --version 0x0\r\n\r\npython3 tensorflow/lite/experimental/micro/tools/make/downloads/AmbiqSuite-Rel2.2.0/tools/apollo3_scripts/create_cust_wireupdate_blob.py --load-address 0x20000 --bin main_nonsecure_ota.bin -i 6 -o main_nonsecure_wire --options 0x1\r\n\r\npython3 tensorflow/lite/experimental/micro/tools/make/downloads/AmbiqSuite-Rel2.2.0/tools/apollo3_scripts/uart_wired_update.py -b 115200 /dev/cu.wchusbserial1420 -r 1 -f main_nonsecure_wire.bin -i 6\r\n\r\nError msg:\r\n\r\ntensorflow/lite/experimental/micro/tools/make/downloads/gcc_embedded//lib/gcc/arm-none-eabi/7.3.1/thumb/v7e-m/fpv4-sp/hard/crtbegin.o\r\narm-none-eabi-objcopy tensorflow/lite/experimental/micro/tools/make/gen/sparkfun_edge2_cortex-m4/bin/micro_speech tensorflow/lite/experimental/micro/tools/make/gen/sparkfun_edge2_cortex-m4/bin/micro_speech.bin -O binary\r\nHeader Size = 0x80\r\noriginal app_size 0x17bb4 ( 97204 )\r\nload_address 0xc000 ( 49152 )\r\napp_size 0x17bb4 ( 97204 )\r\nw0 = 0xcb017c34\r\nSecurity Value 0x10\r\nw2 = 0x10008080\r\naddrWord = 0xc000\r\nversionKeyWord = 0x0\r\nchild0/feature = 0xffffffff\r\nchild1 = 0xffffffff\r\ncrc = 0xb3b6c4ab\r\nWriting to file main_nonsecure_ota.bin\r\nHeader Size = 0x60\r\napp_size 0x17c34 ( 97332 )\r\nWriting to file main_nonsecure_wire.bin\r\nImage from 0x0 to 0x17c34 will be loaded at 0x20000\r\nConnecting with Corvette over serial port /dev/cu.wchusbserial1420...\r\nSending Hello.\r\nReceived response for Hello\r\nReceived Unknown Message\r\nmsgType = 0x0\r\nLength = 0x0\r\n['0x3c', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0']\r\n!!!Wired Upgrade Unsuccessful!!!....Terminating the script\r\n\r\nI tried to change baud rate to 921600 but it did not help.\r\n\r\n\r\nI have earlier worked with Edge v1 device and flashing that device has worked fine with baud rate 921600. The difference is that v1 device has button 14 which does not exist in v2. ", "@piisku78 Thanks for taking a look at my PR and taking the time to report your experience. This is an expected problem that we can fix easily. First I'll explain why and then I'll tell you how to work around it.\r\n\r\n### What\r\nThe Ambiq serial uploader tool is unable to flash the Edge2.\r\n\r\n### Why\r\nThere are a number of differences between the Edge2 and the original Edge board and one of them directly impacts the bootloading process.\r\n\r\n1. BOOT pin. One the Edge the BOOT pin polarity was active low but on the Edge2 it is active high. On the Edge you had manual control of the boot pin but on the Edge2 there is an RC circuit to handle this automatically (given the correct action of the DTR line) ([schematic here](https://cdn.sparkfun.com/assets/8/f/6/1/f/SparkFun_Edge2_Artemis.pdf)). The [codelabs tutorial](https://codelabs.developers.google.com/codelabs/sparkfun-tensorflow/#0) that most people follow uses the vanilla uploader script that comes with the AmbiqSuite SDK - it does not properly wiggle the DTR line and has some additional shortcomings as well. You could either manually install a button/switch to force BOOT HIGH (cumbersome) or you could use our modified tools to upload your compiled binary (combines the preparation steps with bootload for a single nice command). \r\nIf you are trying out this branch/PR (and you've run the make command with a target that properly downloads the dependencies etc...) the most up-to-date copy of the tool is located at tensorflow\\lite\\experimental\\micro\\tools\\make/downloads/AmbiqSuite-Rel2.2.0/boards_sfe/common/tools_sfe/ambiq/ambiq_bin2board.py. Usage is like this:\r\n```\r\npython3 ambiq_bin2board.py --bin \"{build.path}/{build.project_name}.bin\" --load-address-blob 0x20000 --magic-num 0xCB -o {build.path}/{build.project_name} --version 0x0 --load-address-wired 0xC000 -i 6 --options 0x1 -b {upload.sbl_baud} -port \"{serial.port}\" -r 2 {upload.verbose}\r\n```\r\n\r\n**build.path** and **build.name** locate your binary file and are also used to locate intermediate files\r\n**upload.sbl_baud** = 115200 (the Edge2 has a reduced baud rate for this bootloader for improved fallback safety)\r\n**upload.verbose** = either -v or blank\r\n**serial.port** = platform/case dependent. You probably know what to put there\r\n\r\n(Note: you *could* use the improved bootloader (SparkFun Variable Loader) as long as it is present in flash and if you use a properly modified linker script when building TF -- but that is TMI to go into RN)\r\n\r\n2. TARGET. Just worth mentioning that you should use TARGET=sparkfun_edge2 when building for the Edge2 because there are a few other HW differences that need to be accounted for in a unique implementation.\r\n\r\nHope that helps!\r\n\r\n(Also beware that I have run into instability (it would seem) in the Edge2 implementation... more on that in a follow-up comment)", "Instead of the analog microphones on the Edge board the Edge2 uses PDM microphones -- like the apollo3_evb  target. Copying the ```audio_provider.cc``` implementation from apollo3_evb and changing the pin definitions seems like it should be enough to run. (```command_responder.cc``` is copied directly from the sparkfun_edge target because the BSP abstracts away LEDs and the UART port in use is the same)\r\n\r\nHowever when testing this I would see the audio provider function called several times (getting the requested number of sample frames) and then the application would hang in the classification within a convolution operation. \r\n\r\nI only recently received an Apollo3 EVB and so have not yet had time to cross check that operation with that on the Edge2. If you run into this trouble let me know and perhaps we can find a solution. \r\n\r\nDespite that hiccup this PR should still be considered for acceptance right now because all existing functionality remains unscathed but with more up-to-date tools (SDK) and long term support efforts (BSPs)", "Thanks @oclyke, I managed to load the binaries on Edge 2!\r\n\r\nI will next try to get the project working, my app also hangs...", "@piisku78, @oclyke  : Try increasing the stack size.\r\n\r\nTensorflow copies and patches startup_gcc.c already to increase the stack as part of the SDK download.\r\nThe file used to be in tensorflow/lite/experimental/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0/boards/apollo3_evb/examples/hello_world/gcc_patched/ - but may be in a slightly different path with the new SDK.\r\nYou should be able to just manually patch the file for testing.\r\n\r\nRelevant section:\r\n```C\r\n//*****************************************************************************\r\n//\r\n// Reserve space for the system stack.\r\n//\r\n//*****************************************************************************\r\n__attribute__ ((section(\".stack\")))\r\nstatic uint32_t g_pui32Stack[1024*20];\r\n```\r\n", "@suphoff great catch - I must have overlooked that the path changes would affect the patch! Thanks for the suggestion!", "@suphoff I just verified that the patch still properly changes and utilizes the linker script and ```satrtup_gcc.c``` . This is thanks to the fact that the patch function takes the path to the SDK as an argument and this PR adds a variable to track the location of the SDK (which is updated to AmbiqSuiteRel2.2.0)\r\n\r\nI also tried manually doubling the available stack space to no avail. I was eager to believe that increasing the stack would fix the example but it seems like we will need to keep looking. ", "I think the problem lies in audio_provider.cc interruption handler which gets never called. And that is the reason the application hangs... I cannot find any reason why interruption never comes. It looks like it is correctly initialized...\r\n\r\nIs the name of interruption handler correct \"am_pdm0_isr\", I think it was earlier \"am_pdm_isr\". Anyway I tried both but it did not help.\r\n\r\nAre there any other example project which uses microphones on Edge2 which we could you to check the interruption init?", "@piisku78 I thought the same thing, however during my testing I found that commenting out [these lines](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/micro/examples/micro_speech/main_functions.cc#L148-L171) allowed the example to run and report audio data without hanging. (I would have to double-check that the data is non-zero but even if it was shouldn't the model be able to handle an all-zero input?)", "@piisku78 yes there is an example you can run on your Edge2 to demonstrate microphone functionality.\r\n\r\n[SparkFun AmbiqSuiteSDK](https://github.com/sparkfun/AmbiqSuiteSDK)\r\nClone this repo, follow the getting started instructions, and build the example\r\n``` bash\r\ncd boards_sfe/edge2/examples/pdm_fft/gcc\r\nmake bootload\r\n```\r\n\r\nthen connect with a serial terminal and check the output (lists the strongest frequency in the fft)", "I am sure interruption never comes because I set logging in there.\r\n\r\nIn the interruption handler they calculate \"g_latest_audio_timestamp\" which is then used to decide if there is enough audio data available. So the audio data never comes available and the whole app hangs. \r\n\r\nI think the model should be able to handle an all-zero input.", "@oclyke:  I had high hopes for doubling the stack size :-( \r\nDoes it work with sparkfun_edge (V1) with the patch? Does it still work without it?\r\nDo you have it hooked up on jtag? (and if so - what happens?)\r\nHave you tried to disable all cache/readahead functionality in sparkfun_edge2 initialization to keep it as similar as possible to V1 code?", "@oclyke wrote:\r\n> However when testing this I would see the audio provider function called several times (getting the requested number of sample frames) and then the application would hang in the classification within a convolution operation.\r\n\r\nIs this still the case? \r\nIt seems to contradict @piisku78 observations and makes me more suspicious of the flash/sram setup changes.", "@piisku78 @suphoff I think I was wrong. I am double checking right now but I'm starting to recall that I had jerry-rigged the ```LatestAudioTimestamp()``` function to always increment the time and that's why it would continue to run. \r\n\r\nHowever I also think that with that setup I was seeing another problem that caused a crash/hang in the feature processing portion. I had the Edge2 connected to my debugger and consistently saw an error occur within ```DepthwiseConvOptimizedForFilterWidthEight()```\r\n\r\nLet's first work out the ISR problem, then see if there is more to do. In that direction I just verified that in ```audio_provider.cc``` the pdm isr function is marked as ```extern\"C\" am_pdm0_isr``` and in the patched file ```gcc_patched/startup_gcc.c``` the vector table lists ```am_pdm0_isr```. Now I am comparing the PDM setup code between the fft example and TFLM. I will report back soon.", "@suphoff I haven't tried modifying the cache/readahead functionality. I wasn't cognizant of any major difference between Edge V1 and V2 startup code. Are you talking about BSP functions? I'll definitely look into it. If you have something particular in mind let me know, thanks!", "@oclyke :  You may have gotten unlucky and the first audio interrupt always happened during DepthwiseConvOptimizedForFilterWidthEight() when you bypassed the data ready functionality.\r\n\r\nInitAudioRecording sets up most of the MCU (as clearly indicated by the name :joy_cat:  ) and sets up caches/readahead/timings differently in V2 then in V1. But audio interrupt issues seem to be a lot more likely right now.", "Which PDM Config is correct for Edge 2? At least gain values are totally different in these two:\r\n\r\nAmbiq Example FFT: https://github.com/sparkfun/SparkFun_Apollo3_AmbiqSuite_BSPs/blob/ea7d77c0497e627450b4bbea62c5ab03bea96d21/common/examples/pdm_fft/main.c\r\n\r\nam_hal_pdm_config_t g_sPdmConfig =\r\n{\r\n    .eClkDivider = AM_HAL_PDM_MCLKDIV_1,\r\n    .eLeftGain = AM_HAL_PDM_GAIN_0DB,\r\n    .eRightGain = AM_HAL_PDM_GAIN_0DB,\r\n    .ui32DecimationRate = 64,\r\n    .bHighPassEnable = 0,\r\n    .ui32HighPassCutoff = 0xB,\r\n    .ePDMClkSpeed = AM_HAL_PDM_CLK_6MHZ,\r\n    .bInvertI2SBCLK = 0,\r\n    .ePDMClkSource = AM_HAL_PDM_INTERNAL_CLK,\r\n    .bPDMSampleDelay = 0,\r\n    .bDataPacking = 1,\r\n    .ePCMChannels = AM_BSP_PDM_CHANNEL,\r\n    .ui32GainChangeDelay = 1,\r\n    .bI2SEnable = 0, \r\n    .bSoftMute = 0,\r\n    .bLRSwap = 0,\r\n};\r\n\r\n\r\nOr Micro Speech project: https://github.com/sparkfun/tensorflow/blob/master/tensorflow/lite/experimental/micro/examples/micro_speech/sparkfun_edge2/audio_provider.cc\r\n\r\nam_hal_pdm_config_t g_sPdmConfig = {\r\n    .eClkDivider = AM_HAL_PDM_MCLKDIV_1,\r\n    .eLeftGain = AM_HAL_PDM_GAIN_P165DB,\r\n    .eRightGain = AM_HAL_PDM_GAIN_P165DB,\r\n    .ui32DecimationRate =\r\n        48,  // OSR = 1500/16 = 96 = 2*SINCRATE --> SINC_RATE = 48\r\n    .bHighPassEnable = 1,\r\n    .ui32HighPassCutoff = 0x2,\r\n    .ePDMClkSpeed = AM_HAL_PDM_CLK_1_5MHZ,\r\n    .bInvertI2SBCLK = 0,\r\n    .ePDMClkSource = AM_HAL_PDM_INTERNAL_CLK,\r\n    .bPDMSampleDelay = 0,\r\n    .bDataPacking = 0,\r\n    .ePCMChannels = AM_BSP_PDM_CHANNEL,\r\n    .ui32GainChangeDelay = 1,\r\n    .bI2SEnable = 0,\r\n    .bSoftMute = 0,\r\n    .bLRSwap = 0,\r\n};\r\n\r\n", "Great question. The existing configuration (in micro_speech) was copied from the apollo3evb target implementation. Then the BSP-specific channel was used and no other modifications. \r\n\r\nThe apollo3evb likely uses a different model of PDM mic so perhaps the gain values will need to be adjusted in the future. \r\n\r\nP.s. further investigation of the pdm isr coming soon", "@piisku78 with the fix from this commit ([9933b25](https://github.com/tensorflow/tensorflow/pull/31847/commits/9933b25a5d91588e2079ecd595ca3b7a8599e0d9)) you should be able to see the PDM ISR successfully executing as long as you comment out the [offending code](https://github.com/tensorflow/tensorflow/blob/eea8a615899b534928a22ce1bd9073ccd88021ca/tensorflow/lite/experimental/micro/examples/micro_speech/main_functions.cc#L139-L162).", "Yes I can see PDM ISR now.\r\n\r\nHowever the example project does not work with machine learning model. I think the problem lies in how data is fetched from PDM ISR to the model. I set some logging in the src code and managed to run the code with model about 10 seconds before the program got stuck. Also logging made some parts of the program slower and and things worked for a while. In the earlier Edge version of this project there was a timer which was used to keep things in sync. So I suspect things are now not in sync.", "Ahhhh...... That is an interesting observation. Please keep me updated if you get results from testing any synchronization.\r\n\r\nI vaguely remember also getting different behavior depending on what I did with logging... That might be important to keep in mind. In fact if I remember correctly I have gotten positive identifications for 'yes' and 'no' with the Edge2 hardware - but I had so much logging output that I could not see the results except by recording it and searching through later.", "I think there is something around logging and keeping things in sync. I have Forked @oclyke pull request to try and add support for the Sparkfun Artemis RedBoard. The only real changes were pointing to the correct BSP file and updating command_responder.cc to comment out some of the LEDs. I did manage to get the Micro Voice example working.\r\n\r\nMy Fork of the Sparkfun PR, is here: https://github.com/robotastic/tensorflow/commit/ac89c877ea212e9750e386db7bb0187eb0cfe9d7\r\n\r\nInstead turning on LEDs to display results, I made it printout a debug message. I didn't realize the results were already being printed elsewhere. The interesting thing is that if I comment out the debug messages in command_responder.cc, the program hangs. \r\n\r\nI don't have an Edge 2 to test with, but I bet adding `error_reporter->Report(\"\\nYES\");` to line 52 of command_responder.cc and the doing something similar for the rest of the results will get things running.\r\n\r\nI will investigate this more, but there seems to be some timing/interrupt problem. I will try to find the root cause.", "@robotastic Thanks! I will try out your fork.\r\n\r\nEdit: Great! Got it working right away. Can you sign a CLA with google? If you can then I could accept a PR from your fork to add to this PR. You have to make sure that the email associated with the git commit (not strictly the same as your GitHub user email) is accepted in the Google CLA database.\r\n\r\nI will now try to apply your fix to the Edge2 as well -  see if that changes anything.", "Would be nice to unify the directories for all Ambiq boards.\r\nWith the right Makefile framework that passes on options this could be a good showcase for all Artemis modules and Qiic extension (i2c) boards.", "@suphoff Can you elaborate on what you mean by showcasing all the Qwiic extension boards? Do you mean, perhaps, building examples that use other kinds of sensor data?\r\n\r\nI am planning to support all SparkFun Artemis boards with this PR. Anything beyond that would probably be scope creep but we could open another PR that builds on this. \r\n\r\nAs for unifying directories do you mean trying to combine the various ```sparkfun_BOARDNAME``` directories that contain similar implementations like ```debug_log.cc```?", "@robotastic So I was unable to get the same results with the Edge2. First I simply added ```YES``` ```NO``` and ```UNKNOWN``` debug statements like in your fork - I still got a full hang with the PDM initialization indicator LED stuck on. Then in order to make our code as similar as possible I modified ```audio_provider.cc``` and ```command_responder.cc``` so that only one LED was initialized / used (like the RBA). Oddly enough with both of those changes the example appeared not to hang - i.e. the blue led was blinking continuously. However UART output was silent and there was no response for either YES or NO. Also the other three LEDs were on - but I think that's just a consequence of default pin states or something. \r\n\r\nI wonder if this might be more of a stack size/timing problem. So rather than the changes to timing perhaps the stack is not large enough or is configured poorly?\r\n\r\nI'm thinking that to truly understand we will need to dive in with a debug probe. At least your RBA demo will provide a baseline so thanks again for that discovery!\r\n\r\n@robotastic can I send you an Edge2 board to work with?", "@oclyke : Different boards will have different number of LEDs - plugable/optional command responder / output handler .... for the examples could then also be extended (with patches in tutorials) to showcase OLED, LCD, LEDs..", "I have synced the code like this in main loop(). The idea is to wait until there is always 1000ms new sample data in buffer and then do inferencing. \r\n\r\nconst int32_t current_time = LatestAudioTimestamp();\r\nif((current_time == 0) || (current_time > (previous_time + 1000))) {\r\n   \r\n    Here I get samples and do inferencing\r\n\r\n    previous_time = current_time;\r\n  } else {\r\n    error_reporter->Report(\"waiting samples\");\r\n    am_util_delay_ms(100);\r\n  }", "@oclyke I think you could be on to something with the stack sizing or maybe alignment. I found that if I simply print out the response in main_functions.cc and comment out the call to RespondToCommand() it will also work fine. https://github.com/robotastic/tensorflow/commit/b7034f1c62e61d28767a3f70ec1fd79f4d24e2f9#diff-c535c30baa62a841eadd8566a94fe5b7L170-L172\r\n\r\nHowever if I use the RespondToCommand() call and comment out everything but the same debug message in that function, it hangs.\r\n\r\nIt would be great to work with an Edge 2 and try to narrow down the difference. I have a J-Link and can to dig deeper. My email is luke@robotastic.com\r\n\r\n@piisku78 I tried adding this and it did help prevent deadlock... but then inference stopped happening at all.", "I have taken this project and modified it a bit to use my own machine learning model. I am able to run my project now, but I dont use at all the original command_responder.cc so the problem is probably there why it still hangs for you. \r\n\r\nMy problem is that even though it is running I get worse results than on old Edge device. I think audio recognition does not work at all. So I decided to save sample values from g_audio_capture_buffer to a wav file. The sound of my wav does not make sense at all. So I am wondering if the audio processing is still wrong in this new version. Why in Edge v2 version we get the interruption in to am_pdm0_isr(), in Edge v1 it comes to am_adc_isr. Is the sample data after pdm dma transfer in a format which I could take and play as wav or should I still convert the sample data? ", "@robotastic: I tried your branch with RespondToCommand enabled and it works just fine on a redboard ATP. ( With and without burst mode enabled )\r\n", "Has somebody managed to save audio from the microphone and get correct sound?", "@piisku78 : Planning to do that soon as copying/casting the audio data out of the DMA buffer looks suspicious. ", "@piisku78 : Data looks correct but with low amplitude. (Plotted 1s audio)\r\nOn my board (redboard ATP) setting the PDM gain values to AM_HAL_PDM_GAIN_P255DB seems to work well. (no over saturation and good value range) \r\n```C\r\nam_hal_pdm_config_t g_sPdmConfig = {\r\n    .eClkDivider = AM_HAL_PDM_MCLKDIV_1,\r\n    .eLeftGain =  AM_HAL_PDM_GAIN_P255DB, //AM_HAL_PDM_GAIN_P165DB,\r\n    .eRightGain = AM_HAL_PDM_GAIN_P255DB, //AM_HAL_PDM_GAIN_P165DB,\r\n     ....\r\n}\r\n```\r\nPS: PDM produces signed values - the first edge device used an ADC that produced unsigned values.", "I messed around with my code a little and things started to get a little flakey. Simplifying what is in the RespondToCommand() function seems to help. The LEDs are being constantly cleared outside the new_command if statement. Moving those inside helps. Also, moving the LED initialization out into a separate function called as part of setup() makes mores sense. Oddly, also having the static variable in there seemed to cause a problem... which points to some weird stack/memory sizing error.\r\n\r\nIn short, reducing what is in RespondToCommand() seems to help, but that might just be treating the symptom. \r\n\r\nPerhaps the DMA->Buffer is getting interrupted by printing to serial or messing with the LED states.\r\n\r\n@suphoff Could you share the code? I was going to mess around with the gain and also see how directional the microphone is.", "@robotastic : I just hacked up the debug code from the micro_vision for micro_speech.\r\nThis is really ugly code, most comments are unchanged from the micro_vision version  and may now be completely wrong...\r\n\r\n```c\r\nvoid dump_buffer(int16_t* data, int length) {\r\n am_util_stdio_printf(\"+++ frame +++\");\r\n  for (uint32_t i = 0; i < length; i++) {\r\n    if ((i & 0xF) == 0x00) {\r\n      am_util_stdio_printf(\"\\n\\r%8d \", i);\r\n      // this delay is to let itm have time to flush out data.\r\n      am_util_delay_ms(1);\r\n    }\r\n    am_util_stdio_printf(\"%6d \", data[i]);\r\n  }\r\n  am_util_stdio_printf(\"\\n\\r--- frame ---\\n\\r\");\r\n  am_util_delay_ms(1);\r\n}\r\n```\r\n\r\nHooked it up to the interrupt handler so it dumps the audio buffer after 2s of capture and then just loops:\r\n```c\r\nextern \"C\" void am_pdm0_isr(void) {\r\n...\r\n g_latest_audio_timestamp =\r\n        (g_total_samples_captured / (kAudioSampleFrequency / 1000));\r\n  }\r\n// Debug \r\nif (g_latest_audio_timestamp > 2000) {\r\n    dump_buffer( g_audio_capture_buffer, kAudioCaptureBufferSize);\r\n    while(true) {};\r\n  }\r\n...\r\n```\r\nThen hacked up the utilities from micro_vision to read and extract the data from a file of captured serial output.\r\n```python\r\ndef parse_file(inputfile, size):\r\n \r\n  data = None\r\n  bytes_written = 0\r\n  frame_start = False\r\n  frame_stop = False\r\n  frame_list = list()\r\n\r\n  # collect all pixel data into an int array\r\n  for line in inputfile:\r\n    if line == '+++ frame +++\\n':\r\n      frame_start = True\r\n      data = np.zeros(size, dtype=np.int16)\r\n      bytes_written = 0\r\n      continue\r\n    elif line == '--- frame ---\\n':\r\n      frame_stop = True\r\n\r\n    if frame_start and not frame_stop:\r\n      linelist = re.findall(r\"-?[\\w']+\", line)\r\n\r\n      if len(linelist) != 17:\r\n        # drop this frame\r\n        frame_start = False\r\n        continue\r\n\r\n      for item in linelist[1:]:\r\n        data[bytes_written] = int(item, base=10)\r\n        bytes_written += 1\r\n\r\n    elif frame_start and frame_stop:\r\n      if bytes_written == size:\r\n        frame_list.append(data)\r\n        frame_start = False\r\n        frame_stop = False\r\n\r\n  return frame_list\r\n...\r\nframe_list = parse_file(open(args.inputfile), 16000)\r\n...\r\n```\r\n", "DMA error reporting is currently broken on PDM and may cause some of the issues - see : #33955\r\nThe DMA may be starved due to several factors. \r\nCan you apply the change to your sources and report if you can now see DMA errors?\r\n( If so - there are several ways to prevent starvation )", "Thanks @suphoff I am now able to save and play audio samples from Edge 2 microphone correctly. ", "Thanks @suphoff I got recording playback up and running. I have some janky code up here: https://github.com/robotastic/tensorflow/tree/record-speech/tensorflow/lite/experimental/micro/examples/debug_speech\r\n\r\nI also updated to fix the DMA error reporting and I did not get any DMA errors when I set it up to hang. Maybe it hangs before the message can be printed.\r\n\r\nIt tried to refine things down to see if I could isolate the issue. It is weird though. If have the code below inside the **loop()** in _main_functions.cc_ it works fine. When the same exact code is inside the  **RespondToCommand()** function in _command_responder.cc_ it hangs. I have commented out all of the other code in RespondToCommand(). I tried to increase the stack size in the patched version of hello_world, but that didn't help. It seems like calling a function is causing a hang... which doesn't seem right.\r\n\r\n````\r\n  if (is_new_command) {\r\n    uint32_t delay = 60;\r\n    error_reporter->Report(\"\\nHeard %s (%d) @%dms\\n\", found_command, score,\r\n                           current_time);\r\n    am_util_delay_ms(delay);\r\n    if (found_command[0] == 'y') {\r\n      error_reporter->Report(\"\\nYES\");\r\n      //am_hal_gpio_output_set(AM_BSP_GPIO_LED_YELLOW);\r\n    }\r\n    if (found_command[0] == 'n') {\r\n      error_reporter->Report(\"\\nNO\");\r\n      //am_hal_gpio_output_set(AM_BSP_GPIO_LED_RED);\r\n    }\r\n    if (found_command[0] == 'u') {\r\n      error_reporter->Report(\"\\nUNKNOWN\");\r\n      //am_hal_gpio_output_set(AM_BSP_GPIO_LED_GREEN);\r\n    }\r\n````\r\n\r\nhttps://github.com/robotastic/tensorflow/blob/record-speech/tensorflow/lite/experimental/micro/examples/micro_speech/main_functions.cc", "@robotastic: I have a tough time believing in stack issues in RespondToCommand if it survived multiple inference steps before - this also works just fine when I run it on a redboard ATP.\r\n\r\n@piisku78: Is the micro_speech example still hanging on your Edge 2 board?\r\nIf so - could you try to run it on battery? (or at lest with battery inserted)", "@suphoff - Agreed, it didn't make much sense to me either. I went and rebased against upstream Tensorflow and that seemed to fix the problem. \r\n\r\nI have tested against both the Sparkfun Edge 2 and the Redboard Artemis and things seem to work reliably. I have tried adding and removing functions and it is working fine.\r\n\r\nMy code is up here, in the **sparkfun_artemis** branch: https://github.com/robotastic/tensorflow/tree/sparkfun_artemis", "@robotastic, @suphoff, and @piisku78 - thanks for all your input here! Do I understand correctly that syncing with the upstream repo is the main solution here? What else is needed to get such fantastic results as the following?\r\n\r\n> I have tested against both the Sparkfun Edge 2 and the Redboard Artemis and things seem to work reliably.\r\n\r\nAnyone who has signed Google CLAs can add whatever is needed so that this PR can finally go through \ud83e\udd1e ", "I just added the two additional changes that I think I identified from the discussion above. At least now the Edge2 binary appears not to hang (blue led blinks furiously), but I have yet to see anything printed to the terminal after \"PDM DMA Threshold = 16\" (including detections of \"yes\", \"no\", and \"unknown\".\r\n", "I think there is still a problem how the spectrogram is created. The project does not create spectrogram correctly and thus the recognition does not work.\r\n\r\nI have tested this project in the following way:\r\nFirst I saved a short audio file out of a sound which I want to recognize and make a wav file. Also I took the audio samples in Edge 2 audio_provider.cc. The audio quality is good, way better than in old Edge version. I gave this audio file as an input to wav_to_features.py script in Tensorflow repo which creates spectrogram features. Then I hardcoded the spectrogram values in feature_provider.cc and gave this spectrogram as input to machine learning algorithm. I get very good recognition results. This kind of proofs that there is something wrong how this project converts the audio samples to the spectrogram features. Or at least it does not convert the values like the conversion is done during the ML training process. ", "Hello @petewarden, @advaitjain\r\n\r\nCould you please comment my previous finding in above comment. I think you have the best knowhow how this project works and it might be very important finding to get this project working.\r\nSo basically I am saying that this project does create spectrogram in a similar way as it is done during machine learning training process and thus the audio recognition does not work. Or it might also be possible that data is not feed into micro_feature_generator like it is done during in ML training process which cause a diffence in spectrogram. I managed to verify this like I wrote above.\r\n\r\nThis part of the code is in my opinion suspicious:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/micro_features_generator.cc \r\nfrontend_input = input + 160;\r\nI don't understand why in micro_features_generator.cc the input data pointer is always forwarded 160. The data buffer contains audio data which is fetched in feature_provider.cc. Why it is necessary to move pointer 160 always? ", "@piisku78 :\r\n> ...\r\n> This part of the code is in my opinion suspicious:\r\n> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/micro_features_generator.cc\r\n> frontend_input = input + 160;\r\n> I don't understand why in micro_features_generator.cc the input data pointer is always forwarded 160. The data buffer contains audio data which is fetched in feature_provider.cc. Why it is necessary to move pointer 160 always?\r\n\r\nAudio samples are overlapping by 10ms (30ms slice for FFT/... every 20ms)\r\n", "Yes but that overlapping is already calculated in audio_provider.cc::GetAudioSamples()\r\nThe input buffer contains correct data. That's why it is not necessary to move pointer again.", "@piisku78 : The micro speech examples definitely have not seen as much love and refactoring yet as most of tensorflow. FrontendProcessSamples() expects a non overlapping stream of audio data and implements yet another audio data buffer to extract 30ms slots every 20ms.", "I tested by using 1KHz sin audio as an input. In this case I expect to see values in spectrogram image around 1KHz and nothing else. But I see totally something else, here are the values of the first line (first 30ms) in spectrogram image.\r\n\r\n0, 0, 0, 0, 0, 0, 0, 0, 128, 163, 200, 215, 253, 232, 252, 206, 182, 78, 0, 0, 0, 0, 0, 0, 0, 0, 95, 120, 117, 0, 0, 0, 0, 0, 0, 102, 141, 0, 0, 0, 0, 159, 170\r\n\r\nIn the ML training process in is possible to choose another audio spectrogram mode, I tested 'average' and got this which looks perfect. There is only 1 freq pin around 1 KHz which have high value.\r\n\r\n0, 0, 0, 0, 0, 0, 92, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\r\n\r\nUnfortunate only the 'micro' spectrogram is implemented for embedded device, so even if the 'average' shows good results in this case it wont help at the moment. One should first find out how 'average' is implemented in Tensorflow and then make it work in embedded device.  I have no idea why google have chosen 'micro' mode only. Maybe micro should work better in speech recognition. ", "Just to check @piisku78  & @oclyke - are you not able to get detection working at all, or is the recognition rate just low? I have it working pretty reliably on the Edge 2. I put a short video up here: https://twitter.com/TinyMLClub/status/1200780099785830402/video/1 \r\n\r\nMy code is up here: https://github.com/robotastic/tensorflow/tree/sparkfun_artemis \r\nIt looks like I am a little behind the Master branch now.\r\n\r\nIf others will be at the AIOT conf next week, maybe we can meet-up and do some real time debugging.", "Hello\r\n\r\nI got my code working too about week ago. But at the same time Sparkfun sent me an Email \r\nsaying that Edge 2 device is cancelled from production and one should use old Edge device, which I dont want to due to problems in mics. So I thought this Branch is not need anymore. Now I am not sure which device should I take. I would like to try Artemis Nano, should the code work on that out-of-the box?\r\n\r\nI have done a custom machine learning model which detects urban sounds. I have also changed how I feed data to model, so my code has many differences compared to original. On Edge 2 I am able to detect sounds from 10 meter distance with pretty good accuracy. In the company where I work, we have a really existing use-case for a product where we need this. \r\n\r\nI would like to visit summits like AIOT but I work in Switzerland, so I have to plan well in advance.", "@piisku78 That is great! Yea, I had noticed the Edge 2 is no longer listed.\r\nRight now there isn't support for the Artemis boards in TensorFlow, out of the box. It is pretty easy to add though. If you look at the changes I made for the Redboard, you can see how you can add support for the different Artemis based boards. I have a Nano too, so happy to work on some code with you to get that working. I do like the PDM mics on those boards.\r\n\r\nAlso - that sounds like a really cool project. I wanted to try and extended this project  ( https://labs.ideo.com/2018/06/15/how-to-build-your-own-laugh-detector/ )  to see if I could get it to run on an MCU. Do you have any code or pointers you could share on how to do sound classification using TinyML? MY email is: luke@robotastic.com", "@piisku78 @robotastic Hey all I've had a few higher priorities on my plate for a bit but I wanted to stop in and see what's up. \r\n\r\n@robotastic I tried your code again with a completely fresh installation (git clone, checkout sparkfun_artemis, make, upload) and for me there seem to be no hangs but I can't get any detection (it won't even register an 'unknown' sound). I need to dig in more to find out why this is happening. Possibly related is an error I got when I built w/o cleaning up everything first: the board continuously spit out an error regarding feature length ```Requested feature_data_ size -1560243888 doesn't match 1960 Feature generation failed``` I took it as just a spurious error when the re-build worked w/o crashing.\r\n\r\n@piisku78 This beanch is for more than just Edge2 support - it is meant to handle all the Artemis boards w/ PDM microphones. Edge2 was just my proof-of-concept since the audio hardware is (very nearly) identical between all the Artemis based boards. All of these boards are options for you - https://www.sparkfun.com/artemis - and some day soon we are planning to make another board that has the camera interface and is based on the Artemis module. \r\n\r\n\r\n\r\n", "@robotastic P.s. the video you posted on Twitter is inspiring! I wonder why I can't seem to get the same results...\r\n\r\nGoing to follow your [directions](https://tinyml.club/getting-micro-speech-to-work-with-artemis) closely and see if I am missing something obvious \r\n\r\nAha! Your 'artemis' branch is meant to use the ```sparkfun_artemis_redboard``` target huh... I was using ```sparkfun_edge2``` out of habit. (I was pretty confused too b/c your video shows the Edge2 and the instructions refer to the LEDs on the Edge2 but the build command lists ```TARGET=sparkfun_redboard_artemis```\r\n\r\nThe good news is that it works very well when I try it on the RedBoard! (thank you and thank goodness) Is Edge2 supported in your branch as well? Should it be working for me if I use the target ```sparkfun_edge2```? \r\n\r\nLet's put the final nail in this one. I will compare branches and try to replicate this behavior on this PR.", "Alright - strange results. The RedBoard Artemis and the RedBoard Artemis ATP work using identical code (at least above the BSP abstraction layer) however the RedBoard Artemis Nano and the Edge2 do not. \r\n\r\nCould it be that the BSP implementations for the Nano and Edge2 are incorrect? I will dive into that tomorrow. Easiest way to test is by running the PDM FFT example sketch. ", "@oclyke Ah! yes, sorry about that. You are totally right, the target has to be set correctly. I will go update that. It should work for Edge 2 & the Redboard. I just went and double checked... and compiled and loaded. The code I was using also works with the Edge 2. Here are the commands I used with my fork/branch:\r\n````\r\ngmake -f tensorflow/lite/experimental/micro/tools/make/Makefile TARGET=sparkfun_edge2 micro_speech_bin\r\npython3 tensorflow/lite/experimental/micro/tools/make/downloads/AmbiqSuite-Rel2.2.0/boards_sfe/common/tools_sfe/ambiq/ambiq_bin2board.py --bin tensorflow/lite/experimental/micro/tools/make/gen/sparkfun_edge2_cortex-m4/bin/micro_speech.bin --load-address-blob 0x20000 --magic-num 0xCB -o main_nonsecure_ota --version 0x0 --load-address-wired 0xC000 -i 6 --options 0x1 -b 115200 -port /dev/cu.wchusbserial1410 -r 2 -v\r\n````\r\nIt is great to hear that things are starting to work though! I didn't make any changes to Edge 2 code though. I wondering things changed in TF? I have updated to master for a while on my fork... ", "Strange behavior persists. Let me explain:\r\n\r\nWhen this PR is merged it should not only update to the 2.2.0 version of the Ambiq SDK but should also support all SparkFun Apollo3-based boards as targets. Those boards are:\r\n\r\nEdge\r\nEdge2\r\nRedBoard Artemis\r\nRedBoard Artemis ATP\r\nRedBaord Artemis Nano\r\nArtemis Thing Plus\r\n\r\nOf those boards the last 5 are based on the Artemis module and the last 4 use **identical** pin connections for UART and PDM (as well as the same PDM channel selection). The Edge does not use PDM and the Edge2 has PDM connections on different pins (and uses either channel of PDM data since there are two mics)\r\n\r\nDue to the high level of similarity between these boards we've structured the ```audio_provider.cc```, ```command_responder.cc``` and ```debug_log.cc``` implementations abstractly - relying on BSP files to take care of any differences. *Thus* the various targets have all been supported by copying *exactly* the RedBoard Artemis implementation (which works).\r\n\r\nWith this procedure the Artemis RedBoard and Artemis RedBoard ATP both work as expected (which means that yes/no are recognized continuously after building + uploading the micro_speech example with ```TARGET``` set to the value for that board). What is odd is that despite identical hardware, abstraction, and identical BSP files the Edge2, Nano, and Thing Plus hang after initialization and never report any commands. \r\n\r\nSince the implementation is so similar I tried uploading the working RedBoard Artemis binaries onto the Nano and Thing Plus. They **worked**. So the hardware is not the problem. And the source code is identical above the ```libam_bsp.a``` level. \r\n\r\nThat places the suspicion on the BSP libraries. However comparison of the BSP files shows no significant differences. Source files only differ in LED / Button definitions (PDM and UART configurations are identical) and the compiled libraries ```libam_bsp.a``` have only 12 bytes difference. BSP libraries were recently regenerated and released on GitHub - a change which is captured in this PR by the bump to v0.0.4 in the BSPs URL.\r\n\r\n\r\n## Testing\r\n\r\nI've taken the liberty to create a containerized development environment to help debug this (by removing differences between host computers and toolchains). To try it out:\r\n\r\n* Install [Docker](https://hub.docker.com/?overlay=onboarding)\r\n* Then [set up the host](https://github.com/sparkfun/artemis_dev_platform#general-information) (*nix concepts shown here but Windows CMD instructions available at link)\r\n``` bash\r\nADP_NAME=artemis_dev_platform\r\ngit clone --recursive https://github.com/sparkfun/artemis_dev_platform $ADP_NAME\r\ncd $ADP_NAME\r\n./ev # set up the dev environment (builds Docker image)\r\n```\r\n* Then start the interactive container with a symbolic link to your working directory:\r\n```docker run -it --mount type=bind,source=\"$(pwd)\",target=/app $ADP_NAME```\r\n* You now have a Bash prompt that exposes the standardized dev environment. You use that to build tensorflow\r\n``` bash\r\nBUILD_TARGET=sparkfun_redboard_artemis\r\nTFLM_REPO=https://github.com/sparkfun/tensorflow\r\nTFLM_BRANCH=master\r\ngit clone --recursive $TFLM_REPO tensorflow\r\ncd tensorflow\r\ngit checkout $TFLM_BRANCH\r\nmake -f tensorflow/lite/experimental/micro/tools/make/Makefile TARGET=$BUILD_TARGET micro_speech_bin\r\n```\r\n(You can change ```BUILD_TARGET```, ```TFLM_REPO```, and ```TFLM_BRANCH``` to try different combinations easily)\r\n* Now your host machine will have the compiled binary for the target. You are responsible for uploading that binary to the board using your host machine (Docker container can't access serial ports). I use: (relies on Python 3 being available)\r\n``` bash\r\nCOM_PORT=COM4\r\nUPLOAD_TARGET=sparkfun_redboard_artemis_cortex-m4\r\ncd $ADP_NAME\r\nAmbiqSuiteSDK/boards_sfe/common/tools_sfe/scripts/upload_bin_asb.sh -f tensorflow/tensorflow/lite/experimental/micro/tools/make/gen/$UPLOAD_TARGET/bin/micro_speech.bin -p COM4\r\n```\r\n\r\nFinally if you have a SEGGER J-Link and are using VSCode you can very easily set up a debug environment (on your host computer) that allows you to find where the Nano, Thing Plus, and Edge2 are failing. (The Nano, at least, is falling into a HardFault in TF code... I'm feeling deja vu)\r\n\r\nUsing the CortexDebug extension for VSCode and ```launch.json``` [Launch Configuration](https://code.visualstudio.com/docs/editor/debugging#_launch-configurations) and the ```apollo3.svd``` file from the [AmbiqMicro DFP Pack](http://s3.ambiqmicro.com/pack/AmbiqMicro.Apollo_DFP.1.1.0.pack)\r\n\r\n```\r\n{\r\n  // Use IntelliSense to learn about possible attributes.\r\n  // Hover to view descriptions of existing attributes.\r\n  // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387\r\n  \"version\": \"0.2.0\",\r\n  \"configurations\": [\r\n      {\r\n          \"type\": \"cortex-debug\",\r\n          \"request\": \"launch\",\r\n          \"name\": \"Cortex Debug\",\r\n          \"cwd\": \"${workspaceRoot}/tensorflow\", // workspaceRoot refers to the VSCode root workspace\r\n          \"executable\": \"${workspaceRoot}/tensorflow/tensorflow/lite/experimental/micro/tools/make/gen/$UPLOAD_TARGET/bin/micro_speech\",\r\n          \"serverpath\": \"C:/Program Files (x86)/SEGGER/JLink_V642/JLinkGDBServerCL.exe\", // This needs to point to your GDB Server - this example shows one for SEGGER J-Link\r\n          \"servertype\": \"jlink\",\r\n          \"device\": \"AMA3B1KK-KBR\",\r\n          \"interface\": \"swd\", // or \"jtag\" - but usually \"swd\"\r\n          \"serialNumber\": \"\", //if you have more than one J-Link probe add the serial number here \r\n          \"runToMain\": true,\r\n          \"svdFile\": \"C:/AmbiqMicro.Apollo_DFP.1.1.0/SVD/apollo3.svd\",\r\n      }\r\n  ]\r\n}\r\n```\r\n\r\n\r\n@robotastic I still havent been able to get my Edge2 working using your branch. I've tried both natively on my mac (I also had to get ```gmake``` to satisfy the version requirements) and within the development environment I explained above. Sorry for the trouble here. If you have a chance can you try the steps above but use the values that correspond to you work? They are:\r\n```\r\nTFLM_REPO=https://github.com/robotastic/tensorflow.git\r\nTFLM_BRANCH=sparkfun_artemis\r\nBUILD_TARGET=sparkfun_edge2\r\nUPLOAD_TARGET=sparkfun_edge2_cortex-m4\r\n```\r\n\r\nIf that works for you I will truly be at a loss. \r\n\r\n**Everyone - thanks for bearing with this. We are very close.**\r\n\r\nP.s. If you just want to run the example on a Nano or Thing Plus you cheat and just upload the binary that was built for the RedBoard Artemis.", "@oclyke :  The Makefiles for lite/micro allow for easy code specialization on targets. \r\nA single apollo3 directory instead of one per board would prevent code duplication and make the code a lot more maintainable.\r\n\r\nPS: micro is moving out of experimental and in flight PRs will need to be adjusted - so maybe wait on the refactoring until after the move?\r\n@oclyke: If you can provide quality caffeine - I can visit in person and help with refactoring and  knocking out the last issues.\r\n", "@suphoff That's a good point - what should be done is for any of the Artemis boards the Makefile should just point at the correct BSP library and a common implementation. \r\n\r\nWhat kind of adjustments would be needed once micro moves out of experimental? Or do you just mean that some paths will change in documentation?\r\n\r\nI'm not a big coffee drinker but I know a few - I'm sure I could fuel a final push and I would appreciate the help. Are you local to Denver/Boulder?", "@oclyke : Yes - I would set the correct paths to the BSPs in:~/tensorflow_transfer/tensorflow/lite/experimental/micro/tools/make/targets/apollo3evb_makefile\r\nand then use TAGS to differentiate the boards.\r\nThe audio provider could use either analog or PDM capture based on existence of PDM defines  and the command responder  could pick up on the number of LEDs. (and similar things for other examples)\r\nAdding #defines based on board tags would allow further customization.\r\n\r\nMoving micro out of experimental was announced by @petewarden last week - and hopefully this will only involved path changes for code and documentation.  (But I don't know)\r\n\r\nI am local ( Fort Collins) and will ping you offline about getting together for a final push.", "As a step toward code deduplication I modified the apollo3 specific makefile so that any 'sparkfun' targets add 'sparkfun_generic' to the ```ALL_TAGS``` variable. This tells the ```specialize``` function to search ```micro/sparkfun_generic``` and ```micro/examples/$EXAMPLE/sparkfun_generic``` for the various implementations. (Edge baord still works because directories that match the target name have a higher priority) This change eases maintenance but hopefully also will make it easier to close the case on the problems we are seeing here. \r\n\r\nI've just done some testing with the new configuration. Here are results:\r\n* Hardcoding the ```BOARD_BSP_PATH``` to point at the RBA BSP (```$(APOLLO3_SDK)/boards_sfe/redboard_artemis/bsp```) results in functional binaries when compiling for any of the 4 boards in question (RBA, ATP, Nano, and Thing Plus). \r\n* After fixing an LED definition error in the Nano BSP and changing the makefile to use the BSP library indicated by the target name the RBA and ATP continue to work but the Nano and Thing Plus fail - they fail in a similar manner (at least visually - they both enable the FPU and light the LED then hang) (\r\n* Cleaning up the audio_provider didn't reveal anything terribly obvious. There was a conflict between methods of accessing LEDs that could have caused some confusion (but not the kind of problems we are seeing) (in the audio_provider the LED at index 0 of the bsp led array was used to indicate PDM initialization, but in the command responder the standard was to use the AM_BSP_GPIO_LED_BLUE macro and GPIO access directly. Future todo: standardize on a name that doesn't imply LED color)\r\n\r\nAFAIK the only dependence on the BSP is from:\r\n- PDM connections / channel (identical except Edge/Edge2)\r\n- UART connections (identical)\r\n- LED definitions\r\n\r\nDe-duplication of code was a nice step to shine some extra light and reduce hiding places for bugs, but I haven't found any real answers yet. ", "@oclyke impressive work! I tried to get the Docker build environment working, but things didn't work so good on a Mac. It looks like `readlink` works a little differently. I tried to hardwire the docker image name, and that built the image but something still wasn't write and the ARM tools were missing. I will try just working with the raw Dockerfile and not use the ev scripts.", "@robotastic Thanks for the heads-up. I had not tested it on Mac. Just tried it at home and made some changes (swapped  ```readlink``` for [```realpath```](https://github.com/mkropat/sh-realpath)) and added a convenient way to completely blast away old docker configurations (use it at your own risk! ```./docker/nuke.sh```)\r\n\r\nWith those changes I was able to follow the testing instructions above on my Mac (10.14.6) \r\n\r\nKeep me posted on how it goes for you - I am quite curious!\r\n\r\nP.s. I also attached my 'gen' dir for RBA and Edge2 based on the latest. \r\n[build_edge2_rba_12_7_19_1_03.zip](https://github.com/tensorflow/tensorflow/files/3934870/build_edge2_rba_12_7_19_1_03.zip)\r\n", "@oclyke  Progress! I got the image built. It looks like you forgot to add `realpath.sh` to the repo. I was able to google a version though. I was going to try building for Edge 2 using the Sparkfun repo. Unfortunately when I try building it with the target set to `sparkfun_edge2` I get the following error. It looks like there is an extra slash in a Makefile. I gotta run errands, but I will see if I can figure out where later.\r\n\r\n````\r\ng++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -fno-rtti -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -I/boards_sfe/common/third_party/lis2dh12/ -c tensorflow/lite/kernels/kernel_util.cc -o tensorflow/lite/experimental/micro/tools/make/gen/sparkfun_edge2_x86_64/obj/tensorflow/lite/kernels/kernel_util.o\r\nmake: *** No rule to make target 'tensorflow/lite/experimental/micro/tools/make/gen/sparkfun_edge2_x86_64/obj//boards_sfe/common/third_party/lis2dh12/lis2dh12_platform_apollo3.o', needed by 'tensorflow/lite/experimental/micro/tools/make/gen/sparkfun_edge2_x86_64/lib/libtensorflow-microlite.a'.  Stop.\r\n````", "Ok - I just realized that you removed Edge 2 as a target. I got it to build on my computer. I am having some trouble getting it load. I am going to try the bin_2_board script instead.", "Sorry about all that - realpath is a submodule so getting it probably requires a recursive submodule init and simply adding 'sparkfun_edge2' in the list of targets at the top of 'apollo3_makefile.inc' allows it to build for Edge2.\r\n\r\nThe upload_bin_asb script basically just calls bin2board in a slightly more convenient way if you have some environment variables set up.", "Soo.... I went and built my repo / branch, flashed it onto the Edge 2, using the tools in tensorflow, with this command (from outside the docker container):\r\n````\r\npython3 tensorflow/lite/experimental/micro/tools/make/downloads/AmbiqSuite-Rel2.2.0/boards_sfe/common/tools_sfe/ambiq/ambiq_bin2board.py --bin tensorflow/lite/experimental/micro/tools/make/gen/sparkfun_edge2_cortex-m4/bin/micro_speech.bin --load-address-blob 0x20000 --magic-num 0xCB -o main_nonsecure_ota --version 0x0 --load-address-wired 0xC000 -i 6 --options 0x1 -b 115200 -port /dev/cu.wchusbserial1410 -r 2 -v\r\n````\r\nAND IT WORKS!\r\nI am using 115200 as the speed.\r\nCould there be different HW revs of the Edge 2? I think this one says: v21 on it. Maybe your board is borked? Can you get the basic arduino samples to work?\r\n\r\n\r\nI think the Edge 2 target was removed from the Sparkfun Fork. It is no longer here: https://github.com/sparkfun/tensorflow/blob/master/tensorflow/lite/experimental/micro/tools/make/targets/apollo3_makefile.inc#L4\r\n\r\nI think it was this commit: https://github.com/tensorflow/tensorflow/pull/31847/commits/d7c70cc7acadadc87ae7e19b003376bb697c899f", "@robotastic Okay that's wild - perhaps my Edge2 is indeed broken. Sadly we don't have any more in stock so I'm not sure if I can get my hands on others to test with. I'm going to need to table this for a little while this week to work on BLE for Artemis but hopefully suphoff and I can sort out the kinks and finally land this PR soon. ", "It looks like there is a race condition in PDM audio. Just adding a few extra cycles before am_pdm0_isr() reads the interrupt status seems to magically make my nano work.\r\nNeeds some quality time with the Apollo3 Specs and the HAL later this week ...\r\n", "I got also Nano Boards and managed to get my project running on it.\r\n\r\nI have also noticed earlier when testing these different devices that there is sometimes necessary to put some waits to get sound detection working. Otherwise the detection result was always the same, with max output score value. At some point i had 100ms wait in the main loop, however I have never understood why this helps. ", "Ah! That could make sense. One of the changes between the Apollo_EVB audio provider and the SparkFun Generic is that the LED initialization has been removed. I was using the older style code from Apollo_EVB with the LED initialization. That might have added enough delay before PDM_init stuff. That is assuming that custom_am_bsp_low_power_init() gets called first...\r\n\r\nAnyhow, great finds!", "All - we've learned a lot in this thread. For a few reasons I will be closing this PR and creating a new one based on our new stance. \r\n\r\n- TFLu has moved beyond 'experimental' so this is a good time to reorganize\r\n- Commit history became large / maybe convoluted\r\n- Not knowing better I made changes on the 'master' branch of the SparkFun fork (made it hard to work on other small PRs simultaneously)\r\n\r\nI will share the link to the renewed effort soon.", "First of several small PRs here:\r\n[Update lite/experimental/micro to Ambiq Apollo3 SDK 2.2.0](https://github.com/tensorflow/tensorflow/pull/35236)\r\n\r\nNext steps:\r\n- upgrade Edge BSP\r\n- add Edge to Arduino libraries\r\n- add Artemis boards"]}, {"number": 31846, "title": "Update lite/micro for latest Ambiq Apollo3 SDK", "body": "Pulling in the latest SDK from Ambiq to support the Apollo3 microcontroller used in the lite/micro examples 'micro_speech' and 'micro_vision'\r\n\r\nThis is regular maintenance that will keep future development efforts relevant.\r\n\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31846) for more info**.\n\n<!-- need_sender_cla -->", "Oops, I thought only the PR email needed to sign CLA. I will re-try after fixing this (likely in a new PR)"]}, {"number": 31845, "title": "Remove unused code", "body": "```\r\nC:/msys64/mingw64/bin/../lib/gcc/x86_64-w64-mingw32/9.2.0/../../../../x86_64-w64-mingw32/bin/ld.exe: common/shape.o:shape.cc:(.text+0x8c6): multiple definition of `tflite::gpu::StrongShape<(tflite::gpu::Layout)7>::LinearIndex(std::array<int, 4ull> const&) const'; cl/kernels/conv_buffer.o:conv_buffer.cc:(.text$_ZNK6tflite3gpu11StrongShapeILNS0_6LayoutE7EE11LinearIndexERKSt5arrayIiLy4EE[_ZNK6tflite3gpu11StrongShapeILNS0_6LayoutE7EE11LinearIndexERKSt5arrayIiLy4EE]+0x0): first defined here\r\n\r\ncollect2.exe: error: ld returned 1 exit status\r\n```\r\nThis code make an error.", "comments": ["The error and the title of the PR seem at odds with each other", "This function is already defined at common/shape.h:463. And mostly works as same as this removed function. In C++, it can be overload but it is possibly duplicate symbol when the template parametres are same at code using this."]}, {"number": 31844, "title": "Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, LOGISTIC, RESHAPE. Here is a list of operators for which you will need custom implementations: TFLite_Detection_PostProcess.", "body": "**System information**\r\n- Windows 10:\r\n- TensorFlow installed using PIP:\r\n- TensorFlow version 1.14:\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, LOGISTIC, RESHAPE. Here is a list of operators for which you will need custom implementations: TFLite_Detection_PostProcess.\r\n\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\nssd_mobilenet_v1_coco\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nUsing following code to convert the graph file to tflite:\r\n\r\nimport tensorflow as tf\r\ngraph_def_file = \"C:/tensorflow1/models/research/object_detection/inference_graph/tflite_graph.pb\"\r\ninput_arrays = [\"normalized_input_image_tensor\"]\r\ninput_shaps = {\"normalized_input_image_tensor\":[1, 300, 300, 3]}\r\noutput_arrays = ['TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3']\r\n#output_shaps = {\"raw_outputs/box_encodings\":[1, 10, 4]}\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays, input_shaps)\r\ntflite_model = converter.convert()\r\nopen(\"detact.tflite\", \"wb\").write(tflite_model)\r\n", "comments": ["The 'TFLite_Detection_PostProcess' is a custom operator that you need explicitly register, otherwise the interpreter can't find the kernel of this op. You can register by setting converter.allow_custom_ops = true.", "> The 'TFLite_Detection_PostProcess' is a custom operator that you need explicitly register, otherwise the interpreter can't find the kernel of this op. You can register by setting converter.allow_custom_ops = true.\r\n\r\nThanks it worked for me.", "I am closing this issue since it looks to be fixed. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31844\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31844\">No</a>\n"]}, {"number": 31843, "title": "Attempt to build with NDK only in configure.py results in invalid android.bzl", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 (docker)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 1.14.0\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: pip yes\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): Clang from NDK r17b\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n\r\n\r\n**Describe the problem**\r\nI'm attempting to build tensorflow light using android NDK compiler, without plugging in SDK.\r\nTherefore I set `TF_SET_ANDROID_WORKSPACE` to 0, while I have `ANDROID_NDK_HOME` pointing to location of valid NDK installation.\r\n\r\nAttempting to perform such build results in error:\r\n```\r\nERROR: /root/.cache/bazel/_bazel_root/9deba62a2f90185d207a955a930b77dc/external/local_config_android/android.bzl:10:2: indentation error\r\nERROR: error loading package '': Extension 'android.bzl' has errors\r\nERROR: error loading package '': Extension 'android.bzl' has errors\r\n```\r\n\r\nThis happens because configure.py adds ndk location (specified in env variable) by appending to android.bzl that has only pass in body of function.\r\nBut it is added with 2 spaces, while rest of android.bzl was with 4 tabs.\r\n\r\nThis is the first problem, I have to manually remove `pass` and leave only NDK setup\r\nProbably there should not be such difference in tabulation when generating `android.bzl`\r\n\r\nThe second issue, is whether it should be possible to build `//tensorflow/lite:libtensorflowlite.so` with NDK only.\r\nI was able to do so after manually correcting `android.bzl`\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n- Environment variables:\r\n\r\n```\r\nPYTHON_BIN_PATH = <valid location python>\r\nUSE_DEFAULT_PYTHON_LIB_PATH = '1'\r\nTF_ENABLE_XLA = '0'\r\nTF_NEED_ROCM = '0'\r\nTF_NEED_CUDA = '0'\r\nTF_NEED_MPI = '0'\r\nTF_DOWNLOAD_CLANG = '0'\r\nTF_SET_ANDROID_WORKSPACE = '0'\r\nTF_CONFIGURE_IOS = '0'\r\nANDROID_NDK_HOME = <valid location of NDK>\r\nANDROID_NDK_API_LEVEL = '18'\r\nTF_NEED_OPENCL_SYCL = '1'\r\nTF_NEED_COMPUTECPP = '0'\r\nTRISYCL_INCLUDE_DIR = '<path to cloned repo of TrySYCL>/include'\r\nHOST_CXX_COMPILER = '<NDK path>/toolchains/llvm/prebuilt/linux-x86_64/bin/clang++'\r\nHOST_C_COMPILER = '<NDK path>/toolchains/llvm/prebuilt/linux-x86_64/bin/clang'\r\nCC_OPT_FLAGS = '-Wno-sign-compare'\r\n```\r\n\r\n- Run `python configure.py`\r\n- Rim `bazel shutdown`\r\n- Run build with `bazel build --config=opt --config=v2 --cxxopt='--std=c++11' --define=no_tensorflow_py_deps=true --config=android_arm64 //tensorflow/lite:libtensorflowlite.so --verbose_failures`\r\n\r\n**Any other info / logs**\r\nManual correction of `android.bzl` results in successful build, but it seems there are no symbols related to GPU delegate (I guess a separate target is required?)\r\n", "comments": ["Feel free to propose a fix that allows setting the SDK as optional.", "There is something unclear to me currently.\r\n\r\nIs it bazel, who requires SDK in addition to NDK in order to build android related projects, or tensorflow?", "I suspect you'll need to modify something in this file: https://github.com/tensorflow/tensorflow/blob/master/third_party/android/android_configure.bzl", "Ok, thx, I'll test various approaches and will be back with PR", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31843\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31843\">No</a>\n"]}, {"number": 31842, "title": "Tensorflow2.0 Training OOM when tf.function retraces excessively", "body": "<em>When I am training my model using **tf.function** CPU memory is leaking and after some steps of training it is getting killed.</em>\r\n\r\n**System information**\r\n- OS Platform: - Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.0-beta\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.01\r\n- GPU model and memory: p100, 16GB\r\n\r\n```\r\n@tf.function\r\ndef train_step(self, x, step, grad_clip=True, clip_value=1.0):\r\n    with tf.name_scope(\"input_data\"):\r\n        inputs = x[:, :-1]\r\n        targets = x[:, 1:]\r\n\r\n     with tf.GradientTape() as tape:\r\n          predictions, _ = model(inputs, training=True)\r\n          loss = self.get_loss(targets, predictions)\r\n\r\n      with tf.name_scope(\"gradients\"):\r\n          gradients = tape.gradient(loss, self.trainable_variables)\r\n          if grad_clip:\r\n              gradients = [(tf.clip_by_value(grad, -clip_value, clip_value))\r\n                             for grad in gradients]\r\n          self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\r\n\r\n        accuracy = self.get_padded_accuracy(targets, predictions)\r\n        assert self.train_writer is not None\r\n        with tf.name_scope(\"summary_writer\"):\r\n            with self.train_writer.as_default():\r\n                tf.summary.scalar(\"loss\", loss, step=step)\r\n                tf.summary.scalar(\"accuracy\", accuracy, step=step)\r\n       return loss, accuracy\r\n\r\n**Stats of CPU memory\r\nAfter 100 steps - 5.08 GB\r\nAfter 500 steps -  14.8 GB\r\nAfter 1000 steps - 27 GB**\r\n```\r\n        \r\n\r\n\r\n**When is graph mode of training using tf.function CPU memory gets leaked and after some steps, it gets killed but when I train my model using eager mode it works fine, I am using above code for graph mode training as recommended by TensorFlow 2.0 community.**\r\n\r\n", "comments": ["Hi @akanyaani, is the shape of `x` varies between each batch?", "Yes, The shape of x varies between each batch.", "That's the reason why! Because `tf.function` will re-trace the graph due to the variable sequence lengths or variable batch sizes, the number of cached graphs will increase over time if it do not see this shape before. Thus OOM occurred. To avoid re-tracing, you might want to specify the `input_signature`. For detailed explanation, please refer to\r\nhttps://www.tensorflow.org/beta/tutorials/eager/tf_function\r\nhttps://www.tensorflow.org/beta/tutorials/text/transformer (search `tf.function`)", "Thanks for your prompt response.\r\nBut for that I was calling this method for prevent tensorflow to create new graph tf.compat.v1.get_default_graph().finalize() \r\n\r\nBut let me try with the input signature\r\n", "It tried with `@tf.function(input_signature=[tf.TensorSpec(shape=(None, None), dtype=tf.int32)])` with variable sequence length but still, it is happening, but **when I run with a fix sequence length it works fine** but it will cost extra computation. ", "@akanyaani ,\r\nCan you please provide complete code snippet to reproduce the issue reported here.Thanks!", "focus on", "Hi,\r\n\r\nMy model has transformer-based encoder-decoder as explained here.\r\n[https://www.tensorflow.org/beta/tutorials/text/transformer](url)\r\n\r\n", "Hi @akanyaani , Could you provide the complete code (ideally minimized) that we can run and reproduce?  Thanks. ", "Also, if you have been passing Python arguments, you can try passing Tensors instead https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args\r\n", "Hi, @kkimdev\r\nYou can produce the error using this repo by making dynamic batch padding.\r\n`padded_shapes=([-1], [-1])\r\n`\r\nhttps://github.com/akanyaani/gpt-2-tensorflow2.0", "@akanyaani, it looks like the general suspicion is that subsequent calls to `train_step` cause the function to be re-traced (a slow process that should only happen once or twice), likely due to the parameter configuration. For example, it could be because the shape of `x` varies, but it could also depend on the value of `step`.\r\n\r\nI'm not sure I can completely follow your instructions for reproducing it, but here is a way to diagnose this issue: add a `print` call to train_step, like so:\r\n\r\n```\r\ndef train_step(self, x, step, grad_clip=True, clip_value=1.0):\r\n    print('tracing', x, step, grad_clip, clip_value)\r\n    with tf.name_scope(\"input_data\"):\r\n      ...\r\n```\r\n\r\nSince the pure Python functions like `print` are only called during tracing, it's an effective way to tell whether the function is re-traced too much. If it does, it also helps explain why.\r\n\r\nWe've just updated the API docs as well with a bit more relevant information, it's worth checking out here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/eager/def_function.py#L989", "Hi @mdanatg, Sorry for my late response, Actually I tried with one parameter also but the same thing was happening. So I found that the sequence length of x was the culprit. It was working fine with fix sequence length. And I have noticed that if you restart training with different batch size same problem happens. \r\n```\r\ndef train_step(self, x):\r\n    with tf.name_scope(\"input_data\"):\r\n```", "Thanks! Just to double-check, you are still seeing the leak if you try:\r\n\r\n```\r\n@tf.function(input_signature=[tf.TensorSpec(shape=(None, None), dtype=tf.int32)])\r\ndef train_step(self, x):\r\n    print('tracing')\r\n    with tf.name_scope(\"input_data\"):\r\n```\r\n\r\nAnd you see just one \"tracing\" message at the console?", "I met the same problem. Did you solve the same problem?", "Hi @zxk19981227,\r\n\r\nCan you give me some idea about your model architecture, then I will be able to give you the answer.\r\n\r\nThanks", "@mdanatg I have use \u201cprint\u201d in train_step(), it shows that each epoch has a \"print\" work. and my code killed because of OOM.\r\n\r\nare there some solutions for it?\r\n\r\nthank you in advance.", "I'm sorry  to hear that. But I solved my problems by turning off auto\ntrace. Maybe your problem is not the 'print' but the auto trace. You can\ntry this.\nI hope it helps\n\nanna <notifications@github.com> \u4e8e2020\u5e743\u67088\u65e5\u5468\u65e5 \u4e0b\u53486:30\u5199\u9053\uff1a\n\n> @mdanatg <https://github.com/mdanatg> I have use \u201cprint\u201d in train_step(),\n> it shows that each epoch has a \"print\" work. and my code killed because of\n> OOM.\n>\n> are there some solutions for it?\n>\n> thank you in advance.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/31842?email_source=notifications&email_token=AJTMZW4BHJKNCR5OCKDNWXDRGNXVLA5CNFSM4IOHPLUKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEOESGFY#issuecomment-596189975>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AJTMZW3KTZJJQKQFWOLYSDTRGNXVLANCNFSM4IOHPLUA>\n> .\n>\n", "@akanyaani ,\r\nCould you please update on the above comment, let us know if the issue still exist.\r\n", "@akanyaani ,\r\nCould you please update on the above comment", "@akanyaani ,\r\nAutomatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31842\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31842\">No</a>\n", "> That's the reason why! Because `tf.function` will re-trace the graph due to the variable sequence lengths or variable batch sizes, the number of cached graphs will increase over time if it do not see this shape before. Thus OOM occurred. To avoid re-tracing, you might want to specify the `input_signature`. For detailed explanation, please refer to\r\n> https://www.tensorflow.org/beta/tutorials/eager/tf_function\r\n> https://www.tensorflow.org/beta/tutorials/text/transformer (search `tf.function`)\r\n\r\nGreat, it helps. Could you please add a warning in this function to alert users?\r\n"]}, {"number": 31841, "title": "Add skip_mismatch argument to load_weights.", "body": "This PR is analogous to https://github.com/keras-team/keras/pull/8462 in Keras.\r\n\r\nWith it, you can load weights into models that have a mismatch, but use the same names. A practical application would be for transfer learning, where the number of classes influences the size of the weights. It can still be useful to load the weights that are not changed in shape to provide a better starting point.\r\n\r\nNote: I don't have a TPU, I am relying on the unit tests to tell me if it is correctly implemented or not.", "comments": ["@hgaiser Did you get a chance to look on reviewer comments? Please let us know on the update. Thanks!", "Hi, I'm currently on vacation but I will look at it in two weeks.\n\nOn Tue, 27 Aug 2019, 11:30 gbaned, <notifications@github.com> wrote:\n\n> @hgaiser <https://github.com/hgaiser> Did you get a chance to look on\n> reviewer comments? Please let us know on the update. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/31841?email_source=notifications&email_token=AAFO22SMEMDFCDDZ46MLMKTQGT625A5CNFSM4IOHOM5KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD5HILCI#issuecomment-525239689>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAFO22UDGDSVM7IJXZR2YRDQGT625ANCNFSM4IOHOM5A>\n> .\n>\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@tensorflowbutler I will pick it up soon, hopefully.", "@hgaiser Could you please check build failures and resolve the conflicts? Thanks!", "> @hgaiser Could you please check build failures and resolve the conflicts? Thanks!\r\n\r\nI believe this was made redundant by https://github.com/tensorflow/tensorflow/commit/c52ee536c045186476450cf76461d47fb2667e7d#diff-4ee308ea180d49ae81691348531a2b6d . Closing this."]}, {"number": 31840, "title": "TFLite benchmark tool doesn't show complete Op profiling when using use_nnapi=true flag", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: All devices\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): Release 1.14.0\r\n- Python version: 2.7.15+\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\n\r\n**Describe the current behavior**\r\nI am testing the benchmark tool found on this link: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark\r\nI am building and running in Android to benchmark MobileNet V1 model on some devices. I am interested in seeing the profiling information for each of the layers/operations and I am able to do that by adding the enable_op_profiling=true flag. However, I can't see the information when I also add the flag \"--use_nnapi=true\".\r\n\r\nExample with NNAPI switched off:\r\n![image](https://user-images.githubusercontent.com/8417115/63438560-2ab00400-c435-11e9-9c11-cb518270a54e.png)\r\n\r\n\r\nExample with NNAPI switched on:\r\n![image](https://user-images.githubusercontent.com/8417115/63438606-41eef180-c435-11e9-9299-08dee2ad6f9f.png)\r\n\r\n**Describe the expected behavior**\r\nI expect that using the --use_nnapi=true flag will also display op profiling information\r\n\r\n**Code to reproduce the issue**\r\nJust enable --enable_op_profiling=true and --use_nnapi=true flags when running the benchmark on any model. Is it not implemented yet?\r\n", "comments": ["This information is not yet exposed from NNAPI, sadly, and won't be available in the near future.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31840\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31840\">No</a>\n", "similar issue for me\r\nafter following the instructions on the tflite benchmarking tool for desktop the tool worked but didn't show any additional information when using the --enable_op_profiling=true argument\r\nOS: linux 19.04, built benchmarking tool with bazel 0.21 from latest tensorflow ", "@ottovonzastrow: The TF benchmarking tool is separate from the TFLite version. There have been some features added to the TFLite tool that aren't available in the TF version, particularly around op/layer profiling."]}, {"number": 31839, "title": "TF 1.14 RNN Variables does not appear in variables()", "body": "Hi,\r\n\r\nI have a model that inherits ``tf.keras.Model`` (M) that contains a custom multi-layered RNN that inherits ``tf.keras.layers.Layer`` (R). I use the eager mode, so I call the ``build`` function of M before I start training. Inside the ``build`` function of M there are 2 variables that gets initialized. After initializing those variables, M's ``build`` function also calls the ``build`` function of the RNN cells inside R.\r\n\r\nThe problem is, while I was using Tensorflow 1.13.1 all of the variables appeared in the ``M.variables`` list but after I've updated to Tensorflow 1.14 RNN variables are no longer in the list!\r\n\r\nAm I missing something or is there a bug?\r\n\r\n- I am using Python 3.6\r\n- Code runs on CPU\r\n\r\nKind regards", "comments": ["@ceteke Could you provide the code snippet?", "First of all, thank you for your response.\r\n\r\nThis is the build function of M\r\n\r\n```python\r\ndef build(self, input_shape):\r\n    # Build rnn\r\n    self.rnn.build(input_shape)\r\n    # Fully connected layer parameters\r\n    self.kernel = tf.Variable(tf.random_normal(shape=[self.hidden_size, self.horizon]), name='kernel')\r\n    self.bias = tf.Variable(tf.random_normal(shape=[self.horizon]), name='bias')\r\n\r\n    # Set is built variable\r\n    self.built = True\r\n    self.saver = tf.contrib.eager.Saver(self.variables)\r\n```\r\n\r\nBuild function of the RNN which inherits ```tf.keras.layers.Layer```.\r\n\r\n```python\r\ndef build(self, input_shape):\r\n    self.multi_rnn_cell.build(input_shape)\r\n```\r\n\r\nBuild function of the Multi RNN Cell which inherits ```tf.nn.rnn_cell.MultiRNNCell```.\r\n\r\n```python\r\ndef build(self, input_shape):\r\n    for c in self._cells:\r\n        c.build(input_shape)\r\n```\r\n\r\nBuld function of a single RNN cell which inherits ```tf.nn.rnn_cell.RNNCell```\r\n\r\n```python\r\ndef build(self, input_shape):\r\n    self._cell.build(input_shape)\r\n```", "Hi @ceteke, is there any runnable example/script? I also run into the same problem, so I want to check if our codes share some fragments. Thanks!!!", "@ceteke Can you please provide a github gist of the issue. Thanks!", "Closing this issue as it has been inactive for more than 20 days. Please add additional comments and we can open the issue again"]}, {"number": 31838, "title": "Simple Audio Recognition sample couldn't be run in iOS swift - TFLite", "body": "I have successfully run the simple audio recognition sample from the examples folder and successfully generated the tensorflow model output_graph.pb. Then i successfully converted the model to TFLite using the following code.\r\n```\r\nimport tensorflow as tf\r\n\r\nfrozen_graph = '/tmp/graph.pb'\r\n\r\ninput_arrays = [\"decoded_sample_data\"]\r\noutput_arrays = [\"labels_softmax\"]\r\nsupported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                       tf.lite.OpsSet.SELECT_TF_OPS]\r\n\r\nconvertor = tf.lite.TFLiteConverter\r\nconvertor = convertor.from_frozen_graph(graph_def_file=frozen_graph,input_arrays=input_arrays,output_arrays=output_arrays)\r\nconvertor.target_ops = supported_ops\r\n\r\ntflite_model = convertor.convert()\r\nopen(\"converted.tflite\", \"wb\").write(tflite_model)\r\n```\r\n\r\nI have a sample iOS app and i am using the Firebase ML kit's MLModelInterpreter to run the model. I successfully complied and ran the project but at run time during the run i am get the error that \"Unsupported ops\". If the Decode_wave, Audiospectrogram and MFCC ops are not supported then how can the conversion be successfully? How can i run the tflite model in the iOS swift project successfully.\r\n\r\nFor iOS implementation refer- https://firebase.google.com/docs/ml-kit/ios/use-custom-models\r\nOS - OSX - Mojave - 10.14.5\r\nTensorflow version - 1.14.0 stable \r\nEnvironment - Conda\r\nSwift version - 4.2 and 5\r\nXcode - 10.2\r\n\r\n[PB and Lite graphs.zip](https://github.com/tensorflow/tensorflow/files/3525438/PB.and.Lite.graphs.zip)\r\n", "comments": ["Can you explain how to get the value of input_arrays and output_arrays?", "> Can you explain how to get the value of input_arrays and output_arrays?\r\n\r\nI use the tool called \"Netron\" GUI interface. This tool allows you to view the layers, their names, the input and output shapes and datatypes all in a graphical flow chart. https://github.com/lutzroeder/netron", "@KauthamMurugan \r\nIs this still an issue", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31838\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31838\">No</a>\n"]}, {"number": 31837, "title": "KeyError: 'BatchMatMulV2'", "body": "KeyError: 'BatchMatMulV2'\r\n\r\ntrain and save model in tf 1.14 environment,\r\nbut get  --- KeyError: 'BatchMatMulV2'  ---- in tf 1.13 environment,\r\nhow to solve this problem in tf 1.13 environment", "comments": ["wait for help", "In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "when i use below method , load the saved model,  it throw  KeyError: 'BatchMatMulV2' ---- in tf 1.13 environment,  (problem is here: saver = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=clear_devices))\r\n\r\n```\r\ndef load_model(model_folder):\r\n    # We retrieve our checkpoint fullpath\r\n    try:\r\n        checkpoint = tf.train.get_checkpoint_state(model_folder)\r\n        input_checkpoint = checkpoint.model_checkpoint_path\r\n        print(\"[INFO] input_checkpoint:\", input_checkpoint)\r\n    except Exception as e:\r\n        input_checkpoint = model_folder\r\n        print(\"[INFO] Model folder\", model_folder, repr(e))\r\n\r\n    # We clear devices to allow TensorFlow to control on which device it will load operations\r\n    clear_devices = True\r\n    tf.reset_default_graph()\r\n    # We import the meta graph and retrieve a Saver\r\n    saver = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=clear_devices)\r\n\r\n    # We retrieve the protobuf graph definition\r\n\r\n    graph = tf.get_default_graph()\r\n    input_graph_def = graph.as_graph_def()\r\n\r\n    # We start a session and restore the graph weights\r\n    sess = tf.Session()\r\n    saver.restore(sess, input_checkpoint)\r\n\r\n    return sess\r\n```\r\n\r\nbut it work in this method ( i think problem is loading the graph  )\r\n\r\n```\r\ndef get_assignment_map_from_checkpoint(tvars, init_checkpoint):\r\n    \"\"\"Compute the union of the current variables and checkpoint variables.\"\"\"\r\n    assignment_map = {}\r\n    initialized_variable_names = {}\r\n\r\n    name_to_variable = collections.OrderedDict()\r\n    for var in tvars:\r\n        name = var.name\r\n        m = re.match(\"^(.*):\\\\d+$\", name)\r\n        if m is not None:\r\n            name = m.group(1)\r\n        name_to_variable[name] = var\r\n\r\n    init_vars = tf.train.list_variables(init_checkpoint)\r\n\r\n    assignment_map = collections.OrderedDict()\r\n    for x in init_vars:\r\n        (name, var) = (x[0], x[1])\r\n        if name not in name_to_variable:\r\n            continue\r\n        assignment_map[name] = name\r\n        initialized_variable_names[name] = 1\r\n        initialized_variable_names[name + \":0\"] = 1\r\n\r\n    return (assignment_map, initialized_variable_names)\r\n```\r\n\r\n\r\nfinally , i solve the problem by init graph which designed before, \r\n\r\ni also want to know how to solve the problem ,because  init graph is not elegant enough", "addition:\r\n\r\nit  works well in tf 1.14 environment( train and save model in tf 1.14,  can get the graph from meta file )\r\nin tf 1.13 environment, it can not get the graph from meta file, only init graph works.\r\n\r\nthanks", "@YC-wind, can it be possible to share the complete code to reproduce the reported issue. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31837\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31837\">No</a>\n", "I am facing the same issue while trying to convert .meta to pb file in tensorflow version 1.13.1. \r\n\r\nThis is the error:\r\nFile \"/home/taif/.local/lib/python3.7/site-packages/tensorflow/python/framework/importer.py\", line 159, in _RemoveDefaultAttrs\r\n    op_def = op_dict[node.op]\r\n\r\nKeyError: 'BatchMatMulV2'\r\n\r\nThe code:\r\n```\r\nimport tensorflow as tf\r\n\r\nmeta_path = 'fns.ckpt.meta' # Your .meta file\r\noutput_node_names = ['add_37']    # Output nodes\r\n\r\nwith tf.Session() as sess:\r\n\r\n    # Restore the graph\r\n    saver = tf.train.import_meta_graph(meta_path)\r\n\r\n    # Load weights\r\n    saver.restore(sess,\"checkpoints/fns.ckpt\")\r\n\r\n    # Freeze the graph\r\n    frozen_graph_def = tf.graph_util.convert_variables_to_constants(\r\n        sess,\r\n        sess.graph_def,\r\n        output_node_names)\r\n\r\n    # Save the frozen graph\r\n    with open('output_graph.pb', 'wb') as f:\r\n      f.write(frozen_graph_def.SerializeToString())\r\n```\r\n\r\n", "@taiftahmid,\r\nCan you please post a new issue by providing the information asked by the [template](https://github.com/tensorflow/tensorflow/issues/new/choose)?\r\nThe reason for this is we can focus on your specific configuration and problem since the root cause can be unrelated even though the error messages are similar. Thanks!", "I came across the same problem with tf 1.13. \r\nIt solved by tf 1.14."]}, {"number": 31836, "title": "[lite]  cc_library or cc_binary not including libs to build .so", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**so+.out rans ok in adb ,but DllNotFoundException in apk from unity.**\r\n08-22 10:09:17.697 26322 26345 D Unity   : Unable to load library '/data/app/com.abc.cba-ZQdutbR1M-hRhdbPDZ25OA==/lib/arm/libProphet.so', native render plugin support disabled: java.lang.UnsatisfiedLinkError: dlopen failed: cannot locate symbol \"_ZTVN6tflite17MutableOpResolverE\" referenced by \"/data/app/com.abc.cba-ZQdutbR1M-hRhdbPDZ25OA==/lib/arm/libProphet.so\"...\r\n\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):buid\r\ncc_library(\r\n    name = \"Prophet\",\r\n    srcs = [\r\n        \"Prophet.cc\",\r\n        \"minimal.h\",\r\n    ],\r\n    linkopts = tflite_linkopts() + select({\r\n        \"//tensorflow:android\": [\r\n            \"-pie\",  # Android 5.0 and later supports only PIE\r\n            \"-lm\",  # some builtin ops, e.g., tanh, need -lm\r\n        ],\r\n        \"//conditions:default\": [],\r\n    }),\r\n    deps = [\r\n        \"//tensorflow/lite:framework\",\r\n        \"//tensorflow/lite/c:c_api_internal\",\r\n        \"//tensorflow/lite/kernels:builtin_ops\",\r\n    ],\r\n    linkstatic = False,\r\n)\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): linux ubuntu18x64->win10x64 unity-\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:arm7\r\n- TensorFlow installed from (source or binary):source \r\n- TensorFlow version (use command below):14\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):0.26\r\n- GCC/Compiler version (if compiling from source):7.4\r\n- CUDA/cuDNN version: none\r\n- GPU model and memory: none\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nexecute libc++_shared.so+example.so+example.out ok!\r\nexecute unity17+libc.so+libdl.so+ibc++_shared.so+example.so->apk\uff0ccannot find dll.\r\nc++filt shows vtable for tflite::MutableOpResolver\r\n**Describe the expected behavior**\r\napk ok!\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n bazel build --config monolithic --cxxopt=-std=c++11 \\\r\n  --crosstool_top=//external:android/crosstool \\\r\n  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n  --cpu=armeabi-v7a  --verbose_failures\\\r\n  //tensorflow/lite/examples/l17:example(so)\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n`\r\n08-21 21:11:24.373 16862 16884 D Unity   : UnloadTime: 6.400000 ms\r\n08-21 21:11:24.381 16862 16884 D Unity   : UUID: a9c3ed14008066f6 => d958add90437df1f9b9cbdba44b84779\r\n08-21 21:11:24.487 16862 16884 D Unity   : Sensor :        Accelerometer ( 1) ; 0.002396 / 0.00s ; ICM20690 Accelerometer / InvenSense \r\n08-21 21:11:24.512 16862 16884 D Unity   : Choreographer available: Enabling VSYNC timing\r\n08-21 21:11:24.520 16862 16884 D Unity   : Unable to load library '/data/app/com.abc.cba-gPQU27o5p_2Y5PClNwV8OQ==/lib/arm/libProphet.so', native render plugin support disabled: java.lang.UnsatisfiedLinkError: dlopen failed: cannot locate symbol \"_ZTVN6tflite17MutableOpResolverE\" referenced by \"/data/app/com.abc.cba-gPQU27o5p_2Y5PClNwV8OQ==/lib/arm/libProphet.so\"...\r\n08-21 21:11:24.522 16862 16884 E Unity   : Unable to find Prophet\r\n08-21 21:11:24.633 16862 16884 E Unity   : DllNotFoundException: Prophet\r\n08-21 21:11:24.633 16862 16884 E Unity   :   at (wrapper managed-to-native) rlsdk.RLSDKAPI:init (byte[],int)\r\n08-21 21:11:24.633 16862 16884 E Unity   :   at helloworld.Start () [0x00017] in D:\\tflite\\0821\\L17online\\Assets\\helloworld.cs:18 \r\n08-21 21:11:24.633 16862 16884 E Unity   :  \r\n08-21 21:11:24.633 16862 16884 E Unity   : (Filename: D Line: 0)\r\n`", "comments": ["Hi @laohur. You cannot treat the .so output from a cc_library as a self-contained shared library. You need to put all of the deps into a cc_binary. See also [this build rule](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/BUILD#L454) and what it expands to.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31836\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31836\">No</a>\n", "finally, BUILD target cc_binrary() instead"]}, {"number": 31835, "title": "Don't know what to do next,help me please!", "body": "**System information**\r\n- OS Platform and Distribution:windows10\r\n- TensorFlow installed from (source or binary):\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\n- TensorFlow version:r1.13\r\n- Python version:3.6\r\n- Installed using virtualenv? pip? conda?:conda\r\n- Bazel version (if compiling from source):0.21.0\r\n- GCC/Compiler version (if compiling from source):9.1.0\r\n- CUDA/cuDNN version:not using\r\n- GPU model and memory:not using\r\n\r\n\r\n\r\n**Describe the problem**\r\nI want to build a dll for c++ ,and build is successful.But I couldn't find .h file anywhere,so I try to run sh build_all_linux.sh in msys as the tutorial said.But it doesn't work at all,and there is noone ask for this. \r\nThen I also try to build by cmake,it doesn't work too.And the only anser is \"We encourage you to use Bazel instead of Cmake as we no longer support cmake for Windows.\r\nAlso please refer this link for more information on Building Tensorflow on Windows.\"\r\nI'm so lost. Can you provid some completely tutorial for people like me.(I have to make a demo with windows)\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n**bazel head file problem:**\r\n$ sh build_all_linux.sh\r\nPROTOC = \"protoc\"\r\nCC_PREFIX = \"\"\r\nrm -rf /c/programing/tensorflow-r1.13/tensorflow/contrib/makefile/gen\r\nrm -rf tensorflow/core/util/version_info.cc\r\ndownloading https://bitbucket.org/eigen/eigen/get/9f48e814419e.tar.gz\r\ntar.exe: Error opening archive: Failed to open '\\\\.\\tape0'\r\n\r\n**cmake problem:**\r\nCMake Error at tf_core_framework.cmake:332 (add_library):\r\nCannot find source file:\r\n\r\nE:/work_space/projectcpp/tensorflow/tensorflow/tensorflow/core/util/version_info.cc\r\n\r\nTried extensions .c .C .c++ .cc .cpp .cxx .cu .m .M .mm .h .hh .h++ .hm\r\n       .hpp .hxx .in .txx\r\n      Call Stack (most recent call first):\r\n       CMakeLists.txt:587 (include)\r\n", "comments": ["Can you try building TensorFlow from source using Bazel since support for cmake is no longer there. You can follow the steps mentioned in [TensorFlow website](https://www.tensorflow.org/install/source_windows) for reference. Let us know you are stuck anywhere. Also, see the below link if it helps you.Thanks!\r\nhttps://gist.github.com/meteorcloudy/8e5f1aab7c7fa87b16ae28e6f8fd3fd2", "> Can you try building TensorFlow from source using Bazel since support for cmake is no longer there. You can follow the steps mentioned in [TensorFlow website](https://www.tensorflow.org/install/source_windows) for reference. Let us know you are stuck anywhere. Also, see the below link if it helps you.Thanks!\r\n> https://gist.github.com/meteorcloudy/8e5f1aab7c7fa87b16ae28e6f8fd3fd2\r\n\r\nThanks for answering my question!No offence, but you give me a python lib build tutorial. I'm asking for a c++ lib build tutorial.\r\nAs I say,I tried to use bazel to build c++lib, and it's works. But after that I can't find  any \u201c*.h\u201d files in build fold.\r\nShould I just try to copy \"*.h\" from source code by shell, and build my own \r\nAnd there are too many .lib files in build fold, which should be linked to my project.\r\n\r\nHere is my command line\r\nbazel build --config=opt --copt=-nvcc_options=disable-warnings //tensorflow/tools/lib_package:libtensorflow\r\n\r\nthank you again", "I'm sorry, but the GitHub issue tracker is not a support forum. If you are having trouble building TensorFlow, consider asking a question at Stack Overflow instead.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31835\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31835\">No</a>\n"]}, {"number": 31834, "title": "bugfix: use experimental_ref in ExponentialMovingAverage instead", "body": "Fix #31582", "comments": ["The bug has been fixed by dc3534c6 "]}, {"number": 31833, "title": "Add Measure inference Time", "body": "Hi, everyone.\r\n\r\nwhen i used slim example, i wanted to measure inference time. \r\n\r\nso, i added this script.\r\n\r\nthank you! ", "comments": []}, {"number": 31832, "title": "[TF 1.14] [TPU] Errors while training LSTM model on Colab TPU", "body": "I worked a bit with TF2.0 but new to TF1.x working. I want o use Google TPUs and for that I am making a LSTM model in TF1.14 without eager execution. I am reusing code from one of the Tensorflow [tutorials](https://www.tensorflow.org/beta/tutorials/load_data/text#split_the_dataset_into_text_and_train_batches). Below is the code I am executing in Colab while setting the runtime environment to **Python 2** and **TPU**.\r\nOn running it in colab, I am getting error `KeyError: u'flat_filenames'`\r\nand if I run it again (without restarting runtime) it throws following error `InvalidArgumentError: Graph is invalid, contains a cycle with 1 nodes, including: bidirectional/while/Merge, bidirectional/while/Merge_1, bidirectional/while/Merge_2`\r\nTHANK YOU!\r\n\r\n````\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\nimport os\r\nimport pprint\r\nimport tensorflow as tf\r\nimport time\r\nimport numpy as np\r\nfrom tensorflow import keras\r\n\r\nTPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\r\n\r\nDIRECTORY_URL = 'https://storage.googleapis.com/download.tensorflow.org/data/illiad/'\r\nFILE_NAMES = ['cowper.txt', 'derby.txt', 'butler.txt']\r\n\r\nfor name in FILE_NAMES:\r\n  text_dir = tf.keras.utils.get_file(name, origin=DIRECTORY_URL+name)\r\n  \r\nparent_dir = os.path.dirname(text_dir)\r\n\r\nprint (parent_dir)\r\ndef labeler(example, index):\r\n  return example, tf.cast(index, tf.int32)  \r\n\r\nlabeled_data_sets = []\r\n\r\nfor i, file_name in enumerate(FILE_NAMES):\r\n  lines_dataset = tf.data.TextLineDataset(os.path.join(parent_dir, file_name))\r\n  labeled_dataset = lines_dataset.map(lambda ex: labeler(ex, i))\r\n  labeled_data_sets.append(labeled_dataset)\r\n\r\nBUFFER_SIZE = 50000\r\nBATCH_SIZE = 64\r\nTAKE_SIZE = 5000\r\nall_labeled_data = labeled_data_sets[0]\r\nfor labeled_dataset in labeled_data_sets[1:]:\r\n  all_labeled_data = all_labeled_data.concatenate(labeled_dataset)\r\n  \r\nall_labeled_data = all_labeled_data.shuffle(\r\n    BUFFER_SIZE, reshuffle_each_iteration=False)\r\nprint type(all_labeled_data)\r\ntokenizer = tfds.features.text.Tokenizer()\r\n\r\nvocabulary_set = set()\r\nmy_iterator = all_labeled_data.make_initializable_iterator()\r\ntext_tensor, _ = my_iterator.get_next()\r\nwith tf.Session() as sess1: \r\n  sess1.run(my_iterator.initializer)\r\n  try:\r\n    while True:\r\n      text_string = sess1.run(text_tensor)\r\n      #print text_string\r\n      some_tokens = tokenizer.tokenize(text_string)\r\n      vocabulary_set.update(some_tokens)\r\n  except tf.errors.OutOfRangeError:\r\n    pass\r\nvocab_size = len(vocabulary_set)\r\nprint 'Length of Vocab',len(vocabulary_set)\r\nprint 'Done Tokenizing!'\r\n\r\nencoder = tfds.features.text.TokenTextEncoder(vocabulary_set)\r\nwith tf.Session() as sess2:\r\n  sess2.run(my_iterator.initializer)\r\n  try:\r\n    while True:\r\n      text_string = sess2.run(text_tensor)\r\n      encoded_example = encoder.encode(text_string)\r\n      #print encoded_example\r\n  except tf.errors.OutOfRangeError:\r\n    pass\r\n\r\ndef encoder_fn(text_tensor, label):\r\n  #print type(text_tensor)\r\n  #print text_tensor\r\n  encoded_text = encoder.encode(text_tensor)\r\n  padded_text = encoded_text[:4]# limiting the length to 4\r\n  padded_text = np.asarray(padded_text, dtype=np.int32)# chnaging type for tensor type compatibility with TF\r\n  return padded_text, label\r\n\r\ndef encode_map_fn(text, label):\r\n  return tf.py_func(encoder_fn, inp = [text, tf.reshape(label, [])], Tout = [tf.int32, tf.int32])\r\n\r\nall_encoded_data= all_labeled_data.map(encode_map_fn)\r\ntrain_data = all_encoded_data.padded_batch(BATCH_SIZE, padded_shapes=([-1],[]), drop_remainder= True)\r\nmy_iterator1 = train_data.make_initializable_iterator()\r\ntext_tensor, lab = my_iterator1.get_next()\r\nwith tf.Session() as sess2:\r\n  sess2.run(my_iterator1.initializer)\r\n  text_string = sess2.run(text_tensor)\r\n  lab = sess2.run(lab)\r\n  print text_string\r\n  print lab\r\nprint 'Printing an example data done!.'\r\n\r\nall_encoded_data = all_encoded_data.repeat()\r\ntrain_data = all_encoded_data.skip(TAKE_SIZE).shuffle(BUFFER_SIZE)\r\ntrain_data = train_data.padded_batch(BATCH_SIZE, padded_shapes=([-1],[]), drop_remainder= True)\r\n#train_data= train_data.batch(64)\r\ntrain_data = train_data.prefetch(BATCH_SIZE)\r\ntest_data = all_encoded_data.take(TAKE_SIZE)\r\ntest_data = test_data.padded_batch(BATCH_SIZE, padded_shapes=([-1],[]), drop_remainder = True)\r\ntest_data = test_data.prefetch(BATCH_SIZE)\r\n\r\n\r\n\r\nvocab_size += 1\r\n\r\ntf.keras.backend.clear_session()\r\nresolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)\r\ntf.contrib.distribute.initialize_tpu_system(resolver)\r\nstrategy = tf.contrib.distribute.TPUStrategy(resolver)\r\n\r\nwith strategy.scope():\r\n  model = tf.keras.Sequential()\r\n  model.add(tf.keras.layers.Embedding(vocab_size, 64))\r\n  model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, activation= 'tanh', recurrent_activation= 'sigmoid', recurrent_dropout = 0, unroll = False, use_bias= True)))\r\n  # One or more dense layers.\r\n  # Edit the list in the `for` line to experiment with layer sizes.\r\n  for units in [64, 64]:\r\n    model.add(tf.keras.layers.Dense(units, activation='relu'))\r\n\r\n  # Output layer. The first argument is the number of labels.\r\n  model.add(tf.keras.layers.Dense(3, activation='softmax'))\r\n  model.compile(optimizer='adam',\r\n                loss='sparse_categorical_crossentropy',\r\n                metrics=['accuracy'])\r\n###Strategy  end###\r\nsteps = 50000/64\r\nprint model.summary()\r\nprint (steps)\r\n\r\nmodel.fit(train_data, steps_per_epoch = steps,epochs=10)\r\n````\r\n", "comments": ["I have tried on colab with TF 1.14 and was able to reproduce the issue.Please, find the [gist ](https://colab.research.google.com/drive/15MX0aH_W8nyU02743Xt7JlyfziwWhUw3)here.I tried in nightly versions and i am getting the below error `ValueError: Cannot use the given session to evaluate tensor: the tensor's graph is different from the session's graph.`Thanks!", "Thank you for confirming the issue. @ravikyram ", "Sorry for the long delay. Please re-open this issue if this is still reproducible on the latest versions of TensorFlow & Cloud TPUs. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31832\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31832\">No</a>\n"]}, {"number": 31831, "title": "Missing `person_detect.tflite` file", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Mint 19.2 Tina\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (or github SHA if from source): 80c04b80ad66bf95aa3f41d72a6bba5e84a99622\r\n\r\nFiles `person_detect_model_data.h/.cc` in `downloads` directory (downloaded from [here](https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_micro_person_data_grayscale.zip)) mention `person_detect.tflite` file:\r\n\r\n```c\r\n// Automatically created from a TensorFlow Lite flatbuffer using a command like:\r\n// xxd -i person_detect.tflite > person_detect_model_data.cc\r\n```\r\n\r\nIs this file available somewhere?\r\nIs the full TF model available?\r\n", "comments": ["@mariusz-r Please elaborate the issue and context. Thanks!", "I would like to experiment more with `micro_vision` example in TF lite micro and therefore I would like to have the full `person_detect` model. Is it available somewhere?", "The instructions for building and running that example are [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/micro/examples/person_detection/README.md).\r\n\r\nAlthough there are some Bazel rules to compile and run tests of the `person_detect` model, it does not appear that those Bazel rules currently work. The rules to generate the missing file `person_detect_model_data.cc` are only present in the `Make`-based build [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/micro/tools/make/Makefile).", "Thanks! This fully answers my question and I think this issue can be closed.", "Glad your issue is resolved. Thanks @frreiss for the quick answer."]}, {"number": 31830, "title": "fit_generator: number of calls to the validation generator does not match validation_steps", "body": "Installed: pip install tf-nightly-2.0-preview \r\nin a clean environment. Same bug was in tf 1.14 and 2.0-beta1\r\n\r\nThe validation generator is called more often than validation_steps in tensorflow.keras.Model.fit_generator(). If one adds workers=0 as a call parameter everything is correct.\r\nI tracked down the issue to starting the threads (parameter max_queue_size changes the number of calls before the calls seem to be used for validation).\r\n\r\nReproduce the bug:\r\n```\r\nfrom tensorflow.keras.layers import Dense, Input\r\nfrom tensorflow.keras.models import Model\r\nimport numpy as np\r\n\r\ntrain_epoch_size = 10\r\ntest_epoch_size = 20\r\n\r\nclass KerasBatchGenerator(object):\r\n    def generate(self, phase='train'):\r\n        while True:\r\n            if phase == 'train':\r\n                for i in range(train_epoch_size):\r\n                    yield [np.array([0])],[np.array([0])]\r\n            else:\r\n                for i in range(test_epoch_size):\r\n                    print('   i',i)\r\n                    yield [np.array([0])],[np.array([0])]\r\n\r\nkeras_gen_train = KerasBatchGenerator()\r\n\r\ninputs = Input(shape=(1,))\r\nx = Dense(1)(inputs)\r\n\r\nmodel = Model(inputs=inputs, outputs=x)\r\n\r\nmodel.compile(loss='mean_squared_error', optimizer='SGD')\r\nprint(model.summary())\r\n\r\nmodel.fit_generator(keras_gen_train.generate(), train_epoch_size, 2,\r\n                           validation_data=keras_gen_train.generate('test'), validation_steps=test_epoch_size)#, workers = 0)\r\n```\r\n\r\nthe output is\r\n ```\r\n1/10 [==>...........................] - ETA: 1s - loss: 0.0000e+00   i 0\r\n   i 1\r\n   i 2\r\n   i 3\r\n   i 4\r\n   i 5\r\n   i 6\r\n   i 7\r\n   i 8\r\n   i 9\r\n   i 10\r\n   i 11\r\n   i 12\r\n   i 13\r\n   i 14\r\n   i 15\r\n   i 16\r\n   i 17\r\n   i 18\r\n   i 19\r\n   i 0\r\n   i 1\r\n   i 2\r\n   i 3\r\n   i 4\r\n   i 5\r\n   i 6\r\n   i 7\r\n   i 8\r\n   i 9\r\n   i 10\r\n10/10 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\r\n```\r\n\r\nThere are 31 calls to the validation generator (validation_steps + max_queue_size + 1). This might usually be no problem, but it is, if you have to control which validation data match which training data. I use it for transfer learning.\r\n\r\nif you uncomment: ,workers=0) you get the expected result:\r\n ```\r\n1/10 [==>...........................] - ETA: 1s - loss: 0.0000e+00   i 0\r\n   i 1\r\n   i 2\r\n   i 3\r\n   i 4\r\n   i 5\r\n   i 6\r\n   i 7\r\n   i 8\r\n   i 9\r\n   i 10\r\n   i 11\r\n   i 12\r\n   i 13\r\n   i 14\r\n   i 15\r\n   i 16\r\n   i 17\r\n   i 18\r\n   i 19\r\n10/10 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\r\n```\r\n\r\nI tracked down the issue to starting the threads in training_generator.py: enqueuer.start\r\n```\r\ndef _make_enqueued_generator(generator,\r\n                             workers=1,\r\n                             use_multiprocessing=False,\r\n                             max_queue_size=10,\r\n                             shuffle=False):\r\n  \"\"\"Create a buffered queue of next elements of the generator.\"\"\"\r\n  is_sequence = isinstance(generator, data_utils.Sequence)\r\n  enqueuer = None\r\n  if workers > 0:\r\n    if is_sequence:\r\n      enqueuer = data_utils.OrderedEnqueuer(\r\n          generator, use_multiprocessing=use_multiprocessing, shuffle=shuffle)\r\n    else:\r\n      enqueuer = data_utils.GeneratorEnqueuer(\r\n          generator, use_multiprocessing=use_multiprocessing)\r\n    enqueuer.start(workers=workers, max_queue_size=max_queue_size) #here the additional calls happen!\r\n    output_generator = enqueuer.get()\r\n  else:\r\n    if is_sequence:\r\n      output_generator = data_utils.iter_sequence_infinite(generator)\r\n    else:\r\n      output_generator = generator\r\n  return output_generator, enqueuer\r\n\r\n```\r\nbut I am too unfamiliar with python threading to supply a patch, sorry. \r\n\r\nAt the moment the workaround is workers=0! ", "comments": ["Was able to reproduce the error. Github gist is [here](https://colab.research.google.com/gist/gowthamkpr/5687a543ffcf0b2d2a72a2a731152063/untitled104.ipynb)", "@dsmic,\r\nI ran the above code with TF v2.1 and got the same outputs with `workers=0` commented and uncommented. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/9ff532a0d1690d0aefd1a2fb69d9f2a9/2-1-template.ipynb).\r\n\r\nCould you please confirm if the issue is resolved? Thanks!", "\r\n> Could you please confirm if the issue is resolved? Thanks!\r\n\r\n@dsmic,\r\nAny updates regarding this issue? Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31830\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31830\">No</a>\n"]}, {"number": 31829, "title": "Using libtensorflow-core.a to enable training on Android ", "body": "I followed : https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/makefile/README.md\r\n\r\nto build the tensorflow library for Android. Does the lib file contain all the functionalities to enable training on Android by using c++ to call the library. \r\n\r\n", "comments": ["This path is effectively deprecated. Is there a reason you cannot use bazel?", "@jdduke I want the whole tensorflow library for android which includes ops for\r\ntraining as well, hence trying this method. The present process of building\r\nfor Android using Android studio or bazel supports only inference, right?\r\n\r\nOn Thu, Aug 22, 2019 at 10:58 PM Jared Duke <notifications@github.com>\r\nwrote:\r\n\r\n> This path is effectively deprecated. Is there a reason you cannot use\r\n> bazel?\r\n>\r\n> \u2014\r\n> You are receiving this because you authored the thread.\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/31829?email_source=notifications&email_token=AM6VNC7UH3HLYTXQZIPLA53QF3EDHA5CNFSM4IODHRRKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD45ZFCY#issuecomment-523997835>,\r\n> or mute the thread\r\n> <https://github.com/notifications/unsubscribe-auth/AM6VNC3O5GLEMQVP5KUNLQDQF3EDHANCNFSM4IODHRRA>\r\n> .\r\n>\r\n", "try ndk14b , makefile only supports ndk14b as there are headers changes in later ndk, but there is huge performance impact using this ndk , i am trying to compile using latest ndk20 .", "Training support with TensorFlow Lite is in active development, @miaout17 can offer additional updates.", "Hi @ghost!  Have you checked the document on  [On-Device training ](https://www.tensorflow.org/lite/examples/on_device_training/overview) from the 2.8 version yet?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31829\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31829\">No</a>\n"]}, {"number": 31828, "title": "Add the MUL reference kernel to Tensorflow Lite Micro", "body": "This PR adds support for the mul reference kernel to Tensorflow Lite Micro. It supports float and uint8. Broadcast support is not included. The testcases included is copied from the testcases in Tensorflow Lite.", "comments": ["@jenselofsson Could you please resolve the conflicts? Thanks!", "I'm working on getting int8 support, so I'll open up a new PR with support for both int8 and uint8 when it's finished."]}, {"number": 31827, "title": "TfLite is going die with serious and ridiculous bug for a long time ...", "body": "https://github.com/tensorflow/tensorflow/issues/31359", "comments": ["This is duplicate issue of #31359 ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31827\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31827\">No</a>\n"]}, {"number": 31825, "title": "KeyError while loading image data", "body": "Error:\r\nFile \"DataCollection.py\", line 38, in caption_image\r\nreturn \"Image (CC BY 2.0) \" + ' - '.join(attributions[str(image_rel)].split(' - ')[:-1])\r\nKeyError: 'sunflowers\\6166888942_7058198713_m.jpg'\r\n\r\nFunction:\r\ndef caption_image(image_path):\r\nimage_rel = pathlib.Path(image_path).relative_to(data_root)\r\nreturn \"Image (CC BY 2.0) \" + ' - '.join(attributions[str(image_rel)].split(' - ')[:-1])\r\n\r\nIt gives a KeyError while returning the parameter.\r\nAnd every time it gives a KeyError with different image name and path.\r\n\r\nAny idea on this?", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Sure,\r\nHere are the details:\r\n\r\nI am following this tutorial - https://www.tensorflow.org/tutorials/load_data/images \r\nto learn about how to train the model using sets of images.\r\n\r\nBut when I started inspecting the images as shown here: https://www.tensorflow.org/tutorials/load_data/images#inspect_the_images\r\n\r\nIt is not working, it gives me the following error:\r\nFile \"DataCollectionError:.py\", line 38, in caption_image\r\nreturn \"Image (CC BY 2.0) \" + ' - '.join(attributions[str(image_rel)].split(' - ')[:-1])\r\nKeyError: 'sunflowers\\6166888942_7058198713_m.jpg'\r\n\r\nIt gives a KeyError while returning the parameter.\r\nAnd every time it gives a KeyError with different image name and path.\r\n\r\nAny idea on this?\r\n\r\nI am using Windows10 and 64-bit architecture.\r\nAnd I am using TensorFlow r1.14.", "@DKWIPL,\r\nI tried executing the colab but i didn't get any error. Just to verify, are you executing the code in colab or jupyter notebook locally on your system. Thanks!", "NO, I am doing it on my Local System.", "And can you also help me with the tutorial to create and train the model using our own image dataset?\r\nIt will be really useful for us if can help us with this.\r\n\r\nThank You.", "> NO, I am doing it on my Local System.\r\n\r\nIf your doing it on local system, please verify the path of the input data. Thanks!", "Hi,\r\n\r\nI checked my data path. It is proper. And as I skipped that step and moved forward, I started getting the following error. \r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Program Files\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Program Files\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"DataCollection.py\", line 3, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Program Files\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Program Files\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n\r\nI don,t know what is going on in this. It was working by Friday and its suddenly stop working on Monday.", "@DKWIPL \r\n`ImportError: DLL load failed: The specified module could not be found.`\r\nLooks like the tensorflow is not installed properly, Can you uninstall tensorflow and python and reinstall both. Thanks! ", "Hey,\r\n\r\nIt worked from me installing and reinstalling again. But what was the issue?\r\nIt was working before and suddenly it stopped working.", "@DKWIPL, Glad it worked. \r\nFrom the error it looks like Tensorflow and its dependencies are not installed properly. \r\nClosing the issue since its resolved. Thanks ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31825\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31825\">No</a>\n", "#The path problem needs to be solved under windows !!!!!!!!!!!!!!!\r\ndef caption_image(image_path):\r\n\timage_rel = pathlib.Path(image_path).relative_to(data_root)\r\n\timage_rel = str(image_rel).replace(\"\\\\\",\"/\")#################look here\uff01\r\n\timage_rel = image_rel + \" \"\r\n\treturn \"Image (CC BY 2.0) \"+ \" - \".join(attributions[image_rel].split(\" - \")[:-1])", "-----------------------------------------------------------\nAttention: Non-Delivery Report\n-----------------------------------------------------------\n\nThis report is generated by the email server at:\n\n       gophygital.io\n\nThe message with subject:\n\n       \"Re: [tensorflow/tensorflow] KeyError while loading image data (#31825)\"\n\nand attached to this report was not delivered to \nthe following recipients:\n\nAddress: ***@***.***\nReason:  User not found (550)\n--------------\n\n"]}]