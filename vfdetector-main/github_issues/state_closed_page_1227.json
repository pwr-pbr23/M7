[{"number": 16353, "title": "Loosen bounds on `losses_impl_test`. fixes #16238", "body": "This should fix issue #16238 , where this test fails on some architectures.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->"]}, {"number": 16352, "title": "OrderedEnqueuer not imported in keras/utils/__init__.py", "body": "OrderedEnqueuer is not imported with the remaining util objects.\r\n\r\nPlease go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@fmfn Could you please provide more information about the issue? The form that you copied in the issue needs to be filled. We need this information to understand the problem that you are reporting.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 16351, "title": "TfLiteCameraDemo failed to work with NNAPI after commit e6ff665dbe4888aa5fdff8f34c44405acca2ddd1", "body": "I am testing NNAPI by forcing TfLiteCameraDemo to invoking libneuralnetworks.so. It worked correctly though slower. But since commit e6ff665dbe4888aa5fdff8f34c44405acca2ddd1, TfLiteCameraDemo crashes with error message like,\r\n\r\n\t01-24 03:39:36.393 19136 19153 E AndroidRuntime: FATAL EXCEPTION: CameraBackground\r\n\t01-24 03:39:36.393 19136 19153 E AndroidRuntime: Process: com.example.android.tflitecamerademo, PID: 19136\r\n\t01-24 03:39:36.393 19136 19153 E AndroidRuntime: java.lang.IllegalArgumentException: Failed to run on the given Interpreter: NNAPI was requested, but dependent sized tensors being used.\r\n\t01-24 03:39:36.393 19136 19153 E AndroidRuntime: \r\n\t01-24 03:39:36.393 19136 19153 E AndroidRuntime: \tat org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)\r\n\t01-24 03:39:36.393 19136 19153 E AndroidRuntime: \tat org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:95)\r\n\t01-24 03:39:36.393 19136 19153 E AndroidRuntime: \tat org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:123)\r\n\t01-24 03:39:36.393 19136 19153 E AndroidRuntime: \tat org.tensorflow.lite.Interpreter.run(Interpreter.java:104)\r\n\t01-24 03:39:36.393 19136 19153 E AndroidRuntime: \tat com.example.android.tflitecamerademo.ImageClassifier.classifyFrame(ImageClassifier.java:114)\r\n\t01-24 03:39:36.393 19136 19153 E AndroidRuntime: \tat com.example.android.tflitecamerademo.Camera2BasicFragment.classifyFrame(Camera2BasicFragment.java:663)\r\n\t01-24 03:39:36.393 19136 19153 E AndroidRuntime: \tat com.example.android.tflitecamerademo.Camera2BasicFragment.access$900(Camera2BasicFragment.java:69)\r\n\t01-24 03:39:36.393 19136 19153 E AndroidRuntime: \tat com.example.android.tflitecamerademo.Camera2BasicFragment$5.run(Camera2BasicFragment.java:558)\r\n\t01-24 03:39:36.393 19136 19153 E AndroidRuntime: \tat android.os.Handler.handleCallback(Handler.java:790)\r\n\t01-24 03:39:36.393 19136 19153 E AndroidRuntime: \tat android.os.Handler.dispatchMessage(Handler.java:99)\r\n\t01-24 03:39:36.393 19136 19153 E AndroidRuntime: \tat android.os.Looper.loop(Looper.java:164)\r\n\t01-24 03:39:36.393 19136 19153 E AndroidRuntime: \tat android.os.HandlerThread.run(HandlerThread.java:65)\r\n\t01-24 03:39:36.396   626   871 W ActivityManager:   Force finishing activity com.example.android.tflitecamerademo/.CameraActivity\r\n\r\nHere is my patch\r\n\r\n\t index e44c5ae..1ed88eb 100644\r\n\t ---a/tensorflow/contrib/lite/java/demo/app/src/main/java/com/example/android/tflitecamerademo/ImageClassifier.java\r\n\t +++b/tensorflow/contrib/lite/java/demo/app/src/main/java/com/example/android/tflitecamerademo/ImageClassifier.java\r\n\t @@ -91,7 +91,7 @@ public class ImageClassifier {\r\n\t\t\r\n\t   /** Initializes an {@code ImageClassifier}. */\r\n\t   ImageClassifier(Activity activity) throws IOException {\r\n\t-    tflite = new Interpreter(loadModelFile(activity));\r\n\t+    tflite = new Interpreter(loadModelFile(activity), true);\r\n\t     labelList = loadLabelList(activity);\r\n\t     imgData =\r\n\t         ByteBuffer.allocateDirect(\r\n\tdiff --git a/tensorflow/contrib/lite/java/demo/build.gradle b/tensorflow/contrib/lite/java/demo/build.gradle\r\n\tindex dd883d6..9361c71 100644\r\n\t--- a/tensorflow/contrib/lite/java/src/main/java/org/tensorflow/lite/Interpreter.java\r\n\t+++ b/tensorflow/contrib/lite/java/src/main/java/org/tensorflow/lite/Interpreter.java\r\n\t@@ -66,6 +66,13 @@ public final class Interpreter implements AutoCloseable {\r\n\t     }\r\n\t     wrapper = new NativeInterpreterWrapper(modelFile.getAbsolutePath());\r\n\t   }\r\n\t+  public Interpreter(@NotNull File modelFile, boolean nn) {\r\n\t+    if (modelFile == null) {\r\n\t+      return;\r\n\t+    }\r\n\t+    wrapper = new NativeInterpreterWrapper(modelFile.getAbsolutePath());\r\n\t+    wrapper.setUseNNAPI(nn);\r\n\t+  }\r\n\t\r\n\t   /**\r\n\t    * Initializes a {@code Interpreter} with a {@code MappedByteBuffer} to the model file.\r\n\t@@ -76,6 +83,10 @@ public final class Interpreter implements AutoCloseable {\r\n\t   public Interpreter(@NotNull MappedByteBuffer mappedByteBuffer) {\r\n\t     wrapper = new NativeInterpreterWrapper(mappedByteBuffer);\r\n\t   }\r\n\t+  public Interpreter(@NotNull MappedByteBuffer mappedByteBuffer,  boolean nn) {\r\n\t+    wrapper = new NativeInterpreterWrapper(mappedByteBuffer);\r\n\t+    wrapper.setUseNNAPI(nn);\r\n\t+  }\r\n\t \r\n\t   /**\r\n\t    * Runs model inference if the model takes only one input, and provides only one output.\r\n\t   /**\r\n\t    * Runs model inference if the model takes only one input, and provides only one output.\r\n", "comments": ["Anyway better to use NNAPI?", "Confirmed that NN API stopped to work. I actually sent a one-line PR to fix this issue, https://github.com/tensorflow/tensorflow/pull/16256. ", "Thanks for the report and thanks @freedomtan for the fix.", "Closing this out, since it looks like #16256 should fix the problem.  If not, feel free to ping this issue and I'll re-open it.", "This issue rises again with the newest commit 283f03c825312efd3319cb37dc1a412288a536ec\r\n\r\nHere is the exception\r\n\r\n\t02-06 17:37:50.101 10142 10158 E AndroidRuntime: FATAL EXCEPTION: CameraBackground\r\n\t02-06 17:37:50.101 10142 10158 E AndroidRuntime: Process: com.example.android.tflitecamerademo, PID: 10142\r\n\t02-06 17:37:50.101 10142 10158 E AndroidRuntime: java.lang.IllegalArgumentException: Failed to run on the given Interpreter: NNAPI was requested, but dependent sized tensors being used.\r\n\t02-06 17:37:50.101 10142 10158 E AndroidRuntime: \r\n\t02-06 17:37:50.101 10142 10158 E AndroidRuntime: \tat org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)\r\n\t02-06 17:37:50.101 10142 10158 E AndroidRuntime: \tat org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:95)\r\n\t02-06 17:37:50.101 10142 10158 E AndroidRuntime: \tat org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:123)\r\n\t02-06 17:37:50.101 10142 10158 E AndroidRuntime: \tat org.tensorflow.lite.Interpreter.run(Interpreter.java:104)\r\n\t02-06 17:37:50.101 10142 10158 E AndroidRuntime: \tat com.example.android.tflitecamerademo.ImageClassifier.classifyFrame(ImageClassifier.java:114)\r\n\t02-06 17:37:50.101 10142 10158 E AndroidRuntime: \tat com.example.android.tflitecamerademo.Camera2BasicFragment.classifyFrame(Camera2BasicFragment.java:663)\r\n\t02-06 17:37:50.101 10142 10158 E AndroidRuntime: \tat com.example.android.tflitecamerademo.Camera2BasicFragment.access$900(Camera2BasicFragment.java:69)\r\n\t02-06 17:37:50.101 10142 10158 E AndroidRuntime: \tat com.example.android.tflitecamerademo.Camera2BasicFragment$5.run(Camera2BasicFragment.java:558)\r\n\t02-06 17:37:50.101 10142 10158 E AndroidRuntime: \tat android.os.Handler.handleCallback(Handler.java:790)\r\n\t02-06 17:37:50.101 10142 10158 E AndroidRuntime: \tat android.os.Handler.dispatchMessage(Handler.java:99)\r\n\t02-06 17:37:50.101 10142 10158 E AndroidRuntime: \tat android.os.Looper.loop(Looper.java:164)\r\n\t02-06 17:37:50.101 10142 10158 E AndroidRuntime: \tat android.os.HandlerThread.run(HandlerThread.java:65)\r\n\t", "My above patch did not change.", "why the fix is not merged?", "@andrehentz and @tatatodd: confirmed that https://github.com/tensorflow/tensorflow/pull/16256 was gone and I can reproduce what @benshi001 reported with my apps. Can't find why from git log. Maybe it was reverted during code merging?", "I can't tell why that disappeared. I've made the change manually now, and it should be pushed soon."]}, {"number": 16350, "title": "There is an issue with your \"new issue\" page", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- All systems/system independent\r\n\r\n### Describe the problem\r\nIf there are documentation issues, the \"New Issue\" button will direct many people to StackOverflow. The notions of \"bugs and features\" are not universal in that they only apply to software, not to documentation as far as some people are concerned. IMHO the text above would be better if it stated that \"1. It must be a bug or a feature request, or a correction/clarification to documentation\"\r\n\r\n### Source code / logs\r\nNo source involved\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Seriously? Read it \u2013 it specified ALL SYSTEMS/SYSTEM INDEPENDENT.\r\n\r\nYou provide no instructions on how to write or punctuate N/A (would \u201cn/a\u201d or \u201cnot applicable\u201d be acceptable?) and in any case I cannot LEAVE it as \u201cN/A\u201d because there was no \u201cN/A\u201d there to leave in the first place.\r\n\r\nIf the form is machine read, then please provide the parsing instructions that the machine will use (e.g. \u201cplease replace each $$$ in what follows with your details. If your details are not relevant please replace the $$$ with \u2018N/A\u2019\u201d). If you\u2019re not a machine, then please read what is written; if you are a machine, please provide your specifications. The world is not your mind-reader.\r\n\r\nSo seriously: the issue I raised is about an obvious barrier to helping you improve your documentation. Your response inadvertently highlights another barrier.\r\n\r\nSorry to sound so hostile, and it\u2019s nothing personal, but I have found your \u201cQuick Start\u201d documentation among the slowest I have ever worked through.\r\n\r\n\r\n[Alchemy Logo]Paul Qualtrough \u25cf Director\r\nt. +64 9 636 5412 | m. +64 21 062 7393\r\n51 Hastie Avenue, Mangere Bridge, Auckland 2022, New Zealand\r\npaulq@alchemysort.com<mailto:paulq@alchemysort.com> | www.alchemysort.com\r\n\r\nFrom: Alfred [mailto:notifications@github.com]\r\nSent: Thursday, 25 January 2018 9:00 a.m.\r\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\r\nCc: Paul Qualtrough <paulq@alchemysort.com>; Author <author@noreply.github.com>\r\nSubject: Re: [tensorflow/tensorflow] There is an issue with your \"new issue\" page (#16350)\r\n\r\n\r\nThank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\r\nHave I written custom code\r\nOS Platform and Distribution\r\nTensorFlow installed from\r\nTensorFlow version\r\nBazel version\r\nCUDA/cuDNN version\r\nGPU model and memory\r\nExact command to reproduce\r\n\r\n\u2014\r\nYou are receiving this because you authored the thread.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/16350#issuecomment-360254534>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AiDmWzUnUtwJ-8Ci5bhmc7Y1JbdfCbcaks5tN4uxgaJpZM4Rqrzb>.\r\n", "Hi,\r\nYou can try [https://stackoverflow.solutions](https://stackoverflow.solutions) for documenting.\r\nI might adapt it for your needs, it's a young project, kind like the closed SO Documentation beta\r\n\r\n", "/CC @MarkDaoust for docs.\r\n\r\nThank you for the feedback. That's helpful. Kindly note that the butler is a bot, and as such, does not parse things very well.\r\n\r\nWe do process a high volume of github issues, and the use of automated processes has become a necessity for us.\r\n\r\nIf you have specific suggestions, please send a PR for the docs.", "@omatai , I will update the templates as you suggest. Thank you for suggesting this clarification.", "Here are the PR's\r\nhttps://github.com/tensorflow/tensorflow/pull/16635\r\nhttps://github.com/tensorflow/models/pull/3341\r\n", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Github processes confuse me deeply. I have no idea if I'm supposed to respond, who the \"assignee\" is, how to figure out if anything has changed -- everything Github people seem to take for granted. I'm giving up :-(\r\n\r\n[Begin rant] Lack of decent documentation is stifling the IT world. It's as if everyone outside it speaks English, and everyone inside it speaks some obscure dialect of Swahili. It is increasingly impenetrable, and increasingly undocumented. That's fine if you're in a large corporation and can feed off knowledgable others. If you're in a small business, it feels like death via a thousand paper cuts. Automated documentation is inadequate -- consider the difference between \"time flies like an arrow\" and \"fruit flies like an apple\". AI is not at the level where it can be genuinely helpful. I am a native English speaker of 50+ years, but I do not speak Bay Area English or Seattle Area English. How I wish some of those speaking those dialects would take 30 seconds twice a day to go a tiny bit further and consider those of us out here trying to make sense of things.[end rant]", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "> I have no idea if I'm supposed to respond, who the \"assignee\" is, how to figure out if anything has changed.\r\n\r\nThanks for the feedback. We've updated the issue bot to be specific about who it's nagging. \r\n\r\nAnd we've made updates to the issue template."]}, {"number": 16349, "title": "Fix typos", "body": "This PR fixes some typos: `intializes`, `nubmer`, `varibale`, `fuction`, `Ouput`, `avaliable`, `ouput`, and `Explictly`.", "comments": []}, {"number": 16348, "title": "Change RELEASE.md to specify CUDA version", "body": "The RELEASE.md states that \"Prebuilt binaries are now built against CUDA 9 and cuDNN 7.\"\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/15604 says that CUDA 9.1 is not supported.\r\n\r\nCould we change the RELEASE.md so that it says \"Prebuilt binaries are now built against CUDA 9.0 and cuDNN 7.\" until later versions are supported?\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code: N/A\r\nOS Platform and Distribution: Linux\r\nTensorFlow installed from: pip\r\nTensorFlow version: tensorflow-gpu 1.5.0rc1\r\nBazel version: N/A\r\nCUDA/cuDNN version: 9.1 & 7\r\nGPU model and memory: N/A\r\nExact command to reproduce: N/A", "Thank you for noticing this imprecision in the documentation. Would you like to submit the requested change yourself as a pull request?"]}, {"number": 16347, "title": "Golang API to serialize data into tf.Example protos(tfrecords)", "body": "The Golang api [WriteContentsTo](https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go#Tensor.WriteContentsTo) can be used to writes the serialized contents of a tensor to io.Writer, where the tensor is built from golang scalars, slices, and arrays. \r\n\r\nYet there's not a Golang API to serialize data into tf.Example protos(tfrecords).\r\n\r\nFor example, when i want serialize a libsvm into tf.Example protos, i can do this by:\r\n```\r\ndef libsvm2tfrecords(data_source, target_dir, delimiter='\\t'):\r\n    \"\"\"\r\n    a single file should not contain lines more than 1000,000, or we should use libsvm2proto_par\r\n    :param data_source: libsvm file path\r\n    :param target_dir: dir to storage the serialize proto file\r\n    :param delimiter: delimiter for csv reader\r\n    :return:\r\n    \"\"\"\r\n    if not os.path.isfile(data_source):\r\n        raise ValueError('data file passed do not exist or not a file')\r\n\r\n    file_name = os.path.join(target_dir, os.path.splitext(\r\n        os.path.basename(data_source))[0] + '.tfrecords')\r\n    writer = tf.python_io.TFRecordWriter(file_name)\r\n    start = datetime.now()\r\n    line_c = 0\r\n    with open(data_source, 'rb') as rf:\r\n        f_reader = csv.reader(rf, delimiter=delimiter, quotechar='|')\r\n        for row in f_reader:\r\n            line_c += 1\r\n            feature = dict()\r\n            indexes = []\r\n            values = []\r\n            feature.update({'label': _float_feature([float(row[0])])})\r\n            for e in row[1:]:\r\n                index, value = e.split(':')\r\n                indexes.append(int(index))\r\n                values.append(float(value))\r\n                feature.update({'index': _int64_feature(indexes)})\r\n                feature.update({'value': _float_feature(values)})\r\n\r\n            example = tf.train.Example(features=tf.train.Features(feature=feature))\r\n            writer.write(example.SerializeToString())\r\n\r\n        writer.close()\r\n        end = datetime.now()\r\n\r\n        print(\"- consumed time: %ds for %s\" % ((end-start).seconds, data_source))\r\n```\r\nBUT Golang api does not seem to be able to achieve this. ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Closing this as a duplicate of a [StackOverflow query](https://stackoverflow.com/questions/48393098/how-to-convert-data-to-a-serialized-tf-exampletensorflow-tfrecords-using-golan) and #16282 \r\n\r\nFeel free to reopen if I'm mistaken."]}, {"number": 16346, "title": "R1.4", "body": "", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->"]}, {"number": 16345, "title": "tf.nn.sparse_softmax_cross_entropy_with_logits get error:  ValueError: Rank mismatch: Rank of labels (received 1) should equal rank of logits minus 1 (received 4).", "body": "", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "```\r\n with ops.name_scope(scope, \"sparse_softmax_cross_entropy_loss\",\r\n                      [logits, labels, weights]) as scope:\r\n    labels = array_ops.reshape(labels, shape=[array_ops.shape(labels)[0]])\r\n    **logits = array_ops.reshape(logits , shape=[array_ops.shape(logits )[0]])**\r\n    losses = nn.sparse_softmax_cross_entropy_with_logits(\r\n        labels=labels, logits=logits, name=\"xentropy\")\r\n    return compute_weighted_loss(losses, weights, scope=scope)\r\n```\r\nI think logits need to be flatten too"]}, {"number": 16344, "title": "Variance of weights initialized with tf.variance_scaling_initializer is somewhat surprising", "body": "If `tf.variance_scaling_initializer` is called with `distribution='uniform'`, then the weights have variance `scale / n`. However, if `tf.variance_scaling_initializer` is called with `distribution='normal'`, the weights have variance `scale / n * (1 - (4 * norm.pdf(2))/(2 * norm.cdf(2) - 1))`, because the weights are drawn from a normal distribution truncated at +/- 2 std and the scale is not adjusted for this truncation.\r\n\r\nWhile I understand that there are reasons to use a truncated normal distribution, I would argue that the distribution should be scaled to adjust for the truncation, or the fact that the scale is not adjusted for truncation should feature more prominently in the documentation. The current statement that:\r\n\r\n> With `distribution=\"normal\"`, samples are drawn from a truncated normal distribution centered on zero, with `stddev = sqrt(scale / n)`\r\n\r\nis not totally clear, since it's not obvious that `stddev` is referring to the standard deviation of the normal distribution before truncation and not after.\r\n\r\nTo make matters more confusing, unlike `tf.variance_scaling_initializer`, `tf.contrib.layers.variance_scaling_initializer` performs the appropriate correction so that the weights have variance `scale / n`.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code No\r\nOS Platform and Distribution N/A\r\nTensorFlow installed from GitHub\r\nTensorFlow version 2968447d32bdfd0dd6fafabfcd1aafd6dc261803\r\nBazel version N/A\r\nCUDA/cuDNN version N/A\r\nGPU model and memory N/A\r\nExact command to reproduce `tf.variance_scaling_initializer()`", "@fchollet Could you take a look at this request?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @fchollet: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @fchollet: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @fchollet: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @fchollet: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @fchollet: It has been 16 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @fchollet: It has been 108 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @fchollet: It has been 123 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "#18854 adds variance correction for truncated normal. Before it the stddev is before truncation. Now the stddev is after truncation. I guess this has answered your confusion.", "Nagging Assignee @fchollet: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @fchollet: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Fixed by #18854"]}, {"number": 16343, "title": "tf.data.Dataset doesn't provide a good workflow for generating custom samples from large files", "body": "We're given hundreds of data files, each containing many gigabytes worth of sample data in a custom format. As far as I can tell there are only two approaches to extract samples from this using `Dataset`:\r\n\r\n1) `tf.data.Dataset.from_generator(generator=my_custom_reader, ...)`\r\n\r\nCreate a generator which produces samples. This approach is not ideal because this method must be the first dataset in the chain. The generator cannot accept a tensor. Therefore you can't batch and shuffle your list of 100's of filenames (or anything more complex). You also can't make use of `interleave(...)` because the generator can't accept a tensor, and this use case is begging to use `interleave(...)`.\r\n\r\nA solution here might be to provide a method for a generator to accept a tensor, as `tf.py_func(...)` does for functions.\r\n\r\n2) `tf.data.Dataset.map(map_func=tf.py_func(my_custom_reader, ...), ...)`\r\n\r\nThe map function does allow us to shuffle and parallelize the filenames using all of the functionality of the Dataset pipeline, however, with `map`, the files must be read into memory completely, and these files are large. Reading numerous files into memory is infeasible.\r\n\r\nA solution here might be to extend the `map` function to support generators.\r\n\r\nUnless there's an alternative approach, which I didn't glean from the docs or stackoverflow, then this seems to be an inherent limitation and a seemingly reasonable use case on which to base a feature request.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code\r\nN/A\r\n\r\nOS Platform and Distribution\r\nLinux Mint\r\n\r\nTensorFlow installed from\r\npip installer\r\n\r\nTensorFlow version\r\n1.4\r\n\r\nBazel version\r\nN/A\r\n\r\nCUDA/cuDNN version\r\nCUDA8 / cuDNN 6\r\n\r\nGPU model and memory\r\nTitan 1080 / 10GB\r\n\r\nExact command to reproduce\r\nN/A", "I've implemented this process using map functions by splitting large files into filename/index pairs so as to specify a subset of the file to read in each map call. This was not a trivial task. A final sample solution can be found on stack overflow below. I continue to believe that Dataset should be extended to better support this workflow. In particular with respect to supporting generators throughout the pipeline.\r\n\r\nhttps://stackoverflow.com/questions/48471688/using-tensorflows-dataset-pipeline-how-do-i-name-the-results-of-a-map-oper/48589464#48589464\r\n\r\nUpdate to this post:\r\n---------------\r\nThe solution referenced above doesn't work in the case where each sample needs to be different shapes. This code example demonstrates an attempt to return an ndarray of ndarray's which produces an `unsupported object type numpy.ndarray` error in tensorflow. The goal of the example is to demonstrate reading 2 samples of variable length from a file. \r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef mypymap(data_numpy):\r\n  result = np.array([\r\n    np.array([100,101,102]),\r\n    np.array([200,201,202,203])\r\n  ])\r\n  return result\r\n\r\ndata_input = ['fileA','fileB','fileC']\r\n\r\nds = tf.data.Dataset.from_tensor_slices(data_input)\r\nds = ds.map(lambda data_tensor: tf.py_func(mypymap, [data_tensor], Tout=[tf.int64], stateful=False))\r\n\r\nelement = ds.make_one_shot_iterator().get_next()\r\n\r\nwith tf.Session() as sess:\r\n  for _ in range(3):\r\n    print(sess.run(element))\r\n```\r\nSo that leaves the only solution being to incorporate a bunch of logic into my generator (for interleaving, parallel file reads, and shuffling) and putting the generator up at the beginning of the preprocessing pipeline. All things that are better done as part of the `Dataset` pipeline.", "@mrry can you comment?", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "I'm moving this discussion to a related thread here: https://github.com/tensorflow/tensorflow/issues/13101"]}, {"number": 16342, "title": "Fixed documentation formatting", "body": "", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "@mossaab0 please sign the CLA.", "I'll close this. Please reopen when you have signed the CLA."]}, {"number": 16341, "title": "[Bazel/Windows] Don't use -Wl, -lpthread and -lm on Windows", "body": "", "comments": ["Thanks, Derek."]}, {"number": 16340, "title": "Fix typo", "body": "", "comments": []}, {"number": 16339, "title": "Remove path_to_str from the public API", "body": "@martinwicke fyi", "comments": ["The failing test was an unrelated timeout. Merging.", "@martinwicke should this have gone through API review, perhaps?", "This is a correction of an earlier oops regarding API so this is fine.\n\nOn Jan 24, 2018 9:25 AM, \"Rasmus Munk Larsen\" <notifications@github.com>\nwrote:\n\n> @martinwicke <https://github.com/martinwicke> should this have gone\n> through API review, perhaps?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/16339#issuecomment-360208633>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAjO_d6i5r2n1Cgnx2QQr1HzZVRKMIj8ks5tN2drgaJpZM4RqgIU>\n> .\n>\n"]}, {"number": 16338, "title": "Obtain tower id when using tf.contrib.estimator.replicate_model_fn()", "body": "Since tensorflow 1.5, a new API tf.contrib.estimator.replicate_model_fn() is introduced to to replicate a model over GPUs. However, the current API hides the tower id information from the model_fn(). Without knowning tower id, it is difficult to create non-shared local variables/ops per tower. Is it possible to propagate the tower id information to model_fn? Thanks.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 16337, "title": "Fix a bug that capture_tpu_profile only takes absolute logdir path.", "body": "Also removed package dependancy on tensorflow for better compatibility.", "comments": ["Sure, we will have our own repository in the long run, probably after a few releases. But definitely not next week :)"]}, {"number": 16336, "title": "Apply final cherry-picks for 1.5.0 release.", "body": null, "comments": ["I have cherry-picked a prerequisite of 973167c whose absence was causing the tests to fail. Thanks to @panyx0718 for investigating.", "I have cherry-picked yet another prerequisite.", "I've applied a subset of earlier changes to test_util that may allow the test to run, without bringing in other odd dependencies.", "@angersson are we closing this one until bfloat16 is cherrypicked into 1.5 properly?", "@av8ramit Yep."]}, {"number": 16335, "title": "Apply final cherry-picks for 1.5.0 release.", "body": null, "comments": []}, {"number": 16334, "title": "save_steps in MoniteredTrainingSession", "body": "Any reason to omit the `save_steps` in favor of `save_secs` for `CheckpointSaverHook` in `MoniteredTrainingSession`?\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/abf3c6d745c34d303985f210bf9e92cac99ba744/tensorflow/python/training/monitored_session.py#L363\r\n\r\nThe previous `SummarySavorHook` has both `save_steps` and `save_secs` considered\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/abf3c6d745c34d303985f210bf9e92cac99ba744/tensorflow/python/training/monitored_session.py#L358\r\n\r\n- [x] Have I written custom code: no\r\n- [x] OS Platform and Distribution: N/A\r\n- [x] TensorFlow installed from: pip\r\n- [x] TensorFlow version: 1.5\r\n- [x] Bazel version: N/A\r\n- [x] CUDA/cuDNN version: N/A\r\n- [x] GPU model and memory: N/A\r\n- [x] Exact command to reproduce: N/A", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 16333, "title": "Computing gradients of loop variables return None", "body": "### System information\r\n- Windows 10 x64\r\n- Installed from binary\r\n- TensorFlow 1.3.0\r\n- Python 3.6.3\r\n- CuDNN 6.4.6, CUDA 8.0\r\n- NVIDIA GeForce 940M\r\n\r\n### Bug Description\r\n\r\nIssue [15403](https://github.com/tensorflow/tensorflow/issues/15403) was closed since it was labelled as \"not a bug or feature request\". I believe it is indeed a bug. \r\n\r\nThe fundamental problem is that gradients do not seem to be working inside of `tf.while_loop`s. Here is a demonstration of code run inside and outside of a while loop. The code outside of the loop produces a result, whereas the gradient inside of the loop returns None: \r\n```\r\nimport tensorflow as tf\r\n\r\ni = tf.constant(0)\r\nx = tf.constant(3.0)\r\nprint(\"External gradient:\", tf.gradients(x, x)[0])     # Prints Tensor(\"gradients/Fill:0\", shape=(), dtype=float32)\r\n\r\ndef loop_body(i, x, y):\r\n    print(\"internal gradient:\", tf.gradients(x, y)[0]) # Prints None\r\n    return i + 1, x, y\r\n\r\ntf.while_loop( lambda i,x,y: tf.less(i, 5), loop_body, [i, x, x]);\r\n```", "comments": ["In the loop, `x` and `y` has no relationship.\r\n\r\n```\r\ni = tf.constant(0)\r\nx = tf.constant(3.0)\r\n\r\n# now x and i has no computation graph between them\r\nprint(tf.gradients(x, i)[0])    # prints None\r\n\r\ndef loop_body(i, x, y):\r\n    y = x                       # if you add computation graph between x and y\r\n    g = tf.gradients(x, y)[0]\r\n    print(g)                    # prints something\r\n    return i + 1, x, y\r\n\r\ntf.while_loop(lambda i, *_: tf.less(i, 5), loop_body, [i, x, x])\r\n```", "Then I am confused about something... \r\n\r\nIn the above code, the loop_vars are set to [i,x,x]. So I assumed that y=x in the loop_body. Why isn't that the case? \r\n\r\nIs the while loop copying in the loop_vars instead of referencing them? \r\n", "@skye do you understand the Python gradient creation code well enough to understand what is/should be going on here?", "I think this is closed related to #13616", "This is fixed with the latest version of TensorFlow. Feel free to reopen if the issue still persists. Thanks!"]}, {"number": 16332, "title": "Fixes #16314", "body": "Fixes #16314.", "comments": ["@gunan I have noticed that and have been trying to find a solution for a while. This keeps causing a problem with the pre-compiled binaries I distribute with my Scala API for TensorFlow. This change seems to fix the problems I've been having though. I will report if I bump into new issues and look for a solution then, but I need to be able to reproduce.", "The bigger issue is for our pip packages. maybe the flag is not being propagated properly, but even with this flag we run into ABI incompatibility issues in our pip packages.\r\nIf this solves the problem for libtensorflow, that is great! we can solve the pip issue separately.", "@gunan Sounds good! Yeah I'm not using the pip packages at all so I can't really say anything about them. One minor thing to point out would be to maybe add a note on the \"Compiling from Sources\" section of the documentation, for people to use that flag when running the bazel build command. I think we could even just add the compiler option in the bazel build files themselves so developers don't have to worry about it."]}, {"number": 16331, "title": "Compilation failure with gcc-6.4 (gcc-7.2 and clang-4) in ubuntu 17.10", "body": "When compiling master in ubuntu 17.10, compilation fails due to 'too perfect forwarding' in variant_op_registry\r\n`\r\nstd::unordered_map<std::tuple<VariantXOp, StringPiece, StringPiece>, \r\n                     VariantXOpFn, TupleHash>\r\n`\r\nA workaround, by replacing tuple with a struct, provided in PR #16309 for your review.\r\n\r\nThanks,\r\nSami\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Sorry, I forgot these \r\nOS Platform and Distribution: 17.10\r\nTensorFlow installed from: N/A\r\nTensorFlow version: master branch a93f40737d3a7\r\nBazel  0.9.0 & 0.8.0\r\nCUDA/cuDNN: 9.0/7.0\r\nGPU model & memory: compute 7.0\r\nExact command to reproduce: ./configure && bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\nI added these for sake of completeness. PR #16309 merged and fixes the issue. Closing\r\n"]}, {"number": 16330, "title": "Branch 182892876", "body": "resolved one merge conflict in histogram_ops.py", "comments": []}, {"number": 16329, "title": "tf.nn.seq2seq.embedding_rnn_seq2seq", "body": "Have I written custom code: Yes\r\nOS Platform and Distribution: Windows 10\r\nTensorFlow installed from anaconda prompt\r\nTensorFlow version 0.12 & 1.4\r\nBazel version NA\r\nCUDA/cuDNN version NA\r\nGPU model and memory: Floydhub\r\nExact command to reproduce:\r\n\r\nself.outputs, self.states = tf.contrib.legacy_seq2seq.embedding_rnn_seq2seq(self.enc,self.dec, stacked_lstm, xvocab_size, yvocab_size, emb_dim)\r\n\r\nTHE PROBLEM.\r\nI am upgrading working tf0.12 code so I can train on floydhub.  I have replaced  tf.nn.seq2seq.embedding_rnn_seq2seq  with tf.contrib.legacy_seq2seq.embedding_rnn_seq2seq. It produces the following error log:\r\n\r\n\r\n  File \"C:\\Users\\BC\\AppData\\Local\\conda\\conda\\envs\\tf1.4\\lib\\site-packages\\tensorflow\\contrib\\legacy_seq2seq\\python\\ops\\seq2seq.py\", line 358, in embedding_rnn_seq2seq\r\n    encoder_cell = copy.deepcopy(cell)\r\n\r\n  File \"C:\\Users\\BC\\AppData\\Local\\conda\\conda\\envs\\tf1.4\\lib\\copy.py\", line 166, in deepcopy\r\n    y = copier(memo)\r\n\r\n  File \"C:\\Users\\BC\\AppData\\Local\\conda\\conda\\envs\\tf1.4\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 655, in __deepcopy__\r\n    setattr(result, k, copy.deepcopy(v, memo))\r\n\r\n  File \"C:\\Users\\BC\\AppData\\Local\\conda\\conda\\envs\\tf1.4\\lib\\copy.py\", line 155, in deepcopy\r\n    y = copier(x, memo)\r\n\r\n  File \"C:\\Users\\BC\\AppData\\Local\\conda\\conda\\envs\\tf1.4\\lib\\copy.py\", line 218, in _deepcopy_list\r\n    y.append(deepcopy(a, memo))\r\n\r\n  File \"C:\\Users\\BC\\AppData\\Local\\conda\\conda\\envs\\tf1.4\\lib\\copy.py\", line 182, in deepcopy\r\n    y = _reconstruct(x, rv, 1, memo)\r\n\r\n  File \"C:\\Users\\BC\\AppData\\Local\\conda\\conda\\envs\\tf1.4\\lib\\copy.py\", line 297, in _reconstruct\r\n    state = deepcopy(state, memo)\r\n\r\n  File \"C:\\Users\\BC\\AppData\\Local\\conda\\conda\\envs\\tf1.4\\lib\\copy.py\", line 155, in deepcopy\r\n    y = copier(x, memo)\r\n\r\n  File \"C:\\Users\\BC\\AppData\\Local\\conda\\conda\\envs\\tf1.4\\lib\\copy.py\", line 243, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n\r\n  File \"C:\\Users\\BC\\AppData\\Local\\conda\\conda\\envs\\tf1.4\\lib\\copy.py\", line 182, in deepcopy\r\n    y = _reconstruct(x, rv, 1, memo)\r\n\r\n  File \"C:\\Users\\BC\\AppData\\Local\\conda\\conda\\envs\\tf1.4\\lib\\copy.py\", line 297, in _reconstruct\r\n    state = deepcopy(state, memo)\r\n\r\n  File \"C:\\Users\\BC\\AppData\\Local\\conda\\conda\\envs\\tf1.4\\lib\\copy.py\", line 155, in deepcopy\r\n    y = copier(x, memo)\r\n\r\n  File \"C:\\Users\\BC\\AppData\\Local\\conda\\conda\\envs\\tf1.4\\lib\\copy.py\", line 243, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n\r\n  File \"C:\\Users\\BC\\AppData\\Local\\conda\\conda\\envs\\tf1.4\\lib\\copy.py\", line 182, in deepcopy\r\n    y = _reconstruct(x, rv, 1, memo)\r\n\r\n  File \"C:\\Users\\BC\\AppData\\Local\\conda\\conda\\envs\\tf1.4\\lib\\copy.py\", line 297, in _reconstruct\r\n    state = deepcopy(state, memo)\r\n\r\n  File \"C:\\Users\\BC\\AppData\\Local\\conda\\conda\\envs\\tf1.4\\lib\\copy.py\", line 155, in deepcopy\r\n    y = copier(x, memo)\r\n\r\n  File \"C:\\Users\\BC\\AppData\\Local\\conda\\conda\\envs\\tf1.4\\lib\\copy.py\", line 243, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n\r\n  File \"C:\\Users\\BC\\AppData\\Local\\conda\\conda\\envs\\tf1.4\\lib\\copy.py\", line 182, in deepcopy\r\n    y = _reconstruct(x, rv, 1, memo)\r\n\r\n  File \"C:\\Users\\BC\\AppData\\Local\\conda\\conda\\envs\\tf1.4\\lib\\copy.py\", line 297, in _reconstruct\r\n    state = deepcopy(state, memo)\r\n\r\n  File \"C:\\Users\\BC\\AppData\\Local\\conda\\conda\\envs\\tf1.4\\lib\\copy.py\", line 155, in deepcopy\r\n    y = copier(x, memo)\r\n\r\n  File \"C:\\Users\\BC\\AppData\\Local\\conda\\conda\\envs\\tf1.4\\lib\\copy.py\", line 243, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n\r\n  File \"C:\\Users\\BC\\AppData\\Local\\conda\\conda\\envs\\tf1.4\\lib\\copy.py\", line 155, in deepcopy\r\n    y = copier(x, memo)\r\n\r\n  File \"C:\\Users\\BC\\AppData\\Local\\conda\\conda\\envs\\tf1.4\\lib\\copy.py\", line 243, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n\r\n  File \"C:\\Users\\BC\\AppData\\Local\\conda\\conda\\envs\\tf1.4\\lib\\copy.py\", line 182, in deepcopy\r\n    y = _reconstruct(x, rv, 1, memo)\r\n\r\n  File \"C:\\Users\\BC\\AppData\\Local\\conda\\conda\\envs\\tf1.4\\lib\\copy.py\", line 297, in _reconstruct\r\n    state = deepcopy(state, memo)\r\n\r\n  File \"C:\\Users\\BC\\AppData\\Local\\conda\\conda\\envs\\tf1.4\\lib\\copy.py\", line 155, in deepcopy\r\n    y = copier(x, memo)\r\n\r\n  File \"C:\\Users\\BC\\AppData\\Local\\conda\\conda\\envs\\tf1.4\\lib\\copy.py\", line 243, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n\r\n  File \"C:\\Users\\BC\\AppData\\Local\\conda\\conda\\envs\\tf1.4\\lib\\copy.py\", line 155, in deepcopy\r\n    y = copier(x, memo)\r\n\r\n  File \"C:\\Users\\BC\\AppData\\Local\\conda\\conda\\envs\\tf1.4\\lib\\copy.py\", line 218, in _deepcopy_list\r\n    y.append(deepcopy(a, memo))\r\n\r\n  File \"C:\\Users\\BC\\AppData\\Local\\conda\\conda\\envs\\tf1.4\\lib\\copy.py\", line 155, in deepcopy\r\n    y = copier(x, memo)\r\n\r\n  File \"C:\\Users\\BC\\AppData\\Local\\conda\\conda\\envs\\tf1.4\\lib\\copy.py\", line 223, in _deepcopy_tuple\r\n    y = [deepcopy(a, memo) for a in x]\r\n\r\n  File \"C:\\Users\\BC\\AppData\\Local\\conda\\conda\\envs\\tf1.4\\lib\\copy.py\", line 223, in <listcomp>\r\n    y = [deepcopy(a, memo) for a in x]\r\n\r\n  File \"C:\\Users\\BC\\AppData\\Local\\conda\\conda\\envs\\tf1.4\\lib\\copy.py\", line 155, in deepcopy\r\n    y = copier(x, memo)\r\n\r\n  File \"C:\\Users\\BC\\AppData\\Local\\conda\\conda\\envs\\tf1.4\\lib\\copy.py\", line 243, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n\r\n  File \"C:\\Users\\BC\\AppData\\Local\\conda\\conda\\envs\\tf1.4\\lib\\copy.py\", line 182, in deepcopy\r\n    y = _reconstruct(x, rv, 1, memo)\r\n\r\n  File \"C:\\Users\\BC\\AppData\\Local\\conda\\conda\\envs\\tf1.4\\lib\\copy.py\", line 297, in _reconstruct\r\n    state = deepcopy(state, memo)\r\n\r\n  File \"C:\\Users\\BC\\AppData\\Local\\conda\\conda\\envs\\tf1.4\\lib\\copy.py\", line 155, in deepcopy\r\n    y = copier(x, memo)\r\n\r\n  File \"C:\\Users\\BC\\AppData\\Local\\conda\\conda\\envs\\tf1.4\\lib\\copy.py\", line 243, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n\r\n  File \"C:\\Users\\BC\\AppData\\Local\\conda\\conda\\envs\\tf1.4\\lib\\copy.py\", line 174, in deepcopy\r\n    rv = reductor(4)\r\n\r\nTypeError: cannot serialize '_io.TextIOWrapper' object", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Thank you for your email. I think I have now answered your questions. can you help get this code working?\n\nRgds\nBen\n07971 577243\nwww.linkedin.com/in/bencosh/\n\n> On 24 Jan 2018, at 13:11, Alfred <notifications@github.com> wrote:\n> \n> Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\n> Have I written custom code\n> OS Platform and Distribution\n> TensorFlow installed from\n> TensorFlow version\n> Bazel version\n> CUDA/cuDNN version\n> GPU model and memory\n> Exact command to reproduce\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n> \n", "@xiejw can you comment? I'm not sure if legacy_seq2seq is expected to still work...", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Sorry did not notice I was mentioned. I was not the owner. ebrevdo@ should know who's the best person. ", "Nagging Assignee @ebrevdo: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ebrevdo: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ebrevdo: It has been 52 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ebrevdo: It has been 66 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ebrevdo: It has been 81 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ebrevdo: It has been 96 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I believe this has to do with the legacy code not supporting reuse of a layer that's already been used. Since it's legacy I don't think anyone is actively working to fix these issues. See tf.contrib.seq2seq for a cleaner approach to seq2seq models."]}, {"number": 16328, "title": "Gradient computation across multi-GPU", "body": "\r\n------------------------\r\n\r\n### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.4\r\n- **Python version**: 2.7.6\r\n- **CUDA/cuDNN version**: 8.0/6.0\r\n\r\nI am trying to compute global mean and global variance for batch normalization layer across GPUs, both forward and backward should be considered. With `\\sigma^2 = mean(x^2) - mean(x)^2`, the gradient w.r.t. each `x` can be computed independently in the GPU that `x` is attached to. \r\n\r\nHowever, when computing the gradients, I met a problem: without specifying GPU device, `tf.gradient` will use the `\\gpu:0`. I cannot specify each operation of gradient computation because the gradients are computed automatically by the `optimizer` and only gradients of parameters are computed.\r\n\r\nMy question is that if a node is explicitly attached to a GPU device, why the gradient can not be attached to the same GPU device?\r\n\r\nI tried this code and get two timeline files [timelines.zip](https://github.com/tensorflow/tensorflow/files/1649923/timelines.zip) and two snapshots bellow.\r\n\r\n    import tensorflow as tf\r\n    import numpy as np\r\n    from tensorflow.python.client import timeline\r\n    \r\n    N_SAMPLES = 100000000\r\n    \r\n    \r\n    def all_reduce(gpu_num):\r\n        means = []\r\n        x2s = []\r\n        axs = []\r\n        for i in range(gpu_num):\r\n            with tf.device('/cpu:0'):\r\n                x = tf.placeholder(dtype=tf.float32, shape=[N_SAMPLES], name='local_input_%d' % i)\r\n            with tf.device('/gpu:%d'%i):\r\n                ax = tf.multiply(10.0, x, name='local_multiply_%d'%i)\r\n                mean = tf.reduce_mean(ax, name='local_mean_%d'%i)\r\n                x2 = tf.square(ax, name='local_square_%d'%i)\r\n                axs.append(ax)\r\n                means.append(mean)\r\n                x2s.append(x2)\r\n    \r\n        with tf.device('/gpu:0'):\r\n            global_mean = tf.reduce_mean(means, name='global_mean')\r\n            global_var = tf.subtract(tf.reduce_mean(x2s, name='global_x2'),\r\n                                     tf.square(global_mean, name='global_mean_square'),\r\n                                     name='global_sub')\r\n            print global_var.get_shape()\r\n    \r\n        gs = []\r\n        # manually\r\n        # for i in range(gpu_num):\r\n        #     with tf.device('/gpu:%d'%i):\r\n        #         gradient_wrt_mean = tf.gradients(global_mean, axs[i])\r\n        #         gradient_wrt_var = tf.gradients(global_var, axs[i])\r\n        #         gs.append(gradient_wrt_mean)\r\n        #         gs.append(gradient_wrt_var)\r\n    \r\n        # auto by tf\r\n        gradient_wrt_mean = tf.gradients(global_mean, axs)\r\n        gradient_wrt_var = tf.gradients(global_var, axs)\r\n        gs.append(gradient_wrt_var)\r\n        gs.append(gradient_wrt_mean)\r\n    \r\n        for n in tf.get_default_graph().as_graph_def().node:\r\n            print [n.name, n.device]\r\n    \r\n        return global_mean, global_var, axs, gs\r\n    \r\n    \r\n    def main(_):\r\n        gpu_num = 2\r\n        mean_op, var_op, xs, gs = all_reduce(gpu_num)\r\n        x = np.random.randn(N_SAMPLES*gpu_num)\r\n        print np.mean(x), np.var(x)\r\n        feed_dict = dict()\r\n        for i in range(gpu_num):\r\n            feed_dict[xs[i]] = x[i*N_SAMPLES:(i+1)*N_SAMPLES]\r\n    \r\n        run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\n        run_metadata = tf.RunMetadata()\r\n        gpu_options = tf.GPUOptions(allow_growth=False)\r\n        config = tf.ConfigProto(log_device_placement=False, gpu_options=gpu_options)\r\n        sess = tf.Session(config=config)\r\n    \r\n        # mean, var, g = sess.run([\r\n        #     mean_op, var_op, gs\r\n        # ], feed_dict=feed_dict, options=run_options, run_metadata=run_metadata)\r\n        # print mean, var\r\n    \r\n        g = sess.run([\r\n            gs\r\n        ], feed_dict=feed_dict, options=run_options, run_metadata=run_metadata)\r\n    \r\n        # Create the Timeline object, and write it to a json\r\n        tl = timeline.Timeline(run_metadata.step_stats)\r\n        ctf = tl.generate_chrome_trace_format()\r\n        with open('timeline.json', 'w') as f:\r\n            f.write(ctf)\r\n    \r\n    \r\n    if __name__ == '__main__':\r\n        tf.app.run()\r\n\r\nTwo figures:\r\nauto, without specifying GPU device.\r\n![image](https://user-images.githubusercontent.com/13829174/35196526-76c1fa20-fed3-11e7-995f-6c63813acc83.png)\r\n\r\nmanually specifying GPU device.\r\n![image](https://user-images.githubusercontent.com/13829174/35196537-93757eee-fed3-11e7-8df5-986a90a8c0f8.png)\r\n\r\nIf using `tf.gradient` without specifying GPU devices, only a `tf.reduce_mean` operation is done in `/gpu:1`. So is there some easy way that the operations of gradient computation can be assigned automatically to the corresponded GPU device?\r\n\r\n\r\n", "comments": ["https://www.tensorflow.org/api_docs/python/tf/gradients\r\n\r\ntf.gradients(\r\n    ys,\r\n    xs,\r\n    grad_ys=None,\r\n    name='gradients',\r\n    __colocate_gradients_with_ops=False,__\r\n    gate_gradients=False,\r\n    aggregation_method=None,\r\n    stop_gradients=None\r\n)\r\n\r\ncolocate_gradients_with_ops: If True, try colocating gradients with the corresponding op.", "I didn't understand that colocate means the same device... but thanks very much, a problem is solved, with a so simple trick!"]}, {"number": 16327, "title": "iOS does not support a simple tensorflow network", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra 10.13.1\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.4.1\r\n- **Python version**: 2.7.10\r\n- **Bazel version (if compiling from source)**: 0.9.0-homebrew\r\n- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 9.0.0 (clang-900.0.39.2)\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A \r\n\r\n\r\n### Steps I followed:\r\n\r\n- I have trained resnet_v2_50 using slim.\r\n\r\n- I created a script just to run inference, so the input image is a placeholder with name \"input_1\" and the output is the softmax with name \"softmax\". \r\n\r\n- I exported the .pb graph, then I ran \r\n```\r\npython python/tools/freeze_graph.py --input_graph=resnet_v2_50.pb --input_checkpoint=model.ckpt-1 --output_graph=frozen_resnet_v2_50.pb --input_binary=True --output_node_names=\"softmax\"\r\n\r\n``` \r\nto freeze my graph using my checkpoint.\r\n\r\n- I ran \r\n```\r\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph  --inputs=input_1 --in_graph=frozen_resnet_v2_50.pb --outputs=softmax --out_graph=quantized_resnet_v2_50.pb --transforms='add_default_attributes strip_unused_nodes(type=float, shape=\"1,180,180,3\") remove_nodes(op=Identity, op=CheckNumerics) fold_constants(ignore_errors=true) quantize_weights strip_unused_nodes sort_by_execution_order'\r\n\r\n```\r\n to quantize it.\r\n\r\n- I imported the graph and also my labels file into `tensorflow/examples/ios/camera/data` and I ran the model on my iphone 5s (ios 10.3) using xcode 9.2. \r\n\r\n### Problem:\r\nThe app is running on my iphone but I am getting wrong labels and probabilities while the `tensorflow/examples/label_image/label_image.py` script gives me right results. I tested also the same image using label_image.py and xcode simulator and the prediction was different. Is there any bug? I changed this snippet because I don't need normalization of the image and it's right when running with python:\r\n\r\n```\r\nconst int wanted_input_width = 180;\r\nconst int wanted_input_height = 180;\r\nconst int wanted_input_channels = 3;\r\nconst float input_mean = 0.0f;\r\nconst float input_std = 1.0f;\r\n```\r\n\r\nYou can see the node names and the operations of resnet 50 below (here the output node is v/tower_0/resnet_v2_50/predictions/Reshape_1):\r\n\r\ninput_1=>Placeholder\r\nv/tower_0/Reshape/shape=>Const\r\nv/tower_0/Reshape=>Reshape\r\nv/tower_0/split/split_dim=>Const\r\nv/tower_0/split=>Split\r\nv/tower_0/sub/y=>Const\r\nv/tower_0/sub=>Sub\r\nv/tower_0/sub_1/y=>Const\r\nv/tower_0/sub_1=>Sub\r\nv/tower_0/sub_2/y=>Const\r\nv/tower_0/sub_2=>Sub\r\nv/tower_0/concat/axis=>Const\r\nv/tower_0/concat=>ConcatV2\r\nv/tower_0/Reshape_1/shape=>Const\r\nv/tower_0/Reshape_1=>Reshape\r\nv/tower_0/resnet_v2_50/Pad/paddings=>Const\r\nv/tower_0/resnet_v2_50/Pad=>Pad\r\nv/resnet_v2_50/conv1/weights_quantized_max=>Const\r\nv/resnet_v2_50/conv1/weights_quantized_min=>Const\r\nv/resnet_v2_50/conv1/weights_quantized_const=>Const\r\nv/resnet_v2_50/conv1/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/conv1/Conv2D=>Conv2D\r\nv/resnet_v2_50/conv1/biases=>Const\r\nv/tower_0/resnet_v2_50/conv1/BiasAdd=>BiasAdd\r\nv/tower_0/resnet_v2_50/pool1/MaxPool=>MaxPool\r\nv/resnet_v2_50/block1/unit_1/bottleneck_v2/preact/gamma=>Const\r\nv/resnet_v2_50/block1/unit_1/bottleneck_v2/preact/beta=>Const\r\nv/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/preact/Const=>Const\r\nv/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/preact/Const_1=>Const\r\nv/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm\r\nv/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/preact/Relu=>Relu\r\nv/resnet_v2_50/block1/unit_1/bottleneck_v2/shortcut/weights_quantized_max=>Const\r\nv/resnet_v2_50/block1/unit_1/bottleneck_v2/shortcut/weights_quantized_min=>Const\r\nv/resnet_v2_50/block1/unit_1/bottleneck_v2/shortcut/weights_quantized_const=>Const\r\nv/resnet_v2_50/block1/unit_1/bottleneck_v2/shortcut/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/shortcut/Conv2D=>Conv2D\r\nv/resnet_v2_50/block1/unit_1/bottleneck_v2/shortcut/biases=>Const\r\nv/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/shortcut/BiasAdd=>BiasAdd\r\nv/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/weights_quantized_max=>Const\r\nv/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/weights_quantized_min=>Const\r\nv/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/weights_quantized_const=>Const\r\nv/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/Conv2D=>Conv2D\r\nv/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/BatchNorm/gamma=>Const\r\nv/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/BatchNorm/beta=>Const\r\nv/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/BatchNorm/Const=>Const\r\nv/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/BatchNorm/Const_1=>Const\r\nv/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm\r\nv/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/Relu=>Relu\r\nv/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/weights_quantized_max=>Const\r\nv/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/weights_quantized_min=>Const\r\nv/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/weights_quantized_const=>Const\r\nv/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/Conv2D=>Conv2D\r\nv/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/BatchNorm/gamma=>Const\r\nv/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/BatchNorm/beta=>Const\r\nv/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/BatchNorm/Const=>Const\r\nv/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/BatchNorm/Const_1=>Const\r\nv/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm\r\nv/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/Relu=>Relu\r\nv/resnet_v2_50/block1/unit_1/bottleneck_v2/conv3/weights_quantized_max=>Const\r\nv/resnet_v2_50/block1/unit_1/bottleneck_v2/conv3/weights_quantized_min=>Const\r\nv/resnet_v2_50/block1/unit_1/bottleneck_v2/conv3/weights_quantized_const=>Const\r\nv/resnet_v2_50/block1/unit_1/bottleneck_v2/conv3/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/conv3/Conv2D=>Conv2D\r\nv/resnet_v2_50/block1/unit_1/bottleneck_v2/conv3/biases=>Const\r\nv/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/conv3/BiasAdd=>BiasAdd\r\nv/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/add=>Add\r\nv/resnet_v2_50/block1/unit_2/bottleneck_v2/preact/gamma=>Const\r\nv/resnet_v2_50/block1/unit_2/bottleneck_v2/preact/beta=>Const\r\nv/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/preact/Const=>Const\r\nv/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/preact/Const_1=>Const\r\nv/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm\r\nv/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/preact/Relu=>Relu\r\nv/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/weights_quantized_max=>Const\r\nv/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/weights_quantized_min=>Const\r\nv/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/weights_quantized_const=>Const\r\nv/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/Conv2D=>Conv2D\r\nv/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/BatchNorm/gamma=>Const\r\nv/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/BatchNorm/beta=>Const\r\nv/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/BatchNorm/Const=>Const\r\nv/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/BatchNorm/Const_1=>Const\r\nv/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm\r\nv/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/Relu=>Relu\r\nv/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/weights_quantized_max=>Const\r\nv/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/weights_quantized_min=>Const\r\nv/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/weights_quantized_const=>Const\r\nv/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/Conv2D=>Conv2D\r\nv/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/BatchNorm/gamma=>Const\r\nv/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/BatchNorm/beta=>Const\r\nv/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/BatchNorm/Const=>Const\r\nv/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/BatchNorm/Const_1=>Const\r\nv/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm\r\nv/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/Relu=>Relu\r\nv/resnet_v2_50/block1/unit_2/bottleneck_v2/conv3/weights_quantized_max=>Const\r\nv/resnet_v2_50/block1/unit_2/bottleneck_v2/conv3/weights_quantized_min=>Const\r\nv/resnet_v2_50/block1/unit_2/bottleneck_v2/conv3/weights_quantized_const=>Const\r\nv/resnet_v2_50/block1/unit_2/bottleneck_v2/conv3/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/conv3/Conv2D=>Conv2D\r\nv/resnet_v2_50/block1/unit_2/bottleneck_v2/conv3/biases=>Const\r\nv/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/conv3/BiasAdd=>BiasAdd\r\nv/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/add=>Add\r\nv/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/shortcut/MaxPool=>MaxPool\r\nv/resnet_v2_50/block1/unit_3/bottleneck_v2/preact/gamma=>Const\r\nv/resnet_v2_50/block1/unit_3/bottleneck_v2/preact/beta=>Const\r\nv/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/preact/Const=>Const\r\nv/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/preact/Const_1=>Const\r\nv/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm\r\nv/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/preact/Relu=>Relu\r\nv/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/weights_quantized_max=>Const\r\nv/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/weights_quantized_min=>Const\r\nv/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/weights_quantized_const=>Const\r\nv/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/Conv2D=>Conv2D\r\nv/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/BatchNorm/gamma=>Const\r\nv/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/BatchNorm/beta=>Const\r\nv/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/BatchNorm/Const=>Const\r\nv/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/BatchNorm/Const_1=>Const\r\nv/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm\r\nv/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/Relu=>Relu\r\nv/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/Pad/paddings=>Const\r\nv/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/Pad=>Pad\r\nv/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/weights_quantized_max=>Const\r\nv/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/weights_quantized_min=>Const\r\nv/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/weights_quantized_const=>Const\r\nv/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/Conv2D=>Conv2D\r\nv/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/BatchNorm/gamma=>Const\r\nv/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/BatchNorm/beta=>Const\r\nv/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/BatchNorm/Const=>Const\r\nv/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/BatchNorm/Const_1=>Const\r\nv/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm\r\nv/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/Relu=>Relu\r\nv/resnet_v2_50/block1/unit_3/bottleneck_v2/conv3/weights_quantized_max=>Const\r\nv/resnet_v2_50/block1/unit_3/bottleneck_v2/conv3/weights_quantized_min=>Const\r\nv/resnet_v2_50/block1/unit_3/bottleneck_v2/conv3/weights_quantized_const=>Const\r\nv/resnet_v2_50/block1/unit_3/bottleneck_v2/conv3/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/conv3/Conv2D=>Conv2D\r\nv/resnet_v2_50/block1/unit_3/bottleneck_v2/conv3/biases=>Const\r\nv/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/conv3/BiasAdd=>BiasAdd\r\nv/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/add=>Add\r\nv/resnet_v2_50/block2/unit_1/bottleneck_v2/preact/gamma=>Const\r\nv/resnet_v2_50/block2/unit_1/bottleneck_v2/preact/beta=>Const\r\nv/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/preact/Const=>Const\r\nv/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/preact/Const_1=>Const\r\nv/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm\r\nv/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/preact/Relu=>Relu\r\nv/resnet_v2_50/block2/unit_1/bottleneck_v2/shortcut/weights_quantized_max=>Const\r\nv/resnet_v2_50/block2/unit_1/bottleneck_v2/shortcut/weights_quantized_min=>Const\r\nv/resnet_v2_50/block2/unit_1/bottleneck_v2/shortcut/weights_quantized_const=>Const\r\nv/resnet_v2_50/block2/unit_1/bottleneck_v2/shortcut/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/shortcut/Conv2D=>Conv2D\r\nv/resnet_v2_50/block2/unit_1/bottleneck_v2/shortcut/biases=>Const\r\nv/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/shortcut/BiasAdd=>BiasAdd\r\nv/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/weights_quantized_max=>Const\r\nv/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/weights_quantized_min=>Const\r\nv/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/weights_quantized_const=>Const\r\nv/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/Conv2D=>Conv2D\r\nv/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/BatchNorm/gamma=>Const\r\nv/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/BatchNorm/beta=>Const\r\nv/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/BatchNorm/Const=>Const\r\nv/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/BatchNorm/Const_1=>Const\r\nv/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm\r\nv/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/Relu=>Relu\r\nv/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/weights_quantized_max=>Const\r\nv/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/weights_quantized_min=>Const\r\nv/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/weights_quantized_const=>Const\r\nv/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/Conv2D=>Conv2D\r\nv/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/BatchNorm/gamma=>Const\r\nv/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/BatchNorm/beta=>Const\r\nv/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/BatchNorm/Const=>Const\r\nv/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/BatchNorm/Const_1=>Const\r\nv/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm\r\nv/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/Relu=>Relu\r\nv/resnet_v2_50/block2/unit_1/bottleneck_v2/conv3/weights_quantized_max=>Const\r\nv/resnet_v2_50/block2/unit_1/bottleneck_v2/conv3/weights_quantized_min=>Const\r\nv/resnet_v2_50/block2/unit_1/bottleneck_v2/conv3/weights_quantized_const=>Const\r\nv/resnet_v2_50/block2/unit_1/bottleneck_v2/conv3/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/conv3/Conv2D=>Conv2D\r\nv/resnet_v2_50/block2/unit_1/bottleneck_v2/conv3/biases=>Const\r\nv/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/conv3/BiasAdd=>BiasAdd\r\nv/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/add=>Add\r\nv/resnet_v2_50/block2/unit_2/bottleneck_v2/preact/gamma=>Const\r\nv/resnet_v2_50/block2/unit_2/bottleneck_v2/preact/beta=>Const\r\nv/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/preact/Const=>Const\r\nv/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/preact/Const_1=>Const\r\nv/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm\r\nv/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/preact/Relu=>Relu\r\nv/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/weights_quantized_max=>Const\r\nv/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/weights_quantized_min=>Const\r\nv/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/weights_quantized_const=>Const\r\nv/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/Conv2D=>Conv2D\r\nv/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/BatchNorm/gamma=>Const\r\nv/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/BatchNorm/beta=>Const\r\nv/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/BatchNorm/Const=>Const\r\nv/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/BatchNorm/Const_1=>Const\r\nv/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm\r\nv/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/Relu=>Relu\r\nv/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/weights_quantized_max=>Const\r\nv/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/weights_quantized_min=>Const\r\nv/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/weights_quantized_const=>Const\r\nv/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/Conv2D=>Conv2D\r\nv/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/BatchNorm/gamma=>Const\r\nv/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/BatchNorm/beta=>Const\r\nv/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/BatchNorm/Const=>Const\r\nv/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/BatchNorm/Const_1=>Const\r\nv/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm\r\nv/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/Relu=>Relu\r\nv/resnet_v2_50/block2/unit_2/bottleneck_v2/conv3/weights_quantized_max=>Const\r\nv/resnet_v2_50/block2/unit_2/bottleneck_v2/conv3/weights_quantized_min=>Const\r\nv/resnet_v2_50/block2/unit_2/bottleneck_v2/conv3/weights_quantized_const=>Const\r\nv/resnet_v2_50/block2/unit_2/bottleneck_v2/conv3/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/conv3/Conv2D=>Conv2D\r\nv/resnet_v2_50/block2/unit_2/bottleneck_v2/conv3/biases=>Const\r\nv/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/conv3/BiasAdd=>BiasAdd\r\nv/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/add=>Add\r\nv/resnet_v2_50/block2/unit_3/bottleneck_v2/preact/gamma=>Const\r\nv/resnet_v2_50/block2/unit_3/bottleneck_v2/preact/beta=>Const\r\nv/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/preact/Const=>Const\r\nv/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/preact/Const_1=>Const\r\nv/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm\r\nv/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/preact/Relu=>Relu\r\nv/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/weights_quantized_max=>Const\r\nv/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/weights_quantized_min=>Const\r\nv/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/weights_quantized_const=>Const\r\nv/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/Conv2D=>Conv2D\r\nv/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/BatchNorm/gamma=>Const\r\nv/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/BatchNorm/beta=>Const\r\nv/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/BatchNorm/Const=>Const\r\nv/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/BatchNorm/Const_1=>Const\r\nv/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm\r\nv/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/Relu=>Relu\r\nv/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/weights_quantized_max=>Const\r\nv/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/weights_quantized_min=>Const\r\nv/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/weights_quantized_const=>Const\r\nv/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/Conv2D=>Conv2D\r\nv/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/BatchNorm/gamma=>Const\r\nv/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/BatchNorm/beta=>Const\r\nv/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/BatchNorm/Const=>Const\r\nv/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/BatchNorm/Const_1=>Const\r\nv/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm\r\nv/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/Relu=>Relu\r\nv/resnet_v2_50/block2/unit_3/bottleneck_v2/conv3/weights_quantized_max=>Const\r\nv/resnet_v2_50/block2/unit_3/bottleneck_v2/conv3/weights_quantized_min=>Const\r\nv/resnet_v2_50/block2/unit_3/bottleneck_v2/conv3/weights_quantized_const=>Const\r\nv/resnet_v2_50/block2/unit_3/bottleneck_v2/conv3/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/conv3/Conv2D=>Conv2D\r\nv/resnet_v2_50/block2/unit_3/bottleneck_v2/conv3/biases=>Const\r\nv/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/conv3/BiasAdd=>BiasAdd\r\nv/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/add=>Add\r\nv/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/shortcut/MaxPool=>MaxPool\r\nv/resnet_v2_50/block2/unit_4/bottleneck_v2/preact/gamma=>Const\r\nv/resnet_v2_50/block2/unit_4/bottleneck_v2/preact/beta=>Const\r\nv/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/preact/Const=>Const\r\nv/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/preact/Const_1=>Const\r\nv/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm\r\nv/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/preact/Relu=>Relu\r\nv/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/weights_quantized_max=>Const\r\nv/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/weights_quantized_min=>Const\r\nv/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/weights_quantized_const=>Const\r\nv/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/Conv2D=>Conv2D\r\nv/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/BatchNorm/gamma=>Const\r\nv/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/BatchNorm/beta=>Const\r\nv/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/BatchNorm/Const=>Const\r\nv/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/BatchNorm/Const_1=>Const\r\nv/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm\r\nv/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/Relu=>Relu\r\nv/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/Pad/paddings=>Const\r\nv/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/Pad=>Pad\r\nv/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/weights_quantized_max=>Const\r\nv/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/weights_quantized_min=>Const\r\nv/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/weights_quantized_const=>Const\r\nv/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/Conv2D=>Conv2D\r\nv/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/BatchNorm/gamma=>Const\r\nv/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/BatchNorm/beta=>Const\r\nv/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/BatchNorm/Const=>Const\r\nv/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/BatchNorm/Const_1=>Const\r\nv/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm\r\nv/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/Relu=>Relu\r\nv/resnet_v2_50/block2/unit_4/bottleneck_v2/conv3/weights_quantized_max=>Const\r\nv/resnet_v2_50/block2/unit_4/bottleneck_v2/conv3/weights_quantized_min=>Const\r\nv/resnet_v2_50/block2/unit_4/bottleneck_v2/conv3/weights_quantized_const=>Const\r\nv/resnet_v2_50/block2/unit_4/bottleneck_v2/conv3/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/conv3/Conv2D=>Conv2D\r\nv/resnet_v2_50/block2/unit_4/bottleneck_v2/conv3/biases=>Const\r\nv/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/conv3/BiasAdd=>BiasAdd\r\nv/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/add=>Add\r\nv/resnet_v2_50/block3/unit_1/bottleneck_v2/preact/gamma=>Const\r\nv/resnet_v2_50/block3/unit_1/bottleneck_v2/preact/beta=>Const\r\nv/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/preact/Const=>Const\r\nv/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/preact/Const_1=>Const\r\nv/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm\r\nv/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/preact/Relu=>Relu\r\nv/resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/weights_quantized_max=>Const\r\nv/resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/weights_quantized_min=>Const\r\nv/resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/weights_quantized_const=>Const\r\nv/resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/Conv2D=>Conv2D\r\nv/resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/biases_quantized_max=>Const\r\nv/resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/biases_quantized_min=>Const\r\nv/resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/biases_quantized_const=>Const\r\nv/resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/biases=>Dequantize\r\nv/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/BiasAdd=>BiasAdd\r\nv/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/weights_quantized_max=>Const\r\nv/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/weights_quantized_min=>Const\r\nv/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/weights_quantized_const=>Const\r\nv/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/Conv2D=>Conv2D\r\nv/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/BatchNorm/gamma=>Const\r\nv/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/BatchNorm/beta=>Const\r\nv/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/BatchNorm/Const=>Const\r\nv/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/BatchNorm/Const_1=>Const\r\nv/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm\r\nv/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/Relu=>Relu\r\nv/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/weights_quantized_max=>Const\r\nv/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/weights_quantized_min=>Const\r\nv/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/weights_quantized_const=>Const\r\nv/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/Conv2D=>Conv2D\r\nv/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/BatchNorm/gamma=>Const\r\nv/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/BatchNorm/beta=>Const\r\nv/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/BatchNorm/Const=>Const\r\nv/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/BatchNorm/Const_1=>Const\r\nv/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm\r\nv/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/Relu=>Relu\r\nv/resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/weights_quantized_max=>Const\r\nv/resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/weights_quantized_min=>Const\r\nv/resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/weights_quantized_const=>Const\r\nv/resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/Conv2D=>Conv2D\r\nv/resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/biases_quantized_max=>Const\r\nv/resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/biases_quantized_min=>Const\r\nv/resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/biases_quantized_const=>Const\r\nv/resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/biases=>Dequantize\r\nv/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/BiasAdd=>BiasAdd\r\nv/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/add=>Add\r\nv/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/gamma_quantized_max=>Const\r\nv/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/gamma_quantized_min=>Const\r\nv/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/gamma_quantized_const=>Const\r\nv/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/gamma=>Dequantize\r\nv/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/beta_quantized_max=>Const\r\nv/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/beta_quantized_min=>Const\r\nv/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/beta_quantized_const=>Const\r\nv/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/beta=>Dequantize\r\nv/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/Const=>Const\r\nv/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/Const_1=>Const\r\nv/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm\r\nv/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/Relu=>Relu\r\nv/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/weights_quantized_max=>Const\r\nv/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/weights_quantized_min=>Const\r\nv/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/weights_quantized_const=>Const\r\nv/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/Conv2D=>Conv2D\r\nv/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/BatchNorm/gamma=>Const\r\nv/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/BatchNorm/beta=>Const\r\nv/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/BatchNorm/Const=>Const\r\nv/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/BatchNorm/Const_1=>Const\r\nv/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm\r\nv/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/Relu=>Relu\r\nv/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/weights_quantized_max=>Const\r\nv/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/weights_quantized_min=>Const\r\nv/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/weights_quantized_const=>Const\r\nv/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/Conv2D=>Conv2D\r\nv/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/BatchNorm/gamma=>Const\r\nv/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/BatchNorm/beta=>Const\r\nv/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/BatchNorm/Const=>Const\r\nv/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/BatchNorm/Const_1=>Const\r\nv/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm\r\nv/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/Relu=>Relu\r\nv/resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/weights_quantized_max=>Const\r\nv/resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/weights_quantized_min=>Const\r\nv/resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/weights_quantized_const=>Const\r\nv/resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/Conv2D=>Conv2D\r\nv/resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/biases_quantized_max=>Const\r\nv/resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/biases_quantized_min=>Const\r\nv/resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/biases_quantized_const=>Const\r\nv/resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/biases=>Dequantize\r\nv/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/BiasAdd=>BiasAdd\r\nv/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/add=>Add\r\nv/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/gamma_quantized_max=>Const\r\nv/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/gamma_quantized_min=>Const\r\nv/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/gamma_quantized_const=>Const\r\nv/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/gamma=>Dequantize\r\nv/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/beta_quantized_max=>Const\r\nv/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/beta_quantized_min=>Const\r\nv/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/beta_quantized_const=>Const\r\nv/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/beta=>Dequantize\r\nv/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/Const=>Const\r\nv/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/Const_1=>Const\r\nv/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm\r\nv/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/Relu=>Relu\r\nv/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/weights_quantized_max=>Const\r\nv/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/weights_quantized_min=>Const\r\nv/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/weights_quantized_const=>Const\r\nv/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/Conv2D=>Conv2D\r\nv/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/BatchNorm/gamma=>Const\r\nv/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/BatchNorm/beta=>Const\r\nv/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/BatchNorm/Const=>Const\r\nv/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/BatchNorm/Const_1=>Const\r\nv/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm\r\nv/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/Relu=>Relu\r\nv/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/weights_quantized_max=>Const\r\nv/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/weights_quantized_min=>Const\r\nv/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/weights_quantized_const=>Const\r\nv/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/Conv2D=>Conv2D\r\nv/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/BatchNorm/gamma=>Const\r\nv/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/BatchNorm/beta=>Const\r\nv/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/BatchNorm/Const=>Const\r\nv/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/BatchNorm/Const_1=>Const\r\nv/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm\r\nv/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/Relu=>Relu\r\nv/resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/weights_quantized_max=>Const\r\nv/resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/weights_quantized_min=>Const\r\nv/resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/weights_quantized_const=>Const\r\nv/resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/Conv2D=>Conv2D\r\nv/resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/biases_quantized_max=>Const\r\nv/resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/biases_quantized_min=>Const\r\nv/resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/biases_quantized_const=>Const\r\nv/resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/biases=>Dequantize\r\nv/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/BiasAdd=>BiasAdd\r\nv/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/add=>Add\r\nv/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/gamma_quantized_max=>Const\r\nv/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/gamma_quantized_min=>Const\r\nv/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/gamma_quantized_const=>Const\r\nv/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/gamma=>Dequantize\r\nv/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/beta_quantized_max=>Const\r\nv/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/beta_quantized_min=>Const\r\nv/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/beta_quantized_const=>Const\r\nv/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/beta=>Dequantize\r\nv/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/Const=>Const\r\nv/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/Const_1=>Const\r\nv/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm\r\nv/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/Relu=>Relu\r\nv/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/weights_quantized_max=>Const\r\nv/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/weights_quantized_min=>Const\r\nv/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/weights_quantized_const=>Const\r\nv/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/Conv2D=>Conv2D\r\nv/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/BatchNorm/gamma=>Const\r\nv/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/BatchNorm/beta=>Const\r\nv/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/BatchNorm/Const=>Const\r\nv/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/BatchNorm/Const_1=>Const\r\nv/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm\r\nv/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/Relu=>Relu\r\nv/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/weights_quantized_max=>Const\r\nv/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/weights_quantized_min=>Const\r\nv/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/weights_quantized_const=>Const\r\nv/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/Conv2D=>Conv2D\r\nv/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/BatchNorm/gamma=>Const\r\nv/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/BatchNorm/beta=>Const\r\nv/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/BatchNorm/Const=>Const\r\nv/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/BatchNorm/Const_1=>Const\r\nv/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm\r\nv/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/Relu=>Relu\r\nv/resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/weights_quantized_max=>Const\r\nv/resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/weights_quantized_min=>Const\r\nv/resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/weights_quantized_const=>Const\r\nv/resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/Conv2D=>Conv2D\r\nv/resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/biases_quantized_max=>Const\r\nv/resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/biases_quantized_min=>Const\r\nv/resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/biases_quantized_const=>Const\r\nv/resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/biases=>Dequantize\r\nv/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/BiasAdd=>BiasAdd\r\nv/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/add=>Add\r\nv/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/gamma_quantized_max=>Const\r\nv/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/gamma_quantized_min=>Const\r\nv/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/gamma_quantized_const=>Const\r\nv/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/gamma=>Dequantize\r\nv/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/beta_quantized_max=>Const\r\nv/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/beta_quantized_min=>Const\r\nv/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/beta_quantized_const=>Const\r\nv/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/beta=>Dequantize\r\nv/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/Const=>Const\r\nv/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/Const_1=>Const\r\nv/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm\r\nv/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/Relu=>Relu\r\nv/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/weights_quantized_max=>Const\r\nv/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/weights_quantized_min=>Const\r\nv/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/weights_quantized_const=>Const\r\nv/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/Conv2D=>Conv2D\r\nv/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/BatchNorm/gamma=>Const\r\nv/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/BatchNorm/beta=>Const\r\nv/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/BatchNorm/Const=>Const\r\nv/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/BatchNorm/Const_1=>Const\r\nv/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm\r\nv/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/Relu=>Relu\r\nv/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/weights_quantized_max=>Const\r\nv/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/weights_quantized_min=>Const\r\nv/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/weights_quantized_const=>Const\r\nv/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/Conv2D=>Conv2D\r\nv/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/BatchNorm/gamma=>Const\r\nv/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/BatchNorm/beta=>Const\r\nv/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/BatchNorm/Const=>Const\r\nv/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/BatchNorm/Const_1=>Const\r\nv/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm\r\nv/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/Relu=>Relu\r\nv/resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/weights_quantized_max=>Const\r\nv/resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/weights_quantized_min=>Const\r\nv/resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/weights_quantized_const=>Const\r\nv/resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/Conv2D=>Conv2D\r\nv/resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/biases_quantized_max=>Const\r\nv/resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/biases_quantized_min=>Const\r\nv/resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/biases_quantized_const=>Const\r\nv/resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/biases=>Dequantize\r\nv/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/BiasAdd=>BiasAdd\r\nv/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/add=>Add\r\nv/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/shortcut/MaxPool=>MaxPool\r\nv/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/gamma_quantized_max=>Const\r\nv/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/gamma_quantized_min=>Const\r\nv/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/gamma_quantized_const=>Const\r\nv/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/gamma=>Dequantize\r\nv/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/beta_quantized_max=>Const\r\nv/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/beta_quantized_min=>Const\r\nv/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/beta_quantized_const=>Const\r\nv/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/beta=>Dequantize\r\nv/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/Const=>Const\r\nv/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/Const_1=>Const\r\nv/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm\r\nv/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/Relu=>Relu\r\nv/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/weights_quantized_max=>Const\r\nv/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/weights_quantized_min=>Const\r\nv/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/weights_quantized_const=>Const\r\nv/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/Conv2D=>Conv2D\r\nv/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/BatchNorm/gamma=>Const\r\nv/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/BatchNorm/beta=>Const\r\nv/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/BatchNorm/Const=>Const\r\nv/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/BatchNorm/Const_1=>Const\r\nv/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm\r\nv/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/Relu=>Relu\r\nv/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/Pad/paddings=>Const\r\nv/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/Pad=>Pad\r\nv/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/weights_quantized_max=>Const\r\nv/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/weights_quantized_min=>Const\r\nv/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/weights_quantized_const=>Const\r\nv/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/Conv2D=>Conv2D\r\nv/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/BatchNorm/gamma=>Const\r\nv/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/BatchNorm/beta=>Const\r\nv/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/BatchNorm/Const=>Const\r\nv/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/BatchNorm/Const_1=>Const\r\nv/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm\r\nv/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/Relu=>Relu\r\nv/resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/weights_quantized_max=>Const\r\nv/resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/weights_quantized_min=>Const\r\nv/resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/weights_quantized_const=>Const\r\nv/resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/Conv2D=>Conv2D\r\nv/resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/biases_quantized_max=>Const\r\nv/resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/biases_quantized_min=>Const\r\nv/resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/biases_quantized_const=>Const\r\nv/resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/biases=>Dequantize\r\nv/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/BiasAdd=>BiasAdd\r\nv/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/add=>Add\r\nv/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/gamma_quantized_max=>Const\r\nv/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/gamma_quantized_min=>Const\r\nv/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/gamma_quantized_const=>Const\r\nv/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/gamma=>Dequantize\r\nv/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/beta_quantized_max=>Const\r\nv/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/beta_quantized_min=>Const\r\nv/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/beta_quantized_const=>Const\r\nv/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/beta=>Dequantize\r\nv/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/Const=>Const\r\nv/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/Const_1=>Const\r\nv/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm\r\nv/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/Relu=>Relu\r\nv/resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/weights_quantized_max=>Const\r\nv/resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/weights_quantized_min=>Const\r\nv/resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/weights_quantized_const=>Const\r\nv/resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/Conv2D=>Conv2D\r\nv/resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/biases_quantized_max=>Const\r\nv/resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/biases_quantized_min=>Const\r\nv/resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/biases_quantized_const=>Const\r\nv/resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/biases=>Dequantize\r\nv/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/BiasAdd=>BiasAdd\r\nv/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/weights_quantized_max=>Const\r\nv/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/weights_quantized_min=>Const\r\nv/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/weights_quantized_const=>Const\r\nv/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/Conv2D=>Conv2D\r\nv/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/BatchNorm/gamma=>Const\r\nv/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/BatchNorm/beta=>Const\r\nv/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/BatchNorm/Const=>Const\r\nv/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/BatchNorm/Const_1=>Const\r\nv/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm\r\nv/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/Relu=>Relu\r\nv/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/weights_quantized_max=>Const\r\nv/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/weights_quantized_min=>Const\r\nv/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/weights_quantized_const=>Const\r\nv/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/Conv2D=>Conv2D\r\nv/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/BatchNorm/gamma=>Const\r\nv/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/BatchNorm/beta=>Const\r\nv/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/BatchNorm/Const=>Const\r\nv/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/BatchNorm/Const_1=>Const\r\nv/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm\r\nv/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/Relu=>Relu\r\nv/resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/weights_quantized_max=>Const\r\nv/resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/weights_quantized_min=>Const\r\nv/resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/weights_quantized_const=>Const\r\nv/resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/Conv2D=>Conv2D\r\nv/resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/biases_quantized_max=>Const\r\nv/resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/biases_quantized_min=>Const\r\nv/resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/biases_quantized_const=>Const\r\nv/resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/biases=>Dequantize\r\nv/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/BiasAdd=>BiasAdd\r\nv/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/add=>Add\r\nv/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/gamma_quantized_max=>Const\r\nv/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/gamma_quantized_min=>Const\r\nv/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/gamma_quantized_const=>Const\r\nv/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/gamma=>Dequantize\r\nv/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/beta_quantized_max=>Const\r\nv/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/beta_quantized_min=>Const\r\nv/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/beta_quantized_const=>Const\r\nv/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/beta=>Dequantize\r\nv/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/Const=>Const\r\nv/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/Const_1=>Const\r\nv/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm\r\nv/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/Relu=>Relu\r\nv/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/weights_quantized_max=>Const\r\nv/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/weights_quantized_min=>Const\r\nv/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/weights_quantized_const=>Const\r\nv/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/Conv2D=>Conv2D\r\nv/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/BatchNorm/gamma=>Const\r\nv/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/BatchNorm/beta=>Const\r\nv/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/BatchNorm/Const=>Const\r\nv/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/BatchNorm/Const_1=>Const\r\nv/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm\r\nv/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/Relu=>Relu\r\nv/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/weights_quantized_max=>Const\r\nv/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/weights_quantized_min=>Const\r\nv/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/weights_quantized_const=>Const\r\nv/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/Conv2D=>Conv2D\r\nv/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/BatchNorm/gamma=>Const\r\nv/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/BatchNorm/beta=>Const\r\nv/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/BatchNorm/Const=>Const\r\nv/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/BatchNorm/Const_1=>Const\r\nv/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm\r\nv/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/Relu=>Relu\r\nv/resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/weights_quantized_max=>Const\r\nv/resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/weights_quantized_min=>Const\r\nv/resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/weights_quantized_const=>Const\r\nv/resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/Conv2D=>Conv2D\r\nv/resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/biases_quantized_max=>Const\r\nv/resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/biases_quantized_min=>Const\r\nv/resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/biases_quantized_const=>Const\r\nv/resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/biases=>Dequantize\r\nv/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/BiasAdd=>BiasAdd\r\nv/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/add=>Add\r\nv/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/gamma_quantized_max=>Const\r\nv/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/gamma_quantized_min=>Const\r\nv/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/gamma_quantized_const=>Const\r\nv/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/gamma=>Dequantize\r\nv/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/beta_quantized_max=>Const\r\nv/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/beta_quantized_min=>Const\r\nv/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/beta_quantized_const=>Const\r\nv/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/beta=>Dequantize\r\nv/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/Const=>Const\r\nv/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/Const_1=>Const\r\nv/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm\r\nv/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/Relu=>Relu\r\nv/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/weights_quantized_max=>Const\r\nv/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/weights_quantized_min=>Const\r\nv/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/weights_quantized_const=>Const\r\nv/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/Conv2D=>Conv2D\r\nv/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/BatchNorm/gamma=>Const\r\nv/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/BatchNorm/beta=>Const\r\nv/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/BatchNorm/Const=>Const\r\nv/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/BatchNorm/Const_1=>Const\r\nv/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm\r\nv/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/Relu=>Relu\r\nv/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/weights_quantized_max=>Const\r\nv/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/weights_quantized_min=>Const\r\nv/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/weights_quantized_const=>Const\r\nv/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/Conv2D=>Conv2D\r\nv/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/BatchNorm/gamma=>Const\r\nv/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/BatchNorm/beta=>Const\r\nv/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/BatchNorm/Const=>Const\r\nv/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/BatchNorm/Const_1=>Const\r\nv/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm\r\nv/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/Relu=>Relu\r\nv/resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/weights_quantized_max=>Const\r\nv/resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/weights_quantized_min=>Const\r\nv/resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/weights_quantized_const=>Const\r\nv/resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/Conv2D=>Conv2D\r\nv/resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/biases_quantized_max=>Const\r\nv/resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/biases_quantized_min=>Const\r\nv/resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/biases_quantized_const=>Const\r\nv/resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/biases=>Dequantize\r\nv/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/BiasAdd=>BiasAdd\r\nv/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/add=>Add\r\nv/resnet_v2_50/postnorm/gamma_quantized_max=>Const\r\nv/resnet_v2_50/postnorm/gamma_quantized_min=>Const\r\nv/resnet_v2_50/postnorm/gamma_quantized_const=>Const\r\nv/resnet_v2_50/postnorm/gamma=>Dequantize\r\nv/resnet_v2_50/postnorm/beta_quantized_max=>Const\r\nv/resnet_v2_50/postnorm/beta_quantized_min=>Const\r\nv/resnet_v2_50/postnorm/beta_quantized_const=>Const\r\nv/resnet_v2_50/postnorm/beta=>Dequantize\r\nv/tower_0/resnet_v2_50/postnorm/Const=>Const\r\nv/tower_0/resnet_v2_50/postnorm/Const_1=>Const\r\nv/tower_0/resnet_v2_50/postnorm/FusedBatchNorm=>FusedBatchNorm\r\nv/tower_0/resnet_v2_50/postnorm/Relu=>Relu\r\nv/tower_0/resnet_v2_50/pool5/reduction_indices=>Const\r\nv/tower_0/resnet_v2_50/pool5=>Mean\r\nv/resnet_v2_50/logits/weights_quantized_max=>Const\r\nv/resnet_v2_50/logits/weights_quantized_min=>Const\r\nv/resnet_v2_50/logits/weights_quantized_const=>Const\r\nv/resnet_v2_50/logits/weights=>Dequantize\r\nv/tower_0/resnet_v2_50/logits/Conv2D=>Conv2D\r\nv/resnet_v2_50/logits/biases_quantized_max=>Const\r\nv/resnet_v2_50/logits/biases_quantized_min=>Const\r\nv/resnet_v2_50/logits/biases_quantized_const=>Const\r\nv/resnet_v2_50/logits/biases=>Dequantize\r\nv/tower_0/resnet_v2_50/logits/BiasAdd=>BiasAdd\r\nv/tower_0/resnet_v2_50/SpatialSqueeze=>Squeeze\r\nv/tower_0/resnet_v2_50/predictions/Reshape/shape=>Const\r\nv/tower_0/resnet_v2_50/predictions/Reshape=>Reshape\r\nv/tower_0/resnet_v2_50/predictions/Softmax=>Softmax\r\nv/tower_0/resnet_v2_50/predictions/Shape=>Const\r\nv/tower_0/resnet_v2_50/predictions/Reshape_1=>Reshape", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 16326, "title": "Making string values in constant", "body": "We already have a `constant.py` in session_bundle since adding these string values as a constant.", "comments": ["@rmlarsen One build check is failed. Is this a memory issue?", "@rajendraarora16 I assume the failure is unrelated to your change, but since I cannot find any logs, I'll run the tests again to be safe.", "Thanks @rmlarsen :)"]}, {"number": 16325, "title": "Removing os.path", "body": "", "comments": []}, {"number": 16324, "title": "Using math_ops instead of defining separate mulop function", "body": "", "comments": ["@hawkinsp thanks for reviewing."]}]