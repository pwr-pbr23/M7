[{"number": 44437, "title": "Cannot build TFLite flex:delegate package for armhf", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: tensorflow-gpu 2.3.0\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): bazel 3.5.0\r\n- GCC/Compiler version (if compiling from source):  gcc 9.3.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the problem**\r\n\r\nCannot build TFLite flex:delegate package for armhf. \r\n\r\nAttempting to build TFLite flex:delegate for armhf results in the following error:\r\n\r\n(base) trinity@trinity-blade:~/Documents/my-team/my-project/git/tensorflow$ bazel build -c opt --config=elinux_armhf --config=monolithic //tensorflow/lite/delegates/flex:delegate\r\nWARNING: Ignoring JAVA_HOME, because it must point to a JDK, not a JRE.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=211\r\nINFO: Reading rc options for 'build' from /home/trinity/Documents/my-team/my-project/git/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/trinity/Documents/my-team/my-project/git/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\nINFO: Reading rc options for 'build' from /home/trinity/Documents/my-team/my-project/git/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/home/trinity/miniconda3/bin/python3 --action_env PYTHON_LIB_PATH=/home/trinity/miniconda3/lib/python3.8/site-packages --python_path=/home/trinity/miniconda3/bin/python3 --config=xla\r\nINFO: Found applicable config definition build:short_logs in file /home/trinity/Documents/my-team/my-project/git/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/trinity/Documents/my-team/my-project/git/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file /home/trinity/Documents/my-team/my-project/git/tensorflow/.bazelrc: --define=with_xla_support=true\r\nINFO: Found applicable config definition build:elinux_armhf in file /home/trinity/Documents/my-team/my-project/git/tensorflow/.bazelrc: --config=elinux --cpu=armhf\r\nINFO: Found applicable config definition build:elinux in file /home/trinity/Documents/my-team/my-project/git/tensorflow/.bazelrc: --crosstool_top=@local_config_embedded_arm//:toolchain --host_crosstool_top=@bazel_tools//tools/cpp:toolchain\r\nINFO: Found applicable config definition build:monolithic in file /home/trinity/Documents/my-team/my-project/git/tensorflow/.bazelrc: --define framework_shared_object=false\r\nINFO: Found applicable config definition build:linux in file /home/trinity/Documents/my-team/my-project/git/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr/arm-linux-gnueabihf --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/trinity/Documents/my-team/my-project/git/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nINFO: Analyzed target //tensorflow/lite/delegates/flex:delegate (0 packages loaded, 0 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /home/trinity/.cache/bazel/_bazel_trinity/ec49567c9bd407ca66e6e2b6d734da8f/external/icu/BUILD.bazel:33:1: C++ compilation of rule '@icu//:icuuc' failed (Exit 1)\r\nIn file included from external/icu/icu4c/source/common/usetiter.cpp:10:\r\nexternal/icu/icu4c/source/common/unicode/uniset.h:504:29: error: invalid covariant return type for 'virtual icu_66::UnicodeFunctor* icu_66::UnicodeSet::clone() const'\r\n     virtual UnicodeFunctor* clone() const;\r\n                             ^~~~~\r\nIn file included from external/icu/icu4c/source/common/unicode/uniset.h:17,\r\n                 from external/icu/icu4c/source/common/usetiter.cpp:10:\r\n/usr/include/unicode/unifilt.h:80:28: note: overridden function is 'virtual icu_66::UnicodeFilter* icu_66::UnicodeFilter::clone() const'\r\n     virtual UnicodeFilter* clone() const = 0;\r\n                            ^~~~~\r\nTarget //tensorflow/lite/delegates/flex:delegate failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 12.828s, Critical Path: 12.51s\r\nINFO: 31 processes: 31 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nFrom tensorflow repo root:\r\n\r\n$ bazel build -c opt --config=elinux_armhf --config=monolithic //tensorflow/lite/delegates/flex:delegate\r\n\r\n.bazelrc is attached. \r\n[bazelrc.txt](https://github.com/tensorflow/tensorflow/files/5459724/bazelrc.txt)\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Please try it again on TF2.4 or higher. Please reopen this if you still have the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44437\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44437\">No</a>\n"]}, {"number": 44436, "title": "Add GPU implementation for tf.segment_* (prod&max&min)", "body": "This is a PR from JIZHI Team & TaiJi AI platform in Tencent.\r\n\r\nThe GPU kernel implements of a series of sorted segment reduction ops have been created, i.e. \r\n- `tf.segment_prod`\r\n- `tf.segment_max`\r\n- `tf.segment_min`\r\n\r\n\r\nSince `segment_mean` is not a direct binary reduction operation, different from `segment_prod` & `segment_min` & `segment_max`. Therefore, in the actual implementation, I temporarily implemented it separately as `SortedSegmentMeanCustomKernel`, which may cause some inelegance codes. According to @sanjoy 's suggestion, I will proposal its implementation through another pr.\r\n\r\n", "comments": ["@sanjoy Could you please take a look?", "Hi, @sanjoy  I have added `cuda_py_test` for `segment_reduction_ops_test` and the new gpu kernels were tested. Could you please check again?\r\n\r\nOutside of this pr, I noticed that in the current [GPU implementation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/segment_reduction_ops_impl.h#L233) of `segment_sum`, the input `segment_ids` are not checked whether is sorted, which will lead to an unpredictable result when the input is out of order. Thus, I would like to ask that is this a bug? Or is it due of performance considerations that only the user conventions are used to ensure sorted input?\r\n\r\n", "> General comment, can you please split out the implementation of segment mean from the rest into a separate PR? That will simplify the more straightforward non-mean patch.\r\n\r\n@sanjoy Thanks for your suggestion! \r\n\r\nI have updated this pr with the implementation of `segment_mean` splited out, and I will proposal another one for it later. Now this pr is simply for the direct binary reduction operation, i.e. `segment_prod` & `segment_min` & `segment_max`.", "@sanjoy Could you please have a look?", "@sanjoy @gbaned \r\nSorry for some careless compiling problems... I have fixed them already. Could you please check and add a `force-run` label again? Thanks!\r\n\r\n", "@sanjoy Could you please have a look?"]}, {"number": 44435, "title": "[ssd mobileNet v2 model] ValueError: Failed to parse the model: pybind11::init(): factory function returned nullptr.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Catalina\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (or github SHA if from source): 2.3.1\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\npython object_detection/export_tflite_graph_tf2.py \\\r\n    --pipeline_config_path ~/pretrained_models/ssd_mobilenet_v2_320x320_coco17_tpu-8/pipeline.config \\\r\n    --trained_checkpoint_dir~/pretrained_models/ssd_mobilenet_v2_320x320_coco17_tpu-8/checkpoint \\\r\n    --output_directory ~/output_dir\r\n```\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/jaribido/Library/Python/3.7/lib/python/site-packages/tensorflow/lite/python/optimize/calibrator.py\", line 52, in __init__\r\n    _calibration_wrapper.CalibrationWrapper(model_content))\r\nTypeError: pybind11::init(): factory function returned nullptr\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/user_name/Desktop/Dev/models/convert_file.py\", line 32, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"/Users/user_name/Library/Python/3.7/lib/python/site-packages/tensorflow/lite/python/lite.py\", line 710, in convert\r\n    output_tensors)\r\n  File \"/Users/user_name/Library/Python/3.7/lib/python/site-packages/tensorflow/lite/python/lite.py\", line 638, in convert\r\n    result = self._calibrate_quantize_model(result, **flags)\r\n  File \"/Users/user_name/Library/Python/3.7/lib/python/site-packages/tensorflow/lite/python/lite.py\", line 440, in _calibrate_quantize_model\r\n    calibrate_quantize = _calibrator.Calibrator(result)\r\n  File \"/Users/user_name/Library/Python/3.7/lib/python/site-packages/tensorflow/lite/python/optimize/calibrator.py\", line 54, in __init__\r\n    raise ValueError(\"Failed to parse the model: %s.\" % e)\r\nValueError: Failed to parse the model: pybind11::init(): factory function returned nullptr.\r\n\r\n```\r\n# Copy and paste the output here.\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\n# Put link here or attach to the issue.\r\n```\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- Producing wrong results and/or decrease in accuracy\r\n- Producing correct results, but the model is slower than expected (model generated from old converter)\r\n\r\n\r\n**RNN conversion support**\r\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n\r\n**Any other info / logs**\r\nI downloaded ssd mobileNet v2 model from tensorflow zoo. Then I generated the TFLite inference graph savedModel. After which I downloaded coco dataset images and ran the code for TFlite conversion to Full integer quantization. Then I got the error documented above.\r\n", "comments": ["@Aribido-Oluwaseun \r\n\r\nI have tried in colab with TF nightly version(`2.5.0-dev20201029`) and i am not seeing any issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/35fb92d229e046cb28f82f207a547706/untitled101.ipynb?authuser=1). Please, verify once and close the issue. Thanks!", "@ ravikyram, thanks for your reply, using that method gives no issue. Have you tried the full integer quantization?\r\n\r\nPlease try the following rather:\r\n```\r\nsaved_model_dir = '~/research/ssdmobilenet_dir/saved_model'\r\ndir = '~/dataset/test2017/'\r\ntflite_dir = '~/research/ssdmobilenet_dir/quantized_model/'\r\n\r\ntrain_images = tf.keras.preprocessing.image_dataset_from_directory(\r\n        directory=dir, labels='inferred', label_mode='int', class_names=None,\r\n        color_mode='rgb', batch_size=1)\r\n\r\ndef representative_data_gen():\r\n  for i in range(1000):\r\n    image = train_images[i]\r\n    image = tf.io.read_file(image)\r\n    image = tf.io.decode_jpeg(image, channels=3)\r\n    image = tf.image.resize(image, [320, 320])\r\n    image = tf.cast(image / 255., tf.float64)\r\n    image = tf.expand_dims(image, 0)\r\n    yield [image]\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\nconverter.representative_dataset = representative_data_gen\r\ntflite_model = converter.convert()\r\n\r\nwith open(tflite_dir+'model.tflite', 'wb') as f:\r\n  f.write(tflite_model)\r\n```\r\nPlease try loading images for the full integer quantization and verify if it works. The dynamic quantization works for me as well and this bug is not related to that. Thanks..\r\n", "@Aribido-Oluwaseun \r\nMy problem was similar: https://github.com/tensorflow/tensorflow/issues/44091\r\nSolution tensorflow == 2.2.0", "@Rariusz, thanks is it resolved for you?\r\n", "@Aribido-Oluwaseun \r\nYes, @amahendrakar help me to solve the problem however now I have still a problem with TFLite model. The model is reduced to 456 bytes !!!.  And so far I don't know why this is happening and how to solve it. I try different ways but the network is still so reduced.", "@Rariusz  Okay thanks, I will await a comprehensive solution on this bug.", "@Aribido-Oluwaseun You can install tensorflow 2.2. I know that install TensorFlow for object detection is very hard.\r\nI use conda distribution of python with virtual env. ", "@Aribido-Oluwaseun You need to add TFLITE_BUILTINS to `supported_ops`:\r\n```\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.OpsSet.TFLITE_BUILTINS]\r\n```\r\n\r\nFor the SSD model, the final op runs in float (with the rest of the graph in quantized). This, along with using the correct TF version should solve your issue (we started support for TF2 detection models in TFLite *after* TF2.3, so you might need to use the latest version/nightly for conversion).", "@Aribido-Oluwaseun ,\r\nCan you please take a look at above [comment](https://github.com/tensorflow/tensorflow/issues/44435#issuecomment-736742158) and also please try to check in latest stable version v2.6 and let us know if the issue still persists.Thanks", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44435\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44435\">No</a>\n"]}, {"number": 44434, "title": "Fix typos in misc files", "body": "Splitting #43857 by top-level directories.", "comments": []}, {"number": 44433, "title": "Fix typos in python directory", "body": "Splitting #43857 by top-level directories.", "comments": ["Check out this pull request on&nbsp; <a href=\"https://app.reviewnb.com/tensorflow/tensorflow/pull/44433\"><img align=\"absmiddle\"  alt=\"ReviewNB\" height=\"28\" class=\"BotMessageButtonImage\" src=\"https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png\"/></a> \n\n See visual diffs & provide feedback on Jupyter Notebooks. \n\n---\n\n <i>Powered by <a href='https://www.reviewnb.com/?utm_source=gh'>ReviewNB</a></i>", "@Molkree  Can you please resolve conflicts? Thanks!", "@gbaned, done"]}, {"number": 44432, "title": "Add Log1p and DivNoNan gradients", "body": "@saxenasaurabh \r\n\r\nPart of #42668\r\n\r\nNow we are finishing math gradients for ResNet.\r\n\r\n- I haven't found a way to split the test yet but I wonder if we are overtesting the code ? For example,\r\n`ops::Add` is tested here:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/79cdd9533a3566b4b84c374645bf9ccf752cb9f4/tensorflow/python/framework/experimental/unified_api_test.py#L65-L80\r\n\r\nBut the python code is just a binding and the actual logic is already tested somewhere else.\r\n\r\nMoreover, the gradient code is tested twice.\r\n\r\nIn `C++`:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/79cdd9533a3566b4b84c374645bf9ccf752cb9f4/tensorflow/c/eager/gradients_test.cc#L429-L488\r\n\r\nIn `Python`:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/79cdd9533a3566b4b84c374645bf9ccf752cb9f4/tensorflow/python/framework/experimental/unified_api_test.py#L86-L108\r\n\r\n- Please tell me what need to be fixed instead of fixing internally. Because I want to avoid syncing a whole repo which will invalidate the cache and it will take my computer around 10 hours to build the new `LLVM` code. Thank you.", "comments": ["Assigning to @saxenasaurabh , but please feel free to add me back if needed.", "> We need to compute the conjugate here first.\r\n\r\nA silly question, I don't understand why we need conjugate here. AFAIK, the derivate is `ln(x)' = 1/x with x \u2208 C`. Am I missing something ?\r\n\r\n> Could you also add a TODO to add this `control dependency`. We do not have an API for this yet.\r\n\r\nWhere should I add this TODO since `control dependency` is used not only by `Log1pGrad` but many other functions.\r\n\r\n> Do you think there is an advantage of using `DivNoNan` over `grad * Reciprocal(...)` as we do in python?\r\n\r\nI think there is one advantage: It saves us some function calls. In `python`, we could write `grad * Reciprocal( 1 + x )` but in `C++`, we have to call `ops::Mul(Grad, ops::Reciprocal(ops::Add(ops::OnesLike(),X)))`. By using `DivNoNan`, we could reduce it to `ops::DivNoNan(Grad, ops::Add(ops::OnesLike(),X))`\r\n\r\n> This seems to be missing shape broadcasting that we do in python. Could we add that? If that is blocked on some infra work we could add a TODO documenting so.\r\n\r\nI think I will split into 2 PR: one for `Log1p` and one for `DivNoNan`. In addition, I would like to ask if you prefer a bigger PR or small PR since IMO, a bigger PR will be more efficient when you don't have much time to review multiple PRs.\r\n\r\n> Regarding testing, the python bindings are just for proof of concept and to build demos and examples. Eventually the C++ code here will be called from the existing python gradient APIs. We will also add C APIs for GradientTape so we cannot just rely on tests in python.\r\n\r\nWhat if we keep the `C++` tests and remove the `python` tests, does it make sense ?", "@saxenasaurabh Can you please take a look on the above comment from @vnvo2409. Thanks!", "> A silly question, I don't understand why we need conjugate here. AFAIK, the derivate is ln(x)' = 1/x with x \u2208 C. Am I missing something ?\r\n\r\nHere's an extremely rough [sketch](https://photos.app.goo.gl/Doa4bLcgDAGiWx3D8) of a proof for `grad(log(x))` for complex `x`. I just wrote this up so there is possibly a much simpler explanation, but hopefully this gives some clarity.\r\n\r\n> Where should I add this TODO since control dependency is used not only by Log1pGrad but many other functions.\r\n\r\nIf you can find existing gradient functions that are missing this you can add them there as well, but feel free to do that in future PRs. For now we can add this to the gradient functions you are adding.\r\n\r\n> In addition, I would like to ask if you prefer a bigger PR or small PR\r\n\r\nSmall PRs are much much easier to review! I really appreciate you splitting up the large ResNet PR into smaller pieces.\r\n\r\n> What if we keep the C++ tests and remove the python tests, does it make sense ?\r\n\r\nThe experimental python code is going to be useful in the near term to build prototypes of models using C++ gradient functions and it also allows using MLIR. So I would prefer treating both as first class APIs right now and write tests for both. Once thing we could do is use the numerical gradient checker in python as well which would reduce the effort needed to write manual test cases.", "I think we could merge this PR and move forward. I've already added TODOs for later PRs"]}, {"number": 44431, "title": "This model does not contain associated files, and is not a Zip file", "body": "So, i'm trying to use SSD ResNet101 V1 FPN 640x640 (RetinaNet101) found [here ](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md)model for my android application.\r\nAfter converting the model using this script:\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('/content/drive/My Drive/ssd_resnet101_v1_fpn_640x640_coco17_tpu-8/saved_model/',signature_keys=['serving_default'])\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.experimental_new_converter = True\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n\r\ntflite_model = converter.convert()\r\n\r\nwith tf.io.gfile.GFile('model.tflite', 'wb') as f:\r\n  f.write(tflite_model)\r\n\r\n```\r\n\r\nI tried to use it inside the android application, but doesn't work.\r\nWhile the application works just fine using the model from this example\r\n\r\n[Example](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android)\r\n```\r\n\r\nE/AndroidRuntime: FATAL EXCEPTION: main\r\n    Process: com.example.sd_detect, PID: 23556\r\n    java.lang.RuntimeException: Unable to start activity ComponentInfo{com.example.sd_detect/com.example.sd_detect.MainActivity}: java.lang.IllegalStateException: This model does not contain associated files, and is not a Zip file.\r\n        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:3374)\r\n        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:3513)\r\n        at android.app.servertransaction.LaunchActivityItem.execute(LaunchActivityItem.java:83)\r\n        at android.app.servertransaction.TransactionExecutor.executeCallbacks(TransactionExecutor.java:135)\r\n        at android.app.servertransaction.TransactionExecutor.execute(TransactionExecutor.java:95)\r\n        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:2109)\r\n        at android.os.Handler.dispatchMessage(Handler.java:107)\r\n        at android.os.Looper.loop(Looper.java:214)\r\n        at android.app.ActivityThread.main(ActivityThread.java:7682)\r\n        at java.lang.reflect.Method.invoke(Native Method)\r\n        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:516)\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:950)\r\n\r\n```\r\n\r\n", "comments": ["Does the model work as expected on your desktop? I would test that first, before trying on Android. You can use the last sections of [this colab](https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tflite.ipynb) for inspiration.\r\n\r\nDid you modify the example app [code](https://github.com/tensorflow/examples/blob/master/lite/examples/object_detection/android/app/src/main/java/org/tensorflow/lite/examples/detection/DetectorActivity.java#L53) (different model file, different image size, etc)?", "Yes, i changed this line\r\n\r\n`private static final int TF_OD_API_INPUT_SIZE = 300;`\r\n\r\nto this:\r\n\r\n`private static final int TF_OD_API_INPUT_SIZE = 640;`\r\n\r\nAfter googling a bit i found out this question:\r\n[Stackoverflow question](https://stackoverflow.com/questions/64315799/object-detection-on-android-with-tensorflow-lite)\r\n\r\n\r\nCould this be the problem? If that so, how can i attach metadata to it using the script before?", "Hmm, it seems that the model doesn't contain related metadata as the original model. \r\n\r\n@lu-wang-g maybe have more thoughts.", "You'll need to pack as least the label file in the model. You can do this as follows:\r\n```\r\nfrom tflite_support import metadata as _metadata\r\n\r\npopulator = _metadata.MetadataPopulator.with_model_file(model_file)\r\npopulator.load_associated_files([\"your_path_to_label_file\"])\r\npopulator.populate() \r\n```\r\n\r\nThe metadata library can be installed through:\r\n```\r\npip install tflite-support\r\n```\r\n\r\nSee more about how to add metadata [here](https://www.tensorflow.org/lite/convert/metadata#examples).\r\n\r\n[Here](https://stackoverflow.com/questions/64097085/issue-in-creating-tflite-model-populated-with-metadata-for-object-detection/64493506#64493506) is an example of adding metadata to an object detection model (not only the label file), through I think in your case, packing the label file should be good enough.", "Thanks for the reply.\r\n\r\nSo, i have modified the script like this:\r\n\r\n```\r\nimport tensorflow as tf\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(ssd_resnet101_v1_fpn_640x640_coco17_tpu-8/saved_model/',signature_keys=['serving_default'])\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.experimental_new_converter = True\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n\r\ntflite_model = converter.convert()\r\n\r\nwith tf.io.gfile.GFile('model.tflite', 'wb') as f:\r\n  f.write(tflite_model)\r\n \r\nfrom tflite_support import metadata as _metadata\r\npopulator = _metadata.MetadataPopulator.with_model_file(\"model.tflite\")\r\npopulator.load_associated_files([\"labels.txt\"])\r\npopulator.populate() \r\n```\r\n\r\nWith this output:\r\n\r\n```\r\nC:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow_lite_support\\metadata\\metadata.py:342: UserWarning: File, 'labels.txt', does not exsit in the metadata. But packing it to tflite model is still allowed.\r\n  warnings.warn(\r\n\r\n```\r\nNow the application seems to run, but the model can't identify any objects.\r\nIs that caused by something wrong with the script or with other piece of code?\r\n\r\nI changed those lines together with the model:\r\n\r\n```\r\n    private static final boolean TF_OD_API_IS_QUANTIZED = true;\r\n    private static final int TF_OD_API_INPUT_SIZE = 640;\r\n    private static final String TF_OD_API_MODEL_FILE = \"model.tflite\";\r\n    private static final String TF_OD_API_LABELS_FILE = \"labels.txt\";\r\n```\r\n\r\n\r\n\r\nThe complete code is like this (Basically is the example for now, im trying to understand how that works)\r\n\r\n```\r\n    private static final boolean TF_OD_API_IS_QUANTIZED = true;\r\n    private static final int TF_OD_API_INPUT_SIZE = 640;\r\n    private static final String TF_OD_API_MODEL_FILE = \"model.tflite\";\r\n    private static final String TF_OD_API_LABELS_FILE = \"labels.txt\";\r\n    private Detector detector;\r\n    int cropSize = TF_OD_API_INPUT_SIZE;\r\n    private MultiBoxTracker tracker;\r\n\r\n    private int counter = 0;\r\n\r\n@Override\r\n    protected void onCreate(Bundle savedInstanceState) {\r\n        super.onCreate(savedInstanceState);\r\n\r\n\r\n        try {\r\n            detector = TFLiteObjectDetectionAPIModel.create(\r\n                    this,\r\n                            TF_OD_API_MODEL_FILE,\r\n                            TF_OD_API_LABELS_FILE,\r\n                            TF_OD_API_INPUT_SIZE,\r\n                            TF_OD_API_IS_QUANTIZED);\r\n            cropSize = TF_OD_API_INPUT_SIZE;\r\n        } catch (final IOException e) {\r\n            e.printStackTrace();\r\n            Log.e(\"EXCEPTION:\",e.getMessage());\r\n            Toast toast =\r\n                    Toast.makeText(\r\n                            getApplicationContext(), \"Detector could not be initialized\", Toast.LENGTH_SHORT);\r\n            toast.show();\r\n            finish();\r\n        }\r\n\r\n        setContentView(R.layout.activity_main);\r\n        CameraView cameraView = findViewById(R.id.cameraView);\r\n        cameraView.setLifecycleOwner(this); //Automatically handles the camera lifecycle\r\n        cameraView.addFrameProcessor(new FrameProcessor() {\r\n            @Override\r\n            @WorkerThread\r\n            public void process(@NonNull Frame frame) {\r\n                long time = frame.getTime();\r\n                Size size = frame.getSize();\r\n                int format = frame.getFormat();\r\n                int userRotation = frame.getRotationToUser();\r\n                int viewRotation = frame.getRotationToView();\r\n\r\n                int width = size.getWidth();\r\n                int height = size.getHeight();\r\n\r\n                //Create 2 bitmaps, one for the RGB version of the frame detected and one for the NN input\r\n                Bitmap rgbFrameBitmap = Bitmap.createBitmap(width, height, Bitmap.Config.ARGB_8888);\r\n                Bitmap croppedBitmap = Bitmap.createBitmap(cropSize, cropSize, Bitmap.Config.ARGB_8888);\r\n\r\n                //Insert inside rgbBytes the bytes of the frame converted from YUV420SP To ARGB8888\r\n                int[] rgbBytes = new int[width * height];\r\n                ImageUtils.convertYUV420SPToARGB8888((byte[]) frame.getData(), width, height, rgbBytes);\r\n\r\n                //Set the rgbFrameBitmap pixels\r\n                rgbFrameBitmap.setPixels(rgbBytes, 0, width, 0, 0, width, height);\r\n\r\n                //Create a new Canvas\r\n                final Canvas canvas = new Canvas(croppedBitmap);\r\n\r\n\r\n                Matrix frameToCropTransform = ImageUtils.getTransformationMatrix(width, height, cropSize, cropSize, userRotation, false);\r\n                canvas.drawBitmap(rgbFrameBitmap, frameToCropTransform, null);\r\n                //List that contain every object detected in the croppedBitmap\r\n                final List<Detector.Recognition> results = detector.recognizeImage(croppedBitmap);\r\n                Log.d(\"OGGETTI RICONOSCIUTI:\", results.toString());\r\n\r\n                final List<Detector.Recognition> mappedRecognitions = new ArrayList<>();\r\n\r\n                Bitmap cropCopyBitmap = Bitmap.createBitmap(croppedBitmap);\r\n                final Canvas canvas_crop = new Canvas(cropCopyBitmap);\r\n\r\n\r\n                Matrix cropToFrameTransform = new Matrix();\r\n                frameToCropTransform.invert(cropToFrameTransform);\r\n\r\n                //For each element inside the recognized object list we check if we can get the location\r\n                //then if our NN recognize the object with a confidence greater than 0.5,\r\n                //in that case we set the location and we add the item inside the new list\r\n                //then we draw a box over the object in the canvas\r\n\r\n                List<RectF> targets = new ArrayList<>();\r\n                for (final Detector.Recognition result : results) {\r\n                    final RectF location = result.getLocation();\r\n                    if (location != null && result.getConfidence() >= 0.5) {\r\n                        result.setLocation(location);\r\n                        mappedRecognitions.add(result);\r\n                        targets.add(location);\r\n                        Log.d(\"OBJECT: \", result.getTitle());\r\n\r\n\r\n                        cropToFrameTransform.mapRect(location);\r\n                    }\r\n                }\r\n\r\n                OverlayView overlayView = findViewById(R.id.overlayView);\r\n                overlayView.setPreviewHeight(height);\r\n                overlayView.setPreviewWidth(width);\r\n                overlayView.setSensorOrientation(userRotation);\r\n                overlayView.setMappedRecognitions(mappedRecognitions);\r\n                overlayView.setTargets(targets);\r\n                overlayView.draw(canvas_crop);\r\n                Log.e(\"BEST RESULT:\", results.get(0).getTitle());\r\n\r\n                //counter++;\r\n                //if (counter == 60){\r\n                    //ImageUtils.saveBitmapToDCIM(croppedBitmap);\r\n                    //counter = 0;\r\n                //}\r\n            }\r\n        });\r\n    }\r\n```", "Thanks for the feedback!\r\n\r\nAre you compiling using lib_interpreter or lib_task_api? And by saying \" but the model can't identify any objects\", do you mean that there's no bounding boxes rending in the camera preview, or do you mean that the box labels are all wrong?\r\n", "@fanto88  Are you using [these instrcutions](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tf2.md) to generate the TFLite-friendly model, from the download? Note that there is an intermediate step of generating a different SavedModel so that inference on mobile is possible.", "With  \" but the model can't identify any objects\" i mean that using this model there is no bounding boxes rending in the camera preview, while using the model from the example the objects are shown on the preview. But even logging all the objects detect produce an empty string, like here:\r\n\r\n`final List<Detector.Recognition> results = detector.recognizeImage(croppedBitmap);\r\n                Log.d(\"OGGETTI RICONOSCIUTI:\", results.toString());`\r\n\r\nFor the other question i copied the 2 classes from lib_interpreter and im using those. Inside TFLiteObjectDetectionAPIModel.java the tag is TFLiteObjectDetectionAPIModelWithInterpreter, so im pretty sure it's the lib_interpreter.\r\n\r\nFor answering @srjoglekar246 i tried using the istructions you posted but it would output a 1kb tflite model, like i asked here:\r\n\r\n[Github question](https://github.com/tensorflow/tensorflow/issues/44393)\r\n\r\n\r\n\r\n**Edit:**\r\n\r\nAfter a bit of trying now my process is like this:\r\n\r\n1) Download the model i want to use, in this case is: [SSD ResNet101 V1 FPN 640x640 (RetinaNet101)](http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet101_v1_fpn_640x640_coco17_tpu-8.tar.gz)\r\n2) Extract the file\r\n3) Use:\r\n\r\n`pyhon D:/Davide/Uni/SistemiDigitali/models-master/research/object_detection/export_tflite_graph_tf2.py --pipeline_config_path D:/Davide/Uni/SistemiDigitali/model/pipeline.config --trained_checkpoint_dir D:/Davide/Uni/SistemiDigitali/model/checkpoint --output_directory D:\\Davide\\Uni\\SistemiDigitali`\r\n\r\nThis produce a folder called saved_model\r\n\r\n4) Now i use:\r\n\r\n```\r\nimport tensorflow as tf\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('saved_model/',signature_keys=['serving_default'])\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.experimental_new_converter = True\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n\r\ntflite_model = converter.convert()\r\n\r\nwith tf.io.gfile.GFile('model.tflite', 'wb') as f:\r\n  f.write(tflite_model)\r\n \r\nfrom tflite_support import metadata as _metadata\r\npopulator = _metadata.MetadataPopulator.with_model_file(\"model.tflite\")\r\npopulator.load_associated_files([\"labels.txt\"])\r\npopulator.populate() \r\n```\r\n\r\nWhere saved_model is the folder that i got from the step 3)\r\n\r\n5) Copy and paste labels.txt and model.tflite inside assets folder of my project\r\n\r\n6) Since the model is a 640x640, changing the TF_OD_API_INPUT_SIZE variable like this:\r\n\r\n    `private static final int TF_OD_API_INPUT_SIZE = 640;`\r\n\r\n7) Run the application. No error, but no object detected.\r\n\r\nI tried with TF_OD_API_IS_QUANTIZED false and true, nothing change. \r\n\r\nThe labels.txt are from here: [Coco Labels](https://github.com/nightrome/cocostuff#downloads)", "@fanto88 Is the model you now receive after conversion to TFLite >1KB? (The Github issue you linked to seems to be fixed)\r\n\r\nAlso, to get an 8-bit quantized model, you would need to specify `tf.lite.OpsSet.TFLITE_BUILTINS_INT8` as well to `converter.target_spec.supported_op` (I would suggest converting the floating-point model first, without `converter.optimizations`).\r\n\r\nAlso, the models thus generated do *not* have TFLite metadata attached in the TFLite file. @lu-wang-g Is there a script to write the metadata available on Github?", "@srjoglekar246  Yes, now the model using the steps above is more than >1kb. \r\nActually now it detects something, but it's really wrong. Basically the only object it detects is a mirror, even if i point the camera towards a screen, keyboard or whatever. Keep saying only mirror.\r\n\r\nSorry, i don't really understand how to do like you said.\r\n\r\nInstead of \r\n`converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]`\r\n\r\nI should use\r\n`converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.OpsSet.SELECT_TF_OPS]`\r\n\r\nAnd remove this line?\r\n`converter.optimizations = [tf.lite.Optimize.DEFAULT]`\r\n\r\n\r\n", "Sorry about the confusion :-).\r\n\r\nI am suggesting floating-point conversion first, with just this (without anything provided for `converter.optimizations`):\r\n```\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\r\n```\r\n\r\nFor float, `TF_OD_API_IS_QUANTIZED` should be False.\r\n\r\nOnce we can verify that floating-point stuff works, we can so quantization with:\r\n```\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.OpsSet.TFLITE_BUILTINS]\r\n```\r\n\r\nI see that you are indeed attaching the metadata to the model as expected, so that *should* work.", "So, i think i did like you asked. \r\n\r\n```\r\nimport tensorflow as tf\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('saved_model/',signature_keys=['serving_default'])\r\nconverter.experimental_new_converter = True\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\r\n```\r\n\r\nWhen i watch an orange though the camera this is the object the camera detects:\r\n\r\n`2020-11-02 22:38:05.590 9344-9610/com.example.sd_detect D/OGGETTI\u00a0RICONOSCIUTI:: [[0] mirror (7,1%) RectF(-30.282135, 182.18513, 659.00214, 591.1142), [1] mirror (6,4%) RectF(32.083797, -26.561756, 634.7681, 574.84485), [2] orange (4,3%) RectF(292.83243, 296.0695, 515.19543, 497.4301), [3] mirror (4,0%) RectF(158.42299, 302.44458, 608.92426, 606.5084), [4] orange (3,8%) RectF(308.74478, 383.55573, 478.98785, 538.045), [5] orange (3,6%) RectF(373.27847, 407.17923, 564.32715, 590.8782), [6] mirror (3,6%) RectF(62.82507, 117.443695, 635.2874, 497.6333), [7] orange (3,6%) RectF(401.82397, 365.34338, 552.3721, 564.8386), [8] orange (3,3%) RectF(330.21396, 411.48807, 508.9789, 588.4522), [9] orange (3,3%) RectF(216.4775, 175.09775, 587.1851, 517.8119)]`\r\n\r\nSo, i suppose the model actually is working, but something is wrong with the input image, maybe?\r\n\r\nSorry, i'm trying to understand how all of this works. I'm pretty new to Android, AI and Tensorflow", "I think so. @lu-wang-g Can you check if something is wrong with the metadata attached to the model? Is it being given the correct cropped/preprocessed image in this case?", "The pre/post processing should work fine as long as the inputSize is correct. And in this case, the label file are reading correctly, so metadata should have been attached correctly. Seems like it's the quality of the model that causes the issue. I would try using another model and see if it works. ", "Just found that I couldn't got a working model using the script that fanto88 attached previously. The model only have input and output tensors, but no internal ops. See the graph below:\r\n\r\n![image](https://user-images.githubusercontent.com/47436172/97948390-499e6b80-1d45-11eb-977d-a424a6cfc254.png)\r\n", "I see, that would explain why it's not working as expected. \r\nIs there a way were i can find if the problem is the \r\n\r\nStep 1: Export TFLite inference graph\r\nThis step generates an intermediate SavedModel that can be used with the TFLite Converter via commandline or Python API.\r\n\r\nTo use the script:\r\n\r\n```\r\n# From the tensorflow/models/research/ directory\r\npython object_detection/export_tflite_graph_tf2.py \\\r\n    --pipeline_config_path path/to/ssd_model/pipeline.config \\\r\n    --trained_checkpoint_dir path/to/ssd_model/checkpoint \\\r\n    --output_directory path/to/exported_model_directory\r\n```\r\n\r\nOr if the problem is when trying to convert to the tflite version of the model?\r\n\r\nThose are the 3 properties tab shown opening the model with Netron for windows.\r\n\r\nDownloaded model:\r\n![downloadedProp](https://user-images.githubusercontent.com/10716240/97974313-ae9aa580-1dc7-11eb-9cc8-9f6c30fb2709.png)\r\n\r\n\r\nIntermediate SavedModel after export_tflite_graph.py:\r\n![intermediateSavedModelProp](https://user-images.githubusercontent.com/10716240/97974443-e1dd3480-1dc7-11eb-8ba8-41e2cba0d510.png)\r\n\r\n\r\ntflite Model:\r\n![tfliteProp](https://user-images.githubusercontent.com/10716240/97974339-b65a4a00-1dc7-11eb-907a-8ab0a0144b70.png)\r\n\r\n", "@srjoglekar246 do you have any idea of what's wrong with the conversion process?", "@fanto88 Which version of TF are you using? You might need to use the latest nightly version of TF (especially the conversion), since the support for TF2 detection models was added pretty recently.", "This should be the version i'm using. \r\n\r\n![image](https://user-images.githubusercontent.com/10716240/98016025-c260fe80-1dfd-11eb-9eeb-faf003d5417e.png)\r\n", "Hmm, this is strange. Taking a look...\r\n", "It's for sure that the problem is the model and not the input i give to the model itself?\r\n\r\nI can upload the entire project if that could be useful for debug purpose, because i really can't find the problem. I'm reading a lot of guides and watching a lot of videos, but the result are always the same.\r\n", "Could you upload your TFLite file? It might be useful in debugging what you see.", "Having here pretty much the same error you posted \"This model does not contain associated files, and is not a Zip file.\"\r\nWhen I used a model that was converted from YoloV4 (using this guy guide - https://github.com/theAIGuysCode/tensorflow-yolov4-tflite), then I've put the model and the coco labels instead the existing ones in the tensorflow android app demo\r\nand got this error, is it because there is some metadata missing from my model or something wrong with the labels (new to all these stuff aswell so trying to figure out how to make my model (with pre-trained weights that converted from YoloV4 to tf) to work on my tensorflow android app demo  ", "I think @lu-wang-g was using a previous TF version, my bad. @fanto88 From your visualization, it looks like the TFLite model is correct (if you upload it, I can confirm). Something is probably wrong with the metadata or the input to the model.\r\nSorry about the confusion!", "@S3Stellar i've solved the \"is not a zip file\" problem adding the label to the metadata like this:\r\n\r\n```\r\nfrom tflite_support import metadata as _metadata\r\npopulator = _metadata.MetadataPopulator.with_model_file(\"model.tflite\")\r\npopulator.load_associated_files([\"labels.txt\"])\r\npopulator.populate() \r\n```\r\n\r\n\r\n@srjoglekar246 \r\n\r\nThis is the [Link](https://drive.google.com/drive/folders/1qFCjIE3OJkUuLTHrdiSrNVST2ROVVZ0A?usp=sharing) with some files;\r\n-model folder is the model i have download from internet\r\n-SDDetect is the entire project i'm actually using and where i have the problem\r\n-labels.txt are the labels\r\n-model.tflite is the model i have converted\r\n-convertModel.py is the script i use to convert the tensorflow model to tensorflow lite.\r\n\r\n", "FWIW, @fanto88 's model seems fine to me (atleast in terms of the ops & tensors in the graph).", "Thanks for the making it clear @srjoglekar246 and @fanto88 \r\n\r\nI looked into the model and found that the root cause is the mean / std value. Normally the input image of float models need to be normalized to [-1, 1], but this model seems doesn't need to. Therefore in TFLiteObjectDetectionAPIModel, you need to change the following values: \r\n```\r\nprivate static final float IMAGE_MEAN = 0.0f;\r\nprivate static final float IMAGE_STD = 1.0f;\r\n```\r\n\r\nAlso, you'll need to remove the first entry in the label file, which is \"unlabeled\" in this case. When using TFLite object detection post processing op (which is encapsulated in export_tflite_graph_tf2.py), the first entry (usually background) is ignored. Therefore you should remove the entry. Alternatively, you could add an offset when passing the labels (which is what we used to do), such as \r\n```\r\nint labelOffset = 1;\r\nrecognitions.add(\r\n          new Recognition(\r\n              \"\" + i, labels.get((int) outputClasses[0][i] +labelOffset), outputScores[0][i], detection));\r\n```\r\n\r\nThe labelmap.txt in the ref app is confusing. We should update it. But if you unzip the model download from tfhub, you'll see the first entry (which is background) has been removed.", "FYI, labelmap.txt and labels.txt in your case are not used in the ref app. The app reads labels from the label file packed in the model.", "Oh, thanks a lot @srjoglekar246 and @lu-wang-g , really. It finally works.\r\n\r\nCan i ask 2 more question then before closing the question?\r\n\r\n1) how i can calculate the correct value for\r\n```\r\nprivate static final float IMAGE_MEAN = 0.0f;\r\nprivate static final float IMAGE_STD = 1.0f;\r\n```\r\nso that if i will change again the model, now that i know how to do so, i can calculate those values.\r\n\r\n2) Now it works like expected, everything is fine. I tried with keyboard, mouse and other things and seems correct. But it's really slow, like 1 frame per second. Is there a way to speed the model up with some optimitation?", "@fanto88 I would suggest using one of the SSD MobileNet model listed on the [zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md). MobileNet is much more mobile-friendly than ResNet, at the cost of accuracy. You will need to figure out the best tradeoff for your model :-)", "@fanto88 For the question, \"how i can calculate the correct value for?\", unfortunately, there is no easy way to know about it at this point. The only reliable way is to look into the training script and see how the model author preprocessed the image. For Google pretrained tflite model on TFHub, we've populated the correct mean/std value into the metadata, and Task library can automatically pick them up and process the images accordingly.\r\n\r\nHere is some values you could try:\r\n1. For quantized models, normalization can be ignored for image input.\r\n2. For float models, images are usually normalized to [-1, 1].\r\nSee [Normalization and quantization parameters](https://www.tensorflow.org/lite/convert/metadata#normalization_and_quantization_parameters) for more information.\r\n\r\n\r\n", "I see. \r\n\r\nThanks everyone for the help, i really appreciated it. I'm gonna experiment a bit trying to make the model recognize some custom object.\r\n\r\nThanks again,\r\n\r\nDavide.", "Sounds good! I'll close this issue. Feel free to let us know if you encounter other problems. Thanks.", "**Update**:\r\n\r\nThe [Metadata Writer library](https://github.com/tensorflow/tflite-support/tree/master/tensorflow_lite_support/metadata/python/metadata_writers) has been released. See this [Stack OverFlow answer](https://stackoverflow.com/a/64493506/11031225) for details."]}, {"number": 44429, "title": "In the codelab, \"Build a handwritten digit classifier app with TensorFlow Lite\" , At step number 4, the interpreter is not initialized with model, which is causing an NPE.", "body": "https://codelabs.developers.google.com/codelabs/digit-classifier-tflite#3\r\n\r\nCheck the above link, In step number 4, The interpreter is not initialized using the model.\r\nWhich is why it is giving NPE on interpreter instance.", "comments": ["@pradyum619 \r\n\r\nPlease, fill issue template. Are you refering to this [tutorial](https://codelabs.developers.google.com/codelabs/digit-classifier-tflite#5) ?\r\nThanks!", "Also can you please help with the stacktrace? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "https://codelabs.developers.google.com/codelabs/digit-classifier-tflite#3\r\n\r\nCheck the above link, In step number 4, The interpreter is not initialized using the model.\r\nWhich is why it is giving NPE on interpreter instance.", "Thank you for reporting that!\r\n\r\nThe code block\r\n```\r\nprivate fun initializeInterpreter() {\r\n    // TODO: Load the TF Lite model from file and initialize an interpreter.\r\n    // ...\r\n}\r\n```\r\nmeans that developers need to initialize Interpreter instance within this function.", "![image](https://user-images.githubusercontent.com/5379361/98897193-341c0500-24ee-11eb-8df2-12494a9c8e11.png)\r\n\r\nYou're right. There're 3 lines missing from the step 5. I'll update the codelab and republish it.\r\n```\r\nval assetManager = context.assets\r\nval model = loadModelFile(assetManager, \"mnist.tflite\")\r\n\r\n// These lines are missing from step 5\r\nval options = Interpreter.Options()\r\noptions.setUseNNAPI(true)\r\nval interpreter = Interpreter(model, options)\r\n```", "The [codelab](https://developer.android.com/codelabs/digit-classifier-tflite#3) has been updated. Thanks for reporting the issue!\r\n\r\n![image](https://user-images.githubusercontent.com/5379361/99206703-751e5d00-27ff-11eb-9071-48486e562f7a.png)\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44429\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44429\">No</a>\n"]}, {"number": 44427, "title": "ValueError: Structure of Python function inputs does not match input_signature:", "body": "**System information**\r\n- OS Platform and Distribution :CentOS Linux release 7.7.1908\r\n-TensorFlow version:2.3.0\r\n\r\nI try to convert [the tensorflow offical image caption model ](https://www.tensorflow.org/tutorials/text/image_captioning?hl=en)to TFLite model \r\n\r\nI try to convert the `tf.keras.Model `'s encoder and decoder model as following:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nembedding_dim = 256\r\nunits = 512\r\ntop_k = 5000\r\nvocab_size = top_k + 1\r\nfeatures_shape = 2048\r\nattention_features_shape = 64\r\n\r\nclass BahdanauAttention(tf.keras.Model):\r\n    def __init__(self, utils):\r\n        super(BahdanauAttention, self).__init__()\r\n        self.W1 = tf.keras.layers.Dense(utils)\r\n        self.W2 = tf.keras.layers.Dense(utils)\r\n        self.V = tf.keras.layers.Dense(1)\r\n    def call(self, features, hidden):\r\n        # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\r\n\r\n        # hidden shape == (batch_size, hidden_size)\r\n        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\r\n        hidden_with_time_axis_shape = tf.expand_dims(hidden, 1)\r\n\r\n        # score shape == (batch_size, 64, hidden_size)\r\n        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis_shape))\r\n\r\n        # attention_weights shape == (batch_size, 64, 1)\r\n        # you get 1 at the last axis because you are applying score to self.V\r\n        attention_weights = tf.nn.softmax(self.V(score), axis=1)\r\n\r\n        # context_vector shape after sum == (batch_size, hidden_size)\r\n        context_vector = attention_weights * features\r\n        context_vector = tf.reduce_sum(context_vector, axis=1)\r\n\r\n        return context_vector, attention_weights\r\n\r\nclass CNN_Encoder(tf.keras.Model):\r\n    #\u7531\u4e8e\u60a8\u5df2\u7ecf\u63d0\u53d6\u4e86\u7279\u5f81\u5e76\u4f7f\u7528pickle\u8fdb\u884c\u4e86\u8f6c\u50a8\r\n    #\u8be5\u7f16\u7801\u5668\u901a\u8fc7\u5b8c\u5168\u8fde\u63a5\u7684\u5c42\u4f20\u9012\u8fd9\u4e9b\u7279\u5f81\r\n    def __init__(self, embedding):\r\n        super(CNN_Encoder, self).__init__()\r\n        # shape after fc == (batch_size, 64, embedding_dim)\r\n        self.fc = tf.keras.layers.Dense(embedding_dim)\r\n\r\n    # @tf.function(input_signature=[tf.TensorSpec(shape=(1, 64, features_shape),dtype=tf.float32)])\r\n    @tf.function\r\n    def call(self, x):\r\n        x = self.fc(x)\r\n        x = tf.nn.relu(x)\r\n        return x\r\n\r\nclass RNN_Decoder(tf.keras.Model):\r\n    def __init__(self, embedding_dim, units, vocab_size):\r\n        super(RNN_Decoder, self).__init__()\r\n        self.units = units\r\n\r\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\r\n        self.gru = tf.keras.layers.GRU(self.units,\r\n                                       return_sequences=True,\r\n                                       return_state=True,\r\n                                       recurrent_initializer='glorot_uniform',\r\n                                       unroll = True)\r\n        self.fc1 = tf.keras.layers.Dense(self.units)\r\n        self.fc2 = tf.keras.layers.Dense(vocab_size)\r\n\r\n        self.attention = BahdanauAttention(self.units)\r\n\r\n\r\n    @tf.function(input_signature=[tf.TensorSpec(shape=[1, 1], dtype=tf.int32, name='x'),\r\n                                  tf.TensorSpec(shape=[1, 64, 256], dtype=tf.float32, name='feature'),\r\n                                  tf.TensorSpec(shape=[1, 512], dtype=tf.float32, name='hidden')])\r\n    @tf.function\r\n    def call(self, x , features, hidden):\r\n        #\u5c06\u6ce8\u610f\u529b\u5b9a\u4e49\u4e3a\u4e00\u4e2a\u5355\u72ec\u7684\u6a21\u578b\r\n        context_vector, attention_weights = self.attention(features, hidden)\r\n\r\n        #x shape after passing through embedding == (batch_size, 1, embedding_dim)\r\n        x = self.embedding(x)\r\n\r\n        #x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\r\n        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\r\n\r\n        #\u5c06concated\u540e\u7684\u7684\u5411\u91cf\u4f20\u9012\u7ed9GRU\r\n        output, state = self.gru(x)\r\n\r\n        #shape == (batch_size, max_length, hidden_size)\r\n        x = self.fc1(output)\r\n\r\n        #x shape == (batch_size, max_length, hidden_size)\r\n        x = tf.reshape(x, (-1, x.shape[2]))\r\n\r\n        # output shape == (batch_size * max_length, vocab)\r\n        x = self.fc2(x)\r\n\r\n        return x, state, attention_weights\r\n\r\n    def reset_states(self, batch_size):\r\n        return tf.zeros((batch_size, self.units))\r\n\r\nencoder = CNN_Encoder(embedding_dim)\r\ndecoder = RNN_Decoder(embedding_dim, units, vocab_size)\r\n\r\nencoder._set_inputs(tf.TensorSpec(shape=(1, 64, features_shape),dtype=tf.float32))\r\ndecoder._set_inputs([tf.TensorSpec(shape=[1, 1], dtype=tf.int32, name='x'),\r\n                                  tf.TensorSpec(shape=[1, 64, 256], dtype=tf.float32, name='feature'),\r\n                                  tf.TensorSpec(shape=[1, 512], dtype=tf.float32, name='hidden')])\r\n\r\n\r\nencoder_converter = tf.lite.TFLiteConverter.from_keras_model(encoder)\r\ndecoder_converter = tf.lite.TFLiteConverter.from_keras_model(decoder)\r\n\r\nencoder_model = encoder_converter.convert()\r\ndecoder_model = decoder_converter.convert()\r\n\r\nopen(\"encoder_model.tflite\", \"wb\").write(encoder_model)\r\nopen(\"decoder_model.tflite\", \"wb\").write(decoder_model)\r\n```\r\n\r\nThe error messge is \r\n```\r\nValueError: Structure of Python function inputs does not match input_signature:\r\n  inputs: (\r\n    [<tf.Tensor 'x:0' shape=(1, 1) dtype=int32>, <tf.Tensor 'feature:0' shape=(1, 64, 256) dtype=float32>, <tf.Tensor 'hidden:0' shape=(1, 512) dtype=float32>])\r\n  input_signature: (\r\n    TensorSpec(shape=(1, 1), dtype=tf.int32, name='x'),\r\n    TensorSpec(shape=(1, 64, 256), dtype=tf.float32, name='feature'),\r\n    TensorSpec(shape=(1, 512), dtype=tf.float32, name='hidden'))\r\n```\r\nI think the function input is the same as the input signature.How can I fix the problem?", "comments": ["@DavidInWuhanChina \r\nI ran the code and face a different issue please find [gist here](https://colab.research.google.com/gist/Saduf2019/9c44ff1d247f7f12b00eb3f27dc14aea/untitled453.ipynb),please share a colab gist with error reported.", "> \r\n> \r\n> @DavidInWuhanChina\r\n> I ran the code and face a different issue please find [gist here](https://colab.research.google.com/gist/Saduf2019/9c44ff1d247f7f12b00eb3f27dc14aea/untitled453.ipynb),please share a colab gist with error reported.\r\n\r\nI have edit my code, Now it can run in the gist and reproduces the error.", "@jvishnuvardhan \r\nI am able to replicate the issue faced, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/4d41161376601f6d770c0c46f25b5673/untitled455.ipynb).", "@DavidInWuhanChina A quick question? Were you able to train the model? Looks like there is incompatibility in input and input signature of Decoder and/or `BahdanauAttention`. Thanks!", "> \r\n> \r\n> @DavidInWuhanChina A quick question? Were you able to train the model? Looks like there is incompatibility in input and input signature of Decoder and/or `BahdanauAttention`. Thanks!\r\nI have trained the model referring to the tutorials.Here is the[ gist](https://colab.research.google.com/gist/DavidInWuhanChina/f83e3e11009211f3469436bbc069b18a/43753.ipynb).The initial error is `\r\nTypeError: call() missing 2 required positional arguments: 'features' and 'hidden'.How cam I solve it?The title problem came after I add @tf.funcition(input_signature) to he RNN_Decoder 's call() function.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44427\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44427\">No</a>\n"]}, {"number": 44426, "title": "Why XLA need new kernels about op? ", "body": "In TensorFlow code about XLA, I see kernels about many OPs like `[compiler/tf2xla/kernels/concat_op](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/tf2xla/kernels/concat_op.cc)`.  It seems like a repetition of `[core/kernels/concat_op](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/concat_op.cc)`.\r\nWhy Ops like `compiler/tf2xla/kernels/concat_op` are needed? And why not just replace it with `core/kernels/concat_op` to save code? ", "comments": ["hi", "These kernels are very different.  The XLA kernel generates HLO that represents the TF operation while the non-XLA kernel implements the behavior for TF concat."]}, {"number": 44425, "title": "Remove hardcoded int32 conversion", "body": "We will have some silent failure if we convert int64 to int32: it will overflow and maybe get negative shape on Linux. I didn't test on Windows - seems array_ops.shape() no longer throw exception for large shape. So I revert the change here.\r\nTo support larger shape than int32, user should change other places as well.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F44425) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "Usually we expect shapes to be within int32 range. So I think its safe. Is there some code you have that was running into problems?", "> Usually we expect shapes to be within int32 range. So I think its safe. Is there some code you have that was running into problems?\r\n\r\nYes. We do have int64 shape. And this code runs silently convert the shape to int32 and later code throws exception.\r\nSo I think we should let it throw exception when people handle int64 shape.", "@rohan100jain Can you please take a look on the above comment from @oujiafan. Thanks!", "@rohan100jain  Any update on this PR? Please. Thanks!"]}, {"number": 44424, "title": "Encountered unresolved custom op: TensorListReserve and failed to prepare", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04\r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version (or github SHA if from source): training, export model used tensorflow 1.15.0, inference model used tensorflow lite CPP 2.3.1 (master) and 1.15.0 as well.\r\n\r\n**Provide the text output from tflite_convert**\r\nAdded tf.enable_control_flow_v2() to model before exporting.\r\nUsed command (tensorflow 1.15.0) : tflite_convert --saved_model_dir=superpoint_640x480x_control_flow --enable_select_tf_ops --allow_custom_ops --output_file=testing.tflite\r\n```\r\n2020-10-29 14:18:08.106202: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-10-29 14:18:08.127674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 14:18:08.127922: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62\r\npciBusID: 0000:01:00.0\r\n2020-10-29 14:18:08.128040: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2020-10-29 14:18:08.128856: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2020-10-29 14:18:08.129575: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2020-10-29 14:18:08.129739: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2020-10-29 14:18:08.130698: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2020-10-29 14:18:08.131440: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2020-10-29 14:18:08.133886: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-10-29 14:18:08.133976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 14:18:08.134418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 14:18:08.134628: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2020-10-29 14:18:08.134848: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2020-10-29 14:18:08.158045: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\r\n2020-10-29 14:18:08.158687: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b335094940 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-10-29 14:18:08.158725: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-10-29 14:18:08.158925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 14:18:08.159353: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62\r\npciBusID: 0000:01:00.0\r\n2020-10-29 14:18:08.159391: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2020-10-29 14:18:08.159408: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2020-10-29 14:18:08.159426: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2020-10-29 14:18:08.159441: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2020-10-29 14:18:08.159456: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2020-10-29 14:18:08.159471: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2020-10-29 14:18:08.159486: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-10-29 14:18:08.159550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 14:18:08.159921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 14:18:08.160242: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2020-10-29 14:18:08.160279: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2020-10-29 14:18:08.270633: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-10-29 14:18:08.270654: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \r\n2020-10-29 14:18:08.270659: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n2020-10-29 14:18:08.270764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 14:18:08.271031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 14:18:08.271270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 14:18:08.271500: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2968 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2020-10-29 14:18:08.272684: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b337854f60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-10-29 14:18:08.272696: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1050 Ti, Compute Capability 6.1\r\nWARNING:tensorflow:From /home/anaconda3/envs/dynim/lib/python3.7/site-packages/tensorflow_core/lite/python/convert_saved_model.py:60: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\r\nW1029 14:18:08.273251 140612085802816 deprecation.py:323] From /anaconda3/envs/dynim/lib/python3.7/site-packages/tensorflow_core/lite/python/convert_saved_model.py:60: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\r\nINFO:tensorflow:Restoring parameters from superpoint_640x480x_control_flow/variables/variables\r\nI1029 14:18:08.345254 140612085802816 saver.py:1284] Restoring parameters from superpoint_640x480x_control_flow/variables/variables\r\nINFO:tensorflow:The given SavedModel MetaGraphDef contains SignatureDefs with the following keys: {'serving_default'}\r\nI1029 14:18:08.741633 140612085802816 convert_saved_model.py:80] The given SavedModel MetaGraphDef contains SignatureDefs with the following keys: {'serving_default'}\r\nINFO:tensorflow:input tensors info: \r\nI1029 14:18:08.741893 140612085802816 convert_saved_model.py:99] input tensors info: \r\nINFO:tensorflow:Tensor's key in saved_model's tensor_map: image\r\nI1029 14:18:08.742036 140612085802816 convert_saved_model.py:41] Tensor's key in saved_model's tensor_map: image\r\nINFO:tensorflow: tensor name: superpoint/image:0, shape: (1, 480, 640, 1), type: DT_FLOAT\r\nI1029 14:18:08.742112 140612085802816 convert_saved_model.py:43]  tensor name: superpoint/image:0, shape: (1, 480, 640, 1), type: DT_FLOAT\r\nINFO:tensorflow:output tensors info: \r\nI1029 14:18:08.742195 140612085802816 convert_saved_model.py:101] output tensors info: \r\nINFO:tensorflow:Tensor's key in saved_model's tensor_map: descriptors\r\nI1029 14:18:08.742318 140612085802816 convert_saved_model.py:41] Tensor's key in saved_model's tensor_map: descriptors\r\nINFO:tensorflow: tensor name: superpoint/descriptors:0, shape: (1, 480, 640, 256), type: DT_FLOAT\r\nI1029 14:18:08.742387 140612085802816 convert_saved_model.py:43]  tensor name: superpoint/descriptors:0, shape: (1, 480, 640, 256), type: DT_FLOAT\r\nINFO:tensorflow:Tensor's key in saved_model's tensor_map: descriptors_raw\r\nI1029 14:18:08.742443 140612085802816 convert_saved_model.py:41] Tensor's key in saved_model's tensor_map: descriptors_raw\r\nINFO:tensorflow: tensor name: superpoint/descriptors_raw:0, shape: (1, 60, 80, 256), type: DT_FLOAT\r\nI1029 14:18:08.742491 140612085802816 convert_saved_model.py:43]  tensor name: superpoint/descriptors_raw:0, shape: (1, 60, 80, 256), type: DT_FLOAT\r\nINFO:tensorflow:Tensor's key in saved_model's tensor_map: prob_nms\r\nI1029 14:18:08.742545 140612085802816 convert_saved_model.py:41] Tensor's key in saved_model's tensor_map: prob_nms\r\nINFO:tensorflow: tensor name: superpoint/prob_nms:0, shape: (1, 480, 640), type: DT_FLOAT\r\nI1029 14:18:08.742592 140612085802816 convert_saved_model.py:43]  tensor name: superpoint/prob_nms:0, shape: (1, 480, 640), type: DT_FLOAT\r\nINFO:tensorflow:Tensor's key in saved_model's tensor_map: pred\r\nI1029 14:18:08.742646 140612085802816 convert_saved_model.py:41] Tensor's key in saved_model's tensor_map: pred\r\nINFO:tensorflow: tensor name: superpoint/pred:0, shape: (1, 480, 640), type: DT_INT32\r\nI1029 14:18:08.742693 140612085802816 convert_saved_model.py:43]  tensor name: superpoint/pred:0, shape: (1, 480, 640), type: DT_INT32\r\nINFO:tensorflow:Tensor's key in saved_model's tensor_map: logits\r\nI1029 14:18:08.742746 140612085802816 convert_saved_model.py:41] Tensor's key in saved_model's tensor_map: logits\r\nINFO:tensorflow: tensor name: superpoint/logits:0, shape: (1, 60, 80, 65), type: DT_FLOAT\r\nI1029 14:18:08.742793 140612085802816 convert_saved_model.py:43]  tensor name: superpoint/logits:0, shape: (1, 60, 80, 65), type: DT_FLOAT\r\nINFO:tensorflow:Tensor's key in saved_model's tensor_map: prob\r\nI1029 14:18:08.742845 140612085802816 convert_saved_model.py:41] Tensor's key in saved_model's tensor_map: prob\r\nINFO:tensorflow: tensor name: superpoint/prob:0, shape: (1, 480, 640), type: DT_FLOAT\r\nI1029 14:18:08.742892 140612085802816 convert_saved_model.py:43]  tensor name: superpoint/prob:0, shape: (1, 480, 640), type: DT_FLOAT\r\n2020-10-29 14:18:08.743259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 14:18:08.743479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62\r\npciBusID: 0000:01:00.0\r\n2020-10-29 14:18:08.743506: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2020-10-29 14:18:08.743519: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2020-10-29 14:18:08.743530: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2020-10-29 14:18:08.743540: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2020-10-29 14:18:08.743551: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2020-10-29 14:18:08.743561: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2020-10-29 14:18:08.743572: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-10-29 14:18:08.743610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 14:18:08.743803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 14:18:08.743984: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2020-10-29 14:18:08.744021: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-10-29 14:18:08.744028: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \r\n2020-10-29 14:18:08.744034: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n2020-10-29 14:18:08.744104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 14:18:08.744316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 14:18:08.744505: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2968 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nINFO:tensorflow:Restoring parameters from superpoint_640x480x_control_flow/variables/variables\r\nI1029 14:18:08.809329 140612085802816 saver.py:1284] Restoring parameters from superpoint_640x480x_control_flow/variables/variables\r\n2020-10-29 14:18:08.899389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 14:18:08.899605: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2020-10-29 14:18:08.899660: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-10-29 14:18:08.900032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 14:18:08.900221: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62\r\npciBusID: 0000:01:00.0\r\n2020-10-29 14:18:08.900248: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2020-10-29 14:18:08.900261: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2020-10-29 14:18:08.900272: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2020-10-29 14:18:08.900283: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2020-10-29 14:18:08.900295: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2020-10-29 14:18:08.900306: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2020-10-29 14:18:08.900317: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-10-29 14:18:08.900352: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 14:18:08.900544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 14:18:08.900711: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2020-10-29 14:18:08.900730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-10-29 14:18:08.900736: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \r\n2020-10-29 14:18:08.900742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n2020-10-29 14:18:08.900797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 14:18:08.900991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 14:18:08.901166: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2968 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2020-10-29 14:18:08.922872: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: graph_to_optimize\r\n2020-10-29 14:18:08.922897: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   function_optimizer: Graph size after: 675 nodes (0), 1166 edges (0), time = 5.041ms.\r\n2020-10-29 14:18:08.922905: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   function_optimizer: Graph size after: 675 nodes (0), 1166 edges (0), time = 4.969ms.\r\n2020-10-29 14:18:08.922911: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: superpoint_pred_tower0_map_while_body_529\r\n2020-10-29 14:18:08.922918: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-10-29 14:18:08.922924: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-10-29 14:18:08.922930: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: superpoint_pred_tower0_map_while_cond_528\r\n2020-10-29 14:18:08.922936: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-10-29 14:18:08.922942: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\nWARNING:tensorflow:From /home/anaconda3/envs/dynim/lib/python3.7/site-packages/tensorflow_core/lite/python/util.py:249: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.convert_variables_to_constants`\r\nW1029 14:18:08.937568 140612085802816 deprecation.py:323] From /home/anaconda3/envs/dynim/lib/python3.7/site-packages/tensorflow_core/lite/python/util.py:249: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.convert_variables_to_constants`\r\nWARNING:tensorflow:From /home/anaconda3/envs/dynim/lib/python3.7/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.extract_sub_graph`\r\nW1029 14:18:08.937732 140612085802816 deprecation.py:323] From /home/anaconda3/envs/dynim/lib/python3.7/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.extract_sub_graph`\r\nINFO:tensorflow:Froze 72 variables.\r\nI1029 14:18:08.979997 140612085802816 graph_util_impl.py:334] Froze 72 variables.\r\nINFO:tensorflow:Converted 72 variables to const ops.\r\nI1029 14:18:08.988762 140612085802816 graph_util_impl.py:394] Converted 72 variables to const ops.\r\n2020-10-29 14:18:08.996008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 14:18:08.996231: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2020-10-29 14:18:08.996275: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-10-29 14:18:08.996592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 14:18:08.996781: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62\r\npciBusID: 0000:01:00.0\r\n2020-10-29 14:18:08.996805: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2020-10-29 14:18:08.996814: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2020-10-29 14:18:08.996821: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2020-10-29 14:18:08.996830: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2020-10-29 14:18:08.996838: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2020-10-29 14:18:08.996846: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2020-10-29 14:18:08.996853: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-10-29 14:18:08.996885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 14:18:08.997076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 14:18:08.997241: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2020-10-29 14:18:08.997260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-10-29 14:18:08.997265: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \r\n2020-10-29 14:18:08.997268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n2020-10-29 14:18:08.997320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 14:18:08.997593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 14:18:08.997811: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2968 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2020-10-29 14:18:09.044065: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: graph_to_optimize\r\n2020-10-29 14:18:09.044089: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 184 nodes (-67), 191 edges (-60), time = 21.371ms.\r\n2020-10-29 14:18:09.044094: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 184 nodes (0), 191 edges (0), time = 6.549ms.\r\n2020-10-29 14:18:09.044098: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: superpoint_pred_tower0_map_while_body_529\r\n2020-10-29 14:18:09.044102: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 43 nodes (0), 49 edges (0), time = 0.965ms.\r\n2020-10-29 14:18:09.044106: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 43 nodes (0), 49 edges (0), time = 0.767ms.\r\n2020-10-29 14:18:09.044110: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: superpoint_pred_tower0_map_while_cond_528\r\n2020-10-29 14:18:09.044114: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 10 nodes (0), 7 edges (0), time = 0.403ms.\r\n2020-10-29 14:18:09.044119: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 10 nodes (0), 7 edges (0), time = 0.141ms.\r\n```\r\nSame issue when use TFLiteConverter:\r\n```\r\nsaved_model_dir = \"superpoint_640x480x_control_flow\"\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir, input_arrays=[\"superpoint/image\"] , input_shapes={\"superpoint/image\" : [1, 480, 640, 1]}, output_arrays=['superpoint/logits', 'superpoint/prob', 'superpoint/descriptors_raw', 'superpoint/descriptors', 'superpoint/prob_nms', 'superpoint/pred'])\r\nconverter.target_spec.supported_ops = [\r\n  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\r\n  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\r\n]\r\nconverter.allow_custom_ops = True\r\nconverter.experimental_new_converter = True\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_types = [tf.float32]\r\n\r\ntflite_model = converter.convert()\r\nopen(\"testing.tflite\", \"wb\").write(tflite_model)\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\nReproduce with cpp code :\r\n```\r\n#include <iostream>\r\n\r\n#include <tensorflow/tensorflow/lite/interpreter.h>\r\n#include <tensorflow/tensorflow/lite/kernels/register.h>\r\n#include <tensorflow/tensorflow/lite/string_util.h>\r\n#include <tensorflow/tensorflow/lite/model.h>\r\n\r\nint main()\r\n{\r\n    std::unique_ptr<tflite::FlatBufferModel> model = tflite::FlatBufferModel::BuildFromFile(\"testing.tflite\");\r\n    if (!model)\r\n    {\r\n        std::cerr << \"Failed to mmap tflite model\" << std::endl;\r\n        exit(0);\r\n    }\r\n    tflite::ops::builtin::BuiltinOpResolver resolver;\r\n    std::unique_ptr<tflite::Interpreter> interpreter;\r\n    if (tflite::InterpreterBuilder(*model.get(), resolver)(&interpreter) != kTfLiteOk)\r\n    {\r\n        std::cerr << \"Failed to interpreter tflite model\" << std::endl;\r\n        exit(0);\r\n    }\r\n    interpreter->AllocateTensors();\r\n}\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Output Logs**\r\nERROR: Encountered unresolved custom op: TensorListReserve.\r\nERROR: Node number 1 (TensorListReserve) failed to prepare.\r\n\r\nI couldn't allocate input tensor memory, probably because of interpreter->AllocateTensors() failed.\r\n", "comments": ["Solved by adding : **tf.enable_resource_variables()**, tf.enable_control_flow_v2() in export model script, and use this script to convert saved model to tflite\r\n\r\n`converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\nconverter.allow_custom_ops = True\r\n**converter.experimental_new_converter = False**\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\r\ntflite_model = converter.convert()\r\n`\r\n"]}, {"number": 44423, "title": "Questions about quantization aware trained tflite model", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source): 2.3\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nfrozen_graph='./tflite_graph.pb'\r\ninput_arrays=[\"normalized_input_image_tensor\"]\r\noutput_arrays=['TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3']\r\ninput_shapes={\"normalized_input_image_tensor\":[1,300,600,3]}\r\nconverter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(frozen_graph,input_arrays,output_arrays,input_shapes=input_shapes)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.allow_custom_ops=True\r\n\r\ntflite_quant_model = converter.convert()\r\n\r\n# Save the model.\r\nwith open('ssd_mobilenet_v2_traffic_quanaware_int8.tflite', 'wb') as f:\r\n    f.write(tflite_quant_model)\r\n\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n\r\n\r\n\r\n[converted_model.zip](https://github.com/tensorflow/tensorflow/files/5456642/converted_model.zip)\r\n\r\n\r\n\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n\r\n[model.zip](https://github.com/tensorflow/tensorflow/files/5456644/model.zip)\r\n\r\n\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- Producing wrong results and/or decrease in accuracy\r\n- Producing correct results, but the model is slower than expected (model generated from old converter)\r\n\r\nI trained a mobilenet v2 ssd using quantization aware training, and the pb file contains FakeQuantWithMinMaxVars nodes.\r\n\r\nI have three questions:\r\n\r\n1. The size of the tflite model I converted using Python API is bigger than the size of the pb file. Is this correct?\r\n2. Is the quantized tflite model with int8 weights and uint8 activations?\r\n3. If I use the TFLite converter(Python API), do the FakeQuantWithMinMaxVars nodes be replaced with the quantize and dequantize nodes?\r\n\r\n\r\n", "comments": ["2. Currently your code performs dynamic range quantization which may leave some of the data in float format.\r\nFor enforcing `int8` quantization you want to try [integer-only_quantization ](https://www.tensorflow.org/lite/performance/post_training_integer_quant#convert_using_integer-only_quantization)\r\nNote that this approach raises errors where the quantization fails and in such cases you can still move ahead with dynamic range quantization.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44422, "title": "I have been trying to implement a custom tflite model on android studio. Example object detection works. But when I change the tflite file to my model, app gets installed. App closes as soon as it opens saying \"app keeps stopping\". Please help me. I am pretty new to this.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@Bio-reactor \r\n\r\nPlease, fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).Please, let us know which TF version we are using?.\r\n\r\nProvide the exact sequence of commands / steps that you executed before running into the problem.Thanks!", "Hi, please provide more information.\r\n\r\nMost likely, you changed the model but didn't keep the consistent input and output.", "Hi, \r\nI retrained the ssd_mobilenet for custom object recognition. I converted the frozen model into tflite file. I just replaced the tflite file in the assets folder and changed the labelmap.txt file. I checked with just the object detction model that is just using the detect.tflite and labelmap.txt. It works fine. When I change it to my model, the app keeps stopping", "System information\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung a20\r\nTensorFlow installed from (source or binary):\r\nTensorFlow version:\r\nPython version:\r\nInstalled using virtualenv? pip? conda?:\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:n/a\r\nGPU model and memory:n/a", "@Bio-reactor \r\n\r\nJust to verify did you follow the [tutorial](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android).Also, please see input and output are consistant in your model. Thanks!", "@ravikyram yes I am following the same tutorial. I have placed my tflite file and labelmap under assests folder to get that working. I am attaching the same here. App is actually working now. But doesn't detect anything. It throws the following exception:\r\n\r\n2020-11-05 20:35:21.492 30081-30081/org.tensorflow.lite.examples.detection E/tensorflow: CameraActivity: Exception!\r\n    java.lang.IllegalStateException: This model does not contain associated files, and is not a Zip file.\r\n        at org.tensorflow.lite.support.metadata.MetadataExtractor.assertZipFile(MetadataExtractor.java:325)\r\n        at org.tensorflow.lite.support.metadata.MetadataExtractor.getAssociatedFile(MetadataExtractor.java:165)\r\n        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.create(TFLiteObjectDetectionAPIModel.java:127)\r\n        at org.tensorflow.lite.examples.detection.DetectorActivity.onPreviewSizeChosen(DetectorActivity.java:99)\r\n        at org.tensorflow.lite.examples.detection.CameraActivity.onPreviewFrame(CameraActivity.java:200)\r\n        at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1221)\r\n        at android.os.Handler.dispatchMessage(Handler.java:107)\r\n        at android.os.Looper.loop(Looper.java:237)\r\n        at android.app.ActivityThread.main(ActivityThread.java:7860)\r\n        at java.lang.reflect.Method.invoke(Native Method)\r\n        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:493)\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1075)\r\nPlease save the detect1.txt as detect.tflite. Here the uploading of the .tflite is not supported. Please help me on this.\r\n\r\n\r\n[labelmap.txt](https://github.com/tensorflow/tensorflow/files/5495188/labelmap.txt)\r\n[detect1.txt](https://github.com/tensorflow/tensorflow/files/5495193/detect1.txt)\r\n\r\n", "I got it fixed. My app is working. Thank you.", "@Bio-reactor \r\n\r\nPlease, close this thread if your issue was resolved.Thanks!", "Closing the issue since it was resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44422\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44422\">No</a>\n", "please how  you solve it I am new too ans spent two weeks trying everything. pleaseeeee help", "@Bio-reactor\r\nplease how you solve it I am new too ans spent two weeks trying everything. pleaseeeee help\r\n\r\n"]}, {"number": 44421, "title": "how to control the number of variables.data ?", "body": "I use tensorflow 2.2.0 ,python 3.6.9\r\n\r\nwhen I saved my model, how to control the number of files 'variables.data' ?\r\n\r\nin my machine with titan Xp, saved my model has 2 variables.data files that variables.data-00000-of-00002, variables.data-00001-of-00002.\r\n\r\nbut in gcp machine with telsa T4, saved my model has 1 variables.data file that variables.data-00000-of-00001\r\n\r\nit can be control by user?", "comments": ["@kja815,\r\nPlease go through this [training checkpoints guide](https://www.tensorflow.org/guide/checkpoint#saving_from_tfkeras_training_apis) for more information regarding saving checkpoints and let us know if it helps. Thanks!", "@amahendrakar \r\nguide is not helpful for me.\r\nI just want to know how to control the number of files 'variables.data' like variables.data-00000-of-000001.\r\nI want to control the number of files 'variables.data' to fixed number.\r\nbut when I use model.save(), tf make different number of files 'variables.data' for the same model with different machine.\r\n", "@kja815,\r\nThe `max_to_keep` argument of the `tf.train.CheckpointManager` function is used to control the number of checkpoints to be preserved. For more information please take a look at the documentation [here](https://www.tensorflow.org/api_docs/python/tf/train/CheckpointManager). \r\n\r\nAlso check [this gist](https://colab.research.google.com/gist/amahendrakar/2e1ced95f05e058a9c021d671fee6321/44421.ipynb#scrollTo=_3Iud6GWESq2&line=1&uniqifier=1) which demonstrates the same. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44420, "title": "how to use tensorflow_docs after installed ", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["Hi @haiou90, You didn't mention your use case. If it is just for building the API docs then you can follow the guide [here](https://www.tensorflow.org/community/contribute/docs#build_api_docs).\r\n\r\nAlso, it'd be great if you follow the [policies](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md) for raising issues.", "@haiou90 \r\nCould you please clarify the information required, refer to [this link](https://stackoverflow.com/questions/55535518/modulenotfounderror-no-module-named-tensorflow-docs-when-creating-tensorflow) for the doc, and this for [build api doc](https://www.tensorflow.org/community/contribute/docs#build_api_docs).", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44419, "title": "Downgrading the arm linux gnueabihf toolchain causes a linking error when buidling the label_image C++ example", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- Mobile device: n/a\r\n- TensorFlow installed from: source\r\n- TensorFlow version: master (669993e)\r\n- Python version: 3.7\r\n- Installed using: Bazel\r\n- Bazel version: 3.1.0\r\n- GCC/Compiler version: gcc-linaro-7.3.1-2018.05-x86_64_arm-linux-gnueabihf.tar.xz\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n**Describe the problem**\r\nBuilding the TensorFlow Lite `label_image` C++ example with the default _elinux_armhf_  bazel config results in the following error when running the program in the target environment:\r\n\r\n`/lib/libm.so.6: version 'GLIBC_2.27' not found (required by [...] label_image)`\r\n\r\nIn the target environment, doing `strings /lib/libm.so.6 | grep GLIBC` lists versions 2.4, 2.15, 2.18, 2.23, 2.24, and 2.25. Upgrading the environment is not an option. \r\n\r\nTo resolve this, I want to downgrade the arm linux gnu toolchain so that I can compile something that can run on the target environment. I build `label_image` after downgrading to the [7.3-2018.05 linaro toolchain](https://releases.linaro.org/components/toolchain/binaries/7.3-2018.05/) but the build fails due to a linking error:\r\n\r\n```\r\nERROR: /home/georges/dev/geo/tensorflow/tensorflow/lite/examples/label_image/BUILD:15:1: Linking of rule '//tensorflow/lite/examples/label_image:label_image' failed (Exit 1): arm-linux-gnueabihf-gcc failed: error executing command\r\n```\r\nI do not know how to investigate this further and I am not certain that the 7.3-2018.05 linaro toolchain is the toolchain I need to begin with. An attempt to compile with the 6.1-2016.08 and 6.5-2018.12 linaro toolchains produces similar linking errors. Branches for those attempts are also available [here](https://github.com/georgeslabreche/tensorflow/tree/toolchain-arm-linux-6.1-2016.08) and [here](https://github.com/georgeslabreche/tensorflow/tree/toolchain-arm-linux-6.5-2018.12).\r\n \r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n1. Downgrade the arm linux gnu toolchain [in the workspace.bzl and cc_config.bzl.tpl files](https://github.com/georgeslabreche/tensorflow/commit/e64b86ac5091e0853d67ff36368a5cf5a49b3b37). Branch available [here](https://github.com/georgeslabreche/tensorflow/tree/toolchain-arm-linux-7.3-2018.05).\r\n2. `bazel build --verbose_failures --config=elinux_armhf -c opt //tensorflow/lite/examples/label_image:label_image`\r\n3. Error will be thrown during linking.\r\n\r\n**Any other info / logs**\r\n```\r\ngeorges@oppenheimer:~/dev/geo/tensorflow$ bazel build --verbose_failures --config=elinux_armhf -c opt //tensorflow/lite/examples/label_image:label_image\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=245\r\nINFO: Reading rc options for 'build' from /home/georges/dev/geo/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/georges/dev/geo/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\nINFO: Found applicable config definition build:short_logs in file /home/georges/dev/geo/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/georges/dev/geo/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:elinux_armhf in file /home/georges/dev/geo/tensorflow/.bazelrc: --config=elinux --cpu=armhf\r\nINFO: Found applicable config definition build:elinux in file /home/georges/dev/geo/tensorflow/.bazelrc: --crosstool_top=@local_config_embedded_arm//:toolchain --host_crosstool_top=@bazel_tools//tools/cpp:toolchain\r\nINFO: Found applicable config definition build:linux in file /home/georges/dev/geo/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/georges/dev/geo/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nINFO: Analyzed target //tensorflow/lite/examples/label_image:label_image (20 packages loaded, 1240 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /home/georges/dev/geo/tensorflow/tensorflow/lite/examples/label_image/BUILD:15:1: Linking of rule '//tensorflow/lite/examples/label_image:label_image' failed (Exit 1): arm-linux-gnueabihf-gcc failed: error executing command\r\n  (cd /home/georges/.cache/bazel/_bazel_georges/711a8914001bc37d15d9e59048761a9c/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH='/home/georges/bin:/home/georges/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/mnt/c/Program Files/WindowsApps/CanonicalGroupLimited.Ubuntu16.04onWindows_1604.2019.523.0_x64__79rhkp1fndgsc:/mnt/c/Program Files (x86)/Common Files/Oracle/Java/javapath:/mnt/c/Program Files (x86)/Atmel/AVR Tools/AVR32 Toolchain/bin:/mnt/c/Windows/system32:/mnt/c/Windows:/mnt/c/Windows/System32/Wbem:/mnt/c/Windows/System32/WindowsPowerShell/v1.0/:/mnt/c/Windows/System32/OpenSSH/:/mnt/c/Program Files (x86)/Calibre2/:/mnt/c/Program Files/Git/cmd:/mnt/c/Program Files/Microsoft SQL Server/130/Tools/Binn/:/mnt/c/Program Files/Microsoft SQL Server/Client SDK/ODBC/170/Tools/Binn/:/mnt/c/Program Files/dotnet/:/mnt/c/Program Files/Docker/Docker/resources/bin:/mnt/c/ProgramData/DockerDesktop/version-bin:/mnt/c/Users/Georges/AppData/Local/Microsoft/WindowsApps:/mnt/c/Users/Georges/AppData/Local/Programs/Microsoft VS Code/bin:/mnt/c/Users/Georges/AppData/Local/GitHubDesktop/bin:/mnt/c/Users/Georges/Development/Tools/MongoShell/bin:/mnt/c/Users/Georges/.dotnet/tools:/snap/bin' \\\r\n    PWD=/proc/self/cwd \\\r\n    TF2_BEHAVIOR=1 \\\r\n  /home/georges/.cache/bazel/_bazel_georges/711a8914001bc37d15d9e59048761a9c/external/armhf_linux_toolchain/bin/arm-linux-gnueabihf-gcc @bazel-out/armhf-opt/bin/tensorflow/lite/examples/label_image/label_image-2.params)\r\nExecution platform: @local_execution_config_platform//:platform\r\nbazel-out/armhf-opt/bin/tensorflow/lite/nnapi/libnnapi_implementation.a(nnapi_implementation.o): In function `(anonymous namespace)::ASharedMemory_create(char const*, unsigned int)':\r\nnnapi_implementation.cc:(.text._ZN12_GLOBAL__N_120ASharedMemory_createEPKcj+0x14): warning: the use of `tmpnam' is dangerous, better use `mkstemp'\r\nbazel-out/armhf-opt/bin/tensorflow/lite/kernels/internal/libneon_tensor_utils.a(neon_tensor_utils.o): In function `void ruy::RunPack<(ruy::Path)16, ruy::FixedKernelLayout<(ruy::Order)0, 16, 2>, signed char, signed char>(ruy::Tuning, ruy::EMat const&, ruy::PEMat*, int, int)':\r\nneon_tensor_utils.cc:(.text._ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi16ELi2EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi16ELi2EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x9e): undefined reference to `ruy::Pack8bitRowMajorForNeon(unsigned char const*, int, int, int, int, int, int, signed char*, int, int, int*, int, int)'\r\nneon_tensor_utils.cc:(.text._ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi16ELi2EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi16ELi2EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x16a): undefined reference to `ruy::Pack8bitColMajorForNeonOutOfOrder2Cols(ruy::PackParams8bit const&)'\r\nneon_tensor_utils.cc:(.text._ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi16ELi2EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi16ELi2EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x1e4): undefined reference to `ruy::Pack8bitColMajorForNeonOutOfOrder2Cols(ruy::PackParams8bit const&)'\r\nneon_tensor_utils.cc:(.text._ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi16ELi2EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi16ELi2EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x286): undefined reference to `ruy::Pack8bitColMajorForNeonOutOfOrder2Cols(ruy::PackParams8bit const&)'\r\nneon_tensor_utils.cc:(.text._ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi16ELi2EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi16ELi2EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x2f2): undefined reference to `ruy::Pack8bitColMajorForNeonOutOfOrder2Cols(ruy::PackParams8bit const&)'\r\nbazel-out/armhf-opt/bin/tensorflow/lite/kernels/internal/libneon_tensor_utils.a(neon_tensor_utils.o): In function `void ruy::RunPack<(ruy::Path)16, ruy::FixedKernelLayout<(ruy::Order)0, 16, 4>, signed char, signed char>(ruy::Tuning, ruy::EMat const&, ruy::PEMat*, int, int)':\r\nneon_tensor_utils.cc:(.text._ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi16ELi4EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi16ELi4EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0xa8): undefined reference to `ruy::Pack8bitRowMajorForNeon(unsigned char const*, int, int, int, int, int, int, signed char*, int, int, int*, int, int)'\r\nneon_tensor_utils.cc:(.text._ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi16ELi4EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi16ELi4EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x196): undefined reference to `ruy::Pack8bitColMajorForNeonOutOfOrder4Cols(ruy::PackParams8bit const&)'\r\nneon_tensor_utils.cc:(.text._ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi16ELi4EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi16ELi4EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x22e): undefined reference to `ruy::Pack8bitColMajorForNeonOutOfOrder4Cols(ruy::PackParams8bit const&)'\r\nneon_tensor_utils.cc:(.text._ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi16ELi4EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi16ELi4EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x2fc): undefined reference to `ruy::Pack8bitColMajorForNeonOutOfOrder4Cols(ruy::PackParams8bit const&)'\r\nneon_tensor_utils.cc:(.text._ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi16ELi4EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi16ELi4EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x382): undefined reference to `ruy::Pack8bitColMajorForNeonOutOfOrder4Cols(ruy::PackParams8bit const&)'\r\nbazel-out/armhf-opt/bin/tensorflow/lite/kernels/internal/libneon_tensor_utils.a(neon_tensor_utils.o): In function `ruy::RunKernel<ruy::Kernel<(ruy::Path)16, signed char, signed char, int, int> >::Run(ruy::Tuning, ruy::SidePair<ruy::PEMat> const&, void const*, ruy::SidePair<int> const&, ruy::SidePair<int> const&, ruy::EMat*)':\r\nneon_tensor_utils.cc:(.text._ZN3ruy9RunKernelINS_6KernelILNS_4PathE16EaaiiEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE16EaaiiEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x150): undefined reference to `ruy::Kernel8bitNeonOutOfOrder(ruy::KernelParams8bit<4, 2> const&)'\r\nneon_tensor_utils.cc:(.text._ZN3ruy9RunKernelINS_6KernelILNS_4PathE16EaaiiEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE16EaaiiEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x192): undefined reference to `ruy::Kernel8bitNeonOutOfOrder1Col(ruy::KernelParams8bit<4, 2> const&)'\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow/lite/examples/label_image:label_image failed to build\r\nINFO: Elapsed time: 294.772s, Critical Path: 62.65s\r\nINFO: 1243 processes: 1243 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n", "comments": ["Plz try CMake to build ARM binary with custom toolchain.\r\nhttps://www.tensorflow.org/lite/guide/build_cmake_arm", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44419\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44419\">No</a>\n"]}, {"number": 44418, "title": "Can tflite.interpreter.set_tensor set an inter tensor", "body": "**System information**\r\n- OS Platform and Distribution: Windows 10\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 1.15.0\r\n\r\n**Problem Description**\r\nI have converted my model using float fallback quantization. The figure below is the input part of my float fallback model.\r\n\r\n![image](https://user-images.githubusercontent.com/49788188/97520323-e7a4d580-19d5-11eb-8c2e-f9b790d69d55.png)\r\n\r\nFirstly inference: I used tflite.interpreter.set_tensor to set my test_data to the input tensor \"in_tensor\", and stored the value of quantified input data (the output port of \"quantize\" in the figure). \r\n\r\nSecondly inference: I set the data stored above to the tensor which is the output of \"quantize\" in the figure( same as the tensor data stored). \r\n\r\nThe result of the first reasoning is correct, and the result of the second reasoning is incorrect.\r\n\r\nI think it might be tflite.interpreter.set_tensor can only set an input tensor but not an internal tensor. Hoping you giving me some guidance.", "comments": ["@Harleytu,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and the dataset you are using. \r\n\r\nAlso, TensorFlow 1.x is not actively supported, could you please update TensorFlow to v2.3 and check if you are facing the same issue. Thanks!", "@amahendrakar, \r\nThank you for your comment, but I'm sorry that I can't open my source code. \r\n\r\nAnd my layers in the CNN is built by tflearn. I still want to keep it in Tensorflow 1.x if possible. ", "Maybe `tflite.interpreter.set_tensor` can only set an input tensor."]}, {"number": 44417, "title": "Random error with conv_grad_filter_ops", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): TF 2.2\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.0.194 / 8.0.1\r\n- GPU model and memory: GeForce RTX 2070 SUPER\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nWe have a repo, with a few hundreds \"unit\" tests.\r\nDuring the CI, in 5-10% of the runs, one of the largest tests (always the same one) is failing with the following log (repeated many times):\r\n\r\n`tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at conv_grad_filter_ops.cc:1101 : Not found: No algorithm worked!`\r\n\r\nThe test suite is a mix of eager execution and graph mode tests.\r\n\r\nThis particular failing test is building a network built with CNNs (Resnet backbone, then object detection), and it's using the estimator API. Its only exotic feature is the use of RaggedTensors.\r\nAnother test is very similar (but no RaggedTensor) and has no issue.\r\n\r\nBoth these tests are running towards the end of the suite, so after a lot of models/variables have been instantiated and deleted.\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nA more informative error message.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nUnfortunately, this error can be reproduced only randomly and with the whole repo (even changing the order of the tests can have an impact).\r\nI'm aware this ticket is not super helpful, but I hope someone will have an idea about what's happening ...\r\n\r\n\r\n**Other info / logs**\r\n\r\nStack trace looks like:\r\n```\r\n[2020-10-27T07:40:30.393Z] tensorflow.python.framework.errors_impl.NotFoundError: No algorithm worked!\r\n[2020-10-27T07:40:30.393Z] \t [[node SGD/gradients/gradients/.../Conv2D_grad/Conv2DBackpropFilter (defined at ...) ]]\r\n[2020-10-27T07:40:30.393Z] \r\n[2020-10-27T07:40:30.393Z] Errors may have originated from an input operation.\r\n[2020-10-27T07:40:30.393Z] Input Source operations connected to node SGD/gradients/gradients/.../Conv2D_grad/Conv2DBackpropFilter:\r\n[2020-10-27T07:40:30.393Z]  .../BiasAdd (defined at ...)\r\n```\r\n\r\n", "comments": ["@jnd77 \r\nPlease provide with minimal reproducible code or a colab gist with the error reported.", "@Saduf2019 Thanks for looking at it.\r\nAs I mention in the ticket, it's impossible for me to reproduce the error with some minimal code. It's happening randomly even with a stable code base and the same machine.\r\nI'm more hoping on someone being able to suggest what situation could create such error (since the message is not that helpful).\r\nI guess in some cases, CUDA/Cudnn is in some particular state due to the accumulation of past ops (RaggedTensors in particular have changing shapes from one training loop to the next).\r\n\r\nIf you believe not much can be done to help me, then I will close the ticket.\r\n\r\n", "You may want to try this [hack](https://github.com/tensorflow/tensorflow/issues/42728#issuecomment-682479892) suggested by the user.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@ymodak Thanks a lot for your message. I will try revert  to an older version of Cudnn and monitor.", "@ymodak I ended up updating Cudnn to 8.0.4 (looks like the first versions 8.0 and 8.0.1 were unstable), and it looks good so far.\r\nThanks.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44417\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44417\">No</a>\n", "@jnd77 I just posted this workaround on the other linked issue. It may or may not be relevant for you too.\r\n\r\nPut this at the start of your code.\r\n\r\n```\r\nfrom tensorflow.compat.v1 import ConfigProto\r\nfrom tensorflow.compat.v1 import InteractiveSession\r\n\r\nconfig = ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsession = InteractiveSession(config=config)\r\n```", "Thanks @maxeonyx but that wasn't the fix for me.\r\nallow_growth is always true in my setup via the env variable (no choice for RTX cards)."]}, {"number": 44416, "title": "pip install tensorflow fails on macOS and Python 3.9", "body": "**System information**\r\n- OS Platform and Distribution: macOS 10.15.7 (19H2)\r\n- TensorFlow version: tf-nightly\r\n- Python version: 3.9.0\r\n- Installed using virtualenv? pip? conda?: pip, venv\r\n\r\n**Describe the problem**\r\n\r\n```\r\nERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\r\nERROR: No matching distribution found for tensorflow\r\n```\r\n\r\n**Provide the exact sequence of commands that you executed before running into the problem**\r\n\r\n```\r\npython3 -m venv ./test\r\nsource ./test/bin/activate\r\npython3 -m pip install --quiet --upgrade pip\r\npython3 -m pip install --upgrade tensorflow\r\n```\r\n\r\n```\r\npython3 -m venv ./test\r\nsource ./test/bin/activate\r\npython3 -m pip install --quiet --upgrade pip\r\npython3 -m pip install --upgrade nightly\r\n```\r\n\r\n**Any other info / logs**\r\n\r\nHomebrew recently updated to Python 3.9.\r\n\r\n@mihaimaruseac @goldiegadde @Saduf2019", "comments": ["@lutzroeder,\r\nTensorFlow pip packages are compatible with Python 3.5\u20133.8.\r\n\r\nFrom the below link we can see that the whl files are available upto Python v3.8.\r\nhttps://pypi.org/project/tf-nightly/2.4.0.dev20201023/#files\r\n\r\nPython3.9 is not yet supported. Thanks!\r\n", "We have not released anything for python3.9.\r\n\r\nRelease process is as follows:\r\n\r\n1. Ensure all of our dependencies support python3.9 (not yet there)\r\n2. Ensure our code works with all these dependencies on all supported versions of python (at the moment, at least `gast` needs additional support work).\r\n3. Release nightly version supporting the new python version\r\n4. Release a full release.\r\n\r\nAs we are already doing the 2.4 release, python3.9 support for TF will come in 2.5 release at the earliest.\r\n\r\n@ravikyram let's deduplicate to this issue all py39 requests/issues. I'll monitor this one and provide updates as we start adding support.", "Actually, duplicating to #44485 since that one has a better title.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44416\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44416\">No</a>\n", "@mihaimaruseac When is it expected the 2.5 release? I'm asking because I'm having an [issue](https://github.com/tensorflow/tensorflow/issues/45546) with tensorflow versions 2.2, 2.3, 2.4 in which the GPU usage is 0% during the training unless I disable eager execution using `tf.compat.v1.disable_eager_execution()` hopefully the new release fixes the issue.", "The plan we're having is to have one release per quarter."]}, {"number": 44415, "title": "Fix typos in lite directory", "body": "Splitting #43857 by top-level directories.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "Check out this pull request on&nbsp; <a href=\"https://app.reviewnb.com/tensorflow/tensorflow/pull/44415\"><img align=\"absmiddle\"  alt=\"ReviewNB\" height=\"28\" class=\"BotMessageButtonImage\" src=\"https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png\"/></a> \n\n See visual diffs & provide feedback on Jupyter Notebooks. \n\n---\n\n <i>Powered by <a href='https://www.reviewnb.com/?utm_source=gh'>ReviewNB</a></i>", "Hi Dmitry, \r\n\r\nCan you revert changes in `tensorflow/lite/delegates/gpu/cl/kernels/conv_constants.cc` and `conv_constants.h` as it causes conflict? Thank you!\r\n\r\n", "@lintian06, only `tensorflow/lite/delegates/gpu/cl/kernels/conv_constants.cc` was causing conflict, I resolved it."]}, {"number": 44414, "title": "Model not deterministic, even though os.environ['TF_DETERMINISTIC_OPS'] = '1' is set", "body": "in reference of this [kaggle notebook](https://www.kaggle.com/devang/transfer-learning-with-keras-and-efficientnets) i've written this [notebook](https://drive.google.com/file/d/1VyaUGXgaCDIbWbt0-F0iHTIdokxWesJH/view?usp=sharing) to try transfer learning through multiple datasets.\r\n\r\nWith multiple soft runs on the notebook, I came to found out that I can't get deterministic/reproducible model out of it. \r\n\r\nWeird thing is, with the same virtual-environment, I can get deterministic/reproducible from a plain [CNN-MNIST](https://drive.google.com/file/d/1GpTupofffdaEufpMSvDkqnTpjiqP3mwC/view?usp=sharing) code. \r\n\r\nSpent a week on researching and can't find the solution. Any guidance or suggestions are much appreciated. \r\n\r\n[my environment](https://drive.google.com/file/d/1lfM5y2vylh9HkuduargAon9arNQKB9kQ/view?usp=sharing)\r\n", "comments": ["@stevenwong951028,\r\nOn running the code, I am facing an error stating `FileNotFoundError: [Errno 2] No such file or directory: 'D:/img_db/stanford-dogs-dataset/images/Images/'`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/8e0360e4664bb42465019d639a85e82d/44414.ipynb#scrollTo=SDALvhOKhj1m).\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the dataset you are using in the code.\r\n\r\nAlso, the example is fairly complex and would be difficult for us to pinpoint the issue. Can you remove the dependencies + get the example down to the simplest possible repro? That will allow us to debug the issue easily. Thanks!\r\n", "@amahendrakar \r\nhere's a updated [colab code](https://colab.research.google.com/drive/1cqqdmrsQrlA7DDehnoEOXxqIt9WiwIMr?usp=sharing) for reproducing the error (dataset download included in code)\r\n\r\nmight take a while for downloading the dataset and extracting the images", "@stevenwong951028,\r\nOn running the code I did not face any errors. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/80bab1577ed23f14476266acf011c537/44414.ipynb).\r\n\r\n> With multiple soft runs on the notebook, I came to found out that I can't get deterministic/reproducible model out of it.\r\n\r\nCould you please elaborate the issue you are facing and also the behavior you are expecting? Thanks!", "@amahendrakar \r\nThe error is that the code won't get reproducible results. \r\n\r\nTo elaborate on that, in gpu environment, I get different results (accuracy, loss, val_accuracy and val_loss) on every runs. With seeding and tf_deterministic_ops all set.  \r\n\r\nThe weird part is, I can get reproducible results on a plain cnn-mnist (in gpu environment)\r\n\r\nThe results should be reproducible on every runs considering the seedings and tf_deterministic_ops. Which can be obtained in cpu environment, but too time consuming on every runs. ", "@stevenwong951028,\r\nSorry for the delayed response. Please refer this [Stack Overflow Answer](https://stackoverflow.com/questions/50744565/how-to-handle-non-determinism-when-training-on-a-gpu) which shows how to handle non-determinism with `Tensorflow-GPU`. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44413, "title": "[ r2.4 Cherrypick]: Rollback Infer local ip in connect_to_cluster.", "body": "PiperOrigin-RevId: 339573404\r\nChange-Id: I9d9ad47cddff454115931b70a752a2cfb93d5762", "comments": []}, {"number": 44412, "title": "[ROCm] Fix for ROCm CSB breakage - 201028", "body": "The following tests started failing with the weekly merge for 201026\r\n\r\n```\r\n//tensorflow/python:stateful_random_ops_test                             FAILED in 3 out of 3 in 6.1s\r\n//tensorflow/python:stateful_random_ops_test_gpu                         FAILED in 3 out of 3 in 6.1s\r\n```\r\n\r\nThe error message for those failures indicates that the failures are due to lack of support for the `Bitcast` operator on the ROCm platform, and this commit adds that missing support. The support was already available for the CUDA platform, so enabling it for ROCm is easy.\r\n\r\nexample error message\r\n\r\n```\r\n======================================================================\r\nERROR: testSameAsOldRandomOpsGPU (__main__.StatefulRandomOpsTest)\r\ntestSameAsOldRandomOpsGPU (__main__.StatefulRandomOpsTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/stateful_random_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1436, in decorated\r\n    return f(self, *args, **kwargs)\r\n  File \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/stateful_random_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1502, in decorated\r\n    return f(self, *args, **kwargs)\r\n  File \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/stateful_random_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/ops/stateful_random_ops_test.py\", line 451, in testSameAsOldRandomOpsGPU\r\n    self._sameAsOldRandomOps(test_util.gpu_device_name(), GPU_FLOATS)\r\n  File \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/stateful_random_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/ops/stateful_random_ops_test.py\", line 430, in _sameAsOldRandomOps\r\n    compare(dtype, old_normal, new_normal)\r\n  File \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/stateful_random_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/ops/stateful_random_ops_test.py\", line 398, in compare\r\n    self.assertAllEqual(run_old(), run_new())\r\n  File \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/stateful_random_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/ops/stateful_random_ops_test.py\", line 395, in run_new\r\n    return new(dtype, gen)\r\n  File \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/stateful_random_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/ops/stateful_random_ops_test.py\", line 410, in new_normal\r\n    return gen._standard_normal(shape, dtype=dtype)\r\n  File \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/stateful_random_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/ops/stateful_random_ops.py\", line 576, in _standard_normal\r\n    key, counter = self._prepare_key_counter(shape)\r\n  File \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/stateful_random_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/ops/stateful_random_ops.py\", line 630, in _prepare_key_counter\r\n    counter = array_ops.bitcast(counter_key[:counter_size], dtypes.uint64)\r\n  File \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/stateful_random_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/ops/gen_array_ops.py\", line 598, in bitcast\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/stateful_random_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/ops.py\", line 6862, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Could not satisfy device specification '/job:localhost/replica:0/task:0/device:GPU:0'. enable_soft_placement=0. Supported device types [CPU]. All available devices [/job:localhost/replica:0/task:0/device:GPU:0, /job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:CPU:1]. [Op:Bitcast]\r\n```\r\n\r\n\r\n--------------------------------------\r\n\r\n/cc @cheshire @chsigg @nvining-work ", "comments": []}, {"number": 44411, "title": "[Cherrypick r2.4] Delay start of profiler when configured", "body": "", "comments": ["Thanks Mihai, Goldie!"]}, {"number": 44410, "title": "Fix typos in go directory", "body": "Splitting #43857 by top-level directories.", "comments": []}, {"number": 44409, "title": "num_units in GRU and LSTM layers confuse meaning", "body": "In tf.keras.layers.LSTM tf.keras.layers.GRU layers there is a parameter called num_units. I saw a lot of questions over the internet about this parameter. and there is not clear answer for what this parameter mean expect for the obvious meaning which is the shape of the output. some say that its mean that in each layer there num_units of LSTM or GRU units, some say that it is only one unit of LSTM or GRU, but with num_units hidden units (num_units of tanh, and sigmoids for each  gate and so on..) inside the LSTM or GRU layer. \r\nI thought it might be propright that the developers of TF will answer one and for all what is the meaning of this parameter.\r\n\r\nThank You   ", "comments": ["Any update with this question?\r\n\r\n", "@ofiryaish Did you post this question in stackoverflow? Thanks!", "@gowthamkpr Yes, I tried there also after day or two in in which I didn't get response here, but I wasn't lucky, ", "@ofiryaish,\r\nThe argument, `num_units` in an `LSTM Layer` refers to number of `LSTM Units` in that `Layer`, with each `LSTM Unit` comprising the below `Architecture`.\r\n\r\n![image](https://user-images.githubusercontent.com/48206667/99224254-fbd93700-280b-11eb-954d-b7f479a5b84c.png)\r\n\r\n\r\n", "@ofiryaish,\r\nCan you please respond to the above comment. Thanks! ", "@rmothukuru  sorry for not responding.  So if I understand well you are stacking 'num_units' of units in this layer without any connection between them, and you only concatenate their outputs together, right? ", "@ofiryaish,\r\nYes, something like shown in the screenshot below:\r\n\r\n![image](https://user-images.githubusercontent.com/48206667/100097657-db038800-2e82-11eb-8c30-c5ed8e22c55d.png)\r\n\r\nThe above analysis has been done with respect to the below data:\r\n\r\nNumber of Input Features => 3 => F1, F2 and F3\r\n\r\nNumber of Timesteps => 2 => 0 and 1\r\n\r\nNumber of Hidden Layers => 1\r\n\r\n**Number of Units in each Hidden Layer => 5**\r\n\r\nHope this helps. Thanks!", "@rmothukuru \r\nOk, Thank you very much."]}, {"number": 44408, "title": "Fix typos in core directory", "body": "Splitting #43857 by top-level directories.", "comments": []}, {"number": 44407, "title": "axis with int64 is not supported by tflite for strided_slice op", "body": "Hi,\r\nRecentlly, I converted my pytorch model to pb using onnx-tf. I can convert successfully. But during inference, I found that in my pb model, the axis of strided_slice is int64. How can I convert int64 to int32 axis?\r\n\r\nHere is the error:\r\n> ERROR: tensorflow/lite/kernels/strided_slice.cc:155 op_context.begin->type != kTfLiteInt32 (4 != 2)\r\n> ERROR: Node number 22 (STRIDED_SLICE) failed to prepare.\r\n\r\nhttps://github.com/playbar/tfcmake/blob/e5c8319827fd06146f1edcc1557f0189e50da89d/tensorflow/contrib/lite/kernels/strided_slice.cc#L157\r\n\r\nI think this because the axis in slice op is int64 and tflite does not support int64.  Is there a method to solve this problem?\r\n\r\n\r\n - Python version: 3.6\r\n - ONNX version:   1.7\r\n - ONNX-TF version: 1.6\r\n - Tensorflow version: 1.15 \r\n\r\n\r\n\r\n\r\n", "comments": ["@snsun \r\nPlease, refer this #11318, [SO link ](https://stackoverflow.com/questions/47215739/operation-has-type-int32-that-does-not-match-type-int64)and see if it helps you.\r\n\r\nIf the problem still persists please share the exact sequence of commands / steps that you executed before running into the problem. So we will try to reproduce the issue from our end to localize the issue faster. Thanks!", "Thanks. I solved this problem by modifying the onnx-tf code. When onnx-tf is used to convert onnx model to pb model, the begin and end indices are int64. I cast them to int32 and the problem is solved. \r\nBy the way, I noticed a TODO in https://github.com/playbar/tfcmake/blob/e5c8319827fd06146f1edcc1557f0189e50da89d/tensorflow/contrib/lite/kernels/strided_slice.cc#L156\r\nI am curious why int64 is not supported now for strided_slice op. \r\n", "@snsun Casting is right approach here. \r\n\r\nCurrently int64 is not supported. Generally most of the use-cases are implemented on device and want to shrink the model size. For those use-cases, int64 is an overhead in terms of size/memory and speed. \r\n\r\nIf more users are looking for int64 support, then our team will prioritize it and will implement it. Thanks!\r\n\r\nI am closing this issue as this was resolved. Please feel free to reopen if I am mistaken. Thanks!\r\n", "@jvishnuvardhan \r\nHi, \r\ni'm using RaggedTensor for my model.\r\nWhen I converted the model into tflite, I found there is whole lot of FlexStridedSlice in the computation graph like below\r\n![image](https://user-images.githubusercontent.com/26551254/98449118-528fa280-2174-11eb-850c-fa838d025261.png)\r\n\r\nIs this because tflite's stridedslice doesn't support int64?\r\nIf true, can I cast RaggedTensor's row_split into int32?\r\n\r\nI'm also worried how tflite will deal with `-1` index if it ignores the 32th~63th bits.\r\n\r\nThanks", "@snsun \r\n> Thanks. I solved this problem by modifying the onnx-tf code. When onnx-tf is used to convert onnx model to pb model, the begin and end indices are int64. I cast them to int32 and the problem is solved.\r\n> By the way, I noticed a TODO in https://github.com/playbar/tfcmake/blob/e5c8319827fd06146f1edcc1557f0189e50da89d/tensorflow/contrib/lite/kernels/strided_slice.cc#L156\r\n> I am curious why int64 is not supported now for strided_slice op.\r\n\r\nI have the same problem. Can you let me know how I have to modify the onnx-tf code to cast?\r\n", "> @snsun\r\n> \r\n> > Thanks. I solved this problem by modifying the onnx-tf code. When onnx-tf is used to convert onnx model to pb model, the begin and end indices are int64. I cast them to int32 and the problem is solved.\r\n> > By the way, I noticed a TODO in https://github.com/playbar/tfcmake/blob/e5c8319827fd06146f1edcc1557f0189e50da89d/tensorflow/contrib/lite/kernels/strided_slice.cc#L156\r\n> > I am curious why int64 is not supported now for strided_slice op.\r\n> \r\n> I have the same problem. Can you let me know how I have to modify the onnx-tf code to cast?\r\n\r\nSame problem here and wonder how this is done", "> @snsun\r\n> \r\n> > Thanks. I solved this problem by modifying the onnx-tf code. When onnx-tf is used to convert onnx model to pb model, the begin and end indices are int64. I cast them to int32 and the problem is solved.\r\n> > By the way, I noticed a TODO in https://github.com/playbar/tfcmake/blob/e5c8319827fd06146f1edcc1557f0189e50da89d/tensorflow/contrib/lite/kernels/strided_slice.cc#L156\r\n> > I am curious why int64 is not supported now for strided_slice op.\r\n> \r\n> I have the same problem. Can you let me know how I have to modify the onnx-tf code to cast?\r\n\r\nSame problem here and wonder how this is done"]}, {"number": 44406, "title": "Remove patching of the downloaded CMSIS code.", "body": "Patching was added to support the need for full include paths for the Arduino build (and presumably other IDEs).\r\n\r\nHowever, it turns out there is a second find and replace of the include paths that happens somewhere in the project generation makefile magic via the `transform_source.py` script:\r\nhttps://github.com/tensorflow/tensorflow/blob/f9818f12c332e8a35dbc9042af55649be64f30c4/tensorflow/lite/micro/tools/make/helper_functions.inc#L223-L317\r\n\r\nAs a result, we do not need to patch the downloaded CMSIS directory and still support at least the Arduino. This allows us to avoid some of the challenges described in https://github.com/tensorflow/tensorflow/issues/44013 and https://github.com/tensorflow/tensorflow/issues/44261\r\n\r\n\r\nSince exactly how the paths are updated during the Arduino project generation is a bit of a mystery, here are some quick steps to confirm that the Arduino projects continue to have full include paths:\r\n\r\nGenerate the hello_world project:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile clean clean_downloads\r\nmake -f tensorflow/lite/micro/tools/make/Makefile generate_hello_world_arduino_project TARGET=arduino TAGS=cmsis-nn\r\n```\r\n\r\nConfirm that the downloaded CMSIS does not have any modifications to the include paths:\r\n```\r\ncd tensorflow/lite/micro/tools/make/\r\nhead -n 35 downloads/cmsis/CMSIS/NN/Source/ActivationFunctions/arm_nn_activations_q7.c\r\n```\r\n\r\nOutput:\r\n```cc\r\n#include \"arm_math.h\"\r\n#include \"arm_common_tables.h\"\r\n#include \"arm_nnfunctions.h\"\r\n```\r\n\r\nConfirm that the cmsis code within the Arduino tree has full paths:\r\n\r\n```\r\ncd tensorflow/lite/micro/tools/make/\r\nhead -n 35 gen/arduino_x86_64/prj/hello_world/arduino/src/tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Source/ActivationFunctions/arm_nn_activations_q7.c\r\n```\r\n\r\nOutput:\r\n```cc\r\n#include \"tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/DSP/Include/arm_math.h\"\r\n#include \"tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/DSP/Include/arm_common_tables.h\"\r\n#include \"tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnfunctions.h\"\r\n```\r\n\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@mansnils @freddan80: keeping you in the loop that I have confirmed Mans' result that patching the downloaded CMSIS is not needed and this PR removes it so that follow-on changes can be simplified."]}]