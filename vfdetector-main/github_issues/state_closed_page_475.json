[{"number": 39556, "title": "Much worse performance when using mixed precision training (using tensorflow.keras policy)", "body": "Hi! \r\n\r\nI'm training an image generative resnet-based model using MSE loss and Adam optimizer. I'm using the API tf.data to feed the model as fast as I can. Once I achieved a 100% GPU usage during training, I'm seeking for speeding up the process even more by using the experimental mixed_precision Keras API. I've previously tested this API by running this [tutorial](https://github.com/tlkh/pycon-sg19-tensorflow-tutorial) and it works as expected, getting an increase in performance of aproximatley 2x. However, when trying it with my model, the performance is much worse than using regular fp32 computation. I suspect there is something wrong with my model, however, I've tried to follow all the TensorCore performance considerations in order to correclty set the model's hyperparameters, such as channels and batch size being divisible by 8 and input images cast to float16 without any success\r\n\r\n**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 18.04.4 LTS\r\n- TensorFlow installed from (container): tensorflow/tensorflow:nightly-gpu\r\n- TensorFlow version (use command below): Tensorflow:  2.3.0-dev20200514\r\n- Python version: 3.6.9\r\n- CUDA/cuDNN version: V10.1.243 / CUDNN=7.6.4.38-1\r\n- GPU model and memory: RTX 2080 8GB\r\n\r\n**Describe the current behavior**\r\nWorse performance using mixed precision Keras API than using regular fp32 computation.\r\n**Describe the expected behavior**\r\nPerformance increases of aproximately 1.5x compared to fp32 computation.\r\n**Standalone code to reproduce the issue**\r\n[Colab](https://colab.research.google.com/drive/12sx2GQN5kwRke7fPkj5TkgtugN5y1X44?usp=sharing)\r\n\r\n**Other info / logs** \r\n`Compute dtype: float16\r\nVariable dtype: float32\r\nKeras:  2.2.4-tf\r\nTensorflow:  2.3.0-dev20200514\r\nImage format:  channels_last\r\n2020-05-14 19:34:27.261423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-05-14 19:34:27.261615: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1544] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2080 computeCapability: 7.5\r\ncoreClock: 1.815GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.23GiB/s\r\n2020-05-14 19:34:27.261632: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-05-14 19:34:27.261652: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-05-14 19:34:27.261663: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-05-14 19:34:27.261672: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-05-14 19:34:27.261682: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-05-14 19:34:27.261692: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-05-14 19:34:27.261701: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-05-14 19:34:27.261732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-05-14 19:34:27.261931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-05-14 19:34:27.262108: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1686] Adding visible gpu devices: 0\r\n2020-05-14 19:34:27.262119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1085] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-05-14 19:34:27.262124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1091]      0 \r\n2020-05-14 19:34:27.262127: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1104] 0:   N \r\n2020-05-14 19:34:27.262168: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-05-14 19:34:27.262368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-05-14 19:34:27.262551: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1230] Created TensorFlow device (/device:GPU:0 with 6300 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\nDevice : /device:CPU:0\r\n         type : CPU\r\n         desc :\r\n\r\nDevice : /device:XLA_CPU:0\r\n         type : XLA_CPU\r\n         desc :device: XLA_CPU device\r\n\r\nDevice : /device:XLA_GPU:0\r\n         type : XLA_GPU\r\n         desc :device: XLA_GPU device\r\n\r\nDevice : /device:GPU:0\r\n         type : GPU\r\n         desc :device: 0, name: GeForce RTX 2080, pci bus id: 0000:01:00.0, compute capability: 7.5\r\n\r\n2020-05-14 19:34:31.803789: I tensorflow/core/profiler/lib/profiler_session.cc:163] Profiler session started.\r\n2020-05-14 19:34:31.803820: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1365] Profiler found 1 GPUs\r\n2020-05-14 19:34:31.804185: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcupti.so.10.1\r\n2020-05-14 19:34:31.873411: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1487] CUPTI activity buffer flushed\r\nEpoch 1/3\r\n2020-05-14 19:34:39.768955: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-05-14 19:34:40.485835: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-05-14 19:34:43.260095: I tensorflow/compiler/jit/xla_compilation_cache.cc:314] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.`\r\n\r\nThank you in advance.", "comments": ["@alvarobasi,\r\nOn running the code, I am facing an error stating `NameError: name 'list_files' is not defined`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/764ed11bccc8d569b651f7fdf4ad4979/39556.ipynb#scrollTo=P895a6UMnZSo&line=6&uniqifier=1).\r\n\r\nCould you please provide the complete code and the dataset you are using. Thanks!", "Sure, [here](https://colab.research.google.com/drive/12sx2GQN5kwRke7fPkj5TkgtugN5y1X44?usp=sharing) it is. The dataset I\u00b4m using is the Celeba dataset loaded using `tfds.load()`.", "Hi! Any updates about this? I've noticed you got an error while getting the dataset inside the colab. In my case I got the database just by downloading it from this [link ](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html), reading all the files and storing their paths in an array and then loading them to a tf.Dataset by using from_tensor_slices method", "Can you update the colab to get the Dataset so that it can run successfully? Ideally, you would use tfds or just create a random numpy array with `np.rand` to generate the data, since all we are debugging here is performance.", "Sure, now it runs successfully. You can find the colab [here](https://colab.research.google.com/drive/12sx2GQN5kwRke7fPkj5TkgtugN5y1X44?usp=sharing). I created a random sample of aprox 2k matrices with the same shape as the celeba dataset ones. I ran this again on my RTX 2080 and it gave me the same results: much worse performance with mixed_float16... Unfortunatelly, I don't have access to a Tensor Core enabled GPU in colab.\r\n\r\nThanks in advance.", "@alvarobasi, thank you for filing this issue. The slowdown is caused by the last Conv2D in the model. As a temporary workaround, try passing `dtype=\"float32\"` to the final Conv2D layer of your model.\r\n\r\nI can reproduce with a standalone program:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport time\r\n\r\ndef bench(dtype, data_format):\r\n  if data_format == 'NHWC':\r\n    x = tf.random.normal((16, 128, 128, 256), dtype=dtype)\r\n    f = tf.random.normal((9, 9, 256, 3), dtype=dtype)\r\n  else:\r\n    x = tf.random.normal((16, 256, 128, 128), dtype=dtype)\r\n    f = tf.random.normal((9, 9, 256, 3), dtype=dtype)\r\n\r\n  p = tf.constant(0.)\r\n\r\n  # Warmup\r\n  tf.nn.conv2d(x, f, strides=[1, 1, 1, 1], padding='SAME',\r\n               data_format=data_format)\r\n\r\n  start = time.time()\r\n  for _ in range(10):\r\n    tf.nn.conv2d(x, f, strides=[1, 1, 1, 1], padding='SAME',\r\n                 data_format=data_format)\r\n  # Synchronize GPU by sending result of computation to CPU\r\n  p = p + 1.\r\n  p.numpy()\r\n\r\n  end = time.time()\r\n  print('time for %s %s: %s' % (dtype, data_format, end - start))\r\n\r\nbench('float32', 'NHWC')\r\nbench('float32', 'NCHW')\r\nbench('float16', 'NHWC')\r\nbench('float16', 'NCHW')\r\n```\r\n\r\nOn my Titan V, the output is:\r\n\r\n```\r\ntime for float32 NHWC: 0.06822085380554199\r\ntime for float32 NCHW: 0.059490203857421875\r\ntime for float16 NHWC: 0.9011242389678955\r\ntime for float16 NCHW: 0.05471491813659668\r\n```\r\n\r\nThe results are a bit misleading, as TensorFlow [silently converts a float32 NHWC Conv2D to NCHW](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/core/kernels/conv_ops.cc;l=751;drc=c6667ea3f238027a844062f104b191adf4242f54). If I remove the conversion from that file, float32 NHWC is about as slow as float16 NHWC.\r\n\r\n@nluehr is it expected for NHWC to be 15x slower than NCHW in some cases with float16 Volta/Turing? If so, what cases is NHWC slower? I think the best approach is to try to detect when NHWC is slower and if so, fallback to NCHW. Note tensor cores are not used since the filter is not a multiple of 8. \r\n\r\n", "@reedwm I ran your script with cudnn v7 on GV100 and I can get similar perf. In addition, I also gave a shot with cudnn v8 and got: \r\n```\r\ntime for float32 NHWC: 0.060801029205322266\r\ntime for float32 NCHW: 0.05140805244445801\r\ntime for float16 NHWC: 0.09125709533691406\r\ntime for float16 NCHW: 0.04851174354553223\r\n```\r\nThis time, the fp16 NHWC becomes <2x slower than NCHW.\r\n\r\nAfter some digging, I found this is more about the algorithm selection rather than tensor core selection. With cudnn v8, by using the tensor core we can see the perf is boosted from 50ms to 9ms for both NHWC/NCHW cudnn algo0. However, the NCHW supports algo5 (an FFT based algorithm) and it costs only 4.8ms. Unfortunately, this algo5 is not supported in NHWC. So, the above fp16 perf comparison is virtually NHWC algo0 vs NCHW algo5.\r\n\r\nSimilarly, since there is implicit format conversion for fp32, the NHWC/NCHW will both rely on the NCHW algo5 and attain ~5ms.\r\n\r\nAFAIK, the FFT algo doesn't support Tensor core. So, tensor core shouldn't matter, when FFT algo is preferred. For this case, I suggest users upgrade to cudnn v8 and check if the gap can be reduced at least.  ", "Same poor performance on 2080 Ti + TF compiled from source\r\n```\r\ntime for float32 NHWC: 0.12609028816223145\r\ntime for float32 NCHW: 0.08841919898986816\r\ntime for float16 NHWC: 1.4761300086975098\r\ntime for float16 NCHW: 0.06844806671142578\r\n```", "TF 2.4 uses cudnn 8 and the performance is similar with what @kaixih reported. So this issue is far less severe, as now float16 is less than 2x slower, while before it was between 15x and 20x slower. However, 2x slower is still significant.\r\n\r\nPerhaps we should use autotune both NCHW and NHWC and choose the faster one, transposing the input and output if necessary. Ideally grappler would optimizer the data format based on the performance, but I think implementing that is infeasible.\r\n\r\n/CC @sanjoy", "Yes, for this case, the cuDNN doesn't provide faster NHWC fp16 kernel than its NCHW counterparts. We need to sync with our cuDNN team to see if adding new algorithm or adding auto transformation is possible. ", "/CC @timshen91 ", "@alvarobasi,\r\nPerformance seems to be fine when **`tf.nn.conv2d`** is replaced with **`tf.keras.layers.Conv2D`**, with **`Tensorflow Version 2.5`**. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/aec4423e7764b70e7c80761761682ca8/gh_39556.ipynb) of the working code. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 39555, "title": "[Do not submit/review] Python stack trace pipelining sketch", "body": "This is a very rough sketch of Python stack trace eager pipelining.  It's not compilable.\r\n\r\nRemaining work for eager:\r\n1. Our fast Python stack trace class was landed https://github.com/tensorflow/tensorflow/commit/0e517a6fc0dc40926f68f4b7bf9c0337e3bc55a9 but reverted https://github.com/tensorflow/tensorflow/commit/9ab73c7bd10158e9ba4570ad331497254b05aa75 due to a linking issue.  Need to debug and land.\r\n2. Need to clean up this CL and land.  Also, in this CL the stack trace is saved at `AbstractContextInterface` class but a better place is `AbstractOperationInterface`.  \r\n\r\nRemaining work for graph:\r\n1. Expand ExperimentalDebugInfo proto https://github.com/tensorflow/tensorflow/blob/b1e813e2ec9634ec0e6562b836e372e393f3de43/tensorflow/core/framework/node_def.proto#L66 to support saving Python stack trace.\r\n2. Populate those fields when we construct NodeDef during graph building", "comments": ["@saxenasaurabh FYI", "@kkimdev This PR is in draft, any update on this? Please. Thanks!", "Hi, we landed deferred Python stack tracing class https://github.com/tensorflow/tensorflow/commit/299b1bf0ba5cd88ad469b6ab8576f025f1220fcb and a CL for piping the stack trace id through executor was out yesterday and getting reviewed.  (Though this wasn't specifically for tf.data, but for async eager error reporting, so it's a bit different than what's sketched here.)\r\n\r\nWill update here once the CL lands.", "Landed async stack trace error reporting https://github.com/tensorflow/tensorflow/commit/fa85309fb533988e8181becff5eee5644c1b72e3"]}, {"number": 39553, "title": "Error During Model Conversion from Keras to TFLite : AttributeError: 'Model' object has no attribute '_get_save_spec'", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):18.04\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (or github SHA if from source):2.2.0\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\nhttps://colab.research.google.com/drive/1APB9ATKjQHfOq8kq2_SvvNYYz_AVnnZh\r\n```\r\n# Copy and paste here the exact command\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n\r\n**The output from the converter invocation**\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-297-83ed6c530300> in <module>\r\n----> 1 converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n\r\n~/hdd/learning/dl/venv/lib/python3.6/site-packages/tensorflow/lite/python/lite.py in from_keras_model(cls, model)\r\n    430       # signature including the batch dimension specified by the user.\r\n    431       input_signature = _saving_utils.model_input_signature(\r\n--> 432           model, keep_original_batch_size=True)\r\n    433 \r\n    434     func = _saving_utils.trace_model_call(model, input_signature)\r\n~/hdd/learning/dl/venv/lib/python3.6/site-packages/tensorflow/python/keras/saving/saving_utils.py in model_input_signature(model, keep_original_batch_size)\r\n     75     TensorSpecs. This list does not contain the `training` argument.\r\n     76   \"\"\"\r\n---> 77   input_specs = model._get_save_spec(dynamic_batch=not keep_original_batch_size)  # pylint: disable=protected-access\r\n     78   if input_specs is None:\r\n     79     return None\r\n\r\nAttributeError: 'Model' object has no attribute '_get_save_spec'\r\n\r\n![Screenshot from 2020-05-15 00-17-16](https://user-images.githubusercontent.com/17315769/81973844-5b2e7280-9642-11ea-9424-8ed29566c46e.png)\r\n", "comments": ["As mentioned in one of the other thread, I replaced keras with tf.keras. The issue is now solved."]}, {"number": 39552, "title": "TensorFlow Hub hosted Mv1/Mv2 fails execution in tflite runtime", "body": "The mv1 and mv2 quantized models hosted here \r\n(mv1-224 0.25,0.5,0.75,1.0 and mv2-224)\r\n(https://www.tensorflow.org/lite/guide/hosted_models) fails execution on the tflite runtime on my custom app modified from TensorFlow's demo app, running on a Google Pixel 3a mobile. \r\n\r\nThe error was,\r\njava.lang.IllegalArgumentException: Cannot convert between a TensorFlowLite buffer with 150528 bytes and a Java Buffer with 602112 bytes.\r\n\r\nIt seems like there's an int8 vs float32 mismatch somewhere. However, if I manually take an mv2 and use the TFLite Converter (with quantization flag) to convert to a quantized model, it runs perfectly (both the models have the same size (hosted vs converted) hence ruling any issues of Converter not quantizing the model).\r\n\r\nCan you please let me know why the hosted model could be erring out? \r\n", "comments": ["Also, I am unable to generate/convert (to tflite) models for Mv3-224 (small and large) and the existing model fails similarly. \r\n\r\nHowever, the hosted Mnas models work perfectly well", "@vinodganesan Can you please explain what you meant by the following? Give us more details on what you have done taking the hosted models.\r\n> The mv1 and mv2 quantized models hosted here\r\n(mv1-224 0.25,0.5,0.75,1.0 and mv2-224)\r\n(https://www.tensorflow.org/lite/guide/hosted_models) fails execution on the tflite runtime on my custom app modified from TensorFlow's demo app, running on a Google Pixel 3a mobile.\r\n\r\nMy questions are\r\n1. from the hosted models, did you download saved model or tflite quantized model?\r\n2. which model you ported into mobile? What are the steps you followed?\r\n3. what do you mean by \"Also, I am unable to generate/convert (to tflite) models for Mv3-224 (small and large) and the existing model fails similarly.\"\r\n\r\nLet's take one at a time and understand the issue so that we can resolve faster. Thanks!\r\n", "Hi, \r\n\r\nPlease find my answer to your queries below\r\n\r\n1. I downloaded the tflite quantized model, \r\n2. I ported Mobilenet_V1_1.0_224_quant and Mobilenet_V2_1.0_224_quant, using an app modified from the existing tensorflow demo app.. It more or less uses the steps mentioned in this link (https://codelabs.developers.google.com/codelabs/recognize-flowers-with-tensorflow-on-android/)\r\n\r\n3. For Mv3, I took the tflite quantized model provided in https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet and tried running it, it failed too. \r\n\r\nHowever, if I take the float models provided and use TFLiteConverter with OPTIMIZE_FOR_SIZE flag, I get a quantized model with similar size as of that of hosted. However, I was successfully able to run these quantized models that I converted, on the app. \r\n\r\nLet me know if that clarifies. \r\n\r\nThanks,\r\nVinod", "@vinodganesan Can you please create a standalone code to reproduce the issue? what are the sizes of your model (that you converted) and the hosted model? Are they both full integer quantization or float quantization models? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39552\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39552\">No</a>\n"]}, {"number": 39551, "title": "Updating the visibility of TFE_GetServerDef to public", "body": "TFE_GetServerDef is used by the S4TF project here:  ([tensorflow/swift-apis](https://github.com/tensorflow/swift-apis/blob/bb621154f739739ba739d40977acc528ccd8fe1d/Sources/TensorFlow/Core/Runtime.swift))", "comments": ["@ematejska Can you please check @deadshotsb's comments and keep us posted. Thanks!", "Discussed with @jaingaurav.  Closing this pull request in favor of removing reference to TFE_GetServerDev in S4TF."]}, {"number": 39550, "title": "TPU issue using Colab - NotFoundError: 'AnonymousSeedGenerator' is neither a type of a primitive operation nor a name of a function registered in binary running on n-c0cd0e2d-w-0. Make sure the operation or function is registered in the binary running in this process. [Op:TakeDataset]", "body": "**System information**\r\nPip installed TensorFlow \r\nTensorFlow version (use command below): tf-nightly/ 2.2.0-dev20200502\r\nPython version: 3.6.9\r\n\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nWhen I try to run my model I get a NotFoundError when getting my dataset. The code works with a GPU but does not work with TPU.  I was previously getting a NotFoundError that had to do with PyFunc.\r\n\r\n**Describe the expected behavior**\r\nEpochs should run and give a result.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport requests\r\nimport os\r\nurl = 'http://' + os.environ['COLAB_TPU_ADDR'].split(':')[0] + ':8475/requestversion/2.2.0-dev20200312'\r\nresp = requests.post(url)\r\nprint(resp)\r\n%pip install tf-nightly==2.2.0-dev20200505\r\n```\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import backend as K\r\nimport os\r\nimport PIL\r\nimport csv\r\nimport shutil\r\nimport numpy as np\r\nimport sys\r\nfrom PIL import Image\r\nfrom tensorflow.keras import backend as K\r\nimport gc\r\n\r\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras import Model\r\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\r\n# from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\r\n\r\nfrom tensorflow.keras.layers import Dense, Activation, Flatten\r\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\r\n\r\n#*********************** tried this first ***************************************\r\n# resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\r\n# tf.config.experimental_connect_to_cluster(resolver)\r\n# # This is the TPU initialization code that has to be at the beginning.\r\n# tf.tpu.experimental.initialize_tpu_system(resolver)\r\n# strategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n#********************************************************************************\r\n\r\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\r\nprint('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\r\n\r\ntf.config.experimental_connect_to_cluster(tpu)\r\ntf.tpu.experimental.initialize_tpu_system(tpu)\r\nstrategy = tf.distribute.experimental.TPUStrategy(tpu)\r\n\r\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\r\n\r\n\r\nfrom PIL import Image\r\nprint(Image.__file__)\r\n\r\n# import Image\r\nprint(Image.__file__)\r\n```\r\n\r\n#clone and mount drives as needed\r\n\r\n```\r\ndef make_generator():\r\n\r\n    train_datagen = ImageDataGenerator(rescale=1/255, horizontal_flip=True, rotation_range=90)\r\n    \r\n    train_generator = train_datagen.flow_from_directory(\r\n        '/content/plant-path/tfdir/train/',  # This is the source directory for training images\r\n        target_size=(400, 400),  # All images will be resized to 150x150\r\n        batch_size=1, \r\n        \r\n        class_mode='sparse')\r\n    \r\n\r\n    return train_generator\r\n\r\ndef create_model():\r\n         pre_trained_model = InceptionV3(input_shape = (400, 400,3), include_top = False, weights = 'imagenet')\r\n\r\n# pre_trained_model = InceptionResNetV2(include_top=False, weights='imagenet', input_tensor=None, input_shape=(1024, 1024,3))\r\n\r\n         for layer in pre_trained_model.layers:\r\n                    if layer.name == 'mixed1':\r\n                                    break\r\n# #     print(layer.name)\r\n                    layer.trainable = False\r\n\r\n                    last_layer = pre_trained_model.get_layer('mixed7')\r\n                    last_output = last_layer.output\r\n\r\n         from tensorflow.keras.optimizers import RMSprop\r\n         from tensorflow.keras import regularizers\r\n \r\n\r\n         x = Flatten()(last_output)\r\n         x = layers.Dense(1024,  activation= 'relu')(x)\r\n         x = layers.Dropout(.2)(x)\r\n         x = layers.Dense(4, activation= 'softmax')(x)\r\n         modelin = Model(pre_trained_model.input, x)\r\n         return modelin\r\n\r\n\r\n\r\ndef get_callbacks(name_weights, patience_lr):\r\n    mcp_save = ModelCheckpoint(name_weights, save_best_only=True, monitor='val_acc', mode='max')\r\n#     reduce_lr_loss = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=patience_lr, verbose=1, epsilon=1e-4, mode='min')\r\n    return [mcp_save] #, reduce_lr_loss]\r\n\r\n\r\nbatch_size = 456\r\ndef scale(image, label):\r\n    image = tf.squeeze(image)\r\n    return image, label  \r\n\r\ndef get_dataset(i, batchsize = batch_size):\r\n  dataset = tf.data.Dataset.from_generator(make_generator, (tf.float32, tf.float32),\r\n     (tf.TensorShape([400, 400, 3]), tf.TensorShape([None])))\r\n  dataset = dataset.shuffle(buffer_size = 2280)  \r\n  dataset = dataset.cache()\r\n  val = dataset.skip(i*456).take(456).batch(batch_size, drop_remainder=True).prefetch(4)\r\n  \r\n  train = dataset.skip(i*456+456).take(1824).concatenate(dataset.take(456*i)).batch(batch_size, drop_remainder=True).prefetch(15)\r\n  return train, val\r\n```\r\n\r\n#K-fold algo\r\n\r\n\r\n```\r\nfor i in range(5):\r\n  \r\n  #********* this was created to trouble shoot but did not resolve issue\r\n  # dataset = tf.data.Dataset.from_generator(make_generator, (tf.float32, tf.float32),\r\n  #    (tf.TensorShape([1, 400, 400, 3]), tf.TensorShape([None])))\r\n  # dataset = dataset.shuffle(buffer_size = 2280)\r\n    \r\n    # val = dataset.skip(i*456).take(456).batch(batch_size, drop_remainder=True).prefetch(4)\r\n  \r\n    # train = dataset.skip(i*456+456).take(1824).concatenate(dataset.take(456*i)).batch(batch_size, drop_remainder=True).prefetch(15)  \r\n\r\n  #******************************************************  \r\n    \r\n    train, val = get_dataset(i) \r\n    \r\n    name_weights = \"/content/drive/My Drive/Plant/final_model_fold_D512_I400_mix_1_7_\" + str(i) + \".{epoch:02d}-{val_acc:.2f}.h5\"\r\n    callbacks = get_callbacks(name_weights = name_weights, patience_lr=10)\r\n\r\n    \r\n    with strategy.scope():\r\n       modelinc = create_model()\r\n       modelinc.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['acc'])\r\n\r\n    modelinc.fit(\r\n                dataset,\r\n                # batch_size = batch_size,\r\n                # steps_per_epoch=1824/batch_size,\r\n                epochs=25,\r\n                # shuffle=True,\r\n                # verbose=1,\r\n                # validation_data = val\r\n                 ) #,\r\n                #  callbacks = callbacks)\r\n    \r\n    print(modelinc.evaluate(val)) \r\n    K.clear_session()\r\n    \r\n    del name_weights\r\n    del callbacks\r\n\r\n    gc.collect()\r\n```\r\n\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n---------------------------------------------------------------------------\r\nNotFoundError                             Traceback (most recent call last)\r\n<ipython-input-7-4b2615ce6f75> in <module>()\r\n     15   #******************************************************\r\n     16 \r\n---> 17     train, val = get_dataset(i)\r\n     18 \r\n     19     name_weights = \"/content/drive/My Drive/Plant/final_model_fold_D512_I400_mix_1_7_\" + str(i) + \".{epoch:02d}-{val_acc:.2f}.h5\"\r\n\r\n5 frames\r\n<ipython-input-6-c4546546e618> in get_dataset(i, batchsize)\r\n     56   dataset = dataset.shuffle(buffer_size = 2280)\r\n     57   dataset = dataset.cache()\r\n---> 58   val = dataset.skip(i*456).take(456).batch(batch_size, drop_remainder=True).prefetch(4)\r\n     59 \r\n     60   train = dataset.skip(i*456+456).take(1824).concatenate(dataset.take(456*i)).batch(batch_size, drop_remainder=True).prefetch(15)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py in take(self, count)\r\n   1271       Dataset: A `Dataset`.\r\n   1272     \"\"\"\r\n-> 1273     return TakeDataset(self, count)\r\n   1274 \r\n   1275   def skip(self, count):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py in __init__(self, input_dataset, count)\r\n   3727         input_dataset._variant_tensor,  # pylint: disable=protected-access\r\n   3728         count=self._count,\r\n-> 3729         **self._flat_structure)\r\n   3730     super(TakeDataset, self).__init__(input_dataset, variant_tensor)\r\n   3731 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_dataset_ops.py in take_dataset(input_dataset, count, output_types, output_shapes, name)\r\n   6474       return _result\r\n   6475     except _core._NotOkStatusException as e:\r\n-> 6476       _ops.raise_from_not_ok_status(e, name)\r\n   6477     except _core._FallbackException:\r\n   6478       pass\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)\r\n   6810   message = e.message + (\" name: \" + name if name is not None else \"\")\r\n   6811   # pylint: disable=protected-access\r\n-> 6812   six.raise_from(core._status_to_exception(e.code, message), None)\r\n   6813   # pylint: enable=protected-access\r\n   6814 \r\n\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nNotFoundError: 'AnonymousSeedGenerator' is neither a type of a primitive operation nor a name of a function registered in binary running on n-c0cd0e2d-w-0. Make sure the operation or function is registered in the binary running in this process. [Op:TakeDataset]\r\n", "comments": ["At modelinc.fit please change dataset to train \r\nthanks!", "@dipoojo55 \r\nI ran the code shared and i am able to replicate the issue faced please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/da63b63985f128aeb1c984581ff5c257/untitled181.ipynb)", "Any update on this issue? Facing it in an AI Platform Notebook plugged to Cloud TPU v3-8. ", "This is due to the coordinator and TPU worker version don't match. Make sure they both use the same TF version or nightly instance.", "@sayakpaul Can you please check whether the issue is persisting even after matching TF versions as suggested by @rxsang . \r\n\r\nPlease close the issue if this was already resolved after matching the versions. Thanks! ", "@jvishnuvardhan at least check who has opened the issue and then post your suggestions :D\r\n\r\nHow can a non-member close an issue that they haven't created?", "@sayakpaul Sorry for the last comment. Thanks!", "@dipoojo55 Is this still an issue for you? I tried running your code but facing different error (could be due to data that I cannot access). [Here is a gist](https://colab.research.google.com/gist/jvishnuvardhan/f568d9ce7a6f1d9cc70bbb09a7be9fe9/untitled181.ipynb) for reference.  \r\n\r\nCan you please run the code yourself or share a standalone code and data to reproduce the issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "I'm having a similar NotFoundError issue with the ComputeBatchSize op.  @rxsang , can you elaborate on how to make sure the coordinator and worker have the same tf version?\r\n\r\nEdit: I guess we mean the TPU and the machine running the model are using the same TF version.  I got that set and this seems to have resolved the NotFoundError, so I think that's the root cause.", "Yes, you are right. Thanks for the confirmation @dails08 ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39550\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39550\">No</a>\n"]}, {"number": 39549, "title": "TypeError: unsupported operand type(s) for *: 'LSTM' and 'int'", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n\r\n\r\n         self.kernel = self.add_weight(shape=(input_dim, self.units * 4),\r\n                                      name='kernel',\r\n                                      initializer=self.kernel_initializer,\r\n\r\nTypeError: unsupported operand type(s) for *: 'LSTM' and 'int'", "comments": ["@alokssingh \r\n\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.Please, fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nRequest you to provide colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "Thanks @ravikyram  for the quick response. I have resolved it."]}, {"number": 39548, "title": "[INTEL MKL] Fix conv_ops_test and remapper_test", "body": "Fix two C++ test failures related to MKL ops.\r\n\r\n1. conv_ops_test       // MklConvOp does not support EXPLICIT padding\r\n2. remapper_test      // Fusion of MKL Conv and Mkl FusedBatchNorm is not supported\r\n\r\nThe fix is to disable the related tests with MKL build.", "comments": ["This broke Windows build\r\n\r\n```\r\ntensorflow/core/kernels/conv_ops_test.cc(1164): error C2121: '#': invalid character: possibly the result of a macro expansion\r\ntensorflow/core/kernels/conv_ops_test.cc(1153): error C2065: 'ifndef': undeclared identifier\r\ntensorflow/core/kernels/conv_ops_test.cc(1153): error C2146: syntax error: missing '>' before identifier 'INTEL_MKL'\r\ntensorflow/core/kernels/conv_ops_test.cc(1153): error C2065: 'endif': undeclared identifier\r\ntensorflow/core/kernels/conv_ops_test.cc(1164): error C2062: type 'unknown-type' unexpected\r\ntensorflow/core/kernels/conv_ops_test.cc(1153): error C2039: 'type': is not a member of '`global namespace''\r\ntensorflow/core/kernels/conv_ops_test.cc(1180): error C2121: '#': invalid character: possibly the result of a macro expansion\r\ntensorflow/core/kernels/conv_ops_test.cc(1169): error C2065: 'ifndef': undeclared identifier\r\ntensorflow/core/kernels/conv_ops_test.cc(1169): error C2146: syntax error: missing '>' before identifier 'INTEL_MKL'\r\ntensorflow/core/kernels/conv_ops_test.cc(1169): error C2065: 'endif': undeclared identifier\r\ntensorflow/core/kernels/conv_ops_test.cc(1180): error C2062: type 'unknown-type' unexpected\r\ntensorflow/core/kernels/conv_ops_test.cc(1169): error C2039: 'type': is not a member of '`global namespace''\r\ntensorflow/core/kernels/conv_ops_test.cc(1181): fatal error C1019: unexpected #else\r\n```\r\n\r\nAny idea what it might be?"]}, {"number": 39547, "title": "keras training parameter value incorrect", "body": "tensorflow ver: 2.1.0\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nclass MyModel(tf.keras.Model):\r\n\r\n    def __init__(self):\r\n        super(MyModel, self).__init__()\r\n        self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\r\n        self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\r\n        self.dropout = tf.keras.layers.Dropout(0.5)\r\n\r\n    def call(self, inputs, training=None):\r\n        x = self.dense1(inputs)\r\n        if training is True:\r\n            print(\"in training\")\r\n            x = self.dropout(x, training=training)\r\n        elif training is None:\r\n            print(\"training None\")\r\n        else:\r\n            print(\"not in training\")\r\n            \r\n        return self.dense2(x)\r\n\r\nmodel = MyModel()\r\n\r\noptimizer = tf.keras.optimizers.Adam(1e-4)\r\nloss = tf.keras.losses.CategoricalCrossentropy()\r\nmodel.compile(optimizer, loss)\r\nx = tf.random.normal((5,))\r\ny = tf.ones((5,))\r\nmodel.fit(x, y, epochs=1)\r\n```\r\nThe system report \"not in training first\", then \"in training\".\r\n![image](https://user-images.githubusercontent.com/731496/81953648-84e39b80-963a-11ea-8c4e-ac5b49afcb9e.png)\r\n", "comments": ["Was able to reproduce the issue with [TF v2.1](https://colab.research.google.com/gist/amahendrakar/6b23743610aa062475517c8b99923420/39547.ipynb). \r\n\r\nOn running the code with [TF v2.2](https://colab.research.google.com/gist/amahendrakar/f371dada0012444bf75b1e066629e285/39547.ipynb#scrollTo=oBTbHGrfovim) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/422599f4acaf4fa19ee7d223c0a92a80/39547-tf-nightly.ipynb), facing an error stating `    ValueError: Shapes (None, 1) and (None, 5) are incompatible`. Please find the attached gist. Thanks!", "@w19787 Thanks for the issue!\r\n\r\nI fixed the example to pass batched tensors for `x` and `y` (IIUC these were just dummy values to show the error, not the root cause of the error).\r\n\r\nIt is now passing in TF2.2 for me:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nclass MyModel(tf.keras.Model):\r\n\r\n    def __init__(self):\r\n        super(MyModel, self).__init__()\r\n        self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\r\n        self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\r\n        self.dropout = tf.keras.layers.Dropout(0.5)\r\n\r\n    def call(self, inputs, training=None):\r\n        x = self.dense1(inputs)\r\n        if training is True:\r\n            print(\"in training\")\r\n            x = self.dropout(x, training=training)\r\n        elif training is None:\r\n            print(\"training None\")\r\n        else:\r\n            print(\"not in training\")\r\n            \r\n        return self.dense2(x)\r\n\r\nmodel = MyModel()\r\n\r\noptimizer = tf.keras.optimizers.Adam(1e-4)\r\nloss = tf.keras.losses.CategoricalCrossentropy()\r\nmodel.compile(optimizer, loss)\r\nx = tf.random.normal((1, 5))\r\ny = tf.ones((1, 5))\r\nmodel.fit(x, y, epochs=1)\r\n```\r\n\r\nClosing bc I can't repro, but if you are still seeing this issue with TF2.2 please reopen!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39547\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39547\">No</a>\n"]}, {"number": 39545, "title": "Efficient allreduce is not supported Issue", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2 (stable)\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 10.1 / 7.6.4.38\r\n- GPU model and memory: Nvidia Tesla V100 /32GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\nv2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n\r\n**Describe the current behavior**\r\nI have written a custom loop to train a model using tf.strategy.MirroredStrategy().\r\nCurrently, I am training on a machine, equipped with 2 Nvidia V100 Tesla cards. The training is started and continuing as expected. But t I am getting a Warning from both of the GPUS, I feel this can harm the performance of the GPUs\r\n\r\n2020-05-14 15:04:14.880452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30261 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:40:00.0, compute capability: 7.0)\r\n2020-05-14 15:04:14.883196: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 30261 MB memory) -> physical GPU (device: 1, name: Tesla V100-PCIE-32GB, pci bus id: 0000:62:00.0, compute capability: 7.0)\r\nWARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\r\nWARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\r\n2020-05-14 15:05:55.050229: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n", "comments": ["@Nixon59-lab,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code and the dataset you are using to reproduce the issue reported here. Thanks!", "Original Script and dataset can be found here:\r\nhttps://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/image_captioning.ipynb\r\n\r\nModified custom loop:\r\n```\r\n@tf.function\r\ndef train_step(dist_inputs):\r\n\tdef step_fn(inputs):\r\n\t\timg_tensor, target = inputs\r\n\t\tprint(target.shape[0])\r\n\t\tloss = 0\r\n\r\n\t\t# initializing the hidden state for each batch because the captions are not related from image to image\r\n\t\thidden = decoder.reset_state(batch_size=target.shape[0])\r\n\r\n\t\tdec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\r\n\r\n\t\twith tf.GradientTape() as tape:\r\n\t\t\tfeatures = encoder(img_tensor)\r\n\r\n\t\t\tfor i in range(1, target.shape[1]):\r\n\t\t\t\t# passing the features through the decoder\r\n\t\t\t\tpredictions, hidden, _ = decoder(dec_input, features, hidden)\r\n\r\n\t\t\t\tloss += loss_function(target[:, i], predictions)\r\n\t\t\t\t# using teacher forcing\r\n\t\t\t\tdec_input = tf.expand_dims(target[:, i], 1)\r\n\r\n\t\ttotal_loss = (loss / int(target.shape[1]))\r\n\t\ttrainable_variables = encoder.trainable_variables + decoder.trainable_variables\r\n\t\tgradients = tape.gradient(loss, trainable_variables)\r\n\t\toptimizer.apply_gradients(zip(gradients, trainable_variables))\r\n\t\treturn loss, total_loss\r\n\r\n\tper_replica_losses, l_loss = strategy.run(step_fn, args=(dist_inputs,))\r\n\treturn strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_losses,axis=None),strategy.reduce(tf.distribute.ReduceOp.MEAN, l_loss,axis=None)\r\n\r\n\r\nfor epoch in range(start_epoch, EPOCHS):\r\n\tstart = time.time()\r\n\ttotal_loss = 0.0\r\n\tbatch = 0\r\n\tfor x in train_dist_dataset:\r\n\t\tbatch_loss, t_loss = train_step(x)\r\n\t\ttotal_loss += t_loss\r\n\t\tbatch +=1\r\n\r\n\t\tif batch % 100 == 0:\r\n\t\t\tprint ('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy() / BATCH_SIZE), flush=True)\r\n\r\n\ttrain_loss = total_loss / batch", "IndexedSlice are sparse data structures, so TF is using allgather for AllReduce. It probably doesn\u2019t matter if you are only using 2 GPUs, but you could consider convert sparse tensor to a dense one for better AllReduce support.\r\n\r\ncc @dubey ", "Actually, in this case TF is not using allgather, because MirroredStrategy uses legacy nccl ops. \r\n MirroredStrategy could switch to use collective ops, which support allgather.  FYI @guptapriya @yuefengz \r\n\r\nIf the tensor is small and can fit in memory, converting to a dense tensor is worth trying as Bairen suggested above.\r\n\r\n", "@dubey yes we have considered doing this - for some cases, using the collective ops allgather would be more performant and in some cases, converting to dense tensor might actually be faster. We have not had the bandwidth to prioritize this yet, though.", "Hi,\r\nSimilar issue here, \r\n\r\nI have this warning when running a keras multiple input model using functional API. The model works fine and without warnings when runing on single GPU. When I use `tf.distribute.MirroredStrategy` to use two GPUs the final result of the model is ok but I get the warning. I think this can lead to performance issues?\r\n\r\n```\r\ntf.__version__ : 2.2.0\r\ntf.keras.__version__ : 2.3.0-tf\r\nNVIDIA-SMI 410.72       Driver Version: 410.72       CUDA Version: 10.1\r\n\r\n```\r\nThe model I am generating is:\r\n\r\n```\r\ndef build_model_():\r\n\r\ninput_a_size = 200\r\ninput_b_size = 4\r\nnum_classes = 2\r\nlen_embedding = 100\r\n\r\nmirrored_strategy = tf.distribute.MirroredStrategy(['/gpu:0', '/gpu:1'])\r\n\r\nwith mirrored_strategy.scope():\r\n\r\n    input_a = Input(shape=(input_a_size,), name='input_a', dtype=np.uint8)\r\n    input_b = Input(shape=(input_b_size,), name='input_b', dtype=np.float32)\r\n\r\n    x = Embedding(len_embedding, 100)(input_a)\r\n    x = Conv1D(128, 4, activation='relu')(x)\r\n    x = MaxPooling1D(4)(x)\r\n    x = Flatten()(x)\r\n    branch_a = Dense(64, activation='relu')(x)\r\n\r\n    x = Dense(32, activation='relu')(input_b)\r\n    branch_b = Dense(32, activation='relu')(x)\r\n\r\n    concat = Concatenate()([\r\n                            branch_a,\r\n                            branch_b,\r\n                           ])\r\n\r\n    x = Dense(256, activation = 'relu')(concat)\r\n    output = Dense(num_classes, activation='softmax')(x)\r\n\r\n    model = Model(inputs=[\r\n                          input_a,\r\n                          input_b,\r\n                         ],\r\n                  outputs=[output])\r\n        \r\n    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\r\n\r\nmodel.summary()\r\n\r\nreturn model\r\n```\r\n\r\nModel summary:\r\n\r\n```\r\nINFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\r\nModel: \"model\"\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to                     \r\n==================================================================================================\r\ninput_a (InputLayer)            [(None, 200)]        0                                            \r\n__________________________________________________________________________________________________\r\nembedding (Embedding)           (None, 200, 100)     10000       input_a[0][0]                    \r\n__________________________________________________________________________________________________\r\nconv1d (Conv1D)                 (None, 197, 128)     51328       embedding[0][0]                  \r\n__________________________________________________________________________________________________\r\nmax_pooling1d (MaxPooling1D)    (None, 49, 128)      0           conv1d[0][0]                     \r\n__________________________________________________________________________________________________\r\ninput_b (InputLayer)            [(None, 4)]          0                                            \r\n__________________________________________________________________________________________________\r\nflatten (Flatten)               (None, 6272)         0           max_pooling1d[0][0]              \r\n__________________________________________________________________________________________________\r\ndense_1 (Dense)                 (None, 32)           160         input_b[0][0]                    \r\n__________________________________________________________________________________________________\r\ndense (Dense)                   (None, 64)           401472      flatten[0][0]                    \r\n__________________________________________________________________________________________________\r\ndense_2 (Dense)                 (None, 32)           1056        dense_1[0][0]                    \r\n__________________________________________________________________________________________________\r\nconcatenate (Concatenate)       (None, 96)           0           dense[0][0]                      \r\n                                                                 dense_2[0][0]                    \r\n__________________________________________________________________________________________________\r\ndense_3 (Dense)                 (None, 256)          24832       concatenate[0][0]                \r\n__________________________________________________________________________________________________\r\ndense_4 (Dense)                 (None, 2)            514         dense_3[0][0]                    \r\n==================================================================================================\r\nTotal params: 489,362\r\nTrainable params: 489,362\r\nNon-trainable params: 0\r\n__________________________________________________________________________________________________\r\n```\r\n\r\nThe way I am generating the inputs:\r\n\r\n```\r\ninput_a_train.shape: (35000, 200)\r\ninput_b_train.shape: (35000, 4)\r\ny_train.shape: (35000, 2)\r\n```\r\n\r\n```\r\ntrain_dataset = tf.data.Dataset.from_tensor_slices(({\r\n                                                     \"input_a\": input_a_train,\r\n                                                     \"input_b\": input_b_train,\r\n                                                     }, y_train))\r\n<TensorSliceDataset shapes: ({input_a: (200,), input_b: (4,)}, (2,)), types: ({input_a: tf.uint8, input_b: tf.float64}, tf.float32)>\r\n\r\nval_dataset = tf.data.Dataset.from_tensor_slices(({\r\n                                                     \"input_a\": input_a_val,\r\n                                                     \"input_b\": input_b_val,\r\n                                                     }, y_val))\r\n<TensorSliceDataset shapes: ({input_a: (200,), input_b: (4,)}, (2,)), types: ({input_a: tf.uint8, input_b: tf.float64}, tf.float32)>\r\n\r\ntrain_batches = train_dataset.padded_batch(128)\r\nval_batches = val_dataset.padded_batch(128)\r\n```\r\n\r\nI get the warning during the training phase,\r\n\r\n```\r\nhistory = my_model.fit(\r\n    x = train_batches,\r\n    epochs=3,\r\n    verbose = 1,\r\n    validation_data = val_batches,\r\n)\r\n```\r\n\r\nHere is the output:\r\n\r\n```\r\nEpoch 1/3\r\nINFO:tensorflow:batch_all_reduce: 12 all-reduces with algorithm = nccl, num_packs = 1\r\nWARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:batch_all_reduce: 12 all-reduces with algorithm = nccl, num_packs = 1\r\nWARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n274/274 [==============================] - ETA: 0s - loss: 0.1857 - accuracy: 0.9324\r\n...\r\n```", "Closing this thread since this is a known performance issue, and a workaround was found in #41898. If the workaround of using `MultiWorkerMirroredStrategy` explained in #41898 does not help your performance, please open a new issue.", "> IndexedSlice are sparse data structures, so TF is using allgather for AllReduce. It probably doesn\u2019t matter if you are only using 2 GPUs, but you could consider convert sparse tensor to a dense one for better AllReduce support.\r\n> \r\n> cc @dubey \r\n\r\n@byronyi  @dubey could you please clarify which tensor you are referring to should be converted from sparse to dense tensors in the example code? If convert sparse tensors to dense tensors, will this cause Out of Memory issue? Thanks. "]}, {"number": 39544, "title": "NEED FAST HELP! Tensorflow Training Error", "body": "Hi,\r\n\r\nI am trying to code an AI with an linear regression algorithm with tensorflow.\r\nBut my model gives an error message when i try to train it:\r\n\r\nThis is my code\r\nlinear_est.train(train_input_fn)  # train\r\nresult = linear_est.evaluate(eval_input_fn)  # get model metrics/stats by testing on tetsing data\r\n\r\nclear_output()  # clears consoke output\r\nprint(result)  # the result variable is simply a dict of stats about our model\r\n\r\nAnd it says:\r\nInvalidArgumentError: assertion failed: [Labels must be <= n_classes - 1] [Condition x <= y did not hold element-wise:] [x (head/losses/Cast:0) = ] [[8][10][2]...] [y (head/losses/check_label_range/Const:0) = ] [1]\r\n\t [[{{node Assert}}]]\r\n\r\nTake a look at the rest if you want to here:\r\nhttps://drive.google.com/open?id=1kN3fnQQKJk2VdIZvj1XFrxg73C8BKaWz\r\n\r\nBig Thanks and best Regards\r\nJustin\r\n", "comments": ["This is better suited for StackOverflow"]}, {"number": 39543, "title": "Added a set of tests for 16x8.", "body": "This PR is an addition to the PR:\r\nhttps://github.com/tensorflow/tensorflow/pull/36251\r\n\r\nThe command that I used to run them:\r\nbazel run  //tensorflow/lite/testing:generate_examples -- /tmp --zip_to_output=conv_relu --toco=$(pwd)/bazel-bin/tensorflow/lite/toco/toco --jobs 48\r\n\r\nThis PR adds op_tests for the operators that are implemented for the quantization mode: \r\nactivations - int16, weights -int8.\r\n", "comments": ["fyi, the zip test starts to use the new mlir converter now.", "@wwwind Can you please check @liufengdb's comments and keep us posted. Thanks!", "This PR will be merged as a follow-up of this PR https://github.com/tensorflow/tensorflow/pull/36251", "Hi @suharshs Could you please re-approve this PR ? I had to push a fix after CI checks.\r\nThanks!", "Hi @suharshs I pushed a fix. Thanks!", "Hi @rthadur I see that all checks are green now, could we please merge this PR ? \r\nIs there something blocking it ? Thanks!", "For some reason this test is not building and running properly internally. I am resolving the issues, and also trying to figure out why its not run in OSS TF.\r\n\r\nI will try to resolve it to get the missing 16bit tflite testing infra submitted, and update with the details here.", "Ok so I just got this PR merged with some additions to the testing infra.\r\n\r\nIn order to get the PR through i had to remove the leaky relu test and the conv with activation test. Could you add those tests in separate small PRs so that we can narrow down those failures and get those in as well. \r\n\r\nThanks!", "Hi @suharshs Thank you! I will do."]}, {"number": 39542, "title": "Cannot use TimeDistributed with hub.KerasLayer", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): None\r\n- GCC/Compiler version (if compiling from source): None\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: None\r\n\r\n**Describe the current behavior**\r\nWhen trying to apply the `tf.keras.layers.TimeDistributed` layer on top of a `tensorflow_hub.KerasLayer`, an exception (and not a useful one) is being raised\r\n\r\n**Describe the expected behavior**\r\nJust like any other `tf.keras.layers.Layer`, I'd expect one generated by the `tensorflow_hub.KerasLayer` to work\r\n\r\n**Standalone code to reproduce the issue**\r\n```python3\r\nimport os\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow_hub as hub\r\n\r\nos.environ[\"TFHUB_CACHE_DIR\"] = '/tmp/tfhub'\r\n\r\n# Create model\r\nmodel = tf.keras.Sequential([tf.keras.layers.TimeDistributed(hub.KerasLayer(\"https://tfhub.dev/google/bit/s-r101x1/1\",\r\n                                                                            trainable=False)),\r\n                             tf.keras.layers.LSTM(units=512),\r\n                             tf.keras.layers.Dense(units=128,\r\n                                                   activation=tf.nn.relu),\r\n                             tf.keras.layers.Dense(units=1,\r\n                                                   activation=tf.nn.sigmoid)])\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\r\n              metrics=['accuracy'])\r\n\r\n# Fit\r\nmodel.fit(x=np.random.randint(low=0, high=256, size=(1000, 10, 56, 56, 3)).astype(float),\r\n          y=np.random.randint(low=0, high=2, size=(1000,)).astype(float), epochs=1)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\nTraceback: \r\n\r\n2020-05-14 15:40:50.293457: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-05-14 15:40:50.314782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-05-14 15:40:50.315236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:09:00.0 name: GeForce GTX 1070 computeCapability: 6.1\r\ncoreClock: 1.683GHz coreCount: 15 deviceMemorySize: 7.93GiB deviceMemoryBandwidth: 238.66GiB/s\r\n2020-05-14 15:40:50.315361: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\r\n2020-05-14 15:40:50.315403: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory\r\n2020-05-14 15:40:50.315438: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\r\n2020-05-14 15:40:50.315474: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\r\n2020-05-14 15:40:50.315508: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory\r\n2020-05-14 15:40:50.315542: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory\r\n2020-05-14 15:40:50.315577: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory\r\n2020-05-14 15:40:50.315585: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1598] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-05-14 15:40:50.315827: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-05-14 15:40:50.337745: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3493215000 Hz\r\n2020-05-14 15:40:50.338512: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f1b94000b60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-05-14 15:40:50.338545: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-05-14 15:40:50.341227: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-05-14 15:40:50.341251: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      \r\nTraceback (most recent call last):\r\n  File \".../site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-2-0f6a34a2cfff>\", line 27, in <module>\r\n    y=np.random.randint(low=0, high=2, size=(1000,)).astype(float), epochs=1)\r\n  File \".../site-packages/tensorflow/python/keras/engine/training.py\", line 66, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n  File \".../site-packages/tensorflow/python/keras/engine/training.py\", line 848, in fit\r\n    tmp_logs = train_function(iterator)\r\n  File \".../site-packages/tensorflow/python/eager/def_function.py\", line 580, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \".../site-packages/tensorflow/python/eager/def_function.py\", line 627, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \".../site-packages/tensorflow/python/eager/def_function.py\", line 506, in _initialize\r\n    *args, **kwds))\r\n  File \".../site-packages/tensorflow/python/eager/function.py\", line 2446, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \".../site-packages/tensorflow/python/eager/function.py\", line 2777, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \".../site-packages/tensorflow/python/eager/function.py\", line 2667, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \".../site-packages/tensorflow/python/framework/func_graph.py\", line 981, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \".../site-packages/tensorflow/python/eager/def_function.py\", line 441, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \".../site-packages/tensorflow/python/framework/func_graph.py\", line 968, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nNotImplementedError: in user code:\r\n    .../site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\r\n        outputs = self.distribute_strategy.run(\r\n    .../site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\r\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    .../site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\r\n        return self._call_for_each_replica(fn, args, kwargs)\r\n    .../site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\r\n        return fn(*args, **kwargs)\r\n    .../site-packages/tensorflow/python/keras/engine/training.py:531 train_step  **\r\n        y_pred = self(x, training=True)\r\n    .../site-packages/tensorflow/python/keras/engine/base_layer.py:927 __call__\r\n        outputs = call_fn(cast_inputs, *args, **kwargs)\r\n    .../site-packages/tensorflow/python/keras/engine/sequential.py:291 call\r\n        outputs = layer(inputs, **kwargs)\r\n    .../site-packages/tensorflow/python/keras/engine/base_layer.py:927 __call__\r\n        outputs = call_fn(cast_inputs, *args, **kwargs)\r\n    .../site-packages/tensorflow/python/keras/layers/wrappers.py:246 call\r\n        output_shape = self.compute_output_shape(input_shape).as_list()\r\n    .../site-packages/tensorflow/python/keras/layers/wrappers.py:190 compute_output_shape\r\n        child_output_shape = self.layer.compute_output_shape(child_input_shape)\r\n    .../site-packages/tensorflow/python/keras/engine/base_layer.py:699 compute_output_shape\r\n        raise NotImplementedError\r\n    NotImplementedError: \r\n\r\n", "comments": ["I have tried in colab with TF version 2.2 and was able to reproduce the issue.Please. find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/aacbf993c4e2b44c2a30089d2894932e/untitled890.ipynb).Thanks!", "From the error log, it seems that keras wrapper was expecting the tf_hub layer to implement the compute_output_shape() method, but it is not implemented. All the default keras layer has implemented compute_output_shape(), but I think it is missed by tf_hub.KerasLayer.\r\n\r\nAdding @akhorlin from tf-hub side.", "@gowthamkpr can we transfer this issue to tensorflow/hub?", "Yes, please make this an issue in tensorflow/hub and assign it to me. `hub.KerasLayer` should supply its own `.compute_output_shape` but does not.\r\n\r\nThat said, I can get @ravikyram's colab to work by setting `model.compile(..., run_eagerly=True)` (and reducing the batch size from 1000 to 10). That's because default implementation inherited from the Layer base class for the eager case is good enough (as long as  the `hub.KerasLayer(..., output_shape=...)`override isn't used).", "Ack, it seems that I somehow don't have the permission to transfer the issue. The dropdown list doesn't contain hub when I click \"Transfer issue\" on this page. In case this can't be done by any of you, feel free to recreate a new issue on tensorflow/hub.\r\n", "Done. Created https://github.com/tensorflow/hub/issues/596, and closing this issue for now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39542\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39542\">No</a>\n", "I had similar issue and this trick works for me \r\n\r\n```\r\n# Load KerasLayer\r\ninput = ...\r\nlayer = hub.KerasLayer(\"https://tfhub.dev/some/model/1\",)\r\nnet  = layer(input)\r\nnet = tf.keras.Model(input , net)\r\n# Add custom compute_output_shape \r\nnet.compute_output_shape = lambda x : (x[0],x[1],512)\r\n\r\n# now use net to build my model \r\n# Create model\r\nmodel = tf.keras.Sequential([tf.keras.layers.TimeDistributed(net),...])\r\n\r\n```"]}, {"number": 39541, "title": "Autograph decorated functions with tf.map_fn", "body": "This is a test to verify the failure of the case described in https://github.com/tensorflow/tensorflow/issues/26091#issuecomment-628304623\r\n\r\n/cc @dthkao ", "comments": ["Mhh.. why test Is passing?", "I think the issue was fixed a while ago (either in 2.0 or in newer versions, not sure), but the issue ID was never tagged by the fix. I definitely expect the test to pass. Have you seen recent cases when it doesn't work?", "We had problems yesterday with 2.2.0 see https://github.com/tensorflow/addons/pull/1830#issuecomment-628283237 and the [colab](https://github.com/tensorflow/addons/pull/1830#issuecomment-628264997) in that ticket.", "And also with `tf-nightly`", "I see. Do you have a small reproducer in a colab, using @tf.function as decorator? Something simple like this test would do. It might be caused by keyword arguments, if I read through the comments correctly.", "```\r\ndef test_function(x):\r\n    cond = tf.constant(-1)\r\n\r\n    if cond == 0:\r\n        result = x\r\n    else:\r\n        result = x\r\n\r\n    return result\r\n\r\n@tf.function\r\ndef partial_call(x):\r\n    fn = partial(test_function)\r\n    tf.map_fn(fn, x)\r\n```\r\n\r\n```\r\nX = tf.random.uniform(shape=(10, 224, 224, 3))\r\npartial_call(X)\r\n```", "I don't think that's caused by `partial`. The problem is that `map_fn` doesn't call autograph (see below). That's definitely a bug - feel free to file an issue, and if you're interested to help fixing it, I can give a few pointers as well.\r\n\r\nA workaround is to wrap the target of map_fn in tf.function:\r\n\r\n```\r\n@tf.function  # Fails without this, but shouldn't.\r\ndef test_function(x):\r\n    cond = tf.constant(-1)\r\n\r\n    if cond == 0:\r\n        result = x\r\n    else:\r\n        result = x\r\n\r\n    return result\r\n\r\n@tf.function\r\ndef partial_call(x):\r\n    tf.map_fn(test_function, x)\r\n\r\nX = tf.random.uniform(shape=(10, 224, 224, 3))\r\npartial_call(X)\r\n```", "@mdanatg I was posting the same if you see the last commit :smile_cat: \r\nIn the referenced PR yesterday we have solved with the same workaround. \r\nWe could try to solve but do you think that it could be a frequent enough case to fix it?", "It's probably not that frequent, since the functions passed to map_fn are usually simple and don't use control flow. I think it's useful to fix though.", "@mdanatg Do you have a pointer to the code related to `tf.map_fn` tracing?", "Sure. So map_fn needs to apply autograph to the target function before calling it, here:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/8e8c67c3375da3fe8b44e7c11eb1d3fbb2eaa41c/tensorflow/python/ops/map_fn.py#L480\r\n\r\nThe right way to do that is shown in this code that does the same for Dataset.map:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/8e8c67c3375da3fe8b44e7c11eb1d3fbb2eaa41c/tensorflow/python/data/ops/dataset_ops.py#L3223\r\n", "As https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/ops/dataset_ops.py#L83 is it not public need I to duplicate\r\n```\r\nautograph_ctx = lazy_loader.LazyLoader(\r\n    \"autograph_ctx\", globals(),\r\n    \"tensorflow.python.autograph.core.ag_ctx\")\r\nautograph = lazy_loader.LazyLoader(\r\n    \"autograph\", globals(),\r\n    \"tensorflow.python.autograph.impl.api\")\r\n```", "I think it's safest to do that. It might work with a direct import as well, but there are many circular dependencies there.", "I made a commit but It is going to be a little bit longer then I thought so I cannot go ahead fastly just with colab or blindly changes. \r\n\r\nWhat is the fast way to have a local setup inside a docker container and to run this changes and the specific test with bazel?", "This is the most practical way I think: https://www.tensorflow.org/install/source#docker_linux_builds.\r\n\r\nOnce it's working you should be able to run something like `bazel test //tensorflow/python/kernel_tests:map_fn_test`\r\n\r\nThe initial call to bazel can take a while since it needs to rebuild everything. Afterwards it should be much faster.", "It was long time ago that I built all from scratch.. I want Google internal bazel cache :smile: ", "I took a very long time to recompile but I don't know cause It seems that the test doesn't fail.", "Why the test fail only with `@test_util.run_deprecated_v1` decoration and not with `@test_util.run_in_graph_and_eager_modes`?", "Not sure. It's possible that map_fn switches to eager mode even when you have run_in_graph_and_eager_modes. Using tf.function will make sure that's not the case.", "I don't think that we could just use `result_value = autograph.tf_convert(fn(elems_value), autograph_ctx.control_status_ctx())`\r\n\r\nCause we get:\r\n``` TypeError: Could not build a TypeSpec for <function convert.<locals>.decorator.<locals>.wrapper at 0x7f651f655e18> with type function```", "> I don't think that we could just use `result_value = autograph.tf_convert(fn(elems_value), autograph_ctx.control_status_ctx())`\r\n> \r\n> Cause we get:\r\n> ` TypeError: Could not build a TypeSpec for <function convert.<locals>.decorator.<locals>.wrapper at 0x7f651f655e18> with type function`\r\n\r\nCorrect, tf_convert doesn't call the function, it just translates it.\r\n\r\nSo we need something like:\r\n\r\n```\r\nautographed_fn = autograph.tf_convert(fn, autograph_ctx.control_status_ctx())\r\nresult_value = autographed_fn(elems_value)\r\n```", "Sorry I needed to restartthe container after a system upgrade and I got:\r\n```\r\nINFO: Deleting stale sandbox base /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/sandbox\r\n```\r\nSo I will need again hours of recompiling (see https://github.com/tensorflow/build/issues/5 and https://github.com/tensorflow/tensorflow/issues/39560) :disappointed: \r\n\r\nCan you use the Google remote or your local cache to test this a push directly here?", "I can trigger a build which should finish faster.", "Ok I've done a blind commit. If you can start a CI run.", "Kokoro we invoke you \u3053\u3053\u308d :pray: :meat_on_bone: ", "I don't know if you can disclose this but is the Kokoro waiting list only related to the public queue visible with https://github.com/tensorflow/tensorflow/labels/kokoro%3Aforce-run or is there an hidden internal one?", "I'm not sure, actually. Usually it picks up immediately.", "` import/copybara Pending \u2014 Waiting for internal safe review approval` is it a manual approval pass right?", "Yep - looks like the tests passed! Consider updating the PR title.", "> Yep - looks like the tests passed! Consider updating the PR title.\r\n\r\nI hope that the title is clear now."]}, {"number": 39540, "title": "Weights for model net_10 have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`.", "body": "Just added a build to model on checkpointing tutorial instead of creating the state in `__init__` as done in the [tutorial](https://www.tensorflow.org/guide/checkpoint) and im getting this error\r\n```\r\nclass Net(tf.keras.Model):\r\n  \"\"\"A simple linear model.\"\"\"\r\n\r\n  def __init__(self):\r\n    super(Net, self).__init__()\r\n    #self.l1 = tf.keras.layers.Dense(5)\r\n  def build(self,input_shape):\r\n    self.l1 = tf.keras.layers.Dense(5)\r\n    self.dummy = tf.Variable(trainable=True,initial_value=tf.keras.initializers.glorot_normal()(shape=(1,),dtype=tf.float32))\r\n    print('built layers')\r\n  def call(self, x):\r\n    return self.l1(x)\r\n\r\nnet = Net()\r\nnet.build([1,])\r\nnet.save_weights('easy_checkpoint')\r\n```\r\noutput:\r\n```\r\nbuilt layers\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-31-3b54dc506ffd> in <module>\r\n      1 net = Net()\r\n      2 net.build([1,])\r\n----> 3 net.save_weights('easy_checkpoint')\r\n\r\n~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py in save_weights(self, filepath, overwrite, save_format)\r\n   1111         ValueError: For invalid/unknown format arguments.\r\n   1112     \"\"\"\r\n-> 1113     self._assert_weights_created()\r\n   1114     filepath_is_h5 = _is_hdf5_filepath(filepath)\r\n   1115     if save_format is None:\r\n\r\n~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py in _assert_weights_created(self)\r\n   1560                        'Weights are created when the Model is first called on '\r\n   1561                        'inputs or `build()` is called with an `input_shape`.' %\r\n-> 1562                        self.name)\r\n   1563 \r\n   1564   def _graph_network_add_loss(self, symbolic_loss):\r\n\r\nValueError: Weights for model net_10 have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`.\r\n```\r\nEven though the build method is ran\r\n\r\nHere is my hunch: The problem with my code is that the build does not execute the build of `self.l1` but just creates it. Things do work out fine if i add `self.l1` creation in `__init__` and call `super().__build__()` as the first line in Net's build. Things make sense so far but, the code fails again if i replace `super().build(input_shape)` with `self.l1.build(input_shape)`. Also, the code belows shows that all the variables are actually there. So, i am lost again. Any help is much appreciated\r\n```\r\ntf.random.set_seed(42)\r\nclass Net1(tf.keras.Model):\r\n  \"\"\"A simple linear model.\"\"\"\r\n  def __init__(self):\r\n    super(Net1, self).__init__()\r\n    self.l1 = tf.keras.layers.Dense(5)\r\n  def build(self,input_shape):\r\n    super().build(input_shape)\r\n    self.dummy = tf.Variable(trainable=True,initial_value=tf.keras.initializers.glorot_normal()(shape=(1,),dtype=tf.float32))\r\n    print(self.variables)\r\n  def call(self, x):\r\n    return self.l1(x)\r\n\r\nnet = Net1()\r\nnet.build((10,1))\r\nprint('*'*50)\r\nprint(net.variables)\r\n\r\noutput:\r\n[<tf.Variable 'dense_56/kernel:0' shape=(1, 5) dtype=float32, numpy=\r\narray([[ 0.3291242 , -0.11798644, -0.294235  , -0.07103491, -0.9326792 ]],\r\n      dtype=float32)>, <tf.Variable 'dense_56/bias:0' shape=(5,) dtype=float32, numpy=array([0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([0.09575049], dtype=float32)>]\r\n**************************************************\r\n[<tf.Variable 'dense_56/kernel:0' shape=(1, 5) dtype=float32, numpy=\r\narray([[ 0.3291242 , -0.11798644, -0.294235  , -0.07103491, -0.9326792 ]],\r\n      dtype=float32)>, <tf.Variable 'dense_56/bias:0' shape=(5,) dtype=float32, numpy=array([0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([0.09575049], dtype=float32)>]\r\n```\r\n\r\nwhereas,\r\n```\r\ntf.random.set_seed(42)\r\nclass Net1(tf.keras.Model):\r\n  \"\"\"A simple linear model.\"\"\"\r\n\r\n  def __init__(self):\r\n    super(Net1, self).__init__()\r\n    self.l1 = tf.keras.layers.Dense(5)\r\n  def build(self,input_shape):\r\n    self.l1.build(input_shape)\r\n    self.dummy = tf.Variable(trainable=True,initial_value=tf.keras.initializers.glorot_normal()(shape=(1,),dtype=tf.float32))\r\n    print('variables',self.l1.variables,self.dummy)\r\n  def call(self, x):\r\n    return self.l1(x)\r\n\r\nnet = Net1()\r\nnet.build((10,1))\r\nprint(net.variables)\r\n\r\noutput:\r\nvariables [<tf.Variable 'kernel:0' shape=(1, 5) dtype=float32, numpy=\r\narray([[ 0.3291242 , -0.11798644, -0.294235  , -0.07103491, -0.9326792 ]],\r\n      dtype=float32)>, <tf.Variable 'bias:0' shape=(5,) dtype=float32, numpy=array([0., 0., 0., 0., 0.], dtype=float32)>] <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([0.09575049], dtype=float32)>\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-77-35561efcdc2f> in <module>\r\n     15 net = Net1()\r\n     16 net.build((10,1))\r\n---> 17 print(net.variables)\r\n\r\n~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in variables(self)\r\n   1965       A list of variables.\r\n   1966     \"\"\"\r\n-> 1967     return self.weights\r\n   1968 \r\n   1969   @property\r\n\r\n~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py in weights(self)\r\n    498       A list of variables.\r\n    499     \"\"\"\r\n--> 500     return self._dedup_weights(self._undeduplicated_weights)\r\n    501 \r\n    502   @property\r\n\r\n~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py in _undeduplicated_weights(self)\r\n    503   def _undeduplicated_weights(self):\r\n    504     \"\"\"Returns the undeduplicated list of all layer variables/weights.\"\"\"\r\n--> 505     self._assert_weights_created()\r\n    506     weights = []\r\n    507     for layer in self._layers:\r\n\r\n~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py in _assert_weights_created(self)\r\n   1560                        'Weights are created when the Model is first called on '\r\n   1561                        'inputs or `build()` is called with an `input_shape`.' %\r\n-> 1562                        self.name)\r\n   1563 \r\n   1564   def _graph_network_add_loss(self, symbolic_loss):\r\n\r\nValueError: Weights for model net1_40 have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`.\r\n```\r\n", "comments": ["Was able to reproduce the issue with [TF v2.2](https://colab.research.google.com/gist/amahendrakar/2c695cb769635d7f544de999a7a537ad/39540.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/0a9eabd5766c7c16eb71ccc1b65eee0b/39540.ipynb). Please find the attached gist. Thanks!\r\n\r\nGist for updated code with [TF v2.2](https://colab.research.google.com/gist/amahendrakar/916525e4c12c94f85ae3ee59592bde86/39540.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/3d3d5a4918f6144fd142575aa3149549/39540-tf-nightly.ipynb).", "have edited the original post to show exact anomalous (maybe?) behavior", "@amahendrakar can you change the gist to show the change in the post? The original post now contains a much better depiction of unexpected behavior", "any update on this?", "@nitinmnsn  have you found the solution for the issue ?", "Yes. I have solved every issue with TensorFlow. Pytorch. Breathing easy for months now", "That's great :)))", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210603, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/ed3cb34705fba3b9b1e331f914b565b2/39540.ipynb). Thanks!", "Hi There,\n\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \n\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39540\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39540\">No</a>\n"]}, {"number": 39539, "title": "[TFLite] Failed to create Hexagon delegate on Oneplus 5", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Pop Os 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Oneplus 5\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.2\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\n\r\nDevice Information -\r\n\r\n    adb shell getprop ro.product.device\r\n    OnePlus5\r\n\r\n    adb shell getprop ro.board.platform\r\n    msm8998\r\n\r\n($APP_ROOT)/app/src/main/jniLibs\r\n\r\n```\r\n\u251c\u2500\u2500 arm64-v8a\r\n\u2502   \u251c\u2500\u2500 libhexagon_nn_skel.so\r\n\u2502   \u251c\u2500\u2500 libhexagon_nn_skel_v65.so\r\n\u2502   \u2514\u2500\u2500 libhexagon_nn_skel_v66.so\r\n\u2514\u2500\u2500 armeabi-v7a\r\n    \u251c\u2500\u2500 libhexagon_nn_skel.so\r\n    \u251c\u2500\u2500 libhexagon_nn_skel_v65.so\r\n    \u2514\u2500\u2500 libhexagon_nn_skel_v66.so\r\n```\r\n($APP_ROOT)/app/build.gradle\r\n\r\n    apply plugin: 'com.android.application'  \r\n    apply plugin: 'kotlin-android'  \r\n    apply plugin: 'kotlin-android-extensions'  \r\n      \r\n    android {  \r\n      \r\n      compileSdkVersion 29  \r\n      buildToolsVersion \"29.0.3\"  \r\n      \r\n      defaultConfig {  \r\n      applicationId \"com.example.sr_tflite\"  \r\n      minSdkVersion 25  \r\n      targetSdkVersion 29  \r\n      versionCode 1  \r\n      versionName \"1.0\"  \r\n      ndk {  \r\n      abiFilters 'armeabi-v7a', 'arm64-v8a'  \r\n      }  \r\n      \r\n      testInstrumentationRunner \"androidx.test.runner.AndroidJUnitRunner\"  \r\n      }  \r\n      \r\n      buildTypes {  \r\n      release {  \r\n      minifyEnabled false  \r\n      proguardFiles getDefaultProguardFile('proguard-android-optimize.txt'), 'proguard-rules.pro'  \r\n      }  \r\n      aaptOptions {  \r\n      noCompress \"tflite\"  \r\n      }  \r\n     }  \r\n    // To inline the bytecode built with JVM target 1.8 into  \r\n    // bytecode that is being built with JVM target 1.6. (e.g. navArgs)  \r\n      \r\n      \r\n      compileOptions {  \r\n      sourceCompatibility JavaVersion.VERSION_1_8  \r\n      targetCompatibility JavaVersion.VERSION_1_8  \r\n      }  \r\n      kotlinOptions {  \r\n      jvmTarget = \"1.8\"  \r\n      }  \r\n      \r\n    }  \r\n      \r\n    dependencies {  \r\n      def tfl_version = \"0.0.0-nightly\"  \r\n      implementation fileTree(dir: 'libs', include: ['*.jar'])  \r\n        implementation \"org.jetbrains.kotlin:kotlin-stdlib-jdk7:$kotlin_version\"  \r\n      implementation 'androidx.appcompat:appcompat:1.1.0'  \r\n      implementation 'androidx.core:core-ktx:1.2.0'  \r\n      implementation 'com.google.android.material:material:1.1.0'  \r\n      implementation 'androidx.constraintlayout:constraintlayout:1.1.3'  \r\n      implementation 'androidx.navigation:navigation-fragment-ktx:2.0.0'  \r\n      implementation 'androidx.navigation:navigation-ui-ktx:2.0.0'  \r\n      implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'  \r\n      implementation 'org.tensorflow:tensorflow-lite-hexagon:0.0.0-nightly'  \r\n      testImplementation 'junit:junit:4.12'  \r\n      androidTestImplementation 'androidx.test.ext:junit:1.1.1'  \r\n      androidTestImplementation 'androidx.test.espresso:espresso-core:3.2.0'  \r\n      implementation(\"org.tensorflow:tensorflow-lite:${tfl_version}\") { changing = true }  \r\n      implementation(\"org.tensorflow:tensorflow-lite-gpu:${tfl_version}\") { changing = true }  \r\n      implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly'  \r\n    }\r\n\r\nI followed the [TensorFlow Lite Hexagon delegate](https://www.tensorflow.org/lite/performance/hexagon_delegate) guide on Oneplus 5. Tensorflow Lite was failed to create Hexagon delegate.\r\n\r\nLog\r\n\r\n    020-05-14 16:26:19.906 27848-27848/com.example.sr_tflite I/System.out: *******/data/app/com.example.sr_tflite-WR6XEFKEipjZ_vWQYv9MSQ==/lib/arm64\r\n    2020-05-14 16:26:19.947 27848-27848/com.example.sr_tflite V/com.example.sr_tflite: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:1859: Successfully created user PD on domain 0 (attrs 0x0)\r\n    2020-05-14 16:26:19.960 27848-28029/com.example.sr_tflite V/com.example.sr_tflite: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:270: rpc latency thread start\r\n    2020-05-14 16:26:19.961 27848-28027/com.example.sr_tflite E/com.example.sr_tflite: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/apps_std_imp.c:729:Error 45: fopen failed for oemconfig.so. (No such file or directory)\r\n    2020-05-14 16:26:19.961 27848-28027/com.example.sr_tflite E/com.example.sr_tflite: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/apps_std_imp.c:729:Error 45: fopen failed for libhexagon_nn_skel.so. (No such file or directory)\r\n    2020-05-14 16:26:19.961 27848-27848/com.example.sr_tflite D/com.example.sr_tflite: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:983: Error fffffffb: remote handle open domain failed. domain 0, name file:///libhexagon_nn_skel.so?hexagon_nn_domains_skel_handle_invoke&_modver=1.0&_dom=adsp, dlerror cannot open oemconfig.so\r\n    2020-05-14 16:26:19.961 27848-27848/com.example.sr_tflite D/com.example.sr_tflite: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:920: Error ffffffff: remote handle invoke failed. domain 0, handle 0, sc 1010200, pra 0x7fde45af88\r\n    2020-05-14 16:26:19.961 27848-27848/com.example.sr_tflite D/com.example.sr_tflite: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:1034: Error ffffffff: remote handle close failed. error \r\n    2020-05-14 16:26:19.961 27848-27848/com.example.sr_tflite D/com.example.sr_tflite: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:1020: Error fffffffb: remote handle64 open failed. name file:///libhexagon_nn_skel.so?hexagon_nn_domains_skel_handle_invoke&_modver=1.0&_dom=adsp\r\n    2020-05-14 16:26:19.961 27848-27848/com.example.sr_tflite W/tflite: Failed to fetch Hexagon NN version. This might be because you're using incompatible versions of libhexagon_interface and libhexagon_nn_skel. You must use compatible versions. Refer to Tensorflow Lite Hexagon Delegate Guide.\r\n    2020-05-14 16:26:19.961 27848-27848/com.example.sr_tflite I/tflite: Hexagon Delegate is not supported.\r\n    2020-05-14 16:26:19.962 27848-27848/com.example.sr_tflite D/AndroidRuntime: Shutting down VM\r\n    2020-05-14 16:26:19.966 27848-27848/com.example.sr_tflite E/AndroidRuntime: FATAL EXCEPTION: main\r\n        Process: com.example.sr_tflite, PID: 27848\r\n        java.lang.UnsupportedOperationException: This Device doesn't support Hexagon DSP execution.\r\n            at org.tensorflow.lite.experimental.HexagonDelegate.<init>(HexagonDelegate.java:40)\r\n            at com.example.sr_tflite_1.MainActivity$onCreate$1.onClick(MainActivity.kt:97)\r\n            at android.view.View.performClick(View.java:6669)\r\n            at android.view.View.performClickInternal(View.java:6638)\r\n            at android.view.View.access$3100(View.java:789)\r\n            at android.view.View$PerformClick.run(View.java:26145)\r\n            at android.os.Handler.handleCallback(Handler.java:873)\r\n            at android.os.Handler.dispatchMessage(Handler.java:99)\r\n            at android.os.Looper.loop(Looper.java:193)\r\n            at android.app.ActivityThread.main(ActivityThread.java:6898)\r\n            at java.lang.reflect.Method.invoke(Native Method)\r\n            at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:537)\r\n            at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:858)\r\n    2020-05-14 16:26:19.977 27848-28030/com.example.sr_tflite D/OSTracker: OS Event: crash\r\n    2020-05-14 16:26:19.995 27848-27848/com.example.sr_tflite I/Process: Sending signal. PID: 27848 SIG: 9\r\n\r\nI have downloaded the latest hexagon_nn_skel.run (v1.17) from the page but it still says `Failed to fetch Hexagon NN version. This might be because you're using incompatible versions of libhexagon_interface and libhexagon_nn_skel. You must use compatible versions. Refer to Tensorflow Lite Hexagon Delegate Guide. 2020-05-14 16:26:19.961 27848-27848/com.example.sr_tflite I/tflite: Hexagon Delegate is not supported.` Do i need to pull some other so files from somewhere else or my device is not supported.\r\nThanks\r\n\r\n**Describe the expected behavior**\r\n\r\nThe DSP delegate should be initialized as my SoC is in the list of supported hardwares.\r\n", "comments": ["Hi\r\n\r\n1) the title has OnePlus5 and in the body you say Pixel 3, can you please confirm which device you are using.\r\n\r\n2) The logcat has \"fopen failed for libhexagon_nn_skel.so. (No such file or directory)\", please make sure the files are correctly packaged with the app.\r\n\r\n3) please clear you gradle cache to make sure you're pulling the latest nightly and not an old one.\r\n\r\nThanks", "> Hi\r\n> \r\n> 1. the title has OnePlus5 and in the body you say Pixel 3, can you please confirm which device you are using.\r\n> 2. The logcat has \"fopen failed for libhexagon_nn_skel.so. (No such file or directory)\", please make sure the files are correctly packaged with the app.\r\n> 3. please clear you gradle cache to make sure you're pulling the latest nightly and not an old one.\r\n> \r\n> Thanks\r\n\r\n1)Thanks a lot for quick response. I used a template from another user and that's why the body had pixel 3. I'm using a oneplus 5.\r\n\r\n2)I have placed the directory as mentioned in the above body and as given in the guide. This is my directory structure straight from android studio.\r\n![Screenshot from 2020-05-14 23-52-38](https://user-images.githubusercontent.com/17473554/81971156-451eb300-963e-11ea-931d-c6806368a182.png)\r\n\r\n3) I have cleared the gradle cache from ~/.gradle/cache and tried building and running the app again and still the issue persists with the same error log as posted above.\r\n\r\nThanks.", "Few questions/instructions:\r\n\r\nIn the initialize line for the delegate\r\n  (e.g.) hexagonDelegate = new HexagonDelegate(activity);\r\nCan you print \r\n  activity.getApplicationInfo().nativeLibraryDir\r\nand check that the shared lib files are on the device under the same path.\r\n\r\nadb shell ls -al <PATH_PRINTED_FROM_ABOVE>\r\n\r\nIf you can paste the results in the reply will be good. Thanks", "> Few questions/instructions:\r\n> \r\n> In the initialize line for the delegate\r\n> (e.g.) hexagonDelegate = new HexagonDelegate(activity);\r\n> Can you print\r\n> activity.getApplicationInfo().nativeLibraryDir\r\n> and check that the shared lib files are on the device under the same path.\r\n> \r\n> adb shell ls -al <PATH_PRINTED_FROM_ABOVE>\r\n> \r\n> If you can paste the results in the reply will be good. Thanks\r\n\r\nHi, thanks for the prompt reply \r\n1)I am using the Kotlin code so `hexagonDelegate = new HexagonDelegate(activity);` this is initialized like `var tflite_hex: Interpreter? = null` and then `hexagonDelegate = HexagonDelegate(this);` finally `options.addDelegate(hexagonDelegate)`\r\n2) The directory which I'm getting from the `activity.getApplicationInfo().nativeLibraryDir` is \r\n`/data/app/com.example.sr_tflite-wK5_SXPZr9owo3sD9NNzpQ==/lib/arm64` and when I'm running the adb command I'm getting\r\n```\r\nadb shell ls -al /data/app/com.example.sr_tflite-wK5_SXPZr9owo3sD9NNzpQ==/lib/arm64\r\ntotal 16\r\ndrwxr-xr-x 2 system system 4096 2020-05-15 02:39 .\r\ndrwxr-xr-x 3 system system 4096 2020-05-15 02:39 ..\r\n```\r\nI see that these so files are not present in that directory? but I have laid them out in correct structure as stated in the documentation.\r\n\r\nThanks", "Hi,\r\nSorry for the late reply.\r\nYou probably have some other changes in your gradle files that impact packaging files.\r\nYou can try googling how to package share library in your project (in case you have some advanced gradle files).\r\nIf it is packaged correctly and doesn't work feel free to reopen the bug or create new one.\r\n\r\nThanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39539\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39539\">No</a>\n", "> Hi,\r\n> Sorry for the late reply.\r\n> You probably have some other changes in your gradle files that impact packaging files.\r\n> You can try googling how to package share library in your project (in case you have some advanced gradle files).\r\n> If it is packaged correctly and doesn't work feel free to reopen the bug or create new one.\r\n> \r\n> Thanks\r\n\r\nThanks for replying. I tried researching more around this issue and found that maybe the so files which are provided aren't compatible with my phone arch. So I found that you can generate your own so file using the bezel toolkit for dsp delegate using the command `bazel build -c opt --config=android_arm64 tensorflow/lite/experimental/delegates/hexagon/java:tensorflow-lite-hexagon` \r\nBut I'm getting an error \r\n    INFO: Call stack for the definition of repository 'hexagon_nn' which is a third_party_http_archive (rule definition at /home/anidh/tensorflow/third_party/repo.bzl:219:28):  \r\n    - /home/anidh/tensorflow/third_party/hexagon/workspace.bzl:8:5  \r\n    - /home/anidh/tensorflow/tensorflow/workspace.bzl:54:5  \r\n    - /home/anidh/tensorflow/tensorflow/workspace.bzl:108:5  \r\n    - /home/anidh/tensorflow/WORKSPACE:19:1  \r\n    ERROR: /home/anidh/.cache/bazel/_bazel_anidh/b7383c6eddfc2c0b3ec247d3a8b0f2de/external/local_config_cc/BUILD:47:1: in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite '@local_config_cc//:toolchain' does not contain a toolchain for cpu 'arm64-v8a'  \r\n    ERROR: Analysis of target '//tensorflow/lite/experimental/delegates/hexagon/java:tensorflow-lite-hexagon' failed; build aborted: Analysis of target '@local_config_cc//:toolchain' failed; build aborted  \r\n    INFO: Elapsed time: 0.122s  \r\n    INFO: 0 processes.  \r\n    FAILED: Build did NOT complete successfully (0 packages loaded, 1 target configured)\r\n\r\nAny help will be appreciated :)", "@anidh that looks like you didn't run .configure to configure your environment - note that you need to configure android section.", "> > Hi,\r\n> > Sorry for the late reply.\r\n> > You probably have some other changes in your gradle files that impact packaging files.\r\n> > You can try googling how to package share library in your project (in case you have some advanced gradle files).\r\n> > If it is packaged correctly and doesn't work feel free to reopen the bug or create new one.\r\n> > Thanks\r\n> \r\n> Thanks for replying. I tried researching more around this issue and found that maybe the so files which are provided aren't compatible with my phone arch. So I found that you can generate your own so file using the bezel toolkit for dsp delegate using the command `bazel build -c opt --config=android_arm64 tensorflow/lite/experimental/delegates/hexagon/java:tensorflow-lite-hexagon`\r\n> But I'm getting an error\r\n> INFO: Call stack for the definition of repository 'hexagon_nn' which is a third_party_http_archive (rule definition at /home/anidh/tensorflow/third_party/repo.bzl:219:28):\r\n> - /home/anidh/tensorflow/third_party/hexagon/workspace.bzl:8:5\r\n> - /home/anidh/tensorflow/tensorflow/workspace.bzl:54:5\r\n> - /home/anidh/tensorflow/tensorflow/workspace.bzl:108:5\r\n> - /home/anidh/tensorflow/WORKSPACE:19:1\r\n> ERROR: /home/anidh/.cache/bazel/_bazel_anidh/b7383c6eddfc2c0b3ec247d3a8b0f2de/external/local_config_cc/BUILD:47:1: in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite '@local_config_cc//:toolchain' does not contain a toolchain for cpu 'arm64-v8a'\r\n> ERROR: Analysis of target '//tensorflow/lite/experimental/delegates/hexagon/java:tensorflow-lite-hexagon' failed; build aborted: Analysis of target '@local_config_cc//:toolchain' failed; build aborted\r\n> INFO: Elapsed time: 0.122s\r\n> INFO: 0 processes.\r\n> FAILED: Build did NOT complete successfully (0 packages loaded, 1 target configured)\r\n> \r\n> Any help will be appreciated :)\r\n\r\nhello,anidh\r\nI met the same problem with you, have you solved it?", "> > > Hi,\r\n> > > Sorry for the late reply.\r\n> > > You probably have some other changes in your gradle files that impact packaging files.\r\n> > > You can try googling how to package share library in your project (in case you have some advanced gradle files).\r\n> > > If it is packaged correctly and doesn't work feel free to reopen the bug or create new one.\r\n> > > Thanks\r\n> > \r\n> > \r\n> > Thanks for replying. I tried researching more around this issue and found that maybe the so files which are provided aren't compatible with my phone arch. So I found that you can generate your own so file using the bezel toolkit for dsp delegate using the command `bazel build -c opt --config=android_arm64 tensorflow/lite/experimental/delegates/hexagon/java:tensorflow-lite-hexagon`\r\n> > But I'm getting an error\r\n> > INFO: Call stack for the definition of repository 'hexagon_nn' which is a third_party_http_archive (rule definition at /home/anidh/tensorflow/third_party/repo.bzl:219:28):\r\n> > \r\n> > * /home/anidh/tensorflow/third_party/hexagon/workspace.bzl:8:5\r\n> > * /home/anidh/tensorflow/tensorflow/workspace.bzl:54:5\r\n> > * /home/anidh/tensorflow/tensorflow/workspace.bzl:108:5\r\n> > * /home/anidh/tensorflow/WORKSPACE:19:1\r\n> >   ERROR: /home/anidh/.cache/bazel/_bazel_anidh/b7383c6eddfc2c0b3ec247d3a8b0f2de/external/local_config_cc/BUILD:47:1: in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite '@local_config_cc//:toolchain' does not contain a toolchain for cpu 'arm64-v8a'\r\n> >   ERROR: Analysis of target '//tensorflow/lite/experimental/delegates/hexagon/java:tensorflow-lite-hexagon' failed; build aborted: Analysis of target '@local_config_cc//:toolchain' failed; build aborted\r\n> >   INFO: Elapsed time: 0.122s\r\n> >   INFO: 0 processes.\r\n> >   FAILED: Build did NOT complete successfully (0 packages loaded, 1 target configured)\r\n> > \r\n> > Any help will be appreciated :)\r\n> \r\n> hello,anidh\r\n> I met the same problem with you, have you solved it?\r\n\r\nHi @yuqiu1233 \r\n\r\nI didn't get the time to follow up with this issue as I shifted my focus to running models on GPU, but I think you need to run the ./configure script inside the tensorflow repository as provide the location of SDk as well as NDK and bunch of other things. This is how I was able to run things for the GPU. Hope it helps as this is all I know for now.\r\n", "@yuqiu1233 Why do you want to build it ? Did you try to use the prebuilt packages - by putting them in your gradle files ?\r\nCan you explain what issues you had ?\r\n\r\nThanks", "@karimnosseir I have generate three .so files by run tflite_hexagon_nn_skel_v1.20.0.0.run, and put these file in app/src/main/jniLibs. the same problem appear with you like this \u201cFailed to fetch Hexagon NN version. This might be because you're using incompatible versions of libhexagon_interface and libhexagon_nn_skel. You must use compatible versions. Refer to Tensorflow Lite Hexagon Delegate Guide.\u201c \r\n\r\nDid you solve this problem\uff1f\r\n\r\n- use Snapdragon 855 chip\r\n- tensorflow2.1\r\n- python3.6", "@anidh thanks for your reply\uff01", "@yuqiu1233  Which phone are you using ?", "@yuqiu1233 can you paste the logcat around the failure.\r\n\r\nThanks", "@karimnosseir Sorry for the late reply.\r\n\r\nthe logcat as follow\uff0cI also find that no share lib in this path(activity.getApplicationInfo().nativeLibraryDir).\r\n\r\n2020-08-24 09:27:56.012 19062-19062/com.example.myapplication I/com.example.myapplication: Successfully opened fastrpc_shell_3\r\n2020-08-24 09:27:56.004 19062-19062/com.example.myapplication I/e.myapplication: type=1400 audit(0.0:111368): avc: denied { read } for name=\"fastrpc_shell_3\" dev=\"sde48\" ino=51 scontext=u:r:untrusted_app:s0:c241,c256,c512,c768 tcontext=u:object_r:adsprpcd_file:s0 tclass=file permissive=1 app=com.example.myapplication\r\n2020-08-24 09:27:56.039 19062-19062/com.example.myapplication I/com.example.myapplication: Successfully created user PD on domain 3 (attrs 0x0, debug_trace 0x0)\r\n2020-08-24 09:27:56.042 19062-19062/com.example.myapplication I/com.example.myapplication: fastrpc_perf_init: enabled RPC traces (kernel 0, dsp 0) with frequency 1000\r\n2020-08-24 09:27:56.042 19062-19338/com.example.myapplication I/com.example.myapplication: FastRPC latency thread started for QoS\r\n2020-08-24 09:27:56.048 19062-19336/com.example.myapplication W/com.example.myapplication: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/apps_std_imp.c:749: Warning: fopen returned 0x2 for file oemconfig.so. (No such file or directory)\r\n2020-08-24 09:27:56.049 19062-19336/com.example.myapplication W/com.example.myapplication: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/apps_std_imp.c:749: Warning: fopen returned 0x2 for file libhexagon_nn_skel_v66.so. (No such file or directory)\r\n2020-08-24 09:27:56.050 19062-19062/com.example.myapplication E/com.example.myapplication: Error 0x80000406: remote_handle_open_domain: dynamic loading failed for file:///libhexagon_nn_skel_v66.so?hexagon_nn_domains_skel_handle_invoke&_modver=1.0&_dom=cdsp on domain 3 (dlerror cannot open oemconfig.so)\r\n2020-08-24 09:27:56.050 19062-19062/com.example.myapplication E/com.example.myapplication: Error 0x80000406: remote_handle64_open failed for file:///libhexagon_nn_skel_v66.so?hexagon_nn_domains_skel_handle_invoke&_modver=1.0&_dom=cdsp\r\n2020-08-24 09:27:56.050 19062-19062/com.example.myapplication W/tflite: Failed to fetch Hexagon NN version. This might be because you're using incompatible versions of libhexagon_interface and libhexagon_nn_skel. You must use compatible versions. Refer to Tensorflow Lite Hexagon Delegate Guide.\r\n2020-08-24 09:27:56.050 19062-19062/com.example.myapplication I/tflite: Hexagon Delegate is not supported.\r\n2020-08-24 09:27:56.052 19062-19062/com.example.myapplication D/ForceDarkHelper: updateByCheckExcludeList: pkg: com.example.myapplication activity: com.example.myapplication.MainActivity@855d042\r\n2020-08-24 09:27:56.053 19062-19062/com.example.myapplication D/ForceDarkHelper: updateByCheckExcludeList: pkg: com.example.myapplication activity: com.example.myapplication.MainActivity@855d042\r\n2020-08-24 09:27:56.067 19062-19095/com.example.myapplication D/OpenGLRenderer: endAllActiveAnimators on 0x6fc51ad400 (AlertController$RecycleListView) with handle 0x7017cebca0\r\n2020-08-24 09:27:56.887 19062-19062/com.example.myapplication I/Toast: Show toast from OpPackageName:com.example.myapplication, PackageName:com.example.myapplication", "@karimnosseir  \r\nThanks for you help!\r\n\r\nThis problem has been solved by pushing that three .so files to this path(activity.getApplicationInfo().nativeLibraryDir) \r\nmanually.  The problem now is how to package share library in android project. I will google and try other methods.\r\n\r\nI also have another question:\r\nWhether static-sized input must be need for both GPU and DSP accelerate? Is there any way to make the model dynamic input when using GPU or DSP accelerate?\r\n\r\nLook forward to your reply", "Hi @yuqiu1233 Hexagon DSP delegate supports dynamic batch size. it's only available now in C API\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/hexagon/hexagon_delegate.h#L25\r\nand a full example is in this test, this part mainly\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/hexagon/builders/tests/batch_seq_config_test.cc#L105", "@karimnosseir  thanks for you reply!\r\nI think you may misunderstand me, the dynamic input size doesn't means the batch size number but shape of input image.", "@yuqiu1233 Nope i did understand you. Just saying what is supported at the moment :) \r\n\r\nThanks", "@karimnosseir what a pity it is! It is so meaningful in project that dynamic input with GPU or DSP accelerate. Hoping you can finish that function as soon as possible!\r\n\r\nI really appreciate that you have solved the trouble confusing me quite a while!", "I'm using the QNP/SNPE SDK directly but I see the same problem. But I found a solution.\r\n\r\nIf I set minSdkVersion <= 22, DSP works fine.\r\n\r\n```\r\n...\r\n2020-09-29 19:48:44.675 28953-29028/com.domain.example W/com.domain.example: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/apps_std_imp.c:750: Warning: apps_std_fopen_with_env failed with 0xd for libhta_dsp_debug.so (Permission denied)\r\n2020-09-29 19:48:44.676 28953-29017/com.domain.example I/com.domain.example: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:851: remote_handle64_open: Successfully opened handle 0x39f50330 for file:///libsnpe_dsp_v66_domains_v2_skel.so?snpe_dsp_domains_v2_skel_handle_invoke&_modver=1.0&_dom=cdsp on domain 3\r\n2020-09-29 19:48:44.680 28953-29017/com.domain.example E/npu_user_driver: npu_get_property status: 0\r\n...\r\n```\r\n\r\nBut if I set minSdkVersion>=23, I get \r\n\r\n```\r\n...\r\n2020-09-29 20:30:02.957 6678-6822/com.domain.example W/com.domain.example: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/apps_std_imp.c:750: Warning: apps_std_fopen_with_env failed with 0xd for libsnpe_dsp_v66_domains_v2_skel.so (Permission denied)\r\n2020-09-29 20:30:02.958 1227-1309/? W//vendor/bin/cdsprpcd: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/apps_std_imp.c:750: Warning: apps_std_fopen_with_env failed with 0x2 for libsnpe_dsp_v66_domains_v2_skel.so (No such file or directory)\r\n2020-09-29 20:30:02.958 6678-6805/com.domain.example E/com.domain.example: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:815: Error 0x80000406: remote_handle_open_domain: dynamic loading failed for file:///libsnpe_dsp_v66_domains_v2_skel.so?snpe_dsp_domains_v2_skel_handle_invoke&_modver=1.0&_dom=cdsp on domain 3 (dlerror _rtld_map_object_ex: cannot open libsnpe_dsp_v66_domains_v\r\n2020-09-29 20:30:02.958 6678-6805/com.domain.example E/com.domain.example: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:849: Error 0x80000406: remote_handle64_open failed for file:///libsnpe_dsp_v66_domains_v2_skel.so?snpe_dsp_domains_v2_skel_handle_invoke&_modver=1.0&_dom=cdsp\r\n2020-09-29 20:30:02.958 6678-6805/com.domain.example E/com.domain.example: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:379: Error 0x1d: verify_local_handle failed. handle 0x0\r\n2020-09-29 20:30:02.958 6678-6805/com.domain.example E/com.domain.example: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:394: Error 0x1d: get_domain_from_handle failed. handle 0x0\r\n2020-09-29 20:30:02.958 6678-6805/com.domain.example E/com.domain.example: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:766: Error 0x1d: remote_handle64_invoke failed for handle 0x0, method 7 on domain 3 (sc 0x7020200)\r\n2020-09-29 20:30:02.962 6678-6805/com.domain.example E/npu_user_driver: npu_get_property status: 0\r\n...\r\n```\r\n\r\nAnd I cannot use the DSP. Only CPU and GPU delegates.\r\n\r\n\r\nEither this has something to do with the android.bundle.enableUncompressedNativeLibs  thing added to Gradle or the way dlopen is handled in sdkVersion >= 23\r\n\r\nhttps://developer.android.com/about/versions/marshmallow/android-6.0-changes\r\n\r\n> \r\n> This release updates the behavior of the dynamic linker. The dynamic linker now understands the difference between a library\u2019s soname and its path ( public bug 6670), and search by soname is now implemented. Apps which previously worked that have bad DT_NEEDED entries (usually absolute paths on the build machine\u2019s file system) may fail when loaded.\r\n> \r\n> The dlopen(3) RTLD_LOCAL flag is now correctly implemented. Note that RTLD_LOCAL is the default, so calls to dlopen(3) that didn\u2019t explicitly use RTLD_LOCAL will be affected (unless your app explicitly used RTLD_GLOBAL). With RTLD_LOCAL, symbols will not be made available to libraries loaded by later calls to dlopen(3) (as opposed to being referenced by DT_NEEDED entries).\r\n\r\n", "@vikramambrose I have the same error log as you. I set  minSdkVersion<= 22 but it still went wrong. I change build.gradle many times, but it didn't work. Android 10, OnePlus 7.Is any other difference you think may be wrong. May Android 10 didn't allow read DSP files? Your reply may be helpful for me. Thanks a lot.", "> @vikramambrose I have the same error log as you. I set  minSdkVersion<= 22 but it still went wrong. I change build.gradle many times, but it didn't work. Android 10, OnePlus 7.Is any other difference you think may be wrong. May Android 10 didn't allow read DSP files? Your reply may be helpful for me. Thanks a lot.\n\nI upgraded my OnePlus 5 to android 10 and I'm unable to access the DSP from then could be that issue. Will suggest to take a look at Snapdragon Neural Processing Engine and run basic examples there on DSP delegate and see if it works.", "@anidh Thanks for your reply. I have been working on this problem for a long time. Because I have installed an app I developed that always work well on DSP runtime and AIP runtime. But recently I suddenly found it didn't work. I use that app project in my master's degree paper. Thank you. I try on one SNPE SDK demo ( image-classifiers ), and the same error log happened always. I think it may be some permissions of Android 10 or .so files of system. I tried many solutions that didn't work either.  I also sought help form OnePlus tech. Hope it will help. I decide  to wait for OnePlus's reply. Thanks a lot. ", "> @anidh Thanks for your reply. I have been working on this problem for a long time. Because I have installed a apk that always work well on DSP runtime and AIP runtime. But recently I suddenly found it didn't work. I use that app project in my master's degree paper. Thank you. I try on one SNPE SDK demo ( image-classifiers ), and the same error log happened always. I think it may be some permissions of Android 10 or .so files of system. I tried many solutions that didn't work either.  I also sought help form OnePlus tech. Hope it will help. I decide  to wait for OnePlus's reply. Thanks a lot. \n\nI can say that we contacted Qualcomm regarding this issue and they said that it's upto OEM for locking of DSP. I think it's related to the DSP vulnerability found this year. \nI figured out a way to use DSP by rooting my phone and using the selinux policy. This gave me access back to DSP.", "@anidh Yeah, I also found the report about the DSP vulnerability. So I always doubt if OnePlus lock it. I tried root my  phone temporarily and run \"setenforce 0\" , but \"adb shell root\" didn't work and it showed \"Permission denied\". Did you mean root the phone permanently? Thanks, you help me  so much.", "> @anidh Yeah, I also found the report about the DSP vulnerability. So I always doubt if OnePlus lock it. I tried root my  phone temporarily and run \"setenforce 0\" , but \"adb shell root\" didn't work and it showed \"Permission denied\". Did you mean root the phone permanently? Thanks, you help me  so much.\n\nI haven't tried temporary rooting, I rooted my phone permanently. Using the setenforce command it worked for me 1 time and didn't work for me after the reboot. "]}, {"number": 39538, "title": "JNI GPU Bindings Seem Broken", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version: 1.14, 1.15, nightly\r\n- Python version: N/A\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 10.2\r\n- GPU model and memory: Tesla V100\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nSo, I might be crazy, but I think the more recent released builds of `libtensorflow_jni` which have GPU support do not, in fact, have GPU support. Our team was running some performance testing on an instance with a V100, and we noticed that our process wasn't appearing in `nvidia-smi`. On my local machine, I pulled down and dissected the JAR and found something odd: `ldd` appears to not show any dynamic links to any CUDA libraries:\r\n```bash\r\nroot@ffa0a7ff3eed:/# ldd /tf-contents/nightly/libtensorflow_jni.so\r\n\tlinux-vdso.so.1 (0x00007ffc9a9b3000)\r\n\tlibtensorflow_framework.so.2 => /tf-contents/nightly/libtensorflow_framework.so.2 (0x00007fc889320000)\r\n\tlibdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007fc889318000)\r\n\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007fc8891c9000)\r\n\tlibpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007fc8891a6000)\r\n\tlibrt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007fc88919b000)\r\n\tlibstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007fc888fb8000)\r\n\tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007fc888f9d000)\r\n\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007fc888dab000)\r\n\t/lib64/ld-linux-x86-64.so.2 (0x00007fc8a6c00000)\r\nroot@ffa0a7ff3eed:/# ldd /tf-contents/nightly/libtensorflow_framework.so\r\n\tlinux-vdso.so.1 (0x00007ffdda5f5000)\r\n\tlibrt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007f0dabd9d000)\r\n\tlibpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f0dabd7a000)\r\n\tlibdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f0dabd74000)\r\n\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f0dabc25000)\r\n\tlibstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f0daba44000)\r\n\tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f0daba29000)\r\n\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f0dab835000)\r\n\t/lib64/ld-linux-x86-64.so.2 (0x00007f0dad944000)\r\nroot@ffa0a7ff3eed:/# ldd /tf-contents/nightly/libtensorflow_framework.so.2\r\n\tlinux-vdso.so.1 (0x00007ffec27c5000)\r\n\tlibrt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007f7db8332000)\r\n\tlibpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f7db830f000)\r\n\tlibdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f7db8309000)\r\n\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f7db81ba000)\r\n\tlibstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f7db7fd9000)\r\n\tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f7db7fbe000)\r\n\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f7db7dca000)\r\n\t/lib64/ld-linux-x86-64.so.2 (0x00007f7db9ed9000)\r\nroot@ffa0a7ff3eed:/# ldd /tf-contents/nightly/libtensorflow_framework.so.2.0.0\r\n\tlinux-vdso.so.1 (0x00007ffffa3cd000)\r\n\tlibrt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007f6f4328e000)\r\n\tlibpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f6f4326b000)\r\n\tlibdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f6f43265000)\r\n\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f6f43116000)\r\n\tlibstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f6f42f35000)\r\n\tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f6f42f1a000)\r\n\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f6f42d26000)\r\n\t/lib64/ld-linux-x86-64.so.2 (0x00007f6f44e35000)\r\n```\r\nI am seeing the same thing for the latest Maven releases of `libtensorflow_jni_gpu`:\r\n**v1.14.0**\r\n```bash\r\nroot@ffa0a7ff3eed:/# ldd /tf-contents/1.14.0/org/tensorflow/native/linux-x86_64/libtensorflow_framework.so\r\n\tlinux-vdso.so.1 (0x00007fffea2c3000)\r\n\tlibrt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007f9456307000)\r\n\tlibpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f94562e4000)\r\n\tlibdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f94562de000)\r\n\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f945618f000)\r\n\tlibstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f9455fae000)\r\n\tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f9455f93000)\r\n\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f9455d9f000)\r\n\t/lib64/ld-linux-x86-64.so.2 (0x00007f9457f9e000)\r\nroot@ffa0a7ff3eed:/# ldd /tf-contents/1.14.0/org/tensorflow/native/linux-x86_64/libtensorflow_framework.so.1\r\n\tlinux-vdso.so.1 (0x00007fff117d5000)\r\n\tlibrt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007f4e97aca000)\r\n\tlibpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f4e97aa7000)\r\n\tlibdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f4e97aa1000)\r\n\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f4e97952000)\r\n\tlibstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f4e97771000)\r\n\tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f4e97756000)\r\n\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f4e97562000)\r\n\t/lib64/ld-linux-x86-64.so.2 (0x00007f4e99761000)\r\nroot@ffa0a7ff3eed:/# ldd /tf-contents/1.14.0/org/tensorflow/native/linux-x86_64/libtensorflow_framework.so.1.14.0\r\n\tlinux-vdso.so.1 (0x00007fffa79f0000)\r\n\tlibrt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007fe6f6834000)\r\n\tlibpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007fe6f6811000)\r\n\tlibdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007fe6f680b000)\r\n\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007fe6f66bc000)\r\n\tlibstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007fe6f64db000)\r\n\tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007fe6f64c0000)\r\n\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007fe6f62cc000)\r\n\t/lib64/ld-linux-x86-64.so.2 (0x00007fe6f84cb000)\r\nroot@ffa0a7ff3eed:/# ldd /tf-contents/1.14.0/org/tensorflow/native/linux-x86_64/libtensorflow_jni.so\r\n\tlinux-vdso.so.1 (0x00007ffe7c2cc000)\r\n\tlibtensorflow_framework.so.1 => /tf-contents/1.14.0/org/tensorflow/native/linux-x86_64/libtensorflow_framework.so.1 (0x00007f50738d8000)\r\n\tlibdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f50738d0000)\r\n\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f5073781000)\r\n\tlibpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f507375e000)\r\n\tlibrt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007f5073753000)\r\n\tlibstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f5073570000)\r\n\tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f5073555000)\r\n\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f5073363000)\r\n\t/lib64/ld-linux-x86-64.so.2 (0x00007f5087918000)\r\n```\r\n\r\n**v.1.15.0**\r\n```bash\r\nroot@ffa0a7ff3eed:/# ldd /tf-contents/1.15.0/org/tensorflow/native/linux-x86_64/libtensorflow_jni.so\r\n\tlinux-vdso.so.1 (0x00007ffe87dd4000)\r\n\tlibtensorflow_framework.so.1 => /tf-contents/1.15.0/org/tensorflow/native/linux-x86_64/libtensorflow_framework.so.1 (0x00007fbb4df46000)\r\n\tlibdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007fbb4df3e000)\r\n\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007fbb4ddef000)\r\n\tlibpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007fbb4ddcc000)\r\n\tlibrt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007fbb4ddc1000)\r\n\tlibstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007fbb4dbde000)\r\n\tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007fbb4dbc3000)\r\n\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007fbb4d9d1000)\r\n\t/lib64/ld-linux-x86-64.so.2 (0x00007fbb631f2000)\r\nroot@ffa0a7ff3eed:/# ldd /tf-contents/1.15.0/org/tensorflow/native/linux-x86_64/libtensorflow_framework.so.1\r\n\tlinux-vdso.so.1 (0x00007ffe5e258000)\r\n\tlibrt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007f010ef44000)\r\n\tlibpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f010ef21000)\r\n\tlibdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f010ef1b000)\r\n\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f010edcc000)\r\n\tlibstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f010ebeb000)\r\n\tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f010ebd0000)\r\n\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f010e9dc000)\r\n\t/lib64/ld-linux-x86-64.so.2 (0x00007f0110c31000)\r\n```\r\n\r\nIn comparison, take a look at the output for the contents of the 1.5.0 release (note, as mentioned earlier, this dissection was done on my local machine, so that is why there are so many `not found` entries corresponding to the CUDA libs):\r\n```bash\r\nroot@ffa0a7ff3eed:/# ldd /tf-contents/1.5.0/org/tensorflow/native/linux-x86_64/libtensorflow_framework.so\r\n\tlinux-vdso.so.1 (0x00007ffec0f86000)\r\n\tlibcublas.so.9.0 => not found\r\n\tlibcuda.so.1 => not found\r\n\tlibcudnn.so.7 => not found\r\n\tlibcufft.so.9.0 => not found\r\n\tlibcurand.so.9.0 => not found\r\n\tlibcudart.so.9.0 => not found\r\n\tlibdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007fed05cbb000)\r\n\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007fed05b6c000)\r\n\tlibpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007fed05b49000)\r\n\tlibstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007fed05966000)\r\n\tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007fed0594b000)\r\n\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007fed05759000)\r\n\t/lib64/ld-linux-x86-64.so.2 (0x00007fed06b9a000)\r\nroot@ffa0a7ff3eed:/# ldd /tf-contents/1.5.0/org/tensorflow/native/linux-x86_64/libtensorflow_jni.so\r\n\tlinux-vdso.so.1 (0x00007ffd2dbd6000)\r\n\tlibtensorflow_framework.so => /tf-contents/1.5.0/org/tensorflow/native/linux-x86_64/libtensorflow_framework.so (0x00007ff48ba0c000)\r\n\tlibcublas.so.9.0 => not found\r\n\tlibcusolver.so.9.0 => not found\r\n\tlibcudart.so.9.0 => not found\r\n\tlibdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007ff48ba02000)\r\n\tlibpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007ff48b9df000)\r\n\tlibgomp.so.1 => not found\r\n\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007ff48b890000)\r\n\tlibrt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007ff48b885000)\r\n\tlibstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007ff48b6a2000)\r\n\tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007ff48b687000)\r\n\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007ff48b495000)\r\n\t/lib64/ld-linux-x86-64.so.2 (0x00007ff49463b000)\r\n\tlibcublas.so.9.0 => not found\r\n\tlibcuda.so.1 => not found\r\n\tlibcudnn.so.7 => not found\r\n\tlibcufft.so.9.0 => not found\r\n\tlibcurand.so.9.0 => not found\r\n\tlibcudart.so.9.0 => not found\r\n```\r\n\r\nThese same differences were confirmed on our GPU instance. Am I crazy, or is this unexpected?\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nSee Above\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["This looks like a false alarm. I now see that `dso_loader.cc` is responsible for the dynamic library loading, as opposed to listing them in the ELF data.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39538\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39538\">No</a>\n"]}, {"number": 39536, "title": "Failed to pass tf.data.Dataset object to multi-input tf.keras model", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n**Yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n**Linux Ubuntu 18.04**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n**NIL**\r\n- TensorFlow installed from (source or binary):\r\n**Binary**\r\n- TensorFlow version (use command below):\r\n**2.1.0**\r\n- Python version:\r\n**3.6**\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n**10.1 / 7.6.4**\r\n- GPU model and memory:\r\n**RTX 2070 Super, 8GB RAM**\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n```\r\nValueError: Failed to find data adapter that can handle input: (<class 'dict'> containing {\"<class 'str'>\"} keys and {\"<class 'tensorflow.python.data.ops.dataset_ops.ZipDataset'>\"} values), <class 'NoneType'>\r\n```\r\nAn error is encountered when trying to pass multiple ```tf.data.Dataset``` objects to my multi-input Keras model. This is done by passing a dictionary with the keys corresponding to the name of each of my input layers and values corresponding to each ```tf.data.Dataset``` object.\r\nI had no issue passing the same dataset object to a model taking in a single input, but when I tried to combine multiple Model instances into a single Model (for an ensemble CNN), this error is encountered.\r\n\r\nMy network and ensemble code:\r\n```python\r\ndef Net(inputs):\r\n    base_model = DenseNet121(include_top=False, weights='imagenet', input_tensor=inputs)\r\n    output = base_model.get_layer(\"pool3_conv\").output\r\n    x = Conv2D(128, 3, activation='relu', padding='same')(output)\r\n    x = BatchNormalization()(x)\r\n    x = Conv2D(64, 3, activation='relu', padding='same')(x)\r\n    x = BatchNormalization()(x)\r\n    x = Flatten()(x)\r\n    x = Dense(2, activation='softmax', name='clf_output')(x)\r\n\r\n    model = tf.keras.models.Model(inputs=[base_model.input], outputs=[x])\r\n\r\n    return model\r\n\r\ndef create_ensemble(models):\r\n    for i in range(len(models)):\r\n        # Each model is a Net object\r\n        model = models[i]\r\n        for layer in model.layers[1:]:\r\n            layer.trainable = False\r\n            layer._name = 'ensemble_' + str(i+1) + '_' + layer._name\r\n\r\n    stack_inputs = [model.input for model in models]\r\n    stack_outputs = [model.output for model in models]\r\n    merge = Concatenate()(stack_outputs) \r\n    x = Dense(16, activation='relu')(merge)\r\n    x = Dense(2, activation='softmax')(x)\r\n\r\n    model = tf.keras.models.Model(inputs=stack_inputs, outputs=x, name='ensemble')\r\n\r\n    return model\r\n```\r\nMy training process is summarized as follows:\r\n1. Train 5 instances of DenseNet121 models, **in sequence**\r\n2. Create a new ```ensemble```, combining the outputs of the previously trained DenseNet121 models\r\n3. Train the ensembled model **(error occurs on ```model.fit```)**\r\n\r\nMy training code:\r\n```python\r\n    IMAGE_DIR = os.path.join(DATA_DIR, \"images\")\r\n    DEPTH_DIR = os.path.join(DATA_DIR, \"labels\")\r\n    VAL_SPLIT = int(np.floor(0.1 * len(list(paths.list_images(IMAGE_DIR)))))\r\n    print(f\"Validation split is: {VAL_SPLIT} images\")\r\n\r\n    imagePaths = sorted(list(paths.list_images(IMAGE_DIR)))\r\n    depthPaths = sorted(list(paths.list_images(DEPTH_DIR)))\r\n    labels = generate_labels(depthPaths)\r\n\r\n    image_ds = tf.data.Dataset.from_tensor_slices(imagePaths)\r\n    image_ds = create_image_dataset(\r\n        image_ds, batch_size=BS, seed=42, training=True)\r\n\r\n    labels_ds = tf.data.Dataset.from_tensor_slices(labels)\r\n    labels_ds = create_labels_dataset(labels_ds, batch_size=BS, seed=42)\r\n\r\n    # Splitting into training and validation dataset\r\n    image_train, labels_train = image_ds.skip(VAL_SPLIT), labels_ds.skip(VAL_SPLIT)\r\n    image_val, labels_val = image_ds.take(VAL_SPLIT), labels_ds.take(VAL_SPLIT)\r\n\r\n    train_ds = tf.data.Dataset.zip((image_train, labels_train))\r\n    val_ds = tf.data.Dataset.zip((image_val, labels_val))\r\n\r\n    # Building the model\r\n    models = []\r\n    for i in range(1, N_MODELS+1):\r\n        inputs = tf.keras.layers.Input(shape=[224, 224, 3], name=f'input_{i}')\r\n        model = Net(inputs)\r\n        models.append(model)\r\n\r\n    print(\"Checking model architecture: \")\r\n    print(model.summary())\r\n\r\n    opt = Lookahead(RAdam(lr=INIT_LR))\r\n    opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt)\r\n\r\n    # Creating callbacks\r\n    #earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='auto', restore_best_weights=True)\r\n\r\n    reducelr = tf.keras.callbacks.LearningRateScheduler(reduce_lr)\r\n\r\n    # Training the model\r\n    print(\"Training\")\r\n    if args.pretrained == 0:\r\n        for i in range(N_MODELS):\r\n            model = models[i]\r\n            model.compile(loss=\"categorical_crossentropy\", \r\n                          optimizer=opt, \r\n                          metrics=[\"accuracy\"])\r\n\r\n            checkpoint = tf.keras.callbacks.ModelCheckpoint(f\"net_{i}_backup_weights.h5\", \r\n                                                            monitor='val_loss', \r\n                                                            verbose=1, \r\n                                                            save_weights_only=True, \r\n                                                            save_best_only=True)\r\n            \r\n            history = model.fit(train_ds, \r\n                                epochs=EPOCHS, \r\n                                verbose=1,\r\n                                steps_per_epoch=(len(imagePaths)-VAL_SPLIT) // BS, \r\n                                validation_data=(val_ds), \r\n                                validation_steps=VAL_SPLIT // BS, \r\n                                callbacks=[checkpoint, reducelr]) \r\n\r\n            # Saving the model\r\n            print(f\"Saving model {i}\")\r\n            model.save(f\"net_{i}.h5\", include_optimizer=False)\r\n            model.save_weights(f\"net_{i}_weights.h5\")\r\n\r\n    else:\r\n        print(\"Loading pretrained weights\")\r\n        for i in range(args.pretrained):\r\n            models[i].load_weights(f\"net_{i}_weights.h5\")\r\n\r\n    # Creating Ensemble\r\n    print(\"Creating Ensemble\")\r\n    ensemble = create_ensemble(models)\r\n    print(\"Ensemble architecture: \")\r\n    print(ensemble.summary())\r\n\r\n    keras.utils.plot_model(model, \"ensemble.png\", show_shapes=True)\r\n    ensemble.save(\"ensemble.h5\")\r\n\r\n    ensemble_train = {}\r\n    ensemble_val = {}\r\n    for i in range(1, args.pretrained+1):\r\n        ensemble_train[f'input_{i}'] = train_ds\r\n        ensemble_val[f'input_{i}'] = val_ds\r\n\r\n    assert len(ensemble_train) == int(args.pretrained)\r\n    assert len(ensemble_val) == int(args.pretrained)\r\n\r\n    ensemble.compile(loss=\"categorical_crossentropy\", \r\n                     optimizer=opt, \r\n                     metrics=[\"accuracy\"])\r\n\r\n    ensemble_checkpoint = tf.keras.callbacks.ModelCheckpoint(\"ensemble_backup_weights.h5\", \r\n                                                             monitor='val_loss', \r\n                                                             verbose=1, \r\n                                                             save_weights_only=True, \r\n                                                             save_best_only=True)\r\n\r\n    history = ensemble.fit(ensemble_train, \r\n                           epochs=EPOCHS, \r\n                           verbose=1,\r\n                           steps_per_epoch=(len(imagePaths)-VAL_SPLIT) // BS, \r\n                           validation_data=(ensemble_val), \r\n                           validation_steps = VAL_SPLIT // BS, \r\n                           callbacks=[ensemble_checkpoint, reducelr])\r\n   print(\"Saving Ensemble\")\r\n      ensemble.save(\"ensemble.h5\", include_optimizer=False)\r\n      ensemble.save_weights(\"ensemble_weights.h5\")\r\n\r\nif __name__ == '__main__':\r\n    ap = argparse.ArgumentParser()\r\n    ap.add_argument('--pretrained', default=0, type=int, help='number of pretrained models to use')\r\n    main(ap.parse_args())\r\n    sess.close()\r\n```\r\nOutput of ```print(train_ds)```: (batch_size=16, no. of classes=2)\r\n```python\r\n<ZipDataset shapes: ((16, 224, 224, 3), (16, 2)), types: (tf.float32, tf.float32)>\r\n<ZipDataset shapes: ((16, 224, 224, 3), (16, 2)), types: (tf.float32, tf.float32)>\r\n<ZipDataset shapes: ((16, 224, 224, 3), (16, 2)), types: (tf.float32, tf.float32)>\r\n<ZipDataset shapes: ((16, 224, 224, 3), (16, 2)), types: (tf.float32, tf.float32)>\r\n<ZipDataset shapes: ((16, 224, 224, 3), (16, 2)), types: (tf.float32, tf.float32)>\r\n```\r\n\r\n**Describe the expected behavior**\r\nThe ```tf.data.Dataset``` object should not require a 'y' value to be passed to ```model.fit``` explicitly, since I have combined the images and labels through ```tf.data.Dataset.zip```.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Issue resolved by changing code in the network from \r\n```python\r\nstack_inputs = [model.input for model in models]\r\nstack_outputs = [model.output for model in models[\r\n```\r\nto:\r\n```python\r\nstack_outputs = [model(inputs) for model in models]\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39536\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39536\">No</a>\n"]}, {"number": 39535, "title": "Release GPU Memory(VRAM) after tf.keras.backend.clear_session()", "body": "**System information**\r\n- Windows 10 Microsoft Windows [Version 10.0.18362.418]\r\n- TensorFlow 2.0 installed from Conda:\r\n- Python version: 3.6.10\r\n- CUDA/cuDNN version:   NVIDIA-SMI 445.75       Driver Version: 445.75       CUDA Version: 11.0\r\n- GPU model and memory: NVIDIA 2060S\r\n\r\n\r\n**Describe the current behavior**\r\nThere's no command which frees the previously used VRAM. Even deleting the model and the data had no effect on the VRAM.\r\n\r\n**Describe the expected behavior**\r\nAny of these commands should release the VRAM. \r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Dense\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow import math, dtypes\r\nfrom tensorflow import float32 as f32 \r\nfrom tensorflow.keras.optimizers import Adam\r\nfrom tensorflow.keras.layers import Input\r\nimport random\r\nimport numpy as np # linear algebra\r\nimport gc\r\n\r\nrseed=10\r\nnp.random.seed(rseed)\r\nrandom.seed(rseed)\r\ntf.compat.v1.set_random_seed(rseed)\r\n    \r\ndef MMSE( preds,targets, mask_value=0.0):\r\n    tf.print('\\npred',preds)\r\n    tf.print('target',targets)\r\n    mask = dtypes.cast(tf.not_equal(targets,0),f32) \r\n    num_rating = math.reduce_sum(mask) #count ratings\r\n    loss = math.reduce_sum(math.square(mask*(preds - targets))) / num_rating \r\n    return loss\r\n\r\n\r\ninput_dim = Input(shape = (3, ))\r\nmodel = Sequential()\r\nmodel.add(Dense(3,input_dim=3))\r\nmodel.add(Dense(3))\r\nmodel.compile(optimizer = Adam(lr=0.01),loss=[MMSE]) \r\n            \r\ndata  = tf.math.round(tf.random.normal(shape=[5,3]))\r\nhistory = model.fit(data,data, epochs = 1, batch_size = 5,verbose=0, shuffle=False) \r\n\r\ndel input_dim,model,data,history\r\ntf.compat.v1.reset_default_graph()\r\ntf.keras.backend.clear_session()\r\ngc.collect()\r\n\r\n```\r\nI've used nvidia-smi to check the memory-usage.\r\n", "comments": ["It is currently not possible without exiting the Python process due to the fact that many TF internal objects, e.g. GPU memory pool, device streams, do not support clean shutdown. ", "What a pity. Can I somehow estimate it by using the number of trainable parameters and the batch size?", "One could check the model size or the trainable parameters, but this is somehow not satisfying. [Source](https://stackoverflow.com/questions/58513385/how-to-compare-or-measure-the-actual-size-of-models-in-tensorflow)", "@Arktius As this issue has been answered, can you please create a new issue and close this one. Thanks!", "> It is currently not possible without exiting the Python process due to the fact that many TF internal objects, e.g. GPU memory pool, device streams, do not support clean shutdown.\r\n\r\nThis seems like a core deficiency in the library.  Is there  a followup ticket for a line of work to fix this?  "]}, {"number": 39534, "title": "Build tensorflow-lite-with-select-tf-ops Failed", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution   Linux Ubuntu 18.04 \r\n- TensorFlow installed from source\r\n- TensorFlow version: r2.2\r\n- Python version: Python 3.6.9\r\n- Bazel version (if compiling from source): 2.0.0\r\n- GCC/Compiler version (if compiling from source):gcc version 7.5.0\r\n- CUDA/cuDNN version: no GPU\r\n- GPU model and memory:no GPU\r\n\r\nndk-version ndkr18\r\nsdk-version sdk 24.4.1\r\n\r\n**Describe the problem**\r\nwhen I complie tflite as https://www.tensorflow.org/lite/guide/ops_select#android_aar,\r\nthe error comes.\r\nERROR: no such target '//tensorflow/lite/java:tensorflow-lite-with-select-tf-ops': target 'tensorflow-lite-with-select-tf-ops' not declared in package 'tensorflow/lite/java' defined by /tensorflow_src/tensorflow/lite/java/BUILD\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nfirstly , I prepare the environment as the manual https://www.tensorflow.org/install/source#configure_the_build\r\nthen I run  command as https://www.tensorflow.org/lite/guide/ops_select#android_aar\r\nbazel build --cxxopt='--std=c++11' -c opt             \\\r\n  --config=android_arm --config=monolithic          \\\r\n  //tensorflow/lite/java:tensorflow-lite-with-select-tf-ops\r\n \r\nERROR: Skipping '//tensorflow/lite/java:tensorflow-lite-with-select-tf-ops': no such target '//tensorflow/lite/java:tensorflow-lite-with-select-tf-ops': target 'tensorflow-lite-with-select-tf-ops' not declared in package 'tensorflow/lite/java' defined by /tensorflow_src/tensorflow/lite/java/BUILD\r\nWARNING: Target pattern parsing failed.\r\nERROR: no such target '//tensorflow/lite/java:tensorflow-lite-with-select-tf-ops': target 'tensorflow-lite-with-select-tf-ops' not declared in package 'tensorflow/lite/java' defined by /tensorflow_src/tensorflow/lite/java/BUILD\r\n", "comments": ["Hi, ZhuXinyan, the documentation actually shows you should use the target\r\n//tensorflow/lite/java:tensorflow-lite-select-tf-ops\r\nI think you added an extra -with- in your target.", "@ZhuXinyan as @daverim said there is no `-with-` in current doc and code, probably you referred to an older version of the doc. The `BUILD` file was updated and the corresponding doc you referred to was updated too.", "Thanks\uff0cI find the English document is right but the chinese document is expired", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39534\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39534\">No</a>\n"]}, {"number": 39533, "title": "Tensorflow 2.1.0 on Windows: No OpKernel was registered to support Op 'SparseMatMul'", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version: 2.1.0\r\n- Python version: 3.6.0\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: \r\n- GPU model and memory: \r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nWhen I run the following codes: \r\n\r\n```\r\n    def _call(self, inputs):\r\n        x = inputs\r\n\r\n        # dropout\r\n        if self.sparse_inputs:\r\n            x = sparse_dropout(x, 1 - self.dropout, self.num_features_nonzero)\r\n        else:\r\n            x = tf.nn.dropout(x, 1 - self.dropout)\r\n\r\n        # convolve\r\n        supports = tf.matmul(tf.sparse_tensor_to_dense(self.support[0]), tf.diag(self.vars['kernel']), a_is_sparse=True,\r\n                             b_is_sparse=True)\r\n        supports = tf.matmul(supports, tf.sparse_tensor_to_dense(self.support[1]), a_is_sparse=True, b_is_sparse=True)\r\n        pre_sup = dot(x, self.vars['weights_' + str(0)], sparse=self.sparse_inputs)\r\n        output = dot(supports, pre_sup)\r\n\r\n        if self.bias:\r\n            output += self.vars['bias']\r\n\r\n        return self.act(output)\r\n```\r\n\r\nThe following error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\zhuowei\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1367, in _do_call\r\n    return fn(*args)\r\n  File \"C:\\Users\\zhuowei\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1350, in _run_fn\r\n    self._extend_graph()\r\n  File \"C:\\Users\\zhuowei\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1390, in _extend_graph\r\n    tf_session.ExtendSession(self._session)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'SparseMatMul' used by {{node wavelet_convolution_1/MatMul}}with these attrs: [transpose_b=false, Ta=DT_FLOAT, Tb=DT_FLOAT, b_is_sparse=true, a_is_sparse=true, transpose_a=false]\r\nRegistered devices: [CPU, GPU]\r\nRegistered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[wavelet_convolution_1/MatMul]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:/Users/zhuowei/Desktop/GAdv/GWNN-master/GraphWaveletNetwork/train.py\", line 128, in <module>\r\n    sess.run(tf.global_variables_initializer())\r\n  File \"C:\\Users\\zhuowei\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 960, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\Users\\zhuowei\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1183, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"C:\\Users\\zhuowei\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1361, in _do_run\r\n    run_metadata)\r\n  File \"C:\\Users\\zhuowei\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1386, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'SparseMatMul' used by node wavelet_convolution_1/MatMul (defined at \\Users\\zhuowei\\Desktop\\GAdv\\GWNN-master\\GraphWaveletNetwork\\layers.py:320) with these attrs: [transpose_b=false, Ta=DT_FLOAT, Tb=DT_FLOAT, b_is_sparse=true, a_is_sparse=true, transpose_a=false]\r\nRegistered devices: [CPU, GPU]\r\nRegistered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[wavelet_convolution_1/MatMul]]\r\n\r\nProcess finished with exit code 1\r\n```\r\n\r\n`SparseMatMul ` op is unavailable on GPU?\r\n", "comments": ["@zhuo931077127 \r\nCan you please check if you have compatibility issues with tensorflow as per [this comment](https://github.com/sampepose/flownet2-tf/issues/17#issuecomment-350424046) and [this comment](https://github.com/tensorflow/tensorflow/issues/22699#issuecomment-431107100), also please refer to this [link](https://stackoverflow.com/questions/47040860/cannot-run-tensorflow-on-gpu)\r\nAS i ran the code shared by you on tf 2.1 and do not face any error, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/38edcfdaf01b97765221b03504a87901/39533.ipynb)", "@Saduf2019 I ran the code on tf1.10, and this error disappeared."]}, {"number": 39532, "title": "tflite with illegal instruction - Self compile of \"_interpreter_wrapper.so\" ?", "body": "**System information**\r\n- Yocto Poky 3.0\r\n- embedded device with ARM Cortex A9\r\n- https://dl.google.com/coral/python/tflite_runtime-2.1.0.post1-cp37-cp37m-linux_armv7l.whl \r\n- Python version: 3.7\r\n- GCC/Compiler 9.2:\r\n- 512MB RAM:\r\n\r\n**Describe the problem**\r\nI can execute some tflite nets but not all. Not working: \"mobilenet_v1_1.0_224.tflite\" But working: \"mobilenet_v1_1.0_224_quant.tflite\". My suspect is that tensorflow is not compiled for my ARM CPU (Cortex A9) - although there is just one ARM32bit selection for a python whl.\r\nI could recompile a \"libtensorflow-lite.a\" but howto create a python package or at least a substitute for the \"_interpreter_wrapper.so\" ? (this seems to be where the magic happens).\r\n\r\nThe only hint I found was for tensorflow (the big one) handling with docker. No experience with that, but I guess this doesn't use my cross-compiler and options.", "comments": ["Hi terryheo@, do you have some experience with arm linux?", "This is the way to build TFLite PIP package.\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/pip_package", "That is exactly what I am looking for. But I need a closer look - get an error\r\n```\r\n+ python3 setup.py bdist bdist_wheel\r\nTraceback (most recent call last):\r\n  File \"setup.py\", line 35, in <module>\r\n    import numpy\r\nModuleNotFoundError: No module named 'numpy'\r\n```\r\nalthough I installed all required dependencies. Maybe a cross issue.", "I installed the required packages (swig libjpeg-dev zlib1g-dev python3-dev python3-numpy) in my x86 Linux downloaded the requirements. Starting that script will start (until it fails with:\r\n`interpreter_wrapper/interpreter_wrapper_pybind11.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory`\r\nbut compiles for x86 of course (which is not my goal). In my first attempt I did set my SDK environment (intended for C/C++ cross compiling) and there has been no numpy ... installed.  Now I am a bit confused: How can I cross compile/create that? My hope has been to crosscompile tensorflow and wrapper/python stuff is independend from architecture. But is it like that?", "Which command did you used?", "First I did: \r\nmodifiy /tensorflow/lite/tools/make/targets/riscv_makefile.inc (to make it cross compileable and fit to my arm)\r\n```\r\nsource ~/sdk/environment-setup-cortexa9hf-neon-poky-linux-gnueabi // (setup compiler, pathes)\r\n./tensorflow/lite/tools/make/build_rpi_lib.sh\r\n```\r\nthen I get an tensorflow/lite/tools/make/gen/rpi_armv7l/lib/libtensorflow-lite.a\r\n\r\nBut I want to get the python version of it:\r\nSo I executed the above linked commands:\r\n\r\n```\r\nsudo apt install swig libjpeg-dev zlib1g-dev python3-dev python3-numpy\r\nsh tensorflow/lite/tools/make/download_dependencies.sh\r\nsh tensorflow/lite/tools/pip_package/build_pip_package.sh\r\n```\r\nBut this is now on x86. I need to combine both .... ways, so cross compile the tensorflow and build the whl.", "A little step might be to add \"TENSORFLOW_TARGET=\"rpi\" to the build_pip_package.sh. But this is not enough....", "For the cross build of PIP, you need to use Docker based build.\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/pip_package#building-tensorflow-lite-standalone-pip\r\nThis is the current official way.\r\n```\r\nmake BASE_IMAGE=debian:buster PYTHON=python3 TENSORFLOW_TARGET=rpi docker-build\r\n```\r\n\r\nThere is another experimental method which uses ARM toolchain.\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/pip_package#cross-build-for-armhf-python-37\r\n```\r\nCI_DOCKER_EXTRA_PARAMS=\"-e CI_BUILD_PYTHON=python3 -e CROSSTOOL_PYTHON_INCLUDE_PATH=/usr/include/python3.7\" \\\r\n  tensorflow/tools/ci_build/ci_build.sh PI-PYTHON37 \\\r\n  tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh armhf\r\n```\r\nI'm wondering which compile options you need to build for your target.\r\nThere is a similar issue with this. You can also refer https://github.com/tensorflow/tensorflow/issues/39957", "Let's close this. Please let me know if you still have the issue.", "Isn't there a way without docker? Guess docker uses it's own toolchain but I want use my own (yocto) compiler ...."]}, {"number": 39531, "title": "Import error", "body": "import tensorflow as tf\r\ntf.add(1, 2).numpy()\r\n\r\nI ran the above code for non GPU version and got the following error.\r\n\r\n\r\nImportError                               Traceback (most recent call last)\r\n~\\Anaconda3\\New\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59 \r\n\r\n~\\Anaconda3\\New\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in <module>\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow_internal = swig_import_helper()\r\n     29     del swig_import_helper\r\n\r\n~\\Anaconda3\\New\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     23             try:\r\n---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n     25             finally:\r\n\r\n~\\Anaconda3\\New\\lib\\imp.py in load_module(name, file, filename, details)\r\n    241         else:\r\n--> 242             return load_dynamic(name, filename, file)\r\n    243     elif type_ == PKG_DIRECTORY:\r\n\r\n~\\Anaconda3\\New\\lib\\imp.py in load_dynamic(name, path, file)\r\n    341             name=name, loader=loader, origin=path)\r\n--> 342         return _load(spec)\r\n    343 \r\n\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-4-79ec058b6a86> in <module>\r\n----> 1 import tensorflow as tf\r\n      2 tf.add(1, 2).numpy()\r\n\r\n~\\Anaconda3\\New\\lib\\site-packages\\tensorflow\\__init__.py in <module>\r\n     39 import sys as _sys\r\n     40 \r\n---> 41 from tensorflow.python.tools import module_util as _module_util\r\n     42 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader\r\n     43 \r\n\r\n~\\Anaconda3\\New\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>\r\n     48 import numpy as np\r\n     49 \r\n---> 50 from tensorflow.python import pywrap_tensorflow\r\n     51 \r\n     52 # Protocol buffers\r\n\r\n~\\Anaconda3\\New\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     67 for some common reasons and solutions.  Include the entire stack trace\r\n     68 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 69   raise ImportError(msg)\r\n     70 \r\n     71 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Vishal\\Anaconda3\\New\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Vishal\\Anaconda3\\New\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Vishal\\Anaconda3\\New\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Vishal\\Anaconda3\\New\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Vishal\\Anaconda3\\New\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\n\r\n\r\nI upgraded it\r\n\r\n(base) C:\\Users\\Vishal>pip install --upgrade tensorflow\r\nCollecting tensorflow\r\n  Using cached https://files.pythonhosted.org/packages/af/50/d7da24189d95e2084bb1cc350a8e4acdf1b0c9b3d57def7a348f0d9cb062/tensorflow-2.2.0-cp37-cp37m-win_amd64.whl\r\nRequirement already satisfied, skipping upgrade: grpcio>=1.8.6 in c:\\users\\vishal\\anaconda3\\new\\lib\\site-packages (from tensorflow) (1.28.1)\r\nRequirement already satisfied, skipping upgrade: keras-preprocessing>=1.1.0 in c:\\users\\vishal\\anaconda3\\new\\lib\\site-packages (from tensorflow) (1.1.0)\r\nRequirement already satisfied, skipping upgrade: termcolor>=1.1.0 in c:\\users\\vishal\\anaconda3\\new\\lib\\site-packages (from tensorflow) (1.1.0)\r\nRequirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in c:\\users\\vishal\\anaconda3\\new\\lib\\site-packages (from tensorflow) (0.33.4)\r\nRequirement already satisfied, skipping upgrade: astunparse==1.6.3 in c:\\users\\vishal\\anaconda3\\new\\lib\\site-packages (from tensorflow) (1.6.3)\r\nRequirement already satisfied, skipping upgrade: google-pasta>=0.1.8 in c:\\users\\vishal\\anaconda3\\new\\lib\\site-packages (from tensorflow) (0.2.0)\r\nRequirement already satisfied, skipping upgrade: h5py<2.11.0,>=2.10.0 in c:\\users\\vishal\\anaconda3\\new\\lib\\site-packages (from tensorflow) (2.10.0)\r\nRequirement already satisfied, skipping upgrade: protobuf>=3.8.0 in c:\\users\\vishal\\anaconda3\\new\\lib\\site-packages (from tensorflow) (3.11.3)\r\nRequirement already satisfied, skipping upgrade: six>=1.12.0 in c:\\users\\vishal\\anaconda3\\new\\lib\\site-packages (from tensorflow) (1.12.0)\r\nRequirement already satisfied, skipping upgrade: wrapt>=1.11.1 in c:\\users\\vishal\\anaconda3\\new\\lib\\site-packages (from tensorflow) (1.11.2)\r\nRequirement already satisfied, skipping upgrade: gast==0.3.3 in c:\\users\\vishal\\anaconda3\\new\\lib\\site-packages (from tensorflow) (0.3.3)\r\nRequirement already satisfied, skipping upgrade: absl-py>=0.7.0 in c:\\users\\vishal\\anaconda3\\new\\lib\\site-packages (from tensorflow) (0.9.0)\r\nRequirement already satisfied, skipping upgrade: tensorflow-estimator<2.3.0,>=2.2.0 in c:\\users\\vishal\\anaconda3\\new\\lib\\site-packages (from tensorflow) (2.2.0)\r\nRequirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in c:\\users\\vishal\\anaconda3\\new\\lib\\site-packages (from tensorflow) (3.2.1)\r\nRequirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in c:\\users\\vishal\\anaconda3\\new\\lib\\site-packages (from tensorflow) (1.16.4)\r\nRequirement already satisfied, skipping upgrade: scipy==1.4.1; python_version >= \"3\" in c:\\users\\vishal\\anaconda3\\new\\lib\\site-packages (from tensorflow) (1.4.1)\r\nRequirement already satisfied, skipping upgrade: tensorboard<2.3.0,>=2.2.0 in c:\\users\\vishal\\anaconda3\\new\\lib\\site-packages (from tensorflow) (2.2.1)\r\nRequirement already satisfied, skipping upgrade: setuptools in c:\\users\\vishal\\anaconda3\\new\\lib\\site-packages (from protobuf>=3.8.0->tensorflow) (41.0.1)\r\nRequirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in c:\\users\\vishal\\anaconda3\\new\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.14.3)\r\nRequirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\vishal\\anaconda3\\new\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.1)\r\nRequirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in c:\\users\\vishal\\anaconda3\\new\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (0.15.4)\r\nRequirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in c:\\users\\vishal\\anaconda3\\new\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.6.0.post3)\r\nRequirement already satisfied, skipping upgrade: markdown>=2.6.8 in c:\\users\\vishal\\anaconda3\\new\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (3.2.2)\r\nRequirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in c:\\users\\vishal\\anaconda3\\new\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (2.22.0)\r\nRequirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in c:\\users\\vishal\\anaconda3\\new\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.2.8)\r\nRequirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in c:\\users\\vishal\\anaconda3\\new\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (4.0)\r\nRequirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in c:\\users\\vishal\\anaconda3\\new\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (4.1.0)\r\nRequirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in c:\\users\\vishal\\anaconda3\\new\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.3.0)\r\nRequirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in c:\\users\\vishal\\anaconda3\\new\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.17)\r\nRequirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in c:\\users\\vishal\\anaconda3\\new\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.0.4)\r\nRequirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in c:\\users\\vishal\\anaconda3\\new\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2.8)\r\nRequirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\vishal\\anaconda3\\new\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.24.2)\r\nRequirement already satisfied, skipping upgrade: certifi>=2017.4.17 in c:\\users\\vishal\\anaconda3\\new\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2019.6.16)\r\nRequirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in c:\\users\\vishal\\anaconda3\\new\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.8)\r\nRequirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in c:\\users\\vishal\\anaconda3\\new\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.0)\r\nRequirement already satisfied, skipping upgrade: zipp>=0.5 in c:\\users\\vishal\\anaconda3\\new\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.5.1)\r\nInstalling collected packages: tensorflow\r\n  Found existing installation: tensorflow 2.0.0\r\n    Uninstalling tensorflow-2.0.0:\r\n      Successfully uninstalled tensorflow-2.0.0\r\nSuccessfully installed tensorflow-2.2.0\r\n\r\n(base) C:\\Users\\Vishal>\r\n\r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n\n* For TF-GPU - See point 1\n* For TF-CPU - See point 2\n\n-----------------------------------------------------------------------------------------------\n\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\n*TF Version >= 1.13 requires CUDA 10.0 and TF Version < 1.13 (till TF 1.5) requires CUDA 9.0.*\n\n* If you have above configuration and using _**Windows**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n  * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n* If you have above configuration and using _**Ubuntu/Linux**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n  * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n* If error still persists then, apparently your CPU model does not support AVX instruction sets.\n  * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\n Therefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n* Try Google Colab to use TensorFlow.\n  * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true).You get pre-installed latest stable TF version. Also you can use```pip install``` to install any other preferred TF version.\n  * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n  * All you need is a good internet connection and you are all set.\n* Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*", "I started using google colab which is great.\r\nThank you", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39531\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39531\">No</a>\n"]}, {"number": 39530, "title": "replace layer in resnet", "body": "Is it possible to remove/replace the lower layers of a pretrained ResNet50 model in tf.keras.applications? trying to replace the first Conv2D layer with a new one. The only examples I've seen online do it for adding to the top of the network, not the first few layers.\r\n\r\nFor instance, I've tried doing this:\r\n\r\n```\r\nimport tensorflow as tf\r\npretrained_resnet = tf.keras.applications.ResNet50(include_top=False, weights='imagenet')\r\ninputs = tf.keras.Input(shape=(256,256,1))\r\nx = tf.keras.layers.ZeroPadding2D()(inputs)\r\nx = tf.keras.layers.Conv2D(filters=64,\r\n                           kernel_size=(7,7),\r\n                           strides=(2,2),\r\n                           padding='same')(x)\r\noutputs = pretrained_resnet.layers[3](x)\r\ntest = tf.keras.Model(inputs, pretrained_resnet.output)\r\n```\r\nBut it gives an error.", "comments": ["How did you resolve this @coded5282 ?", "I've attempted multiple work arounds for this exact issue. I have been unable to fix it. I suggest you either copy the code in the tf.keras implementation ResNet, then slice out the layers in the ResNet construction code and tack them on after your custom layer code. This allows you to get essentially `pretrained_resnet[3:]`. Then copy the imagenet weights across."]}, {"number": 39529, "title": "Add complex64/complex128 support for tf.math.l2_normalize", "body": "This PR tries to address the issue raised in #39522 where\r\nthere was no complex support for tf.math.l2_normalize.\r\n\r\nThis PR adds complex support for tf.math.l2_normalize.\r\n\r\nThis PR fixes #39522.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 39528, "title": "ValueError: Error when checking target: expected Output to have 2 dimensions, but got array with shape (631, 80, 2641)", "body": "Alright, after I use GlobalAveragePooling1D, I encounter another problem. It's the dimension.\r\n\r\nThe shape of outputs of the model is 2 dimensions (batch_size, features) while the shape of the Y Train is 3 dimensions.\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-30-f6e17343a06b> in <module>()\r\n     10           batch_size=batch_size,\r\n     11           epochs=training_epoch,\r\n---> 12           validation_split=0.1)\r\n\r\n2 frames\r\n/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py in standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix)\r\n    133                         ': expected ' + names[i] + ' to have ' +\r\n    134                         str(len(shape)) + ' dimensions, but got array '\r\n--> 135                         'with shape ' + str(data_shape))\r\n    136                 if not check_batch_axis:\r\n    137                     data_shape = data_shape[1:]\r\n\r\nValueError: Error when checking target: expected Output to have 2 dimensions, but got array with shape (631, 80, 2641)\r\n\r\n```\r\nI have two choices. First is changing the shape of Y Train into two dimensions. Second is changing the model.\r\n\r\nThe model is following the model I (Bi-LSTM) in \"Neural Models for Sequence Chunking\". It said to \"average the input vectors\".\r\n\r\nI will post the code here. Reminder, I'm using Google CoLab\r\n\r\n```\r\n# Padding\r\nX = [[word2idx[w[0]] for w in s] for s in quran_sentences]\r\nX = pad_sequences(maxlen=max_length, sequences=X, padding=\"post\",value=word2idx[\"PAD\"])\r\n\r\ny = [[tag2idx[w[1]] for w in s] for s in quran_sentences]\r\ny = pad_sequences(maxlen=max_length, sequences=y, padding=\"post\", value=tag2idx[\"O\"])\r\ny = [to_categorical(i, num_classes=n_tags) for i in y]\r\n\r\nX_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42)\r\n\r\n# Parameters\r\nhidden_state_encoder_size = 100\r\n\r\n# hidden_state_decoder_size = 200\r\n\r\nbatch_size = 64\r\n\r\ntraining_epoch = 200\r\n\r\nembedding_size = 80\r\n\r\ndropout_rate = 0.5\r\n\r\n# Model\r\n# Input\r\ninputs = Input(shape=(max_length,), name=\"Input\")\r\n\r\n# Embedding\r\n# Output = (batch_size, input_length, output_dim)\r\nembed = Embedding(input_dim=n_words+1,\r\n                  output_dim=embedding_size,\r\n                  input_length=max_length,\r\n                  name=\"Embedding\")(inputs)\r\n\r\n# Bi-LSTM\r\n# Output = (batch_size, steps, features)\r\nencoder = Bidirectional(LSTM(units=hidden_state_encoder_size,\r\n                             return_sequences=True,\r\n                             dropout=dropout_rate,\r\n                             name=\"LSTM\"),\r\n                        name=\"Bi-LSTM\")\r\n\r\nhidden_states = encoder(embed)\r\n\r\n# Average\r\n# Output = (batch_size, features)\r\naverage = GlobalAveragePooling1D(name=\"Average\")(hidden_states)\r\n\r\n# Outputs\r\noutputs = Dense(n_tags,\r\n                activation=\"softmax\",\r\n                name=\"Output\")(average)\r\n\r\nmodel = Model(inputs, outputs, name=\"Sequence Chunking\")\r\n\r\n# Compile & Train\r\n# Compile\r\nmodel.compile(optimizer='rmsprop',\r\n              loss='categorical_crossentropy',\r\n              metrics=[\"accuracy\"])\r\n\r\n# run training\r\nmodel.fit(X_tr, np.array(y_tr),\r\n          batch_size=batch_size,\r\n          epochs=training_epoch,\r\n          validation_split=0.1)\r\n```", "comments": ["@TryArie \r\n\r\nLooks like code is incomplete.Which version of Tensorflow you are using?\r\nRequest you to share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Sorry, I just found another way to do it. So I close this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39528\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39528\">No</a>\n"]}, {"number": 39527, "title": "[tflite] Java binding for fp16 in NNAPI delegate", "body": "Add Java binding to use the newly added `allow_fp16` in NNAPI delegate.\r\nThis is a follow-up of https://github.com/tensorflow/tensorflow/pull/39444", "comments": ["tested with something like\r\n```diff\r\ndiff --git a/tensorflow/lite/java/demo/app/src/main/java/com/example/android/tflitecamerademo/ImageClassifier.java b/tensorflow/lite/java/demo/app/src/main/java/com/example/android/tflitecamerademo/ImageClassifier.java\r\nindex 2e483d8921..e44940dfac 100644\r\n--- a/tensorflow/lite/java/demo/app/src/main/java/com/example/android/tflitecamerademo/ImageClassifier.java\r\n+++ b/tensorflow/lite/java/demo/app/src/main/java/com/example/android/tflitecamerademo/ImageClassifier.java\r\n@@ -183,7 +183,9 @@ public abstract class ImageClassifier {\r\n   }\r\n \r\n   public void useNNAPI() {\r\n-    nnapiDelegate = new NnApiDelegate();\r\n+    NnApiDelegate.Options options = new NnApiDelegate.Options();\r\n+    options.setAllowFp16(true);\r\n+    nnapiDelegate = new NnApiDelegate(options);\r\n     tfliteOptions.addDelegate(nnapiDelegate);\r\n     recreateInterpreter();\r\n   }\r\n```", "Hi jdduke@, can you take a look, I'm not too sure about nnapi related work.", "@freedomtan can you please check this error, its blocking internal submission \r\n\r\n`/tensorflow/lite/java/src/test/java/org/tensorflow/lite/InterpreterTest.java:75: warning: [deprecation] setAllowFp16PrecisionForFp32(boolean) in Options has been deprecated\r\n                .setAllowFp16PrecisionForFp32(false)\r\n                ^/tensorflow/lite/java/src/test/java/org/tensorflow/lite/InterpreterTest.java:397: warning: [deprecation] setAllowFp16PrecisionForFp32(boolean) in Options has been deprecated\r\n            new Interpreter.Options().setUseNNAPI(true).setAllowFp16PrecisionForFp32(true));\r\n                                                       ^\r\nerror: warnings found and -Werror specified`", "@rthadur Thanks. Yes, I can reproduce the problem with `bazel build --config opt  tensorflow/lite/java:InterpreterTest`. Should I simply add ` @SuppressWarnings(\"deprecation\")` to ignore those two warnings or rewrite the two tests?", "@freedomtan I believe you can add `@SuppressWarnings(\"deprecation\")` , @jdduke is that fine ?", "That's fine, but now that you mention, we should add a sanity test in NnApiDelegateTest.java as well. Thanks.", "@jdduke and @rthadur  I suppressed warnings in `//tensorflow/lite/java:InterpreterTest` and added an `allowFp16` test to `//tensorflow/lite/java:NnApiDelegateTest`. Tested them on my desktop Linux machine. Dunno how to run tests on real android devices though."]}, {"number": 39525, "title": "Fix tests when `tf._major_api_version` does not exist", "body": "I think cherrypicking the `_major_api_version` change was not complete as there are tests where `._major_api_version` is not found. Thus tests fail", "comments": []}, {"number": 39524, "title": "Can't access resource variable using LookupResource in a custom op, probably because of binary incompatibility", "body": "**System information**\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX Catalina\r\nTensorFlow installed from (source or binary): binary\r\nTensorFlow version: 1.14.0\r\nPython version: 3.7\r\nInstalled using virtualenv? pip? conda?: conda\r\nGCC/Compiler version (for custom op): \r\n\r\nConfigured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX.sdk/usr/include/c++/4.2.1\r\nApple clang version 11.0.3 (clang-1103.0.32.59)\r\nTarget: x86_64-apple-darwin19.4.0\r\nThread model: posix\r\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\r\n\r\n**Describe the problem**\r\nI have a runtime error when i run zmq_ops which is a modified version of tensorpacks's zmq_ops\uff0cand source code is in the attach files\r\n[zmq_ops.tar.gz](https://github.com/tensorflow/tensorflow/files/4625555/zmq_ops.tar.gz)\r\n\r\nwhen i run `benchmark.py` script\uff0cit raise the following errors:\r\n2020-05-14 09:16:55.871310: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at zmq_pull_op.cc:61 : Invalid argument: Trying to access resource using the wrong type. Expected N10tensorpack13ZMQConnectionE got N10tensorpack13ZMQConnectionE\r\n2020-05-14 09:16:55.871310: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at zmq_pull_op.cc:61 : Invalid argument: Trying to access resource using the wrong type. Expected N10tensorpack13ZMQConnectionE got N10tensorpack13ZMQConnectionE\r\n2020-05-14 09:16:55.871357: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at zmq_pull_op.cc:61 : Invalid argument: Trying to access resource using the wrong type. Expected N10tensorpack13ZMQConnectionE got N10tensorpack13ZMQConnectionE\r\n2020-05-14 09:16:55.871373: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at zmq_pull_op.cc:61 : Invalid argument: Trying to access resource using the wrong type. Expected N10tensorpack13ZMQConnectionE got N10tensorpack13ZMQConnectionE\r\n2020-05-14 09:16:55.871321: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at zmq_pull_op.cc:61 : Invalid argument: Trying to access resource using the wrong type. Expected N10tensorpack13ZMQConnectionE got N10tensorpack13ZMQConnectionE\r\n2020-05-14 09:16:55.871403: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at zmq_pull_op.cc:61 : Invalid argument: Trying to access resource using the wrong type. Expected N10tensorpack13ZMQConnectionE got N10tensorpack13ZMQConnectionE\r\n2020-05-14 09:16:55.871421: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at zmq_pull_op.cc:61 : Invalid argument: Trying to access resource using the wrong type. Expected N10tensorpack13ZMQConnectionE got N10tensorpack13ZMQConnectionE\r\n2020-05-14 09:16:55.871377: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at zmq_pull_op.cc:61 : Invalid argument: Trying to access resource using the wrong type. Expected N10tensorpack13ZMQConnectionE got N10tensorpack13ZMQConnectionE\r\n\r\nissue #25113 have the same problem\uff0cbut I can't find any solutions", "comments": ["@vinowan \r\nCan you please share simple stand alone code so we could replicate the issue faced.", "the source code is in the attach file zmq_ops.tar.gz, you can install the custom op with command \r\n`pip install .` and run benchmark.py scripts in it ", "@vinowan \r\nIs it possible for you to create a  colab gist for the same and share so we could analyse the error faced.", "this error only occurs in mac OSX, colab gist may not reproduce this error ", "I've got this issue too. Is it relevant to `typeid`?", "https://github.com/tensorflow/tensorflow/issues/25113", "We've implemented a custom op using:\r\n```\r\ntemplate<typename ResourceType>\r\nStatus makeResourceHandle(OpKernelContext* context, int out_idx, ResourceType **resource_) {\r\n  static std::atomic<int64> id;\r\n  Tensor* handle_tensor;\r\n  TF_CHECK_STATUS(context->allocate_output(out_idx, TensorShape({}), &handle_tensor));\r\n\r\n  ResourceType *resource = new ResourceType();\r\n  const auto resource_name = typeid(ResourceType).name() + std::to_string(id++);\r\n  ResourceHandle handle = MakePerStepResourceHandle<ResourceType>(context, resource_name);\r\n  TF_CHECK_STATUS(CreateResource(context, handle, resource));\r\n  handle_tensor->scalar<ResourceHandle>()() = handle;\r\n\r\n  *resource_ = resource;\r\n  return Status::OK();\r\n}\r\n```\r\n\r\nIs there any chance that compiling tensorflow and our custom op on two machines (both are MacOs) would cause this issue?", "@vinowan @liyinhgqw a fix has been submitted at [1823f87](https://github.com/tensorflow/tensorflow/commit/1823f877359bb138c57a005c30aba8832dfa79fb#diff-991a6b786e16708ba1e6f5c9926cf151) and should be available in [2.3.0-rc0](https://github.com/tensorflow/tensorflow/releases/tag/v2.3.0-rc0)", "@vinowan Is this still an issue? Can you please verify once and close the issue if this was already resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39524\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39524\">No</a>\n"]}, {"number": 39523, "title": "Is there a way to convert a custom keras.utils.Sequence custom class to a tf.Data pipeline?", "body": "When I was building up my data pipeline, the Tensorflow docs were very insistent that generators are unsafe for multiprocessing, and that the best way to build up a multiprocessing streaming pipeline is to extend tensorflow.keras.utils.Sequence into your own custom class. This is written here: https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence\r\n\r\nSo I did that, but now Tensorflow is telling me that Sequence extensions are ALSO not ideal for multiprocessing through the warning message `multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.`. So now the recommendation is to use tf.Data. And, as it were, I keep running into deadlocks 4~ epochs into training now. \r\n\r\nIs there no converter between an existing sequence class and a tf.Data pipeline? It seems bizarre that the EXACT thing the Sequence extension class is recommended for seems to no longer work, and now only a brand new type of data pipeline will do the multiprocessing job. At the very least, this should be updated in the Sequence docs. ", "comments": ["Can you please explain more about your customs class. To understand the difference between keras.utils.Sequence  and a tf.Data pipeline you can take a look at the following [question](https://stackoverflow.com/questions/55852831/tf-data-vs-keras-utils-sequence-performance). ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing this issue as it has been inactive for 2 weeks. Please add additional comments for us to open this issue again. Thanks!", "I was facing exactly the same question. In particular when upgrading certain routines such as data generation from TF1 to TF2. The [mentioned statement in the documentation](https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence#notes) seems confusing. I am also facing deadlocks irregularly and in a non-reproducible manner. When using generators, derived from `keras.utils.Sequence` and passed e.g. to `tf.keras.Model.fit` (as `fit_generator` is deprecated), is it the same internally as using `tf.data.Dataset.from_generator`?\r\n\r\nIf so, would that be a recommended way to switch over to `tf.data` without having to rewrite custom data processing.  If not, what would be the recommended way? \r\n\r\nI too think the documentation is very confusing at that point...", "Hey @MaxSchambach, glad to hear that I'm not alone in this issue! Unfortunately, there doesn't seem to be a good way to move over to tf.Data without significant rewriting. I still do think this should be updated in the docs, so I'd like to reopen the issue.\r\n\r\nHowever, if it's any consolation, there ARE ways to get around deadlocking using the current Sequence framework. It's very hacky and customized to our problem, but it works. I didn't write that particular fix, but the general overview is that you need a pretty good understanding of the multiprocessing library. You need to use the multiprocessing Lock() functionality to continually .acquire() or .release() whatever your data is (Spark files in our case) to ensure that the underlying Tensorflow threads don't try to grab onto multiple files at the same time, all which calling the garbage collector to immediately collect any stray data and prevent memory leaks. I don't think I'd really be able to share a code snippet of how it works, purely due to how specific to our problem it is. \r\n\r\nPersonally, I'd just move to Pytorch if your data loading setup is complex enough to have to use stuff like Sequences or tf.Data. Tensorflow doesn't seem well designed for unusual training schemes. ", "Hi @Abhishaike , @MaxSchambach. Thank you for your points and suggestions regarding the issue with Sequence dataloader. I came across this issue trying to overcome the same situation. Using `tf.keras.utils.Sequence` type dataloader with `tf.keras.utils.OrderedEnqueuer` seems to work for me in tf 2.0 while using a custom training loop (not `tf.keras.Model.fit_generator` method) with multiprocessing. According to the answer in [this thread](https://stackoverflow.com/questions/59213040/is-the-order-of-batches-guaranteed-in-keras-orderedenqueuer), the OrderedEnqueuer seems to ensure the order of data loading with multiple workers without having to dig much into multiprocessing. The `tf.keras.Model.fit_generator` method in tf 2.0 uses OrderedEnqueuer (as far as i understood from the [source code](https://github.com/tensorflow/tensorflow/blob/v2.0.0/tensorflow/python/keras/engine/training_generator.py#L500-L522)) and there was no deadlock issue while training and playing around with number of `workers` and `max_queue_size` resulted in better performace. With `tf.keras.Model.fit` method in tf 2.0, i also run into deadlock. Seems like in tensorflow 2.2 this is fixed and works efficiently with `tf.keras.Model.fit` method. In all these cases, the dataloader was Sequence type and not `tf.data` type, as my data preprocessing involved some python specific dependencies. Still i guess it is better to use the `tf.data` pipeline if the data preprocessing can be written in tensorflow. ", "Training just stopped at the very first epoch with this message: \r\n\r\n```\r\nTensorFlow 2.4.1\r\n\r\nWARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing \r\nnondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\r\n```\r\n\r\nIt's really disappointing to get such an issue. Rewriting `tf.keras.utils.Sequence` to `tf.data.Dataset` is not a smooth transaction. The one which was the recommended way now is not safe to use! I wonder what we may see for `tf.data`! This issue must be opened and resolved. It's clear why people move to `PyTorch`, no offense but it makes totally sense. \r\n\r\n> Personally, I'd just move to Pytorch if your data loading setup is complex enough to have to use stuff like Sequences or tf.Data. Tensorflow doesn't seem well designed for unusual training schemes.", "I am currently facing the same issue. I read somewhere that `tf.keras.utils.Sequence` is the proper way to implement a custom data generator, but when I try using multiprocessing and workers, I get that warning and training does not start. It's been almost a year since this issue was opened. Has there been any progress ??", "It seems that TF Keras is sensitive to Sequence implementations not being thread-safe or process-safe. I've been having horrible problems migrating my data pipelines using generators/sequences to TF 2. But there are some observations and **possible bugs in TF Keras**. Assuming we use a Sequence with multiprocessing (and TF 2.3).\r\n\r\n- `Sequence.__getitem__` is not accessed only from subprocesses but also from the main process!\r\n  -`__getitem__(0)` is called from `tensorflow.python.keras.engine.data_adapter.KerasSequenceAdapter._peek_and_restore()`: `return x[0], x`\r\n  - it peeks at the first batch to sniff the shape\r\n  - it's called prior forking/spawning the subprocess\r\n- `Sequence.on_epoch_end()` is also called from the main process\r\n\r\nThus even if we lazily initialize some thread-unsafe state within the Sequence instance it gets initialized in the main process and then copied to the subprocess! Also modifying the state in the hook modifies only the instance in the main process not in the subprocess.\r\n\r\nI tried to use the basic [Keras MNIST convnet example](https://keras.io/examples/vision/mnist_convnet/), wrapped the arrays with a Sequence and trace the process ids: https://gist.github.com/bzamecnik/dcc1d1a39f3e4fa7ac5733d80b79fa2d (code + logs)\r\n\r\n**In general Keras supports Sequences and multiprocessing** (at least in TF 2.3) but if there's anything thread-unsafe in the Sequence it fails.\r\n", "I have a hack\r\n(I'm very not sure that this is the perfect decision, but it works and works without a lot of changes)\r\n\r\nFor example, you have a class inherited from keras.utils.Sequence\r\n\r\n```\r\nclass DataGenerator(keras.utils.Sequence):\r\n    def __init__(self):\r\n        #Some initialization\r\n    \r\n    def __len__(self):\r\n        return number_of_steps_in_epoche   #number of batches\r\n\r\n    def __getitem__(self, index):\r\n        data = load(index)\r\n        X, y = data[\"X\"], data[\"y\"]\r\n        return X, y\r\n```\r\n\r\nand code that uses it, for example:\r\n\r\n```\r\ndata_generator = DataGenerator()\r\n\r\nmodel.fit(x=data_generator)\r\n```\r\n \r\n\r\n---------------A hack---------------\r\nAs we can see method ```__getitem__``` is private. Let's make its public copy and an attribute for the length in ```__init__``` method. All would look like that:\r\n\r\n```\r\nclass DataGenerator(keras.utils.Sequence):\r\n    def __init__(self):\r\n        #Some initialization\r\n        self.len = self.__len__()    #an attribute for the length\r\n    \r\n    def __len__(self):\r\n        return number_of_steps_in_epoche\r\n\r\n    def __getitem__(self, index):\r\n        data = load(index)\r\n        X, y = data['X'], data['y']\r\n        return X, y\r\n\r\n    def getitem(self, index):\r\n        return self.__getitem__(index)\r\n```\r\n\r\nAnd now the most interesting part: \r\n1. we create a simple generator inside which we call public ```getitem``` method of ```keras.utils.Sequence instance```\r\n2. from this simple generator we create ```tf.data.Dataset``` object\r\n\r\nIt would look like this:\r\n\r\n```\r\ndata_generator = DataGenerator()\r\n\r\ndef gen_data_generator():\r\n    for i in range(data_generator.len):\r\n        yield data_generator.getitem(i)    #edited regaring to @Inigo-13 comment\r\n\r\ndata_dataset =  tf.data.Dataset.from_generator(gen_data_generator, output_signature=(\r\n        tf.TensorSpec(shape=(None, your_shapes), dtype=tf.float32),\r\n        tf.TensorSpec(shape=(None, ), dtype=tf.float32)))  #according to tf.data.Dataset.from_generator documentation we have to specify output_signature\r\n\r\nmodel.fit(x=data_dataset)\r\n```\r\n\r\nAnd one more advantage: it's easy to return to the previous implementation.", "> I have a hack\r\n> (I'm very not sure that this is the perfect decision, but it works and works without a lot of changes)\r\n> \r\n> For example, you have a class inherited from keras.utils.Sequence\r\n> \r\n> ```\r\n> class DataGenerator(keras.utils.Sequence):\r\n>     def __init__(self):\r\n>         #Some initialization\r\n>     \r\n>     def __len__(self):\r\n>         return number_of_steps_in_epoche   #number of batches\r\n> \r\n>     def __getitem__(self, index):\r\n>         data = load(index)\r\n>         X, y = data[\"X\"], data[\"y\"]\r\n>         return X, y\r\n> ```\r\n> \r\n> and code that uses it, for example:\r\n> \r\n> ```\r\n> data_generator = DataGenerator()\r\n> \r\n> model.fit(x=data_generator)\r\n> ```\r\n> \r\n> ---------------A hack---------------\r\n> As we can see method `__getitem__` is private. Let's make its public copy and an attribute for the length in `__init__` method. All would look like that:\r\n> \r\n> ```\r\n> class DataGenerator(keras.utils.Sequence):\r\n>     def __init__(self):\r\n>         #Some initialization\r\n>         self.len = self.__len__()    #an attribute for the length\r\n>     \r\n>     def __len__(self):\r\n>         return number_of_steps_in_epoche\r\n> \r\n>     def __getitem__(self, index):\r\n>         data = load(index)\r\n>         X, y = data['X'], data['y']\r\n>         return X, y\r\n> \r\n>     def getitem(self, index):\r\n>         return self.__getitem__(index)\r\n> ```\r\n> \r\n> And now the most interesting part:\r\n> \r\n> 1. we create a simple generator inside which we call public `getitem` method of `keras.utils.Sequence instance`\r\n> 2. from this simple generator we create `tf.data.Dataset` object\r\n> \r\n> It would look like this:\r\n> \r\n> ```\r\n> data_generator = DataGenerator()\r\n> \r\n> def gen_data_generator():\r\n>     for i in range(data_generator.len):\r\n>         yield data_generator.getitem()\r\n> \r\n> data_dataset =  tf.data.Dataset.from_generator(gen_data_generator, output_signature=(\r\n>         tf.TensorSpec(shape=(None, your_shapes), dtype=tf.float32),\r\n>         tf.TensorSpec(shape=(None, ), dtype=tf.float32)))  #according to tf.data.Dataset.from_generator documentation we have to specify output_signature\r\n> \r\n> model.fit(x=data_dataset)\r\n> ```\r\n> \r\n> And one more advantage: it's easy to return to the previous implementation.\r\n\r\nHi!\r\nA question related to the solution proposed here...what about the \"index\" parameter when calling the custom method \r\n\"getitem\" from the Keras Sequence?", "> Hi!\r\n> A question related to the solution proposed here...what about the \"index\" parameter when calling the custom method\r\n> \"getitem\" from the Keras Sequence?\r\n\r\nYou are right, thank you, I forgot to copy indexing!\r\nIt would look like this:\r\n\r\n```\r\ndef gen_data_generator():\r\n    for i in range(data_generator.len):\r\n        yield data_generator.getitem(i)\r\n```\r\n\r\nWas:  ```yield data_generator.getitem()```\r\nHas to be: ```yield data_generator.getitem(i)```\r\n\r\nI've also edited my first answer."]}]