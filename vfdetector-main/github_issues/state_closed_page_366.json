[{"number": 43074, "title": " tf.data.experimental.make_csv_dataset seems to fail on this dataset; https://archive.ics.uci.edu/ml/datasets/Parkinsons+Telemonitoring", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Used sample code using api given in tensorflow 2.0 (as opposed to using a stock example script provided in TensorFlow):\r\n- colab:\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow present in the colab, version 2.3.0:\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nThe data set was downloaded from https://archive.ics.uci.edu/ml/datasets/Parkinsons+Telemonitoring and the following was used to read the dataset, that resulted in an error:\r\nSample data:\r\nsubject#,age,sex,test_time,motor_UPDRS,total_UPDRS,Jitter(%),Jitter(Abs),Jitter:RAP,Jitter:PPQ5,Jitter:DDP,Shimmer,Shimmer(dB),Shimmer:APQ3,Shimmer:APQ5,Shimmer:APQ11,Shimmer:DDA,NHR,HNR,RPDE,DFA,PPE\r\n1,72,0,5.6431,28.199,34.398,0.00662,3.38e-005,0.00401,0.00317,0.01204,0.02565,0.23,0.01438,0.01309,0.01662,0.04314,0.01429,21.64,0.41888,0.54842,0.16006\r\n1,72,0,12.666,28.447,34.894,0.003,1.68e-005,0.00132,0.0015,0.00395,0.02024,0.179,0.00994,0.01072,0.01689,0.02982,0.011112,27.183,0.43493,0.56477,0.1081\r\n1,72,0,19.681,28.695,35.389,0.00481,2.462e-005,0.00205,0.00208,0.00616,0.01675,0.181,0.00734,0.00844,0.01458,0.02202,0.02022,23.047,0.46222,0.54405,0.21014\r\n1,72,0,25.647,28.905,35.81,0.00528,2.657e-005,0.00191,0.00264,0.00573,0.02309,0.327,0.01106,0.01265,0.01963,0.03317,0.027837,24.445,0.4873,0.57794,0.33277\r\n1,72,0,33.642,29.187,36.375,0.00335,2.014e-005,0.00093,0.0013,0.00278,0.01703,0.176,0.00679,0.00929,0.01819,0.02036,0.011625,26.126,0.47188,0.56122,0.19361\r\n1,72,0,40.652,29.435,36.87,0.00353,2.29e-005,0.00119,0.00159,0.00357,0.02227,0.214,0.01006,0.01337,0.02263,0.03019,0.009438,22.946,0.53949,0.57243,0.195\r\n1,72,0,47.649,29.682,37.363,0.00422,2.404e-005,0.00212,0.00221,0.00637,0.04352,0.445,0.02376,0.02621,0.03488,0.07128,0.01326,22.506,0.4925,0.54779,0.17563\r\n1,72,0,54.64,29.928,37.857,0.00476,2.471e-005,0.00226,0.00259,0.00678,0.02191,0.212,0.00979,0.01462,0.01911,0.02937,0.027969,22.929,0.47712,0.54234,0.23844\r\n1,72,0,61.669,30.177,38.353,0.00432,2.854e-005,0.00156,0.00207,0.00468,0.04296,0.371,0.01774,0.02134,0.03451,0.05323,0.013381,22.078,0.51563,0.61864,0.20037\r\n\r\nThe date was sucessfuly read into data set using:\r\ntypes=[tf.int32,tf.int32,tf.int32,tf.float64,tf.float64,tf.float64,tf.float64,tf.float64,tf.float64,tf.float64,tf.float64,tf.float64,tf.float64,tf.float64,tf.float64,tf.float64,tf.float64,tf.float64,tf.float64,tf.float64,tf.float64,tf.float64]\r\nadataset = tf.data.experimental.CsvDataset(data, types, header=True)\r\n\r\nHowever, when attempted to read from the following, it generates error:\r\ncolumn_names=['subject#','age','sex','test_time','motor_UPDRS','total_UPDRS','Jitter(%)','Jitter(Abs)',\r\n              'Jitter:RAP','Jitter:PPQ5','Jitter:DDP','Shimmer','Shimmer(dB)','Shimmer:APQ3','Shimmer:APQ5',\r\n              'Shimmer:APQ11','Shimmer:DDA','NHR','HNR','RPDE','DFA','PPE']\r\nlabels=column_names.pop(4)\r\nlabels=[labels,column_names.pop(4)]\r\nprint(labels)\r\nprint(column_names)\r\nbatch_size = 32\r\n\r\ntrain_dataset = tf.data.experimental.make_csv_dataset(\r\n    data,\r\n    batch_size,\r\n    column_names=column_names,\r\n    label_name=labels,\r\n    num_epochs=1)\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n\r\nGenerates error:\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-12-a43b43d3b38b> in <module>()\r\n      6     column_names=column_names,\r\n      7     label_name=labels,\r\n----> 8     num_epochs=1)\r\n\r\n2 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/experimental/ops/readers.py in _next_csv_row(filenames, num_cols, field_delim, use_quote_delim, header, file_io_fn)\r\n    123         if len(csv_row) != num_cols:\r\n    124           raise ValueError(\r\n--> 125               \"Problem inferring types: CSV row has different number of fields \"\r\n    126               \"than expected.\")\r\n    127         yield csv_row\r\n\r\nValueError: Problem inferring types: CSV row has different number of fields than expected.\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1JM1iQk5ZXWo9O9JwKpab58aQyYYERty0?usp=sharing\r\n\r\n", "comments": ["Please unlock you colab example cause it requires authentication.", "@harica \r\nError reported does not seem like a bug or feature request for tensorflow, which can be seen only after you share complete stand alone code for us to analyse.\r\nAs per the error reported:\r\nYou may refer to [this link](https://stackoverflow.com/questions/59843548/python-valueerror-csv-row-has-different-number-of-fields-with-csv-and-tensorf) and let us know if it helps.", "Thank you very much for your fast response to my problem filed as bug. I do not believe, your stack flow referenced item is applicable in this case.  Please let me know, if you can replicate my problem, using the attached file.\n\nIn my example, I have shown, that one of the tensorflow API's can read the csv files as below:\ntypes=[tf.int32,tf.int32,tf.int32,tf.float64,tf.float64,tf.float64,tf.float64,tf.float64,tf.float64,tf.float64,tf.float64,tf.float64,tf.float64,tf.float64,tf.float64,tf.float64,tf.float64,tf.float64,tf.float64,tf.float64,tf.float64,tf.float64]\nadataset = tf.data.experimental.CsvDataset(data, types, header=True)\n\ntherefore, there is no issue with files.\n\nHowever, When I use the  following:\ncolumn_names=['subject#','age','sex','test_time','motor_UPDRS','total_UPDRS','Jitter(%)','Jitter(Abs)',\n              'Jitter:RAP','Jitter:PPQ5','Jitter:DDP','Shimmer','Shimmer(dB)','Shimmer:APQ3','Shimmer:APQ5',\n              'Shimmer:APQ11','Shimmer:DDA','NHR','HNR','RPDE','DFA','PPE']\nlabels=column_names.pop(4)\nlabels=[labels,column_names.pop(4)]\n\n\nbatch_size = 32\n\ntrain_dataset = tf.data.experimental.make_csv_dataset(\n    data,\n    batch_size,\n    column_names=column_names,\n    label_name=labels,\n    num_epochs=1)\n\nThe API call generates the error as below:\n\n---------------------------------------------------------------------------\n\n\nValueError                                Traceback (most recent call last)\n\n\n<ipython-input-12-a43b43d3b38b> in <module>()\n      6     column_names=column_names,\n      7     label_name=labels,\n----> 8     num_epochs=1)\n\n\n\n________________________________\n2 frames\n________________________________\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/experimental/ops/readers.py in _next_csv_row(filenames, num_cols, field_delim, use_quote_delim, header, file_io_fn)\n    123         if len(csv_row) != num_cols:\n    124           raise ValueError(\n--> 125               \"Problem inferring types: CSV row has different number of fields \"\n    126               \"than expected.\")\n    127         yield csv_row\n\n\n\nValueError: Problem inferring types: CSV row has different number of fields than expected.\n\n\n\nI believe, the problem is arising from the other API is due to labels containing two elements, instead of one (That is my guess). When this api is used to generate data set from UCI dataset, the API generates, the error.\n\nTherefore, either this is a bug, when attempting to create a two label (output) dataset or a feature request to be able to have two labels as outputs.\n\nAs I have shared the notebook link, Please let me know, where is the error or what I am doing wrong ....\n\nI am also attaching the datafile for your replication of the problem I am seeing.\n\nThank you very much for your attention towards this matter....\n\nHari Kunamneni\n(408) 910-5384\nhkunamneni@hotmail.com<mailto:hkunamneni@hotmail.com>\nThis electronic message transmission contains information from Hari P. Kunamneni, Patent Agent,  that may be confidential or privileged. The information in this message is only for the use of the intended recipient. If you are not the intended recipient, be aware that any disclosure, copying, distribution or other use of the contents of this electronic message is strictly prohibited. If you have received this electronic transmission in error, please notify the sender immediately.\n\n________________________________\nFrom: Saduf2019 <notifications@github.com>\nSent: Wednesday, September 9, 2020 6:54 AM\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: harica <hkunamneni@hotmail.com>; Mention <mention@noreply.github.com>\nSubject: Re: [tensorflow/tensorflow] tf.data.experimental.make_csv_dataset seems to fail on this dataset; https://archive.ics.uci.edu/ml/datasets/Parkinsons+Telemonitoring (#43074)\n\n\n@harica<https://github.com/harica>\nError reported does not seem like a bug or feature request for tensorflow, which can be seen only after you share complete stand alone code for us to analyse.\nAs per the error reported:\nYou may refer to this link<https://stackoverflow.com/questions/59843548/python-valueerror-csv-row-has-different-number-of-fields-with-csv-and-tensorf> and let us know if it helps.\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/43074#issuecomment-689578319>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AAGK6QYEG5UEVRGBYMJ37ELSE6CHRANCNFSM4RCC3DCA>.\n", "Can you let me know how to unlock the notebook?\nSorry, I do not know how to do it...\n\n\nHari Kunamneni\n(408) 910-5384\nhkunamneni@hotmail.com<mailto:hkunamneni@hotmail.com>\nThis electronic message transmission contains information from Hari P. Kunamneni, Patent Agent,  that may be confidential or privileged. The information in this message is only for the use of the intended recipient. If you are not the intended recipient, be aware that any disclosure, copying, distribution or other use of the contents of this electronic message is strictly prohibited. If you have received this electronic transmission in error, please notify the sender immediately.\n\n________________________________\nFrom: bhack <notifications@github.com>\nSent: Wednesday, September 9, 2020 6:01 AM\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: harica <hkunamneni@hotmail.com>; Author <author@noreply.github.com>\nSubject: Re: [tensorflow/tensorflow] tf.data.experimental.make_csv_dataset seems to fail on this dataset; https://archive.ics.uci.edu/ml/datasets/Parkinsons+Telemonitoring (#43074)\n\n\nPlease unlock you colab example cause it requires authentication.\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/43074#issuecomment-689547159>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AAGK6Q5KF6VLYOJ5Y6Z6DJ3SE54CBANCNFSM4RCC3DCA>.\n", "Okay, I have updated the link, so that any one with access to link can access the notebook.\n\n...\n\n\n\nHari Kunamneni\n(408) 910-5384\nhkunamneni@hotmail.com<mailto:hkunamneni@hotmail.com>\nThis electronic message transmission contains information from Hari P. Kunamneni, Patent Agent,  that may be confidential or privileged. The information in this message is only for the use of the intended recipient. If you are not the intended recipient, be aware that any disclosure, copying, distribution or other use of the contents of this electronic message is strictly prohibited. If you have received this electronic transmission in error, please notify the sender immediately.\n\n________________________________\nFrom: bhack <notifications@github.com>\nSent: Wednesday, September 9, 2020 6:01 AM\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: harica <hkunamneni@hotmail.com>; Author <author@noreply.github.com>\nSubject: Re: [tensorflow/tensorflow] tf.data.experimental.make_csv_dataset seems to fail on this dataset; https://archive.ics.uci.edu/ml/datasets/Parkinsons+Telemonitoring (#43074)\n\n\nPlease unlock you colab example cause it requires authentication.\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/43074#issuecomment-689547159>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AAGK6Q5KF6VLYOJ5Y6Z6DJ3SE54CBANCNFSM4RCC3DCA>.\n", "To be concise, currently the api used does not support more than one label *multiple outputs) for the dataset as it takes only one string. If I am wrong, please let me know. Below is the API, description:\r\n\r\nlabel_name | A optional string corresponding to the label column. If provided, the data for this column is returned as a separate\u00a0Tensor\u00a0from the features dictionary, so that the dataset complies with the format expected by a\u00a0tf.Estimator.train\u00a0or\u00a0tf.Estimator.evaluate\u00a0input function.\r\n-- | --\r\n\r\n\r\n", "By the way, label when provided as string, the API, does create the dataset.", "> ValueError: Problem inferring types: CSV row has different number of fields than expected.\r\n\r\n@harica I don't think it is a bug you problem is that you have removed `column_names` elements with `pop` so it is mismatching csv row elements.\r\n```\r\ntrain_dataset = tf.data.experimental.make_csv_dataset(\r\n    data,\r\n    batch_size,\r\n    column_names=column_names,\r\n    field_delim=',',\r\n    label_name=labels[0],\r\n    num_epochs=1)\r\n```", "Okay, I agree, it is not a bug as the API defined right now.\nHowever, the API, lacks capability to have multiple labels(multiple outputs) for the network. Therefore, the API may need to be enhanced to provide a way to pass more than one label column ...\n\n\n\nHari Kunamneni\n(408) 910-5384\nhkunamneni@hotmail.com<mailto:hkunamneni@hotmail.com>\nThis electronic message transmission contains information from Hari P. Kunamneni, Patent Agent,  that may be confidential or privileged. The information in this message is only for the use of the intended recipient. If you are not the intended recipient, be aware that any disclosure, copying, distribution or other use of the contents of this electronic message is strictly prohibited. If you have received this electronic transmission in error, please notify the sender immediately.\n\n________________________________\nFrom: bhack <notifications@github.com>\nSent: Wednesday, September 9, 2020 9:00 AM\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: harica <hkunamneni@hotmail.com>; Mention <mention@noreply.github.com>\nSubject: Re: [tensorflow/tensorflow] tf.data.experimental.make_csv_dataset seems to fail on this dataset; https://archive.ics.uci.edu/ml/datasets/Parkinsons+Telemonitoring (#43074)\n\n\nValueError: Problem inferring types: CSV row has different number of fields than expected.\n\n@harica<https://github.com/harica> I don't think it is a bug you problem is that you have removed column_names elements with pop so it is mismatching csv row elements.\n\ntrain_dataset = tf.data.experimental.make_csv_dataset(\n    data,\n    batch_size,\n    column_names=column_names,\n    field_delim=',',\n    label_name=labels[0],\n    num_epochs=1)\n\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/43074#issuecomment-689657899>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AAGK6Q5SMOOILQMOHE43CWLSE6RCNANCNFSM4RCC3DCA>.\n", "@harica I think that you can close this and open a new feature request issue just for that proposal API change.", "As per request, this issue is closed. And feature enhancement request has been opened: #43083", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43074\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43074\">No</a>\n"]}, {"number": 43072, "title": "Duplicate code in `tf.python.keras.engine.training.Model.evaluate()`, `train_on_batch()` and `test_on_batch()`", "body": "There is some duplicate code in methods ``tf.python.keras.engine.training.Model.evaluate()``, ``train_on_batch()`` and ``test_on_batch()``. I am not sure to have time to make a clean pull request so I report the duplicate and a solution here and someone may be able to fix it sooner.\r\n\r\nThe duplicate code is at the end of each method:\r\n```python\r\n    logs = tf_utils.to_numpy_or_python_type(logs)\r\n    if return_dict:\r\n      return logs\r\n    else:\r\n      results = [logs.get(name, None) for name in self.metrics_names]\r\n      if len(results) == 1:\r\n        return results[0]\r\n      return results\r\n```\r\n\r\nI propose to factorize this code into a new private method ``_format_logs_results(self, logs, return_dict)``. Very easy fix, it seems, but not useless. It would allow to operate on the logs just before the return statement all in one.", "comments": ["If this is not done already can I work on it? Thanks.", "The code has been moved from `tf.python.keras.engine.training.Model.evaluate()` to [tf.keras.Model.evaluate()](https://www.tensorflow.org/api_docs/python/tf/keras/Model?version=nightly#evaluate), and I can see the changes in code as well in the file [here](https://github.com/keras-team/keras/blob/master/keras/engine/training.py#L1567-L1733). \r\nLet us know if you still need any changes in the above linked code. Thanks!\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "It looks okay to me. There are a few differences with the original code but I suppose there have been other updates that I haven't followed. Good job. I'm closing the issue.", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 43071, "title": "TFLite model gives random outputs when running on Android", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, Android\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung Galaxy S10E\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.7.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: \r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nOn desktop, my saved_model works perfectly fine. I converted it to TFLite format, tested it using Python API, and it also worked perfectly fine. However, when I deployed it to an Android phone, it would give very bizarre and often very random outputs. To be more specific, the output is supposed to be a 1x256x256x1 tensor with each value normally falling in the range of 0.05 ~ 1. However, on android, the values often drop below 1e-27 or reach above 1e25, seemingly at random. Sometimes the first run would give the correct values, but the subsequent runs would output smaller and smaller values. \r\n\r\n**Describe the expected behavior**\r\nThe TFLite model running on android should return outputs with values similar to that when running on desktop.\r\n\r\n**Standalone code to reproduce the issue**\r\nAndroid code\r\n```\r\n    private var options = Interpreter.Options()\r\n    private val modelFile = File(assetFilePath(context, segmentationModulePath))\r\n    private var interpreter: Interpreter\r\n    private var imageProcessor: ImageProcessor\r\n\r\n    init {\r\n        options.addDelegate(GpuDelegate())\r\n        options.setNumThreads(4)\r\n        interpreter = Interpreter(modelFile, options)\r\n        imageProcessor = ImageProcessor.Builder()\r\n            .add(ResizeOp(IMAGE_WIDTH, IMAGE_HEIGHT, ResizeOp.ResizeMethod.BILINEAR))\r\n            .add(NormalizeOp(IMAGE_MEAN, IMAGE_STD))\r\n            .build()\r\n    }\r\n\r\n    fun loadImage(bitmap: Bitmap): TensorImage {\r\n        val image = TensorImage(DataType.FLOAT32)\r\n        image.load(bitmap)\r\n        return imageProcessor.process(image)\r\n    }\r\n// The main executing function\r\n    fun infer(bmp: Bitmap): Bitmap {\r\n        val input = loadImage(bmp)\r\n        val output = TensorBuffer.createFixedSize(intArrayOf(1, 256, 256, 1), DataType.FLOAT32)\r\n        val start = System.nanoTime()\r\n        interpreter.run(input.buffer, output.buffer)\r\n// Omitted \r\n    }\r\n    companion object {\r\n        private const val IMAGE_WIDTH = 256\r\n        private const val IMAGE_HEIGHT = 256\r\n        private const val IMAGE_MEAN = 127.5f\r\n        private const val IMAGE_STD = 127.5f\r\n    }\r\n```\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\nThe model is attached.\r\n[shm.zip](https://github.com/tensorflow/tensorflow/files/5194486/shm.zip)\r\n", "comments": ["Not sure how you could invoke \"input.buffer\", since TensorImage does not have the field \"buffer\". Please try using getBuffer() instead. You image is passed in as a Bitmap, calling getBuffer() will convert the Bitmap instance into a TensorBuffer instance, and then return the ByteBuffer representation of the underlying image.", "@lu-wang-g The code is in Kotlin, `input.buffer` is equivalent to `input.getBuffer()` in Java\r\n", "Could you please verify if input.buffer changes when invoking loadImage(bmp) multiple times? ", "Sorry for the late reply, input.buffer remains the same when invoked multiple times\r\n\r\n", "No worries. Just want to confirm the issue, do you mean that when invoking the following code for multiple times: \r\n   interpreter.run(input.buffer, output.buffer),\r\noutput.buffer may vary even if feeding the same input.buffer?", "Yes", "OK, one more question, do you use multithreading in your code, not the multithreading of the interpreter through setNumThreads() (that is fine), but the multithreading of your entire inference? Could it be possible that input.buffer is changed during interpreter.run()? Note that the interpreter API is not thread safe.", "No, I use no multithreading in my code", "I'll need to dig into the issue and see what happened exactly. Your model's input and output look very similar as our [style transfer models](https://www.tensorflow.org/lite/models/style_transfer/overview#performance_benchmarks). Could you please verify if the issue exist with our style transfer models? And then I can use that model to reproduce the issue.", "@ryanaleksander  It looks like you are using an older Version of Tensorflow. Many bugs have been fixed in the latest version. Could you please execute your code using Latest  stable Version of TF 2.6 and let us know if the issue still persists? Please refer to the [link ](https://www.tensorflow.org/lite/examples/style_transfer/overview#performance_benchmarks) and let us know if it helps? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43071\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43071\">No</a>\n"]}, {"number": 43070, "title": "Don't try to link JSON headers to fix JSON in TF_SYSTEM_LIBS", "body": "This is not required and now avoids conflicting settings of INCLUDEPATH when protobuf and JSONCpp is installed into different prefixes\r\n\r\nWe have used this patch successfully in compiling TF 2.0, 2.1 and 2.2 on our system, so I'd say it works.\r\n\r\nCCing @cbalint13 and @perfinion to maybe verify on their builds.", "comments": ["Woops, already done in #42516"]}, {"number": 43069, "title": "[hlo] Unbreak hlo-legalize-to-lhlo test", "body": "The legalization of mlhlo.ReturnOp to lhlo.TerminatorOp by using BufferAssignmentReturnOpConverter fails since the Memref typed results (or the Memref typed operands of Return operation) are set to stay as results after legalization but lhlo.TerminatorOp doesn't accept any operands. Therefore, BufferAssignmentReturnOpConverter must be replaced with a manual conversion that removes all operands of mlhlo.ReturnOp and inserts copy operations in their places.", "comments": ["Please resolve conflicts with the recent LLVM integrate."]}, {"number": 43068, "title": "Model Predict problem when used one data at a time", "body": "**System information**\r\n- Have I written custom code **Yes**\r\n- OS Platform and Distribution : **docker tensorflow/tensorflow:2.2.0-gpu**\r\n- TensorFlow version (use command below): **2.2.0**\r\n- Python version: **3.6.9**\r\n- CUDA/cuDNN version: **V10.1.243**\r\n- GPU model and memory: **TITAN X (Pascal) 12Go**\r\n\r\n**Describe the current behavior**\r\n\r\nI am currently trying to learn the sum of 2 digits thanks to Keras.\r\nMy input is an array of 2 ints and output is one value.\r\n\r\nThe problem is in the predict function :\r\nWhen using << model.predict >> with a batch, it works perfectly !\r\nHowever, when trying to predict one data at a time, << model.predict >> doesn't work as it should and return a warning :\r\n\r\n> WARNING:tensorflow:Model was constructed with shape (None, 2) for input Tensor(\"input_1:0\", shape=(None, 2), dtype=float32), but it was called on an input with incompatible shape (None, 1).\r\n\r\n**Describe the expected behavior**\r\n\r\nUsing the exact same code in docker tensorflow/tensorflow:1.9.0-py3, both method (batch or one data at a time) gives the same results.\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1kkXVQAKHlztpqjn1XaBPNdbwJz21JOWa?usp=sharing", "comments": ["I have tried in colab with TF versions 2.3, nightly versions(`2.4.0-dev20200908`) and was able to reproduce the issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/930e5b200a6aa3939fdad7b1eef7b928/untitled326.ipynb). Thanks!", "@aumombelli Can you try:\r\n```\r\n#anormal\r\nfor i in range(5):\r\n    print(\"Index: {0} X_data: {1} - Predict on X_data: {2}\".format(i, x_data[i], model.predict([[x_data[i]]])))\r\n```", "> @aumombelli Can you try:\r\n> \r\n> ```\r\n> #anormal\r\n> for i in range(5):\r\n>     print(\"Index: {0} X_data: {1} - Predict on X_data: {2}\".format(i, x_data[i], model.predict([[x_data[i]]])))\r\n> ```\r\n\r\nHere is the result :\r\n\r\n> WARNING:tensorflow:Model was constructed with shape (None, 2) for input Tensor(\"input_1:0\", shape=(None, 2), dtype=float32), but it was called on an input with incompatible shape (None, 1).\r\n> Index: 0 X_data: [2 2] - Predict on X_data: [[2.0177724]\r\n>  [2.0177724]]\r\n> Index: 1 X_data: [0 5] - Predict on X_data: [[0.01705379]\r\n>  [5.016993  ]]\r\n> Index: 2 X_data: [3 8] - Predict on X_data: [[3.0168335]\r\n>  [8.016044 ]]\r\n> Index: 3 X_data: [0 1] - Predict on X_data: [[0.01705379]\r\n>  [1.0175974 ]]\r\n> Index: 4 X_data: [0 5] - Predict on X_data: [[0.01705379]\r\n>  [5.016993  ]]\r\n\r\nThe notebook has been modified accordingly.\r\n", "I've tested in @ravikyram Colab. You need to try with `pip install tensorflow-gpu==2.3.0`", "> I've tested in @ravikyram Colab. You need to try with `pip install tensorflow-gpu==2.3.0`\r\n\r\nI added `!pip install tensorflow-gpu==2.3.0` to my notebook but I get the same result as earlier.", "Yes sorry it was that on the second run of that cell the warning disappeared.\r\nOn the single predict your shape is (2,) so you could test with:\r\n\r\n```\r\n#anormal\r\nfor i in range(5):\r\n    x= tf.expand_dims(x_data[i], axis=0)\r\n    print(x.shape)\r\n    print(\"Index: {0} X_data: {1} - Predict on X_data: {2}\".format(i, x, model.predict(x)))\r\n```", "It worked as expected ! Thanks @bhack \r\nIf I understand correctly it seems that numpy array does not transform correctly into a tf.Tensor, is that right ?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43068\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43068\">No</a>\n", "@aumombelli Using indexing It is like:\r\n```\r\n#anormal\r\nfor i in range(5):\r\n    x = x_data[None,i,:]\r\n    x_pred = model(x, training=False)\r\n    print(\"Index: {0} x: {1} - Predict on y: {2}\".format(i, x, x_pred))\r\n    y = tf.expand_dims(x_data[i],axis=0)\r\n    y_pred = model(x,training=False)\r\n    print(\"Index: {0} x: {1} - Predict on y: {2}\".format(i, y, y_pred))\r\n    assert x_pred == y_pred\r\n```", "Ok I understand ! Thanks a lot !"]}, {"number": 43067, "title": "TFLu: Don't construct PoolParams if not needed in CMSIS int8 kernel", "body": "`PoolParams` are not needed for optimized CMSIS int8 average pool kernels, so this PR moves the creation inside the if statement.\r\n\r\n/cc @freddan80", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 43066, "title": "TFLM: update for supporting new ARC MLI backend", "body": "This pull request updates support new ARC MLI backend on Himax WE1 EVB", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43066) for more info**.\n\n<!-- need_author_consent -->", "@googlebot I consent.", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43066) for more info**.\n\n<!-- ok -->"]}, {"number": 43065, "title": "No gradients provided for any variable", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.7.7\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nThe problem is in the learn function, and more specifically the actor network's update with tape gradient.\r\n\r\nI am trying to calculate the gradients with respect to the model's trainable variables. I use tf.reduce_mean() to calculate loss that i am going to use. I have tried to define a @tf.function to calculate the reduced mean, but that doesn't help either. \r\nThere are trainable variables, the loss value is not zero. The buffer and network files are not included here, i know, but maybe there could be a fix found with just looking at the code.\r\n\r\nWhat's interesting to me is that before the problem, i also calculate gradients for the other networks without a problem.\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\n\"\"\" Twin delayed deep deterministic policy gradient agent: \"\"\"\r\nimport tensorflow as tf\r\nimport matplotlib.pyplot as plt\r\nimport os\r\nimport datetime\r\nimport numpy as np\r\nfrom tensorflow.keras.optimizers import Adam\r\nfrom Networks import ActorNetwork, CriticNetwork\r\nfrom Buffer import Buffer\r\n\r\n\r\nclass Agent:\r\n    def __init__(self,\r\n                 lr_actor,\r\n                 lr_critic,\r\n                 num_actions,\r\n                 num_states,\r\n                 gamma,\r\n                 tau,\r\n                 batch_size,\r\n                 b_size,\r\n                 fc1_actor,\r\n                 fc2_actor,\r\n                 fc1_critic,\r\n                 fc2_critic\r\n                 ):\r\n        # Saving variables:\r\n        self.num_actions = num_actions\r\n        self.num_states = num_states\r\n        self.gamma = gamma\r\n        self.tau = tau\r\n        self.batch_size = batch_size\r\n        self.buffer = Buffer(buffer_size=b_size, batch_size=batch_size, num_states=num_states, num_action=num_actions)\r\n\r\n        # Networks:\r\n        self.actor = ActorNetwork(n_actions=num_actions, fc1_dims=fc1_actor, fc2_dims=fc2_actor, name='Actor')\r\n        self.target_actor = ActorNetwork(n_actions=num_actions, fc1_dims=fc1_actor, fc2_dims=fc2_actor,\r\n                                         name='ActorTarget')\r\n        self.critic1 = CriticNetwork(n_actions=num_actions, fc1_dims=fc1_critic, fc2_dims=fc2_critic, name=\"Critic1\")\r\n        self.critic2 = CriticNetwork(n_actions=num_actions, fc1_dims=fc1_critic, fc2_dims=fc2_critic, name=\"Critic2\")\r\n        self.target_critic1 = CriticNetwork(n_actions=num_actions, fc1_dims=fc1_critic, fc2_dims=fc2_critic,\r\n                                            name=\"CriticTarget1\")\r\n        self.target_critic2 = CriticNetwork(n_actions=num_actions, fc1_dims=fc1_critic, fc2_dims=fc2_critic,\r\n                                            name=\"CriticTarget2\")\r\n\r\n        # Compiling the networks:\r\n        self.actor.compile(optimizer=Adam(learning_rate=lr_actor))\r\n        self.target_actor.compile(optimizer=Adam(learning_rate=lr_actor))\r\n        self.critic1.compile(optimizer=Adam(learning_rate=lr_critic))\r\n        self.critic2.compile(optimizer=Adam(learning_rate=lr_critic))\r\n        self.target_critic1.compile(optimizer=Adam(learning_rate=lr_critic))\r\n        self.target_critic2.compile(optimizer=Adam(learning_rate=lr_critic))\r\n\r\n        self.update_target_networks()\r\n\r\n    def learn(self, timestep):\r\n        # If there is not enough data, just return, don't learn from zeroes\r\n        if self.buffer.counter < self.batch_size:\r\n            return\r\n        \"\"\" \r\n        Learning comes after: \r\n        - choosing action\r\n        - implementing that action\r\n        - storing new variables in buffer\r\n        \"\"\"\r\n        s_batch, a_batch, r_batch, ns_batch = self.buffer.batch_sample()\r\n        s_batch = tf.convert_to_tensor(s_batch, dtype=tf.float64)\r\n        a_batch = tf.convert_to_tensor(a_batch, dtype=tf.float64)\r\n        r_batch = tf.convert_to_tensor(r_batch, dtype=tf.float32)\r\n        ns_batch = tf.convert_to_tensor(ns_batch, dtype=tf.float64)\r\n\r\n        next_action = self.actor(ns_batch)\r\n        noise = tf.random.normal(shape=(1, self.num_actions), mean=0.0, stddev=0.2)\r\n        noise = np.clip(noise, -0.5, 0.5)\r\n        next_action += noise\r\n        next_action = np.clip(next_action, -1, 1)\r\n\r\n        # next_action = tf.convert_to_tensor(next_action)\r\n        \"\"\" Update of the critic networks with gradient \"\"\"\r\n        with tf.GradientTape(persistent=True) as tape:\r\n            # Target Q values via target critic networks (next state, next action)\r\n            q1_ = tf.squeeze(self.target_critic1(ns_batch, next_action))\r\n            q2_ = tf.squeeze(self.target_critic2(ns_batch, next_action))\r\n\r\n            # Choose minimum from these two values for the double Q update rule\r\n            q_ = tf.math.minimum(q1_, q2_)\r\n            # Calculate actual Q value:\r\n            y = r_batch + (self.gamma * q_)\r\n\r\n            # Current Q values via critic networks (state, action)\r\n            q1 = self.critic1(s_batch, a_batch)\r\n            q2 = self.critic2(s_batch, a_batch)\r\n\r\n            # Loss is calculated as the sum of the MSE loss between target Q value and q1, q2\r\n            q1_loss = tf.keras.losses.MSE(y, q1)\r\n            q2_loss = tf.keras.losses.MSE(y, q2)\r\n            q_loss = q1_loss + q2_loss\r\n\r\n            # Optimize critic networks:\r\n        critic_gradient = tape.gradient(q_loss, self.critic1.trainable_variables)\r\n        self.critic1.optimizer.apply_gradients(\r\n            zip(critic_gradient, self.critic1.trainable_variables))\r\n\r\n        critic_gradient = tape.gradient(q_loss, self.critic2.trainable_variables)\r\n        self.critic2.optimizer.apply_gradients(\r\n            zip(critic_gradient, self.critic2.trainable_variables))\r\n        del tape\r\n\r\n        if timestep % 2:\r\n            # Update actor with gradient\r\n            with tf.GradientTape() as tape:\r\n                actions = tf.convert_to_tensor(self.actor(s_batch))\r\n                critic_value = tf.squeeze(-self.critic1(s_batch, actions))\r\n                actor_loss = tf.math.reduce_mean(critic_value)\r\n\r\n            actor_grad = tape.gradient(actor_loss, self.actor.trainable_variables)\r\n            self.actor.optimizer.apply_gradients(\r\n                zip(actor_grad, self.actor.trainable_variables))\r\n\r\n            self.update_target_networks()\r\n\r\n    def update_target_networks(self):\r\n\r\n        # Update target critics: - soft update with tau\r\n\r\n        new_weights_1 = []\r\n        target_variables_1 = self.target_critic1.weights\r\n        for i, variable in enumerate(self.critic1.weights):\r\n            new_weights_1.append(variable * self.tau + target_variables_1[i] * (1 - self.tau))\r\n        self.target_critic1.set_weights(new_weights_1)\r\n\r\n        new_weights_2 = []\r\n        target_variables_2 = self.target_critic2.weights\r\n        for i, variable in enumerate(self.critic2.weights):\r\n            new_weights_2.append(variable * self.tau + target_variables_2[i] * (1 - self.tau))\r\n        self.target_critic2.set_weights(new_weights_2)\r\n\r\n        # Update target actor:\r\n        new_weights_3 = []\r\n        target_variables_3 = self.target_actor.weights\r\n        for i, variable in enumerate(self.actor.weights):\r\n            new_weights_3.append(variable * self.tau + target_variables_3[i] * (1 - self.tau))\r\n        self.target_actor.set_weights(new_weights_3)", "comments": ["@godanyitamas,\r\nOn running the given code, I am facing an error stating `ModuleNotFoundError: No module named 'Networks'`. \r\n\r\nAlso the code provided is fairly complex, hence it would be difficult for us to pinpoint the issue. Can you remove the dependencies and get the example down to the simplest possible repro? That will allow us to debug the issue easily. Thanks!", "@godanyitamas,\r\nPlease go through the comments from this similar issue [#1511](https://github.com/tensorflow/tensorflow/issues/1511) and let us know if it helps. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43064, "title": "16 bit integer quantization", "body": "Is there a way to use the TensorFlow Lite Converter for 16-bit quantization? If so, how should it be done?", "comments": ["@peter197321 \r\nPlease refer to [this link](https://www.tensorflow.org/lite/performance/post_training_float16_quant) and let us know.\r\n", "The link you share is about a float 16-bit quantization (Post-training float16 quantization) but I'm looking for integer 16-bit quantization... ", "@peter197321 \r\nRefer to [this link](https://www.tensorflow.org/lite/performance/post_training_quantization#integer_only_16-bit_activations_with_8-bit_weights_experimental), it is 16 bit int.", "It is missing ops implementation e.g.: RESIZE_NEAREST_NEIGHBOR and failing to quantize at this point.\r\n\r\n`RuntimeError: Quantization to 16x8-bit not yet supported for op: 'RESIZE_NEAREST_NEIGHBOR'.`", "@peter197321 \r\nWe see that the issue template has not been filled, can you please let us know the tf version used.\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\nPlease refer to this issue #33893 with same error.", "Used the latest tf-nightly build (supposed it's 2.4) and not working - the issue is about int8 ...? \r\nthe request is for in16.", "@peter197321 \r\nPlease move this issue to closed status as the code error you reported above is tracked at #43080.\r\nAlso please refer to [this link](https://github.com/tensorflow/tensorflow/issues/42024#issuecomment-668744454) and let us know if it helps."]}, {"number": 43063, "title": "imx8 tflite opencl ", "body": "platform:imx8\r\ntensorflow:2.3\r\n \r\ntflite support **Vivante** OpenCL device **gc7000l** ???\r\ni can not found **Vivante** in code   tensorflow/tensorflow/lite/delegates/gpu/cl/cl_device.h\r\n**enum class Vendor { QUALCOMM, MALI, POWERVR, NVIDIA, AMD, INTEL, UNKNOWN }**", "comments": ["The current OpenCL implementation still relies on libEGL and libGLESv2 which are available on Android.\r\nAre these libraries available on your iMX8m device?\r\n\r\n@impjdi correct me if I'm wrong.", "libGLESv3.a and libEGL.a are required for OpenGL; libOpenCL-pixel.so is required for OpenCL.  However, the way our delegate is set up, you will need both (with the latter being optional).\r\n\r\nOpenCL is linked in dynamically; the delegate will try a dlopen(\"libOpenCL-pixel.so\") and decide OpenCL's availability.  I don't know what imx8 is and what Vivante is, but you may have to navigate around that.", "BTW, I've found a way to use OpenCL with non Android devices.\r\nYou can build the TFLite benchmark tool which supports OpenCL GPU delegate as following.\r\n\r\n$ bazel build --config=elinux_aarch64 -c opt --copt=-DCL_DELEGATE_NO_GL --copt=-DEGL_NO_X11 tensorflow/lite/tools/benchmark:benchmark_model"]}, {"number": 43062, "title": "TF_lite Convert using integer-only quantization", "body": "import tensorflow as tf\r\nsaved_model_dir='D:/sfz/tf_sfz'\r\ndef representative_data_gen():\r\n  for input_value in tf.data.Dataset.from_tensor_slices(**train_images**).batch(1).take(100):\r\n      yield [input_value]\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_data_gen\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\ntflite_model_quant = converter.convert()\r\nopen(\"D:/sfz/tf_sfz/model25.tflite\",\"wb\").write(tflite_model_quant)\r\n\r\nthe **train_images** don't know what it is,I only know to select a part of the representative data, I don't know what to do next?The training data is JPG file and TXT file\uff0cThey are in a folder, how do I make **train_images**?What type of **train_images** is it?Can you fix the code a little bit\r\n", "comments": ["@huafeihuayu,\r\nThis question is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) since it is not a TensorFlow bug or feature request. There is also a larger community that reads questions there. Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43061, "title": "[tflite] Processing time is high in Windows 10 but better in Ubuntu 16.0", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): From binary\r\n- TensorFlow version (use command below): 2.3\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI am running [this](https://github.com/metalwhale/hand_tracking) project. When run on an Ubuntu 16.04 machine it takes delay of ~0.2 S. The processor is I3, and RAM 4GB. But when I ran this on a Windows 10 machine having I7 processor and RAM 4GB, the delay is increased. It varies in the range of 0.4-0.6 S. I tried the TFLite runtime also. But delay is increased. \r\n**Describe the expected behavior**\r\nRunning on a processor with high spec should be giving a better performance.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\nhttps://github.com/metalwhale/hand_tracking\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.", "comments": ["@Sreerag-ibtl \r\nIs it possible for you to share a simple stand alone code to replicate the issue reported or share a colab gist.", "@Sreerag-ibtl Please provide performance profiling using following tool with --enable_op_profiling=true. https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark\r\n\r\nFrom there we can see which op is not performing well.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43060, "title": "Training of a simple model with KerasLayer hangs on macOSX with tf 2.3.0", "body": "**System information**\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\ncustom code\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nmacOS X 10.15.4 (19E287)\r\n\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n\r\n- TensorFlow version (use command below):\r\nv2.3.0-rc2-23-gb36436b087 2.3.0\r\n\r\n\r\n- Python version:\r\nPython 3.7.3\r\n\r\n**Describe the current behavior**\r\nSimple model training hangs on macOS X with tf 2.3.0, but works in Colab and on macOS X with tf 2.2.0\r\n\r\n**Describe the expected behavior**\r\nshould wourk\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\n\r\nimport tensorflow as tf\r\nimport tensorflow_hub as hub\r\nimport tensorflow_text as tf_text\r\n\r\n\r\nprint(tf.__version__)\r\nprint(tf.keras.__version__)\r\n\r\nEMBEDDING = \"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\"\r\n\r\nembed = hub.KerasLayer(EMBEDDING, dtype=tf.string, trainable=True)\r\n\r\ns1 = tf.keras.Input(shape=[], dtype=tf.string)\r\ns2 = tf.keras.Input(shape=[], dtype=tf.string)\r\n\r\nv1 = embed(s1)\r\nv2 = embed(s2)\r\n\r\ncd = tf.reduce_sum(tf.multiply(v1, v2), axis=-1)\r\n\r\ntrain_model = tf.keras.Model(inputs=[s1, s2], outputs=[cd])\r\noptimizer = tf.optimizers.SGD(learning_rate=0.001)\r\n\r\ni1 = tf.constant([\"x\", \"y\", \"z\"])\r\ni2 = tf.constant([\"a\", \"b\", \"c\"])\r\nc0 = tf.constant([1.0, 1.0, 1.0])\r\n\r\ntrain_model.compile(optimizer=optimizer, loss=\"mse\", metrics=[\"mse\"])\r\ntrain_model.fit(x=[i1, i2], y=c0, batch_size=1, epochs=5, verbose=2)\r\n\r\n\r\n```\r\n", "comments": ["@andreyryabov \r\n\r\nI have tried in colab with TF version 2.3 and i am not seeing any issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/eca6c8c8564c33f6d0dd2f9b5248ca6b/untitled325.ipynb).Can you please share error log you are getting. Thanks!", "I does work in colab, as I mentioned in the description.  It freezes only when launched macOS. I traced it's a deadlock on some conditional variable deep in C++ code. There are no errors in the log since the code just hangs on .fit()", "@andreyryabov In the ticket I see `v2.3.0-rc2-23-gb36436b087`. What Is your version?", "My version of TF:  2.3.0,   GIT: v2.3.0-rc2-23-gb36436b087\r\nreturned by command:\r\n `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n", "Can you reproduce this on your machine with tf-nightly?", "The problem reproduces with tf-nightly. ", "What is your installed version of `tensorflow-hub`?", "tensorflow-hub: 0.9.0", "Can you just try to temp change your TFHUB cache dir on a new one with:\r\n```\r\nimport os\r\nos.environ[\"TFHUB_CACHE_DIR\"] =<you_new_selected_cache_dir>\r\n```", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43060\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43060\">No</a>\n"]}, {"number": 43059, "title": "Update CMSIS-NN archive to support SVDF.", "body": "Update archive links to use latest CMSIS-NN version.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 43058, "title": "Update CONTRIBUTING.md", "body": " * Fixed formatting that somehow slipped through the cracks during the PR review.\r\n * Updated the TOC with gh-md-toc --insert CONTRIBUTING.md\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 43057, "title": "Update CONTRIBUTING.md", "body": "FIxed formatting that somehow slipped through the cracks during the PR review.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 43056, "title": "Merge pull request #1 from tensorflow/master", "body": "Up-rev to TF HEAD", "comments": []}, {"number": 43055, "title": "LSTM input dropout bug", "body": "occurs when _could_use_gpu_kernel is True", "comments": ["see [this issue](https://github.com/tensorflow/tensorflow/issues/40215) for details ", "Thanks for sending the PR, and sorry for dropping the ball on the Github issue. Let me comment on the https://github.com/tensorflow/tensorflow/issues/40215", "@qlzh727 Any update on this PR? Please. Thanks!", "The related #40215 has been closed. Closing this PR since it is not needed any more."]}, {"number": 43054, "title": "Tensorflow Bazel Build ", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.3\r\n- Python version: 3.8.5\r\n- Installed using virtualenv? pip? conda?: No\r\n- Bazel version (if compiling from source): 3.5.0\r\n- CUDA/cuDNN version: 11/8.0.3\r\n- GPU model and memory: GTX 1650\r\n\r\nERROR: An error occurred during the fetch of repository 'local_config_cuda':\r\n   Traceback (most recent call last):\r\n        File \"C:/users/soria/downloads/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1377, column 38, in _cuda_autoconf_impl\r\n                _create_local_cuda_repository(repository_ctx)\r\n        File \"C:/users/soria/downloads/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1162, column 17, in _create_local_cuda_repository\r\n                cc = find_cc(repository_ctx)\r\n        File \"C:/users/soria/downloads/tensorflow/third_party/gpus/cuda_configure.bzl\", line 216, column 34, in find_cc\r\n                return _get_msvc_compiler(repository_ctx)\r\n        File \"C:/users/soria/downloads/tensorflow/third_party/gpus/cuda_configure.bzl\", line 133, column 26, in _get_msvc_compiler\r\n                return find_msvc_tool(repository_ctx, vc_path, \"cl.exe\").replace(\"\\\\\", \"/\")\r\n        File \"C:/users/soria/_bazel_soria/s7y7zhp3/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 404, column 27, in find_msvc_tool\r\n                if _is_vs_2017_or_2019(vc_path) or _is_msbuildtools(vc_path):\r\n        File \"C:/users/soria/_bazel_soria/s7y7zhp3/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 261, column 19, in _is_vs_2017_or_2019\r\n                return vc_path.find(\"2017\") != -1 or vc_path.find(\"2019\") != -1\r\nError: 'NoneType' value has no field or method 'find'\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': no such package '@local_config_cuda//cuda': 'NoneType' value has no field or method 'find'\r\nWARNING: Target pattern parsing failed.\r\nERROR: no such package '@local_config_cuda//cuda': 'NoneType' value has no field or method 'find'\r\nINFO: Elapsed time: 7.514s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n    currently loading: tensorflow/tools/pip_package", "comments": ["Can you check https://github.com/tensorflow/tensorflow/issues/43048#issuecomment-688966840?", "And also https://www.tensorflow.org/install/source_windows?hl=en#install_visual_c_build_tools_2019", "I tried installling both and it works now, but i encountered a new error.\r\n\r\nERROR: C:/users/soria/downloads/tensorflow/tensorflow/core/util/BUILD:823:17: ProtoCompile tensorflow/core/util/event_pb2.py failed (Exit -1073741795): protoc.exe failed: error executing command\r\n  cd C:/users/soria/_bazel_soria/s7y7zhp3/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0\r\n    SET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin;C:\\WINDOWS;C:\\WINDOWS\\System32;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\r\n    SET PYTHON_BIN_PATH=C:/Users/soria/AppData/Local/Programs/Python/Python38/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/soria/AppData/Local/Programs/Python/Python38/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TF2_BEHAVIOR=1\r\n    SET TF_CONFIGURE_IOS=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=7.5\r\n    SET TF_NEED_CUDA=1\r\n  bazel-out/x64_windows-opt/bin/external/com_google_protobuf/protoc.exe --python_out=bazel-out/x64_windows-opt/bin -I. -Iexternal/com_google_protobuf/python -Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf/python tensorflow/core/util/event.proto\r\nExecution platform: @local_execution_config_platform//:platform\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 811.126s, Critical Path: 24.10s\r\nINFO: 167 processes: 167 local.\r\nFAILED: Build did NOT complete successfully", "What version are you trying to compile?", "HEAD (master) (2.3.0)", "Master or 2.3.0?", "Master sorry for confusion", "Can you check if you are compliant with others requirements at https://www.tensorflow.org/install/source_windows?", "Also check if the current master is building sucessful in the CI for windows: https://github.com/tensorflow/tensorflow#continuous-build-status", "I double checked all the requirements and make sure that i completed it. I looked at https://github.com/tensorflow/tensorflow#continuous-build-status and it says Windows GPU failing. What should i do? checkout to r2.3? and build from there?", "> What should i do? checkout to r2.3? and build from there?\r\n\r\nYes It is what is suggested to check if it is master or you environment but please note that r2.3 doesn't support CUDA 11", "I see so i should be downgrading my CUDA to 10.1?", "If you want to try r2.3 yes.", "Thank you", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43054\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43054\">No</a>\n"]}, {"number": 43052, "title": "DLL load fail _pywrap_tensorflow_internal during CPU-only installation", "body": "I am trying to install the CPU-only version of tf.\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64bit (v.1912)\r\n- TensorFlow installed from (source or binary): pip (v.2.3.0), conda (v.2.1.0)\r\n- TensorFlow version: 2.3.0\r\n- Python version: 3.7.0 and 3.8.3\r\n- Installed using virtualenv? conda and without using an environment\r\n- CUDA/cuDNN version: I didn't install as my GPU doesn't support tf\r\n- GPU model and memory: Intel HD 520\r\n- CPU Model: Intel\u00ae Core\u2122 Prozessor i5-6200U (based on issue #39007 I checked for AVX support, the [datasheet](https://ark.intel.com/content/www/us/en/ark/products/88193/intel-core-i5-6200u-processor-3m-cache-up-to-2-80-ghz.html) says it supports Intel\u00ae AVX2)\r\n\r\nI tried to install tensorflow to an existing environment, which already didn't work. Trying to the fix the issue, I uninstalled anaconda (incl. caches, pip, and conda) and installed miniconda. Running the following commands:\r\n```\r\n$ conda create --name tf_test\r\n$ activate tf_test\r\n$ pip install tensorflow\r\n$ python\r\n>>>import tensorflow\r\n```\r\nI am getting the same error as before, which stays the same also without activating the conda environment:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\sebas\\miniconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\sebas\\miniconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.eager import context\r\n  File \"C:\\Users\\sebas\\miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 35, in <module>\r\n    from tensorflow.python import pywrap_tfe\r\n  File \"C:\\Users\\sebas\\miniconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tfe.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\sebas\\miniconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 83, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\sebas\\miniconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: Das angegebene Modul wurde nicht gefunden. \r\n(*The specified module could not be found.*) \r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n\r\nThanks for your help!", "comments": ["Tensorflow doesn't directly support conda setup but you can follow https://docs.anaconda.com/anaconda/user-guide/tasks/tensorflow/", "@sRassmann \r\nYou could be facing this issue because of the following reasons\r\n\r\nYou you running 32-bit Python or 32-bit OS\r\nPlease take a look at the [system requirements](https://www.tensorflow.org/install/pip#system-requirements) and check if you have the correct dependencies installed.\r\n\r\nAlso, check these similar duplicate issues: #42058 #41596 #40459 #39007 #38916 #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 .\r\n\r\nIn case you still face issues after verifying the above, please create a issue on  [Anaconda repo.](https://github.com/ContinuumIO/anaconda-issues/issues), and move this to closed status.\r\nThanks!\r\n", "@Saduf2019 & @bhack Thank you for your help!!\r\n\r\nI performed the following steps:\r\n- Completely uninstalled everything (ana-/miniconda and related folders)\r\n- Installed vanilla python 3.7.0\r\n- run ```$ pip install tensorflow``` \r\n- Now ```$ python``` and ```>>> import tensorflow``` worked seamlessly (besides the warning for not using GPU)\r\n\r\nInterestingly, after I reinstalled miniconda, tensorflow also worked within conda environments. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43052\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43052\">No</a>\n", "@sRassmann\r\nGlad to hear we could help you resolve the issue, thanks for the update."]}, {"number": 43051, "title": "Implement horizontal input fusion.", "body": "Extend horizontal fusion to support fusion of reduction instructions.\r\n\r\nThe PR for the code generation of parallel reduction is [here](https://github.com/tensorflow/tensorflow/pull/42717). Potential performance gain can also be inferred by the numbers listed in the code gen PR.\r\n\r\n ", "comments": ["@thomasjoerg could you help to review this PR when you have a moment? Thanks!", "@thomasjoerg ping~"]}, {"number": 43050, "title": "Added config based tests for tf.data dispatcher and worker servers", "body": "This PR extends the test cases for:\r\n\r\n- `tf.data.experimental.service.DispatchServer`\r\n- `tf.data.experimental.service.WorkerServer`\r\n\r\nand modifies the supported protocol docstring for the above two classes.\r\n\r\nThe \"grpc+local\" protocol was not being supported while testing using:\r\n\r\n```python\r\ndef testStartDispatcherWithProtocolConfig(self):\r\n    config = server_lib.DispatcherConfig(port=5000, protocol=\"grpc+local\")\r\n    dispatcher = server_lib.DispatchServer(config=config, start=False)\r\n    dispatcher.start()\r\n```\r\nThe test was erroring out with:\r\n\r\n```console\r\nRuntimeError: No credentials factory has been registered for protocol grpc+local. The available types are: [ grpc ]\r\n```\r\n", "comments": ["@aaudiber, thanks for the review. Made the changes based on portpicker for picking a random unused port.", "@kvignesh1420 Can you please resolve conflicts? Thanks!", "@aaudiber I was a bit dicey about using `threading.Lock()` in `pick_unused_port()` function but included it anyway for additional thread safety while running the tests. Let me know if you feel otherwise."]}, {"number": 43049, "title": "How to use unified memory in TF2.1", "body": "Hi, I am trying to test unified memory performance. But I cannot find any introduction about how to use unified memory in tf2.1. Could you please offer some help? Thanks a lot!\r\n", "comments": ["Can you check https://github.com/tensorflow/tensorflow/issues/29840? /cc @jaingaurav @aaroey ", "Yes. But I still have some problem about multi-gpu training with unified memory, and I put some new information in that issue. You can check it. @bhack ", "Hi, For this https://github.com/tensorflow/tensorflow/issues/29840#issuecomment-689284948 , do you have any idea? Where can I find more information about training with UVM in tensorflow? @ravikyram @bhack ", "Can you try setting the env var `TF_FORCE_UNIFIED_MEMORY` to `true`?", "Hi @JF-D!Could you reply to above response ?", "@mohantym I am sorry. I didn't continue to try unified memory with tf, and I am working on something else. So, I think this issue can be closed.", "Ok @JF-D !Closing issue then. Feel free to open a new issue if you face any problem.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43049\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43049\">No</a>\n"]}, {"number": 43048, "title": "ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1.  It must be a bug, a feature request, or a significant problem with the\r\n    documentation (for small docs fixes please send a PR instead).\r\n2.  The form below must be filled out.\r\n3.  It shouldn't be a TensorBoard issue. Those go\r\n    [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**:\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n-   **TensorFlow installed from (source or binary)**:\r\n-   **TensorFlow version (use command below)**:\r\n-   **Python version**:\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:\r\n-   **GPU model and memory**:\r\n-   **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nOn Win10 using Python 3.7 64bit\r\ntrying to install tensorflow with \r\npip3 install --user --upgrade tensorflow this gives\r\nERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\r\n\r\ntried just plain pip install tensorflow as well same exception is thrown as far as I can tell I am using latest version of pip and I have admin rights/", "comments": ["What Is your version of `pip`?", "20.2.3\r\n\r\nFrom: bhack <notifications@github.com>\r\nSent: Tuesday, September 8, 2020 10:26 AM\r\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\r\nCc: Rick Smith <rsmith@willisconsulting.com>; Author <author@noreply.github.com>\r\nSubject: Re: [tensorflow/tensorflow] ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none) (#43048)\r\n\r\n[ -External Email- ]\r\n\r\nWhat Is your version of pip?\r\n\r\n\u2014\r\nYou are receiving this because you authored the thread.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/43048#issuecomment-688912716>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AE24D6YQV5UVOMADIQNTHPLSEY5FZANCNFSM4RAAZMQA>.\r\n", "Can you try with an updated version of  Python 3.7.x or Python 3.8.x?", "Yes,\r\n\r\nI tried switching over to python 3.8.1 32 ands  using\r\n\r\nPip install tensorflow\r\n\r\nSame exception in pip\r\nERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\r\nERROR: No matching distribution found for tensorflow\r\n\r\nFrom: bhack <notifications@github.com>\r\nSent: Tuesday, September 8, 2020 10:51 AM\r\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\r\nCc: Rick Smith <rsmith@willisconsulting.com>; Author <author@noreply.github.com>\r\nSubject: Re: [tensorflow/tensorflow] ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none) (#43048)\r\n\r\n[ -External Email- ]\r\n\r\nCan you try with an updated version of Python 3.7.x or Python 3.8.x?\r\n\r\n\u2014\r\nYou are receiving this because you authored the thread.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/43048#issuecomment-688930287>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AE24D65HJTRH4RJCOW3XB73SEZAGLANCNFSM4RAAZMQA>.\r\n", "What is `32`? 64bit python is required.", "Ok, I got it to 3.8.5 64bit same difference\r\n\r\nERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\r\nERROR: No matching distribution found for tensorflow\r\n\r\nFrom: bhack <notifications@github.com>\r\nSent: Tuesday, September 8, 2020 11:02 AM\r\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\r\nCc: Rick Smith <rsmith@willisconsulting.com>; Author <author@noreply.github.com>\r\nSubject: Re: [tensorflow/tensorflow] ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none) (#43048)\r\n\r\n[ -External Email- ]\r\n\r\nWhat is 32? 64bit python is required.\r\n\r\n\u2014\r\nYou are receiving this because you authored the thread.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/43048#issuecomment-688937499>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AE24D62ABY5ECAC5CIZ6II3SEZBODANCNFSM4RAAZMQA>.\r\n", "Can you post the output of `pip3 --version`?", "Also if you can the outputs of:\r\n`python3.8 -VV`\r\n`python3.8 -m pip install tensorflow`", "Ok,\r\n\r\nI got it to install and now it\u2019s throwing this:\r\n\r\n2020-09-08 11:39:35.785180: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n2020-09-08 11:39:35.785443: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n\r\nFrom: bhack <notifications@github.com>\r\nSent: Tuesday, September 8, 2020 11:23 AM\r\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\r\nCc: Rick Smith <rsmith@willisconsulting.com>; Author <author@noreply.github.com>\r\nSubject: Re: [tensorflow/tensorflow] ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none) (#43048)\r\n\r\n[ -External Email- ]\r\n\r\nAlso if you can the outputs of:\r\npython3.8 -VV\r\npython3.8 -m pip install tensorflow\r\n\r\n\u2014\r\nYou are receiving this because you authored the thread.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/43048#issuecomment-688951170>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AE24D62R6NUKA6CXFK6ILRDSEZD4NANCNFSM4RAAZMQA>.\r\n", ">\r\n> Install the Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017, and 2019. Starting with the TensorFlow 2.1.0 version, the msvcp140_1.dll file is required from this package (which may not be provided from older redistributable packages). The redistributable comes with Visual Studio 2019 but can be installed separately:\r\n\r\n  1. Go to the [Microsoft Visual C++ downloads](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads/),\r\n  2. Scroll down the page to the Visual Studio 2015, 2017 and 2019 section.\r\n  3. Download and install the Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017 and 2019 for your platform.\r\n\r\n", "Thanks, did that, same difference\u2026 Seems to be related to CUDA and GPU\u2026\r\nhttps://stackoverflow.com/questions/59823283/could-not-load-dynamic-library-cudart64-101-dll-on-tensorflow-cpu-only-install\r\n\r\nI don\u2019t have CUDA, can I get on without it I am not using GPU acceleration at this time so?\r\n\r\nFrom: bhack <notifications@github.com>\r\nSent: Tuesday, September 8, 2020 11:47 AM\r\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\r\nCc: Rick Smith <rsmith@willisconsulting.com>; Author <author@noreply.github.com>\r\nSubject: Re: [tensorflow/tensorflow] ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none) (#43048)\r\n\r\n[ -External Email- ]\r\n\r\nInstall the Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017, and 2019. Starting with the TensorFlow 2.1.0 version, the msvcp140_1.dll file is required from this package (which may not be provided from older redistributable packages). The redistributable comes with Visual Studio 2019 but can be installed separately:\r\n\r\n  1.  Go to the Microsoft Visual C++ downloads<https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads/>,\r\n  2.  Scroll down the page to the Visual Studio 2015, 2017 and 2019 section.\r\n  3.  Download and install the Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017 and 2019 for your platform.\r\n\r\n\u2014\r\nYou are receiving this because you authored the thread.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/43048#issuecomment-688966840>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AE24D6ZWMXNIWJP22PAR4HTSEZGXLANCNFSM4RAAZMQA>.\r\n", "try to use `python3.8 -m pip install tensorflow-cpu`", "Yes! That did it,\r\n\r\nTHANKS MUCH!\r\n\r\nFrom: bhack <notifications@github.com>\r\nSent: Tuesday, September 8, 2020 12:07 PM\r\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\r\nCc: Rick Smith <rsmith@willisconsulting.com>; Author <author@noreply.github.com>\r\nSubject: Re: [tensorflow/tensorflow] ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none) (#43048)\r\n\r\n[ -External Email- ]\r\n\r\ntry to use python3.8 -m pip install tensorflow-cpu\r\n\r\n\u2014\r\nYou are receiving this because you authored the thread.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/43048#issuecomment-688980069>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AE24D64FWWGN7QAKYPSTACLSEZJDNANCNFSM4RAAZMQA>.\r\n", "Can you close this?\r\nThanks."]}, {"number": 43047, "title": "Cardinality returns unknown for a dataset with (string, float), but also for dataset with float variable alone", "body": "Cardinality returns unknown from a dataset with (string, float) but also for dataset with float variable alone. \r\n\r\nI created data stored in a csv files without header representing text and a float number corresponding to this text (NALU like dataset). \r\n\r\nI susptect that it is not possible to find the size of the data since the first column is a string it gives me an UNKNOWN cardinality (-2) back.\r\n\r\nDataset creation: \r\n```py\r\nimport tensorflow_datasets as tfds\r\nimport tensorflow as tf\r\n\r\nwith open(\"file.csv\", \"w\") as f:\r\n    for text, number in [(\"one\", 1.0), (\"two\", 2.0)]:\r\n        f.write(f\"{text:s}, {number:.1f}\\n\")\r\n\r\nds = tf.data.experimental.CsvDataset(\r\n    filenames=\"file.csv\", record_defaults=[tf.string, tf.float32], header=False)\r\n\r\ntf.data.experimental.cardinality(ds)\r\n```\r\n\r\nOne would expect that when creating a dataset with the float column only, or mapping the dataset to the target only, the cardinality would be returned properly, but the next mapping stil has UNKNOWN cardinality.\r\n\r\n```py\r\n# check first method:\r\nwith open(\"file1.csv\", \"w\") as f:\r\n    for number in [1.0, 2.0]:\r\n        f.write(f\"{number:.1f}\\n\")\r\n\r\nds1 = tf.data.experimental.CsvDataset(\r\n    filenames=\"file1.csv\", record_defaults=[tf.float32], header=False)\r\n\r\ntf.data.experimental.cardinality(ds1)\r\n#returns -2\r\n\r\n# check second method\r\ndef get_target_pyfn(text, target):\r\n    target = tf.py_function(lambda text, target: target, inp=[text, target], Tout=tf.float32)\r\n    target.set_shape([])\r\n    return target\r\n\r\ntf.data.experimental.cardinality( ds.map(get_target_pyfn))\r\n#returns -2\r\n```\r\n\r\n\r\n**System information**\r\ntensorflow version = 2.3.0\r\nPython virtual environment is setup with pyproject.toml in https://github.com/lynochka/parser/blob/bf2a006bee210b62552618dceb181101655753ff/pyproject.toml\r\n", "comments": ["@lynochka \r\nPlease fill in issue template for us to help. tf version, simple stand alone code to replicate the issue faced or share a colab gist with issue reported with error logs.", "Check https://github.com/tensorflow/tensorflow/issues/30529#issuecomment-510814608", "Update the issue with tensorflow = 2.3.0, the code is standalone now that the imports are added  ", "From the link you gave, looks like it is less of a bug and more of a feature request ?", ">tf.data.experimental.UNKNOWN_CARDINALITY if the analysis fails to determine the number of elements in dataset (e.g. when the dataset source is a file).", "If you see I've just quoted the official documentation.\r\nIt Is by design.", "@jsimsa Can we close this?", "Correct, this is working as intended. If you wish to compute the cardinality of a dataset whose cardinality is not known statically (and thus requires iterating over the entire dataset), you can do so via:\r\n\r\n```\r\nnum_elements = 0\r\nfor _ in dataset:\r\n  num_elements = num_elements + 1\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43047\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43047\">No</a>\n"]}, {"number": 43046, "title": "TensorFlow send/recv operations scheduling", "body": "Link to the documentation: \r\nhttps://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf\r\nlink to the OpKernels implementation:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/sendrecv_ops.h\r\n\r\n## Description of issue:\r\nWhen a Tensorflow execute a graph, it partitions it into subgraphs s.t. each subgraph consists of nodes assigned to a single device. Value copy operations between those graphs are introduced in the form of Send and Recv nodes.\r\n\r\n![image](https://user-images.githubusercontent.com/18283933/92479762-a3befc80-f1ec-11ea-9b1d-c37069ca10d5.png)\r\n\r\nSend/Recv nodes are OpKernels that being executed on the CPU but trigger a memory copy operation (host to device or device to host). The framework support executing these operations in any order by using the Rendezvous object. Due to Recv1 OpKernel, we can be sure that OpKernel2 will be executed only when its input is ready. The question is about Send1 OpKernel. How does the framework schedule it? Since Send1 executed on CPU it should be executed immediately after calling to OpKernel1->compute in this case data dependency may be broken. Or maybe Send1 somehow belongs to the same subgraph as OpKernel1 even though they are being executed on different devices?\r\n\r\n", "comments": ["/cc @keveman", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there\r\n\r\nHaving said that - the diagram is inaccurate. Corresponding sends and recvs are on different devices - Send1 should be on Device1 and Recv2 should be on Device2. Send1 therefore has a data dependency on Op1 and it runs after that.", "@rohan100jain I have asked in StackOverflow too: \r\nhttps://stackoverflow.com/questions/63793814/tensorflow-sour-code-send-recv-operations-scheduling\r\n\r\n1. Doesn't Send/Recv ops CPU nodes? As far as know, for example, GPU doesn't have an implementation of this kernel.\r\n\r\n2. Regarding the data dependency, there is a data dependency between  OpKernel1 and OpKernel2 and still, we have to synchronize them with send and recv nodes. \r\n"]}, {"number": 43045, "title": "TFX for Mobile: Additional dtypes", "body": "**System information**\r\n- MacOS\r\n- tensorflow==2.3.0.rc\r\n- tfx==0.22.0\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\ndef _get_serve_tf_examples_fn(model, tf_transform_output):\r\n        \"\"\"Returns a function applies TFT.\"\"\"\r\n\r\n        model.tft_layer = tf_transform_output.transform_features_layer()\r\n\r\n        @tf.function\r\n        def serve_tf_examples_fn(text):\r\n            \"\"\"Returns the output to be used in the serving signature.\"\"\"\r\n            reshaped_text = tf.reshape(text, [-1, 1])\r\n\r\n            transformed_features = model.tft_layer({\"text\": reshaped_text})\r\n\r\n            outputs = model(transformed_features)\r\n            return {'outputs': outputs}\r\n\r\n        return serve_tf_examples_fn\r\n\r\n\r\n    signatures = {\r\n        'serving_default':\r\n            _get_serve_tf_examples_fn(model,\r\n                                        tft_output).get_concrete_function(\r\n                                            tf.TensorSpec(\r\n                                                shape=[None],\r\n                                                dtype=tf.float32,\r\n                                                name='examples')),\r\n    }\r\n\r\n    model.save(serving_dir, save_format='tf', signatures=signatures)\r\n\r\n\r\n    converter = tf.lite.TFLiteConverter.from_saved_model(serving_dir)\r\n    converter.experimental_new_converter=True\r\n    converter.target_spec.supported_ops = [\r\n            tf.lite.OpsSet.TFLITE_BUILTINS,\r\n            tf.lite.OpsSet.SELECT_TF_OPS\r\n        ]\r\n    tflite_model = converter.convert()\r\n\r\nValueError: Input 1 of node StatefulPartitionedCall was passed bool from unknown:0 incompatible with expected resource.\r\n```\r\n\r\nTFX provides a powerful ecosystem for running production-grade ML pipelines, where one of its most important components is the Transform component. Currently, it seems that the tflite converter is incapable of receiving an input string and performing the tokenization in the tfgraph. Is this expected to be released in the near future? Same holds true for image decoding: https://github.com/tensorflow/tfx/tree/master/tfx/examples/mnist takes arrays as input rather than raw image strings. \r\n", "comments": ["Is your example runnable?", "@Sruinard Could you run your conversion code with the the tf-nightly version? We've made some progress in better supporting on saved model format in the upcoming TF 2.4.0 version.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43045\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43045\">No</a>\n"]}, {"number": 43044, "title": "TF_TensorToPyArray is hard coded for equality, limits performance enhancements", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  Yes, modified TF code with overriding existing Op \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 18.04):  4.15.0-115-generic\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  x86_64 24 core processor\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 1.15\r\n- Python version: Python 3.7.6\r\n- Bazel version (if compiling from source): Build label: 0.26.1\r\n- GCC/Compiler version (if compiling from source): gcc version 7.5.0\r\n- CUDA/cuDNN version: Not using\r\n- GPU model and memory: Not using\r\n\r\n**Describe the current behavior**\r\nI am running one CNN model(which is sequential and similar to VGGNET) which has convolution and FC layers. \r\nFor getting optimal performance, I am using o/p buffers which are allocated with allocate_persistent() API and I am reusing these buffers multiple times as next layer i/p ans new o/p as Network is pretty sequential.\r\nBasically i am having two buffer, using it like ping pong model (keep switching b/w those in alternate way). This way making sure that buffer will always be cached in memory. \r\nFor achieving this i am pre-allocating big chunks with allocate_persistent() API and for every Op execution i use the same buffers by setting with different shape.\r\n\r\nWith convolutions it worked fine and i see some performance gains too . However once FC layer gets executed it throws below exception. Because of this exception, I am not able to run my network.\r\n\r\n_INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InternalError'>, ndarray was 1921920 bytes but TF_Tensor was 12331253760 bytes\r\nI0907 13:07:13.556839 140574226294592 coordinator.py:224] Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InternalError'>, ndarray was 1921920 bytes but TF_Tensor was 12331253760 bytes_\r\n\r\nAfter looking into code, I found function TF_TensorToPyArray() gets called, _line 471 in tensorflow/python/lib/core/ndarray_tensor.cc_, which compare the ndarray bytes with TF_tensor bytes, which is fine, But why does it need to be exact match?\r\nIt should have been below comparison, instead of exact match. If i do below change it works fine and i get the right results too.\r\nPyArray_NBYTES(py_array)) >\r\nTF_TensorByteSize(tensor.get()\r\n\r\nThis way same buffers can be used for different Op execution(in sequential case), just we need to change the shape every time.\r\n\r\n**My pseudo code:**\r\n// below global variables will be allocated with allocate_persistent() API\r\n//allocated with max buffer shape\r\nTensorOut1\r\nTensorOut2 \r\n\r\n//Overrided compute() kernel for these Ops\r\nconvolutionOp1(input1, TensorOut1)   //with set_shape(which is public now) i am changing to required shape for this conv o/p     \r\nconvolutionOp2(TensorOut1, TensorOut2) // same as above comment\r\nconvolutionOp3(TensorOut2, TensorOut1) // same as above comment\r\nconvolutionOp4(TensorOut1, TensorOut2) //same as above comment\r\n.\r\n.\r\nfullyConnected(TensorOut1, TensorOut2)  \r\n\r\nOnce n/w is fully executed, I get below error. \r\nReason is Output of FC is 2D shape(changed from set_shape). But no. of bytes allocated is more, as we are working with pre allocated buffers. \r\n\r\nINFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InternalError'>, ndarray was 1921920 bytes but TF_Tensor was 12331253760 bytes\r\nI0907 13:07:13.556839 140574226294592 coordinator.py:224] Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InternalError'>, ndarray was 1921920 bytes but TF_Tensor was 12331253760 bytes\r\n\r\n\r\n**Describe the expected behavior**\r\n_I had to do two below changes in existing code_\r\n1. Make set_shape(const TensorShape& shape) to public in tensorflow/core/framework/tensor.h\r\n2. Change below line in tensorflow/python/lib/core/ndarray_tensor.cc ate line 471\r\nexisting:  } else if (static_cast<size_t>(PyArray_NBYTES(py_array)) !=\r\nnew:  } else if (static_cast<size_t>(PyArray_NBYTES(py_array)) >\r\n \r\nWith these i see expected gain in performance and correctness too. I am looking for some alternative to #1 \r\nwhile #2 limits my use case, why are we comparing for equality? above suggested alternative will give this kind of flexibility to TF users\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@avinashcpandey \r\nPlease share simple stand alone code for us to replicate issue faced, or if possible share a colab gist with the error reported.\r\nCan you please try with 2.x versions and let us know if you still face any performance issue.", "Hi @Saduf2019 \r\n\r\nIts difficult for me to share the code. As I described my logic above in **current behavior and pseudo code** section, what we are trying to achieve. I need below information, so that i can do cleaner implementation.\r\n\r\nBasically i am doing below two things for \r\n_1. Making set_shape(const TensorShape& shape) to **public** scope  in tensorflow/core/framework/tensor.h\r\n2. Change below line in tensorflow/python/lib/core/ndarray_tensor.cc ate line 471\r\nexisting: } else if (static_cast<size_t>(PyArray_NBYTES(py_array)) !=\r\nnew: } else if (static_cast<size_t>(PyArray_NBYTES(py_array)) >_\r\n\r\nI am looking for some alternative to #1 (so that i can modify the shape as per my need)\r\nwhile #2 limits my use case, \r\nQuestion is why are we comparing for equality of bytes? why not just shape? \r\nAbove suggested alternative will give this kind of flexibility to TF users to work with PerAllocated buffer coming from application and compare only the shape not the actual underlying bytes.  Bytes should not less than the use case, greater should be fine. \r\n\r\nThanks in Advance. ", "Hi @jvishnuvardhan @penpornk ,\r\nDo we have any update on this issue?\r\n", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information."]}, {"number": 43043, "title": "GradientTape.batch_jacobian returns the wrong type in case of constant functions", "body": "**System information**\r\nGoogle Colaboratory with TF 2.3.0, git version v2.3.0-0-gb36436b087\r\n\r\n**Describe the current behavior**\r\ntf.GradientTape.batch_jacobian of a constant function with return type float64 returns a zero matrix of type float32\r\n\r\n**Describe the expected behavior**\r\nreturn a zero matrix of type float64\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\n@tf.function\r\ndef foo(val):\r\n  return tf.constant([[3.0],[4.0]],dtype=tf.dtypes.float64)\r\n\r\nc = tf.constant([[1.],[2.]],dtype=tf.dtypes.float64)\r\nwith tf.GradientTape() as g:\r\n\t\t\tg.watch(c)\r\n\t\t\tf = foo(c)\r\n\r\nprint(g.batch_jacobian(f,c)) \r\n```\r\n\r\n**Other info / logs**\r\n\r\nWorkaround: setting the option unconnected_gradients to 'zero'\r\n```\r\n@tf.function\r\ndef foo(val):\r\n  return tf.constant([[3.0],[4.0]],dtype=tf.dtypes.float64)\r\n\r\nc = tf.constant([[1.],[2.]],dtype=tf.dtypes.float64)\r\nwith tf.GradientTape() as g:\r\n\t\t\tg.watch(c)\r\n\t\t\tf = foo(c)\r\n\r\nprint(g.batch_jacobian(f,c,unconnected_gradients='zero'))\r\n```", "comments": ["I have tried in colab with TF versions 2.3, nightly versions(`2.4.0-dev20200908`) and was able to reproduce the issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/35d2bc5bc15e34e22c1ce942dbb92918/untitled324.ipynb). Thanks!", "If you let interact the watched `c` with `f` it will return the correct dtype:\r\n\r\n```\r\nwith tf.GradientTape() as g:\r\n      c = tf.constant([[3.],[2.]],dtype=tf.dtypes.float64)\r\n      g.watch(c)\r\n      f = foo()*c\r\n      print(f)\r\nprint(c)\r\nprint(g.batch_jacobian(f,c))\r\n```", "@SimonWeitzhofer Can you please check @bhack suggestion and let us know whether the issue resolved for you. Thanks!", "The function f in @bhack suggestion is not constant anymore. So, no, if the suggestion is not to use constant functions, then I don't think this is a proper solution.\r\n\r\nActually there is a workaround, namely setting `unconnected_gradients='zero'` as described in the bug-report.\r\n\r\nI would be ok with tensorflow returning None or throwing an exception. Returning a float32 tensor, however, seems wrong to me.", "Mine was not a solution it was just to check that when it is not constant anymore the dtype is working correctly.\r\nYou problem is related that in the default case `unconnect_gradients='null'` we are not handling any specific `dtype` on the source.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/31f0b2159751642a146d70561ac5095a3c4722ff/tensorflow/python/eager/pywrap_tfe_src.cc#L2804-L2814\r\n\r\n/cc @edloper ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43043\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43043\">No</a>\n"]}]