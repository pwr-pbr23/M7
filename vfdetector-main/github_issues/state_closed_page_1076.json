[{"number": 20994, "title": "can not build tensoflow inference library", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nno\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nlinux ubuntu 16.04 virtual machine \r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\ndeeplearning@deep-learning-virtual-machine:~/Downloads/tensorflow-master$ /home/deeplearning/.bazel/bin/bazel build -c opt --jobs 1 --local_resources 5000,1.0,1.0 //tensorflow/contrib/android:libtensorflow_inference.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a --verbose_failures\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:avgpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:bounds_check.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops_common.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops_gradients.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_activations.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_attention.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_backward_cuboid_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_backward_spatial_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_cuboid_convolution.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_pooling.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_softmax.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_spatial_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_volume_patch.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:fifo_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:maxpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:ops_util.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:ops_util.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:padding_fifo_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:pooling_ops_common.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:pooling_ops_common.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:queue_base.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:queue_op.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:typed_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_entry.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_scorer.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_search.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_decoder.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_loss_util.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:naming.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:naming.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler:devices.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler:devices.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler:grappler_item.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler:utils.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler:utils.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler/clusters:cluster.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler/clusters:cluster.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler/inputs:utils.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler/inputs:utils.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler/optimizers:graph_optimizer.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler/optimizers:graph_rewriter.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler/optimizers:graph_rewriter.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler/optimizers:layout_optimizer.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler/optimizers:layout_optimizer.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler/optimizers:meta_optimizer.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler/optimizers:meta_optimizer.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler/optimizers:model_pruner.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler/optimizers:model_pruner.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nINFO: Found 1 target...\r\nINFO: From Compiling external/protobuf/src/google/protobuf/io/coded_stream.cc:\r\nexternal/protobuf/src/google/protobuf/io/coded_stream.cc: In member function 'google::protobuf::int64 google::protobuf::io::CodedInputStream::ReadVarint32Fallback(google::protobuf::uint32)':\r\nexternal/protobuf/src/google/protobuf/io/coded_stream.cc:444:12: warning: 'temp' may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     return temp;\r\n            ^\r\nINFO: From ProtoCompile tensorflow/core/example/example.pb.cc:\r\nbazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-py3-opt/genfiles/external/protobuf/src: warning: directory does not exist.\r\nERROR: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:887:1: C++ compilation of rule '//tensorflow/core:android_tensorflow_lib_lite' failed (Exit 1): arm-linux-androideabi-gcc failed: error executing command \r\n  (cd /home/deeplearning/.cache/bazel/_bazel_deeplearning/c1eb141ed59057a0511bd96e3345b74f/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/home/deeplearning/bin:/home/deeplearning/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/deeplearning/bin \\\r\n    PWD=/proc/self/cwd \\\r\n  external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-gcc -fstack-protector-strong -fpic -ffunction-sections -funwind-tables -no-canonical-prefixes -fno-canonical-system-headers '-march=armv7-a' '-mfpu=vfpv3-d16' '-mfloat-abi=softfp' -mthumb -Os -g -DNDEBUG -MD -MF bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-py3-opt/bin/tensorflow/core/_objs/android_tensorflow_lib_lite/tensorflow/core/common_runtime/function.d '-frandom-seed=bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-py3-opt/bin/tensorflow/core/_objs/android_tensorflow_lib_lite/tensorflow/core/common_runtime/function.o' -DEIGEN_MPL2_ONLY -iquote . -iquote bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-py3-opt/genfiles -iquote external/protobuf -iquote bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-py3-opt/genfiles/external/protobuf -iquote external/bazel_tools -iquote bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-py3-opt/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-py3-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-py3-opt/genfiles/external/local_config_sycl -isystem external/protobuf/src -isystem bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-py3-opt/genfiles/external/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/eigen_archive -isystem bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-py3-opt/genfiles/external/eigen_archive -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-mfpu=neon' '-std=c++11' -DTF_LEAN_BINARY -O2 -Os '--sysroot=external/androidndk/ndk/platforms/android-14/arch-arm' -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/backward -c tensorflow/core/common_runtime/function.cc -o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-py3-opt/bin/tensorflow/core/_objs/android_tensorflow_lib_lite/tensorflow/core/common_runtime/function.o)deeplearning@deep-learning-virtual-machine:~/Downloads/tensorflow-master$ \r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "@shivaniag thank you for the response but now i am using tensorflow dependency in android studio for inference. Can you please guide me that am i going in right direction or not? Using .jar and .so files is necessary or adding the dependency can do the same for me? I am a newbie for deep learning and tensorflow", "You can use an aar of tflite to do mobile inference. Please look at the tflite documentation https://www.tensorflow.org/mobile/tflite/", "@rabiaqayyum: take a look at the docs that Andrew pointed to and let me know if you have questions (and we can improve the documentation to better serve users like yourself).\r\nThere are sample Android demos that use the aar for on-device inference for different pre-tested models, and if you're looking to convert your models, the Developer guide will help (note that as mentioned in the docs, there isn't full op coverage yet).", "Nagging Assignees @aselle, @alanchiao: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 20993, "title": "Update fold_old_batch_norms.cc", "body": "Fixed my previous fix, we only want to copy if there is a data_format attribute. ", "comments": ["@mingxingtan Fixed #17602 which incorrectly only copied when there was not an attribute."]}, {"number": 20992, "title": "CheckpointSaverListener before_save", "body": "code:\r\n```\r\n--->listener\r\n\r\n listeners = [\r\n            EvalListener(estimator,\r\n                         lambda: input.input_fn(mode=tf.estimator.ModeKeys.EVAL, params=params,\r\n                                                data_path=params.eval_dir))\r\n        ]\r\n\r\n--->train \r\n        estimator.train(input_fn=train_input_fn, max_steps=FLAGS.max_steps, saving_listeners=listeners)\r\n\r\nclass EvalListener(CheckpointSaverListener):\r\n    def __init__(self, parent, input_fn, name='eval_data'):\r\n\r\n        self.parent = parent\r\n        self.input_fn = input_fn\r\n        self.name = name\r\n        self.history = {}\r\n---> before save\r\n    def before_save(self, session, global_step_value):\r\n        accuracy_spec = self.parent.evaluate(input_fn=self.input_fn, name=self.name)\r\n        for k, v in sorted(six.iteritems(accuracy_spec)):\r\n            if k == ACCU_HEAD:\r\n                if v >= self.history[k][1]:\r\n                    #   TODO\r\n                    #   if this time is better than last time, the model will be save\r\n                    print()\r\n```\r\ni'd wonder whether i can save the model when the accuracy score is better than last, if not the model will not be saved!", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "sorry, \r\nubuntu OS\r\ntensorflow gpu 1.8\r\ni wonder know whether i can stop the model to be saved if current accuracy is not better than last time, what i can do in the CheckpointSaverListener's before_save function.", "AFAIK you can't use the hook to save. You could probably override the Saver object, but probably what you want to do is to modify the outer scope calling the saving.", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 20991, "title": "NotFoundError with tf.add_check_numerics_ops()", "body": "I worked on Tensorflow 1.8, and I used tf.add_check_numerics_ops() to check my model.\r\nBut it fails if I add tf.add_check_numerics_ops().\r\n```\r\nNotFoundError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to find any matching files for model\r\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\r\n```\r\n\r\nmy code is here:\r\n```\r\nwith tf.Session() as sess:\r\n    logits, end_points = deeplab(inputs, reuse=False, is_training=False)\r\n    sess.run(tf.global_variables_initializer())\r\n    saver = tf.train.Saver()\r\n    print 'Model established'\r\n    saver.restore(sess, './models/xception65/model.ckpt')\r\n    print 'Weight Loaded'\r\n    checker = tf.add_check_numerics_ops()\r\n    checker, logits_v = sess.run([checker, logits], feed_dict={inputs: img})\r\n```\r\n\r\nIf I remove `checker` and `tf.add_check_numerics_ops()`, my code works fine.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Sorry for lacking information.\r\nOS : Ubuntu 14.04\r\nTensorflow: 1.8 installed from pip.\r\nCUDA: 9\r\nGPU model: GTX 1060 6GB\r\nAnd as for the function `deeplab`, it's a warpper I wrote for deeplabv3+, which is available in tensorflow/models, using `slim` to establish model.", "Interesting, does look like `add_check_numerics_ops` is messing with restore ops. For the moment please run `tf.add_check_numerics_ops()` before constructing the `tf.train.Saver()`.\r\n\r\nShort reproduction (or at least a similar error):\r\n```\r\nimport tensorflow as tf\r\n\r\nv = tf.get_variable(shape=[], name=\"v\")\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    saver = tf.train.Saver()\r\n    checker = tf.add_check_numerics_ops()  # Works if swapped with the previous line\r\n    saver.save(sess, '/tmp/model.ckpt')\r\n    saver.restore(sess, '/tmp/model.ckpt')\r\n    print(sess.run([v, checker]))\r\n```", "Thanks for your help! I'll try to run check ops before constructing Saver. ", "Nagging Assignee @allenlavoie: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 20990, "title": "Convert_variables_to_constants return None after import", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes, minimal example below\r\n- **OS Platform and Distribution**:Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: using pip3\r\n- **TensorFlow version (use command below)**: git version: v1.8.0-0-g93bc2e2072 tf version 1.8.0\r\n- **Python version**: 3.5.2\r\n- **CUDA/cuDNN version**: not using (because of old GPU)\r\n- **GPU model and memory**: geforce 265\r\n- **Exact command to reproduce**:\r\n1. Convert a graph using tf.graph_util.convert_variables_to_constants\r\n2. Import resulting graph using tf.import_graph_def\r\n3. Eval the variables you want\r\n(4. you get \"None\" on eval)\r\n\r\n### The problem\r\nAfter converting variables to constant\r\nImporting the graph_def results in a graph that always eval to None\r\n\r\n```\r\n# minimal testing\r\nwith tf.Graph().as_default() as graphy:\r\n    with tf.Session() as sessy:\r\n        # declaring placeholder\r\n        place_holder = tf.placeholder(dtype=tf.float32,shape=[3])\r\n        tmp_feed = {place_holder:[1,1,1]}\r\n        \r\n        # declaring vars and outputs\r\n        rand = tf.random_uniform([3])\r\n        the_var = tf.Variable(rand,name=\"the_var\") # depend only on var\r\n        out = tf.add(the_var,place_holder, name=\"the_result_node\") # depend on var and placeholder\r\n        out2 = tf.multiply(place_holder,2.0, name=\"the_place_holder\") # depend only on placeholder\r\n        \r\n        # getting names of outputs & vars\r\n        out_tensor = [the_var,out,out2]\r\n        out_names = [t.name.split(\":\")[0] for t in out_tensor]\r\n        var_names = [place_holder.name.split(\":\")[0]] # setting place_holder as blacklist as it must not be replaced by a constant ( doing this or not doesn't change result)\r\n        \r\n        # doing a first run to get a run comparison \r\n        sessy.run(tf.global_variables_initializer())\r\n        for t in out_tensor:\r\n            print(\"first eval\",t.name,sessy.run(t, feed_dict=tmp_feed))\r\n        \r\n        # freezing the graph\r\n        frozen_graphy = tf.graph_util.convert_variables_to_constants(\r\n                                                sess=sessy,\r\n                                                 input_graph_def=graphy.as_graph_def(),\r\n                                                 output_node_names=out_names,\r\n                                                 variable_names_blacklist=var_names)\r\n        print(\"freezing done, exported:\", out_names)\r\n        \r\n        # session has been closed seemingly by the convert\r\n            \r\n    # eval to check if the graph was actually modified. < return different values >\r\n    with tf.Session(graph=graphy) as sessu:\r\n        sessu.run(tf.global_variables_initializer())\r\n        for t in out_tensor:\r\n            print(\"second eval of graph1\",t.name,sessu.run(t, feed_dict=tmp_feed))\r\n        \r\n\r\n#print(frozen_graphy) # just checking what it look like\r\n\r\nprefixe = \"frozen_import\"\r\nwith tf.Graph().as_default() as grapho:\r\n    with tf.Session() as sesso:\r\n        print(\"importing frozen graph vars:\", out_names)\r\n        out_list = tf.import_graph_def(frozen_graphy,return_elements=out_names,name=prefixe)\r\n        print(\"outlist:\",out_list)\r\n        \r\n        # creating the feed dict for the placeholder\r\n        input_name = prefixe+\"/\"+var_names[0]\r\n        frozen_feed_dict={input_name+\":0\":[1,1,1]}\r\n        \r\n        # second eval to compare < return None >\r\n        sesso.run(tf.global_variables_initializer())\r\n        for t in out_list:\r\n            print(t.name,\" =\",sesso.run(t,feed_dict=frozen_feed_dict))\r\n            \r\n        # eval of placeholder: < return its value >\r\n        p_h = grapho.get_tensor_by_name(input_name+\":0\")\r\n        print(p_h.name, sesso.run(p_h, feed_dict=frozen_feed_dict))\r\n```\r\n", "comments": ["And I checked a bit the output graph_def a bit, it does seem to hold the values of the variables:\r\n```\r\nnode {\r\n  name: \"the_var\"\r\n  op: \"Const\"\r\n  attr {\r\n    key: \"dtype\"\r\n    value {\r\n      type: DT_FLOAT\r\n    }\r\n  }\r\n  attr {\r\n    key: \"value\"\r\n    value {\r\n      tensor {\r\n        dtype: DT_FLOAT\r\n        tensor_shape {\r\n          dim {\r\n            size: 3\r\n          }\r\n        }\r\n        tensor_content: \"\\344\\336:?<\\306\\223>4\\231@?\"\r\n      }\r\n    }\r\n  }\r\n}\r\n```", "@svsgoogle: Can you take a look at this?", "Ok, it seems after a lot a trying i found the issue:\r\nThe import of the frozen graph return **Nodes** and not tensor.\r\nSo in order to get a result out by running it, you must pass a `node.values()`  to the `sess.run()`.\r\n\r\nThis seems to be due to the fact that the converter takes node as parameter for outputs. I use `split(:)[0]` to remove the index ouput to turn `Tensor name` into `Node name`. And the import return `Operation` and not `Tensor`.\r\n\r\nAs said by the doc of Session.run():\r\n\r\n```\r\nrun(\r\n    fetches,\r\n    feed_dict=None,\r\n    options=None,\r\n   run_metadata=None\r\n)\r\n```\r\n> The fetches argument may be a single graph element, or an arbitrarily nested list, tuple, namedtuple, dict, or OrderedDict containing graph elements at its leaves. A graph element can be one of the following types:\r\n> \r\n>     An tf.Operation. The corresponding fetched value will be None. [...]\r\n\r\nI don't quite understand the point of using such way of handling `operation` by tensorflow. As not raising an exception and returning `None` mislead debug attempts..."]}, {"number": 20989, "title": "Whether tensorflow supports image training or retraining on c++ API", "body": "Tensorflow has supported image training or retraining on python\r\nhttps://www.tensorflow.org/hub/tutorials/image_retraining\r\nbut that tutorials doesn't have any tutorials about  training or retraining on c++ API.\r\nAnd here are very few discussions about this.\r\nWhether tensorflow supports image training or retraining on c++ API?\r\nTHX\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 20988, "title": "[tflite label_image] get output size from output tensor ", "body": "1. get output size from the output tensor\r\n2. add a command line option to specify number of results\r\n\r\naddress the issue https://github.com/tensorflow/tensorflow/issues/20485 found by @jbuisson1", "comments": ["Nagging Assignee @rmlarsen: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 20987, "title": "install tf 1.4 or up in Virtualenv still suffers this problem, uninstall enum and install enum34 do not help.", "body": "", "comments": ["@csjunxu can you please elaborate on your issue? Please file out the template above and comment with any additional information.", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 20986, "title": "tf.data.Iterator has_next(), next(), feature request", "body": "Request:\r\nhas_next()  // opt that check if next data is present at graph execution time without resorting to python try/except\r\nnext()  // op that advance the iterator by one at graph execution time\r\n\r\nReason:\r\nhas_next():\r\nIterator seems to be designed for supervised learning. Data is fed from one end, OutOfRangeError occurs when all the data has been used and training stops (if repeat is not set).\r\n\r\nIn reinforcement learning, the end of episode (last example in file) can be a significant source of information during graph execution time. Catch the exception and run an additional ad-hoc training cycle for the last example is aweful.\r\n\r\nnext():\r\nIn RL, the data can be sequential and has specific time stamps. During graph execution time, the environment written in tensorflow should be able to advance the Iterator using tf.while_loop with iterator.next() until some condition is met \r\n\r\nfor example, advance Iterator in a tf.while_loop until time stamp associated with that example is greater than some internal state (variable).\r\n\r\nPerhaps the get_next() method should be parameter such that the node can be optionally required to re-executed after every visit. or just have a next() method and let us figure out the appropriate dependency control.\r\n\r\nHave I written custom code: Yes\r\nOS Platform and Distribution: Ubuntu 16.04 LTS\r\nTensorFlow installed from: pip\r\nTensorFlow version: 1.8\r\nBazel version: 0.15.2\r\nCUDA/cuDNN version: Cuda 9.0, cuDNN v7\r\nGPU model and memory: GTX 1070 8GB\r\nExact command to reproduce: N/A\r\nMobile device: N/A", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "I recently added `tf.contrib.data.get_next_as_optional()` (9811b72df7347a4ec2ee884a2165775340d70e75), which makes it possible to consume values from an iterator in a `tf.while_loop()` without raising an exception at the end of the sequence. This should make it possible to implement your RL use case.\r\n\r\nAs I commented on #20255, we have no plans to add `Iterator.has_next()`, but a combination of `tf.contrib.data.get_next_as_optional()` and local variables should suffice to implement your use case."]}, {"number": 20985, "title": "Update saver.py", "body": "Fix device placement of save_op for ResourceVariable.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@agarwal-ashish This touches code that you modified recently, so could you please take a look?"]}, {"number": 20984, "title": "Can't convert .pb file to .lite, \"toco: error: argument --output_file is required\"", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04 virtual machine that is run on legitimate Windows 10 OS\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: github.com/googlecodelabs/tensorflow-for-poets-2\r\n- **TensorFlow version (use command below)**: 1.9.0 (also tested 1.5.0, and 1.8.0)\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: 0.15.2 \r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A? the tf_env_collect.sh program didnt pull any information on this\r\n- **GPU model and memory**:Rx480 with 4gb of ram\r\n- **Exact command to reproduce**: \r\n/home/(user name)/.local/bin/toco \\\r\n \u2014 input_file=tf_files/retrained_graph.pb \\\r\n \u2014 output_file=tf_files/optimized_graph.lite \\\r\n \u2014 input_format=TENSORFLOW_GRAPHDEF \\\r\n \u2014 output_format=TFLITE \\\r\n \u2014 input_shape=1,224,224,3 \\\r\n \u2014 input_array=input \\\r\n \u2014 output_array=final_result \\\r\n \u2014 inference_type=FLOAT \\\r\n \u2014 input_data_type=FLOAT\r\n\r\ntaken from THIS tutorial:\r\nhttps://heartbeat.fritz.ai/working-through-a-tensorflow-lite-tutorial-on-windows-10-e27ee0e8b8cc\r\n\r\n(I followed the tutorial pretty much word for word, virtual ubuntu machine and everything since TOCO never worked originally on my windows machine.)\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.git\r\n\r\nWhen I executed the command above after following the directions of that tutorial, originally with tensorflow 1.5.0 installed, the command failes saying \r\n\"Check failed: parsed_toco_flags.input_format.specified() Missing required flag: input_format\r\nAborted (core dumped)\"\r\nI'm a novice in tensor flow programming but doesn't the command specify input_format?\r\n\r\n\r\nI looked on other github issues with a similar problem and the experts suggested upgrading tensorflow to 1.8.0 which only results in issues like missing module attributes or import errors. Its probably much more likely that I'm doing something wrong rather than there being a bug with tensorflow, but I'm just so confused as to what mistake I made; I need some expert help. \r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nv. 1.5.0 \r\nubuntu@ubuntu:~$ /home/ubuntu/.local/bin/toco \\\u2014 input_file=tf_files/retrained_graph.pb \\\u2014 output_file=tf_files/optimized_graph.lite \\\u2014 input_format=TENSORFLOW_GRAPHDEF \\\u2014 output_format=TFLITE \\\u2014 input_shape=1,224,224,3 \\\u2014 input_array=input \\\u2014 output_array=final_result \\\u2014 inference_type=FLOAT \\\u2014 input_data_type=FLOAT\r\n2018-07-20 01:55:55.974573: F tensorflow/contrib/lite/toco/toco_cmdline_flags.cc:165] Check failed: parsed_toco_flags.input_format.specified() Missing required flag: input_format\r\nAborted (core dumped)\r\n\r\nv. 1.8.0\r\nubuntu@ubuntu:~$ /home/ubuntu/.local/bin/toco \\\u2014 input_file=tf_files/retrained_graph.pb \\\u2014 output_file=tf_files/optimized_graph.lite \\\u2014 input_format=TENSORFLOW_GRAPHDEF \\\u2014 output_format=TFLITE \\\u2014 input_shape=1,224,224,3 \\\u2014 input_array=input \\\u2014 output_array=final_result \\\u2014 inference_type=FLOAT \\\u2014 input_data_type=FLOAT\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/.local/bin/toco\", line 7, in <module>\r\n    from tensorflow.contrib.lite.toco.python.toco_wrapper import main\r\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 63, in <module>\r\n    from tensorflow.python.framework.framework_lib import *  # pylint: disable=redefined-builtin\r\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/framework_lib.py\", line 104, in <module>\r\n    from tensorflow.python.framework.importer import import_graph_def\r\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\", line 32, in <module>\r\n    from tensorflow.python.framework import function\r\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 36, in <module>\r\n    from tensorflow.python.ops import resource_variable_ops\r\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 35, in <module>\r\n    from tensorflow.python.ops import variables\r\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 40, in <module>\r\n    class Variable(checkpointable.CheckpointableBase):\r\nAttributeError: module 'tensorflow.python.training.checkpointable' has no attribute 'CheckpointableBase'\r\n\r\nv. 1.9.0\r\nubuntu@ubuntu:~$ /home/ubuntu/.local/bin/toco \\\u2014 input_file=tf_files/retrained_graph.pb \\\u2014 output_file=tf_files/optimized_graph.lite \\\u2014 input_format=TENSORFLOW_GRAPHDEF \\\u2014 output_format=TFLITE \\\u2014 input_shape=1,224,224,3 \\\u2014 input_array=input \\\u2014 output_array=final_result \\\u2014 inference_type=FLOAT \\\u2014 input_data_type=FLOAT\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/.local/bin/toco\", line 7, in <module>\r\n    from tensorflow.contrib.lite.python.tflite_convert import main\r\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 81, in <module>\r\n    from tensorflow.python import keras\r\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/keras/__init__.py\", line 24, in <module>\r\n    from tensorflow.python.keras import activations\r\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/keras/activations/__init__.py\", line 22, in <module>\r\n    from tensorflow.python.keras._impl.keras.activations import elu\r\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/__init__.py\", line 21, in <module>\r\n    from tensorflow.python.keras._impl.keras import activations\r\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/activations.py\", line 23, in <module>\r\n    from tensorflow.python.keras._impl.keras import backend as K\r\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/backend.py\", line 38, in <module>\r\n    from tensorflow.python.layers import base as tf_base_layers\r\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 25, in <module>\r\n    from tensorflow.python.keras.engine import base_layer\r\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/__init__.py\", line 21, in <module>\r\n    from tensorflow.python.keras.engine.base_layer import InputSpec\r\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 33, in <module>\r\n    from tensorflow.python.keras import backend\r\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/keras/backend/__init__.py\", line 22, in <module>\r\n    from tensorflow.python.keras._impl.keras.backend import abs\r\nImportError: cannot import name 'abs'", "comments": ["**TensorFlow 1.5**\r\nIt seems like copy and paste caused a few issues. Replace the `\u2014` with two dashes (`--`) and make sure there is no space between the dashes and the name of the flag (ex. `--input_name` instead of `\u2014 input_array`).\r\n\r\n**TensorFlow 1.8**\r\n`toco` is not a supported command in 1.8.\r\n\r\n**TensorFlow 1.9**\r\nIt seems there is another thread describing this issue ([issue/46](https://github.com/tensorflow/probability/issues/46)). That thread suggests the following steps:\r\n1. Uninstall `tensorflow`\r\n2. Uninstall `protobuf`\r\n3. Reinstall `tensorflow`", "Nagging Assignees @gargn, @andrehentz: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 20983, "title": "[Bug] tf.py_func : Bad tensor -> np.array conversion in py_func (return [] with dtype=int64 is returned as dtype=float64) ", "body": "### Issue template:\r\n\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes (minimal example attached inline)\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OSX 10.12.6\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: Probably\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: ('v1.5.0-0-g37aa430d84', '1.5.0')\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: `python py_func_failure.py`\r\n\r\n### Describe the problem\r\n\r\nThis bug occurs when a `tf.py_func` returns an empty python list `[]` which is intended to be a tensor of type `tf.int64` (as defined in the `Tout=` of the `py_func`). \r\n\r\nIn the `ops.script_ops.py` the `_convert` static method is unable to tell the `numpy dtype` of the incoming tensor when it is an empty list.  This causes the `_convert` method to execute a `np.asarray([], dtype=None, order=\"C\")` which gives us a `array([], dtype=float64)` instead of a `array([], dtype=int64)` which then does not agree with the output tensor description in the py_func. \r\n\r\n### Source code / logs\r\n\r\nSee gist here for the example that reproduces this issue: https://gist.githubusercontent.com/sabhiram/3f5eaf7e566ef9aefb3ae6e5b8d2edb0/raw/182ab6bc72b5b7b13fd55964c72030cbc53f7cb3/py_func_failure.py\r\n\r\nInlined here:\r\n```\r\nimport tensorflow as tf\r\n\r\ndef test_func(x):\r\n    \"\"\" Builds a list of ints with length `x`.\r\n    \"\"\"\r\n    return [i for i in range(x)],\r\n\r\n\r\ndef main():\r\n    t0 = tf.constant(0, dtype=tf.int64)\r\n    t0 = tf.py_func(test_func, [t0], tf.int64)\r\n\r\n    t1 = tf.constant(1, dtype=tf.int64)\r\n    t1 = tf.py_func(test_func, [t1], tf.int64)\r\n\r\n    with tf.Session() as sess:\r\n        sess.run(t1)            # OK\r\n        sess.run(t0)            # 0-th value is double, expects int64\r\n\r\nif __name__ == \"__main__\":\r\n    print(tf.GIT_VERSION, tf.VERSION)\r\n    main()\r\n```", "comments": ["For what its worth, I think this would be easy enough to fix in the `core.ops.script_ops.py` file by keeping track of the tf output types when the py_func gets registered (similar to how `EagerFunc`s are stored). \r\n\r\nThen during graph exec time, after we have results in an array (or scalar) - we can query the Tout list and convert to ndarrays correctly.  I was able to crudely implement this just to unblock my use case.\r\n\r\nOne other w/a from the client side is to be explicit with the list in the py_func like so:\r\n```\r\ndef test_func(x):\r\n    arr = np.array([], dtype=np.int64)\r\n    for i in range(x):\r\n        arr = np.append(arr, i)\r\n    return arr,\r\n```\r\n", "Here is a branch that attempts to address this shortcoming: https://github.com/recogni/tensorflow/tree/dev-recogni/bug_20983", "Nagging Assignee @shivaniag: It has been 13 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Marking this as contribution welcome, could you submit a PR for this. Thanks.\r\n", "@shivaniag - PR submitted and attached to this about 16 days ago :)\r\n\r\nSee this PR that is awaiting merge once CI is good to go: https://github.com/tensorflow/tensorflow/pull/21038", "Thanks! "]}, {"number": 20982, "title": "Keras Tensorboard callback and eager execution", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: \r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Google Colab\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**: 1.9.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**: Colab GPUs\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nTo make it short, Keras's Tensorboard callback doesn't work in the eager execution mode.\r\n\r\n### Source code / logs\r\nRuntimeErrorTraceback (most recent call last)\r\n<ipython-input-4-95ebde3dc4e8> in <module>()\r\n     57 X = np.random.random((20, 10)).astype(np.float32)\r\n     58 Y = np.random.random((20, 5)).astype(np.float32)\r\n---> 59 model.fit(x={'input' : X}, y={'clustering' : Y}, batch_size=1, epochs=10, callbacks=[tensorboard])\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training.pyc in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\r\n   1331           initial_epoch=initial_epoch,\r\n   1332           steps_per_epoch=steps_per_epoch,\r\n-> 1333           validation_steps=validation_steps)\r\n   1334     else:\r\n   1335       return training_arrays.fit_loop(\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training_eager.pyc in fit_loop(model, inputs, targets, sample_weights, class_weight, val_inputs, val_targets, val_sample_weights, batch_size, epochs, verbose, callbacks, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\r\n    977       callback_model = model\r\n    978 \r\n--> 979     callbacks.set_model(callback_model)\r\n    980 \r\n    981     callbacks.set_params({\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/callbacks.pyc in set_model(self, model)\r\n     68   def set_model(self, model):\r\n     69     for callback in self.callbacks:\r\n---> 70       callback.set_model(model)\r\n     71 \r\n     72   def on_epoch_begin(self, epoch, logs=None):\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/callbacks.pyc in set_model(self, model)\r\n    761         if hasattr(layer, 'output'):\r\n    762           tf_summary.histogram('{}_out'.format(layer.name), layer.output)\r\n--> 763     self.merged = tf_summary.merge_all()\r\n    764 \r\n    765     if self.write_graph:\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/summary/summary.pyc in merge_all(key, scope)\r\n    309   if _context.executing_eagerly():\r\n    310     raise RuntimeError(\r\n--> 311         'Merging tf.summary.* ops is not compatible with eager execution. '\r\n    312         'Use tf.contrib.summary instead.')\r\n    313   summary_ops = _ops.get_collection(key, scope=scope)\r\n\r\nRuntimeError: Merging tf.summary.* ops is not compatible with eager execution. Use tf.contrib.summary instead.\r\n", "comments": [" \r\nDuplicate of #20372"]}, {"number": 20981, "title": "Link lib and header where ./configure expects", "body": "", "comments": ["Cherrypicking a fix made directly to 1.10 branch back to master", "@case540, thanks for cherry-pick. but there is still an issue on latest master branch.\r\n\r\nIn https://github.com/tensorflow/tensorflow/commit/95ec73a1c8b5174ec2221c2b5ecaf179c9deef48, libnccl2=2.2.13-1+cuda9.0 is installed, but libnccl-dev=2.2.13-1+cuda9.0 is not installed, so /usr/include/nccl.h is missing, which will cause configure.py failure.\r\n\r\n "]}, {"number": 20980, "title": "High loss of accuracy when using TFLite on Android", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Only on Android (modified the TFLiteClassifier.java class to output the results in a more convenient manner and commented the call to applyFilter() so the model doesn't base its detection on other images than the one I am giving to it)\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OSX 10.13.3\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: Samsung Galaxy S6\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: v1.8.0-0-g93bc2e2072 1.8.0\r\n- **Python version**: Python 3.6.0 :: Anaconda 4.3.1 (x86_64)\r\n- **Bazel version (if compiling from source)**: Build label: 0.14.1-homebrew\r\n- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 9.1.0 (clang-902.0.39.2)\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**:  NVIDIA GeForce GT 650M 1 GB\r\n- **Exact command to reproduce**: \r\n```\r\ncd ../tensorflow-for-poets-2/\r\npython -m scripts.retrain --bottleneck_dir=bottlenecks --how_many_training_steps=$STEPS --model_dir=/Users/jean-baptistebuisson/new_training_dir/model/ --summaries_dir=/Users/jean-baptistebuisson/new_training_dir/training_summaries/mobilenet_1.0_224 --output_graph=/Users/jean-baptistebuisson/new_training_dir/$NAME.pb --output_labels=/Users/jean-baptistebuisson/new_training_dir/retrained_labels.txt --architecture=mobilenet_1.0_224 --image_dir=/Users/jean-baptistebuisson/tf_files/tw --tfhub_module https://tfhub.dev/google/imagenet/mobilenet_v1_100_224/feature_vector/1\r\ncd ../new_training_dir/\r\ncd ../tensorflow/\r\nbazel-bin/tensorflow/contrib/lite/toco/toco --input_file=../new_training_dir/$NAME.pb --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --output_file=$NAME.tflite --inference_type=FLOAT --inference_input_type=FLOAT --input_arrays=input --output_array=final_result --input_shapes=1,224,224,3\r\ncd ../new_training_dir/\r\n```\r\n\r\n### Describe the problem\r\n\r\nThe above commands let me retrain a Mobilenet model with 2 categories (positive and negative) and outputs a .pb and a .tflite model that both work fine when tested with label_image with the same data (100 positive images and 100 negative):\r\nFor the .pb model:\r\n`True positive: 97; true negative: 98; false positive: 2; false negative: 3`\r\nFor the .tflite model:\r\n`True positive: 100; true negative: 100; false positive: 0; false negative: 0`\r\n\r\nThen I import the model on Android and run on the same images I get the following result:\r\n`True Positive: 100; True Negative: 0; False Positive: 100; False Negative: 0`\r\n\r\n\r\nI don't know what is messing with the detection but on Android everything is detected as positive.\r\nHere are the .pb and .tflite models I am using:\r\n[retrainedMNetV1_7_8000.pb.zip](https://github.com/tensorflow/tensorflow/files/2211815/retrainedMNetV1_7_8000.pb.zip)\r\n[retrainedMNetV1_7_8000.tflite.zip](https://github.com/tensorflow/tensorflow/files/2211816/retrainedMNetV1_7_8000.tflite.zip)\r\n\r\nAnd here is my modified TFLiteClassifier.java class:\r\n[TFLiteClassifier.java.zip](https://github.com/tensorflow/tensorflow/files/2211823/TFLiteClassifier.java.zip)\r\n\r\n", "comments": ["@jbuisson1 : How are you feeding images to the classifier?", "Here is the code where I feed an image to the classifier (f is the File of the current image to be fed)\r\n\r\n```\r\nBitmapFactory.Options options = new BitmapFactory.Options()\r\noptions.inSampleSize = 8;\r\nfinal Bitmap b = BitmapFactory.decodeFile(f.getAbsolutePath(), options);\r\nArrayList<TFLiteRecognition> res = new ArrayList<>();\r\ntry {\r\n    res = Globals.lightClassifier.classifyFrame(b);\r\n} catch (IllegalStateException e) {\r\n    Log.d(TAG, \"testImage: \" + e.getMessage());\r\n}\r\n```", "@jbuisson1 : My suspicion without looking at the images will be that images fed to classifier are not correct or not in the same format as during the training. You can try doing a sanity check of images that you are feeding to the classifier.\r\nTry dumping the images to sdcard after you do \r\nbitmap.getPixels(...) in convertBitmapToByteBuffer.  Check if they match with the images during the training phase after pre-processing.\r\nLet me know if that helps. You can also try running the non-TFLite model on Android and check if there is an accuracy loss. I suspect you will see a loss there as well, which would mean an error in the pre-processing phase.\r\n", "You were right, I was changing the image's size before feeding it to the classifier.\r\nThanks!", "Hi @jbuisson1. I face the same problem right now and I'm not sure what I'm supposed to do. In your case, do you mean you change the image size in the Android or the model training? Desperate for help...Thanks in advance.", "In case anyone has the same problem in the future, the solution for me was to make sure that all dimensions were the same - width, height, IMAGE_MEAN, and IMAGE_STD. \r\nIf you are using one of the tensorflow examples, you need to update all these values accordingly. Also make sure the image's size also matches the hardcoded width and height values.", "@DoritoDog I see that codelab example (flower classification) shows moblenetv2 training with 0-1 based pre-processing, but in android after tflite conversion in floating point model, they use -1, 1 based preprocessing.\r\nCould you please help by justifying or confirming that its wrong there."]}, {"number": 20979, "title": " Switching to Eigen version with HIP support. ", "body": "This PR does the following things.\r\n\r\n1. Update the Eigen commit pointer to the first Eigen version with HIP support\r\n\r\n2. Update TF codebase with changes necessitated by the API changes in the\r\n   Eigen version with HIP support. Basically some APIs names/ header files\r\n   were updated from *Cuda* to *Gpu*\r\n\r\n3. Resurrects the eigen patch file to workaround an eigen change (commit : 4af74f577a4fc09dcfa202064e1291038d2046da) which breaks the TF build\r\n   There is bug filed on Eigen to track this failure\r\n   http://eigen.tuxfamily.org/bz/show_bug.cgi?id=1569\r\n   A recent eigen commit fixes that bug. The eiegn patch file can be removed once we advance the eigen commit pointer to a commit that contains the fix", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "CLAs look good, thanks!\n\n<!-- ok -->", "@deven-amd unfortunately this PR cannot be accepted, since it would break TF internally at Google. The internal version of Eigen at Google has diverged from upstream and importing the HIP-related patches is not possible at the moment. We will have a better estimate by the end of next week for how long it will take to remedy this situation.", "@benoitsteiner @yifeif I am adding the \"ready to pull\" label in order to be able to pull this manually. DO NOT MERGE OR SUBMIT THE COPYBARA CL.", "@deven-amd It looks like you missed a Cuda reference here:\r\n\r\nINFO: From Compiling tensorflow/core/util/cuda_kernel_helper_test.cu.cc:\r\ntensorflow/core/util/cuda_kernel_helper_test.cu.cc(134): error: namespace \"Eigen\" has no member \"CudaStreamDevice\"\r\n1 error detected in the compilation of \"/tmpfs/tmp/tmpxft_0000386f_00000000-6_cuda_kernel_helper_test.cu.cpp1.ii\".\r\n\r\n\r\nI think this should be changed to GpuStreamDevice.", "@rmlarsen you are right.\r\n\r\nWhile I did run the eigen unit tests (CUDA and HIP) with these changes, for tesnorflow, only the HIP side was tested. That would explain why I missed the error in the CUDA test. Sorry about that, please apply the fix as need, and thank you.", "also for all the CI failures that are listed, please let me know if there is anything that needs to be done on my end. I tried looking at the \"Details\" of the failures. It lists the failing targets but not much more information beyond that ", "@deven-amd could you please pull rebase and push again?", "@drpngx \r\n\r\ndid the rebase and push. \r\n\r\nalso added the `CudaStreamDevice --> GpuStreamDevice` update in `tensorflow/core/util/cuda_kernel_helper_test.cu.cc`", "NOTICE: This PR cannot be accepted at the moment. We are working on bringing our internal version of Eigen in sync with upstream and will be able to accept this PR shortly, probably within a week or so.", "@rmlarsen ping?", "@deven-amd could you pull rebase and push again?", "> @deven-amd could you pull rebase and push again?\r\n\r\n@drpngx done.", "Removing the merge label. Waiting for @rmlarsen to confirm.", "@rmlarsen \r\n\r\nI see the following commit today, which I believe is a super-set of this PR\r\nhttps://github.com/tensorflow/tensorflow/commit/cf02d61a83bf14d553b81e1cee6ba604f05a7758\r\n\r\nPlease confirm, and if correct, we can close out this PR\r\n\r\ndeven\r\n", "Nagging Assignee @rmlarsen: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@deven-amd This PR is no longer necessary. TensorFlow was switched to a version of Eigen supporting HIP on November 7. We have done weekly incremental updates since and are now comfortable moving forward."]}, {"number": 20978, "title": "Can I use this code to compute the receptive field of a model which I have created on my own. I am talking about calculating receptive fields of not popular conv nets.", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 20977, "title": "Fix typo in install_python3.6_pip_packages.sh", "body": "", "comments": []}, {"number": 20976, "title": "map_fn removes a dimension", "body": "I am trying to remap values in an input Tensor using a defaultdict.\r\n\r\n    class MyDataSet(object):\r\n        def __init__(self):\r\n            self.class_map = MyDataSet.remap_class()\r\n\r\n        @staticmethod\r\n        def remap_class():\r\n            class_remap = defaultdict(lambda: 11)\r\n            class_remap[128] = 0  \r\n            class_remap[130] = 1  \r\n            class_remap[132] = 2\r\n            # ...\r\n\r\n        def parser(self, serialized_example):\r\n            features = tf.parse_single_example(\r\n                serialized_example,\r\n                features={\r\n                    'image': tf.FixedLenFeature([], tf.string),\r\n                    'label': tf.FixedLenFeature([], tf.string),\r\n                })\r\n            label = tf.decode_raw(features['label'], tf.uint8)\r\n            label.set_shape([256 * 512])\r\n            label = tf.cast(tf.reshape(label, [256, 512]), tf.int32)\r\n    \r\n            output_label = tf.map_fn(lambda x: self.class_map(x), label)\r\n\r\n        #...\r\n        dataset = tf.data.TFRecordDataset(filenames).repeat()\r\n        dataset = dataset.map(self.parser, num_parallel_calls=batch_size)\r\n\r\n\r\nThe label shape is (256,512) but the output_label shape is (256,).  I can't change this shape with tf.reshape() or output_label.set_shape().\r\n\r\nThis seems to be a bug. ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Isn't that expected? For each sample you get the class?", "This are for semantic segmentation which is pixel-based, not box or column based.", "OS Platform and Distribution: Ubuntu 17.10\r\n\r\nTensorFlow version: 1.8\r\n\r\nCUDA/cuDNN version: \r\nCuda compilation tools, release 9.0, V9.0.176 /\r\n#define CUDNN_MAJOR 7\r\n#define CUDNN_MINOR 0\r\n#define CUDNN_PATCHLEVEL 5\r\n \r\nGPU model and memory: NVIDIA Titan V,  Driver Version: 387.34, 12057MiB\r\n", "So the `class_map` takes `x, y` and returns one number? Or does it take `256 * x + y`?", "For each (x,y) class_map should take the value {0, 255} and translate it into another value {0, 255}", "Could you print what comes out of `class_map` to make sure that's a 2d tensor maybe?", "It may be 2D but the shape is (256,).  And it won't let me reshape it.  I'm now using a workaround which is flattening the tensor to 1D, doing the mapping and then reshaping it to 2D.", "Nagging Assignee @drpngx: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "OK closing since there is a resolution. If you're unhappy about `class_map`, feel free to open another PR.", "That is completely unacceptable as an answer.", "Hi @KevinLucidyne and @johnsrude, I didn't mean to be rude here. What I meant was: the problem is with `class_map`. If you think it is wrong and should be fixed, please open another PR. Thanks.", "@drpngx  A question on class_map would be exactly the same question.", "@KevinLucidyne that's fine."]}, {"number": 20975, "title": "Toco not working on Windows", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a\r\n- **TensorFlow installed from (source or binary)**: source, pip install --upgrade tf-nightly\r\n- **TensorFlow version (use command below)**: GIT: 'v1.9.0-rc2-798-gc818bf016d', VERSION: '1.10.0-dev20180719'\r\n- **Python version**: 3.6.6\r\n- **Bazel version (if compiling from source)**: \r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n### Python API\r\n`import tensorflow as tf\r\n\r\nconverter = tf.contrib.lite.TocoConverter.from_keras_model_file(\"model.h5\")\r\ntflite_model = converter.convert()\r\nopen(\"model.tflite\", \"wb\").write(tflite_model)`\r\nand \r\n### Command-line\r\n`toco`\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nI'm trying to convert a keras model to the tflite format, but the sample code provided on the website doesn't seem to work. As advised in my [other issue](https://github.com/tensorflow/tensorflow/issues/20826), I installed tf-nightly. I used a venv to get a clean environment but for some reason, other issues have arisen. In the case of the Python API, 'lite' seems to be missing from 'tensorflow.contrib' whereas when I run 'toco' from the command line, it raises a ModuleNotFoundError as shown below. Any help would be greatly appreciated.\r\n\r\n### Source code / logs\r\n### Python API\r\n`Traceback (most recent call last):\r\n  File \"sandbox/run.py\", line 3, in <module>\r\n    converter = tf.contrib.lite.TocoConverter.from_keras_model_file(\"model.h5\")\r\n  File \"C:\\beta\\lib\\site-packages\\tensorflow\\python\\util\\lazy_loader.py\", line 54, in __getattr__\r\n    return getattr(module, item)\r\nAttributeError: module 'tensorflow.contrib' has no attribute 'lite'`\r\n\r\n### Command-line (running toco)\r\n`Traceback (most recent call last):\r\n  File \"c:\\users\\user\\appdata\\local\\programs\\python\\python36\\Lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"c:\\users\\user\\appdata\\local\\programs\\python\\python36\\Lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\beta\\Scripts\\toco.exe\\__main__.py\", line 5, in <module>\r\nModuleNotFoundError: No module named 'tensorflow.contrib.lite.python.tflite_convert'`\r\n", "comments": ["Bump", "Yes, unfortunately, we do not currently support tooling on Windows. You can use docker to workaround this or a linux instance. We hope to fix this in the future.", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "The TOCO converter should compile properly on Windows now. We're still working to make sure it gets built along with the standard TensorFlow pip builds.", "The TOCO converter was not working for me on Windows 10. For those still having problems, I used the Windows Built-In Subsystem for Linux (good tutorial here https://www.youtube.com/watch?v=xzgwDbe7foQ) as I don't have knowledge of Docker or similar programs. This allowed me to use TOCO.", "@Qryptoc: Can you be a bit more specific about it not working? Did it fail to compile? Or did you hit a runtime failure?", "@jdduke something is wrong with Tensorflow_wrap_toco here.Im running on win 10 when I try to run a sample code:\r\n`import numpy as np\r\nimport tensorflow as tf\r\n\r\nmodel = tf.keras.models.Sequential()\r\nmodel.add(tf.keras.layers.Dense(2, input_shape=(3,)))\r\nmodel.add(tf.keras.layers.RepeatVector(3))\r\nmodel.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(3)))\r\nmodel.compile(loss=tf.keras.losses.MSE,optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),metrics=[tf.keras.metrics.categorical_accuracy],sample_weight_mode='temporal')\r\n\r\nx = np.random.random((1, 3))\r\ny = np.random.random((1, 3, 3))\r\nmodel.train_on_batch(x, y)\r\nmodel.predict(x)\r\n\r\n\r\nkeras_file = \"keras_model.h5\"\r\ntf.keras.models.save_model(model, keras_file)\r\n\r\nconverter = tf.contrib.lite.TocoConverter.from_keras_model_file(keras_file)\r\ntflite_model = converter.convert()\r\nopen(\"converted_model.tflite\", \"wb\").write(tflite_model)`\r\nand this popped up:\r\n\r\n> Traceback (most recent call last):\r\n  File \"E:\\adwqe.py\", line 22, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"C:\\Users\\thang\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\contrib\\lite\\python\\lite.py\", line 439, in convert\r\n    **converter_kwargs)\r\n  File \"C:\\Users\\thang\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\contrib\\lite\\python\\convert.py\", line 309, in toco_convert_impl\r\n    input_data.SerializeToString())\r\n  File \"C:\\Users\\thang\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\contrib\\lite\\python\\convert.py\", line 109, in toco_convert_protos\r\n    (stdout, stderr))\r\nRuntimeError: TOCO failed see console for info.\r\nb'Traceback (most recent call last):\\r\\n  File \"c:\\\\users\\\\thang\\\\appdata\\\\local\\\\programs\\\\python\\\\python35\\\\lib\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\tensorflow_wrap_toco.py\", line 18, in swig_import_helper\\r\\n    fp, pathname, description = imp.find_module(\\'_tensorflow_wrap_toco\\', [dirname(__file__)])\\r\\n  File \"c:\\\\users\\\\thang\\\\appdata\\\\local\\\\programs\\\\python\\\\python35\\\\lib\\\\imp.py\", line 296, in find_module\\r\\n    raise ImportError(_ERR_MSG.format(name), name=name)\\r\\nImportError: No module named \\'_tensorflow_wrap_toco\\'\\r\\n\\r\\nDuring handling of the above exception, another exception occurred:\\r\\n\\r\\nTraceback (most recent call last):\\r\\n  File \"c:\\\\users\\\\thang\\\\appdata\\\\local\\\\programs\\\\python\\\\python35\\\\lib\\\\runpy.py\", line 193, in _run_module_as_main\\r\\n    \"__main__\", mod_spec)\\r\\n  File \"c:\\\\users\\\\thang\\\\appdata\\\\local\\\\programs\\\\python\\\\python35\\\\lib\\\\runpy.py\", line 85, in _run_code\\r\\n    exec(code, run_globals)\\r\\n  File \"C:\\\\Users\\\\thang\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python35\\\\Scripts\\\\toco_from_protos.exe\\\\__main__.py\", line 5, in <module>\\r\\n  File \"c:\\\\users\\\\thang\\\\appdata\\\\local\\\\programs\\\\python\\\\python35\\\\lib\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\toco_from_protos.py\", line 22, in <module>\\r\\n    from tensorflow.contrib.lite.toco.python import tensorflow_wrap_toco\\r\\n  File \"c:\\\\users\\\\thang\\\\appdata\\\\local\\\\programs\\\\python\\\\python35\\\\lib\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\tensorflow_wrap_toco.py\", line 29, in <module>\\r\\n    _tensorflow_wrap_toco = swig_import_helper()\\r\\n  File \"c:\\\\users\\\\thang\\\\appdata\\\\local\\\\programs\\\\python\\\\python35\\\\lib\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\tensorflow_wrap_toco.py\", line 21, in swig_import_helper\\r\\n    import _tensorflow_wrap_toco\\r\\nImportError: No module named \\'_tensorflow_wrap_toco\\'\\r\\n'", "> The TOCO converter was not working for me on Windows 10. For those still having problems, I used the Windows Built-In Subsystem for Linux (good tutorial here https://www.youtube.com/watch?v=xzgwDbe7foQ) as I don't have knowledge of Docker or similar programs. This allowed me to use TOCO.\r\n\r\nI was folowing the [tutorial ](https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2-tflite/#2) and had a problem running the toco.\r\n\r\nThis was the solution for me\r\nit was not easy because  you need to have the exact version of tensorflow, and need to find the toco file but here it goes:\r\n\r\non windows 10\r\ninstall Windows Subsystem for Linux [see ](https://blog.revolutionanalytics.com/2017/12/r-in-the-windows-subsystem-for-linux.html)\r\n\r\nso now you have linux subsystem and you installed Ubunto from store\r\n\r\nnext step setup your linux machine to have all the need python modules.\r\n1. install pip3\r\n```\r\nNote: I had a problem installing pip3, needed to install first pip, then pip3\r\nsudo apt-get install python-pip\r\nsudo apt-get install python3-pip\r\n\r\n```\r\n2. install tensorflow 1.7 - \r\npip3 install --upgrade  \"tensorflow==1.7.*\"\r\nnote - this is very important other versions may not work\r\n3. if you try to run toco you would get : \r\n```\r\nCommand 'toco' not found, did you mean:\r\n\r\n  command 'todo' from deb devtodo\r\n\r\nTry: sudo apt install <deb name>\r\n```\r\nso where is toco?\r\n\r\n4. go to folder ~/.local/bin/\r\nthere you would find toco.\r\nthis is a python file so run it `python3 ~/.local/bin/toco`\r\nnow you can run the \"exe\" of toco\r\nto convert you can run the command explained in https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2-tflite/#2\r\n\r\njust change the -graph_def_file=tf_files/retrained_graph.pb to --input_file=tf_files/retrained_graph.pb \r\nif you are following the tutorial, then go to the git repo folder - \"tensorflow-for-poets-2\"\r\nand run from there: \r\npython3 ~/.local/bin/toco --input_file=tf_files/retrained_graph.pb --output_file=tf_files/optimized_graph.lite --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --input_shape=1,224,224,3 --input_array=input --output_array=final_result --inference_type=FLOAT --input_data_type=FLOAT\r\n\r\nHope this helps someone\r\n\r\n\r\nNote: you can access you windows folders under mnt\r\nexample\r\n`/mnt/c/Users/-----/source/repos/tensorflow-for-poets-2`\r\n\r\n", "You are using a 1.7 as the version. Please try something newer like 1.12 or\ntf-nightly. It was certainly not available in 1.7.\n\n-A\n\n\n\n\nOn Wed, Nov 7, 2018 at 6:14 AM mwindowshz <notifications@github.com> wrote:\n\n> The TOCO converter was not working for me on Windows 10. For those still\n> having problems, I used the Windows Built-In Subsystem for Linux (good\n> tutorial here https://www.youtube.com/watch?v=xzgwDbe7foQ) as I don't\n> have knowledge of Docker or similar programs. This allowed me to use TOCO.\n>\n> I was folowing the tutorial\n> <https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2-tflite/#2>\n> and had a problem running the toco.\n>\n> This was the solution for me\n> it was not easy because you need to have the exact version of tensorflow,\n> and need to find the toco file but here it goes:\n>\n> on windows 10\n> install Windows Subsystem for Linux see\n> <https://blog.revolutionanalytics.com/2017/12/r-in-the-windows-subsystem-for-linux.html>\n>\n> so now you have linux subsystem and you installed Ubunto from store\n>\n> next step setup your linux machine to have all the need python modules.\n>\n>    1. install pip3\n>\n> Note: I had a problem installing pip3, needed to install first pip, then pip3\n> sudo apt-get install python-pip\n> sudo apt-get install python3-pip\n>\n>\n>\n>    1. install tensorflow 1.7 -\n>    pip3 install --upgrade \"tensorflow==1.7.*\"\n>    note - this is very important other versions may not work\n>    2. if you try to run toco you would get :\n>\n> Command 'toco' not found, did you mean:\n>\n>   command 'todo' from deb devtodo\n>\n> Try: sudo apt install <deb name>\n>\n> so where is toco?\n>\n>    1. go to folder ~/.local/bin/\n>    there you would find toco.\n>    this is a python file so run it python3 ~/.local/bin/toco\n>    now you can run the \"exe\" of toco\n>    to convert you can run the command explained in\n>    https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2-tflite/#2\n>\n> just change the -graph_def_file=tf_files/retrained_graph.pb to\n> --input_file=tf_files/retrained_graph.pb\n> if you are following the tutorial, then go to the git repo folder -\n> \"tensorflow-for-poets-2\"\n> and run from there:\n> python3 ~/.local/bin/toco --input_file=tf_files/retrained_graph.pb\n> --output_file=tf_files/optimized_graph.lite\n> --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE\n> --input_shape=1,224,224,3 --input_array=input --output_array=final_result\n> --inference_type=FLOAT --input_data_type=FLOAT\n>\n> Hope this helps someone\n>\n> Note: you can access you windows folders under mnt\n> example\n> /mnt/c/Users/-----/source/repos/tensorflow-for-poets-2\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/20975#issuecomment-436635893>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAT52l74AFddV8KSTLWhCDW31InWnfCFks5usurIgaJpZM4VWuJ4>\n> .\n>\n", "So I'm using tensorflow version 1.12 but when I run toco --help I get the error \r\nNo module named 'tensorflow.contrib.lite.python.tflite_convert'\r\n\r\n![toco_error](https://user-images.githubusercontent.com/14350271/48180193-8c611080-e2f0-11e8-81ab-3013b8ad9ddf.png)\r\n", "I get the same error as @tajwane, does someone know the solution?", "I found a solution. I posted my script on GitHub: https://github.com/Lexicographical/KerasToTFLite\r\nYou need to run the script using Ubuntu on Windows though.", "@Lexicographical and @Qryptoc I have installed ubuntu in windows but how I can run my python file?\r\nwhen I run that file at that time I got errors numpy is not defined, no one dependency is in it.\r\nhow can I use anaconda's environment here like an anaconda prompt?\r\nplease tell me both I waiting for an answer\r\n\r\nThank you both", "Build tensforflow in windows 10: https://www.linkedin.com/pulse/object-detection-part-1-cuda-100-tensorflow-112-homan-huang/\r\nInstall tensorflow 1.12:     pip3 install tensorflow\r\nInstall tf_nightly:     pip3 install tf_nightly\r\nFix the python library: https://stackoverflow.com/questions/48435006/modulenotfounderror-no-module-named-tensorflow-contrib-lite-toco-python", "I run into the same issues, so to be able to find it in the terminal:\r\n>  pip3 install tf_nightly\r\n\r\nBe careful to use `pip3` and not `pip`, or you will run into the error `ImportError: cannot import name abs`."]}, {"number": 20974, "title": "object detection api tflite export_tflite_ssd_graph  error ", "body": "when i ran above SSD model\r\n\r\nxport CONFIG_FILE=~/**ssd_mobilenet_v1_0.75_depth_quantized_300x300_coco14_sync_2018_07_03**/pipeline.config\r\nexport CHECKPOINT_PATH=~/ssd_mobilenet_v1_0.75_depth_quantized_300x300_coco14_sync_2018_07_03/model.ckpt\r\n\r\npython object_detection/export_tflite_ssd_graph.py \\\r\n\r\n>\u200a\u2014\u200apipeline_config_path=$CONFIG_FILE \\\r\n\r\n>\u200a\u2014\u200atrained_checkpoint_prefix=$CHECKPOINT_PATH \\\r\n\r\n>\u200a\u2014\u200aoutput_directory=$OUTPUT_DIR \\\r\n\r\n>\u200a\u2014\u200aadd_postprocessing_op=true\r\n\r\n/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n\r\nfrom ._conv import register_converters as _register_converters\r\n\r\nOMP: Warning #181: OMP_PROC_BIND: ignored because KMP_AFFINITY has been defined\r\n\r\n2018\u201307\u201319 15:44:19.888752: I tensorflow/core/common_runtime/process_util.cc:63] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n\r\n2018\u201307\u201319 15:44:22.853028: W tensorflow/core/framework/op_kernel.cc:1318] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: Key BoxPredictor_0/BoxEncodingPredictor/act_quant/max not found in checkpoint\r\n\r\nTraceback (most recent call last):\r\n\r\nFile \u201c/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u201d, line 1322, in _do_call\r\n\r\nreturn fn(*args)\r\n\r\nFile \u201c/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u201d, line 1307, in _run_fn\r\n\r\noptions, feed_dict, fetch_list, target_list, run_metadata)\r\n\r\nFile \u201c/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u201d, line 1409, in _call_tf_sessionrun\r\n\r\nrun_metadata)\r\n\r\ntensorflow.python.framework.errors_impl.NotFoundError: Key BoxPredictor_0/BoxEncodingPredictor/act_quant/max not found in checkpoint\r\n\r\n[[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, \u2026, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=\u201d/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n\r\nFile \u201cobject_detection/export_tflite_ssd_graph.py\u201d, line 137, in <module>\r\n\r\ntf.app.run(main)\r\n\r\nFile \u201c/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/platform/app.py\u201d, line 126, in run\r\n\r\n_sys.exit(main(argv))\r\n\r\nFile \u201cobject_detection/export_tflite_ssd_graph.py\u201d, line 133, in main\r\n\r\nFLAGS.max_classes_per_detection)\r\n\r\nFile \u201c/home/ubuntu/models/research/object_detection/export_tflite_ssd_graph_lib.py\u201d, line 261, in export_tflite_graph\r\n\r\ninitializer_nodes=\u2019\u2019)\r\n\r\nFile \u201c/home/ubuntu/models/research/object_detection/exporter.py\u201d, line 72, in freeze_graph_with_def_protos\r\n\r\nsaver.restore(sess, input_checkpoint)\r\n\r\nFile \u201c/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u201d, line 1802, in restore\r\n\r\n{self.saver_def.filename_tensor_name: save_path})\r\n\r\nFile \u201c/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u201d, line 900, in run\r\n\r\nrun_metadata_ptr)\r\n\r\nFile \u201c/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u201d, line 1135, in _run\r\n\r\nfeed_dict_tensor, options, run_metadata)\r\n\r\nFile \u201c/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u201d, line 1316, in _do_run\r\n\r\nrun_metadata)\r\n\r\nFile \u201c/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u201d, line 1335, in _do_call\r\n\r\nraise type(e)(node_def, op, message)\r\n\r\ntensorflow.python.framework.errors_impl.NotFoundError: Key BoxPredictor_0/BoxEncodingPredictor/act_quant/max not found in checkpoint\r\n\r\n[[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, \u2026, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=\u201d/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\r\n\r\nCaused by op \u2018save/RestoreV2\u2019, defined at:\r\n\r\nFile \u201cobject_detection/export_tflite_ssd_graph.py\u201d, line 137, in <module>\r\n\r\ntf.app.run(main)\r\n\r\nFile \u201c/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/platform/app.py\u201d, line 126, in run\r\n\r\n_sys.exit(main(argv))\r\n\r\nFile \u201cobject_detection/export_tflite_ssd_graph.py\u201d, line 133, in main\r\n\r\nFLAGS.max_classes_per_detection)\r\n\r\nFile \u201c/home/ubuntu/models/research/object_detection/export_tflite_ssd_graph_lib.py\u201d, line 261, in export_tflite_graph\r\n\r\ninitializer_nodes=\u2019\u2019)\r\n\r\nFile \u201c/home/ubuntu/models/research/object_detection/exporter.py\u201d, line 67, in freeze_graph_with_def_protos\r\n\r\ntf.import_graph_def(input_graph_def, name=\u2019\u2019)\r\n\r\nFile \u201c/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u201d, line 432, in new_func\r\n\r\nreturn func(*args, **kwargs)\r\n\r\nFile \u201c/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\u201d, line 513, in import_graph_def\r\n\r\n_ProcessNewOps(graph)\r\n\r\nFile \u201c/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\u201d, line 303, in _ProcessNewOps\r\n\r\nfor new_op in graph._add_new_tf_operations(compute_devices=False): # pylint: disable=protected-access\r\n\r\nFile \u201c/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u201d, line 3540, in _add_new_tf_operations\r\n\r\nfor c_op in c_api_util.new_tf_operations(self)\r\n\r\nFile \u201c/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u201d, line 3540, in <listcomp>\r\n\r\nfor c_op in c_api_util.new_tf_operations(self)\r\n\r\nFile \u201c/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u201d, line 3428, in _create_op_from_tf_operation\r\n\r\nret = Operation(c_op, self)\r\n\r\nFile \u201c/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u201d, line 1718, in __init__\r\n\r\nself._traceback = self._graph._extract_stack() # pylint: disable=protected-access\r\n\r\nNotFoundError (see above for traceback): Key BoxPredictor_0/BoxEncodingPredictor/act_quant/max not found in checkpoint\r\n\r\n[[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, \u2026, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=\u201d/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]", "comments": ["Thanks. This checkpoint link will be updated today.\r\nPlease start with this checkpoint meanwhile:\r\nhttps://storage.googleapis.com/download.tensorflow.org/models/tflite/ssd_mobilenet_v1_0.75_depth_300x300_quant_pets_2018_06_29.zip\r\n", "The checkpoints have been updated in the detection model zoo."]}, {"number": 20973, "title": "TFLITE object detection api  SSD model export issue toco", "body": "Hi, while running\r\n\r\nexport CONFIG_FILE=~/ssdlite_mobilenet_v2_coco_2018_05_09/pipeline.config\r\nexport CHECKPOINT_PATH=~/ssdlite_mobilenet_v2_coco_2018_05_09/model.ckpt\r\n\r\nbazel run -c opt tensorflow/contrib/lite/toco:toco\u200a\u2014\u200a\u2014 input_file=$OUTPUT_DIR/tflite_graph.pb\u200a\u2014\u200aoutput_file=$OUTPUT_DIR/detect.tflite\u200a\u2014\u200ainput_shapes=1,300,300,3\u200a\u2014\u200ainput_arrays=normalized_input_image_tensor\u200a\u2014\u200aoutput_arrays=\u2019TFLite_Detection_PostProcess\u2019,\u2019TFLite_Detection_PostProcess:1',\u2019TFLite_Detection_PostProcess:2',\u2019TFLite_Detection_PostProcess:3'\u200a\u2014\u200ainference_type=QUANTIZED_UINT8\u200a\u2014\u200amean_values=128\u200a\u2014\u200astd_values=128\u200a\u2014\u200achange_concat_input_ranges=false\u200a\u2014\u200aallow_custom_ops\r\n\r\nINFO: Analysed target //tensorflow/contrib/lite/toco:toco (0 packages loaded).\r\n\r\nINFO: Found 1 target\u2026\r\n\r\nTarget //tensorflow/contrib/lite/toco:toco up-to-date:\r\n\r\nbazel-bin/tensorflow/contrib/lite/toco/toco\r\n\r\nINFO: Elapsed time: 0.392s, Critical Path: 0.01s\r\n\r\nINFO: 0 processes.\r\n\r\nINFO: Build completed successfully, 1 total action\r\n\r\nINFO: Running command line: bazel-bin/tensorflow/contrib/lite/toco/toco \u2018\u200a\u2014\u200ainput_file=/home/ubuntu/tflite/tflite_graph.pb\u2019 \u2018\u200a\u2014\u200aoutput_file=/home/ubuntu/tflite/detect.tflite\u2019 \u2018\u200a\u2014\u200ainput_shapes=1,300,300,3\u2019 \u2018\u200a\u2014\u200ainput_arrays=normalized_input_image_tensor\u2019 \u2018\u200a\u2014\u200aoutput_arrays=TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3\u2019 \u2018INFO: Build completed successfully, 1 total action\r\n\r\n2018\u201307\u201319 15:32:40.725811: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TFLite_Detection_PostProcess\r\n\r\n2018\u201307\u201319 15:32:40.742690: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1070 operators, 1570 arrays (0 quantized)\r\n\r\n2018\u201307\u201319 15:32:40.791000: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1070 operators, 1570 arrays (0 quantized)\r\n\r\n2018\u201307\u201319 15:32:40.849051: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 116 operators, 310 arrays (1 quantized)\r\n\r\n2018\u201307\u201319 15:32:40.852027: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 116 operators, 310 arrays (1 quantized)\r\n\r\n2018\u201307\u201319 15:32:40.853807: F tensorflow/contrib/lite/toco/tooling_util.cc:1621] Array FeatureExtractor/MobilenetV2/Conv/Relu6, which is an input to the DepthwiseConv operator producing the output array FeatureExtractor/MobilenetV2/expanded_conv/depthwise/Relu6, is lacking min/max data, which is necessary for quantization. Either target a non-quantized output format, or change the input graph to contain min/max information, or pass\u200a\u2014\u200adefault_ranges_min= and\u200a\u2014\u200adefault_ranges_max= if you do not care about the accuracy of results.", "comments": ["Please start with Mobilenet v1 SSD to make sure the pipeline works.\r\nhttps://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193\r\nFor the above model you are passing, it seems it is a float model, not a quantized model.\r\nhttps://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md\r\n", "I am having a similar issue with ssdlite_mobilenet_v2"]}, {"number": 20972, "title": "Estimator does not work with tf.contrib.cudnn_rnn.CudnnGRU", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**:  v1.9.0-0-g25c197e023 1.9.0\r\n- **Python version**: 3.6.3\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9.0, 7.0.5\r\n- **GPU model and memory**: GTX 1070 8G\r\n- **Exact command to reproduce**: https://gist.github.com/matthew-z/57d531fa8ca5616b59afb75c1fd9d9f7\r\n \r\n### Describe the problem\r\n\r\nIt raises `ValueError: All tensors of a saveable object must be on the same device: cudnn_gru/opaque_kernel_saveable` if `CudnnGRU` is used in `model_fn` of a customized estimator. However,`CudnnLSTM` `CudnnRNNTanh`, and `CudnnRNNRelu` work well\r\n\r\n  \r\n### Source code / logs\r\n\r\nSource Code:\r\nhttps://gist.github.com/matthew-z/57d531fa8ca5616b59afb75c1fd9d9f7\r\n\r\nLogs:\r\nhttps://gist.github.com/matthew-z/f43c2188aba7c62c971cf0127fe5b80d\r\n", "comments": ["@fchollet : Can you recommend someone to look at this?", "Met with the exactly same problem with official TF pip 1.12, cuda 9.0, cudnn 7.0", "I've had the exact same problem as well using a tf Estimator with TF 1.11, CUDA 9.0 and CuDNN 7.0. A similar error to matthew-z was raised when using CudnnGRU, but CudnnLSTM and CudnnRNNTanh work as expected for me too.", "1.7 / 1.12 meet the same problem, while CudnnLSTM works well", "Same here on TF 1.13 / CUDA 10.0. CudnnGRU doesn't work, but CudnnLSTM works fine. Any update?", "Same for me. Using TF 1.13 and CUDA 10.0. CudnnLSTM works fine, but not CudnnGRU.", "Same on TF 1.14 CUDA 10 CuDNN 7.", "We see that you are using old version of Tensorflow which is officially considered as end of life, We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions .Please open a new issue in case you face any errors, we will get you the right help .Thanks!", "Thanks @mohantym. We abandoned TF due to persistent issues that happened during the 2.0 transition. Feel free to close.", "Ok! Closing the issue .Feel free  to open a new issue in case you face new errors.", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 20971, "title": "Fix inappropriate argument names in c_api.cc", "body": "In Python3.7, a word **async** became a [**new reserved keyword**](https://docs.python.org/3.7/whatsnew/3.7.html).\r\nUnfortunately, some functions in **c_api.cc** use the word **async** as argument names.\r\nDuring tensorflow building, **c_api.cc** is converted to python wrapper by **swig** tool.\r\n**Swig** uses the same name **async** in the wrapper functions.\r\nThis causes the syntax error in python 3.7.\r\nThe tensorflow building with python 3.7 thus ends up with failure without this fix.", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "How can I commit suggestion codes for this issue? I encountered **an authentication error** during PUSH ORIGIN from GitHub Desktop. \r\n\"Authentication failed. You may not have permission to access the repository or the repository may have been archived. Open options and verify that you're signed in with an account that has permission to access this repository.\"", "@jungpilyu please resolve conflicts.", "This PR seems to change a lot more than what is described in the PR desc.", "I signed CLA. When will it be confirmed? In order to commit, I need to be given permission to access to repo. Whom do I have to ask?", "@jungpilyu I am not sure what you are asking. You need to create your own repo with a clone of TensorFlow, make the changes there and submit a pull request here to have the changes committed to the official repo. See https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md", "@jungpilyu I am not sure what you are asking. You need to create your own repo that is a for of TensorFlow, make the changes there and submit a pull request here to have the changes committed to the official repo.", "@rmlarsen Thank you for the guidance. I am newbie in contributing to open source. Now I become to know how to do that owing to you! Thank you again. "]}, {"number": 20970, "title": "Update or-tools to v6.7.2", "body": "This fix updates or-tools from 253f795 (dated 03/21/2017)\r\nto the latest versioned release version of v6.7.2\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Looks like a flake, but retrying tests just in case."]}, {"number": 20969, "title": "Fix custom plugin sample test", "body": "", "comments": []}, {"number": 20968, "title": "tensorRT error '*** stack smashing detected ***: python terminated', ", "body": "I am using tensorflow version 1.7, cuda8.0, python 2.7 to optimize the tensorflow VGG-16 model using the code below:-\r\n\r\n\r\n    vgg_checkpoint = 'trained_models/models/vgg16/vgg_16.ckpt'\r\n    image_decoded = tf.placeholder(dtype=tf.float32, shape=[None, None, None, 3], name=\"input\")\r\n    logits, _ = vgg_16(image_decoded, is_training=False)\r\n    saver = tf.train.Saver()\r\n    saver.restore(sess, vgg_checkpoint)\r\n\r\n    input_graph_def = sess.graph.as_graph_def()\r\n    output_graph_def = tf.graph_util.convert_variables_to_constants(\r\n        sess,\r\n        input_graph_def,\r\n        ['vgg_16/fc8/squeezed'],\r\n    )\r\n    frozen_graph = tf.graph_util.remove_training_nodes(output_graph_def)\r\n    output_graph = \"../model/tensorRT_check.pb\"\r\n    with tf.gfile.GFile(output_graph, \"wb\") as f:\r\n        f.write(frozen_graph.SerializeToString())\r\n    trt_graph = trt.create_inference_graph(\r\n        input_graph_def=frozen_graph,\r\n        outputs=['input', 'vgg_16/fc8/squeezed'],\r\n        max_batch_size=1,\r\n        max_workspace_size_bytes=1 << 25,\r\n        precision_mode='FP16',\r\n        minimum_segment_size=50\r\n    )\r\n    \r\n\r\nand I am getting following error.\r\n\r\n`2018-07-19 19:00:34.093716: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-07-19 19:00:34.149018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-07-19 19:00:34.149335: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties: \r\nname: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 11.92GiB freeMemory: 11.33GiB\r\n2018-07-19 19:00:34.149350: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0\r\n2018-07-19 19:00:34.395658: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-07-19 19:00:34.395685: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0 \r\n2018-07-19 19:00:34.395691: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N \r\n2018-07-19 19:00:34.395901: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7324 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)\r\nConverted 32 variables to const ops.\r\n2018-07-19 19:00:41.076819: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count >= 8): 1\r\n*** stack smashing detected ***: python terminated\r\nAborted (core dumped)\r\n`\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Yes this is still an issue\n\nOn Sat, Aug 4, 2018 at 12:27 AM Alfred Sorten Wolf <notifications@github.com>\nwrote:\n\n> It has been 14 days with no activity and the awaiting response label was\n> assigned. Is this still an issue?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/20968#issuecomment-410344819>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AXrRmwkZr-zkApAOFqT48YL2HqWt_T5Dks5uNJ0PgaJpZM4VWb46>\n> .\n>\n\n\n-- \n*Regards,*\n\n*Nirmal Jith O U*\n", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 20967, "title": "[XLA] HloEvaluator - use TF_ASSIGN_OR_RETURN for HandleCall, HandleFusion and HandleConditional", "body": "Currently HandleCall in the HloEvaluator uses `ConsumeValueOrDie` to get the\r\nvalue instead of `TF_ASSIGN_OR_RETURN`. This means that the compiler will fail\r\nwhen running the HLO ConstantFolding pass on a graph with a call instruction\r\nwhich has an unhandled instruction in its subcompuation.\r\n\r\nExample:\r\n```\r\n%call {\r\n  %constant.1 = f32[] constant(0)\r\n  %constant.2 = f32[] constant(1)\r\n  %rng.0.3 = f32[] rng(f32[] %constant.1, f32[] %constant.2), distribution=rng_uniform\r\n}\r\n\r\nENTRY %cluster {\r\n  ROOT %call = f32[] call(), to_apply=%call\r\n}\r\n```", "comments": ["Hi @vrv, this pretty simple change has been open for almost 2 months now, meaning it's starting to get merge conflicts, should this be reassigned?", "My apologies for the delay in review.  I've added @kayzhu, who wrote the HloEvaluator.  I'll just leave some notes in passing, Kay will do the full review.", "Sorry for the delay -- I had not notice this change. Thank you for the fix! I have minor similar comment to Todd's, it otherwise looks great.", "Thank you for your feedback, I've made the requested changes.", "Thanks for the feedback @kayzhu, I've made all the requested changes.", "LGTM ", "Thanks, I think @tatatodd also needs to approve?", "@tatatodd another look?", "@georgepaw sorry for the delay triggered tests again.", "@rmlarsen @kayzhu the test failed because there are new tests cases on master which I have fixed now", "Could you run `clang-format-3.6.0`? thanks.", "@drpngx sorry, I thought I already did! I fixed the styling now :)", "Can you look into the XLA failures?", "I'm not sure these test failures are related, there have been changes to that test recently https://github.com/tensorflow/tensorflow/commit/adb904fb99ec706dbbe11b4e35c227ff3bb7127c.", "Hi, it has been almost 3 months since this PR has been opened, could we look at merging this in please?", "Let's test this again.", "Can you look into the XLA failures? It doesn't seem to build. More failures on GPU.", "The broken tests seem unrelated as they are problems with CUDA for example:\r\n\r\nbazel-out/k8-opt/bin/external/nccl_archive/libsum.a(sum_reduce.cu.o): In function `__sti____cudaRegisterAll()':\r\n11102\r\ntmpxft_00002633_00000000-5_sum_reduce.cu.cudafe1.cpp:(.text.startup+0x1d): undefined reference to `__cudaRegisterLinkedBinary_48_tmpxft_00002633_00000000_6_sum_reduce_cu_cpp1_ii_50717b28'\r\n", "@georgepaw could you please resolve the conflicts?", "@georgepaw  Gentle reminder to resolve the conflicts. ", "@georgepaw gentle ping", "I have merged the branch and resolved the conflict myself, I think this patch is ready to be merged."]}, {"number": 20966, "title": "switch for nccl", "body": "in configure.py(r1.10):\r\n\r\n> line 1132 to 1141\r\n\r\n```\r\n    if is_windows():\r\n      nccl_lib_path = 'lib/x64/nccl.lib'\r\n    elif is_linux():\r\n      nccl_lib_path = '/usr/lib/x86_64-linux-gnu/libnccl.so.%s' % tf_nccl_version\r\n      #nccl_lib_path = 'lib64/libnccl.so.%s' % tf_nccl_version\r\n    elif is_macos():\r\n      nccl_lib_path = 'lib/libnccl.%s.dylib' % tf_nccl_version\r\n\r\n    nccl_lib_path = os.path.join(nccl_install_path, nccl_lib_path)\r\n    nccl_hdr_path = os.path.join(nccl_install_path, '/usr/include/nccl.h')#'include/nccl.h')\r\n```\r\nin case of install nccl by `sudo apt install libnccl2=2.2.13-1+cuda9.0 libnccl-dev=2.2.13-1+cuda9.0\r\n`", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.10\r\n- **Python version**:  2.7.12\r\n- **Bazel version (if compiling from source)**: 0.14.1\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: 9.0/7.1.2\r\n- **GPU model and memory**: GTX850M 2GB"]}, {"number": 20965, "title": "Typo in tf.Session fixed", "body": "", "comments": []}]