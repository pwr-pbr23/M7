[{"number": 34085, "title": "Tensorflow fails to build due to \" no such package '@keras_applications_archive// BUILD file not found in directory ''\"", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Source (latest)\r\n- TensorFlow version: 2.0.0\r\n- Python version: 3.7.4\r\n- Installed using virtualenv? pip? conda?: miniconda\r\n- Bazel version (if compiling from source): 0.29.1\r\n- GCC/Compiler version (if compiling from source): 8.2.0\r\n- CUDA/cuDNN version: 10.1/7.6\r\n- GPU model and memory: V100\r\n\r\nYesterday, 11-6-19 I was able to successfully build tensorflow from source using the install script that I wrote here:\r\nhttps://gist.github.com/AmericanEnglish/5c522541f1c4a648c24db344477608f4#file-install-sh\r\n\r\nHowever as of today I now receive this error:\r\n```\r\nERROR: /home/user/tests/tensorflowTest/fromsource/fresh/tensorflow/tensorflow/python/keras/BUILD:19:1: no such package '@keras_applications_archive//': BUILD file not found in directory '' of external repository @keras_applications_archive and referenced by '//tensorflow/python/keras:keras'\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@keras_applications_archive//': BUILD file not found in directory '' of external repository @keras_applications_archive\r\nINFO: Elapsed time: 5.218s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (80 packages loaded, 146 targets configured)\r\n```\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nSee the above mentioned gist. I've further consolidated a lot of commands into module loads via lmod. The loading of CUDA 10.1 and cuDNN are done via the lmod system.\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nI should mention that my miniconda environment has keras_applications and keras_preprocessing installed as per the tensorflow build from source guide.", "comments": ["After an hour or two of fiddling I stumbled across this issue:\r\nhttps://github.com/bazelbuild/bazel/issues/1985 .\r\nWhich was similar to my own. It turns out that after building tensorflow yesterday it seems that bazel did not remove anything.\r\nMy temp dir was:\r\n/home/user/.tmp/\r\n\r\nMy successful build from yesterday was:\r\n/home/user/fullDownload/\r\nMy failed build was in:\r\n/home/user/moduleBased/\r\n\r\nThe leftovers in the shared tmp folder led to Bazel's inability to figure find anything because it thought that things had already been generated but they had not been. \r\n\r\nI hope my wording isn't confusing. TL;DR :\r\n\r\nIf you've previously built tensorflow, successful or not, clear out the area you're using as your tmp folder entirely as Bazel does not do it automatically after building. Additionally these old files will cause future build problems if you're attempting another build in a new place\r\n"]}, {"number": 34084, "title": "[Codelab] Update digit classifier Android app to target API level 29", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- Android Studio version: 3.5\r\n\r\n**Describe the problem**\r\nThe Android app package available from https://codelabs.developers.google.com/codelabs/digit-classifier-tflite/index.html?index=..%2F..index#2 has a targetSdkVersion of \"28\". This should be updated to \"29\".\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n1. Download codelab ZIP file\r\n2. Open the Android Studio project: lite/codelabs/digit_classifier/android/start/\r\n3. View the project's app/build.gradle file.\r\n", "comments": ["We're not taking advantage of any API 29 features, and 28 is the standard targetSdkVersion for the Play Store."]}, {"number": 34083, "title": "Can't find tensorflow.examples.tutorial", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): None\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): v1.12.1-17556-g63c45aacf3 2.0.0\r\n- Python version: Python 3.7.4\r\n- Bazel version (if compiling from source):0.27.1\r\n- GCC/Compiler version (if compiling from source):7.4.0\r\n- CUDA/cuDNN version: V10.1.243\r\n- GPU model and memory: GP107GL [Quadro P1000]  -- 4031MiB \r\n\r\n**Describe the current behavior**\r\n```\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nModuleNotFoundError:  No module named 'tensorflow.examples'\r\n```\r\nAs the error sugests, can't find the module. Am I missing something while generating the wheel package from bazel? \r\n\r\n**Describe the expected behavior**\r\nIs there in the source code repo, hence should work. No monkey-fixes please. I have tried local copying the directory, but then some other tensorflow module is missing (tensorflow.contrib and goes on).  \r\n\r\n**Code to reproduce the issue**\r\n`python -c 'from tensorflow.examples.tutorials import input_data'`\r\n\r\n\r\n**Other info / logs**\r\n`bazel build --verbose_failures  --config=monolithic //tensorflow/tools/pip_package:build_pip_package`\r\nCommand I used to create the wheel package from tensorflow source.\r\n", "comments": ["Yes. You should refer to this [table](https://www.tensorflow.org/install/source_windows#gpu) and create wheel package with tested configuration.", "@nikochiko, I tried bezel 0.26.1 before this. It was stuck at the same issue.\r\nI really doubt the version numbers is causing the problem here. ", "Issue replicating for TF2.0 version.Kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/4a0e6b790a2eeddfbb0578f00ddb44a3/34083.ipynb) of colab.Thanks!", "I believe there's a typo in your import. \r\nCan you try with;\r\n```import tensorflow.examples.tutorials```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34083\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34083\">No</a>\n", "@ymodak It still doesn't work. Strangely, it was working in the colab link yesterday, but not [today](https://colab.research.google.com/gist/wady101/d2f43ab524740bb817ea68383a53d67c/34083.ipynb).  Can you give this a try? I also tried it on the local build I have installed (build no. mentioned in the issue) and it still gives me a similar error.\r\nI have updated the issue. I am getting two slightly different errors - I am definitely sure now, that I am missing something in my bezel install command. \r\nThe [gist](https://colab.research.google.com/gist/wady101/d2f43ab524740bb817ea68383a53d67c/34083.ipynb) colab gives me: \r\n```\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nModuleNotFoundError: No module named 'tensorflow.examples.tutorials'\r\n```\r\nLocal build (build no. in issue) gives me:\r\n```\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nModuleNotFoundError: No module named 'tensorflow.examples'\r\n```", "@wady101 you need to set the PYTHONPATH\r\n`\r\nexport PYTHONPATH=$tensorflow_examples_root\r\n`", "@Leslie-Fang sorry, what is $tensorflow_examples_root? Couldn't find any mention of env variable online.\r\n\r\n Also should I set this before building from source or after? ", "@wady101 I suspect it's because of all tensorflow version. \r\nDid you write your benchmark or models by yourself?", "No @Leslie-Fang , I am trying to get a piece of code to run https://github.com/layog/Accurate-Binary-Convolution-Network\r\n\r\nStill, what do I keep PYTHONPATH? Can you echo it on your pc and show it here? Thanks ", "@wady101 Sorry, I mis-read your error message.\r\nI thought it was tensorflow_examples couldn't be find.\r\n\r\nI suspect your error message relates with the tf version as you can see  https://github.com/layog/Accurate-Binary-Convolution-Network is using TF1.4(tensorflow-gpu==1.4.1)", "@Leslie-Fang No worries. I have used the TF1.4.1, but that still doesn't work (I still get the same error message). The error message is always there for any version I use. I assume is there a flag I am missing when I am building from source? ", "TF 1.4.1 is obsolete. Please install latest version of TF and import again.\r\nCurrent google colab hosts TF 1.15.0 version an can successfully import.\r\n```from tensorflow.examples.tutorials.mnist import input_data```", "@ymodak I have tried all three versions - 1.4.1, 1.12.1 and latest master branch - all three give me the same error (Using both \"building from source\" and \"pip\"). \r\nThe two things I ask for is ; \r\n* Is there a flag being missed in the bezel command I have used to build tensorflow (see ISSUE)? Do I have to export some path to PYTHONPATH as mentioned by @Leslie-Fang ? \r\n* If not, is there a workaround which will solve my problem? \r\n\r\n\r\nRepository I am trying to run - https://github.com/layog/Accurate-Binary-Convolution-Network/blob/master/ABC-layer-inference-support.ipynb [5th cell in Jupyter nb gives me error] \r\nThanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34083\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34083\">No</a>\n", "I'm using tensorflow-cpu==2.1.0 and I had the same issue \r\n\r\n`ModuleNotFoundError:  No module named 'tensorflow.examples'`\r\n\r\nI solve this downloading manually the directory called \"tutorials\" from tensorflow repo\r\n[https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/tutorials](url)\r\n\r\nand placed it in my virtual env directory \r\n`myenv\\Lib\\site-packages\\tensorflow_core\\examples\\`\r\n\r\nafter that it works fine", "Thanks @oscar7692 worked for me as well that way. life saver."]}, {"number": 34082, "title": "--config=rocm build works with bazel `0.26.1` but has link errors with `0.29.1`", "body": "We are trying to figure out why the `--config=rocm` TF build breaks (link errors) with bazel version `0.29.1`, but works fine with `0.26.1`.\r\n\r\nHow do we get a better understanding of what changed on the bazel side, to introduce the build errors, and how to fix them?\r\n\r\nthanks\r\n\r\n-------------------\r\n\r\n/cc @whchung @parallelo @sunway513 ", "comments": ["@dslomov @scentini @laszlocsomor\r\nWho helped us move to bazel 1.0\r\nIs there a changelog for bazel we can point people to?\r\nI think there are some github issue labels bazel uses, but not sure how to find them.", "@gunan: See @scentini's list: https://github.com/tensorflow/tensorflow/pull/33415#issuecomment-551175252", "I think the `--config=rocm` issues come from a different incompatible change:\r\n[`--incompatible_do_not_split_linking_cmdline`](https://github.com/bazelbuild/bazel/issues/7687)\r\nI believe @meteorcloudy is working on fixing this. ", "Yes, this issue has been resolved by https://github.com/tensorflow/tensorflow/commit/72d0facc37e377d89d8034bb825bf0e93a4f810d", "@meteorcloudy yes and no.\r\n\r\nThe compile issues are resolved by the commit you referenced, but the same commit also introduces a bunch of runtime failures (symbol not found errors from HIP runtime, when attempting to launch GPU kernels). I have filed a PR (https://github.com/tensorflow/tensorflow/pull/34081)  to revert that commit, so that we can get the ROCm CSB back to a known passing state.\r\n\r\nCan I request you, to describe \r\n* what changed within bazel that causes the link-time failures.\r\n* what was done in the commit (that you reference above) to fix those link-time failures.\r\n\r\nIf we have a better understanding of that, it would help us also figure out why that commit leads to the runtime errors that we see.\r\n\r\nThanks\r\n\r\ndeven\r\n\r\n", "@deven-amd \r\nOh, sorry for that. Apparently we didn't run tests on TensorFlow CI for ROCm build, that's why the runtime failure was not caught.\r\n\r\nThe original build error was caused by https://github.com/bazelbuild/bazel/issues/7687. This incompatible change is in 0.29.1, and it caused the order of the linking flags to change. The compiler is sensitive to the flag order, which leads to the problem.\r\n\r\n[72d0fac](https://github.com/tensorflow/tensorflow/commit/72d0facc37e377d89d8034bb825bf0e93a4f810d ) does two things:\r\n\r\n- Removed unused toolchain definitions for Mac and Windows. The ROCm toolchain derives from the GPU toolchain, but IIUC ROCm is Linux only. This could make it easier to maintain.\r\n- To fix the order issue, I added all missing features to the toolchain definition so that Bazel won't patch the toolchain and messes up the flag order. This was based on Bazel's unix_cc_toolchain_config.bzl. This fixed the order but probably introduced some change in the linking flags.\r\n\r\nCan you tell me what exactly the error message looks like?\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "here is one example of the runtime error we see\r\n\r\n```\r\nterminate called after throwing an instance of 'std::runtime_error'                                                                                                                                                                                                \r\n  what():  Missing metadata for __global__ function: _ZN5Eigen8internal15EigenMetaKernelINS_15TensorEvaluatorIKNS_14TensorAssignOpINS_9TensorMapINS_6TensorIfLi1ELi1EiEELi16ENS_11MakePointerEEEKNS_18TensorCwiseUnaryOpINS0_12scalar_rightIffNS0_17scalar_product\\\r\n_opIffEELb0EEEKNS4_INS5_IKfLi1ELi1EiEELi16ES7_EEEEEENS_9GpuDeviceEEEiEEvT_T0_                                                                                                                                                                                      \r\nFatal Python error: Aborted      \r\n```", "is there a way to modify the https://github.com/tensorflow/tensorflow/commit/72d0facc37e377d89d8034bb825bf0e93a4f810d commit to not use the gold linker? \r\n\r\n", "think I may have found the cause...\r\nthis line ( https://github.com/tensorflow/tensorflow/blob/master/third_party/gpus/crosstool/BUILD.rocm.tpl#L93) ?\r\n\r\nMy guess is that specifying the the `-Wl--gc-sections` option somehow makes the linker discard some of the information related to the gpu kernel binary blobs resulting in the runtime error.\r\n\r\nremoving this option makes one of the previously failing tests, pass. \r\ntrying this out now on all the failing tests...will post the result here when done\r\n\r\n", "@meteorcloudy \r\n\r\nall of the runtime failures we were observing in the ROCm CSB tests go away when I remove the `-Wl,--gc-sections` from the link options. \r\n\r\nI will file a PR shortly to have this change upstreamed. \r\n\r\nWe can close out this issue, if you feel this is an acceptable solution.\r\n", "Filed https://github.com/tensorflow/tensorflow/pull/34177", "@deven-amd That's great! Thanks for fixing this, I'll help you import #34177 and look into enabling some tests for ROCm build to prevent regression like this."]}, {"number": 34081, "title": "[ROCm] Revert \"Update ROCm toolchain configuration\"", "body": "This reverts commit 72d0facc37e377d89d8034bb825bf0e93a4f810d.\r\n\r\nThe above PR breaks the ROCm CSB. \r\n\r\nWhile the commit being reverted, cleans up the files associated with the ROCm toolchain configuration, it also introduces some new compile/link options which we suspect are leading to the failures we are getting the ROCm CSB. The switch to use the \"gold\" linker is one suspect.\r\n\r\nWas there any testing done for the commit? We would like to understand the setup being used internally to test ROCm builds, so that we can get a better understanding of why things passed on your end. \r\n\r\nthanks\r\n\r\n-----------------------------------\r\n\r\n\r\nNote that including this PR, we now have 4 oustanding PRs which need to be merged to get the ROCm CSB passing again. \r\n\r\n* PR #33806 \r\n* PR #33896 \r\n* PR #34050 \r\n* this PR\r\n\r\nNote that PR #33896 has a merge conflict that was introduced by the commit being rolled back in this PR. So this PR will need to be accepted first. That will undo the merge conflict in PR #33896 at which point it can be accepted as well\r\n\r\n------------------------------------------\r\n\r\n@chsigg @whchung \r\n\r\n", "comments": ["+ @dagamayank @tatianashp  for awareness\r\n\r\nA commit went into TensorFlow mainline which changed build scripts for ROCm platform but didn't necessarily pass all required tests. Also maintainers for ROCm port were not notified.", "Found the cause of the failures introduced by the commit being reverted in this PR\r\n\r\nsee issue https://github.com/tensorflow/tensorflow/issues/34082 for details.\r\n\r\nClosing out this PR, as newly filed PR https://github.com/tensorflow/tensorflow/pull/34177 supersedes it"]}, {"number": 34080, "title": "Can't apply map on Dataset", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Manjaro\r\n- TensorFlow installed from (source or binary): pip repository, binary\r\n- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0\r\n- Python version: 3.7.4\r\n- CUDA/cuDNN version: N/A (CPU)\r\n- GPU model and memory: N/A (CPU)\r\n\r\n**Describe the current behavior**\r\nTrying to apply a function to any dataset (created either from `from_tensor_slices` or `from_generator`) returns the following error : \r\n\r\n> TypeError: Failed to convert object of type <class 'tensorflow.python.data.ops.dataset_ops._VariantDataset'> to Tensor. Contents: <_VariantDataset shapes: (2,), types: tf.int64>. Consider casting elements to a supported type.\r\n\r\n**Describe the expected behavior**\r\nThe operation should return no error.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n@tf.function\r\ndef expand(x):\r\n    return tf.expand_dims(x, axis = 2)\r\n\r\nm = np.array([[1, 1],\r\n              [1, 1]])\r\nds = tf.data.Dataset.from_tensor_slices(m)\r\nds = ds.apply(expand)\r\n```\r\n**Note:** Try to apply any other function would returns the same error.\r\n\r\n**Other info / logs**\r\nFull stack trace : \r\n\r\n> ---------------------------------------------------------------------------\r\n> TypeError                                 Traceback (most recent call last)\r\n> <ipython-input-72-07c8595ab9d1> in <module>\r\n>       9                  [1, 1]])\r\n>      10 ds = tf.data.Dataset.from_tensor_slices(m)\r\n> ---> 11 ds = ds.apply(expand)\r\n> \r\n> /usr/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py in apply(self, transformation_func)\r\n>    1367           dataset.\r\n>    1368     \"\"\"\r\n> -> 1369     dataset = transformation_func(self)\r\n>    1370     if not isinstance(dataset, DatasetV2):\r\n>    1371       raise TypeError(\r\n> \r\n> /usr/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n>     455 \r\n>     456     tracing_count = self._get_tracing_count()\r\n> --> 457     result = self._call(*args, **kwds)\r\n>     458     if tracing_count == self._get_tracing_count():\r\n>     459       self._call_counter.called_without_tracing()\r\n> \r\n> /usr/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)\r\n>     501       # This is the first call of __call__, so we have to initialize.\r\n>     502       initializer_map = object_identity.ObjectIdentityDictionary()\r\n> --> 503       self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n>     504     finally:\r\n>     505       # At this point we know that the initialization is complete (or less\r\n> \r\n> /usr/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n>     406     self._concrete_stateful_fn = (\r\n>     407         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n> --> 408             *args, **kwds))\r\n>     409 \r\n>     410     def invalid_creator_scope(*unused_args, **unused_kwds):\r\n> \r\n> /usr/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n>    1846     if self.input_signature:\r\n>    1847       args, kwargs = None, None\r\n> -> 1848     graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n>    1849     return graph_function\r\n>    1850 \r\n> \r\n> /usr/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n>    2148         graph_function = self._function_cache.primary.get(cache_key, None)\r\n>    2149         if graph_function is None:\r\n> -> 2150           graph_function = self._create_graph_function(args, kwargs)\r\n>    2151           self._function_cache.primary[cache_key] = graph_function\r\n>    2152         return graph_function, args, kwargs\r\n> \r\n> /usr/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n>    2039             arg_names=arg_names,\r\n>    2040             override_flat_arg_shapes=override_flat_arg_shapes,\r\n> -> 2041             capture_by_value=self._capture_by_value),\r\n>    2042         self._function_attributes,\r\n>    2043         # Tell the ConcreteFunction to clean up its graph once it goes out of\r\n> \r\n> /usr/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n>     913                                           converted_func)\r\n>     914 \r\n> --> 915       func_outputs = python_func(*func_args, **func_kwargs)\r\n>     916 \r\n>     917       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n> \r\n> /usr/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n>     356         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n>     357         # the function a weak reference to itself to avoid a reference cycle.\r\n> --> 358         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n>     359     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n>     360 \r\n> \r\n> /usr/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n>     903           except Exception as e:  # pylint:disable=broad-except\r\n>     904             if hasattr(e, \"ag_error_metadata\"):\r\n> --> 905               raise e.ag_error_metadata.to_exception(e)\r\n>     906             else:\r\n>     907               raise\r\n> \r\n> TypeError: in converted code:\r\n> \r\n>     <ipython-input-72-07c8595ab9d1>:6 expand  *\r\n>         return tf.expand_dims(x, axis = 2)\r\n>     /usr/lib/python3.7/site-packages/tensorflow_core/python/util/dispatch.py:180 wrapper\r\n>         return target(*args, **kwargs)\r\n>     /usr/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py:325 expand_dims_v2\r\n>         return gen_array_ops.expand_dims(input, axis, name)\r\n>     /usr/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_array_ops.py:2465 expand_dims\r\n>         \"ExpandDims\", input=input, dim=axis, name=name)\r\n>     /usr/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py:530 _apply_op_helper\r\n>         raise err\r\n>     /usr/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py:527 _apply_op_helper\r\n>         preferred_dtype=default_dtype)\r\n>     /usr/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1296 internal_convert_to_tensor\r\n>         ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n>     /usr/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py:286 _constant_tensor_conversion_function\r\n>         return constant(v, dtype=dtype, name=name)\r\n>     /usr/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py:227 constant\r\n>         allow_broadcast=True)\r\n>     /usr/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py:265 _constant_impl\r\n>         allow_broadcast=allow_broadcast))\r\n>     /usr/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py:545 make_tensor_proto\r\n>         \"supported type.\" % (type(values), values))\r\n> \r\n>     TypeError: Failed to convert object of type <class 'tensorflow.python.data.ops.dataset_ops._VariantDataset'> to Tensor. Contents: <_VariantDataset shapes: (2,), types: tf.int64>. Consider casting elements to a supported type.\r\n> \r\n\r\n\r\n", "comments": ["I have tried on colab with TF version 2.0,2.1.0-dev20191107 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/28816e71acab1a419fce416cec340b14/untitled339.ipynb). Thanks!\r\n", "@NightWinkle Can you please explain what you want to accomplish?  Are you trying to expand dim of tensor or dataset? Instead of `ds.apply`, i used map function as follows to expand dim. Please take a look at the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/5c03552749e1cdd40365ed7301a8c1b3/untitled339.ipynb).  Thanks!\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n@tf.function\r\ndef expand(x):\r\n    return tf.expand_dims(x, axis = -1)\r\n\r\nm = np.array([[1, 1],\r\n              [1, 1]],dtype=np.float32)\r\nm=tf.convert_to_tensor(m,dtype=tf.float32)\r\nds = tf.data.Dataset.from_tensor_slices(m)\r\n#ds = ds.apply(expand)\r\nds = ds.map(lambda x:tf.expand_dims(x, axis=-1))\r\nprint(ds)\r\n```\r\n\r\nPlease close the issue if it was resolved for you. Thanks!", "Indeed, it felt to me like it should have been a Dataset operation, but `expand_dims` takes only tensors as input.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34080\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34080\">No</a>\n"]}, {"number": 34079, "title": "Update activations.py", "body": "All spelling corrected in the documentation of the code.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34079) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34079) for more info**.\n\n<!-- ok -->"]}, {"number": 34078, "title": "Example custom training loop failed when using tf.train.MomentumOptimizer", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): None\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): On google codelab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): On google codelab\r\n- TensorFlow version (use command below): 1.15.0\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): \r\n- CUDA/cuDNN version: Run on cpu.\r\n- GPU model and memory: Run on cpu.\r\n\r\ndocs/site/en/r1/tutorials/distribute/training_loops.ipynb\r\n\r\nI run tensorflow/docs/blob/master/site/en/r1/tutorials/distribute/training_loops.ipynb using google colab and changing the optimizer to tf.train.MomentumOptimizer() and it gives me the following error:\r\n\r\n```\r\nInternalError: Invalid variable reference. [[node Momentum/update_0_7/update_dense_1/bias/ResourceApplyMomentum (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1748) ]]\r\n```\r\n\r\n**Describe the expected behavior**\r\nI was expecting the custom training loop can be run using different optimizers.\r\n\r\n**Code to reproduce the issue**\r\nPlease goto training_loops.ipynb and open google colab from there.\r\n", "comments": ["@yuqcraft, Please provide the google colab link to expedite the trouble-shooting process. Thanks!\r\n", "Hey @gadagashwini, thanks for the reply. Here's the link:\r\n\r\nhttps://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r1/tutorials/distribute/training_loops.ipynb\r\n\r\nIf you change the optimizer to  tf.train.MomentumOptimizer(). You'll see the error.", "@yuqcraft, I changed the line `optimizer = tf.train.MomentumOptimizer('Momentum',0.001)`. It worked as expected. Can you check once and let me know if its still an error. Thanks!", "@gadagashwini \r\n\r\nWhen I changed to what you have: tf.train.MomentumOptimizer('Momentum',0.001) and I got this error:\r\n\r\nUnimplementedError: Cast string to float is not supported\r\n\t [[node Momentum_2/update_0/update_conv2d_12/kernel/Cast (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1748) ]]\r\n\r\nThe codelab default tf version is 1.15.0. According to manual, I changed it to tf.train.MomentumOptimizer(momentum=0.001, learning_rate=1e-3), then I still got the following error:\r\n\r\nInternalError: Invalid variable reference.\r\n\t [[node Momentum_3/update_0_7/update_dense_15/bias/ResourceApplyMomentum (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1748) ]]\r\n\r\n\r\n", "Issue is replicating on colab with Tf 1.15.0. Thanks!", "Please find the github gist [here](https://colab.sandbox.google.com/gist/gowthamkpr/4b65b08bc4676a2d04927a1ec594f6df/copy-of-training_loops.ipynb). Thanks!", "Thanks, @yuqcraft for the report. This seems to be an issue with the code rather than an issue with the optimizer. Please post to [StackOverflow](http://stackoverflow.com), where there is a larger community to answer such questions.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34078\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34078\">No</a>\n", "> Please find the github gist [here](https://colab.sandbox.google.com/gist/gowthamkpr/4b65b08bc4676a2d04927a1ec594f6df/copy-of-training_loops.ipynb). Thanks!\r\n\r\nHey @gadagashwini,\r\n\r\nWhen I run the github gist, I still got the same error..\r\n\r\n---------------------------------------------------------------------------\r\nInternalError                             Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in _do_call(self, fn, *args)\r\n   1364     try:\r\n-> 1365       return fn(*args)\r\n   1366     except errors.OpError as e:\r\n\r\n6 frames\r\nInternalError: Invalid variable reference.\r\n\t [[{{node Momentum/update_0_7/update_dense_1/bias/ResourceApplyMomentum}}]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInternalError                             Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in _do_call(self, fn, *args)\r\n   1382                     '\\nsession_config.graph_options.rewrite_options.'\r\n   1383                     'disable_meta_optimizer = True')\r\n-> 1384       raise type(e)(node_def, op, message)\r\n   1385 \r\n   1386   def _extend_graph(self):\r\n\r\nInternalError: Invalid variable reference.\r\n\t [[node Momentum/update_0_7/update_dense_1/bias/ResourceApplyMomentum (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1748) ]]\r\n\r\nOriginal stack trace for 'Momentum/update_0_7/update_dense_1/bias/ResourceApplyMomentum':\r\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)"]}, {"number": 34077, "title": "Patch absl after download to work around an nvcc bug.", "body": "The nvcc bug will be fixed in the CUDA 10.2 release.", "comments": ["@bas-aarts Can you please resolve conflicts? Thanks!", "@mihaimaruseac , @r4nt, in the hope to avoid another merge conflict for this PR, any change to have someone review this? Thanks."]}, {"number": 34076, "title": "Fixed assert_shapes broken code in documentation", "body": "Issue #33974", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34076) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "@MarkDaoust Can I know what all additional changes have to be done?\r\n", "Hi I did the reformatting to show you how for your next PR.\r\n\r\nBut what Email address did you sign the CLA with? \r\n\r\nYour commits are made with : \"38752758+dubesar@users.noreply.github.com\"\r\n\r\n", "@MarkDaoust  yes I have signed the cla with dubeysarvesh5525@gmail.com", "@MarkDaoust I have fixed the email problem as the '38752758+dubesar@users.noreply.github.com' was shown as I had kept the email address private.", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34076) for more info**.\n\n<!-- cla_yes -->", "Cool, for any future contributions be sure to configure your git/github so that that email address gets set as the author.", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34076) for more info**.\n\n<!-- need_sender_cla -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34076) for more info**.\n\n<!-- cla_yes -->", "@MarkDaoust Do I have to make more changes as some of the CL checks are failing?", "I'm not sure, it looks like the builder failed, not this code, all try pulling it.", "@MarkDaoust  Yes, I observed all the pull request is failing.", "Please make the PR against `master` branch, not `r2.0`."]}, {"number": 34075, "title": "Building TensorFlow 2 with bazel fails", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (Windows 10):\r\n- TensorFlow installed from (source):\r\n- TensorFlow version: 2.0 (master branch on 07 Nov 2019, to be exact)\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: No\r\n- Bazel version (if compiling from source): 0.29.1\r\n- GCC/Compiler version (if compiling from source):  8.1.0\r\n- CUDA/cuDNN version: 10.0/7\r\n- GPU model and memory: GeForce GTX 1050 15.88 GB RAM\r\n\r\n\r\n\r\nTrying to compile TF 2.0 with Bazel, ends up in a failure.\r\nI am struggling this isssue for quite a while, looked it up online, but havn't found any solution. Several running configuration (with or w/o Cuda, with or w/o creating zip file, etc) eventually leading to the same failure.\r\n\r\nMy actions:\r\n1. git checkout master (in the TF git repo)\r\n2. bazel clean\r\n3. configure (all defaults except for python path and Cuda)\r\n4. bazel build --config=cuda --define=no_tensorflow_py_deps=true tensorflow:tensorflow_cc.dll\r\n\r\nFailure message:\r\nERROR: C:/users/shahar/git/tensorflow/tensorflow/core/BUILD:2537:1: Executing genrule //tensorflow/core:version_info_gen failed (Exit 5)\r\nLAUNCHER ERROR: Cannot launch process: \"C:/Program Files/WindowsApps/PythonSoftwareFoundation.Python.3.7_3.7.1520.0_x64__qbz5n2kfra8p0/python.exe\" C:\\users\\shahar\\_bazel_shahar\\duchsbgv\\execroot\\org_tensorflow\\bazel-out\\x64_windows-opt\\bin\\tensorflow\\tools\\git\\gen_git_source.zip --generate external/local_config_git/gen/spec.json external/local_config_git/gen/head external/local_config_git/gen/branch_ref bazel-out/x64_windows-opt/bin/tensorflow/core/util/version_info.cc --git_tag_override=\r\nReason: (error: 5): Access is denied.\r\n\r\nWhat am i doing wrong?", "comments": ["According to the documentation, building `TensorFlow 2.0` wheel has only been tested with `Bazel 0.26.1`. Refer to this [table](https://www.tensorflow.org/install/source_windows#gpu).", "@ShaharATterrain, \r\nPlease include the full error log. \r\nTensorflow 2.0 supports bazel 0.26.1. Downgrade the bazel version and run configure.py, follow the steps mentioned [here](https://www.tensorflow.org/install/source_windows#build_the_pip_package). Thanks! ", "@nikochiko \r\n@gadagashwini ,\r\n- I have downgraded to bazel 0.26.1, yet the problem remains.\r\n- The link provided explains how to build the **python** API, and i need the **C++** API. Besides that, I was following these instructions to begin with.\r\n- This is the log, from the first ERROR message: \r\n```\r\nINFO: From Compiling external/grpc/src/cpp/util/time_cc.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-std=c++14'\r\nERROR: C:/users/shahar/git/tensorflow/tensorflow/core/BUILD:2537:1: Executing genrule //tensorflow/core:version_info_gen failed (Exit 5): bash.exe failed: error executing command\r\n  cd C:/users/shahar/_bazel_shahar/duchsbgv/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin;C:\\WINDOWS;C:\\WINDOWS\\System32;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\libnvvp;C:\\Program Files\\AdoptOpenJDK\\jdk-13.0.0.33-hotspot\\bin;C:\\Program Files (x86)\\Intel\\Intel(R) Management Engine Components\\iCLS\\;C:\\Program Files\\Intel\\Intel(R) Management Engine Components\\iCLS\\;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\Windows\\System32\\OpenSSH\\;C:\\Program Files (x86)\\Intel\\Intel(R) Management Engine Components\\DAL;C:\\Program Files\\Intel\\Intel(R) Management Engine Components\\DAL;C:\\Program Files (x86)\\Intel\\Intel(R) Management Engine Components\\IPT;C:\\Program Files\\Intel\\Intel(R) Management Engine Components\\IPT;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;C:\\Program Files\\mingw-w64\\x86_64-8.1.0-posix-seh-rt_v6-rev0\\mingw64\\bin;C:\\Program Files\\Boost\\bin;C:\\Program Files\\bazel;C:\\Program Files\\Git\\cmd;C:\\Program Files\\dotnet\\;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\msys64\\usr\\bin;C:\\Users\\Shahar\\AppData\\Local\\Microsoft\\WindowsApps\r\n    SET PYTHON_BIN_PATH=C:/python/python.exe\r\n    SET PYTHON_LIB_PATH=C:/python/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TF2_BEHAVIOR=1\r\n    SET TF_CONFIGURE_IOS=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1\r\n    SET TF_NEED_CUDA=1\r\n  C:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/x64_windows-opt/bin/tensorflow/tools/git/gen_git_source.exe --generate external/local_config_git/gen/spec.json external/local_config_git/gen/head external/local_config_git/gen/branch_ref \"bazel-out/x64_windows-opt/bin/tensorflow/core/util/version_info.cc\" --git_tag_override=${GIT_TAG_OVERRIDE:-}\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nLAUNCHER ERROR: Cannot launch process: \"C:/python/python.exe\" C:\\users\\shahar\\_bazel_shahar\\duchsbgv\\execroot\\org_tensorflow\\bazel-out\\x64_windows-opt\\bin\\tensorflow\\tools\\git\\gen_git_source.zip --generate external/local_config_git/gen/spec.json external/local_config_git/gen/head external/local_config_git/gen/branch_ref bazel-out/x64_windows-opt/bin/tensorflow/core/util/version_info.cc --git_tag_override=\r\nReason: (error: 5): Access is denied.\r\n\r\nTarget //tensorflow:tensorflow_cc.dll failed to build\r\nINFO: Elapsed time: 522.968s, Critical Path: 77.69s\r\nINFO: 633 processes: 633 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nIf it's better that i extract the error log in any other format, please tell me which.\r\nThanks.\r\n![Capture1](https://user-images.githubusercontent.com/52371777/68541621-6f9b6500-03aa-11ea-82f2-d8f8cd662aae.PNG)\r\n", "has anyone fixed this?im having the same problem on bazel 0.26.1...tensorflow 1.15...please help", "@soorajvinod123 \r\nPlease check [this link](https://stackoverflow.com/questions/58753233/building-tensorflow-2-with-bazel-0-29-1-on-windows-10-fails) with same error. Can you please upgrade to later versions of tf and let us know if you still face any issues.\r\n@ShaharATterrain \r\nIs this still an issue?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Has anyone found any fix for `access is denied` error whole building?", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34075\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34075\">No</a>\n"]}, {"number": 34074, "title": "build failed from source on windows 10", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWindows 10\r\n\r\n- TensorFlow version:\r\n1.14\r\n- Python version:\r\n3.5.4\r\n- Bazel version (if compiling from source):\r\n0.24.1\r\n\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\nCUDA10.0 & cuDNN7\r\n- GPU model and memory:\r\nGeForce GTX1050Ti\r\nCPU:\r\ni7-7700HQ\r\n\r\n- Command\r\nI've tried \r\n```\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n```\r\nand \r\n```\r\nbazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\n```\r\nnone of them works.\r\n\r\n**Describe the problem**\r\n\r\nI just follow the instructions of [https://www.tensorflow.org/install/source_windows](url), but it doesn't work.\r\n\r\n**Error Log**\r\n```\r\nERROR: D:/libs/tensorflow/tensorflow/BUILD:745:1: Executing genrule //tensorflow:tf_python_api_gen_v1 failed (Exit 127): bash.exe failed: error executing command\r\n  cd C:/users/donke/_bazel_donkey/4aoyooqm/execroot/org_tensorflow\r\n  SET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\libnvvp;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\Users\\donke\\.dnx\\bin;C:\\Program Files\\Microsoft DNX\\Dnvm\\;C:\\Program Files\\Microsoft SQL Server\\130\\Tools\\Binn\\;D:\\dev\\bin;C:\\WINDOWS\\System32\\OpenSSH\\;C:\\Program Files\\CMake\\bin;C:\\Program Files\\MATLAB\\R2016a\\runtime\\win64;C:\\Program Files\\MATLAB\\R2016a\\bin;C:\\Program Files\\MATLAB\\R2016a\\polyspace\\bin;D:\\dev\\x86\\vc14\\bin;C:\\Program Files\\nodejs\\;D:\\mysql-8.0.16-winx64\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\extras\\CUPTI\\libx64;C:\\Program Files\\Git\\cmd;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;C:\\msys64\\usr\\bin;C:\\Users\\donke\\AppData\\Local\\Programs\\Python\\Python35;C:\\Users\\donke\\AppData\\Local\\Programs\\Python\\Python35\\Scripts;C:\\Program Files\\dotnet\\;C:\\Users\\donke\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\donke\\AppData\\Roaming\\npm;C:\\msys64\\usr\\bin;\r\n    SET PYTHON_BIN_PATH=C:/Users/donke/AppData/Local/Programs/Python/Python35/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/donke/AppData/Local/Programs/Python/Python35/lib/site-packages\r\n    SET TF_CONFIGURE_IOS=0\r\n    SET TF_DOWNLOAD_CLANG=0\r\n    SET TF_NEED_CUDA=0\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n  C:/msys64/usr/bin/bash.exe bazel-out/x64_windows-opt/genfiles/tensorflow/tf_python_api_gen_v1.genrule_script.sh\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 18.207s, Critical Path: 1.84s\r\nINFO: 1 process: 1 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "comments": ["@YujieLu \r\n Can you please attach a complete log of `./configure` . Thanks!", "```\r\npython configure.py\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.24.1 installed.\r\nPlease specify the location of python. [Default is C:\\Users\\donke\\AppData\\Local\\Programs\\Python\\Python35\\python.exe]:\r\n\r\n\r\nFound possible Python library paths:\r\n  C:\\Users\\donke\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [C:\\Users\\donke\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]:\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]:\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nFound CUDA 10.0 in:\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/lib/x64\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/include\r\nFound cuDNN 7 in:\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/lib/x64\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/include\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]:\r\n\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\n\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:\r\nEigen strong inline overridden.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=gdr            # Build with GDR support.\r\n        --config=verbs          # Build with libverbs support.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=noignite       # Disable Apache Ignite support.\r\n        --config=nokafka        # Disable Apache Kafka support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\n```", "@YujieLu This issue page might help: [https://github.com/tensorflow/tensorflow/issues/21362](https://github.com/tensorflow/tensorflow/issues/21362)", "Got a similar issue, stuck on it for a whole week. have you solved it now? @YujieLu ", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34074\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34074\">No</a>\n"]}, {"number": 34073, "title": "Problem with fit_generator when using a generator for training and a fixed set for validation.", "body": "**System information**\r\n- I have written custom code. Code is available here as a Google Colab notebook: https://drive.google.com/open?id=1xp5LES0HiEEPWozrD4HR5DPPa-dce1mH\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): The code was run in Google Colab, with the version of all packages defined by Google Colab at this date (07 Nov 2019). \r\n- TensorFlow version: 2.0.0\r\n- Python version: 3.6.7\r\n\r\n**Describe the current behavior**\r\nThe provided code, originally written by Xifeng Guo for Keras with Tensorflow 1.2 as backend, was updated to run in Tensorflow 2.0. The code builds a Capsule Network as defined by Hinton's paper  'Dynamic Routing Between Capsules'. It is trained/tested with MNIST dataset.\r\n\r\nThe code gives two options: either training with model.fit (it works perfectly) or with model.fit_generator, where a generator makes only shifting augmentation (the problem is here!). \r\nThis part of the code remained as it was originally written by Mr. Guo, as I think it does not need to be updated (see below, where the part of model.fit was commented out). Note that in the original implementation of CapsNets there is an encoder (x is input, y is output) and a decoder (y is input, x is output), thus the input of the network is x=[x_train, y_train] and the output is y=[y_train, x_train]. \r\n\r\n\r\n\r\n    \"\"\"\r\n    # Training without data augmentation:\r\n    model.fit([x_train, y_train], [y_train, x_train], \r\n              batch_size=args.batch_size, \r\n              epochs=args.epochs,\r\n              validation_data=[[x_test, y_test], [y_test, x_test]], \r\n              callbacks=[log, tb, checkpoint, lr_decay])\r\n    \"\"\"\r\n\r\n    # Begin: Training with data augmentation ----------------------------------#\r\n    def train_generator(x, y, batch_size, shift_fraction=0.):\r\n      train_datagen = ImageDataGenerator(width_shift_range=shift_fraction,\r\n                                          height_shift_range=shift_fraction)  \r\n      generator = train_datagen.flow(x, y, batch_size=batch_size)\r\n      while True:\r\n        x_batch, y_batch = generator.next()\r\n        yield ([x_batch, y_batch], [y_batch, x_batch])\r\n\r\n    # Training with data augmentation. If shift_fraction=0 then no augmentation.\r\n    model.fit_generator(generator=train_generator(x_train, y_train, \r\n                                                  args.batch_size, \r\n                                                  args.shift_fraction),\r\n                        steps_per_epoch=5, #int(y_train.shape[0] / args.batch_size),\r\n                        epochs=args.epochs,\r\n                        validation_data=[[x_test, y_test], [y_test, x_test]],\r\n                        callbacks=[log, tb, checkpoint, lr_decay])\r\n    # End: Training with data augmentation ------------------------------------#\r\n\r\nIf fit_generator is used (with the indicated generator), there is a problem when the function ends generating training samples and starts using the validation set to test the model, giving this error:\r\n\r\n> ValueError: could not broadcast input array from shape (100,28,28,1) into shape (100)\r\n\r\nRemember that the MNIST dataset contains gray-scale images of 28x28 pixels, and the batch_size here is 100 images.\r\n\r\nInterestingly, if I change the validation_data line from ...\r\n\r\n> validation_data=[[x_test, y_test], [y_test, x_test]],\r\n\r\n... to a generator with no augmentation...\r\n\r\n> validation_data=train_generator(x_test, y_test, args.batch_size)\r\n\r\n... it works perfectly.\r\nAnd, again, if I do not use the fit_generator but the model.fit, it also works fine.\r\n\r\n**Describe the expected behavior**\r\nAs described, fit_generator should be able to deal with validation_data correctly.\r\n\r\n**Code to reproduce the issue**\r\nThe issue can be easily reproducible in the following notebook (just run all the cells!). I have reduced the steps_per_epoch=5 to go fast to the point where the code breaks.\r\nhttps://drive.google.com/open?id=1xp5LES0HiEEPWozrD4HR5DPPa-dce1mH", "comments": ["Issue replicating for the given code in TF-2.0.Thanks!", "I think this is now obsolete. As of https://github.com/tensorflow/tensorflow/commit/ac20030c96d37e980333b604402ef6dba48ef5e2, `fit_generator` just calls `fit` (which supports generators by turning them into Datasets). Previously `fit_generator` was using a separate, somewhat antiquated code path. With the consolidation both endpoints are now using the more modern codepath, and when I test your colab with the most recent `tf-nightly` it runs without issue. I'm going to close this as fixed; feel free to re-open if you still encounter issues.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34073\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34073\">No</a>\n", "Thanks!\r\nYou are right, the nightly version works perfectly. I did not occur to me to try tf-nightly before submitting the issue. "]}, {"number": 34072, "title": "AttributeError: 'Module' object has no attribute 'app'", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Mojave 10.14.5\r\n- TensorFlow version (use command below): Tensorflow for poets 2\r\n- Python version: 2.7.17\r\n\r\n**Describe the current behavior**\r\nI tried to run the training for \"tensorflow for poets 2\" and it shows me this:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\r\n    \"__main__\", fname, loader, pkg_name)\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/runpy.py\", line 72, in _run_code\r\n    exec code in run_globals\r\n  File \"/Users/fabian/tensorflow-for-poets-2/scripts/retrain.py\", line 1326, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\nAttributeError: 'module' object has no attribute 'app'\r\n```\r\n\r\n\r\nI'm not so familiar with coding, but is there a way to resolve this problem?\r\nThanks", "comments": ["tf.app is a v1 feature. Use tf.compat.v1.app instead.\r\n"]}, {"number": 34071, "title": "[Bug] Wrong device placement for tf.constant with int32", "body": "Colab example: https://colab.research.google.com/drive/1iCyTZhKzco4CjxY17g37jKG9GS35-NGb\r\nRemember to use GPU instance\r\n\r\nWhen dtype is int32, tf.constant didn't place tensor on gpu but cpu.\r\n\r\n```python\r\nwith tf.device(\"/gpu:0\"):\r\n  a = tf.constant([0,1], dtype=tf.float32)\r\nprint(a.device)\r\n# '/job:localhost/replica:0/task:0/device:GPU:0'\r\n\r\nwith tf.device(\"/gpu:0\"):\r\n  b = tf.constant([0,1], dtype=tf.int32)\r\nprint(b.device)\r\n# '/job:localhost/replica:0/task:0/device:CPU:0'\r\n```\r\n\r\n\r\n\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n", "comments": ["I have tried on colab with TF version 2.0 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/3dffedfdab275f96a166afa5963db220/untitled340.ipynb).Thanks!", "I think this is intentional, see for example this comment: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/eager/pywrap_tensor.cc#L352-L379", "I didn't find any documentation about this and it makes no sense to me (although it's intentional). You can add op like `constant_cpu` to force allocating tensor on cpu, but should not break the consistence without any documentation. ", "At least it should have some logs telling user it's allocated on cpu", "@VoVAllen I'd be happy to review PRs adding documentation / logging that would have helped you figure this out more easily.\r\n\r\nCC @jaingaurav ", "I believe this is intentional as can be seen in #28051. ", "@samikama I would say although it's intentional it's also necessary to mention it somewhere in docs, since this is super counter intuitive. I'm not sure that whether all the output of int32 will on the cpu, so I'm afraid I may not be able to help on the documentation. \r\n", "A fix went in for this and other problems with `tf.constant` in 97cdd4d16a81a349696f10451b7d564bfa99664f, which will make it create CPU tensors irrespective of device settings. This fixes other bugs, including one often requested one for \"always placing string tensors on the CPU.\" The documentation was updated to be explicit about this.\r\n\r\nThere were a few solution alternatives, but the \"always CPU tensor\" was the most consistent, simple, and the only one which did not introduce performance regressions. One of the ideas behind the solution is that the constant you create with, say,\r\n\r\n```python\r\nx = tf.constant(very_large_numpy_array)\r\n```\r\n\r\nIs both potentially large, and _already_ on the CPU, so it makes sense for `tf.constant` to just keep things there. With the default device policy (see `tf.config.experimental.set_device_policy`), the CPU tensor will be automatically copied to the GPU whenever needed (only once, and the copy will be cached so there's no performance drawbacks). If you want to explicitly place the tensor on the GPU (essentially controlling when the  copy happens), you can use `tf.identity` under the proper `tf.device` context.", "That makes more sense if the gpu copy is cached. I discovered this problem when debugging with tf code's performance. Let's say if I had an op `gather_row(index, tensor)` and `index` is tf.int32. And it executes on gpu and requires both index and tensor on gpu. So the index here would be copied only once for multiple execution, right?", "So all tensors created with `tf.constant` are now on host memory irrelevant to the device context now? No matter what its type is?", "@VoVAllen: You are absolutely correct about the caching and as part of this placement policy we have added the functionality such that the GPU copy is indeed cached.", "Closing the issue since I believe this issue is now resolved. However, please feel free to continue discussion or ask questions."]}, {"number": 34070, "title": "Keras named inputs lost with SavedModel format", "body": "**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Google Colab, TF 2.0**\r\n\r\n**Describe the current behavior**\r\nIf I define a Keras model with named inputs (`Input(name=\"x_embedding\")`), I can use a Dict as input with the original model or after saving to h5, but not with the new SavedModel format: The name of my input layer is changed from `x_embedding` to `input_1`.\r\n\r\n**Describe the expected behavior**\r\nThe model loaded from SavedModel should keep the named inputs.\r\n\r\n**Code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/19ICXHeL4tzAN9z1LCi-3Su8l5X2_NhxM\r\n\r\n```python\r\ntry:\r\n  %tensorflow_version 2.x\r\nexcept Exception:\r\n  pass\r\n  \r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.keras.layers import Input, Dense\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.models import load_model\r\n\r\ninput_seq = Input(shape=(1,), name=\"x_embedding\")\r\nmodel = Model(inputs=input_seq, outputs=input_seq)\r\nmodel.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")\r\n\r\nx = {\"x_embedding\": np.array([0.1] * 100)}\r\ny = np.array([0] * 100)\r\n\r\nmodel.fit(x,y)\r\n\r\n# Works with original model\r\nres = model.evaluate(x,y)\r\nprint(\"model.evaluate =\", res)\r\n\r\n# Works with h5 model\r\nmodel.save(\"my_h5Model.h5\", save_format=\"h5\")\r\nmodel2 = load_model(\"my_h5Model.h5\")\r\nres = model2.evaluate(x,y)\r\nprint(\"model2.evaluate =\", res)\r\n\r\n# Doesn't work with SavedModel format\r\nmodel.save(\"my_SavedModel\", save_format=\"tf\")\r\nmodel3 = load_model(\"my_SavedModel\")\r\nres = model3.evaluate(x,y)\r\nprint(\"model3.evaluate =\", res)\r\n\r\n# Work with SavedModel format if I rename x_embedding -> input_1\r\nx2 = {\"input_1\": np.array([0.1] * 100)}\r\nres = model3.evaluate(x2,y)\r\nprint(\"model3.evaluate =\", res)\r\n```\r\n**Other info / logs**\r\n`ValueError: No data provided for \"input_1\". Need data for each key in: ['input_1']`", "comments": ["Still an issue with `tf-nightly==2.1.0.dev20191126`", "@louisjc This is fixed with the latest tf-nightly(1/22): here is an updated [gist](https://colab.research.google.com/drive/19ICXHeL4tzAN9z1LCi-3Su8l5X2_NhxM?usp=sharing)\r\n\r\n\r\n", "Great, thanks @goldiegadde !", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34070\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34070\">No</a>\n"]}, {"number": 34069, "title": "InternalError:  Blas GEMM launch failed", "body": "**System information**\r\n- Uusing a stock example script provided in TensorFlow (first guide with mnist dataset), so code is 100% right. And this problem haunts me in every model :(\r\n- OS Platform: Windows 10\r\n- TensorFlow installed from (source or binary): pip3 install tensorflow-gpu . And i have a quustion here: shouild i install keras? I think its already in tf 2.0\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.7.5\r\n- CUDA/cuDNN version: 10.1 / 7.6.5\r\n- GPU model and memory: GTX 1060 3GB\r\n\r\n\r\n**Describe the problem**\r\nI train the model described in the guides on the official tf website, so there can be no problems with the code. When I start learning the model, I have a bug from below. This happens not only on this code, but also on others. I have the latest versions of cuda and cudnn, visual studio 2017.\r\n\r\n**Interesting**\r\nOn one of the sites I saw a solution to the problem as follows: conda update --all. Indeed, after this command and reboot I can train the model, but only ONCE, the next model will be with the same error.\r\n\r\n**Code to reproduce the issue**\r\nmodel = keras.Sequential([\r\n    keras.layers.Flatten(input_shape=(28, 28)),\r\n    keras.layers.Dense(128, activation='relu'),\r\n    keras.layers.Dense(10, activation='softmax')\r\n])\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\nmodel.fit(train_images, train_labels, epochs=10)\r\n\r\n**Error Log**\r\n---------------------------------------------------------------------------\r\nInternalError                             Traceback (most recent call last)\r\n<ipython-input-10-41cd0b44e416> in <module>\r\n      2 \r\n      3 model.fit(partial_train_data, partial_train_targets,\r\n----> 4         epochs=num_epochs, batch_size=1, verbose=0)\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    726         max_queue_size=max_queue_size,\r\n    727         workers=workers,\r\n--> 728         use_multiprocessing=use_multiprocessing)\r\n    729 \r\n    730   def evaluate(self,\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\r\n    322                 mode=ModeKeys.TRAIN,\r\n    323                 training_context=training_context,\r\n--> 324                 total_epochs=epochs)\r\n    325             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)\r\n    326 \r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\r\n    121         step=step, mode=mode, size=current_batch_size) as batch_logs:\r\n    122       try:\r\n--> 123         batch_outs = execution_function(iterator)\r\n    124       except (StopIteration, errors.OutOfRangeError):\r\n    125         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py in execution_function(input_fn)\r\n     84     # `numpy` translates Tensors to values in Eager mode.\r\n     85     return nest.map_structure(_non_none_constant_value,\r\n---> 86                               distributed_function(input_fn))\r\n     87 \r\n     88   return execution_function\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in __call__(self, *args, **kwds)\r\n    455 \r\n    456     tracing_count = self._get_tracing_count()\r\n--> 457     result = self._call(*args, **kwds)\r\n    458     if tracing_count == self._get_tracing_count():\r\n    459       self._call_counter.called_without_tracing()\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in _call(self, *args, **kwds)\r\n    518         # Lifting succeeded, so variables are initialized and we can run the\r\n    519         # stateless function.\r\n--> 520         return self._stateless_fn(*args, **kwds)\r\n    521     else:\r\n    522       canon_args, canon_kwds = \\\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in __call__(self, *args, **kwargs)\r\n   1821     \"\"\"Calls a graph function specialized to the inputs.\"\"\"\r\n   1822     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n-> 1823     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   1824 \r\n   1825   @property\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _filtered_call(self, args, kwargs)\r\n   1139          if isinstance(t, (ops.Tensor,\r\n   1140                            resource_variable_ops.BaseResourceVariable))),\r\n-> 1141         self.captured_inputs)\r\n   1142 \r\n   1143   def _call_flat(self, args, captured_inputs, cancellation_manager=None):\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1222     if executing_eagerly:\r\n   1223       flat_outputs = forward_function.call(\r\n-> 1224           ctx, args, cancellation_manager=cancellation_manager)\r\n   1225     else:\r\n   1226       gradient_name = self._delayed_rewrite_functions.register()\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in call(self, ctx, args, cancellation_manager)\r\n    509               inputs=args,\r\n    510               attrs=(\"executor_type\", executor_type, \"config_proto\", config),\r\n--> 511               ctx=ctx)\r\n    512         else:\r\n    513           outputs = execute.execute_with_cancellation(\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     65     else:\r\n     66       message = e.message\r\n---> 67     six.raise_from(core._status_to_exception(e.code, message), None)\r\n     68   except TypeError as e:\r\n     69     keras_symbolic_tensors = [\r\n\r\n~\\Anaconda3\\lib\\site-packages\\six.py in raise_from(value, from_value)\r\n\r\nInternalError:  Blas GEMM launch failed : a.shape=(1, 13), b.shape=(13, 64), m=1, n=64, k=13\r\n\t [[node sequential/dense/MatMul (defined at C:\\Users\\Gera\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1751) ]] [Op:__inference_distributed_function_785]\r\n\r\nFunction call stack:\r\ndistributed_function", "comments": ["@germanjke ,\r\nCan you please refer links of these similar issue's and solutions present in them and see if it solves the issue, [1 ](https://github.com/tensorflow/tensorflow/issues/11812)[2](https://stackoverflow.com/questions/37337728/tensorflow-internalerror-blas-sgemm-launch-failed) and [3](https://stackoverflow.com/questions/43990046/tensorflow-blas-gemm-launch-failed?rq=1).Thanks!", "> @germanjke ,\r\n> Can you please refer links of these similar issue's and solutions present in them and see if it solves the issue, [1 ](https://github.com/tensorflow/tensorflow/issues/11812)[2](https://stackoverflow.com/questions/37337728/tensorflow-internalerror-blas-sgemm-launch-failed) and [3](https://stackoverflow.com/questions/43990046/tensorflow-blas-gemm-launch-failed?rq=1).Thanks!\r\n\r\n@oanush \r\nI got the decision on the first link. The problem was that I had discovered too many JupyterNotebooks. When I closed them, launched one and rebooted kernel everything works. Thank you very much!", "Thank you,closing since issue is resolved.", "i have done everything listed here but none is working"]}, {"number": 34068, "title": "TF2 keras.models.load_model fails with custom metrics (both h5 and tf format)", "body": "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.6\r\n- TensorFlow installed from (source or binary): pip install tensorflow==2.0.0-beta1\r\n- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d382ca 2.0.0\r\n- Python version: v3.6.7:6ec5cf24b7, Oct 20 2018, 03:02:14\r\n\r\n**Describe the current behavior**\r\n\r\nI have a custom metric in my model and using `tf.keras.models.load_model` with `compile=True` after saving it results in an error in almost all cases, whereas I use the `custom_objects` argument according to [the documentation](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/models/load_model).\r\n\r\nI tried to pass my custom metric with two strategies: by passing a custom function `custom_accuracy` to the `tf.keras.Model.compile` method, or by subclassing the `MeanMetricWrapper` class and giving an instance of my subclass named `CustomAccuracy` to `tf.keras.Model.compile`.\r\n\r\nI also tried the two different saving format available: h5 and tf. Here are my results:\r\n\r\n- with tf format:\r\n    - with custom function: \r\n    fail with `ValueError` message `Unknown metric function:custom_accuracy`\r\n    - with subclassed metric:\r\n    fail with `ValueError` message `Unknown metric function: CustomAccuracy`\r\n- with h5 format:\r\n    - with custom function: \r\n    success\r\n    - with subclassed metric:\r\n    fail with `TypeError` message `must be str, not ABCMeta`\r\n\r\nNote that given the complete error logs (see below), the error with h5 format and subclassed metric is in fact the same as the error with the tf format. The `TypeError` occurs when the code tries to raise the `ValueError`.\r\n\r\n**Describe the expected behavior**\r\n\r\nThis should not fail in any case, except if I am using the `custom_objects` argument wrong. The documentation could be a little expanded on that matter by the way.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\nprint('Using Tensorflow version {} (git version {})'.format(tf.version.VERSION, tf.version.GIT_VERSION))\r\n\r\nfrom tensorflow.python.keras.metrics import MeanMetricWrapper\r\nfrom tensorflow.python.keras.metrics import accuracy\r\n\r\ndef custom_accuracy(y_true, y_pred):\r\n    return accuracy(y_true, y_pred)\r\n\r\nclass CustomAccuracy(MeanMetricWrapper):\r\n\r\n    def __init__(self, **kwargs):\r\n        super(CustomAccuracy, self).__init__(custom_accuracy, **kwargs)\r\n        \r\ndef make_model():\r\n    inp = tf.keras.Input(shape=(2,))\r\n    x = tf.keras.layers.Dense(4)(inp)\r\n    return tf.keras.Model(inp, x)\r\n\r\n\r\nfor save_format in ['tf', 'h5']:\r\n    \r\n    print(\"\\nTrying with save_format='{}':\\n\".format(save_format))\r\n    \r\n    model_with_function = make_model()\r\n    model_with_function.compile(loss='mse', metrics=[custom_accuracy])\r\n    model_with_function.save('/tmp/model_with_function' + '.' + save_format, \r\n                             save_format=save_format)\r\n    \r\n    try:\r\n        new_model = tf.keras.models.load_model('/tmp/model_with_function' + '.' + save_format, \r\n                                               custom_objects={'custom_accuracy': custom_accuracy}, \r\n                                               compile=True)\r\n        print(\"model_with_function loaded with the following metrics:\")\r\n        print(new_model.metrics)\r\n    except Exception as e:\r\n        print(\"model_with_function not loaded with the following error:\")\r\n        print(type(e))\r\n        print(e)\r\n        \r\n    model_with_subclass = make_model()\r\n    model_with_subclass.compile(loss='mse', metrics=[CustomAccuracy()])\r\n    model_with_subclass.save('/tmp/model_with_subclass' + '.' + save_format, \r\n                             save_format=save_format)\r\n    \r\n    try:\r\n        new_model = tf.keras.models.load_model('/tmp/model_with_subclass' + '.' + save_format, \r\n                                               custom_objects={'CustomAccuracy': CustomAccuracy}, \r\n                                               compile=True)\r\n        print(\"model_with_subclass loaded with the following metrics:\")\r\n        print(new_model.metrics)\r\n    except Exception as e:\r\n        print(\"model_with_subclass not loaded with the following error:\")\r\n        print(type(e))\r\n        print(e)\r\n```\r\n\r\n**Other info / logs**\r\nThe logs are the same in the 3 error cases (to get them with the code above, just add `raise`at the end of the `except` blocks):\r\n```\r\n<ipython-input-14-ac0a72b492dc> in <module>\r\n     48         new_model = tf.keras.models.load_model('/tmp/model_with_subclass' + '.' + save_format, \r\n     49                                                custom_objects={'CustomAccuracy': CustomAccuracy},\r\n---> 50                                                compile=True)\r\n     51         print(\"model_with_function loaded with the following metrics:\")\r\n     52         print(new_model.metrics)\r\n\r\n/path/to/tensorflow_core/python/keras/saving/save.py in load_model(filepath, custom_objects, compile)\r\n    148   if isinstance(filepath, six.string_types):\r\n    149     loader_impl.parse_saved_model(filepath)\r\n--> 150     return saved_model_load.load(filepath, compile)\r\n    151 \r\n    152   raise IOError(\r\n\r\n/path/to/tensorflow_core/python/keras/saving/saved_model/load.py in load(path, compile)\r\n     91     if model._training_config is not None:  # pylint: disable=protected-access\r\n     92       model.compile(**saving_utils.compile_args_from_training_config(\r\n---> 93           model._training_config))  # pylint: disable=protected-access\r\n     94 \r\n     95   return model\r\n\r\n/path/to/tensorflow_core/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    455     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    456     try:\r\n--> 457       result = method(self, *args, **kwargs)\r\n    458     finally:\r\n    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n/path/to/tensorflow_core/python/keras/engine/training.py in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\r\n    354     with K.get_graph().as_default():\r\n    355       # Save all metric attributes per output of the model.\r\n--> 356       self._cache_output_metric_attributes(metrics, weighted_metrics)\r\n    357 \r\n    358       # Set metric attributes on model.\r\n\r\n/path/to/tensorflow_core/python/keras/engine/training.py in _cache_output_metric_attributes(self, metrics, weighted_metrics)\r\n   1899         output_shapes.append(output.shape.as_list())\r\n   1900     self._per_output_metrics = training_utils.collect_per_output_metric_info(\r\n-> 1901         metrics, self.output_names, output_shapes, self.loss_functions)\r\n   1902     self._per_output_weighted_metrics = (\r\n   1903         training_utils.collect_per_output_metric_info(\r\n\r\n/path/to/tensorflow_core/python/keras/engine/training_utils.py in collect_per_output_metric_info(metrics, output_names, output_shapes, loss_fns, is_weighted)\r\n    811     metrics_dict = OrderedDict()\r\n    812     for metric in metrics:\r\n--> 813       metric_name = get_metric_name(metric, is_weighted)\r\n    814       metric_fn = get_metric_function(\r\n    815           metric, output_shape=output_shapes[i], loss_fn=loss_fns[i])\r\n\r\n/path/to/tensorflow_core/python/keras/engine/training_utils.py in get_metric_name(metric, weighted)\r\n    985       return metric\r\n    986 \r\n--> 987     metric = metrics_module.get(metric)\r\n    988     return metric.name if hasattr(metric, 'name') else metric.__name__\r\n    989   else:\r\n\r\n/path/to/tensorflow_core/python/keras/metrics.py in get(identifier)\r\n   2855 def get(identifier):\r\n   2856   if isinstance(identifier, dict):\r\n-> 2857     return deserialize(identifier)\r\n   2858   elif isinstance(identifier, six.string_types):\r\n   2859     return deserialize(str(identifier))\r\n\r\n/path/to/tensorflow_core/python/keras/metrics.py in deserialize(config, custom_objects)\r\n   2849       module_objects=globals(),\r\n   2850       custom_objects=custom_objects,\r\n-> 2851       printable_module_name='metric function')\r\n   2852 \r\n   2853 \r\n\r\n/path/to/tensorflow_core/python/keras/utils/generic_utils.py in deserialize_keras_object(identifier, module_objects, custom_objects, printable_module_name)\r\n    178     config = identifier\r\n    179     (cls, cls_config) = class_and_config_for_serialized_keras_object(\r\n--> 180         config, module_objects, custom_objects, printable_module_name)\r\n    181 \r\n    182     if hasattr(cls, 'from_config'):\r\n\r\n/path/to/tensorflow_core/python/keras/utils/generic_utils.py in class_and_config_for_serialized_keras_object(config, module_objects, custom_objects, printable_module_name)\r\n    163     cls = module_objects.get(class_name)\r\n    164     if cls is None:\r\n--> 165       raise ValueError('Unknown ' + printable_module_name + ': ' + class_name)\r\n    166   return (cls, config['config'])\r\n    167 \r\n```\r\n\r\n", "comments": ["@durandg12 \r\nI tried reproducing the code in colab using TF 2.0 beta1, TF 2.0 and i am seeing different error messages. Kindly , provide minimal stand alone reproducible code,it  helps us in localizing the issue faster.Please, find the gist [here.](https://colab.sandbox.google.com/gist/ravikyram/1780a9fe7e8d0e39cf52ba6e973d22e6/untitled342.ipynb) Thanks!", "@ravikyram \r\nI have looked at your gist.\r\nThe error messages in your gist for tf2.0.0 are exactly the same as mine.\r\nThere is also a deprecation warning that I have too but that I hadn't copied in my first message because it didn't seem relevant. \r\n\r\nFor tf2.0.0-beta1 the error message is effectively different but it comes from the `compile` method because I call it without an `optimizer` argument. I have added `optimizer='adam'` in my `compile` call and now the output is the same for 2.0.0 and 2.0.0-beta1. So the code now looks like this:\r\n```\r\nimport tensorflow as tf\r\n\r\nprint('Using Tensorflow version {} (git version {})'.format(tf.version.VERSION, tf.version.GIT_VERSION))\r\n\r\nfrom tensorflow.python.keras.metrics import MeanMetricWrapper\r\nfrom tensorflow.python.keras.metrics import accuracy\r\n\r\ndef custom_accuracy(y_true, y_pred):\r\n    return accuracy(y_true, y_pred)\r\n\r\nclass CustomAccuracy(MeanMetricWrapper):\r\n\r\n    def __init__(self, **kwargs):\r\n        super(CustomAccuracy, self).__init__(custom_accuracy, **kwargs)\r\n        \r\ndef make_model():\r\n    inp = tf.keras.Input(shape=(2,))\r\n    x = tf.keras.layers.Dense(4)(inp)\r\n    return tf.keras.Model(inp, x)\r\n\r\n\r\nfor save_format in ['tf', 'h5']:\r\n    \r\n    print(\"\\nTrying with save_format='{}':\\n\".format(save_format))\r\n    \r\n    model_with_function = make_model()\r\n    model_with_function.compile(optimizer='adam', loss='mse', metrics=[custom_accuracy])\r\n    model_with_function.save('/tmp/model_with_function' + '.' + save_format, \r\n                             save_format=save_format)\r\n    \r\n    try:\r\n        new_model = tf.keras.models.load_model('/tmp/model_with_function' + '.' + save_format, \r\n                                               custom_objects={'custom_accuracy': custom_accuracy}, \r\n                                               compile=True)\r\n        print(\"model_with_function loaded with the following metrics:\")\r\n        print(new_model.metrics)\r\n    except Exception as e:\r\n        print(\"model_with_function not loaded with the following error:\")\r\n        print(type(e))\r\n        print(e)\r\n        \r\n    model_with_subclass = make_model()\r\n    model_with_subclass.compile(optimizer='adam', loss='mse', metrics=[CustomAccuracy()])\r\n    model_with_subclass.save('/tmp/model_with_subclass' + '.' + save_format, \r\n                             save_format=save_format)\r\n    \r\n    try:\r\n        new_model = tf.keras.models.load_model('/tmp/model_with_subclass' + '.' + save_format, \r\n                                               custom_objects={'CustomAccuracy': CustomAccuracy}, \r\n                                               compile=True)\r\n        print(\"model_with_function loaded with the following metrics:\")\r\n        print(new_model.metrics)\r\n    except Exception as e:\r\n        print(\"model_with_function not loaded with the following error:\")\r\n        print(type(e))\r\n        print(e)\r\n```\r\n\r\nI think that my code was already minimal as it just:\r\n\r\n1.  created the simplest custom accuracy possible\r\n2. created the simplest MLP possible\r\n3. compiled the MLP with the custom accuracy\r\n4. saved the MLP\r\n5. loaded the MLP\r\n\r\nI don't know how I can make it simpler. Except if you want the same piece of code but without the `print` calls and without the `try` and `except` blocks. In this case here it is:\r\n```\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.python.keras.metrics import MeanMetricWrapper\r\nfrom tensorflow.python.keras.metrics import accuracy\r\n\r\ndef custom_accuracy(y_true, y_pred):\r\n    return accuracy(y_true, y_pred)\r\n\r\nclass CustomAccuracy(MeanMetricWrapper):\r\n\r\n    def __init__(self, **kwargs):\r\n        super(CustomAccuracy, self).__init__(custom_accuracy, **kwargs)\r\n        \r\ndef make_model():\r\n    inp = tf.keras.Input(shape=(2,))\r\n    x = tf.keras.layers.Dense(4)(inp)\r\n    return tf.keras.Model(inp, x)\r\n\r\nSAVE_FORMAT = 'h5'\r\n#SAVE_FORMAT = 'tf'\r\n\r\nmodel_with_function = make_model()\r\nmodel_with_function.compile(optimizer='adam', loss='mse', metrics=[custom_accuracy])\r\nmodel_with_function.save('/tmp/model_with_function' + '.' + SAVE_FORMAT, \r\n                         save_format=SAVE_FORMAT)\r\n\r\n\r\nnew_model = tf.keras.models.load_model('/tmp/model_with_function' + '.' + SAVE_FORMAT, \r\n                                           custom_objects={'custom_accuracy': custom_accuracy}, \r\n                                           compile=True)\r\n\r\n    \r\n#model_with_subclass = make_model()\r\n#model_with_subclass.compile(loss='mse', metrics=[CustomAccuracy()])\r\n#model_with_subclass.save('/tmp/model_with_subclass' + '.' + SAVE_FORMAT, \r\n#                         save_format=SAVE_FORMAT)\r\n#\r\n#new_model = tf.keras.models.load_model('/tmp/model_with_subclass' + '.' + SAVE_FORMAT, \r\n#                                           custom_objects={'CustomAccuracy': CustomAccuracy}, \r\n#                                           compile=True)\r\n```\r\nbut you have to manually comment or uncomment some parts if you want to observe all four cases.\r\n\r\nIronically, adding an optimizer for tf2.0.0-beta1 makes the code less minimal.", "@durandg12 \r\n\r\nThanks for the detailed explanation. After adding `optimizer='adam'` in compile call i am able to reproduce the same error message in both  TF 2.0.0 and 2.0.0-beta1. Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/17024d04ca0bc9a7e28cec4244715c7a/untitled355.ipynb).Thanks!", "@durandg12 Thanks for the detailed report. As of now there is no solution available. There are some workarounds suggested [here](https://github.com/tensorflow/tensorflow/issues/32612). There is a PR https://github.com/tensorflow/tensorflow/pull/33229 to resolve an issue similar to this issue. Please follow the PR and test it once it is approved and released in `tf-nightly`. Thanks!", "I have reviewed the issue you linked. It seems to be the same problem indeed. I had also found the workaround of loading without compile but as @somedadaism said [this post](https://github.com/tensorflow/tensorflow/issues/32612#issuecomment-534248672) it is not satisfying.\r\n\r\nSo right now the best workaround is to use a custom function and pass it to the `compile`method and not subclassing `MeanMetricWrapper`. But this only worked with `h5`format and not `tf`format, for which I don't find a satisfying workaround.\r\n\r\n@jvishnuvardhan I did not try the PR yet, I am not sure how to do it. Am I supposed to create a new virualenv and install `tf-nightly` in it? How do I know when the PR is approved and released?", "@durandg12 If you have a solution to an issue in Tensorflow, you can raise PR by going [here](https://github.com/tensorflow/tensorflow/pulls). You can edit related TF source code with your solution, test it locally, then checkit into PR. Tensorflow Team will review it and responds. If everything is looking good, then it will be approved and then merged into TF source code. [This](https://www.tensorflow.org/community/contribute) is a very good resource to start contributing. Hope this helps. Thanks!", "@jvishnuvardhan my question was more focused on your last sentence, as I know what is a PR in general. My question is how do I do this:\r\n\r\n> Please follow the PR and test it once it is approved and released in `tf-nightly`\r\n\r\nI see that the PR is actually awaiting review so it is not approved yet. Once it is approved, what steps do I need to follow? Can you confirm that I just have to set a new virtual env up, run `pip install tf-nightly`, and then try my example code?", "Once it is approved, you don't need to do anything. After approval, it will be merged into `tf-nightly`.  During the approval process, Reviewer will guide you through the steps if any required. Thanks!", "@durandg12 As of now https://github.com/tensorflow/tensorflow/pull/33229 was approved but not merged. Once it is merged, you can use `tf-nightly` to test it. Thanks! ", "@durandg12 Can you try `tf-nightly` tomorrow as the related PR merged. Please let us know whether it solved your issue or not. Thanks!", "@durandg12 Looks like `load_model` is working for both the cases when the model is saved in 'h5` format. [Here](https://colab.sandbox.google.com/gist/jvishnuvardhan/95b9d359762b766dba337e0cfa0c248b/untitled355.ipynb) is the gist. Thanks!\r\n\r\nBoth the cases are still failing when the model was saved in `tf` format. Thanks!", "I have seen your gist, and after installing tf-nightly I have been able to replicate it on my laptop, thank you.\r\n\r\nThe only small difference I see is that locally I have an additional warning:\r\n`WARNING: Logging before flag parsing goes to stderr.`", "This is fixed latest tf-nightly version `'2.2.0-dev20200123'`. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34068\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34068\">No</a>\n", "I have tested and the issue is indeed fixed.\r\n\r\nBut we shall note that the `tf`mode still raises a warning : \r\n```\r\nWARNING:tensorflow:From /path/to/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1809: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nIf using Keras pass *_constraint arguments to layers.\r\n```", "Hello! Was this ever solved for saving/loading custom metrics in SavedModel format opposed to .h5?", "I had the same issue, only my error was:\r\n\r\n> Unable to restore custom object of type _tf_keras_metric currently. Please make sure that the layer implements `get_config`and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\r\n\r\nI had subclassed `MeanMetricWrapper`, so it couldn't possibly have been a lack of implementing `get_config` and `from_config`, and I had already made up the `custom_objects` dict which had:\r\n\r\n```python\r\n{\r\n    \"mase_metric\": MeanAbsoluteScaledErrorMetric,\r\n    \"smape_metric\": SymmetricMeanAbsolutePercentErrorMetric,\r\n}\r\n```\r\nEverything was referenced correctly in the main script (model would run manually and through hyperparameter searches), but I kept getting this error whenever I tried loading the saved TF model. I then switched to saving/loading an H5 model instead, and got an error stating that `MeanAbsoluteScaledErrorMetric` wasn't included in `custom_objects`. So I updated it to:\r\n\r\n```python\r\n{\r\n    \"MeanAbsoluteScaledErrorMetric\": MeanAbsoluteScaledErrorMetric,\r\n    \"SymmetricMeanAbsolutePercentErrorMetric\": SymmetricMeanAbsolutePercentErrorMetric,\r\n    \"mase_metric\": MeanAbsoluteScaledErrorMetric,\r\n    \"smape_metric\": SymmetricMeanAbsolutePercentErrorMetric,\r\n}\r\n```\r\n...and then it worked. I then switched back to the TF model and it kept working. So in the end, I suppose somewhere in the loader it's not respecting the key/value relationship in `custom_objects` and only looking for the class name in the keys."]}, {"number": 34067, "title": "TF.distribute.MirroredStrategy() crashes ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Progress Linux 5+ (engywuck-backports) (Linux Debian Buster)\r\n- TensorFlow installed from (source or binary): binary (problem happens from source aswell)\r\n- TensorFlow version (use command below): v1.12.1-16854-g6778662 2.1.0-dev20191028\r\n- Python version: Python 3.7.3\r\n- CUDA/cuDNN version: 10.0/7.0\r\n- GPU model and memory: 2x Asus GeForxe RTX 2080 Ti, Compute Capability 7.5, No NVLink\r\n\r\n**Describe the current behavior**\r\nSince we are not allowed to share our data, I tried to reproduce our problem with a dataset from tensorflow_datasets.\r\nThe current code might not make much sense, but I am able to deliver a reproducible code with it. \r\nTraining on only one-gpu  works without a problem, with tf.distribute.MirroredStrategy() it crashes (see dump).\r\n\r\nWhat we already tried:\r\n- build tensorflow from source\r\n- build tensorflow from source against cuda10.1\r\n- using tensorflow via pip: tensorflow-gpu\r\n\r\n**Describe the expected behavior**\r\nTf.distribute.MirroredStrategy() should lead to similar Results like training on one-gpu only.\r\n\r\n**Code to reproduce the issue**\r\nI tried to reproduce the problem using google-colab. but since only one gpu is provided, it is not really reproducible.\r\nI tried it with two virtual GPUs, but it didn't lead to similar behavior like our problem.\r\nhttps://colab.research.google.com/drive/1hmqt9KdWoheKajkHl_J6dJgoHneKoJP3\r\n\r\nOn my set-up i use following code:\r\n``` python\r\n# -*- coding: utf-8 -*-\r\n\r\nimport numpy as np\r\nfrom tensorflow.keras.applications.resnet50 import ResNet50\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\n\r\nLENGTH_DATASET = 17509\r\nNUM_CLASSES = 9\r\nIMG_SHAPE = (256, 256, 3)\r\nBATCH_SIZE = 32\r\n\r\n\r\ndef mymap_func(features):\r\n    return features[\"image\"], features[\"label\"]\r\n\r\n\r\nAUTOTUNE = tf.data.experimental.AUTOTUNE\r\n\r\n# create input pipeline\r\ndataset = tfds.load(name=\"deep_weeds\", split=\"train\")\r\ndataset = dataset.map(mymap_func,\r\n                      num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\ndataset = dataset.cache()\r\ndataset = dataset.shuffle(buffer_size=LENGTH_DATASET, seed=42,\r\n                          reshuffle_each_iteration=True)\r\ndataset = dataset.batch(batch_size=BATCH_SIZE, drop_remainder=True).repeat()\r\ndataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\r\n\r\n\r\n# create model\r\nimg_width, img_height = 270, 270\r\n\r\nshape, classes = (img_width, img_height, 1), 3\r\n\r\nstrategy = tf.distribute.MirroredStrategy()\r\nprint(\"Number of devices in strategy: {}\".format(strategy.num_replicas_in_sync))\r\n\r\nwith strategy.scope():\r\n\r\n  model = ResNet50(include_top=True,\r\n                       weights=None,\r\n                       input_tensor=None,\r\n                       input_shape=IMG_SHAPE,\r\n                       pooling=None,\r\n                       classes=NUM_CLASSES)\r\n\r\n  model.compile(optimizer=tf.optimizers.Adam(),\r\n                    loss='sparse_categorical_crossentropy',\r\n                    metrics=[\"accuracy\"])\r\n\r\ntrain_steps = np.ceil(LENGTH_DATASET / BATCH_SIZE)\r\nhistory = model.fit(\r\n        x=dataset,\r\n        epochs=10,\r\n        verbose=1,\r\n        steps_per_epoch=train_steps,\r\n        use_multiprocessing=False,\r\n        workers=8)\r\n```\r\n\r\n**Other info / logs**\r\n\r\npython dump of above script\r\n``` bash\r\npython src/test/multi_gpu_training_colab.py \r\n2019-11-07 10:44:52.905250: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2019-11-07 10:44:52.950697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1546] Found device 0 with properties: \r\nname: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545\r\npciBusID: 0000:86:00.0\r\n2019-11-07 10:44:52.951317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1546] Found device 1 with properties: \r\nname: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545\r\npciBusID: 0000:af:00.0\r\n2019-11-07 10:44:52.951554: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-11-07 10:44:52.952809: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-11-07 10:44:52.953947: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-11-07 10:44:52.954263: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-11-07 10:44:52.955764: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-11-07 10:44:52.956986: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-11-07 10:44:52.960430: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-11-07 10:44:52.962770: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1674] Adding visible gpu devices: 0, 1\r\n2019-11-07 10:44:52.963103: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n2019-11-07 10:44:53.001056: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2100000000 Hz\r\n2019-11-07 10:44:53.008873: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5460110 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2019-11-07 10:44:53.008905: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2019-11-07 10:44:53.233141: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5521500 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2019-11-07 10:44:53.233177: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5\r\n2019-11-07 10:44:53.233185: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): GeForce RTX 2080 Ti, Compute Capability 7.5\r\n2019-11-07 10:44:53.234101: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1546] Found device 0 with properties: \r\nname: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545\r\npciBusID: 0000:86:00.0\r\n2019-11-07 10:44:53.234646: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1546] Found device 1 with properties: \r\nname: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545\r\npciBusID: 0000:af:00.0\r\n2019-11-07 10:44:53.234685: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-11-07 10:44:53.234699: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-11-07 10:44:53.234711: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-11-07 10:44:53.234723: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-11-07 10:44:53.234736: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-11-07 10:44:53.234748: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-11-07 10:44:53.234760: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-11-07 10:44:53.237620: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1674] Adding visible gpu devices: 0, 1\r\n2019-11-07 10:44:53.237669: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-11-07 10:44:53.239881: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1087] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-11-07 10:44:53.239900: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1093]      0 1 \r\n2019-11-07 10:44:53.239912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1106] 0:   N N \r\n2019-11-07 10:44:53.239922: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1106] 1:   N N \r\n2019-11-07 10:44:53.242394: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1232] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10312 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:86:00.0, compute capability: 7.5)\r\n2019-11-07 10:44:53.243757: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1232] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10312 MB memory) -> physical GPU (device: 1, name: GeForce RTX 2080 Ti, pci bus id: 0000:af:00.0, compute capability: 7.5)\r\nNumber of devices in strategy: 2\r\nTrain for 548.0 steps\r\nEpoch 1/10\r\n2019-11-07 10:45:11.764680: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-11-07 10:45:13.747993: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-11-07 10:45:15.307911: E tensorflow/stream_executor/cuda/cuda_driver.cc:948] failed to synchronize the stop event: CUDA_ERROR_ILLEGAL_INSTRUCTION: an illegal instruction was encountered\r\n2019-11-07 10:45:15.307949: E tensorflow/stream_executor/gpu/gpu_timer.cc:55] Internal: Error destroying CUDA event: CUDA_ERROR_ILLEGAL_INSTRUCTION: an illegal instruction was encountered\r\n2019-11-07 10:45:15.307957: E tensorflow/stream_executor/gpu/gpu_timer.cc:60] Internal: Error destroying CUDA event: CUDA_ERROR_ILLEGAL_INSTRUCTION: an illegal instruction was encountered\r\n2019-11-07 10:45:15.307992: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 8B (8 bytes) from device: CUDA_ERROR_ILLEGAL_INSTRUCTION: an illegal instruction was encountered\r\n2019-11-07 10:45:15.308001: E tensorflow/stream_executor/stream.cc:5452] Internal: Failed to enqueue async memset operation: CUDA_ERROR_ILLEGAL_INSTRUCTION: an illegal instruction was encountered\r\n2019-11-07 10:45:15.308017: W tensorflow/core/kernels/gpu_utils.cc:68] Failed to check cudnn convolutions for out-of-bounds reads and writes with an error message: 'Failed to load in-memory CUBIN: CUDA_ERROR_ILLEGAL_INSTRUCTION: an illegal instruction was encountered'; skipping this check. This only means that we won't check cudnn for out-of-bounds reads and writes. This message will only be printed once.\r\n2019-11-07 10:45:15.308032: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 8B (8 bytes) from device: CUDA_ERROR_ILLEGAL_INSTRUCTION: an illegal instruction was encountered\r\n2019-11-07 10:45:15.308044: I tensorflow/stream_executor/stream.cc:4963] [stream=0x62f2a40,impl=0x62f1230] did not memzero GPU location; source: 0x7fcf977fbfd0\r\n2019-11-07 10:45:15.308500: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: cuDNN launch failure : input shape([16,3,262,262]) filter shape([7,7,3,64])\r\n         [[{{node replica_1/resnet50/conv1_conv/Conv2D}}]]\r\n         [[loss/mul/_10]]\r\n2019-11-07 10:45:15.308571: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: cuDNN launch failure : input shape([16,3,262,262]) filter shape([7,7,3,64])\r\n         [[{{node replica_1/resnet50/conv1_conv/Conv2D}}]]\r\n         [[metrics/accuracy/div_no_nan/AddN_1/_32]]\r\n2019-11-07 10:45:15.308780: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: cuDNN launch failure : input shape([16,3,262,262]) filter shape([7,7,3,64])\r\n         [[{{node replica_1/resnet50/conv1_conv/Conv2D}}]]\r\n2019-11-07 10:45:15.332853: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2019-11-07 10:45:15.333219: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n  1/548 [..............................] - ETA: 2:39:09Traceback (most recent call last):\r\n  File \"src/test/multi_gpu_training_colab.py\", line 81, in <module>\r\n    workers=8)\r\n  File \"/home/sam2/workspace/python_venvs/tf-2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 778, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/home/sam2/workspace/python_venvs/tf-2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 338, in fit\r\n    total_epochs=epochs)\r\n  File \"/home/sam2/workspace/python_venvs/tf-2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 128, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n  File \"/home/sam2/workspace/python_venvs/tf-2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 86, in execution_function\r\n    distributed_function(input_fn))\r\n  File \"/home/sam2/workspace/python_venvs/tf-2/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/sam2/workspace/python_venvs/tf-2/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 632, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/home/sam2/workspace/python_venvs/tf-2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2339, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"/home/sam2/workspace/python_venvs/tf-2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1589, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"/home/sam2/workspace/python_venvs/tf-2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1670, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/home/sam2/workspace/python_venvs/tf-2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 521, in call\r\n    ctx=ctx)\r\n  File \"/home/sam2/workspace/python_venvs/tf-2/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found.\r\n  (0) Internal:  cuDNN launch failure : input shape([16,3,262,262]) filter shape([7,7,3,64])\r\n         [[node replica_1/resnet50/conv1_conv/Conv2D (defined at usr/lib/python3.7/threading.py:917) ]]\r\n         [[loss/mul/_10]]\r\n  (1) Internal:  cuDNN launch failure : input shape([16,3,262,262]) filter shape([7,7,3,64])\r\n         [[node replica_1/resnet50/conv1_conv/Conv2D (defined at usr/lib/python3.7/threading.py:917) ]]\r\n0 successful operations.\r\n1 derived errors ignored. [Op:__inference_distributed_function_36243]\r\n\r\nFunction call stack:\r\ndistributed_function -> distributed_function\r\n\r\n2019-11-07 10:45:15.777040: I tensorflow/stream_executor/stream.cc:1990] [stream=0x5ab9030,impl=0x62f1330] did not wait for [stream=0x62f2a40,impl=0x62f1230]\r\n2019-11-07 10:45:15.777095: I tensorflow/stream_executor/stream.cc:4938] [stream=0x5ab9030,impl=0x62f1330] did not memcpy host-to-device; source: 0x7fcf8007e000\r\n2019-11-07 10:45:15.777129: E tensorflow/stream_executor/stream.cc:332] Error recording event in stream: Error recording CUDA event: CUDA_ERROR_ILLEGAL_INSTRUCTION: an illegal instruction was encountered; not marking stream as bad, as the Event object may be at fault. Monitor for further errors.\r\n2019-11-07 10:45:15.777161: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_INSTRUCTION: an illegal instruction was encountered\r\n2019-11-07 10:45:15.777181: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:273] Unexpected Event status: 1\r\n```\r\nThe device interconnect matrix seems a bit odd, but we don't know if thats an issue for a distributed strategy:\r\n```\r\n2019-11-07 10:44:53.239881: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1087] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-11-07 10:44:53.239900: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1093]      0 1 \r\n2019-11-07 10:44:53.239912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1106] 0:   N N \r\n2019-11-07 10:44:53.239922: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1106] 1:   N N \r\n```\r\nIt seems the low level drivers work fine, see the dumps of nvidia-smi, nvidia-smi topo, cuda deviceQuery and NCCL All reduce test.\r\n[nvidia_smi_topo.txt](https://github.com/tensorflow/tensorflow/files/3818892/nvidia_smi_topo.txt)\r\n[nvidia_smi.txt](https://github.com/tensorflow/tensorflow/files/3818894/nvidia_smi.txt)\r\n[nccl_allreduce.txt](https://github.com/tensorflow/tensorflow/files/3818895/nccl_allreduce.txt)\r\n[cuda_device_query.txt](https://github.com/tensorflow/tensorflow/files/3818897/cuda_device_query.txt)\r\n\r\n\r\n", "comments": ["Hi - if the code works with 1 GPU but not 2, perhaps the issue is with how we are handling the gradient all reduce across the 2 GPUs in absence of NVLINK. To test this hypothesis, can you re-run your code with the following change:\r\n\r\n```\r\nstrategy = tf.distribute.MirroredStrategy(\r\n    cross_device_ops=tf.distribute.ReductionToOneDevice(reduce_to_device=\"cpu:0\"))\r\n```\r\n\r\nThis should force it to do the communication through the CPU. \r\n\r\n\r\n", "cc @dubey @yuefengz ", "> Hi - if the code works with 1 GPU but not 2, perhaps the issue is with how we are handling the gradient all reduce across the 2 GPUs in absence of NVLINK. To test this hypothesis, can you re-run your code with the following change:\r\n> \r\n> ```\r\n> strategy = tf.distribute.MirroredStrategy(\r\n>     cross_device_ops=tf.distribute.ReductionToOneDevice(reduce_to_device=\"cpu:0\"))\r\n> ```\r\n> \r\n> This should force it to do the communication through the CPU.\r\n\r\nthanks for your reply. \r\nunfortunately it crashes with more or less the same error messages :\r\n``` bash\r\nTrain for 274.0 steps\r\nEpoch 1/10\r\n2019-11-11 08:41:12.723892: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-11-11 08:41:14.713167: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-11-11 08:41:16.401051: E tensorflow/stream_executor/cuda/cuda_driver.cc:948] failed to synchronize the stop event: CUDA_ERROR_ILLEGAL_INSTRUCTION: an illegal instruction was encountered\r\n2019-11-11 08:41:16.401096: E tensorflow/stream_executor/gpu/gpu_timer.cc:55] Internal: Error destroying CUDA event: CUDA_ERROR_ILLEGAL_INSTRUCTION: an illegal instruction was encountered\r\n2019-11-11 08:41:16.401106: E tensorflow/stream_executor/gpu/gpu_timer.cc:60] Internal: Error destroying CUDA event: CUDA_ERROR_ILLEGAL_INSTRUCTION: an illegal instruction was encountered\r\n2019-11-11 08:41:16.401149: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 8B (8 bytes) from device: CUDA_ERROR_ILLEGAL_INSTRUCTION: an illegal instruction was encountered\r\n2019-11-11 08:41:16.401162: E tensorflow/stream_executor/stream.cc:5452] Internal: Failed to enqueue async memset operation: CUDA_ERROR_ILLEGAL_INSTRUCTION: an illegal instruction was encountered\r\n2019-11-11 08:41:16.401182: W tensorflow/core/kernels/gpu_utils.cc:68] Failed to check cudnn convolutions for out-of-bounds reads and writes with an error message: 'Failed to load in-memory CUBIN: CUDA_ERROR_ILLEGAL_INSTRUCTION: an illegal instruction was encountered'; skipping this check. This only means that we won't check cudnn for out-of-bounds reads and writes. This message will only be printed once.\r\n2019-11-11 08:41:16.401197: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 8B (8 bytes) from device: CUDA_ERROR_ILLEGAL_INSTRUCTION: an illegal instruction was encountered\r\n2019-11-11 08:41:16.401209: I tensorflow/stream_executor/stream.cc:4963] [stream=0x5a6a5c0,impl=0x5a68db0] did not memzero GPU location; source: 0x7fbf9bffcfd0\r\n2019-11-11 08:41:16.401718: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: cuDNN launch failure : input shape([32,3,262,262]) filter shape([7,7,3,64])\r\n         [[{{node replica_1/resnet50/conv1_conv/Conv2D}}]]\r\n         [[Adam/AddN_59/_1310]]\r\n2019-11-11 08:41:16.401802: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: cuDNN launch failure : input shape([32,3,262,262]) filter shape([7,7,3,64])\r\n         [[{{node replica_1/resnet50/conv1_conv/Conv2D}}]]\r\n         [[FusedBatchNormGradV3_30/_1010]]\r\n2019-11-11 08:41:16.407012: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: cuDNN launch failure : input shape([32,3,262,262]) filter shape([7,7,3,64])\r\n         [[{{node replica_1/resnet50/conv1_conv/Conv2D}}]]\r\n2019-11-11 08:41:16.407636: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2019-11-11 08:41:16.408077: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n  1/274 [..............................] - ETA: 1:13:30Traceback (most recent call last):\r\n  File \"src/test/multi_gpu_training_tf_dataset.py\", line 59, in <module>\r\n    workers=8)\r\n  File \"/home/sam2/workspace/python_venvs/tf-2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 778, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/home/sam2/workspace/python_venvs/tf-2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 338, in fit\r\n    total_epochs=epochs)\r\n  File \"/home/sam2/workspace/python_venvs/tf-2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 128, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n  File \"/home/sam2/workspace/python_venvs/tf-2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 86, in execution_function\r\n    distributed_function(input_fn))\r\n  File \"/home/sam2/workspace/python_venvs/tf-2/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/sam2/workspace/python_venvs/tf-2/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 632, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/home/sam2/workspace/python_venvs/tf-2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2339, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"/home/sam2/workspace/python_venvs/tf-2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1589, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"/home/sam2/workspace/python_venvs/tf-2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1670, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/home/sam2/workspace/python_venvs/tf-2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 521, in call\r\n    ctx=ctx)\r\n  File \"/home/sam2/workspace/python_venvs/tf-2/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found.\r\n  (0) Internal:  cuDNN launch failure : input shape([32,3,262,262]) filter shape([7,7,3,64])\r\n         [[node replica_1/resnet50/conv1_conv/Conv2D (defined at usr/lib/python3.7/threading.py:917) ]]\r\n  (1) Internal:  cuDNN launch failure : input shape([32,3,262,262]) filter shape([7,7,3,64])\r\n         [[node replica_1/resnet50/conv1_conv/Conv2D (defined at usr/lib/python3.7/threading.py:917) ]]\r\n         [[Adam/AddN_59/_1310]]\r\n0 successful operations.\r\n1 derived errors ignored. [Op:__inference_distributed_function_32351]\r\n\r\nFunction call stack:\r\ndistributed_function -> distributed_function\r\n\r\n2019-11-11 08:41:16.724607: I tensorflow/stream_executor/stream.cc:1990] [stream=0x5230f90,impl=0x5a68eb0] did not wait for [stream=0x5a6a5c0,impl=0x5a68db0]\r\n2019-11-11 08:41:16.724650: I tensorflow/stream_executor/stream.cc:4938] [stream=0x5230f90,impl=0x5a68eb0] did not memcpy host-to-device; source: 0x7fc3102069c0\r\n2019-11-11 08:41:16.724695: E tensorflow/stream_executor/stream.cc:332] Error recording event in stream: Error recording CUDA event: CUDA_ERROR_ILLEGAL_INSTRUCTION: an illegal instruction was encountered; not marking stream as bad, as the Event object may be at fault. Monitor for further errors.\r\n2019-11-11 08:41:16.724711: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_INSTRUCTION: an illegal instruction was encountered\r\n2019-11-11 08:41:16.724722: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:273] Unexpected Event status: 1\r\nAborted\r\n```\r\ni got access to a nvidia dgx workstation over the weekend to run the script. there the script runs without any problems. but it is using an older nvidia-driver and linux kernel version.  i'm guessing our problem is somewhere in the API between kernel / driver / tensorflow. is there any way to debug these interfaces or logs containing more information ? ", "Oh I see. yeah in that case this sounds like a driver/TF mismatch problem. @zongweiz can someone on your team help? ", "@chsigg, any thoughts? Thanks.", "Any updates? I experienced a similar problem. The training does not start with the following code (Last line is `Epoch 1/25` - but there is no error message). The memory of the GPUs remains allocated afterwards (restart must be forced). \r\n\r\n```\r\nimport numpy as np\r\n\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\nimport tensorflow.keras.layers as layers\r\nimport tensorflow.keras.backend as K\r\n\r\nNUM_WORKERS = 2\r\n\r\ndata = np.random.uniform(-1.0, 1.0, (10000, 192, 256, 3))\r\nlabels = np.random.uniform(-1.0, 1.0, (10000, 10))\r\n\r\n\r\ndef get_model():\r\n    inputs = keras.Input(shape=(192, 256, 3), name='rgb_input')\r\n\r\n    net = layers.Conv2D(16, 9, 2, 'same', activation='relu')(inputs)\r\n    net = layers.Flatten()(net)\r\n    net = layers.Dense(10, activation=K.tanh)(net)\r\n\r\n    return keras.Model(inputs=inputs, outputs=net)\r\n\r\n\r\nstrategy = tf.distribute.MirroredStrategy()\r\n\r\nwith strategy.scope():\r\n\r\n    model = get_model()\r\n    model.compile(\r\n        optimizer=keras.optimizers.Adam(3e-4),\r\n        loss=keras.losses.MeanSquaredError(),\r\n        metrics=[keras.metrics.MeanAbsoluteError()])\r\n\r\nmodel.fit(data, labels, batch_size=128 * NUM_WORKERS, epochs=25)\r\n```\r\n\r\n \r\nBut if i change the strategy in the following way, then it starts (the utilization is bad, but it works).\r\n```\r\nstrategy = tf.distribute.MirroredStrategy(\r\n    cross_device_ops=tf.distribute.ReductionToOneDevice(reduce_to_device=\"cpu:0\"))\r\n```\r\n\r\n\r\nSetup:\r\n- Ubuntu 18.04\r\n- tensorflow-gpu==2.0.0 (installed via pip)\r\n- GeForce GTX 1080 TI\r\n- python 3.6 (also tried python 2.7)\r\n- CUDA / CuDNN: 10.0/7.6.0\r\n", "@patrick-schulte do you have a device-interconnect matrix with y inside?", "@m4tts yes", "@m4tts Does it make a difference if you run with the environment variable `TF_DISABLE_RZ_CHECK` set to `1`?  I'm wondering if this is a problem with the [custom PTX](https://github.com/tensorflow/tensorflow/blob/f6216b136033506de2737a16cc29c2ff221decd5/tensorflow/stream_executor/gpu/redzone_allocator.cc#L115) we use to check for errors in cuDNN.", "sorry for the late response.\r\n\r\nafter lots of desparation I found the [gpu-burn test](https://github.com/wilicc/gpu-burn).\r\nas it seemed, our GPU was faulty. the test had returned the following :\r\n```\r\n./gpu_burn 120\r\nGPU 0: GeForce RTX 2080 Ti (UUID: [HIDDEN])\r\nGPU 1: GeForce RTX 2080 Ti (UUID: [HIDDEN])\r\nInitialized device 0 with 11019 MB of memory (10788 MB available, using 9709 MB of it), using FLOATS\r\nInitialized device 1 with 11019 MB of memory (10788 MB available, using 9709 MB of it), using FLOATS\r\n10.8%  proc'd: 8456 (11934 Gflop/s) - 6644 (10153 Gflop/s)   errors: 0 - 1052638274  (WARNING!)  temps: 32 C - 39 C \r\n        Summary at:   Tue Nov 12 09:38:10 CET 2019\r\n\r\n21.7%  proc'd: 17516 (11934 Gflop/s) - 14496 (10153 Gflop/s)   errors: 0 - 630154421  (WARNING!)  temps: 32 C - 39 C   \r\n        Summary at:   Tue Nov 12 09:38:23 CET 2019\r\n\r\n32.5%  proc'd: 25972 (11519 Gflop/s) - 22348 (10151 Gflop/s)   errors: 0 - -1754308719  (WARNING!)  temps: 60 C - 39 C \r\n        Summary at:   Tue Nov 12 09:38:36 CET 2019\r\n\r\n43.3%  proc'd: 34428 (11513 Gflop/s) - 30200 (10151 Gflop/s)   errors: 0 - 1988913133  (WARNING!)  temps: 60 C - 39 C  \r\n        Summary at:   Tue Nov 12 09:38:49 CET 2019\r\n\r\n53.3%  proc'd: 42884 (11387 Gflop/s) - 36844 (10153 Gflop/s)   errors: 0 - 14688317  (WARNING!)  temps: 60 C - 39 C  C \r\n        Summary at:   Tue Nov 12 09:39:01 CET 2019\r\n\r\n64.2%  proc'd: 51944 (11391 Gflop/s) - 44696 (10153 Gflop/s)   errors: 0 - 2095542503  (WARNING!)  temps: 60 C - 39 C  \r\n        Summary at:   Tue Nov 12 09:39:14 CET 2019\r\n\r\n75.0%  proc'd: 59796 (11391 Gflop/s) - 52548 (10154 Gflop/s)   errors: 0 - -1156948305  (WARNING!)  temps: 60 C - 39 C \r\n        Summary at:   Tue Nov 12 09:39:27 CET 2019\r\n\r\n85.8%  proc'd: 68252 (11092 Gflop/s) - 60400 (10161 Gflop/s)   errors: 0 - -1474718451  (WARNING!)  temps: 60 C - 39 C \r\n        Summary at:   Tue Nov 12 09:39:40 CET 2019\r\n\r\n96.7%  proc'd: 77312 (11090 Gflop/s) - 67648 (10158 Gflop/s)   errors: 0 - 1678997606  (WARNING!)  temps: 60 C - 39 C  \r\n        Summary at:   Tue Nov 12 09:39:53 CET 2019\r\n\r\n100.0%  proc'd: 80332 (11090 Gflop/s) - 70668 (10157 Gflop/s)   errors: 0 - -1670660404  (WARNING!)  temps: 60 C - 39 C \r\nKilling processes.. done\r\n\r\nTested 2 GPUs:\r\n        GPU 0: OK\r\n        GPU 1: FAULTY\r\n \r\n```\r\nI returned the GPU to the distributor on got a new one. and now everything works like a charm.\r\n\r\n@sanjoy I was not able to test this, since the GPU was allready shipped to the distributor. but it would make sense that there was an error in cuDNN if the GPU was faulty. maybe there would be a way tensorflow could test this or give a hint in that direction?\r\nI thought of every possibility, but it never came to my mind, that the GPU could be faulty because every CUDA and NCCLTest was fine. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34067\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34067\">No</a>\n", "> maybe there would be a way tensorflow could test this or give a hint in that direction?\r\n\r\nI think it is difficult to go from `CUDNN_STATUS_INTERNAL_ERROR` to \"GPU may be faulty\" since there are many other reasons why cuDNN will return this error.\r\n\r\nHowever, maybe we should link to the  [gpu-burn](https://github.com/wilicc/gpu-burn) test from the TF docs as something users should consider?  CC @dynamicwebpaige ", "> However, maybe we should link to the [gpu-burn](https://github.com/wilicc/gpu-burn) test from the TF docs as something users should consider? \r\n\r\nI think this is a great idea. I don't know how often GPUs actually break down but in my case, I never really considered it. To have it linked in a troubleshooting or similar section would be really helpful. \r\n\r\n", "I had similar problem, but with training only on one GPU without any strategy. The training freezed after `tf.data` prefetched first batches. \r\n\r\nSetting `TF_DISABLE_RZ_CHECK=1` helped, I also did `gpu-burn`, but no errors have shown."]}, {"number": 34066, "title": "TF2.0 distributed training fails after adding an embedding layer", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): somewhat\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nTry to train with `MultiWorkerMirroredStrategy` a simple model with and without an Embedding layer. In the former case the distribute training fails (see below). The following code is executed on two workers with `TF_CONFIG={\"cluster\": {\"worker\": [\"localhost:65535\",\"localhost:65532\"]}, \"task\": {\"index\": 0, \"type\": \"worker\"}}` and `TF_CONFIG={\"cluster\": {\"worker\": [\"localhost:65535\",\"localhost:65532\"]}, \"task\": {\"index\": 1, \"type\": \"worker\"}}`, respectively.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.optimizers import Adam\r\nfrom tensorflow.keras import losses\r\nimport numpy as np\r\n\r\nbatch_size = 32\r\n\r\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n\r\nNUM_WORKERS = strategy.num_replicas_in_sync\r\nGLOBAL_BATCH_SIZE = batch_size * NUM_WORKERS\r\nN_CAT = 47\r\n\r\ndef some_func(*args: tf.Tensor):\r\n    tensor_dict_x, tensor_dict_y = {}, {}\r\n\r\n    for index in range(1):\r\n        tensor_dict_x[\r\n            f\"input_{index+1}\"\r\n        ] = tf.expand_dims(args[index], axis=-1)\r\n        tensor_dict_y[\r\n            f\"dense\"\r\n        ] = tf.expand_dims(args[index], axis=-1)\r\n\r\n    return tensor_dict_x, tensor_dict_y\r\n\r\ndef read_data():\r\n    train_data = np.random.randint(1,N_CAT, size=9000)\r\n    val_data = np.random.randint(1,N_CAT, size=999)\r\n\r\n    dataset_train = (tf.data.Dataset.from_tensor_slices(train_data)\r\n                     .prefetch(-1)\r\n                     .map(map_func=some_func)\r\n                     .batch(batch_size=GLOBAL_BATCH_SIZE)\r\n                     .shuffle(1000)\r\n                     .repeat())\r\n    dataset_val = (tf.data.Dataset.from_tensor_slices(val_data)\r\n                   .prefetch(-1)\r\n                   .map(map_func=some_func)\r\n                   .batch(batch_size=GLOBAL_BATCH_SIZE)\r\n                   .shuffle(1000)\r\n                   .repeat())\r\n\r\n    return dataset_train, dataset_val\r\n\r\nwith strategy.scope():\r\n    optimizer = Adam(lr=0.1)\r\n    loss = losses.sparse_categorical_crossentropy\r\n    model = build_and_compile_model(optimizer, loss)\r\n\r\ndataset_train, dataset_val = read_data()\r\nmodel.fit(x=dataset_train,\r\n          epochs=5,\r\n          steps_per_epoch=9000//batch_size,\r\n          validation_data=dataset_val,\r\n          validation_steps=999//batch_size,\r\n)\r\n```\r\n\r\nwhere `build_and_compile_model`:\r\n```\r\ndef build_and_compile_model(optimizer, loss):\r\n    my_input = tf.keras.layers.Input(shape=(1,))\r\n    my_dense = tf.keras.layers.Dense(N_CAT)(my_input)\r\n\r\n    model = tf.keras.Model(my_input, my_dense)\r\n\r\n    model.compile(optimizer=optimizer,loss=loss)\r\n\r\n    return model\r\n```\r\nThe above case works fine with the output:\r\n```\r\n2019-11-07 09:23:57.387629: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-11-07 09:23:57.410325: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz\r\n2019-11-07 09:23:57.411228: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562e2c78c6a0 executing computations on platform Host. Devices:\r\n2019-11-07 09:23:57.411241: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\n2019-11-07 09:23:57.413306: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:258] Initialize GrpcChannelCache for job worker -> {0 -> localhost:65535, 1 -> localhost:65532}\r\n2019-11-07 09:23:57.414734: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:365] Started server with target: grpc://localhost:65535\r\nWARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\r\nWARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\nWARNING:tensorflow:ModelCheckpoint callback is not provided. Workers will need to restart training if any fails.\r\n2019-11-07 09:24:09.204390: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:400] Cannot find shardable dataset, adding a shard node at the end of the dataset instead. This may have performance implications.\r\nTrain for 281 steps, validate for 31 steps\r\nEpoch 1/5\r\n2019-11-07 09:24:09.208710: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:400] Cannot find shardable dataset, adding a shard node at the end of the dataset instead. This may have performance implications.\r\n281/281 [==============================] - 4s 13ms/step - loss: 15.9838 - val_loss: 16.1050\r\nEpoch 2/5\r\n281/281 [==============================] - 1s 3ms/step - loss: 16.0496 - val_loss: 16.1015\r\nEpoch 3/5\r\n281/281 [==============================] - 1s 3ms/step - loss: 16.0210 - val_loss: 16.1007\r\nEpoch 4/5\r\n281/281 [==============================] - 1s 3ms/step - loss: 16.0517 - val_loss: 16.0995\r\nEpoch 5/5\r\n281/281 [==============================] - 1s 3ms/step - loss: 16.0147 - val_loss: 16.0992\r\n2019-11-07 09:24:16.090800: W tensorflow/core/common_runtime/eager/context.cc:290] Unable to destroy server_ object, so releasing instead. Servers don't support clean shutdown.\r\n\r\nProcess finished with exit code 0\r\n```\r\n\r\nHowever, adding just one `Embedding` layer so that `build_and_compile_model` now is:\r\n```\r\ndef build_and_compile_model(optimizer, loss):\r\n\r\n    my_input = tf.keras.layers.Input(shape=(1,))\r\n    emb_layer = tf.keras.layers.Embedding(N_CAT,5)\r\n    emb_inp = emb_layer(my_input)\r\n    my_dense = tf.keras.layers.Dense(N_CAT)(emb_inp)\r\n\r\n    model = tf.keras.Model(my_input, my_dense)\r\n\r\n    model.compile(optimizer=optimizer,loss=loss)\r\n\r\n    return model\r\n```\r\n\r\nleads to the error:\r\n```\r\n2019-11-07 09:26:45.812362: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-11-07 09:26:45.834388: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz\r\n2019-11-07 09:26:45.835018: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ea66430da0 executing computations on platform Host. Devices:\r\n2019-11-07 09:26:45.835032: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\n2019-11-07 09:26:45.836899: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:258] Initialize GrpcChannelCache for job worker -> {0 -> localhost:65535, 1 -> localhost:65532}\r\n2019-11-07 09:26:45.838638: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:365] Started server with target: grpc://localhost:65535\r\nWARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\r\nWARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\nWARNING:tensorflow:ModelCheckpoint callback is not provided. Workers will need to restart training if any fails.\r\n2019-11-07 09:26:50.351284: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:400] Cannot find shardable dataset, adding a shard node at the end of the dataset instead. This may have performance implications.\r\nTrain for 281 steps, validate for 31 steps\r\nEpoch 1/5\r\n2019-11-07 09:26:50.355581: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:400] Cannot find shardable dataset, adding a shard node at the end of the dataset instead. This may have performance implications.\r\n  1/281 [..............................] - ETA: 9:05 - loss: 9.83752019-11-07 09:26:52.321935: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingGather with Internal: [_Derived_]Inconsistent output shapes, got [16], but expected is [64].\r\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\r\nAdditional GRPC error information:\r\n{\"created\":\"@1573115212.321897522\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"[_Derived_]Inconsistent output shapes, got [16], but expected is [64].\\n\\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\",\"grpc_status\":13}\r\n2019-11-07 09:26:52.321956: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: [_Derived_]Inconsistent output shapes, got [16], but expected is [64].\r\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\r\nAdditional GRPC error information:\r\n{\"created\":\"@1573115212.321897522\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"[_Derived_]Inconsistent output shapes, got [16], but expected is [64].\\n\\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\",\"grpc_status\":13}\r\n2019-11-07 09:26:52.321966: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Internal: [_Derived_]Inconsistent output shapes, got [16], but expected is [64].\r\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\r\nAdditional GRPC error information:\r\n{\"created\":\"@1573115212.321897522\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"[_Derived_]Inconsistent output shapes, got [16], but expected is [64].\\n\\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\",\"grpc_status\":13}\r\n2019-11-07 09:26:52.321971: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: [_Derived_]Inconsistent output shapes, got [16], but expected is [64].\r\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\r\nAdditional GRPC error information:\r\n{\"created\":\"@1573115212.321897522\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"[_Derived_]Inconsistent output shapes, got [16], but expected is [64].\\n\\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\",\"grpc_status\":13}\r\n2019-11-07 09:26:52.321976: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingGather with Internal: [_Derived_]Inconsistent output shapes, got [16], but expected is [64].\r\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\r\nAdditional GRPC error information:\r\n{\"created\":\"@1573115212.321897522\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"[_Derived_]Inconsistent output shapes, got [16], but expected is [64].\\n\\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\",\"grpc_status\":13}\r\n2019-11-07 09:26:52.321996: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: [_Derived_]Inconsistent output shapes, got [16], but expected is [64].\r\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\r\nAdditional GRPC error information:\r\n{\"created\":\"@1573115212.321897522\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"[_Derived_]Inconsistent output shapes, got [16], but expected is [64].\\n\\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\",\"grpc_status\":13}\r\n2019-11-07 09:26:52.322010: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Internal: [_Derived_]Inconsistent output shapes, got [16], but expected is [64].\r\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\r\nAdditional GRPC error information:\r\n{\"created\":\"@1573115212.321897522\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"[_Derived_]Inconsistent output shapes, got [16], but expected is [64].\\n\\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\",\"grpc_status\":13}\r\n2019-11-07 09:26:52.322017: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: [_Derived_]Inconsistent output shapes, got [16], but expected is [64].\r\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\r\nAdditional GRPC error information:\r\n{\"created\":\"@1573115212.321897522\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"[_Derived_]Inconsistent output shapes, got [16], but expected is [64].\\n\\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\",\"grpc_status\":13}\r\n2019-11-07 09:26:52.322045: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Cancelled: [_Derived_]Cancelled\r\nAdditional GRPC error information:\r\n{\"created\":\"@1573115212.321980682\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Cancelled\",\"grpc_status\":1}\r\n2019-11-07 09:26:52.322051: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Cancelled: [_Derived_]Cancelled\r\nAdditional GRPC error information:\r\n{\"created\":\"@1573115212.321980682\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Cancelled\",\"grpc_status\":1}\r\n2019-11-07 09:26:52.322053: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at collective_ops.cc:125 : Internal: [_Derived_]Inconsistent output shapes, got [16], but expected is [64].\r\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\r\nAdditional GRPC error information:\r\n{\"created\":\"@1573115212.321897522\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"[_Derived_]Inconsistent output shapes, got [16], but expected is [64].\\n\\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\",\"grpc_status\":13}\r\n2019-11-07 09:26:52.322054: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at collective_ops.cc:234 : Internal: [_Derived_]Inconsistent output shapes, got [16], but expected is [64].\r\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\r\nAdditional GRPC error information:\r\n{\"created\":\"@1573115212.321897522\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"[_Derived_]Inconsistent output shapes, got [16], but expected is [64].\\n\\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\",\"grpc_status\":13}\r\n2019-11-07 09:26:52.322101: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: [_Derived_]Inconsistent output shapes, got [16], but expected is [64].\r\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\r\nAdditional GRPC error information:\r\n{\"created\":\"@1573115212.321897522\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"[_Derived_]Inconsistent output shapes, got [16], but expected is [64].\\n\\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\",\"grpc_status\":13}\r\n\t [[Adam/allreduce_1/CollectiveGather]]\r\n2019-11-07 09:26:52.322097: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at collective_ops.cc:125 : Internal: [_Derived_]Inconsistent output shapes, got [16], but expected is [64].\r\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\r\nAdditional GRPC error information:\r\n{\"created\":\"@1573115212.321897522\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"[_Derived_]Inconsistent output shapes, got [16], but expected is [64].\\n\\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\",\"grpc_status\":13}\r\n2019-11-07 09:26:52.322123: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at collective_ops.cc:234 : Cancelled: [_Derived_]Cancelled\r\nAdditional GRPC error information:\r\n{\"created\":\"@1573115212.321980682\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Cancelled\",\"grpc_status\":1}\r\n2019-11-07 09:26:52.322108: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at collective_ops.cc:234 : Internal: [_Derived_]Inconsistent output shapes, got [16], but expected is [64].\r\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\r\nAdditional GRPC error information:\r\n{\"created\":\"@1573115212.321897522\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"[_Derived_]Inconsistent output shapes, got [16], but expected is [64].\\n\\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\",\"grpc_status\":13}\r\nTraceback (most recent call last):\r\n  File \".../.PyCharmCE2019.2/config/scratches/scratch_4.py\", line 71, in <module>\r\n    validation_steps=999//batch_size,\r\n  File \".../lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 728, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File .../lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 789, in fit\r\n    *args, **kwargs)\r\n  File \".../lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 776, in wrapper\r\n    mode=dc.CoordinatorMode.INDEPENDENT_WORKER)\r\n  File \".../lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_coordinator.py\", line 853, in run_distribute_coordinator\r\n    task_id, session_config, rpc_layer)\r\n  File \".../mostly-engine-tf20-p374/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_coordinator.py\", line 360, in _run_single_worker\r\n    return worker_fn(strategy)\r\n  File \".../mostly-engine-tf20-p374/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 771, in _worker_fn\r\n    return method(model, **kwargs)\r\n  File \".../mostly-engine-tf20-p374/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 324, in fit\r\n    total_epochs=epochs)\r\n  File \".../mostly-engine-tf20-p374/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 123, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n  File \".../mostly-engine-tf20-p374/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 86, in execution_function\r\n    distributed_function(input_fn))\r\n  File \".../mostly-engine-tf20-p374/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 457, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \".../mostly-engine-tf20-p374/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 487, in _call\r\n    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n  File \".../mostly-engine-tf20-p374/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1823, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"/.../mostly-engine-tf20-p374/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1141, in _filtered_call\r\n    self.captured_inputs)\r\n  File \".../lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1224, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager)\r\n  File \".../lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 511, in call\r\n    ctx=ctx)\r\n  File \".../lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InternalError:  [_Derived_]Inconsistent output shapes, got [16], but expected is [64].\r\n\t [[node Adam/allreduce_1/CollectiveGather_1 (defined at /miniconda3/envs/mostly-engine-tf20-p374/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]]\r\nAdditional GRPC error information:\r\n{\"created\":\"@1573115212.321897522\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"[_Derived_]Inconsistent output shapes, got [16], but expected is [64].\\n\\t [[node Adam/allreduce_1/CollectiveGather_1 (defined at /miniconda3/envs/mostly-engine-tf20-p374/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]]\",\"grpc_status\":13}\r\n\t [[Adam/allreduce_1/CollectiveGather]] [Op:__inference_distributed_function_888]\r\n\r\nFunction call stack:\r\ndistributed_function\r\n\r\n2019-11-07 09:26:52.540347: W tensorflow/core/common_runtime/eager/context.cc:290] Unable to destroy server_ object, so releasing instead. Servers don't support clean shutdown.\r\n\r\nProcess finished with exit code 1\r\n```", "comments": ["Sounds like a similar issue to [#33339](https://github.com/tensorflow/tensorflow/issues/33339), which has been fixed. Closing this issue now, but feel free to reopen if this is still a problem.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34066\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34066\">No</a>\n"]}, {"number": 34065, "title": "Update check_ops.py", "body": "", "comments": ["Please make the PR against master, not `r2.0` branch. Release branches are mostly frozen after release, they only accept cherrypicks relevant to security related patch releases."]}, {"number": 34064, "title": "Work group size selection in OpenGL", "body": "\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): v2.0.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nHi, I am running tensorflow lite v2.0.0 on my Mali-G72 and Adreno 630 GPUs. I have read some of the tf lite source codes in order to see how you make use of OpenGL. \r\n\r\nIn /tensorflow/tensorflow/lite/delegates/gpu/gl/workgroups/calculator_from_metadata.cc, I noticed you are trying to choose a proper work group size. However, it seems it did not really use calculator_from_metadata but to use default_calculator_ when running on my GPUs. Intuitively, I believe that the choice of work group size influences the performance and a choice from calculator_from_metadata should be better. But calculator_from_metadata is not working because there is not any data saved in the vector workgroups_. \r\nHave you finished this part, yet? I would appreciate it if you can share any information about this.\r\n\r\n\r\n", "comments": ["@Lumosis \r\n\r\nHi there, thanks for digging deeper into the code and noticing this detail :)\r\n\r\nYou are very welcome to experiment with the workgroup sizes.  What we have observed in the past is that Adreno is more susceptible to workgroup changes, and can get up to 30% faster if you choose a good workgroup size or also 30% slower if you choose a bad one.  Now, with Mali devices, we strangely only saw a drop of performance no matter what we did, so we decided to leave Mali as is.  We might want some advice from Mali experts who is not present nearby our team =/", "@impjdi \r\n\r\nHi impjdi, I wonder which model you were using to test performance for different group sizes. I tested using different group sizes (on Adreno) on single op model, like add_1024x1024, but did not find large fluctuations. The inference time basically fluctuated with 2% range. Could you share some information?", "Yeah, I don't think we tweaked any ops like ADD.  IIRC we only investigated\nin CONV & DWCONV.  Our paper that describes our investigation in workgroup\nsizes is here: https://arxiv.org/abs/1907.01989\n\nOf course, it may have changed since the paper; the paper was written in\nApril, we had someone else look at it since then, and there was some recent\nsubmission around the workgroup size calculation.\n\nOn Tue, Nov 12, 2019 at 1:11 AM Lumosis <notifications@github.com> wrote:\n\n> @impjdi <https://github.com/impjdi>\n>\n> Hi impjdi, I wonder which model you were using to test performance for\n> different group sizes. I tested using different group sizes (on Adreno) on\n> single op model, like add_1024x1024, but did not find large fluctuations.\n> The inference time basically fluctuated with 2% range. Could you share some\n> information?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/34064?email_source=notifications&email_token=ACKKUT4UWSQND3IKOCSDJ43QTJXNXA5CNFSM4JKCYU7KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEDZRAMA#issuecomment-552800304>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACKKUT7NB37LTS6F3HV7SWDQTJXNXANCNFSM4JKCYU7A>\n> .\n>\n", "@impjdi do you have any update about Mali GPU wg selection?", "Unfortunately, no.  We have played around with Mali workgroup sizes, but haven't seen much performance difference which is not intuitive.", "@impjdi Thanks for sharing the information. I basically did the same thing, and I don't have any idea about what MALI wg sizes actually affect. It's also weird that for some TFLite models, I found that devices with older MALI GPU models (say G72 and T880) run even 1.5-2x faster than ones with newer models (G76).", "Could you please let us know if this issue still persists ? If it is resolved then please feel free to move this issue to close status ? please refer [link1](https://stackoverflow.com/questions/19477348/opencl-opengl-interop-dimensions-of-a-renderbuffer-in-relation-to-workgroup-size), [link2](https://stackoverflow.com/questions/11466162/work-group-sizes) . Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34064\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34064\">No</a>\n"]}, {"number": 34063, "title": "[Intel MKL] add missing attr", "body": "add missing attr for QuantizedDepthwiseConv2D", "comments": ["@penpornk Thanks for your comments about the API, I have update the api file based on your command."]}, {"number": 34062, "title": "BatchNorm generates NaN moving_variance on GPU with fused set to True for some inputs", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.15.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: the one on Colab\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nI have an input of shape (1, 1, 1, num_channels), and I run it through a tf.keras.layers.BatchNormalization in training mode.\r\n\r\nWhen running on CPU (fused or not) or GPU (not fused), the batch norm has the expected moving_variance of [0.99, 0.99 ...]\r\n\r\nWhen running on GPU with fused=True, the moving_variance is [nan, nan, ...]\r\n\r\n**Describe the expected behavior**\r\n\r\nThe moving variance should not be nan\r\n\r\n**Code to reproduce the issue**\r\nCheck this [gist](https://colab.research.google.com/gist/jnd77/38ae530b17ee84e7b3907e3010c8529a/untitled0.ipynb)\r\n\r\n**Other info / logs**\r\nIt works fine on CPU.\r\n", "comments": ["Hi !!\r\nThe common reason for NaN can be the following :\r\n\r\n1. Gradient Blow up\r\n2. Bad learning rate policy and params\r\n3. Faulty Loss function\r\n4. stride larger than kernel size in \"Pooling\" layer\r\n\r\nIf possible check for the above problems. ", "Hi. Thanks for the advice.\r\nAs you can see in the gist, I'm not running any model or training. It happens only for certain shapes of inputs (batch size, width and height equal to 1, but maybe for some others ?!), so I guess there is a bug in the fused op on GPU for this corner case ...\r\n", "I have tried on colab with TF version 1.15 on GPU with  `fused=True`,`fused=False`  and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/40cbe108f064e2d38bbda7b215f4e69a/untitled0.ipynb).However i am not seeing the issue with CPU. Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/ae83c64e7650839fbfb1c8e86c7eda2c/untitled343.ipynb).Thanks!", "@jnd77 -- a lot has changed since this issue was first posted. Is this still an issue in TF v2? Can you try with tf-nightly or the most recent release of tensorflow?", "@karmel --  still an issue. I updated the [gist](https://colab.research.google.com/gist/jnd77/9a9a60621f9382b99e2dcb63eeae8af5/untitled0.ipynb) with TF 2.1.", "It appears `fused=False` uses the biased variance (meaning, [Bessel's correction](https://en.wikipedia.org/wiki/Bessel%27s_correction) is not used) while `fused=True` uses the unbiased variance. The fact there is a difference is bad, but that is a separate issue. If you take the unbiased variance of a single element, you compute 0/0, so arguably the correct result is getting NaNs. This is why passing `fused=True` gets NaNs.\r\n\r\nThe CPU has a special check if there is only one batch element [here](https://github.com/tensorflow/tensorflow/blob/9cc8c98a6017f7337d6b61f53118bcaa222bcd37/tensorflow/core/kernels/fused_batch_norm_op.cc#L176) which is why this doesn't occur on the CPU.", "This is also still an issue in tf 2.2.\r\n\r\n``` python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nimport os\r\n\r\nx = np.random.randn(1, 1, 1, 1).astype(np.float32)\r\nc = tf.constant(x)\r\n\r\nbn = tf.keras.layers.BatchNormalization()\r\nbn(x, training=True)\r\nprint(bn.moving_variance) # nan\r\n\r\nbn = tf.keras.layers.BatchNormalization(epsilon=100)\r\nbn(x, training=True)\r\nprint(bn.moving_variance) # nan\r\n\r\nbn = tf.keras.layers.BatchNormalization(fused=False)\r\nbn(x, training=True)\r\nprint(bn.moving_variance) # good\r\n\r\nwith tf.device('/cpu:0'):\r\n    bn = tf.keras.layers.BatchNormalization()\r\n    bn(x, training=True)\r\n    print(bn.moving_variance)  # good\r\n```", "@ben-davidson-6 \r\nI ran the code shared on tf-nightly, please refer to this [gist here](https://colab.research.google.com/gist/Saduf2019/6f22e28af9bdf9b3a3044322dfcd7e05/untitled431.ipynb) and let us know if the issue persist.", "@Saduf2019 Thanks. It works for me.", "@jnd77 \r\nThank you for the update, glad its resolved. Please move the issue to closed status.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34062\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34062\">No</a>\n", "@jnd77 @Saduf2019 \r\nThe issue has not been resolved.\r\nThe reason it appears to be resolved in tf-nightly is because it is running on the CPU only.\r\nI installed tf-nightly-gpu(2.5.0-dev20201029) on a local machine and the issue still persists.", "> @jnd77 @Saduf2019\r\n> The issue has not been resolved.\r\n> The reason it appears to be resolved in tf-nightly is because it is running on the CPU only.\r\n> I installed tf-nightly-gpu(2.5.0-dev20201029) on a local machine and the issue still persists.\r\n\r\nPlease refer to the gist, the run time used is GPU. Please create a new issue if you face any using a new template.", "@Saduf2019\r\nI tried running this [gist](https://colab.research.google.com/gist/Saduf2019/6f22e28af9bdf9b3a3044322dfcd7e05/untitled431.ipynb) but as discussed [here](https://github.com/tensorflow/tensorflow/issues/42957) GPUs don't work on nightly builds on colab at the moment even though it is selected as the runtime. If you run `print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))` after switching to tf-nightly, there won't be any GPUs available.", "Hello all,\r\n\r\nI had the exact same issue, I had to change my network and put in a Flatten layer before the BN, and then a Reshape() as needed after it as a work around. This seems to have helped me, i don't know if it will help others. Hasn't affected effectiveness badly for me."]}, {"number": 34061, "title": "this in response to issue #34030", "body": "how to assign a different gpu to each session in a process .\r\n\r\n#34030", "comments": ["This belongs in a blog post, not in the source tree. PRs should change code."]}, {"number": 34060, "title": "undefined reference to `absl::M", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@yaolove ,\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.\r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "@yaolove ,\r\nAny update on the issue ?Thanks!", "just a bug of my source,already solve it ,thanks\r\n\r\n\r\n------------------&nbsp;\u539f\u59cb\u90ae\u4ef6&nbsp;------------------\r\n\u53d1\u4ef6\u4eba:&nbsp;\"oanush\"<notifications@github.com&gt;;\r\n\u53d1\u9001\u65f6\u95f4:&nbsp;2019\u5e7411\u670822\u65e5(\u661f\u671f\u4e94) \u4e0b\u53484:54\r\n\u6536\u4ef6\u4eba:&nbsp;\"tensorflow/tensorflow\"<tensorflow@noreply.github.com&gt;;\r\n\u6284\u9001:&nbsp;\"\u8d85\"<442622355@qq.com&gt;;\"Mention\"<mention@noreply.github.com&gt;;\r\n\u4e3b\u9898:&nbsp;Re: [tensorflow/tensorflow] undefined reference to `absl::M (#34060)\r\n\r\n\r\n\r\n\r\n@yaolove ,\r\n Any update on the issue ?Thanks!\r\n \r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub, or unsubscribe."]}, {"number": 34059, "title": "Using Cuda 10 and getting ImportError: libcublas.so.8.0: cannot open shared object file: No such file or directory", "body": "I am running an instance of tensorflow and am having the issue of ImportError: libcublas.so.8.0: cannot open shared object file: No such file or directory\r\n\r\nIs this because I am using CUDA version 10.1. Not quite sure how to proceed\r\n\r\n**Code: sudo python2 train.py --hypes hypes/overfeat_rezoom.json --logdir output/ --gpu 0**\r\n\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 4, in <module>\r\n    import tensorflow.contrib.slim as slim\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 72, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n**ImportError: libcublas.so.8.0: cannot open shared object file: No such file or directory**\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\n**System information**\r\n- OS Platform and Distribution:  Linux Ubuntu 18.04:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.4.1\r\n- Python version: 2.7.15+\r\n- Installed using: pip\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: Tesla V100\r\n", "comments": ["@aarac, Tensorflow version 1.4 supports CUDA 8. Please take a look at [tested build configuration](https://www.tensorflow.org/install/source#tested_build_configurations). Either downgrade the CUDA version to 8 or install Tensorflow version greater than 1.13.0. Let us know how it progresses. Thanks! ", "@aarac, Were you able to resolve this issue. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34059\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34059\">No</a>\n"]}, {"number": 34058, "title": "CheckNumerics takes long time", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": []}, {"number": 34057, "title": "TFLite Micro, bugfix for MicroAllocator::AllocateTensors()", "body": "Latest master:\r\ntensorflow/lite/experimental/micro/micro_allocator.cc\r\n\r\n`MicroAllocator::AllocateTensors()`\r\n\r\n```\r\n    for (size_t n = 0; n < op->outputs()->size(); ++n) {\r\n      const int tensor_index = op->outputs()->Get(n);\r\n      TensorInfo* current = &tensor_info[tensor_index];\r\n      if ((current->first_created == -1) || (current->first_created > i)) {\r\n        current->first_created = i;\r\n      }\r\n```\r\n\r\nThe operations list is iterated in reverse so need to change:\r\n```(current->first_created > i)```\r\n to \r\n```(current->first_created < i)```\r\n\r\n\r\n\r\n", "comments": ["The latest copy of the file has the updated fix -- [tensorflow/lite/experimental/micro/micro_allocator.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/micro/micro_allocator.cc#L279)\r\n\r\nClosing the issue as it seems to be resolved.", "I am sorry but I don't follow your logic on why the **largest** number for **first_created** should win, the search order shouldn't make any difference. As far as I can tell we are looking for the first operator using the tensor (minimum) which recommends \"i &lt; current-&gt; first_created\". Additionally I think that a similar mistake was made in the last_used logic - here you look for the largest number. I feel confirmed by testing a complex graph with the logic in master versus my reversed implementation.", "I was testing mobilenetv2 and found that the model didn't work without the suggested change.\r\nIt was clear that RAM was incorrectly being shared by concurrent tensors.\r\n\r\nFrom what I can tell, the logic is trying the find the very first operation and the very last operation the tensor is used. Thus, we want to find the smallest operation index (i.e. smallest current->first_created) and the largest operation index (i.e. largest current->last_used)\r\n\r\nWith this logic, the search order makes a difference as (current->first_created > i) causes lower operation ids to not be considered.", "I see a similar problem (wrong comparison) with the calculation of last_used (here the code in master wrongly looks for the minimum). I was quite surprised to see inputs with a first_used of greater than zero and outputs with a last_use smaller than the maximum. I agree to your description of what this code should do (find the min of first and the max of last). [My test case is an unrolled RNN with parallel paths, early outputs and inputs into late operators]\r\n\r\nStill I don't see why the order of comparison makes any difference to the end result as to me this clearly is a min/max search (random order should also be fine here).\r\n\r\nHonestly I have a hard time reading it expressed as \"if (first>i) first=i\" instead of \"if (i<first) first=i\" . My understanding of your patch is that it changes it to calculate first as the maximum of candidates \"if (i>first) first=i\". (I swapped the sides for better readability (according to my feelings))\r\n\r\nMaybe the correct fix for our problem is to change the logic for last_used instead ;-)", "From what I can tell there is a regression in the latest master:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/micro_allocator.cc#L177\r\nThe inverted logic is back.\r\n\r\nOnce `current->first_created` changes from -1 to the current operation index `i`, the  `current->first_created > i` logic will never be used as the operation indices are iterated in reverse order, largest to smallest index.\r\n\r\n"]}, {"number": 34056, "title": "TFLite Micro allocator, need to set quantized_dimension", "body": "In the latest master, \r\n\r\ntensorflow/lite/experimental/micro/micro_allocator.cc\r\n\r\nIn `InitializeRuntimeTensor()` near line 365, need to add:\r\n\r\n```\r\nquantization->quantized_dimension = src_quantization->quantized_dimension();\r\n```\r\n", "comments": ["As the issues seems to be resolved in [Line 490] (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/micro/micro_allocator.cc#L490), we're closing it. Reopen if it hasn't been resolved with the following information:\r\n\r\nProvide the updated the line number where the `quantized_dimension` needs to be set from the latest git commit. The function `InitializeRuntimeTensor()` seems to start from a new [Line 376] (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/micro/micro_allocator.cc#L376) now . \r\n\r\n"]}]