[{"number": 6995, "title": "Fixed issue in _call_location() when 2nd frame is `None`", "body": "`AttributeError: 'NoneType' object has no attribute 'f_code'` occurs when the second frame is `None`. ", "comments": []}, {"number": 6994, "title": "A tutorial for new dynamic seq2seq", "body": "New `contrib.seq2seq` made it into master - exciting development! However, the only examples are in the tests. For my project, I've implemented a small but complete example against a toy copy task. Here it is: [notebook](https://github.com/ematvey/tensorflow-seq2seq-tutorials/blob/master/3-seq2seq-native-new.ipynb), [code](https://github.com/ematvey/tensorflow-seq2seq-tutorials/blob/master/model_new.py).\r\n\r\nCan I contribute it? I do realise that API is likely not yet complete. I\u2019ll be happy to keep it updated and improve it if needed.", "comments": ["@ematvey : Thanks very much for doing this! Glad to see it. I'd encourage you to publish your findings, but AFAIK, the new `seq2seq` API isn't in a state to provide official tutorials for just yet. \r\n\r\nCCing @ebrevdo so that he is aware of your notebook, but closing this out for now."]}, {"number": 6993, "title": "tensorflow install-gpu error on window 10 ", "body": "I installed the tensorflow on window. \r\nand \r\npip install tensorflow is successfully imported \r\nbut pip install tensorflow-gpu brought some issues when they imported \r\nhere is the issues.\r\n=================================================\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:119] Couldn't open CUDA library cublas64_80.dll\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_blas.cc:2294] Unable to load cuBLAS DSO.\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:119] Couldn't open CUDA library cudnn64_5.dll\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:3459] Unable to load cuDNN DSO\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:119] Couldn't open CUDA library cufft64_80.dll\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_fft.cc:344] Unable to load cuFFT DSO.\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:119] Couldn't open CUDA library nvcuda.dll\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_diagnostics.cc:165] hostname: ???\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: DLL \ucd08\uae30\ud654 \ub8e8\ud2f4\uc744 \uc2e4\ud589\ud560 \uc218 \uc5c6\uc2b5\ub2c8\ub2e4.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 54, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 21, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow')\r\n  File \"C:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 60, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: DLL \ucd08\uae30\ud654 \ub8e8\ud2f4\uc744 \uc2e4\ud589\ud560 \uc218 \uc5c6\uc2b5\ub2c8\ub2e4.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 54, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 21, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow')\r\n  File \"C:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\n\r\nError importing tensorflow.  Unless you are using bazel,\r\nyou should not try to import tensorflow from its source directory;\r\nplease exit the tensorflow source tree, and relaunch your python interpreter\r\nfrom there.\r\n==================================================\r\nany Idea?", "comments": ["From the error message, it looks like TensorFlow can't find the CUDA DLLs it needs to use your GPU. Make sure you've installed CUDA 8.0 and cuDNN 5.1 and added the directories containing their DLLs to your `%PATH%` environment variable.", "I'm using my old version lenovo lap-top with Intel Hd graphics 4000. it seemed like does not support the CUDA.  I guess my GPU does not support tensorflow requirement is it right?", "The standard distribution of TensorFlow requires CUDA support for GPU acceleration. You might be able to use the experimental OpenCL support, but it is currently Linux-only. Documentation for the OpenCL support is available here:\r\n\r\nhttps://github.com/benoitsteiner/tensorflow-opencl/blob/master/tensorflow/g3doc/get_started/os_setup.md#optional-install-opencl-experimental-linux-only\r\n"]}, {"number": 6992, "title": "Feature request: LU Decomposition", "body": "I suggest adding LU decomposition with partial pivoting to Tensorflow. The implementation must support rectangular matrices as well, and not only square matrices. Tensorflow already uses LU decomposition for solving linear systems of equations.\r\n\r\nLU decomposition can be used as an efficient algorithm for finding the range of a matrix and its low rank approximation. The advantages over QR and SVD are that LU is more computationally efficient and is very suited for GPUs giving a significant speed boost to many computational tasks.\r\n\r\nSee the following papers:\r\n\r\n1) Li, H., Linderman, G. C., Szlam, A., Stanton, K. P., Kluger, Y., & Tygert, M. (2017). Algorithm 971: An Implementation of a Randomized Algorithm for Principal Component Analysis. ACM Transactions on Mathematical Software (TOMS), 43(3), 28\r\n2) Shabat, G., Shmueli, Y., Aizenbud, Y., & Averbuch, A. (2016). Randomized LU decomposition. Applied and Computational Harmonic Analysis.\r\n3) Li, H., Kluger, Y., & Tygert, M. (2016). Randomized algorithms for distributed computation of principal component analysis and singular value decomposition. arXiv preprint arXiv:1612.08709.\r\n\r\n", "comments": ["@rmlarsen for comment", "@rmlarsen already implemented tf.matrix_solve with lu decomposition:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/b10f50ff15944badb7262a207f6628dfa52d6a9d/tensorflow/core/kernels/matrix_solve_op.cc\r\n\r\nIt would be nice to expose the decomposition, like tf.cholesky_solve, so that it can be reused on the next solve if A doesn't change.", "Yeah, but from what I remember from Eigen's documentation (perhaps I am wrong), their LU decomposition works only for square matrices. An LU decomposition for rectangular matrices is also very important similar to Scipy and MKL.", "Both would be useful - my current use case is for square matrices. This could also be taken from cuSOLVER like cholesky.\n\nI don't have time to work on this, but it would involve copying the cholesky op and primarily changing potrf to getrf. Writing a gradient would be the hard bit.\n\n> Le 27 mai 2017 \u00e0 20:35, gilgils <notifications@github.com> a \u00e9crit :\n> \n> Yeah, but from what I remember from Eigen's documentation (perhaps I am wrong), their LU decomposition works only for square matrices. An LU decomposition for rectangular matrices is also very important similar to Scipy and MKL.\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n> \n", "Hi,\r\n\r\nJust noticed the issue. I have modified some code for LU in lu_op.cc. Is this related to this issue?\r\nThe tensorflow ops code:\r\nhttps://github.com/zhuangh/tensorflow/blob/1f6d70444866f08b46ae8c3243893bf1e5b54776/tensorflow/core/kernels/lu_op.cc\r\nwhere L, U, P, Q can be reused, A = P^-1 LU Q^-1\r\n\r\nThe script of using this ops: https://gist.github.com/zhuangh/6831ac69ffc70e0add7378c1c7764347#file-lu_ops_test-py\r\n\r\n @mccajm \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "I am still working on the pull request #16185 for parts of this request with @rmlarsen ", "@tensorflowbutler @rmlarsen sorry for the late, I am still working on the requirements from the review and merging the code conflicts. Please give me more time. Thanks.", "@tensorflowbutler fixed the conflicts and test bench errors after pulling and merging the latest tf master branch. working on other enhancements.", "Hi @rmlarsen \r\n\r\nI posted a related question in the (closed) pull request https://github.com/tensorflow/tensorflow/pull/16185\r\n\r\nI am currently actively working on that. \r\n\r\nRegards,", "Hi, I think the majority part of the LU ops is done in the PR #21885 lu_op.cc. Probably, need to add more tests in lu_op_test.py. Thanks\r\n", "@ zhuangh Hi, any update on this ? Can we close the issue if this is completed ?", "Hi @harshini-gadige, the pull request #21885 is still under review. thanks.", "@gilgils As mentioned by @rmlarsen I think this was resolved by LU op [developed](https://github.com/tensorflow/tensorflow/pull/21885#issuecomment-445085277). I am closing this issue but open a new issue if you think Imade a mistake. Thanks!"]}, {"number": 6991, "title": "Clean up code block in sparse_tensor_dense_matmul", "body": "The documentation section that gives an [output of a benchmark for `sparse_tensor_dense_matmul`](https://www.tensorflow.org/versions/master/api_docs/python/sparse_ops/math_operations#sparse_tensor_dense_matmul) is formatted incorrectly- this patch should fix the HTML rendering problems.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 6990, "title": "make tensorflow/contrib/layers:feature_column_ops_test medium to fix \u2026", "body": "\u2026Jenkins test timeout.\r\n\r\nIE, happened when testing PR here\r\nhttps://github.com/tensorflow/tensorflow/pull/6909", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 6989, "title": "Branch 145127746", "body": "", "comments": ["Are these tests broken internally?\r\nJenkins, test this please.", "Close and we'll push again with internal fixes."]}, {"number": 6988, "title": "More cherrypicks into r1.0", "body": "", "comments": ["Jenkins, test this please."]}, {"number": 6987, "title": "enable hdfs support for windows", "body": "", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "Thanks for adding this, Guenther! The change looks good to me, assuming the tests pass.\r\n\r\nI assume none of our automated tests use this because of the Hadoop distribution dependency. Have you confirmed that it works manually as expected?", "yes, I tested this against hadoop 2.6.4 running on a linux box. \r\nTried to read files for training as well as save() / restore() - worked all ok (actually cool that gfile works).\r\nThe biggest hurdle is that you need the hadoop binaries on the windows side and there is no good binary distribution.", "@tensorflow-jenkins test this please", "The test timeout in \"Linux CPU Tests (Python 3)\" seems to be an unrelated flake. Merging the PR now.", "PR merged. Thanks, @guschmue and reviewers.", "Some notes for windows users:\r\n1. jvm.dll need be in PATH dirs. (As what you did for LD_LIBRARY_PATH on Linux)\r\nset PATH=%PATH%;%JAVA_HOME%\\jre\\bin\\server\r\n\r\n2.  Wildcards in CLASSPATH do not work, but if you expend all of the wildcards, it is too long to fit in an env var. You could set classpath in this way:\r\nhadoop classpath --jar c.jar\r\nset CLASSPATH=c.jar\r\n", "I think this is very helpful for people trying to setup hdfs on windows. Let me see if I can add this to the hdfs howto.", "@guschmue Sure. Please do it."]}, {"number": 6986, "title": "Add methods to fill and read Tensors from Java NIO buffers.", "body": "- This eliminates extra copies on Android apps where buffers are allocated in\r\n  native memory (e.g., camera2 ImageReader Images, Bitmaps).", "comments": ["Can one of the admins verify this patch?"]}, {"number": 6985, "title": "API changes: Missing RNN Module (continued)", "body": "(This is a continuation of issue #6984, which was closed before I could respond.)\r\nThank you.  But the question is still unanswered.  \r\nGiven that `tf.contrib.rnn` no longer exists.  What is the replacement for it?  I've read the docs, but haven't found an answer.  \r\nIn other words, what method is used to fuse together multiple RNN cells?\r\nI've tried the following:\r\n```\r\nlstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(n_hidden)\r\noutputs, states = tf.contrib.rnn.MultiRNNCell(lstm_cell_1, Xnew)\r\n```\r\nBut get the following error message:\r\n```\r\nTypeError: 'MultiRNNCell' object is not iterable\r\n```\r\nSo, What method is used to fuse together multiple RNN cells?", "comments": ["I posted an update on #6984  as well:\r\nhttps://github.com/tensorflow/tensorflow/issues/6984#issuecomment-274183963\r\n\r\nIn summary, I think you're looking for `tf.contrib.rnn.static_rnn`. Let us know if that is it.\r\n\r\nThat said, if possible, please do use the latest release if possible. Code at head is subject to change and will be hard to constantly keep up with. See my comment in the other issue.", "Hi Asim,\r\nI will continue to respond to the other issue.   \r\nI assume that you can still see it, even though it was closed?\r\nAside: It would be really helpful to give us a few minutes to respond before closing issues.  :-)\r\nWe can close this and I will continue on the other issue.\r\nThanks.\r\nKevin", "Apologies, bad judgement on my part on whether it was a slam dunk or not :)  If not, I wouldn't have closed the issue. Closing this one out, we can continue there."]}, {"number": 6984, "title": "API changes: Missing RNN module", "body": "This is a continuation #6973.  Given the ongoing API changes with r0.12 and the shortage of documentation/samples, it seems that API questions must be posed here.  \r\nWe had previously working code as follows:\r\n```\r\nlstm_cell_1 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\r\noutputs, states = tf.nn.rnn(lstm_cell_1, Xnew, dtype=tf.float32)\r\n```\r\nThe new code should be something like:\r\n```\r\nlstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(n_hidden)\r\noutputs, states = tf.contrib.rnn(lstm_cell_1, Xnew, dtype=tf.float32)\r\n```\r\nBut `tf.contrib.rnn` no longer exists.  I've tried `tf.contrib.rnn.MultiRNNCell` and `tf.contrib.rnn.fused_rnn_cell` but am told that `TypeError: 'module' object is not callable`.\r\nWhat is the new method to fuse together Basic LSTM cells?", "comments": ["cc @ebrevdo , the expert on all things RNN related", "Apologies for the churn and confusion.\r\n@ebrevdo and @xiejw can comment, but I believe the reasoning for the separation is because we're moving some parts into core.\r\n\r\n`BasicLSTMCell` can be found in `tf.nn.rnn_cell.BasicLSTMCell`. Same for `MultiRNNCell`.\r\n\r\nThe [API docs](https://www.tensorflow.org/api_docs/python/) are a good place to search for these.\r\n\r\nAgain, apologies, this churn should become a non-issue after 1.0.\r\n", "Thank you.  But my question still stands.  \r\nGiven that `tf.contrib.rnn` does not exist.  What is the replacement for it?  I've read the docs, but haven't found an answer.  \r\nWhat method is used to fuse together multiple RNN cells?\r\nI've tried the following:\r\n```\r\nlstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(n_hidden)\r\noutputs, states = tf.contrib.rnn.MultiRNNCell(lstm_cell_1, Xnew)\r\n```\r\nBut get the following error message:\r\n```\r\nTypeError: 'MultiRNNCell' object is not iterable\r\n```\r\nSo, What method is used to fuse together multiple RNN cells?", "I apologize, I misunderstood, it seem you're looking for where these functions are at head, as opposed to the last release, right? The docs, FWIW, talk about the release and not the current state of the head of the repository.\r\n\r\nIs it possible for you to use the latest release (0.12.1) for now (in which your original sample with `tf.nn.rnn_cell.BasicLSTMCell` and `tf.nn.rnn` should work)? Code at head is frequently changing and will provide a moving target. Once release, the compatibility tool at https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/compatibility should help you switch between releases.\r\n\r\nAs of right now, is `tf.contrib.rnn.static_rnn` what you're looking for?", "Hi Asim,\r\nThanks for the help.  I will delete the other issue.  \r\nWhen an issue is closed, I am never quite sure if anyone ever sees it again, so I generally create a new one.  Closed issues seem to disappear.  It would be nice if we could be given a short while to respond.\r\nI will change to r0.12.1.  I just need to find the `pip` to force a downgrade.\r\n\r\nFYI.  I changed the code to:\r\n```\r\nlstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(n_hidden)\r\noutputs, states = tf.contrib.rnn.static_rnn(lstm_cell_1, Xnew, dtype=tf.float32)\r\n```\r\nFor the record, here is the error message:\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-27-ae6b63faf2e1> in <module>()\r\n     43 l2 = lambda_loss_amount * sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\r\n     44 # L2 loss prevents this overkill neural network to overfit the data\r\n---> 45 cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y)) + l2 # Softmax loss\r\n     46 optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Adam Optimizer\r\n     47 \r\n\r\n/Users/kevin/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.pyc in softmax_cross_entropy_with_logits(_sentinel, labels, logits, dim, name)\r\n   1576   \"\"\"\r\n   1577   _ensure_xent_args(\"softmax_cross_entropy_with_logits\", _sentinel,\r\n-> 1578                     labels, logits)\r\n   1579 \r\n   1580   # TODO(pcmurray) Raise an error when the labels do not sum to 1. Note: This\r\n\r\n/Users/kevin/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.pyc in _ensure_xent_args(name, sentinel, labels, logits)\r\n   1531   if sentinel is not None:\r\n   1532     raise ValueError(\"Only call `%s` with \"\r\n-> 1533                      \"named arguments (labels=..., logits=..., ...)\" % name)\r\n   1534   if labels is None or logits is None:\r\n   1535     raise ValueError(\"Both labels and logits must be provided.\")\r\n\r\nValueError: Only call `softmax_cross_entropy_with_logits` with named arguments (labels=..., logits=..., ...)\r\n```", "I'm a bit confused about your error message above. That is when compiling from source at head, right? In general, I'd be weary to spending time to debug that (since the code is changing frequently), but if for some reason you do need to use sources at head please to provide a full snippet that reproduces the error. For example, the following does not lead to any errors:\r\n\r\n```python\r\nimport tensorflow as tf\r\nlstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(1)\r\noutputs, states = tf.contrib.rnn.static_rnn(lstm_cell_1, [tf.constant([[1.0],[1.0]], dtype=tf.float32)], dtype=tf.float32)\r\n```\r\n\r\nThat said, hopefully 0.12.1 works out for you. Do let me know.", "Hi Asim,\r\nNo troubles.  But yes, that was from 0.12.head.\r\nI only reported the error for your information.\r\nI just downgraded to 0.12.1.  For the record, I used this command:\r\n```\r\npip install --ignore-installed --upgrade 'tensorflow==0.12.1'\r\n```\r\nGive me a minute and I'll report on the status after I rollback.", "Okay.  Its fixed.\r\nThank you for your help!\r\nWe can close this now!\r\nHave a good weekend!", "Hi @kevinashaw ,\r\n\r\nI have the same issue. Are you using windows or linux? Is the downgrade version working for windows?\r\n\r\nThanks,\r\nHarry", "Downgrade version worked well for me.  Though I am now in the process of moving to 1.0.1.\r\nI used the following to force the version:\r\n```\r\npip install tensorflow==0.12.1\r\n```"]}, {"number": 6983, "title": "graph_metric.py removed?", "body": "Hello, \r\n\r\nIs graph_metric.py removed from TF? This is a useful feature for me. Can we add it back?\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/fe454464681b036ff7fed3e42c6bb541fa52dd7c/tensorflow/python/tools/graph_metrics.py\r\n\r\n", "comments": ["I think it has been replaced by the more flexible tfprof tool: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/tfprof and https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/tfprof\r\n\r\n(not sure what the difference between the two directories is, @petewarden )"]}, {"number": 6982, "title": "tensorflow MNIST getting error IOError: Not a gzipped file", "body": "When I try to run the basic tutorial example I get this as a return:\r\n\r\nTraceback (most recent call last):\r\n  File \"TensorFlowBasicTutorial\", line 76, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 43, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"TensorFlowBasicTutorial\", line 23, in main\r\n    mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py\", line 213, in read_data_sets\r\n    train_images = extract_images(f)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py\", line 53, in extract_images\r\n    magic = _read32(bytestream)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py\", line 35, in _read32\r\n    return numpy.frombuffer(bytestream.read(4), dtype=dt)[0]\r\n  File \"/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/gzip.py\", line 268, in read\r\n    self._read(readsize)\r\n  File \"/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/gzip.py\", line 303, in _read\r\n    self._read_gzip_header()\r\n  File \"/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/gzip.py\", line 197, in _read_gzip_header\r\n    raise IOError, 'Not a gzipped file'\r\nIOError: Not a gzipped file\r\n\r\n\r\nI uninstalled tensorflow and reinstalled tensor flow with no results. \r\nI have seen some information about installing the file locally, but then how do import the data? I am rather new to python so I do not know how to do this.\r\n\r\nThanks!", "comments": ["seems like network problems can cause this error, try solution here?\r\nhttps://perrohunter.com/fix-ioerror-not-a-gzipped-file-in-tensorflow-docker-example/", "Closing due to lack of recent activity. Please update the issue if it persists and we will reopen.", "HI,\r\ni encounter same issue, struggle a lot trying this and that, the simple answer for which i tried randomly and works for me is unzip the file manually and keep only unzipped files to your directory and try to run the command, cheers ", "I am facing the same problem.", "Yep, still an active problem\r\n\r\n`y_train = np.frombuffer(lbpath.read(), np.uint8, offset=8)\r\n  File \"C:\\Program Files\\Python37\\lib\\gzip.py\", line 276, in read\r\n    return self._buffer.read(size)\r\n  File \"C:\\Program Files\\Python37\\lib\\gzip.py\", line 463, in read\r\n    if not self._read_gzip_header():\r\n  File \"C:\\Program Files\\Python37\\lib\\gzip.py\", line 411, in _read_gzip_header\r\n    raise OSError('Not a gzipped file (%r)' % magic)\r\nOSError: Not a gzipped file (b'<!')`"]}, {"number": 6981, "title": "AttributeError: 'module' object has no attribute 'inv'", "body": "Hi there,\r\n\r\nI have an issue when I am trying to inverse a tensor as shown below\r\n\r\n>>> import tensorflow as tf\r\n>>> a = tf.placeholder(tf.float32)\r\n>>> tf.inv(a)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: 'module' object has no attribute 'inv'\r\n\r\nOperation System: mac os x 10.11.6\r\nPython 2.7.13\r\nI installed tensor flow using pip install tensorflow \r\n>>> tf.__version__\r\n'0.12.1'\r\n\r\nIf someone could help out, that would be cool! Thanks!", "comments": ["We're going through some API churn as we try to stabilize things for the 1.0 release (and can provide stability for all 1.x release as per our [versioning policy](https://www.tensorflow.org/resources/versions)). See [RELEASE.md](https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md) for changes.\r\n\r\nIn this particular case:\r\n`tf.inv` has been renamed to be `tf.reciprocal` (component-wise reciprocal) to avoid confusion with `np.inv` which is matrix inversion.\r\n\r\nApologies for the churn, once we hit 1.0, things should be much more stable.\r\n\r\nHope that help!", "It\u2019s perfect! Thanks a bunch!\n\n\n> On Jan 20, 2017, at 10:53 AM, Asim Shankar <notifications@github.com> wrote:\n> \n> We're going through some API churn as we try to stabilize things for the 1.0 release (and can provide stability for all 1.x release as per our versioning policy <https://www.tensorflow.org/resources/versions>). See RELEASE.md <https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md> for changes.\n> \n> In this particular case:\n> tf.inv has been renamed to be tf.reciprocal (component-wise reciprocal) to avoid confusion with np.inv which is matrix inversion.\n> \n> Apologies for the churn, once we hit 1.0, things should be much more stable.\n> \n> Hope that help!\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub <https://github.com/tensorflow/tensorflow/issues/6981#issuecomment-274149585>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AB2BMlYAf8FW9ji2ddwKnkejtgHvHWESks5rUQKQgaJpZM4Lpnly>.\n> \n\n"]}, {"number": 6980, "title": "Images are missing (unreachable) from the documentation page", "body": "In the documentation for tfdbg: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/how_tos/debugger/index.md\r\n\r\nthere are missing images\r\n", "comments": ["Thank you for reporting! We don't upload images to github, to save space. They are on the devsite documentation."]}, {"number": 6979, "title": "Feature Request: extend platform.flags (~ python-gflags)", "body": "Hello everyone,\r\n\r\nI would like to suggest to incorporate more flags into the platform.flags package.\r\n\r\nIn particular:\r\nDEFINE_multi\r\nDEFINE_multi_float\r\nDEFINE_multi_int\r\n\r\nThank you,\r\nPhilip", "comments": ["I believe things are transitioning away from platform.flags and new code is suggested to use `argparse`, could you do those things in `argparse` instead?", "Yes, please use argparse instead. We do not intend to extend the current flags library. Thanks!"]}, {"number": 6978, "title": "OpenCL Support for TensorFlow -- When is official support coming for macOS?", "body": "When will an official build of TensorFlow for the latest release be available for use with OpenCL-enabled graphics cards?  Should I fork a version and then hope to commit?  Is this something the core team is interested in because I have a NVIDIA-enabled iMac that is >3 years old and I cannot accelerate my computation for different projects.  What is possible within the next year?  ", "comments": ["Closing this out as a duplicate of #22 , which tracks OpenCL support."]}, {"number": 6977, "title": "iPython Tensorflow import error", "body": "I am trying to install Tensorflow GPU on a different environment than my original anaconda root. So I followed the instruction on the website, installed CUDA 8, cuDNN 5.1, and tensorflow on a new Tensorflow environment. \r\n\r\nWhen I execute the import tensorflow on Spyder - iPython I get his error:\r\n\r\n![tsfl4](https://cloud.githubusercontent.com/assets/21122937/22152365/7e7ebde0-df1a-11e6-8392-4a9c974e3f4b.jpeg)\r\n\r\nBut when I try to import it from Python then it works fine:\r\n\r\n![tsfl 5](https://cloud.githubusercontent.com/assets/21122937/22152403/b7f02208-df1a-11e6-950e-0538306440de.jpeg)\r\n\r\nPS: The same happens when I tried to install the simple tensorflow on my original root environment, works on python but not on ipython. \r\n\r\nAny suggestions please? Thank you.", "comments": ["This question is probably better asked on [stackoverflow](http://stackoverflow.com/questions/tagged/tensorflow) as we try to keep the github issues focused on bugs and feature requests. ", "Thanks but are you sure this is not a bug? It works on Python but not on iPython. This is weird. ", "Apparently there was a bug https://github.com/spyder-ide/spyder/issues/3987.\r\nThanks @asimshankar "]}, {"number": 6976, "title": "Unexpected behavior in tensorflow's distributed training", "body": "Hi Tensorflowers,\r\n\r\nI was doing some distributed training experiments on tensorflow v0.11.0.\r\nI modified the ResNet code from the official model zoos [here](https://github.com/tensorflow/models/tree/master/resnet) to be the one can do distributed training.\r\nIn the experiments, ResNet56 is used for training on CIFAR10 data set and the settings follow the original [paper](https://arxiv.org/abs/1512.03385).\r\n\r\nWhen I set the number of parameter server = 1, worker = 1, I expected the behavior would be the same as the single gpu one (which is the original code in the [repo](https://github.com/tensorflow/models/tree/master/resnet)).\r\nIt turns out that there is a huge performance gap. Please see the following figure.\r\n![resnet_cifar10_1ps1worker_tf](https://cloud.githubusercontent.com/assets/1206058/22150624/fd6332d2-df54-11e6-8091-7aeeb4770c5b.png)\r\n**x-axis**: time in second, **y-axis**: testing error\r\n**tf-0**: single gpu version, **tf-1**: distributed version with # ps=1, # worker=1\r\nBoth code were run by 160 epochs and with the same parameter/learning rate schedule.\r\n\r\nThe single gpu version can achieve 7% error rate (which is consistent with the original paper), but the distributed one is stalled at 12% error.\r\nI think there might be something wrong as the performance should be similar for both cases.\r\nCould you check if that is the case? (or maybe I used the wrong way to do distributed training)\r\nPlease feel free to ask if you have any problem with the setting. Thanks.\r\n\r\nThe single gpu version code can be found [here](https://github.com/tensorflow/models/blob/d93ffd0b69253ee3c3a40a19a4c86ac6975bd570/resnet/resnet_main.py) (I used the earlier 0.11 compatible version)\r\nThe code for distributed version can be found [here](https://gist.github.com/infwinston/8a7c86a75d3177bac4737903cc4c4fe3).\r\n\r\nSome detailed settings:\r\n```\r\nbatch size=128, num_residual_units=9, relu_leakiness=0\r\n```\r\n\r\nThe command I launched:\r\n```\r\nps0\r\n> export CUDA_VISIBLE_DEVICES=\"\"; python resnet_dist.py --ps_hosts=\"localhost:50000\" --worker_hosts=\"localhost:50001\" --job_name=\"ps\" --task_id=\"0\" --batch_size=128 --dataset='cifar10' --train_data_path=cifar10/data_batch* --log_root=./tmp-log-root/ --num_gpus=1 --mode train\r\nworker0\r\n> export CUDA_VISIBLE_DEVICES=\"0\"; python resnet_dist.py --ps_hosts=\"localhost:50000\" --worker_hosts=\"localhost:50001\" --job_name=\"worker\" --task_id=\"0\" --batch_size=128 --dataset='cifar10' --train_data_path=cifar10/data_batch* --log_root=./tmp-log-root/ --num_gpus=1 --mode train\r\n```\r\n\r\n### Environment info\r\nOperating System:\r\nUbuntu 14.04\r\n\r\nInstalled version of CUDA and cuDNN:\r\nCUDA 7.5, cuDNN 5.1\r\n```\r\n> ls -l /usr/local/cuda/lib/libcud*\r\n-rw-r--r-- 1 root root 189170 Oct 25 22:51 /usr/local/cuda/lib/libcudadevrt.a\r\nlrwxrwxrwx 1 root root     16 Oct 25 22:51 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5\r\nlrwxrwxrwx 1 root root     19 Oct 25 22:51 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\r\n-rwxr-xr-x 1 root root 311596 Oct 25 22:51 /usr/local/cuda/lib/libcudart.so.7.5.18\r\n-rw-r--r-- 1 root root 558020 Oct 25 22:51 /usr/local/cuda/lib/libcudart_static.a\r\n```\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n282823b877f173e6a33bbc9d4b9ad7dd8413ada6\r\n2. The output of `bazel version`\r\n```\r\nBuild label: 0.4.2\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Dec 7 18:47:11 2016 (1481136431)\r\nBuild timestamp: 1481136431\r\nBuild timestamp as int: 1481136431\r\n```\r\nI built tensorflow v0.11.0 by myself.\r\n", "comments": ["From the graph, it looks that one of the runs \"got lucky\". These kind of jumps indicate your network is badly tuned and, say, larger network, could have more consistent increase over time.\r\n\r\nIt seems possible that small differences in behavior of distributed version might make a badly tuned network be sometimes better. IE, using two processes instead of one may introduce small difference in timing, which affects training (ie, summary threads pull data from the input queue, so the timing of summary thread scheduling affects which data the network sees during training).\r\n\r\nAlso, you are using SyncReplicas optimizer for your distributed version, whereas a more close comparison would use regular SGD. This list is for bugs/feature request, I think you need to isolate the difference more reliably to be sure this is a bug in TensorFlow (ie, perhaps also by transitioning to SavedModel instead of Supervisor to reduce some thread-scheduling based randomness)", "This question is probably better suited for [stackoverflow](http://stackoverflow.com/questions/tagged/tensorflow) as we try to keep the github issues list focused on bugs and feature requests.", "Thanks for your response.\r\nBut actually I don't think it's the case that one just got lucky.\r\nThere might be some randomness but the performance gap should not be such huge I think.\r\nThe jump of curve is because the learning rate decreased and indeed it is reproducible.\r\nI have done several experiments on it and found that is the case.\r\nAlso, this kind of curve is consistent with the one in the original [paper](https://arxiv.org/pdf/1512.03385v1.pdf) (See Fig. 6) and many people can reproduce it by using the same settings (e.g., [[1]](http://torch.ch/blog/2016/02/04/resnets.html), [[2]](https://github.com/ppwwyyxx/tensorpack/tree/master/examples/ResNet), or more examples [here](https://github.com/KaimingHe/deep-residual-networks#third-party-re-implementations))\r\nBut you're right, I should report the one without using SyncReplicas first. I am already working on this.\r\n\r\nAnyway, I did not have a direct proof to something wrong in tensorflow but if you think this is not the right place to discuss then I can move to stackoverflow.\r\nThanks.\r\n", "Yes, the jump seems consistent with learning rate getting divided by 10. In your distributed version, your learning rate is a constant, so I guess that would explain why it doesn't have the jump?\r\n\r\nDiscussing on closed issues is fine, it's just that open issues are a sign that it needs attention from someone from core team to triage/fix the issue.", "@yaroslavvb Thanks!\r\nI think I made a mistake in the distributed version code... I wrongly thought I can still control the learning rate by assigning [here](https://gist.github.com/infwinston/8a7c86a75d3177bac4737903cc4c4fe3#file-resnet_dist-py-L150).\r\nBut the situation is different from the original one as I used an another optimizer instead.\r\nSorry about the mistake and many thanks for pointing out this. I think it should be normal now.", "@yaroslavvb Thanks for your help last time. I've faced another issue related to this. Could you please take a look on my stackoverflow [post](http://stackoverflow.com/questions/42006967/scalability-issues-related-to-distributed-tensorflow) if you have time? Thanks a lot."]}, {"number": 6975, "title": "TF-549 Adds unsorted segment max Op", "body": "Adds unsorted segment max. Fixes #549, Continuation of #4051.\r\n@drpngx, rebased to current master and squashed the commits.\r\nHope we'll manage this time!", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "Fixed the Unit-tests, they should run through now, does jenkins need to be retriggered?", "@tensorflow-jenkins test this please", "Changed the names of member functors now", "@tensorflow-jenkins test this please", "@rmlarsen there are still pending changes to be made", "@vrv OK, thanks for clarifying.", "Addressed the second concern now", "@vrv is this good to go?", "@andydavis1 I assume this is okay now?", "yes, changes were approved.", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please\r\n", "The test failures are unrelated, I believe. Merging.", "Thank you for your incredible patience @nikste !! Glad to have finally got this in :)", "Are there any plans for the GPU implementation? @nikste "]}, {"number": 6974, "title": "Race condition in EventFileWriter", "body": "There is a race condition here in `EventFileWriter`: [here](https://github.com/tensorflow/tensorflow/blob/287db3a9b0701021f302e7bb58af5cf89fdcd424/tensorflow/python/summary/writer/event_file_writer.py#L68)\r\n\r\n```python\r\n    if not gfile.IsDirectory(self._logdir):\r\n      gfile.MakeDirs(self._logdir)\r\n```\r\n\r\nIt is possible for multiple concurrent threads and/or processes to get to this code simultaneously, both check `IsDirectory`, return `False`, then both try creating the directory, and have one succeed and one error.\r\n\r\nThis can be fixed by by catching the `AlreadyExistsError` from `MakeDirs` and handling it gracefully.\r\n\r\n(this is a real issue that we have encountered in daily usage and causes us issues, not just a hypothetical)", "comments": ["I don't believe it is recommended to have two distinct `EventFileWriters` writing to the same log directory as that could lead to two of them writing to the same file simultaneously, which will certainly be incorrect.\r\n\r\nCould you elaborate on how you run into this situation and why you need to have concurrent `EventFileWriters` in the same log directory? Can you have your concurrent writers writing to different directories?", "I agree that this can be handled more effectively by fixing user code, but it is still an issue that should be fixed and should be causing at most a warning (not a crash).\r\n\r\nThe reason this is happening is because we are training in parallel using MPI, and so we have many copies of identical MPI processes that run the same model (but communicate with each other to reduce gradients). If the processes are identical they will each write a log of the things they are doing to each directory. This _could_ be handled by either doing a per-rank log directory or by having only a single rank write logs, and that is how we will handle this for the time being.\r\n\r\nMore importantly, the reason this happens in a context like this is incredibly confusing and non-deterministic, and in general this situation is more appropriately dealt with by catching the error rather than by checking for the directory existing up-front; you can have a variety of other situations in which the directory does not initially exist but then gets created, and probably not all of those situations are user errors.", "We are also experimenting with MPI for training, seems like catching/ignoring `AlreadyExistsError ` is a useful fix.", "It won't be as simple as that, since we'd want to ensure that the underlying [C++ `EventsWriter`](https://github.com/tensorflow/tensorflow/blob/754048a0453a04a761e112ae5d99c149eb9910dd/tensorflow/core/util/events_writer.h#L43) doesn't end up with the same filename and that we are not writing concurrently to the same underlying file.\r\n\r\nBut that said, it shouldn't be too hard to fix the whole chain. Contributions welcome!", "These are somewhat orthogonal issues. I can trigger this issue without doing multiple training jobs at once \u2013 for example, I can have a loop that runs a small TensorFlow script with logging over and over again and writing to dir `log-dir`, and concurrently another process that is constantly looping and doing `rm -rf log-dir`. Eventually you'll hit this race condition. \r\n\r\nDo you think a fix for this issue could be merged _without_ having to drag in coordination between multiple processes doing training? That may be our specific use case, but it has nothing to do with what's causing the bug...", "In particular, the using-the-same-filename issue is _already_ present: suppose the directory already exists in advance, and you run two jobs training and logging to the same directory. They might write to the same file already as it is now \u2013 changing the semantics here does not affect that bug. Since these are two separate bugs they can be fixed in two separate issues and PRs.", "I was thinking about things working end-to-end, but as you correctly point out, there is no strict dependency. \r\n\r\nThat said, I'm not sure if the right fix is to catch and ignore the `AlreadyExistsError` or if there should be no error in the first place. For example, the underlying C++ operation should _not_ be failing (https://github.com/tensorflow/tensorflow/blob/5e5dc97dd3523509ce5f536d7be5122d016fc6b5/tensorflow/core/platform/file_system.h#L107) \r\n\r\nSo I'd like to understand why the error is being thrown. Doesn't happen in this simple test:\r\n```python\r\nimport tensorflow as tf\r\ntf.platform.gfile.MakeDirs('/tmp/foo')\r\ntf.platform.gfile.MakeDirs('/tmp/foo')\r\n```\r\n\r\nLet me dig around a bit as to why you're getting this error, or if you have any pointers, that would be help.", "It seems there is some inconsistency between implementations of `FileSystem::CreateDir`. For example, [posix_file_system.cc:230](https://github.com/tensorflow/tensorflow/blob/5e5dc97dd3523509ce5f536d7be5122d016fc6b5/tensorflow/core/platform/posix/posix_file_system.cc#L230) fails when the directory already exist, while the [test filesystem](https://github.com/tensorflow/tensorflow/blob/ec7929b878926c39255254e9aea992f0bc65aa68/tensorflow/core/platform/file_system_test.cc#L48) does not.\r\n\r\nWe'll look into making these `FileSystem` implementations consistent and having `FileSystem::RecursivelyCreateDir` and consequently `gfile.MakeDirs` not fail when the directory already exists.\r\n", "Thanks!"]}, {"number": 6973, "title": "ImportError: No module named nn.rnn", "body": "I'm having trouble accessing the rnn models.  The following commands:\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.nn.rnn import rnn, rnn_cell\r\n```\r\nyield the following error:\r\n```\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-4-f6712a17b199> in <module>()\r\n----> 1 from tensorflow.nn.rnn import rnn, rnn_cell\r\n\r\nImportError: No module named nn.rnn\r\n```\r\nAnd the line:\r\n```\r\nlstm_cell_1 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\r\n```\r\nyields:\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-5-f0f106c78703> in <module>()\r\n----> 1 lstm_cell_1 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\r\n\r\nAttributeError: 'module' object has no attribute 'rnn_cell'\r\n\r\n```\r\nThe version is  `'0.12.head'` and I'm working on MacOS 10.12.2.  I'm working off git hash #7c16e2e23.\r\nI have been following the commentary in [#2685](https://github.com/tensorflow/tensorflow/issues/2685) and others.\r\nFirst, I installed setuptools:\r\n```\r\npip install --upgrade -I setuptools\r\n```\r\nand then the latest python:\r\n```\r\npip install --ignore-installed --upgrade tensorflow-0.12.1-py2-none-any.whl \r\n```\r\nIs this a real issue, or a problem with my setup?", "comments": ["We've been moving modules around it preparation for the 1.0 release so that only API that we will keep stable is in the `tf` module and its submodules (excluding `tf.contrib`). Much of the `rnn` functionality is now in `tf.contrib.rnn`.\r\n\r\nHope that helps.", "Thanks again!  -K"]}, {"number": 6972, "title": "Master", "body": "This reverts commit 0552f304028f2b5541704cfde3736ba7387e1df0.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "?"]}, {"number": 6971, "title": "Invalid argument: Cannot parse tensor from proto:  dtype: DT_FLOAT", "body": "@lukaszkaiser I am facing the same problem. I have same specification as the previous one. My code is\r\n`Final_dataset.csv` contains 456 X 147457\r\n\r\n    `train = pd.read_csv('Final_dataset.csv')\r\n     x = tf.placeholder(tf.float32, shape=[None, 128*128*3])\r\n     y_ = tf.placeholder(tf.float32, shape=[None, 128*256*3])\r\n     W = tf.Variable(tf.zeros([128*128*3,128*256*3]))\r\n     b = tf.Variable(tf.zeros([128*256*3]))`\r\n\r\nWhenever I run below command\r\n     `sess.run(tf.global_variables_initializer())`\r\n\r\nI am getting this error\r\n\r\n    `W tensorflow/core/framework/op_kernel.cc:965] Invalid argument: Cannot parse tensor from proto:    dtype: DT_FLOAT\r\n      tensor_shape {\r\n          dim {\r\n             size: 49152\r\n          }\r\n          dim {\r\n             size: 98304\r\n          }\r\n      }\r\n      float_val: 0\r\n\r\n      E tensorflow/core/common_runtime/executor.cc:390] Executor failed to create kernel. Invalid argument: Cannot parse tensor from proto: dtype: DT_FLOAT\r\n      tensor_shape {\r\n         dim {\r\n            size: 49152\r\n         }\r\n        dim {\r\n            size: 98304\r\n        }\r\n     }\r\n    float_val: 0\r\n\r\n\t [[Node: zeros_2 = Const[dtype=DT_FLOAT, value=<Invalid TensorProto: dtype: DT_FLOAT tensor_shape { dim { size: 49152 } dim { size: 98304 } } float_val: 0>, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n       Traceback (most recent call last):\r\n        File \"<stdin>\", line 1, in <module>\r\n        File \"/home/lokesh/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 766, in run\r\n    run_metadata_ptr)\r\n         File \"/home/lokesh/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 964, in _run\r\n    feed_dict_string, options, run_metadata)\r\n        File \"/home/lokesh/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\r\n    target_list, options, run_metadata)\r\n       File \"/home/lokesh/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\r\n    raise type(e)(node_def, op, message)\r\n     tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot parse tensor from proto: dtype: DT_FLOAT\r\n     tensor_shape {\r\n       dim {\r\n          size: 49152\r\n        }\r\n        dim {\r\n            size: 98304\r\n        }\r\n     }\r\n      float_val: 0\r\n\r\n\t [[Node: zeros_2 = Const[dtype=DT_FLOAT, value=<Invalid TensorProto: dtype: DT_FLOAT tensor_shape { dim { size: 49152 } dim { size: 98304 } } float_val: 0>, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n\r\n    Caused by op u'zeros_2', defined at:\r\n    File \"<stdin>\", line 1, in <module>\r\n    File \"/home/lokesh/.local/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 1437, in zeros\r\n    output = constant(zero, shape=shape, dtype=dtype, name=name)\r\n     File \"/home/lokesh/.local/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.py\", line 169, in constant\r\n    attrs={\"value\": tensor_value, \"dtype\": dtype_value}, name=name).outputs[0]\r\n     File \"/home/lokesh/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n     File \"/home/lokesh/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\n    InvalidArgumentError (see above for traceback): Cannot parse tensor from proto: dtype: DT_FLOAT\r\n    tensor_shape {\r\n     dim {\r\n        size: 49152\r\n     }\r\n     dim {\r\n       size: 98304\r\n     }\r\n     }\r\n     float_val: 0\r\n\r\n\t [[Node: zeros_2 = Const[dtype=DT_FLOAT, value=<Invalid TensorProto: dtype: DT_FLOAT tensor_shape { dim { size: 49152 } dim { size: 98304 } } float_val: 0>, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n`", "comments": ["Without the dataset (or, preferably, a smaller one for testing) it is not possible to reproduce your problem. Please prepare a smaller dataset and a minimal python code that causes the problem, then attach the small CSV file and the python file so that we can reproduce the problem. Thanks.", "Please also include all the other information asked for in the new issue template, like the version of TensorFlow being used.", "Specifications: \r\nOperating System: ubuntu 16.04\r\nCPU version, no cuda installed\r\nI installed that from pip3\r\ntensorflow version: 0.10, link: [https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0-cp34-cp34m-linux_x86_64.whl](https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0-cp34-cp34m-linux_x86_64.whl)\r\n\r\nTest file:\r\n[Test.zip](https://github.com/tensorflow/tensorflow/files/719558/Test.zip)\r\n\r\nAbove is the test file contains 10 rows X 147457 columns.\r\n\r\nSo the code is:\r\n \r\n     train = pd.read_csv('Test.csv')\r\n     x = tf.placeholder(tf.float32, shape=[None, 128*128*3])\r\n     y_ = tf.placeholder(tf.float32, shape=[None, 128*256*3])\r\n     W = tf.Variable(tf.zeros([128*128*3,128*256*3]))\r\n     b = tf.Variable(tf.zeros([128*256*3])\r\n\r\nAs soon as I run this command:  `sess.run(tf.global_variables_initializer())`\r\nError comes ans it is same as that of above.\r\n", "It would help if you could provide the full program to execute. In this case, after correcting for some syntax errors in your snippet above, I'm assuming it was something like:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport pandas as pd\r\ntrain = pd.read_csv('Test.csv')\r\nx = tf.placeholder(tf.float32, shape=[None, 128*128*3])\r\ny_ = tf.placeholder(tf.float32, shape=[None, 128*256*3])\r\nW = tf.Variable(tf.zeros([128*128*3,128*256*3]))\r\nb = tf.Variable(tf.zeros([128*256*3]))\r\ntf.Session().run(tf.global_variables_initializer())\r\n```\r\nA couple of things:\r\n\r\n- `tf.global_variables_initializer()` did not exist in release 0.10, which you pointed to. It was added in 0.12, so are you sure you're using 0.10?\r\n- I'm not quite sure how the `csv` file is playing a role here since the code snipped does nothing with it. Do you have the same problem without the `pd.read_csv` line?\r\n- The `W` matrix is pretty big (roughly 18GB, given its shape of (128*128*3) x (128*256*3)), so I'm guessing you're running this on a pretty beefy machine :)\r\n\r\nIf I scale down the weight matrix just so that I can quickly run it without needing 18GB of RAM (`W = tf.Variable(tf.zeros([128*3,256*3]))`) , the modified snippet works out just fine in 0.12 but fails in 0.10 because `tf.global_variables_initializer()` does not exist. \r\n\r\nI suspect there is some other code that you're running that is leading to this problem, and you're not using 0.10 but some other version.\r\n\r\nTo provide useful help, it would help to have a self-contained reproducible example with the exact version you're using (you can use `print tf.__version__` to get to that as well)", "@asimshankar Sorry the tensorflow version is\r\n      `>>> print(tf.__version__)` \r\n              `0.12.1`\r\nI am trying to run it on my local system only. I have 8GB of RAM and 1TB HDD.\r\n\r\nThe error is coming without the `pd.read_csv` line also.\r\n\r\n **I suspect there is some other code that you're running that is leading to this problem, and you're not using 0.10 but some other version.**\r\n    No @asimshankar,  what I am doing is that I am pasting the code one by one on the terminal in the sequence given above. But as soon as I am entering the command `sess.run(tf.global_variables_initializer())` I am getting the error.\r\n\r\n I am changing the code that is given [here](https://www.tensorflow.org/tutorials/mnist/pros/). In this link the code is for the grey image, I am changing it **to make it work for the original image**(containing RGB).\r\n\r\nBelow is the code that I am using\r\n[For test.py.tar.gz](https://github.com/tensorflow/tensorflow/files/720822/For.test.py.tar.gz)\r\nCode is very similar to as shown [here](https://www.tensorflow.org/tutorials/mnist/pros/)\r\n", "I'm sorry, I am unable to reproduce your problem. Please list out the full code you're running, starting from a clean slate. For example, I ran the following snippet and it succeeds. Does it work for your? If it does, what precise changes cause it to fail?\r\n\r\n```python\r\npython\r\nimport tensorflow as tf\r\nprint tf.__version__\r\nx = tf.placeholder(tf.float32, shape=[None, 128*128*3])\r\ny_ = tf.placeholder(tf.float32, shape=[None, 128*256*3])\r\nW = tf.Variable(tf.zeros([128*3,256*3]))\r\nb = tf.Variable(tf.zeros([128*256*3]))\r\ntf.Session().run(tf.global_variables_initializer())\r\n```\r\n\r\nNote that I have changed the shape of the `W` variable to be much smaller.\r\nIf you're using the size mentioned in your snippet earlier of `[128*128*3,128*256*3]` on your 8GB machine, then you will run into trouble as this tensor would require 18GB and will cause your system to thrash heavily.", "@asimshankar when I am running the last command of your code( I have changed the shape of W as you said) `tf.Session().run(tf.global_variables_initializer())` then I am getting the error. \r\nI have sent you my code that I am running in the zip folder.\r\n\r\nTo be precise I want to work with the colored image, so I am modifying the code given[here](https://www.tensorflow.org/tutorials/mnist/pros/).\r\nHope you understand!\r\n", "Is there anybody to help me?", "Facing similar issue:\r\n`W tensorflow/core/framework/op_kernel.cc:965] Invalid argument: Cannot parse tensor from proto: dtype: DT_FLOAT\r\ntensor_shape {\r\n  dim {\r\n    size: 377788\r\n  }\r\n  dim {\r\n    size: 377724\r\n  }\r\n}\r\nfloat_val: 0\r\n`", "Even I am facing same issue. Is there any solution for same?\r\n\r\n2017-06-02 16:20:59.440852: W tensorflow/core/framework/op_kernel.cc:1142] Invalid argument: Cannot parse tensor from proto: dtype: DT_FLOAT\r\ntensor_shape {\r\n  dim {\r\n    size: 15900942\r\n  }\r\n  dim {\r\n    size: 53003\r\n  }\r\n}\r\nfloat_val: 0\r\n", "## same issue here, when run \r\n```\r\n            with tf.device('/gpu:1'):\r\n                self.sess.run(tf.global_variables_initializer())\r\n```\r\n\r\nLimit:                 68719476736\r\nInUse:                        2816\r\nMaxInUse:                  4197120\r\nNumAllocs:                      54\r\nMaxAllocSize:              4194304\r\n\r\n2017-07-27 20:05:05.729344: W tensorflow/core/common_runtime/bfc_allocator.cc:277] *___________________________________________________________________________________________________\r\n2017-07-27 20:05:07.050444: W tensorflow/core/framework/op_kernel.cc:1148] Invalid argument: Cannot parse tensor from proto: dtype: DT_FLOAT\r\ntensor_shape {\r\n  dim {\r\n    size: 200704\r\n  }\r\n  dim {\r\n    size: 802816\r\n  }\r\n}\r\nfloat_val: 0\r\n\r\n\r\nProcess finished with exit code 137 (interrupted by signal 9: SIGKILL)", "+1 on this issue", "+1\r\n017-11-07 17:01:24.884678: W tensorflow/core/framework/op_kernel.cc:1182] Invalid argument: Cannot parse tensor from proto: dtype: DT_FLOAT\r\ntensor_shape {\r\n  dim {\r\n    size: 832\r\n  }\r\n  dim {\r\n    size: 832\r\n  }\r\n  dim {\r\n    size: 832\r\n  }\r\n  dim {\r\n    size: 216\r\n  }\r\n}\r\nfloat_val: 0\r\n\r\n2017-11-07 17:01:24.884742: E tensorflow/core/common_runtime/executor.cc:643] Executor failed to create kernel. Invalid argument: Cannot parse tensor from proto: dtype: DT_FLOAT\r\ntensor_shape {\r\n  dim {\r\n    size: 832\r\n  }\r\n  dim {\r\n    size: 832\r\n  }\r\n  dim {\r\n    size: 832\r\n  }\r\n  dim {\r\n    size: 216\r\n  }\r\n}\r\nfloat_val: 0\r\n\r\n\t [[Node: conv12/W/Momentum/Initializer/zeros = Const[_class=[\"loc:@conv12/W\"], dtype=DT_FLOAT, value=<Invalid TensorProto: dtype: DT_FLOAT tensor_shape { dim { size: 832 } dim { size: 832 } dim { size: 832 } dim { size: 216 } } float_val: 0>, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n2017-11-07 17:01:24.896778: W tensorflow/core/framework/op_kernel.cc:1182] Invalid argument: Cannot parse tensor from proto: dtype: DT_FLOAT\r\ntensor_shape {\r\n  dim {\r\n    size: 832\r\n  }\r\n  dim {\r\n    size: 832\r\n  }\r\n  dim {\r\n    size: 832\r\n  }\r\n  dim {\r\n    size: 216\r\n  }\r\n}\r\nfloat_val: 0\r\n\r\n2017-11-07 17:01:24.897232: E tensorflow/core/common_runtime/executor.cc:643] Executor failed to create kernel. Invalid argument: Cannot parse tensor from proto: dtype: DT_FLOAT\r\ntensor_shape {\r\n  dim {\r\n    size: 832\r\n  }\r\n  dim {\r\n    size: 832\r\n  }\r\n  dim {\r\n    size: 832\r\n  }\r\n  dim {\r\n    size: 216\r\n  }\r\n}\r\nfloat_val: 0\r\n", "+1\r\n```\r\nUsing TensorFlow backend.\r\nTrain on 2000 samples, validate on 500 samples\r\nEpoch 1/50\r\n2017-11-19 14:57:23.563145: W tensorflow/core/framework/op_kernel.cc:1182] Invalid argument: Cannot parse tensor from proto: dtype: DT_FLOAT\r\ntensor_shape {\r\n  dim {\r\n    size: 7200\r\n  }\r\n  dim {\r\n    size: 230400\r\n  }\r\n}\r\nfloat_val: 0\r\n\r\n2017-11-19 14:57:23.569100: E tensorflow/core/common_runtime/executor.cc:643] Executor failed to create kernel. Invalid argument: Cannot parse tensor from proto: dtype: DT_FLOAT\r\ntensor_shape {\r\n  dim {\r\n    size: 7200\r\n  }\r\n  dim {\r\n    size: 230400\r\n  }\r\n}\r\nfloat_val: 0\r\n\r\n\t [[Node: training/Adadelta/Const_2 = Const[dtype=DT_FLOAT, value=<Invalid TensorProto: dtype: DT_FLOAT tensor_shape { dim { size: 7200 } dim { size: 230400 } } float_val: 0>, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n2017-11-19 14:57:23.574125: W tensorflow/core/framework/op_kernel.cc:1182] Invalid argument: Cannot parse tensor from proto: dtype: DT_FLOAT\r\ntensor_shape {\r\n  dim {\r\n    size: 230400\r\n  }\r\n  dim {\r\n    size: 7200\r\n  }\r\n}\r\nfloat_val: 0\r\n\r\n2017-11-19 14:57:23.574245: E tensorflow/core/common_runtime/executor.cc:643] Executor failed to create kernel. Invalid argument: Cannot parse tensor from proto: dtype: DT_FLOAT\r\ntensor_shape {\r\n  dim {\r\n    size: 230400\r\n  }\r\n  dim {\r\n    size: 7200\r\n  }\r\n}\r\nfloat_val: 0\r\n\r\n\t [[Node: training/Adadelta/Const = Const[dtype=DT_FLOAT, value=<Invalid TensorProto: dtype: DT_FLOAT tensor_shape { dim { size: 230400 } dim { size: 7200 } } float_val: 0>, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1323, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1302, in _run_fn\r\n    status, run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\", line 473, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot parse tensor from proto: dtype: DT_FLOAT\r\ntensor_shape {\r\n  dim {\r\n    size: 230400\r\n  }\r\n  dim {\r\n    size: 7200\r\n  }\r\n}\r\nfloat_val: 0\r\n\r\n\t [[Node: training/Adadelta/Const = Const[dtype=DT_FLOAT, value=<Invalid TensorProto: dtype: DT_FLOAT tensor_shape { dim { size: 230400 } dim { size: 7200 } } float_val: 0>, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"autoencoder.py\", line 38, in <module>\r\n    validation_data=(validation_set, validation_set))\r\n  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\", line 1650, in fit\r\n    validation_steps=validation_steps)\r\n  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\", line 1213, in _fit_loop\r\n    outs = f(ins_batch)\r\n  File \"/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\", line 2350, in __call__\r\n    session = get_session()\r\n  File \"/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\", line 188, in get_session\r\n    session.run(tf.variables_initializer(uninitialized_vars))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 889, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1120, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\r\n    options, run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1336, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot parse tensor from proto: dtype: DT_FLOAT\r\ntensor_shape {\r\n  dim {\r\n    size: 230400\r\n  }\r\n  dim {\r\n    size: 7200\r\n  }\r\n}\r\nfloat_val: 0\r\n\r\n\t [[Node: training/Adadelta/Const = Const[dtype=DT_FLOAT, value=<Invalid TensorProto: dtype: DT_FLOAT tensor_shape { dim { size: 230400 } dim { size: 7200 } } float_val: 0>, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\nCaused by op 'training/Adadelta/Const', defined at:\r\n  File \"autoencoder.py\", line 38, in <module>\r\n    validation_data=(validation_set, validation_set))\r\n  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\", line 1627, in fit\r\n    self._make_train_function()\r\n  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\", line 990, in _make_train_function\r\n    loss=self.total_loss)\r\n  File \"/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\", line 87, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/keras/optimizers.py\", line 346, in get_updates\r\n    accumulators = [K.zeros(shape) for shape in shapes]\r\n  File \"/usr/local/lib/python3.5/dist-packages/keras/optimizers.py\", line 346, in <listcomp>\r\n    accumulators = [K.zeros(shape) for shape in shapes]\r\n  File \"/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\", line 675, in zeros\r\n    return variable(tf.constant_initializer(0., dtype=tf_dtype)(shape),\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/init_ops.py\", line 200, in __call__\r\n    self.value, dtype=dtype, shape=shape, verify_shape=verify_shape)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/constant_op.py\", line 214, in constant\r\n    name=name).outputs[0]\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): Cannot parse tensor from proto: dtype: DT_FLOAT\r\ntensor_shape {\r\n  dim {\r\n    size: 230400\r\n  }\r\n  dim {\r\n    size: 7200\r\n  }\r\n}\r\nfloat_val: 0\r\n\r\n\t [[Node: training/Adadelta/Const = Const[dtype=DT_FLOAT, value=<Invalid TensorProto: dtype: DT_FLOAT tensor_shape { dim { size: 230400 } dim { size: 7200 } } float_val: 0>, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n```\r\n", "Same problem here.\r\n\r\nWindows 10 x64, Python 3.5.2, TF 1.4.0, 8Gb RAM", "the same problem \r\n\r\n\r\n`InvalidArgumentError (see above for traceback): Cannot parse tensor from proto: dtype: DT_FLOAT\r\ntensor_shape {\r\n  dim {\r\n    size: 150528\r\n  }\r\n  dim {\r\n    size: 150528\r\n  }\r\n}\r\nfloat_val: 0\r\n\r\n\t [[Node: training/SGD/Const = Const[dtype=DT_FLOAT, value=<Invalid TensorProto: dtype: DT_FLOAT tensor_shape { dim { size: 150528 } dim { size: 150528 } } float_val: 0>, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n`", "Same Problem, pls guide me\r\n\r\nInvalidArgumentError: Cannot parse tensor from proto: dtype: DT_FLOAT\r\ntensor_shape {\r\n  dim {\r\n    size: 3840000\r\n  }\r\n  dim {\r\n    size: 1024\r\n  }\r\n}\r\nfloat_val: 0", "@lokesh005 @rsathishr @wangzhenhui1992 @pgalilea @huyvohcmc @pribadihcr @LeviZell @Lukeeeeee @mayankj08 @pj-parag - Sorry to hear this, but it would help if more detailed information was provided. Is everyone using 1.4.0? Can you provide a short snippet that reproduces the problem?\r\n\r\nFor example, in a [previous comment](https://github.com/tensorflow/tensorflow/issues/6971#issuecomment-274227371) I had provided a sample snippet (which doesn't reproduce the problem for me though). If that snippet causes you trouble, please do provide the full output (including the version information printed by the line `print(tf.__version__)`). @lokesh005 - I'm not clear from your responses as to which version you're using, and you pointed to \"modifications\" to the tutorial code, but without knowing what those modifications are, it will be hard to diagnose the issue. \r\n\r\nLooking forward to some more detailed information.\r\n\r\nThanks.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Closing this due to inactivity. If this problem is encountered, please feel free to reopen but do provide all the details asked for in the new issue template and in https://github.com/tensorflow/tensorflow/issues/6971#issuecomment-352522044 \r\n\r\nA more detailed description on how to reproduce the problem will help us diagnose it. Thanks!", "I experienced the same issue. Reducing vector size solved the problem, alternatively one can try machine with more RAM.", "Same Issue:\r\n\r\n\r\nSTACK TRACE...\r\n\r\n\r\n\r\nCreating and compiling model...\r\n\r\nFitting model...\r\nTrain on 15 samples, validate on 3 samples\r\nEpoch 1/20\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _do_call(self, fn, *args)\r\n   1326     try:\r\n-> 1327       return fn(*args)\r\n   1328     except errors.OpError as e:\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1305                                    feed_dict, fetch_list, target_list,\r\n-> 1306                                    status, run_metadata)\r\n   1307 \r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\contextlib.py in __exit__(self, type, value, traceback)\r\n     87             try:\r\n---> 88                 next(self.gen)\r\n     89             except StopIteration:\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py in raise_exception_on_not_ok_status()\r\n    465           compat.as_text(pywrap_tensorflow.TF_Message(status)),\r\n--> 466           pywrap_tensorflow.TF_GetCode(status))\r\n    467   finally:\r\n\r\nInvalidArgumentError: Cannot parse tensor from proto: dtype: DT_FLOAT\r\ntensor_shape {\r\n  dim {\r\n    size: 200000000\r\n  }\r\n  dim {\r\n    size: 1024\r\n  }\r\n}\r\nfloat_val: 0\r\n\r\n\t [[Node: training_6/SGD/Const_8 = Const[dtype=DT_FLOAT, value=<Invalid TensorProto: dtype: DT_FLOAT tensor_shape { dim { size: 200000000 } dim { size: 1024 } } float_val: 0>, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-45-fcfc7f69b24a> in <module>()\r\n     20                 validation_data=([X1_valid, X2_valid], y_valid),\r\n     21                 verbose=1,\r\n---> 22                 callbacks=callbacks)\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\r\n   1596                               initial_epoch=initial_epoch,\r\n   1597                               steps_per_epoch=steps_per_epoch,\r\n-> 1598                               validation_steps=validation_steps)\r\n   1599 \r\n   1600     def evaluate(self, x, y,\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py in _fit_loop(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\r\n   1181                     batch_logs['size'] = len(batch_ids)\r\n   1182                     callbacks.on_batch_begin(batch_index, batch_logs)\r\n-> 1183                     outs = f(ins_batch)\r\n   1184                     if not isinstance(outs, list):\r\n   1185                         outs = [outs]\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py in __call__(self, inputs)\r\n   2268                 value = (indices, sparse_coo.data, sparse_coo.shape)\r\n   2269             feed_dict[tensor] = value\r\n-> 2270         session = get_session()\r\n   2271         updated = session.run(self.outputs + [self.updates_op],\r\n   2272                               feed_dict=feed_dict,\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py in get_session()\r\n    165     if not _MANUAL_VAR_INIT:\r\n    166         with session.graph.as_default():\r\n--> 167             _initialize_variables()\r\n    168     return session\r\n    169 \r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py in _initialize_variables()\r\n    339     if uninitialized_variables:\r\n    340         sess = get_session()\r\n--> 341         sess.run(tf.variables_initializer(uninitialized_variables))\r\n    342 \r\n    343 \r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    893     try:\r\n    894       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 895                          run_metadata_ptr)\r\n    896       if run_metadata:\r\n    897         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1122     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n   1123       results = self._do_run(handle, final_targets, final_fetches,\r\n-> 1124                              feed_dict_tensor, options, run_metadata)\r\n   1125     else:\r\n   1126       results = []\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1319     if handle is None:\r\n   1320       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\r\n-> 1321                            options, run_metadata)\r\n   1322     else:\r\n   1323       return self._do_call(_prun_fn, self._session, handle, feeds, fetches)\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _do_call(self, fn, *args)\r\n   1338         except KeyError:\r\n   1339           pass\r\n-> 1340       raise type(e)(node_def, op, message)\r\n   1341 \r\n   1342   def _extend_graph(self):\r\n\r\nInvalidArgumentError: Cannot parse tensor from proto: dtype: DT_FLOAT\r\ntensor_shape {\r\n  dim {\r\n    size: 200000000\r\n  }\r\n  dim {\r\n    size: 1024\r\n  }\r\n}\r\nfloat_val: 0\r\n\r\n\t [[Node: training_6/SGD/Const_8 = Const[dtype=DT_FLOAT, value=<Invalid TensorProto: dtype: DT_FLOAT tensor_shape { dim { size: 200000000 } dim { size: 1024 } } float_val: 0>, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n\r\nCaused by op 'training_6/SGD/Const_8', defined at:\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\r\n    app.launch_new_instance()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\r\n    app.start()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\r\n    ioloop.IOLoop.instance().start()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\r\n    super(ZMQIOLoop, self).start()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\r\n    handler_func(fd_obj, events)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\r\n    self._handle_recv()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\r\n    self._run_callback(callback, msg)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\r\n    callback(*args, **kwargs)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\r\n    return self.dispatch_shell(stream, msg)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\r\n    handler(stream, idents, msg)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\r\n    user_expressions, allow_stdin)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-45-fcfc7f69b24a>\", line 22, in <module>\r\n    callbacks=callbacks)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1575, in fit\r\n    self._make_train_function()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 960, in _make_train_function\r\n    loss=self.total_loss)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\", line 87, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py\", line 165, in get_updates\r\n    moments = [K.zeros(shape) for shape in shapes]\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py\", line 165, in <listcomp>\r\n    moments = [K.zeros(shape) for shape in shapes]\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 625, in zeros\r\n    return variable(tf.constant_initializer(0., dtype=tf_dtype)(shape),\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py\", line 203, in __call__\r\n    verify_shape=verify_shape)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 106, in constant\r\n    attrs={\"value\": tensor_value, \"dtype\": dtype_value}, name=name).outputs[0]\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2630, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1204, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): Cannot parse tensor from proto: dtype: DT_FLOAT\r\ntensor_shape {\r\n  dim {\r\n    size: 200000000\r\n  }\r\n  dim {\r\n    size: 1024\r\n  }\r\n}\r\nfloat_val: 0\r\n\r\n\t [[Node: training_6/SGD/Const_8 = Const[dtype=DT_FLOAT, value=<Invalid TensorProto: dtype: DT_FLOAT tensor_shape { dim { size: 200000000 } dim { size: 1024 } } float_val: 0>, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n", "You can find some solutions here\r\nhttps://stackoverflow.com/questions/46941747/tensorflow-error-cannot-parse-tensor-from-proto", "SAME PROBLEM\r\n```\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\nd:\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _do_call(self, fn, *args)\r\n   1322     try:\r\n-> 1323       return fn(*args)\r\n   1324     except errors.OpError as e:\r\n\r\nd:\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1301                                    feed_dict, fetch_list, target_list,\r\n-> 1302                                    status, run_metadata)\r\n   1303 \r\n\r\nd:\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)\r\n    472             compat.as_text(c_api.TF_Message(self.status.status)),\r\n--> 473             c_api.TF_GetCode(self.status.status))\r\n    474     # Delete the underlying status object from memory otherwise it stays alive\r\n\r\nInvalidArgumentError: Cannot parse tensor from proto: dtype: DT_FLOAT\r\ntensor_shape {\r\n  dim {\r\n    size: 35\r\n  }\r\n  dim {\r\n    size: 165\r\n  }\r\n  dim {\r\n    size: 138601\r\n  }\r\n  dim {\r\n    size: 277203\r\n  }\r\n}\r\nfloat_val: 0\r\n\r\n\t [[Node: Variable_6/ADAM_OPTIMIZER/Initializer/zeros = Const[_class=[\"loc:@Variable_6\"], dtype=DT_FLOAT, value=<Invalid TensorProto: dtype: DT_FLOAT tensor_shape { dim { size: 35 } dim { size: 165 } dim { size: 138601 } dim { size: 277203 } } float_val: 0>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n\r\n```"]}, {"number": 6970, "title": "RNN weights aren't being restored on saver.restore calls", "body": "### Environment info\r\n##Operating System:##\r\nUbuntu 16.04\r\n##Installed version of CUDA and cuDNN: ##\r\nCuda 8.0,  CuDNN 5.1 \r\n##tensorflow version##\r\n0.12\r\n\r\n### What other attempted solutions have you tried?\r\nNot create the graph before using the model. \r\n\r\n### Logs or other output that would be helpful\r\n[SO question](http://stackoverflow.com/questions/41737918/word-embeddings-change-between-sessions-even-if-saved)", "comments": ["Please avoid crossposting from stackoverflow. We try to keep the github issues list focused on bugs and feature requests and all other questions are better left on stackoverflow.\r\n\r\nTaking a cursory look though, it's hard to determine whether this is a bug or not since it depends on what is happening in `self._maybe_initialize()` etc., which might be reinitializing the embeddings on each run. If you do really suspect a bug, please do file an issue with all the information asked for in the \"New Issue\" template, including a way to reproduce the error, ideally with as little code beyond what is necessary to reproduce it.\r\n\r\nApologies if I'm coming across as a bit terse, that's not the intention. If you do suspect a bug, please do feel free to recreate an issue with instructions to reproduce it. Thanks!"]}, {"number": 6969, "title": "Go: Unable to import bool tensors in Go", "body": "Hi Tensorflow/Go team,\r\n\r\nFollowing python code writes a graph with a bool tensor \"z\", which fails to import from a Go code.\r\n```\r\n\r\nimport tensorflow as tf\r\nz = tf.equal(0, 0, name=\"z\")\r\nsess = tf.Session()\r\nprint sess.run(z)\r\ntf.train.write_graph(sess.graph_def, \"/tmp/load\", \"boolTest.pb\", False)\r\n```\r\n\r\nAnd here is the Go code\r\n```\r\n\r\npackage bugs\r\n\r\nimport (\r\n\t\"fmt\"\r\n\ttf \"github.com/tensorflow/tensorflow/tensorflow/go\"\r\n\t\"io/ioutil\"\r\n\t\"testing\"\r\n)\r\n\r\nfunc TestBoolTest(t *testing.T) {\r\n\tmodel, err := ioutil.ReadFile(\"/tmp/load/boolTest.pb\")\r\n\tif err != nil {\r\n\t\tt.Fatal(err)\r\n\t}\r\n\tgraph := tf.NewGraph()\r\n\tif err := graph.Import(model, \"\"); err != nil {\r\n\t\tt.Fatal(err)\r\n\t}\r\n\r\n\tz := graph.Operation(\"z\").Output(0)\r\n\tsess, err := tf.NewSession(graph, nil)\r\n\tif err != nil {\r\n\t\tt.Fatal(err)\r\n\t}\r\n\tdefer sess.Close()\r\n\r\n\tzOut, err := sess.Run(nil, []tf.Output{z}, nil)\r\n\tif err != nil {\r\n\t\tt.Fatal(err)\r\n\t}\r\n\r\n\tfmt.Println(zOut[0].Value().(bool))\r\n}\r\n\r\n```\r\nReturns error:\r\n\r\n=== RUN   TestBoolTest\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n--- FAIL: TestBoolTest (0.04s)\r\npanic: BUG: Please report at https://github.com/tensorflow/tensorflow/issues with the note: Go TensorFlow 0.12.head: unable to decode Tensor of type 10 and shape [] - unsupported type bool [recovered]\r\n        panic: BUG: Please report at https://github.com/tensorflow/tensorflow/issues with the note: Go TensorFlow 0.12.head: unable to decode Tensor of type 10 and shape [] - unsupported type bool\r\n\r\ngoroutine 5 [running]:\r\npanic(0x5087c0, 0xc420012720)\r\n        /usr/lib/golang/src/runtime/panic.go:500 +0x1a1\r\ntesting.tRunner.func1(0xc4200b8180)\r\n        /usr/lib/golang/src/testing/testing.go:579 +0x25d\r\npanic(0x5087c0, 0xc420012720)\r\n        /usr/lib/golang/src/runtime/panic.go:458 +0x243\r\ngithub.com/tensorflow/tensorflow/tensorflow/go.(*Tensor).Value(0xc420010300, 0x0, 0xc420065f38)\r\n        /home/sdeoras/go/src/github.com/tensorflow/tensorflow/tensorflow/go/tensor.go:176 +0x411\r\nbitbucket.hgst.com/x/tensorflow.git/bugs.TestBoolTest(0xc4200b8180)\r\n        /home/sdeoras/go/src/bitbucket.hgst.com/x/tensorflow.git/bugs/boolTest_test.go:32 +0x25c\r\ntesting.tRunner(0xc4200b8180, 0x53b728)\r\n        /usr/lib/golang/src/testing/testing.go:610 +0x81\r\ncreated by testing.(*T).Run\r\n        /usr/lib/golang/src/testing/testing.go:646 +0x2ec\r\nexit status 2\r\nFAIL    bitbucket.hgst.com/x/tensorflow.git/bugs        0.143s\r\n", "comments": ["This was a silly oversight, will have this fixed shortly.", "Thank you @asimshankar for your support.", "Thank you all. It is working!"]}, {"number": 6968, "title": "Error in `python': double free or corruption (!prev)", "body": "I'm consistently getting this error when stopping training (CTRL+C) on version built from head on Jan17. On other hand, running on version from Jan5 head does not exhibit this behavior\r\n\r\ntf.__git__version = '0.12.1-1934-g27fca7d-dirty'\r\n\r\n```\r\nsession.run completed in 0.01 sec with .0.500000 acc\r\nsession.run completed in 0.02 sec with .0.000000 acc\r\n^CTraceback (most recent call last):\r\n  File \"train.py\", line 247, in <module>\r\n    a,_ = sess.run([train_acc,optimizer], feed_dict)\r\n  File \"/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 767, in run\r\n    run_metadata_ptr)\r\n  File \"/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 965, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1015, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1022, in _do_call\r\n    return fn(*args)\r\n  File \"/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1004, in _run_fn\r\n    status, run_metadata)\r\nKeyboardInterrupt\r\n*** Error in `python': double free or corruption (!prev): 0x00000000016c55d0 ***\r\nAborted (core dumped)\r\n```\r\n\r\nLooking at core, it looks like dictionary deletion.\r\n\r\n```\r\n#0  0x00007fe9cbf8a01f in _int_free (av=0x7fe9cc2c9760 <main_arena>, p=<optimized out>, have_lock=0) at malloc.c:3996\r\n#1  0x00007fe9cceb500a in dict_dealloc (mp=0x7fe9558073c8) at Objects/dictobject.c:1596\r\n#2  0x00007fe9cced121f in subtype_dealloc (self=0x7fe95580a080) at Objects/typeobject.c:1193\r\n#3  0x00007fe9cceb023f in free_keys_object (keys=0x24f9620) at Objects/dictobject.c:354\r\n#4  0x00007fe9cced3936 in type_clear (type=0x24f9c68) at Objects/typeobject.c:3270\r\n#5  0x00007fe9ccf8a97c in delete_garbage (old=<optimized out>, collectable=<optimized out>) at Modules/gcmodule.c:866\r\n#6  collect (generation=2, n_collected=0x0, n_uncollectable=0x0, nofail=1) at Modules/gcmodule.c:1014\r\n#7  0x00007fe9ccf8aedd in _PyGC_CollectNoFail () at Modules/gcmodule.c:1605\r\n#8  0x00007fe9ccf5e6d5 in PyImport_Cleanup () at Python/import.c:428\r\n#9  0x00007fe9ccf6a90e in Py_Finalize () at Python/pylifecycle.c:576\r\n#10 0x00007fe9ccf891b9 in Py_Main (argc=<optimized out>, argv=<optimized out>) at Modules/main.c:789\r\n#11 0x0000000000400add in main (argc=2, argv=0x7ffde1cf3f98) at ./Programs/python.c:65\r\n```\r\n", "comments": ["I suspect this error is connected to jemalloc since that got added recently. Turning on tcmalloc through `export LD_PRELOAD=\"/usr/lib/libtcmalloc.so.4\"` gets rid of the error", "It's possible that the malloc changes are responsible... @jhseu would know best, since he made those changes.", "I haven't been able to reproduce it yet. Do you happen to have a script that I can use?\r\n\r\nIt seems unlikely to be related to the jemalloc change. We don't (and technically can't) override Python's malloc/free. My guess is that setting tcmalloc is just hiding the error and that the issue is in the script itself.\r\n\r\nIf you have time, mind disabling jemalloc in ./configure and trying again?", "I think I'll close this for now as unreproducible and reopen when someone can provide more info", "Not sure if this is related but I am unable to build with jemalloc on a cluster with gcc 4.8.2 and an older version of libc.so.6 (old enough that I have to build tensor flow from source). The build fails with a message:\r\n\r\nERROR:~/.cache/bazel/_bazel_/e924d9c3ba75314415252c6f4f93bb86/external/jemalloc/BUILD:10:1: C++ compilation of rule '@jemalloc//:jemalloc' failed: gcc failed: error executing command /opt/apps/compilers/gcc/4.8.2/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/opt/apps/compilers/gcc/4.8.2/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object ... (remaining 38 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\n\r\nDisabling jemalloc at the configure stage means I can finish the tensor flow build and install. I can then train my models (i.e. model.fit()) on the cluster but when I try to  run model.predict_prob() I am getting this error:\r\n*** glibc detected *** python: double free or corruption (!prev): 0x00000000013fcda0 ***\r\n======= Backtrace: =========\r\n/lib64/libc.so.6[0x3e22e75e66]\r\n/lib64/libc.so.6[0x3e22e789b3]\r\n/lib64/ld-linux-x86-64.so.2(_dl_deallocate_tls+0x67)[0x3e226112f7]\r\n/lib64/libpthread.so.0[0x3e2320675d]\r\n/lib64/libpthread.so.0[0x3e232078ea]\r\n/lib64/libpthread.so.0(pthread_join+0xd4)[0x3e232081f4]\r\n/opt/apps/compilers/gcc/4.8.2/lib64/libstdc++.so.6(_ZNSt6thread4joinEv+0x27)[0x7f657b6263c7]\r\n/users/k1511981/.pyenv/versions/t2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so(+0x2567e60)[0x7f657dde2e60]\r\n/users/k1511981/.pyenv/versions/t2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so(_ZN10tensorflow6thread10ThreadPool4ImplD0Ev+0xb3)[0x7f657ddbf5b3]\r\n/users/k1511981/.pyenv/versions/t2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so(_ZN10tensorflow6thread10ThreadPoolD1Ev+0x1a)[0x7f657ddbfc1a]\r\n/users/k1511981/.pyenv/versions/t2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so(_ZN10tensorflow10FileSystem16GetMatchingPathsERKSsPSt6vectorISsSaISsEE+0x592)[0x7f657dddf8a2]\r\n/users/k1511981/.pyenv/versions/t2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so(_ZN10tensorflow3Env16GetMatchingPathsERKSsPSt6vectorISsSaISsEE+0x9b)[0x7f657dddbc1b]\r\n/users/k1511981/.pyenv/versions/t2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so(+0xadc66b)[0x7f657c35766b]\r\n/users/k1511981/.pyenv/versions/t2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so(+0xade610)[0x7f657c359610]\r\n\r\nSo just wondering if there is some conflict with compiler/glibc libraries that is causing these issues?", "Strangely, I'm getting the exact same stack trace for https://github.com/tensorflow/tensorflow/pull/7338 (and it happens even with jemalloc disabled). Pretty hard to track down the root cause...", "Can confirm this. I ran into this on my CI server and the the following fixed it:\r\n\r\n```\r\nsudo apt-get install libtcmalloc-minimal4\r\nexport LD_PRELOAD=\"/usr/lib/libtcmalloc_minimal.so.4\"\r\n```\r\n\r\n", "What Linux distribution and version were you running?\r\n\r\nIn https://github.com/tensorflow/tensorflow/pull/7338, it seemed to be an issue with pypi's numpy on Ubuntu 14.04. Compiling from source fixed it.", "Using Ubuntu 14.04 - https://circleci.com/docs/build-image-trusty/. I was using the tf1.0-rc2 pip package.\r\n\r\n", "So I think this is some interaction between pypi's numpy and Ubuntu 14.04. Either upgrading to Ubuntu 16.04 or building numpy from source (pip install --no-binary) fixes it in every case I've tried (GPU/no GPU, jemalloc disabled/enabled, Python 2.7/3.4/3.5)", "Hi, I got a similar `Error in \"python\": double free or corruption (!prev)` error. Using tf 0.12, built from source, today with Ubuntu 14.04.\r\n\r\nThis was solved with @dennybritz 's fix, four posts above. Thank you very much @dennybritz \r\n\r\nWill try upgrading to 16.04 as @jhseu recommends", "Would it be possible to rebuild the Docker images on gcr.io? The underlying `nvidia/cuda:8.0-cudnn5-devel` uses Ubuntu 16.04 now, but the `gcr.io/tensorflow/tensorflow:1.0.0-rc2-devel-gpu` image still uses Ubuntu 14.04.", "I'm updating all the Docker images, but the gcr.io ones won't be updated until after 1.0. In the meantime, you can start from nvidia/cuda:8.0-cudnn5-devel-ubuntu16.04 and just add `RUN pip install tensorflow_gpu`", "Sorry, I mean the Docker images for the 1.0.0 RCs specifically \u2013 or at least any upcoming ones?\r\n\r\nThis is actually a bit weird to me, because the `nvidia/cuda:8.0-cudnn5-devel` (with no OS suffix) image on Docker Hub has been pointing at Ubuntu 16.04 since at least 24 days ago, which predates all the TensorFlow 1.0.0 RC releases.", "Yeah, Docker caches image versions and we had 14.04 cached. @caisq is planning to upgrade those.", "To narrow down the issue: someone internally noticed that this crash only happens when numpy is installed with OpenBLAS support on Ubuntu 14.04. I haven't tested whether upgrading libopenblas fixes it.\r\n\r\nSo, if you're on Ubuntu, the workaround is to make sure you don't have libopenblas-dev installed and `pip install --no-binary=:all: numpy`\r\n\r\nIf someone encounters this bug and has time to test out newer versions of libopenblas-dev, that'd be useful.", "I found this problem when using 1.0.0 docker image `gcr.io/tensorflow/tensorflow:1.0.0-devel-py3`. Is this the source of the problem? Do I need to do anything to my containers/images/dockerfile to remove this issue?", "The nightly docker images are on Ubuntu 16.04, which should make the problem go away.  For now, I'd recommend using the nightly.\r\n\r\nThe official builds will be on Ubuntu 16.04 when TensorFlow 1.1 is out.", "nightly docker?", "@brando90: Nightly TF docker images are pushed to Docker Hub. Example command line to use them:\r\ndocker run -it --rm tensorflow/tensorflow:nightly /bin/bash\r\nnvidia-docker run -it --rm tensorflow/tensorflow:nightly-gpu /bin/bash\r\nnvidia-docker run -it --rm tensorflow/tensorflow:nightly-gpu-py3 /bin/bash", "ah ok, thanks. I will try nightly.\r\n\r\nWhen is 1.1 out?", "Sometime in March", "I am getting segmentation fault at exit on Ubuntu 14.04, Tensorflow 1.0.0 from pip. Using tcmalloc makes things worse: it crashes on session creation.", "@crimsonlander stack traces might be useful, as well as possible ideas of what could be special about your configuration, since I've used tcmalloc with TensorFlow from 0.9 on Ubuntu 14.04 with no crashes", "@yaroslavvb I had the issue with LD_PRELOAD=\"/usr/lib/libtcmalloc_minimal.so.4\". Setting it to LD_PRELOAD=\"/usr/lib/libtcmalloc.so.4\" resolved the issue. Both from standard 14.04 repository.", "@crimsonlander Try the numpy/disabling OpenBLAS workaround above? Upgrading to Ubuntu 16.04 is the most reliable fix, though.", "@jhseu is internal testing done on 16.04 now? We've been holding off on upgrading from 14.04 partly because TF testing was on 14.04 ", "@yaroslavvb Yep, Jenkins and all our Docker images use 16.04 now.", "I have met the same problem when I ran the tutorial python script in mnist : fully_connected_feed.py...\r\nAfter executing 2 episodes it stopped and printed \"*** Error in 'usr/bin/python3.4': double free or corruption(!prev) 0x000000000242efa0***\"\r\nThen I ran it again ( python fully_connected_feed.py), it still stopped after 2 episodes and print \"Error in '/usr/bin/python3.4':invalid pointer : 0x0000000002960900***\"\r\nDoes it matter? and how to fixed it? What should I do?", "As of march 9, i get this error testing on im2txt on all docker images up to 1.0.1, but the error is not in latest-devel", "I am facing this issue regardless of Ubuntu version used (Xenial vs Trusty) or Docker image (Devel or Nightly or 1.0). If numpy is installed via apt-get python-numpy or gets installed while installing say OpenCV . A simple\r\n`import numpy \r\nimport tensorflow` \r\nis sometimes sufficient to cause a segmentation fault. I also think that this happened due to changes in the upstream Dockerfile, rather than tensorflow itself. since even when I had pinned dockerfile to a specific version of Tensorflow it stopped working few weeks ago. ", "@AKSHAYUBHAT Your error is unrelated.\r\n\r\nLooking at the stack trace, it's pulling a symbol from torch when it shouldn't be:\r\n`/usr/local/lib/python2.7/dist-packages/torch/lib/libshm.so(_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l+0x1c5)[0x7f3edd107235]`\r\n\r\nNo guesses as to why that's happening without understanding your environment.", "A quick glance at pytorch code: it's exporting some libstdc++ symbols with RTLD_GLOBAL when it shouldn't be. The bug is likely in pytorch.", "@jhseu Thanks a lot! \r\n", "I checked and it started working after removing pytorch, so pytorch was the likely culprit. ", "I got the same error when I just import cv2, I installed opencv3 after installing opencv2\r\n\r\nand this caused the below error:\r\n\r\nThe code that causes this error is just\r\nimport cv2\r\nprint m\r\n\r\n\r\n```\r\nNameError: name 'm' is not defined\r\n*** Error in `python': double free or corruption (out): 0x0000000000d74fd0 ***\r\n======= Backtrace: =========\r\n/lib/x86_64-linux-gnu/libc.so.6(+0x777e5)[0x7fdecbba67e5]\r\n/lib/x86_64-linux-gnu/libc.so.6(+0x7fe0a)[0x7fdecbbaee0a]\r\n/lib/x86_64-linux-gnu/libc.so.6(cfree+0x4c)[0x7fdecbbb298c]\r\n/usr/lib/x86_64-linux-gnu/libprotobuf.so.9(_ZN6google8protobuf8internal28DestroyDefaultRepeatedFieldsEv+0x1f)[0x7fdebc49c8af]\r\n/usr/lib/x86_64-linux-gnu/libprotobuf.so.9(_ZN6google8protobuf23ShutdownProtobufLibraryEv+0x8b)[0x7fdebc49bb3b]\r\n/usr/lib/x86_64-linux-gnu/libmirprotobuf.so.3(+0x20329)[0x7fde6fd4f329]\r\n/lib64/ld-linux-x86-64.so.2(+0x10c17)[0x7fdecc125c17]\r\n/lib/x86_64-linux-gnu/libc.so.6(+0x39ff8)[0x7fdecbb68ff8]\r\n/lib/x86_64-linux-gnu/libc.so.6(+0x3a045)[0x7fdecbb69045]\r\n/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf7)[0x7fdecbb4f837]\r\npython(_start+0x29)[0x49d9d9]\r\n\r\n```", "For people who run into this type of issue on Arch Linux:\r\n\r\n`*** glibc detected *** python: double free or corruption (!prev): 0x00000000013fcda0 ***\r\n======= Backtrace: =========\r\n/lib64/libc.so.6[0x3e22e75e66]\r\n/lib64/libc.so.6[0x3e22e789b3]\r\n/lib64/ld-linux-x86-64.so.2(_dl_deallocate_tls+0x67)[0x3e226112f7]\r\n/lib64/libpthread.so.0[0x3e2320675d]\r\n/lib64/libpthread.so.0[0x3e232078ea]\r\n/lib64/libpthread.so.0(pthread_join+0xd4)[0x3e232081f4] `\r\n..... and so on\r\n\r\nInstall the gperftools package\r\n\r\n`sudo pacman -S gperftools` \r\n\r\nIt will most likely solve the issue.", "I got this same issue trying to run real data benchmark with Horovod on TF 1.4.0rc1, with Open MPI OpenIB transport (which installs memory hooks). TCP transport is unaffected.\r\n\r\n```\r\nGenerating model                                                                                                                                                 [992/9693]\r\n*** Error in `python': double free or corruption (!prev): 0x0000000001f721f0 ***\r\n[opusgpu39-wbu2:32804] *** Process received signal ***\r\n[opusgpu39-wbu2:32804] Signal: Aborted (6)\r\n[opusgpu39-wbu2:32804] Signal code:  (-6)\r\n[opusgpu39-wbu2:32804] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0xf890)[0x7f0c06bba890]\r\n[opusgpu39-wbu2:32804] [ 1] /lib/x86_64-linux-gnu/libc.so.6(gsignal+0x37)[0x7f0c05f12067]\r\n[opusgpu39-wbu2:32804] [ 2] /lib/x86_64-linux-gnu/libc.so.6(abort+0x148)[0x7f0c05f13448]\r\n[opusgpu39-wbu2:32804] [ 3] /lib/x86_64-linux-gnu/libc.so.6(+0x731b4)[0x7f0c05f501b4]\r\n[opusgpu39-wbu2:32804] [ 4] /lib/x86_64-linux-gnu/libc.so.6(+0x7898e)[0x7f0c05f5598e]\r\n[opusgpu39-wbu2:32804] [ 5] /lib/x86_64-linux-gnu/libc.so.6(+0x79696)[0x7f0c05f56696]\r\n[opusgpu39-wbu2:32804] [ 6] /lib64/ld-linux-x86-64.so.2(_dl_deallocate_tls+0x58)[0x7f0c06dd9958]\r\n[opusgpu39-wbu2:32804] [ 7] /lib/x86_64-linux-gnu/libpthread.so.0(+0x7107)[0x7f0c06bb2107]\r\n[opusgpu39-wbu2:32804] [ 8] /lib/x86_64-linux-gnu/libpthread.so.0(+0x721f)[0x7f0c06bb221f]\r\n[opusgpu39-wbu2:32804] [ 9] /lib/x86_64-linux-gnu/libpthread.so.0(pthread_join+0xe4)[0x7f0c06bb44d4]\r\n[opusgpu39-wbu2:32804] [10] /usr/lib/x86_64-linux-gnu/libstdc++.so.6(_ZNSt6thread4joinEv+0x27)[0x7f0b69baa837]\r\n[opusgpu39-wbu2:32804] [11] /home/asergeev/mpi/venv-nccl/local/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so(+0x5131d0)[0x7f0b708b61d0]\r\n[opusgpu39-wbu2:32804] [12] /home/asergeev/mpi/venv-nccl/local/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN10tensorflow6thread10ThreadP$\r\nol4ImplD0Ev+0xbb)[0x7f0b7088d43b]\r\n[opusgpu39-wbu2:32804] [13] /home/asergeev/mpi/venv-nccl/local/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN10tensorflow6thread10ThreadP$\r\nolD1Ev+0x1a)[0x7f0b7088d73a]\r\n[opusgpu39-wbu2:32804] [14] /home/asergeev/mpi/venv-nccl/local/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN10tensorflow10FileSystem16Ge$\r\nMatchingPathsERKSsPSt6vectorISsSaISsEE+0x56c)[0x7f0b708b2aec]\r\n[opusgpu39-wbu2:32804] [15] /home/asergeev/mpi/venv-nccl/local/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN10tensorflow3Env16GetMatchin$\r\nPathsERKSsPSt6vectorISsSaISsEE+0xa3)[0x7f0b708aca43]\r\n[opusgpu39-wbu2:32804] [16] /home/asergeev/mpi/venv-nccl/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so(_Z16GetMatchingFilesRKSsP9TF_S$\r\natus+0x4b)[0x7f0b7227090b]\r\n[opusgpu39-wbu2:32804] [17] /home/asergeev/mpi/venv-nccl/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so(+0x1080604)[0x7f0b72273604]\r\n[opusgpu39-wbu2:32804] [18] python(PyEval_EvalFrameEx+0x614)[0x4cddf4]\r\n[opusgpu39-wbu2:32804] [19] python(PyEval_EvalCodeEx+0x401)[0x4cc4f1]\r\n[opusgpu39-wbu2:32804] [20] python(PyEval_EvalFrameEx+0x6500)[0x4d3ce0]\r\n[opusgpu39-wbu2:32804] [21] python(PyEval_EvalCodeEx+0x401)[0x4cc4f1]\r\n[opusgpu39-wbu2:32804] [22] python(PyEval_EvalFrameEx+0x5e0a)[0x4d35ea]\r\n[opusgpu39-wbu2:32804] [23] python(PyEval_EvalCodeEx+0x401)[0x4cc4f1]\r\n[opusgpu39-wbu2:32804] [24] python(PyEval_EvalFrameEx+0x5e0a)[0x4d35ea]\r\n[opusgpu39-wbu2:32804] [25] python(PyEval_EvalCodeEx+0x401)[0x4cc4f1]\r\n[opusgpu39-wbu2:32804] [26] python(PyEval_EvalFrameEx+0x6500)[0x4d3ce0]\r\n[opusgpu39-wbu2:32804] [27] python(PyEval_EvalCodeEx+0x401)[0x4cc4f1]\r\n[opusgpu39-wbu2:32804] [28] python(PyEval_EvalFrameEx+0x6500)[0x4d3ce0]\r\n[opusgpu39-wbu2:32804] [29] python(PyEval_EvalCodeEx+0x401)[0x4cc4f1]\r\n[opusgpu39-wbu2:32804] *** End of error message ***\r\n```\r\n\r\nI was able to make it work by adding `-x LD_PRELOAD=/usr/local/lib/libtcmalloc.so.4.4.5`.\r\n\r\nSeems other folks are still hitting this issue in other use cases, too.  Any ideas or plans for the fix?", "We still don't think there's a bug in TensorFlow here. Any Python module that messes with memory allocation can cause this, so perhaps trying importing those last?", "I ran into this and other \"Error in `python'\" issues using the module mayavi.mlab with TensorFlow when creating multiple mlab figures. @jhseu 's suggestion to import the module after TF fixed it instantly. Thank you.", "I don't know why I send the command:\r\nbazel-bin/im2txt/train --input_file_pattern=\"${MSCOCO_DIR}/train-?????-of-00256\" --inception_checkpoint_file=\"${INCEPTION_CHECKPOINT}\" --train_dir=\"${MODEL_DIR}/train\" --train_inception=false --number_of_steps=1000000\r\n\r\nthere have a error:\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:126] Couldn't open CUDA library libcufft.so.8.0. LD_LIBRARY_PATH:\r\nI tensorflow/stream_executor/cuda/cuda_fft.cc:344] Unable to load cuFFT DSO.\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\n*** Error in `/usr/bin/python': double free or corruption (!prev): 0x000000000231f8e0 ***\r\nI don't know the error", "I was facing the same issue and later I found that I wrote a Destructor of the class which was clearing the memory but I had already cleared it in another function which i was calling before the Destructor and then repeated the same which caused same error when I removed the Destructor  Problem Solved. (I was loading my preTrained Model) ", "I am experiencing this issue while restoring partial weights/biases thru the method: tensorflow.contrib.framework.python.ops.assign_from_checkpoint. Currently working on CUDA 8/9, tf r1.4 w/ native source build, and horovod 0.11.2. I am also planning to upgrade tf to very recent version or successor (r1.5), and then I'll leave some progress about error.\r\n\r\nBelow is an example of process backtrace. (automatically generated)\r\n[process_dead_backtrace.txt](https://github.com/tensorflow/tensorflow/files/1882253/process_dead_backtrace.txt)\r\n\r\n", "I used jemalloc instead of tcmalloc and the problem was also solved on CentOS 6. \r\nJust install jemalloc and run with LD_PRELOAD=/usr/local/lib/libjemalloc.so\r\n (follow this post: https://zapier.com/engineering/celery-python-jemalloc/)", "> Can confirm this. I ran into this on my CI server and the the following fixed it:\r\n> \r\n> ```\r\n> sudo apt-get install libtcmalloc-minimal4\r\n> export LD_PRELOAD=\"/usr/lib/libtcmalloc_minimal.so.4\"\r\n> ```\r\n\r\nI am at Ubuntu16.04 machine. After following the above steps, it still gave the same error.", "I use pip install --no-binary=:all: --force-reinstall numpy,\r\nsolve the problem", "> > Can confirm this. I ran into this on my CI server and the the following fixed it:\r\n> > ```\r\n> > sudo apt-get install libtcmalloc-minimal4\r\n> > export LD_PRELOAD=\"/usr/lib/libtcmalloc_minimal.so.4\"\r\n> > ```\r\n> \r\n> I am at Ubuntu16.04 machine. After following the above steps, it still gave the same error.\r\n\r\nyes,me too", "> I use pip install --no-binary=:all: --force-reinstall numpy,\r\n> solve the problem\r\n\r\nThis fixed the issue for me when running TF Object Detection. Numpy was updated to 1.17.3 from 1.14.6."]}, {"number": 6967, "title": "Build from sources, build issues", "body": "Hi, everyone.\r\nI'm trying to build tensorflow from sources as https://www.tensorflow.org/get_started/os_setup#installing_from_sources illustrated.\r\nhowever, when I came to the step bazel build -c opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\nit followed with \"[2,302 / 3,143] Still waiting for 200 jobs to complete: \" with more than 20 hours.\r\nhow long does it need to finish the job.\r\nthanks.\r\n\r\nOperating System:\r\nCentOS6\r\nno GPU\r\nThe commit hash: 49bab39b2a1be64879ba95515ab6130b7b951be0\r\nbazel version: Build label: 0.4.3\r\n\r\nLogs\r\n```\r\n[2,302 / 3,143] Still waiting for 200 jobs to complete:\r\n      Running (standalone):\r\n        Compiling tensorflow/core/kernels/svd_op_complex64.cc, 77639 s\r\n        Compiling tensorflow/core/kernels/svd_op_complex128.cc, 77060 s\r\n        Compiling tensorflow/core/kernels/svd_op_double.cc, 25386 s\r\n      Scheduling:\r\n        Linking tensorflow/python/libpython_op_gen_main.a [for host], 80391 s\r\n        Linking tensorflow/contrib/cudnn_rnn/libcudnn_rnn_ops_op_lib.lo [for host], 80308 s\r\n        Linking tensorflow/core/libnn_ops_op_lib.lo [for host], 80308 s\r\n        Linking tensorflow/contrib/layers/libbucketization_op_op_lib.lo [for host], 80308 s\r\n        Linking tensorflow/contrib/tensor_forest/libtensor_forest_ops_op_lib.lo [for host], 80308 s\r\n        Linking tensorflow/contrib/layers/libsparse_feature_cross_op_op_lib.lo [for host], 80307 s\r\n        Linking tensorflow/core/libresource_variable_ops_op_lib.lo [for host], 80306 s\r\n        Linking tensorflow/core/libcontrol_flow_ops_op_lib.lo [for host], 80287 s\r\n        Linking tensorflow/core/libuser_ops_op_lib.lo [for host], 80287 s\r\n        ... 188 more jobs\r\n\r\n\r\nInformation with \"top\"\r\ntop - 09:34:57 up 1 day,  4:49,  4 users,  load average: 5.17, 6.59, 6.21\r\nTasks: 219 total,   1 running, 217 sleeping,   0 stopped,   1 zombie\r\nCpu(s):  0.2%us,  0.4%sy,  0.0%ni, 50.1%id, 49.3%wa,  0.0%hi,  0.0%si,  0.0%st\r\nMem:   1004140k total,   939776k used,    64364k free,      576k buffers\r\nSwap:  4046844k total,  2836996k used,  1209848k free,    27784k cached\r\n\r\n   PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND                                                                                                                                                                      \r\n 29135 root      20   0  694m  11m 1884 D  1.0  1.2   0:04.05 yumBackend.py                                                                                                                                                                 \r\n 26945 root      20   0  898m 149m 1636 D  0.7 15.3   4:19.76 cc1plus                                                                                                                                                                       \r\n 28266 root      20   0  768m 127m 1540 D  0.7 13.0   1:38.66 cc1plus                                                                                                                                                                       \r\n    20 root      20   0     0    0    0 S  0.3  0.0   0:37.56 events/1                                                                                                                                                                      \r\n    48 root      20   0     0    0    0 S  0.3  0.0   1:53.14 kblockd/2                                                                                                                                                                     \r\n    57 root      20   0     0    0    0 S  0.3  0.0   0:05.53 ata_sff/3                                                                                                                                                                     \r\n    73 root      20   0     0    0    0 S  0.3  0.0   3:46.86 kswapd0                                                                                                                                                                       \r\n 26939 root      20   0  922m 150m 1940 D  0.3 15.4   4:26.14 cc1plus                                                                                                                                                                       \r\n 29203 root      20   0 15148 1348  964 R  0.3  0.1   0:00.02 top          \r\n```\r\n\r\n\r\nEnglish poor, thanks.", "comments": ["It certainly should not take 20 hours to compile. From your `top` output, it doesn't seem like tis chugging on CPU either. What architecture are you running this on? Which version of gcc? If you cancel the build and retry, does it still get stuck?", "Closing due to lack of recent activity. Please update the issue if it persists and we will reopen."]}, {"number": 6966, "title": "Fix cudnn file name", "body": "", "comments": ["@tensorflow-jenkins test this please"]}]