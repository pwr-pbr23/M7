[{"number": 54477, "title": "SavedModel way slower than h5 in loading", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 11\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.8.0\r\n- Python version: 3.7.7\r\n- Bazel version (if compiling from source): 5.0.0\r\n- GCC/Compiler version (if compiling from source): 7.3\r\n- CUDA/cuDNN version: 11.2/8.1\r\n- GPU model and memory: T4\r\n\r\n\r\nI have found some issues about this, but I didn't find any solution/explanation about it. Loading a SavedModel is way slower than loading the same model in h5df. Is there any way to speed this up? I find hard to justiy the use SavedModel (even if it is the preferred formar) if it takes 4x-5x longer to load in my code.\r\n\r\n![image](https://user-images.githubusercontent.com/61322372/155090488-110b9206-7b85-4b95-8df1-586bf17de3d5.png)\r\n\r\n", "comments": ["@SergioG-M \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 54476, "title": "What operations are supported in tf.lite.OpsSet.TFLITE_BUILTINS_INT8", "body": "To convert a Keras model to tflite format using post-training quantization, we use `tf.lite.OpsSet.TFLITE_BUILTINS_INT8`. Is there any list of operations that are supported when this flag is set? ", "comments": ["Hi @mrtpk123 ! Can you check this [link](https://www.tensorflow.org/lite/guide/ops_compatibility) for answer?", "HI @mohantym, Thank you for your response. The [link has the list of all operators](https://www.tensorflow.org/mlir/tfl_ops) that are supported by TFlite. I want to know which operations are supported when we use `tf.lite.OpsSet.TFLITE_BUILTINS_INT8` flag.\r\n\r\nHere is the context:\r\nI have a `tf.transpose` in the model and when this model is converted to tflite using `tf.lite.OpsSet.TFLITE_BUILTINS_INT8`  I get an error - `'tf.Transpose' op is neither a custom op nor a flex op`. But, [transpose is a supported](https://www.tensorflow.org/mlir/tfl_ops#tfltranspose_mlirtfltransposeop) operation. When I add in `tf.lite.OpsSet.SELECT_TF_OPS`, I get the model converted to TFlite but it has dequantize and quantize operations before and after transpose layer (which is not expected). Moreover the tf.transpose is converted as `flextranspose`.\r\n\r\n![image](https://user-images.githubusercontent.com/76564442/155113899-f32931b6-0312-4c35-b432-51899df161c3.png)\r\n", "@mrtpk123 ! Can you please share a simple stand alone code which help us expedite the issue?", "Yep. Below is the code:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport pathlib\r\n\r\n# model\r\ninputs = tf.keras.Input(shape=(384, 544, 20))\r\nx = tf.keras.layers.Reshape(target_shape= [384, 544, 5, 2, 2])(inputs)\r\nx = tf.keras.layers.Lambda(lambda x : tf.transpose(x, perm=[0, 1, 2, 4, 5, 3]))(x)\r\nx = tf.keras.layers.Reshape(target_shape= [384, 544, 20])(x)\r\nx = tf.nn.depth_to_space(x, block_size=2)\r\nmodel = tf.keras.Model(inputs=inputs, outputs=x, name=\"model\")\r\npath_tf_model = \"./test.h5\"\r\nmodel.save(path_tf_model)\r\nmodel.summary()\r\n\r\n# convert\r\nlen_dataset = 2\r\ndef representative_data_gen():\r\n    for idx in range(0, len_dataset): # 384, 544, 20\r\n        input_value1 = np.random.rand(1, 384, 544, 20).astype(np.float32) * 255\r\n        input_value1 = input_value1.astype(np.int) - 128\r\n        input_value1 = input_value1.astype(np.float32)    \r\n        yield [input_value1]\r\n\r\nmodel = tf.keras.models.load_model(path_tf_model)\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model) \r\n\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n# converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.OpsSet.SELECT_TF_OPS]\r\nconverter.inference_input_type = tf.int8\r\nconverter.inference_output_type = tf.int8\r\n\r\nconverter.representative_dataset = representative_data_gen\r\n\r\ntflite_model_quant = converter.convert()\r\n\r\ntflite_models_dir = pathlib.Path(\"./\")\r\ntflite_models_dir.mkdir(exist_ok=True, parents=True)\r\n\r\ntflite_model_quant_file = tflite_models_dir/\"test.tflite\"\r\ntflite_model_quant_file.write_bytes(tflite_model_quant)\r\n\r\n```\r\n\r\n![image](https://user-images.githubusercontent.com/76564442/155125609-8c8609fa-3477-44da-ae92-e57064bbfa51.png)\r\n[models_transform_test.zip](https://github.com/tensorflow/tensorflow/files/8116293/models_transform_test.zip)\r\n\r\n\r\n\r\n\r\n", "Hi @chunduriv ! Could you please look at this issue? Attaching gist in [2.7](https://colab.sandbox.google.com/gist/mohantym/d4493e523bd7d0d1e32927fbbb6fcee6/github_54476.ipynb#scrollTo=GRICClPvrAU-), [2.8](https://colab.sandbox.google.com/gist/mohantym/8f32b9689c59e24333ca8c09b63b9b3a/github_54476.ipynb#scrollTo=pf9KClzehTs4) and [nightly](https://colab.sandbox.google.com/gist/mohantym/173ce2d0fdf404491678554c33fd9c48/github_54476.ipynb#scrollTo=pf9KClzehTs4)(2.9.0dev) for reference.", "From https://github.com/tensorflow/tensorflow/issues/53702, can I deduce that inputs with greater than four dimensions are not natively supported for Transpose operation?", "All the OPS in TFLite is not having support for `int8`, you can use the flag  for OPS which supports TFLite quint8 in their input and output [here](https://www.tensorflow.org/mlir/tfl_ops).\r\nTo avoid error, like you have tried you can mention like below \r\n`converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.OpsSet.SELECT_TF_OPS]`\r\nWhich would apply `TFLITE_BUILTINS_INT8` if OP is supported, else it will apply `SELECT_TF_OPS`", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54476\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54476\">No</a>\n"]}, {"number": 54475, "title": "Error message of `tf.nn.gelu` with `uint16` input is misleading ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.7.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nfeatures = tf.zeros([3, 4], dtype=tf.uint16)\r\ntf.nn.gelu(features)\r\n```\r\nThrows `TypeError`\r\n```\r\nTypeError: Cannot convert 0.5 to EagerTensor of dtype uint16\r\n```\r\n**Describe the current behavior**\r\nThe current message is misleading, as it seems to be some computation error. If `tf.nn.gelu` does not accept `uint16` inputs, the message should be a standard message like\r\n```\r\nValue for attr 'T' of uint16 is not in the list of allowed values:\r\n```\r\nSimilar to `tf.nn.crelu`:\r\n```\r\nimport tensorflow as tf\r\nfeatures = tf.zeros([3, 4], dtype=tf.uint16)\r\ntf.nn.crelu(features)\r\n# InvalidArgumentError: Value for attr 'T' of uint16 is not in the list of allowed values: bfloat16, half, float, double, int8, int16, int32, int64, complex64, complex128; NodeDef: {{node Neg}}; Op<name=Neg; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]> [Op:Neg]\r\n```\r\n\r\n\r\n**Describe the expected behavior**\r\n`tf.nn.gelu` should have better error message in this case.", "comments": ["Thank you for reporting , will update the code soon. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54475\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54475\">No</a>\n"]}, {"number": 54473, "title": "Building tensorflowlite.so for aarch_64 failed ", "body": "\r\n\r\n**System information**\r\n- OS Platform and Distribution (Windows 10):\r\n- Mobile device ( for RPI4) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source (cloned the repo)\r\n- TensorFlow version: latest\r\n- Python version: 3.7.9\r\n- Installed using virtualenv? pip? conda?: none\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): g++\r\n- CUDA/cuDNN version: \r\n- GPU model and memory: gtx 1060 6gb\r\n\r\n\r\n\r\n**Describe the problem**\r\nI followed this tutorial for cross compilation of aarch64 \r\nhttps://www.tensorflow.org/lite/guide/build_arm\r\n1- cloned tensorflow repo \r\n2- ran `bazel build --config=elinux_aarch64 -c opt //tensorflow/lite:libtensorflowlite.so`\r\n\r\n\r\n**Any other info / logs**\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=211\r\nINFO: Reading rc options for 'build' from c:\\users\\mohamed_gamal\\downloads\\tensorflow_src\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=C:/Users/Mohamed_Gamal/AppData/Local/Microsoft/WindowsApps/python.exe\r\nINFO: Reading rc options for 'build' from c:\\users\\mohamed_gamal\\downloads\\tensorflow_src\\.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=//tensorflow/tools/toolchains/java:tf_java_toolchain --host_java_toolchain=//tensorflow/tools/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils\r\nINFO: Found applicable config definition build:short_logs in file c:\\users\\mohamed_gamal\\downloads\\tensorflow_src\\.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file c:\\users\\mohamed_gamal\\downloads\\tensorflow_src\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:elinux_aarch64 in file c:\\users\\mohamed_gamal\\downloads\\tensorflow_src\\.bazelrc: --config=elinux --cpu=aarch64 --distinct_host_configuration=true\r\nINFO: Found applicable config definition build:elinux in file c:\\users\\mohamed_gamal\\downloads\\tensorflow_src\\.bazelrc: --crosstool_top=@local_config_embedded_arm//:toolchain --host_crosstool_top=@bazel_tools//tools/cpp:toolchain\r\nINFO: Found applicable config definition build:windows in file c:\\users\\mohamed_gamal\\downloads\\tensorflow_src\\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --features=compiler_param_file --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:monolithic in file c:\\users\\mohamed_gamal\\downloads\\tensorflow_src\\.bazelrc: --define framework_shared_object=false\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/9df1c45bd751eeff53f6811501e015c7eba4b536.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nWARNING: Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nINFO: Repository local_config_embedded_arm instantiated at:\r\n  C:/users/mohamed_gamal/downloads/tensorflow_src/WORKSPACE:15:14: in <toplevel>\r\n  C:/users/mohamed_gamal/downloads/tensorflow_src/tensorflow/workspace2.bzl:868:19: in workspace\r\n  C:/users/mohamed_gamal/downloads/tensorflow_src/tensorflow/workspace2.bzl:118:34: in _tf_toolchains\r\nRepository rule arm_linux_toolchain_configure defined at:\r\n  C:/users/mohamed_gamal/downloads/tensorflow_src/tensorflow/tools/toolchains/embedded/arm-linux/arm_linux_toolchain_configure.bzl:34:48: in <toplevel>\r\nERROR: An error occurred during the fetch of repository 'local_config_embedded_arm':\r\n   Traceback (most recent call last):\r\n        File \"C:/users/mohamed_gamal/downloads/tensorflow_src/tensorflow/tools/toolchains/embedded/arm-linux/arm_linux_toolchain_configure.bzl\", line 23, column 9, in _arm_linux_toolchain_configure_impl\r\n                _tpl(repository_ctx, \"cc_config.bzl\", {\r\n        File \"C:/users/mohamed_gamal/downloads/tensorflow_src/tensorflow/tools/toolchains/embedded/arm-linux/arm_linux_toolchain_configure.bzl\", line 6, column 28, in _tpl\r\n                repository_ctx.template(\r\nError in template: Unable to load package for //tensorflow/tools/toolchains/embedded/arm-linux:cc_config.bzl.tpl: BUILD file not found in any of the following directories. Add a BUILD file to a directory to mark it as a package.\r\n - C:/users/mohamed_gamal/downloads/tensorflow_src/tensorflow/tools/toolchains/embedded/arm-linux\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1596824487 -0400\"\r\nDEBUG: Repository io_bazel_rules_docker instantiated at:\r\n  C:/users/mohamed_gamal/downloads/tensorflow_src/WORKSPACE:23:14: in <toplevel>\r\n  C:/users/mohamed_gamal/downloads/tensorflow_src/tensorflow/workspace0.bzl:110:34: in workspace\r\n  C:/users/mohamed_gamal/_bazel_mohamed_gamal/wmmhak3q/external/bazel_toolchains/repositories/repositories.bzl:35:23: in repositories\r\nRepository rule git_repository defined at:\r\n  C:/users/mohamed_gamal/_bazel_mohamed_gamal/wmmhak3q/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>\r\nERROR: C:/users/mohamed_gamal/downloads/tensorflow_src/tensorflow/lite/BUILD:1158:24: //tensorflow/lite:libtensorflowlite.so depends on @local_config_embedded_arm//:toolchain in repository @local_config_embedded_arm which failed to fetch. no such package '@local_config_embedded_arm//': Unable to load package for //tensorflow/tools/toolchains/embedded/arm-linux:cc_config.bzl.tpl: BUILD file not found in any of the following directories. Add a BUILD file to a directory to mark it as a package.\r\n - C:/users/mohamed_gamal/downloads/tensorflow_src/tensorflow/tools/toolchains/embedded/arm-linux\r\nERROR: Analysis of target '//tensorflow/lite:libtensorflowlite.so' failed; build aborted: Analysis failed\r\nINFO: Elapsed time: 0.724s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)\r\n", "comments": ["Hi @mohamedgamal7 ! Could you try with Bazel 4.2.2/5.0.0 ? if you face any error on  [llvm package](https://github.com/tensorflow/tensorflow/issues/54257#issuecomment-1033387603) ,you can download and install it locally using Cmake . ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54473\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54473\">No</a>\n"]}, {"number": 54472, "title": "No module named 'keras.optimizer'", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@Messiri4 \r\nIn order to expedite the trouble-shooting process here,Could you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose),\r\nThanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "@Messiri4 Please refer this [thread](https://stackoverflow.com/questions/39081910/importerror-no-module-named-keras-optimizers) and let us know if it helps?Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54472\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54472\">No</a>\n"]}, {"number": 54471, "title": "TF1.15 configuartion with CUDA 11.4 and cuDNN 8", "body": "<em>Hi everybody, I am wondering if it's possible to use TF1.15 with CUDA 11.4 and cuDNN 8. The problem that I have is that the migration of the code to TF2 is painful and I have no root privileges on this machine to modify the CUDA drivers.</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 20.04\r\n- TensorFlow installed from pip: version 1.15\r\n- Python version: python 3.7.12 in virtaulenv\r\n- CUDA/cuDNN version: CUDA 11.4 and cuDNN 8\r\n- GPU model and memory: NVIDIA A100-SMX-80GB\r\n\r\n\r\nWhen I run my script TF looks for libraries of CUDA 10 and cuDNN 7, such as libcudart.10.so, libcublas.10.so, etc...\r\n\r\nThank you!", "comments": ["@Wolpes11 ,\r\nWe see that you are using tf version 1.15, 1.x is not actively supported, please update to latest stable  v2.7 or v2.8 from [here](https://www.tensorflow.org/install) and let us know if you are facing same issue.Also please take a look at this [link](https://www.tensorflow.org/install/source#linux) for tested build configurations.It helps.Thanks!", "Thank you for your prompt reply.\r\nI know that tf 1.15 is no longer supported but with version 2.x my code doesn't work and it takes too much to convert the code (and I'm not sure I can obtain the same results).\r\nAccording to this [link](https://docs.nvidia.com/deeplearning/frameworks/tensorflow-release-notes/rel_21-10.html#rel_21-10) there should be a way to configure tf 1.15 with the latest CUDA libraries, even if I can't figure out how.", "@Wolpes11,\r\n\r\nWe have tested in Google Colab with `TF1.15` and CUDA `11.1` and it is working as expected. Please can you install build from source using `bazel`. \r\n\r\nMake sure your branch is - `git checkout TF r1.15` and on `./configure` mention the CUDA version as `11.4` and set the appropriate CUDA and cuDNN path.\r\n\r\nPlease refer this [guide](https://www.tensorflow.org/install/source).Thanks!\r\n ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54471\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54471\">No</a>\n"]}, {"number": 54469, "title": "Tensorflow import error, undefined symbol", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Arch\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Arch community repository\r\n- TensorFlow version: 2.8.0\r\n- Python version: 3.10\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): 11.1.0\r\n- CUDA/cuDNN version: 11.5\r\n- GPU model and memory: Nvidia GTX GeForce 1050ti 4GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nimport tensorflow leads to the following import error:\r\n\r\n```\r\n>>> import tensorflw\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'tensorflw'\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"/home/beany/.local/lib/python3.10/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 60, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: /home/beany/.local/lib/python3.10/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/beany/.local/lib/python3.10/site-packages/tensorflow/__init__.py\", line 37, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"/home/beany/.local/lib/python3.10/site-packages/tensorflow/python/__init__.py\", line 36, in <module>\r\n    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\r\n  File \"/home/beany/.local/lib/python3.10/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 75, in <module>\r\n    raise ImportError(\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/beany/.local/lib/python3.10/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 60, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: /home/beany/.local/lib/python3.10/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n>>> import tensorflow\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@bunbun205,\r\n\r\nFor `Tensorflow 2.8`, can you please try `CUDA` and `cuDNN` versions as shown below \r\n\r\n![image](https://user-images.githubusercontent.com/74177924/154927657-6e9d354d-8363-45a0-b4e8-63f282e89b21.png)\r\n\r\nFor more details, please refer tested build configurations [here](https://github.com/tensorflow/docs/blob/cbaa96238c823b39e4433c843823895cef903343/site/en/install/source.md#gpu).Thanks!\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "> @bunbun205,\r\n> \r\n> For `Tensorflow 2.8`, can you please try `CUDA` and `cuDNN` versions as shown below\r\n> \r\n> ![image](https://user-images.githubusercontent.com/74177924/154927657-6e9d354d-8363-45a0-b4e8-63f282e89b21.png)\r\n> \r\n> For more details, please refer tested build configurations [here](https://github.com/tensorflow/docs/blob/cbaa96238c823b39e4433c843823895cef903343/site/en/install/source.md#gpu).Thanks!\r\n\r\nHow do you \"try\" this?\r\n\r\nEdit: I installed the previous build TensorFlow 2.7.1 and it fixed my issue. Perhaps there's a bug somewhere with the current build for Linux distributions. I've pip installed the same TensorFlow 2.8.0 build on my Windows device and importing tensorflow worked out fine.", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54469\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54469\">No</a>\n"]}, {"number": 54468, "title": "[PluggableDevice] Enable tensor list kernels on Windows", "body": null, "comments": ["Could someone also review this small change? This is all that's needed to enable tensor list kernels on Windows.", "@rohan100jain @gbaned Is it something that could get merged soon-ish? We're targeting this change for 2.9 to enable pluggable devices on Windows.\r\n\r\nThanks!"]}, {"number": 54467, "title": "[TFlite] Different shared library size between bazel and CMake on Android64.", "body": "Hello, \r\nI want to compile android_arm64 shared library and I compile **library1** using\r\n **bazel build -c opt --config=android_arm64 tensorflow/lite/libtensorflowlite.so  --verbose_failures**\r\nin docker which is built by tflite-android.Dockerfile. \r\n\r\nMeanwhile I compile **library2** using CMake command **cmake -DCMAKE_TOOLCHAIN_FILE=/opt/android-ndk-r20b/build/cmake/android.toolchain.cmake -DANDROID_ABI=arm64-v8a -DBUILD_SHARED_LIBS=ON -DCMAKE_SYSTEM_NAME=Android ../tensorflow/lite && make -j4**, CMake version is 3.16.\r\n\r\nWhat bothers me is why library1's size is 3.9M and library2's size is 60M.", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54467\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54467\">No</a>\n"]}, {"number": 54466, "title": "[mhlo]: add select canonicalization pattern", "body": null, "comments": ["Thanks.I will update later.", "It seem like infra tool error.\r\n```\r\n[31m\u001b[1mERROR: \u001b[0mAn error occurred during the fetch of repository 'local_config_rocm':\r\n```"]}, {"number": 54465, "title": "An op outside of the function building code is being passed a \"Graph\" tensor", "body": "I am using tf 2.3.0, and did not explicitly  set the eager mode\u3002\r\n\r\nI defined a memory network layer, and call it from another layer\u3002 It succeeded during the training step. However, I got the following error during the evaluation step\u3002\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/fabriszhou/PycharmProjects/tf-model/test/msmt/test_memory.py\", line 94, in <module>\r\n    mymodel = model.get_train_model()\r\n  File \"/Users/fabriszhou/PycharmProjects/tf-model/models/msmt.py\", line 374, in get_train_model\r\n    long_cluster = self.memory.predict_process([query_emb, long_emb, isActive_vals])  # \u62ff\u5230\u957f\u671f\u7684\u5174\u8da3\u8868\u5f81\r\n  File \"/Users/fabriszhou/PycharmProjects/tf-model/pkgs/tf/extend_layers.py\", line 2499, in predict_process\r\n    read_attention = compute_cosine_similarity(read_query, self.key_memory, temperature=self.temperature)\r\n  File \"/Users/fabriszhou/PycharmProjects/tf-model/pkgs/tf/extend_layers.py\", line 2322, in compute_cosine_similarity\r\n    normed_memory = tf.nn.l2_normalize(memory, axis=1)  # [n_memory, dim]\r\n  File \"/opt/anaconda3/envs/tf23/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/opt/anaconda3/envs/tf23/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py\", line 666, in l2_normalize_v2\r\n    square_sum = math_ops.reduce_sum(math_ops.square(x), axis, keepdims=True)\r\n  File \"/opt/anaconda3/envs/tf23/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 10309, in square\r\n    x, name=name, ctx=_ctx)\r\n  File \"/opt/anaconda3/envs/tf23/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 10347, in square_eager_fallback\r\n    ctx=ctx, name=name)\r\n  File \"/opt/anaconda3/envs/tf23/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\", line 75, in quick_execute\r\n    raise e\r\n  File \"/opt/anaconda3/envs/tf23/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\nTypeError: An op outside of the function building code is being passed\r\na \"Graph\" tensor. It is possible to have Graph tensors\r\nleak out of the function building context by including a\r\ntf.init_scope in your function building code.\r\nFor example, the following function will fail:\r\n  @tf.function\r\n  def has_init_scope():\r\n    my_constant = tf.constant(1.)\r\n    with tf.init_scope():\r\n      added = my_constant * 2\r\nThe graph tensor has name: add:0\r\n\r\n\r\nHere is my source code\u3002\r\n\r\ndef compute_cosine_similarity(query, memory, temperature=0.1):\r\n    \"\"\"\r\n    Compute cosine similarity between query and memory.\r\n\r\n    Args:\r\n        query: (batch_size, embedding_dim)\r\n        memory: (n_memory, embedding_dim)\r\n        temperature: hyper-parameter to control the final probabilities of softmax.\r\n\r\n    Returns:\r\n        Attention in the shape of (batch_size, n_memory)\r\n    \"\"\"\r\n\r\n    normed_query = tf.nn.l2_normalize(query, axis=1)  # [batch, dim]\r\n    query = tf.expand_dims(normed_query, axis=1)  # [batch, 1, dim]\r\n\r\n    normed_memory = tf.nn.l2_normalize(memory, axis=1)  # [n_memory, dim]\r\n    memory = tf.expand_dims(normed_memory, axis=0)  # [1, n_memory, dim]\r\n\r\n    similarity = tf.reduce_sum(tf.multiply(query, memory), axis=2)  # [batch, n_memory]\r\n    similarity = tf.nn.softmax(similarity / temperature, axis=1)  # [batch, n_memory]\r\n\r\n    return similarity\r\n\r\nclass MemoryLayer(tf.keras.layers.Layer):\r\n    def __init__(self, controller_layers, controller_hidden_act='relu', controller_output_act=None, n_clusters=100, key_dims=8, long_dims=8, short_dims=8, temperature=0.1, alpha=0.1, is_short=False, name=\"memory_layer\", **kwargs):\r\n        super(MemoryLayer, self).__init__(name=name, trainable=True, **kwargs)\r\n        self.controller_layers = controller_layers\r\n        self.controller_hidden_act = controller_hidden_act\r\n        self.controller_output_act = controller_output_act\r\n        self.n_clusters = n_clusters\r\n        self.key_dims = key_dims\r\n        self.long_dims = long_dims\r\n        self.short_dims = short_dims\r\n        self.temperature = temperature\r\n        self.alpha = alpha\r\n        self.is_short = is_short\r\n\r\n        self.key_memory = tf.compat.v1.get_variable(\r\n            f\"{self.name}_key_memory\",\r\n            shape=[self.n_clusters, self.key_dims],\r\n            initializer=tf.compat.v1.glorot_normal_initializer())\r\n        # self.key_memory = self.add_weight(f\"{self.name}_key_memory\",\r\n        #                       shape=(self.n_clusters, self.key_dims),\r\n        #                       initializer=tf.keras.initializers.GlorotNormal(),\r\n        #                       trainable=False)\r\n\r\n        #\r\n        # self.long_memory = self.add_weight(f\"{self.name}_long_memory\",\r\n        #                       shape=(self.n_clusters, self.long_dims),\r\n        #                       initializer=tf.keras.initializers.GlorotNormal(),\r\n        #                       trainable=False)\r\n\r\n        self.long_memory = tf.compat.v1.get_variable(\r\n            f\"{self.name}_long_memory\",\r\n            shape=[self.n_clusters, self.key_dims],\r\n            initializer=tf.compat.v1.glorot_normal_initializer())\r\n\r\n        if is_short:\r\n            self.short_memory =  self.add_weight(f\"{self.name}_short_memory\",\r\n                              shape=(self.n_clusters, self.long_dims),\r\n                              initializer=tf.keras.initializers.GlorotNormal(),\r\n                              trainable=False)\r\n\r\n            self.short_erase_layer = DNNLayer([self.short_dims], 'sigmoid',\r\n                                             name=f\"{self.name}_short_erase_layer\")  # short earse layer\r\n            self.short_add_layer = DNNLayer([self.short_dims], 'tanh', name=f\"{self.name}_short_add_layer\")  #short add layer\r\n\r\n        self.write_controller = DNNLayer(controller_layers, controller_hidden_act, controller_output_act) #write controller\r\n\r\n        self.read_controller = DNNLayer(controller_layers, controller_hidden_act, controller_output_act) #read controller\r\n\r\n        self.key_erase_layer = DNNLayer([self.key_dims], 'sigmoid', name=f\"{self.name}_key_erase_layer\") #key erase layer\r\n        self.key_add_layer = DNNLayer([self.key_dims], 'tanh', name=f\"{self.name}_key_add_layer\") #key add layer\r\n\r\n        self.long_erase_layer = DNNLayer([self.long_dims], 'sigmoid', name=f\"{self.name}_long_erase_layer\")  #long erase layer\r\n        self.long_add_layer = DNNLayer([self.long_dims], 'tanh', name=f\"{self.name}_long_add_layer\")  # long add layer\r\n\r\n        # self.read_sim_layer = SimLayer()\r\n        # self.write_sim_layer = SimLayer()\r\n\r\n\r\n    @tf.function\r\n    def call(self, inputs, training=None, **kwargs):\r\n        attr_emb = inputs[0]  # [batch_size, attr_emb]\r\n        long_emb = inputs[1] #[batch, long_dims] embedding\r\n        isActive = inputs[2]  # [batch_size, 1]\r\n\r\n        active_status = tf.cast(tf.transpose(isActive, perm=[1,0]), dtype='float32')\r\n\r\n        # denominator = tf.math.count_nonzero(isActive) \r\n\r\n        #\uff1e0\u624d\u66f4\u65b0\r\n        if training:\r\n            #write memory\r\n            #[batch_size, key_dims]\r\n            write_query = self.write_controller(attr_emb) \r\n\r\n            write_query_add = tf.reduce_mean(write_query, axis=0, keepdims=True)\r\n\r\n            #[batch, n_clusters]\r\n            # write_attention = compute_cosine_similarity(write_query, self.key_memory, temperature=self.temperature)\r\n\r\n            # write_attention = self.write_sim_layer([write_query, self.key_memory], temperature=self.temperature)\r\n            #\r\n            # #[1, n_clusters]\r\n            # write_memory_attention = tf.matmul(active_status, write_attention) / tf.cast(1024, dtype=tf.float32)\r\n            # write_memory_attention = tf.matmul(active_status, write_attention) / tf.cast(1024, dtype=tf.float32)\r\n\r\n            #[n_clusters, 1]\r\n            # write_memory_attention = tf.transpose(write_memory_attention, perm=[1, 0])\r\n\r\n            #write key memory\r\n            # key_erase_vector = self.key_erase_layer(write_query) #[batch, key_dims]\r\n            # key_erase_vector = tf.matmul(active_status, key_erase_vector) / tf.cast(denominator, dtype=tf.float32) #[1,key_dims]\r\n            #\r\n            # key_add_vector = tf.reduce_mean(self.key_add_layer(write_query), axis=0, keepdims=True)#[batch, key_dims]\r\n            # key_add_vector = tf.matmul(active_status, key_add_vector) / tf.cast(denominator, dtype=tf.float32)  # [1,key_dims]\r\n\r\n            self.key_memory = self.key_memory \\\r\n                               + write_query_add\r\n\r\n            # * (1. - self.alpha * tf.matmul(write_memory_attention, key_erase_vector)) \\\r\n\r\n            #write long_memory\r\n            # long_erase_vector = self.long_erase_layer(long_emb)  # [batch, long_dims]\r\n            # long_erase_vector = tf.matmul(active_status, long_erase_vector) / tf.cast(denominator, dtype=tf.float32)  # [1,long_dims]\r\n            #\r\n            # long_add_vector = self.long_add_layer(long_emb)  # [batch, long_dims]\r\n            # long_add_vector = tf.matmul(active_status, long_add_vector) / tf.cast(denominator, dtype=tf.float32)  # [1,key_dims]\r\n\r\n            self.long_memory = self.long_memory\r\n                              # * (1. - self.alpha * tf.matmul(write_memory_attention, long_erase_vector)) \\\r\n                              # + self.alpha * tf.matmul(write_memory_attention, long_add_vector)\r\n\r\n        #read memory\r\n        #[batch, key_dims]\r\n        read_query = self.read_controller(tf.stop_gradient(attr_emb))\r\n\r\n         #[batch, n_clusters]\r\n        read_attention = compute_cosine_similarity(read_query, self.key_memory, temperature=self.temperature)\r\n\r\n        # read_attention = self.read_sim_layer([read_query, self.key_memory], temperature=self.temperature)\r\n\r\n        long_cluster_emb = tf.matmul(read_attention, self.long_memory) #[batch, long_dims] \u957f\u671f\u5174\u8da3\u8868\u5f81\r\n\r\n        return long_cluster_emb\r\n\r\n    def predict_process(self, inputs):\r\n        read_query = self.read_controller(inputs[0])\r\n        # read_attention = self.read_sim_layer([read_query, self.key_memory], temperature=self.temperature)\r\n        read_attention = compute_cosine_similarity(read_query, self.key_memory, temperature=self.temperature)\r\n\r\n        long_cluster_emb = tf.matmul(read_attention, self.long_memory)  # [batch, long_dims] \u957f\u671f\u5174\u8da3\u8868\u5f81\r\n\r\n        return long_cluster_emb\r\n\r\n    def get_config(self):\r\n        config = super(MemoryLayer, self).get_config()\r\n        config.update({\r\n            'controller_layers': self.controller_layers,\r\n            'controller_hidden_act': self.controller_hidden_act,\r\n            'controller_output_act': self.controller_output_act,\r\n            'n_clusters': self.n_clusters,\r\n            'key_dims': self.key_dims,\r\n            'long_dims': self.long_dims,\r\n            'short_dims': self.short_dims,\r\n            'temperature': self.temperature,\r\n            'alpha': self.alpha,\r\n            'is_short': self.is_short,\r\n            'name': self.name\r\n        })\r\n        return config\r\n\r\n    @classmethod\r\n    def from_config(cls, config):\r\n        return cls(**config)", "comments": ["Hi @pnuzyf ! Could you provide the standalone code as Colab gist ? Thanks!", "> \r\n\r\nthanks for  your prompt response. since we change a lot for the input layer and embedding layer, I should provide all the source code. Is that ok?", "Ok @pnuzyf ! Can you try again after disabling eager execution with `\"tf.compat.v1.disable_eager_execution()\" `command . Please share your code as Colab gist with source code to proceed further. Thanks!", "you can clone the source code from https://github.com/pnuzyf/tf-model.git, the test code locates in test/msmt/test_memory.py if I set \"tf.compat.v1.disable_eager_execution()\", anthor error comes up.", "hello, could you run the test code?", "Hi! @pnuzyf ! I was getting import error at line mentioned with \"pkgs\" . Could you please attach a requirement.txt file in your source code to install all dependencies?", "really\uff1f could you show me the error\uff1f I think there is nothing special dependency, just follow the hint when you run the code.", " in test_memory.py, you should change the argument of custom_layer_file_path\uff0c and  the correponding config_file, export_path, pack_path and data_path etc.\r\n\r\nmodel = MsMtModel(input, num_scenarios, moe_num_experts, moe_expert_layers, ordered_task_names, layer_number, ple_dict, tower_dict, task_output_act,\r\n                     tower_dependencies_dict={},\r\n                     custom_layer_file_path=\"/Users/fabriszhou/PycharmProjects/tf-model/test/msmt/custom/custom_layers.py\", is_parallel=False, short_cut=True,\r\n                     cut_tower=cut_tower_dict, is_memory=True, controller_layers=controller_layers, controller_output_act=controller_output_act, key_dims=key_dims,\r\n                     long_dims=long_dims,\r\n                     name='msmt')\r\n\r\n", "Hi @pnuzyf ! Could you please look at these relevant threads for answer? Link [1](https://github.com/tensorflow/tensorflow/issues/32889#issuecomment-880245700) , [2.](https://colab.sandbox.google.com/gist/mohantym/2cedb7497a1c768d986a1d194c8843be/github_54465.ipynb#scrollTo=9RvlhcTdFdII) .\r\n\r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) for further assistance.\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999) . Thanks!", "I have posted the issue to keras repo.will anybody take over this issue? ", "> Hi @pnuzyf ! Could you please look at these relevant threads for answer? Link [1](https://github.com/tensorflow/tensorflow/issues/32889#issuecomment-880245700) , [2.](https://colab.sandbox.google.com/gist/mohantym/2cedb7497a1c768d986a1d194c8843be/github_54465.ipynb#scrollTo=9RvlhcTdFdII) .\r\n> \r\n> Please post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) for further assistance. To know more see; https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999 . Thanks!\r\n\r\nhello, could you test it with some IDE? such as pycharm?  ", "@pnuzyf !  It will be tracked there in Keras repo. But please post a simple stand alone code there. As debugging a entire source code is a tedious job. I will check in IDE and update you in mean while . Thank you!", " Closing this issue for now. Thank you!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54465\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54465\">No</a>\n", "> @pnuzyf ! It will be tracked there in Keras repo. But please post a simple stand alone code there. As debugging a entire source code is a tedious job. I will check in IDE and update you in mean while . Thank you!\r\n\r\nWhen I added MemoryLayer to Msmt\uff0c I got the above error, without MemoryLayer, it's ok."]}, {"number": 54464, "title": "RNN with nested inputs", "body": "I would like to implement a recurrent GAT model which takes two inputs of shape: [(batch_size, time, N, F), [batch_size,  time, N, N)].  The hidden size should have shape (batch_size, N, hidden_feature).  I've structured the class as follows:\r\n```python\r\nclass NestedCell(keras.layers.Layer):\r\n    def __init__(self, nodes, features, channels, attention_heads, **kwargs):\r\n        self.tot_nodes = nodes\r\n        self.nodes_features = features\r\n        self.hidden_size = channels * attention_heads\r\n        self.state_size = [tf.TensorShape([self.tot_nodes, self.hidden_size])]\r\n        self.output_size = [tf.TensorShape([self.tot_nodes, self.tot_nodes]),\r\n                            tf.TensorShape([self.tot_nodes, self.hidden_size])]\r\n        super(NestedCell, self).__init__(**kwargs)\r\n```\r\nThe problem is that when I feed a list of two inputs:\r\n```python\r\nN = 10\r\nf = 5\r\ncell = NestedCell(N, f, 10,10)\r\ni1 = tf.keras.Input((None, N, f))\r\ni2 = tf.keras.Input((None, N, N))\r\nrnn = tf.keras.layers.RNN(cell)\r\no = rnn([i1, i2])\r\n```\r\nI get the error that the initial state is not compatible with the cell.state_size:\r\n```\r\nValueError: An `initial_state` was passed that is not compatible with `cell.state_size`. Received `state_spec`=ListWrapper([InputSpec(shape=(None, None, 10, 10), ndim=4)]); however `cell.state_size` is [TensorShape([10, 100])]\r\n```\r\nHow should I structure the class to accept such input shape?", "comments": ["@claCase,\r\n\r\nThis issue is related to Keras. Please post this on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more please check [here](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999). Thanks!", "@chunduriv Ok thank you I've posted the question in the keras repo.", "@claCase, Can you please close this issue, since it is tracked https://github.com/keras-team/keras/issues/16103. Thanks!"]}, {"number": 54463, "title": "Internal error `Blas xGEMV launch failed` on Tensorflow v2.8.0 for the same block of codes that runs perfectly well on Tensorflow v2.4.1", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Unknown\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): v2.8.0-rc1-32-g3f878cff5b6 2.8.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): N.A.\r\n- GCC/Compiler version (if compiling from source): N.A.\r\n- CUDA/cuDNN version: CUDA 11.2.1\r\n- GPU model and memory: Tesla T4 / 16 GB\r\n\r\n**Describe the current behavior**\r\nRunning a block of code with Tensorflow v2.8.0 / Cuda 11.2 / CuDNN 8.1 returns an internal error `Blas xGEMV launch failed` when it runs perfectly well with Tensorflow v2.4.1 / Cuda 11.0 / CuDNN 8.0.\r\n\r\n**Describe the expected behavior**\r\nReturn the same output as Tensorflow v2.4.1 / Cuda 11.0 / CuDNN 8.0.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing): N.A.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nThe following block of code works perfectly well with Tensorflow v2.4.1 / Cuda 11.0 / CuDNN 8.0, but not with Tensorflow v2.8.0 / Cuda 11.2 / CuDNN 8.1.\r\n```\r\nimport tensorflow as tf\r\nempty_image = tf.zeros(shape=[1280, 1280, 3], dtype=tf.float32)\r\ngray_image = tf.image.rgb_to_grayscale(empty_image)\r\n```\r\n\r\nAn important point to note is that when I reduce the `shape` of `empty_image` to `[512, 512, 3]`, there is no issue. However, I believe this is not a device memory issue as I can reproduce this with GeForce RTX 2080 Ti 11 GB as well as Tesla T4 16 GB.\r\n\r\n**Other info / logs** \r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/ubuntu/miniconda3/envs/docrec/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n  File \"/home/ubuntu/miniconda3/envs/docrec/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 7186, in raise_from_not_ok_status\r\n    raise core._status_to_exception(e) from None  # pylint: disable=protected-access\r\ntensorflow.python.framework.errors_impl.InternalError: Blas xGEMV launch failed : a.shape=[1,1638400,3], b.shape=[1,3,1], m=1638400, n=1, k=3 [Op:MatMul]\r\n```\r\n\r\n", "comments": ["Currently, the workaround for me is to use CUDA 11.1 with cuDNN 8.1.1. I arrived at this after finding out that Google Colab has TensorFlow 2.8.0 installed but runs on CUDA 11.1, although [TensorFlow's compatibility matrix](https://www.tensorflow.org/install/source#gpu) recommends CUDA 11.2. When I installed CUDA 11.1, which is usually bundled with cuDNN 8.0.x, TensorFlow threw an error saying it requires cuDNN 8.1.x. Hence, upgrading cuDNN to 8.1.1 does the trick. \r\n\r\nHaving said that, I believe the reported bug is something to be looked at and addressed. I have a feeling this problem would appear in all TensorFlow versions that recommends CUDA 11.2 and cuDNN 8.1, i.e., TensorFlow >= 2.5.0, and I am saying this because I was getting the same error after downgrading to TensorFlow 2.7.0 on CUDA 11.2 and cuDNN 8.1.\r\n\r\nFor those who has CUDA 11.1 installed with cuDNN 8.0.x on Ubuntu 18.04 / 20.04, the following commands would upgrade your cuDNN version from 8.0.x to 8.1.1.\r\n```\r\nwget https://developer.download.nvidia.com/compute/redist/cudnn/v8.1.1/cudnn-11.2-linux-x64-v8.1.1.33.tgz -O /tmp/cudnn-11.2-linux-x64-v8.1.1.33.tgz\r\ntar -xzvf /tmp/cudnn-11.2-linux-x64-v8.1.1.33.tgz -C /tmp/\r\nsudo cp /tmp/cuda/include/cudnn*.h /usr/local/cuda/include\r\nsudo cp /tmp/cuda/lib64/libcudnn* /usr/local/cuda/lib64\r\nsudo chmod a+r /usr/local/cuda/include/cudnn*.h /usr/local/cuda/lib64/libcudnn*\r\n```", "@arvindrajan92 ,\r\nGoogle Colab has TensorFlow 2.8.0 installed and runs on CUDA 11.2 and I was able to execute the given code without any issues.Please find the gist [here](https://github.com/tensorflow/tensorflow/issues/54463).Thanks!", "> @arvindrajan92 , Google Colab has TensorFlow 2.8.0 installed and runs on CUDA 11.2 and I was able to execute the given code without any issues.Please find the gist [here](https://github.com/tensorflow/tensorflow/issues/54463).Thanks!\r\n\r\nhi @tilakrayal, thank you for getting back to me. your gist brings me back to this issue though. could you check your link please? also, [this](https://colab.research.google.com/drive/1t6IQMTyn0bGBC1Fsd-j2mNf6eOgJF-o0?usp=sharing) is my google colab notebook which says CUDA 11.1 when i execute `nvcc --version`", "@arvindrajan92, \r\nGiven configurations [Tested build configuration](https://www.tensorflow.org/install/source#gpu) were tested on different platforms. \r\n\r\nThis error is due to \r\n\r\n- OOM error -GPU is running out of memory\r\n- Doesn't have enough compute capacity\r\n- There's a driver issue.\r\n\r\nCan you verify the memory usage with nvidia-smi? If you have any other\r\nprocesses using the GPU. And also check CUDA compute capability for the given nvidia drivers. \r\n\r\n   ", "> @arvindrajan92, Given configurations [Tested build configuration](https://www.tensorflow.org/install/source#gpu) were tested on different platforms.\r\n> \r\n> This error is due to\r\n> \r\n> * OOM error -GPU is running out of memory\r\n> * Doesn't have enough compute capacity\r\n> * There's a driver issue.\r\n> \r\n> Can you verify the memory usage with nvidia-smi? If you have any other processes using the GPU. And also check CUDA compute capability for the given nvidia drivers.\r\n\r\nThank you for taking a look at this issue @gadagashwini. Please allow me to address your points.\r\n\r\n**OOM error -GPU is running out of memory**\r\nBelow is the block of codes and outputs when ran on AWS Deep Learning AMI GPU CUDA 11.2.1 (Ubuntu 20.04) 20220208. I have attached below the screenshot from nvidia-smi after running the codes. I can confirm that the GPU did not run out of memory and there are no other processes using the GPU as you can see from nvidia-smi. Furthermore, running these codes has not used more than 1GB of GPU memory.\r\n\r\n```\r\n[GCC 7.5.0] :: Anaconda, Inc. on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> physical_devices = tf.config.list_physical_devices('GPU')\r\n2022-02-22 12:46:37.311936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-02-22 12:46:41.624784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-02-22 12:46:41.625446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n>>> tf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n>>> empty_image = tf.zeros(shape=[1280, 1280, 3], dtype=tf.float32)\r\n2022-02-22 12:47:11.120757: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2022-02-22 12:47:11.121428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-02-22 12:47:11.122072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-02-22 12:47:11.122620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-02-22 12:47:12.496265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-02-22 12:47:12.496875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-02-22 12:47:12.497420: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-02-22 12:47:12.497989: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13795 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\r\n>>> gray_image = tf.image.rgb_to_grayscale(empty_image)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/ubuntu/miniconda3/envs/docrec/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n  File \"/home/ubuntu/miniconda3/envs/docrec/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\", line 7186, in raise_from_not_ok_status\r\n    raise core._status_to_exception(e) from None  # pylint: disable=protected-access\r\ntensorflow.python.framework.errors_impl.InternalError: Blas xGEMV launch failed : a.shape=[1,1638400,3], b.shape=[1,3,1], m=1638400, n=1, k=3 [Op:MatMul]\r\n```\r\n![image](https://user-images.githubusercontent.com/18135970/155136261-99632e9c-376f-4044-80fb-2fb62b2c4083.png)\r\n\r\n**Doesn't have enough compute capacity**\r\nFrom the code block above, you can see that the compute capability is 7.5. Is this not enough?\r\n\r\n**There's a driver issue.**\r\nI can reproduce this on any of the AWS's AMI with GPU and CUDA 11.2.1 installed. Similarly, I can reproduce this on my local machine with Geforce RTX 3060 which has compute capability of 8.6 where the NVIDIA driver, CUDA 11.2.1 and cuDNN 8.1 are freshly installed. However, I don't see this issue on any of AWS's AMI with CUDA 11.1.1 installed after upgrading cuDNN to version 8.1 (from version 8.0) - I observe the same behaviour when installing CUDA 11.1.1 and cuDNN 8.1 on my local machine with Geforce RTX 3060.\r\n\r\nAre you able to run this on a physical machine with CUDA 11.2.1 and cuDNN 8.1 without issues? ", "Hi @gadagashwini, are you still looking into this issue? Thanks.", "@arvindrajan92,\r\n\r\n> However, I don't see this issue on any of AWS's AMI with CUDA 11.1.1 installed after upgrading cuDNN to version 8.1 (from version 8.0) - I observe the same behaviour when installing CUDA 11.1.1 and cuDNN 8.1 on my local machine with Geforce RTX 3060.\r\n\r\nIndeed this is expected behaviour. As per the [Tensorflow document](https://www.tensorflow.org/install/source#gpu), CUDA 11.2 and cuDNN 8.1 are compatible versions. I could run the given code on CUDA 11.2 with cuDNN 8.1. Thanks!\r\n", "It may be a bug of cublas. [cublas 11.4](https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#cublas-11.4.0) resolved an issue:\r\n> Some gemv cases were producing incorrect results if the matrix dimension (n or m) was large, for example 2^20.\r\n\r\nIn your case, m=1638400>2^20. As cublas is not open-source, it's unclear what versions of cublas have this issue.", "Thank you @njzjz, looking at `tensorflow.python.framework.errors_impl.InternalError: Blas xGEMV launch failed : a.shape=[1,1638400,3], b.shape=[1,3,1], m=1638400, n=1, k=3 [Op:MatMul]`, seems like it is probably due to the bug in cuBLAS.  When I change `shape` to `[512, 512, 3]`, I am getting the expected output.\r\n\r\nFrom trying out different versions of CUDA, seems like the bug is introduced in CUDA 11.2 and only resolved in CUDA 11.4. I don't see TensorFlow throwing the error in CUDA 11.1.\r\n\r\nHi @gadagashwini, I am happy to close the issue since it is a bug in cuBLAS 11.2. I suppose this is something to keep in mind so that upcoming TensorFlow versions are not built against CUDA 11.3, which may also have the same bug in cuBLAS.", "@arvindrajan92,\r\nThanks for confirming. Since the issue is more related to cuBLAS, will move this to closure. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54463\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54463\">No</a>\n"]}, {"number": 54462, "title": "C++ prebuilt libs", "body": "Are there any plans to release prebuilt C++ libs like the ones for C here https://www.tensorflow.org/install/lang_c? ", "comments": ["@kommander, Work on C++ Tensorflow binary release is going on. Take a look at similar issue [#54294](https://github.com/tensorflow/tensorflow/issues/54294#issuecomment-1040465740)", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54462\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54462\">No</a>\n"]}, {"number": 54460, "title": "gradient_function/gradient_tapes with device annotations", "body": "**System information**\r\n- TensorFlow version (you are using): 2.8\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nFeature request: adding `colocate_gradients_with_ops` option to `tf.gradients` in 1.x to `tf.GradientTape` in 2.x.\r\n\r\nThis feature request/issue was 1st mentioned in https://github.com/tensorflow/tensorflow/issues/33688 almost two years ago by @olesalscheider . Allow me to quickly review the op in which the current behavior (still hold in tf 2.8) is described.\r\n\r\n> Currently GradientTape.gradient() is executed on the device of the scope it is called in. Have a look at the following code:\r\n> \r\n```\r\nwith tf.GradientTape() as tape:\r\n    with tf.device('/gpu:1'):\r\n        x = f1(input)\r\n    with tf.device('/gpu:2'):\r\n        x = f2(x)\r\n    with tf.device('/gpu:0'):\r\n        g = tape.gradient(x, f_vars)\r\n```\r\n> Here all gradient calculations will be carried out by GPU:0 and all variables needed for the gradient calculation will also be allocated on GPU:0. This is a problem if these temporary variables are too large to fit into the VRAM of GPU:0.\r\n> \r\n> Please provide a way to execute the backward functions on the device of the corresponding forward function and allocate temporary variables for gradient calculation there. This allows to split a large model and distribute it among as many GPUs as necessary.\r\n> \r\n\r\nAlso, @olesalscheider provided a pr https://github.com/tensorflow/tensorflow/commit/a64ff0f2cda9d4e35ea450d4e945009a90ddee9a to achieved such feature and it get merged at the beginning, but shortly get rollbacked due to certain performance issues. \r\n\r\nI opened another pr https://github.com/tensorflow/tensorflow/pull/54510/commits/c292044a64700ecc3fa419b8247dfb1b61cf3ce7 to fit the current master (tf2.8) and build and test it. It looks to me the gradient can now be split correctly according to the device annotation.\r\n\r\n**Will this change the current api? How?**\r\nIt will allow tf.GradientTape to do gradient in the device of the corresponding forward function. Hence, it would be possible to split the training of large models into as many possible GPUs as necessary\r\n\r\n**Who will benefit with this feature?**\r\nAnyone who wants to train large models that do not fit into the VRAM of a single GPU.\r\n\r\n**Any Other info.**\r\n\r\nIt looks to me that the original post has gone silence... So I raise the issue again here to draw more attentions. It is a very import feature for peoples working in large image segmentation tasks. Some times the input tensor is so large and the model can not even be fitted into a single A100 card. \r\n\r\nMoreover, I noticed that in the discussion flow in the original post that there are some other work around like split into different tapes. I tried but with no luck. all gradient calculation still be allocated on gpu:0, and moreover, splitting tape would be very different for model like u-net which has a lot of skip-connections.\r\n\r\nLast but not the least, I know for a fact that there is the Mash-Tensorflow which propose to do such job, but a native tensorflow support would be also very useful for people work on large models.", "comments": ["I built with https://github.com/tensorflow/tensorflow/pull/54510/commits/c292044a64700ecc3fa419b8247dfb1b61cf3ce7 and tested it on 3090 cards. Below is an minimum working example:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers, Input, Model\r\n\r\n# if additional flags are needed, define it here.\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\n\r\n# Currently, memory growth needs to be the same across GPUs\r\nfor gpu in gpus:\r\n    print(gpu)\r\n    tf.config.experimental.set_memory_growth(gpu, True)\r\n\r\ninput_size = (256, 256, 256)\r\ninput_tensor = tf.ones((1,) + input_size + (1,), dtype=tf.float32)\r\n\r\ndef part1(it):\r\n    x = layers.Conv3D(8, 3, padding='same')(it)\r\n    x = layers.Conv3D(8, 5, padding='same')(x)\r\n    x = layers.Conv3D(8, 7, padding='same')(x)\r\n    ot = layers.Conv3D(1, 3, padding='same')(x)\r\n    return Model(it, ot, name='part1')\r\n\r\n\r\ndef part2(it):\r\n    x = layers.Conv3D(8, 3, padding='same')(it)\r\n    x = layers.Conv3D(8, 5, padding='same')(x)\r\n    x = layers.Conv3D(8, 7, padding='same')(x)\r\n    ot = layers.Conv3D(8, 3, padding='same')(x)\r\n    return Model(it, ot, name='part2')\r\n\r\nit1_ = Input(shape=input_size + (1,))\r\nit2_ = Input(shape=input_size + (1,))\r\n\r\nPart1 = part1(it1_)\r\nPart2 = part2(it2_)\r\n\r\nPart1.summary()\r\nPart2.summary()\r\n\r\nprint(f'tf version: {tf.__version__}')\r\n\r\nwith tf.GradientTape() as tape:\r\n    with tf.device('/gpu:0'):\r\n        x_ = Part1(input_tensor)\r\n        print(f'forward part1 done', flush=True)\r\n\r\n    with tf.device('/gpu:1'):\r\n        ot_ = Part2(x_)\r\n        print(f'forward part2 done', flush=True)\r\n\r\n    g = tape.gradient(ot_, Part2.trainable_variables + Part1.trainable_variables)\r\n    print(f'backward part1&2 done', flush=True)\r\n```\r\n\r\nThe above code will roughly take 9800MB on GPU0 and GPU1 each, and 19800MB if you put everything on GPU0", "I close this issue as the pr https://github.com/tensorflow/tensorflow/pull/54510/commits/c292044a64700ecc3fa419b8247dfb1b61cf3ce7 did provide what I desired. Thx!"]}, {"number": 54459, "title": "StringLookup fails as first layer in a Sequential model in TF 2.8.0", "body": "**System information**\r\n\r\n<details>\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 18.04 (Google Colab)**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**\r\n- TensorFlow installed from (source or binary): **binary (preinstalled on Google Colab)**\r\n- TensorFlow version (use command below): **v2.8.0-0-g3f878cff5b6 2.8.0**\r\n- Python version: **3.7.12**\r\n- Bazel version (if compiling from source): **N/A**\r\n- GCC/Compiler version (if compiling from source): **N/A**\r\n- CUDA/cuDNN version: **N/A**\r\n- GPU model and memory: **N/A**\r\n\r\n</details>\r\n\r\n**Describe the current behavior**\r\n\r\nUsing a `tf.keras.layers.StringLookup` layer as the first layer in a `Sequential` model raises an exception when calling the model: `UnimplementedError: Exception encountered when calling layer \"sequential\" (type Sequential). Cast string to int64 is not supported [Op:Cast]` (see full stacktrace below).\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ncat = [\"Paris\", \"Singapore\", \"Auckland\"]\r\nstr_lookup_layer = tf.keras.layers.StringLookup()\r\nstr_lookup_layer.adapt(cat)\r\nlookup_and_embed = tf.keras.Sequential([\r\n    str_lookup_layer,\r\n    tf.keras.layers.Embedding(input_dim=str_lookup_layer.vocabulary_size(),\r\n                              output_dim=2)\r\n])\r\nlookup_and_embed(tf.constant([[\"Paris\"], [\"Singapore\"], [\"Auckland\"]]))  # ERROR!\r\n```\r\n\r\nThis code is available in this [gist](https://colab.research.google.com/gist/ageron/4ba943dac246ade26699b7fe8fa38fae/tensorflow-issue-54459.ipynb).\r\n\r\n**Describe the expected behavior**\r\nThis should work like it did in TensorFlow 2.7.1, I believe it's a regression. It should output something like this:\r\n\r\n```\r\n<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\r\narray([[-0.02887753, -0.01268407],\r\n       [ 0.04601531, -0.02668235],\r\n       [ 0.03409723, -0.03205377]], dtype=float32)>\r\n```\r\n\r\n**Other info / logs**\r\n\r\nInstead, it raises this exception. Full stacktrace:\r\n\r\n<details>\r\n\r\n```stacktrace\r\n---------------------------------------------------------------------------\r\nUnimplementedError                        Traceback (most recent call last)\r\n[<ipython-input-2-ee4b4b94a15e>](https://localhost:8080/#) in <module>()\r\n      7                               output_dim=2)\r\n      8 ])\r\n----> 9 lookup_and_embed(tf.constant([[\"Paris\"], [\"Singapore\"], [\"Auckland\"]]))\r\n\r\n1 frames\r\n[/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py](https://localhost:8080/#) in error_handler(*args, **kwargs)\r\n     65     except Exception as e:  # pylint: disable=broad-except\r\n     66       filtered_tb = _process_traceback_frames(e.__traceback__)\r\n---> 67       raise e.with_traceback(filtered_tb) from None\r\n     68     finally:\r\n     69       del filtered_tb\r\n\r\n[/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py](https://localhost:8080/#) in raise_from_not_ok_status(e, name)\r\n   7184 def raise_from_not_ok_status(e, name):\r\n   7185   e.message += (\" name: \" + name if name is not None else \"\")\r\n-> 7186   raise core._status_to_exception(e) from None  # pylint: disable=protected-access\r\n   7187 \r\n   7188 \r\n\r\nUnimplementedError: Exception encountered when calling layer \"sequential\" (type Sequential).\r\n\r\nCast string to int64 is not supported [Op:Cast]\r\n\r\nCall arguments received:\r\n  \u2022 inputs=tf.Tensor(shape=(3, 1), dtype=string)\r\n  \u2022 training=None\r\n  \u2022 mask=None\r\n```\r\n\r\n</details>\r\n\r\n\r\n", "comments": ["Hi @ageron !\r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999) . Thanks!", "Oh right, will do, thanks.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54459\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54459\">No</a>\n"]}, {"number": 54457, "title": "Install latest version but not find GPU", "body": "laptop: acer nitro\r\nGPU: RTX3070\r\nwindows: 10\r\npython: 3.9.10\r\nCUDA: cuda_11.6.0_511.23_windows\r\ncnnCUDA: cudnn_8.3.2.44_windows\r\n\r\nI try to install tensorflow but its not work\r\nI try to install from https://www.tensorflow.org/install/source_windows but its a lot complicated\r\n\r\nso I use pip and install\r\n`pip install tensorflow`\r\n`pip install tensorflow-gpu`\r\n`pip install tf-nightly-gpu --uesr`\r\n\r\nbut it has this problem\r\n```\r\n  WARNING: The scripts estimator_ckpt_converter.exe, import_pb_to_tensorboard.exe, saved_model_cli.exe, tf_upgrade_v2.exe, tflite_convert.exe, toco.exe and toco_from_protos.exe are installed in 'C:\\Users\\snipe\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\r\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\r\nWARNING: Ignoring invalid distribution -f-nightly-gpu (c:\\users\\snipe\\appdata\\roaming\\python\\python39\\site-packages)\r\nWARNING: Ignoring invalid distribution -f-nightly-gpu (c:\\users\\snipe\\appdata\\roaming\\python\\python39\\site-packages)\r\nWARNING: Ignoring invalid distribution -f-nightly-gpu (c:\\users\\snipe\\appdata\\roaming\\python\\python39\\site-packages)\r\nWARNING: Ignoring invalid distribution -f-nightly-gpu (c:\\users\\snipe\\appdata\\roaming\\python\\python39\\site-packages)\r\nWARNING: Ignoring invalid distribution -f-nightly-gpu (c:\\users\\snipe\\appdata\\roaming\\python\\python39\\site-packages)\r\nWARNING: Ignoring invalid distribution -f-nightly-gpu (c:\\users\\snipe\\appdata\\roaming\\python\\python39\\site-packages)\r\nWARNING: Ignoring invalid distribution -f-nightly-gpu (c:\\users\\snipe\\appdata\\roaming\\python\\python39\\site-packages)\r\nWARNING: Ignoring invalid distribution -f-nightly-gpu (c:\\users\\snipe\\appdata\\roaming\\python\\python39\\site-packages)\r\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\ntensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, but you have tf-estimator-nightly 2.9.0.dev2022021909 which is incompatible.\r\ntensorflow-gpu 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, but you have tf-estimator-nightly 2.9.0.dev2022021909 which is incompatible.\r\nSuccessfully installed absl-py-1.0.0 keras-nightly-2.9.0.dev2022021908 libclang-13.0.0 packaging-21.3 pyparsing-3.0.7 tb-nightly-2.9.0a20220218 tf-estimator-nightly-2.9.0.dev2022021909 tf-nightly-gpu-2.9.0.dev20220218\r\nWARNING: Ignoring invalid distribution -f-nightly-gpu (c:\\users\\snipe\\appdata\\roaming\\python\\python39\\site-packages)\r\nWARNING: Ignoring invalid distribution -f-nightly-gpu (c:\\users\\snipe\\appdata\\roaming\\python\\python39\\site-packages)\r\nWARNING: Ignoring invalid distribution -f-nightly-gpu (c:\\users\\snipe\\appdata\\roaming\\python\\python39\\site-packages)\r\nWARNING: You are using pip version 21.2.4; however, version 22.0.3 is available.\r\nYou should consider upgrading via the 'C:\\Python39\\python.exe -m pip install --upgrade pip' command.\r\n```\r\n\r\nI install Python39 in `c:\\Python39` but it use `C:\\Users\\snipe\\AppData\\Roaming\\Python\\Python39\\Scripts` don't know why.\r\n\r\nafter run this code I get this error:\r\nhttps://gist.github.com/lepotatoguy/df494922d0aaff48e62e0797589513ca\r\n```\r\nTensorFlow Version: 2.9.0-dev20220218\r\nNum GPUs Available:  0\r\n[C:\\Users\\snipe\\AppData\\Local\\Temp\\ipykernel_6776\\3718260442.py:6](): DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\r\n  assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer.  You are using {}'.format(tf.__version__)\r\n[C:\\Users\\snipe\\AppData\\Local\\Temp\\ipykernel_6776\\3718260442.py:11](): UserWarning: No GPU found. Please ensure you have installed TensorFlow correctly\r\n  warnings.warn('No GPU found. Please ensure you have installed TensorFlow correctly')\r\n```\r\n\r\n**I think is better if we have a cli install with specific version for windows, linux, and mac**\r\n\r\n![image](https://user-images.githubusercontent.com/35699848/154803720-b7dca916-42a0-4bed-ab1c-27fd30597280.png)\r\n", "comments": ["Hello, have you tried going through this [thread ](https://www.tensorflow.org/install/gpu) and reading through the necessary hardware and software requirements? ", "Solved. It should to unzip `cuDNN` and copy to Nvidia folder. I download the exe link not the zip. I think the exe its do every thnik it self", "@hdpklm ,\r\nCan you  please take a look at this [issue](https://github.com/tensorflow/tensorflow/issues/38990) with the similar error.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54457\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54457\">No</a>\n"]}, {"number": 54452, "title": "[TF:TRT] Update test helpers to handle bool tensors and weights", "body": "Enable the handling of bool tensors for in TF-TRT test helper subroutines.", "comments": ["Thanks @bixia1 for the review. Note that this is still a work in progress. Apart from fixing the issues that you pointed out, I plan further refactoring of the helper routines. I will let you know once it is ready for review.", "Closing this, in favor of an updated version #55177 "]}, {"number": 54451, "title": "Issue created for Rollback of PR #54426: Add complex support for tf.math.atan", "body": "Merged PR #54426 is rolled back in ef14b5b7f51357afc8fb131c3c35f90137269160.\n    Please follow up with the reviewer and close this issue once its resolved.", "comments": ["Changes are submitted again internally after speaking to other team. Closing this issue."]}, {"number": 54447, "title": "mean_squared_error accuracy", "body": "My currenct observation with \"mean_squared_error\" loss,\r\nIf you use mean_squared_error loss and metrics= [\"acc\"] , \r\nduring training you can't see any meaningfull accuracy,\r\nI wrote my custom acc. function , it's very hard for me to make a PR, because of project complexty,\r\n\r\nHere is my fn. \r\n```\r\ndef mean_squared_error_acc(y_true, y_pred):\r\n    loss = tf.losses.mean_squared_error(y_true,y_pred)    \r\n    return 100 * ( 1 - tf.math.sqrt(loss))\r\n\r\n```\r\n( by the way, I'm at very begining of my ML experience, and this issue would be meaningless , please lead me to right direction if I'm wrong) \r\n\r\n--Edit--\r\nI use one of my colab experiments to reproduce issue\r\nThis is colab link\r\nhttps://colab.research.google.com/drive/1hYG4RmPaUlnEcRm28NE6Y3HxIMoq1DTM?usp=sharing\r\n\r\nThese are the cells related the issue : \r\n\r\nhttps://colab.research.google.com/drive/1hYG4RmPaUlnEcRm28NE6Y3HxIMoq1DTM#scrollTo=lL3kJjI-cXLS&line=4&uniqifier=1\r\n\r\nhttps://colab.research.google.com/drive/1hYG4RmPaUlnEcRm28NE6Y3HxIMoq1DTM#scrollTo=eYb8YEiRvVuK&line=3&uniqifier=1", "comments": ["@firatsarlar,\r\nPlease post this issue on [keras-team/keras repo](https://github.com/keras-team/keras/issues).\r\nTo know more refer to:\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999", "@firatsarlar please update the issue on keras, we can discuss there\r\n", "currently, I can do it for myself to get rid of every model load extra code for additional acc. object\r\nthank you\r\n\r\nby the way is not there an internal issue forwarding, splitting, editing system , like this team\r\nwhat an English,\r\ni'm not a native speekaer, please do try to understanda what i wanted to say\r\n", "@firatsarlar ,\r\nCan you please post this issue on [keras-team/keras repo](https://github.com/keras-team/keras/issues) as this issue is more related to Keras.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54447\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54447\">No</a>\n"]}, {"number": 54446, "title": "Can't install Tensorflow 2.x.x on Jetson boards with pip", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Jetson - Jp 4.6\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version: 2.x.x\r\n- Python version: 3.6/3.8\r\n- Installed using virtualenv? pip? conda?: Pip/virtualenv\r\n- CUDA/cuDNN version: 11.3\r\n- GPU model and memory: Jetson AGX/ 32GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nCannot install Tensorflow on Jetson boards with ease of pip.\r\n\r\npip install tensorflow\r\n", "comments": ["Hi @LITDataScience ! Did you try in a virtual environment after installing dependencies as this [thread](https://docs.nvidia.com/deeplearning/frameworks/install-tf-jetson-platform/index.html) suggests? Thanks!", "@LITDataScience - As suggested by @mohantym , please use the Nvidia Link to install TF on Jetson (https://docs.nvidia.com/deeplearning/frameworks/install-tf-jetson-platform/index.html).  Nvidia has built a wheel file to work specifically for Jetson.  So please use that wheel file link in the docs to install in Jetson.  ARM builds can't be installed with `pip install tensorflow`.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54446\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54446\">No</a>\n"]}, {"number": 54445, "title": "Save and load a model with PReLU activation results in error", "body": "There is very specific scenarios when PReLU activation failed during `tf.keras.models.load`\r\n\r\n- Create and save the model via keras.Model.save\r\n- Load the model, make modifications to the Layer that including the PReLU activation\r\n- Save the modified Model and load it again, PReLU layer failed during `build` when initializing the `alpha` parameters\r\n\r\nNote: this issue disappear when I subclassing the `PReLU` that is replacing all `PReLU` with `ParamReLU`\r\nor specify the `share_axes` argument also solve the problem\r\n\r\n```python\r\nclass ParamReLU(PReLU):\r\n    ...\r\n```\r\n\r\n---\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 20.04**, **Windows 10**\r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version (use command below): **2.6.0, 2.6.1, 2.7.0, 2.8.0**; **v2.8.0-rc1-32-g3f878cff5b6 2.8.0**\r\n- Python version: **3.7.10**\r\n- CUDA/cuDNN version: **11.2/8.10**\r\n- GPU model and memory: **RTX 3080 / 10GB**\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): **no**\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import Conv2D, Input, PReLU\r\n\r\nx = Input((1, 1, 1))\r\ny = Conv2D(8, 1)(x)\r\ny = PReLU()(y)\r\ny = Conv2D(40, 1)(y)\r\nmodel = Model(x, y)\r\n\r\nmodel.save('/tmp/model_abc')\r\nmodel_new = tf.keras.models.load_model('/tmp/model_abc')\r\nlayer1 = model_new.get_layer('conv2d')\r\nlayer2 = model_new.get_layer('p_re_lu')\r\nlayer3 = model_new.get_layer('conv2d_1')\r\n\r\n# === prune the last filter of layer1\r\ncfg = layer1.get_config()\r\ncfg['filters'] = 7\r\nlayer1_copy = layer1.__class__.from_config(cfg)\r\nlayer1_copy.build((None, 1, 1, 1))\r\nlayer1_copy.set_weights([layer1.kernel.numpy()[..., :7],\r\n                         layer1.bias.numpy()[:7]])\r\ny = layer1_copy(model_new.inputs[0])\r\n\r\n# remove the last alpha of PReLU\r\nlayer2_copy = layer2.__class__.from_config(layer2.get_config())\r\nlayer2_copy.build(y.shape)\r\nlayer2_copy.set_weights([layer2.alpha.numpy()[..., :7]])\r\ny = layer2_copy(y)\r\n\r\n# remove 1 input filter layer3\r\ncfg = layer3.get_config()\r\nlayer3_copy = layer3.__class__.from_config(cfg)\r\nlayer3_copy.build(y.shape)\r\nlayer3_copy.set_weights([layer3.kernel.numpy()[..., :7, :],\r\n                         layer3.bias.numpy()])\r\ny = layer3_copy(y)\r\nmodel_prune = Model(model_new.inputs, y)\r\n\r\n# everything OK here\r\nprint(model_prune.summary())\r\n\r\n# === error happening here when we save and load the model again\r\nmodel_prune.save('/tmp/model_abc_prune')\r\nmodel_prune_new = tf.keras.models.load_model('/tmp/model_abc_prune')\r\n```\r\n\r\n**Other info / logs** \r\n```\r\nTraceback (most recent call last):\r\n  File \"...\", line 44, in <module>\r\n    model_prune_new = tf.keras.models.load_model('/tmp/model_abc_prune')\r\n  File \"/home/trung/miniconda3/envs/denoise/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n  File \"/home/trung/miniconda3/envs/denoise/lib/python3.7/site-packages/keras/initializers/initializers_v2.py\", line 145, in __call__\r\n    return tf.zeros(shape, dtype)\r\nValueError: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.\r\n```\r\n\r\n", "comments": ["@trungnt13,\r\n\r\nThis issue is related to Keras. Please post this on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more please check [here](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999). Thanks!", "Thanks, I have reposted the issue to Keras here\r\nhttps://github.com/keras-team/keras/issues/16116", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54445\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54445\">No</a>\n"]}, {"number": 54444, "title": "Fixed using invalid iterator (CID:20570650)", "body": "`std::vector::erase()` invalidates iterators and references at or after the point of the erase. Return value of `std::vector::erase()` shall be used to initialize the iterator (Return value: Iterator following the last removed element. )", "comments": ["@gbaned , any problem merging this PR to mainline? So far it was approved 4 times already :-). ", "> @gbaned , any problem merging this PR to mainline? So far it was approved 4 times already :-).\r\n\r\nHi @robert-kalmar  There was an internal import issue, hence it was approved 4 times. It is looks good now.  Thank you."]}, {"number": 54443, "title": "I want to add support for a neuro accelerator", "body": "Hi.\r\nI want to add to tf support for its own neural gas pedal like CUDA, how difficult is it to implement such a thing? ", "comments": ["@Muzantip ,\r\nCan you please elaborate about your Feature that supports your statement. Also, please specify the Use Cases for this feature. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 54441, "title": "Add appropriate PyObject type check for bool", "body": "This PR fixes an issue where PyObject type in tf's C bindings\r\ndoes not check if an input is a boolean and will always cast\r\nto bool. As an result, an invalid type like int can be passed\r\nto an arg expecting bool:\r\n```\r\nimport tensorflow as tf\r\nx = [0.5, 1.0, 2.0, 4.0]\r\naxis = 0\r\nexclusive = -1\r\nreverse = -1\r\nres_1 = tf.math.cumsum(x, axis=axis, exclusive=exclusive, reverse=reverse)\r\nprint(res_1) # tf.Tensor([7. 6. 4. 0.], shape=(4,), dtype=float32)\r\n```\r\n\r\nNote other PyObejct types (e.g., in ParseIntValue) does perform check in\r\ntensorflow/python/eager/pywrap_tfe_src.cc\r\n\r\nThis PR adds appropriate check in the input type when PyObject path\r\nis invoked.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 54440, "title": "\"ValueError: Setting hub.KerasLayer.trainable = True is unsupported when loading from the TF1 Hub format.\" when running Object Detection with TensorFlow Lite Model Maker tutorial on Windows 10. Tutorial ran well on Mac book", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Window 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):2.8.0\r\n- Python version:3.9.7\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: 11.0\r\n- GPU model and memory: NVIDIA GeForce MX150  16GB Ram\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nEncountered \r\n\r\n    ValueError: Setting hub.KerasLayer.trainable = True is unsupported when loading from the TF1 Hub format.\r\n\r\nWhen running below tutorial for training custom model for object detection using Tensorflow Lite Model Maker\r\n\r\nhttps://www.tensorflow.org/lite/tutorials/model_maker_object_detection\r\n\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\ncustom model to be trained.\r\n\r\nSimilar set up is working in Mac book. Please advise on how to get the custom model training working on Windows 10 machine.\r\n\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nOn Windows 10 machine, download the Anaconda installer. Follow default settings to install Anaconda\r\n\r\nOpen Anaconda prompt and create environment test by\r\n\r\nconda create -name test python=3.9\r\n\r\nactivate environment by\r\n\r\n(base) C:\\Users\\dev>conda activate test\r\n\r\n(test) C:\\Users\\dev>\r\n\r\nInstall tflite-model-maker by \r\n\r\n(test) C:\\Users\\dev>pip install tflite-model-maker\r\n\r\nafter installation completed, enter python prompt\r\n\r\n(test) C:\\Users\\dev>python\r\nPython 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>>\r\n\r\nexecute the instructions adapted from \r\n\r\nhttps://www.tensorflow.org/lite/tutorials/model_maker_object_detection\r\n\r\n(test) C:\\Users\\dev>python\r\nPython 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import numpy as np\r\n>>> import os\r\n>>> from tflite_model_maker.config import QuantizationConfig\r\n>>> from tflite_model_maker.config import ExportFormat\r\n>>> from tflite_model_maker import model_spec\r\n>>> from tflite_model_maker import object_detector\r\n>>> import tensorflow as tf\r\n>>> assert tf.__version__.startswith('2')\r\n>>> spec = model_spec.get('efficientdet_lite0')\r\n2022-02-18 12:41:00.098002: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2022-02-18 12:41:01.311912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1342 MB memory:  -> device: 0, name: NVIDIA GeForce MX150, pci bus id: 0000:01:00.0, compute capability: 6.1\r\n>>> train_data, validation_data, test_data = object_detector.DataLoader.from_csv('C:/Users/dev/Documents/achiever/tensorFlow/sub_salads.csv')\r\nINFO:tensorflow:Cache will be stored in C:\\Users\\dev\\AppData\\Local\\Temp\\tmpafosi8_7 with prefix filename train_47472d894c199311b64931ed1b6dae18. Cache_prefix is C:\\Users\\dev\\AppData\\Local\\Temp\\tmpafosi8_7\\train_47472d894c199311b64931ed1b6dae18\r\nINFO:tensorflow:Cache will be stored in C:\\Users\\dev\\AppData\\Local\\Temp\\tmpv_nxxrzt with prefix filename val_47472d894c199311b64931ed1b6dae18. Cache_prefix is C:\\Users\\dev\\AppData\\Local\\Temp\\tmpv_nxxrzt\\val_47472d894c199311b64931ed1b6dae18\r\nINFO:tensorflow:Cache will be stored in C:\\Users\\dev\\AppData\\Local\\Temp\\tmpdfguu885 with prefix filename test_47472d894c199311b64931ed1b6dae18. Cache_prefix is C:\\Users\\dev\\AppData\\Local\\Temp\\tmpdfguu885\\test_47472d894c199311b64931ed1b6dae18\r\nINFO:tensorflow:On image 0\r\nINFO:tensorflow:On image 0\r\nINFO:tensorflow:On image 0\r\n>>> model = object_detector.create(train_data, model_spec=spec, batch_size=8, train_whole_model=True, validation_data=validation_data)\r\nINFO:tensorflow:Retraining the models...\r\nWARNING:tensorflow:The size of the validation_data (3) is smaller than batch_size (8). Ignore the validation_data.\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\dev\\anaconda3\\envs\\test\\lib\\site-packages\\tensorflow_examples\\lite\\model_maker\\core\\task\\object_detector.py\", line 260, in create\r\n    object_detector.train(train_data, validation_data, epochs, batch_size)\r\n  File \"C:\\Users\\dev\\anaconda3\\envs\\test\\lib\\site-packages\\tensorflow_examples\\lite\\model_maker\\core\\task\\object_detector.py\", line 123, in train\r\n    return self.model_spec.train(self.model, train_ds, steps_per_epoch,\r\n  File \"C:\\Users\\dev\\anaconda3\\envs\\test\\lib\\site-packages\\tensorflow_examples\\lite\\model_maker\\core\\task\\model_spec\\object_detector_spec.py\", line 264, in train\r\n    train.setup_model(model, config)\r\n  File \"C:\\Users\\dev\\anaconda3\\envs\\test\\lib\\site-packages\\tensorflow_examples\\lite\\model_maker\\third_party\\efficientdet\\keras\\train.py\", line 113, in setup_model\r\n    model.build((None, *config.image_size, 3))\r\n  File \"C:\\Users\\dev\\anaconda3\\envs\\test\\lib\\site-packages\\keras\\engine\\training.py\", line 440, in build\r\n    self.call(x, **kwargs)\r\n  File \"C:\\Users\\dev\\anaconda3\\envs\\test\\lib\\site-packages\\tensorflow_examples\\lite\\model_maker\\third_party\\efficientdet\\keras\\train_lib.py\", line 885, in call\r\n    cls_outputs, box_outputs = self.base_model(inputs, training=training)\r\n  File \"C:\\Users\\dev\\anaconda3\\envs\\test\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n  File \"C:\\Users\\dev\\anaconda3\\envs\\test\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 692, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nValueError: Exception encountered when calling layer \"keras_layer\" (type KerasLayer).\r\n\r\nin user code:\r\n\r\n    File \"C:\\Users\\dev\\anaconda3\\envs\\test\\lib\\site-packages\\tensorflow_hub\\keras_layer.py\", line 213, in call  *\r\n        self._check_trainability()\r\n    File \"C:\\Users\\dev\\anaconda3\\envs\\test\\lib\\site-packages\\tensorflow_hub\\keras_layer.py\", line 272, in _check_trainability  *\r\n        raise ValueError(\r\n\r\n    ValueError: Setting hub.KerasLayer.trainable = True is unsupported when loading from the TF1 Hub format.\r\n\r\n\r\nCall arguments received:\r\n  \u2022 inputs=tf.Tensor(shape=(None, 320, 320, 3), dtype=float32)\r\n  \u2022 training=False\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nRelated to issue:\r\n\r\nhttps://github.com/tensorflow/hub/issues/841\r\n\r\nI have since uninstall Python 3.8.6, Anaconda3 and reinstalled Anaconda3. I tried running the above in Anaconda environment with python 3.9 installed. However, the error persisted.\r\n\r\nI repeat the same setup on my Mac book and it worked. Thus, this looks to be a Windows 10 specific issue.\r\n\r\nattached is Anaconda exported environment from windows:\r\n\r\n[winEnv.yml.txt](https://github.com/tensorflow/tensorflow/files/8094267/winEnv.yml.txt)\r\n\r\nand the working Anaconda exported environment from Mac Book\r\n\r\n[mac.yml.txt](https://github.com/tensorflow/tensorflow/files/8094270/mac.yml.txt)\r\n\r\n\r\n\r\n", "comments": ["@FlyWong,\r\nIn order to expedite the trouble-shooting process, please provide above mentioned custom model and dataset to reproduce the issue. Thanks!", "Hi Chunduriv, I got the issue just by following this tutorial. So you can use the code and dataset to reproduce the issue.\r\nhttps://www.tensorflow.org/lite/tutorials/model_maker_object_detection\r\n", "@FlyWong,\r\n\r\nI am able to run quickstart tutorial using `salads_ml_use` dataset without any issue. Please find the [gist](https://colab.research.google.com/gist/chunduriv/11d88d27fedfe2d3af75115215098459/model_maker_object_detection.ipynb)  here for reference. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54440\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54440\">No</a>\n"]}, {"number": 54439, "title": "error with functools32", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): centos7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version:1.15\r\n- Python version:3.6.4\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):0.26.1\r\n- GCC/Compiler version (if compiling from source):7.3.0\r\n- CUDA/cuDNN version:none\r\n- GPU model and memory:cpu-only\r\n\r\n**Describe the problem**\r\nI compiled the file `tensorflow-1.15.5-cp36-cp36m-linux_x86_64.whl` by using the tensorflw@r1.15, when i use `pip install tensorflow-1.15.5-cp36-cp36m-linux_x86_64.whl`,  there is a problem:\r\n\r\nDownloading functools32-3.2.3-2.tar.gz (31 kB)\r\n    ERROR: Command errored out with exit status 1:\r\n     command: /root/anaconda3/envs/tf_test/bin/python -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-zi7j89fx/functools32_fe3235542fe548578e556e08b3efc74b/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-zi7j89fx/functools32_fe3235542fe548578e556e08b3efc74b/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /tmp/pip-pip-egg-info-oymtginn\r\n         cwd: /tmp/pip-install-zi7j89fx/functools32_fe3235542fe548578e556e08b3efc74b/\r\n    Complete output (1 lines):\r\n    This backport is for Python 2.7 only.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@lhy2749 ,\r\nWe see that you are using tf version 1.15, 1.x is not actively supported, please update to latest stable version 2.8 and let us know if you are facing same issue.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54439\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54439\">No</a>\n"]}, {"number": 54438, "title": "Fix typo in docstring for `make_csv_dataset_v2`", "body": "replace the proper variable name in given example code", "comments": ["Hi @gadagashwini , please don't use a \"Update <file>\" commit message. Instead, please try to follow [proper commit etiquette](https://cbea.ms/git-commit/)", "Thank you @mihaimaruseac. From now, I will follow the standard commit etiquette. "]}, {"number": 54437, "title": "libXNNPACK.so: internal symbol `xnn_f16_ibilinear_ukernel__neonfp16arith_c8' isn't defined", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.2 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 56e2e916929746cd6c415c2a402a46349bffa7e0 (commit id)\r\n- Python version: N/A\r\n- Installed using virtualenv? pip? conda?:  N/A\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): 9.3.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI was trying to build TFLite with XNNPACK delegate natively on [an ARM-based board](https://www.khadas.com/vim3) using CMake. Previously I successfully built TFLite as static library, but when I tried to link it to my program, many errors of undefined references related to `ruy` happened. So I turned to compile TFLite as shared library instead, but got the following error:\r\n```\r\n[ 21%] Linking CXX shared library libXNNPACK.so\r\n/usr/bin/ld: CMakeFiles/XNNPACK.dir/src/init.c.o: in function `init':\r\ninit.c:(.text+0x139c): undefined reference to `xnn_f16_ibilinear_ukernel__neonfp16arith_c8'\r\n/usr/bin/ld: init.c:(.text+0x13a0): undefined reference to `xnn_f16_ibilinear_ukernel__neonfp16arith_c8'\r\n/usr/bin/ld: libXNNPACK.so: internal symbol `xnn_f16_ibilinear_ukernel__neonfp16arith_c8' isn't defined\r\n/usr/bin/ld: final link failed: bad value\r\ncollect2: error: ld returned 1 exit status\r\nmake[2]: *** [_deps/xnnpack-build/CMakeFiles/XNNPACK.dir/build.make:6532: _deps/xnnpack-build/libXNNPACK.so] Error 1\r\nmake[1]: *** [CMakeFiles/Makefile2:5486: _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/all] Error 2\r\nmake: *** [Makefile:152: all] Error 2\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\ncmake -B tflite-build -DBUILD_SHARED_LIBS=ON tensorflow/tensorflow/lite\r\ncmake --build tflite-build -j 4\r\n```\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Hi @fengyuentau ! Can you check below command  after declaring [ARM 64 SDK tool chain](https://www.tensorflow.org/lite/guide/build_cmake#cross-compilation) in path and removing -DBUILD_SHARED_LIBS=ON flag ? Thanks!\r\n \r\n`cmake -DCMAKE_TOOLCHAIN_FILE=<CMakeToolchainFileLoc> ../tensorflow/lite/`", "@mohantym Thanks for the reply, but TFLite is compiled natively on ARM in my case.", "> Hi @fengyuentau ! Can you check below command after declaring [ARM 64 SDK tool chain](https://www.tensorflow.org/lite/guide/build_cmake#cross-compilation) in path and removing -DBUILD_SHARED_LIBS=ON flag ? Thanks!\r\n> \r\n> `cmake -DCMAKE_TOOLCHAIN_FILE=<CMakeToolchainFileLoc> ../tensorflow/lite/`\r\n\r\nI met the same problem and I can compile static library successfully, but failed with \"-DBUILD_SHARED_LIBS=ON\" on Android64.  \"error: undefined reference to `xnn_f16_ibilinear_ukernel__neonfp16arith_c8'\"", "@mjp9527 Did you have undefined reference related to `ruy` when linking TFLite static library to your programe?", "> @mjp9527 Did you have undefined reference related to `ruy` when linking TFLite static library to your programe?\r\n\r\n@fengyuentau ......I was wrong,  this error appeared again when linking TFLite static library.\r\n![image](https://user-images.githubusercontent.com/54735487/154673663-238ab5bc-a780-47b4-97e2-f5f68b0d43e9.png)\r\n", "This was fixed in XNNPACK in google/XNNPACK@ba7e4bf72936ad7dcb750707eb519cd9c56bc1c9, and the latest revision of TensorFlow shouldn't have this issue.", "@Maratyszcza Let me check which version of XNNPACK is using when I compiled TLite.", "@Maratyszcza The last compilation I tried was 3 days ago, but somehow CMake scripts downloaded XNNPACK at https://github.com/google/XNNPACK/commit/d605aa76904aa1c5ea98bb7a0a4bafc4886b8a0a, which was 6 days ago (The fix you mention was 5 days ago). Do you know how to force using latest XNNPACK?", "I wonder if `pip install tflite-runtime` comes with XNNPACK delegate for aarch64, which can be a workaround for now.", "XNNPACK revision used in the build is specified [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/cmake/modules/xnnpack.cmake#L26). You'd probably need to remove all configuration artifacts and reconfigure TFLite after updating this line.\r\n\r\n", "@Maratyszcza Thanks for your information! I pulled the latest Tensorflow which has a newer XNNPACK with the patch you mentioned previously. It can finish compilation without errors.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54437\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54437\">No</a>\n"]}, {"number": 54436, "title": "Using `+` in custom residual Keras `Layer` does not create correct model graph", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.15.6\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): v2.8.0-rc1-32-g3f878cff5b6 2.8.0\r\n- Python version: 3.7.12\r\n\r\nAlso reproducible on various hosted Jupyter environments (Kaggle, Colab) with and without GPU.\r\n\r\n**Describe the current behavior**\r\n\r\nIn custom residual block implementation with keras APIs, `+` yields broken graph.\r\n\r\n```python\r\ndef call(self, x, training=False):\r\n    for block in self.blocks:\r\n        h = x\r\n        for conv in block:\r\n            h = conv(h, training=training)\r\n\r\n        x = x + h\r\n\r\n    return x\r\n```\r\n\r\nThis is the resulting graph:\r\n![res-block-broken](https://user-images.githubusercontent.com/7884451/154602227-346a939a-275f-4508-84ff-8b0163e5ac13.png)\r\n\r\n**Describe the expected behavior**\r\n\r\nThis code produces the correct graph, where each `add` is a separate instance of `keras.layers.Add`.\r\nThe `+` operator should produce the same graph.\r\n\r\n```python\r\ndef call(self, x, training=False):\r\n    for block, add in zip(self.blocks, self.adds):\r\n        h = x\r\n        for conv in block:\r\n            h = conv(h, training=training)\r\n\r\n        x = add([x, h])\r\n\r\n    return x\r\n```\r\n![res-block-working](https://user-images.githubusercontent.com/7884451/154602398-cd61f319-cfee-4bcc-89a3-b1b50e49109d.png)\r\n\r\nI will add that this isn't just a visualization issue. My model would not train until after I identified this problem and applied the fixed implementation described above. This was causing serious issues with my gradients and the model could not learn because it was too deep without the skip connections.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): I would consider it, but leaning towards no.\r\n- Briefly describe your candidate solution(if contributing): N/A\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nfrom typing import Optional, Tuple, Union\r\n\r\nimport tensorflow as tf\r\nimport tensorflow.nn\r\nimport tensorflow.keras.backend as K\r\nimport tensorflow.keras.layers as L\r\nfrom tensorflow import keras\r\n\r\ndef plot_model(model, shape):\r\n    inputs = keras.Input(shape[1:])\r\n    ones = tf.ones(shape)\r\n    model(ones)  # I think needed to properly init graph for plotting\r\n    outputs = model.call(inputs)\r\n    wrapped_model = keras.Model(inputs, outputs)\r\n    return tensorflow.keras.utils.plot_model(\r\n        wrapped_model, expand_nested=True, show_shapes=True)\r\n\r\nclass ConvBnAct(L.Layer):\r\n\r\n    def __init__(\r\n        self,\r\n        out_channels: int,\r\n        kernel_size: Union[int, Tuple[int]],\r\n        stride: Union[int, Tuple[int]],\r\n        activation: Optional[str] = 'swish',\r\n        use_bias=False,\r\n        use_batch_norm=True,\r\n        data_format='channels_last'\r\n            ):\r\n        super().__init__()\r\n\r\n        self.out_channels = out_channels\r\n        self.kernel_size = kernel_size\r\n        self.stride = stride\r\n        self.use_bias = use_bias\r\n        self.data_format = data_format\r\n\r\n        self.activation = L.Activation(activation)\r\n        self.act_type = activation\r\n\r\n        bn_axis = 1 if data_format == 'channels_first' else -1\r\n        self.batch_norm = L.BatchNormalization(\r\n            axis=bn_axis) if use_batch_norm else None\r\n\r\n    def build(self, input_shape):\r\n        self.conv = L.Conv2D(\r\n            self.out_channels,\r\n            self.kernel_size,\r\n            input_shape=input_shape[1:],\r\n            padding='same',\r\n            strides=self.stride,\r\n            activation=None,\r\n            use_bias=self.use_bias,\r\n            data_format=self.data_format,\r\n            )\r\n\r\n    def call(self, inputs, training=False):\r\n        x = self.conv(inputs)\r\n\r\n        if self.batch_norm:\r\n            x = self.batch_norm(x, training=training)\r\n\r\n        if self.activation:\r\n            x = self.activation(x)\r\n\r\n        return x\r\n\r\nclass ResBlock(L.Layer):\r\n\r\n    def __init__(\r\n        self,\r\n        blocks: int,\r\n        shortcut=True,\r\n        data_format='channels_last'\r\n    ):\r\n        super().__init__()\r\n        self.n_blocks = blocks\r\n        self.shortcut = shortcut\r\n        self.data_format = data_format\r\n\r\n    def build(self, input_shape):\r\n        channel_axis = 1 if self.data_format == 'channels_first' else -1\r\n        channels = input_shape[channel_axis]\r\n\r\n        self.blocks = []\r\n        for i in range(self.n_blocks):\r\n            block = [\r\n                ConvBnAct(channels, kernel_size=1, stride=1, data_format=self.data_format),\r\n                ConvBnAct(channels, kernel_size=3, stride=1, data_format=self.data_format)\r\n                ]\r\n            self.blocks.append(block)\r\n\r\n    def call(self, x, training=False):\r\n        for block in self.blocks:\r\n            h = x\r\n            for conv in block:\r\n                h = conv(h, training=training)\r\n\r\n            x = x + h if self.shortcut else h\r\n\r\n        return x\r\n\r\nif __name__ == '__main__':\r\n    i = keras.Input((24, 24, 3))\r\n    r = ResBlock(2, True)\r\n    plot_model(r, (1, 24, 24, 3))\r\n```\r\n", "comments": ["@jacoblubecki I guess you cannot add layers using the '+' operator, as I think it would be confusing if that was allowed", "@Cheril311 Sorry if it was a bit unclear -- the `+` operator is being used to add the output tensors of the layers (_NOT_ adding the layers themselves).\r\n\r\nBoth versions of the code work during the forward pass. When setting random seeds and `TF_DETERMINISTIC_OPS=1`, _**the outputs of each version are identical**_.\r\n\r\nProblems only appear during training because the gradients are not computed correctly for the `+`-version due to the malformed computation graph, and the training problems are entirely non-obvious because the backwards pass executes and the model's weights do get updated, but the model fails to converge because the gradients aren't correct.", "@jacoblubecki \r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\nThank you!", "@sushreebarsa I have filed a ticket here:  https://github.com/keras-team/keras/issues/16098\r\n\r\nI replaced my `ConvBnAct` with the following:\r\n```python\r\nclass ConvBnAct(object):\r\n\r\n    def __init__(\r\n        self,\r\n        out_channels: int,\r\n        kernel_size: Union[int, Tuple[int]],\r\n        stride: Union[int, Tuple[int]],\r\n        activation: Optional[str] = 'swish',\r\n        use_bias=False,\r\n        use_batch_norm=True,\r\n        data_format='channels_last'\r\n            ):\r\n        super().__init__()\r\n        self.kernel = tf.ones((kernel_size, kernel_size, 3, 3))\r\n\r\n    def __call__(self, inputs, training=False):\r\n        x = tf.nn.conv2d(inputs, self.kernel, strides=1, padding='SAME')\r\n        return x\r\n```\r\n\r\nThen I used tensorboard to trace two versions of `ResBlock`.\r\n\r\nKeras version using only `tensorflow` APIs (still inherits from Keras `Layer`, but otherwise only using `tf` namespace):\r\n```python\r\nclass ResBlock(L.Layer):\r\n\r\n    def __init__(self, blocks: int, data_format='channels_last'):\r\n        super().__init__()\r\n        self.n_blocks = blocks\r\n\r\n        self.blocks = []\r\n        for i in range(self.n_blocks):\r\n            block = [\r\n                ConvBnAct(3, kernel_size=1, stride=1, data_format=self.data_format),\r\n                ConvBnAct(3, kernel_size=3, stride=1, data_format=self.data_format)\r\n                ]\r\n            self.blocks.append(block)\r\n\r\n    def call(self, x, training=False):\r\n        for block in self.blocks:\r\n            h = x\r\n            for conv in block:\r\n                h = conv(h, training=training)\r\n\r\n            x = x + h\r\n\r\n        return x\r\n```\r\nAnd pure tensorflow version (with no reference to any Keras APIs):\r\n```python\r\nclass ResBlock(object):\r\n\r\n    def __init__(self, blocks: int, data_format='channels_last'):\r\n        super().__init__()\r\n        self.n_blocks = blocks\r\n\r\n        self.blocks = []\r\n        for i in range(self.n_blocks):\r\n            block = [\r\n                ConvBnAct(3, kernel_size=1, stride=1, data_format=data_format),\r\n                ConvBnAct(3, kernel_size=3, stride=1, data_format=data_format)\r\n                ]\r\n            self.blocks.append(block)\r\n\r\n    def __call__(self, x, training=False):\r\n        for block in self.blocks:\r\n            h = x\r\n            for conv in block:\r\n                h = conv(h, training=training)\r\n\r\n            x = x + h\r\n\r\n        return x\r\n```\r\n\r\nThe Keras version is still broken on tensorboard, but the pure version with no Keras APIs appears to work fine.", "@jacoblubecki Could you please let us know if we can move this issue to closed status as we are addressing the other ticket in  [keras-team/keras repo.](https://github.com/keras-team/keras/issues) ?\r\nThanks!", "@sushreebarsa Sure thing, thanks for the redirect.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54436\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54436\">No</a>\n"]}]