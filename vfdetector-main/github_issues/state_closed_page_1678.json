[{"number": 2563, "title": "Fix python3 incompatibility in encode_audio_op_test.py", "body": "This fixes test failure as in: http://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_CONTAINER_TYPE=CPU,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=cpu-slave/108/console\nChange: 123507798\n", "comments": []}, {"number": 2562, "title": "Fixed up include paths for iOS example", "body": "", "comments": []}, {"number": 2561, "title": "Added iOS example", "body": "This change adds in an example of using the static library generated by the makefile in an iOS app, and allows optimization flags to be passed to the makefile on the command line.\n", "comments": ["Good to see tensorflow finally supported on iOS (#16), I wonder what does this PR mean for #1631?\n", "I'm receiving this error running tensorflow/contrib/makefile/compile_ios_tensorflow.sh.\n\n~/Developer/libs/tensorflow/tensorflow/contrib/makefile/gen/lib/libtensorflow-core-armv7.a -arch armv7 -miphoneos-version-min=9.2 -Xlinker -S -Xlinker -x -Xlinker -dead_strip -all_load -L~/Developer/libs/tensorflow/tensorflow/contrib/makefile/gen/protobuf_ios/lib -lz -lstdc++ -lprotobuf -lm\nld: internal error: atom not found in symbolIndex(__ZN10tensorflow10RunOptions15set_trace_levelENS_21RunOptions_TraceLevelE) for architecture armv7\n\nAny idea of what's wrong?\n", "What version of Xcode are you running? I've seen that with versions before 7.3.\n", "XCode Version 7.2 (7C68). I'll try updating. Thanks!\n"]}, {"number": 2560, "title": "New Feature: 4D Tensor Input for LSTM RNN Neural Network", "body": "Currently TF RNN LSTM accepts this tensor rank:   (ht, mt)  of rank 1\n\n```\n (ht_o, mt_o) = LSTM(ht, mt, W)\n with   ht size is (1 to d)   ,  same for mt\n```\n\nWould it be possible to accept rank 2 or rank 3 for the input state of RNN LSTM\n    ht size  (1 to m, 1 to s, 1 to d)      : Rank 3 \n\nIt would allows Multi-dimensionnal input such as :\n    Input  size  (NSample, Mtype, Nchar, Nsequence) \n\nEspecially, it makes sense for Grid LSTM.\n", "comments": ["RNNs are being reworked,\nyou're better off implementing them yourself in tensorflow (just follow one of the many github tutorials),\nautomatic differentiation will take care of computing the correct gradients.\n", "@Nessphoro that answer is incorrect.  RNNs are a stable API.\n\nWe will add support for higher rank inputs and nested tuples of inputs and outputs in a backwards compatible way in the future (i can't give estimates, but imagine it'll happen this summer sometime)\n", "@ebrevdo I've though that some work was being done to change how RNNs and LSTMs work because of https://github.com/tensorflow/tensorflow/blob/82ff4cd8b0d541ede107d34d8eecc769c91dda11/tensorflow/models/rnn/rnn_cell.py\nAnd the stale documentation in master with regards to that.\n\nAnd in fact this commit, by you.\nhttps://github.com/tensorflow/tensorflow/commit/396b586d7a172bac9c37343528764add7a84ea86\n", "We just moved the code into tf.nn.rnn_cell.*, tf.nn.rnn, tf.nn.dynamic_rnn,\ntf.nn.seq2seq, etc.\n\nOn Wed, Jun 1, 2016 at 3:45 PM, Pavlo Malynin notifications@github.com\nwrote:\n\n> @ebrevdo https://github.com/ebrevdo I've though that some work was\n> being done to change how RNNs and LSTMs work because of\n> https://github.com/tensorflow/tensorflow/blob/82ff4cd8b0d541ede107d34d8eecc769c91dda11/tensorflow/models/rnn/rnn_cell.py\n> And the stale documentation in master with regards to that.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2560#issuecomment-223147293,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/ABtim9pCTcs7xW6mp6aIzgU2UzsdRlCMks5qHguPgaJpZM4IpHF0\n> .\n", "Thanks!\n\nThe docs on the tutorial for seq2seq learning refer to the models/rnn namespace still.\n", "@ebrevdo: Feel free to reassign or mark this contributions welcome if that's best. \n", "This is being worked on internally.  I'll see if we have an external github account for the contributor.\n", "@kosklain Here you go! \n", "@arita297 Hey, the changes are now in (more concretely, [this commit](https://github.com/tensorflow/tensorflow/commit/b5ad739fe9939e953416636ae8e188a5feacbcdf)). As you can see in the docs, you can do what you want by specifying objects that inherit from RNNCell (just like your Grid LSTM would), and then the `output_size` and `state_size` properties can specify multi-dimensional sizes by using tensorflow TensorShapes.\n\nSo if you have\n`(output, next_state) = YourCell(input, prev_state)`\nsuch that output is size `(o1,o2,o3)`, and next_state is `(h, c)` with both `h` and `c` having size `(s1, s2, s3)`, then your properties should look like:\n\n```\n @property\n def state_size(self):\n   return (tf.TensorShape((s1, s2, s3)), tf.TensorShape((s1, s2, s3))\n\n@property\ndef output_size(self):\n  return tf.TensorShape((o1, o2, o3))\n```\n\nWith that, you should be able to operate with multidimensional input/output/states.\n", "@kosklain Should we close the issue, or is there more to do?\n", "@girving this can now be closed, with that commit the feature should be currently fully supported by all rnn modules that exist in `ops/rnn.py`.\n", "@kosklain : Sure, sounds good.\nShall we update from the master branch ?\n   -it b.gcr.io/tensorflow/tensorflow\n", "@arita297 I believe docker will only get you at most the 0.8 release. To get the bleeding-edge master branch you should download the sources and compile.\n", "Hello,\n\n@kosklain : Ok, Sure\n\nDoes 0.10c release now contains this ?\n\nhttps://www.tensorflow.org/versions/r0.10\n\nThanks\n"]}, {"number": 2559, "title": "Need binary release with CUDA 8.0 support", "body": "https://developer.nvidia.com/cuda-release-candidate-download\n", "comments": ["so far have this problem with build \n`ERROR: /tensorflow/tensorflow/stream_executor/BUILD:5:1: C++ compilation of rule '//tensorflow/stream_executor:stream_executor' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object ... (remaining 100 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\ntensorflow/stream_executor/cuda/cuda_blas.cc: In member function 'virtual bool perftools::gputools::cuda::CUDABlas::DoBlasGemm(perftools::gputools::Stream*, perftools::gputools::blas::Transpose, perftools::gputools::blas::Transpose, tensorflow::uint64, tensorflow::uint64, tensorflow::uint64, float, const perftools::gputools::DeviceMemory<Eigen::half>&, int, const perftools::gputools::DeviceMemory<Eigen::half>&, int, float, perftools::gputools::DeviceMemory<Eigen::half>*, int)':\ntensorflow/stream_executor/cuda/cuda_blas.cc:1683:22: error: 'CUBLAS_DATA_HALF' was not declared in this scope\n       CUDAMemory(a), CUBLAS_DATA_HALF, lda,\n                      ^\n`\n", "#2556 fixes this issue\n", "Building with bazel with GPU support still gives the same error for CUDA 8.0\n", "@kashif - :thumbsup:  Temporarily adding your changes in source (`master` branch) seems to work for CUDA 8.0 per this command succeeding.\n\n```\nbazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\n```\n\nI needed to upgrade bazel as well, but that's just an FYI.\n", "@peyush - not sure why it's failing for you.\n", "#2614 is merged on master. We will cherry pick it for 0.9 tomorrow and build the artifacts...\n", "Ran into an error very near the end of the build:\n\nERROR: /Users/Peace/Projects/external/tensorflow/tensorflow/contrib/session_bundle/example/BUILD:38:1: Executing genrule //tensorflow/contrib/session_bundle/example:half_plus_two failed: bash failed: error executing command\n  (cd /private/var/tmp/_bazel_Peace/25fdd91e698016ec32416f69e3fa9b23/execroot/tensorflow && \\\n  exec env - \\\n    PATH=/Library/Frameworks/Python.framework/Versions/3.5/bin:/Developer/NVIDIA/CUDA-8.0/bin:/opt/local/bin:/opt/local/sbin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/opt/local/lib/:/Developer/NVIDIA/CUDA-8.0/bin/ \\\n    TMPDIR=/var/folders/0j/lmh6kxcx54s8b1cxcxb15n8w0000gn/T/ \\\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; rm -rf /tmp/half_plus_two; /Users/Peace/Projects/venvs/deep/bin/python bazel-out/host/bin/tensorflow/contrib/session_bundle/example/export_half_plus_two; cp -r /tmp/half_plus_two/\\* bazel-out/local_darwin-opt/genfiles/tensorflow/contrib/session_bundle/example/half_plus_two'): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 245.\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.8.0.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.5.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.8.0.dylib locally\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nINFO: Elapsed time: 2067.414s, Critical Path: 2055.33s\n\nPython throws open a window saying it quit unexpectedly, and displays a python path that is different than the one I provided to the ./configure script. So perhaps numpy or some other requirement is not being satisfied (Edit: I checked the other path and numpy does exist there also). Any ideas on how I can fix this?\n\nBazel version: bazel release 0.2.3-2016-06-02 (@c728a63)\n", "@jendap @jstaker7 did this get resolved? will CUDA 8 be supported when installing via pip?\n", "@Sohojoe Thanks for the followup. Not yet. \n\nExpected output during compilation:\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n\nWhat I get:\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\n\nSo somehow it's failing when trying to open libcuda.so.1and no other errors are outputted, so it is hard to debug. I am using OS X, so perhaps that's the problem; things might now be playing nice yet in that regard.\n", "@jendap @jstaker7 Any update on this? According to #2614 this has been resolved?\n", "That bug was a duplicate of this one, it hasn't been resolved.  Basically we either decide to build additional PIPs for an unsupported release candidate, or we close this bug and tell people they can build from sources (which they sort of can, modulo a bunch of bazel-related issues that are being resolved).\n", "Building TF from source with CUDA 8 currently fails in numerous ways, not all of which have workarounds.\n\nhttps://github.com/tensorflow/tensorflow/issues/4105\n\nhttps://github.com/tensorflow/tensorflow/issues/4190\n\nhttps://github.com/tensorflow/tensorflow/issues/4214\n\nIf anyone has a recipe for how to successfully complete the compilation from source, it'd be fantastic. The environment I'm thinking is along the lines of \"use some popular Linux distribution with default package versions and add CUDA 8 to it\". E.g.:\n- CUDA 8.0 RC + the gcc patch\n- cuDNN 5.1\n- nvidia-driver 367 or 370-beta\n- Ubuntu 16.04\n- python 2.7\n\nA decent set of enabled compute capabilities, up to and including 6.1 (for the new Pascal GPUs) would be great.\n", "I followed section 6 of this [post](http://www.computervisionbytecnalia.com/en/2016/06/deep-learning-development-setup-for-ubuntu-16-04-xenial/) and succeeded building tensorflow with CUDA 8 and cuDNN 5 under Ubuntu 16.04 and Python 2.7,  in the second try (I forgot what went wrong in the first try. It might be that something was not right during setup during ./configure phase. If certain combination of configure parameters fails, try cuDNN 5 instead of cuDNN 5.1). You will receive numerous warnings during compilation and installation but eventually importing tensorflow into python works...\n\nP.S. I did not follow any other setup procedure in that post, but from the headlines of each section in that post, my setup is probably similar to what is said in the post.\n", "http://wp.me/p7GvOc-2H  Tutorial I made for building from Sources with Ubuntu 16.04 and Cuda 8.0 RC w/ cuDNN 5.1\n", "All the compilation failures I've seen before only happen on the master branch. None of these issues persists when I switch to the r0.10 branch. Last night I was able to complete a build, Ubuntu 16.04, CUDA 8.0 RC + the compiler patch, cuDNN 5.1, nvidia-driver-370, python-2.7, and compute capability 6.1 (for Pascal GPU) - as soon as I switched to r0.10.\n", "@FlorinAndrei Congratulations on that build! \ud83d\udc4d Can you share your choices for `./configure` and any other details that you found tricky?\n\nWhat GPU hardware are you using? I noticed you are using the 370 Nvidia driver.\n\nAs for me:\n- GeForce GTX 1080\n- Nvidia Driver: 367.44 driver\n- Ubuntu 16.04\n- CUDA 8.0 RC + the compiler patch\n- cuDNN 5.1.5\n\nI have gotten my Nvidia driver, CUDA, and cuDNN to build. That wasn't obvious at all. But I haven't got my TensorFlow build to work yet. \n", "@xpe - Clone this repo https://github.com/FlorinAndrei/ml-setup and checkout the ubuntu1604 branch. Then just follow the README and run the Ansible playbooks.\n\nThe overview is this (look at the playbooks to fill in details such as extra packages to install):\n- install Ubuntu 16.04\n- install linux-headers, dkms, build-essentials, python development packages\n- do a dist-upgrade\n- enable ppa:graphics-drivers/ppa\n- download and install CUDA 5, the ggc patch, and cuDNN, all from the runfiles\n  - sh {{ packages_folder }}/{{ cuda_file }} --silent --toolkit --samples --verbose --override\n  - sh {{ packages_folder }}/{{ cuda_patch1_file }} --silent --accept-eula\n  - do the manual install for cuDNN\n- create CUDA_HOME and other env vars per CUDA docs\n- apt-get install nvidia-370 (this must happen after CUDA; 370 is now the stable version of the Linux driver, no need to go back to 367)\n- install Java from ppa:webupd8team/java and Bazel from http://storage.googleapis.com/bazel-apt per official docs\n\nThis is where the Ansible playbooks stop. You could just run Ansible and it will take you this far. I have not automated the steps after that.\n\nThe manual process after that is:\n- clone TensorFlow repo, checkout the r0.10 branch (**this is the crucial step**; the master branch keeps failing, whereas r0.10 compiled on first try)\n- run ./configure and accept default values except:\n  - enable GPU\n  - CUDA version must be: 8.0 (autodetect doesn't work)\n  - compute capability: whatever it is that your card supports\n- bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\n- bazel-bin/tensorflow/tools/pip_package/build_pip_package $HOME\n\nAnd that should be it.\n\nI've tested some pretty complex models and I have not seen any strange failures this far.\n", "@FlorinAndrei Excellent work! Thank you!\n\nFor others: note that the [Nvidia drivers page](http://www.geforce.com/drivers) does not necessarily list results in chronological order. The first time around, I missed the latest driver. Like Florin referred to above, at the current time, 370.28 is the latest stable driver for many of the Geforce product line:\n\n> http://www.geforce.com/drivers/results/107408\n> Linux x64 (AMD64/EM64T) Display Driver\n> Version   370.28\n> Release Date  Thu Sep 08, 2016\n", "Does anyone successed install TensorFlow with following combination?\n- Ubuntu 16.04 (Just newly installed and apt upgraded)\n- Nvidia Driver: nvidia-370 (sudo apt install nvidia-370)\n- JAVA 8 and Bazel 0.3.1 (install from official package):\n\n```\nchmod +x bazel-0.3.1-installer-linux-x86_64.sh\n./bazel-0.3.1-installer-linux-x86_64.sh --user\n```\n- gcc: 5.4.0 (default with 16.04)\n- CUDA 8.0.44 (for GTX 1070, installed from official .run file without driver)\n\n```\nexport CUDA_HOME=/usr/local/cuda-8.0\nexport LD_LIBRARY_PATH=\"$CUDA_HOME/lib64:$LD_LIBRARY_PATH\"\nexport PATH=\"$CUDA_HOME/bin:$PATH\"\n```\n- cuDNN 5.1.5 (installed from official package)\n- tensorflow master branch (clone from official github)\n- python 2 and python 3 (install from ubuntu official source)\n\nThen run the tensorflow/configure:\n\n```\n./configure\nPlease specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] N\nNo Google Cloud Platform support will be enabled for TensorFlow\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] \nNo Hadoop File System support will be enabled for TensorFlow\nFound possible Python library paths:\n  /usr/local/lib/python2.7/dist-packages\n  /usr/lib/python2.7/dist-packages\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\n/usr/lib/python3/dist-packages\nDo you wish to build TensorFlow with GPU support? [y/N] y\nGPU support will be enabled for TensorFlow\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \nPlease specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 5.1.5\nPlease specify the location where cuDNN 5.1.5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\nPlease note that each additional compute capability significantly increases your build time and binary size.\n[Default is: \"3.5,5.2\"]: \n```\n\nDone with\n\n```\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\n.\nINFO: All external dependencies fetched successfully.\nConfiguration finished\n```\n\nand I run the building:\n\n```\nbazel build -c opt --config=cuda\n```\n\nsome error occured immediately:\n\n```\nERROR: /home/devymex/.cache/bazel/_bazel_devymex/05af4cc48fb50d1cc8f7e879f4c1ce83/external/local_config_cuda/crosstool/BUILD:4:1: Traceback (most recent call last):\n    File \"/home/devymex/.cache/bazel/_bazel_devymex/05af4cc48fb50d1cc8f7e879f4c1ce83/external/local_config_cuda/crosstool/BUILD\", line 4\n        error_gpu_disabled()\n    File \"/home/devymex/.cache/bazel/_bazel_devymex/05af4cc48fb50d1cc8f7e879f4c1ce83/external/local_config_cuda/crosstool/error_gpu_disabled.bzl\", line 3, in error_gpu_disabled\n        fail(\"ERROR: Building with --config=c...\")\nERROR: Building with --config=cuda but TensorFlow is not configured to build with GPU support. Please re-run ./configure and enter 'Y' at the prompt to build with GPU support.\n...\n```\n\nI've tried to:\n-     reinstall Ubuntu, many times;\n-     reinstall Bazel (from 3party source), many times too.\n-     try different Bazel edition (0.3.1, 0.3.0)\n-     change to the older CUDA edition 8.0.27 (CUDA 7 doesn't support GTX 1070)\n-     try different cuda paths \"/usr/local/cuda\" and \"/usr/local/cuda-8.0\" (all of them are exist)\n-     re-run ./configure many times and re-run bazel build -c opt --config=cuda\n-     run \"bazel clean\" before re-run ./configure\n-     re-download offical tensorflow package or re-clone official tensorflow github repo\n-     delete path ~/.config/bazel before ./configure\n-     python2 and python3\n-     I've tried all possible action that may related to this error.\n\nbut the error always occured and with identical message.\n\nI've try to use tensorflow r0.10 package (download from git hub), the error message were became to:\n\n```\nERROR: no such package '@local_config_cuda//crosstool': BUILD file not found on package path.\nERROR: no such package '@local_config_cuda//crosstool': BUILD file not found on package path.\n```\n\nwith the tensorflow r0.10, above tried actions has no effect to these two error message.\nI'm so frustrated after four days failure. Does someone could help me? Thanks!\n", "@devymex I have the exact same problem (symptoms / errors), just on CentOS 7.2\n", "@leezu I also tried to edit the ./tensorflow/tools/baze.rc, commented the following line:\n`#build:cuda --crosstool_top=@local_config_cuda//crosstool:toolchain`\nthen run the building command and it began to compile, but after a while it stucked in another error:\n\n```\nERROR: /home/devymex/Software/tensorflow/tensorflow/stream_executor/BUILD:5:1: C++ compilation of rule '//tensorflow/stream_executor:stream_executor' failed: gcc failed: error executing command \n  (cd /home/devymex/.cache/bazel/_bazel_devymex/05af4cc48fb50d1cc8f7e879f4c1ce83/execroot/tensorflow && \\\n  exec env - \\\n    LD_LIBRARY_PATH=/usr/local/cuda/lib64 \\\n    PATH=/usr/local/cuda-8.0/bin:/home/devymex/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \\\n```\n\nI'm absolutely devastated.\n", "Update: We're actively working on CUDA 8.0 support. One potential blocker so far is #4931.\n", "@jart: FWIW, CUDA 8.0 has been working fine for us with TF r0.10 and I just tried r0.11, it seems fine too.\nThis is my fixes on the official Dockerfile:\n\n```\ndiff --git a/tensorflow/tools/docker/Dockerfile.devel-gpu b/tensorflow/tools/docker/Dockerfile.devel-gpu\nindex b4dc923..771838c 100644\n--- a/tensorflow/tools/docker/Dockerfile.devel-gpu\n+++ b/tensorflow/tools/docker/Dockerfile.devel-gpu\n@@ -1,4 +1,4 @@\n-FROM nvidia/cuda:7.5-cudnn5-devel\n+FROM nvidia/cuda:8.0-cudnn5-devel\n\n MAINTAINER Craig Citro <craigcitro@google.com>\n\n@@ -87,8 +87,7 @@ WORKDIR /tensorflow\n # Configure the build for our CUDA configuration.\n ENV CUDA_PATH /usr/local/cuda\n ENV CUDA_TOOLKIT_PATH /usr/local/cuda\n-ENV CUDNN_INSTALL_PATH /usr/lib/x86_64-linux-gnu\n-ENV LD_LIBRARY_PATH /usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\n+ENV LD_LIBRARY_PATH /usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH\n ENV TF_NEED_CUDA 1\n ENV TF_CUDA_COMPUTE_CAPABILITIES=3.0,3.5,5.2\n```\n", "0.11rc1 has CUDA 8 and was posted in official downloads last Friday :\nhttps://www.tensorflow.org/versions/r0.11/get_started/os_setup.html\n\nOn Sun, Oct 23, 2016 at 1:38 PM, Steven Shi notifications@github.com\nwrote:\n\n> Any updates on this?\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2559#issuecomment-255612754,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AABaHGb9KX9KNN9InXf-JcvvUEpT2Vc3ks5q28WvgaJpZM4IpGKc\n> .\n", "@yaroslavvb  solved, works.\nfinally, thanks. \n", "@devymex @leezu  \n\ntry \n$export TF_NEED_CUDA=1\n$./configure\n"]}, {"number": 2558, "title": "Push changes from internal: 123496073", "body": "No merge conflict.\n", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "ready to merge\n"]}, {"number": 2557, "title": "added Install instructions for OS X GPU", "body": "for issue #2522\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks, this is helpful! Will assign to @martinwicke since I think he's setting up the Mac OS X GPU test machine and this should be useful to validate the instructions :)\n", "I have also a pretty good list of steps to do to install on macosx: https://gist.github.com/Mistobaan/dd32287eeb6859c6668d that could be integrated. \n", "@Mistobaan yes there is a newer cuda toolkit that fixes the xcode issue. I am just about to make a cask pull-request to update cuda, after which there will be no need to downgrade xcode etc.\n", "@Mistobaan ok i spoke too soon, seems like some kind soul has already done that... so no need to downgrade xcode \ud83c\udf89 \n", "BTW, I just tried these instructions after upgrading MacOS and TensorFlow to today's head and they don't seem to work. Something about libcudart not being loaded because I'm running a \"restricted program\". Sudo doesn't help\n\n```\nbash-3.2$ echo $DYLD_LIBRARY_PATH\n/usr/local/cuda/lib\n\nbash-3.2$ find /usr/local/cuda -name libcudart.7.5.dylib\n/usr/local/cuda/lib/libcudart.7.5.dylib\n\nbash-3.2$ bazel-bin/tensorflow/python/session_ops_test\n\ndyld: warning, LC_RPATH $ORIGIN/../../_solib_darwin/_U_S_Sthird_Uparty_Sgpus_Scuda_Ccudart___Uthird_Uparty_Sgpus_Scuda_Slib in /Users/yaroslavvb/tfimmediate_fresh.gpu/tensorflow/_python_build/tensorflow/python/_pywrap_tensorflow.so being ignored in restricted program because it is a relative path\ndyld: warning, LC_RPATH third_party/gpus/cuda/lib in /Users/yaroslavvb/tfimmediate_fresh.gpu/tensorflow/_python_build/tensorflow/python/_pywrap_tensorflow.so being ignored in restricted program because it is a relative path\ndyld: warning, LC_RPATH third_party/gpus/cuda/extras/CUPTI/lib in /Users/yaroslavvb/tfimmediate_fresh.gpu/tensorflow/_python_build/tensorflow/python/_pywrap_tensorflow.so being ignored in restricted program because it is a relative path\nTraceback (most recent call last):\n  File \"<console>\", line 1, in <module>\n  File \"/Users/yaroslavvb/tfimmediate_fresh.gpu/tensorflow/_python_build/tensorflow/__init__.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/Users/yaroslavvb/tfimmediate_fresh.gpu/tensorflow/_python_build/tensorflow/python/__init__.py\", line 48, in <module>\n    from tensorflow.python import pywrap_tensorflow\n  File \"/Users/yaroslavvb/tfimmediate_fresh.gpu/tensorflow/_python_build/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\n    _pywrap_tensorflow = swig_import_helper()\n  File \"/Users/yaroslavvb/tfimmediate_fresh.gpu/tensorflow/_python_build/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\nImportError: dlopen(/Users/yaroslavvb/tfimmediate_fresh.gpu/tensorflow/_python_build/tensorflow/python/_pywrap_tensorflow.so, 10): Library not loaded: @rpath/libcudart.7.5.dylib\n  Referenced from: /Users/yaroslavvb/tfimmediate_fresh.gpu/tensorflow/_python_build/tensorflow/python/_pywrap_tensorflow.so\n  Reason: image not found\n\n\n```\n", "@yaroslavvb can you try `xcode-select --install` and then try again?\n", "Yes, tried that, restarted, same issue.\n\nOn Wed, Jun 1, 2016 at 10:14 AM, Dr. Kashif Rasul notifications@github.com\nwrote:\n\n> @yaroslavvb https://github.com/yaroslavvb can you try xcode-select\n> --install and then try again?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/2557#issuecomment-223061679,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AABaHCqorwoJnQtIockTMRBWMr32Nqsfks5qHb3cgaJpZM4IpDEQ\n> .\n", "Seems like it's possibly coming from this line in ImageLoader. Somehow\n\"context.processIsRestricted\" is set when I'm running with config=cuda\nhttp://opensource.apple.com//source/dyld/dyld-210.2.3/src/ImageLoaderMachO.cpp\n\nif ( context.processIsRestricted  && (context.mainExecutable == this) ) {\n                        dyld::warn(\"LC_RPATH %s in %s being ignored in restricted\nprogram because of @loader_path\\n\", path, this->getPath());\n                        break;\n                    }\n\nOn Wed, Jun 1, 2016 at 11:54 AM, Yaroslav Bulatov yaroslavvb@gmail.com\nwrote:\n\n> Yes, tried that, restarted, same issue.\n> \n> On Wed, Jun 1, 2016 at 10:14 AM, Dr. Kashif Rasul <\n> notifications@github.com> wrote:\n> \n> > @yaroslavvb https://github.com/yaroslavvb can you try xcode-select\n> > --install and then try again?\n> > \n> > \u2014\n> > You are receiving this because you were mentioned.\n> > Reply to this email directly, view it on GitHub\n> > https://github.com/tensorflow/tensorflow/pull/2557#issuecomment-223061679,\n> > or mute the thread\n> > https://github.com/notifications/unsubscribe/AABaHCqorwoJnQtIockTMRBWMr32Nqsfks5qHb3cgaJpZM4IpDEQ\n> > .\n", "@yaroslavvb yes seems like its part of OS X's System Integrity Protection (SIP) feature... wondering why it works for me...\n", "@yaroslavvb how did you install the cuda toolkit?\n", "Using brew. I'll try disabling this SIP feature and update in few hours\nOn Jun 1, 2016 12:35 PM, \"Dr. Kashif Rasul\" notifications@github.com\nwrote:\n\n> @yaroslavvb https://github.com/yaroslavvb how did you install the cuda\n> toolkit?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/2557#issuecomment-223100791,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AABaHJMg-DxumqCiQEA8aUk9QcTMav5bks5qHd7ygaJpZM4IpDEQ\n> .\n", "@yaroslavvb i installed via the nvidia installer and my \n\n``` bash\n$ csrutil status\nSystem Integrity Protection status: enabled.\n```\n", "Hm, after disabling SIP I'm seeing this [segfault](http://pastebin.com/bbzsTMPq) in dylib, looks something's broken in trying to load those libs from relative path\n", "@kashif btw, how old is your TensorFlow version where you got this to work? (I've only tested this on version from this morning). Stack trace looks as if it's trying to read past end of string for DSO filename location. Looks like this logic got touched about a month ago:\nhttps://github.com/tensorflow/tensorflow/pull/664/files#diff-1e480f00c75a5f10cda6d37f7b6e3e3dR75\n", "@yaroslavvb my tensorflow is about 4 days old... i can merge in master and try to compile again...\n", "@kashif 4 days old is pretty recent, seems likely problem is somewhere else\n\nOn Wed, Jun 1, 2016 at 2:49 PM, Dr. Kashif Rasul notifications@github.com\nwrote:\n\n> @yaroslavvb https://github.com/yaroslavvb my tensorflow is about 4 days\n> old... i can merge in master and try to compile again...\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/2557#issuecomment-223135531,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AABaHKSzZAPkwO0RbcUmcOusJYt0Tg_Iks5qHf5xgaJpZM4IpDEQ\n> .\n", "I also tried to follow these instructions and got an error\n`\n(py35)Dans-iMac:tensorflow dan$ bazel build -c opt //tensorflow/tools/pip_package:build_pip_package\nERROR: /Users/dan/tensorflow/tensorflow/core/BUILD:87:1: //tensorflow/core:protos_all_py: no such attribute 'imports' in 'py_library' rule.\nERROR: /Users/dan/tensorflow/tensorflow/tools/pip_package/BUILD:23:1: Target '//tensorflow/core:framework_headers' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:build_pip_package'.\nERROR: Loading failed; build aborted.\nINFO: Elapsed time: 0.199s\n`\n\nI'd successfully been running the prebuilt binary, which I removed before starting these instructions.  \n", "Update, I installed CUDA from official website, ran ./configure again, rebuilt and now it works.\n", "OK, I believe this is an issue of environment variables, probably the `DYLD_LIBRARY_PATH`. I was testing things by running `bazel test` which spawns it's own environment and probably doesn't set that path correctly\n\nso this fails\n`bazel test -c opt --config=cuda tensorflow/python:depthwise_conv_op_test\n`\nbut then this passes\n`bazel-bin/tensorflow/python/depthwise_conv_op_test\n`\n", "We've had problems with environment variables in bazel test before. :/\n\nIt'll take me a while to actually get to installing the GPU test machine. I'll probably have an update then. For now, this looks good. Clarifications to the instructions are always welcome.\n", "OK, I just did SMC reset, which somehow reset my SIP and this stopped working with same `being ignored in restricted program because it is a relative path` message, and then after disabling SIP, it started working again. So disabling SIP seems to be a requirement for MacOS 10.11.5 (15F34)\n"]}, {"number": 2556, "title": "cuda 8 api uses cudaDataType for 16bit float type", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n", "@vrv, @martinwicke, I've confirmed that with this patch, TensorFlow would build with Cuda 8. Feel free to merge this change. We can always improve it later. \n", "test this please\n", "@martinwicke @caisq our Jenkins is broken   \n\nMerging since this change was verified by @zheng-xq\n", "@zheng-xq  would something like:\n\n``` c++\n#if CUDA_VERSION >= 8000\n#define TF_CUDA_DATA_HALF CUDA_R_16F\n#else\n#define TF_CUDA_DATA_HALF CUBLAS_DATA_HALF\n#endif\n```\n\nand then I use `TF_CUDA_DATA_HALF` in the `cublasSgemmEx()`?\n", "@kashif, thanks for getting to this quickly. \n\nPlease name it SE_CUDA_DATA_HALF, since this is stream-executor code, which TensorFlow only hosts a fork temporarily. Other than that, it looks fine. \n", "@tensorflow-jenkins test this please\n", "Thank you @kashif!\n", "thank you! @jendap \n"]}, {"number": 2555, "title": "cleans up warning/errors tensorflow/stream_executor ", "body": "- the delete variables are not used anywhere (why keep them around?)\n- tests seem to be running as before.\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n", "Looks ready to merge, @vrv?\n"]}, {"number": 2554, "title": "Virtualenv installation failure", "body": "Environment info\nOperating System: OS X 10.10.3\n\nWe followed the \"Download and Setup\" docs and tried to create a Virtualenv environment in the directory ~/tensorflow but it didn't work well.\n\n```\n$ virtualenv --system-site-packages ~/tensorflow\n\nUsing base prefix '/Applications/Canopy.app/appdata/canopy-1.7.2.3327.macosx-x86_64/Canopy.app/Contents'\nNew python executable in /Users/xxx/tensorflow/tensorflow/bin/python\ndyld: Library not loaded: @rpath/Python\n  Referenced from: /Users/xxx/tensorflow/tensorflow/bin/python\n  Reason: no suitable image found.  Did find:\n    /Users/xxx/tensorflow/tensorflow/bin/../Python: not a file\nERROR: The executable /Users/xxx/tensorflow/tensorflow/bin/python is not functioning\nERROR: It thinks sys.prefix is u'/Users/xxx/tensorflow' (should be u'/Users/xxx/tensorflow/tensorflow')\nERROR: virtualenv is not compatible with this system or executable\n```\n\nIn my execution environment, -p option is needed as below.\n\n```\n  $ virtualenv --system-site-packages -p /Library/Frameworks/Python.framework/Versions/3.5/bin/python3 ~/tensorflow\n```\n\nI think the result of the above command is influenced by the execution environment, so please add the -p option.\n", "comments": ["It seems like you must have installed Python 3 on your own before installing tensorflow. On my El Capitan machine I do not have /Library/Frameworks/Python.framework at all. Could you let me know how you did that so I can attempt to reproduce this. \n", "Closing for now, but please comment if more information is available and I'm happy to reopen.\n", "I came across this thread while trying to solve my issue. Checking to see if this has a solution. On my OS Sierra 10.12.3 machine, I have Conda 1.4.3 (python 3.x) installed. Thanks. ", "I have the same question with @jkjangles . On my OS Sierra 10.12.3 machine, I have Conda 4.3.16, python 3.5.2 installed. Thanks a lot. Has @jkjangles solved this problem?", "Yes, it is solved. install via conda.\n\nOn Tue, Apr 11, 2017 at 7:02 PM, yuanzhong90 <notifications@github.com>\nwrote:\n\n> I have the same question with @jkjangles <https://github.com/jkjangles> .\n> On my OS Sierra 10.12.3 machine, I have Conda 4.3.16, python 3.5.2\n> installed. Thanks a lot. Has @jkjangles <https://github.com/jkjangles>\n> solved this problem?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/2554#issuecomment-293424697>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AZV4WQtvvJXM8Ep-CMSgLmvPQuhiznntks5rvAabgaJpZM4IpAxL>\n> .\n>\n", "virtualenv --system-site-packages -p /System/Library/Frameworks/Python.framework/Versions/3.6/bin/python3 /Users/lile/developer/TensorFlow\r\nRunning virtualenv with interpreter /System/Library/Frameworks/Python.framework/Versions/3.6/bin/python3\r\nUsing base prefix '/System/Library/Frameworks/Python.framework/Versions/3.6'\r\nNew python executable in /Users/lile/developer/TensorFlow/bin/python3\r\nAlso creating executable in /Users/lile/developer/TensorFlow/bin/python\r\nERROR: The executable /Users/lile/developer/TensorFlow/bin/python3 is not functioning\r\nERROR: It thinks sys.prefix is '/System/Library/Frameworks/Python.framework/Versions/3.6' (should be '/Users/lile/developer/TensorFlow')\r\nERROR: virtualenv is not compatible with this system or executable", "If you are still struggling, and experiencing this issue:\r\n\r\n`ERROR: virtualenv is not compatible with this system or executable`\r\n\r\nHave you tried installing virtualenv with conda and trying again?\r\n\r\n`conda install virtualenv`"]}, {"number": 2553, "title": "Add support for circle_ci or travis upon pull request. ", "body": "It would be good to have an automatic and simple first continuous integration rule that will run when a pull request is submitted/updated. We could leverage some of the free services out there to handle this task. I presume both the compilation and the testing will be quite slow but it will provide a first line of filtering. The commands would be the same as in the ci_build so no active maintenance should be required. \n", "comments": ["We haven't prioritized this since we always needed to run all of our internal tests anyways, but it definitely seems like it would be handy for contributors to get an earlier feedback.\n", "Any of the \"CI as a service\" would not work...\n\nThey can't run tests because compile alone takes more cpu and memory than limits on any of those services for open source projects - our jenkins nodes have far more cpu and memory and the builds take 15-30 minutes because they are cached. Does any of the free services persist workspace between builds? Even with cache it would not fit free tier limits (with some it would not fit even within paid limits).\n\nEven our sanity job which takes 30 seconds to run in our current setup would take far longer. For example validation of BUILD files using \"bazel --nobuild\" would have to install bazel, fetch external dependencies - more than 1GB of source code from many git repositories and zip files...\n\nThere are a lot more more problems I know of. And we have not even tried it...\n\nGiven that said we might be able to enable some light build to be automatically trigger on ci.tensorflow.org. Not the full suite as it is for the time being too expensive.\n\n@Mistobaan is there something in particular you would think should be triggered automatically?\n", "Closing as infeasible as @jendap discussed.  @Mistobaan: Please let us know if there are particular things we should fire off automatically.\n"]}, {"number": 2552, "title": "Is it possible to calculate two kinds of gradient separately in tensorflow", "body": "```\nw_1   = tf.get_variable(\"w_1\", shape)   \nw_2   = tf.get_variable(\"w_2\", shape) \noutput = tf.mul(w_1, w_2)\n.....\n.....\noptimizer = tf.train.AdamOptimizer(alpha).minimize(self.cost)\n```\n\nAs we know, when we run \"optimizer\", tensorflow will caculate gradient and update w_1 & w_2.\n\nBut what i want to do is, first, I want to **treat w_1 as a constant**, I just want to caculate gradient and update **only w_2**. Second, **treat w_2 as a constant** and caculate gradient and update **only w_1**. I want to **take turns** to do these things. \n\nActually, I have seen this  #before: #834. But I use **BasicLSTMCell** module.  I try this code: `print (tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES))`, it shows there are **four** kinds of parameter in my neural network, **which means besides w_1 and w_2, there are other two parameters in  BasicLSTMCell.** \nSo, if I use such as `var_list=[w_1]`, the other two parameters in BasicLSTMCell  can not be optimized, How can I do it? \n", "comments": ["Yes, it is possible, and I added an [answer on Stack Overflow](http://stackoverflow.com/a/37575977/3574081) that covers your case. Feel free to post any follow-up discussion over there!\n", "Trigger update_date\n"]}, {"number": 2551, "title": "[WIP] [tf.learn] API restructure", "body": "API restructure to unify pieces and support a wider variety of models (such as multi head, unsupervised, etc)\n\nDone:\n- Estimator now uses `(features, targets, mode) -> (prediction, loss, train_op)`. Moved `optimize_loss` into `TensorFlowEstimator` (it's now a \"simple\" Estimator).\n- Removed `classification` flag from Estimator.\n\nTodo:\n- Integrate BaseEstimator and Estimator.\n- Add Classifier and Regressor subclasses.\n- Make sure TensorFlowEstimator supports multi-feature.\n- Integrate TensorFlowDNN/Linear with DNN/Linear Classifiers and Regressors.\n\nCC @terrytangyuan @ispirmustafa @martinwicke \n", "comments": ["Pulling in.\n", "See 123527855.\n", "Awesome! So this will be included as part of internal branch merge later instead, right? @martinwicke \n", "Yes. I will close this PR then. The skflow examples have to be adapted to\nuse the new DNNClassifier and DNNRegressor. If someone wants to do that on\ntop of this PR it will continue working when the internal code is merged\n(hopefully later today). That's one of the major impediments to removing\nthe TensorFlowDNNEstimator,Regressor classes. As far as I can see at least,\nIllia may know more.\nOn Sun, May 29, 2016 at 12:00 Yuan (Terry) Tang notifications@github.com\nwrote:\n\n> Awesome! So this will be included as part of internal branch merge later\n> instead, right? @martinwicke https://github.com/martinwicke\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/2551#issuecomment-222376773,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AAjO_fZ-El4vJkoEmd0QWttp9DqrvNqQks5qGeJPgaJpZM4IpAVb\n> .\n"]}, {"number": 2550, "title": "edit_distance_op.cc contains suspicious explicit divide by zeros", "body": "In edit_distance_op.cc, there are several code pieces like:\n\nif (normalize_) output_t(loc) /= 0.0;\n\nis there any specific purpose doing that? As I know, dividing by 0 is not an advocated idea.\n", "comments": ["uh, good question.  @ebrevdo \n", "Probably a bug. Will take a closer look on Monday.\n", "Working on this now.\n", "Fixed at head.\n"]}, {"number": 2549, "title": "[install problems] Failed to setup for development from the latest source(2016.5.28).", "body": "### Environment info\n\nOperating System:\n`Ubuntu 14.04`\nInstalled version of CUDA and cuDNN: \n\n```\n-rw-r--r-- 1 root root   322936  8\u6708 16  2015 /usr/local/cuda-7.5/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root       16  8\u6708 15  2015 /usr/local/cuda-7.5/lib64/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root       19  8\u6708 15  2015 /usr/local/cuda-7.5/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root   383336  8\u6708 15  2015 /usr/local/cuda-7.5/lib64/libcudart.so.7.5.18\n-rw-r--r-- 1 root root   720192  8\u6708 16  2015 /usr/local/cuda-7.5/lib64/libcudart_static.a\nlrwxrwxrwx 1 root root       13  3\u6708 22 10:04 /usr/local/cuda-7.5/lib64/libcudnn.so -> libcudnn.so.4\nlrwxrwxrwx 1 root root       17  3\u6708 22 10:03 /usr/local/cuda-7.5/lib64/libcudnn.so.4 -> libcudnn.so.4.0.7\n-rwxrwxrwx 1 root root 61453024  3\u6708 22 10:01 /usr/local/cuda-7.5/lib64/libcudnn.so.4.0.7\n-rw-r--r-- 1 root root 62025862  3\u6708 22 10:01 /usr/local/cuda-7.5/lib64/libcudnn_static.a\n```\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n\nIf installed from sources, provide the commit hash:\n### Steps to reproduce\n\n1.\n2.\n3.\n### What have you tried?\n1. uninstall(including deleting the whole package) and reinstall many times, .\n### Logs or other output that would be helpful\n\n(part of the logs)\n\n```\nexternal/protobuf/python/google/protobuf/pyext/message.cc: In instantiation of 'bool google::protobuf::python::CheckAndGetInteger(PyObject*, T*, PyObject*, PyObject*) [with T = long long unsigned int; PyObject = _object]':\nexternal/protobuf/python/google/protobuf/pyext/message.cc:601:45:   required from here\nexternal/protobuf/python/google/protobuf/pyext/message.cc:554:37: warning: deprecated conversion from string constant to 'char*' [-Wwrite-strings]\nINFO: From Unknown tensorflow/core/protobuf/master_service.pb.h:\nbazel-out/local_linux-opt/genfiles/external/protobuf/src: warning: directory does not exist.\nINFO: From Unknown tensorflow/core/protobuf/worker_service.pb.h:\nbazel-out/local_linux-opt/genfiles/external/protobuf/src: warning: directory does not exist.\nINFO: From Compiling tensorflow/python/pywrap_tensorflow.cc:\nbazel-out/local_linux-opt/bin/tensorflow/python/pywrap_tensorflow.cc: In function 'PyObject* _wrap_GetMatchingFiles(PyObject*, PyObject*)':\nbazel-out/local_linux-opt/bin/tensorflow/python/pywrap_tensorflow.cc:5659:40: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n     for (int i = 0; i < converted.size(); ++i) {\n                                        ^\nbazel-out/local_linux-opt/bin/tensorflow/python/pywrap_tensorflow.cc: In function 'PyObject* _wrap_PyRecordReader_New(PyObject*, PyObject*)':\nbazel-out/local_linux-opt/bin/tensorflow/python/pywrap_tensorflow.cc:3934:111: warning: 'arg2' may be used uninitialized in this function [-Wmaybe-uninitialized]\n     result = (tensorflow::io::PyRecordReader *)tensorflow::io::PyRecordReader::New((string const &)*arg1,arg2);\n                                                                                                               ^\nAt global scope:\ncc1plus: warning: unrecognized command line option \"-Wno-self-assign\" [enabled by default]\nTarget //tensorflow/tools/pip_package:build_pip_package up-to-date:\n  bazel-bin/tensorflow/tools/pip_package/build_pip_package\nINFO: Elapsed time: 206.866s, Critical Path: 50.10s\n```\n\nI setup the development by using `python setup.py develop`.\nHowever, the result is always `ImportError: No module named 'tensorflow'`\n\nI think this problem  may be related to the protobuf according to the logs above.  I tried to reinstall many times, but I still can't figure out how to fix this.\n\nAny help is appreciated!\n", "comments": []}, {"number": 2548, "title": "[tf.learn] Added Mnist example that uses RNN model.", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", " I signed it\n\n---\n\nbaoblackcoal@hotmail.com\n\nFrom: Tensorflow Jenkinsmailto:notifications@github.com\nDate: 2016-05-28 11:49\nTo: tensorflow/tensorflowmailto:tensorflow@noreply.github.com\nCC: baoblackcoalmailto:baoblackcoal@hotmail.com; Authormailto:author@noreply.github.com\nSubject: Re: [tensorflow/tensorflow] Added Mnist example that uses RNN model. (#2548)\n\nCan one of the admins verify this patch?\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHubhttps://github.com/tensorflow/tensorflow/pull/2548#issuecomment-222288148, or mute the threadhttps://github.com/notifications/unsubscribe/ANYEMk0C8F6dN4t4g9RLkCS1NUDzz9iHks5qF7tVgaJpZM4Io_G2.\n", "make sure to sign the CLA with the email that you used for your git commit.  otherwise we can't accept this\n", "@vrv thanks, i have fixed it, check again please.\n", "You need to say \"I signed it!\" (with exclamation mark at the end) and\nrespond on the github issue.\n\nThanks!\n\nOn Sat, May 28, 2016 at 12:39 AM, baoblackcoal notifications@github.com\nwrote:\n\n> I signed it\n> \n> ---\n> \n> baoblackcoal@hotmail.com\n> \n> From: googlebotmailto:notifications@github.com\n> Date: 2016-05-28 11:49\n> To: tensorflow/tensorflowmailto:tensorflow@noreply.github.com\n> CC: baoblackcoalmailto:baoblackcoal@hotmail.com; Author<mailto:\n> author@noreply.github.com>\n> Subject: Re: [tensorflow/tensorflow] Added Mnist example that uses RNN\n> model. (#2548)\n> \n> Thanks for your pull request. It looks like this may be your first\n> contribution to a Google open source project. Before we can look at your\n> pull request, you'll need to sign a Contributor License Agreement (CLA).\n> \n> ???? Please visit https://cla.developers.google.com/ to sign.\n> \n> Once you've signed, please reply here (e.g. I signed it!) and we'll\n> verify. Thanks.\n> \n> ---\n> - If you've already signed a CLA, it's possible we don't have your GitHub\n>   username or you're using a different email address. Check your existing CLA\n>   datahttps://cla.developers.google.com/clas and verify that your email\n>   is set on your git commits<\n>   https://help.github.com/articles/setting-your-email-in-git/>.\n> - If you signed the CLA as a corporation, please let us know the company's\n>   name.\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub<\n> https://github.com/tensorflow/tensorflow/pull/2548#issuecomment-222288150>,\n> or mute the thread<\n> https://github.com/notifications/unsubscribe/ANYEMkHlNc6JWtTpoEQnHy2iHZuOW28Iks5qF7tYgaJpZM4Io_G2>.\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/2548#issuecomment-222295421,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AAKtfjEjaK7YySE2gCxVLAQtokghiAWcks5qF_EmgaJpZM4Io_G2\n> .\n\n## \n\nBest regards,\nIllia Polosukhin\n", "@googlebot I signed it!\n", "@terrytangyuan Nightly built version work correctly, thanks \n", "@ilblackdragon I have said \"I signed it!\", but @googlebot ignored me. should i pull another request ?\n", "What email did you use to sign the CLA?  And what email is on the git commits of this PR?\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@ilblackdragon @terrytangyuan do skflow examples not generally come with tests?\n", "@vrv thanks your patience.  i found that all email(CLA, git commit and github account) must be the same, googlebot agree with me under this condition.\n", "@vrv Not yet. Some examples require other libraries, e.g. h5py, dask, pandas, etc. Some examples take too long to run. I'll probably add some tests for them after refactoring. \n", "@tensorflow-jenkins Test this please\n", "@vrv Ping. @ilblackdragon We'll probably need to update the examples if we are deprecating things soon. \n"]}, {"number": 2547, "title": "Confusing embedding_lookup docstring, incorrect use of the word \"partition\"", "body": "The wording in the docstring for nn.embedding_lookup is confusing and refers to subsets as partitions (https://www.tensorflow.org/versions/r0.8/api_docs/python/nn.html#embedding_lookup)\n\nA partition of a set X is a set of nonempty subsets of X such that every element x in X is in exactly one of these subsets (https://en.wikipedia.org/wiki/Partition_of_a_set)\n\nHere is an example of incorrect usage:\n\"For instance, 13 ids are split across 5 partitions as: [[0, 5, 10], [1, 6, 11], [2, 7, 12], [3, 8], [4, 9]]\"\nThat is actually 1 partition of the 13 ids, not 5 partitions.\n\nAnd this is just really confusing:\n\"If len(params) > 1, each element id of ids is partitioned between the elements of params according to the partition_strategy. In all strategies, if the id space does not evenly divide the number of partitions, each of the first (max_id + 1) % len(params) partitions will be assigned one more id.\"\n", "comments": ["It's an interesting point. It is true that the mathematical definition of a partition is as you say. On the other hand the idea that a partition is actually synonymous with \"disjoint subset\" is actually really common in CS. Consider disk partitions and memory partitions where a \"disk partition\" is considered the subset of the disk. So I'm not sure... someone who thinks about things from that common usage would find this totally clear.\n\nThe argument for changing it is that the documentation sort of uses both interpretations.  \"This function is used to perform parallel lookups on the list of tensors in params. It is a generalization of tf.gather(), where params is interpreted as a partition of a larger embedding tensor.\"  Here we should probably say \"interpreted as partitions of a larger embedding tensor\" to be consistent with the CS common usage.\n", "Closing automatically due to lack of recent activity."]}, {"number": 2546, "title": "\"bazel test --config=cuda\" can't find \"libcurand.so.7.5\"", "body": "For instance (today's head, Bazel 0.2.3 on Ubuntu)\n`\nbazel test -c opt --config=cuda tensorflow/contrib/distributions:chi2_test\nI tensorflow/stream_executor/dso_loader.cc:102] Couldn't open CUDA library libcurand.so.7.5. LD_LIBRARY_PATH: \n`\n\nI'm setting `LD_LIBRARY_PATH=/usr/local/cuda/lib64`, but `bazel test` starts own environment where `LD_LIBRARY_PATH` is empty. Strangely though, it finds libcublas/libcudnn/libcufft which are in the same directory as `libcurand.so.7.5`\n\nA work-around it to set `LD_LIBRARY_PATH`, and run the test harness through Python stub rather than through blaze test\n\n```\nexport LD_LIBRARY_PATH=\"/usr/local/cuda/lib64:$LD_LIBRARY_PATH\"\nbazel test -c opt --config=cuda tensorflow/contrib/distributions:chi2_test\nbazel-bin/tensorflow/contrib/distributions/chi2_test\n\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.7.5 locally\n\n```\n", "comments": ["`ls -al /usr/local/cuda/lib64/libcu*` and report that here? Do you have any special bazelrc entries?\n", "```\nyaroslavvb@lenin:~$ cat ~/.bazelrc\nstartup --max_idle_secs=1000000\n\n```\n\n```\nyaroslavvb@lenin:~$ ls -al /usr/local/cuda/lib64/libcu*\n-rw-r--r-- 1 root root  28585480 May 24 18:12 /usr/local/cuda/lib64/libcublas_device.a\nlrwxrwxrwx 1 root root        16 May 24 18:12 /usr/local/cuda/lib64/libcublas.so -> libcublas.so.7.5\nlrwxrwxrwx 1 root root        19 May 24 18:12 /usr/local/cuda/lib64/libcublas.so.7.5 -> libcublas.so.7.5.18\n-rwxr-xr-x 1 root root  23938736 May 24 18:12 /usr/local/cuda/lib64/libcublas.so.7.5.18\n-rw-r--r-- 1 root root  28220076 May 24 18:12 /usr/local/cuda/lib64/libcublas_static.a\n-rw-r--r-- 1 root root    322936 May 24 18:12 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root        16 May 24 18:12 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root        19 May 24 18:12 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root    383336 May 24 18:12 /usr/local/cuda/lib64/libcudart.so.7.5.18\n-rw-r--r-- 1 root root    720192 May 24 18:12 /usr/local/cuda/lib64/libcudart_static.a\nlrwxrwxrwx 1 root root        13 May 24 20:48 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5\nlrwxrwxrwx 1 root root        17 May 24 20:48 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.0.5\n-rwxr-xr-x 1 root root  59909104 May 24 20:48 /usr/local/cuda/lib64/libcudnn.so.5.0.5\n-rw-r--r-- 1 root root  58775484 May 24 20:48 /usr/local/cuda/lib64/libcudnn_static.a\nlrwxrwxrwx 1 root root        15 May 24 18:12 /usr/local/cuda/lib64/libcufft.so -> libcufft.so.7.5\nlrwxrwxrwx 1 root root        18 May 24 18:12 /usr/local/cuda/lib64/libcufft.so.7.5 -> libcufft.so.7.5.18\n-rwxr-xr-x 1 root root 111231960 May 24 18:12 /usr/local/cuda/lib64/libcufft.so.7.5.18\n-rw-r--r-- 1 root root 115104400 May 24 18:12 /usr/local/cuda/lib64/libcufft_static.a\nlrwxrwxrwx 1 root root        16 May 24 18:12 /usr/local/cuda/lib64/libcufftw.so -> libcufftw.so.7.5\nlrwxrwxrwx 1 root root        19 May 24 18:12 /usr/local/cuda/lib64/libcufftw.so.7.5 -> libcufftw.so.7.5.18\n-rwxr-xr-x 1 root root    447664 May 24 18:12 /usr/local/cuda/lib64/libcufftw.so.7.5.18\n-rw-r--r-- 1 root root     42206 May 24 18:12 /usr/local/cuda/lib64/libcufftw_static.a\nlrwxrwxrwx 1 root root        17 May 24 18:12 /usr/local/cuda/lib64/libcuinj64.so -> libcuinj64.so.7.5\nlrwxrwxrwx 1 root root        20 May 24 18:12 /usr/local/cuda/lib64/libcuinj64.so.7.5 -> libcuinj64.so.7.5.18\n-rwxr-xr-x 1 root root   5751400 May 24 18:12 /usr/local/cuda/lib64/libcuinj64.so.7.5.18\n-rw-r--r-- 1 root root   1649726 May 24 18:12 /usr/local/cuda/lib64/libculibos.a\nlrwxrwxrwx 1 root root        16 May 24 18:12 /usr/local/cuda/lib64/libcurand.so -> libcurand.so.7.5\nlrwxrwxrwx 1 root root        19 May 24 18:12 /usr/local/cuda/lib64/libcurand.so.7.5 -> libcurand.so.7.5.18\n-rwxr-xr-x 1 root root  51765952 May 24 18:12 /usr/local/cuda/lib64/libcurand.so.7.5.18\n-rw-r--r-- 1 root root  51992564 May 24 18:12 /usr/local/cuda/lib64/libcurand_static.a\nlrwxrwxrwx 1 root root        18 May 24 18:12 /usr/local/cuda/lib64/libcusolver.so -> libcusolver.so.7.5\nlrwxrwxrwx 1 root root        21 May 24 18:12 /usr/local/cuda/lib64/libcusolver.so.7.5 -> libcusolver.so.7.5.18\n-rwxr-xr-x 1 root root  37034328 May 24 18:12 /usr/local/cuda/lib64/libcusolver.so.7.5.18\n-rw-r--r-- 1 root root  16613348 May 24 18:12 /usr/local/cuda/lib64/libcusolver_static.a\nlrwxrwxrwx 1 root root        18 May 24 18:12 /usr/local/cuda/lib64/libcusparse.so -> libcusparse.so.7.5\nlrwxrwxrwx 1 root root        21 May 24 18:12 /usr/local/cuda/lib64/libcusparse.so.7.5 -> libcusparse.so.7.5.18\n-rwxr-xr-x 1 root root  36816424 May 24 18:12 /usr/local/cuda/lib64/libcusparse.so.7.5.18\n-rw-r--r-- 1 root root  44445334 May 24 18:12 /usr/local/cuda/lib64/libcusparse_static.a\n\n```\n", "Only suggestion I have left, before asking bazel folks is to make sure --spawn_strategy=standalone and --genrule_strategy=standalone are set\n", "OK, I'll try it and update. One thing that comes to mind, is that the other 4 libraries are loaded successfully, and I remember seeing some BUILD-warning about those libraries in earlier version of bazel, so perhaps there's some BUILD-logic for those 4 libraries\n\n```\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally\n\n```\n", "Doesn't help (at 07db1806460cbebbbf3abe9174a65eb52c8a63e0, 4 commits behind now)\n\n```\nyaroslavvb@lenin:~/tfimmediate_src.gpu/tensorflow$ bazel test -c opt --config=cuda  --spawn_strategy=standalone --genrule_strategy=standalone //tensorflow/python:batch_matrix_band_part_op_test --test_output=streamed\n\n\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:102] Couldn't open CUDA library libcurand.so.7.5. LD_LIBRARY_PATH: \nI tensorflow/stream_executor/cuda/cuda_rng.cc:333] Unable to load cuRAND DSO.\n\n```\n\nRunning stub directly works\n\n```\n\nyaroslavvb@lenin:~/tfimmediate_src.gpu/tensorflow$ bazel-bin/tensorflow/python/batch_matrix_band_part_op_test\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.7.5 locally\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX 980\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.2155\npciBusID 0000:03:00.0\nTotal memory: 4.00GiB\nFree memory: 3.53GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x1c34f10\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 1 with properties: \nname: GeForce GTX 980\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.2155\npciBusID 0000:04:00.0\nTotal memory: 4.00GiB\n\n```\n", "Perhaps the absence of `curand` in [`master/third_party/gpus/cuda/BUILD`](https://github.com/tensorflow/tensorflow/blob/master/third_party/gpus/cuda/BUILD) is causing this? I'm having a similar issue; I can't see how this could work through Bazel without `curand` being configured [here](https://github.com/tensorflow/tensorflow/blob/master/third_party/gpus/cuda/BUILD#L161) at a minimum.\n", "@yaroslavvb: Does @rayglover-ibm's tweak fix your problem? \n", "His solution seems like it would address the issue. At the same time, all the tests pass, so it's not clear TensorFlow actually uses curand for anything.\n\nSimplest way to reproduce the warning:\n`bazel test -c opt --config=cuda --test_output=streamed tensorflow/contrib/distributions:chi2_test`\n", "I'll close it since it doesn't seem to affect anyone. The person who adds some cuRAND-using-op will discover this anyway since their bazel tests will fail\n"]}, {"number": 2545, "title": "Work around for problem with Thumb linking", "body": "There's an occasional problem with linking armv7 code with the makefile, which results in a bogus linker error. One way around it is to disable Thumb code. See https://discussions.apple.com/thread/6037322?tstart=0 for more details.\n", "comments": ["Jenkins, test this please.\n"]}, {"number": 2544, "title": "Updated Eigen version in Makefile", "body": "This cleans up assignment in the makefile, and switches to the latest Eigen version.\n", "comments": ["LGTM, merging.\n"]}, {"number": 2543, "title": "grammar/typo: \"like variable\" -> \"like a variable\"", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n"]}, {"number": 2542, "title": "Improve complex128 support (again)", "body": "This PR adds `complex128` to various places where it was still missing (mostly tests and docstrings).\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n"]}, {"number": 2541, "title": "Add meshgrid function", "body": "I needed to use something similar to [`np.meshgrid`](http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.meshgrid.html) in tensorflow and decided to write my own version.\nThis can be implemented in pure Python by making use of `tf.reshape` and `tf.tile`.\nI've placed the function in `array_ops.py` and added a simple test that compares the output to the numpy version.\n\nOne thing that's still missing is testing whether the inputs all have rank 1.\nSeems like I might have to use `tf.rank` to check this at runtime.\nWould `Assert` ops be the right tool for that?\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks!  Yes, `tf.Assert` is the right tool.  I'm tempted to not require it since it mucks up the code quite a bit and doesn't affect the correct path, but I agree it's the right thing to do.\n", "Thanks for the comments!\nI've pushed a new commit that addresses them.\nI'm requiring all `tf.Assert`s for each of the outputs in case the user doesn't use all of them.\n", "Cool, two more small comments and then I'll fire off the tests.\n", "Thanks!\nI've pushed another commit.\n", "Jenkins, test this please.\n", "@martinwicke: Do you understand the rather unhelpful \"invalid syntax\" lint error?  The line looks perfectly reasonable to me.\n", "Just noticed that one of the new lines was longer than 80 characters.\nHere's the old build: http://ci.tensorflow.org/job/tensorflow-pull-requests-sanity/383/console\n\nEdit: I've tried running pylint myself, but didn't get the syntax error\n", "@chrisburr just pointed out that you can't use `*args` with explicit keyword arguments in Python 2,\nand that's what's causing the syntax error.\nI'll update the PR to use `**kwargs`.\n", "Okay, I've adjusted the code to work with Python 2 and improved the docstring slightly.\nThis should be ready to go now.\n", "Currently unknown keyword args are silently dropped; we should report an error instead.  It's unfortunate that Python 2 doesn't have keyword-only arguments.\n", "Yeah, it's always sad when this happens.\nI've pushed a new commit that tries to mimic the error that Python usually raises.\n", "Thanks!  Jenkins, test this please.\n", "Jenkins, test this please.\n", "@caisq, @gunan: Do you know what's going on with the ci.tensorflow.org failure?\n", "Looks like Jenkins is healthy again.\nCan someone re-run the tests?\n", "Jenkins, test this please.\n", "Seems like Jenkins is using a numpy version before `1.9.0` that doesn't have the `dtype` kwarg for `np.linspace`\nhttp://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.linspace.html\n", "I've pushed a new commit that should fix the failing test.\nHere's a link to the failed log output for reference: http://ci.tensorflow.org/job/tensorflow-pull-requests-cpu-python3/660/console\n", "Jenkins, test this please.\n", "@ibab: Looks like you'll have to drop the 1 argument test since numpy doesn't support it for the version Jenkins uses.  Ours should, though.\n", "Okay, I've removed the 1 dim case.\n", "Jenkins, test this please.\n", "Ready to merge @girving  ?\n", "@vrv: Yes!  Thank you for yet another contribution @ibab! \n"]}, {"number": 2540, "title": "tf.select unnecessarily propagates NaNs in gradients", "body": "tf.select() will give NaN gradients even if the source of the NaNs is not selected. See example script below.\n\nimport tensorflow as tf\nx = tf.placeholder(tf.float32)\ny = tf.select(x>0, 0., tf.exp(x))\nz = tf.gradients(y,x)[0]\n\nwith tf.Session() as sess:\n    yv,zv = sess.run([y,z],{x: 1e10})\n    print(yv) # correctly outputs 0\n    print(zv) # this is NaN, but should be 0\n", "comments": ["Somewhat related, Select gradient is also failing the test when there's GPU\n\nbazel test -j 1 -c opt --config=cuda tensorflow/core:ops_math_grad_test --test_output=streamed\n\n```\n[ RUN      ] MathGradTest.Select\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:782] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:03:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:782] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX 980, pci bus id: 0000:04:00.0)\nE tensorflow/core/common_runtime/executor.cc:334] Executor failed to create kernel. Not found: No registered '_Arg' OpKernel for GPU devices compatible with node n0 = _Arg[T=DT_BOOL, index=0]()\n\n```\n", "This propagation is inevitable, actually.  If we introduce the extra variable\n\n```\nw = tf.exp(x)\n```\n\nthen `tf.gradients` correctly computes `dy/dw = 0`.  It then computes\n\n```\ndy/dx = w * dy/dw = nan * 0 = nan\n```\n\nI.e., the issue is unrelated to select.\n\n@yaroslavvb: That seems like an unrelated issue, so it should be filed separately if it's still a problem. \n", "Trigger update_date\n", "@girving It's not clear to me why the propagation of NaN is inevitable, and I think this issue should be re-opened.\n\nIf `w = tf.select(c, t, e)`, then `dw/dx = select(c, dt/dx, de/dx)` \n\nTensorflow 0.9 (and likely 0.10, but have not tested) do not compute this correctly. For instance,\n`d/dx[ select(True, t, e) ] != dt/dx` for some cases.\n\nSee the following example:\n\n``` python\nimport tensorflow as tf\n\nx = tf.constant(0.0)\n\nt = x\ne = 1. / x\n\nw = tf.select(True, t, e)\n\nwith tf.Session() as sess:\n    print 'w:', sess.run(w)\n    print 'dw/dx:', sess.run(tf.gradients(w, x)[0])\n    print 'dt/dx:', sess.run(tf.gradients(t, x)[0])\n```\n\noutputs (with Tensorflow 0.9)\n\n```\nw: 0.0\ndw/dx: nan\ndt/dx: 1.0\n```\n\nThese NaN's cause NaN gradients, which causes NaN weights and unnecessarily breaks training a net.\n", "After reading some code and docs, I think I see the difficulty with `select` gradient.\n\nThe `select` gradient is here: https://github.com/tensorflow/tensorflow/blob/4addf4b5806cd731949c6582a83f5824599cd1ef/tensorflow/python/ops/math_grad.py#L688\n\nGradients are only taken on the output of an op with respect to the input (and then multiplied by the accumulated gradient from objective to output of op), so in my example above the `select` gradient computes `dw/dt=1` and `dw/de=0`. This gives `dL/dt=dL/dw` and `dL/de=0` for objective `L`.\n\nHowever, the graph traversal algorithm to compute gradient continues, and computes `dt/dx = 1, de/dx = -Infinity`, which finally gives `dL/dx = dL/dt * dt/dx + dL/de * de/dx = dL/dt * 1 + 0 * Inf = dL/dt + NaN = NaN`.\n\nThe issue is the `0 * Inf = NaN` term, and the fact that this computation doesn't happen in the select node (happens in the divide node in this case).\n\nI spent a few minutes thinking of possible solutions to this, and none of them are pretty or seem worth implementing. \nOne option is to pass a mask of hard zero gradients around during gradient computation. In this case, after each node computes it's gradients, the hard zero gradients can be reset to 0. This isn't a very appealing option.\nThe other option is to significantly change the gradient propagation algorithm. Rather than computing gradient of objective with respect to op input, compute gradient of op output with respect to op input for all ops first. Then do a multiplicative accumulate pass. This has the downside of significant framework rewrite, and is unworkable because gradient of op output wrt op input can be high order tensors (gradient of matrix multiply wrt inputs is very large).\nIn summary, there doesn't seem to be a good way to make Tensorflow compute the select gradient correctly. I wrote this largely for my own understanding of why \"[NaN gradient] is inevitable\", hopefully this can be useful to someone else who happens across the issue.\n", "BTW, it's not just the issue with select, code below also gives NaN.\n\nx = tf.placeholder(tf.float32)\ny = tf.exp(x)\nz = tf.exp(-y)\ngrad = tf.gradients(z,[x])[0]\nprint sess.run(grad, feed_dict={x: 1e10})\n\nThe underlying cause is that  chain rule gives f' \\* g' as derivative of\nf(g(x)) so with because 1e10 overflows we get infinity_0 in both cases.\nOne could argue that limiting arguments that justify inf_0=NaN should not\napply in case of tf.select, and substitute special gradient for tf.select\nusing gradient_override_map during runtime\n\nOn Mon, Sep 26, 2016 at 9:35 AM, Eric Martin notifications@github.com\nwrote:\n\n> After reading some code and docs, I think I see the difficulty with select\n> gradient.\n> \n> The select gradient is here: https://github.com/tensorflow/\n> tensorflow/blob/4addf4b5806cd731949c6582a83f58\n> 24599cd1ef/tensorflow/python/ops/math_grad.py#L688\n> \n> Gradients are only taken on the output of an op with respect to the input\n> (and then multiplied by the accumulated gradient from objective to output\n> of op), so in my example above the select gradient computes dw/dt=1 and\n> dw/de=0. This gives dL/dt=dL/dw and dL/de=0 for objective L.\n> \n> However, the graph traversal algorithm to compute gradient continues, and\n> computes dt/dx = 1, de/dx = -Infinity, which finally gives dL/dx = dL/dt\n> - dt/dx + dL/de \\* de/dx = dL/dt \\* 1 + 0 \\* Inf = dL/dt + NaN = NaN.\n> \n> The issue is the 0 \\* Inf = NaN term, and the fact that this computation\n> doesn't happen in the select node (happens in the divide node in this case).\n> \n> I spent a few minutes thinking of possible solutions to this, and none of\n> them are pretty or seem worth implementing.\n> One option is to pass a mask of hard zero gradients around during gradient\n> computation. In this case, after each node computes it's gradients, the\n> hard zero gradients can be reset to 0. This isn't a very appealing option.\n> The other option is to significantly change the gradient propagation\n> algorithm. Rather than computing gradient of objective with respect to op\n> input, compute gradient of op output with respect to op input for all ops\n> first. Then do a multiplicative accumulate pass. This has the downside of\n> significant framework rewrite, and is unworkable because gradient of op\n> output wrt op input can be high order tensors (gradient of matrix multiply\n> wrt inputs is very large).\n> In summary, there doesn't seem to be a good way to make Tensorflow compute\n> the select gradient correctly. I wrote this largely for my own\n> understanding of why \"[NaN gradient] is inevitable\", hopefully this can be\n> useful to someone else who happens across the issue.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2540#issuecomment-249623937,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AABaHE-b1wrlAAYk-694eHsOS3GT9xdXks5qt_RsgaJpZM4Io0ms\n> .\n", "The exponent example is a little different because a intermediate computation (`y` in your example) is infinity, so it's not too surprising that the gradient is also problematic. The `epsilon * Inf` is fundamental to the gradient computation here, while it's not obviously so in the `select` case.\n\nI completely agree that limiting arguments for `0 * Inf = NaN` do not apply for select, because the gradient in select is set to 0 to specify that the gradient with respect to the entire non-selected subgraph is exactly 0.\nI wasn't familiar with `gradient_override_map`, but I just read the documentation and I don't see how it can fix the problem as I believe the problem comes from the combination of backprop (multiplicatively accumulating gradients) and that `0 * x != 0` for all `x`. @yaroslavvb , did you have a specific idea about how to fix `select` with `gradient_override_map`?\n", "I also had this problem and I needed to use a workaround. Instead of writing `log = tf.where(non_zero, tf.log(logarg), tf.zeros_like(logarg))` I needed to select values with `tf.boolean_mask` and then scatter them back into the original shape by doing:\r\n```\r\nlogarg = tf.boolean_mask(logarg, non_zero)\r\nlog = tf.log(logarg)\r\nidx = tf.to_int32(tf.where(non_zero))\r\nlog = tf.scatter_nd(idx, log, tf.shape(non_zero))\r\n```", "In case anyone else comes across this looking for a solution... here's another way to work around this issue (implementing what Tim was doing in the original post):\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nx = tf.placeholder(tf.float32)\r\n# y = tf.where(x > 0, 0., tf.exp(x))\r\n\r\n# trick: we're not using the result of safe_exp when x > 0, so we can\r\n# substitute a safe value for x in that case\r\n# it doesn't really matter what we put in here, as long as the backward pass\r\n# returns some finite value\r\nsafe_exp = tf.exp(tf.where(x > 0, 1.0, x))\r\ny = tf.where(x > 0, 0., safe_exp)\r\n\r\nz, = tf.gradients(y, x)\r\n\r\nsess = tf.InteractiveSession()\r\nyv, zv = sess.run([y, z], {x: 1e10})\r\nprint(yv) # correctly outputs 0\r\nprint(zv) # now correctly outputs 0\r\n\r\nyv, zv = sess.run([y, z], {x: -1})\r\nprint(yv) # correctly outputs e^(-1)\r\nprint(zv) # correctly outputs e^(-1)\r\n```", "See also: https://stackoverflow.com/questions/33712178/tensorflow-nan-bug/42497444#42497444"]}, {"number": 2539, "title": "Base merge", "body": "", "comments": ["Can one of the admins verify this patch?\n"]}, {"number": 2538, "title": "Upstream changes from internal", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n"]}, {"number": 2537, "title": "Enable complex number types for tile op", "body": "I've enabled `complex64` and `complex128` on the CPU for `tf.tile` and added them to the tests.\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n"]}, {"number": 2536, "title": "zlib.h error when compiling tutorials_example_trainer, but zlib is there", "body": "### Environment info\n\nOperating System: Red Hat Enterprise Linux Server release 7.2\n\nInstalled version of CUDA and cuDNN: CUDA 7, cuDNN 4\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n-rw-r--r-- 1 root root 179466 Jan 26 22:19 /usr/local/cuda/lib/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Jan 26 22:19 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.0\nlrwxrwxrwx 1 root root     19 Jan 26 22:19 /usr/local/cuda/lib/libcudart.so.7.0 -> libcudart.so.7.0.28\n-rwxr-xr-x 1 root root 303052 Jan 26 22:19 /usr/local/cuda/lib/libcudart.so.7.0.28\n-rw-r--r-- 1 root root 546514 Jan 26 22:19 /usr/local/cuda/lib/libcudart_static.a\n\ncuDNN is installed for the local user only.\n\nIf installed from sources, provide the commit hash:\n\nb289bc7a50fc0254970c60aaeba01c33de61a728\n### Steps to reproduce\n\nI have followed the instructions at: https://www.tensorflow.org/versions/r0.8/get_started/os_setup.html#requirements \n\nSince I am not root, I had to install everything for the local user, but it seems to have worked. \n\nI get an error at this line:\n\nbazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\n\nSaying:\n\nERROR: [...]/vlad/.cache/bazel/_bazel_vlad/6607a39fc04ec931b523fac975ff3100/external/png_archive/BUILD:23:1: Executing genrule @png_archive//:configure failed: bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\n[...]/vlad/.cache/bazel/_bazel_vlad/6607a39fc04ec931b523fac975ff3100/tensorflow/external/png_archive/libpng-1.2.53 [...]/vlad/.cache/bazel/_bazel_vlad/6607a39fc04ec931b523fac975ff3100/tensorflow\n/tmp/tmp.pCUaj9eIKr [...]/vlad/.cache/bazel/_bazel_vlad/6607a39fc04ec931b523fac975ff3100/tensorflow/external/png_archive/libpng-1.2.53 [...]/vlad/.cache/bazel/_bazel_vlad/6607a39fc04ec931b523fac975ff3100/tensorflow\n\n... a bunch more lines until:\n\nchecking for pow... no\nchecking for pow in -lm... yes\nchecking for zlibVersion in -lz... no\n**configure: error: zlib not installed**\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 57.196s, Critical Path: 22.12s\n### What have you tried?\n\nI installed zlib, and the following program compiles with g++\n\n```\n#include <cstdio>\n#include <zlib.h>\n\nint main()\n{\n    printf(\"Hello world\");\n    return 0;\n}\n```\n\nI have the following in my .bashrc:\n\nexport LD_LIBRARY_PATH=\"$HOME/local/cuda/lib64:$LD_LIBRARY_PATH\"\nexport LD_LIBRARY_PATH=\"$HOME/bin/zlibdev/lib:$LD_LIBRARY_PATH\"\nexport CPATH=\"$HOME/local/cuda/include:$CPATH\"\nexport CPATH=\"$HOME/bin/zlibdev/include:$CPATH\"\nexport LIBRARY_PATH=\"$HOME/local/cuda/lib64:$LIBRARY_PATH\"\n\nexport PKG_CONFIG_PATH=\"$HOME/bin/zlibdev/lib/pkgconfig\"\n\nWhy can't bazel / tensorflow find zlib.h? It's there and accessible.\n", "comments": ["I'm not entirely sure -- since it's a ./configure problem for png (an external repo), pulling in @damienmg or @davidzchen in case they know what might be wrong.\n", "This happens because the environment isn't shipped in the genrule. A skylark auto-conf kinda rules could be used instead to generate the genrule. After seeing the code, it might even be possible to ship the zlib imported by a remote repository as a parameter for libpng\n\nAs a workaround, adding the list of export to [that line](https://github.com/tensorflow/tensorflow/blob/master/png.BUILD#L30) should do the trick.\n", "@damienmg can you please tell how to add the list of export there?\n", "replace\n\n``` python\n    cmd = \"pushd external/png_archive/%s; workdir=$$(mktemp -d -t tmp.XXXXXXXXXX); cp -a * $$workdir; pushd $$workdir; ./configure --enable-shared=no --with-pic=no; popd; popd; cp $$workdir/config.h $(@D); rm -rf $$workdir;\" % prefix_dir,\n```\n\nwith something like:\n\n``` python\n    cmd = \"\\n\".join([\n       \"export LD_LIBRARY_PATH='{LD_LIBRARY_PATH}'\",\n       \"export CPATH='{CPATH}'\",\n       \"export LIBRARY_PATH='{LIBRARY_PATH}'\",\n       \"export PKG_CONFIG_PATH='{PKG_CONFIG_PATH}'\",\n        \"pushd external/png_archive/\" + prefix_dir,\n        \"workdir=$$(mktemp -d -t tmp.XXXXXXXXXX)\",\n        \"cp -a * $$workdir\",\n        \"pushd $$workdir\",\n        \"./configure --enable-shared=no --with-pic=no\",\n        \"popd\",\n        \"popd\",\n        \"cp $$workdir/config.h $(@D)\",\n        \"rm -rf $$workdir\",\n    ],\n```\n\nWhere you replace `{LD_LIBRARY_PATH}`, `{CPATH}`, `{LIBRARY_PATH}`, `{PKG_CONFIG_PATH}` by the values from your environment\n", "I have this:\n\n```\ngenrule(\n    name = \"configure\",\n    srcs = glob(\n        [\"**/*\"],\n        exclude = [prefix_dir + \"/config.h\"],\n    ),\n    outs = [prefix_dir + \"/config.h\"],\n    cmd = \"\\n\".join([\n       \"export LD_LIBRARY_PATH='/home/mateinfo/vlad/bin/zlibdev/lib'\",\n       \"export CPATH='/home/mateinfo/vlad/bin/zlibdev/include'\",\n       \"export PKG_CONFIG_PATH='/home/mateinfo/vlad/bin/zlibdev/lib/pkgconfig'\",\n        \"pushd external/png_archive/\" + prefix_dir,\n        \"workdir=$$(mktemp -d -t tmp.XXXXXXXXXX)\",\n        \"cp -a * $$workdir\",\n        \"pushd $$workdir\",\n        \"./configure --enable-shared=no --with-pic=no\",\n        \"popd\",\n        \"popd\",\n        \"cp $$workdir/config.h $(@D)\",\n        \"rm -rf $$workdir\",\n    ]),\n)\n```\n\nBut the same error appears. If I run:\n\n`bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`\n\nThen I get:\n\n```\nERROR: /home/mateinfo/vlad/.cache/bazel/_bazel_vlad/6607a39fc04ec931b523fac975ff3100/external/grpc/BUILD:485:1: C++ compilation of rule '@grpc//:grpc_unsecure' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object ... (remaining 40 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nexternal/grpc/src/core/compression/message_compress.c:41:18: fatal error: zlib.h: No such file or directory\n #include <zlib.h>\n                  ^\ncompilation terminated.\n```\n\nzlib.h is in the CPATH location:\n\n```\n[vlad@headnode ~]$ ls /home/mateinfo/vlad/bin/zlibdev/include\nzconf.h  zlib.h\n```\n\nHave I edited the cmd line wrong?\n", "Can you add a line `cxx_builtin_include_directory: \"/home/mateinfo/vlad/bin/zlibdev/include\"` at https://github.com/tensorflow/tensorflow/blob/master/third_party/gpus/crosstool/CROSSTOOL#L62?\n", "That worked. Thank you very much, and I apologize for the late reply.\n", "@damienmg Is there a version of that fix we could incorporate?\n", "girving: no we are down to auto-configuration of the nvcc crosstool again. @davidzchen is working on it AFAICT.\n", "I should have a PR for the cuda autoconf later this week.\n", "Here is the tracking bug for the cuda autoconf: #2873\n", "Closing since the original issue seems to have been resolved.\n", "I am having a similar problem. After making the suggested changes in the configuration files, I got past the first zlib error, but I'm still getting the second one: \n\n```\nERROR: /hardmnt/silvermoon0/home/rocha/.cache/bazel/_bazel_rocha/1d1a96c2ccf291e17b589991ea556cc1/external/png_archive/BUILD:33:1: C++ compilation of rule '@png_archive//:png' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command \n  (cd /hardmnt/silvermoon0/home/rocha/.cache/bazel/_bazel_rocha/1d1a96c2ccf291e17b589991ea556cc1/execroot/tensorflow0.10 && \\\n  exec env - \\\n    PATH=/home/rocha/anaconda2/bin:/home/rocha/anaconda2/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/local/sbin:/usr/bin:/usr/sbin:/bin:/sbin:/home/rocha/.local/bin:/home/rocha/bin:/home/rocha/.local/jdk1.8.0_101//bin \\\n  third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -g0 -MD -MF bazel-out/host/bin/external/png_archive/_objs/png/external/png_archive/libpng-1.2.53/pngwutil.d -iquote external/png_archive -iquote bazel-out/host/genfiles/external/png_archive -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/host/genfiles/external/png_archive/libpng-1.2.53 -isystem external/bazel_tools/tools/cpp/gcc3 -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fno-canonical-system-headers -c external/png_archive/libpng-1.2.53/pngwutil.c -o bazel-out/host/bin/external/png_archive/_objs/png/external/png_archive/libpng-1.2.53/pngwutil.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nIn file included from external/png_archive/libpng-1.2.53/pngwutil.c:16:0:\nexternal/png_archive/libpng-1.2.53/png.h:548:18: fatal error: zlib.h: No such file or directory\n #include \"zlib.h\"\n                  ^\ncompilation terminated.\n```\n\nBy the way, the current tensorflow version I'm working with is r0.10, and the CROSSTOOL file has a .tpl extension. Line 62 points to a comment in my version, so it evidently has changed a bit.\n", "@erickrf - The main difference is that the `cuda_configure` rule generates a CROSSTOOL file from the CROSSTOOL.tpl, filling in values auto-detected from the system and the user input from the `configure` script, such as CUDA version. You should be able to still add a `cxx_builtin_include_directory` entry to the CROSSTOOL file for now until we add autoconfiguration for libraries such as zlib.\n", "So, I did add that line. the CROSSTOOL file has the following `cxx_builtin_include_directory` entries:\n\n```\n  cxx_builtin_include_directory: \"/home/rocha/.local/include\"\n  cxx_builtin_include_directory: \"/usr/lib/gcc/\"\n  cxx_builtin_include_directory: \"/usr/local/include\"\n  cxx_builtin_include_directory: \"/usr/include\"\n```\n\nBut there they are:\n\n```\n$ ls /home/rocha/.local/include\nzconf.h  zlib.h\n```\n", "Ok, I got it to work. If it is of use to anyone, I added to the CROSSTOOL file the following lines. At line 99, after `compiler_flag: \"-fPIE\"`:\n\n```\ncompiler_flag: \"-I/home/rocha/.local/include\"\n```\n\nAnd at line 126, after `linker_flag: \"-Wl,--hash-style=gnu\"`:\n\n```\nlinker_flag: \"-L/home/rocha/.local/lib\"\n```\n"]}, {"number": 2535, "title": "seq2seq running problem", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System:\nUbuntu 14 \nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n\nIf installed from sources, provide the commit hash:\n### Steps to reproduce\n1. copy all files in models/rnn/translate/ to ~/myworkfold/ \n2. run the translate.py like this: python translate.py --data_dir ./ --train_dir ./   ... (it works well )\n3. But, when i change the code from \"from tensorflow.model.rnn.translate import seq2seq_model\" to \"import seq2seq_model\" in translate.py (in order to use the file seq2seq.py in myworkfold, note that all files are copied form models/rnn/translate/ wothout any change ) \n   it errors like this:\n   Traceback (most recent call last):\n   File \"my_translate.py\", line 279, in <module>\n     tf.app.run()\n   File \"/home/ubuntu/yangqichuan/my_tensorflow/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\n     sys.exit(main(sys.argv))\n   File \"my_translate.py\", line 276, in main\n     train()\n   File \"my_translate.py\", line 142, in train\n     model = create_model(sess, False)\n   File \"my_translate.py\", line 121, in create_model\n     forward_only=forward_only)\n   File \"/home/ubuntu/yangqichuan/my_tensorflow/my_seq2seq/my_seq2seq_model.py\", line 152, in **init**\n     softmax_loss_function=softmax_loss_function)\n   File \"/home/ubuntu/yangqichuan/my_tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/seq2seq.py\", line 961, in model_with_buckets\n     decoder_inputs[:bucket[1]])\n   File \"/home/ubuntu/yangqichuan/my_tensorflow/my_seq2seq/my_seq2seq_model.py\", line 151, in <lambda>\n     lambda x, y: seq2seq_f(x, y, False),\n   File \"/home/ubuntu/yangqichuan/my_tensorflow/my_seq2seq/my_seq2seq_model.py\", line 115, in seq2seq_f\n     feed_previous=do_decode)\n   TypeError: embedding_attention_seq2seq() takes at least 6 arguments (7 given)\n## What have you tried?\n\n1.i do the same operation for model  LSTM models/rnn/ptb . tensorflow works well\n2. if i change back the translate.py file it also works well\n\nWhat happened ?\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n", "comments": ["There was a code change not that long ago that added a new argument (embedding_size) to the call of embedding_attention_seq2seq. It looks a bit like you've mixed the versions, maybe copied a newer file while still using old TensorFlow PIP?\n", "Closing, since @lukaszkaiser seems to have explained the problem.  We don't support mixing models defined in the tensorflow repo with a different version of tensorflow.\n", "Thanks a lot! It's the problem. TF 0.8 has a new way to call function embedding_attention_seq2seq.\n"]}, {"number": 2534, "title": " load the  readable data", "body": "``` py\ndataset = np.ndarray(shape=(len(image_files), image_size, image_size),\n                         dtype=np.float32)\nfor image_index, image in enumerate(image_files):\n    ...\nnum_images = image_index + 1\ndataset = dataset[0:num_images, :, :]\n```\n\nBy doing so, `num_images` will always equal to `len(image_files)`. \nSo  the code here, `dataset = dataset[0:num_images, :, :]` cannot  select the valid (readable) images.\nBut from the output, it works just fine. \nWhy it work fine??\nI think the job can be done by the following code.\n\n`````` py\ndataset = np.ndarray(shape=(len(image_files), image_size, image_size),\n                         dtype=np.float32)\nnum_images = 0\nfor  image in image_files:\n    ...\n    num_images =  num_images + 1\ndataset = dataset[0:num_images, :, :]\n```\"\n``````\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "Thanks ! And I signed it!\n\n\u5728 2016-05-27 18:19:13\uff0c\"googlebot\" notifications@github.com \u5199\u9053\uff1a\n\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n\ud83d\udcddPlease visit https://cla.developers.google.com/ to sign.\n\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify. Thanks.\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address. Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Good catch!\n"]}]