[{"number": 6784, "title": "Enhance cuda kernel helper.", "body": "Changes on functionality:\r\n1. Use native atomicAdd for double if `__CUDA_ARCH__ >= 600`.\r\n2. Add max wrappers for float, double and Eigen::half.\r\n3. Add `SetConstant` and `ReplaceValue` kernels. They are useful for example, when doing element-wise max: `SetConstant` to MIN, do max(), then `ReplaceValue` MIN with some friendly values, like zeros.\r\n\r\nChanges on readability:\r\n1. `SetZero(const int nthreads, T* bottom_diff)` to `SetZero(const int nthreads, T* data)`, `bottom_diff` maybe is from Caffe ;-), and is irrelevant here.\r\n2. Reorder Add and Max wrappers, to make them better grouped together.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Could we add a test for this?", "Sure! I know how to write a test for an op in python, but what's the best practice for writing a test for these kind of lower level functions? Any code I can follow?", "Fabulous! We have two kinds of `cc_test`s, one for gpu, and one for CPU. This will go in the GPU ones. You can look at `tensorflow/core/BUILD` and see what was done for `common_runtime/gpu/gpu_bfc_allocator_test.cc`. You'll create one build rule for yourself and put something in a file called `tensorflow/core/util/cuda_kernel_helper_test.cc` and follow the regular gtest template, like the BFC allocator. Let me know if you need more help.", "It looks like there are a number of independent changes in this patch.  Can you please split them up into separate commits?  (You can leave it as one PR if you like, or split it up into multiple PRs.)\r\n\r\nSplitting up the patch makes our lives as reviewers much easier, since the difficulty of reviewing a patch is roughly quadratic in the size of the patch.  It also makes it easier for us to verify that each change is appropriately tested.\r\n\r\nFeel free to ask if you need help with the git commands for splitting up the change.  There are lots of tutorials on the web, but in general, `git add -p`, `git commit --amend` and `git rebase -i` are your friends.", "@yangyanli could you address @jlebar's comments, please?", "Can one of the admins verify this patch?", "I've done the commit splits @jlebar @rmlarsen . Will add the tests as suggested by @drpngx a bit later, as it's during my Chinese New Year ;-)", "Hi @drpngx I've added a bit test for it, in commit 28fd861, however, I cannot get it compile. Could you help on it?", "Jenkins, test this please.", "What error are you getting?", "You are including `thrust/host_vector.h`. It's not part of our dependencies. Can you do without it?", "Any updates @yangyanli  ?", "Closing for now to keep our open PR list smaller, but please feel free to open another PR once you have time to contribute this -- it's very much appreciated!"]}, {"number": 6783, "title": "TensorFlow GPU, CUDA_ERROR_LAUNCH_FAILED on tf.one_hot()", "body": "Use tf.one_hot() on Windows 10, GPU, Nvidia 970.  Error CUDA_ERROR_LAUNCH_FAILED happens.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nI have post an issue in http://stackoverflow.com/questions/41115476/tensorflow-gpu-cuda-error-launch-failed-on-tf-one-hot .  No solution found.  More people report the same issue.\r\n\r\n### Environment info\r\n\r\n- TensorFlow 0.12.0-rc1\r\n- Python 3.5\r\n- CUDA 8.0\r\n- cuDNN 5.1\r\n- OS: Windows 10\r\n- GPU: GeForce GTX 970\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n```\r\nimport tensorflow as tf\r\nidx_0 = tf.placeholder(tf.int64, [None])\r\nmask = tf.one_hot(idx_0, 3, axis=-1)\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\na = sess.run([mask],feed_dict={idx_0:[0,1,2]})\r\nprint(a)\r\n```\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n1. Run the code in same machine, Linux, GPU.  No error.\r\n2. Run the code in same machine, Windows, CPU.  No error.\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n\r\n```\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:1177] could not synchronize on CUDA context: CUDA_ERROR_LAUNCH_FAILED :: No stack trace available\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED\r\nF c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_util.cc:370] GPU sync failed\r\n```", "comments": ["There is an issue with memory allocation on GTX 970 when trying to go over 3.5GB but in this case it seems related to #6509 and Windows/GPU specific.", "@mrry do you think this should be closed out as a duplicate of #6509?", "Yes, let's close this as a duplicate, since it looks like it's the same problem (i.e. `tf.one_hot()` is broken for Windows/GPU)."]}, {"number": 6782, "title": "./configure has issues after copying the tensorflow codebase to a different machine", "body": "I copied my modified tensorflow codebase to a different machine. After running ./configure, I got the following errors. How can I fix it?\r\n\r\nERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:17:3: //external:eigen_archive: no such attribute 'urls' in 'new_http_archive' rule.\r\nERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:17:3: //external:eigen_archive: missing value for mandatory attribute 'url' in 'new_http_archive' rule.\r\nERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:28:3: //external:libxsmm_archive: no such attribute 'urls' in 'new_http_archive' rule.\r\nERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:28:3: //external:libxsmm_archive: missing value for mandatory attribute 'url' in 'new_http_archive' rule.\r\nERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:44:3: //external:com_googlesource_code_re2: no such attribute 'urls' in 'http_archive' rule.\r\nERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:44:3: //external:com_googlesource_code_re2: missing value for mandatory attribute 'url' in 'http_archive' rule.\r\nERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:54:3: //external:gemmlowp: no such attribute 'urls' in 'http_archive' rule.\r\nERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:54:3: //external:gemmlowp: missing value for mandatory attribute 'url' in 'http_archive' rule.\r\nERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:64:3: //external:farmhash_archive: no such attribute 'urls' in 'new_http_archive' rule.\r\nERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:64:3: //external:farmhash_archive: missing value for mandatory attribute 'url' in 'new_http_archive' rule.\r\nERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:80:3: //external:highwayhash: no such attribute 'urls' in 'http_archive' rule.\r\nERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:80:3: //external:highwayhash: missing value for mandatory attribute 'url' in 'http_archive' rule.\r\nERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:90:3: //external:nasm: no such attribute 'urls' in 'new_http_archive' rule.\r\nERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:90:3: //external:nasm: missing value for mandatory attribute 'url' in 'new_http_archive' rule.\r\nERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:101:3: //external:jpeg: no such attribute 'urls' in 'new_http_archive' rule.\r\nERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:101:3: //external:jpeg: missing value for mandatory attribute 'url' in 'new_http_archive' rule.\r\nERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:112:3: //external:png_archive: no such attribute 'urls' in 'new_http_archive' rule.\r\nERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:112:3: //external:png_archive: missing value for mandatory attribute 'url' in 'new_http_archive' rule.\r\nERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:123:3: //external:gif_archive: no such attribute 'urls' in 'new_http_archive' rule.\r\nERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:123:3: //external:gif_archive: missing value for mandatory attribute 'url' in 'new_http_archive' rule.\r\nERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:135:3: //external:six_archive: no such attribute 'urls' in 'new_http_archive' rule.\r\nERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:135:3: //external:six_archive: missing value for mandatory attribute 'url' in 'new_http_archive' rule.\r\nERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:151:3: //external:protobuf: no such attribute 'urls' in 'http_archive' rule.\r\nERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:151:3: //external:protobuf: missing value for mandatory attribute 'url' in 'http_archive' rule.\r\nERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:161:3: //external:gmock_archive: no such attribute 'urls' in 'new_http_archive' rule.\r\nERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:161:3: //external:gmock_archive: missing value for mandatory attribute 'url' in 'new_http_archive' rule.\r\nERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:187:3: //external:pcre: no such attribute 'urls' in 'new_http_archive' rule.\r\nERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:187:3: //external:pcre: missing value for mandatory attribute 'url' in 'new_http_archive' rule.\r\nERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:198:3: //external:swig: no such attribute 'urls' in 'new_http_archive' rule.\r\nERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:198:3: //external:swig: missing value for mandatory attribute 'url' in 'new_http_archive' rule.\r\nERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:210:3: //external:curl: no such attribute 'urls' in 'new_http_archive' rule.\r\nERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:210:3: //external:curl: missing value for mandatory attribute 'url' in 'new_http_archive' rule.\r\nERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:233:3: //external:grpc: no such attribute 'urls' in 'new_http_archive' rule.\r\nERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:233:3: //external:grpc: missing value for mandatory attribute 'url' in 'new_http_archive' rule.\r\nERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:256:3: //external:linenoise: no such attribute 'urls' in 'new_http_archive' rule.\r\nERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:256:3: //external:linenoise: missing value for mandatory attribute 'url' in 'new_http_archive' rule.\r\nERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:269:3: //external:llvm: no such attribute 'urls' in 'new_http_archive' rule.\r\nERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:269:3: //external:llvm: missing value for mandatory attribute 'url' in 'new_http_archive' rule.\r\nERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:280:3: //external:jsoncpp_git: no such attribute 'urls' in 'new_http_archive' rule.\r\nERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:280:3: //external:jsoncpp_git: missing value for mandatory attribute 'url' in 'new_http_archive' rule.\r\nERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:296:3: //external:boringssl: no such attribute 'urls' in 'http_archive' rule.\r\nERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:296:3: //external:boringssl: missing value for mandatory attribute 'url' in 'http_archive' rule.\r\nERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:306:3: //external:nanopb_git: no such attribute 'urls' in 'new_http_archive' rule.\r\nERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:306:3: //external:nanopb_git: missing value for mandatory attribute 'url' in 'new_http_archive' rule.\r\nERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:322:3: //external:zlib_archive: no such attribute 'urls' in 'new_http_archive' rule.\r\nERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:322:3: //external:zlib_archive: missing value for mandatory attribute 'url' in 'new_http_archive' rule.\r\nERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': error loading package 'external': Could not load //external package.\r\nERROR: missing fetch expression. Type 'bazel help fetch' for syntax and help.", "comments": ["Thanks for reaching out. Your other machine isn't running Bazel \u22650.4.2. Installing the latest Bazel will do the trick."]}, {"number": 6781, "title": "Java API: How to implement a PlaceHolder", "body": "I've been loving experimenting with @asimshankar 's Java API, and finally got it working tonight with a model that i've trained myself based of of inception. However, I noticed his comment in his tutorial/demo java class that says:\r\n\r\n> Since the graph is being constructed once per execution here, we can use a constant for the input image. If the graph were to be re-used for multiple input images, a placeholder would have been more appropriate.\r\n\r\nI'd be very interested in seeing how to do this with the java API. I want to be able to optimize using the same graph for a bunch of images which would be fed into the graph in real-time, so optimization is important. Would a modification to the graph be necessary in order to use a placeholder, or is it just a different manipulation of the API? I couldn't find any references to a placeholder class or method. Any help is much appreciated. ", "comments": ["Thanks for reaching out. We try to keep this issue tracker limited to bugs and feature requests. We recommend posting on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) for community-driven support. Unless of course @asimshankar is interesting in weighing in on this question.", "@souterjk : Such questions are better suited for Stackoverflow as @jart pointed out. That said, use of the placeholder is like any other operation. The following example shows construction of a trivial graph to add two float tensors and then repeated invocation of the single graph using placeholders:\r\n\r\n```java\r\npackage example;\r\n\r\nimport org.tensorflow.DataType;\r\nimport org.tensorflow.Graph;\r\nimport org.tensorflow.Output;\r\nimport org.tensorflow.Session;\r\nimport org.tensorflow.Tensor;\r\n\r\npublic class Example {\r\n  public static void main(String[] args) {\r\n    try (Graph g = new Graph();\r\n\t Session s = new Session(g)) {\r\n      // Construct a graph to add two float Tensors, using placeholders.\r\n      Output x = g.opBuilder(\"Placeholder\", \"x\").setAttr(\"dtype\", DataType.FLOAT).build().output(0);\r\n      Output y = g.opBuilder(\"Placeholder\", \"y\").setAttr(\"dtype\", DataType.FLOAT).build().output(0);\r\n      Output z = g.opBuilder(\"Add\", \"z\").addInput(x).addInput(y).build().output(0);\r\n      // Execute the graph multiple times, each time with a different value of x and y\r\n      float[] X = new float[]{1,2,3};\r\n      float[] Y = new float[]{4,5,6};\r\n      for (int i = 0; i < X.length; i++) {\r\n        try (Tensor tx = Tensor.create(X[i]);\r\n\t     Tensor ty = Tensor.create(Y[i]);\r\n\t     Tensor tz = s.runner().feed(\"x\", tx).feed(\"y\", ty).fetch(\"z\").run().get(0)) {\r\n\t  System.out.println(X[i] + \" + \" + Y[i] + \" = \" + tz.floatValue());\r\n\t}\r\n      }\r\n    }\r\n  }\r\n}\r\n```", "@asimshankar the remaining gap in defining a placeholder op is how to specify the shape of the placeholder (otherwise a tensor of any shape is allowed).   The OpBuilder can't yet articulate the `shape` attribute.", "@EronWright : As you gathered, the use of placeholders doesn't _require_ that the shape be specified, but you're right, we do want that ability.\r\n\r\nFeel free to file a new issue for that, though I do plan to have some rudimentary support for a Shape type and setting attributes soon."]}, {"number": 6780, "title": "crash when run distributed training", "body": "When I run distributed training on tensorflow 0.12, everything ok at first, loss and global step was printed.\r\nBut after thoudsands of steps, following errors appear\r\n\r\ntensorflow.python.framework.errors_impl.UnavailableError: {\"created\":\"@1484093728.844289839\",\"description\":\"EOF\",\"file\":\"external/grpc/src/core/lib/iomgr/tcp_posix.c\",\"file_line\":235,\" grpc_status\":14}\r\n\r\nand\r\n\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device to node 'save/RestoreV2_26': Could not satisfy explicit device specification\r\n\r\nand\r\n\r\nE tensorflow/core/distributed_runtime/master_session.cc:1372] Cleanup partition error: Unavailable", "comments": ["Can you give complete backtrace? I can't see what device it was trying to assign to.\r\n\r\nAlso, googling error code shows that this code can be returned when connection can't be established.\r\n\r\nCurrently if connection is dropped, TensorFlow session can become invalid and process needs to be restarted. Not sure if anything can be done about error messages @mrry ", "@yaroslavvb Here's the complete backtrace:)\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/dump/11/nm-local-dir/usercache/admin/appcache/application_1477026424983_244601/container_e04_1477026424983_244601_01_000116/app/install/learning_to_match/bootstrap/train_on_yarn.py\", line 42, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 43, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"/dump/11/nm-local-dir/usercache/admin/appcache/application_1477026424983_244601/container_e04_1477026424983_244601_01_000116/app/install/learning_to_match/bootstrap/train_on_yarn.py\", line 38, in main\r\n    train().start(unused_args=unused_argv)\r\n  File \"/dump/11/nm-local-dir/usercache/admin/appcache/application_1477026424983_244601/container_e04_1477026424983_244601_01_000116/app/install/yarn_bootstrap.py\", line 94, in start\r\n    self.worker_do(server, cluster_spec, FLAGS.task_index)\r\n  File \"/dump/11/nm-local-dir/usercache/admin/appcache/application_1477026424983_244601/container_e04_1477026424983_244601_01_000116/app/install/learning_to_match/bootstrap/train_on_yarn.py\", line 27, in worker_do\r\n    distribution_training.run_training(server.target, cluster_spec, task_id)\r\n  File \"/dump/11/nm-local-dir/usercache/admin/appcache/application_1477026424983_244601/container_e04_1477026424983_244601_01_000116/app/install/learning_to_match/bootstrap/../train/distribution_training.py\", line 62, in run_training\r\n    max_steps=train_max_steps\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py\", line 191, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 355, in fit\r\n    max_steps=max_steps)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 733, in _train_model\r\n    max_steps=max_steps)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/graph_actions.py\", line 301, in _monitored_train\r\n    None)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 473, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 628, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 595, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 729, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 595, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 766, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 964, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.UnavailableError: {\"created\":\"@1484093728.844289839\",\"description\":\"EOF\",\"file\":\"external/grpc/src/core/lib/iomgr/tcp_posix.c\",\"file_line\":235,\"grpc_status\":14}\r\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:197] Initialize GrpcChannelCache for job ps -> {0 -> hadoop0560.et2.tbsite.net:25868, 1 -> hadoop0138.et2.tbsite.net:27562, 2 -> hadoop0177.et2.tbsite.net:26103, 3 -> hadoop0712.et2.tbsite.net:26106, 4 -> hadoop5021.et2.tbsite.net:29662, 5 -> hadoop0616.et2.tbsite.net:26974, 6 -> hadoop0635.et2.tbsite.net:29028, 7 -> hadoop0580.et2.tbsite.net:29141, 8 -> hadoop0587.et2.tbsite.net:28455, 9 -> hadoop1845.et2.tbsite.net:27397, 10 -> hadoop1892.et2.tbsite.net:28091, 11 -> hadoop0511.et2.tbsite.net:27469, 12 -> hadoop0744.et2.tbsite.net:25843, 13 -> hadoop0621.et2.tbsite.net:25316, 14 -> hadoop0067.et2.tbsite.net:26703, 15 -> hadoop0612.et2.tbsite.net:28886}\r\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:197] Initialize GrpcChannelCache for job worker -> {0 -> hadoop0743.et2.tbsite.net:28429, 1 -> hadoop0245.et2.tbsite.net:28221, 2 -> hadoop0633.et2.tbsite.net:25817, 3 -> hadoop0729.et2.tbsite.net:26892, 4 -> hadoop0642.et2.tbsite.net:29372, 5 -> hadoop1006.et2.tbsite.net:26049, 6 -> hadoop5035.et2.tbsite.net:26419, 7 -> hadoop0718.et2.tbsite.net:26232, 8 -> hadoop0947.et2.tbsite.net:27491, 9 -> hadoop5028.et2.tbsite.net:25950, 10 -> hadoop1873.et2.tbsite.net:27579, 11 -> hadoop0618.et2.tbsite.net:27564, 12 -> hadoop0172.et2.tbsite.net:26764, 13 -> hadoop0637.et2.tbsite.net:28835, 14 -> hadoop1863.et2.tbsite.net:28968, 15 -> hadoop0614.et2.tbsite.net:26038, 16 -> hadoop0715.et2.tbsite.net:29649, 17 -> hadoop0735.et2.tbsite.net:25061, 18 -> hadoop1003.et2.tbsite.net:27781, 19 -> hadoop5047.et2.tbsite.net:26782, 20 -> hadoop0738.et2.tbsite.net:25868, 21 -> hadoop5041.et2.tbsite.net:29389, 22 -> hadoop1872.et2.tbsite.net:25251, 23 -> hadoop0891.et2.tbsite.net:25213, 24 -> hadoop0785.et2.tbsite.net:29996, 25 -> hadoop0809.et2.tbsite.net:29229, 26 -> hadoop1836.et2.tbsite.net:29542, 27 -> hadoop0592.et2.tbsite.net:28786, 28 -> hadoop0714.et2.tbsite.net:28822, 29 -> hadoop0572.et2.tbsite.net:27784, 30 -> hadoop1838.et2.tbsite.net:29014, 31 -> hadoop0600.et2.tbsite.net:29302, 32 -> hadoop0178.et2.tbsite.net:25334, 33 -> hadoop0471.et2.tbsite.net:25353, 34 -> hadoop0649.et2.tbsite.net:25981, 35 -> hadoop1848.et2.tbsite.net:26250, 36 -> hadoop0569.et2.tbsite.net:28590, 37 -> hadoop0589.et2.tbsite.net:25475, 38 -> hadoop0771.et2.tbsite.net:29900, 39 -> hadoop5030.et2.tbsite.net:25125, 40 -> hadoop1890.et2.tbsite.net:27007, 41 -> hadoop0946.et2.tbsite.net:28429, 42 -> hadoop1859.et2.tbsite.net:26312, 43 -> hadoop1828.et2.tbsite.net:25200, 44 -> hadoop0762.et2.tbsite.net:27702, 45 -> hadoop5038.et2.tbsite.net:27372, 46 -> hadoop0609.et2.tbsite.net:29673, 47 -> hadoop1878.et2.tbsite.net:29987, 48 -> hadoop0704.et2.tbsite.net:25593, 49 -> hadoop0815.et2.tbsite.net:26288, 50 -> hadoop0488.et2.tbsite.net:26090, 51 -> hadoop0826.et2.tbsite.net:28780, 52 -> hadoop0890.et2.tbsite.net:27610, 53 -> hadoop0713.et2.tbsite.net:26176, 54 -> hadoop0845.et2.tbsite.net:28818, 55 -> hadoop1827.et2.tbsite.net:29269, 56 -> hadoop0767.et2.tbsite.net:26084, 57 -> hadoop1853.et2.tbsite.net:26311, 58 -> hadoop0837.et2.tbsite.net:29223, 59 -> hadoop0256.et2.tbsite.net:28043, 60 -> hadoop0728.et2.tbsite.net:26120, 61 -> hadoop0535.et2.tbsite.net:26474, 62 -> hadoop0857.et2.tbsite.net:27474, 63 -> hadoop0482.et2.tbsite.net:28653, 64 -> hadoop0737.et2.tbsite.net:26960, 65 -> hadoop0848.et2.tbsite.net:27215, 66 -> hadoop0606.et2.tbsite.net:29632, 67 -> hadoop0613.et2.tbsite.net:26596, 68 -> hadoop5007.et2.tbsite.net:27294, 69 -> hadoop0698.et2.tbsite.net:29914, 70 -> hadoop0768.et2.tbsite.net:26747, 71 -> hadoop0545.et2.tbsite.net:27286, 72 -> hadoop0578.et2.tbsite.net:26621, 73 -> hadoop0789.et2.tbsite.net:27617, 74 -> hadoop0699.et2.tbsite.net:27052, 75 -> hadoop0627.et2.tbsite.net:27065, 76 -> hadoop0804.et2.tbsite.net:29234, 77 -> hadoop0821.et2.tbsite.net:27680, 78 -> hadoop5048.et2.tbsite.net:25832, 79 -> hadoop5033.et2.tbsite.net:27200, 80 -> hadoop0756.et2.tbsite.net:26536, 81 -> hadoop0689.et2.tbsite.net:26522, 82 -> hadoop0851.et2.tbsite.net:29934, 83 -> hadoop0541.et2.tbsite.net:27118, 84 -> hadoop0647.et2.tbsite.net:27308, 85 -> hadoop0579.et2.tbsite.net:27040, 86 -> hadoop5055.et2.tbsite.net:26841, 87 -> hadoop1882.et2.tbsite.net:26220, 88 -> hadoop0615.et2.tbsite.net:25089, 89 -> hadoop1902.et2.tbsite.net:26867, 90 -> localhost:27983, 91 -> hadoop0807.et2.tbsite.net:28033, 92 -> hadoop5014.et2.tbsite.net:29535, 93 -> hadoop0746.et2.tbsite.net:29884, 94 -> hadoop0250.et2.tbsite.net:26868, 95 -> hadoop0706.et2.tbsite.net:29045, 96 -> hadoop0854.et2.tbsite.net:29149, 97 -> hadoop0185.et2.tbsite.net:26971, 98 -> hadoop0467.et2.tbsite.net:25830, 99 -> hadoop0611.et2.tbsite.net:28720}\r\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:211] Started server with target: grpc://localhost:27983\r\nINFO:tensorflow:Using config: {'save_summary_steps': 100000, '_num_ps_replicas': 16, '_task_type': u'worker', '_environment': u'cloud', '_is_chief': False, 'save_checkpoints_secs': 60, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f11f9b57d50>, 'tf_config': intra_op_parallelism_threads: 4\r\ninter_op_parallelism_threads: 4\r\ngpu_options {\r\n  per_process_gpu_memory_fraction: 1\r\n}\r\n```\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/dump/11/nm-local-dir/usercache/admin/appcache/application_1477026424983_244601/container_e04_1477026424983_244601_01_000116/app/install/learning_to_match/bootstrap/train_on_yarn.py\", line 42, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 43, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"/dump/11/nm-local-dir/usercache/admin/appcache/application_1477026424983_244601/container_e04_1477026424983_244601_01_000116/app/install/learning_to_match/bootstrap/train_on_yarn.py\", line 38, in main\r\n    train().start(unused_args=unused_argv)\r\n  File \"/dump/11/nm-local-dir/usercache/admin/appcache/application_1477026424983_244601/container_e04_1477026424983_244601_01_000116/app/install/yarn_bootstrap.py\", line 94, in start\r\n    self.worker_do(server, cluster_spec, FLAGS.task_index)\r\n  File \"/dump/11/nm-local-dir/usercache/admin/appcache/application_1477026424983_244601/container_e04_1477026424983_244601_01_000116/app/install/learning_to_match/bootstrap/train_on_yarn.py\", line 27, in worker_do\r\n    distribution_training.run_training(server.target, cluster_spec, task_id)\r\n  File \"/dump/11/nm-local-dir/usercache/admin/appcache/application_1477026424983_244601/container_e04_1477026424983_244601_01_000116/app/install/learning_to_match/bootstrap/../train/distribution_training.py\", line 62, in run_training\r\n    max_steps=train_max_steps\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py\", line 191, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 355, in fit\r\n    max_steps=max_steps)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 733, in _train_model\r\n    max_steps=max_steps)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/graph_actions.py\", line 297, in _monitored_train\r\n    hooks=all_hooks) as super_sess:\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 447, in __init__\r\n    self._sess = _RecoverableSession(self._coordinated_creator)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 618, in __init__\r\n    _WrappedSession.__init__(self, self._sess_creator.create_session())\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 505, in create_session\r\n    self.tf_sess = self._session_creator.create_session()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 374, in create_session\r\n    self._master, config=self._config)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py\", line 346, in wait_for_session\r\n    sess)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py\", line 467, in _try_run_local_init_op\r\n    sess.run(self._local_init_op)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 766, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 964, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device to node 'save/RestoreV2_26': Could not satisfy explicit device specification '/job:ps/task:4/device:CPU:0' because no devices matching that specification are registered in this process; available devices: /job:ps/replica:0/task:0/cpu:0, /job:ps/replica:0/task:1/cpu:0, /job:ps/replica:0/task:10/cpu:0, /job:ps/replica:0/task:11/cpu:0, /job:ps/replica:0/task:12/cpu:0, /job:ps/replica:0/task:13/cpu:0, /job:ps/replica:0/task:14/cpu:0, /job:ps/replica:0/task:15/cpu:0, /job:ps/replica:0/task:2/cpu:0, /job:ps/replica:0/task:3/cpu:0, /job:ps/replica:0/task:5/cpu:0, /job:ps/replica:0/task:6/cpu:0, /job:ps/replica:0/task:7/cpu:0, /job:ps/replica:0/task:8/cpu:0, /job:ps/replica:0/task:9/cpu:0, /job:worker/replica:0/task:0/cpu:0, /job:worker/replica:0/task:1/cpu:0, /job:worker/replica:0/task:10/cpu:0, /job:worker/replica:0/task:11/cpu:0, /job:worker/replica:0/task:12/cpu:0, /job:worker/replica:0/task:13/cpu:0, /job:worker/replica:0/task:14/cpu:0, /job:worker/replica:0/task:15/cpu:0, /job:worker/replica:0/task:16/cpu:0, /job:worker/replica:0/task:17/cpu:0, /job:worker/replica:0/task:18/cpu:0, /job:worker/replica:0/task:19/cpu:0, /job:worker/replica:0/task:2/cpu:0, /job:worker/replica:0/task:20/cpu:0, /job:worker/replica:0/task:21/cpu:0, /job:worker/replica:0/task:22/cpu:0, /job:worker/replica:0/task:23/cpu:0, /job:worker/replica:0/task:24/cpu:0, /job:worker/replica:0/task:25/cpu:0, /job:worker/replica:0/task:26/cpu:0, /job:worker/replica:0/task:27/cpu:0, /job:worker/replica:0/task:28/cpu:0, /job:worker/replica:0/task:29/cpu:0, /job:worker/replica:0/task:3/cpu:0, /job:worker/replica:0/task:30/cpu:0, /job:worker/replica:0/task:31/cpu:0, /job:worker/replica:0/task:32/cpu:0, /job:worker/replica:0/task:33/cpu:0, /job:worker/replica:0/task:34/cpu:0, /job:worker/replica:0/task:35/cpu:0, /job:worker/replica:0/task:36/cpu:0, /job:worker/replica:0/task:37/cpu:0, /job:worker/replica:0/task:38/cpu:0, /job:worker/replica:0/task:39/cpu:0, /job:worker/replica:0/task:4/cpu:0, /job:worker/replica:0/task:40/cpu:0, /job:worker/replica:0/task:41/cpu:0, /job:worker/replica:0/task:42/cpu:0, /job:worker/replica:0/task:43/cpu:0, /job:worker/replica:0/task:44/cpu:0, /job:worker/replica:0/task:45/cpu:0, /job:worker/replica:0/task:46/cpu:0, /job:worker/replica:0/task:47/cpu:0, /job:worker/replica:0/task:48/cpu:0, /job:worker/replica:0/task:49/cpu:0, /job:worker/replica:0/task:5/cpu:0, /job:worker/replica:0/task:50/cpu:0, /job:worker/replica:0/task:51/cpu:0, /job:worker/replica:0/task:52/cpu:0, /job:worker/replica:0/task:53/cpu:0, /job:worker/replica:0/task:54/cpu:0, /job:worker/replica:0/task:55/cpu:0, /job:worker/replica:0/task:56/cpu:0, /job:worker/replica:0/task:57/cpu:0, /job:worker/replica:0/task:58/cpu:0, /job:worker/replica:0/task:59/cpu:0, /job:worker/replica:0/task:6/cpu:0, /job:worker/replica:0/task:60/cpu:0, /job:worker/replica:0/task:61/cpu:0, /job:worker/replica:0/task:62/cpu:0, /job:worker/replica:0/task:63/cpu:0, /job:worker/replica:0/task:64/cpu:0, /job:worker/replica:0/task:65/cpu:0, /job:worker/replica:0/task:66/cpu:0, /job:worker/replica:0/task:67/cpu:0, /job:worker/replica:0/task:68/cpu:0, /job:worker/replica:0/task:69/cpu:0, /job:worker/replica:0/task:7/cpu:0, /job:worker/replica:0/task:70/cpu:0, /job:worker/replica:0/task:71/cpu:0, /job:worker/replica:0/task:72/cpu:0, /job:worker/replica:0/task:73/cpu:0, /job:worker/replica:0/task:74/cpu:0, /job:worker/replica:0/task:75/cpu:0, /job:worker/replica:0/task:76/cpu:0, /job:worker/replica:0/task:77/cpu:0, /job:worker/replica:0/task:78/cpu:0, /job:worker/replica:0/task:79/cpu:0, /job:worker/replica:0/task:8/cpu:0, /job:worker/replica:0/task:80/cpu:0, /job:worker/replica:0/task:81/cpu:0, /job:worker/replica:0/task:82/cpu:0, /job:worker/replica:0/task:83/cpu:0, /job:worker/replica:0/task:84/cpu:0, /job:worker/replica:0/task:85/cpu:0, /job:worker/replica:0/task:86/cpu:0, /job:worker/replica:0/task:87/cpu:0, /job:worker/replica:0/task:88/cpu:0, /job:worker/replica:0/task:89/cpu:0, /job:worker/replica:0/task:9/cpu:0, /job:worker/replica:0/task:90/cpu:0, /job:worker/replica:0/task:91/cpu:0, /job:worker/replica:0/task:92/cpu:0, /job:worker/replica:0/task:93/cpu:0, /job:worker/replica:0/task:94/cpu:0, /job:worker/replica:0/task:95/cpu:0, /job:worker/replica:0/task:96/cpu:0, /job:worker/replica:0/task:97/cpu:0, /job:worker/replica:0/task:98/cpu:0, /job:worker/replica:0/task:99/cpu:0\r\n     [[Node: save/RestoreV2_26 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:ps/task:4/device:CPU:0\"](save/Const, save/RestoreV2_26/tensor_names, save/RestoreV2_26/shape_and_slices)]]\r\n\r\nCaused by op u'save/RestoreV2_26', defined at:\r\n  File \"/dump/11/nm-local-dir/usercache/admin/appcache/application_1477026424983_244601/container_e04_1477026424983_244601_01_000116/app/install/learning_to_match/bootstrap/train_on_yarn.py\", line 42, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 43, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"/dump/11/nm-local-dir/usercache/admin/appcache/application_1477026424983_244601/container_e04_1477026424983_244601_01_000116/app/install/learning_to_match/bootstrap/train_on_yarn.py\", line 38, in main\r\n    train().start(unused_args=unused_argv)\r\n  File \"/dump/11/nm-local-dir/usercache/admin/appcache/application_1477026424983_244601/container_e04_1477026424983_244601_01_000116/app/install/yarn_bootstrap.py\", line 94, in start\r\n    self.worker_do(server, cluster_spec, FLAGS.task_index)\r\n  File \"/dump/11/nm-local-dir/usercache/admin/appcache/application_1477026424983_244601/container_e04_1477026424983_244601_01_000116/app/install/learning_to_match/bootstrap/train_on_yarn.py\", line 27, in worker_do\r\n    distribution_training.run_training(server.target, cluster_spec, task_id)\r\n  File \"/dump/11/nm-local-dir/usercache/admin/appcache/application_1477026424983_244601/container_e04_1477026424983_244601_01_000116/app/install/learning_to_match/bootstrap/../train/distribution_training.py\", line 62, in run_training\r\n    max_steps=train_max_steps\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py\", line 191, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 355, in fit\r\n    max_steps=max_steps)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 733, in _train_model\r\n    max_steps=max_steps)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/graph_actions.py\", line 297, in _monitored_train\r\n    hooks=all_hooks) as super_sess:\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 447, in __init__\r\n    self._sess = _RecoverableSession(self._coordinated_creator)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 618, in __init__\r\n    _WrappedSession.__init__(self, self._sess_creator.create_session())\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 505, in create_session\r\n    self.tf_sess = self._session_creator.create_session()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 372, in create_session\r\n    self._scaffold.finalize()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 163, in finalize\r\n    self._saver.build()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1030, in build\r\n    restore_sequentially=self._restore_sequentially)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 620, in build\r\n    restore_sequentially, reshape)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 404, in _AddShardedRestoreOps\r\n    name=\"restore_shard\"))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 361, in _AddRestoreOps\r\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 200, in restore_op\r\n    [spec.tensor.dtype])[0])\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 441, in restore_v2\r\n    dtypes=dtypes, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): Cannot assign a device to node 'save/RestoreV2_26': Could not satisfy explicit device specification '/job:ps/task:4/device:CPU:0' because no devices matching that specification are registered in this process; available devices: /job:ps/replica:0/task:0/cpu:0, /job:ps/replica:0/task:1/cpu:0, /job:ps/replica:0/task:10/cpu:0, /job:ps/replica:0/task:11/cpu:0, /job:ps/replica:0/task:12/cpu:0, /job:ps/replica:0/task:13/cpu:0, /job:ps/replica:0/task:14/cpu:0, /job:ps/replica:0/task:15/cpu:0, /job:ps/replica:0/task:2/cpu:0, /job:ps/replica:0/task:3/cpu:0, /job:ps/replica:0/task:5/cpu:0, /job:ps/replica:0/task:6/cpu:0, /job:ps/replica:0/task:7/cpu:0, /job:ps/replica:0/task:8/cpu:0, /job:ps/replica:0/task:9/cpu:0, /job:worker/replica:0/task:0/cpu:0, /job:worker/replica:0/task:1/cpu:0, /job:worker/replica:0/task:10/cpu:0, /job:worker/replica:0/task:11/cpu:0, /job:worker/replica:0/task:12/cpu:0, /job:worker/replica:0/task:13/cpu:0, /job:worker/replica:0/task:14/cpu:0, /job:worker/replica:0/task:15/cpu:0, /job:worker/replica:0/task:16/cpu:0, /job:worker/replica:0/task:17/cpu:0, /job:worker/replica:0/task:18/cpu:0, /job:worker/replica:0/task:19/cpu:0, /job:worker/replica:0/task:2/cpu:0, /job:worker/replica:0/task:20/cpu:0, /job:worker/replica:0/task:21/cpu:0, /job:worker/replica:0/task:22/cpu:0, /job:worker/replica:0/task:23/cpu:0, /job:worker/replica:0/task:24/cpu:0, /job:worker/replica:0/task:25/cpu:0, /job:worker/replica:0/task:26/cpu:0, /job:worker/replica:0/task:27/cpu:0, /job:worker/replica:0/task:28/cpu:0, /job:worker/replica:0/task:29/cpu:0, /job:worker/replica:0/task:3/cpu:0, /job:worker/replica:0/task:30/cpu:0, /job:worker/replica:0/task:31/cpu:0, /job:worker/replica:0/task:32/cpu:0, /job:worker/replica:0/task:33/cpu:0, /job:worker/replica:0/task:34/cpu:0, /job:worker/replica:0/task:35/cpu:0, /job:worker/replica:0/task:36/cpu:0, /job:worker/replica:0/task:37/cpu:0, /job:worker/replica:0/task:38/cpu:0, /job:worker/replica:0/task:39/cpu:0, /job:worker/replica:0/task:4/cpu:0, /job:worker/replica:0/task:40/cpu:0, /job:worker/replica:0/task:41/cpu:0, /job:worker/replica:0/task:42/cpu:0, /job:worker/replica:0/task:43/cpu:0, /job:worker/replica:0/task:44/cpu:0, /job:worker/replica:0/task:45/cpu:0, /job:worker/replica:0/task:46/cpu:0, /job:worker/replica:0/task:47/cpu:0, /job:worker/replica:0/task:48/cpu:0, /job:worker/replica:0/task:49/cpu:0, /job:worker/replica:0/task:5/cpu:0, /job:worker/replica:0/task:50/cpu:0, /job:worker/replica:0/task:51/cpu:0, /job:worker/replica:0/task:52/cpu:0, /job:worker/replica:0/task:53/cpu:0, /job:worker/replica:0/task:54/cpu:0, /job:worker/replica:0/task:55/cpu:0, /job:worker/replica:0/task:56/cpu:0, /job:worker/replica:0/task:57/cpu:0, /job:worker/replica:0/task:58/cpu:0, /job:worker/replica:0/task:59/cpu:0, /job:worker/replica:0/task:6/cpu:0, /job:worker/replica:0/task:60/cpu:0, /job:worker/replica:0/task:61/cpu:0, /job:worker/replica:0/task:62/cpu:0, /job:worker/replica:0/task:63/cpu:0, /job:worker/replica:0/task:64/cpu:0, /job:worker/replica:0/task:65/cpu:0, /job:worker/replica:0/task:66/cpu:0, /job:worker/replica:0/task:67/cpu:0, /job:worker/replica:0/task:68/cpu:0, /job:worker/replica:0/task:69/cpu:0, /job:worker/replica:0/task:7/cpu:0, /job:worker/replica:0/task:70/cpu:0, /job:worker/replica:0/task:71/cpu:0, /job:worker/replica:0/task:72/cpu:0, /job:worker/replica:0/task:73/cpu:0, /job:worker/replica:0/task:74/cpu:0, /job:worker/replica:0/task:75/cpu:0, /job:worker/replica:0/task:76/cpu:0, /job:worker/replica:0/task:77/cpu:0, /job:worker/replica:0/task:78/cpu:0, /job:worker/replica:0/task:79/cpu:0, /job:worker/replica:0/task:8/cpu:0, /job:worker/replica:0/task:80/cpu:0, /job:worker/replica:0/task:81/cpu:0, /job:worker/replica:0/task:82/cpu:0, /job:worker/replica:0/task:83/cpu:0, /job:worker/replica:0/task:84/cpu:0, /job:worker/replica:0/task:85/cpu:0, /job:worker/replica:0/task:86/cpu:0, /job:worker/replica:0/task:87/cpu:0, /job:worker/replica:0/task:88/cpu:0, /job:worker/replica:0/task:89/cpu:0, /job:worker/replica:0/task:9/cpu:0, /job:worker/replica:0/task:90/cpu:0, /job:worker/replica:0/task:91/cpu:0, /job:worker/replica:0/task:92/cpu:0, /job:worker/replica:0/task:93/cpu:0, /job:worker/replica:0/task:94/cpu:0, /job:worker/replica:0/task:95/cpu:0, /job:worker/replica:0/task:96/cpu:0, /job:worker/replica:0/task:97/cpu:0, /job:worker/replica:0/task:98/cpu:0, /job:worker/replica:0/task:99/cpu:0\r\n     [[Node: save/RestoreV2_26 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:ps/task:4/device:CPU:0\"](save/Const, save/RestoreV2_26/tensor_names, save/RestoreV2_26/shape_and_slices)]]\r\n```\r\n\r\n```\r\nINFO:tensorflow:loss = 5.55723, step = 1431908\r\nE tensorflow/core/distributed_runtime/master_session.cc:1372] Cleanup partition error: Unavailable: {\"created\":\"@1484095048.299027908\",\"description\":\"EOF\",\"file\":\"external/grpc/src/core/lib/iomgr/tcp_posix.c\",\"file_line\":235,\"grpc_status\":14}\r\nTraceback (most recent call last):\r\n  File \"/dump/11/nm-local-dir/usercache/admin/appcache/application_1477026424983_244601/container_e04_1477026424983_244601_01_000123/app/install/learning_to_match/bootstrap/train_on_yarn.py\", line 42, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 43, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"/dump/11/nm-local-dir/usercache/admin/appcache/application_1477026424983_244601/container_e04_1477026424983_244601_01_000123/app/install/learning_to_match/bootstrap/train_on_yarn.py\", line 38, in main\r\n    train().start(unused_args=unused_argv)\r\n  File \"/dump/11/nm-local-dir/usercache/admin/appcache/application_1477026424983_244601/container_e04_1477026424983_244601_01_000123/app/install/yarn_bootstrap.py\", line 94, in start\r\n    self.worker_do(server, cluster_spec, FLAGS.task_index)\r\n  File \"/dump/11/nm-local-dir/usercache/admin/appcache/application_1477026424983_244601/container_e04_1477026424983_244601_01_000123/app/install/learning_to_match/bootstrap/train_on_yarn.py\", line 27, in worker_do\r\n    distribution_training.run_training(server.target, cluster_spec, task_id)\r\n  File \"/dump/11/nm-local-dir/usercache/admin/appcache/application_1477026424983_244601/container_e04_1477026424983_244601_01_000123/app/install/learning_to_match/bootstrap/../train/distribution_training.py\", line 62, in run_training\r\n    max_steps=train_max_steps\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py\", line 191, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 355, in fit\r\n    max_steps=max_steps)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 733, in _train_model\r\n    max_steps=max_steps)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/graph_actions.py\", line 301, in _monitored_train\r\n    None)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 473, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 628, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 595, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 729, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 595, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 766, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 964, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.UnavailableError: {\"created\":\"@1484095048.299027908\",\"description\":\"EOF\",\"file\":\"external/grpc/src/core/lib/iomgr/tcp_posix.c\",\"file_line\":235,\"grpc_status\":14}\r\n```", "So it looks like the crash happens in Estimator. This kind of `UnavailableError` can happen if your network connection is interrupted, but a higher level framework should detect these and recreate session rather than crashing everything. cc @ispirmustafa since he's the last person to modify this code", "i have this problem too, but it seems that just restart it with supervisor is a trick solution", "So the issue is that network conditions can interrupt the connection which\nresults in UnavailableError.\nSupervisor doesn't handle this, and session may become invalid, so you need\nto restart session yourself (or just restart the whole program).\n\nMonitoredSession/MonitoredTraining session improve on Supervisor by adding\nretries in presence of UnavailableError\n\nOn Sat, Jan 14, 2017 at 12:05 AM, Eric Yue <notifications@github.com> wrote:\n\n> i have this problem too, but it seems that just restart it with supervisor\n> is a trick solution\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6780#issuecomment-272609422>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AABaHBSlw3BqwbPpFKDg1T9025e_HLEZks5rSIHGgaJpZM4LgItj>\n> .\n>\n", "@buptjz does restarting Supervisor/moving to MonitoredTrainingSession solve the issue?", "Agreed, I'll add recoverable support to UnavailableError.", "Not sure if it's related to this issue, but I observe UnavailableError with MonitoredTrainingSession if Parameter Server is restarted: #7767 ", "Sending out a fix for this right now.", "Fixed.", "help! How to fix this daunting problem  :("]}, {"number": 6779, "title": "Training Slows Down -- But Speeds up If Entire Net is Reloaded", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nThere are a few issues but none that simulate the same problem.\r\n\r\n### Environment info\r\nOperating System: Ubuntu 14.04, tf 0.12\r\n\r\nInstalled version of CUDA and cuDNN: 8.0\r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide: tensorflow pip gpu 0.12\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\n### Problem\r\n\r\nI am training a network, but after about 4000 backward passes, the training of the network starts to slow down. I have tried reloading the network after saving it. And again, 4000 steps later it starts to slow down. This means that the slow down is independent of what global step the training is on. The network size never changes, which is usually the problem with these slow downs. Average Step times are like this:\r\n\r\n`2.4, 2.4, 2.4, 4.6, 3.9, 12.2, 4.2, 2.4, 6.7`\r\n\r\nI am feeding the model with placeholders, so there is no FIFO Queue problems, and feeding it with text data so its pretty light. I have tested this on three separate machines and the same behavior is replicated throughout them. It is the actual tensorflow `session.run` command that slows it down tremendously.\r\n\r\nWhen I look at the gpu usage, both gpu's are at 0 percent the entire time (even during the `session.run` command). They spike up only when the actual forward and backward passes occur but remain dormant the rest of the time. I'm using allocator type 2 in the `tf.gradient` operation. This perhaps is the most revealing part.\r\n\r\nMy `htop` indicates there's no swap memory problems at all. \r\n\r\nFrom what I have concluded the problem must be with tensorflow. The fact that You can reload the model and its completely fine for a little bit is really weird. I've also found that running the following command does not help:\r\n\r\n`sync; echo 3 > /proc/sys/vm/drop_caches`\r\n\r\n```\r\nINFO:tensorflow:global step 2001 learning rate 0.0003882 step-time 2.94 perplexity 28.97 loss 3.3662\r\nINFO:tensorflow:get_batch_step_time 0.016 actual_step_time 2.929\r\nINFO:tensorflow:16:01:09 01/10/17 EST\r\nINFO:tensorflow:global step 2251 learning rate 0.0003867 step-time 2.94 perplexity 27.17 loss 3.3022\r\nINFO:tensorflow:get_batch_step_time 0.016 actual_step_time 2.920\r\nINFO:tensorflow:16:13:25 01/10/17 EST\r\nINFO:tensorflow:global step 2501 learning rate 0.0003853 step-time 2.94 perplexity 26.03 loss 3.2594\r\nINFO:tensorflow:get_batch_step_time 0.016 actual_step_time 2.929\r\nINFO:tensorflow:16:25:48 01/10/17 EST\r\nINFO:tensorflow:global step 2751 learning rate 0.0003838 step-time 2.97 perplexity 24.80 loss 3.2109\r\nINFO:tensorflow:get_batch_step_time 0.016 actual_step_time 2.956\r\nINFO:tensorflow:16:44:37 01/10/17 EST\r\nINFO:tensorflow:global step 3001 learning rate 0.0003824 step-time 4.52 perplexity 23.77 loss 3.1686\r\nINFO:tensorflow:get_batch_step_time 0.016 actual_step_time 4.500\r\nINFO:tensorflow:16:58:05 01/10/17 EST\r\nINFO:tensorflow:global step 3251 learning rate 0.0003809 step-time 3.23 perplexity 23.10 loss 3.1396\r\nINFO:tensorflow:get_batch_step_time 0.016 actual_step_time 3.216\r\nINFO:tensorflow:17:18:34 01/10/17 EST\r\nINFO:tensorflow:global step 3501 learning rate 0.0003795 step-time 4.92 perplexity 21.82 loss 3.0828\r\nINFO:tensorflow:get_batch_step_time 0.015 actual_step_time 4.902\r\n```\r\n\r\nAny help would be greatly appreciated!\r\n", "comments": ["What if you use `tcmalloc`?", "One way to narrow this down is to use google performance tools to take performance profile before and after slowdown, and see if there's some obvious difference in where the time is spent", "Hey @yaroslavvb thanks for your response. I will use `tcmalloc` and try to diagnose what exactly is going on. Do you recommend just running it like this?\r\n\r\n`LD_PRELOAD=/usr/lib/libtcmalloc.so.4 HEAPPROFILE=/tmp/profile python timestep_network.py`", "Something like\n\nexport LD_PRELOAD=\"/usr/lib/libtcmalloc_and_profiler.so.4\"\nexport CPUPROFILE=\"cpu-profile\"\npython lm_benchmark_grpc.py --num_gpus=1\n\nexport exe=`which python`\ngoogle-pprof --pdf $exe $CPUPROFILE > profiling_output.pdf\n\n\nOn Tue, Jan 10, 2017 at 6:59 PM, Nick Shah <notifications@github.com> wrote:\n\n> Hey @yaroslavvb <https://github.com/yaroslavvb> thanks for your response.\n> I will use tcmalloc and try to diagnose what exactly is going on. Do you\n> recommend just running it like this?\n>\n> LD_PRELOAD=/usr/lib/libtcmalloc.so.4 HEAPPROFILE=/tmp/profile python\n> timestep_network.py\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6779#issuecomment-271764841>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AABaHLawIUrOEAKit_v34cn1YJGtYILlks5rREWagaJpZM4LgF36>\n> .\n>\n", "Hey @yaroslavvb I just wanted to update you and let you know for right now, `tcmalloc` seems to have resolved the issue. I haven't fully trained all the way but so far, it does seem to be pretty stable! Thanks! I'll close for right now."]}, {"number": 6778, "title": "Parameter to MergeFrom() must be instance of same class: expected TensorProto got Variable. for field Value.tensor", "body": "I am trying to save variables through checkpoints to introduce fault tolerance to my program. The following is my configuration:-\r\n\r\n```\r\nChiefSessionCreator = tf.train.ChiefSessionCreator(scaffold=None, master='grpc://localhost:2222', config=None, checkpoint_dir='/home/chaitanya/tensorflow/codes/checkpoints')\r\nsummary_hook = tf.train.SummarySaverHook(save_steps=None, save_secs=10, output_dir='/home/chaitanya/tensorflow/codes/savepoints', summary_writer=None, scaffold=None, summary_op=tf.Summary(tf.Summary.Value(tensor=y)))\r\nsaver = tf.train.Saver([tf.Variable(y)])\r\nsaver_hook = tf.train.CheckpointSaverHook(checkpoint_dir='/home/chaitanya/tensorflow/codes/checkpoints', save_secs=10, save_steps=None, saver=saver, checkpoint_basename='model.ckpt', scaffold=None)\r\n```\r\n\r\nIn this, the [CheckpointSaveHook](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard2/tf.train.CheckpointSaverHook.md#tftraincheckpointsaverhook__init__checkpoint_dir-save_secsnone-save_stepsnone-savernone-checkpoint_basenamemodelckpt-scaffoldnone-listenersnone-checkpointsaverhookinit) saves tf.Variable in saver and [SummarySaverHook](https://www.tensorflow.org/versions/master/api_docs/python/train/training_utilities?authuser=2#SummarySaverHook) takes a tensor input in summary_op. The error I get is:-\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"add_1.py\", line 35, in <module>\r\n    summary_hook = tf.train.SummarySaverHook(save_steps=None, save_secs=10, output_dir='/tensorflow/savepoints', summary_writer=None, scaffold=None, summary_op=tf.Summary(tf.Summary.Value(tensor=y)))\r\nTypeError: Parameter to MergeFrom() must be instance of same class: expected TensorProto got Tensor. for field Value.tensor\r\n```\r\n\r\nHow do I rectify this issue?", "comments": ["Thanks for reaching out. Is that the complete backtrace?", "This is the complete error I get:-\r\n```\r\nWARNING:tensorflow:From add_1.py:25 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\r\nInstructions for updating:\r\nUse `tf.global_variables_initializer` instead.\r\nTraceback (most recent call last):\r\n  File \"add_1.py\", line 34, in <module>\r\n    summary_hook = tf.train.SummarySaverHook(save_steps=None, save_secs=10, output_dir='/home/chaitanya/tensorflow/codes/savepoints', summary_writer=None, scaffold=None, summary_op=tf.Summary(tf.Summary.Value(tensor=y)))\r\n  File \"/usr/local/lib/python2.7/dist-packages/google/protobuf/internal/python_message.py\", line 532, in init\r\n    _ReraiseTypeErrorWithFieldName(message_descriptor.name, field_name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/google/protobuf/internal/python_message.py\", line 460, in _ReraiseTypeErrorWithFieldName\r\n    six.reraise(type(exc), exc, sys.exc_info()[2])\r\n  File \"/usr/local/lib/python2.7/dist-packages/google/protobuf/internal/python_message.py\", line 530, in init\r\n    copy.MergeFrom(new_val)\r\n  File \"/usr/local/lib/python2.7/dist-packages/google/protobuf/internal/python_message.py\", line 1233, in MergeFrom\r\n    \"expected %s got %s.\" % (cls.__name__, type(msg).__name__))\r\nTypeError: Parameter to MergeFrom() must be instance of same class: expected TensorProto got Tensor. for field Value.tensor\r\n```", "Maybe what you want is `tf.Summary(value=[tf.Summary.Value(...)])`.\r\n\r\nIn the future please direct questions like these to [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow). We try to keep this issue tracker limited to bugs and feature requests.", "I tired your solution and it gives the same error. Below is the link to my Stackoverflow query:-\r\n\r\n[https://stackoverflow.com/questions/41539934/tensorflow-parameter-to-mergefrom-must-be-instance-of-same-class-expected-ten](https://stackoverflow.com/questions/41539934/tensorflow-parameter-to-mergefrom-must-be-instance-of-same-class-expected-ten)\r\n\r\n", "At second glance, you're most likely not going to get a response to your question, because you don't indicate in your question what `y` is.", "I have added more details to the question. Thanks for the tip."]}, {"number": 6777, "title": "Compiled fails on Ubuntu 16.04 server", "body": "Hi, there,\r\n\r\nI compiled the latest v1.0.0-alpha version on a Ubuntu 16.04 server with GPU support. Here is the error I got:\r\n```\r\n/home/xxu/Extern/tensorflow/tensorflow/core/kernels/BUILD:863:1: error while parsing .d file: /home/xxu/.cache/bazel/_bazel_xxu/09a841ffbaae1d24115959abeaae27c9/execroot/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/core/kernels/_objs/gather_functor_gpu/tensorflow/core/kernels/gather_functor_gpu.cu.pic.d (No such file or directory).\r\nIn file included from ./tensorflow/core/framework/register_types.h:21:0,\r\n                 from ./tensorflow/core/kernels/gather_functor_gpu.cu.h:23,\r\n                 from tensorflow/core/kernels/gather_functor_gpu.cu.cc:20:\r\nbazel-out/local_linux-opt/genfiles/tensorflow/core/framework/resource_handle.pb.h:12:2: error: #error This file was generated by a newer version of protoc which is\r\n #error This file was generated by a newer version of protoc which is\r\n  ^\r\nbazel-out/local_linux-opt/genfiles/tensorflow/core/framework/resource_handle.pb.h:13:2: error: #error incompatible with your Protocol Buffer headers. Please update\r\n #error incompatible with your Protocol Buffer headers.  Please update\r\n  ^\r\nbazel-out/local_linux-opt/genfiles/tensorflow/core/framework/resource_handle.pb.h:14:2: error: #error your headers.\r\n #error your headers.\r\n  ^\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```\r\n\r\nVersions of packages: \r\n1. The protoc --version is 3.1.0. I installed both C++ and python version;\r\n2. CUDA 7.5, cudnn 5\r\n\r\nPS: I compile it without cuda and everything goes well.\r\n", "comments": ["Maybe `@protobuf//:protoc` needs to be added to the `data` attribute of `//tensorflow/tools/pip_package:build_pip_package` and then the script should `export PROTOC=...` to the location of the one built by bazel before calling `bdist_wheel`.\r\n\r\nAs a workaround, you might want to install https://github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz on your system from source.\r\n\r\nWhat should we do about this @gunan?", "This is curious.\r\nMaybe there are two different versions of protoc?\r\nThis message looks interesting to me\r\n```\r\nThis file was generated by a newer version of protoc which is incompatible with your Protocol Buffer headers. Please update your headers.\r\n```\r\n\r\nWhere did you download protobuf packages you installed?\r\n\r\n@jhseu in case he can help.", "@gunan Files like [protobuf_optimized_pip.sh](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/protobuf/protobuf_optimized_pip.sh) lead me to believe that `setup.py bdist_wheel` is doing unholy things as part of the build process, like getting protoc from /usr/bin. Exporting that environment variable in the pip package script would most likely make things a little more hermetically sealed. However it's a shame that Bazel isn't building the pip package. That seems better than delegating to a system build.", "That is exclusively for ci_build, where we build the cpp optimized protobuf pip package.\r\nThat one never gets touched by anything else in our codebase.\r\n\r\nBazel is powerful, it can build a lot of things but IMHO we need to be careful about balancing. \r\nIn a perfect world we let bazel build everything it can.\r\nIn real life, the more we let bazel build things, the more painful our developer experience is going to be as most developers use 2-core/4-core machines. Together with bazel bugs, building tensorflow from sources becomes more and more painful with more dependencies.", "@gunan @jart Thanks your response!\r\n\r\nThe server indeed has multiple version of protobuf because some users run Caffe which need protobuf v2.5. I tested the libprotobuf header in ```/usr/local/include``` and version of protoc. Both of them are v3.1.0. It is downloaded and compiled from release page [https://github.com/google/protobuf/releases/tag/v3.1.0](https://github.com/google/protobuf/releases/tag/v3.1.0).\r\n\r\nFinally I successfully compiled the latest tensorflow. And I want to provide some information for the tensorflow team:\r\nI download CUDA 8 and recompile from source, it works. It seems it is problem with CUDA 7.5 and CUDNN 5.1.3. Therefore, the very wierd information provided is wrong. "]}, {"number": 6776, "title": "Siamese Inception model", "body": "Already tried on StackOverflow - no success.\r\nPlease, help.\r\n\r\nHow to implement such network in TensorFlow?\r\n```\r\n    (input_1)    (input_2)\r\n          \\         /\r\n(Inception(GoogLeNet) without softmax layer)\r\n          /         \\\r\n    (output_1)    (output_2)\r\n             \\   / \r\n        (Contrastive loss) \r\n```\r\n\r\nI need to reuse weights in Inception for transfer learning.\r\n", "comments": ["This list is for feature requests/bugs in tensorflow. Same people also monitor stackoverflow"]}, {"number": 6775, "title": "Issue when using binary tensorflow shared library", "body": "I generated a shared library of tensorflow using the following command:\r\n\r\n> bazel build -c opt --config=cuda //tensorflow/cc:libtensorflow.so\r\n\r\nit was generated without any error and it's located in /usr/local/lib/libtensorflow.so.\r\n\r\nBUT, when I add its dependency to my CMake file: \r\n\r\n> set(XXXX_DEPENDENCIES ${XXXX_DEPENDENCIES} ${Tensorflow_LIBRARIES} PARENT_SCOPE)\r\n\r\nand I try to build my project I got the following:\r\n```\r\n:-1: warning: libcudart.so.7.5, needed by /usr/local/lib/libtensorflow.so, not found (try using -rpath or -rpath-link)\r\n/usr/local/lib/libtensorflow.so:-1: error: undefined reference to `cudaMemcpyAsync'\r\n/usr/local/lib/libtensorflow.so:-1: error: undefined reference to `cudaConfigureCall'\r\n/usr/local/lib/libtensorflow.so:-1: error: undefined reference to `cudaGetDeviceCount'\r\n/usr/local/lib/libtensorflow.so:-1: error: undefined reference to `__cudaUnregisterFatBinary'\r\n/usr/local/lib/libtensorflow.so:-1: error: undefined reference to `cudaStreamQuery'\r\n/usr/local/lib/libtensorflow.so:-1: error: undefined reference to `cudaStreamAddCallback'\r\n/usr/local/lib/libtensorflow.so:-1: error: undefined reference to `cudaSetupArgument'\r\n/usr/local/lib/libtensorflow.so:-1: error: undefined reference to `cudaGetDeviceProperties'\r\n/usr/local/lib/libtensorflow.so:-1: error: undefined reference to `cudaGetErrorString'\r\n/usr/local/lib/libtensorflow.so:-1: error: undefined reference to `__cudaRegisterFunction'\r\n/usr/local/lib/libtensorflow.so:-1: error: undefined reference to `__cudaRegisterFatBinary'\r\n/usr/local/lib/libtensorflow.so:-1: error: undefined reference to `cudaLaunch'\r\n```", "comments": ["This may be better addressed at stackoverflow (here's a related issue, answered without config=cuda) -- http://stackoverflow.com/questions/38256180/how-to-make-shared-libraries-with-bazel-at-tensorflow", "I would guess this has to do with CUDA libraries not being found by your cmake procedure", "I concur. If you have reason to suspect this might be a bug in TensorFlow, please let us know, and I'll re-open this issue."]}, {"number": 6774, "title": "`import_meta_graph` appends `_1` to node in GraphDef but doesn't add `_1` to Variable name in Collection", "body": "In an example below, second `import_meta_graph` will create variable nodes `[a, a_1]`, but corresponding global variables collection has variables `[a, a]`. So now `report_uninitialized_variables` is empty, even though there's an uninitialized variable `a_1` in the graph. Example below crashes with `uninitialized` error.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ntf.reset_default_graph()\r\nsess = tf.Session()\r\ntf.Variable(tf.ones(()), name='a')\r\nsess.run(tf.global_variables_initializer())\r\nsaver = tf.train.Saver()\r\nsaver.save(sess, 'dummy')\r\n        \r\ntf.reset_default_graph()\r\nsess = tf.Session()\r\nsaver = tf.train.import_meta_graph('dummy.meta')\r\nsaver = tf.train.import_meta_graph('dummy.meta')\r\nsaver.restore(sess, './dummy')\r\nsess.run(tf.initialize_all_variables())\r\nsess.run(tf.report_uninitialized_variables())  # => prints empty\r\nsess.run(\"a_1:0\")   # => crashes with a_1 not initialized\r\n```", "comments": ["@sherrym maybe simplest solution is to just throw an error when `import_meta_graph` tries to add an op that exists in the graph already?", "This issue still exists in r1.2.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Is this still a problem?", "I dunno....metagraph is seems to be getting replaced with SavedModel anyway, I'll close it unless someone complains", "I've confirmed that this issue still exists in version `1.5.0-dev20171219`.\r\n\r\nTo run the example you need to add \"./\" to this line:  `saver.save(sess, './dummy')`\r\n\r\nThen it fails with:\r\n\r\n```\r\nFailedPreconditionError: Attempting to use uninitialized value a_1\r\n\t [[Node: _retval_a_1_0_0 = _Retval[T=DT_FLOAT, index=0, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](a_1)]]\r\n```", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "@yaroslavvb are you in a position to send a fix?", "I think @yaroslavvb was probably right to close it earlier. While I confirmed that is is reproducible, that's doesn't say anything about it's priority."]}, {"number": 6773, "title": "svhn digit recongnition", "body": "is posible add svhn digit recongnition to Scikit flow integrated in the framework now? ", "comments": ["@ilblackdragon would it make sense to mark this Contributions Welcome?", "I will close this issue, as we moved models out of core tensorflow."]}, {"number": 6772, "title": "Branch 144094196", "body": "", "comments": ["Build timed out after 90min.\r\n\r\nJenkins, test this please.", "@hawkinsp added LLVM dependency is causing builds to time out.\r\nWe should change builds to avoid building XLA&LLVM.", "How come we are building XLA? I thought configure used `TF_ENABLE_XLA=0`.", "TF_ENABLE_XLA=1 is currently set in the builder configuration.\n\nBut simply disabling that isn't enough, we also need to blacklist\n//tensorflow/compiler/... from the set of targets to test.\n\nOn Tue, Jan 10, 2017 at 4:18 PM drpngx <notifications@github.com> wrote:\n\n> How come we are building XLA? I thought configure used TF_ENABLE_XLA=0.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/6772#issuecomment-271700479>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAVTBJyn_ZPqLTRxhLJGTnuAc60yOctFks5rQ_WbgaJpZM4Lfxyu>\n> .\n>\n", "We could use `$(bazel query \"tensorflow/... except tensorflow/compiler/...\")` for `TF_BUILD_BAZEL_TARGET` `ci_parameterized_build.sh`... Is there a more elegant way?", "move all tensorflow/compiler/ BUILD files, and load them conditionally?\r\n\r\n@jart any idea?", "Working on a fix, will test and see how it works.\n\nOn Tue, Jan 10, 2017 at 2:28 PM, drpngx <notifications@github.com> wrote:\n\n> move all tensorflow/compiler/ BUILD files, and load them conditionally?\n>\n> @jart <https://github.com/jart> any idea?\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/6772#issuecomment-271718352>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHlCOcq-6WTMqGk6_QtjcrotAZ41djboks5rRAYLgaJpZM4Lfxyu>\n> .\n>\n", "If LLVM can't be fixed, the bazel query proposal in https://github.com/tensorflow/tensorflow/pull/6772#issuecomment-271717205 sounds good. I'm just not positive that query will work. Maybe we need something like `tensorflow/... except rdeps(@llvm//..., tensorflow/...)`."]}, {"number": 6771, "title": "Match new softmax_cross_entropy_with_logits api", "body": "Fixed mnist_softmax_xla to match TF 1.0 Alpha API for tf.nn.softmax_cross_entropy_with_logits.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please. (ignore estimator_test errors)"]}, {"number": 6770, "title": "Update mnist.py", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->"]}, {"number": 6769, "title": "TF Learn TypeError: DataType float64 for attr 'T' not in list of allowed values: float32, float16", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\ntf.cast()\r\n\r\n### Environment info\r\nOperating System: \r\nmasOSSierra\r\njupyter notebook  \r\ntensforflow v0.12.1\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\nNo\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed: pip install tensorflow\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.tensforflow v0.12.1\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nI try to run the examples of MNIST from this repository with tensorflow learn, but to read in data with Pandas and use sklearn StrandardScale to scale the data beforehand.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/mnist.py\r\n\r\n### What other attempted solutions have you tried?\r\ntry to cast DataFrame into float32 with\r\nX_train = X_train.astype(np.float32)\r\ntry to cast each column with\r\ntf.cast(col, tf.float32)\r\nbut after use feature_columns = learn.infer_real_valued_columns_from_input(X_train)\r\nfeature_columns dtype just turn to tf.float64\r\n(tried and didn't find attribute from source code that I can change dtype here)\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n\r\n\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-102-1fdec574ca0f> in <module>()\r\n----> 1 classifier.fit(X_train, y_train, batch_size=100, steps=20000) #, monitors=[validation_monitor])\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/util/deprecation.pyc in new_func(*args, **kwargs)\r\n    189             _call_location(), decorator_utils.get_qualified_name(func),\r\n    190             func.__module__, arg_name, date, instructions)\r\n--> 191       return func(*args, **kwargs)\r\n    192     new_func.__doc__ = _add_deprecated_arg_notice_to_docstring(\r\n    193         func.__doc__, date, instructions)\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in fit(self, x, y, input_fn, steps, batch_size, monitors, max_steps)\r\n    353                              steps=steps,\r\n    354                              monitors=monitors,\r\n--> 355                              max_steps=max_steps)\r\n    356     logging.info('Loss for final step: %s.', loss)\r\n    357     return self\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in _train_model(self, input_fn, steps, feed_fn, init_op, init_feed_fn, init_fn, device_fn, monitors, log_every_steps, fail_on_nan_loss, max_steps)\r\n    697       # cases, but will soon be deleted after the subclasses are updated.\r\n    698       # TODO(b/32664904): Update subclasses and delete the else-statement.\r\n--> 699       train_ops = self._get_train_ops(features, labels)\r\n    700       if isinstance(train_ops, model_fn_lib.ModelFnOps):  # Default signature\r\n    701         train_op = train_ops.train_op\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in _get_train_ops(self, features, labels)\r\n   1050       `ModelFnOps` object.\r\n   1051     \"\"\"\r\n-> 1052     return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.TRAIN)\r\n   1053 \r\n   1054   def _get_eval_ops(self, features, labels, metrics):\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in _call_model_fn(self, features, labels, mode)\r\n   1019                                           params=self.params)\r\n   1020       else:\r\n-> 1021         model_fn_results = self._model_fn(features, labels, mode=mode)\r\n   1022     else:\r\n   1023       model_fn_results = self._model_fn(features, labels)\r\n\r\n<ipython-input-96-18f3ebce1f4a> in conv_model(feature, target, mode)\r\n     10                                     activation_fn=tf.nn.relu)\r\n     11 \r\n---> 12         h_pool1 = max_pool_2x2(h_conv1)\r\n     13 \r\n     14     with tf.variable_scope('conv_layer2'):\r\n\r\n<ipython-input-95-7b3697815c2b> in max_pool_2x2(tensor_in)\r\n      1 def max_pool_2x2(tensor_in):\r\n----> 2     return tf.nn.max_pool(tensor_in, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.pyc in max_pool(value, ksize, strides, padding, data_format, name)\r\n   1615                                 padding=padding,\r\n   1616                                 data_format=data_format,\r\n-> 1617                                 name=name)\r\n   1618 \r\n   1619 \r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.pyc in _max_pool(input, ksize, strides, padding, data_format, name)\r\n   1596   result = _op_def_lib.apply_op(\"MaxPool\", input=input, ksize=ksize,\r\n   1597                                 strides=strides, padding=padding,\r\n-> 1598                                 data_format=data_format, name=name)\r\n   1599   return result\r\n   1600 \r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc in apply_op(self, op_type_name, name, **keywords)\r\n    580             for base_type in base_types:\r\n    581               _SatisfiesTypeConstraint(base_type,\r\n--> 582                                        _Attr(op_def, input_arg.type_attr))\r\n    583             attrs[input_arg.type_attr] = attr_value\r\n    584             inferred_from[input_arg.type_attr] = input_name\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc in _SatisfiesTypeConstraint(dtype, attr_def)\r\n     58           \"DataType %s for attr '%s' not in list of allowed values: %s\" %\r\n     59           (dtypes.as_dtype(dtype).name, attr_def.name,\r\n---> 60            \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\r\n     61 \r\n     62 \r\n\r\nTypeError: DataType float64 for attr 'T' not in list of allowed values: float32, float16\r\n\r\n\r\nMany thanks.", "comments": ["What if you cast to float32 before feeding into maxpool?", "@yaroslavvb It works. Big thanks.", "@yaroslavvb getting similar issue with conv when running with float64\r\n```\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'Conv2D' with these attrs.  Registered devices: [CPU,GPU], Registered kernels:\r\n  device='GPU'; T in [DT_FLOAT]\r\n  device='GPU'; T in [DT_HALF]\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='CPU'; T in [DT_HALF]\r\n\r\n\t [[Node: conv1_3/Conv2D = Conv2D[T=DT_DOUBLE, data_format=\"NHWC\", padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](inputs/negatives, conv1/weights/read)]]\r\n\r\n```", "My solution didn't work?", "nope, I cast to float32 before maxpool and to float64 after", "It sounds like you are running conv with float64 which isn't supported, you have to cast to float32 before conv", "oh, that's what I was trying to do. Got it. Thx!"]}, {"number": 6768, "title": "Add \"Hide empty panes\" option to TensorBoard", "body": "When filtering runs, empty panes show up if a hidden run has a summary in a scope that the visible runs don't have. This makes TensorBoard very messy when comparing different machine learning models with different variable scopes, and so on. Could an option to hide empty panes be added?\r\n\r\nExample (note the empty LSTM scope):\r\n![image](https://cloud.githubusercontent.com/assets/1595907/21809055/56b42e02-d746-11e6-9356-37ccd16cee4c.png)\r\n", "comments": ["@dandelionmane This feature request sounds pretty reasonable. Is there any reason why we would want to display empty panes at all?", "@dandelionmane, thoughts on this? It would be a pretty easy fix, I'd wager.", "The same should apply to panes that contain only empty plots.", "Why not hide empty plots all together?", "Migrated this to the TensorBoard repo."]}, {"number": 6767, "title": "dynamic_rnn slower using master code then 0.12.1 release code", "body": "For master code, git rev-parse HEAD \r\nec7929b878926c39255254e9aea992f0bc65aa68\r\n\r\nsame code running for 0.12.1\r\n batch_size:[256] batches/s:[4.76] insts/s:[1217.74] \r\n batch_size:[256] batches/s:[5.39] insts/s:[1379.17] \r\n batch_size:[256] batches/s:[5.11] insts/s:[1306.94] \r\n batch_size:[256] batches/s:[5.05] insts/s:[1292.61] \r\n\r\nfor master code:\r\nbatch_size:[256] batches/s:[4.18] insts/s:[1069.37] \r\n batch_size:[256] batches/s:[4.77] insts/s:[1220.00] \r\n batch_size:[256] batches/s:[4.81] insts/s:[1231.52] \r\nbatch_size:[256] batches/s:[4.55] insts/s:[1164.99] \r\n\r\n\r\n", "comments": ["@ebrevdo is there some existing benchmark that tracks dynamic_rnn?", "There have been a number of chanes.  There are internal benchmarks but no red flags.\r\n\r\nAny chance you could send the chrome traces (from tf.Timeline) from 0.12 and master?  We're looking for:\r\n\r\n1. All ops still running on GPU\r\n2. All ops still running on the same GPU\r\n\r\ndepending on which one of these is different, I can help more.", "@ebrevdo I'm not quite understand 'chrome trances tf.Timeline' how to ?\r\nI find below warnings might these hurt perfomance?\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n", "Here are instructions for generating timeline: https://github.com/tensorflow/tensorflow/issues/1824#issuecomment-225754659\r\n\r\nThose warnings were added recently and they are probably irrelevant to your case", "Not sure if it is ok just to trace one sess run step, below are the two generated chrome trances.\r\n[v0.12_timeline.ctf.json.txt](https://github.com/tensorflow/tensorflow/files/704029/v0.12_timeline.ctf.json.txt)\r\n[v1.0_timeline.ctf.json.txt](https://github.com/tensorflow/tensorflow/files/704028/v1.0_timeline.ctf.json.txt)\r\n\r\n", "@ebrevdo Is this still an issue?\r\nCould you comment and/or close this issue if it is resolved?", "I think there has been a regression in tf 1.4; i don't think it's specific to dynamic_rnn, but other models are seeing it too.  We can close this as a duplicate but the timeline here is useful.  However, I do think someone from core TF team should be looking at these TF performance regressions in v1.4.", "Actually; i see this is for TF 1.0 vs tf 1.2.  @chenghuige what is the performance like with TF nightly?", "@chenghuige , ping?", "Nagging Assignee @ebrevdo: It has been 197 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Is this still an issue? I'm closing it; please reopen if it is."]}, {"number": 6766, "title": "softmax_cross_entropy_with_logits aborts the process, if a tensor with zero first dimension is passed as an argument", "body": "### Environment info\r\n\r\nOperating System: Ubuntu 16.04\r\nInstalled version of CUDA and cuDNN: CUDA-8.0, CUDNN 5.1.5 \r\n\r\nTensorflow version: 0.12.1 installed from\r\nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.1-cp35-cp35m-linux_x86_64.whl\r\n\r\nReproduced also using tf-0.11.0, CUDA-7.5, CUDNN-5.1.3\r\n\r\n### Minimal reproducible example \r\n\r\n```\r\nimport tensorflow as tf\r\ny = tf.placeholder(\"int64\", [None], \"y\")\r\none_hot_y=tf.one_hot(y,10)\r\nce = tf.nn.softmax_cross_entropy_with_logits(one_hot_y, one_hot_y)\r\nsess = tf.Session()\r\nsess.run(ce, {y: []})\r\n```\r\nResult on GPU: \r\n```\r\nE tensorflow/core/common_runtime/bfc_allocator.cc:244] tried to allocate 0 bytes\r\nW tensorflow/core/common_runtime/allocator_retry.cc:32] Request to allocate 0 bytes\r\nF tensorflow/core/common_runtime/gpu/gpu_device.cc:104] EigenAllocator for GPU ran out of memory when allocating 0. See error logs for more detailed info.\r\nAborted (core dumped)\r\n```\r\nResult on CPU: \r\n```\r\narray([], dtype=float32)\r\n```", "comments": ["@zheng-xq Here's a GPU memory alloc issue. Reported both here and on Stack Overflow: http://stackoverflow.com/questions/41530966/memory-error-with-eigenallocator", "Hi, @jrosti \r\nHave this problem been solved?\r\nI met the same problem in `r1.01`", "I still have this problem which seems to arise when using tensorflow fold.", "Still getting this issue, as of 06/02/2017 when using tf.gather_nd(...) and softmax_cross_entropy_with_logits(...) together.", "Anyone looking into this? I'm also experiencing this issue using TensorFlow 1.2.0 (v1.2.0-rc2-21-g12f033d). In my case, a `tf.while_loop` is being used (in conjunction with `tf.TensorArray`); however, the same error occurs if I swap out the `tf.while_loop` for a `while` statement.\r\n\r\n**Backtrace** (abbr):\r\n```\r\n2017-06-24 05:42:28.894580: E tensorflow/core/common_runtime/bfc_allocator.cc:244] tried to allocate 0 bytes\r\n2017-06-24 05:42:28.894638: W tensorflow/core/common_runtime/allocator_retry.cc:32] Request to allocate 0 bytes\r\n2017-06-24 05:42:28.894647: E tensorflow/core/common_runtime/bfc_allocator.cc:244] tried to allocate 0 bytes\r\n2017-06-24 05:42:28.894657: W tensorflow/core/common_runtime/allocator_retry.cc:32] Request to allocate 0 bytes\r\n2017-06-24 05:42:28.895314: E tensorflow/core/common_runtime/bfc_allocator.cc:378] tried to deallocate nullptr\r\n2017-06-24 05:42:28.895406: E tensorflow/core/common_runtime/bfc_allocator.cc:378] tried to deallocate nullptr\r\n2017-06-24 05:42:28.896185: W tensorflow/core/framework/op_kernel.cc:1158] Resource exhausted: Ran out of GPU memory when allocating 0 bytes for\r\n         [[Node: while/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](while/Reshape, while/Reshape_1)]]\r\n2017-06-24 05:42:28.896209: W tensorflow/core/framework/op_kernel.cc:1158] Resource exhausted: Ran out of GPU memory when allocating 0 bytes for\r\n         [[Node: while/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](while/Reshape, while/Reshape_1)]]\r\n.\r\n.\r\n.\r\n2017-06-24 05:42:28.905449: W tensorflow/core/framework/op_kernel.cc:1158] Resource exhausted: Ran out of GPU memory when allocating 0 bytes for\r\n         [[Node: while/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](while/Reshape, while/Reshape_1)]]\r\n2017-06-24 05:42:28.905669: W tensorflow/core/framework/op_kernel.cc:1158] Resource exhausted: Ran out of GPU memory when allocating 0 bytes for\r\n         [[Node: while/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](while/Reshape, while/Reshape_1)]]\r\n```\r\n\r\nA bit further down,\r\n```\r\nCaused by op 'while/SoftmaxCrossEntropyWithLogits', defined at:\r\n  ...\r\n  File \"scripts/gpu_experiment.py\", line 400, in build_backend\r\n    backend['o'] = build_outputs(config, backend.get('o', None))\r\n  File \"scripts/gpu_experiment.py\", line 363, in build_outputs\r\n    loop_vars = tf.while_loop(loop_cond, _build_outputs, loop_vars)\r\n    ...\r\n    cross_ent = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels)\r\n```", "I'm also experiencing this issue and error message is same as  @j-wilson   . I am fine tuning object detction api on faster_rcnn_inception_resnet_v2_atrous_coco_11_06_2017 model.ckpt.", "Also having the same problem in tf 1.3. Would be great if it threw a more descriptive exception.", "I hit this issue.  After some investigating I realized I was feeding in an empty tensor.  Not sure if that is the only thing that can cause it, but that was my problem at least.\r\n\r\nIt was easy to fix, but it certainly would be nice if it threw a more descriptive error.", "Massive thanks. Empty tensor is the cause of my problem too. @metachi ", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "I'm facing same problem even checking if tensor is empty or not:\r\n```\r\n\r\n loss = K.switch(tf.size(y_true) > 0,\r\n                    tf.nn.softmax_cross_entropy_with_logits(logits=y_pred, labels=y_true, dim=1),\r\n                    tf.constant(0.0))\r\n```", "@filipetrocadoferreira Have you tried to wrap the conditional branches into lambda functions? \r\n```\r\nloss = K.switch(tf.size(y_true) > 0,\r\n                    lambda: tf.nn.softmax_cross_entropy_with_logits(logits=y_pred, labels=y_true, dim=1),\r\n                    lambda: tf.constant(0.0))\r\n```\r\nThis should allow a lazy execution of the branches. (see [tf.cond](https://www.tensorflow.org/api_docs/python/tf/cond))", "wow, this seems to work. Can you link to an explanation?", "Added PR #16051 for a fix.", "@filipetrocadoferreira I can\u2019t find any link with an extensive explanation (I tought it was explained in tf.cond \u2019s documentation but actually it is not). In short, if you don\u2019t define the branches as functions, they will be both executed regardless of the condition. This explains why you still had the problem despite checking the tensor\u2019s size."]}, {"number": 6765, "title": "Tensorflow v0.12 tf.nn has no module rnn_cell", "body": "`Import tensorflow as tf`\r\n\r\n`a = tf.nn.rnn_cell.LSTMCell(100)`\r\n\r\nresults in:\r\n\r\n> AttributeError: 'module' object has no attribute 'rnn_cell'\r\n\r\nTensorflow 0.12\r\n\r\nls -l 'cuda':\r\n-rw-r--r-- 1 root root 189170 Jan  9 10:49 /usr/local/cuda/lib/libcudadevrt.a\r\nlrwxrwxrwx 1 root root     16 Jan  9 10:49 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5\r\nlrwxrwxrwx 1 root root     19 Jan  9 10:49 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\r\n-rwxr-xr-x 1 root root 311596 Jan  9 10:49 /usr/local/cuda/lib/libcudart.so.7.5.18\r\n-rw-r--r-- 1 root root 558020 Jan  9 10:49 /usr/local/cuda/lib/libcudart_static.a\r\n\r\nCuda 7.5\r\nCudnn 5\r\n\r\ngit rev-parse HEAD:\r\nec7929b878926c39255254e9aea992f0bc65aa68\r\n\r\nBazel Version:\r\nBuild label: 0.4.3\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Thu Dec 22 12:31:25 2016 (1482409885)\r\nBuild timestamp: 1482409885\r\nBuild timestamp as int: 1482409885\r\n\r\n\r\n", "comments": ["It's been moved to contrib"]}, {"number": 6764, "title": "MNIST batch test", "body": "The test code uses test data as whole. It requires a lot of memory.\r\nI modified it to test every 50 samples and average the results.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "The indentation is wrong? What do you mean to implement here?", "We're trying to keep the documentation short and sweet. This makes it more complex. I think that most cards available today come with 4GB+ of memory, right?", "@drpngx Experts have expensive GPUs with large memory. I think tutorials are for most people not only for experts. But if you think this modification is unnecessary, please close this issue.", "OK, closing for now. If we see some activity in stackoverflow or our github issues around that, we'll revive the thread. Thanks!", "It would be great to have this patch either as a part of the existing samples or properly in the documentation. As @rmekdma writes, most of the people do not have access to >4GB of GPU memory. Batch processing allows for running the samples of the lower range of graphic cards. Also as I see it, a lot of people have been facing the OOM issue as can be seen by the references to #136. Thanks.", "agree with \u201cmost cards available today come with 4GB+ of memory\u201d, but i do not think all people have this \"cards\", especially on many laptop which is not newer."]}, {"number": 6763, "title": "Android: build shared library using Makefile with r0.12, migrating from r0.11", "body": "I've been successfully using r0.11 Makefile with couple of additions (see below) to build shared library for Android. \r\nNow trying to migrate to r0.12 with same Makefile modifications but getting a SIGSEGVs at model initialization.\r\n\r\nI followed [contrib/makefile/README.md](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/makefile/README.md). I've checked https://github.com/tensorflow/tensorflow/issues/6166 as well.\r\n\r\nComparing r0.12 changes with r0.11 I've noticed some things:\r\n- both libtensorflow-core.a and lib.so decreased in size from ~200MB and ~100MB in r0.11 to ~40MB and ~40MB in r0.12\r\n- trying to revert optimization from -O2 back to -O0 causing compilation error in downloads/gemmlowp/\r\n```shell\r\n/Users/oleg/opt/ndk-bundle-r12e/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64/bin/arm-linux-androideabi-g++ --std=c++11 -DIS_SLIM_BUILD -fno-exceptions -DNDEBUG -DNOTFDBG -O0 --sysroot /Users/oleg/opt/ndk-bundle-r12e/platforms/android-14/arch-arm -Wno-narrowing -march=armv7-a -mfloat-abi=softfp -mfpu=neon -fPIE -fPIC -MT /Users/oleg/tensorflow/tensorflow/contrib/makefile/gen/obj/tensorflow/core/kernels/meta_support.o -MMD -MP -MF /Users/oleg/tensorflow/tensorflow/contrib/makefile/gen/dep//tensorflow/core/kernels/meta_support.Td -I/Users/oleg/opt/ndk-bundle-r12e/sources/android/support/include -I/Users/oleg/opt/ndk-bundle-r12e/sources/cxx-stl/gnu-libstdc++/4.9/include -I/Users/oleg/opt/ndk-bundle-r12e/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi/include -I. -I/Users/oleg/tensorflow/tensorflow/contrib/makefile/downloads/ -I/Users/oleg/tensorflow/tensorflow/contrib/makefile/downloads/eigen -I/Users/oleg/tensorflow/tensorflow/contrib/makefile/downloads/gemmlowp -I/Users/oleg/tensorflow/tensorflow/contrib/makefile/gen/protobuf/include -I/Users/oleg/tensorflow/tensorflow/contrib/makefile/gen/proto/ -I/Users/oleg/tensorflow/tensorflow/contrib/makefile/gen/proto_text/ -c tensorflow/core/kernels/meta_support.cc -o /Users/oleg/tensorflow/tensorflow/contrib/makefile/gen/obj/tensorflow/core/kernels/meta_support.o\r\nIn file included from /Users/oleg/tensorflow/tensorflow/contrib/makefile/downloads/gemmlowp/meta/transform_kernels.h:239:0,\r\n                 from ./tensorflow/core/kernels/meta_support.h:23,\r\n                 from tensorflow/core/kernels/meta_support.cc:18:\r\n/Users/oleg/tensorflow/tensorflow/contrib/makefile/downloads/gemmlowp/meta/transform_kernels_arm_32.h: In static member function 'static void gemmlowp::meta::Transform1DKernel<InType, OutType, gemmlowp::meta::BiasAdd<Type>, kernel_size, leftovers>::Transform(const InType*, const gemmlowp::meta::BiasAdd<Type>&, OutType*) [with InType = unsigned char; OutType = int; int kernel_size = 16; int leftovers = 0; Type = unsigned char]':\r\n/Users/oleg/tensorflow/tensorflow/contrib/makefile/downloads/gemmlowp/meta/transform_kernels_arm_32.h:5605:24: error: 'asm' operand has impossible constraints\r\n         \"cc\", \"memory\");\r\n                        ^\r\nmake: *** [/Users/oleg/tensorflow/tensorflow/contrib/makefile/gen/obj/tensorflow/core/kernels/meta_support.o] Error 1\r\n```\r\n- gemmlowp was downloaded but never used for Android in headers search path (-I) in r0.11\r\n- reverting new flags -mfloat-abi=softfp  and -mfpu=neon by commenting them out allows to build libtensorflow-core.a and libtensorflow.so artifacts with ~100MB in size using -O0\r\n\r\nAfter applying  all changes from above I'm still getting SIGSEGVs. What else could I try? Looks like I'm missing something. I might be wrong thinking Makefile is a good way to go, but it flexible and allows to add required kernels/*_op.cc straight to tf_op_files.txt. What is a right way to build lib.so for Android?\r\n\r\n### Makefile additions \r\n```Makefile\r\nifeq ($(TARGET),ANDROID)\r\n...\r\nCXXFLAGS +=\\\r\n...\r\n-fPIC\r\n\r\nSO_NAME  := libtensorflow.so\r\nSO_PATH  := $(LIBDIR)$(SO_NAME)\r\n\r\n$(SO_PATH): $(LIB_OBJS)\r\n\t@echo \"------->>>>>>>>>  SO PATH started ------>>>>>>>>>\"\r\n\t@mkdir -p $(dir $@)\r\n\t$(CXX) $(CXXFLAGS) $(INCLUDES) \\\r\n\t-shared -rdynamic -o $(SO_PATH)  $(LIB_OBJS) \\\r\n\t$(LIBFLAGS) $(LDFLAGS) $(LIBS)\r\n```\r\n\r\n### Environment:\r\nOS X: 10.12\r\nNDK: r12e\r\nCUDA: not installed\r\nSHA-1\r\nr0.12 `4d924e796368163eff11a8151e8505715345f58d`\r\nr0.11 `e39376b6e9c9541e1bd8f15333b6994046a84d16`\r\n................\r\nBuild label: 0.4.1-homebrew\r\nBuild target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Nov 30 13:26:35 2016 (1480512395)\r\nBuild timestamp: 1480512395\r\nBuild timestamp as int: 1480512395\r\n\r\n", "comments": ["Can you describe more about your device and the sigsegv you're seeing?\r\n\r\nMaybe not related to your issue, but Ndk 12b is suggested (12e does not exist afaik), as well as Bazel 0.4.2\r\n\r\nAre you building with build_all_android.sh? That is the preferred way to build using the Makefile system.\r\n\r\nBazel is also a fully supported build method, e.g.:\r\n```\r\nbazel build -c opt tensorflow/core:android_tensorflow_lib \\\r\n --crosstool_top=//external:android/crosstool \\\r\n --cpu=armeabi-v7a \\\r\n --host_crosstool_top=@bazel_tools//tools/cpp:toolchain\r\n```", "There's a lot to digest here. @petewarden might want to be aware of all this commentary concerning the makefile build.\r\n\r\n@oorlov you might want to file an issue at https://github.com/google/gemmlowp asking them why it doesn't compile with -O0.", "@andrewharp you are right, my NDK is Pkg.Revision = 12.1.2977051 that is r12b. Misspelled a folder inspired by ndk-r10e.\r\n\r\nAnother thing I've realized is that after all changes (disabled neon, and OPTFLAGS := -O0) I'm getting ~110MB libtensorflow.so artifact that **DOES** work in the same way as r0.11\r\n\r\nLooks like I messed up some things yesterday like android version or ndk version were different between libprotobuf and libtensorflow.so\r\nSo to prevent this mess I've thoroughly built and tested artifacts with different environments:\r\n\r\n...  | neon  |  OPTFLAGS  | Android | Device                | Result       | Log\r\n--- | -----|--------------|---------|-----------------|-----------|---\r\n1    | On   | -O2                | 21           | Galaxy S7, 6.0.1 | SIGSEGV | [neon.o2.android21.txt](https://github.com/tensorflow/tensorflow/files/698359/neon.o2.android21.txt) \r\n2    | On   | -O2                | 21           | Pixel, 7.1.1         | SIGSEGV |   [neon.o2.android21.txt](https://github.com/tensorflow/tensorflow/files/698359/neon.o2.android21.txt) \r\n3    | On   | -O2                | 21           | Galaxy Note 3, 5.0.1 | SIGSEGV |  [neon.o2.android21.txt](https://github.com/tensorflow/tensorflow/files/698359/neon.o2.android21.txt) \r\n4    | OFF   | -O2                | 21           | Galaxy S7, 6.0.1 | SIGSEGV |  [no-neon.o2.android21.txt](https://github.com/tensorflow/tensorflow/files/698367/no-neon.o2.android21.txt)\r\n5   | OFF   | -O2                | 21           | Pixel, 7.1.1 | SIGSEGV |  [no-neon.o2.android21.txt](https://github.com/tensorflow/tensorflow/files/698367/no-neon.o2.android21.txt)\r\n6    | OFF   | -O2                | 21           | Galaxy Note 3, 5.0.1 | SIGSEGV |  [no-neon.o2.android21.txt](https://github.com/tensorflow/tensorflow/files/698367/no-neon.o2.android21.txt)\r\n7    | OFF   | -O0                | 21           | Galaxy S7, 6.0.1 | Ok |  [no-neon.o0.android21.txt](https://github.com/tensorflow/tensorflow/files/698368/no-neon.o0.android21.txt)\r\n8   | OFF   | -O0                | 21           | Pixel, 7.1.1 | Ok |  [no-neon.o0.android21.txt](https://github.com/tensorflow/tensorflow/files/698368/no-neon.o0.android21.txt)\r\n9    | OFF   | -O0                | 21           | Galaxy Note 3, 5.0.1 | Ok |  [no-neon.o0.android21.txt](https://github.com/tensorflow/tensorflow/files/698368/no-neon.o0.android21.txt)\r\n10    | On   | -O2                | 21           | Any | Compilation Error |  [neon.o2.android21.txt](https://github.com/tensorflow/tensorflow/files/698384/neon.o2.android21.txt)\r\n\r\nThese builds are created on clean MacBook with OSX El Capitan 10.11.6, NDK r12b + libtool + autoconf. Neither Android SDK, nor Bazel installed.\r\n\r\nSeems like -O2 causes problems, not neon flag, but I cannot confirm it due to compilation error.\r\n\r\nI'm also getting Ok builds when Android version is set to android-14. I run them successfully on Moto X v1, Android 4.4.4 and Samsung/GreatCall Touch3, Android 4.3 [no-neon.o0.android14.txt](https://github.com/tensorflow/tensorflow/files/698439/no-neon.o0.android14.txt)\r\n\r\n@jart Thank you for suggestion, though I'm hesitating here because the only scenario I have in my hands is a tensorflow build that might be affected by system misconfiguration or my misunderstanding. If it is clear that on my side everything is correct issue will be filed.\r\n", "@andrewharp Checked Android build with Bazel, followed these instuctions:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/android\r\nIncluding\r\n```\r\nbazel build -c opt tensorflow/core:android_tensorflow_lib \\\r\n --crosstool_top=//external:android/crosstool \\\r\n --cpu=armeabi-v7a \\\r\n --host_crosstool_top=@bazel_tools//tools/cpp:toolchain\r\n```\r\n\r\nBazel build script for libtensorflow_inference.so generates different compilation error:\r\n\r\n```\r\nERROR: /Users/alivecor/tensorflow-bazel/tensorflow/contrib/android/BUILD:71:1: Linking of rule '//tensorflow/contrib/android:libtensorflow_inference.so' failed: sandbox-exec failed: error executing command /usr/bin/sandbox-exec -f /private/var/tmp/_bazel_alivecor/b205624d5e3f9e00e1da3a96f45ffc17/bazel-sandbox/5d59ce41-1e8b-441a-8550-c21d6358aaa7-749/sandbox.sb ... (remaining 32 argument(s) skipped).\r\ntensorflow/core/ops/math_ops.cc:1822: error: undefined reference to 'ceil'\r\nexternal/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/cmath:185: error: undefined reference to 'ceilf'\r\ntensorflow/core/ops/nn_ops.cc:64: error: undefined reference to 'floorf'\r\nexternal/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/cmath:185: error: undefined reference to 'ceilf'\r\n```\r\nLog: [bazel-android-inference.txt](https://github.com/tensorflow/tensorflow/files/700122/bazel-android-inference.txt), Verbose: [bazel-android-inference-verbose.txt](https://github.com/tensorflow/tensorflow/files/700219/bazel-android-inference-verbose.txt)\r\n\r\nNow I'm confused. It happens on more than one machine, so I don't believe they share same misconfiguration problems. \r\nWhat should be my next step? Should I stay on Makefile with neon and optimization switched off?Wouldn't it affect model execution? Or need to do extra steps for Bazel build?\r\n\r\n\r\n### Environment\r\nMachine 1, (clean, OSX reinstalled): \r\nOSX: 10.11.6\r\nNDK: r12b\r\nCUDA: not installed\r\nBAZEL:\r\nBuild label: 0.4.3-homebrew\r\nBuild target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Thu Dec 22 15:20:22 2016 (1482420022)\r\nBuild timestamp: 1482420022\r\nBuild timestamp as int: 1482420022\r\n\r\nMachine 2:\r\nOSX 10.12\r\nNDK: r12b\r\nCUDA: not installed\r\nBAZEL:\r\nBuild label: 0.4.1-homebrew\r\nBuild target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Nov 30 13:26:35 2016 (1480512395)\r\nBuild timestamp: 1480512395\r\nBuild timestamp as int: 1480512395\r\n\r\n\r\n", "@oorlov //tensorflow/contrib/android:libtensorflow_inference.so was missing a couple of link targets in 0.12 that provide the necessary definitions. This has since been fixed, but if you want to patch 0.12 just add:\r\n\r\n```\r\n        \"-llog\",\r\n        \"-lm\",\r\n```\r\n\r\nto the linkopts section and it should build.", "@oorlov does the comment above address your issue?\r\n\r\n@andrewharp do you think there's anything actionable here on the part of TensorFlow?", "Hey guys! \r\nBazel build works pretty well for me and I get both x86 and armv7 libraries for android-14 and it seems working well on my devices. Nothing is changed for Makefile, so I will stick to Bazel build for now.\r\n\r\nThank you for assistance!\r\n", "@oorlov Great! \r\n\r\nWas looking at the actual segfault you had (cannot locate symbol \"rand\" referenced by libtensorflow_android.so) and it seems to be related to this: http://stackoverflow.com/questions/27091001/how-to-use-mkfifo-using-androids-ndk\r\n\r\nSuggestions there were to lower the Android native API and see if the problem is resolved, or add \" -fPIE -pie\" flags to the build. However as we don't see this problem in our Makefile build otherwise, there may be some other difference with your local addition.\r\n\r\n@jart I think the original Makefile problem was due to the new lib he added, and the Bazel issue has already been fixed, so there's nothing to be done on the TF side of things.", "@andrewharp\r\n\r\nIt is a different SIGSEGV. It seems presence of Motorola 4.4.4 device in SIGSEGV logs are misleading. If you check SIGSEGV related logs you will find all of them about binary compiled for android-21 when MotoX device is android-14. \r\nSo when you run such binary on Android 4.4.4 (android-19) device it dies with `cannot locate symbol \"rand\" referenced by libtensorflow_android.so` and it is expected.\r\n\r\nThough if you run it on Android 5.0+ device it is supposed to work properly (S7, Pixel). And it does for no-neon.O0 binaries (While MotoX 4.4.4 still dies with the same error, see [no-neon.o2.android21.txt](https://github.com/tensorflow/tensorflow/files/705254/no-neon.o2.android21.txt). Yes, compiling binary for android-14, no-neon, -O0 does a fix for MotoX 4.4.4. \r\n\r\nThe SIGSEGV I'm reporting is still there for any -O2 builds compiled for both android-21 and android-14 binaries.\r\n", "@oorlov Sorry, which log shows the other SIGSEGV? I only see ones that refer to a missing definition for \"rand\" (including both the failing Pixel logs).\r\n\r\nGoogling this led me to http://stackoverflow.com/questions/28504875/android-ndk-cannot-load-libc-shared-so-gets-cannot-locate-symbol-rand-refe, which in turn points to the previous link I posted.", "@andrewharp  for example [neon.o2.android21.txt](https://github.com/tensorflow/tensorflow/files/698359/neon.o2.android21.txt)\r\nThis is about android-21 devices Galaxy S7, 6.0.1, Pixel, 7.1.1 Galaxy Note 3, 5.0.1, so \"rand\" error is not applicable for them. MotoX is there only by a mistake.\r\n", "Thanks, I see now -- I'd just been scrolling to the end and then scanning for the start of the error out of habit.\r\n\r\nSigsev 11s typically happen due to invalid memory access. Hard to say what is actually going wrong without seeing code and debugging, but it could be a bug in the code and not a necessarily a build flag issue. Certain builds can be arbitrarily more picky about enforcing valid memory access depending on how objects get laid out or how the codepaths access the memory (e.g. with or without NEON), so it's possible that there's a hidden problem even in the \"successful\" variants.\r\n\r\nNot clear to me that the issue lies in TF, though, so unless there is a minimal reproducible example you can share that narrows it down to TF, I don't think this is something we can support."]}, {"number": 6762, "title": "configure assumes ldconfig can be called and that CuDNN is installed in a system library location", "body": "The configure script calls \"ldconfig\" to locate the CuDNN library but this makes 2 assumptions that can be wrong: \r\n1. It assumes that \"ldconfig\" can be called. However, on some GNU/Linux systems such as openSUSE Leap 42.1, only root has it in their path. (On Leap 42.1, normal users get the desired output from \"/sbin/ldconfig -p\".)\r\n2. It assumes that the CuDNN library is installed in a system library location that is searched by ldconfig. However, users may install it in their home directory and use LD_LIBRARY_PATH to load it for various reasons. I, for example, was not fully satisfied from reading the licence terms that I am allowed to share the library with other users of the system and therefore did not install it system-wide as root.\r\n\r\nSeverity is low as normally the CuDNN library location should be auto-detected from the CuDNN library path before \"ldconfig\" is tried.\r\n\r\nHow to reproduce:\r\n1. git clone this repo on a system with ldconfig not in your PATH\r\n2. cd tensorflow and run ./configure\r\n3. when asked for the CuDNN path, enter a valid and readable path (to avoid other errors), for example /usr/bin, that does not contain CuDNN\r\n4. Error message: ./configure: line 337: ldconfig: command not found\r\n\r\nWorkaround options:\r\n- First double check the CuDNN library path (the default path suggests that the last component \"lib64\" or \"lib\" should be omitted) and verify that standard folder names are used under this location for library and header files, for example \"lib64\" and \"include\".\r\n- If you can call /sbin/ldconfig and if CuDNN is in a standard system library location the user can temporarily add /sbin to their PATH before running \"./configure\"\r\n- Edit the \"configure\" script to provide the location manually, for example CUDNN_PATH_FROM_LDCONFIG=\"$HOME/local/cuda/lib64/libcudnn.so\"\r\n\r\nA fix should test whether \"ldconfig\" can be called before trying to call it. Also try /sbin/ldocnfig. Given that the library should normally be found already earlier in the code using the user-provided CuDNN path, a simple solution may be to set CUDNN_PATH_FROM_LDCONFIG to the empty string if ldconfig cannot be called.\r\n\r\nIf you like to automate the locating of the library as much as possible, you could furthermore check all locations in LD_LIBRARY_PATH.", "comments": ["Thanks for bringing this to our attention. It would be nice if the configure script was smarter about discovering this library. As for ldconfig, wouldn't it work if it was just changed to `/sbin/ldconfig`? Where else could it be?\r\n\r\nPlease be advised that OpenSUSE isn't in our support matrix. Our goal is to support Ubuntu Linux 14 LTE to 16. Mac OS X El Capitan. CentOS 7 and above. So it might be best if we marked this contributions welcome.", "TLDR: A solution may be to set PATH=\"$PATH\":/sbin at the start of the configure script. This way, if the system or the user has a preferred \"ldconfig\" somewhere else it will be used and if not the script will automatically fall back to /sbin/ldconfig.\r\n\r\nThe FHS http://www.pathname.com/fhs/pub/fhs-2.3.html only mentions \"ldconfig\" in the /sbin section. From this point of view, it should be safe to assume that \"ldconfig\" is always in /sbin (if it is installed at all -- the FHS says it's optional). However, it's best practise to respect the preferences expressed in PATH.\r\n\r\nMy first thought was that \"ldconfig\" cannot be in /sbin on the supported platforms as otherwise the \"configure\" script would not work on these platforms because normal users don't have /sbin in their PATHs. However, most Ubuntu users have sudoer rights. That's probably why this issue hasn't been noticed before.", "That makes sense. In that case, we'd be grateful if you joined the ranks of TensorFlow contributors and sent us the one line change you proposed. CC me and I'll review it.", "Trivial code change #6848 rejected as drpngx insists on Google's Contributor License Agreement to be signed and I don't have time to review it and the documents it refers to.\r\n\r\nAnybody with CLA please take the 30 seconds to implement \"temporarily add /sbin to PATH for ldconfig call\" in your own way and contribute a patch (total time <5 minutes). (Don't copy my solution. Even though I would be ok with you copying it as I think it is trivial code, a Google bot may detect that it is identical or similar to a previously rejected pull request and reject it on these grounds.)", "drpngx was only following Google policy. The policy is designed to protect the interests of the TensorFlow community. It's applied evenly and uniformly to all contributions. So I hope you'll reconsider. I could easily make the change myself. I just feel that since you were able to identify an issue with TensorFlow along with its solution, you deserve to be able to take public credit as the person who solved it."]}, {"number": 6761, "title": "variable_scope behave differently for master code and release 0.12.1", "body": "For master code, git rev-parse HEAD\r\nec7929b878926c39255254e9aea992f0bc65aa68\r\n\r\nThe problem is one of my code used to work for release 0.12.1 fail for master code, which use adagrad as optimizer.\r\nValueError: Variable OptimizeLoss/w_h/Adagrad/ does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\r\n\r\nI traced this and find below two simple codes can reproduce, the second one works for both 0.12.1 release and master, but the first one only works for 0.12.1 and fail master code. \r\nI wonder what's the diff here ?  code 2 style is suggested ? Why code 1 will fail ? Also for code 1 if I use gradient desc instead of adgrad will not fail. Similar error can occur when using lstm cell.\r\n\r\ncode 1:\r\n\r\n\r\n      loss, accuracy = model.build_graph(X, y)  \r\n\r\n      tf.get_variable_scope().reuse_variables()    \r\n\r\n      eval_loss, eval_accuracy = model.build_graph(eval_X, eval_y)    \r\n\r\ncode 2:\r\n\r\n\r\n    with tf.variable_scope(\"mlp\") as scope:    \r\n\r\n        loss, accuracy = model.build_graph(X, y)    \r\n\r\n        scope.reuse_variables()    \r\n\r\n        eval_loss, eval_accuracy = model.build_graph(eval_X, eval_y)    \r\n\r\n\r\n", "comments": ["Also master code will fail when using cell(), I'm don't know how to workaround. \r\n\r\n    \r\n    for i in range(max_steps):    \r\n\r\n      with tf.variable_scope(\"RNN\", reuse=True if i > 0 else None):    \r\n\r\n        (output, state) = self.cell(last_symbol, state)  \r\nValueError: Variable seq2seq/run/decode/RNN/lstm_cell/weights does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\r\n", "@martinwicke do you think some of the 1.0 work might have caused this?", "@lukaszkaiser or @ebrevdo , can you comment?", "It looks like some interaction between scopes and the initializer change (#5813). Could you provide a small but fully self-standing example code, so we can investigate further?", " @lukaszkaiser  for my code LSTM part I now get workaround by adding \"RNN\" scope when init self.cell \r\n  \r\n      with tf.variable_scope(\"RNN\"):   \r\n\r\n          self.cell =  ... \r\n\r\nbut right now I can not reproduce error using simple self-standing example code.\r\n", "I think these 2 things might be unrealted: rnn cells migrated, and optimizers changed, so there might be 2 things. Could you prepare some small code to illustrate the remaining problems? Thanks!", "@lukaszkaiser  For RNN cell problem. I find out the reason.  \r\nWhen training I used  dynamic_rnn without setting sope, and tf v0.12 defualt scope is 'RNN'\r\nwhile tf v1.0 defualt scope is 'rnn'. So that cause the error. Not a bug.", "Terrific. I'm glad we got to the bottom of things. I'm going to close this one out now.", "Hmm, the RNN was only the second problem. How about the first one? If it's still there, please reopen and, in the best case, attach some simple code that exposes the problem. Thanks!", "@lukaszkaiser  adagrad problem also find out the cause.  I think it is my code problem, I should use addtional scope and set variable reused in that scope, before using tf.contrib.layers.optimize_loss.\r\nThough tf v0.12 can work but I think tv v1.0's behavior is ok not a bug also. Thanks lukaszkaiser and jart!\r\n\r\nBelow code can reproduce this, it works with tf v0.12 fail v1.0 without using addtional scope for train and evaluate before layers.optimize.\r\nhttps://github.com/chenghuige/tensorflow-example/blob/master/tests/adagrad_optimize_scope.py", "@chenghuige Thanks for the clarification. Do you think that there's anything actionable on the part of TensorFlow that we could do better?", "@jart Tensorflow is such a great framework! I can always get help quickly whenever facing problem:)\r\nThere are adequate documentations, but still the more the better.\r\nFor me currently focusing on dynamic seq2seq, as I have filed in another bug, I find it's hard to read seq2seq code. \r\nIt would be great to see more example code showing the usage of context state and dynamic seq2seq with beam search. \r\nAlso might need more documents on TensorArray.", "We're working on a cleaner seq2seq API now. Hope to release within the next\nmonth.\n\nOn Jan 13, 2017 12:43 AM, \"allen\" <notifications@github.com> wrote:\n\n> @jart <https://github.com/jart> Tensorflow is such a greate framework! I\n> can always get help quickly whenever facing problem:)\n> There are adequate documentations, but still the more the better.\n> For me currently focusing on dynamic seq2seq, as I have filed in another\n> bug, I find it's hard to read seq2seq code.\n> It would be great to see more example code showing the usage of context\n> state and dynamic seq2seq with beam search.\n> Also might need more documents on TensorArray.\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6761#issuecomment-272390302>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim9ykrF7d1_bpNdrt_bcH_ZbdRsH8ks5rRzlAgaJpZM4LfOUA>\n> .\n>\n", "@ebrevdo cool, looking forward to new seq2seq."]}, {"number": 6760, "title": "distributed example fail on GPU", "body": "When I run `https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dist_test/python/mnist_replica.py`, the distributed example\r\nI can run it with CPU(1 ps and 2 workers all on CPU in same machine )\r\nbut when I run it with GPU, I set the GPU\r\nnumber to 2(1 ps on CPU and 2 workers on GPU in same machine), the first worker runs normally, but when I start the second one, it shows the following errors\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/mlabs/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 972, in _do_call\r\n    return fn(*args)\r\n  File \"/home/mlabs/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 954, in _run_fn\r\n    status, run_metadata)\r\n  File \"/home/mlabs/anaconda3/lib/python3.5/contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"/home/mlabs/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/errors.py\", line 463, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors.InvalidArgumentError: WhereOp: Race condition between counting the number of true elements and writing them.  When counting, saw 1363 elements; but when writing their indices, saw 12 elements.\r\n\t [[Node: report_uninitialized_variables/boolean_mask/Where = Where[_device=\"/job:worker/replica:0/task:1/cpu:0\"](report_uninitialized_variables/boolean_mask/Reshape_1)]]\r\n\t [[Node: report_uninitialized_variables/boolean_mask/Where_G11 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:1/gpu:1\", send_device=\"/job:worker/replica:0/task:1/cpu:0\", send_device_incarnation=-7465067838139069765, tensor_name=\"edge_29_report_uninitialized_variables/boolean_mask/Where\", tensor_type=DT_INT64, _device=\"/job:worker/replica:0/task:1/gpu:1\"]()]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"mnist_replica.py\", line 266, in <module>\r\n    tf.app.run()\r\n  File \"/home/mlabs/anaconda3/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 30, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"mnist_replica.py\", line 223, in main\r\n    config=sess_config)\r\n  File \"/home/mlabs/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/supervisor.py\", line 722, in prepare_or_wait_for_session\r\n    max_wait_secs=max_wait_secs)\r\n  File \"/home/mlabs/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/session_manager.py\", line 349, in wait_for_session\r\n    is_ready, not_ready_msg = self._model_ready(sess)\r\n  File \"/home/mlabs/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/session_manager.py\", line 435, in _model_ready\r\n    return self._ready(self._ready_op, sess, \"Model not ready\")\r\n  File \"/home/mlabs/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/session_manager.py\", line 404, in _ready\r\n    ready_value = sess.run(op)\r\n  File \"/home/mlabs/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 717, in run\r\n    run_metadata_ptr)\r\n  File \"/home/mlabs/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 915, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/home/mlabs/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 965, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/home/mlabs/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 985, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors.InvalidArgumentError: WhereOp: Race condition between counting the number of true elements and writing them.  When counting, saw 1363 elements; but when writing their indices, saw 12 elements.\r\n\t [[Node: report_uninitialized_variables/boolean_mask/Where = Where[_device=\"/job:worker/replica:0/task:1/cpu:0\"](report_uninitialized_variables/boolean_mask/Reshape_1)]]\r\n\t [[Node: report_uninitialized_variables/boolean_mask/Where_G11 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:1/gpu:1\", send_device=\"/job:worker/replica:0/task:1/cpu:0\", send_device_incarnation=-7465067838139069765, tensor_name=\"edge_29_report_uninitialized_variables/boolean_mask/Where\", tensor_type=DT_INT64, _device=\"/job:worker/replica:0/task:1/gpu:1\"]()]]\r\n\r\nCaused by op 'report_uninitialized_variables/boolean_mask/Where', defined at:\r\n  File \"mnist_replica.py\", line 266, in <module>\r\n    tf.app.run()\r\n  File \"/home/mlabs/anaconda3/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 30, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"mnist_replica.py\", line 201, in main\r\n    global_step=global_step)\r\n  File \"/home/mlabs/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/supervisor.py\", line 310, in __init__\r\n    ready_op=ready_op, ready_for_local_init_op=ready_for_local_init_op)\r\n  File \"/home/mlabs/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/supervisor.py\", line 399, in _init_ready_op\r\n    ready_op = variables.report_uninitialized_variables()\r\n  File \"/home/mlabs/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\", line 1167, in report_uninitialized_variables\r\n    return array_ops.boolean_mask(variable_names_tensor, variables_mask)\r\n  File \"/home/mlabs/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 950, in boolean_mask\r\n    return _apply_mask_1d(tensor, mask)\r\n  File \"/home/mlabs/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 925, in _apply_mask_1d\r\n    indices = squeeze(where(mask), squeeze_dims=[1])\r\n  File \"/home/mlabs/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 3072, in where\r\n    result = _op_def_lib.apply_op(\"Where\", input=input, name=name)\r\n  File \"/home/mlabs/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 749, in apply_op\r\n    op_def=op_def)\r\n  File \"/home/mlabs/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2380, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/home/mlabs/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1298, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): WhereOp: Race condition between counting the number of true elements and writing them.  When counting, saw 1363 elements; but when writing their indices, saw 12 elements.\r\n\t [[Node: report_uninitialized_variables/boolean_mask/Where = Where[_device=\"/job:worker/replica:0/task:1/cpu:0\"](report_uninitialized_variables/boolean_mask/Reshape_1)]]\r\n\t [[Node: report_uninitialized_variables/boolean_mask/Where_G11 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:1/gpu:1\", send_device=\"/job:worker/replica:0/task:1/cpu:0\", send_device_incarnation=-7465067838139069765, tensor_name=\"edge_29_report_uninitialized_variables/boolean_mask/Where\", tensor_type=DT_INT64, _device=\"/job:worker/replica:0/task:1/gpu:1\"]()]]\r\n\r\n```", "comments": ["@michaelisard ", "There was a same error in https://github.com/tensorflow/tensorflow/issues/4033 which has been caused by PS crashing with CUDA_OUT_OF_MEMORY, could this be the issue here as well?", "Yes, I need to use CUDA_VISIBLE_DEVICES='' to disable the ps using memory, in this way, I can run multiple GPUs. Is this a bug? since PS should not use memory, and I have already set `sess_config.gpu_options.allow_growth = True`", "This appears to be a duplicate of #6378 which @zheng-xq should be taking a look into soon.", "sorry, I don't know how to use the distributed tensorflow, can you tell me how to use it?Just like how to write the python and how to execute it on terminal? ", "or may be some link is ok. All the website i see don't tell me how to execute the python code on terminal. "]}, {"number": 6759, "title": "Fix build errors on AVX2+ hosts with -march=native.", "body": "third_party/eigen3/unsupported/Eigen/CXX/FixedPoint and third_party/eigen3/unsupported/Eigen/CXX/Tensor use different paths to reach into unsupported/Eigen/CXX/src/Tensor. The former's paths cause the build errors documented in #6558. This commit removes the includes from third_party/eigen3/unsupported/Eigen/CXX/FixedPoint pointing into third_party/eigen3/unsupported/Eigen/CXX/FixedPoint, because they do not appear to be necessary.\r\n\r\n@benoitsteiner @rmlarsen The includes were added in #6323 -- do you happen to remember why they're there?\r\n\r\n/cc @drpngx If this PR is accepted, #6723 should be closed.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "(I asked because I don't have access to all the platforms)", "Jenkins, test this please.\r\n\r\nPlease ignore the estimator_test errors.", "@drpngx Thank you very much!", "I believe these were added prematurely. I think Benoit is looking to add fixed point matrix-matrix multiplies in the future that need thread pool support. But you can remove them in the meantime.", "@tensorflow-jenkins test this, please", "need merge 1e4d6f1c32be198a682518f1afce03c017c0aa11 into r1.0 branch."]}, {"number": 6758, "title": "Branch 144040081", "body": "", "comments": ["Pushing even the contrib estimator test broke."]}, {"number": 6757, "title": "Fix pip filenames.", "body": "", "comments": ["Jenkins, test this please.", "Approve despite flaky timeout in two Linux CPU tests."]}, {"number": 6756, "title": "Branch 144035212", "body": "", "comments": []}, {"number": 6755, "title": "OSX Fail to run tensorflow/contrib/makefile/download_dependencies.sh", "body": "Running on OSX\r\n\r\nRun \r\n\r\nsh tensorflow/contrib/makefile/download_dependencies.sh in the home directory\r\n\r\nAnd I get \r\n\r\ndownloading https://bitbucket.org/eigen/eigen/get/60578b474802.tar.gz\r\ndownloading https://github.com/google/gemmlowp/archive/a6f29d8ac48d63293f845f2253eccbf86bc28321.tar.gz\r\ndownloading https://github.com/google/googletest/archive/release-1.8.0.tar.gz\r\ndownloading https://github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz\r\ndownloading https://github.com/google/re2/archive/b94b7cd42e9f02673cd748c1ac1d16db4052514c.tar.gz\r\nsed: tensorflow/contrib/makefile/downloads/eigen/Eigen/src/Core/arch/NEON/Complex.h: No such file or directory\r\n\r\nHow can I solve this problem? Thanks a lot!!\r\n", "comments": ["Make sure your curl support https. Sometimes it is a problem for OSX. And the error message is muted in the download script.", "What did you do specifically to solve the issue? I'm having the same problem here on Mac OS X"]}]