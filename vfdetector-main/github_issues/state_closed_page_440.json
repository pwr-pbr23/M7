[{"number": 40658, "title": "tf2.2-tensorrt error", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution: DOCKER IMAGE:tensorflow/tensorflow:2.2.0-gpu\r\n- GPU model and memory:Tesla K40m, 12GB\r\n\r\n**Standalone code to reproduce the issue**\r\nI want to use tensorrt api in tensorflow to speed up model inference.\r\n## I simply created a model in savedmodel format with the following code\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nmodel = tf.keras.applications.ResNet50(\r\n    include_top=True,\r\n    weights=None,\r\n    input_shape=(300, 300, 3),\r\n    classes=30\r\n)\r\n\r\nmodel.compile(optimizer='adam', loss=tf.keras.losses.categorical_crossentropy)\r\n\r\ndata = np.random.normal(size=(3, 300, 300,3)).astype(np.float32)\r\n\r\nmodel.predict(data)\r\n\r\nmodel.save('original_saved_model')\r\n\r\n## I am going to use the following code for inferred acceleration\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nfrom absl import app\r\nfrom absl import flags\r\n\r\nFLAGS = flags.FLAGS\r\nflags.DEFINE_enum('mode', 'FP16', ['FP16', 'FP32'], 'FP16 or FP32')\r\nflags.DEFINE_string('original_model_dir', None, 'Original saved model dir')\r\nflags.DEFINE_string('output_dir', None, 'Output model dir')\r\n\r\nflags.mark_flags_as_required(['mode', 'original_model_dir', 'output_dir'])\r\n\r\n\r\ndef my_input_fn():\r\n    for _ in range(2):\r\n        # input1 = tf.random.normal(shape=(3, 300, 300, 3))\r\n        input1 = np.random.normal(size=(1, 3, 300, 300, 3)).astype(np.float32)\r\n        yield input1\r\n\r\n\r\ndef main(argv):\r\n    del argv\r\n    params = tf.experimental.tensorrt.ConversionParams(\r\n        precision_mode=FLAGS.mode,\r\n        max_batch_size=10,\r\n        maximum_cached_engines=16)\r\n    converter = tf.experimental.tensorrt.Converter(\r\n        input_saved_model_dir=FLAGS.original_model_dir, conversion_params=params)\r\n    converter.convert()\r\n    converter.build(input_fn=my_input_fn)\r\n    converter.save(FLAGS.output_dir)\r\n\r\n\r\nif __name__ == '__main__':\r\n    app.run(main)\r\n\r\n## But something wrong\r\n#### I simply thought change code `if not first_input` to `if first_input is not None` may ok. Maybe i am too naive.\r\n## The running log below\r\nroot@f973fbbb7e16:/niuyifeng/remote# python tensorrt_use_pre_engine.py --original_model_dir=original_saved_model/ --output_dir=use_pre_engine\r\n/usr/local/lib/python3.6/dist-packages/absl/flags/_validators.py:359: UserWarning: Flag --mode has a non-None default value; therefore, mark_flag_as_required will pass even if flag is not specified in the command line!\r\n  'command line!' % flag_name)\r\n2020-06-22 08:41:09.212475: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6\r\nINFO:tensorflow:Linked TensorRT version: (6, 0, 1)\r\nI0622 08:41:09.212588 139998758545216 trt_convert.py:264] Linked TensorRT version: (6, 0, 1)\r\nINFO:tensorflow:Loaded TensorRT version: (6, 0, 1)\r\nI0622 08:41:09.212863 139998758545216 trt_convert.py:265] Loaded TensorRT version: (6, 0, 1)\r\n2020-06-22 08:41:09.534433: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-06-22 08:41:09.541097: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:3b:00.0 name: Tesla K40m computeCapability: 3.5\r\ncoreClock: 0.745GHz coreCount: 15 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 268.58GiB/s\r\n2020-06-22 08:41:09.541222: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-06-22 08:41:09.541262: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-06-22 08:41:09.543652: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-06-22 08:41:09.543997: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-06-22 08:41:09.546345: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-06-22 08:41:09.547615: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-06-22 08:41:09.547674: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-06-22 08:41:09.549420: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2020-06-22 08:41:09.549693: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n2020-06-22 08:41:09.560237: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2100000000 Hz\r\n2020-06-22 08:41:09.563950: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f5258000b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-06-22 08:41:09.563997: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-06-22 08:41:09.646046: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x8e78220 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-06-22 08:41:09.646096: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K40m, Compute Capability 3.5\r\n2020-06-22 08:41:09.648938: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:3b:00.0 name: Tesla K40m computeCapability: 3.5\r\ncoreClock: 0.745GHz coreCount: 15 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 268.58GiB/s\r\n2020-06-22 08:41:09.649015: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-06-22 08:41:09.649044: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-06-22 08:41:09.649082: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-06-22 08:41:09.649108: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-06-22 08:41:09.649134: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-06-22 08:41:09.649160: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-06-22 08:41:09.649185: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-06-22 08:41:09.653889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2020-06-22 08:41:09.653937: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-06-22 08:41:09.946823: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-06-22 08:41:09.946877: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \r\n2020-06-22 08:41:09.946895: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \r\n2020-06-22 08:41:09.953775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10650 MB memory) -> physical GPU (device: 0, name: Tesla K40m, pci bus id: 0000:3b:00.0, compute capability: 3.5)\r\n2020-06-22 08:41:17.862854: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n2020-06-22 08:41:17.862987: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-06-22 08:41:17.865596: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:3b:00.0 name: Tesla K40m computeCapability: 3.5\r\ncoreClock: 0.745GHz coreCount: 15 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 268.58GiB/s\r\n2020-06-22 08:41:17.865680: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-06-22 08:41:17.865701: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-06-22 08:41:17.865729: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-06-22 08:41:17.865760: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-06-22 08:41:17.865791: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-06-22 08:41:17.865906: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-06-22 08:41:17.865925: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-06-22 08:41:17.869346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2020-06-22 08:41:17.869396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-06-22 08:41:17.869412: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \r\n2020-06-22 08:41:17.869473: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \r\n2020-06-22 08:41:17.874638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10650 MB memory) -> physical GPU (device: 0, name: Tesla K40m, pci bus id: 0000:3b:00.0, compute capability: 3.5)\r\n2020-06-22 08:41:17.938253: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: graph_to_optimize\r\n2020-06-22 08:41:17.938355: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: Graph size after: 1201 nodes (878), 1861 edges (1538), time = 32.534ms.\r\n2020-06-22 08:41:17.938368: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0.703ms.\r\n2020-06-22 08:41:20.273359: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n2020-06-22 08:41:20.273507: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-06-22 08:41:20.276428: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:3b:00.0 name: Tesla K40m computeCapability: 3.5\r\ncoreClock: 0.745GHz coreCount: 15 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 268.58GiB/s\r\n2020-06-22 08:41:20.276638: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-06-22 08:41:20.276660: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-06-22 08:41:20.276682: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-06-22 08:41:20.276703: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-06-22 08:41:20.276848: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-06-22 08:41:20.276865: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-06-22 08:41:20.276880: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-06-22 08:41:20.281167: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2020-06-22 08:41:20.281210: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-06-22 08:41:20.281226: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \r\n2020-06-22 08:41:20.281256: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \r\n2020-06-22 08:41:20.286624: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10650 MB memory) -> physical GPU (device: 0, name: Tesla K40m, pci bus id: 0000:3b:00.0, compute capability: 3.5)\r\n2020-06-22 08:41:21.637672: I tensorflow/compiler/tf2tensorrt/segment/segment.cc:460] There are 6 ops of 3 different types in the graph that are not converted to TensorRT: Identity, NoOp, Placeholder, (For more information see https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#supported-ops).\r\n2020-06-22 08:41:21.720564: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:643] Number of TensorRT candidate segments: 1\r\n2020-06-22 08:41:21.909022: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:745] Replaced segment 0 consisting of 505 nodes by TRTEngineOp_0.\r\n2020-06-22 08:41:22.604272: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: tf_graph\r\n2020-06-22 08:41:22.604326: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 881 nodes (-320), 1221 edges (-640), time = 410.577ms.\r\n2020-06-22 08:41:22.604400: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   layout: Graph size after: 885 nodes (4), 1225 edges (4), time = 252.41ms.\r\n2020-06-22 08:41:22.604411: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 883 nodes (-2), 1223 edges (-2), time = 124.309ms.\r\n2020-06-22 08:41:22.604420: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   TensorRTOptimizer: Graph size after: 379 nodes (-504), 380 edges (-843), time = 621.076ms.\r\n2020-06-22 08:41:22.604445: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 325 nodes (-54), 326 edges (-54), time = 11.802ms.\r\n2020-06-22 08:41:22.604474: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: TRTEngineOp_0_native_segment\r\n2020-06-22 08:41:22.604490: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 507 nodes (0), 522 edges (0), time = 75.445ms.\r\n2020-06-22 08:41:22.604514: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   layout: Graph size after: 507 nodes (0), 522 edges (0), time = 168.448ms.\r\n2020-06-22 08:41:22.604546: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 507 nodes (0), 522 edges (0), time = 80.639ms.\r\n2020-06-22 08:41:22.604560: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   TensorRTOptimizer: Graph size after: 507 nodes (0), 522 edges (0), time = 22.492ms.\r\n2020-06-22 08:41:22.604569: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 507 nodes (0), 522 edges (0), time = 77.95ms.\r\n2020-06-22 08:41:25.885707: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:982] Building a new TensorRT engine for TRTEngineOp_0 with input shapes: [[3,300,300,3]]\r\n2020-06-22 08:41:25.885767: I tensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc:1153] Linked TensorRT version: 6.0.1\r\n2020-06-22 08:41:25.885966: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6\r\n2020-06-22 08:41:25.885989: I tensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc:1154] Loaded TensorRT version: 6.0.1\r\n2020-06-22 08:41:25.888300: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6\r\n2020-06-22 08:41:26.226543: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Calling isShapeTensor before the entire network is constructed may result in an inaccurate result.\r\n2020-06-22 08:41:26.227079: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Half2 support requested on hardware without native FP16 support, performance will be negatively affected.\r\nTraceback (most recent call last):\r\n  File \"tensorrt_use_pre_engine.py\", line 48, in <module>\r\n    app.run(main)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"tensorrt_use_pre_engine.py\", line 43, in main\r\n    converter.build(input_fn=my_input_fn)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/compiler/tensorrt/trt_convert.py\", line 1172, in build\r\n    if not first_input:\r\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\r\n\r\n", "comments": ["Was able to reproduce the issue with TF v2.2 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/0a7b88d8611f8e19d23bde3a3e7156ce/40658.ipynb). Thanks!", "@niuyifeng1213,\r\nPlease take a look at [this similar](https://github.com/tensorflow/tensorflow/issues/32563) issue and let us know if it helps. Thanks!", "I changed the code to the following. It succeeded.Thank you very much\r\n\r\n```\r\ndef my_input_fn():\r\n    for _ in range(2):\r\n        # input1 = tf.random.normal(shape=(3, 300, 300, 3))\r\n        input1 = list(np.random.normal(size=(1, 3, 300, 300, 3)).astype(np.float32))\r\n        yield input1\r\n```"]}, {"number": 40657, "title": "[Windows] Debug link odr", "body": "1. The PDB is too big for tensorflow_cc.\r\nSince Bazel is a pain to use, changes to `tensorflow/BUILD` are useless but if there are expert, I would be more than happy to update the PR.\r\nOtherwise I will still have to split the build in two:\r\n```\r\nbazel build -c gdb --extratoolchain...clang tensorflow_framework\r\nbazel build -c gdb --extratoolchain...clang --linkopt=\"/DEBUG:NONE\" tensorflow_cc\r\n```\r\n\r\n2. If you prefer a [[noreturn]]/throw or pure virtual, let me now...", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40657) for more info**.\n\n<!-- need_sender_cla -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40657) for more info**.\n\n<!-- ok -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40657) for more info**.\n\n<!-- ok -->", "@googlebot I signed it!", "@Niram7777 Can you please check @chsigg's comments and keep us posted. Thanks!", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@Niram7777  Any update on this PR? and please resolve conflicts Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 40656, "title": "Tensorflow creating too many threads", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): via pip\r\n- TensorFlow version (use command below): v1.12.1-26458-gc251e83 2.2.0-rc0\r\n- Python version: 3.7.3\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 10.2\r\n- GPU model and memory: 2x V100 (32GB) or 4x V100 (32GB)\r\n\r\n**Describe the current behavior**\r\n\r\nI am trying to run a simple model through some custom cross-validation code using Distributed Training and python multiproccessing to speed up execution. I get nearly through the first fold of CV when the command \"watch -n1 nvidia-smi\" (used to monitor GPU usage) starts giving an error of \"fork failed: Resource temporary unavailable\", i.e. the maximum number of threads has been reached. This is when the code hangs. I use this via LSF, so after some time where I notice that no progress has been made, I kill off the job myself, at which time, I get an error saying \"Runtime error: can't start new thread\"\r\n\r\n**Describe the expected behavior**\r\n\r\nAll folds of the CV code are finshed and some metrics are calculated.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nI have ~100K image files, and their name gives their respective label. I first load them all to get them into cache:\r\n\r\n```\r\ndef load_file(path):\r\n\r\n    arr = np.load(path)['arr_0'][0:None:resolution, 0:None:resolution, :fields]\r\n    for field in range(arr.shape[-1]):\r\n\r\n        scaler = StandardScaler()\r\n        scaler.fit(arr[:, :, field])\r\n        arr[:, :, field] = scaler.transform(arr[:, :, field])\r\n    \r\n    name = path.replace(\".npz\", \"\").split(\"/\")[-1]\r\n\r\n    if \"no\" in name:\r\n        cat, start_lat, end_lat, start_lon, end_lon, date = name.split(\"_\")\r\n        label = 0\r\n    else:\r\n        cat, press, wind, start_lat, end_lat, start_lon, end_lon, date = name.split(\"_\")\r\n        cat = int(cat)\r\n        if cat < 1:\r\n            label = 0\r\n        else:\r\n            label = 1\r\n            \r\n    return arr, label\r\n\r\npool = mp.Pool(int(mp.cpu_count()))\r\nres = list(tqdm.tqdm(pool.imap(load_file, cat1_files), total=len(cat1_files)))\r\nres = list(tqdm.tqdm(pool.imap(load_file, cat2_files), total=len(cat2_files)))\r\nres = list(tqdm.tqdm(pool.imap(load_file, cat3_files), total=len(cat3_files)))\r\nres = list(tqdm.tqdm(pool.imap(load_file, cat4_files), total=len(cat4_files)))\r\nres = list(tqdm.tqdm(pool.imap(load_file, cat5_files), total=len(cat5_files)))\r\nres = list(tqdm.tqdm(pool.imap(load_file, no_files), total=len(no_files)))\r\n```\r\n\r\nI then create a Generator to load the data in the model. The previously opened pool is used here as well to speed up loading of files:\r\n\r\n```\r\nclass DataGenerator(tf.keras.utils.Sequence):\r\n    \r\n    def __init__(self, paths, batch_size=32, dim=(86, 128, 5), shuffle=True, data_aug=True):\r\n        self.dim = dim\r\n        self.batch_size = batch_size\r\n        self.paths = paths\r\n        self.shuffle = shuffle\r\n        self.on_epoch_end()\r\n\r\n    def __len__(self):\r\n        'Denotes the number of batches per epoch'\r\n        return int(np.floor(len(self.paths) / self.batch_size))\r\n\r\n    def __getitem__(self, index):\r\n        'Generate one batch of data'\r\n        # Generate indexes of the batch\r\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\r\n\r\n        # Find list of IDs\r\n        paths_temp = [self.paths[k] for k in indexes]\r\n\r\n        # Generate data\r\n        X, y = self.__data_generation(paths_temp)\r\n\r\n        return X, y\r\n\r\n    def on_epoch_end(self):\r\n        self.indexes = np.arange(len(self.paths))\r\n        if self.shuffle == True:\r\n            np.random.shuffle(self.indexes)\r\n            \r\n    def __data_generation(self, paths_temp):\r\n        \r\n        X = np.zeros((self.batch_size, self.dim[0], self.dim[1], self.dim[2]), dtype=\"float32\")\r\n        y = np.zeros(self.batch_size)\r\n        pool_res = list(pool.imap(load_file, paths_temp))\r\n        \r\n        for i in range(len(pool_res)):\r\n            X[i], y[i] = pool_res[i]\r\n            \r\n        return X, y\r\n```\r\n\r\nThe penultimate step is to create a function that instantiates a new model. This is a fairly simple CNN classifier:\r\n\r\n```\r\ndef create_model(shape):\r\n    \r\n    mirrored_strategy = tf.distribute.MirroredStrategy()\r\n    with mirrored_strategy.scope():\r\n\r\n        model = models.Sequential() \r\n\r\n        model.add(layers.Conv2D(8, (2, 2), padding=\"same\", strides=(1, 1), input_shape=(shape))) \r\n        model.add(layers.ReLU())\r\n        model.add(layers.MaxPooling2D((2, 2), strides=1)) \r\n        model.add(layers.Conv2D(16, (1, 1), strides=(1, 1))) \r\n        model.add(layers.ReLU())\r\n        model.add(layers.MaxPooling2D((2, 2), strides=1))\r\n        model.add(layers.Conv2D(32, (2, 2), strides=(1, 1))) \r\n        model.add(layers.ReLU())\r\n        model.add(layers.MaxPooling2D((2, 2), strides=1))\r\n        model.add(layers.Conv2D(64, (2, 2), strides=(1, 1))) \r\n        model.add(layers.ReLU())\r\n        model.add(layers.MaxPooling2D((2, 2), strides=1))\r\n        model.add(layers.Conv2D(128, (2, 2), strides=(1, 1))) \r\n        model.add(layers.ReLU())\r\n        model.add(layers.MaxPooling2D((2, 2), strides=1))\r\n        model.add(layers.Conv2D(256, (2, 2), strides=(1, 1))) \r\n        model.add(layers.ReLU())\r\n        model.add(layers.MaxPooling2D((2, 2), strides=1))\r\n\r\n        model.add(layers.Flatten()) \r\n        model.add(layers.Dense(128))\r\n        model.add(layers.ReLU())\r\n        model.add(layers.Dense(64))\r\n        model.add(layers.ReLU())\r\n        model.add(layers.Dense(32))\r\n        model.add(layers.ReLU())\r\n        model.add(layers.Dense(1, activation='sigmoid')) \r\n\r\n        METRICS = [\r\n              tf.keras.metrics.TruePositives(name='tp'),\r\n              tf.keras.metrics.FalsePositives(name='fp'),\r\n              tf.keras.metrics.TrueNegatives(name='tn'),\r\n              tf.keras.metrics.FalseNegatives(name='fn'), \r\n              tf.keras.metrics.BinaryAccuracy(name='accuracy'),\r\n              tf.keras.metrics.Precision(name='precision'),\r\n              tf.keras.metrics.Recall(name='recall'),\r\n              tf.keras.metrics.AUC(name='auc'),\r\n        ]\r\n\r\n    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=METRICS) \r\n    \r\n    return model\r\n```\r\n\r\nFinally, the Cross-Validation code, which trains a model on each fold and saves the model:\r\n\r\n```\r\ntrain_fold_paths = []\r\nval_fold_paths = []\r\n\r\nfor fold in range(k):\r\n    \r\n    cat1_fold = cat1_train_files[int(fold/k*len(cat1_train_files)) : int((fold+1)/k*len(cat1_train_files))]\r\n    cat2_fold = cat2_train_files[int(fold/k*len(cat2_train_files)) : int((fold+1)/k*len(cat2_train_files))]\r\n    cat3_fold = cat3_train_files[int(fold/k*len(cat3_train_files)) : int((fold+1)/k*len(cat3_train_files))]\r\n    cat4_fold = cat4_train_files[int(fold/k*len(cat4_train_files)) : int((fold+1)/k*len(cat4_train_files))]\r\n    cat5_fold = cat5_train_files[int(fold/k*len(cat5_train_files)) : int((fold+1)/k*len(cat5_train_files))]  \r\n    yes_fold = cat1_fold + cat2_fold + cat3_fold + cat4_fold + cat5_fold\r\n    no_fold = no_train_files[int(fold/k*len(no_train_files)) : int((fold+1)/k*len(no_train_files))]\r\n    \r\n    fold_data = yes_fold + no_fold\r\n    \r\n    val_fold_paths.append(fold_data)\r\n    \r\n    fold_data = no_fold + yes_fold\r\n    train_fold_paths.append(fold_data)\r\n\r\nhistory_folds = []\r\nmodel_folds = []\r\n\r\nfor i_fold in range(k):\r\n    \r\n    train_folds = list(np.arange(i_fold, k-1+i_fold)%k)\r\n\r\n    train_paths = []\r\n    for fold_index in train_folds:\r\n        if train_paths != None:\r\n            train_paths += train_fold_paths[fold_index]\r\n        else:\r\n            train_paths = train_fold_paths[fold_index]\r\n            \r\n    val_paths = val_fold_paths[(k+i_fold-1)%k]\r\n    \r\n    dummy_file, _ = load_file(cat1_files[0])\r\n    \r\n    params_train = {'dim': dummy_file.shape, 'batch_size': batch_size, 'shuffle': True, 'data_aug': False}\r\n    params_val = {'dim': dummy_file.shape, 'batch_size': batch_size, 'shuffle': False, 'data_aug': False}\r\n\r\n    training_generator = DataGenerator(train_paths, **params_train)\r\n    validation_generator = DataGenerator(val_paths, **params_val)\r\n    \r\n    model = create_model(dummy_file.shape)\r\n\r\n    print(\"Training Fold \" + str(i_fold+1))\r\n    print(\"Training Fold \" + str(i_fold+1), file=console_out)\r\n        \r\n    history = model.fit_generator(generator=training_generator, validation_data=validation_generator, use_multiprocessing=True, workers=int(0.5*mp.cpu_count()), \r\n                                  verbose=1, epochs=epochs, max_queue_size = int(len(train_paths)/batch_size))\r\n    \r\n    history_folds.append(history)\r\n        \r\n    model.save(fold_model_name + str(i_fold+1))\r\n    tf.keras.backend.clear_session()\r\n    \r\n    tf.keras.experimental.terminate_keras_multiprocessing_pools(grace_period=0.1, use_sigkill=True)\r\n\r\n```\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nOutput of \"ulimit -a\":\r\n\r\n```\r\ncore file size          (blocks, -c) 0\r\ndata seg size           (kbytes, -d) unlimited\r\nscheduling priority             (-e) 0\r\nfile size               (blocks, -f) unlimited\r\npending signals                 (-i) 3090396\r\nmax locked memory       (kbytes, -l) unlimited\r\nmax memory size         (kbytes, -m) unlimited\r\nopen files                      (-n) 48000\r\npipe size            (512 bytes, -p) 8\r\nPOSIX message queues     (bytes, -q) 819200\r\nreal-time priority              (-r) 0\r\nstack size              (kbytes, -s) 2097151\r\ncpu time               (seconds, -t) unlimited\r\nmax user processes              (-u) 4096\r\nvirtual memory          (kbytes, -v) unlimited\r\nfile locks                      (-x) unlimited\r\n\r\n```\r\n\r\nPart of Stacktrace:\r\n\r\n```\r\nException in thread Thread-31178:\r\nTraceback (most recent call last):\r\n  File \"/apps/contrib/jaspy/miniconda_envs/jaspy3.7/m3-4.6.14/lib/python3.7/threading.py\", line 917, in _bootstrap_inner\r\n    self.run()\r\n  File \"/apps/contrib/jaspy/miniconda_envs/jaspy3.7/m3-4.6.14/lib/python3.7/threading.py\", line 865, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/home/users/dgalea/.local/lib/python3.7/site-packages/tensorflow/python/keras/utils/data_utils.py\", line 843, in _run\r\n    with closing(self.executor_fn(_SHARED_SEQUENCES)) as executor:\r\n  File \"/home/users/dgalea/.local/lib/python3.7/site-packages/tensorflow/python/keras/utils/data_utils.py\", line 732, in <lambda>\r\n    self.executor_fn = lambda _: get_pool_class(False)(workers)\r\n  File \"/apps/contrib/jaspy/miniconda_envs/jaspy3.7/m3-4.6.14/lib/python3.7/multiprocessing/dummy/__init__.py\", line 124, in Pool\r\n    return ThreadPool(processes, initializer, initargs)\r\n  File \"/apps/contrib/jaspy/miniconda_envs/jaspy3.7/m3-4.6.14/lib/python3.7/multiprocessing/pool.py\", line 802, in __init__\r\n    Pool.__init__(self, processes, initializer, initargs)\r\n  File \"/apps/contrib/jaspy/miniconda_envs/jaspy3.7/m3-4.6.14/lib/python3.7/multiprocessing/pool.py\", line 176, in __init__\r\n    self._repopulate_pool()\r\n  File \"/apps/contrib/jaspy/miniconda_envs/jaspy3.7/m3-4.6.14/lib/python3.7/multiprocessing/pool.py\", line 241, in _repopulate_pool\r\n    w.start()\r\n  File \"/apps/contrib/jaspy/miniconda_envs/jaspy3.7/m3-4.6.14/lib/python3.7/multiprocessing/dummy/__init__.py\", line 51, in start\r\n    threading.Thread.start(self)\r\n  File \"/apps/contrib/jaspy/miniconda_envs/jaspy3.7/m3-4.6.14/lib/python3.7/threading.py\", line 847, in start\r\n    _start_new_thread(self._bootstrap, ())\r\nRuntimeError: can't start new thread\r\n\r\n```", "comments": ["@dgaleareading \r\nPlease share complete executable code such that we can replicate the issue faced, i ran the code shared and face different error, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/db23ed9f34b0f9851738ffa6d2ac9d62/untitled237.ipynb) or if possible please share a colab gist with the reported issue for us to analyse.", "@Saduf2019 I have put the whole notebook on my Google Drive at https://drive.google.com/file/d/1O9phSN7moTo-5KQNqPPQ4Qfl9OJLqZvd/view?usp=sharing", "@dgaleareading \r\nI ran the code shared, please let us know if [this gist](https://colab.research.google.com/gist/Saduf2019/90c0bd531b8915ba97889d0de2271000/untitled239.ipynb) confirm your issue.", "Unfortunately it does not as you don't have any of the correct input files and thus it is failing in the loading routines. A reminder that this code doesn't actually fail, it creates too many threads, at which point it hangs infinitly. ", "Hi @dgaleareading, given your error message \"fork failed: Resource temporary unavailable\" it sounds like you are creating too many processes and exceeding your max of 4096. [Pool](https://docs.python.org/3.4/library/multiprocessing.html?highlight=process) uses process based parallelism, which will also create more processes, and then you're also calling model.fit_generator with use_multiprocess=True.\r\n\r\nA couple of questions to help with investigation:\r\n1) do you have this same problem when running without distributed training?\r\n2) can you try fit_generator with use_multiprocessing=False\r\n\r\nAlso, can you try running [pstree](https://man7.org/linux/man-pages/man1/pstree.1.html) to see which process is creating a lot of child processes?", "Hi @nikitamaia, thanks for your reply. Here are my answers:\r\n\r\n>1. do you have this same problem when running without distributed training?\r\n\r\nYes, I still have the same problem without distributed training\r\n\r\n>2. can you try fit_generator with use_multiprocessing=False\r\n\r\nI still get the same problem with use_multiprocessing=False\r\n\r\n>Also, can you try running pstree to see which process is creating a lot of child processes?\r\n\r\nMy system is down for a few days (due to a power cut at the HPC site), but I will put the readout of the pstree command whenever it is bck online.", "@nikitamaia I have managed to run pstree while my code was running. These results were obtained during the first epoch of my first fold.\r\n\r\n[full_tree.txt](https://github.com/tensorflow/tensorflow/files/4827892/full_tree.txt) has the full expanded tree returned by `pstree -ac` while [collapsed_full_tree.txt](https://github.com/tensorflow/tensorflow/files/4827890/collapsed_full_tree.txt) has the slightly more compressed tree returned by `pstree -a`\r\n\r\nI have also run the command only for my user and the output is given in [user_tree.txt](https://github.com/tensorflow/tensorflow/files/4827893/user_tree.txt) with the collapsed version in [collapsed_user_tree.txt](https://github.com/tensorflow/tensorflow/files/4827891/collapsed_user_tree.txt)\r\n\r\nIn the above results, there is the line `180*[{python3}]` which seems to be unnecessarily large. Do you have any idea on how I can fix my issue?", "UPDATE:  My code, without the distributed learning, works with TF 1.14, Keras 2.3 and Python 3.6.8. I think it would be useful to see why it doesn't work in pure TF2.", "Hm that is very mysterious indeed. Have you tried loading all data and generating the folds (without training) to see if the process of loading the data is causing the large number of processes? ", "Yes I have but it did not soolve the problem.", "Can you clarify what you mean? Did you still get the same \"fork failed\" error when trying to load the data and create the folds without training?", "Yes, I still get  \"fork failed\" when trying to load the data and creating the folds without training. Would that mean that there is something wrong in the data pipeline?", "Yes I think so. Sounds like the use of pool in the multiprocessing library is causing excessive creation of processes.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 40655, "title": "the memory couldn't be released after training", "body": "\r\nhi,dear\r\nhelp wanted,\r\nwhen I see the memory after a training in While\r\nconfused me that the memory can't be released in time sleep\r\nthe codes down for reference\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport time\r\ndef train_func():\r\n    model = tf.keras.Sequential() #\r\n    model.add(tf.keras.layers.Dense(512,activation='relu',input_shape=(784,)))\r\n    model.add(tf.keras.layers.Dense(256,activation='relu'))\r\n    model.add(tf.keras.layers.Dense(10,activation='softmax'))\r\n    model.summary()\r\n\r\n\r\n    X_train=np.random.randn(10000,784)\r\n    y_train=np.random.randint(0,10,size=10000)\r\n\r\n    y_train=tf.keras.utils.to_categorical(y_train,10)\r\n    model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\r\n    model.fit(X_train,y_train,batch_size=64,epochs=1)\r\n    \r\nwhile True:\r\n    train_func()\r\n    print(\"training over !!!\")\r\n    time.sleep(1000)\r\n```\r\n\r\nthe memory \r\n```\r\n10000/10000 [==============================] - 1s 59us/sample - loss: 2.3762 - acc: 0.1027\r\ntraining over !!!\r\n```\r\n![image](https://user-images.githubusercontent.com/35590066/85261428-b99b0c00-b49e-11ea-8d13-10974796160b.png)\r\n\r\nSO how to release the memory ?\r\n\r\nthx\r\n\r\n", "comments": ["of course could ignore the **while**\r\nas follows\r\n```\r\ntrain_func()\r\nprint(\"training over !!!\")\r\ntime.sleep(1000)\r\n```\r\ncould also see the same situation\r\n", "@ucasiggcas \r\n\r\nCan you please fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.I have tried in colab with TF version-gpu  2.2 and i noticed its hanging.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/6cec38b478643cd5db62981198cc0a70/untitled57.ipynb).Thanks!\r\n\r\n", "hi,dear\r\nany version could try,\r\nONLY need GPU,\r\nI just want to know the memory after training\r\ncould you see the pic by \r\n`nvidia-smi`\r\n", "@ucasiggcas Please take a look at similar [issue](https://github.com/tensorflow/tensorflow/issues/36465) and let me know if it helps. Thanks!", "hi,thx\r\nhave tried the codes,SUCCESS\r\n\r\n```\r\nprocess_eval = multiprocessing.Process(target=evaluate, args=(...))\r\nprocess_eval.start()\r\nprocess_eval.join()\r\n```\r\nwill close\r\n"]}, {"number": 40654, "title": "Fix GCC 10.1 compile error.", "body": "This PR fix compile error using gcc 10.1\r\n\r\n**Description**\r\n\r\nThere is an ```auto``` inference failure error fixed by this PR:\r\n\r\n```\r\n~/rpmbuild/BUILD/tensorflow/_bazel_cbalint/4c79ce0d14678d18eb8640cac68aaf03/execroot/org_tensorflow ~/rpmbuild/BUILD/tensorflow/tensorflow\r\ntensorflow/python/lib/core/bfloat16.cc: In function \u2018bool tensorflow::{anonymous}::Initialize()\u2019:\r\ntensorflow/python/lib/core/bfloat16.cc:664:36: error: no match for call to \u2018(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [6], <unresolved overloaded function type>, const std::array<int, 3>&)\u2019\r\n  664 |                       compare_types)) {\r\n      |                                    ^\r\ntensorflow/python/lib/core/bfloat16.cc:637:25: note: candidate: \u2018tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>\u2019\r\n  637 |   auto register_ufunc = [&](const char* name, PyUFuncGenericFunction fn,\r\n      |                         ^\r\ntensorflow/python/lib/core/bfloat16.cc:637:25: note:   no known conversion for argument 2 from \u2018<unresolved overloaded function type>\u2019 to \u2018PyUFuncGenericFunction\u2019 {aka \u2018void (*)(char**, const long int*, const long int*, void*)\u2019}\r\ntensorflow/python/lib/core/bfloat16.cc:668:36: error: no match for call to \u2018(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [10], <unresolved overloaded function type>, const std::array<int, 3>&)\u2019\r\n  668 |                       compare_types)) {\r\n      |                                    ^\r\ntensorflow/python/lib/core/bfloat16.cc:637:25: note: candidate: \u2018tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>\u2019\r\n  637 |   auto register_ufunc = [&](const char* name, PyUFuncGenericFunction fn,\r\n      |                         ^\r\ntensorflow/python/lib/core/bfloat16.cc:637:25: note:   no known conversion for argument 2 from \u2018<unresolved overloaded function type>\u2019 to \u2018PyUFuncGenericFunction\u2019 {aka \u2018void (*)(char**, const long int*, const long int*, void*)\u2019}\r\ntensorflow/python/lib/core/bfloat16.cc:671:77: error: no match for call to \u2018(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [5], <unresolved overloaded function type>, const std::array<int, 3>&)\u2019\r\n  671 |   if (!register_ufunc(\"less\", CompareUFunc<Bfloat16LtFunctor>, compare_types)) {\r\n      |                                                                             ^\r\ntensorflow/python/lib/core/bfloat16.cc:637:25: note: candidate: \u2018tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>\u2019\r\n  637 |   auto register_ufunc = [&](const char* name, PyUFuncGenericFunction fn,\r\n      |                         ^\r\ntensorflow/python/lib/core/bfloat16.cc:637:25: note:   no known conversion for argument 2 from \u2018<unresolved overloaded function type>\u2019 to \u2018PyUFuncGenericFunction\u2019 {aka \u2018void (*)(char**, const long int*, const long int*, void*)\u2019}\r\ntensorflow/python/lib/core/bfloat16.cc:675:36: error: no match for call to \u2018(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [8], <unresolved overloaded function type>, const std::array<int, 3>&)\u2019\r\n  675 |                       compare_types)) {\r\n      |                                    ^\r\ntensorflow/python/lib/core/bfloat16.cc:637:25: note: candidate: \u2018tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>\u2019\r\n  637 |   auto register_ufunc = [&](const char* name, PyUFuncGenericFunction fn,\r\n      |                         ^\r\ntensorflow/python/lib/core/bfloat16.cc:637:25: note:   no known conversion for argument 2 from \u2018<unresolved overloaded function type>\u2019 to \u2018PyUFuncGenericFunction\u2019 {aka \u2018void (*)(char**, const long int*, const long int*, void*)\u2019}\r\ntensorflow/python/lib/core/bfloat16.cc:679:36: error: no match for call to \u2018(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [11], <unresolved overloaded function type>, const std::array<int, 3>&)\u2019\r\n  679 |                       compare_types)) {\r\n      |                                    ^\r\ntensorflow/python/lib/core/bfloat16.cc:637:25: note: candidate: \u2018tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>\u2019\r\n  637 |   auto register_ufunc = [&](const char* name, PyUFuncGenericFunction fn,\r\n      |                         ^\r\ntensorflow/python/lib/core/bfloat16.cc:637:25: note:   no known conversion for argument 2 from \u2018<unresolved overloaded function type>\u2019 to \u2018PyUFuncGenericFunction\u2019 {aka \u2018void (*)(char**, const long int*, const long int*, void*)\u2019}\r\ntensorflow/python/lib/core/bfloat16.cc:683:36: error: no match for call to \u2018(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [14], <unresolved overloaded function type>, const std::array<int, 3>&)\u2019\r\n  683 |                       compare_types)) {\r\n      |                                    ^\r\ntensorflow/python/lib/core/bfloat16.cc:637:25: note: candidate: \u2018tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>\u2019\r\n  637 |   auto register_ufunc = [&](const char* name, PyUFuncGenericFunction fn,\r\n      |                         ^\r\ntensorflow/python/lib/core/bfloat16.cc:637:25: note:   no known conversion for argument 2 from \u2018<unresolved overloaded function type>\u2019 to \u2018PyUFuncGenericFunction\u2019 {aka \u2018void (*)(char**, const long int*, const long int*, void*)\u2019}\r\n```\r\n\r\n* Relevant toolchain versions:\r\n\r\n```\r\n$ gcc -v\r\nUsing built-in specs.\r\nCOLLECT_GCC=gcc\r\nCOLLECT_LTO_WRAPPER=/usr/libexec/gcc/x86_64-redhat-linux/10/lto-wrapper\r\nOFFLOAD_TARGET_NAMES=nvptx-none\r\nOFFLOAD_TARGET_DEFAULT=1\r\nTarget: x86_64-redhat-linux\r\nConfigured with: ../configure --enable-bootstrap --enable-languages=c,c++,fortran,objc,obj-c++,ada,go,d,lto --prefix=/usr --mandir=/usr/share/man --infodir=/usr/share/info --with-bugurl=http://bugzilla.redhat.com/bugzilla --enable-shared --enable-threads=posix --enable-checking=release --enable-multilib --with-system-zlib --enable-__cxa_atexit --disable-libunwind-exceptions --enable-gnu-unique-object --enable-linker-build-id --with-gcc-major-version-only --with-linker-hash-style=gnu --enable-plugin --enable-initfini-array --with-isl --enable-offload-targets=nvptx-none --without-cuda-driver --enable-gnu-indirect-function --enable-cet --with-tune=generic --with-arch_32=i686 --build=x86_64-redhat-linux\r\nThread model: posix\r\nSupported LTO compression algorithms: zlib zstd\r\ngcc version 10.1.1 20200618 (Red Hat 10.1.1-2) (GCC) \r\n```\r\n```\r\n$ python --version\r\nPython 3.9.0b3+\r\n```\r\n", "comments": ["IIUC, the const-ness of PyUFuncGenericFunction changed (see https://github.com/numpy/numpy/pull/15251). C-style casting the function pointer would work, but I think it would be cleaner to overload the function. I will give that a try.", "This should be fixed now. Thanks a lot for the PR, it helped me find the culprit a lot quicker. I hope you are fine that I solved it slightly differently now.", "> This should be fixed now. Thanks a lot for the PR, it helped me find the culprit a lot quicker. I hope you are fine that I solved it slightly differently now.\r\n\r\nThis is not on 2.3.1", "I'm getting the same error on Catalina.\r\n\r\ntensorflow/python/lib/core/bfloat16.cc:667:8: error: no matching function for call to object of type '(lambda at tensorflow/python/lib/core/bfloat16.cc:637:25)'\r\n  if (!register_ufunc(\"not_equal\", CompareUFunc<Bfloat16NeFunctor>,\r\n       ^~~~~~~~~~~~~~\r\ntensorflow/python/lib/core/bfloat16.cc:637:25: note: candidate function not viable: no overload of 'CompareUFunc' matching 'PyUFuncGenericFunction' (aka 'void (*)(char **, const long *, const long *, void *)') for 2nd argument\r\n  auto register_ufunc = [&](const char* name, PyUFuncGenericFunction fn,\r\n\r\n```\r\ngcc -v\r\nUsing built-in specs.\r\nCOLLECT_GCC=gcc\r\nCOLLECT_LTO_WRAPPER=/opt/local/libexec/gcc/x86_64-apple-darwin19/10.2.0/lto-wrapper\r\nTarget: x86_64-apple-darwin19\r\nConfigured with: /opt/local/var/macports/build/_opt_bblocal_var_buildworker_ports_build_ports_lang_gcc10/gcc10/work/gcc-10.2.0/configure --prefix=/opt/local --build=x86_64-apple-darwin19 --enable-languages=c,c++,objc,obj-c++,lto,fortran,jit --libdir=/opt/local/lib/gcc10 --includedir=/opt/local/include/gcc10 --infodir=/opt/local/share/info --mandir=/opt/local/share/man --datarootdir=/opt/local/share/gcc-10 --with-local-prefix=/opt/local --with-system-zlib --disable-nls --program-suffix=-mp-10 --with-gxx-include-dir=/opt/local/include/gcc10/c++/ --with-gmp=/opt/local --with-mpfr=/opt/local --with-mpc=/opt/local --with-isl=/opt/local --enable-stage1-checking --disable-multilib --enable-lto --enable-libstdcxx-time --with-build-config=bootstrap-debug --with-as=/opt/local/bin/as --with-ld=/opt/local/bin/ld --with-ar=/opt/local/bin/ar --with-bugurl=https://trac.macports.org/newticket --enable-host-shared --disable-tls --with-pkgversion='MacPorts gcc10 10.2.0_5' --with-sysroot=/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk\r\nThread model: posix\r\nSupported LTO compression algorithms: zlib\r\ngcc version 10.2.0 (MacPorts gcc10 10.2.0_5) \r\n```\r\nIs there a fix?"]}, {"number": 40653, "title": "Incorrect error message for valid input of tf.math.segment_*", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary):  binary\r\n- TensorFlow version (use command below):  v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version: 3.7.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nWhen passing a 0D tensor for `data` and an empty array for `segment_ids`, `tf.math.segment_*` (e.g., `tf.math.segment_mean`) throws an error saying  \r\n- In command line: `InvalidArgumentError: segment_ids should be the same size as dimension 0 of input.`  \r\n- In Colab: crash due to core dumped at 'F tensorflow/core/framework/tensor_shape.cc:435] Check failed: d < dims() (0 vs. 0)'\r\n\r\nFor a 0D tensor `data`,  the first dimension of `segment_ids` does not exist, so shouldn't an empty array be the valid input? \r\nAlso, the same input behaves differently in Colab and command line. This is also strange behavior.\r\n\r\n**Describe the expected behavior**\r\nIf the input above is valid, I would expect no error thrown. If invalid, I would expect a more straightforward error message and an update in the documentation so that it specifies `data` tensor should not be scalar.  \r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ntf.math.segment_mean(np.uint16(10), np.array([]).astype('int64'), name=None)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Was able to reproduce the issue with TF v2.2 and TF-nightly. Running the code on Colab crashes the session, whereas running the code on terminal throws an error stating `Check failed: d < dims() (0 vs. 0)`.\r\n\r\nPlease find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/7eed4878232eb4ffaf58e5f86257987e/40653.ipynb). Thanks!", "Added a PR #40693 to improve the error message.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40653\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40653\">No</a>\n"]}, {"number": 40652, "title": "Gradient with respect to input returns None using GradientTape().", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- TensorFlow version (use command below): 2.2.0\r\n\r\n**Describe the current behavior**\r\nI am trying to calculate gradient for a KL divergence scalar loss value wrt input. \r\nImplementation has been with two approaches and returns [None] in both cases.\r\n\r\n1. Differentiate wrt to a tensor. Output: None\r\n2. Differentiate wrt to a Variable. Output: None\r\n\r\nThe following works with TensorFlow 1.x which used tf.gradients() however, GradientTape() is returning None and it very frustrating now. As I have been trying to overcome this for the last 3 weeks. Please provide some support. \r\n\r\nThe colab link is provided below.\r\n\r\n**Describe the expected behavior**\r\nGradient matrix should contain values and until that I can't work on building my model.\r\n\r\n**Standalone code to reproduce the issue**\r\nUpdated link: \r\n[https://colab.research.google.com/drive/1hwFX3VMzKtfRjqd5-EG-0WePvtCbmChM?usp=sharing](url)\r\n**Other info/logs** \r\n\r\nThe below code is taken from this source: \r\n[https://gist.github.com/divamgupta/c778c17459c1f162e789560d5e0b2f0b]\r\n\r\nAlso followed a similar issue on this link: [https://github.com/tensorflow/tensorflow/issues/36596](url) . But it doesn't help me to solve my problem.\r\n\r\nAny help is highly appreciated. Thanks for your time", "comments": ["@lokesharma-dev \r\nThe colab gist shared is empty, can you please check and share again or paste the code here.", "@Saduf2019 \r\nI have and the link is working. Please confirm the same.\r\nhttps://colab.research.google.com/drive/1hwFX3VMzKtfRjqd5-EG-0WePvtCbmChM?usp=sharing", "I am able to replicate the issue faced, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/35a6dcdb1ef38079e49606bf44bb3c74/untitled238.ipynb)", "isn't this is a coding error instead of a bug?  You are asking for gradient of `kl` with respect to `r`, but `kl` only depends on `p_logit` and `p_logit_r` inside gradienttape. `p_logit_r` need to interact with `r` after the tape watches `r`, cannot interact prior to the watch.", "That was my first attempt, however, it leads to a numpy Attribute error and I have cross-checked elsewhere. All variables are Tensor objects. Error attached for the below modification. Thanks.\r\n<img width=\"1156\" alt=\"Screenshot 2020-06-22 at 16 12 16\" src=\"https://user-images.githubusercontent.com/63437596/85297700-624b6a80-b4a3-11ea-9e29-b8d54f3fe121.png\">\r\n\r\n`model_input = Input((2,))\r\np_logit = model(model_input)\r\np = Activation('softmax')(model_input)\r\n\r\nr = tf.random.uniform(shape=tf.shape(model_input))\r\nr = make_unit_norm(r)\r\n#p_logit_r = model(model_input + 10*r)\r\nprint('Shape of the noise tensor: ', r)\r\n\r\nwith tf.GradientTape(watch_accessed_variables=False) as tape:\r\n  tape.watch(r)\r\n  p_logit_r = model(model_input + 10*r)\r\n  kl = tf.reduce_mean(compute_kld(p_logit, p_logit_r))\r\n\r\ngrad_kl = tape.gradient(kl, r)\r\n\r\nprint(\"Shape of Gradient Matrix : \", grad_kl)`\r\n\r\nUpdated link: [https://colab.research.google.com/drive/1hwFX3VMzKtfRjqd5-EG-0WePvtCbmChM?usp=sharing#scrollTo=GJna81gE2OrE](url)\r\n\r\n", "that is because `r` is not an eager tensor because of how you created it, shape is not well defined. You cannot use the shape of a symbolic tensor as the shape to create an eager tensor. Moreover, why `model_input + 10*r`, since `model_input` is keras symbolic tensor waiting for input but why you put the symbolic input tensor to the input of the model? That does not make sense.\r\n\r\nI think you are confusing Tensorflow 1 API with Tensorflow 2 which is eager executed by default.", "whatever you are trying to do, slightly modifying your code and this one will run without error.\r\n\r\n```python3\r\n# Import Libraries\r\nfrom numpy.random import seed\r\nseed(0)\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport tensorflow as tf\r\ntf.random.set_seed(0)\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense, Input, Activation\r\nfrom sklearn.metrics import accuracy_score\r\nfrom sklearn import datasets\r\n\r\n\r\n# Generate dataset\r\ncircles = datasets.make_circles(n_samples=1000, noise=.05, factor=.3, random_state=3)\r\ncircles_test = datasets.make_circles(n_samples=1000, noise=0, factor=.3, random_state=1)\r\n\r\nn_points = 8\r\ninds = list(np.where(circles[1] == 0)[0][:n_points]) + list(np.where(circles[1] == 1)[0][:n_points])\r\n\r\nx_train = circles[0][inds]\r\ny_train = circles[1][inds]\r\ny_train_cat = tf.keras.utils.to_categorical(circles[1][inds])\r\n\r\nx_test = circles_test[0][inds]\r\ny_test = circles_test[1][inds]\r\ny_test_cat = tf.keras.utils.to_categorical(circles_test[1][inds])\r\n\r\nn_classes = int(np.max(y_train)+1)\r\n\r\n# Plot data points\r\nplt.scatter(x_test[:,0], x_test[:,1], c=y_test, s=20, cmap='winter', edgecolors='none', alpha=0.005)\r\nplt.scatter(x_train[:,0], x_train[:,1], c=y_train, s=20, cmap='winter', edgecolors='k')\r\n\r\n# VAT Model\r\nmodel = Sequential()\r\nmodel.add(Dense(100, activation='relu', input_shape=(2,)))\r\nmodel.add(Dense(2, activation='softmax'))\r\nmodel.compile('sgd', 'categorical_crossentropy', metrics=['accuracy'])\r\n\r\ndef compute_kld(p_logit, q_logit):\r\n  p = tf.nn.softmax(p_logit)\r\n  q = tf.nn.softmax(q_logit)\r\n  kl_score = tf.reduce_sum( p * (tf.math.log(p+1e-16) - tf.math.log(q+1e-16)), axis = 1)\r\n  print(type(kl_score))\r\n  return kl_score # lower kl means closer the distributions are\r\n\r\ndef make_unit_norm(x):\r\n    return x/(tf.reshape(tf.sqrt(tf.reduce_sum(tf.pow(x, 2.0), axis=1)), [-1, 1]) + 1e-16)\r\n\r\n\r\nr = tf.random.uniform(shape=(64, 2))\r\nr = make_unit_norm(r)\r\n\r\nwith tf.GradientTape(watch_accessed_variables=False) as tape:\r\n  tape.watch(r)\r\n  p_logit_r = model(10*r)\r\n  kl = tf.reduce_mean(compute_kld(r, p_logit_r))\r\n\r\ngrad_kl = tape.gradient(kl, r)\r\n\r\nprint(grad_kl)\r\n```", "@henrysky Thanks for your suggestion. It helped me to understand how to differentiate with respect to the input.  However, the correct solution to this issue would be the following. \r\n`r = tf.random.uniform(shape=tf.shape(v), minval=0.001, maxval=0.005)\r\nr = tf.add(v,r)\r\n\r\nwith tf.GradientTape() as tape:\r\n  tape.watch(r)\r\n  h1 = layers.LSTM(units=128, name='LSTM_layer_r')(r)\r\n  p_logit_r = layers.Dense(units=10, name='Dense_layer_r')(h1)\r\n  kl = compute_kld(p_logit, p_logit_r)\r\ngrad = tape.gradient(kl, r)\r\n\r\nprint(grad)`\r\n\r\nJust in case someone comes across a similar issue. This will help them. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40652\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40652\">No</a>\n"]}, {"number": 40651, "title": "Keras Model produces nan on fit", "body": "**System information**\r\n- Google Colab Python 3\r\n\r\n\r\n**Bug Description**\r\nI have a confusing problem. Before I call `.fit()` my model can produce output from inputs (see below).\r\n\r\n```python\r\n\r\n>>> print(f0.predict(x))\r\n>>> print(f1.predict([x, x]))\r\n\r\n[[16.913166 ]\r\n [ 6.2363415]\r\n [10.625664 ]\r\n ...\r\n [13.86934  ]\r\n [15.4675865]\r\n [16.353233 ]]\r\n[[-0.94484174]\r\n [-1.0965291 ]\r\n [-0.829823  ]\r\n ...\r\n [-1.0362396 ]\r\n [-0.18886864]\r\n [-0.3576672 ]]\r\n```\r\n\r\nBut after calling fit all I get are nans\r\n```python\r\n>>> print(f0.predict(x))\r\n>>> print(f1.predict([x, x]))\r\n\r\n[[16.913166 ]\r\n [ 6.2363415]\r\n [10.625664 ]\r\n ...\r\n [13.86934  ]\r\n [15.4675865]\r\n [16.353233 ]]\r\n[[nan]\r\n [nan]\r\n [nan]\r\n ...\r\n [nan]\r\n [nan]\r\n [nan]]\r\n```\r\n\r\nHere is the code for generating the model. Error is _not_ dependent on the data as I can reproduce this with any sklearn dataset as an input.\r\n\r\n```python\r\ndef create_base_model(p, units=20):\r\n    # Specs\r\n    input_layer = k.layers.Input((p,))\r\n\r\n    penultimate_layer = k.layers.Dense(units, activation='elu')(input_layer)\r\n    penultimate_layer = k.layers.Dense(p)(penultimate_layer)\r\n\r\n    training_output = k.layers.Dense(1)(penultimate_layer)\r\n\r\n    # Models\r\n    boost_model = k.Model(inputs=input_layer, outputs=penultimate_layer)\r\n    training_model = k.Model(inputs=input_layer, outputs=training_output)\r\n\r\n    # Compile and export\r\n    training_model.compile(optimizer='sgd', loss='mse')\r\n    return training_model, boost_model\r\n\r\n\r\ndef create_staged_model(p: int, model: k.Model, units=20):\r\n    # Freeze prior model\r\n    for layer_i in model.layers:\r\n        layer_i.trainable = False\r\n\r\n    # Specs\r\n    input_layer = k.layers.Input((p,))\r\n\r\n    penultimate_layer = k.layers.concatenate([model.output, input_layer], axis=-1)\r\n    penultimate_layer = k.layers.Dense(units, activation='elu')(penultimate_layer)\r\n    penultimate_layer = k.layers.Dense(p)(penultimate_layer)\r\n\r\n    training_output = k.layers.Dense(1)(penultimate_layer)\r\n\r\n    # Models\r\n    boost_model = k.Model(inputs=[model.input, input_layer], outputs=penultimate_layer)\r\n    training_model = k.Model(inputs=[model.input, input_layer], outputs=training_output)\r\n\r\n    # Compile and export\r\n    training_model.compile(optimizer='sgd', loss='mse')\r\n    return training_model, boost_model\r\n\r\n\r\nfit_kwargs = dict(\r\n    epochs=50, \r\n    validation_split=0.1, \r\n    callbacks=[\r\n        k.callbacks.ReduceLROnPlateau(factor=.5, patience=5, min_lr=1e-6),\r\n        k.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\r\n    ]\r\n)\r\n\r\n\r\nf0, _ = create_base_model(p)\r\nf0.fit(x, y, **fit_kwargs)  # Works fine\r\n\r\nf1, _ = create_staged_model(p, model0)\r\nf1.fit([x, x], y, **fit_kwargs)  # Breaks on fit\r\n```\r\n\r\nNote that this nan issue occurs immediately\r\n```\r\nEpoch 1/50\r\n282/282 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan - lr: 0.0100\r\nEpoch 2/50\r\n282/282 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan - lr: 0.0100\r\nEpoch 3/50\r\n282/282 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan - lr: 0.0100\r\nEpoch 4/50\r\n282/282 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan - lr: 0.0100\r\nEpoch 5/50\r\n282/282 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan - lr: 0.0100\r\nEpoch 6/50\r\n282/282 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan - lr: 0.0050\r\nEpoch 7/50\r\n282/282 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan - lr: 0.0050\r\nEpoch 8/50\r\n282/282 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan - lr: 0.0050\r\nEpoch 9/50\r\n282/282 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan - lr: 0.0050\r\n```\r\n", "comments": ["@arose13 \r\n\r\nI have tried in colab with TF version 2.2 and i am seeing the below error messaage.(N`ameError: name 'p' is not defined`).Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/c5b3678e7d8743f2301dca44e74d283b/untitled56.ipynb).Please, help me with reproducible code, so it helps us in localzing the issue faster.Thanks!", "I have no idea if my changed affected your gist so I'm sharing my URL here of your colab. [https://colab.research.google.com/gist/arose13/a580640c042278b82d6d7dc424070592/untitled56.ipynb](https://colab.research.google.com/gist/arose13/a580640c042278b82d6d7dc424070592/untitled56.ipynb )\r\n\r\nI added summary prints so you can a little more about what is going on. I'm also having a different error than in your Colab but the observed behaviour is still the same (and strange).", "@arose13 \r\n\r\nUse tensorflow.keras rather than keras will fix the problem. ", "> @arose13\r\n> \r\n> Use tensorflow.keras rather than keras will fix the problem.\r\n\r\n@yixingfu No that's not true. That does, however, explain my ravikyram code gave a different error from me.\r\n\r\nWith that fix in his code you can see the error here.\r\nhttps://colab.research.google.com/gist/arose13/a580640c042278b82d6d7dc424070592/untitled56.ipynb#scrollTo=_xMSSo8p8KtE", "@arose13  You are right (but I think I did see that worked before I replied above. Not sure if I modified anything else at that time.)\r\n\r\nIf f0 is not trained, training f1 does not have problem.  Also if I change optimizer from 'sgd' to 'adam' it does not have nan.  Maybe it is a sgd problem caused by the training result of f0?", "Incredibly clever fix @yixingfu !\r\n\r\nI am so incredibly confused, do you have any intuition why this works?\r\n\r\nf0 appears normal and I can `.predict()` etc, so why would something as arbitrary as the 'sgd' -> 'adam' work (it has worked). \r\n\r\n**Note 1**: Only changed the second model `f1` to 'adam' fixes it. Changing only `f0` does not. This continues to make me believe that somehow the problem is with how `f1` is created (created by `create_staged_model()`).\r\n\r\n**Note 2**: The reason why it is important is that I must train the staged models (eg `f1` ) with stochastic gradient descent.  ", "I did not face any issues while running your code in Tf Nightly 2.6, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/b412e966a3fb4b6c2dbd202c36563713/40651.ipynb) and confirm us if the issue is fixed. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40651\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40651\">No</a>\n"]}, {"number": 40650, "title": "Additional tests for _jvp_dispatch", "body": "Batching for regular ops executing eagerly. This is will later be changed to test implementation finalized in #40678 ", "comments": ["This test is taken care of in #40678. Additional tests can go here. Will reopen when necessary."]}, {"number": 40649, "title": "AttributeError: 'StringIntLabelMapItem' object has no attribute 'keypoints'", "body": "I'm following the tutorial (https://www.youtube.com/watch?v=HjiBbChYRDw) and got into a problem of getting the category index from the labelmap.\r\n\r\nPATH_TO_LABELS = 'training/labelmap.pbtxt'\r\n![keypoints](https://user-images.githubusercontent.com/60057917/85229903-28d11b80-b41f-11ea-8579-82c6da4bbf7a.JPG)\r\n", "comments": ["@chuasonglin,\r\nIn order to reproduce the issue reported here, could you please provide the complete code, the TensorFlow version and the dataset you are using. Thanks!", "I just managed to resolve it. Thank you\r\n\r\nBasically, change two things:\r\nfrom utils import label_map_util \r\n----> from object_detection.utils import label_map_util \r\nfrom utils import visualization_utils as vis_util \r\n----> from object_detection.utils import visualization_utils as vis_util", "> I just managed to resolve it. Thank you\r\n\r\nThank you for the update @chuasonglin. Marking this issue as closed, as it is resolved. Thanks!"]}, {"number": 40648, "title": "Inconsistant tf.name_scope behaviour with Custom model in TF2.X.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Google Colab Environment**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**\r\n- TensorFlow installed from (source or binary): **Binary (Colab Pre-Installed)**\r\n- TensorFlow version (use command below): **2.3.0-nightly**\r\n- Python version: **3.6.9**\r\n- Bazel version (if compiling from source): **N/A**\r\n- GCC/Compiler version (if compiling from source): **N/A**\r\n- CUDA/cuDNN version: **N/A (Colab CPU Environment used)**\r\n- GPU model and memory: **N/A**\r\n\r\n**Describe the current behavior**\r\nI am trying to use tf.name_scope inside a custom model which is inherited from tf.keras.Model and it has custom layers which are inherited from tf.keras.layers.Layer.\r\n\r\n**Describe the expected behavior**\r\nI expect the scope_name that is added with the help of tf.name_scope as a prefix to the weights name. E.g. weight \"conv1/conv2d_35/kernel:0\" should be named as \"Conv_Block/conv1/conv2d_35/kernel:0\" when used with tf.name_scope(\"Conv_Block).\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nclass Config:\r\n    weight_decay = 0.0005\r\n    weight_init_seed = 42\r\n    bnorm_momentum = 0.9\r\n\r\nconfig = Config()\r\n\r\nclass ConvLayer(tf.keras.layers.Layer):\r\n    def __init__(self, name, filters, k_size=(3, 3), strides=1, padding='same', use_bnorm=True):\r\n        super(ConvLayer, self).__init__(name=name, trainable=True, dtype=tf.float32)        \r\n\r\n        self.use_bnorm = use_bnorm\r\n\r\n        self.weight_decay = config.weight_decay\r\n        self.k_reg = tf.keras.regularizers.l2(self.weight_decay)\r\n        self.k_init = tf.keras.initializers.GlorotNormal(seed=config.weight_init_seed)\r\n        self.b_init = tf.zeros_initializer()\r\n\r\n        self.conv = tf.keras.layers.Conv2D(filters=filters, kernel_size=k_size, strides=strides, padding=padding, activation=None, use_bias=not self.use_bnorm, kernel_initializer=self.k_init, bias_initializer=self.b_init, kernel_regularizer=self.k_reg)\r\n        if self.use_bnorm:\r\n            self.bn = tf.keras.layers.BatchNormalization(momentum=config.bnorm_momentum)\r\n        self.mx_pool = tf.keras.layers.MaxPool2D(pool_size=(3, 3), strides=2, padding=padding)\r\n\r\n        self.act = tf.nn.relu\r\n\r\n    def call(self, x, training=False):\r\n        if self.use_bnorm:\r\n            return self.act(self.mx_pool(self.bn(self.conv(x), training=training)))\r\n        else:\r\n            return self.act(self.mx_pool(self.conv(x)))\r\n\r\n\r\nclass DenseLayer(tf.keras.layers.Layer):\r\n    def __init__(self, name, units, use_bnorm=True, use_act=True):\r\n        super(DenseLayer, self).__init__(name=name)\r\n        self.use_bnorm = use_bnorm\r\n        self.use_act = use_act\r\n        \r\n        self.weight_decay = config.weight_decay\r\n        self.k_reg = tf.keras.regularizers.l2(self.weight_decay)\r\n        self.k_init = tf.keras.initializers.GlorotNormal(seed=config.weight_init_seed)\r\n        self.b_init = tf.zeros_initializer()\r\n\r\n        self.fc = tf.keras.layers.Dense(units=units, activation=None, use_bias=not self.use_bnorm, kernel_initializer=self.k_init, bias_initializer=self.b_init, kernel_regularizer=self.k_reg)\r\n        if self.use_bnorm:\r\n            self.bn = tf.keras.layers.BatchNormalization(momentum=config.bnorm_momentum)\r\n        \r\n        if self.use_act:\r\n            self.act = tf.nn.relu\r\n\r\n    def call(self, x, training=False):\r\n        if self.use_bnorm:\r\n            op = self.bn(self.fc(x), training=training)\r\n        else:\r\n            op = self.fc(x)\r\n\r\n        if self.use_act:\r\n            return self.act(op)\r\n        else:\r\n            return op\r\n\r\n\r\nclass ClassifierModel(tf.keras.Model):\r\n    def __init__(self, num_classes=2):\r\n        super(ClassifierModel, self).__init__(name='ClassifierModel')\r\n        self.num_classes = num_classes\r\n        \r\n        # input will be of shape : [None, 128, 128, 3]\r\n\r\n        with tf.name_scope(\"Conv_Block\"):\r\n            self.conv1 = ConvLayer('conv1', 32, (3, 3), 1, 'same', True)\r\n            # [64 x 64 x 32]\r\n            \r\n            self.conv2 = ConvLayer('conv2', 48, (3, 3), 1, 'same', True)\r\n            # [32 x 32 x 48]\r\n            \r\n            self.conv3 = ConvLayer('conv3', 72, (3, 3), 1, 'same', True)\r\n            # [16 x 16 x 72]\r\n\r\n            self.conv4 = ConvLayer('conv4', 96, (3, 3), 1, 'same', True)\r\n            # [8 x 8 x 96]\r\n\r\n            self.conv5 = ConvLayer('conv5', 128, (3, 3), 1, 'same', True)\r\n            # [4 x 4 x 128]\r\n\r\n            self.gap = tf.keras.layers.GlobalAveragePooling2D()\r\n            self.mx_pool = tf.keras.layers.MaxPool2D(pool_size=(3, 3), strides=2, padding='same')\r\n\r\n        with tf.name_scope(\"Dense_Block\"):\r\n            self.fc1 = DenseLayer('fc1', 64, True)\r\n            # [64]\r\n\r\n            self.fc2 = DenseLayer('fc2', 2, False, False)\r\n            # [2]\r\n    \r\n    def call(self, x, training=False):\r\n        # x         : [B, 128, 128, 3]\r\n        # training  : True / False - used for batch norm.\r\n        \r\n        layer_1 = self.conv1(x, training=training)\r\n        layer_1 = self.mx_pool(layer_1)\r\n\r\n        layer_2 = self.conv2(layer_1, training=training)\r\n        layer_2 = self.mx_pool(layer_2)\r\n\r\n        layer_3 = self.conv3(layer_2, training=training)\r\n        layer_3 = self.mx_pool(layer_3)\r\n\r\n        layer_4 = self.conv4(layer_3, training=training)\r\n        layer_4 = self.mx_pool(layer_4)\r\n\r\n        layer_5 = self.conv5(layer_4, training=training)\r\n        layer_5 = self.mx_pool(layer_5)\r\n\r\n        gap = self.gap(layer_5)\r\n\r\n        fc1 = self.fc1(gap, training=training)\r\n\r\n        logits = self.fc2(fc1, training=None)\r\n        outputs = tf.nn.softmax(logits)\r\n\r\n        return logits, outputs\r\n\r\n\r\nclf = ClassifierModel()\r\nclf.build(input_shape=tf.TensorShape([None, 128, 128, 3]))\r\n\r\nfor v in clf.trainable_variables:\r\n    print(v.name, \"\\t\\t\", v.shape)\r\n```\r\n\r\nGenerated output is as below:\r\n```\r\nconv1/conv2d_35/kernel:0 \t\t (3, 3, 3, 32)\r\nconv1/batch_normalization_41/gamma:0 \t\t (32,)\r\nconv1/batch_normalization_41/beta:0 \t\t (32,)\r\nconv2/conv2d_36/kernel:0 \t\t (3, 3, 32, 48)\r\nconv2/batch_normalization_42/gamma:0 \t\t (48,)\r\nconv2/batch_normalization_42/beta:0 \t\t (48,)\r\nconv3/conv2d_37/kernel:0 \t\t (3, 3, 48, 72)\r\nconv3/batch_normalization_43/gamma:0 \t\t (72,)\r\nconv3/batch_normalization_43/beta:0 \t\t (72,)\r\nconv4/conv2d_38/kernel:0 \t\t (3, 3, 72, 96)\r\nconv4/batch_normalization_44/gamma:0 \t\t (96,)\r\nconv4/batch_normalization_44/beta:0 \t\t (96,)\r\nconv5/conv2d_39/kernel:0 \t\t (3, 3, 96, 128)\r\nconv5/batch_normalization_45/gamma:0 \t\t (128,)\r\nconv5/batch_normalization_45/beta:0 \t\t (128,)\r\nfc1/dense_12/kernel:0 \t\t (128, 64)\r\nfc1/batch_normalization_46/gamma:0 \t\t (64,)\r\nfc1/batch_normalization_46/beta:0 \t\t (64,)\r\nfc2/dense_13/kernel:0 \t\t (64, 2)\r\nfc2/dense_13/bias:0 \t\t (2,)\r\n```\r\nOutput should be as below:\r\n```\r\nConv_Block/conv1/conv2d_35/kernel:0 \t\t (3, 3, 3, 32)\r\nConv_Block/conv1/batch_normalization_41/gamma:0 \t\t (32,)\r\nConv_Block/conv1/batch_normalization_41/beta:0 \t\t (32,)\r\nConv_Block/conv2/conv2d_36/kernel:0 \t\t (3, 3, 32, 48)\r\nConv_Block/conv2/batch_normalization_42/gamma:0 \t\t (48,)\r\nConv_Block/conv2/batch_normalization_42/beta:0 \t\t (48,)\r\nConv_Block/conv3/conv2d_37/kernel:0 \t\t (3, 3, 48, 72)\r\nConv_Block/conv3/batch_normalization_43/gamma:0 \t\t (72,)\r\nConv_Block/conv3/batch_normalization_43/beta:0 \t\t (72,)\r\nConv_Block/conv4/conv2d_38/kernel:0 \t\t (3, 3, 72, 96)\r\nConv_Block/conv4/batch_normalization_44/gamma:0 \t\t (96,)\r\nConv_Block/conv4/batch_normalization_44/beta:0 \t\t (96,)\r\nConv_Block/conv5/conv2d_39/kernel:0 \t\t (3, 3, 96, 128)\r\nConv_Block/conv5/batch_normalization_45/gamma:0 \t\t (128,)\r\nConv_Block/conv5/batch_normalization_45/beta:0 \t\t (128,)\r\nDense_Block/fc1/dense_12/kernel:0 \t\t (128, 64)\r\nDense_Block/fc1/batch_normalization_46/gamma:0 \t\t (64,)\r\nDense_Block/fc1/batch_normalization_46/beta:0 \t\t (64,)\r\nDense_Block/fc2/dense_13/kernel:0 \t\t (64, 2)\r\nDense_Block/fc2/dense_13/bias:0 \t\t (2,)\r\n```\r\n\r\n**Colab Notebook link:**  https://colab.research.google.com/drive/1MxMsIYVy7vGiY6jctw5OVkHUPGabHu9Z?usp=sharing\r\n\r\n**Other info / logs** \r\nThe same issue is reproduced in my local machine where Python 3.6.10 is installed with TensorFlow 2.1.0.", "comments": ["Same issue", "I am able to replicate this issue, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/367b460e6f592bd3a677cb312ca6d0cf/untitled233.ipynb)", "This is intended behavior. Please refer https://github.com/tensorflow/tensorflow/issues/28558 to know more.\r\nYou may [explicitly name layers](https://github.com/tensorflow/tensorflow/issues/28558#issuecomment-503598352) by passing name arg.\r\n```python\r\n tf.keras.layers.Dense(name='your own layer name')\r\n```\r\n", "> This is intended behavior. Please refer #28558 to know more.\r\n> You may [explicitly name layers](https://github.com/tensorflow/tensorflow/issues/28558#issuecomment-503598352) by passing name arg.\r\n> \r\n> ```python\r\n>  tf.keras.layers.Dense(name='your own layer name')\r\n> ```\r\n\r\nThanks for the reply. Actually I am looking for grouping similar layers in model definition. Like in ResNet we can group a single residual block and name it 'ResBlock_1', 'ResBlock_2' and so on. It would be very easy to use name_scope and prefix all the layers inside it with a single name rather than passing that name into all the sub-layers and sub-layers' of those sub-layers' to change their name.", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\nThanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40648\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40648\">No</a>\n", "> This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n> Thanks!\r\n\r\nBut it is not as per the expected behavior. I am not asking for possible work around. I am expecting tf.name_scope to add a prefix for all the children variables' name. That's what it was doing in TF1.X. How come this behavior is changed?"]}, {"number": 40647, "title": "Describe Conv1D kernel_size as height, not length?", "body": "For a 1D convolution, is the use of the word, \"length,\" somewhat misleading? Wouldn't height be more appropriate?\r\n\r\n> https://github.com/tensorflow/tensorflow/blob/54b51b10585e2e05124c708f8c5b2d41d3a728bb/tensorflow/python/keras/layers/convolutional.py#L425\r\n\r\n![image](https://user-images.githubusercontent.com/16389820/85224546-2f1cb480-b399-11ea-95fe-5e6d6ba6fad8.png)\r\n", "comments": ["@HashRocketSyntax \r\n\r\nRequest to fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.\r\n\r\nRequest you to share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "You actually posted an example why length makes much more sense. Here the length refers to the length of the subsequence the kernel is applied on. So in your example it describes the number of \"words\" in each subsequence each kernel is applied to -> length.\r\n\r\nHeight when referring to sequences does not make much sense. Hope this explanation helps.", "Ah, gotcha. `length*width*height`\r\nUsed on its own, the word length is pretty ambiguous. \r\n\r\nlength = number of rows?", "@HashRocketSyntax Yes Here the length is equal to the number of words the filter is applied upon(in the above case the filter is applied on 2 words at a time. So, the length is 2*1) . For more information, please post this question on stack overflow as its not related to bug/performance, build/install or feature request. "]}, {"number": 40646, "title": "Failed to get convolution algorithm", "body": "\r\n\r\n**System information**\r\n\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- TensorFlow installed from: Conda  using ``` condo install tensorflow-gpu```\r\n- TensorFlow version: tensorflow-gpu 2.2.0\r\n- IDE: PyCharm\r\n- Python version: 3.7.7\r\n- CUDA/cuDNN version: 10.1/ 7.6.5\r\n- GPU model and memory: Nvidia GeForce 940 MX. Compute Capability 5.0\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nI get the following error when I run  ```model.fit()``` in PyCharm:\r\n\r\n  ...: history = model.fit(\r\n  ...:     train_data_gen,\r\n  ...:     epochs=epochs,\r\n  ...:     validation_data=val_data_gen,)\r\nEpoch 1/80\r\n2020-06-21 14:35:26.768642: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-06-21 14:35:26.940647: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-06-21 14:35:28.215703: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2020-06-21 14:35:28.224418: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\nTraceback (most recent call last):\r\n  File \"/home/adisha512/miniconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-8-0cb462f62c0c>\", line 4, in <module>\r\n    validation_data=val_data_gen,)\r\n  File \"/home/adisha512/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 66, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n  File \"/home/adisha512/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 848, in fit\r\n    tmp_logs = train_function(iterator)\r\n  File \"/home/adisha512/miniconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 580, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/adisha512/miniconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 644, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/home/adisha512/miniconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2420, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"/home/adisha512/miniconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1665, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"/home/adisha512/miniconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1746, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/home/adisha512/miniconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 598, in call\r\n    ctx=ctx)\r\n  File \"/home/adisha512/miniconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.UnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[node sequential/conv2d/Conv2D (defined at <ipython-input-8-0cb462f62c0c>:4) ]] [Op:__inference_train_function_1350]\r\nFunction call stack:\r\ntrain_function\r\n\r\nI tried going through some articles but couldn't get the solution.\r\n\r\nAlso, When I create the model, then this is what I get in the console:\r\n\r\n2020-06-21 14:29:33.299080: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-06-21 14:29:33.336130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-21 14:29:33.337117: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce 940MX computeCapability: 5.0\r\ncoreClock: 1.189GHz coreCount: 3 deviceMemorySize: 1.96GiB deviceMemoryBandwidth: 37.33GiB/s\r\n2020-06-21 14:29:33.354558: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-06-21 14:29:33.384143: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-06-21 14:29:33.398202: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-06-21 14:29:33.405597: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-06-21 14:29:33.411914: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-06-21 14:29:33.416370: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-06-21 14:29:33.428501: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-06-21 14:29:33.428926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-21 14:29:33.430408: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-21 14:29:33.431523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2020-06-21 14:29:33.432550: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2020-06-21 14:29:33.446572: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2699905000 Hz\r\n2020-06-21 14:29:33.446977: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f691c002e50 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-06-21 14:29:33.447022: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-06-21 14:29:33.447375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-21 14:29:33.448181: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce 940MX computeCapability: 5.0\r\ncoreClock: 1.189GHz coreCount: 3 deviceMemorySize: 1.96GiB deviceMemoryBandwidth: 37.33GiB/s\r\n2020-06-21 14:29:33.448268: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-06-21 14:29:33.448301: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-06-21 14:29:33.448329: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-06-21 14:29:33.448356: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-06-21 14:29:33.448383: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-06-21 14:29:33.448410: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-06-21 14:29:33.448438: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-06-21 14:29:33.448603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-21 14:29:33.449537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-21 14:29:33.450258: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2020-06-21 14:29:33.450395: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-06-21 14:29:33.497965: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-06-21 14:29:33.497998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \r\n2020-06-21 14:29:33.498009: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \r\n2020-06-21 14:29:33.498209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-21 14:29:33.498590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-21 14:29:33.498928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-21 14:29:33.499229: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1538 MB memory) -> physical GPU (device: 0, name: GeForce 940MX, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\n2020-06-21 14:29:33.501220: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x555cf5679cb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-06-21 14:29:33.501249: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce 940MX, Compute Capability 5.0\r\n\r\n\r\n", "comments": ["Try adding this line to the top of your script after your imports:\r\n```python\r\nfrom tensorflow.compat.v1 import ConfigProto, InteractiveSession\r\n\r\nconfig = ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.6\r\nsess = InteractiveSession(config=config)\r\n```\r\n\r\nThe error you mentioned is practically always to do with GPU initialization/configuration options when using CUDA.", "Yeah, that worked. Thank you!!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40646\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40646\">No</a>\n", "if @iobtl solution outputs some errors try\r\n`physical_devices = tf.config.list_physical_devices('GPU')`\r\n`tf.config.experimental.set_memory_growth(physical_devices[0], True)`\r\nworked for me"]}, {"number": 40645, "title": "i can not run examples with downloadPredictFloat16ModelFile", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nhttps://github.com/tensorflow/examples/tree/master/lite/examples/style_transfer/android\r\n\r\n## Description of issue (what needs changing):\r\ni follow the description\r\n\r\nthen run this project\r\n\r\nlog\r\n```\r\n> Task :app:downloadPredictFloat16ModelFile\r\nUnable to get progress logger. Download progress will not be displayed.\r\n\r\n> Task :app:downloadPredictFloat16ModelFile FAILED\r\n\r\nFAILURE: Build failed with an exception.\r\n\r\n* What went wrong:\r\nExecution failed for task ':app:downloadPredictFloat16ModelFile'.\r\n> org.apache.http.conn.HttpHostConnectException: Connect to tfhub.dev:443 [tfhub.dev/172.217.27.142] failed: Connection timed out: connect\r\n\r\n* Try:\r\nRun with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights.\r\n\r\n* Get more help at https://help.gradle.org\r\n\r\nDeprecated Gradle features were used in this build, making it incompatible with Gradle 7.0.\r\nUse '--warning-mode all' to show the individual deprecation warnings.\r\nSee https://docs.gradle.org/6.1.1/userguide/command_line_interface.html#sec:command_line_warnings\r\n\r\nBUILD FAILED in 24s\r\n7 actionable tasks: 1 executed, 6 up-to-date\r\n\r\n```\r\n\r\n", "comments": ["@cowcomic \r\nPlease share the tensorflow version and simple stand alone code for us to replicate the issue faced.", "it solved.\r\ni use a proxy for gradle\r\nthen everything is ok\r\n\r\n", "so how to use proxy for gradle please? For the result of China Government, we normal Chinese cant use Google at random!", "I try to deal with this question more than 4h, but I couldn't find a suitable way to solve it!", "> so how to use proxy for gradle please? For the result of China Government, we normal Chinese cant use Google at random!\r\n\r\nhi brother,could you solve this problem "]}, {"number": 40644, "title": "R1.8", "body": "Complie C++ project", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40644) for more info**.\n\n<!-- need_sender_cla -->", "@litcheng Thank you for your interest. We don't merge release branches back into master. \r\nCC @mihaimaruseac @chanshah"]}, {"number": 40643, "title": "hello", "body": "helo", "comments": ["Check out this pull request on&nbsp; <a href=\"https://app.reviewnb.com/tensorflow/tensorflow/pull/40643\"><img align=\"absmiddle\"  alt=\"ReviewNB\" height=\"28\" class=\"BotMessageButtonImage\" src=\"https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png\"/></a> \n\n Review Jupyter notebook visual diffs & provide feedback on notebooks. \n\n---\n\n <i>Powered by <a href='https://www.reviewnb.com'>ReviewNB</a></i>", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40643) for more info**.\n\n<!-- need_sender_cla -->", "This does not look like a valid PR"]}, {"number": 40642, "title": "keras.callbacks.ModelCheckPoint has a problem when filepath of working directory is long", "body": "**System information**\r\n- \r\n- OS Platform and Distribution: Windows 10\r\n- TensorFlow version: 2.1.0\r\n- Python version: 3.7.4\r\n\r\n**Issue**\r\nIf the filepath to the current working directory is long, then keras.callbacks.ModelCheckPoint fails when creating a checkpoint. It raises a `NotFoundError`.\r\n\r\nTwo forms of the messages that I've encountered are:\r\n\r\n> NotFoundError: Failed to create a directory: issues_with_mixed_slashes\\too_damn_buggy\\checkpoint_1/variables; No such file or directory\r\n\r\nOR\r\n\r\n> NotFoundError: Failed to create a NewWriteableFile: issues_with_mixed_slashes\\too_damn_buggy\\checkpoint_1\\variables\\variables_temp_af3a37dbc2fc491b87d8afc1eab1ef3a/part-00000-of-00001.data-00000-of-00001.tempstate5339521814986627484 : The system cannot find the path specified.\r\n> ; No such process [Op:SaveV2]\r\n\r\nSome working directories produce the first kind and some others produce the second kind.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimport os\r\nimport numpy as np\r\n\r\nn=10\r\nx = np.random.random((n,1))\r\ny = np.random.random((n,1))\r\n\r\n# set `new_cwdir` to a really long path for the two commented out lines below.\r\n# os.mkdir(new_cwdir)\r\n# os.chdir(new_cwdir)\r\n\r\nall_callbacks=[]\r\ncheckpoint_filepath = \"issues_with_mixed_slashes\\\\too_damn_buggy\\\\checkpoint_{epoch}\"\r\nall_callbacks.append(\r\n    tf.keras.callbacks.ModelCheckpoint(\r\n        filepath=checkpoint_filepath, monitor='loss', verbose=1))\r\n\r\nmodel = tf.keras.models.Sequential()\r\nmodel.add(tf.keras.layers.Dense(1, input_shape=x.shape[-1:]))\r\nmodel.add(tf.keras.layers.Dense(1))\r\n\r\nepochs = 2\r\nmodel.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.MeanAbsoluteError())\r\nhistory = model.fit(x=x, y=y, epochs=epochs, callbacks=all_callbacks)\r\n```\r\nIf you set the working directory to something closer to the root directory, the problem will not occur.\r\n\r\n**More info** \r\nI suspect that the mixing of `/` and `\\`  when TensorFlow tries to create a new directory is (or is part of) the cause.", "comments": ["@princyok \r\n\r\nI have tried in colab with TF versions 2.1,2.2 and i am not seeing any issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/ecffe407c4f1e17fb91f9090f466104a/untitled53.ipynb).Thanks!", "@ravikyram\r\n\r\nI verified and I'm not seeing any issue in colab as well. I also tried with another machine and it didn't occur. But it still occurs in a different machine. Both Windows 10, 64-bit. I'm not sure why.", "@princyok \r\n\r\nThis is not a bug in Tensorflow and something specific to your machine. Please, close this thread as this issue is not related to Tensorflow.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40642\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40642\">No</a>\n", "Found a workaround to this problem from [issue #32505](https://github.com/tensorflow/tensorflow/issues/32505). Use `os.path.join` to create the filepath.  \r\n\r\nI initially thought it shouldn't make a meaningful difference, but I stopped getting the errors when I used `os.path.join`. It's worked out so far. I still feel there is an underlying issue with how TensorFlow creates new directories on Windows when saving models, but this workaround worked so far.\r\n\r\nSo in the standalone code above, it will be: \r\n`checkpoint_filepath = os.path.join(\"issues_with_mixed_slashes\", \"too_damn_buggy\", \"checkpoint_{epoch}\")`\r\n\r\n", "The earlier post was a temporary solution, arguably not a solution (I still don't see why it should've made any difference, but it temporarily did, and that was almost certainly a fluke). \r\n\r\nPERMANENT SOLUTION: Remove the limit that Windows has on the maximum length of path (which is 260 characters). Set the LongPathsEnabled key in Registry to 1."]}, {"number": 40641, "title": "Add new appendable file", "body": "@mihaimaruseac \r\nThis PR add NewAppendableFile, Cleanup and fix a small error.", "comments": []}, {"number": 40640, "title": "\"Could not find a version that satisfies the requirement tensorflow==1.15.3\"", "body": "**System information**\r\n- windows 10\r\n- TensorFlow not installed because doing so doesn't work\r\n- TensorFlow version: 1.15.3\r\n\r\n\r\n\r\n\r\n**Describe the problem**\r\nsee the title; that happens when I try to pip install.\r\n\r\nthis is all related to #40459 and the rest of the relevant info is findable there.\r\n", "comments": ["@Fennecai,\r\nPlease check if you have the latest version of pip installed. You can upgrade pip using the below command. \r\n\r\n`python3 -m pip install --upgrade pip`\r\n\r\nAlso, please check the Python version installed on your machine. As per the [PyPI website](https://pypi.org/project/tensorflow/1.15.3/#files), TF v1.15.3 package is supported only on Python 3.5, 3.6 and 3.7. \r\n\r\nThanks!", "i am having the same issue . i have the latest pip  available but while installing  tensorflow :\r\nERROR: Could not find a version that satisfies the requirement tensorflow-cpu (from versions: none)\r\nERROR: No matching distribution found for tensorflow-cpu\r\n", "> i am having the same issue . i have the latest pip available but while installing tensorflow :\r\n> ERROR: Could not find a version that satisfies the requirement tensorflow-cpu (from versions: none)\r\n> ERROR: No matching distribution found for tensorflow-cpu\r\n\r\n@ghost-cmd,\r\nPlease check if you have met all the system requirements for installing TensorFlow from [this link](https://www.tensorflow.org/install/pip#system-requirements). \r\n\r\nIf you are still facing the same error, please submit a new issue form [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40640\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40640\">No</a>\n", "Downgrade from python 3.9 to 3.7 worked for me"]}, {"number": 40639, "title": "C++ compilation of rule '//tensorflow/core/kernels:softsign_op' ", "body": "Under Windows-10 I have tried to build [TensorFlow 2.2 from the source](https://www.tensorflow.org/install/source_windows) and it ends with the following error\r\n\r\n```\r\nERROR: C:/tensorflow/tensorflow/core/kernels/BUILD:4748:1: C++ compilation of rule '//tensorflow/core/kernels:softsign_op' failed (Exit 2)\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\ucrt\\corecrt_search.h(14): fatal error C1083: Cannot open include file: 'stddef.h': No such file or directory\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 9402.886s, Critical Path: 4760.75s\r\nINFO: 6735 processes: 6735 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "comments": ["@peter197321 \r\nCan you please refer to these links and let us know if it helps:\r\n[link](https://wiki.qt.io/Cannot_open_include_file_stddef_h_No_such_file_or_directory) [link1](https://social.msdn.microsoft.com/Forums/vstudio/en-US/0f580a73-6f11-4aa6-abbe-6bf1438734c2/standard-files-quotstddefhquot-quotstringhquot-missing-in-visual-studio-2015-rc?forum=vclanguage) [link2](https://github.com/weidai11/cryptopp/issues/781#issuecomment-454076517)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40639\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40639\">No</a>\n"]}, {"number": 40638, "title": "Keras layer weights/sublayers getting deleted when creating a model with them. model.summary() / plot_model still shows those weights as part of graph though", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google colab enviroment\r\n\r\n- TensorFlow installed from (source or binary): Google colab default \r\n- Python version: Python 3, Google colab default \r\n\r\n- CUDA/cuDNN version:   Google colab default  \r\n- GPU model and memory:  Tested on both Google colab p-100 GPU and CPU\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n>2020-06-20 21:44:17.003371: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\nv2.2.0-0-g2b96f3662b 2.2.0\r\n\r\n**Describe the current behavior**\r\n\r\nI created a new model using two layers from and old model. However, now all of the layers/weights from the old model are Not showing up in the new model. \r\n\r\n`model.summary()` and \r\n\r\n```\r\ntf.keras.utils.plot_model(\r\n    model, to_file='model.png', show_shapes=False, show_layer_names=True,\r\n    rankdir='TB', expand_nested=False, dpi=96\r\n)\r\n```\r\n\r\nstill has those weights, so I think they're a part of the graph.  But when I print them out, those weights/layers are missing altogether \r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nAll weights from component layers to should be in the model. \r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nHere is a Colabnotebook with a minimal example that reproduced the issue. \r\n\r\nhttps://colab.research.google.com/drive/1n3_XNhdgH6Qo7GT-M570lIKWAoU3TML5?usp=sharing\r\n\r\nAnd here is the code\r\n\r\n```\r\n!pip install transformers --q\r\n%tensorflow_version 2.x\r\n\r\nfrom transformers import TFBertModel, AutoModel, TFRobertaModel, AutoTokenizer\r\nimport tensorflow as tf\r\nimport tensorflow_addons as tfa\r\n\r\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\r\n\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nfrom copy import deepcopy\r\n\r\nlogger = tf.get_logger()\r\nlogger.info(tf.__version__)\r\n\r\n\r\ndef get_mini_models():\r\n    tempModel = TFRobertaModel.from_pretrained('bert-base-uncased', from_pt=True)\r\n\r\n    layer9 = deepcopy(tempModel.layers[0].encoder.layer[8])\r\n    layer10 = deepcopy(tempModel.layers[0].encoder.layer[9])\r\n\r\n    inputHiddenVals = tf.keras.Input(shape=[None, None], dtype=tf.float32, name='input_Q',\r\n                                    batch_size=None) \r\n\r\n    hidden1 = layer9((inputHiddenVals, None, None))\r\n    hidden2 = layer10((hidden1[0], None, None))\r\n    modelNew = tf.keras.Model(inputs=inputHiddenVals, outputs=hidden2)\r\n\r\n    del tempModel\r\n\r\n    return modelNew\r\n\r\n@tf.function\r\ndef loss_fn(_, probs):\r\n    bs = tf.shape(probs)[0]\r\n    labels = tf.eye(bs, bs)\r\n    return tf.losses.categorical_crossentropy(labels,\r\n                                              probs,\r\n                                              from_logits=True)\r\n\r\nmodel = get_mini_models()\r\nmodel.compile(loss=loss_fn,\r\n                optimizer=tfa.optimizers.AdamW(weight_decay=1e-4, learning_rate=1e-5, \r\n                                                epsilon=1e-06))\r\n\r\n# Get model and layers directly to compare\r\ntempModel = TFRobertaModel.from_pretrained('bert-base-uncased', from_pt=True)\r\nlayer9 = deepcopy(tempModel.layers[0].encoder.layer[8])\r\nlayer10 = deepcopy(tempModel.layers[0].encoder.layer[9])\r\n\r\n# Only one layer, and that layer also has missing weights. \r\nfor i, var in enumerate(model.weights):\r\n    print(model.weights[i].name)\r\n\r\n# Full weights for one layer \r\nfor i, var in enumerate(layer9.weights):\r\n    print(layer9.weights[i].name)\r\n\r\n# Test what correct output should be \r\n\r\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\r\ninputt = tokenizer.encode('This is a sentence', return_tensors='tf')\r\noutt = tempModel(inputt)[0]\r\n\r\n# Test model output. Not the same. \r\n\r\nmodel(outt)\r\n\r\n# Model summary somehow lists the weights \r\nmodel.summary()\r\n\r\n# Model diagram shows the correct connections between all the layers. \r\n\r\ntf.keras.utils.plot_model(\r\n    model, to_file='model.png', show_shapes=False, show_layer_names=True,\r\n    rankdir='TB', expand_nested=False, dpi=96\r\n)\r\n\r\n```\r\n\r\nEdit: I also tried making the layers from scratch, and setting the weights directly, and got the same result. Here's a colab notebook that does this. https://colab.research.google.com/drive/1EC_fObSp9lUsj_PFaYgFtRI93ErPYmU9?usp=sharing\r\n\r\nAnd here's the code \r\n\r\n```\r\n!pip install transformers --q\r\n%tensorflow_version 2.x\r\n\r\nfrom transformers import TFBertModel, AutoModel, TFRobertaModel, AutoTokenizer\r\n\r\nimport tensorflow as tf\r\nimport tensorflow_addons as tfa\r\n\r\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\r\n\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras.layers import (Dense,\r\n                                     Dropout)\r\nimport numpy as np\r\nimport os\r\n\r\nlogger = tf.get_logger()\r\nlogger.info(tf.__version__)\r\n\r\nclass TFBertSelfAttention2(tf.keras.layers.Layer):\r\n    def __init__(self, config, **kwargs):\r\n        super().__init__(**kwargs)\r\n        if config.hidden_size % config.num_attention_heads != 0:\r\n            raise ValueError(\r\n                \"The hidden size (%d) is not a multiple of the number of attention \"\r\n                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads)\r\n            )\r\n\r\n        self.num_attention_heads = config.num_attention_heads\r\n        assert config.hidden_size % config.num_attention_heads == 0\r\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\r\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\r\n\r\n        self.query = tf.keras.layers.Dense(\r\n            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"query_2\"\r\n        )\r\n        self.key = tf.keras.layers.Dense(\r\n            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"key_2\"\r\n        )\r\n        self.value = tf.keras.layers.Dense(\r\n            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"value_2\"\r\n        )\r\n\r\n        self.dropout = tf.keras.layers.Dropout(config.attention_probs_dropout_prob)\r\n\r\n    def transpose_for_scores(self, x, batch_size):\r\n        x = tf.reshape(x, (batch_size, -1, self.num_attention_heads, self.attention_head_size))\r\n        return tf.transpose(x, perm=[0, 2, 1, 3])\r\n\r\n    def call(self, inputs, training=False):\r\n        hidden_states, attention_mask, head_mask, output_attentions = inputs\r\n\r\n        batch_size = shape_list(hidden_states)[0]\r\n        mixed_query_layer = self.query(hidden_states)\r\n        mixed_key_layer = self.key(hidden_states)\r\n        mixed_value_layer = self.value(hidden_states)\r\n\r\n        query_layer = self.transpose_for_scores(mixed_query_layer, batch_size)\r\n        key_layer = self.transpose_for_scores(mixed_key_layer, batch_size)\r\n        value_layer = self.transpose_for_scores(mixed_value_layer, batch_size)\r\n\r\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\r\n        attention_scores = tf.matmul(\r\n            query_layer, key_layer, transpose_b=True\r\n        )  # (batch size, num_heads, seq_len_q, seq_len_k)\r\n        dk = tf.cast(shape_list(key_layer)[-1], tf.float32)  # scale attention_scores\r\n        attention_scores = attention_scores / tf.math.sqrt(dk)\r\n\r\n        if attention_mask is not None:\r\n            # Apply the attention mask is (precomputed for all layers in TFBertModel call() function)\r\n            attention_scores = attention_scores + attention_mask\r\n\r\n        # Normalize the attention scores to probabilities.\r\n        attention_probs = tf.nn.softmax(attention_scores, axis=-1)\r\n\r\n        # This is actually dropping out entire tokens to attend to, which might\r\n        # seem a bit unusual, but is taken from the original Transformer paper.\r\n        attention_probs = self.dropout(attention_probs, training=training)\r\n\r\n        # Mask heads if we want to\r\n        if head_mask is not None:\r\n            attention_probs = attention_probs * head_mask\r\n\r\n        context_layer = tf.matmul(attention_probs, value_layer)\r\n\r\n        context_layer = tf.transpose(context_layer, perm=[0, 2, 1, 3])\r\n        context_layer = tf.reshape(\r\n            context_layer, (batch_size, -1, self.all_head_size)\r\n        )  # (batch_size, seq_len_q, all_head_size)\r\n\r\n        outputs = (\r\n            (context_layer, attention_probs) if cast_bool_to_primitive(output_attentions) is True else (context_layer,)\r\n        )\r\n\r\n        return outputs\r\n\r\n\r\nclass TFBertSelfOutput2(tf.keras.layers.Layer):\r\n    def __init__(self, config, **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.dense = tf.keras.layers.Dense(\r\n            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense2\"\r\n        )\r\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm2\")\r\n        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\r\n\r\n    def call(self, inputs, training=False):\r\n        hidden_states, input_tensor = inputs\r\n\r\n        hidden_states = self.dense(hidden_states)\r\n        hidden_states = self.dropout(hidden_states, training=training)\r\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\r\n        return hidden_states\r\n\r\n\r\nclass TFBertAttention2(tf.keras.layers.Layer):\r\n    def __init__(self, config, **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.self_attention = TFBertSelfAttention2(config, name=\"self2\")\r\n        self.dense_output = TFBertSelfOutput2(config, name=\"output2\")\r\n\r\n    def prune_heads(self, heads):\r\n        raise NotImplementedError\r\n\r\n    def call(self, inputs, training=False):\r\n        input_tensor, attention_mask, head_mask, output_attentions = inputs\r\n\r\n        self_outputs = self.self_attention(\r\n            [input_tensor, attention_mask, head_mask, output_attentions], training=training\r\n        )\r\n        attention_output = self.dense_output([self_outputs[0], input_tensor], training=training)\r\n        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\r\n        return outputs\r\n\r\n\r\nclass TFBertIntermediate2(tf.keras.layers.Layer):\r\n    def __init__(self, config, **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.dense = tf.keras.layers.Dense(\r\n            config.intermediate_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense2\"\r\n        )\r\n        if isinstance(config.hidden_act, str):\r\n            self.intermediate_act_fn = ACT2FN[config.hidden_act]\r\n        else:\r\n            self.intermediate_act_fn = config.hidden_act\r\n\r\n    def call(self, hidden_states):\r\n        hidden_states = self.dense(hidden_states)\r\n        hidden_states = self.intermediate_act_fn(hidden_states)\r\n        return hidden_states\r\n\r\n\r\nclass TFBertOutput2(tf.keras.layers.Layer):\r\n    def __init__(self, config, **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.dense = tf.keras.layers.Dense(\r\n            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense2\"\r\n        )\r\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm2\")\r\n        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\r\n\r\n    def call(self, inputs, training=False):\r\n        hidden_states, input_tensor = inputs\r\n\r\n        hidden_states = self.dense(hidden_states)\r\n        hidden_states = self.dropout(hidden_states, training=training)\r\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\r\n        return hidden_states\r\n\r\n\r\nclass TFBertLayer2(tf.keras.layers.Layer):\r\n    def __init__(self, config, **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.attention = TFBertAttention2(config, name=\"attention2\")\r\n        self.intermediate = TFBertIntermediate2(config, name=\"intermediate2\")\r\n        self.bert_output = TFBertOutput2(config, name=\"output2\")\r\n\r\n    def call(self, inputs, training=False):\r\n        hidden_states, attention_mask, head_mask, output_attentions = inputs\r\n\r\n        attention_outputs = self.attention(\r\n            [hidden_states, attention_mask, head_mask, output_attentions], training=training\r\n        )\r\n        attention_output = attention_outputs[0]\r\n        intermediate_output = self.intermediate(attention_output)\r\n        layer_output = self.bert_output([intermediate_output, attention_output], training=training)\r\n        outputs = (layer_output,) + attention_outputs[1:]  # add attentions if we output them\r\n        return outputs\r\n\r\n\r\nclass TFBertSelfAttention(tf.keras.layers.Layer):\r\n    def __init__(self, config, **kwargs):\r\n        super().__init__(**kwargs)\r\n        if config.hidden_size % config.num_attention_heads != 0:\r\n            raise ValueError(\r\n                \"The hidden size (%d) is not a multiple of the number of attention \"\r\n                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads)\r\n            )\r\n\r\n        self.num_attention_heads = config.num_attention_heads\r\n        assert config.hidden_size % config.num_attention_heads == 0\r\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\r\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\r\n\r\n        self.query = tf.keras.layers.Dense(\r\n            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"query_\"\r\n        )\r\n        self.key = tf.keras.layers.Dense(\r\n            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"key_\"\r\n        )\r\n        self.value = tf.keras.layers.Dense(\r\n            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"value_\"\r\n        )\r\n\r\n        self.dropout = tf.keras.layers.Dropout(config.attention_probs_dropout_prob)\r\n\r\n    def transpose_for_scores(self, x, batch_size):\r\n        x = tf.reshape(x, (batch_size, -1, self.num_attention_heads, self.attention_head_size))\r\n        return tf.transpose(x, perm=[0, 2, 1, 3])\r\n\r\n    def call(self, inputs, training=False):\r\n        hidden_states, attention_mask, head_mask, output_attentions = inputs\r\n\r\n        batch_size = shape_list(hidden_states)[0]\r\n        mixed_query_layer = self.query(hidden_states)\r\n        mixed_key_layer = self.key(hidden_states)\r\n        mixed_value_layer = self.value(hidden_states)\r\n\r\n        query_layer = self.transpose_for_scores(mixed_query_layer, batch_size)\r\n        key_layer = self.transpose_for_scores(mixed_key_layer, batch_size)\r\n        value_layer = self.transpose_for_scores(mixed_value_layer, batch_size)\r\n\r\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\r\n        attention_scores = tf.matmul(\r\n            query_layer, key_layer, transpose_b=True\r\n        )  # (batch size, num_heads, seq_len_q, seq_len_k)\r\n        dk = tf.cast(shape_list(key_layer)[-1], tf.float32)  # scale attention_scores\r\n        attention_scores = attention_scores / tf.math.sqrt(dk)\r\n\r\n        if attention_mask is not None:\r\n            # Apply the attention mask is (precomputed for all layers in TFBertModel call() function)\r\n            attention_scores = attention_scores + attention_mask\r\n\r\n        # Normalize the attention scores to probabilities.\r\n        attention_probs = tf.nn.softmax(attention_scores, axis=-1)\r\n\r\n        # This is actually dropping out entire tokens to attend to, which might\r\n        # seem a bit unusual, but is taken from the original Transformer paper.\r\n        attention_probs = self.dropout(attention_probs, training=training)\r\n\r\n        # Mask heads if we want to\r\n        if head_mask is not None:\r\n            attention_probs = attention_probs * head_mask\r\n\r\n        context_layer = tf.matmul(attention_probs, value_layer)\r\n\r\n        context_layer = tf.transpose(context_layer, perm=[0, 2, 1, 3])\r\n        context_layer = tf.reshape(\r\n            context_layer, (batch_size, -1, self.all_head_size)\r\n        )  # (batch_size, seq_len_q, all_head_size)\r\n\r\n        outputs = (\r\n            (context_layer, attention_probs) if cast_bool_to_primitive(output_attentions) is True else (context_layer,)\r\n        )\r\n\r\n        return outputs\r\n\r\n\r\nclass TFBertSelfOutput(tf.keras.layers.Layer):\r\n    def __init__(self, config, **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.dense = tf.keras.layers.Dense(\r\n            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\r\n        )\r\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\r\n        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\r\n\r\n    def call(self, inputs, training=False):\r\n        hidden_states, input_tensor = inputs\r\n\r\n        hidden_states = self.dense(hidden_states)\r\n        hidden_states = self.dropout(hidden_states, training=training)\r\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\r\n        return hidden_states\r\n\r\n\r\nclass TFBertAttention(tf.keras.layers.Layer):\r\n    def __init__(self, config, **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.self_attention = TFBertSelfAttention(config, name=\"self\")\r\n        self.dense_output = TFBertSelfOutput(config, name=\"output\")\r\n\r\n    def prune_heads(self, heads):\r\n        raise NotImplementedError\r\n\r\n    def call(self, inputs, training=False):\r\n        input_tensor, attention_mask, head_mask, output_attentions = inputs\r\n\r\n        self_outputs = self.self_attention(\r\n            [input_tensor, attention_mask, head_mask, output_attentions], training=training\r\n        )\r\n        attention_output = self.dense_output([self_outputs[0], input_tensor], training=training)\r\n        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\r\n        return outputs\r\n\r\n\r\nclass TFBertIntermediate(tf.keras.layers.Layer):\r\n    def __init__(self, config, **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.dense = tf.keras.layers.Dense(\r\n            config.intermediate_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\r\n        )\r\n        if isinstance(config.hidden_act, str):\r\n            self.intermediate_act_fn = ACT2FN[config.hidden_act]\r\n        else:\r\n            self.intermediate_act_fn = config.hidden_act\r\n\r\n    def call(self, hidden_states):\r\n        hidden_states = self.dense(hidden_states)\r\n        hidden_states = self.intermediate_act_fn(hidden_states)\r\n        return hidden_states\r\n\r\n\r\nclass TFBertOutput(tf.keras.layers.Layer):\r\n    def __init__(self, config, **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.dense = tf.keras.layers.Dense(\r\n            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\r\n        )\r\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\r\n        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\r\n\r\n    def call(self, inputs, training=False):\r\n        hidden_states, input_tensor = inputs\r\n\r\n        hidden_states = self.dense(hidden_states)\r\n        hidden_states = self.dropout(hidden_states, training=training)\r\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\r\n        return hidden_states\r\n\r\n\r\nclass TFBertLayer(tf.keras.layers.Layer):\r\n    def __init__(self, config, **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.attention = TFBertAttention(config, name=\"attention\")\r\n        self.intermediate = TFBertIntermediate(config, name=\"intermediate\")\r\n        self.bert_output = TFBertOutput(config, name=\"output\")\r\n\r\n    def call(self, inputs, training=False):\r\n        hidden_states, attention_mask, head_mask, output_attentions = inputs\r\n\r\n        attention_outputs = self.attention(\r\n            [hidden_states, attention_mask, head_mask, output_attentions], training=training\r\n        )\r\n        attention_output = attention_outputs[0]\r\n        intermediate_output = self.intermediate(attention_output)\r\n        layer_output = self.bert_output([intermediate_output, attention_output], training=training)\r\n        outputs = (layer_output,) + attention_outputs[1:]  # add attentions if we output them\r\n        return outputs\r\n\r\nconfigBase = {\r\n  \"attention_probs_dropout_prob\": 0.1,\r\n  \"bos_token_id\": 0,\r\n  \"eos_token_id\": 2,\r\n  \"hidden_act\": \"gelu\",\r\n  \"hidden_dropout_prob\": 0.1,\r\n  \"hidden_size\": 768,\r\n  \"initializer_range\": 0.02,\r\n  \"intermediate_size\": 3072,\r\n  \"layer_norm_eps\": 1e-05,\r\n  \"max_position_embeddings\": 514,\r\n  \"model_type\": \"roberta\",\r\n  \"num_attention_heads\": 12,\r\n  \"num_hidden_layers\": 12,\r\n  \"pad_token_id\": 1,\r\n  \"type_vocab_size\": 1,\r\n  \"vocab_size\": 50265\r\n}\r\n\r\nclass AttrDict(dict):\r\n    def __init__(self, *args, **kwargs):\r\n        super(AttrDict, self).__init__(*args, **kwargs)\r\n        self.__dict__ = self\r\n\r\nconfig = AttrDict(configBase)\r\n\r\ndef get_initializer(initializer_range=0.02):\r\n    \"\"\"Creates a `tf.initializers.truncated_normal` with the given range.\r\n    Args:\r\n        initializer_range: float, initializer range for stddev.\r\n    Returns:\r\n        TruncatedNormal initializer with stddev = `initializer_range`.\r\n    \"\"\"\r\n    return tf.keras.initializers.TruncatedNormal(stddev=initializer_range)\r\n\r\n\r\ndef gelu(x):\r\n    \"\"\" Gaussian Error Linear Unit.\r\n    Original Implementation of the gelu activation function in Google Bert repo when initially created.\r\n        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\r\n        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\r\n        Also see https://arxiv.org/abs/1606.08415\r\n    \"\"\"\r\n    cdf = 0.5 * (1.0 + tf.math.erf(x / tf.math.sqrt(2.0)))\r\n    return x * cdf\r\n\r\nACT2FN = {\r\n    \"gelu\": tf.keras.layers.Activation(gelu),\r\n}\r\n\r\ndef shape_list(x):\r\n    \"\"\"Deal with dynamic shape in tensorflow cleanly.\"\"\"\r\n    static = x.shape.as_list()\r\n    dynamic = tf.shape(x)\r\n    return [dynamic[i] if s is None else s for i, s in enumerate(static)]\r\n\r\ndef cast_bool_to_primitive(bool_variable, default_tensor_to_true=False):\r\n    \"\"\"Function arguments can be inserted as boolean tensor\r\n        and bool variables to cope with keras serialization\r\n        we need to cast `output_attentions` to correct bool\r\n        if it is a tensor\r\n    Args:\r\n        default_tensor_to_true: bool, if tensor should default to True\r\n        in case tensor has no numpy attribute\r\n    \"\"\"\r\n    # if bool variable is tensor and has numpy value\r\n    if tf.is_tensor(bool_variable):\r\n        if hasattr(bool_variable, \"numpy\"):\r\n            return bool(bool_variable.numpy())\r\n        elif default_tensor_to_true:\r\n            return True\r\n\r\n    # else variable is bool\r\n    return bool_variable\r\n\r\ndef get_2_transformerLayerP(numb):\r\n    tokenizer = AutoTokenizer.from_pretrained('allenai/biomed_roberta_base')\r\n    inputt = tokenizer.encode('This is a sentence', return_tensors='tf')\r\n    tempModel = TFRobertaModel.from_pretrained('allenai/biomed_roberta_base', from_pt=True)\r\n    outt = tempModel(inputt)[0]\r\n\r\n    t_layer11 = TFBertLayer(config, name=\"layer_._{}\".format(11+numb))\r\n    t_layer12 = TFBertLayer2(config, name=\"layer_._{}\".format(12+numb))\r\n\r\n    t_layer11((outt, None, None, None))\r\n    t_layer12((outt, None, None, None))\r\n\r\n    t_layer11.set_weights( tempModel.layers[0].encoder.layer[10].get_weights() )\r\n    t_layer12.set_weights( tempModel.layers[0].encoder.layer[11].get_weights() )\r\n\r\n    t_layer12.intermediate.intermediate_act_fn = tf.keras.activations.tanh\r\n\r\n    del tokenizer\r\n    del tempModel\r\n\r\n    return t_layer11, t_layer12\r\n\r\ndef get_mini_models():\r\n    P_trans11, P_trans12 = get_2_transformerLayerP(6)\r\n\r\n    inputHiddenVals = tf.keras.Input(shape=[None, None], dtype=tf.float32, name='input_Q',\r\n                                    batch_size=None) \r\n\r\n    P_outputs = P_trans11((inputHiddenVals, None, None, None))[0]\r\n    P_outputsFinal = P_trans12((P_outputs, None, None, None))[0]\r\n    modelNew = tf.keras.Model(inputs=inputHiddenVals, outputs=P_outputsFinal)\r\n\r\n    return modelNew\r\n\r\n@tf.function\r\ndef loss_fn(_, probs):\r\n\r\n    bs = tf.shape(probs)[0]\r\n    labels = tf.eye(bs, bs)\r\n    return tf.losses.categorical_crossentropy(labels,\r\n                                              probs,\r\n                                              from_logits=True)\r\n\r\nmodel = get_mini_models()\r\nmodel.compile(loss=loss_fn,\r\n                optimizer=tfa.optimizers.AdamW(weight_decay=1e-4, learning_rate=1e-5, \r\n                                                epsilon=1e-06))\r\n\r\nfor i, var in enumerate(model.trainable_weights):\r\n    print(model.trainable_weights[i].name)\r\n\r\n```", "comments": ["@Santosh-Gupta \r\n\r\nI have tried in colab with TF version 2.2 .Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/0191e12b7c6d9afeb80ccc009870b255/untitled52.ipynb).Is this the expected behavior?.Thanks!", "Hey\r\n\r\n> @Santosh-Gupta\r\n> \r\n> I have tried in colab with TF version 2.2 .Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/0191e12b7c6d9afeb80ccc009870b255/untitled52.ipynb).Is this the expected behavior?.Thanks!\r\n\r\nThis was a little tricky to checkout, so I split up the cells so some of the outputs are viewable\r\n\r\nhttps://colab.research.google.com/gist/ravikyram/0191e12b7c6d9afeb80ccc009870b255/untitled52.ipynb\r\n\r\nThe output of cell 6 is \r\n\r\n```\r\ntf_roberta_model_1/roberta/encoder/layer_._8/attention/self/query/kernel:0\r\ntf_roberta_model_1/roberta/encoder/layer_._8/attention/self/query/bias:0\r\ntf_roberta_model_1/roberta/encoder/layer_._8/attention/self/key/kernel:0\r\ntf_roberta_model_1/roberta/encoder/layer_._8/attention/self/key/bias:0\r\ntf_roberta_model_1/roberta/encoder/layer_._8/attention/self/value/kernel:0\r\ntf_roberta_model_1/roberta/encoder/layer_._8/attention/self/value/bias:0\r\n```\r\n\r\nwhich contains all the model weights. There are weights defined in the model that are missing. \r\n\r\nIt should look something like the output of cell 7, which prints out the weights of both of the layers which comprise the model. \r\n\r\n```\r\ntf_roberta_model_2/roberta/encoder/layer_._8/attention/self/query/kernel:0\r\ntf_roberta_model_2/roberta/encoder/layer_._8/attention/self/query/bias:0\r\ntf_roberta_model_2/roberta/encoder/layer_._8/attention/self/key/kernel:0\r\ntf_roberta_model_2/roberta/encoder/layer_._8/attention/self/key/bias:0\r\ntf_roberta_model_2/roberta/encoder/layer_._8/attention/self/value/kernel:0\r\ntf_roberta_model_2/roberta/encoder/layer_._8/attention/self/value/bias:0\r\ntf_roberta_model_2/roberta/encoder/layer_._8/attention/output/dense/kernel:0\r\ntf_roberta_model_2/roberta/encoder/layer_._8/attention/output/dense/bias:0\r\ntf_roberta_model_2/roberta/encoder/layer_._8/attention/output/LayerNorm/gamma:0\r\ntf_roberta_model_2/roberta/encoder/layer_._8/attention/output/LayerNorm/beta:0\r\ntf_roberta_model_2/roberta/encoder/layer_._8/intermediate/dense/kernel:0\r\ntf_roberta_model_2/roberta/encoder/layer_._8/intermediate/dense/bias:0\r\ntf_roberta_model_2/roberta/encoder/layer_._8/output/dense/kernel:0\r\ntf_roberta_model_2/roberta/encoder/layer_._8/output/dense/bias:0\r\ntf_roberta_model_2/roberta/encoder/layer_._8/output/LayerNorm/gamma:0\r\ntf_roberta_model_2/roberta/encoder/layer_._8/output/LayerNorm/beta:0\r\ntf_roberta_model_2/roberta/encoder/layer_._9/attention/self/query/kernel:0\r\ntf_roberta_model_2/roberta/encoder/layer_._9/attention/self/query/bias:0\r\ntf_roberta_model_2/roberta/encoder/layer_._9/attention/self/key/kernel:0\r\ntf_roberta_model_2/roberta/encoder/layer_._9/attention/self/key/bias:0\r\ntf_roberta_model_2/roberta/encoder/layer_._9/attention/self/value/kernel:0\r\ntf_roberta_model_2/roberta/encoder/layer_._9/attention/self/value/bias:0\r\ntf_roberta_model_2/roberta/encoder/layer_._9/attention/output/dense/kernel:0\r\ntf_roberta_model_2/roberta/encoder/layer_._9/attention/output/dense/bias:0\r\ntf_roberta_model_2/roberta/encoder/layer_._9/attention/output/LayerNorm/gamma:0\r\ntf_roberta_model_2/roberta/encoder/layer_._9/attention/output/LayerNorm/beta:0\r\ntf_roberta_model_2/roberta/encoder/layer_._9/intermediate/dense/kernel:0\r\ntf_roberta_model_2/roberta/encoder/layer_._9/intermediate/dense/bias:0\r\ntf_roberta_model_2/roberta/encoder/layer_._9/output/dense/kernel:0\r\ntf_roberta_model_2/roberta/encoder/layer_._9/output/dense/bias:0\r\ntf_roberta_model_2/roberta/encoder/layer_._9/output/LayerNorm/gamma:0\r\ntf_roberta_model_2/roberta/encoder/layer_._9/output/LayerNorm/beta:0\r\n```\r\n\r\nSo one layer is missing altogether. And in the other layer, weights are missing as well. ", "The missing weights issue doesn't seem to happen if the architecture is made using keras model subclassing. \r\n\r\nI made a colab gist showing the architecture made using model subclassing, and the same architecture made using the functional API, with the latter resulting in missing trainable weights. \r\n\r\nhttps://colab.research.google.com/gist/Santosh-Gupta/273361f873e4daf572fddea691b1f325/missingtrainablevars.ipynb\r\n\r\nThe gist uses the huggingface transformers library to create the layers, so the code is clearer to look through, though it doesn't show the layer code like in my previous colab notebook. \r\n\r\nAnother issue; using model subclassing may not be a viable workaround for TPU training my architecture; It looks like TPU training does not support subclassed model, as described in this stackoverflow post https://stackoverflow.com/questions/60444486/use-tf-distribute-strategies-with-tf-keras-model-subclassing\r\n\r\n>ValueError: We currently do not support distribution strategy with a `Sequential` model that is created without `input_shape`/`input_dim` set in its first layer or a subclassed model.\r\n\r\nI am also running into a different issue with the subclassed model, which is described in a different github issue (not sure if its related to model subclassing) \r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/41074 \r\n\r\n\r\n\r\n", "I tried both the nightly version of tf 2.2.0 and the tf 2.3.0-rc1 in case there was some sort of fix, but both resulted in missing trainable variables for using the functional API. \r\n\r\nFor convenience, here is a github gist of the code I ran. \r\n\r\nhttps://colab.research.google.com/gist/Santosh-Gupta/b02668b47744655ce32e6d427357cb35/missingtrainablevars_nightly_2-3-0-rc1.ipynb", "I wanted to check if the inference was the same for both models, and they are. I tested it using both the Transformers library and creating each layer from scratch (Transformers library only used to copy/set consistent weights)\r\n\r\nHere are the colab gists\r\n\r\nhttps://colab.research.google.com/gist/Santosh-Gupta/ec9f7c8a189a8d99e73d96fbe728aef8/model_weight_debug_scratch_public_inference.ipynb\r\n\r\nhttps://colab.research.google.com/gist/Santosh-Gupta/766a27c1500a330cba6f479805dad27d/missingtrainablevarsinference.ipynb\r\n\r\nI sort of went through the from scratch version in details, and I am unable to figure out why this is happening for the functional api. From my understanding if the Keras Model fit function, if the weights aren't in the list of the trainable_variables, then they won't receive gradient updates. And this issue seems prone to any Keras model that uses custom layers with the functional API. \r\n\r\nIs this something anyone using custom Keras layers with the Functional API should be worried about? ", "Hi @Santosh-Gupta:\r\n\r\nIt looks like what's going on is:\r\nThe layers currently enter a 'functional api construction' mode only if all of the inputs in the first argument come from other Keras layers. However, you have `None` included in the inputs in the first positional arg, so it's not triggering functional api construction.\r\n\r\nThat causes the layer to get 'inlined' in the outer functional model rather than correctly included. You should be able to work around this by changing the layer api so Nones should not get passed in.\r\n\r\nWe have a major cleanup/refactoring of the Functional API mostly done that makes the functional api triggering much clearer (if *any* symbolic values appear in the inputs) & sorts out a number of other issues as well. But, that will only land in 2.4. It's not immediately obvious if we can squeeze a fix into tf 2.3 as the RC is already out.", "Thanks for the reply! I am wondering what you mean by 'outer functional model', I tried googling 'keras outer functional model' and 'keras outer model' but I wasn't able to find anything. \r\n\r\nFor clarity, should sending tuples/lists be avoided when sending input to layers, or just `None`s should be avoided?\r\n\r\nFor example, is this ok?\r\n\r\n` P_trans11((inputHiddenVals, someOtherInputNotFromKersaLayer))`\r\n\r\nOr should it only be this\r\n\r\n` P_trans11(inputHiddenVals)`\r\n\r\n\r\n", "Ah sorry by outer functional model I just meant the functional model you are building (as opposed to the layers inside of the model).\r\n\r\nEssentially rather than constructing the functional model as:\r\ninputs -> layer 1 -> layer 2 -> outputs\r\n(which is what should be expected),\r\n\r\nit ends up inlining all the contents of layer 1 & layer 2:\r\ninputs -> first op in layer 1 -> nested sublayer in layer 1 -> etc. -> first op in layer 2 ... -> outputs\r\nAnd so the model lacks a reference to layer 1 & layer 2 themselves and misses any weights that weren't contained in their subweights.\r\n\r\n> For clarity, should sending tuples/lists be avoided when sending input to layers, or just Nones should be avoided?\r\n\r\nAny data structure passed to the first positional arg should contain only symbolic values produced from tf.keras.inputs/other keras layers. For following positional args & any keyword args, you can use arbitrary data structures that may or may not contain symbolic values.\r\n\r\nWhat you're experiencing is specifically an issue when a data structure passed to the first positional arg contains an item that is not a symbolic functional input/output.\r\n\r\n---------------------------------\r\nThis specific behavior is a historical edge case dating back to when Keras layers only ever accepted a single positional argument that could not be an arbitrary data structure, and all of the inputs had to be symbolic keras inputs/outputs. Unfortunately it's caused this surprising behavior when combined w/ other functionality that has been added since (automatically turning tf op layers into keras layers).\r\nSo, historically trying to pass in `None`s like you're doing would have triggered a (hard to interpret) error message, because TF/Keras wouldn't be able to inline the tf ops inside the functional model when it calls the layer. Now it silently behaves in a way you didn't expect because tf ops *can* be used during functional API construction.\r\n\r\n", "I am wondering if a possible workaround is either a layer or model that can append `None`s to the inputs/outputs. \r\n\r\nSo something like \r\n\r\n```\r\nclass transformer_IO(tf.keras.layers.Layer):\r\n    def call(self, input):\r\n        return (input, None, None, None)\r\n```\r\n\r\nor \r\n\r\n```\r\nclass transformer_IO_model(tf.keras.Model):\r\n    def call(self, input):\r\n        return (input, None, None, None)\r\n```\r\n\r\nUsing either as-is results in \"AttributeError: 'NoneType' object has no attribute 'shape'\". I am wondering if this sort of workaround is worth further exploring, or if I should just work on redesigning the layers. \r\n\r\nFor convenience, here is a colab gist of my attempts \r\n\r\nhttps://colab.research.google.com/gist/Santosh-Gupta/3b5a2c6bc288c58eda992e08da1986ee/workaroundattempt.ipynb\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "Layers in a functional API can only output tensors/data structures of tensors.\r\n\r\nAs a workaround in the same vein though if you don't want to redesign the underlying layers:\r\nYou can have a layer contain a nested transformer layer, and pass the input args + Nones to the nested layer. You can then use this layer in a functional model w/o issues.\r\n\r\nAlternatively if you're okay trying the tf nightlies, you can experiment with the refactoring I mentioned earlier by directly flipping our internal experimental flag:\r\n```\r\nfrom tensorflow.python.keras.engine import keras_tensor\r\nkeras_tensor.enable_keras_tensors()\r\n```\r\nI believe it should fix your issue and it should make the functional api generally much more reliable, but like I said it will only be landing in 2.4.", "> from tensorflow.python.keras.engine import keras_tensor\r\n> keras_tensor.enable_keras_tensors()\r\n\r\nThis solved the issue, at least all the weight show up now. For people checking in on this, I used tf-nightly 2.4.0.dev20200715 (just in case it breaks in a future version of nightly)\r\n\r\nI'll be testing out training soon. ", "Update on this issue: We weren't able to get a fix into 2.3 because it can't be done safely separately from the functional api internals refactoring, but we were able to add raising a meaningful error message for 2.3 in this setting rather than silently missing some of the weights.\r\n\r\n(The proper fix is still in the nightlies guarded by the Functional API refactoring, as mentioned above.)", "The functional API refactoring has landed in the nightlies, so this should now be fixed in the nightlies.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40638\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40638\">No</a>\n"]}, {"number": 40637, "title": "UnidentifiedImageError when I try to train my model", "body": "UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x63a807fb0>\r\n\r\nUsing Keras library\r\nOS: macOS Catalina\r\nPython version: 3.7.6\r\nPillow version: PIL 7.1.2\r\nKeras version: 2.3.1\r\n\r\n```\r\nclassifier.fit_generator(training_set,\r\nsteps_per_epoch = 914,\r\nepochs = 25,\r\nvalidation_data = test_set,\r\nvalidation_steps = 237)\r\n```\r\n![83977734-8134f300-a920-11ea-980d-6309847697c8](https://user-images.githubusercontent.com/59297399/85206340-fea82c80-b33e-11ea-8277-6708e7f6b554.png)\r\n", "comments": ["@anis-agwan,\r\nIn order to reproduce the issue reported here, could you please provide the complete code, the TensorFlow version and the dataset you are using. Thanks!", "I have used only Keras for the model training", "Complete code and the dataset is kind of personal so I can't actually show, just a bunch of chat screenshots.\r\n\r\n```\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Convolution2D\r\nfrom keras.layers import MaxPooling2D\r\nfrom keras.layers import Flatten\r\nfrom keras.layers import Dense\r\nfrom keras.callbacks import ModelCheckpoint\r\n\r\nclassifier = Sequential()\r\n\r\nclassifier.add(Convolution2D(32, 3, 3, input_shape = (64, 64, 3), activation = 'relu'))\r\nclassifier.add(MaxPooling2D(pool_size = (2, 2)))\r\n\r\nclassifier.add(Convolution2D(32, 3, 3, activation = 'relu'))\r\nclassifier.add(MaxPooling2D(pool_size = (2, 2)))\r\n\r\nclassifier.add(Flatten())\r\n\r\nclassifier.add(Dense(output_dim = 128, activation = 'relu'))\r\nclassifier.add(Dense(output_dim = 1, activation = 'sigmoid'))\r\n\r\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\r\n\r\nfrom keras.preprocessing.image import ImageDataGenerator\r\n\r\ntrain_datagen = ImageDataGenerator(\r\n        rescale = 1./255,\r\n        shear_range = 0.2,\r\n        zoom_range = 0.2,\r\n        horizontal_flip = True)\r\n\r\ntest_datagen = ImageDataGenerator(rescale=1./255)\r\n\r\ntraining_set = train_datagen.flow_from_directory(\r\n        'dataset/training_set',\r\n        target_size=(64, 64),\r\n        batch_size=32,\r\n        class_mode='binary')\r\n\r\ntest_set = test_datagen.flow_from_directory(\r\n        'dataset/test_set',\r\n        target_size=(64, 64),\r\n        batch_size=32,\r\n        class_mode='binary') \r\n\r\ncheckpoint = ModelCheckpoint(\"best_model.hdf5\", monitor='loss', verbose=1,\r\n    save_best_only=True, mode='auto', period=1)\r\n\r\nclassifier.fit_generator(\r\n        training_set,\r\n        steps_per_epoch=914,\r\n        epochs=10,\r\n        validation_data=test_set,\r\n        validation_steps=239,\r\n        callbacks=[checkpoint])\r\n\r\n\r\n\r\nclassifier.save(\"model2.h5\")\r\n```", "@anis-agwan,\r\nI was able to run the code without any issues with a sample dataset. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/4640d6115e0e5f668852eff867e38a7c/40637.ipynb). \r\n\r\nAlso, please make sure your folders are in the below structure. \r\n```\r\ndataset    \r\n\u2502\r\n\u2514\u2500\u2500\u2500training_set\r\n\u2502   \u2514\u2500\u2500\u2500abc\r\n\u2502   \u2514\u2500\u2500\u2500xyz\r\n\u2502   \r\n\u2514\u2500\u2500\u2500test_set\r\n    \u2514\u2500\u2500\u2500abc\r\n    \u2514\u2500\u2500\u2500xyz\r\n```\r\n\r\nThanks!", "@amahendrakar  Is it a possibility that my dataset is corrupted?\r\n", ">Is it a possibility that my dataset is corrupted?\r\n\r\n@anis-agwan,\r\nSeems like it. To confirm that it is not a TensorFlow issue, could you please check if you are able to run the code using a different dataset. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "I had a similar issue while I was using `model.predict` on 130,000 images. My script crashed after a few hours. \r\n\r\n```\r\ntest_datagen = ImageDataGenerator(rescale=1./255)\r\ntest_generator = test_datagen.flow_from_directory(\r\n    dataset_path,\r\n    classes=['test'],\r\n    target_size=(528, 528),\r\n    batch_size=BATCH_SIZE,\r\n    # don't generate labels\r\n    class_mode=None,\r\n    # don't shuffle\r\n    shuffle=False)\r\nprobabilities = model.predict(test_generator, steps=steps)\r\n```\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/murarda/.vscode/extensions/ms-python.python-2020.11.371526539/pythonFiles/lib/python/debugpy/__main__.py\", line 45, in <module>\r\n    cli.main()\r\n  File \"/home/murarda/.vscode/extensions/ms-python.python-2020.11.371526539/pythonFiles/lib/python/debugpy/../debugpy/server/cli.py\", line 430, in main\r\n    run()\r\n  File \"/home/murarda/.vscode/extensions/ms-python.python-2020.11.371526539/pythonFiles/lib/python/debugpy/../debugpy/server/cli.py\", line 267, in run_file\r\n    runpy.run_path(options.target, run_name=compat.force_str(\"__main__\"))\r\n  File \"/usr/lib/python3.6/runpy.py\", line 263, in run_path\r\n    pkg_name=pkg_name, script_name=fname)\r\n  File \"/usr/lib/python3.6/runpy.py\", line 96, in _run_module_code\r\n    mod_name, mod_spec, pkg_name, script_name)\r\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/var/www/img4drm/src/scripts/test_fs.py\", line 77, in <module>\r\n    probabilities = model.predict(test_generator, steps=steps)\r\n  File \"/srv/virtualenvs/smfr/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 130, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n  File \"/srv/virtualenvs/smfr/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 1579, in predict\r\n    steps_per_execution=self._steps_per_execution)\r\n  File \"/srv/virtualenvs/smfr/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py\", line 1117, in __init__\r\n    model=model)\r\n  File \"/srv/virtualenvs/smfr/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py\", line 916, in __init__\r\n    **kwargs)\r\n  File \"/srv/virtualenvs/smfr/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py\", line 786, in __init__\r\n    peek, x = self._peek_and_restore(x)\r\n  File \"/srv/virtualenvs/smfr/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py\", line 920, in _peek_and_restore\r\n    return x[0], x\r\n  File \"/srv/virtualenvs/smfr/lib/python3.6/site-packages/keras_preprocessing/image/iterator.py\", line 65, in __getitem__\r\n    return self._get_batches_of_transformed_samples(index_array)\r\n  File \"/srv/virtualenvs/smfr/lib/python3.6/site-packages/keras_preprocessing/image/iterator.py\", line 230, in _get_batches_of_transformed_samples\r\n    interpolation=self.interpolation)\r\n  File \"/srv/virtualenvs/smfr/lib/python3.6/site-packages/keras_preprocessing/image/utils.py\", line 114, in load_img\r\n    img = pil_image.open(io.BytesIO(f.read()))\r\n  File \"/srv/virtualenvs/smfr/lib/python3.6/site-packages/PIL/Image.py\", line 2818, in open\r\n    raise IOError(\"cannot identify image file %r\" % (filename if filename else fp))\r\nOSError: cannot identify image file <_io.BytesIO object at 0x7f80113590a0>\r\n```\r\n\r\nTo reproduce it, just create a test.txt file with few lines then rename it as test.jpg.\r\nIt is PIL that raises the exception, but it would be great to get the wrong filename.\r\n\r\nThanks.\r\n\r\nD\r\n"]}, {"number": 40636, "title": "Fix TypeError when sparse.from_dense is called with tf.string", "body": "This PR tries to fix the issue raised in #40633 where\r\nsparse.from_dense throws out TypeError with tf.string as input.\r\n\r\nThe issue was that from_dense uses `tf.constant(0, dtype)` to get\r\nthe zero value which fails on tf.string. This PR changes to `zeros_like`\r\nto work with tf.string.\r\n\r\nThis PR fixes #40633.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 40635, "title": "Loss function not accessible when overriding optimizer_v2.OptimizerV2", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWindows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nn/a\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\nTensorFlow version: 2.1.0\r\n- Python version:\r\nPython 3.6.10 |Anaconda, Inc.| (default, Mar 23 2020, 17:58:33) [MSC v.1916 64 bit (AMD64)]\r\n- Bazel version (if compiling from source):\r\nn/a\r\n- GCC/Compiler version (if compiling from source):\r\nn/a\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\nn/a\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n\r\n== tensorflow import ============================================\r\nTraceback (most recent call last):\r\n\r\n  File \"<string>\", line 1, in <module>\r\n\r\nModuleNotFoundError: No module named 'tensorflow'\r\n\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\ntf_env_collect.sh: line 145: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n\r\n== tensorflow installed from info ==================\r\n\r\n== python version  ==============================================\r\n(major, minor, micro, releaselevel, serial)\r\n(3, 8, 2, 'final', 0)\r\n\r\n\r\n== bazel version  ===============================================\r\n\r\n\r\n\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\nv2.1.0-rc2-17-ge5bf8de410 2.1.0\r\n\r\n**Describe the current behavior**\r\n\r\nThere is no access to the loss function within the methods that are allowed to be overridden in the optimizer_v2.OptimizerV2 class, i.e _resource_apply_dense(). The result is that weights do not change within during training. I have tried overriding _compute_gradients(),but it is never called. \r\n\r\nAs can be seen from the output at the last line, the weights do not change.\r\n\r\n**Describe the expected behavior**\r\n\r\nI want to be able to change the weights based upon the loss. In this case to change them randomly if the loss has not decreased. However, to do this one of the methods I am allowed to override should be passed the loss function.\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nhttps://colab.research.google.com/github/ruperty/colab/blob/master/optimizer_test.ipynb\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["I am able to replicate the issue, please find the [gist here.](https://colab.research.google.com/gist/Saduf2019/c0cb51b51cc736dbb1f1aab3cc49e31b/untitled240.ipynb)", "@ruperty \r\n\r\nIs this till an issue, could you please try in latest TF version [2.4 or nightly] and let us know.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40635\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40635\">No</a>\n"]}, {"number": 40634, "title": "Looping over tf.data.Dataset very slow compared to that of Numpy Array", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): N/A. Can be replicated in Google Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): N/A\r\n- TensorFlow version (use command below): 2.2 and tf-nightly\r\n- Python version: 3.6.7 (Google Colab)\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**: `For-Loop` over samples of a `tf.data.Dataset` is much slower (almost 500 times) than looping over the corresponding `numpy array`.\r\n\r\n**Describe the expected behavior**: Looping over `tf.data.Dataset` shouldn't take so long.\r\n\r\n**Standalone code to reproduce the issue**: This is the [Colab link](https://colab.research.google.com/gist/rakeshmothukuru1/c411d6de49f6cfb3d610d05c54a6d172/so_62453823.ipynb) to reproduce the issue.\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport time\r\n\r\nprint(tf.__version__)\r\n\r\na = np.ones(100000, dtype=np.float32)\r\n\r\nstart_time = time.time()\r\nfor x in a:\r\n    pass\r\nprint(time.time() - start_time)\r\n\r\nstart_time = time.time()\r\nfor x in tf.data.Dataset.from_tensor_slices(a):\r\n    pass\r\nprint(time.time() - start_time)\r\n```", "comments": ["Hi @rakeshmothukuru1, try wrapping your code in a @tf.function and the time should be almost the same for both loops. @tf.function will use autograph to transform your Python Eager code into graph-compatible TensorFlow ops.", "@rakeshmothukuru1 \r\n\r\nAs @nikitamaia suggested try wrapping your code in @tf.function and you will see better performance or almost same performance with tf.data.Dataset than Numpy array.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/00d036886c185865d5054ce43bee6370/untitled51.ipynb).Please, verify once and close this issue if you feel your query has been answered.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "I think that the code in colab is not really addressing the problem here. Looping over datasets is considerably slower than looping over numpy arrays (I can imagine the strength of datasets is not just on looping on arrays). \r\n\r\nIn esence, the print inside of the function is only executed at tracing, right? Is it really measuring the time spend in the loop? Or is it just measuring the time it takes to construct a part of the graph?\r\n\r\nThis issue should be open again. In TF2.4.1 looping over an array is at least 64 times slower in my machine, we now can use tf.timestamp to measure this at execution time (as long as the order of execution of the loop and the mark for the start time does not change when the graph is optimized.) In the worst case one needs to use a profiler to measure that.", "This code addresses different ways (not too different though) to check the performance, they all return similar times\r\nhttps://colab.research.google.com/drive/1NGQ-_f75s-8KG8elNZrGYqjtnyUsOfV4#scrollTo=1MZTQH_IumI7"]}, {"number": 40633, "title": "SparseTensor from dense conversion error when dtype tf.string", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): conda/binary\r\n- TensorFlow version (use command below): unknown 2.1.0\r\n- Python version: 3.7.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 10.2/7.6.5\r\n- GPU model and memory: GTX 1070 8GB\r\n\r\n**Describe the current behavior**\r\nFails to convert the Tensor to a SparseTensor representation\r\n\r\n**Describe the expected behavior**\r\nb should contain the SparseTensor representation of a\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\na = tf.constant(list(\"ababa\"))\r\nprint(a)\r\nb = tf.sparse.from_dense(a)\r\nprint(b)\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 264, in <module>\r\n    b = tf.sparse.from_dense(a)\r\n  File \"/home/alex/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/sparse_ops.py\", line 121, in from_dense\r\n    math_ops.not_equal(tensor, array_ops.constant(0, tensor.dtype)))\r\n  File \"/home/alex/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py\", line 258, in constant\r\n    allow_broadcast=True)\r\n  File \"/home/alex/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py\", line 266, in _constant_impl\r\n    t = convert_to_eager_tensor(value, ctx, dtype)\r\n  File \"/home/alex/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py\", line 96, in convert_to_eager_tensor\r\n    return ops.EagerTensor(value, ctx.device_name, dtype)\r\nTypeError: Cannot convert 0 to EagerTensor of dtype string\r\n", "comments": ["Added a PR #40636 for the fix.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40633\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40633\">No</a>\n"]}, {"number": 40632, "title": "Concatenating weights in Keras custom layer using `add_weight` fails while computing gradients", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): N/A. It can be reproduced in Google Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): N/A\r\n- TensorFlow version (use command below): 2.2.0 and tf-nightly\r\n- Python version: 3.6.7 (Google Colab)\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**: While writing a `custom layer`, I need to have multiple `weight` matrices `concatenated` together. If I do this in the `build function` using `tf.concatenate`, I get the error `ValueError: No gradients provided for any variable:...`, however, if I make a `list of weights` in `build` and `concatenate` them in `call` it works.\r\n\r\n**Describe the expected behavior**: It should work with `tf.concatenate` as well.\r\n\r\n**Standalone code to reproduce the issue**: This is the [Colab link](https://colab.research.google.com/gist/rakeshmothukuru1/90bcb59075b6b5e60d00ffb060270a29/so_62425584.ipynb) to reproduce the issue.\r\n\r\nProviding the code below as well.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.keras.optimizers import Adam\r\n\r\nprint(tf.__version__)\r\n\r\nclass MultiInputLinear(tf.keras.layers.Layer):\r\n    def __init__(self, output_dim=32, n_inputs=2):\r\n        super(MultiInputLinear, self).__init__()\r\n        self.output_dim = output_dim\r\n        self.n_inputs = n_inputs\r\n\r\n\r\n    def build(self, input_shapes):\r\n        self.input_dim = input_shapes[0][1]\r\n\r\n        self.W = tf.concat(\r\n            [\r\n                self.add_weight(\r\n                    name=f'W_{i}',\r\n                    shape=(self.input_dim, self.output_dim),\r\n                    initializer='random_normal',\r\n                    trainable=True\r\n                ) for i in range(self.n_inputs)\r\n            ], axis=0\r\n        )\r\n\r\n    def call(self, inputs):  \r\n        supports = tf.concat(inputs, axis=-1)        \r\n        return tf.matmul(supports, self.W)\r\n\r\nN = 100\r\nA = [np.random.normal(size=(N, N)) for _ in range(2)]\r\ny = np.random.binomial(1, .1, size=(N, 32))\r\n\r\nA_in = [tf.keras.layers.Input(batch_size=N, shape=(N, )) for _ in range(2)]\r\nY = MultiInputLinear(y.shape[1], 2)(A_in)\r\n\r\nmodel = tf.keras.models.Model(inputs=A_in, outputs=Y)\r\nmodel.compile(loss='categorical_crossentropy', optimizer=Adam())\r\n\r\nmodel.fit(A, y, batch_size=N)\r\n```\r\n**Working Code** :\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.keras.optimizers import Adam\r\n\r\nclass MultiInputLinear(tf.keras.layers.Layer):\r\n    def __init__(self, output_dim=32, n_inputs=2):\r\n        super(MultiInputLinear, self).__init__()\r\n        self.output_dim = output_dim\r\n        self.n_inputs = n_inputs\r\n\r\n\r\n    def build(self, input_shapes):\r\n        self.input_dim = input_shapes[0][1]\r\n\r\n        self.W_list = [\r\n                self.add_weight(\r\n                    name=f'W_{i}',\r\n                    shape=(self.input_dim, self.output_dim),\r\n                    initializer='random_normal',\r\n                    trainable=True\r\n                ) for i in range(self.n_inputs)\r\n            ]\r\n\r\n    def call(self, inputs):  \r\n        supports = tf.concat(inputs, axis=-1)\r\n        W = tf.concat(self.W_list, axis=0)\r\n\r\n        return tf.matmul(supports, W)\r\n\r\nN = 100\r\nA = [np.random.normal(size=(N, N)) for _ in range(2)]\r\ny = np.random.binomial(1, .1, size=(N, 32))\r\n\r\nA_in = [tf.keras.layers.Input(batch_size=N, shape=(N, )) for _ in range(2)]\r\nY = MultiInputLinear(y.shape[1], 2)(A_in)\r\n\r\nmodel = tf.keras.models.Model(inputs=A_in, outputs=Y)\r\nmodel.compile(loss='categorical_crossentropy', optimizer=Adam())\r\n\r\nmodel.fit(A, y, batch_size=N)\r\n```\r\n\r\n**Other info / logs** :     `ValueError: No gradients provided for any variable: ['multi_input_linear_4/W_0:0', 'multi_input_linear_4/W_1:0'].`\r\n", "comments": ["I am able to replicate this issue, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/31bf133f06118129d0da64a8456ccdcd/untitled238.ipynb)", "In this case, the tf.concat in the first build produces an eager tensor that is not tracked when assigned to the .W attribute, as build always runs in an eager context. The correct way to do this is to concat in the call(), as shown in the second example. In general, avoid computations that produce tensors in the build() method (though computations that produce Variables are okay).\r\n\r\nWe recognize that is a fine line, and hard to reason about as a user. We will add some documentation to the build method to try to explain this more completely.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40632\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40632\">No</a>\n"]}, {"number": 40631, "title": "@tf.function decorated functions cannot access class varaiables, tf.print() doesn't work", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Catalina 10.15.5\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version: 3.6.10\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: Training on CPU\r\n\r\n**Describe the current behavior**\r\n\r\nI am trying to create a Q Learning algorithm using TF2. I have this class variable called memory that keeps increasing in size as training goes on. Inside the @tf.function decorated weight update function, when I print the size of memory, it seems to be always 1 (doesn't seem to increase). Here are the different configurations with which I tried this code,\r\n\r\n1. With **tf.compat.v1.disable_eager_execution()** and **@tf.function** decorator\r\n    - The tf.print() call inside this decorated function doesn't work, so I am not sure if this is actually training the model or not\r\n2. With only **@tf.function** decorator,\r\n    - The tf.print() call inside the decorated function works, but the class variable size is always constant (1).\r\n3. With only **tf.compat.v1.disable_eager_execution()**,\r\n    - The code raises this error `TypeError: 'Tensor' object does not support item assignment`\r\n    - I tried to convert the Tensor into a numpy array, but it raises this error `'Tensor' object has no attribute 'numpy'`\r\n4. With neither **tf.compat.v1.disable_eager_execution()** nor **@tf.function** decorator,\r\n    - The code works as expected but is extremely slow (because of the eager execution I presume)\r\n\r\n**Describe the expected behavior**\r\n\r\n\r\n1. The **@tf.function** decorator to be able to handle class variables that change during runtime.\r\n2. The **tf.print()** calls to work even when **tf.compat.v1.disable_eager_execution()** is called\r\n\r\nAs listed above, **method 1** seems to work as there are no errors, but there is no way for me to check whether my model is actually training or not as the `tf.print()` call doesn't seem to work. Ideally, I want method 1 to work as this is the fastest form of execution that matches the speeds of a similar implementation in TF1.1\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport numpy as np\r\nimport random\r\nimport tqdm\r\nimport yfinance as yf\r\n\r\n\r\n# tf.compat.v1.disable_eager_execution()\r\nticker = \"AAPL\"\r\ndf_full = yf.Ticker(\"{}\".format(ticker)).history(\"max\").reset_index()\r\ndf = df_full.copy()[\"Close\"]\r\ndata = df.copy()\r\n\r\n\r\nclass DQN:\r\n    def __init__(self, data, lookBack=30,\r\n                 gamma=0.95, epsilon=0.5,\r\n                 epsilonMin=0.01, epsilonDecay=0.8,\r\n                 learningRate=0.001, money=10000):\r\n        # NOT IMPORTANT\r\n        self.lookBack = lookBack\r\n        self.initialMoney = money\r\n        self.actionSize = 3\r\n\r\n        self.data = data\r\n        self.gamma = gamma\r\n        self.epsilon = epsilon\r\n        self.epsilonMin = epsilonMin\r\n        self.epsilonDecay = epsilonDecay\r\n        self.learningRate = learningRate\r\n\r\n        self.memory = []\r\n\r\n    def buildModel(self):\r\n        keras.backend.clear_session()\r\n        model = keras.models.Sequential()\r\n        model.add(keras.layers.Dense(256, input_shape=[self.lookBack],\r\n                                     activation=\"relu\"))\r\n        model.add(keras.layers.Dense(self.actionSize))\r\n\r\n        self.optimizer = keras.optimizers.RMSprop(lr=self.learningRate,\r\n                                                  epsilon=0.1,\r\n                                                  rho=0.99)\r\n        self.lossFunc = keras.losses.mean_squared_error\r\n        model.compile(loss=\"mse\", optimizer=self.optimizer)\r\n        self.model = model\r\n\r\n    def getAction(self, state):\r\n        # NOT IMPORTANT\r\n        if random.random() <= self.epsilon:\r\n            return random.randrange(self.actionSize)\r\n        else:\r\n            return np.argmax(self.model.predict(state)[0])\r\n\r\n    def createDataset(self):\r\n        # NOT IMPORTANT\r\n        tmp = self.data.copy()\r\n        tmp = tmp.diff(1).dropna().values\r\n        shape = tmp.shape[:-1] + (tmp.shape[-1] - self.lookBack + 1,\r\n                                  self.lookBack)\r\n        strides = tmp.strides + (tmp.strides[-1],)\r\n        self.dataset = np.lib.stride_tricks.as_strided(tmp, shape=shape,\r\n                                                       strides=strides)\r\n\r\n    def getReward(self, action, currentPrice):\r\n        # NOT IMPORTANT\r\n        return 0\r\n\r\n    # @tf.function\r\n    def updateWeights(self):\r\n        # IMPORTANT\r\n        tf.print(len(self.memory))\r\n        if len(self.memory) >= self.batchSize:\r\n            endIndex = len(self.memory)\r\n            startIndex = endIndex - self.batchSize\r\n            batchData = []\r\n            for i in range(startIndex, endIndex):\r\n                batchData.append(self.memory[i])\r\n            X = np.zeros((self.batchSize, self.lookBack))\r\n            Y = np.zeros((self.batchSize, self.actionSize))\r\n            states = np.array([item[0] for item in batchData])\r\n            newStates = np.array([item[3] for item in batchData])\r\n            Q = self.model(states)\r\n            QNext = self.model(newStates)\r\n            for i in range(len(batchData)):\r\n                state, action, reward, nextState = batchData[i]\r\n                target = Q[i]\r\n                target[action] = reward\r\n                target[action] += self.gamma * np.max(QNext[i])\r\n\r\n                X[i] = state\r\n                Y[i] = target\r\n            self.model.train_on_batch(X, Y)\r\n            if self.epsilon > self.epsilonMin:\r\n                self.epsilon *= self.epsilonDecay\r\n\r\n    def train(self, epochs=200, logFreq=1):\r\n        # IMPORTANT\r\n        for epoch in range(epochs):\r\n            self.profit = 0\r\n            self.money = self.initialMoney\r\n            for timeStep in tqdm.tqdm(range(self.lookBack, len(self.data)-1)):\r\n                currentPrice = data[timeStep]\r\n                currentState = self.dataset[timeStep-self.lookBack]\r\n                nextState = self.dataset[timeStep-self.lookBack+1]\r\n\r\n                action = self.getAction(currentState.reshape(1, -1))\r\n\r\n                reward = self.getReward(action, currentPrice)\r\n\r\n                self.memory.append((currentState, action, reward, nextState))\r\n\r\n                self.updateWeights()\r\n\r\n\r\ntest = DQN(data)\r\ntest.createDataset()\r\ntest.buildModel()\r\ntest.train(100)\r\n```\r\n\r\nThe important functions are marked as so. Please take a look at those functions, all others are just helper functions for reproducibility.\r\n\r\n**Other info / logs** \r\n\r\n1. Entire traceback for **method 3**, \r\n\r\n```\r\n/Users/aakashsasikumar/.pyenv/versions/3.6.10/lib/python3.6/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\r\n  warnings.warn(msg)\r\nWARNING:tensorflow:From /Users/aakashsasikumar/.pyenv/versions/3.6.10/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nIf using Keras pass *_constraint arguments to layers.\r\n  0%|                                                                                                                        | 0/9933 [00:00<?, ?it/s]2020-06-20 12:13:31.271860: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-06-20 12:13:31.286799: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fa9662abf90 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-06-20 12:13:31.286826: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n  0%|                                                                                                                        | 0/9933 [00:00<?, ?it/s]\r\nTraceback (most recent call last):\r\n  File \"/Users/aakashsasikumar/Documents/Code/Python/StockMate/Misc/Benchmarks/QLearningAgentTF2.py\", line 122, in <module>\r\n    test.train(100)\r\n  File \"/Users/aakashsasikumar/Documents/Code/Python/StockMate/Misc/Benchmarks/QLearningAgentTF2.py\", line 116, in train\r\n    self.updateWeights()\r\n  File \"/Users/aakashsasikumar/Documents/Code/Python/StockMate/Misc/Benchmarks/QLearningAgentTF2.py\", line 91, in updateWeights\r\n    target[action] = reward\r\nTypeError: 'Tensor' object does not support item assignmenttext\r\n```\r\n2. Entire traceback for **method 3** when I try converting the tf.Tensor into a numpy array,\r\n```text\r\n/Users/aakashsasikumar/.pyenv/versions/3.6.10/lib/python3.6/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\r\n  warnings.warn(msg)\r\nWARNING:tensorflow:From /Users/aakashsasikumar/.pyenv/versions/3.6.10/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nIf using Keras pass *_constraint arguments to layers.\r\n  0%|                                                                                                                        | 0/9933 [00:00<?, ?it/s]2020-06-20 12:15:24.714805: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-06-20 12:15:24.728233: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fbdd92625a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-06-20 12:15:24.728260: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n  0%|                                                                                                                        | 0/9933 [00:00<?, ?it/s]\r\nTraceback (most recent call last):\r\n  File \"/Users/aakashsasikumar/Documents/Code/Python/StockMate/Misc/Benchmarks/QLearningAgentTF2.py\", line 122, in <module>\r\n    test.train(100)\r\n  File \"/Users/aakashsasikumar/Documents/Code/Python/StockMate/Misc/Benchmarks/QLearningAgentTF2.py\", line 116, in train\r\n    self.updateWeights()\r\n  File \"/Users/aakashsasikumar/Documents/Code/Python/StockMate/Misc/Benchmarks/QLearningAgentTF2.py\", line 90, in updateWeights\r\n    target = Q[i].numpy()\r\nAttributeError: 'Tensor' object has no attribute 'numpy'\r\n```\r\n", "comments": ["@AakashSasikumar \r\n\r\nI have tried in colab with TF version 2.2 and i am seeing the below error message.(`AttributeError: 'DQN' object has no attribute 'batchSize'`).Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/b3ad60005941bf9a96aa5a392c17a23b/untitled50.ipynb).Thanks!", "Sorry about that. This should fix it.\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport numpy as np\r\nimport random\r\nimport tqdm\r\nimport yfinance as yf\r\n\r\n\r\ntf.compat.v1.disable_eager_execution()\r\nticker = \"AAPL\"\r\ndf_full = yf.Ticker(\"{}\".format(ticker)).history(\"max\").reset_index()\r\ndf = df_full.copy()[\"Close\"]\r\ndata = df.copy()\r\n\r\n\r\nclass DQN:\r\n    def __init__(self, data, lookBack=30,\r\n                 gamma=0.95, epsilon=0.5,\r\n                 epsilonMin=0.01, epsilonDecay=0.8,\r\n                 learningRate=0.001, money=10000):\r\n        # NOT IMPORTANT\r\n        self.lookBack = lookBack\r\n        self.initialMoney = money\r\n        self.actionSize = 3\r\n        self.batchSize = 32\r\n\r\n        self.data = data\r\n        self.gamma = gamma\r\n        self.epsilon = epsilon\r\n        self.epsilonMin = epsilonMin\r\n        self.epsilonDecay = epsilonDecay\r\n        self.learningRate = learningRate\r\n\r\n        self.memory = []\r\n\r\n    def buildModel(self):\r\n        keras.backend.clear_session()\r\n        model = keras.models.Sequential()\r\n        model.add(keras.layers.Dense(256, input_shape=[self.lookBack],\r\n                                     activation=\"relu\"))\r\n        model.add(keras.layers.Dense(self.actionSize))\r\n\r\n        self.optimizer = keras.optimizers.RMSprop(lr=self.learningRate,\r\n                                                  epsilon=0.1,\r\n                                                  rho=0.99)\r\n        self.lossFunc = keras.losses.mean_squared_error\r\n        model.compile(loss=\"mse\", optimizer=self.optimizer)\r\n        self.model = model\r\n\r\n    def getAction(self, state):\r\n        # NOT IMPORTANT\r\n        if random.random() <= self.epsilon:\r\n            return random.randrange(self.actionSize)\r\n        else:\r\n            return np.argmax(self.model.predict(state)[0])\r\n\r\n    def createDataset(self):\r\n        # NOT IMPORTANT\r\n        tmp = self.data.copy()\r\n        tmp = tmp.diff(1).dropna().values\r\n        shape = tmp.shape[:-1] + (tmp.shape[-1] - self.lookBack + 1,\r\n                                  self.lookBack)\r\n        strides = tmp.strides + (tmp.strides[-1],)\r\n        self.dataset = np.lib.stride_tricks.as_strided(tmp, shape=shape,\r\n                                                       strides=strides)\r\n\r\n    def getReward(self, action, currentPrice):\r\n        # NOT IMPORTANT\r\n        return 0\r\n\r\n    @tf.function\r\n    def updateWeights(self):\r\n        # IMPORTANT\r\n        tf.print(len(self.memory))\r\n        if len(self.memory) >= self.batchSize:\r\n            endIndex = len(self.memory)\r\n            startIndex = endIndex - self.batchSize\r\n            batchData = []\r\n            for i in range(startIndex, endIndex):\r\n                batchData.append(self.memory[i])\r\n            X = np.zeros((self.batchSize, self.lookBack))\r\n            Y = np.zeros((self.batchSize, self.actionSize))\r\n            states = np.array([item[0] for item in batchData])\r\n            newStates = np.array([item[3] for item in batchData])\r\n            Q = self.model(states)\r\n            QNext = self.model(newStates)\r\n            for i in range(len(batchData)):\r\n                state, action, reward, nextState = batchData[i]\r\n                target = Q[i]\r\n                target[action] = reward\r\n                target[action] += self.gamma * np.max(QNext[i])\r\n\r\n                X[i] = state\r\n                Y[i] = target\r\n            self.model.train_on_batch(X, Y)\r\n            if self.epsilon > self.epsilonMin:\r\n                self.epsilon *= self.epsilonDecay\r\n\r\n    def train(self, epochs=200, logFreq=1):\r\n        # IMPORTANT\r\n        for epoch in range(epochs):\r\n            self.profit = 0\r\n            self.money = self.initialMoney\r\n            for timeStep in tqdm.tqdm(range(self.lookBack, len(self.data)-1)):\r\n                currentPrice = data[timeStep]\r\n                currentState = self.dataset[timeStep-self.lookBack]\r\n                nextState = self.dataset[timeStep-self.lookBack+1]\r\n\r\n                action = self.getAction(currentState.reshape(1, -1))\r\n\r\n                reward = self.getReward(action, currentPrice)\r\n\r\n                self.memory.append((currentState, action, reward, nextState))\r\n\r\n                self.updateWeights()\r\n\r\n\r\ntest = DQN(data)\r\ntest.createDataset()\r\ntest.buildModel()\r\ntest.train(100)\r\n```", "@AakashSasikumar \r\n\r\nI have tried in colab with TF version 2.2 .Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/45b4bed9448d501d4bb87ba9a00cd37f/untitled55.ipynb).You are also seeing the same behavior?.Thanks!", "@ravikyram \r\n\r\nFor 3 out of the 4 methods I am getting the same behavior. But for the second one, where you commented out `tf.compat.v1.disable_eager_execution()` and left `@tf.function` as it is, the `tf.print()` function works. Here is my output,\r\n\r\n```\r\n  0%|                                       | 0/9935 [00:00<?, ?it/s]1\r\n  0%|                             | 1/9935 [00:00<1:10:16,  2.36it/s]1\r\n1\r\n1\r\n1\r\n1\r\n1\r\n  0%|                               | 7/9935 [00:00<49:59,  3.31it/s]1\r\n1\r\n1\r\n1\r\n1\r\n1\r\n1\r\n1\r\n1\r\n1\r\n1\r\n1\r\n1\r\n1\r\n  0%|                              | 21/9935 [00:00<35:22,  4.67it/s]1\r\n1\r\n1\r\n1\r\n1\r\n1\r\n1\r\n1\r\n1\r\n1\r\n1\r\n1\r\n1\r\n1\r\n1\r\n  0%|                              | 36/9935 [00:00<25:05,  6.58it/s]1\r\n1\r\n1\r\n1\r\n1\r\n1\r\n1\r\n1\r\n1\r\n1\r\n1\r\n1\r\n  0%|\u258f                             | 48/9935 [00:00<18:00,  9.15it/s]1\r\n1\r\n1\r\n1\r\n1\r\n1\r\n1\r\n1\r\n1\r\n  1%|\u258f                             | 57/9935 [00:00<13:10, 12.49it/s]1\r\n1\r\n1\r\n1\r\n1\r\n1\r\n1\r\n1\r\n1\r\n  1%|\u258f                             | 66/9935 [00:01<09:47, 16.79it/s]\r\n\r\n```\r\n\r\nAlso, I stopped the code before it finished, that's why the output is much smaller. In fact, I would suggest you do the same, it doesn't need to train for 100 epochs. When you are running it, just modify the last line from `test.train(100)` to `test.train(5)`.\r\n\r\nAnother thing to note is that I am not running this on google colab, I am running it locally in my terminal. Would that make any difference?", "Also, here are a few warnings that I get when tensorflow is imported. Not sure if its gonna help, but I'll add it nonetheless.\r\n\r\n```\r\n2020-06-24 17:40:15.940898: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-06-24 17:40:15.952995: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fac845b3a00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-06-24 17:40:15.953014: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n```", "Option 2 is definitely the recommended method, you should not need to disable eager execution.\r\n\r\nGlancing over the code, a few things that tf.function definitely doesn't work well with, so the output of tf.print is expected:\r\n * self.memory is a Python list; you should use a Tensor or RaggedTensor instead\r\n * tf.function doesn't monitor class members for retracing; you should instead design updateWeights using a functional style - pass all inputs as arguments, and return all calculated values\r\n * mutations, like X[i] = state are still not supported; for this case, I think accumulating the values using a TensorArray is better\r\n\r\nI recommend reading this guide to get a better intuition on how tf.function works: https://www.tensorflow.org/guide/function\r\nThe AutoGraph guide also contains a bit more information on how is the tf.function code transformed into a TF graph: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/index.md\r\n", "@mdanatg \r\nThank you, I will take a look into TensorArrays and making updateWeights() functional.\r\n\r\nThe thing is, Option 1(where eager mode is disabled) is much faster for me, nearly 10 times faster than Option 2. Is it safe for me to assume that Option 1 works just as Option 2 provided the inputs are same? If not, then I would have to go back to using TF1.0, as this is the only other way I'm getting similar speeds as Option 1.", "Yes, the expectation is that Option 1 has the same output as Option 2. But without eager execution, Option 1 just builds a graph and never runs it - you should not see any tf.print outputs at all, as you'd need all sorts of control dependencies sessions and initializers boilerplate - which explains the apparent speed-up. Using TF 2 is definitely recommended because it's easier to use and has much better support, and we can still optimize the slow parts with tf.function.", "@mdanatg \r\nI think I have a little better understanding of how tf.function works. I looked through some of the documentation and tutorials, but I haven't seen any guide on how to modify a tensor inside a tf.function. I tried converting it into a tf.Variable() but there was an error saying `tf.function-decorated function tried to create variables on non-first call`. After some searching around, I realized this was a done to limit the number of variables that are created. But what is the recommended way to manipulate a tensor inside a tf.function.\r\n\r\nI modified my code so that the below part is a function decorated with tf.function.\r\n```python\r\nX = tf.TensorArray(tf.float32, size=0, dynamic_size=True, clear_after_read=False)\r\nY = tf.TensorArray(tf.float32, size=0, dynamic_size=True, clear_after_read=False)\r\nwith tf.GradientTape() as tape:\r\n    Q = self.model(states)\r\n    QNext = self.model(newStates)\r\n    for i in range(len(states)):\r\n        state, action, reward, _ = states[i], actions[i], rewards[i], newStates[i]\r\n        target = tf.Variable(Q[i], name=\"temp\")\r\n        nextQ = QNext[i][tf.math.argmax(QNext[i])]\r\n        newVal = reward + self.gamma * nextQ\r\n        target[action].assign(newVal)\r\n\r\n        X.write(i, state)\r\n        Y.write(i, target)\r\n    loss = self.lossFunc(self.model(X.stack()), Y.stack())\r\n    grads = tape.gradient(loss, self.model.trainable_weights)\r\n    self.optimizer.apply_gradients(zip(grads, self.model.trainable_weights))\r\n```\r\nAs you can see above, I want to calculate a value with an output from the model. Do I have to further split these into smaller functions and decorate them with tf.functions?", "Modifying tensors is one of the pain points we're currently looking at. `Variable` is definitely not recommended, because operations on `Variable` are not differentiable.\r\n\r\nIn the meantime, I recommend using `scatter_nd_update`, which would approximately look like this:\r\n\r\n```\r\ntarget = Q[i]\r\ntarget = tf.tensor_scatter_nd_update(target, [[action]], [newval]])\r\n```\r\nThis might appear as if it creates unnecessary copies, but the TF runtime can optimize those away.\r\n\r\nAnother option is to use a local TensorArray, along with `unstack()`, `write()`, `stack()`, but that is not as nice and less efficient, too.", "Alright. I think we can go ahead and close this issue now. Thanks a lot", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40631\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40631\">No</a>\n"]}, {"number": 40630, "title": "In model.fit, have each metric given a new line in display. ", "body": "\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nInstead of having each metric on one line in the display for model.fit, have each metric on a new line.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nThose who are trying to monitor a ton of metrics. In my particular case, I am trying to track down the cause of exploding gradients, so I'm monitoring the norm of each trainable_weight. This makes for a very long line to side-scroll through. \r\n\r\n", "comments": ["@Santosh-Gupta \r\nYou might want to consider alternative ways to monitor learning process. Depending on your needs you could save the return value of the fit method ('history') as a csv. Or just passing to plotting method of your choice. You could also use tensorboard to plot the training metrics and analyze the weights. "]}, {"number": 40629, "title": "python sequence example context_features  Unsupported FixedLenSequenceFeature", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):   tf 2.2\r\n- Are you willing to contribute it (Yes/No):\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n   */miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/parsing_ops.py:548 parse_sequence_example  **\r\n        context_features, [VarLenFeature, FixedLenFeature, RaggedFeature])\r\n    /data1/arc_seven/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/parsing_config.py:461 from_features\r\n        (type(feature).__name__, feature))\r\n\r\n    ValueError: Unsupported FixedLenSequenceFeature FixedLenSequenceFeature(shape=(), dtype=tf.float32, allow_missing=-1, default_value=None).\r\n\r\n\r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\n\r\n**Any Other info.**\r\n", "comments": ["when the tf.data sequence example context_features support FixedLenSequenceFeature", "@darrenwang00 Can you please share the code for us to reproduce this issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "CLosing this issue as it has been awaiting response for more than 2 weeks. Please add additional comments for us to open this issue again. Thanks!"]}]