[{"number": 15526, "title": "Branch 179628764", "body": "Push", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@caisq could you take a look at the failure in `tensorflow/python/debug/wrappers/grpc_wrapper.py`? Looks like python3 does not have `sys.maxint`. Thanks!", "@yifeif will do, it's interesting that the Kokoro tests didn't catch it. Might be a mac+python3 specific thing.", "@tensorflow-jenkins test this please", "@yifeif, thanks for the attempt to fix the sys.maxint related failures. I think you need to remove the `import sys` lines from both files as well. Otherwise there will be linter errors.", "@yifeif Ah - never mind about `import sys`: they were already there for other purposes."]}, {"number": 15525, "title": "Op type not registered HashTableV2 error while deploying model in cloud ml", "body": "Problem while deploying model in Tensorflow 1.4.\r\n\r\n**Code :**\r\n\r\n```python\r\ndef model_fn(features, labels, mode):\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        tf.keras.backend.set_learning_phase(True)\r\n    else:\r\n        tf.keras.backend.set_learning_phase(False)\r\n\r\n    input_feature = features['x']\r\n    table = lookup.index_table_from_file(vocabulary_file='vocab.txt', num_oov_buckets=1, default_value=-1)\r\n    text = tf.squeeze(input_feature, [1])\r\n    words = tf.string_split(text)\r\n    dense_words = tf.sparse_tensor_to_dense(words, default_value=PADWORD)\r\n    numbers = table.lookup(dense_words)\r\n    padding = tf.constant([[0, 0], [0, MAX_LEN]])\r\n    padded = tf.pad(numbers, padding)\r\n    sliced = tf.slice(padded, [0, 0], [-1, MAX_LEN])\r\n    print('words_sliced={}'.format(words))\r\n\r\n    embeds = tf.keras.layers.Embedding(MAX_FEATURES+1, 128, input_length=MAX_LEN)(sliced)\r\n\r\n    print('words_embed={}'.format(embeds))\r\n    f1 = tf.keras.layers.Dropout(0.2)(embeds)\r\n    f1 = tf.keras.layers.Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1)(f1)\r\n    f1 = tf.keras.layers.GlobalAveragePooling1D()(f1)\r\n    f1 = tf.keras.layers.Dense(hidden_dims)(f1)\r\n    f1 = tf.keras.layers.Dropout(0.5)(f1)\r\n    f1 = tf.keras.layers.Activation('relu')(f1)\r\n    logits = tf.keras.layers.Dense(11)(f1)\r\n\r\n    predictions_dict = {\r\n        'class': tf.argmax(logits, 1),\r\n        'prob': tf.nn.softmax(logits)\r\n    }\r\n\r\n    '''prediction_output = tf.estimator.export.PredictOutput({\"classes\": tf.argmax(input=logits, axis=1),\r\n                                                           \"probabilities\": tf.nn.softmax(logits,\r\n                                                                                          name=\"softmax_tensor\")})'''\r\n\r\n    if mode == tf.estimator.ModeKeys.PREDICT:\r\n        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions_dict, export_outputs={\r\n            'prediction': tf.estimator.export.PredictOutput(predictions_dict)\r\n        })\r\n\r\n    loss = tf.losses.sparse_softmax_cross_entropy(labels, logits=logits)\r\n\r\n    if mode == tf.contrib.learn.ModeKeys.TRAIN:\r\n        train_op = tf.contrib.layers.optimize_loss(loss, tf.contrib.framework.get_global_step(), optimizer='Adam',\r\n                                                   learning_rate=0.001)\r\n        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\r\n\r\n    eval_metrics_ops = {\r\n        'accuracy': tf.metrics.accuracy(labels=labels, predictions=predictions_dict['class']),\r\n        'precision': tf.metrics.precision(labels=labels, predictions=predictions_dict['class']),\r\n        'recall': tf.metrics.recall(labels=labels, predictions=predictions_dict['class'])\r\n    }\r\n    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metrics_ops)\r\n\r\ndef get_train_record(record):\r\n    vector = tf.decode_csv(record, DEFAULTS, use_quote_delim=True)\r\n    return vector[1:], vector[0]\r\n\r\ndef preprocess(text):\r\n    text = text.lower()\r\n    result = ' '.join([word for word in text.split() if word not in (stop_words)])\r\n    return result\r\n\r\n\r\ndef build_vocab(file_name, vocab_file_name):\r\n    df = pd.read_csv(file_name, header=None, sep=',', skiprows=[1], names=['product', 'consumer_complaint_narrative'])\r\n    df['consumer_complaint_narrative'] = df['consumer_complaint_narrative'].apply(preprocess)\r\n    print(df['consumer_complaint_narrative'][0])\r\n    vocab_processor = tflearn.preprocessing.VocabularyProcessor(max_document_length=MAX_FEATURES, min_frequency=10,\r\n                                                                tokenizer_fn=tflearn.preprocessing.tokenizer)\r\n    vocab_processor.fit(df['consumer_complaint_narrative'])\r\n    with gfile.Open(vocab_file_name, 'wb') as f:\r\n        f.write(\"{}\\n\".format(PADWORD))\r\n        for word, index in vocab_processor.vocabulary_._mapping.items():\r\n            f.write(\"{}\\n\".format(word))\r\n    nwords = len(vocab_processor.vocabulary_)\r\n    print('{} words into {}'.format(nwords, vocab_file_name))\r\n\r\n\r\ndef input_fn(file_name, batch_size, repeat_count, shuffle=False):\r\n    def _input_fn():\r\n        data_set = tf.data.TextLineDataset(filenames=file_name)\r\n        data_set = data_set.map(get_train_record)\r\n        if shuffle:\r\n            data_set = data_set.shuffle(shuffle)\r\n        data_set = data_set.repeat(repeat_count)\r\n        batch = data_set.batch(batch_size)\r\n        iterator = batch.make_one_shot_iterator()\r\n        features, labels = iterator.get_next()\r\n        return {'x': features}, labels\r\n\r\n    return _input_fn()\r\n\r\n\r\ndef get_train_spec(file_name, batch_size, repeat_count):\r\n    return tf.estimator.TrainSpec(input_fn=lambda: input_fn(file_name, batch_size, repeat_count, shuffle=True), max_steps=1000)\r\n\r\n\r\ndef get_test_spec(file_name, batch_size, repeat_count=1):\r\n    return tf.estimator.EvalSpec(input_fn=lambda: input_fn(file_name, batch_size, repeat_count, shuffle=True))\r\n\r\n\r\ndef serving_input_fn():\r\n    feature_tensor = tf.placeholder(tf.string, [None])\r\n    # features = tf.py_func(preprocess, [feature_tensor], tf.string)\r\n    features = tf.expand_dims(feature_tensor, -1)\r\n    return tf.estimator.export.ServingInputReceiver({'x': features}, {'x': features})\r\n\r\nfinance_classifier = tf.estimator.Estimator(model_fn=model_fn, model_dir=model_dir)\r\n\r\nprint('\\n Training .....')\r\nfinance_classifier.train(input_fn=lambda: input_fn('dataset/train.csv', batch_size, repeat_count=5, shuffle=True))\r\n\r\nprint('\\n Evaluating.....')\r\neval_results = finance_classifier.evaluate(input_fn=lambda: input_fn('dataset/valid.csv', batch_size, repeat_count=1,\r\n                                                                  shuffle=False))\r\nfor key in eval_results:\r\n    print(\" {} was {}\".format(key, eval_results[key]))\r\n\r\nprint('\\n Exporting')\r\nexported_model_dir = finance_classifier.export_savedmodel(model_dir, serving_input_receiver_fn=serving_input_fn)\r\ndecoded_model_dir = exported_model_dir.decode(\"utf-8\")\r\n```\r\n\r\n[Screenshot](https://drive.google.com/open?id=1FAmoo9zCBJBAG2IFdySb_r4s1OV6YExn)\r\n\r\nBut when I tried with tf1.2 with some changes in the code in model_fn. Basically not using tf.keras but using tf.contrib.keras it was working. **is this bug ?**", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "@jart I posted the question [here](https://stackoverflow.com/questions/47917183/op-type-not-registered-hashtablev2-in-tensorflow-1-4-1-while-deploying-in-cloud)", "Hey @kishorenayar , Getting the same error...\r\ndid you find any solution...", "@visshvesh, I will let you know once I found the solution. Please do the same. The ML Engine supports TensorFlow 1.4 but the default runtime version is TensorFlow 1.2. So we need to set the version to 1.4 and check again. I will retest it and let you know soon.", "@visshvesh use the following command in cloud-ml when deploying the model\r\n\r\n**gcloud ml-engine versions create ${MODEL_VERSION} --model=${MODEL_NAME} --origin=${MODEL_BINARIES} --runtime-version=1.4**"]}, {"number": 15524, "title": "Fix Golang readme installation instruction formatting", "body": "This looks like it got pasted in incorrectly", "comments": ["Can one of the admins verify this patch?"]}, {"number": 15523, "title": "Clear the decref cache after calls to py_func to ensure no leaks", "body": "Fixes #15518", "comments": []}, {"number": 15522, "title": "line 123", "body": "buffer failed to be initialized when data_index== len(data) (at the end of file it basically failed to go to the top) and gives the following error << TypeError: sequence index must be integer, not 'slice'\r\n>> one work around is to replace line 123 with << buffer.extend(data[:span]) >> ", "comments": ["Please fill out the issue template with all required information or we won't be able to help.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing because the issue template has not been filled out."]}, {"number": 15521, "title": "Bug/Feature: constant_folding FP16 ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nRHEL 7\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n- **TensorFlow version (use command below)**:\r\n('v1.3.0-rc1-6207-ge210cb1', '1.4.0')\r\n- **Python version**: \r\npython2.7\r\n- **Bazel version (if compiling from source)**:\r\nbazel 0.9.0\r\n- **GCC/Compiler version (if compiling from source)**:\r\ngcc 4.8\r\n- **Exact command to reproduce**:\r\n\r\npython tensorflow/models/tutorial/images/convolutional.py --use_fp16\r\n\r\n### Describe the problem\r\nWhen using FP16 in e.g. the case listed above, the following error occurs:\r\nE tensorflow/core/grappler/optimizers/constant_folding.cc:1272] Unexpected type half\r\nE tensorflow/core/grappler/optimizers/constant_folding.cc:1242] Unexpected type half\r\n\r\nWhen looking into constant_folding.cc, i found out FP16 support exists at given lines, but is commented out.\r\nWhy is this not yet included in Tensorflow?\r\n\r\nAs far as I can see, this is more of a feature request than a bug report, since these code lines simply check if computational effort can be reduced (if the matrix is zero or one)\r\n\r\nAlso when using above command, the model trains until step 1100, then the learning rate drops to 0 (a known FP16 problem). Still it's annoying to encounter this in an official tutorial file.\r\n\r\n", "comments": ["@rmlarsen Any particular reason DT_HALF was left commented-out in constant_folding.cc? cl/177491247", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "@KillPinguin thank for bringing this to our attention.\r\n@jart The code doesn't compile with fp16. We would welcome a PR adding support. In the meantime, I'll change the code just skip the optimization without spamming the error log. It is at worst a missed optimization, but certainly not an error.", "The change was submitted internally and should be visible on github within a day or so."]}, {"number": 15520, "title": "ar", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 15519, "title": "fix typos", "body": "fix typos", "comments": ["Can one of the admins verify this patch?"]}, {"number": 15518, "title": "Possible memory leak with tf.py_func() with distributed Tensorflow?", "body": "When running Tensorflow as an distributed process to provide data with tf.data, it gradually consumes more and more memory, and finally consumes all memory of the system.\r\n\r\nScripts to reproduce:\r\nWe use a dummy dataset which produce [128, 28, 28, 1] tensors. \r\nCase1: Without distribute, which works fine, it will only consume 429Mb memory, no matter how many batches we run.\r\nCodes in `test1.py`:\r\n```\r\n#test1.py\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tqdm import tqdm\r\n\r\ndef dataset_generator():\r\n    while True:\r\n        yield np.random.uniform(size=[28, 28, 1]).astype(np.float32)\r\ndataset = tf.data.Dataset.from_generator(dataset_generator, tf.float32)\r\ndataset = dataset.batch(128)\r\nvalue = dataset.make_one_shot_iterator().get_next()\r\n\r\nsess = tf.Session()\r\nfor _ in tqdm(range(100000), ascii=True):\r\n    sess.run(value)\r\n```\r\n\r\nCase: With distribute, it will consumes more and more memory while running more and more batches. It consumes 10+Gb with less than 1M batches. Use the following two commands in two processes to run the `test2.py`:\r\n```\r\nCUDA_VISIBLE_DEVICES=\"\" python test2.py dataset\r\nCUDA_VISIBLE_DEVICES=\"\" python test2.py test\r\n```\r\nCodes in `test2.py`\r\n```\r\n# test2.py\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tqdm import tqdm\r\nimport sys\r\ndef main(role):\r\n    def dataset_generator():\r\n        while True:\r\n            yield np.random.uniform(size=[28, 28, 1]).astype(np.float32)\r\n    cluster = tf.train.ClusterSpec({'dataset': ['localhost:2001'], 'test': ['localhost:2002']})\r\n    if role == 'dataset':\r\n        server = tf.train.Server(cluster, 'dataset', 0)\r\n    elif role == 'test':\r\n        server = tf.train.Server(cluster, 'test', 0)\r\n    else:\r\n        raise ValueError(\"Uknown role {}.\".format(role))\r\n    with tf.device('/job:dataset/task:0')    :\r\n        dataset = tf.data.Dataset.from_generator(dataset_generator, tf.float32)\r\n        dataset = dataset.batch(128)\r\n        value = dataset.make_one_shot_iterator().get_next()\r\n    if role == 'dataset':\r\n        server.join()\r\n    elif role == 'test':\r\n        sess = tf.Session(target=server.target)\r\n        for _ in tqdm(range(100000000), ascii=True):\r\n            sess.run(value)\r\n            \r\nif __name__ == \"__main__\":\r\n    main(sys.argv[1])\r\n```\r\n\r\nTensorflow: v1.4.0-rc1-11-g130a514 1.4.0\r\nOS: ubuntu mate 16.04.1\r\nPython: 3.6.1 (conda 4.3.30)\r\n\r\n\r\n", "comments": ["I think this is an issue in `tf.py_func()` (which `Dataset.from_generator()` uses internally). The following program exhibits the same memory leak:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tqdm import tqdm\r\nimport sys\r\ndef main(role):\r\n    cluster = tf.train.ClusterSpec({'dataset': ['localhost:2001'],\r\n                                    'test': ['localhost:2002']})\r\n    if role == 'dataset':\r\n        server = tf.train.Server(cluster, 'dataset', 0)\r\n    elif role == 'test':\r\n        server = tf.train.Server(cluster, 'test', 0)\r\n    else:\r\n        raise ValueError(\"Uknown role {}.\".format(role))\r\n\r\n    with tf.device('/job:dataset/task:0'):\r\n        result = tf.py_func(\r\n            lambda: np.random.uniform(size=[28, 28, 1]).astype(np.float32),\r\n            inp=[], Tout=tf.float32)\r\n\r\n    if role == 'dataset':\r\n        server.join()\r\n    elif role == 'test':\r\n        sess = tf.Session(target=server.target)\r\n        for _ in tqdm(range(100000000), ascii=True):\r\n            sess.run(result)\r\n            \r\nif __name__ == \"__main__\":\r\n    main(sys.argv[1])\r\n```\r\n\r\nNote that the memory leak is in the `\"/job:dataset/task:0\"` process (which executes the `tf.py_func()` op) and not the `\"/job:test/task:0\"` process (which creates the session). Also note that this setup isn't intended to be supported, but I suppose the note in  [the `tf.py_func()` docs](https://www.tensorflow.org/api_docs/python/tf/py_func) could be read as allowing it:\r\n\r\n> The operation must run in the same address space as the Python program that calls `tf.py_func()`. If you are using distributed TensorFlow, you must run a `tf.train.Server` in the same process as the program that calls `tf.py_func()` and you must pin the created operation to a device in that server (e.g. using with tf.device():).\r\n\r\n(In a sense, the program \"gets lucky\" because both processes call `tf.py_func()` in the same order, and so the same identifier is used for the registered Python function in each process.)\r\n\r\nI think the source of the leak is the \"decref cache\", which holds references to Python arrays that are passed without copying into the TensorFlow runtime, and can only be cleared when the GIL is held. The [`ClearDecrefCache()`](https://github.com/tensorflow/tensorflow/blob/810394550571c5feb333cb6da66afb4b20c3bd85/tensorflow/python/lib/core/ndarray_tensor_bridge.cc#L52) function is only called in the session code (and some TF Eager code): for example at the [beginning](https://github.com/tensorflow/tensorflow/blob/810394550571c5feb333cb6da66afb4b20c3bd85/tensorflow/python/client/tf_session_helper.cc#L88) and [end](https://github.com/tensorflow/tensorflow/blob/810394550571c5feb333cb6da66afb4b20c3bd85/tensorflow/python/client/tf_session_helper.cc#L151) of a `sess.run()` call. Since the cache is filling up in `\"/job:dataset/task:0\"` and it is only being cleared in `\"/job:test/task:0\"`, we see a memory leak in `\"/job:dataset/task:0\"`.\r\n\r\nI'll assign this to @alextp, who added this mechanism, and might be able to suggest a fix or workaround.", "It's easy to clear the decref cache while we hold the gil to call the py_func, so I'll add this and hopefully this bug will go away. Is tqdm necessary for this bug to happen or are you using it just for styling?\r\n", "AFAICT it's just for styling!"]}, {"number": 15517, "title": "fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h'", "body": "I am new to tensorflow, when I compile tensorfolw with VS2015 in Windows 7, I got the following error, could anyone help on this, thanks!\r\nfatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h'", "comments": ["1. Please fill the form. Especially, how did you run cmake\r\n2. Apparently, you are using the wrong configuration.", "I downloaded and installed Anaconda5, git, Swig 3.0.12, tensorflow source code and then generated VS sln file with cmake GUI(I didn't use command line tool). what should I do, if with the command line?\r\nThanks!\r\n", "I suggest you run cmake from commandline. Please read the README.md file under tensorflow/contrib/cmake dir carefully, make sure you set all the options correctly ", "@snnn Thanks for your help,  I followed the readme file and made some progress, but I got new problem like following:\r\n\r\n \"fatal: unable to access 'https://boringssl.googlesource.com/boringssl/': Fail\r\n ed to connect to boringssl.googlesource.com port 443: Timed out\r\n fatal: clone of 'https://boringssl.googlesource.com/boringssl' into submodule\r\n  path 'D:/tensorflow/tensorflow/contrib/cmake/build/grpc/src/grpc/third_party\r\n /boringssl-with-bazel' failed\"\r\n\r\nI checked the google and fond the same Issue #6 in tensorflow/serving, but it looks not work for the last tensorflow, do you have  idea how to solve this problem?\r\n", "Sorry, if it's a bug, I'll try to fix it. If it's not, here is not the right place to ask help.", "@snnn It looks a connection problem with 'https://boringssl.googlesource.com/boringssl/', is it possible chage the URL to 'https://github.com/google/boringssl.git'?\r\nThanks!", "I think the problem is the URL 'https://boringssl.googlesource.com/boringssl/' is not work for me, since I am not using tensorflow distributed currently, I set \"option(tensorflow_ENABLE_GRPC_SUPPORT \"Enable gRPC support\" OFF)\" in the file \"tensorflow/tensorflow/contrib/cmake/CMakeLists.txt\", and then CMake and build again. It is compiled success now.\r\nthanks for your help! @snnn ", "@cj741 I got the same error \"fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h'\", when I compile tensorfolw with VS2015 in Windows 10, can you tell me how you solve the problem? thanks.", "@SURFZJY  If your source code is on disk C, try opening vs in admin mode", "> @cj741 I got the same error \"fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h'\", when I compile tensorfolw with VS2015 in Windows 10, can you tell me how you solve the problem? thanks.\r\n\r\nHave you solved the problem? How to solve it?", "> > @cj741 I got the same error \"fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h'\", when I compile tensorfolw with VS2015 in Windows 10, can you tell me how you solve the problem? thanks.\r\n> \r\n> Have you solved the problem? How to solve it?\r\n\r\nhave not tried the method, because i gave up the project laterly."]}, {"number": 15516, "title": "[XLA] Define LANG_CXX11 for >= VS 2015", "body": "In MSVC, __cplusplus == 199711 even when /std:c++latest is set because MSVC is still not \"fully\" C++11 compliant.\r\n\r\nSplit from #15310.\r\n\r\n#15213", "comments": ["Can one of the admins verify this patch?", "It's not needed. It's already in cl.exe's command line arg. ", "@snnn I ran into a lot of compile errors due to `LANG_CXX11` not defined in several XLA `cc_library` (that was before I noticed your old PR #14531). Now that all `cc_library` will be replaced with `tf_cc_library` (#15466), this should be less of a problem.\r\n\r\nI know `LANG_CXX11` is already defined in [`get_win_copts`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorflow.bzl#L155), but since there is a way to just detect MSVC and define `LANG_CXX11` in header file, this should be better than command line flag. I was hoping to remove `LANG_CXX11` from `get_win_copts` and just rely on it being defined in header file.\r\n\r\nWhat do you think?", "Either is ok to me, but, keep consistent. I don't want to see  macro redefined warnings ", "Jenkins, test this please."]}, {"number": 15515, "title": "[XLA] Use simplified version of TF_ASSIGN_OR_RETURN for MSVC", "body": "This simplified version also works for GCC as well actually.\r\n\r\nSplit from #15310.\r\n\r\n#15213", "comments": ["Can one of the admins verify this patch?", "@rongjiecomputer What errors do you encounter without this patch?  It'd be nice to keep these special-cases due to compiler differences to a minimum.", "Example:\r\n\r\n```cpp\r\n  TF_ASSIGN_OR_RETURN(a, foo(10));\r\n```\r\n\r\nIn GCC, it will be expanded into:\r\n\r\n```cpp\r\nauto _status_or_value0 = (foo(10)); if (!_status_or_value0.ok()) { return _status_or_value0.status(); } a = std::move(_status_or_value0.ValueOrDie());\r\n```\r\n\r\nBut in MSVC, it will be expanded into:\r\n\r\n```cpp\r\n(a, foo(10));\r\n```\r\n\r\nWith warning C4003: not enough actual parameters for macro 'TF_STATUS_MACRO_GET_VARIADIC_IMPL'\r\n\r\nYou can see in https://stackoverflow.com/questions/47752606/msvc-preprocessor-bug , StackOverflower said the original implementation is not standard-compliant. It works in GCC because GCC allows `__VA_ARGS__` to be empty as a compiler extension. MSVC does not allow that.\r\n\r\nThe new implementation works in GCC as well. Can I just remove the old implementation and use the new one in all compilers?", "@rongjiecomputer I see, thanks for the info.\r\n\r\nI don't know the history behind why this macro was written the way it is.  And it's used all over the place, internally as well.\r\n\r\nPerhaps the best thing to do is to try your suggestion.  Can you update your PR to use your new implementation for all compilers?\r\n\r\nOnce you're done, ping this issue.  If all the tests pass, we can try merging it.", "@tatatodd Done. It might surprise you that the diff is incredibly smaller after removing the old implementation.", "I looked into this a bit more.\r\n\r\nI think you're right that the original code was relying on non-standard empty `__VA_ARGS__` behavior.  Which means that another possible fix would be to add a dummy arg to the invocation of TF_STATUS_MACRO_GET_VARIADIC_IMPL:\r\n\r\n```\r\n// Trailing 0 is a dummy value.\r\n#define TF_ASSIGN_OR_RETURN(...) \\\r\n  TF_STATUS_MACRO_GET_VARIADIC_IMPL(__VA_ARGS__, TF_ASSIGN_OR_RETURN_IMPL_3, \\\r\n                                    TF_ASSIGN_OR_RETURN_IMPL_2, 0) \\\r\n```\r\n\r\nNote that the internal version that this is based on was written to deal with an optional 3rd argument to TF_ASSIGN_OR_RETURN.  But when it was ported to XLA, the optional 3rd argument was removed, but the underlying machinery was retained.  Except that it no longer accepts the optional 3rd argument at all.\r\n\r\nTagging @eliben since I believe he originally ported this bit of logic.\r\n\r\nTo summarize, I think your PR should work fine and makes things simpler.  So thanks, and let's get this merged!  :)", "Jenkins, test this please."]}, {"number": 15514, "title": "[XLA] Remove unused dlfcn.h and implement sincos[f] for MSVC", "body": "Split from #15310.\r\n\r\n#15213", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 15513, "title": "[XLA] Define ssize_t for Windows", "body": "Split from #15310.\r\n\r\n#15213", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "GPU CC CI test is failing:\r\n\r\n```\r\ntensorflow/core/platform/cloud/file_block_cache_test.cc:464\r\nValue of: WaitForNotificationWithTimeout(&notification, 10000)\r\n  Actual: false\r\nExpected: true\r\nTimeout waiting for concurrent thread to start.\r\n```\r\n\r\nSee https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/cloud/file_block_cache_test.cc#L464\r\n\r\nShould be unrelated to my change.", "It breaks windows bazel build", "@snnn Sorry for the breakage. I did not run a rebuild after moving the `ssize_t` typedef to new file `platform/windows/integral_types.h`. Can you check if applying #15579 fixes the issue? Thanks!", "I'd prefer to put the change in build_config.bzl "]}, {"number": 15512, "title": "[XLA] Hide GCC 7.1.1 workaround from MSVC", "body": "Split from #15310.\r\n\r\n#15213", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Ubuntu Python3 CI test is failing:\r\n\r\n```\r\n//tensorflow/python/keras:data_utils_test                               TIMEOUT in 482.0s\r\n```\r\n\r\nIt seems completely unrelated to my change. What should I do?", "@rongjiecomputer I think that's fine.  Let's wait for the Windows Cmake Tests to finish up, and then someone with write access will be able to merge the changes in."]}, {"number": 15511, "title": "[XLA] Fix std::array initialization", "body": "Split from #15310.\r\n\r\n#15213", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 15510, "title": "[XLA] Use tensorflow::port::Aligned{Malloc,Free}", "body": "Split from #15310.\r\n\r\n#15213", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 15509, "title": "[XLA] Explicitly include <numeric>", "body": "`std::accumulate` comes from `<numeric>`, but `<numeric>` is not implicitly included by `<algorithm>` in MSVC.\r\n\r\nSplit from #15310.\r\n\r\n#15213", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 15508, "title": "[XLA] Add 'const' to custom comparator", "body": "Split from #15310.\r\n\r\n#15213", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 15507, "title": "[XLA] Use os.path for path manipulation", "body": "Split from #15310.\r\n\r\n#15213", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 15506, "title": "bugfix: long is 32 bits on Windows", "body": "Please avoid to use 'long' as data type.\r\n\r\n```c++\r\nint main()\r\n{\r\n\tlong long threshold = 1L << 31;    \r\n\tstd::cout << threshold << std::endl;\r\n\treturn 0;\r\n}\r\n```\r\nresult:\r\n-2147483648\r\n\r\n", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please."]}, {"number": 15505, "title": "Unable to generate my_frozen_graph.pb due to missing conv.ckpt checkpoint file", "body": "------------------------\r\n\r\n### System information\r\nLinux ubuntu 16.04\r\nCUDA 9.1\r\ncuDNN 7\r\nTensorFlow version ('v1.4.0-19-ga52c8d9', '1.4.1')\r\n\r\nAdditional info:\r\n\r\n2017-12-20 07:36:22.294690: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU sup\r\nports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX\r\n2 FMA\r\n2017-12-20 07:36:22.360228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] succe\r\nssful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA \r\nnode, so returning NUMA node zero\r\n2017-12-20 07:36:22.360615: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found de\r\nvice 0 with properties: \r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:00:04.0\r\ntotalMemory: 11.17GiB freeMemory: 8.42GiB\r\n2017-12-20 07:36:22.360643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating\r\n TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, \r\ncompute capability: 3.7)\r\n\r\n**Exact command to reproduce**:\r\nAccording to https://www.tensorflow.org/versions/master/tutorials/audio_recognition#custom_training_data, specifically:\r\n\r\nTo train...\r\n```\r\npython tensorflow/examples/speech_commands/train.py\r\n```\r\nAfter training is finished, run freeze.py to generate a `my_frozen_graph.pb` which will later be used by `label_wav` for prediction.\r\n\r\n```\r\npython tensorflow/examples/speech_commands/freeze.py \\\r\n--start_checkpoint=/tmp/speech_commands_train/conv.ckpt-18000 \\\r\n--output_file=/tmp/my_frozen_graph.pb\r\n```\r\n\r\n### Describe the problem\r\nAfter training the speech commands app:\r\nA `conv.ckpt-18000` file should appear after training is complete. \r\n\r\nHowever the only files which appear are ones with extensions:\r\n\r\n- *.data-00000-of-00001\r\n- *.meta\r\n- *.index\r\n- checkpoint\r\n\r\nI've tried tensorflow releases 1.4.0-rc1, 1.4.0 and 1.4.1 but no luck generating a `conv.ckpt-18000` file.\r\n\r\nI've even tried:\r\n```\r\nwith tf.Session() as sess:\r\n    new_saver = tf.train.import_meta_graph('/tmp/speech_commands_train/conv.ckpt-18000.meta')\r\n    new_saver.restore(sess, tf.train.latest_checkpoint('./'))\r\n```\r\n\r\nwhich errors out due to:\r\n\r\n`ValueError: No op named DecodeWav in defined operations.`\r\n\r\n### Source code / logs\r\n```\r\nls -ltr /tmp/speech_commands_train/\r\ntotal 18348\r\n...\r\n-rw-r--r-- 1 root root  121649 Dec 20 07:07 conv.pbtxt\r\n-rw-r--r-- 1 root root      60 Dec 20 07:07 conv_labels.txt\r\n-rw-r--r-- 1 root root     315 Dec 20 09:17 conv.ckpt-17600.index\r\n-rw-r--r-- 1 root root 3646008 Dec 20 09:17 conv.ckpt-17600.data-00000-of-00001\r\n-rw-r--r-- 1 root root   75448 Dec 20 09:17 conv.ckpt-17600.meta\r\n-rw-r--r-- 1 root root     315 Dec 20 09:18 conv.ckpt-17700.index\r\n-rw-r--r-- 1 root root 3646008 Dec 20 09:18 conv.ckpt-17700.data-00000-of-00001\r\n-rw-r--r-- 1 root root   75448 Dec 20 09:18 conv.ckpt-17700.meta\r\n-rw-r--r-- 1 root root     315 Dec 20 09:18 conv.ckpt-17800.index\r\n-rw-r--r-- 1 root root 3646008 Dec 20 09:18 conv.ckpt-17800.data-00000-of-00001\r\n-rw-r--r-- 1 root root   75448 Dec 20 09:18 conv.ckpt-17800.meta\r\n-rw-r--r-- 1 root root     315 Dec 20 09:19 conv.ckpt-17900.index\r\n-rw-r--r-- 1 root root 3646008 Dec 20 09:19 conv.ckpt-17900.data-00000-of-00001\r\n-rw-r--r-- 1 root root   75448 Dec 20 09:19 conv.ckpt-17900.meta\r\n-rw-r--r-- 1 root root     315 Dec 20 09:20 conv.ckpt-18000.index\r\n-rw-r--r-- 1 root root 3646008 Dec 20 09:20 conv.ckpt-18000.data-00000-of-00001\r\n-rw-r--r-- 1 root root     433 Dec 20 09:20 checkpoint\r\n-rw-r--r-- 1 root root   75448 Dec 20 09:20 conv.ckpt-18000.meta\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "I have the same issue. Any help would be appreciated.\r\n\r\nLinux ubuntu 16.04 (x86_64)\r\nTensorFlow version '1.4.1' and it was installed with 'pip3 install tensorflow-gpu'\r\nCUDA V8.0.61\r\ncuDNN 6\r\nGPU: NVIDIA Corporation GK104 [GeForce GTX 770]\r\n", "@jyr1v  &  @donigian  \r\nUse the same configurations to freeze the graph as you used during training, like you may have changed --model_architecture,  --window_stride_ms, --sample_rate, etc\r\n", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "not an issue for me anymore", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 53 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 15504, "title": "Fix typos", "body": "", "comments": ["Can one of the admins verify this patch?"]}, {"number": 15503, "title": "Behavior change of tf.app.flags parsing boolean args", "body": "tf version 1.5.0-dev20171219\r\nfor example below args\r\nflags.DEFINE_boolean('pre_calc_image_feature', False, '')\r\n\r\nwhen using tf 1.4.1 it is ok to do --pre_calc_image_feature 0\r\nwhich got FLAGS.pre_calc_image_feature == False.\r\nBut for tf 1.5 you must use --pre_calc_image_feature=0 if you still use --pre_calc_image_feature 0\r\nthen you will get FLAGS.pre_calc_image_feature == True. \r\n\r\nNot sure if this is a bug or just by design but personally I think tf version 1.4.1 is better handling this case.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "ubuntu 16.04\r\npip install tensorflow-nightly-gpu\r\ntf version 1.5.0-dev20171219\r\ncuda 9.0 cudnn 7.0\r\ngtx1080ti 11g ", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "This is probably beacuse we switched to using absl flags. @yilei, can this behavior be changed?", "Looks like so, that's a subtle difference between previous `tf.flags` and `absl.flags`. In `absl.flags` boolean flags are specified with:\r\n\r\n```\r\n--pre_calc_image_feature\r\n--nopre_calc_image_feature\r\n--pre_calc_image_feature=true/false/1/0/t/f\r\n```\r\n\r\nThe `0` in `--pre_calc_image_feature 0` is treated as a positional argument. Unfortunately we can't change this behavior in `absl.flags`.", "Closing since this behavior cannot be fixed.", "This is regrettable. We never documented tf.flags other than its existence (we never committed to details). I wish we had never included it in tf -- everybody really should just use argparse (or whatever else). "]}, {"number": 15502, "title": "Use BSD fnmatch on Windows", "body": "#15501 \r\n\r\nIt has a BSD-3-Clause copyright. \r\n", "comments": ["Can one of the admins verify this patch?", "Close it because @jart suggests Env::MatchPath should be moved onto the FileSystem interface."]}, {"number": 15501, "title": "Env::MatchPath has different behavior on Windows and Linux", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 10\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\n1.4.0\r\n- **Python version**: \r\n3.5.4\r\n- **Bazel version (if compiling from source)**:\r\nNone\r\n- **GCC/Compiler version (if compiling from source)**:\r\nNone\r\n- **CUDA/cuDNN version**:\r\nNone\r\n- **GPU model and memory**:\r\nNone\r\n- **Exact command to reproduce**:\r\n\r\n\r\n\r\n### Describe the problem\r\nFor example,\r\n\r\nname | pattern |  result on windows | result on Linux\r\n-------|---------|----------------------|--------------\r\na/a,  |   \\*,         |   Matches                 |  No match\r\na/a/a.txt| \\*/\\*  |   Matches                 |  No match\r\n\r\nAs GcsFileSystem uses this function for result filtering,  GcsFileSystem has different behavior on Windows and Linux. It's odd to me. \r\n\r\n### Source code / logs\r\n```c++\r\n#include <Windows.h>\r\n#include <Shlwapi.h>\r\n#include <iostream>\r\nint main()\r\n{\r\n   std::cout << PathMatchSpec(L\"aaa\\\\bbb\\\\ccc.txt\", L\"*\") << std::endl;\r\n    return 0;\r\n}\r\n```\r\n\r\n```c\r\n#include <stdio.h>\r\n#include <fnmatch.h>\r\n\r\nint main(){\r\n  printf(\"%d\\n\",fnmatch(\"aaa/bbb/ccc.txt\",\"*/*\",FNM_PATHNAME));\r\n  return 0;\r\n}\r\n```", "comments": ["The Windows API difference and the splash damage to GCS doesn't sound great. Maybe Env::MatchPath needs to be moved onto the FileSystem interface and then fnmatch() written from scratch for Windows.\r\n\r\nI haven't examined these functions as closely as you. Could you help me gain a better understanding of what sort of impact you can envision this issue having?\r\n\r\ncc: @mrry ", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 145 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!", "@tensorflowbutler @jart @snnn @mrry this is still an issue\r\nsee https://github.com/tensorflow/tensorflow/issues/19378#issuecomment-412760300"]}, {"number": 15500, "title": "fix pooling1D dimension bug", "body": "When `data_format` is `channels_last`, input is `NWC`, so we have to `expand_dim(1)` to make it become `NHWC`. Then we apply pooling on `W` which is the 3rd dimention.\r\n\r\nWhen `data_format` is `channels_first`, input is `NCW`, so we have to `expand_dim(2)` to make it become `NCHW`. Then we apply pooling on `W`, which is the 4th dimention.\r\n\r\nRELNOTES: Fixed wrong handling of 1D pooling (pooling was not happening on the correct dimension).", "comments": ["Can one of the admins verify this patch?", "Any comment from the admins?", "@yifeif  Am I wrong with the implementation detail about 1D pooling?", "May I ask about the review progress?", "@martinwicke ", "Sorry. Busy times.\r\n\r\nThis looks fine to me on the face of it, but I'm worried about the fact that there were no tests. Could you add tests that check that the 1D case works as expected?\r\n\r\nThanks!", "Thank you for your review, I'll add some test to `python/layers/pooling_test.py`", "@martinwicke  There are 4 test cases in `pooling_test.py`, but I think the cases for `channels_first` are wrong. I fix it in 211e5a4 . \r\n\r\nBTW, how to trigger CI to re-run the test? I would like to see the test failing status", "Jenkins, test this please.", "We're killing Jenkins. I've triggered the tests.", "@martinwicke done, plz trigger CI again. Thanks a lot!", "@martinwicke could you approve this, please?", "Thanks the review from @martinwicke !"]}, {"number": 15499, "title": "Go bindings: No shape inference function exists for op 'CreateSummaryFileWriter'", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch Linux\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.4.1\r\n- **Python version**: NA (using Go bindings)\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: cuda-9.1.85-1 cudnn-7.0.5-1\r\n- **GPU model and memory**: GTX 1060 6GB\r\n- **Exact command to reproduce**: See below\r\n\r\n### Describe the problem\r\nCalling `op.CreateSummaryFileWriter()` panics with: `panic: failed to add operation \"CreateSummaryFileWriter\": No shape inference function exists for op 'CreateSummaryFileWriter', did you forget to define it?`\r\n\r\n### Source code / logs\r\n```\r\npackage main\r\n\r\nimport (\r\n\ttf \"github.com/tensorflow/tensorflow/tensorflow/go\"\r\n\t\"github.com/tensorflow/tensorflow/tensorflow/go/op\"\r\n)\r\n\r\nfunc main() {\r\n\ts := op.NewScope()\r\n\twriter := op.SummaryWriter(s)\r\n\tcreateSummaryWriter := op.CreateSummaryFileWriter(s,\r\n\t\twriter,\r\n\t\top.Const(s.SubScope(\"log_dir\"), \"tb_logs\"),\r\n\t\top.Const(s.SubScope(\"max_queue\"), int32(10)),\r\n\t\top.Const(s.SubScope(\"flush_millis\"), int32(1000)),\r\n\t\top.Const(s.SubScope(\"filename_suffix\"), \"tb_demo\"),\r\n\t)\r\n\tscalar := op.Const(s.SubScope(\"scalar\"), float32(3.1415))\r\n\tstep := op.Const(s.SubScope(\"step\"), int64(1))\r\n\ttag := op.Const(s.SubScope(\"tag\"), \"foo_scalar\")\r\n\tsummary := op.ScalarSummary(s, tag, scalar)\r\n\tmerged := op.MergeSummary(s, []tf.Output{summary})\r\n\twrite := op.WriteSummary(s, writer, step, scalar, tag, merged)\r\n\tcloseSummaryWriter := op.CloseSummaryWriter(s, writer)\r\n\tgraph, err := s.Finalize()\r\n\tif err != nil {\r\n\t\tpanic(err)\r\n\t}\r\n\tsess, err := tf.NewSession(graph, nil)\r\n\tif err != nil {\r\n\t\tpanic(err)\r\n\t}\r\n\t_, err = sess.Run(nil, nil, []*tf.Operation{createSummaryWriter})\r\n\tif err != nil {\r\n\t\tpanic(err)\r\n\t}\r\n\t_, err = sess.Run(nil, nil, []*tf.Operation{write})\r\n\tif err != nil {\r\n\t\tpanic(err)\r\n\t}\r\n\t_, err = sess.Run(nil, nil, []*tf.Operation{closeSummaryWriter})\r\n\tif err != nil {\r\n\t\tpanic(err)\r\n\t}\r\n}\r\n```\r\nReturns:\r\n```\r\n2017-12-19 20:39:06.314322: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\npanic: failed to add operation \"CreateSummaryFileWriter\": No shape inference function exists for op 'CreateSummaryFileWriter', did you forget to define it? (Stacktrace: goroutine 1 [running]:\r\nruntime/debug.Stack(0xc420082130, 0x13eb530, 0x14298a0)\r\n\t/usr/lib/go/src/runtime/debug/stack.go:24 +0xa7\r\ngithub.com/tensorflow/tensorflow/tensorflow/go/op.(*Scope).UpdateErr(0xc42007c180, 0x4d9527, 0x17, 0x750140, 0xc42000e068)\r\n\t/home/isaac/go/src/github.com/tensorflow/tensorflow/tensorflow/go/op/scope.go:120 +0x72\r\ngithub.com/tensorflow/tensorflow/tensorflow/go/op.(*Scope).AddOperation(0xc42007c180, 0x4d9527, 0x17, 0x4d9527, 0x17, 0xc4200820f0, 0x5, 0x5, 0x0, 0xc4200181a0)\r\n\t/home/isaac/go/src/github.com/tensorflow/tensorflow/tensorflow/go/op/scope.go:85 +0xfd\r\ngithub.com/tensorflow/tensorflow/tensorflow/go/op.CreateSummaryFileWriter(0xc42007c180, 0xc420010200, 0x0, 0xc420010210, 0x0, 0xc420010220, 0x0, 0xc420010230, 0x0, 0xc420010240, ...)\r\n\t/home/isaac/go/src/github.com/tensorflow/tensorflow/tensorflow/go/op/wrappers.go:16474 +0x2cc\r\nmain.main()\r\n\t/home/isaac/go/src/github.com/is8ac/gotf/tb_demo.go:11 +0x282\r\n)\r\n\r\ngoroutine 1 [running]:\r\nmain.main()\r\n\t/home/isaac/go/src/github.com/is8ac/gotf/tb_demo.go:27 +0x7cc\r\nexit status 2\r\n```", "comments": ["Thanks for the report.\r\n\r\nThis was fixed by https://github.com/tensorflow/tensorflow/commit/75e6675c19024a70d27015f9f52f8eba60024803 which is not part of the 1.4 release of the C library.\r\n\r\nFor now, the workaround will be to either build the C library (`libtensorflow.so` and `libtensorflow_framework.so`) from source. This will be fixed with the 1.5 release.\r\n", "Thank you. I shall try building from source."]}, {"number": 15498, "title": "gradients_1 generated in graph but not connected with AdamOptimizer ", "body": "System information: \r\nOS platform: Linux Unbuntu 16.04\r\nTensorflow install from: pip install...\r\nTensorflow version: 1.4.0\r\nPython version: py2.7\r\n\r\nProblem:\r\nI want to write a CNN classification network for MNIST dataset. And I write a class named MNIST_classification, then define '_build_model()' and '_train_phase()' in this class. In main function, I define a object of MNIST_classification class, and callback the '_train_phase()' to start a training process. But I found in Graph(), there are two gradients generated at each computation node, i.e. \"gradients\" and \"gradients_1\". I use tf.gradients(loss, train_vars) to print all gradients' names and get '/gradients_1/*', but within the Graph(), gradients_1 are not connected with a AdamOptimizer, which leads to no backward propagation update for each trainable variable...\r\n\r\n \r\n![image](https://user-images.githubusercontent.com/33562173/34189925-f3c6c4ac-e578-11e7-8b8f-3de98611d415.png)\r\n\r\nSource code:\r\n\r\nimport tensorflow as tf\r\nfrom utils import utils\r\nimport numpy as np\r\n\r\nclass mnist_classification(object):\r\n\r\n    def __init__(self, sess, graph, train_param={'num_of_epoches': 1000,\r\n                                        'num_of_classes': 10,\r\n                                        'log_dir': './log',\r\n                                        'model_dir': './model',\r\n                                        'batch_size': 128,\r\n                                        'learn_rate': 1e-4,\r\n                                        'max_iter': 5000,\r\n                                        'dim_feat': 28}):\r\n\r\n        self.num_of_epoches = train_param['num_of_epoches']\r\n        self.log_dir      = train_param['log_dir']\r\n        self.model_dir    = train_param['model_dir']\r\n        self.batch_size   = train_param['batch_size']\r\n\r\n        self.learn_rate = train_param['learn_rate']\r\n        self.max_iter   = train_param['max_iter']\r\n\r\n        self.dim_feat = train_param['dim_feat']\r\n        self.num_of_classes = train_param['num_of_classes']\r\n\r\n        self.sess = sess\r\n        self.graph = graph\r\n\r\n        # !Build-up MNIST classification model...\r\n        assert self.sess.graph is self.graph\r\n        self._build_model()\r\n\r\n\r\n\r\n    def _convolution_block(self, inp_feat, kernel_size, num_of_kernel_channels, conv_strides, conv_padding, var_scope):\r\n        '''\r\n            Function:\r\n                        _convolution_block, i.e. convolution + Maxpooling + ReLU\r\n            Input:\r\n                    [1] <tensor> inp_feat, i.e. input feature, dimension->[batch_size, height, width, channel]\r\n                    [2] <int32>  kernel_size\r\n                    [3] <int32> num_of_kernel_channels\r\n                    [4] <int32> conv_strides\r\n                    [5] <string> conv_padding\r\n                    [5] <string> var_scope\r\n            Output:\r\n                    <tensor> activ\r\n        '''\r\n        try:\r\n            num_of_feat_channels = inp_feat.shape[3].value\r\n        except:\r\n            num_of_feat_channels = 1\r\n\r\n        with tf.variable_scope(var_scope):\r\n            weights = tf.get_variable(name='conv_weights', shape=[kernel_size, kernel_size, num_of_feat_channels, num_of_kernel_channels],\r\n                                      initializer=tf.random_normal_initializer(stddev=0.1))\r\n\r\n            biases = tf.get_variable(name='conv_biases', shape=[num_of_kernel_channels], initializer=tf.zeros_initializer())\r\n\r\n        # !Convolution layer...\r\n        conv = tf.nn.conv2d(inp_feat, weights, strides=conv_strides, padding=conv_padding, name='conv', data_format='NHWC') + biases\r\n\r\n        # !Activation layer...\r\n        activ = tf.nn.relu(conv)\r\n\r\n        # !Pooling layer...\r\n        pool = tf.nn.max_pool(activ,ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\r\n\r\n        return pool\r\n\r\n\r\n\r\n\r\n\r\n    def _fc_layer(self, inp_feat, num_of_outputs, var_scope):\r\n        '''\r\n            Function:\r\n                        _fc_layer\r\n            Input:\r\n                        [1] inp_feat, dimension->[batch_size, dim_feat]\r\n                        [2] num_of_outputs\r\n                        [3] var_scope: reuse=True\r\n            Output:\r\n                        fc\r\n        '''\r\n\r\n        dim_feat = inp_feat.shape[1].value\r\n\r\n        with tf.variable_scope(var_scope):\r\n\r\n            fc_weights = tf.get_variable(name='fc_weights', dtype=tf.float32, shape=[dim_feat, num_of_outputs], initializer=tf.random_normal_initializer(stddev=0.1))\r\n            fc_bias    = tf.get_variable(name='fc_bias',    dtype=tf.float32, shape=[num_of_outputs], initializer=tf.zeros_initializer())\r\n\r\n\r\n            # tf.nn.xw_plus_b(x, weights, bias) = tf.matmul(x, weights) + biases\r\n            fc = tf.nn.xw_plus_b(x=inp_feat, weights=fc_weights, biases=fc_bias)\r\n\r\n            return fc\r\n\r\n\r\n\r\n    def _build_model(self):\r\n\r\n        self.digit = tf.placeholder(dtype=tf.float32, shape=[self.batch_size, self.dim_feat, self.dim_feat, 1])\r\n        self.label = tf.placeholder(dtype=tf.int64, shape=[self.batch_size, self.num_of_classes])\r\n\r\n        # # !One-hot encoding for label...\r\n        # with tf.name_scope('label_trans'):\r\n        #     self.new_label = utils._array_sparse_to_dense(self.label, self.num_of_classes)\r\n\r\n        conv1 = self._convolution_block(inp_feat=self.digit, conv_strides=[1,1,1,1], conv_padding='SAME', kernel_size=5, num_of_kernel_channels=32, var_scope='conv1')\r\n\r\n        conv2 = self._convolution_block(inp_feat=conv1, conv_strides=[1,1,1,1], conv_padding='SAME', kernel_size=5, num_of_kernel_channels=64, var_scope='conv2')\r\n\r\n        conv3 = self._convolution_block(inp_feat=conv2, conv_strides=[1, 1, 1, 1], conv_padding='SAME', kernel_size=7,num_of_kernel_channels=64, var_scope='conv3')\r\n\r\n        flatt = tf.layers.flatten(conv3, name='flatten')\r\n\r\n        fc1 = self._fc_layer(inp_feat=flatt, num_of_outputs=1024, var_scope='fc1')\r\n        fc1 = tf.nn.relu(fc1)\r\n\r\n        fc2 = self._fc_layer(inp_feat=fc1, num_of_outputs=10, var_scope='fc2')\r\n\r\n        self.predict = tf.nn.softmax(logits=fc2, dim=-1)\r\n\r\n        self.loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(onehot_labels=self.label, logits=fc2))\r\n\r\n        # !Define MNIST classification accuracy...\r\n        correct = tf.equal(tf.argmax(self.predict, 1), tf.argmax(self.label, 1))\r\n        self.accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\r\n\r\n        # !Define training operations...\r\n        self.train_op = tf.train.AdamOptimizer(self.learn_rate).minimize(self.loss)\r\n\r\n\r\n\r\n\r\n\r\n    def _train(self, img_file, lab_file):\r\n\r\n        # !Load data into memory...\r\n        img, n_rows, n_cols = utils._read_MNIST_file(img_file, fmt='>IIII')\r\n        lab, _, _ = utils._read_MNIST_file(lab_file, fmt='>II')\r\n\r\n\r\n        # !Intialize the global_variables and local_variables...\r\n        self.sess.run(tf.global_variables_initializer())\r\n        self.sess.run(tf.local_variables_initializer())\r\n\r\n        # !List all trainable variables...\r\n        train_vars = tf.trainable_variables()\r\n        for var in train_vars:\r\n            print var.name\r\n\r\n        # !Add gradients into tensorboard summary...\r\n        gradients = tf.gradients(self.loss, train_vars)\r\n        for ii in range(len(gradients)):\r\n            if gradients[ii]!=None:\r\n                tf.summary.histogram(train_vars[ii].name, gradients[ii])\r\n\r\n        # !Add loss into tensorboard summary...\r\n        tf.summary.scalar('loss', self.loss)\r\n        tf.summary.scalar('accuracy', self.accuracy)\r\n\r\n        tf.summary.image('input_image', self.digit)\r\n\r\n        summary_writer = tf.summary.FileWriter(\"./log\", self.sess.graph)\r\n\r\n        merged = tf.summary.merge_all()\r\n\r\n        # !Start training process...\r\n        for ii_epoch in range(self.num_of_epoches):\r\n            for ii_iter in range(self.max_iter):\r\n\r\n\r\n                img_batch = utils._randomly_sample(img, self.batch_size)\r\n                img_batch = np.expand_dims(img_batch, axis=3) / 255\r\n\r\n                lab_batch = utils._randomly_sample(lab, self.batch_size)\r\n                lab_batch = utils._array_sparse_to_dense(np.int64(lab_batch), num_of_classes=self.num_of_classes)\r\n\r\n                _, pred, los, acc, summary= self.sess.run([self.train_op, self.predict, self.loss, self.accuracy, merged], feed_dict={self.digit: img_batch, self.label: lab_batch})\r\n\r\n                if ( (ii_epoch * self.max_iter + ii_iter) % 100 == 0):\r\n                    summary_writer.add_summary(summary, ii_epoch * self.max_iter + ii_iter)\r\n                    print(pred[0,:])\r\n                    print(lab_batch[0])\r\n\r\n                print('!Loss at No.%d Epoch, No.%d Iteration=%.5f' % (ii_epoch, ii_iter, los))\r\n                print('!Accuracy at No.%d Epoch, No.%d Iteration=%.5f' % (ii_epoch, ii_iter, acc))\r\n\r\n\r\n\r\nMain function:\r\nfrom utils.utils import _array_sparse_to_dense\r\nfrom utils.utils import _read_MNIST_file\r\nfrom matplotlib import pyplot\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom model.MNIST_classification import mnist_classification\r\nimport os\r\nimport sys\r\n\r\n\r\n# !Check if tensorflow version == '1.4.0', API\r\nassert tf.__version__ == '1.4.0'\r\n\r\n\r\n\r\n# !Set system default encoding method == 'utf-8'\r\nreload(sys)\r\nsys.setdefaultencoding('utf-8')\r\n\r\n\r\nos.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\r\n\r\n\r\nwith tf.Graph().as_default() as graph:\r\n    with tf.Session() as sess:\r\n        MNIST_inst = mnist_classification(sess=sess, graph=graph)\r\n\r\n        MNIST_inst._train(img_file='./data/train-images.idx3-ubyte', lab_file='./data/train-labels.idx1-ubyte')\r\n\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 15497, "title": "Image Input for GridLSTM", "body": "I would like to use GridLSTM for a handwriting recognition task. Unfortunately, the documentation is lacking info on how to input images into GridLSTMs. \r\n", "comments": ["@selcouthlyBlue I found this https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/GridLSTMCell.   ", "@yap770813901 I saw that one already. One thing I'm confused about that is the parameter `num_frequency_blocks`. \r\n\r\nAlso, there is another GridLSTMCell implementation (Grid2LSTMCell to be exact) in tf.contrib.grid_rnn.", "Until I know what `num_frequency_blocks` is for in contrib.rnn.GridLSTMCell, I'm going to use tf.contrib.grid_rnn. I'm doing a learning test on tf.contrib.grid_rnn 's grid_rnn_cell that involves what the core rnn accepts using these cells. Here's what I got so far:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.grid_rnn.python.ops import grid_rnn_cell\r\n\r\n\r\ndef reshape_to_rnn_dims(tensor, shape):\r\n    reshaped_tensor = tf.reshape(tensor, shape)\r\n    split_tensor = tf.split(reshaped_tensor, shape[1], 1)\r\n    return split_tensor\r\n\r\n\r\nclass GridLSTMCellTest(tf.test.TestCase):\r\n    def setUp(self):\r\n        self.num_features = 1\r\n        self.time_steps = 1\r\n        self.batch_size = 1\r\n        tf.reset_default_graph()\r\n        self.input_layer = tf.placeholder(tf.float32, [self.batch_size, self.time_steps, self.num_features])\r\n        self.cell = grid_rnn_cell.Grid1LSTMCell(num_units=8)\r\n\r\n    def test_simple_grid_rnn(self):\r\n        self.input_layer = reshape_to_rnn_dims(self.input_layer, [-1, self.num_features * self.time_steps])\r\n        tf.nn.static_rnn(self.cell, self.input_layer, dtype=tf.float32)\r\n\r\n    def test_dynamic_grid_rnn(self):\r\n        tf.nn.dynamic_rnn(self.cell, self.input_layer, dtype=tf.float32)\r\n\r\n\r\nclass BidirectionalGridRNNCellTest(tf.test.TestCase):\r\n    def setUp(self):\r\n        self.num_features = 1\r\n        self.time_steps = 1\r\n        self.batch_size = 1\r\n        tf.reset_default_graph()\r\n        self.input_layer = tf.placeholder(tf.float32, [self.batch_size, self.time_steps, self.num_features])\r\n        self.cell_fw = grid_rnn_cell.Grid1LSTMCell(num_units=8)\r\n        self.cell_bw = grid_rnn_cell.Grid1LSTMCell(num_units=8)\r\n\r\n    def test_simple_bidirectional_grid_rnn(self):\r\n        self.input_layer = reshape_to_rnn_dims(self.input_layer, [-1, self.num_features * self.time_steps])\r\n        tf.nn.static_bidirectional_rnn(self.cell_fw, self.cell_fw, self.input_layer, dtype=tf.float32)\r\n\r\n    def test_bidirectional_dynamic_grid_rnn(self):\r\n        tf.nn.bidirectional_dynamic_rnn(self.cell_fw, self.cell_bw, self.input_layer, dtype=tf.float32)\r\n\r\n\r\nif __name__ == '__main__':\r\n    tf.test.main()\r\n\r\n```", "And here's one using contrib rnn:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.grid_rnn.python.ops import grid_rnn_cell\r\nfrom tensorflow.contrib import rnn\r\n\r\n\r\nclass GridRNNTest(tf.test.TestCase):\r\n    def setUp(self):\r\n        self.num_features = 1\r\n        self.time_steps = 1\r\n        self.batch_size = 1\r\n        tf.reset_default_graph()\r\n        self.input_layer = tf.placeholder(tf.float32, [self.batch_size, self.time_steps, self.num_features])\r\n        self.cell = grid_rnn_cell.Grid1LSTMCell(num_units=8)\r\n\r\n    def test_simple_grid_rnn(self):\r\n        self.input_layer = tf.unstack(self.input_layer, self.time_steps, 1)\r\n        rnn.static_rnn(self.cell, self.input_layer, dtype=tf.float32)\r\n\r\nclass BidirectionalGridRNNTest(tf.test.TestCase):\r\n    def setUp(self):\r\n        self.num_features = 1\r\n        self.time_steps = 1\r\n        self.batch_size = 1\r\n        tf.reset_default_graph()\r\n        self.input_layer = tf.placeholder(tf.float32, [self.batch_size, self.time_steps, self.num_features])\r\n        self.cell_fw = grid_rnn_cell.Grid1LSTMCell(num_units=8)\r\n        self.cell_bw = grid_rnn_cell.Grid1LSTMCell(num_units=8)\r\n\r\n    def test_simple_bidirectional_grid_rnn(self):\r\n        self.input_layer = tf.unstack(self.input_layer, self.time_steps, 1)\r\n        rnn.static_bidirectional_rnn(self.cell_fw, self.cell_bw, self.input_layer, dtype=tf.float32)\r\n\r\n\r\nclass StackBidirectionalGridRNNTest(tf.test.TestCase):\r\n    def setUp(self):\r\n        self.num_features = 1\r\n        self.time_steps = 1\r\n        self.batch_size = 1\r\n        tf.reset_default_graph()\r\n        self.input_layer = tf.placeholder(tf.float32, [self.batch_size, self.time_steps, self.num_features])\r\n        self.cells_fw = [grid_rnn_cell.Grid1LSTMCell(num_units=8) for _ in range(2)]\r\n        self.cells_bw = [grid_rnn_cell.Grid1LSTMCell(num_units=8) for _ in range(2)]\r\n\r\n    '''\r\n    def test_stack_bidirectional_grid_rnn(self):\r\n        self.input_layer = tf.unstack(self.input_layer, self.time_steps, 1)\r\n        rnn.stack_bidirectional_rnn(self.cells_fw, self.cells_fw, self.input_layer, dtype=tf.float32)\r\n    '''\r\n\r\n\r\nif __name__ == '__main__':\r\n    tf.test.main()\r\n\r\n```", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}]