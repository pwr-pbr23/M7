[{"number": 17365, "title": "Fix the error activation function link in custom_estimators.md", "body": "Described as PR title.", "comments": []}, {"number": 17364, "title": "Interleaving multiple datasets together", "body": "@mrry The current interleave functionality is basically a interleaved flat-map taking as input a single dataset. Given the current API, what's the best way to interleave multiple datasets together? Say they have already been constructed and I have a list of them. I want to produce elements from them alternatively and I want to support lists with more than 2 datasets (i.e., stacked zips and interleaves would be pretty ugly).\r\n\r\nThanks! :)", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "I posted a question [here](https://stackoverflow.com/questions/49058913/interleaving-multiple-tensorflow-datasets-together) but I'm not sure how to mention @mrry there.", "  ```python\r\n  # Preprocess 4 files concurrently.\r\n  filenames = tf.data.Dataset.list_files(\"/path/to/data/train*.tfrecords\")\r\n  dataset = filenames.apply(\r\n      tf.contrib.data.parallel_interleave(\r\n          lambda filename: tf.data.TFRecordDataset(filename),\r\n          cycle_length=4))\r\n  ```\r\n[parallel_interleave](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/data/python/ops/interleave_ops.py#L35)\r\n[read_dataset](https://github.com/tensorflow/models/blob/master/research/object_detection/utils/dataset_util.py#L106)"]}, {"number": 17363, "title": "zeros() not tracking shape based on input tensors", "body": "```\r\n>>> shape = [constant([2])[0], 3]\r\n>>> reshape([1,2,3,4,5,6], shape)\r\n<tf.Tensor 'Reshape_13:0' shape=(2, 3) dtype=int32>\r\n>>> zeros(shape)\r\n<tf.Tensor 'zeros_2:0' shape=(?, 3) dtype=float32>\r\n```\r\nIf reshape can understand the input values well enough to set a static shape for its output, shouldn't zeros() be able to?\r\n\r\nThis is using tensorflow 1.5", "comments": ["Ah, I believe in this case reshape() is getting all the information it needs from the static shape of the input tensor, and the other known constants in the shape.  I don't think this is a bug, then."]}, {"number": 17362, "title": "Missing documentation for tf.train.Saver ", "body": "The documentation for tf.train.Saver (https://www.tensorflow.org/api_docs/python/tf/train/Saver) is not explaining how to actually use the saver. The crucial missing part:\r\n* `saver = tf.train.Saver()` needs to be executed right before finalizing the graph", "comments": ["Hi Marcel, can you clarify, please?\r\n\r\nThe code snippet on that page looks like:\r\n```\r\n...\r\n# Create a saver.\r\nsaver = tf.train.Saver(...variables...)\r\n# Launch the graph and train, saving the model every 1,000 steps.\r\nsess = tf.Session()\r\nfor step in xrange(1000000):\r\n    sess.run(..training_op..)\r\n    if step % 1000 == 0:\r\n        # Append the step number to the checkpoint name:\r\n        saver.save(sess, 'my-model', global_step=step)\r\n```\r\n\r\nThere's also the [Save and Restore guide](https://www.tensorflow.org/programmers_guide/saved_model#save_variables).\r\n", "Code is not a replacement for documentation ;) It would be good to have a) a fully functional example, b) a textual description of what's structurally happening. Especially what I mentioned above, what you need to execute in which order. ", "I'll work on updating the documentation.", "@dreamflasher Is this what you were looking for? https://www.tensorflow.org/api_docs/python/tf/train/Saver", "@Ayush517 That's perfect! Thanks for adding the example."]}, {"number": 17361, "title": "Feature request: Let Dataset.padded_batch() return a sequence_length tensor as a feature", "body": "### System information\r\n- **Have I written custom code**: Yes\r\n- **OS Platform and Distribution**: MacOS High Sierra v10.13.3\r\n- **TensorFlow installed from (source or binary)**: pip\r\n- **TensorFlow version (use command below)**: 1.5.0\r\n- **Python version**: 3.6.4 :: Anaconda, Inc.\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **GCC/Compiler version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: Intel Iris Pro 1536 MB\r\n- **Exact command to reproduce**: n/a\r\n\r\n### Describe the problem\r\nI am building a `tf.data.Dataset` that should return batches for model built around `tf.nn.dynamic_rnn`. The data consists of variable length time series, and i use `Dataset.padded_batch` to build batch tensors from these time series. When operating on variable length data, we want to pass a `sequence_length` tensor to `tf.nn.dynamic_rnn`. This tensor is also useful to extract the final valid outputs of the rnn for each series in the batch. It is possible to generate `sequence_length` in the model itself, knowing the padding symbol used by `Dataset.padded_batch`. This is unsatisfactory, however, as the padding symbol can be in the domain of the time series function, and thus, it is hard to know if you are looking at padding or real data.\r\n\r\n### Feature request\r\nI suggest modifying `Dataset.padded_batch` in such a way that it can optionally return a `sequence_length` tensor as one of it's features, storing the _true_ padding information. I think this would be very convenient for work with time series.\r\n", "comments": ["@mrry what are your thoughts on this.", "The difficulty here is that `Dataset.padded_batch()` is quite low level currently has no concept of \"sequences\", and it might complicate the interface to do so. \r\nI think I'd prefer this specific calculation to be separate from the padded batch, e.g. in a `Dataset.map()` that precedes the `Dataset.padded_batch()` and reads off the appropriate length using `tf.shape()`.\r\nIf there were a higher-level API for handling sequential data, this feature might make more sense. For example, c54a6ce4b53172569caa19991ec36be04121a359 added `tf.contrib.data.batch_by_sequence_length()`, which wraps `Dataset.padded_batch()`, and it could make sense to expose the sequence length from that API somehow.", "Nagging Assignee @mrry: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing this because the original feature request is unlikely to be fulfilled. Feel free to open another issue against the higher-level training APIs for sequence model training though."]}, {"number": 17360, "title": "C++ api: use of op::Attrs methods in gradients", "body": "The generated op::Attrs struct returns new instances on its chainable methods, and doesn't change the original object.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/d7d7f4eea5f3a2e63e12c803d02f56726c0e0513/tensorflow/cc/framework/cc_op_gen.cc#L701\r\n\r\nThere are a few related issues e.g. https://github.com/tensorflow/tensorflow/blob/6fdb9ad1baf7686a75f9e660178f7ac595e7fc2e/tensorflow/cc/gradients/nn_grad.cc#L164 where the code assumes the underlying object is being mutated and the parameters don't actually pass through.\r\n\r\nI guess there might be a couple of ways forward, depending on how Tensorflow prefers the C++ API:\r\n\r\n1. Decide the Attrs chaining methods mutate the underlying object and fix the code generation.\r\n2. Decide the Attrs chaining methods return new instances, and fix the uses.\r\n\r\nSuggestions?\r\n\r\nFwiw if option 2, it might be nice to add TF_MUST_USE_RESULT to the generated API. (Unfortunately a [long-standing bug in gcc](https://gcc.gnu.org/bugzilla/show_bug.cgi?id=38172) means this may be unreliable as an actual error across versions of gcc that contributors may use.)\r\n\r\n/cc @suharshs @keveman ", "comments": ["@kbsriram thanks for pointing out the bug. I am not a fan of mutable state, so my natural inclination is towards option 2. Adding `TF_MUST_USE_RESULT` to the generated API would simply point out the existing erroneous uses as compiler errors.", "@keveman thanks for the note! Sgtm - don't have a strong opinion on this. But would like to make progress on fixes and (for option 2 as you note) add a bit of a compile-time safety net for new code.\r\n\r\nTentatively predicated on option 2, suggestions on next steps? If someone is already on it, awesome - otherwise, I can sign up for a pull request on this."]}, {"number": 17359, "title": "Lack of documentation about: saving-restoring graphs & other languages API about this", "body": "I think that one of the main features of TensorFlow is its portability. I want to exploit it in a huge code written in C, where a small but computational heavy part can be computed using GPU through TF. So I started to study the problem about saving a graph, loading it to another code (possibly in another programming language) and passing it data to perform some computations.\r\n\r\nI'm finding very difficult to understand the documentation. I try to point the main problems:\r\n1- there is no C API documentation despite it is the only \"other language\" which satisfies the stability promises, there is only a link to the `c_api.h` file in the repo that I find very difficult to understand;\r\n2- it is very unclear how one can save a graph which can be restored and loaded in another code written in C (or maybe C++), and I don't find where is explained how one can run the graph in the other code passing data to it. Indeed, people had to write many tutorials, or ask many questions on StackOverflow which are unfortunately very outdated (e.g. [here](https://medium.com/jim-fleming/loading-a-tensorflow-graph-with-the-c-api-4caaff88463f) and [here](https://stackoverflow.com/questions/38947658/tensorflow-saving-into-loading-a-graph-from-a-file));\r\n3- as far as I can see there are many ways to do save/restoring but it is unclear when you should use one of these way or another. E.g. `saver.save` -> `saver.restore`; `tf.train.write_graph` -> `tf.import_graph_def` or C analogue `TF_GraphImportGraphDef`; `tf.saved_model.builder.SavedModelBuilder` -> `saved_model.loader.load` or C analogue `LoadSavedModel`.\r\n\r\nThe more I explore the documentation about this topic, the more questions arises, but I hope to have explained enough the issues I'm facing studying it. Shortly, I could say simply that we need a schematic and coherent page which can summarize the procedures about saving, loading, running graph at least from-to any languages which satisfy the stability promises, maybe with links to the specific standard methods and explanation about which of these ones could be used in a certain situation and why.\r\n", "comments": ["The C_API is easy to understand. It is an issue of writing in C such that the LOC is very large. But it is pretty straightforward to restore a graph in different languages.\r\n\r\n[These examples for C,C++,Go](https://github.com/PatWie/tensorflow_inference) work. Instead of wasting time by reading \"tutorials\", reading the TF source is more helpful. You probably cannot write a a small guide for loading a graph in C.\r\n\r\nThere is only one way of restoring a graph. The other stuff is syntactic sugar. In your issue description your probably confuse restoring a graph vs. restoring parameters.", "Indeed I can surely admit that I'm not so good understanding codes \"from scratch\" and I can be quite confused, but since I think in general this could be true for many people, this is the reason why well written documentations are so important. I'd like to contribute but first I need to understand the subject :)\r\n\r\nApart the general issue, I thank you for your answer, I'm watching your examples and they are interesting.", "Hey @iurilarosa ,\r\n\r\ni am having the same issues as you. I do not really like the documentation for C++. Therefore i created a litte github repository for dealing tensorflow with C++. Unfortunatly i am stucked with the restoring problem. At least i was able to save some checkpoints. (But i am using CMake)\r\n\r\nhttps://github.com/PinkySan/TensorflowHandlingTests\r\n\r\nIf you want to share it and support. ", "@sukritiramesh Can you help?", "Hi, thanks for the examples provided by @PatWie and another one found in #8033 I managed how to save and restore a graph in C (and also in Python). However, I absolutely can't say I understood everything, I took weeks to understand how the C APIs work and without the help I found I'm sure I'd still groping now. If someone could setup properly a webpage for the C APIs documentation I would be happy to contribute to write it for the part I understood.\r\nApart this, I still don't understand completely the purposes and the differences between \r\n- `tf.train.Saver()` with its `save` method -> model.meta files \r\n- `tf.train.write_graph` -> graph.pb files\r\n\r\nsince when I load a graph I used so far only the .meta files.\r\n@PinkySan thank you, unfortunately I'm bounded to use only C :(\r\nThank you everyone.", "[To restore the model and use it this is a good blog.](http://cv-tricks.com/tensorflow-tutorial/save-restore-tensorflow-models-quick-complete-tutorial/)", "Nagging Assignee @tatianashp: It has been 119 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 17358, "title": "Distributed training: Evaluation and inference best practices", "body": "I understand tensorflow distributed training and I implemented my own script.\r\n\r\nWhat I want to do now is to integrate the possibility of assigning some workers the task of asynchronously evaluate the model.\r\n\r\nLet's say we have 6 workers, what I want to do is to use 4 of them to do asynchronous training, one to periodically evaluate the model and another one to periodically make inference on it.\r\n\r\nMy intuition to achieve this goal is to do the following:\r\n\r\n```\r\n...\r\nelif FLAGS.job_name == \"worker\":\r\n\r\n    if FLAGS.task_index <= (len(cluster_dict[\"worker\"][:-2]) - 1):\r\n         logging.info(\"Training worker started\")\r\n         ...\r\n        with tf.device(tf.train.replica_device_setter(\r\n                worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\r\n                cluster=cluster,\r\n                ps_tasks=len(cluster_dict[\"ps\"])\r\n            )):\r\n                train_model = Model(\r\n                    mode=tf.contrib.learn.ModeKeys.TRAIN\r\n                )\r\n               with tf.train.MonitoredTrainingSession(\r\n                    is_chief=(FLAGS.task_index == 0),\r\n                    master=server.target,\r\n                    checkpoint_dir=ckpt_dir,\r\n                    config=config_proto,\r\n                    hooks=hooks\r\n                ) as mon_sess:\r\n                    while not mon_sess.should_stop():\r\n                        res = train_model.train(...)\r\n                        ...\r\n\r\n   elif FLAGS.task_index == (len(cluster_dict[\"worker\"][-2]) - 1):\r\n         logging.info(\"Evaluation worker started\")\r\n         ...\r\n        with tf.device(tf.train.replica_device_setter(\r\n                worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\r\n                cluster=cluster,\r\n                ps_tasks=len(cluster_dict[\"ps\"])\r\n            )):\r\n                eval_model = Model(\r\n                    mode=tf.contrib.learn.ModeKeys.EVAL\r\n                )\r\n                ...\r\n\r\n   elif FLAGS.task_index == (len(cluster_dict[\"worker\"][-1]) - 1):\r\n        logging.info(\"Inference worker started\")\r\n        ...\r\n        with tf.device(tf.train.replica_device_setter(\r\n                worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\r\n                cluster=cluster,\r\n                ps_tasks=len(cluster_dict[\"ps\"])\r\n            )):\r\n                infer_model = Model(\r\n                    mode=tf.contrib.learn.ModeKeys.INFER\r\n                )\r\n                ...\r\n```\r\n\r\nNow, what about the evaluation and inference sessions? \r\nFor training, I can use ```tf.train.MonitoredTrainingSession```, but for evaluation and inference I don't see such a cozy solution and the only possibility that I see is to use ```tf.Session```.\r\n\r\nRegarding the actual evaluation and inference loop, I thought to use a while loop inside which the worker periodically calls  ```eval_model.eval(...)``` or  ```infer_model.infer(...)```, but this means that the evaluation is performed considering the time and not considering the global_step and the only meaning that I can give to \"periodically\" is to sleep the thread.\r\n\r\nWhat do you think about this solution? Is it the correct way to asynchronously perform training, evaluation, and inference?\r\n\r\nAlberto\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 17357, "title": "Fix some minor typos in get_started docs to keep consistent", "body": "Described as the PR title.", "comments": []}, {"number": 17355, "title": "minor grappler controller bugs", "body": "* fixed (str.format bug when using HierarchicalController methods with verbose=True\r\n* fixed binary str vs str mismatch bug in controller.py and hierarchical_controller.py", "comments": ["Nagging Assignee @benoitsteiner: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 17354, "title": "Throw Exception instead of Crash when out of BFC memory allocation", "body": "For huge application model, the memory is easy to overflow and the framework will quit after plenty of Out-of-memory logs. Is it possible to just quickly throw an Exception in python runtime instead of killing the whole program?", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "@angersson Hi. This is a bug of Tensorflow, not a question.", "Can you reopen this issue?", "Currently Tensorflow did a force quit when OOM which is not catchable by python runtime.", "If you believe you've found a bug, file another issue that clearly indicates the problem and includes replicate-able code and the other required information from [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!"]}, {"number": 17352, "title": "[Suggestion]SVD GPU Op is not efficient than CPU.", "body": "when I used TF 1.2 version, there is no problem with using SVD\r\n\r\nbut few days ago i updated my tensorflow version to 1.5 and my code get extremely slow.\r\n\r\nso I tested all possibility and  checked issues\r\n\r\nfinally i found that someone commited SVD GPU op made by cudasolver. in june, 2017\r\n\r\nhe said that it contain memcopy and it cause bottleneck. it is true and it make svd GPU op extremely slow than former version.\r\n\r\nso please remove that op. and if possible, please make proper SVD GPU op  and gradients, and solve problem that tf.svd return nan value.\r\n\r\ni uploaded my naive solution to avoid nan value problem and code for compute svd's gradients.\r\n\r\nhttps://github.com/InhaDeeplearningGroup/Academic_research/blob/master/LSH/tensorflow_slim/svdGradients.py\r\n\r\n### System information\r\n- Linux Ubuntu 16.04\r\n- install by binary\r\n- TF version : i tested in 1.2, 1.3, 1.4,1.5 \r\n- python3.5\r\n- CUDA : i tested in 7.5, 8.0, 9.0\r\n- GPU : gtx1070", "comments": ["@rmlarsen  Can you comment on this?", "It's a known issue. Doing SVD in numpy can be 20x faster than in TensorFlow\r\nhttps://github.com/tensorflow/tensorflow/issues/13222#issuecomment-331642490\r\n\r\nSimilar with matrix inverses, it's faster to do it in numpy and feed the results back into tf using `feed_dict`", "cc @shamanDevel ", "@yaroslavvb yes it's a known issue. so why still have to maintain this inefficient op. i think former version is better than this. i didn't check time but i think at least it is similar with numpy.", "@sseung0703 they are both available, by default GPU is used if `tf.testing.is_gpu_available()`, but you can force CPU version by using `with tf.device('cpu:0'):` block. \r\n\r\nI agree that it's a regression, defaulting to such slow kernel for SVD gives another way for newcomers to shoot themselves in the foot.", "@yaroslavvb it is not good solution. it still has bottleneck cause of communication, but former version is not.\r\ntest it in TF 1.2 and later version. tf.svd in TF 1.2 is much faster than later version. \r\nmaybe i can change it by installing source. but you know \"defaulting to such slow kernel for SVD gives another way for newcomers to shoot themselves in the foot\".\r\n", "@sseung0703 tf 1.2 didn't have GPU version of SVD, so it would have communication bottleneck as well.", "@yaroslavvb oh, i didn't know that if there is no gpu op, it forcely use cpu.\r\nOk, so i use your way until tf.svd's gpu op work properly.\r\nbut i still don't know why TF maintain this op.\r\n", "This issue seems to have been resolved, has not had any extra discussion, and is already a known issue, so I'm closing it to keep the issue tracker focused."]}, {"number": 17351, "title": "Examples and tutorial missing from pip tensorflow package", "body": "After searching for and eventually posting a question on StackOverflow (https://stackoverflow.com/questions/49007742/tensorflow-examples-and-tutorials-missing-from-the-pip-package) I was recommended to report this as a bug:\r\n\r\nWhen installing Tensorflow (e.g. tensorflow-1.5.0-cp36-cp36m-manylinux1_x86_64.whl from https://pypi.python.org/pypi/tensorflow/1.5.0) from pip, there is only the tutorials/mnist example in tensorflow/examples. It would be nice to have the examples available without having to get them separately from GitHub. Is there a reason why the rest of the examples are not included? Can they be included in future releases?", "comments": ["@yifeif any comments on this?\r\n", "We want to keep our distribution as small as possible. As tutorials are not part of the APIs, we don't plan to include them in the standard package. `git clone` the repo would be the recommended way to get them. Hope this answer your question @spiiph.", "Thanks for the response.", "I have had a similar issue with that. To overcome this issue without downloading the whole repository, since I already installed the newest version through pip, I used SVN to download the specific part related to the examples,\r\n\r\nFor example https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples is the path to the examples folder on the github repository.\r\n\r\nTo use it with SVN, one simply replace the `tree/master` with `trunk` in the repo URL, resulting in the following command:\r\n\r\n`svn checkout https://github.com/tensorflow/tensorflow/trunk/tensorflow/examples`\r\n"]}, {"number": 17350, "title": "Feature request: Make div operator that sets 0/0=0 instead of 0/0=NaN", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: \r\nYes, code provided below\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 10 Pro, Version 1709\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary\r\n- **TensorFlow version (use command below)**:\r\n1.5.0\r\n- **Python version**: \r\n3.6.4\r\n- **Bazel version (if compiling from source)**:\r\n&mdash;\r\n- **GCC/Compiler version (if compiling from source)**:\r\n&mdash;\r\n- **CUDA/cuDNN version**:\r\nUsing processor, not GPU\r\n- **GPU model and memory**:\r\nUsing processor, not GPU\r\n- **Exact command to reproduce**:\r\nRun code provided below\r\n\r\n### Describe the problem\r\nWhen I divide 0/0 elements in TensorFlow, I get 0/0=NaN values. However, I want a division operator that returns 0/0=0 for these elements. Currently, I am using a workaround for this with a where operator, as can be seen in the d variable below. However, this decreases readability, is tedious when writing code with many div operators, and is probably less efficient than coding this functionality into source.\r\n\r\n### Source code / logs\r\n```\r\nimport tensorflow as tf\r\n\r\na = tf.constant([0], dtype=tf.float32)\r\nb = tf.constant([0], dtype=tf.float32)\r\nc = tf.divide(a, b)\r\nd = tf.where(tf.less(a, 1e-7), a, a/b)\r\n\r\nwith tf.Session() as sess:\r\n    print(sess.run(c))\r\n    print(sess.run(e))\r\n```", "comments": ["@mrry Any thoughts on this? ", "Yes, I agree that it is a problem indeed, and I proposed to add safe_div op in  #15706.", "I have no objections, but I am not responsible for the math library, so we'd want to find another reviewer. Tentatively marking as \"Contributions welcome\" for now.", "This is fundamentally false and shouldn't be added as a default. if this is a needed addition, there needs to be a config flag, cl flag, threshold for floats, or some other qualification. This is a breaking change. If truly necessary add a new operator or have the user implement a user function."]}, {"number": 17349, "title": "RuntimeError: TOCO failed see console for info.", "body": "I tried your way but its giving error. . . Kindly help. . . tf.version = 1.6.0rc1 and Ubuntu\r\n\r\n**_#ERROR is as follows:_**\r\n\r\nConverted 2 variables to const ops.\r\nTraceback (most recent call last):\r\n\r\nFile \"tf.py\", line 94, in \r\ntflite_model=tf.contrib.lite.toco_convert(sess.graph_def,[x],[mxc])\r\n\r\nFile \"/home/siteurl/anaconda3/envs/osrco/lib/python3.6/site-packages/tensorflow/contrib/lite/python/lite.py\", line 212, in toco_convert\r\ninput_data.SerializeToString())\r\n\r\nFile \"/home/siteurl/anaconda3/envs/osrco/lib/python3.6/site-packages/tensorflow/contrib/lite/python/lite.py\", line 134, in toco_convert_protos\r\n(stdout, stderr))\r\n\r\n### RuntimeError: TOCO failed see console for info.\r\nb'2018-03-01 14:40:13.614956: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171]\r\n\r\nConverting unsupported operation: VariableV2\\n2018-03-01 14:40:13.615016: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171]\r\n\r\nConverting unsupported operation: Assign\\n2018-03-01 14:40:13.615059: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171]\r\n\r\nConverting unsupported operation: VariableV2\\n2018-03-01 14:40:13.615083: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171]\r\n\r\nConverting unsupported operation: Assign\\n2018-03-01 14:40:13.615118: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171]\r\n\r\nConverting unsupported operation: Log\\n2018-03-01 14:40:13.615212: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171]\r\n\r\nConverting unsupported operation: DynamicStitch\\n2018-03-01 14:40:13.615283: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171]\r\n\r\nConverting unsupported operation: Reciprocal\\n2018-03-01 14:40:13.615341: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171]\r\n\r\nConverting unsupported operation: BroadcastGradientArgs\\n2018-03-01 14:40:13.615384: F tensorflow/contrib/lite/toco/import_tensorflow.cc:973]\r\n\r\nCheck failed: GetBoolAttr(node, \"transpose_b\") == false (1 vs. 0)\\nAborted (core dumped)\\n'\r\nNone\r\n\r\n**_#CODE is as follows:_**\r\n\r\nimport tensorflow as tf\r\nimport pandas as pd\r\nimport numpy as np\r\nimport tempfile\r\nimport subprocess\r\ntf.contrib.lite.tempfile = tempfile\r\ntf.contrib.lite.subprocess = subprocess\r\n\r\nfrom tensorflow.python.tools import freeze_graph\r\nfrom tensorflow.python.tools import optimize_for_inference_lib\r\n\r\nprint(tf.version)\r\ndata=pd.read_csv('iris.data',names=['f1','f2','f3','f4','f5'])\r\n\r\ns=np.asarray([1,0,0])\r\nve=np.asarray([0,1,0])\r\nvi=np.asarray([0,0,1])\r\n\r\ndata['f5']=data['f5'].map({'Iris-setosa':s,'Iris-versicolor':ve,'Iris-virginica':vi})\r\n\r\n#print(data)\r\n\r\ndata=data.iloc[np.random.permutation(len(data))]\r\n\r\nprint(data)\r\n\r\ndata=data.reset_index(drop=True)\r\n\r\n#training data\r\ntrainFeats=data.ix[0:105,['f1','f2','f3','f4']]\r\ntemp=data['f5']\r\ntrainlabels=temp[0:106]\r\n\r\ny=tf.placeholder(tf.float32,shape=[106, 3])\r\n#weight and bias\r\nm=tf.Variable(tf.zeros([4,3]))\r\nx=tf.placeholder(tf.float32,shape=[106,4],name=\"Input\")\r\nc=tf.Variable(tf.zeros([3]))\r\nmxc = tf.nn.softmax((tf.matmul(x, m) + c) ,name=\"output\")\r\n\r\nloss = tf.reduce_mean(-tf.reduce_sum(y * tf.log(mxc), reduction_indices=[1]))\r\n\r\ntrain_step = tf.train.AdamOptimizer(0.01).minimize(loss)\r\n\r\nsess = tf.InteractiveSession()\r\ninit = tf.initialize_all_variables()\r\nsess.run(init)\r\n\r\n#_=tf.contrib.tflite.Convert(sess.graph_def,[x],[mxc])\r\n\r\n#number of interations\r\nepoch=2000\r\nfor step in range(epoch):\r\nprint(sess.run([train_step,loss], feed_dict={x: trainFeats, y:[t for t in trainlabels.as_matrix()]}))\r\n\r\n#testData=data.ix[130,['f1','f2','f3','f4']]\r\n#testDataInFrormat=testData.reshape(1,4)\r\n#print(sess.run(tf.argmax(mxc),feed_dict={x:testDataInFrormat}))\r\n\r\ntf.train.write_graph(sess.graph_def,'pbtxtFiles/','savegraph.pbtxt',as_text=True)\r\n\r\ntf.train.Saver().save(sess,'pbtxtFiles/model.ckpt')\r\n\r\nMODEL_NAME = 'iris'\r\ninput_graph_path = 'pbtxtFiles/savegraph.pbtxt'\r\ncheckpoint_path = 'pbtxtFiles/model.ckpt'\r\ninput_saver_def_path = \"\"\r\ninput_binary = False\r\noutput_node_names = \"output\"\r\nrestore_op_name = \"save/restore_all\"\r\nfilename_tensor_name = \"save/Const:0\"\r\noutput_frozen_graph_name = 'pbtxtFiles/frozen_model_'+MODEL_NAME+'.pb'\r\noutput_optimized_graph_name = 'pbtxtFiles/optimized_inference_model_'+MODEL_NAME+'.pb'\r\nclear_devices = True\r\n\r\nfreeze_graph.freeze_graph(input_graph_path, input_saver_def_path,\r\ninput_binary, checkpoint_path, output_node_names,\r\nrestore_op_name, filename_tensor_name,\r\noutput_frozen_graph_name, clear_devices, \"\")\r\n\r\noutput_graph_def = optimize_for_inference_lib.optimize_for_inference(\r\nsess.graph_def,\r\n[\"Input\"], # an array of the input node(s)\r\n[\"output\"], # an array of output nodes\r\ntf.float32.as_datatype_enum)\r\n\r\ntflite_model=tf.contrib.lite.toco_convert(sess.graph_def,[x],[mxc])\r\nopen(\"wow.tflite\",\"w\").write(tflite_model)\r\n\r\n**_#IRIS.DATA is this:_**\r\n\r\n5.1,3.5,1.4,0.2,Iris-setosa\r\n4.9,3.0,1.4,0.2,Iris-setosa\r\n4.7,3.2,1.3,0.2,Iris-setosa\r\n4.6,3.1,1.5,0.2,Iris-setosa\r\n5.0,3.6,1.4,0.2,Iris-setosa\r\n5.4,3.9,1.7,0.4,Iris-setosa\r\n4.6,3.4,1.4,0.3,Iris-setosa\r\n5.0,3.4,1.5,0.2,Iris-setosa\r\n4.4,2.9,1.4,0.2,Iris-setosa\r\n4.9,3.1,1.5,0.1,Iris-setosa\r\n5.4,3.7,1.5,0.2,Iris-setosa\r\n4.8,3.4,1.6,0.2,Iris-setosa\r\n4.8,3.0,1.4,0.1,Iris-setosa\r\n4.3,3.0,1.1,0.1,Iris-setosa\r\n5.8,4.0,1.2,0.2,Iris-setosa\r\n5.7,4.4,1.5,0.4,Iris-setosa\r\n5.4,3.9,1.3,0.4,Iris-setosa\r\n5.1,3.5,1.4,0.3,Iris-setosa\r\n5.7,3.8,1.7,0.3,Iris-setosa\r\n5.1,3.8,1.5,0.3,Iris-setosa\r\n5.4,3.4,1.7,0.2,Iris-setosa\r\n5.1,3.7,1.5,0.4,Iris-setosa\r\n4.6,3.6,1.0,0.2,Iris-setosa\r\n5.1,3.3,1.7,0.5,Iris-setosa\r\n4.8,3.4,1.9,0.2,Iris-setosa\r\n5.0,3.0,1.6,0.2,Iris-setosa\r\n5.0,3.4,1.6,0.4,Iris-setosa\r\n5.2,3.5,1.5,0.2,Iris-setosa\r\n5.2,3.4,1.4,0.2,Iris-setosa\r\n4.7,3.2,1.6,0.2,Iris-setosa\r\n4.8,3.1,1.6,0.2,Iris-setosa\r\n5.4,3.4,1.5,0.4,Iris-setosa\r\n5.2,4.1,1.5,0.1,Iris-setosa\r\n5.5,4.2,1.4,0.2,Iris-setosa\r\n4.9,3.1,1.5,0.1,Iris-setosa\r\n5.0,3.2,1.2,0.2,Iris-setosa\r\n5.5,3.5,1.3,0.2,Iris-setosa\r\n4.9,3.1,1.5,0.1,Iris-setosa\r\n4.4,3.0,1.3,0.2,Iris-setosa\r\n5.1,3.4,1.5,0.2,Iris-setosa\r\n5.0,3.5,1.3,0.3,Iris-setosa\r\n4.5,2.3,1.3,0.3,Iris-setosa\r\n4.4,3.2,1.3,0.2,Iris-setosa\r\n5.0,3.5,1.6,0.6,Iris-setosa\r\n5.1,3.8,1.9,0.4,Iris-setosa\r\n4.8,3.0,1.4,0.3,Iris-setosa\r\n5.1,3.8,1.6,0.2,Iris-setosa\r\n4.6,3.2,1.4,0.2,Iris-setosa\r\n5.3,3.7,1.5,0.2,Iris-setosa\r\n5.0,3.3,1.4,0.2,Iris-setosa\r\n7.0,3.2,4.7,1.4,Iris-versicolor\r\n6.4,3.2,4.5,1.5,Iris-versicolor\r\n6.9,3.1,4.9,1.5,Iris-versicolor\r\n5.5,2.3,4.0,1.3,Iris-versicolor\r\n6.5,2.8,4.6,1.5,Iris-versicolor\r\n5.7,2.8,4.5,1.3,Iris-versicolor\r\n6.3,3.3,4.7,1.6,Iris-versicolor\r\n4.9,2.4,3.3,1.0,Iris-versicolor\r\n6.6,2.9,4.6,1.3,Iris-versicolor\r\n5.2,2.7,3.9,1.4,Iris-versicolor\r\n5.0,2.0,3.5,1.0,Iris-versicolor\r\n5.9,3.0,4.2,1.5,Iris-versicolor\r\n6.0,2.2,4.0,1.0,Iris-versicolor\r\n6.1,2.9,4.7,1.4,Iris-versicolor\r\n5.6,2.9,3.6,1.3,Iris-versicolor\r\n6.7,3.1,4.4,1.4,Iris-versicolor\r\n5.6,3.0,4.5,1.5,Iris-versicolor\r\n5.8,2.7,4.1,1.0,Iris-versicolor\r\n6.2,2.2,4.5,1.5,Iris-versicolor\r\n5.6,2.5,3.9,1.1,Iris-versicolor\r\n5.9,3.2,4.8,1.8,Iris-versicolor\r\n6.1,2.8,4.0,1.3,Iris-versicolor\r\n6.3,2.5,4.9,1.5,Iris-versicolor\r\n6.1,2.8,4.7,1.2,Iris-versicolor\r\n6.4,2.9,4.3,1.3,Iris-versicolor\r\n6.6,3.0,4.4,1.4,Iris-versicolor\r\n6.8,2.8,4.8,1.4,Iris-versicolor\r\n6.7,3.0,5.0,1.7,Iris-versicolor\r\n6.0,2.9,4.5,1.5,Iris-versicolor\r\n5.7,2.6,3.5,1.0,Iris-versicolor\r\n5.5,2.4,3.8,1.1,Iris-versicolor\r\n5.5,2.4,3.7,1.0,Iris-versicolor\r\n5.8,2.7,3.9,1.2,Iris-versicolor\r\n6.0,2.7,5.1,1.6,Iris-versicolor\r\n5.4,3.0,4.5,1.5,Iris-versicolor\r\n6.0,3.4,4.5,1.6,Iris-versicolor\r\n6.7,3.1,4.7,1.5,Iris-versicolor\r\n6.3,2.3,4.4,1.3,Iris-versicolor\r\n5.6,3.0,4.1,1.3,Iris-versicolor\r\n5.5,2.5,4.0,1.3,Iris-versicolor\r\n5.5,2.6,4.4,1.2,Iris-versicolor\r\n6.1,3.0,4.6,1.4,Iris-versicolor\r\n5.8,2.6,4.0,1.2,Iris-versicolor\r\n5.0,2.3,3.3,1.0,Iris-versicolor\r\n5.6,2.7,4.2,1.3,Iris-versicolor\r\n5.7,3.0,4.2,1.2,Iris-versicolor\r\n5.7,2.9,4.2,1.3,Iris-versicolor\r\n6.2,2.9,4.3,1.3,Iris-versicolor\r\n5.1,2.5,3.0,1.1,Iris-versicolor\r\n5.7,2.8,4.1,1.3,Iris-versicolor\r\n6.3,3.3,6.0,2.5,Iris-virginica\r\n5.8,2.7,5.1,1.9,Iris-virginica\r\n7.1,3.0,5.9,2.1,Iris-virginica\r\n6.3,2.9,5.6,1.8,Iris-virginica\r\n6.5,3.0,5.8,2.2,Iris-virginica\r\n7.6,3.0,6.6,2.1,Iris-virginica\r\n4.9,2.5,4.5,1.7,Iris-virginica\r\n7.3,2.9,6.3,1.8,Iris-virginica\r\n6.7,2.5,5.8,1.8,Iris-virginica\r\n7.2,3.6,6.1,2.5,Iris-virginica\r\n6.5,3.2,5.1,2.0,Iris-virginica\r\n6.4,2.7,5.3,1.9,Iris-virginica\r\n6.8,3.0,5.5,2.1,Iris-virginica\r\n5.7,2.5,5.0,2.0,Iris-virginica\r\n5.8,2.8,5.1,2.4,Iris-virginica\r\n6.4,3.2,5.3,2.3,Iris-virginica\r\n6.5,3.0,5.5,1.8,Iris-virginica\r\n7.7,3.8,6.7,2.2,Iris-virginica\r\n7.7,2.6,6.9,2.3,Iris-virginica\r\n6.0,2.2,5.0,1.5,Iris-virginica\r\n6.9,3.2,5.7,2.3,Iris-virginica\r\n5.6,2.8,4.9,2.0,Iris-virginica\r\n7.7,2.8,6.7,2.0,Iris-virginica\r\n6.3,2.7,4.9,1.8,Iris-virginica\r\n6.7,3.3,5.7,2.1,Iris-virginica\r\n7.2,3.2,6.0,1.8,Iris-virginica\r\n6.2,2.8,4.8,1.8,Iris-virginica\r\n6.1,3.0,4.9,1.8,Iris-virginica\r\n6.4,2.8,5.6,2.1,Iris-virginica\r\n7.2,3.0,5.8,1.6,Iris-virginica\r\n7.4,2.8,6.1,1.9,Iris-virginica\r\n7.9,3.8,6.4,2.0,Iris-virginica\r\n6.4,2.8,5.6,2.2,Iris-virginica\r\n6.3,2.8,5.1,1.5,Iris-virginica\r\n6.1,2.6,5.6,1.4,Iris-virginica\r\n7.7,3.0,6.1,2.3,Iris-virginica\r\n6.3,3.4,5.6,2.4,Iris-virginica\r\n6.4,3.1,5.5,1.8,Iris-virginica\r\n6.0,3.0,4.8,1.8,Iris-virginica\r\n6.9,3.1,5.4,2.1,Iris-virginica\r\n6.7,3.1,5.6,2.4,Iris-virginica\r\n6.9,3.1,5.1,2.3,Iris-virginica\r\n5.8,2.7,5.1,1.9,Iris-virginica\r\n6.8,3.2,5.9,2.3,Iris-virginica\r\n6.7,3.3,5.7,2.5,Iris-virginica\r\n6.7,3.0,5.2,2.3,Iris-virginica\r\n6.3,2.5,5.0,1.9,Iris-virginica\r\n6.5,3.0,5.2,2.0,Iris-virginica\r\n6.2,3.4,5.4,2.3,Iris-virginica\r\n5.9,3.0,5.1,1.8,Iris-virginica\r\n\r\nKindly help MR. @aselle\r\nKindly help MR. @facaiy\r\nKindly help MR. @leandroBorgesFerreira\r\nKindly help MR. @javierluraschi\r\nKindly help MR. @tensorflowbutler", "comments": ["above code is not giving any output. . . _wow.tflite_ is not even created. . . Please help. . .\r\n\r\n", "> Check failed: GetBoolAttr(node, \"transpose_b\") == false (1 vs. 0)\r\n\r\nThat means you have a matmul with transposition, but that isn't supported yet.", "@SanthoshMKunthe  I think this might be fixed now. Would you mind giving it another try?", "This does not appear to be fixed on master as of today", "The presence of the Assign and Variable ops means this is not an inference graph. You need to use the optimize_for_inference_script for it to work.\r\n", "Problem is that you're not freezing the graph, which converts variables into constants.\r\n\r\nBefore you call the TOCO convert, you need to convert variables to constants to produce a frozen graph def. Then pipe that frozen graph def into the TOCO convert.\r\n\r\nRight now, you're referencing the sess.graph_def instead of the frozen graph.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "hi \r\n@tofulawrence i met the same wrong ,with the info :\r\nRuntimeError: TOCO failed see console for info.\r\nb'e:\\\\anaconda3\\\\lib\\\\site-packages\\\\h5py\\\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\\r\\n  from ._conv import register_converters as _register_converters\\r\\nTraceback (most recent call last):\\r\\n  File \"e:\\\\anaconda3\\\\lib\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\tensorflow_wrap_toco.py\", line 18, in swig_import_helper\\r\\n    fp, pathname, description = imp.find_module(\\'_tensorflow_wrap_toco\\', [dirname(__file__)])\\r\\n  File \"e:\\\\anaconda3\\\\lib\\\\imp.py\", line 297, in find_module\\r\\n    raise ImportError(_ERR_MSG.format(name), name=name)\\r\\nImportError: No module named \\'_tensorflow_wrap_toco\\'\\r\\n\\r\\nDuring handling of the above exception, another exception occurred:\\r\\n\\r\\nTraceback (most recent call last):\\r\\n  File \"e:\\\\anaconda3\\\\lib\\\\runpy.py\", line 193, in _run_module_as_main\\r\\n    \"__main__\", mod_spec)\\r\\n  File \"e:\\\\anaconda3\\\\lib\\\\runpy.py\", line 85, in _run_code\\r\\n    exec(code, run_globals)\\r\\n  File \"E:\\\\Anaconda3\\\\Scripts\\\\toco_from_protos.exe\\\\__main__.py\", line 5, in <module>\\r\\n  File \"e:\\\\anaconda3\\\\lib\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\toco_from_protos.py\", line 22, in <module>\\r\\n    from tensorflow.contrib.lite.toco.python import tensorflow_wrap_toco\\r\\n  File \"e:\\\\anaconda3\\\\lib\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\tensorflow_wrap_toco.py\", line 28, in <module>\\r\\n    _tensorflow_wrap_toco = swig_import_helper()\\r\\n  File \"e:\\\\anaconda3\\\\lib\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\tensorflow_wrap_toco.py\", line 20, in swig_import_helper\\r\\n    import _tensorflow_wrap_toco\\r\\nModuleNotFoundError: No module named \\'_tensorflow_wrap_toco\\'\\r\\n'\r\nNone\r\ncould you give me some advice", "INFO:\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\nTensorFlow installed from (source or binary): Anaconda \r\nTensorFlow version (use command below): 1.11\r\nPython version: 3.5\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\nExact command to reproduce:\r\n\r\nHi, \r\nI get the same error as the above. And I've been using the example given in [converter python api guide](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/convert/python_api.md)\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nmodel = tf.keras.models.Sequential()\r\nmodel.add(tf.keras.layers.Dense(2, input_shape=(3,)))\r\nmodel.add(tf.keras.layers.RepeatVector(3))\r\nmodel.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(3)))\r\nmodel.compile(loss=tf.keras.losses.MSE,\r\n              optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),\r\n              metrics=[tf.keras.metrics.categorical_accuracy],\r\n              sample_weight_mode='temporal')\r\nx = np.random.random((1, 3))\r\ny = np.random.random((1, 3, 3))\r\nmodel.train_on_batch(x, y)\r\nmodel.predict(x)\r\nkeras_file = \"keras_model.h5\"\r\ntf.keras.models.save_model(model, keras_file)\r\nconverter = tf.contrib.lite.TFLiteConverter.from_keras_model_file(keras_file)\r\ntflite_model = converter.convert()`\r\n\r\n\r\nI ran this code and this is the error I get:\r\n\r\n> RuntimeError: TOCO failed see console for info.\r\nb'Traceback (most recent call last):\\r\\n  File \"C:\\\\Users\\\\sgavvala\\\\AppData\\\\Roaming\\\\Python\\\\Python35\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\tensorflow_wrap_toco.py\", line 18, in swig_import_helper\\r\\n    fp, pathname, description = imp.find_module(\\'_tensorflow_wrap_toco\\', [dirname(__file__)])\\r\\n  File \"c:\\\\users\\\\sgavvala\\\\appdata\\\\local\\\\continuum\\\\anaconda3\\\\envs\\\\tensorflow\\\\lib\\\\imp.py\", line 297, in find_module\\r\\n    raise ImportError(_ERR_MSG.format(name), name=name)\\r\\nImportError: No module named \\'_tensorflow_wrap_toco\\'\\r\\n\\r\\nDuring handling of the above exception, another exception occurred:\\r\\n\\r\\nTraceback (most recent call last):\\r\\n  File \"c:\\\\users\\\\sgavvala\\\\appdata\\\\local\\\\continuum\\\\anaconda3\\\\envs\\\\tensorflow\\\\lib\\\\runpy.py\", line 193, in _run_module_as_main\\r\\n    \"__main__\", mod_spec)\\r\\n  File \"c:\\\\users\\\\sgavvala\\\\appdata\\\\local\\\\continuum\\\\anaconda3\\\\envs\\\\tensorflow\\\\lib\\\\runpy.py\", line 85, in _run_code\\r\\n    exec(code, run_globals)\\r\\n  File \"C:\\\\Users\\\\sgavvala\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\envs\\\\tensorflow\\\\Scripts\\\\toco_from_protos.exe\\\\__main__.py\", line 5, in <module>\\r\\n  File \"C:\\\\Users\\\\sgavvala\\\\AppData\\\\Roaming\\\\Python\\\\Python35\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\toco_from_protos.py\", line 22, in <module>\\r\\n    from tensorflow.contrib.lite.toco.python import tensorflow_wrap_toco\\r\\n  File \"C:\\\\Users\\\\sgavvala\\\\AppData\\\\Roaming\\\\Python\\\\Python35\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\tensorflow_wrap_toco.py\", line 28, in <module>\\r\\n    _tensorflow_wrap_toco = swig_import_helper()\\r\\n  File \"C:\\\\Users\\\\sgavvala\\\\AppData\\\\Roaming\\\\Python\\\\Python35\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\tensorflow_wrap_toco.py\", line 20, in swig_import_helper\\r\\n    import _tensorflow_wrap_toco\\r\\nImportError: No module named \\'_tensorflow_wrap_toco\\'\\r\\n'\r\nNone\r\n\r\nMy plan is to convert a keras model with my custom layers into tflite quantized version. Figured I would start with a given example but that doesn't seem to run.", "I am getting the same error. I am trying to load from a saved model which is saved in the form of a pb file generated from simple_save()\r\n\r\n TOCO failed see console for info.\r\nb'Traceback (most recent call last):\\r\\n  File \"C:\\\\anaconda3\\\\envs\\\\for Quantization\\\\lib\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\tensorflow_wrap_toco.py\", line 18, in swig_import_helper\\r\\n    fp, pathname, description = imp.find_module(\\'_tensorflow_wrap_toco\\', [dirname(__file__)])\\r\\n  File \"C:\\\\anaconda3\\\\envs\\\\for Quantization\\\\lib\\\\imp.py\", line 297, in find_module\\r\\n    raise ImportError(_ERR_MSG.format(name), name=name)\\r\\nImportError: No module named \\'_tensorflow_wrap_toco\\'\\r\\n\\r\\nDuring handling of the above exception, another exception occurred:\\r\\n\\r\\nTraceback (most recent call last):\\r\\n  File \"C:\\\\anaconda3\\\\envs\\\\for Quantization\\\\Scripts\\\\toco_from_protos-script.py\", line 6, in <module>\\r\\n    from tensorflow.contrib.lite.toco.python.toco_from_protos import main\\r\\n  File \"C:\\\\anaconda3\\\\envs\\\\for Quantization\\\\lib\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\toco_from_protos.py\", line 22, in <module>\\r\\n    from tensorflow.contrib.lite.toco.python import tensorflow_wrap_toco\\r\\n  File \"C:\\\\anaconda3\\\\envs\\\\for Quantization\\\\lib\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\tensorflow_wrap_toco.py\", line 28, in <module>\\r\\n    _tensorflow_wrap_toco = swig_import_helper()\\r\\n  File \"C:\\\\anaconda3\\\\envs\\\\for Quantization\\\\lib\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\tensorflow_wrap_toco.py\", line 20, in swig_import_helper\\r\\n    import _tensorflow_wrap_toco\\r\\nModuleNotFoundError: No module named \\'_tensorflow_wrap_toco\\'\\r\\n'\r\nNone", "This is a code from the documentation (https://www.tensorflow.org/lite/convert/python_api)\r\n<code>\r\n<img width=\"1129\" alt=\"Screen Shot 2019-03-23 at 4 40 26 PM\" src=\"https://user-images.githubusercontent.com/26170275/54865364-66617780-4d8a-11e9-8f5d-f0551153a643.png\">\r\n\r\nI am getting this error:\r\n<error>\r\n<img width=\"1134\" alt=\"Screen Shot 2019-03-23 at 4 41 13 PM\" src=\"https://user-images.githubusercontent.com/26170275/54865372-7f6a2880-4d8a-11e9-88fc-e75770df3b50.png\">\r\n\r\nMy tensorFlow version is '1.12.0'\r\nPlease help me.", "> This is a code from the documentation (https://www.tensorflow.org/lite/convert/python_api)\r\n> ` <img alt=\"Screen Shot 2019-03-23 at 4 40 26 PM\" width=\"1129\" src=\"https://user-images.githubusercontent.com/26170275/54865364-66617780-4d8a-11e9-8f5d-f0551153a643.png\">`\r\n> \r\n> ` I am getting this error: <img alt=\"Screen Shot 2019-03-23 at 4 41 13 PM\" width=\"1134\" src=\"https://user-images.githubusercontent.com/26170275/54865372-7f6a2880-4d8a-11e9-88fc-e75770df3b50.png\"> My tensorFlow version is '1.12.0' Please help me.`\r\n\r\nHave you solved your problem? I met the same one.", "@iamhankai .\r\nWhat i did for converting model to TFlite is, used **tflite_convert** command in terminal.\r\nExample: `tflite_convert   --output_file=too.tflite   --keras_model_file=keras_model.h5`\r\nInput file: ' keras_model.h5'\r\nOutput file: 'too.tflite'", "> @iamhankai .\r\n> What i did for converting model to TFlite is, used **tflite_convert** command in terminal.\r\n> Example: `tflite_convert --output_file=too.tflite --keras_model_file=keras_model.h5`\r\n> Input file: ' keras_model.h5'\r\n> Output file: 'too.tflite'\r\n\r\nThanks~"]}, {"number": 17348, "title": "when run ./bazel-bin/tensorflow/python/tools/freeze_graph AttributeError: 'int' object attribute '__doc__' is read-only", "body": "tensorflow r1.5\r\npython2.7\r\nfollowed by the tensorflow lite tutorial:\r\nhttps://github.com/tensorflow/tensorflow/tree/r1.5/tensorflow/contrib/lite\r\nmy command line:\r\n```shell\r\n./bazel-bin/tensorflow/python/tools/freeze_graph    --input_graph=~/Code/mobilenet_v1_1.0_224_frozen.pb     --input_checkpoint=~/Code/mobilenet_v1_1.0_224.ckpt     --input_binary=true --output_graph=~/code/frozen.pb     --output_node_names=MobileNet/Predictions/Reshape_1\r\nand get:\r\nTraceback (most recent call last):\r\n  File \"/home/zero/Code/tmp/tensorflow-r1.5/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 45, in <module>\r\n    from tensorflow.core.framework import graph_pb2\r\n  File \"/home/zero/Code/tmp/tensorflow-r1.5/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/home/zero/Code/tmp/tensorflow-r1.5/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 63, in <module>\r\n    from tensorflow.python.framework.framework_lib import *\r\n  File \"/home/zero/Code/tmp/tensorflow-r1.5/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/framework/framework_lib.py\", line 102, in <module>\r\n    from tensorflow.python.framework.importer import import_graph_def\r\n  File \"/home/zero/Code/tmp/tensorflow-r1.5/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/framework/importer.py\", line 33, in <module>\r\n    from tensorflow.python.framework import function\r\n  File \"/home/zero/Code/tmp/tensorflow-r1.5/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/framework/function.py\", line 38, in <module>\r\n    from tensorflow.python.ops import variable_scope as vs\r\n  File \"/home/zero/Code/tmp/tensorflow-r1.5/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/ops/variable_scope.py\", line 192, in <module>\r\n    \"\"\"\r\nAttributeError: 'int' object attribute '__doc__' is read-only\r\n```", "comments": ["You're using a frozen graph as input. Note that the tutorial says:\r\n\r\n```\r\nbazel-bin/tensorflow/python/tools/freeze_graph\\\r\n    --input_graph=/tmp/mobilenet_v1_224.pb \\ <- not frozen\r\n    --input_checkpoint=/tmp/checkpoints/mobilenet-10202.ckpt \\\r\n    --input_binary=true --output_graph=/tmp/frozen_mobilenet_v1_224.pb \\\r\n    --output_node_names=MobileNet/Predictions/Reshape_1\r\n```\r\n\r\nIf you have any more problems, please ask a question on [Stack Overflow](http://stackoverflow.com/questions/tagged/tensorflow-lite) to help you see if you've found a bug. Thanks!"]}, {"number": 17347, "title": "[suggestion] New option for dynamic_decode function", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.4\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: \r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 8\r\n- **GPU model and memory**: 1080ti, 11GB\r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\n\r\nI found that while using `dynamic_decode` with option `maximum_iterations`, The first priority of output length is the maximum length of given decoder input when **maximum length of decoder input < maximum_iteration**. Can you provide a new option that I can just fix the size of decoder output size, not considering the maximum length of given decoder input.\r\n\r\n### example code\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n\r\nbatch_size = 4\r\nhidden_size = 5\r\n\r\n# [batch, sequence_length, depth]\r\n\r\nmemory = tf.get_variable('memory', [4, 10, 5], dtype = tf.float32)\r\nlen_enc = tf.Variable([5,6,7,8])\r\n\r\n# [batch, sequence_length, depth]\r\nembd_dec_input = tf.get_variable('dec_input', [4, 8, 5], dtype = tf.float32)\r\nlen_dec = tf.Variable([3,4,5,6])\r\n\r\n\r\n# Create an attention mechanism\r\nattention_mechanism = tf.contrib.seq2seq.LuongAttention(\r\n        hidden_size, memory,\r\n        memory_sequence_length=len_enc)\r\n\r\n# Build decoder cell\r\ndecoder_cell = tf.nn.rnn_cell.GRUCell(hidden_size)\r\n\r\ndecoder_cell = tf.contrib.seq2seq.AttentionWrapper(\r\n        decoder_cell, attention_mechanism,\r\n        attention_layer_size=hidden_size)\r\n\r\n# Helper for decoder cell\r\nhelper = tf.contrib.seq2seq.TrainingHelper(\r\n    embd_dec_input, len_dec\r\n)\r\n\r\n# Decoder initial state\r\ninitial_state = decoder_cell.zero_state(dtype = tf.float32, batch_size = batch_size)\r\n\r\n# Decoder\r\ndecoder = tf.contrib.seq2seq.BasicDecoder(\r\n    decoder_cell, helper, initial_state,\r\n    output_layer=None)\r\n\r\n# Simply fix the decoder output length as the length of decoder input\r\nmax_iter = 8\r\n\r\noutputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, impute_finished = True, maximum_iterations = max_iter)\r\n```\r\n\r\n**Even my maximum length of decoder input length is 6, I want the decoder to output with maximum length 8**", "comments": ["Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you."]}, {"number": 17346, "title": "Add default whl file location and minor update comments", "body": "Hi, after trying these dist_test scripts, open this PR for:\r\n  - `./local_test.sh` need a required arg `WHL_FILE_LOCATION`\r\n    - Add default whl file location URL\r\n  - Update comments and README\r\n\r\n@caisq @av8ramit PTAL, thanks!", "comments": ["@caisq Could you take a look at this please?", "@martinwicke @jhaux PTAL, thanks!"]}, {"number": 17345, "title": "Fix error : ConvNDLSTMCell does not pass name parameter", "body": "If I create a ConvNDLSTMCell in tensorflow.contrib.rnn, name parameter cannot be passed through ND-ConvLSTMCell classes. So simply fix it.", "comments": []}, {"number": 17344, "title": "Does the size of embedding have a huge influence on the training speed?", "body": "In my text classification task, I use vectors to represent the words. After I enlarge the size of vocabulary (word dimension is still 128), from 200000 to 1000000, the training step time arise from 0.1s to 0.4s. Here is the word embedding tensor creatation:\r\n\r\n`tf.get_variable('embedding', shape, dtype=dtype, return tf.get_variable('embedding', shape, dtype=dtype, initializer=tf.random_normal_initializer(stddev=0.1), trainable=True)`\r\n\r\nAnd the sentences are made of word ids and vectors will be looked up from the embedding tensor above.\r\n\r\n`inputs_embedding = tf.contrib.layers.embedding_lookup_unique(embedding, inputs)`\r\n\r\nSo what confused me is that why the size of vocabulary has a significant impact on the training speed. Is that normal ?", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 17343, "title": "Very general question about .tflite", "body": "Is the tensorflow lite always have quantized calculation and output?\r\nOr it depends on the model's input type?\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 17342, "title": "can't use all the CPU cores", "body": "I'm running the [ptb](https://github.com/petewarden/tensorflow_makefile/blob/master/tensorflow/models/rnn/ptb/ptb_word_lm.py) example on my 48-core Intel Skylake CPUs (two sockets).\r\n\r\nHowever, I found only 10 of the 48 cores are being used (I used the 'top' command). I want to use all the cores.\r\n\r\nI tried the following setting, but it did not work\r\n\r\nconfig_proto = tf.ConfigProto(allow_soft_placement=soft_placement, inter_op_parallelism_threads=4, intra_op_parallelism_threads=12)\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux 3.10.0-693.17.1.el7.x86_64 #1 SMP\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\n1.5.0\r\n- **Python version**: \r\n2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n5.4.0\r\n- **CUDA/cuDNN version**:\r\nDid not use GPU\r\n- **GPU model and memory**:\r\nDid not use GPU\r\n- **Exact command to reproduce**:\r\n\r\n\r\n", "comments": ["Using more cores can decrease performance. It's not clear that ptb_word_lm.py example can make use of 48 cores", "Thanks for the input, @yaroslavvb!\r\n\r\nThis question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 17341, "title": "Cannot import Tensorflow on a system that has anaconda installed, though tensorflow installed successfully", "body": "I installed tensorflow  on ubntu 16.04 by using \r\n``` \r\npip  install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.5.0-cp27-none-linux_x86_64.whl\r\n```\r\nit sounds like that it is installed correct, after installation it shows that:\r\n```\r\nCollecting setuptools (from protobuf>=3.4.0->tensorflow-gpu==1.5.0)\r\n  Using cached setuptools-38.5.1-py2.py3-none-any.whl\r\nRequirement already up-to-date: funcsigs>=1; python_version < \"3.3\" in ./anaconda2/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow-gpu==1.5.0)\r\nRequirement already up-to-date: pbr>=0.11 in ./anaconda2/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow-gpu==1.5.0)\r\nInstalling collected packages: numpy, tensorflow-gpu, setuptools\r\n  Found existing installation: numpy 1.14.0\r\n    Uninstalling numpy-1.14.0:\r\n      Successfully uninstalled numpy-1.14.0\r\n  Found existing installation: tensorflow-gpu 1.5.0\r\n    Uninstalling tensorflow-gpu-1.5.0:\r\n      Successfully uninstalled tensorflow-gpu-1.5.0\r\n  Found existing installation: setuptools 38.4.0\r\n    Uninstalling setuptools-38.4.0:\r\n      Successfully uninstalled setuptools-38.4.0\r\nSuccessfully installed numpy-1.14.1 setuptools-38.5.1 tensorflow-gpu-1.5.0\r\n```\r\nNow Im tryin to import tensorflow and i receive the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/home/alireza/anaconda2/lib/python2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/home/alireza/anaconda2/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/alireza/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/alireza/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/alireza/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/alireza/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n\r\nAny help or suggestion on what should i do??", "comments": ["It seems you still need to install cuda9.0", "@impanyu I already have cuda 8, isnot that enough?", "I think you should upgrade to  cuda9.0", "I will try that to see if it works..."]}, {"number": 17340, "title": "Could some ops(assert,switch...) be removed in inference model ?", "body": "I want to convert faster-rcnn model to tensorRT, but these ops can't be converted,because invalid dtype or non data value, so I want to know how to solve the question.", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 17339, "title": "tf 1.6rc1 feed_dict is slow on multi-socket machines", "body": "It seems as if 1.6rc1 introduces a copy in `feed_dict` on machines with >1 physical socket. Similar to https://github.com/tensorflow/tensorflow/issues/17233, but this also happens if numpy array is aligned. Fetching array from tensorflow, then feeding it back in happens at single-core memcpy speed. Downgrading to 1.5 restores fast behavior (4x faster)\r\n\r\nhttps://github.com/diux-dev/cluster/blob/master/yuxin_numpy/tf_numpy_benchmark.py\r\n```\r\n# 1.6 rc1 on p3.16xlarge (2 Xeon V4 sockets) = slow\r\n__git_version__: v1.6.0-rc1-607-g0bde713c06\r\nhttps://github.com/tensorflow/tensorflow/commit/0bde713c06\r\npython tf_numpy_benchmark.py --benchmark=feed_cpu_tensor --allocator=tf --num-iters=51 # 10.7\r\nfeed_cpu_tensor               :   2.5 GB/sec, min: 39.89, median: 40.18, mean: 40.26\r\n\r\n# switch to tf 1.5, things are fast\r\npip install tensorflow\r\npython tf_numpy_benchmark.py --benchmark=feed_cpu_tensor --allocator=tf --num-iters=51 # 10.7\r\nfeed_cpu_tensor               :  13.0 GB/sec, min:  7.67, median:  8.88, mean:  9.26\r\n\r\n# switch to p3.8xlarge machine (1 socket), and latest TF, things are also fast\r\n# tensorflow.__git_version__ = v1.6.0-rc1-562-g26ae3287a1\r\npython tf_numpy_benchmark.py --benchmark=feed_cpu_tensor --allocator=tf --num-iters=51 # 10.7\r\nfeed_cpu_tensor               :  10.5 GB/sec, min:  9.49, median: 10.83, mean: 10.86\r\n```", "comments": ["This artifact disappeared in 1.6 release\r\n\r\nhttps://docs.google.com/document/d/1U3x2DwfXLMYU6hbSjmB8yqBtvEFXIndAIdH9iwYP7LI/edit#heading=h.wqwu9nnpvc4z"]}, {"number": 17338, "title": "Don't use NCHW or NHCW for conv1d", "body": "Fixes deprecation warning when using tf.layers.conv1d", "comments": ["These options were explicitly deprecated, so it seems silly to convert from the new way to the deprecated way:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/b07680459a88224fce83daa7b3b70bcc62b9c896/tensorflow/python/ops/nn_ops.py#L2366-L2375", "@protoget @reedwm I have tested that this PR works as expected. `tf.nn.conv1d` handles that conversion:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/6fdb9ad1baf7686a75f9e660178f7ac595e7fc2e/tensorflow/python/ops/nn_ops.py#L2428-L2435\r\n\r\n`NCHW` and `NHWC` are explicitly deprecated, so as-is `tf.layers.conv1d` (which uses the function I'm modifying) raises a deprecation warning no matter what you do.\r\n\r\n"]}, {"number": 17337, "title": "inception tutorial: added png to allowed extensions, made print statements clearer", "body": "something very minor. it'll probably save a couple people some headaches", "comments": ["\nThanks for your pull request. t looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to `go/cla#troubleshoot`.\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Can we at least leave the print statement in there? It'd be an easier way to let people know that only jpegs are supported.", "Sure, please make the change and I'll approve.", "Actually inside `tensorflow/python/ops/gen_image_ops.py` for the version installed on my machine by pip3 I see a method with this signature:\r\n\r\n```\r\n@tf_export('image.decode_png')\r\ndef decode_png(contents, channels=0, dtype=_dtypes.uint8, name=None):\r\n```\r\n\r\nHowever, I'm not seeing that file in this repo. Do you know why that might be, and if I can still use it?"]}, {"number": 17336, "title": "Feature Request: Axis support for scatter update", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch Linux\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.6.0\r\n- **Python version**: N/A (applicable to both Python and Go bindings)\r\n- **Bazel version (if compiling from source)**: 0.10.1\r\n- **GCC/Compiler version (if compiling from source)**: 7.3.0\r\n- **CUDA/cuDNN version**: N/A (CPU only)\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: See below\r\n\r\n### Describe the problem\r\nGatherV2 allows one to specify axis. ResourceScatterUpdate however does not.\r\n\r\nUse case: Consider a variable of shape `[2, 3, 5]`. It may be useful to update the first dimension with data of shape `[2]` at indices specified in the last two dimensions. AFAICT, it is not possible to do this with the current ResourceScatterNdUpdate or ResourceScatterUpdate operations. Using a variable with transposed dimensions such as `[3, 5, 2]` is a potential workaround, but is inelegant and would require that it be wrapped in transpose operations.\r\n\r\nCan this be achieved using existing operations, and if not, is it worth create a new operation or extending an existing operation to supports it?\r\n", "comments": ["@alextp Can you look at this?", "It would be nice to have the corresponding `tf.scatter` function like how `tf.gather` works out the sparse details for `tf.scatter_nd`.", "You might be able to do something like this with resource strided slice assign.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Strided slice assign is acceptable for my particular use case.\r\nAs far as I'm concerned, this issue can be closed. However this may still be a desirable feature for other use cases."]}, {"number": 17335, "title": " Fix markdown error in documentation.", "body": "Newline in the middle of links was preventing their rendering.", "comments": []}, {"number": 17334, "title": "Still Seeing AVX Warnings on TF 1.6 Docker Image", "body": "### System information\r\n```\r\n== cat /etc/issue ===============================================\r\nLinux node014-jupyter-20170708-132328 4.4.0-1022-aws #31-Ubuntu SMP Tue Jun 27 11:27:55 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.2 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nYes\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux node014-jupyter-20170708-132328 4.4.0-1022-aws #31-Ubuntu SMP Tue Jun 27 11:27:55 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.0)\r\nprotobuf (3.2.0)\r\ntensorflow-gpu (1.2.1)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.2.1\r\ntf.GIT_VERSION = v1.2.0-5-g435cdfc\r\ntf.COMPILER_VERSION = v1.2.0-5-g435cdfc\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /usr/local/cuda/extras/CUPTI/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nTue Aug  1 17:13:34 2017\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.66                 Driver Version: 375.66                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla K80           Off  | 0000:00:1E.0     Off |                    0 |\r\n| N/A   59C    P0    67W / 149W |      0MiB / 11439MiB |      0%   E. Process |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61\r\n\r\n== cat /etc/issue ===============================================\r\nLinux node008-jupyter-20180228-164013 4.4.0-1050-aws #59-Ubuntu SMP Tue Jan 30 19:57:10 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.3 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nYes\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux node008-jupyter-20180228-164013 4.4.0-1050-aws #59-Ubuntu SMP Tue Jan 30 19:57:10 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.14.0)\r\nprotobuf (3.5.0.post1)\r\ntensorflow (1.6.0)\r\ntensorflow-tensorboard (1.5.0)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.6.0\r\ntf.GIT_VERSION = v1.6.0-0-gd2e24b6\r\ntf.COMPILER_VERSION = v1.6.0-0-gd2e24b6\r\nSanity check: array([1], dtype=int32)\r\n/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /usr/local/cuda/extras/CUPTI/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nWed Feb 28 17:32:05 2018\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 390.12                 Driver Version: 390.12                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla M60           On   | 00000000:00:1E.0 Off |                    0 |\r\n| N/A   43C    P0    45W / 150W |     11MiB /  7618MiB |      0%   E. Process |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0.176\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart_static.a\r\n\r\n== cat /etc/issue ===============================================\r\nLinux node008-jupyter-20180228-164013 4.4.0-1050-aws #59-Ubuntu SMP Tue Jan 30 19:57:10 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.3 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nYes\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux node008-jupyter-20180228-164013 4.4.0-1050-aws #59-Ubuntu SMP Tue Jan 30 19:57:10 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.14.0)\r\nprotobuf (3.5.0.post1)\r\ntensorflow (1.6.0)\r\ntensorflow-tensorboard (1.5.0)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.6.0\r\ntf.GIT_VERSION = v1.6.0-0-gd2e24b6\r\ntf.COMPILER_VERSION = v1.6.0-0-gd2e24b6\r\nSanity check: array([1], dtype=int32)\r\n/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /usr/local/cuda/extras/CUPTI/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nWed Feb 28 17:35:44 2018\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 390.12                 Driver Version: 390.12                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla M60           On   | 00000000:00:1E.0 Off |                    0 |\r\n| N/A   49C    P0    46W / 150W |     11MiB /  7618MiB |      0%   E. Process |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0.176\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart_static.a\r\n```\r\n\r\n```\r\nv1.6.0-0-gd2e24b6 1.6.0\r\n```\r\nImage: `tensorflow/tensorflow:1.6.0-devel-gpu-py3`\r\n\r\n### Describe the problem\r\nI am seeing this warning message even though I thought that AVX instructions should be used in 1.6:\r\n```\r\n2018-02-28 17:33:10.974670: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n```\r\nThis is the same warning I saw on 1.5.", "comments": ["@gunan Can you look at this when you have a chance?", "@gunan ?", "Swamped with other things these days, I will check when I get the chance.", "I am still seeing this issue with the TF1.7 image.", "and 1.8rc0", "Nagging Assignee @tatatodd: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "and 1.8 final", "@case540 @av8ramit any idea why our pip packages in dockerfiles are not built with avx enabled?", "Taking a look.", "Dev version for 1.7.1 has been fixed. 1.8.1 will also include the same patch. Thanks for filing @cancan101 "]}]