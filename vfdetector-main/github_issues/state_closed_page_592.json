[{"number": 35915, "title": "[TFMicro] Adding cwrapper example, adding generic Cortex m4 compiler option example for building library", "body": "- adding example cwrapper which generates the projects with simple api\r\n- adding library option to the makefile template \r\n- make project have the compiler path defined setting so they are some what correct when generated", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35915) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35915) for more info**.\n\n<!-- ok -->", "wasn't ready to commit yet. Just checking the CLA is working."]}, {"number": 35914, "title": "Update version numbers for TensorFlow 1.15.1", "body": "Before merging this PR, please double check that it has correctly updated\n`core/public/version.h`, `tools/pip_package/setup.py`, and\n`tensorflow/tensorflow.bzl`. Also review the execution notes below:\n\n```\nMajor: 1 -> 1\nMinor: 15 -> 15\nPatch: 0 -> 1\n\nWARNING: Below are potentially instances of lingering old version string \n\"1.15.0\" in source directory \"tensorflow/\" that are not updated by this script. \nPlease check them manually!\ntensorflow/tools/pip_package/setup.py:65:1.15.0\ntensorflow/tools/pip_package/setup.py:98:1.15.0\ntensorflow/tools/ci_build/release/common.sh:242:1.15.0\ntensorflow/tools/ci_build/release/common.sh:245:1.15.0\ntensorflow/lite/experimental/micro/examples/micro_speech/README.md:591:1.15.0\ntensorflow/lite/experimental/micro/examples/micro_speech/train_speech_model.ipyn\nb:126:1.15.0\n\nWARNING: Below are potentially instances of lingering old version string \n\"1.15.0\" in source directory \"tensorflow/\" that are not updated by this script. \nPlease check them manually!\ntensorflow/tools/pip_package/setup.py:65:1.15.0\ntensorflow/tools/pip_package/setup.py:98:1.15.0\ntensorflow/tools/ci_build/release/common.sh:242:1.15.0\ntensorflow/tools/ci_build/release/common.sh:245:1.15.0\ntensorflow/lite/experimental/micro/examples/micro_speech/README.md:591:1.15.0\ntensorflow/lite/experimental/micro/examples/micro_speech/train_speech_model.ipyn\nb:126:1.15.0\n```", "comments": []}, {"number": 35913, "title": "Update release notes for TensorFlow 2.0.1", "body": "This PR is intentionally incomplete. One of the Release Owners for 2.0.1\nneeds to fill in the internal release notes for this version before the PR gets\nsubmitted. Click on the :pencil2: icon in the header for `RELEASE.md` under\n\"Files Changed\" above.", "comments": []}, {"number": 35912, "title": "Update release notes for TensorFlow 1.15.1", "body": "This PR is intentionally incomplete. One of the Release Owners for 1.15.1\nneeds to fill in the internal release notes for this version before the PR gets\nsubmitted. Click on the :pencil2: icon in the header for `RELEASE.md` under\n\"Files Changed\" above.", "comments": ["Not yet, the CVE needs ~3 days from GitHub.", "Ah. Should we add the CVE when we have it? Or is it better to release now? I suppose the affects versions are listed in the cve anyway. ", "I'll add the CVE, I'm still working on getting the builds fixed. Just didn't want to forget to update the notes."]}, {"number": 35911, "title": "Keras model.fit not calling Sequence.on_epoch_end()", "body": "**System information**\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu\r\n- TensorFlow installed from (source or binary): binary from pip install tensorflow\r\n- TensorFlow version: v2.1.0-rc2-17-ge5bf8de 2.1.0\r\n- Python version: 3.6.7\r\n\r\n**Describe the current behavior**\r\non_epoch_end() from a tf.keras.utils.Sequence is ***not*** called on epoch end while training using model.fit().\r\n\r\n**Describe the expected behavior**\r\non_epoch_end() from a tf.keras.utils.Sequence should be called on epoch end while training using model.fit().\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\nclass ZerosFirstEpochOnesAfter(tf.keras.utils.Sequence):\r\n    def __init__(self):\r\n        self.is_epoch_0 = True\r\n\r\n    def __len__(self):\r\n        return 2\r\n\r\n    def on_epoch_end(self):\r\n        print('on_epoch_end')\r\n        self.is_epoch_0 = False\r\n\r\n    def __getitem__(self, item):\r\n        if self.is_epoch_0:\r\n            print(\"First epoch\")\r\n            return np.zeros((16, 1)), np.zeros((16,))\r\n        else:\r\n            return np.ones((16, 1)), np.ones((16,))\r\n\r\n\r\nif __name__ == '__main__':\r\n    model = tf.keras.Sequential()\r\n    model.add(tf.keras.layers.Dense(1, input_dim=1, activation=\"softmax\"))\r\n\r\n    model.compile(\r\n        optimizer='Adam',\r\n        loss='binary_crossentropy',\r\n        metrics=['accuracy']\r\n    )\r\n\r\n    model.fit(ZerosFirstEpochOnesAfter(), epochs=5,)\r\n```\r\n\r\n**Other info / logs**\r\n\r\nNote that print('on_epoch_end') is not called anywhere and \"First epoch\" is printed every epoch.\r\n\r\nRelevant api docs:\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence?version=stable\r\n\r\n```\r\nWARNING:tensorflow:sample_weight modes were coerced from\r\n  ...\r\n    to\r\n  ['...']\r\nTrain for 2 steps\r\nEpoch 1/5\r\nFirst epoch\r\nFirst epoch\r\n2020-01-15 17:57:31.979489: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2/2 [==============================] - 1s 260ms/step - loss: 15.2492 - accuracy: 0.0000e+00\r\nEpoch 2/5\r\nFirst epoch\r\nFirst epoch\r\n2/2 [==============================] - 0s 6ms/step - loss: 15.2492 - accuracy: 0.0000e+00\r\nEpoch 3/5\r\nFirst epoch\r\nFirst epoch\r\n2/2 [==============================] - 0s 6ms/step - loss: 15.2492 - accuracy: 0.0000e+00\r\nEpoch 4/5\r\nFirst epoch\r\nFirst epoch\r\n2/2 [==============================] - 0s 6ms/step - loss: 15.2492 - accuracy: 0.0000e+00\r\nEpoch 5/5\r\nFirst epoch\r\nFirst epoch\r\n2/2 [==============================] - 0s 6ms/step - loss: 15.2492 - accuracy: 0.0000e+00\r\n```", "comments": ["@bfs15,\r\nWas able to reproduce the issue. Please find the Gist [here](https://colab.sandbox.google.com/gist/amahendrakar/a1e87e390d7255c123bf0b28a52f70d3/35911.ipynb). Thanks!", "@bfs15 You could use `Keras.callbacks`to view on internal states and statistics of the model during training. [Here](https://colab.sandbox.google.com/gist/jvishnuvardhan/ddf725e66122230653747793994e3eaf/35911.ipynb) is a gist on using `callbacks` for checking a status `on_batch_end`. Thanks!", "Well yes, while Sequence.on_epoch_end doesn't work, Callback.on_epoch_end does.\r\nBut as [the docs state Sequence.on_epoch_end should be called automatically just like Callback.on_epoch_end. ](https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence?version=stable#on_epoch_end)\r\n\r\nThe problem is that the Sequence I'm making needs to shuffle its dataset every epoch (it's actually triplet hard mining). I don't want model metrics and logs, the Sequence object needs to be alerted to shuffle the dataset for the next epoch.\r\n\r\nI am quite sure this is a bug since this worked on tf 2.0.\r\n\r\nBut given that, the workaround I am currently using is creating a Callback whose sole purpose is to call the Sequence.on_epoch_end on its own on_epoch_end.\r\n\r\nhttps://colab.research.google.com/gist/bfs15/fd18263f788a071225c60cedaf126748/35911.ipynb", "I also have encountered the same issue. Keras model.fit needs at least `steps_per_epoch * epochs` batches without calling on_epoch_end. It sounds not-conformant with the specification.", "I have the same problem too:\r\n\r\n- TensorFlow 2.1.0 (installed with conda)\r\n- Python 3.7.4\r\n- Windows 10", "Thanks for the issue! This is fixed in the latest tf-nightly, please try with `pip install -U tf-nightly`", "Hi,\r\nIs this fixed if I install tensorflow by pip install tensorflow-gpu==2.1.0?\r\nOr can someone point out which snippet of source code (or which commit) is corrected after this is solved?\r\n\r\nThx", "From what I understand it will always be broken on 2.1.0, if they release a fix it will be on 2.1.x\r\nIf you are stuck on 2.1.0 and/or don't want to mess with tf-nightly, the notebook I posted above has a snipped to duct tape this issue using Callbacks.\r\nThe important part is the `OnEpochEnd` class and the usage on `OnEpochEnd([seq.on_epoch_end])`, which registers the Sequence on_epoch_end to be called by OnEpochEnd.on_epoch_end.\r\nhttps://colab.research.google.com/gist/bfs15/fd18263f788a071225c60cedaf126748/35911.ipynb", "I've encountered the same issue. During my test OnEpochEnd is not called in Tensorflow 2.0.2. After upgrading to Tensorflow 2.2.0 OnEpochEnd is being called and the issue is solved.", "I have some problem when I am trying to run the code. \r\nmodel.fit([enc_inp, dec_inp],decoder_final_output,epochs=4)\r\nTypeError: 'NoneType' object is not callable\r\n\r\ncan anyone help me to solve this problem?", "> can anyone help me to solve this problem?\r\n\r\n@Abdullah-sta,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!", "good job"]}, {"number": 35910, "title": "Issue with the Interpreter on Android ", "body": "I've to files of the same model that contains the same bytes. I'm trying to initialize the interpreter with both of them, but only with one works. I attach below the two files, only the indentation  seems different but the row bytes at the inside are the same. The first file was created in python saving the converted model, the second was created passing the raw bytes throw the network an storing them in a new file in kotlin\r\n[Archive.zip](https://github.com/tensorflow/tensorflow/files/4067466/Archive.zip)\r\n\r\n", "comments": ["@loreProgetto,\r\nPlease fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose) to analyse the issue . Thanks!", "@loreProgetto, Please fill the issue Template. Thanks", "@loreProgetto, Please take a look at [Android quickstart](https://www.tensorflow.org/lite/guide/android) Tensorflow doc. Thanks!", "@loreProgetto What are you trying to do? You can just initialize two interpreters with the same model file, instead of having two different files?\r\nThe models indeed seem similar, so I am not sure whats going wrong.\r\nCould you paste your code snippet?", "@loreProgetto, Any update?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 35909, "title": "OSError: SavedModel file does not exist at: main.h5/{saved_model.pbtxt|saved_model.pb}", "body": "**System information**\r\n- Custom code provided below\r\n- OS Platform and Distribution : Linux Raspbian\r\n- Raspberry pi\r\n- TensorFlow installed from apt repos:\r\n- TensorFlow version (use command below): tensorflow 1.14\r\n- Python version: python 3\r\n\r\n**Describe the current behavior**\r\nAt running, the following line raise the following error:\r\n``` python\r\nface_recognition_model = tf.keras.models.load_model('face_recognition_model.h5')\r\n```\r\n\r\n> \r\n> OSError: SavedModel file does not exist at: main.h5/{saved_model.pbtxt|saved_model.pb}\r\n> **Describe the expected behavior**\r\nExpected to load the model for later prediction\r\n\r\n**Code to reproduce the issue**\r\n``` python\r\n\r\ndetection_graph = tf.Graph()\r\nwith detection_graph.as_default():\r\n    od_graph_def = tf.GraphDef()\r\n    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\r\n        serialized_graph = fid.read()\r\n        od_graph_def.ParseFromString(serialized_graph)\r\n        tf.import_graph_def(od_graph_def, name='')\r\nlabel_map = label_map_util.load_labelmap(PATH_TO_LABELS)\r\ncategories = label_map_util.convert_label_map_to_categories(label_map,\r\n                                                            max_num_classes=NUM_CLASSES,\r\n                                                            use_display_name=True)\r\ncategory_index = label_map_util.create_category_index(categories)\r\nface_recognition_model = tf.keras.models.load_model('face_recognition_model.h5')\r\n```\r\n\r\n**Other info / logs**\r\nI provided a snippet to reproduce that error. The all code is using a mobilnet trained by transfer learning to detect faces and a classification network is trained and used on keras for facial recognition. The code is working just fine on my computer and i have absolutely no clue why it's not working on my raspberry pi. I can provide the complete code if it's required.\r\n", "comments": ["@niccle27 ,\r\nCan you please provide complete code to reproduce ?Thanks!", "@oanush \r\nThanks a lot for trying to help me,\r\n\r\nSince it's part of a school project, i wasn't able to have access to the raspberry before now, so basically one more precision : python version i'm using is Python 3.7.3\r\n\r\nAfter further try to limit the code required to reproduce the issue, :\r\n\r\n```python\r\nimport tensorflow as tf\r\nface_recognition_model = tf.keras.models.load_model('face_recognition_model.h5')\r\n```\r\nis enough to reproduce the issue, here is the complete Error : \r\n\r\n> 2020-01-16 14:18:22.697745: E tensorflow/core/platform/hadoop/hadoop_file_system.cc:132] HadoopFileSystem load error: libhdfs.so: cannot open shared object file: No such file or directory\r\nTraceback (most recent call last):\r\n  File \"testBug.py\", line 17, in <module>\r\n    face_recognition_model = tf.keras.models.load_model('face_recognition_model.h5')\r\n  File \"/home/pi/Desktop/DLClem/RecognitionInference/venvRecognition/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py\", line 149, in load_model\r\n    loader_impl.parse_saved_model(filepath)\r\n  File \"/home/pi/Desktop/DLClem/RecognitionInference/venvRecognition/lib/python3.7/site-packages/tensorflow_core/python/saved_model/loader_impl.py\", line 83, in parse_saved_model\r\n    constants.SAVED_MODEL_FILENAME_PB))\r\nOSError: SavedModel file does not exist at: face_recognition_model.h5/{saved_model.pbtxt|saved_model.pb}\r\n\r\nI'm not using hadoop at all, therefore i don't really know where that line is coming from. I verified and that library isn't present in my Ubuntu 18.04 desktop either (no error message though)\r\n\r\nAny idea ? \r\n", "@niccle27 ,\r\nFrom the error log i can see that saved_model- `face_recognition_model.h5`  does not exist, make sure you import the file to the right path and then try loading the model using load_model. \r\nYou can provide us the code being used so that i can help you out with the resolving the issue.Thanks!", "Sorry for the late reply, \r\nI manage to identify the problem, the path was alright. The problem concern the h5py package installed using pip on the raspberry pi. Apparently there seem to be some issue with it. I managed to fix it by installing using the debian repository : sudo apt-get install python3-h5py.\r\n\r\nThis is indeed irrelevant to tensorflow, therefore the issue can be closed.\r\n\r\nThanks a lot !\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35909\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35909\">No</a>\n", "I have the same problem, but after use the install command you mentioned before, it didn't work and get the same error again, i.e., SaveModel file does not exit.", "@JinkaiGUAN did u find the solution?\r\n\r\n> I have the same problem, but after use the install command you mentioned before, it didn't work and get the same error again, i.e., SaveModel file does not exit.\r\n\r\n", "I have just met the same problem when using Cloud9...... \uff08Haven't solved it yet.", "I have the same problem using my tensorflow (based on 20.08) docker container downloaded from nvidia catalog with TF 2.2. \r\nError\r\nself.model = load_model(model_path)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/save.py\", line 189, in load_model\r\n    loader_impl.parse_saved_model(filepath)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/loader_impl.py\", line 113, in parse_saved_model\r\n    constants.SAVED_MODEL_FILENAME_PB))\r\nOSError: SavedModel file does not exist at: ./analytics/models/faceembedding/facenet_keras.h5/{saved_model.pbtxt|saved_model.pb}\r\n\r\nI have run this code before and it worked (another environment).", "> I have just met the same problem when using Cloud9...... \uff08Haven't solved it yet.\r\n\r\nWell, I have solved this by setting the AWS access key in the virtual environment.... So I think if you are still facing this problem try thinking about the access part!(relative path, access to S3 and so on.)", "@Jerrrrykun can you explain how to set the AWS access key. I also have the same problem with my code i.e.\r\nOSError: SavedModel file does not exist at: checkpoints/Test_UCSDped2_Model.h5/{saved_model.pbtxt|saved_model.pb}", "The solution is  found here \r\nhttps://stackoverflow.com/questions/61699140/oserror-savedmodel-file-does-not-exist-at-dnn-mpg-model-h5-saved-model-pbt\r\n\r\nimport os\r\nwhatever = load_model(os.path.join(\"./dnn/\",\"mpg_model.h5\"))\r\n\r\n\r\nNote. - single dot not double before the slash\"/\""]}, {"number": 35908, "title": "MKL DNN 1.0 integration - refactoring MKL conv forward op", "body": "This PR is intended to refactor the current implementation of forward conv ops with MKL-DNN\r\nv1.0 integration:\r\n(1) Abstract macro definitions from individual source files to a common header file (mkl_types)\r\nfor better code reuse (in coming PR's)\r\n(2) Refactor based on Google coding style. For instance, replace CHECK_NOTNULL to DCHECK.\r\n\r\nBackground:\r\nWe (Intel) will submit a sequence of PR's with MKL-DNN v1.0 integration (around 24 MKL ops).\r\nAnd this PR will be fundamental for the near future PR's.\r\n\r\nReference of previously merged PR:\r\n#30549", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35908) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 35907, "title": "[INTEL MKL] Refactor the implementation of forward conv ops w MKL-DNN v1.0 integration", "body": "This PR is intended to refactor the current implementation of forward conv ops with MKL-DNN\r\nv1.0 integration:\r\n(1) Abstract macro definitions from individual source files to a common header file (mkl_types)\r\nfor better code reuse (in coming PR's)\r\n(2) Refactor based on Google coding style. For instance, replace CHECK_NOTNULL to DCHECK.\r\n\r\nBackground:\r\nWe (Intel) will submit a sequence of PR's with MKL-DNN v1.0 integration (around 24 MKL ops).\r\nAnd this PR will be fundamental for the near future PR's.\r\n\r\nReference of previously merged PR:\r\n#30549", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35907) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 35906, "title": "global numbering scheme of  tf.keras.layers.BatchNormalization layers seems like a wrong strategy", "body": "<em>I trained a model in Tensorflow and used 36 convolution layers, each holding a batch normalization layer. What I do not understand is why the numbering of batch normalization layer is global instead of starting from 0 for every name scope? \r\nI am not going to create the layers with the same name_scope in the same order in every model, sometimes I will insert a few layers before a particular layer and would like to restore the batch norm parameters from other model checkpoint, but it's not possible because batch norm layer is numbered globally, and because I created this layer at a step numerically bigger than the step at which I created the same layer in other model, the names will be different and I won't be able to restore parameters in the traditional way. </em>\r\n\r\n**System information**\r\n-\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Mint\r\n- TensorFlow installed from (source or binary): tf-gpu anaconda \r\n- TensorFlow version : tf.2.0.0\r\n- Python version: 3.7.4\r\n\r\n\r\nHere is the list of batch norm parameters in my original model, mind the layer: batch_normalization_18 , this name is batch_normalization_18 because this is the 19th (18 is in numbering because index starts from 0) batchnorm layer created.\r\n\r\n['Train/vgg_deconv_RGB_LSTM/Block1_conv1/batch_normalization/gamma:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block1_conv1/batch_normalization/beta:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block1_conv1/batch_normalization/moving_mean:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block1_conv1/batch_normalization/moving_variance:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block1_conv2/batch_normalization_1/gamma:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block1_conv2/batch_normalization_1/beta:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block1_conv2/batch_normalization_1/moving_mean:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block1_conv2/batch_normalization_1/moving_variance:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block2_conv1/batch_normalization_2/gamma:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block2_conv1/batch_normalization_2/beta:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block2_conv1/batch_normalization_2/moving_mean:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block2_conv1/batch_normalization_2/moving_variance:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block2_conv2/batch_normalization_3/gamma:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block2_conv2/batch_normalization_3/beta:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block2_conv2/batch_normalization_3/moving_mean:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block2_conv2/batch_normalization_3/moving_variance:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block3_conv1/batch_normalization_4/gamma:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block3_conv1/batch_normalization_4/beta:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block3_conv1/batch_normalization_4/moving_mean:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block3_conv1/batch_normalization_4/moving_variance:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block3_conv2/batch_normalization_5/gamma:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block3_conv2/batch_normalization_5/beta:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block3_conv2/batch_normalization_5/moving_mean:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block3_conv2/batch_normalization_5/moving_variance:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block3_conv3/batch_normalization_6/gamma:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block3_conv3/batch_normalization_6/beta:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block3_conv3/batch_normalization_6/moving_mean:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block3_conv3/batch_normalization_6/moving_variance:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block3_conv4/batch_normalization_7/gamma:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block3_conv4/batch_normalization_7/beta:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block3_conv4/batch_normalization_7/moving_mean:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block3_conv4/batch_normalization_7/moving_variance:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block4_conv1/batch_normalization_8/gamma:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block4_conv1/batch_normalization_8/beta:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block4_conv1/batch_normalization_8/moving_mean:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block4_conv1/batch_normalization_8/moving_variance:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block4_conv2/batch_normalization_9/gamma:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block4_conv2/batch_normalization_9/beta:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block4_conv2/batch_normalization_9/moving_mean:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block4_conv2/batch_normalization_9/moving_variance:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block4_conv3/batch_normalization_10/gamma:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block4_conv3/batch_normalization_10/beta:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block4_conv3/batch_normalization_10/moving_mean:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block4_conv3/batch_normalization_10/moving_variance:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block4_conv4/batch_normalization_11/gamma:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block4_conv4/batch_normalization_11/beta:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block4_conv4/batch_normalization_11/moving_mean:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block4_conv4/batch_normalization_11/moving_variance:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block5_conv1/batch_normalization_12/gamma:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block5_conv1/batch_normalization_12/beta:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block5_conv1/batch_normalization_12/moving_mean:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block5_conv1/batch_normalization_12/moving_variance:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block5_conv2/batch_normalization_13/gamma:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block5_conv2/batch_normalization_13/beta:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block5_conv2/batch_normalization_13/moving_mean:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block5_conv2/batch_normalization_13/moving_variance:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block5_conv3/batch_normalization_14/gamma:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block5_conv3/batch_normalization_14/beta:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block5_conv3/batch_normalization_14/moving_mean:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block5_conv3/batch_normalization_14/moving_variance:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block5_conv4/batch_normalization_15/gamma:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block5_conv4/batch_normalization_15/beta:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block5_conv4/batch_normalization_15/moving_mean:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block5_conv4/batch_normalization_15/moving_variance:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block6_conv1/batch_normalization_16/gamma:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block6_conv1/batch_normalization_16/beta:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block6_conv1/batch_normalization_16/moving_mean:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block6_conv1/batch_normalization_16/moving_variance:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block6_conv2/batch_normalization_17/gamma:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block6_conv2/batch_normalization_17/beta:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block6_conv2/batch_normalization_17/moving_mean:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Block6_conv2/batch_normalization_17/moving_variance:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Dlock6_conv1/batch_normalization_18/gamma:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Dlock6_conv1/batch_normalization_18/beta:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Dlock6_conv1/batch_normalization_18/moving_mean:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Dlock6_conv1/batch_normalization_18/moving_variance:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Dlock6_conv2/batch_normalization_19/gamma:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Dlock6_conv2/batch_normalization_19/beta:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Dlock6_conv2/batch_normalization_19/moving_mean:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Dlock6_conv2/batch_normalization_19/moving_variance:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Dlock5_conv1/batch_normalization_20/gamma:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Dlock5_conv1/batch_normalization_20/beta:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Dlock5_conv1/batch_normalization_20/moving_mean:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Dlock5_conv1/batch_normalization_20/moving_variance:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Dlock5_conv2/batch_normalization_21/gamma:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Dlock5_conv2/batch_normalization_21/beta:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Dlock5_conv2/batch_normalization_21/moving_mean:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Dlock5_conv2/batch_normalization_21/moving_variance:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Dlock5_conv3/batch_normalization_22/gamma:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Dlock5_conv3/batch_normalization_22/beta:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Dlock5_conv3/batch_normalization_22/moving_mean:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Dlock5_conv3/batch_normalization_22/moving_variance:0']\r\n\r\n\r\nNow, I created another model, where I am creating the same layers, but I am inserting two layers before this 19th layer, and hence, this will be given a different name just because of this reason. \r\n\r\n ['Train/vgg_deconv_RGB_LSTM/Dlock6_conv1/batch_normalization_20/beta:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Dlock6_conv1/batch_normalization_20/gamma:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Dlock6_conv1/batch_normalization_20/moving_mean:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Dlock6_conv1/batch_normalization_20/moving_variance:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Dlock6_conv2/batch_normalization_21/beta:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Dlock6_conv2/batch_normalization_21/gamma:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Dlock6_conv2/batch_normalization_21/moving_mean:0',\r\n 'Train/vgg_deconv_RGB_LSTM/Dlock6_conv2/batch_normalization_21/moving_variance:0',\r\n 'Train/vgg_deconv_RGB_LSTM/extra_conv1_first/batch_normalization_18/beta:0',\r\n 'Train/vgg_deconv_RGB_LSTM/extra_conv1_first/batch_normalization_18/gamma:0',\r\n 'Train/vgg_deconv_RGB_LSTM/extra_conv1_first/batch_normalization_18/moving_mean:0',\r\n 'Train/vgg_deconv_RGB_LSTM/extra_conv1_first/batch_normalization_18/moving_variance:0',\r\n 'Train/vgg_deconv_RGB_LSTM/extra_conv2_second/batch_normalization_19/beta:0',\r\n 'Train/vgg_deconv_RGB_LSTM/extra_conv2_second/batch_normalization_19/gamma:0',\r\n 'Train/vgg_deconv_RGB_LSTM/extra_conv2_second/batch_normalization_19/moving_mean:0',\r\n 'Train/vgg_deconv_RGB_LSTM/extra_conv2_second/batch_normalization_19/moving_variance:0']\r\n\r\n\r\nYou can see, now batch_normalization_18 name is given to another layer, and my original layer, which was given batch_normalization_18 in previous model, it is given the name of \r\nbatch_normalization_20, even though all the variable scope and name scope is same. \r\n\r\nShould'nt it be like local numbering for every name_scope? What's the point behind global numbering? \r\n\r\nPlease let me know if any more information is needed. \r\n", "comments": ["@rcr1994 ,\r\nCan you please provide the complete code to replicate the issue ?Thanks!", "> @rcr1994 ,\r\n> Can you please provide the complete code to replicate the issue ?Thanks!\r\n\r\nYes, sure. The code is below: \r\n\r\n\r\n```\r\nimport tensorflow as tf\r\ntf.compat.v1.disable_eager_execution()\r\nimport numpy as np\r\n\r\ngraph_1 = tf.compat.v1.Graph()\r\nwith graph_1.as_default():\r\n    input_ = np.ones((10,25,25,3))\r\n    input_t = tf.constant(input_)\r\n    \r\n    with tf.compat.v1.variable_scope('first_scope'):\r\n        batchnorm_layer = tf.compat.v1.keras.layers.BatchNormalization(axis = -1)\r\n        first_layer = batchnorm_layer(inputs = input_t,training = True)\r\n        \r\n    with tf.compat.v1.variable_scope('second_scope'):\r\n        batchnorm_layer = tf.compat.v1.keras.layers.BatchNormalization(axis = -1)\r\n        second_layer = batchnorm_layer(inputs = first_layer,training = True)\r\n        \r\n    with tf.compat.v1.variable_scope('third_scope'):\r\n        batchnorm_layer = tf.compat.v1.keras.layers.BatchNormalization(axis = -1)\r\n        third_layer = batchnorm_layer(inputs = second_layer,training = True)\r\n        \r\n    with tf.compat.v1.variable_scope('fourth_scope'):\r\n        batchnorm_layer = tf.compat.v1.keras.layers.BatchNormalization(axis = -1)\r\n        fourth_layer = batchnorm_layer(inputs = third_layer,training = True)\r\n    \r\n    allvar_1 = tf.compat.v1.global_variables()\r\n\r\n\r\ngraph_2 = tf.compat.v1.Graph()\r\nwith graph_2.as_default():\r\n    input_ = np.ones((10,25,25,3))\r\n    input_t = tf.constant(input_)\r\n    \r\n    with tf.compat.v1.variable_scope('first_scope'):\r\n        batchnorm_layer = tf.compat.v1.keras.layers.BatchNormalization(axis = -1)\r\n        first_layer = batchnorm_layer(inputs = input_t,training = True)\r\n        \r\n    with tf.compat.v1.variable_scope('second_scope'):\r\n        batchnorm_layer = tf.compat.v1.keras.layers.BatchNormalization(axis = -1)\r\n        second_layer = batchnorm_layer(inputs = first_layer,training = True)\r\n        \r\n\r\n> This is the extra layer I add, which has a totally different variable scope\r\n\r\n    with tf.compat.v1.variable_scope('some_random_scope'):\r\n        batchnorm_layer = tf.compat.v1.keras.layers.BatchNormalization(axis = -1)\r\n        some_random_layer = batchnorm_layer(inputs = second_layer,training = True)  \r\n    \r\n    with tf.compat.v1.variable_scope('third_scope'):\r\n        batchnorm_layer = tf.compat.v1.keras.layers.BatchNormalization(axis = -1)\r\n        third_layer = batchnorm_layer(inputs = some_random_layer,training = True)\r\n        \r\n    with tf.compat.v1.variable_scope('fourth_scope'):\r\n        batchnorm_layer = tf.compat.v1.keras.layers.BatchNormalization(axis = -1)\r\n        fourth_layer = batchnorm_layer(inputs = third_layer,training = True)\r\n    \r\n    allvar_2 = tf.compat.v1.global_variables()>\r\n```\r\n\r\n**The output for both the variables are:** \r\n\r\n**allvar_1:** \r\n```\r\n[<tf.Variable 'first_scope/batch_normalization/gamma:0' shape=(3,) dtype=float32>,\r\n <tf.Variable 'first_scope/batch_normalization/beta:0' shape=(3,) dtype=float32>,\r\n <tf.Variable 'first_scope/batch_normalization/moving_mean:0' shape=(3,) dtype=float32>,\r\n <tf.Variable 'first_scope/batch_normalization/moving_variance:0' shape=(3,) dtype=float32>,\r\n <tf.Variable 'second_scope/batch_normalization_1/gamma:0' shape=(3,) dtype=float32>,\r\n <tf.Variable 'second_scope/batch_normalization_1/beta:0' shape=(3,) dtype=float32>,\r\n <tf.Variable 'second_scope/batch_normalization_1/moving_mean:0' shape=(3,) dtype=float32>,\r\n <tf.Variable 'second_scope/batch_normalization_1/moving_variance:0' shape=(3,) dtype=float32>,\r\n <tf.Variable 'third_scope/batch_normalization_2/gamma:0' shape=(3,) dtype=float32>,\r\n <tf.Variable 'third_scope/batch_normalization_2/beta:0' shape=(3,) dtype=float32>,\r\n <tf.Variable 'third_scope/batch_normalization_2/moving_mean:0' shape=(3,) dtype=float32>,\r\n <tf.Variable 'third_scope/batch_normalization_2/moving_variance:0' shape=(3,) dtype=float32>,\r\n <tf.Variable 'fourth_scope/batch_normalization_3/gamma:0' shape=(3,) dtype=float32>,\r\n <tf.Variable 'fourth_scope/batch_normalization_3/beta:0' shape=(3,) dtype=float32>,\r\n <tf.Variable 'fourth_scope/batch_normalization_3/moving_mean:0' shape=(3,) dtype=float32>,\r\n <tf.Variable 'fourth_scope/batch_normalization_3/moving_variance:0' shape=(3,) dtype=float32>]\r\n```\r\n\r\n\r\n**allvar_2:**\r\n```\r\n[<tf.Variable 'first_scope/batch_normalization/gamma:0' shape=(3,) dtype=float32>,\r\n <tf.Variable 'first_scope/batch_normalization/beta:0' shape=(3,) dtype=float32>,\r\n <tf.Variable 'first_scope/batch_normalization/moving_mean:0' shape=(3,) dtype=float32>,\r\n <tf.Variable 'first_scope/batch_normalization/moving_variance:0' shape=(3,) dtype=float32>,\r\n <tf.Variable 'second_scope/batch_normalization_1/gamma:0' shape=(3,) dtype=float32>,\r\n <tf.Variable 'second_scope/batch_normalization_1/beta:0' shape=(3,) dtype=float32>,\r\n <tf.Variable 'second_scope/batch_normalization_1/moving_mean:0' shape=(3,) dtype=float32>,\r\n <tf.Variable 'second_scope/batch_normalization_1/moving_variance:0' shape=(3,) dtype=float32>,\r\n <tf.Variable 'some_random_scope/batch_normalization_2/gamma:0' shape=(3,) dtype=float32>,\r\n <tf.Variable 'some_random_scope/batch_normalization_2/beta:0' shape=(3,) dtype=float32>,\r\n <tf.Variable 'some_random_scope/batch_normalization_2/moving_mean:0' shape=(3,) dtype=float32>,\r\n <tf.Variable 'some_random_scope/batch_normalization_2/moving_variance:0' shape=(3,) dtype=float32>,\r\n <tf.Variable 'third_scope/batch_normalization_3/gamma:0' shape=(3,) dtype=float32>,\r\n <tf.Variable 'third_scope/batch_normalization_3/beta:0' shape=(3,) dtype=float32>,\r\n <tf.Variable 'third_scope/batch_normalization_3/moving_mean:0' shape=(3,) dtype=float32>,\r\n <tf.Variable 'third_scope/batch_normalization_3/moving_variance:0' shape=(3,) dtype=float32>,\r\n <tf.Variable 'fourth_scope/batch_normalization_4/gamma:0' shape=(3,) dtype=float32>,\r\n <tf.Variable 'fourth_scope/batch_normalization_4/beta:0' shape=(3,) dtype=float32>,\r\n <tf.Variable 'fourth_scope/batch_normalization_4/moving_mean:0' shape=(3,) dtype=float32>,\r\n <tf.Variable 'fourth_scope/batch_normalization_4/moving_variance:0' shape=(3,) dtype=float32>]\r\n```\r\n\r\nIf you have a look, you will see, the variables with the same variables_scope are named differently, because we are using a global numbering. I think that does'nt makes sense. looking forward for your opinion.\r\n", "still awaiting the response...", "Issue replicating replicating for[ tf-nightly](https://colab.sandbox.google.com/gist/oanush/d6fd3ccc61a2653c16eeee42e837b4f3/35906.ipynb),thanks!", "Is there any progress on this issue? The different numbering scheme between tf.keras.layers and tf.layers makes it difficult to reload checkpoints that were produced through the other API.\r\n\r\nEdit: as a temporary solution:\r\n\r\n- I replaced all `tf.keras.layers` layers by `tf.layers` layers in the code,\r\n- loaded the checkpoint using `model.load_weights`,\r\n- re-saved it with `model.save_weights` (this saves in a format where the numbering scheme doesn't matter),\r\n- reverted the code to use `tf.keras.layers`\r\n- now the new weight file can be loaded using `model.load_weights`.", "Was able to reproduce issue in Tf Nightly 2.6.0-dev20210524, please find the [gist](https://colab.research.google.com/gist/sachinprasadhs/43d5e0b89b76db855d57fb9fc1a81e6f/35650.ipynb#scrollTo=kGg_oY4k3pqV) here. Thanks!", "Hi There,\n\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \n\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35906\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35906\">No</a>\n"]}, {"number": 35905, "title": "Fix the comparison of integer expressions of different signedness", "body": "This PR fixes a warning when building TF from source:\r\n```\r\ntensorflow/core/platform/numbers.cc:65:21: warning: comparison of integer expressions of different signedness: 'int' and 'std::basic_string<char>::size_type' {aka 'long unsigned int'} [-Wsign-compare]\r\n```", "comments": []}, {"number": 35904, "title": "Pydoc fixes for StructuredTensor", "body": "Minor fixes for StructuredTensor pydocs that had incorrect example code and Python 2 style print statements.", "comments": ["~@rthadur is this something you can review?~\r\nOops, sorry I just saw a review was requested yesterday, thanks!\r\n", "@BryanCutler Can you please resolve conflicts? Thanks!", "@gbaned I resolved conflicts and updated to reflect some recent changes. Tests look like they failed due to time out. Thanks for taking a look!", "Thank you for reviewing @edloper , and congrats on the baby!", "@BryanCutler Can you please fix build failures ? Thanks!", "@gbaned and @edloper the tests have now passed, is this ok to merge?"]}, {"number": 35903, "title": "ImportError: DLL load failed, module not found", "body": "Traceback (most recent call last):\r\n  File \"C:\\Users\\laue\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\laue\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\laue\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\laue\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\laue\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: Das angegebene Modul wurde nicht gefunden.#module not found#\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\laue\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Users\\laue\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\laue\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\laue\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\laue\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\laue\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\laue\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\laue\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\laue\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\laue\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\laue\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\laue\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: Das angegebene Modul wurde nicht gefunden.\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": ["What is cpu model/make? Does it support avx instruction sets?", "@laue1234 , \r\nAlso make sure to see the hardware requirement from these links [linux](https://www.tensorflow.org/install/source#setup_for_linux_and_macos) and [windows](https://www.tensorflow.org/install/source_windows#setup_for_windows).Let us know if it helped.Thanks!", "@laue1234,\r\nInstalling the latest version of Microsoft Visual C++ from [this](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads) link worked for some users. Please check issue [#35749](https://github.com/tensorflow/tensorflow/issues/35749#issuecomment-573313402) for reference. Thanks!", "Downgrading to tensorflow 2.0 worked for me. Thanks", "@laue1234 ,\r\nPlease feel free to close the issue if it is resolved.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35903\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35903\">No</a>\n"]}, {"number": 35902, "title": "Unexpected behavior calling tf.keras.Model.call() with named parameters", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): `yes`\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Linux Ubuntu 18.04.3 LTS`\r\n- TensorFlow installed from (source or binary): `binary`\r\n- TensorFlow version (use command below): `2.0.2`\r\n- Python version: `Python 3.7.4 Anaconda`\r\n- CUDA/cuDNN version: `ROCM`:\r\n```\r\nPackage: rocm-libs\r\nVersion: 3.0.6\r\nPriority: optional\r\nSection: devel\r\nMaintainer: Advanced Micro Devices Inc.\r\nInstalled-Size: 13.3 kB\r\nDepends: rocfft, rocrand, rocblas, hipblas, rocsparse, hipsparse, rocalution, rocprim, rocthrust, hipcub\r\nHomepage: https://github.com/RadeonOpenCompute/ROCm\r\nDownload-Size: 802 B\r\nAPT-Manual-Installed: yes\r\nAPT-Sources: http://repo.radeon.com/rocm/apt/debian xenial/main amd64 Packages\r\nDescription: Radeon Open Compute (ROCm) Runtime software stack\r\n```\r\n- GPU model and memory: AMD Radeon VII\r\n\r\nThe full environment script does not work for my machine, but:\r\n```\r\n`python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n```\r\n```\r\n>>> v2.0.0-rocm-3-g0826c3a 2.0.2\r\n```\r\n\r\n**Describe the current behavior**\r\n\r\nI am getting odd behavior when calling a `tf.keras.Model`'s `call` method when using the names of the method's parameters. The method works as expected when using position only arguments, but breaks when using the names. However, when I call my `model_instance.call()` with the names of the method parameters, things work as expected. It is making me wonder which `__call__` method I am calling when simply running `model_instance()`.\r\n\r\n**Describe the expected behavior**\r\n\r\nUsing the names of the parameters in a `tf.keras.Model`'s `call` method should not be raising an error.\r\n\r\n**Code to reproduce the issue**\r\n\r\nFirst, a little bit of setup showing that calling a `tf.keras.layers.Attention` instance from a function works with and without using the names of the positional arguments in a user defined function, `call`:\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\ndef call(q, v, k, mask_q=None, mask_v=None):\r\n    \"\"\" Call attention instance \"\"\"\r\n    return attn(inputs=[q, v, k], mask=[mask_q, mask_v])\r\n\r\nx = tf.random.uniform((1, 2, 2))\r\nattn = tf.keras.layers.Attention(use_scale=True)\r\n```\r\n```python\r\n# position arguments work well\r\ncall(x, x, x)\r\n```\r\n```\r\n>>> <tf.Tensor: id=89, shape=(1, 2, 2), dtype=float32, numpy=\r\narray([[[0.62968266, 0.6612503 ],\r\n        [0.6235384 , 0.73767066]]], dtype=float32)>\r\n```\r\n```python\r\n# naming the parameters also fine here\r\ncall(q=x, v=x, k=x)\r\n```\r\n```\r\n>>> <tf.Tensor: id=89, shape=(1, 2, 2), dtype=float32, numpy=\r\narray([[[0.62968266, 0.6612503 ],\r\n        [0.6235384 , 0.73767066]]], dtype=float32)>\r\n```\r\nThings start getting weird when doing something similar within a `tf.keras.Model`:\r\n```python\r\nclass MyAttention(tf.keras.Model):\r\n    \r\n    def __init__(self):\r\n        super(MyAttention, self).__init__()\r\n        self.attention = tf.keras.layers.Attention(use_scale=True)\r\n        \r\n    def call(self, q, v, k, mask_q=None, mask_v=None):\r\n        return self.attention(inputs=[q, v, k], mask=[mask_q, mask_v])\r\n\r\n\r\nmy_attention = MyAttention()\r\n```\r\n```python\r\n# Still works with positional arguments\r\nmy_attention(x, x, x)\r\n```\r\n```\r\n>>> <tf.Tensor: id=106, shape=(1, 2, 2), dtype=float32, numpy=\r\narray([[[0.62968266, 0.6612503 ],\r\n        [0.6235384 , 0.73767066]]], dtype=float32)>\r\n```\r\n```python\r\n# Breaks when naming the arguments in my_attention:\r\nmy_attention(q=x, v=x, k=x)\r\n```\r\n```\r\n>>> \r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-15-5fa3b47998d9> in <module>\r\n----> 1 needs_attention(q=x, v=x, k=x)\r\n\r\nTypeError: __call__() missing 1 required positional argument: 'inputs'\r\n```\r\nFinally, if I explicitly call `my_attention.call()`:\r\n```python\r\nmy_attention.call(q=x, v=x, k=x)\r\n```\r\n```\r\n>>> <tf.Tensor: id=106, shape=(1, 2, 2), dtype=float32, numpy=\r\narray([[[0.62968266, 0.6612503 ],\r\n        [0.6235384 , 0.73767066]]], dtype=float32)>\r\n```\r\n**Other info / logs**\r\nHere is a gist to show this behavior:\r\nhttps://gist.github.com/yngtodd/f3bda25503a9611765ab33c1178db48c\r\n", "comments": ["Interestingly, if I add `**kwargs` to `MyAttention`:\r\n```python\r\nclass MyAttention(tf.keras.Model):\r\n    \r\n    def __init__(self):\r\n        super(MyAttention, self).__init__()\r\n        self.attention = Attention(use_scale=True)\r\n        \r\n    def call(self, q, v, k, mask_q=None, mask_v=None, **kwargs):\r\n        \"\"\" Print **kwargs, then call tf.keras.layers.Attention \"\"\"\r\n        for key, value in kwargs.items(): \r\n            print(f'{key} == {value}') \r\n        return self.attention(inputs=[q, v, k], mask=[mask_q, mask_v])\r\n```\r\nI can check to see if that `my_attention.call` is being used:\r\n```python\r\n# as expected:\r\nmy_attention(x, x, x, extra_arg='hi')\r\n```\r\n```python\r\n>>> extra_arg == hi\r\n<tf.Tensor: id=58, shape=(1, 2, 2), dtype=float32, numpy=\r\narray([[[0.74538416, 0.40650344],\r\n        [0.6511749 , 0.39531568]]], dtype=float32)>\r\n```\r\n```python\r\n# fails to print the kwarg, complains about missing positional arg `inputs`\r\nmy_attention(q=x, v=x, k=x, extra_arg='hi')\r\n```\r\n```python\r\n>>>\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-15-c2133110731e> in <module>\r\n----> 1 needs_attention(q=x, v=x, k=x, extra_arg='hi')\r\n\r\nTypeError: __call__() missing 1 required positional argument: 'inputs'\r\n```\r\n```python\r\n# it only takes leaving out the name of the first parameter for it to work again\r\nmy_attention(x, v=x, k=x, extra_arg='hi')\r\n```\r\n```python\r\n>>> extra_arg == hi\r\n<tf.Tensor: id=63, shape=(1, 2, 2), dtype=float32, numpy=\r\narray([[[0.74538416, 0.40650344],\r\n        [0.6511749 , 0.39531568]]], dtype=float32)>\r\n```", "I could replicate the issue with Tf 2.0.\r\nPlease find the gist [here](https://colab.sandbox.google.com/drive/1ufQ_fR0ifTYaiTZ-MNcBJ7hmqq89is_Z?authuser=1#scrollTo=RgOa3-NOCjxe). Thanks!", "Can't reproduce this error with latest tf-nightly 2.2.0.dev20200203. Probably the issue is fixed by the recent changes.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35902\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35902\">No</a>\n"]}, {"number": 35901, "title": "FusedBatchNormV3 and AddV2 need custom implementations.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro, Version 1903\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de410 2.1.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 10.2.89\r\n- GPU model and memory: GeForce GTX 1050 2.00GiB\r\n\r\n**Describe the current behavior**\r\n\r\nI have created the simplest case of sequential neural network which only contains one Batch Normalization layer. Then I have tried to convert this model to Tensorflow Lite but then I got following message:\r\n\r\n    Here is a list of operators for which you will need custom implementations: FusedBatchNormV3.\r\n\r\nWhen I tried to set parameter `fused=False` I got following message:\r\n\r\n    Here is a list of operators for which you will need custom implementations: AddV2.\r\n\r\n**Describe the expected behavior**\r\n\r\nI expect converted Tensorflow Lite model with optimized Batch Normalization weights.\r\n\r\n**Code to reproduce the issue**\r\n\r\n    import tensorflow as tf\r\n\r\n    tflite_path = 'test.tflite'\r\n    keras_path = 'test.h5'\r\n\r\n    bn = tf.keras.layers.BatchNormalization(\r\n        # fused=False,\r\n        input_shape = (128,128,3)\r\n    )\r\n\r\n    # Create model\r\n    k_model = tf.keras.models.Sequential()\r\n    k_model.add(bn)\r\n    k_model.save(keras_path)\r\n    \r\n    # Convert keras model to tflite\r\n    converter = tf.lite.TFLiteConverter.from_keras_model(k_model)\r\n    tflite_model = converter.convert()\r\n    open(tflite_path, 'wb').write(tflite_model)\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["This model appears to convert with `tf-nightly` (`pip install tf-nightly`) version `2.1.0.dev20200102`. We also recommend using `converter.experimental_new_converter = True`. However, that didn't seem necessary for me to get the model converting.", "I have an update regarding this issue. \r\n\r\nI have tried installing `tf-nightly` version `2.1.0.dev20200102` on existing machine but the problem was still there.\r\nWhen I tried to run the same script on a different machine (Windows 10 machine version 1903 with clean `Anaconda3 Python 3.7` installation) both `tf-nightly` and `tensorflow 2.1` were working correctly (exactly as I described in expected behavior).", "Taking a closer look into the op, `FusedBatchNormV3` has been supported since June 2019. Given that this issue is only occurring in one installation of TensorFlow, it appears this is an installation related issue. Can you try reinstalling TensorFlow on the machine giving the error or provide additional information about your installation that might help with debugging?", "I was able to find a core problem.\r\nLast year I have built `Tensorflow 1.13` python package from source. During installation python had prompted me to use `--user` flag for installation.  By using `--user` flag TensorFlow was installed in `AppData/Roaming/Python` location. Because of that python interpreter was referencing some TensorFlow python scripts from `AppData/Roaming/Python` location despite me crating clean new virtual environment.\r\nI had solved this issue by deleting TensorFlow from `AppData/Roaming/Python` location.\r\nThank you for your assistance.", "Closing because it is resolved as an installation issue."]}, {"number": 35900, "title": "Dataset padded_batch does not work as documented", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): A little\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab (ubuntu-based)\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1.0-rc1\r\n- Python version:  3.7\r\n\r\n**Describe the current behavior**\r\n\r\ncalling `Dataset.padded_batch([batch_size], [output_shape], padding_values=1)`  fails with the following error:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-38-6fccac1ccecf> in <module>()\r\n     20 ds_train = ds_train.padded_batch(BATCH_SIZE, padded_shapes)\r\n     21 \r\n---> 22 ds_test = ds_test.padded_batch(BATCH_SIZE, padded_shapes, padding_values=padded_values)\r\n\r\n3 frames\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/data/util/nest.py in assert_shallow_structure(shallow_tree, input_tree, check_types)\r\n    297       raise TypeError(\r\n    298           \"If shallow structure is a sequence, input must also be a sequence. \"\r\n--> 299           \"Input has type: %s.\" % type(input_tree))\r\n    300 \r\n    301     if check_types and not isinstance(input_tree, type(shallow_tree)):\r\n\r\nTypeError: If shallow structure is a sequence, input must also be a sequence. Input has type: <class 'int'>.\r\n```\r\n\r\nNote that this does not fail if one uses the default value of `None`\r\n\r\n**Describe the expected behavior**\r\n\r\nShould pad the data with the value in `padding_values`.  \r\n\r\nAlso, the error message could be friendly by telling me what type it expects. \r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\n\r\nBATCH_SIZE = 64\r\n\r\nds, ds_info = tfds.load(\"imdb_reviews/subwords8k\", with_info=True, as_supervised=True)\r\nds_train, ds_test = ds[\"train\"], ds[\"test\"]\r\n\r\noutput_shapes_train = tf.compat.v1.data.get_output_shapes(ds_train)\r\npadded_shapes = output_shapes_train  # (TensorShape([None]), TensorShape([]))\r\npadded_values = -1\r\n\r\nds_train = ds_train.padded_batch(BATCH_SIZE, padded_shapes)  # does not fail here\r\nds_test = ds_test.padded_batch(BATCH_SIZE, padded_shapes, padding_values=padded_values)  # but does fail here\r\n```\r\n\r\n\r\n**Other info / logs**\r\nhttps://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=stable#padded_batch\r\n\r\nDocumentation seems pretty clear that the second part should work.", "comments": ["@dhpollack ,\r\nCan you please provide complete code? When tried executing given code I got error log `NameError: name 'output_shapes_train' is not defined`,find [gist of colab](https://colab.sandbox.google.com/gist/oanush/a35082df991ff743ad242ff1a6393ead/35900.ipynb) for reference.Thanks!", "@oanush sorry about that.  I forgot about the line `output_shapes_train = tf.compat.v1.data.get_output_shapes(ds_train)`, but edited the above code with it.  \r\n\r\nI was playing with the code on colab and you can see the notebook [here](https://colab.research.google.com/drive/12cUqrvcoBDCCB7EYg5M8AiiA4VmrplVy)", "I ran the code on your gist with the additional line and got the error that I've described above.  On my notebook I was using tf 2.1.0-rc1 rather than 2.2 in your gist and get the same result.", "Issue replicating,thanks!", "The structure of `padding_values` needs to match the structure of your dataset elements (so that you can specify different padding values for different structure components).\r\n\r\nThe [documentation](https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=stable#padded_batch) calls this out, including an example of a dataset whose elements are tuples.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35900\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35900\">No</a>\n", "@jsimsa the documentation is at best unclear. If I want to pad my tensor in all dimensions with -1 then why can't I use a simple integer? This seems like the most practical case and it isn't clear to me how to do this. I'm willing to admit this might not be a bug but in that case it's a feature request to allow for simple types to be propagated logically. ", "You can do the following:\r\n\r\n```\r\nds = tf.data.Dataset.from_tensors((0.0, 1))\r\npadding_values = tf.nest.map_structure(lambda _: -1, ds.element_spec)\r\nprint(padding_values) # prints (-1, -1)\r\n```", "The feature you are requesting seems reasonable and would make a good candidate for an external contribution. If you create a feature request, please cc me on it so that I can tag it as contributions welcome. ", "Ok, still seems unintuitive but that's a very helpful answer. I'll give it a shot. Thanks for clearing that up for me. \ud83d\udc4d\ud83c\udffc", "Dude this function `padded_batch` is horrible... Must be rethinked in my point of view"]}, {"number": 35899, "title": "keras and tf.keras has different output", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n\r\n\r\n- ubuntu 16.04\r\nI create two conda virtualenv.\r\nfirst:\r\n- tensorflow2.1  gpu  installed by pip\r\n- python 3.6\r\n- cuda 10.1 installed by conda\r\n- cudnn 7.6 installed by conda \r\n- gpu Nvidia TitanXp with 12GB memory\r\nsecond:\r\n- tensorflow1.15 cpu \r\n- keras 2.2.4 installed by conda \r\n- python 3.6\r\n\r\n**Describe the current behavior**\r\n\r\n**In the first virtualenv,I used code as below shows:**\r\n`from sklearn.datasets import make_regression\r\nfrom tensorflow.keras.layers import Dense\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.optimizers import SGD\r\nfrom matplotlib import pyplot\r\n\r\nX, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=1)\r\n\r\nn_train = 500\r\ntrainX, testX = X[:n_train, :], X[n_train:, :]\r\ntrainy, testy = y[:n_train], y[n_train:]\r\n\r\nmodel = Sequential()\r\nmodel.add(Dense(25, input_dim=20, activation='relu', kernel_initializer='he_uniform'))\r\nmodel.add(Dense(1, activation='linear'))\r\n\r\nopt = SGD(lr=0.01, momentum=0.9,clipnorm=1)\r\nmodel.compile(loss='mean_squared_error', optimizer=opt)\r\n\r\nhistory = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=100, verbose=0)\r\n\r\ntrain_mse = model.evaluate(trainX, trainy, verbose=0)\r\ntest_mse = model.evaluate(testX, testy, verbose=0)\r\nprint('Train: %.3f, Test: %.3f' % (train_mse, test_mse))\r\n\r\npyplot.title('Mean Squared Error')\r\npyplot.plot(history.history['loss'], label='train')\r\npyplot.plot(history.history['val_loss'], label='test')\r\npyplot.legend()\r\npyplot.show()`\r\n**In the second virtualenv,I just change the code from tensorflow.keras to keras**\r\n\r\n**Describe the expected behavior**\r\n`opt = SGD(lr=0.01, momentum=0.9,clipnorm=1)` this code with and without  parameter clipnorm\r\nwill have different performance in second virtualenv(keras) but the first virtualenv(tensorflow2.1 tf.keras) just show same output.So is the parameter clipnorm is useless in tensorflow2.1 ?\r\n\r\n", "comments": ["@DLlearn Can you please share a standalone code as a colab gist or jupyter notebook. I tried running your code in `tf-nightly`, and the `loss` defined in the model results `nan`. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/b2b63e162f03cdf352e45a2c32dc4ac1/untitled762.ipynb). Thanks!", "> @DLlearn Can you please share a standalone code as a colab gist or jupyter notebook. I tried running your code in `tf-nightly`, and the `loss` defined in the model results `nan`. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/b2b63e162f03cdf352e45a2c32dc4ac1/untitled762.ipynb). Thanks!\r\n\r\n@jvishnuvardhan check [pure keras](https://colab.research.google.com/drive/1Xfaxtelvx695b_7JgPcgWoAG4OEHoNZX) colab gist,and [tf keras](https://colab.research.google.com/drive/1fp5VBqLI1kRO2BGSyNEUkoTuuaGSiQDX),compare this two colab gist,you can find the parameter *clipnorm* of SGD optimizer in tensorflow.keras is useless, I do not know why tensorflow keep it as a parameter.", "I just ran it. Looks like this has been fixed. Please try and verify it in tf-nightly-gpu.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35899\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35899\">No</a>\n"]}, {"number": 35898, "title": "fix open qoute in docs", "body": "An unclosed qoute mess up the docs.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35898) for more info**.\n\n<!-- need_sender_cla -->", "We will not be accepting changes to release branch unless it is a security fix , please make your changes against master.\r\ncc @mihaimaruseac "]}, {"number": 35897, "title": "tensorflow program crashes with `CUDA_ERROR_ILLEGAL_ADDRESS` with `tf.boolean_mask` used.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): docker image `nvcr.io/nvidia/tensorflow:19.09-py3`\r\n- ~Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:~\r\n- TensorFlow installed from (source or binary): see docker image\r\n- TensorFlow version (use command below): 1.14.0+nv\r\n- Python version: 3.6.8\r\n- ~Bazel version (if compiling from source):~\r\n- ~GCC/Compiler version (if compiling from source):~\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: **Tesla V100-PCIE-16GB**\r\n\r\n**Describe the current behavior**\r\nwith the following code added in `model_fn`, program crashes with `CUDA_ERROR_ILLEGAL_ADDRESS`:\r\n```python\r\n           def create_mean_metrics(t_logits, t_label, v_label):\r\n                t_filtered = tf.boolean_mask(t_logits, tf.equal(t_map_label, v_label))\r\n                return tf.metrics.mean_tensor(tf.reduce_mean(t_filtered), tf.size(t_filtered))\r\n\r\n            eval_metrics = {\r\n                'mean_perfect': create_mean_metrics(t_logits, t_map_label, 0),\r\n                'mean_excellent': create_mean_metrics(t_logits, t_map_label, 1),\r\n                'mean_good': create_mean_metrics(t_logits, t_map_label, 2),\r\n                'mean_fair': create_mean_metrics(t_logits, t_map_label, 3),\r\n                'mean_bad': create_mean_metrics(t_logits, t_map_label, 5),\r\n            }\r\n\r\n            return tf.estimator.EstimatorSpec(\r\n                mode=mode,\r\n                loss=t_eval_loss,\r\n                eval_metric_ops=eval_metrics,\r\n            )\r\n```\r\nif the `eval_metrics` part removed, the program will run without crashes.\r\n\r\n**Describe the expected behavior**\r\nprogram run without crashing.\r\n\r\n**Other info / logs**\r\n```\r\n2020-01-15 11:07:08.705510: I tensorflow/stream_executor/cuda/ptxas_utils.cc:202] \r\n2020-01-15 11:07:11.712068: E tensorflow/stream_executor/cuda/cuda_driver.cc:1048] Internal: could not synchronize on CUDA stream: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered :: *** Begin stack trace ***\r\n        tensorflow::CurrentStackTrace()\r\n        stream_executor::gpu::GpuDriver::SynchronizeStream(stream_executor::gpu::GpuContext*, CUstream_st*)\r\n        stream_executor::gpu::GpuExecutor::BlockHostUntilDone(stream_executor::Stream*)\r\n        stream_executor::StreamExecutor::BlockHostUntilDone(stream_executor::Stream*)\r\n        stream_executor::Stream::BlockHostUntilDone()\r\n\r\n\r\n\r\n\r\n\r\n        tensorflow::XlaRunOp::Compute(tensorflow::OpKernelContext*)\r\n        tensorflow::BaseGPUDevice::ComputeHelper(tensorflow::OpKernel*, tensorflow::OpKernelContext*)\r\n        tensorflow::BaseGPUDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*)\r\n\r\n\r\n        Eigen::ThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int)\r\n        std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\r\n\r\n\r\n        clone\r\n*** End stack trace ***\r\n\r\n2020-01-15 11:07:11.765522: E tensorflow/stream_executor/cuda/cuda_driver.cc:1032] could not synchronize on CUDA context: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered :: *** Begin stack trace ***\r\n        tensorflow::CurrentStackTrace()\r\n        stream_executor::gpu::GpuDriver::SynchronizeContext(stream_executor::gpu::GpuContext*)\r\n        stream_executor::StreamExecutor::SynchronizeAllActivity()\r\n        tensorflow::XlaCompilationCache::~XlaCompilationCache()\r\n        tensorflow::XlaCompilationCache::~XlaCompilationCache()\r\n        tensorflow::ResourceMgr::Clear()\r\n        tensorflow::DirectSession::~DirectSession()\r\n        tensorflow::DirectSession::~DirectSession()\r\n        tensorflow::SessionRef::Close()\r\n        TF_CloseSession\r\n\r\n\r\n        _PyEval_EvalFrameDefault\r\n\r\n\r\n        _PyEval_EvalFrameDefault\r\n\r\n\r\n        _PyEval_EvalFrameDefault\r\n\r\n\r\n        _PyEval_EvalFrameDefault\r\n\r\n\r\n        _PyEval_EvalFrameDefault\r\n\r\n\r\n        _PyEval_EvalFrameDefault\r\n\r\n\r\n\r\n        _PyEval_EvalFrameDefault\r\n        _PyFunction_FastCallDict\r\n\r\n        _PyObject_FastCallDict\r\n        PyObject_CallFunctionObjArgs\r\n        _PyEval_EvalFrameDefault\r\n\r\n\r\n\r\n        _PyEval_EvalFrameDefault\r\n\r\n\r\n\r\n        _PyEval_EvalFrameDefault\r\n\r\n\r\n\r\n        _PyEval_EvalFrameDefault\r\n\r\n\r\n\r\n        _PyEval_EvalFrameDefault\r\n\r\n\r\n        _PyEval_EvalFrameDefault\r\n\r\n\r\n        _PyEval_EvalFrameDefault\r\n        _PyFunction_FastCallDict\r\n\r\n\r\n\r\n        _PyObject_FastCallKeywords\r\n\r\n        _PyEval_EvalFrameDefault\r\n\r\n\r\n\r\n        _PyEval_EvalFrameDefault\r\n\r\n        _PyFunction_FastCallDict\r\n\r\n\r\n\r\n        _PyObject_FastCallKeywords\r\n\r\n        _PyEval_EvalFrameDefault\r\n\r\n\r\n\r\n        _PyEval_EvalFrameDefault\r\n\r\n\r\n        _PyEval_EvalFrameDefault\r\n\r\n\r\n        _PyEval_EvalFrameDefault\r\n\r\n\r\n        _PyEval_EvalFrameDefault\r\n\r\n\r\n\r\n        _PyEval_EvalFrameDefault\r\n\r\n\r\n        _PyEval_EvalFrameDefault\r\n\r\n\r\n        _PyEval_EvalFrameDefault\r\n\r\n\r\n\r\n        _PyEval_EvalFrameDefault\r\n\r\n\r\n        _PyEval_EvalFrameDefault\r\n\r\n        PyEval_EvalCode\r\n\r\n        PyRun_FileExFlags\r\n        PyRun_SimpleFileExFlags\r\n        Py_Main\r\n        main\r\n        __libc_start_main\r\n        _start\r\n*** End stack trace ***\r\n\r\n2020-01-15 11:07:11.765871: E tensorflow/compiler/jit/xla_compilation_cache.cc:53] Error synchronizing activity while waiting for all programs to complete\r\n2020-01-15 11:07:11.768797: E tensorflow/stream_executor/cuda/cuda_driver.cc:763] failed to unload module 0x7f66709c3ec0; leaking: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered\r\n2020-01-15 11:07:11.776140: E tensorflow/stream_executor/cuda/cuda_driver.cc:763] failed to unload module 0x7f66748a71f0; leaking: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1356, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1341, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1429, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found.\r\n  (0) Internal: Failed to complete all kernels launched on stream 0x1f50d730: could not synchronize on CUDA stream: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered\r\n         [[{{node cluster_0_1/xla_run}}]]\r\n         [[Identity/_941]]\r\n  (1) Internal: Failed to complete all kernels launched on stream 0x1f50d730: could not synchronize on CUDA stream: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered\r\n         [[{{node cluster_0_1/xla_run}}]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"hvd_combine_model_train_fidelity.py\", line 684, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"hvd_combine_model_train_fidelity.py\", line 464, in main\r\n    max_steps=num_train_steps, hooks=training_hooks)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 367, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1158, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1192, in _train_model_default\r\n    saving_listeners)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1480, in _train_with_estimator_spec\r\n    log_step_count_steps=log_step_count_steps) as mon_sess:\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 584, in MonitoredTrainingSession\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 1007, in __init__\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 725, in __init__\r\n    self._sess = _RecoverableSession(self._coordinated_creator)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 1200, in __init__\r\n    _WrappedSession.__init__(self, self._create_session())\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 1205, in _create_session\r\n    return self._sess_creator.create_session()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 878, in create_session\r\n    hook.after_create_session(self.tf_sess, self.coord)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/hooks/hooks.py\", line 180, in after_create_session\r\n    self._evaluate(session)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/hooks/hooks.py\", line 203, in _evaluate\r\n    output_dir=self._eval_dir)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1609, in _evaluate_run\r\n    config=self._session_config)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/evaluation.py\", line 272, in _evaluate_once\r\n    session.run(eval_ops, feed_dict)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 754, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 1252, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 1353, in run\r\n    raise six.reraise(*original_exc_info)\r\n  File \"/usr/local/lib/python3.6/dist-packages/six.py\", line 693, in reraise\r\n    raise value\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 1338, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 1411, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 1169, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 950, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1173, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1350, in _do_run\r\n    run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1370, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found.\r\n  (0) Internal: Failed to complete all kernels launched on stream 0x1f50d730: could not synchronize on CUDA stream: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered\r\n         [[{{node cluster_0_1/xla_run}}]]\r\n         [[Identity/_941]]\r\n  (1) Internal: Failed to complete all kernels launched on stream 0x1f50d730: could not synchronize on CUDA stream: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered\r\n         [[{{node cluster_0_1/xla_run}}]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n```\r\n", "comments": ["Apologies for the delay in response. Is this still an issue? \r\nTF 1.14 supports cuda 10.0 ", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35897\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35897\">No</a>\n"]}, {"number": 35896, "title": "Python crashed after importing Tensorflow", "body": "**System information**\r\n- OS Platform and Distribution: Windows 10 Enterprise\r\n- TensorFlow installed from (source or binary): pip install tensorflow\r\n- TensorFlow version: 2.1.0\r\n- Python version: 3.6.7\r\n- Installed using virtualenv? pip? conda?: Virtual environment created with Conda, tensorflow installed with pip\r\n- CUDA/cuDNN version: v10.1 / v7.6.5\r\n- GPU model and memory: Nvidia Titan Xp\r\n\r\n**Describe the problem**\r\nFollowing the instruction provided online, I have successfully installed Tensorflow. To verify my installation, I have tried to import tensorflow in python. However, Python crashed immediately after importing tensorflow library. I have also tried to install through:\r\n`pip install tensorflow-gpu==2.1.0`\r\nUnfortunately, I experienced the same problem. \r\n\r\n**Any other info / logs**\r\nBefore the crashing of Python, such message has been printed:\r\n> 2020-01-15 11:51:56.544389: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n", "comments": ["Can you try to create a virtual environment and install TF again? Thanks!", "What is the error message from the crash?", "> Can you try to create a virtual environment and install TF again? Thanks!\r\n\r\nThanks for your reply! I installed TF with \"VirtualEnv\" just now and this time Python has imported TF successfully. Nevertheless, the following message has been printed and I hope this is not a problem. \r\n\r\n> 2020-01-16 07:45:45.912186: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-01-16 07:45:52.562035: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-01-16 07:45:52.605482: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: TITAN Xp computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 30 deviceMemorySize: 12.00GiB deviceMemoryBandwidth: 510.07GiB/s\r\n2020-01-16 07:45:52.617555: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-01-16 07:45:52.691494: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-01-16 07:45:52.748697: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-01-16 07:45:52.779087: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-01-16 07:45:52.844045: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-01-16 07:45:52.885591: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-01-16 07:45:53.265967: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-01-16 07:45:53.274034: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-01-16 07:45:53.282230: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2020-01-16 07:45:53.293759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: TITAN Xp computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 30 deviceMemorySize: 12.00GiB deviceMemoryBandwidth: 510.07GiB/s\r\n2020-01-16 07:45:53.309840: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-01-16 07:45:53.314702: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-01-16 07:45:53.322320: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-01-16 07:45:53.330254: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-01-16 07:45:53.335417: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-01-16 07:45:53.341735: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-01-16 07:45:53.348488: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-01-16 07:45:53.354031: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-01-16 07:45:54.440229: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-01-16 07:45:54.448401: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0\r\n2020-01-16 07:45:54.456322: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N\r\n2020-01-16 07:45:54.464534: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9592 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n\r\nIt is kind strange why Python will crash when we install TF in the virtual environment created with Conda (that is what I have done, create virtual environment in Conda and install TF with pip). Anyway, I guess I will just stick to \"VirtualEnv\" whenever I use TF.  ", "> What is the error message from the crash?\r\n\r\nThanks for your reply, there is actually no error message, the Python just crashed after printing:\r\n\r\n> 2020-01-15 11:51:56.544389: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll", "> > Can you try to create a virtual environment and install TF again? Thanks!\r\n> \r\n> Thanks for your reply! I installed TF with \"VirtualEnv\" just now and this time Python has imported TF successfully. Nevertheless, the following message has been printed and I hope this is not a problem.\r\n> \r\n> > 2020-01-16 07:45:45.912186: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n> > 2020-01-16 07:45:52.562035: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n> > 2020-01-16 07:45:52.605482: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:\r\n> > pciBusID: 0000:01:00.0 name: TITAN Xp computeCapability: 6.1\r\n> > coreClock: 1.582GHz coreCount: 30 deviceMemorySize: 12.00GiB deviceMemoryBandwidth: 510.07GiB/s\r\n> > 2020-01-16 07:45:52.617555: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n> > 2020-01-16 07:45:52.691494: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n> > 2020-01-16 07:45:52.748697: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n> > 2020-01-16 07:45:52.779087: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n> > 2020-01-16 07:45:52.844045: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n> > 2020-01-16 07:45:52.885591: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n> > 2020-01-16 07:45:53.265967: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n> > 2020-01-16 07:45:53.274034: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n> > 2020-01-16 07:45:53.282230: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n> > 2020-01-16 07:45:53.293759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:\r\n> > pciBusID: 0000:01:00.0 name: TITAN Xp computeCapability: 6.1\r\n> > coreClock: 1.582GHz coreCount: 30 deviceMemorySize: 12.00GiB deviceMemoryBandwidth: 510.07GiB/s\r\n> > 2020-01-16 07:45:53.309840: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n> > 2020-01-16 07:45:53.314702: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n> > 2020-01-16 07:45:53.322320: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n> > 2020-01-16 07:45:53.330254: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n> > 2020-01-16 07:45:53.335417: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n> > 2020-01-16 07:45:53.341735: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n> > 2020-01-16 07:45:53.348488: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n> > 2020-01-16 07:45:53.354031: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n> > 2020-01-16 07:45:54.440229: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> > 2020-01-16 07:45:54.448401: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0\r\n> > 2020-01-16 07:45:54.456322: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N\r\n> > 2020-01-16 07:45:54.464534: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9592 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n> \r\n> It is kind strange why Python will crash when we install TF in the virtual environment created with Conda (that is what I have done, create virtual environment in Conda and install TF with pip). Anyway, I guess I will just stick to \"VirtualEnv\" whenever I use TF.\r\n\r\n@ginsanity15,\r\nI see that you are successfully able to import Tensorflow. Please feel free to close the issue if it is resolved. Thanks!", "Following the request of @amahendrakar , I will close this issue now. Thank you all for your attention and help!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35896\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35896\">No</a>\n", "Had the same issue in anaconda env. What worked for was: \r\n1 `conda uninstall tensorflow` \r\n2 `conda install tensorflow-gpu==2.1.0`"]}, {"number": 35895, "title": "Change collection.Sequence to collection.abc.Sequence in keras recurrent.py 2", "body": "see https://github.com/tensorflow/tensorflow/issues/35892", "comments": ["Thanks for reporting the issue, since py2 is end of life as of 2020/1/1, we probably can change to use py3 symbol. I will need to check with TF build team and see if they have already drop the py2 test, otherwise this will cause build failure in py2.7.\r\n\r\n@gunan, is it safe to migrate to py3 only symbol?", "Unfortunately, we have not deprecated/migrated all the internal workflows yet.\r\nSo we cannot accept any changes that would break python2 yet.", "Thanks for the reply. Closing this PR and related issue now.", "@gunan\r\nthe python 3.9 release is scheduled for October 2020. \r\nhttps://www.python.org/dev/peps/pep-0596/", "I understand, but for a while, we still have to have changes that are compatible with python 2.7.\r\nMaybe we can have the line switch between collections.abc or collections based on python version?"]}, {"number": 35894, "title": "Documentation on Tensorflow 2.1 with TPU", "body": "Are you going to update your TPU documentation https://cloud.google.com/tpu/docs/colabs with 2.1 release?", "comments": ["Sorry, different product. There's a 'Send feedback' button on that page.\r\n\r\nThere's an updated TPU guide on tensorflow.org here: https://www.tensorflow.org/guide/tpu\r\nAnd we're working to update a few of our long-running notebooks."]}, {"number": 35893, "title": "Change collection.Sequence to collection.abc.Sequence in keras recurrent.py", "body": "see https://github.com/tensorflow/tensorflow/issues/35892", "comments": []}, {"number": 35892, "title": "Deprecation notice for collection.Sequence in recurrent.py", "body": "@qlzh727\r\n\r\n**Describe the current behavior**\r\n\r\nWhen running unit tests the following deprecation note appears\r\n\r\n```\r\nlib/python3.7/site-packages/tensorflow_core/python/keras/layers/recurrent.py:808: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\r\n  if (isinstance(inputs, collections.Sequence)\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nNo deprecation notice\r\n\r\nLine 808 needs a minor modification\r\n```\r\n...\r\n  if (isinstance(inputs, collections.abc.Sequence)\r\n...\r\n```\r\n\r\nSee [python 3 docs](https://docs.python.org/3/library/collections.abc.html#collections.abc.Sequence)\r\n", "comments": ["Since we still have to support py2 within Google for now, we can't migrate to py3 only code yet.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35892\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35892\">No</a>\n", "Looks like this has been fixed the proper way now:\r\n```python\r\ntry:\r\n  from collections import abc as collections_abc  # pylint: disable=g-import-not-at-top\r\nexcept ImportError:  # For Python 2\r\n  import collections as collections_abc  # pylint: disable=g-import-not-at-top\r\n```"]}, {"number": 35891, "title": "[ROCm] Fix for //tensorflow/python/eager/benchmarks/resnet50:*", "body": "This replaces several deprecated API calls with their correct v2 forms.", "comments": ["@ekuznetsov139 can you please check the build failures.", "The build failure is as follows\r\n\r\n> ERROR: /tmpfs/bazel_output/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/external/io_bazel_rules_closure/closure/compiler/BUILD:19:1: @io_bazel_rules_closure//closure/compiler:compiler depends on @com_google_javascript_closure_compiler//:com_google_javascript_closure_compiler in repository @com_google_javascript_closure_compiler which failed to fetch. no such package '@com_google_javascript_closure_compiler//': java.io.IOException: Error downloading [https://mirror.bazel.build/repo1.maven.org/maven2/com/google/javascript/closure-compiler-unshaded/v20190528/closure-compiler-unshaded-v20190528.jar, http://repo1.maven.org/maven2/com/google/javascript/closure-compiler-unshaded/v20190528/closure-compiler-unshaded-v20190528.jar] to /tmpfs/bazel_output/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/external/com_google_javascript_closure_compiler/closure-compiler-unshaded-v20190528.jar: GET returned 501 HTTPS Required\r\n> ERROR: Analysis of target '//tensorflow/core:example_protos_closure' failed; build aborted: Evaluation of aspect @io_bazel_rules_closure//closure/protobuf:closure_proto_library.bzl%closure_proto_aspect on //tensorflow/core:example_protos failed: com.google.devtools.build.lib.packages.RepositoryFetchException: no such package '@com_google_javascript_closure_compiler//': java.io.IOException: Error downloading [https://mirror.bazel.build/repo1.maven.org/maven2/com/google/javascript/closure-compiler-unshaded/v20190528/closure-compiler-unshaded-v20190528.jar, http://repo1.maven.org/maven2/com/google/javascript/closure-compiler-unshaded/v20190528/closure-compiler-unshaded-v20190528.jar] to /tmpfs/bazel_output/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/external/com_google_javascript_closure_compiler/closure-compiler-unshaded-v20190528.jar: GET returned 501 HTTPS Required\r\n> \r\n\r\nIt has nothing to do with my patch.", "@ekuznetsov139 sanity failures are due to this issue https://support.sonatype.com/hc/en-us/articles/360041287334 , hopefully this will get fixed soon."]}, {"number": 35890, "title": "[ROCm] Fix for //tensorflow/python/kernel_tests/signal:mel_ops_test", "body": "The test //tensorflow/python/kernel_tests/signal:mel_ops_test fails on ROCm in eager mode by slightly exceeding the error tolerance.\r\nThis patch replaces the calls to exp and log with more accurate expm1 and log1p (which is something that occurs automatically in graph mode but not in eager mode), bringing the error back below threshold.", "comments": ["@ekuznetsov139 can you please fix sanity build errors ?", "gentle ping", "gentle ping", "@ekuznetsov139 Can you please address Ubuntu Sanity errors? Thanks!"]}, {"number": 35888, "title": "Loaded model fails on inference", "body": "**System information**\r\n- TF version 2.1.0, and I have also has this error on TF 2.2.0-dev20200114\r\n- Python 3.7\r\n\r\n**Describe the current behavior**\r\n- Similar to #35527, when I save and then load my model, it fails upon actually using the model, citing the inputs being different to what was expected.\r\n- When I run my code (see below) I get the following error:\r\n```\r\nValueError: Could not find matching function to call loaded from the SavedModel. Got:\r\n  Positional arguments (3 total):\r\n    * (<tf.Tensor 'inputs:0' shape=(1, 10) dtype=int32>, <tf.Tensor 'inputs_1:0' shape=(1, 10) dtype=int32>)\r\n    * False\r\n    * None\r\n  Keyword arguments: {}\r\n\r\nExpected these arguments to match one of the following 4 option(s):\r\n\r\nOption 1:\r\n  Positional arguments (3 total):\r\n    * [TensorSpec(shape=(None, 10), dtype=tf.int32, name='input_ids'), TensorSpec(shape=(None, 10), dtype=tf.int32, name='attention_mask')]\r\n    * True\r\n    * None\r\n  Keyword arguments: {}\r\n\r\nOption 2:\r\n  Positional arguments (3 total):\r\n    * [TensorSpec(shape=(None, 10), dtype=tf.int32, name='input_ids'), TensorSpec(shape=(None, 10), dtype=tf.int32, name='attention_mask')]\r\n    * False\r\n    * None\r\n  Keyword arguments: {}\r\n\r\nOption 3:\r\n  Positional arguments (3 total):\r\n    * [TensorSpec(shape=(None, 10), dtype=tf.int32, name='inputs/0'), TensorSpec(shape=(None, 10), dtype=tf.int32, name='inputs/1')]\r\n    * True\r\n    * None\r\n  Keyword arguments: {}\r\n\r\nOption 4:\r\n  Positional arguments (3 total):\r\n    * [TensorSpec(shape=(None, 10), dtype=tf.int32, name='inputs/0'), TensorSpec(shape=(None, 10), dtype=tf.int32, name='inputs/1')]\r\n    * False\r\n    * None\r\n  Keyword arguments: {}\r\n```\r\n- My reasoning for making this issue when #35527 already exists is that my code to reproduce the issue is more succinct and so should hopefully be easier to troubleshoot.\r\n\r\n**Describe the expected behavior**\r\n- The model should load and behave in the exact same manner in which it was saved (I.e. not crash when doing inference on data). This is currently not the case.\r\n\r\n**Code to reproduce the issue**\r\n```import tensorflow as tf\r\n\r\n\r\n############# Create a model using TF and the popular transformers NLP package ###########\r\n\r\nclass TagModelCreator:\r\n\r\n    def __init__(self, language_model):\r\n        self.language_model = language_model\r\n\r\n    def create(self, num_classes, max_seq_len, get_token_type_ids=False):\r\n\r\n        input_modules = []\r\n        \r\n        input_modules.append(tf.keras.layers.Input(shape=(max_seq_len), dtype='int32', name='input_ids'))\r\n        input_modules.append(tf.keras.layers.Input(shape=(max_seq_len), dtype=\"int32\", name='attention_mask'))\r\n\r\n        lang_layer = self.language_model(input_modules)\r\n        linear_layer = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(num_classes, name='classifier'))(lang_layer[0])\r\n        model = tf.keras.Model(inputs=input_modules, outputs=linear_layer)\r\n\r\n        return model\r\n\r\nfrom transformers import TFAutoModel\r\n\r\nmodel_name = \"bert-base-uncased\"\r\nlanguage_model = TFAutoModel.from_pretrained(model_name)\r\ntagging_model_creator = TagModelCreator(language_model)\r\narbitrary_class_num = 2\r\narbitrary_sequence_length = 10\r\ntagging_model = tagging_model_creator.create(arbitrary_class_num, arbitrary_sequence_length)\r\n\r\n\r\n\r\n\r\n######### Create some spoof data to see how the model handles the data ####################\r\n\r\ndef data_generator():\r\n    yield (([0]*arbitrary_sequence_length, [1]*arbitrary_sequence_length))\r\n\r\ninput_types = ((tf.int32, tf.int32))\r\ninput_shape = ((tf.TensorShape([None]), tf.TensorShape([None])))\r\n\r\ntf_dataset = tf.data.Dataset.from_generator(data_generator, input_types, input_shape).batch(7)\r\n\r\n\r\n\r\n\r\n\r\n\r\n######### Use the spoof data on the model, to confirm that it does inference on the data without errors########\r\n\r\nfor example_input in tf_dataset:\r\n    test_output = tagging_model(example_input)\r\n    break\r\n\r\nprint(test_output)\r\nprint(\"Inference is done correctly BEFORE re-loading the model\")\r\n\r\n\r\n\r\n\r\n\r\n\r\n######## Save and reload the model #################\r\n\r\ntf.keras.models.save_model(model=tagging_model,\r\n                                       filepath=\"test_model_save.tf\",\r\n                                       save_format=\"tf\",\r\n                                       include_optimizer=True\r\n                                      )\r\n\r\nreloaded_model = tf.keras.models.load_model(filepath=\"test_model_save.tf\")\r\n\r\n\r\n\r\n\r\n\r\n\r\n####### Try to repeat the inference as above #########\r\nfor example_input in tf_dataset:\r\n    test_output = reloaded_model(example_input)\r\n    break\r\n```\r\n\r\n**Other info / logs**\r\nThe full output of the above code, including printouts and the error stack trace is\r\n```\r\ntf.Tensor(\r\n[[[-0.6191008  -0.12756673]\r\n  [-0.89110005  0.06499487]\r\n  [-0.8666591  -0.02111167]\r\n  [-0.8456675  -0.08551306]\r\n  [-0.853022   -0.15643758]\r\n  [-0.8632274  -0.20486367]\r\n  [-0.8571876  -0.24682882]\r\n  [-0.8400811  -0.2774819 ]\r\n  [-0.8864943  -0.32766515]\r\n  [-0.8612056  -0.3529073 ]]], shape=(1, 10, 2), dtype=float32)\r\nInference is done correctly BEFORE re-loading the model\r\nWARNING:tensorflow:From C:\\Users\\Peter\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nIf using Keras pass *_constraint arguments to layers.\r\nINFO:tensorflow:Assets written to: test_model_save.tf\\assets\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-1-9d120ebc3322> in <module>\r\n     80 ####### Try to repeat the inference as above #########\r\n     81 for example_input in tf_dataset:\r\n---> 82     test_output = reloaded_model(example_input)\r\n     83     break\r\n     84 \r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    820           with base_layer_utils.autocast_context_manager(\r\n    821               self._compute_dtype):\r\n--> 822             outputs = self.call(cast_inputs, *args, **kwargs)\r\n    823           self._handle_activity_regularization(inputs, outputs)\r\n    824           self._set_mask_metadata(inputs, outputs, input_masks)\r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\saving\\saved_model\\utils.py in return_outputs_and_add_losses(*args, **kwargs)\r\n     57     inputs = args[inputs_arg_index]\r\n     58     args = args[inputs_arg_index + 1:]\r\n---> 59     outputs, losses = fn(inputs, *args, **kwargs)\r\n     60     layer.add_loss(losses, inputs)\r\n     61     return outputs\r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\saving\\saved_model\\utils.py in wrap_with_training_arg(*args, **kwargs)\r\n    111         training,\r\n    112         lambda: replace_training_and_call(True),\r\n--> 113         lambda: replace_training_and_call(False))\r\n    114 \r\n    115   # Create arg spec for decorated function. If 'training' is not defined in the\r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\utils\\tf_utils.py in smart_cond(pred, true_fn, false_fn, name)\r\n     57         pred, true_fn=true_fn, false_fn=false_fn, name=name)\r\n     58   return smart_module.smart_cond(\r\n---> 59       pred, true_fn=true_fn, false_fn=false_fn, name=name)\r\n     60 \r\n     61 \r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\framework\\smart_cond.py in smart_cond(pred, true_fn, false_fn, name)\r\n     54       return true_fn()\r\n     55     else:\r\n---> 56       return false_fn()\r\n     57   else:\r\n     58     return control_flow_ops.cond(pred, true_fn=true_fn, false_fn=false_fn,\r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\saving\\saved_model\\utils.py in <lambda>()\r\n    111         training,\r\n    112         lambda: replace_training_and_call(True),\r\n--> 113         lambda: replace_training_and_call(False))\r\n    114 \r\n    115   # Create arg spec for decorated function. If 'training' is not defined in the\r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\saving\\saved_model\\utils.py in replace_training_and_call(training)\r\n    106     def replace_training_and_call(training):\r\n    107       set_training_arg(training, training_arg_index, args, kwargs)\r\n--> 108       return wrapped_call(*args, **kwargs)\r\n    109 \r\n    110     return tf_utils.smart_cond(\r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in __call__(self, *args, **kwds)\r\n    566         xla_context.Exit()\r\n    567     else:\r\n--> 568       result = self._call(*args, **kwds)\r\n    569 \r\n    570     if tracing_count == self._get_tracing_count():\r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in _call(self, *args, **kwds)\r\n    604       # In this case we have not created variables on the first call. So we can\r\n    605       # run the first trace but we should fail if variables are created.\r\n--> 606       results = self._stateful_fn(*args, **kwds)\r\n    607       if self._created_variables:\r\n    608         raise ValueError(\"Creating variables on a non-first call to a function\"\r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\function.py in __call__(self, *args, **kwargs)\r\n   2360     \"\"\"Calls a graph function specialized to the inputs.\"\"\"\r\n   2361     with self._lock:\r\n-> 2362       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n   2363     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   2364 \r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\function.py in _maybe_define_function(self, args, kwargs)\r\n   2701 \r\n   2702       self._function_cache.missed.add(call_context_key)\r\n-> 2703       graph_function = self._create_graph_function(args, kwargs)\r\n   2704       self._function_cache.primary[cache_key] = graph_function\r\n   2705       return graph_function, args, kwargs\r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   2591             arg_names=arg_names,\r\n   2592             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 2593             capture_by_value=self._capture_by_value),\r\n   2594         self._function_attributes,\r\n   2595         # Tell the ConcreteFunction to clean up its graph once it goes out of\r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    976                                           converted_func)\r\n    977 \r\n--> 978       func_outputs = python_func(*func_args, **func_kwargs)\r\n    979 \r\n    980       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in wrapped_fn(*args, **kwds)\r\n    437         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n    438         # the function a weak reference to itself to avoid a reference cycle.\r\n--> 439         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    440     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n    441 \r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\saved_model\\function_deserialization.py in restored_function_body(*args, **kwargs)\r\n    260         .format(_pretty_format_positional(args), kwargs,\r\n    261                 len(saved_function.concrete_functions),\r\n--> 262                 \"\\n\\n\".join(signature_descriptions)))\r\n    263 \r\n    264   concrete_function_objects = []\r\n\r\nValueError: Could not find matching function to call loaded from the SavedModel. Got:\r\n  Positional arguments (3 total):\r\n    * (<tf.Tensor 'inputs:0' shape=(1, 10) dtype=int32>, <tf.Tensor 'inputs_1:0' shape=(1, 10) dtype=int32>)\r\n    * False\r\n    * None\r\n  Keyword arguments: {}\r\n\r\nExpected these arguments to match one of the following 4 option(s):\r\n\r\nOption 1:\r\n  Positional arguments (3 total):\r\n    * [TensorSpec(shape=(None, 10), dtype=tf.int32, name='input_ids'), TensorSpec(shape=(None, 10), dtype=tf.int32, name='attention_mask')]\r\n    * True\r\n    * None\r\n  Keyword arguments: {}\r\n\r\nOption 2:\r\n  Positional arguments (3 total):\r\n    * [TensorSpec(shape=(None, 10), dtype=tf.int32, name='input_ids'), TensorSpec(shape=(None, 10), dtype=tf.int32, name='attention_mask')]\r\n    * False\r\n    * None\r\n  Keyword arguments: {}\r\n\r\nOption 3:\r\n  Positional arguments (3 total):\r\n    * [TensorSpec(shape=(None, 10), dtype=tf.int32, name='inputs/0'), TensorSpec(shape=(None, 10), dtype=tf.int32, name='inputs/1')]\r\n    * True\r\n    * None\r\n  Keyword arguments: {}\r\n\r\nOption 4:\r\n  Positional arguments (3 total):\r\n    * [TensorSpec(shape=(None, 10), dtype=tf.int32, name='inputs/0'), TensorSpec(shape=(None, 10), dtype=tf.int32, name='inputs/1')]\r\n    * False\r\n    * None\r\n  Keyword arguments: {}\r\n```\r\n", "comments": ["@Peter-Devine ,\r\nWhen tried executing the given code, i faced the error in the [gist](https://colab.sandbox.google.com/gist/oanush/12ce101e13f8ca657b2eee85c78ee6b3/35888.ipynb) provided.Please provide us the complete code to replicate the issue.Thanks!", "Sorry, you need to install the `transformers` package from Huggingface (https://github.com/huggingface/transformers)\r\n\r\nI.e. in the first cell, run the following code:\r\n```\r\npip install transformers==2.3.0\r\n```\r\n\r\nAlso, why is your second cell the following?\r\n```\r\nimport sklearn\r\nimport sys\r\nfrom sklearn.pipeline import Pipeline, FeatureUnion\r\nfrom Transformers import TextTransformer\r\n```\r\nYou do not need this cell to run the code that I sent.\r\n\r\nThanks", "I got similar issue. Here is my code to reproduce the error.\r\n\r\n\r\n    input_ = tf.keras.Input(shape=[10, 30, 32])\r\n    block = tf.keras.layers.Dense(128)\r\n    rnn_block = tf.keras.layers.GRU(128, return_sequences=False)\r\n\r\n    x = block(input_)\r\n    x = tf.unstack(x, axis=1)\r\n    x_stacks = tf.TensorArray(tf.float32, size=10, element_shape=[None, 128])\r\n\r\n    tf.while_loop(lambda i, x_i: tf.less(i, 10), lambda i, x_i: [i + 1, x_i.write(i, rnn_block(x[i]))],\r\n                  loop_vars=[0, x_stacks])\r\n    x_stacks = x_stacks.stack()\r\n    x_stacks = tf.transpose(x_stacks, [1, 0, 2])\r\n\r\n    model = tf.keras.Model(inputs=[input_], outputs=[x_stacks])\r\n\r\n    model.save(\"/tmp\", save_format=\"tf\")\r\n    del model\r\n\r\n    model = tf.keras.models.load_model(\"/tmp\")\r\n\r\n**Error Traces:**\r\n\r\n2020-01-24 14:19:44.856990: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\nWARNING:tensorflow:From /home/andy/Applications/anaconda/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nIf using Keras pass *_constraint arguments to layers.\r\nTraceback (most recent call last):\r\n  File \"/home/andy/Projects/Python/milo/tf_analyst/exp.py\", line 106, in <module>\r\n    model_load()\r\n  File \"/home/andy/Projects/Python/milo/tf_analyst/exp.py\", line 98, in model_load\r\n    model = tf.keras.models.load_model(\"/tmp\")\r\n  File \"/home/andy/Applications/anaconda/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/save.py\", line 150, in load_model\r\n    return saved_model_load.load(filepath, compile)\r\n  File \"/home/andy/Applications/anaconda/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/load.py\", line 89, in load\r\n    model = tf_load.load_internal(path, loader_cls=KerasObjectLoader)\r\n  File \"/home/andy/Applications/anaconda/lib/python3.6/site-packages/tensorflow_core/python/saved_model/load.py\", line 552, in load_internal\r\n    export_dir)\r\n  File \"/home/andy/Applications/anaconda/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/load.py\", line 119, in __init__\r\n    self._finalize()\r\n  File \"/home/andy/Applications/anaconda/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/load.py\", line 157, in _finalize\r\n    created_layers={layer.name: layer for layer in node.layers})\r\n  File \"/home/andy/Applications/anaconda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py\", line 1903, in reconstruct_from_config\r\n    process_node(layer, node_data)\r\n  File \"/home/andy/Applications/anaconda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py\", line 1851, in process_node\r\n    output_tensors = layer(input_tensors, **kwargs)\r\n  File \"/home/andy/Applications/anaconda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 773, in __call__\r\n    outputs = call_fn(cast_inputs, *args, **kwargs)\r\n  File \"/home/andy/Applications/anaconda/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/utils.py\", line 59, in return_outputs_and_add_losses\r\n    outputs, losses = fn(inputs, *args, **kwargs)\r\n  File \"/home/andy/Applications/anaconda/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/andy/Applications/anaconda/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 615, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/home/andy/Applications/anaconda/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 497, in _initialize\r\n    *args, **kwds))\r\n  File \"/home/andy/Applications/anaconda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2389, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/home/andy/Applications/anaconda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2703, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/home/andy/Applications/anaconda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2593, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/home/andy/Applications/anaconda/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\", line 978, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/home/andy/Applications/anaconda/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 439, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/home/andy/Applications/anaconda/lib/python3.6/site-packages/tensorflow_core/python/saved_model/function_deserialization.py\", line 262, in restored_function_body\r\n    \"\\n\\n\".join(signature_descriptions)))\r\nValueError: Could not find matching function to call loaded from the SavedModel. Got:\r\n  Positional arguments (1 total):\r\n    * Tensor(\"inputs:0\", shape=(None, 10, 30, 128), dtype=float32)\r\n  Keyword arguments: {}\r\n\r\nExpected these arguments to match one of the following 1 option(s):\r\n\r\nOption 1:\r\n  Positional arguments (1 total):\r\n    * [TensorSpec(shape=(None, 10, 30, 128), dtype=tf.float32, name='inputs/0')]\r\n  Keyword arguments: {}\r\n", "i also got the problem. code is here.\r\n`tf.__version__\r\n'2.0.0'\r\n`\r\n`class MyModel(tf.keras.Model):`\r\n `   def __init__(self,num_classes=10):`\r\n `       super().__init__()`\r\n `   #define my layers here`\r\n`        inputs=tf.keras.Input(shape=(28,28))`\r\n`        self.x0=tf.keras.layers.Flatten()`\r\n`        self.x1=tf.keras.layers.Dense(512,activation='relu',name='d1')`\r\n`        self.x2=tf.keras.layers.Dropout(0.2)`\r\n`        self.predictions=tf.keras.layers.Dense(10,activation=tf.nn.softmax,name='d2')`\r\n`    def call(self,inputs):`\r\n`        x=self.x0(inputs)`\r\n`        x=self.x1(x)`\r\n`        x=self.x2(x)`\r\n`        return self.predictions(x)`\r\n#load data\r\n`mnist=tf.keras.datasets.mnist`\r\n`(trainx,trainy),(testx,testy)=mnist.load_data()`\r\n`epochs=10`\r\n`batchsize=32`\r\n`train_x,test_x=tf.cast(trainx/255.,tf.float32),tf.cast(testx/255.,tf.float32)`\r\n`train_y,test_y=tf.cast(trainy,tf.int64),tf.cast(testy,tf.int64)`\r\n`batch_size=32`\r\n`buffer_size=10000`\r\n`epochs=2`\r\n`train_dataset=tf.data.Dataset.from_tensor_slices((train_x,train_y)).batch(32).shuffle(10000)`\r\n`train_dataset=train_dataset.map(lambda x,y:(tf.image.flip_left_right(x),y))`\r\n`train_dataset=train_dataset.repeat()`\r\n`test_dataset = tf.data.Dataset.from_tensor_slices((test_x, test_y)).batch(batch_size).shuffle(10000)`\r\n`train_dataset=test_dataset.repeat()`\r\n`steps_per_epoch=len(train_x)//batch_size\r\n`\r\n`\r\n#train model\r\nmodel5=MyModel()\r\noptimiser=tf.keras.optimizers.Adam()\r\nmodel5.compile(optimizer=optimiser,loss='sparse_categorical_crossentropy', metrics = ['accuracy'])\r\nmodel5.fit(train_dataset,epochs=epochs,steps_per_epoch=steps_per_epoch)\r\n`\r\n\r\nwhen i save the model it works.\r\n`model5.save('./model_name.tf')`\r\n`INFO:tensorflow:Assets written to: ./model_name.tf\\assets`\r\nbut when i try to load the saved model,\r\n`from tensorflow.keras.models import load_model\r\n`\r\nmodel6=load_model('./model_name.tf')`\r\nit raise exception:\r\n`\r\n`\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-353-c27051c30434> in <module>\r\n      1 from tensorflow.keras.models import load_model\r\n----> 2 model6=load_model('./model_name.tf')\r\n\r\nD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\save.py in load_model(filepath, custom_objects, compile)\r\n    148   if isinstance(filepath, six.string_types):\r\n    149     loader_impl.parse_saved_model(filepath)\r\n--> 150     return saved_model_load.load(filepath, compile)\r\n    151 \r\n    152   raise IOError(\r\n\r\nD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\saved_model\\load.py in load(path, compile)\r\n     84   # TODO(kathywu): Add saving/loading of optimizer, compiled losses and metrics.\r\n     85   # TODO(kathywu): Add code to load from objects that contain all endpoints\r\n---> 86   model = tf_load.load_internal(path, loader_cls=KerasObjectLoader)\r\n     87 \r\n     88   if isinstance(model, RevivedModel) and compile:\r\n\r\nD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\saved_model\\load.py in load_internal(export_dir, tags, loader_cls)\r\n    539       loader = loader_cls(object_graph_proto,\r\n    540                           saved_model_proto,\r\n--> 541                           export_dir)\r\n    542       root = loader.get(0)\r\n    543     root.tensorflow_version = meta_graph_def.meta_info_def.tensorflow_version\r\n\r\nD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\saved_model\\load.py in __init__(self, *args, **kwargs)\r\n    101   def __init__(self, *args, **kwargs):\r\n    102     super(KerasObjectLoader, self).__init__(*args, **kwargs)\r\n--> 103     self._finalize()\r\n    104 \r\n    105   def _finalize(self):\r\n\r\nD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\saved_model\\load.py in _finalize(self)\r\n    130           # Since this revived object is technically a subclassed model (even if\r\n    131           # the original model is functional/sequential), inputs should be set.\r\n--> 132           node._set_inputs(inputs)\r\n    133       if isinstance(node, RevivedLayer):\r\n    134         if hasattr(node.keras_api, 'layer_regularization_losses'):\r\n\r\nD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py in _set_inputs(self, inputs, outputs, training)\r\n   2707           kwargs['training'] = training\r\n   2708       try:\r\n-> 2709         outputs = self(inputs, **kwargs)\r\n   2710       except NotImplementedError:\r\n   2711         # This Model or a submodel is dynamic and hasn't overridden\r\n\r\nD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    840                     not base_layer_utils.is_in_eager_or_tf_function()):\r\n    841                   with auto_control_deps.AutomaticControlDependencies() as acd:\r\n--> 842                     outputs = call_fn(cast_inputs, *args, **kwargs)\r\n    843                     # Wrap Tensors in `outputs` in `tf.identity` to avoid\r\n    844                     # circular dependencies.\r\n\r\nD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\saved_model\\utils.py in return_outputs_and_add_losses(*args, **kwargs)\r\n     55     inputs = args[inputs_arg_index]\r\n     56     args = args[inputs_arg_index + 1:]\r\n---> 57     outputs, losses = fn(inputs, *args, **kwargs)\r\n     58     layer.add_loss(losses, inputs)\r\n     59     return outputs\r\n\r\nD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in __call__(self, *args, **kwds)\r\n    455 \r\n    456     tracing_count = self._get_tracing_count()\r\n--> 457     result = self._call(*args, **kwds)\r\n    458     if tracing_count == self._get_tracing_count():\r\n    459       self._call_counter.called_without_tracing()\r\n\r\nD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in _call(self, *args, **kwds)\r\n    501       # This is the first call of __call__, so we have to initialize.\r\n    502       initializer_map = object_identity.ObjectIdentityDictionary()\r\n--> 503       self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n    504     finally:\r\n    505       # At this point we know that the initialization is complete (or less\r\n\r\nD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n    406     self._concrete_stateful_fn = (\r\n    407         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n--> 408             *args, **kwds))\r\n    409 \r\n    410     def invalid_creator_scope(*unused_args, **unused_kwds):\r\n\r\nD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n   1846     if self.input_signature:\r\n   1847       args, kwargs = None, None\r\n-> 1848     graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n   1849     return graph_function\r\n   1850 \r\n\r\nD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _maybe_define_function(self, args, kwargs)\r\n   2148         graph_function = self._function_cache.primary.get(cache_key, None)\r\n   2149         if graph_function is None:\r\n-> 2150           graph_function = self._create_graph_function(args, kwargs)\r\n   2151           self._function_cache.primary[cache_key] = graph_function\r\n   2152         return graph_function, args, kwargs\r\n\r\nD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   2039             arg_names=arg_names,\r\n   2040             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 2041             capture_by_value=self._capture_by_value),\r\n   2042         self._function_attributes,\r\n   2043         # Tell the ConcreteFunction to clean up its graph once it goes out of\r\n\r\nD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    913                                           converted_func)\r\n    914 \r\n--> 915       func_outputs = python_func(*func_args, **func_kwargs)\r\n    916 \r\n    917       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\nD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in wrapped_fn(*args, **kwds)\r\n    356         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n    357         # the function a weak reference to itself to avoid a reference cycle.\r\n--> 358         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    359     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n    360 \r\n\r\nD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\saved_model\\function_deserialization.py in restored_function_body(*args, **kwargs)\r\n    260         .format(_pretty_format_positional(args), kwargs,\r\n    261                 len(saved_function.concrete_functions),\r\n--> 262                 \"\\n\\n\".join(signature_descriptions)))\r\n    263 \r\n    264   concrete_function_objects = []\r\n\r\nValueError: Could not find matching function to call loaded from the SavedModel. Got:\r\n  Positional arguments (2 total):\r\n    * Tensor(\"inputs:0\", shape=(None, 28, 28), dtype=float32)\r\n    * Tensor(\"training:0\", shape=(), dtype=bool)\r\n  Keyword arguments: {}\r\n\r\nExpected these arguments to match one of the following 4 option(s):\r\n\r\nOption 1:\r\n  Positional arguments (2 total):\r\n    * TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='inputs')\r\n    * True\r\n  Keyword arguments: {}\r\n\r\nOption 2:\r\n  Positional arguments (2 total):\r\n    * TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='inputs')\r\n    * False\r\n  Keyword arguments: {}\r\n\r\nOption 3:\r\n  Positional arguments (2 total):\r\n    * TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='input_1')\r\n    * True\r\n  Keyword arguments: {}\r\n\r\nOption 4:\r\n  Positional arguments (2 total):\r\n    * TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='input_1')\r\n    * False\r\n  Keyword arguments: {}\r\n`", "Thanks for reporting this bug, this should be fixed in the latest nightly.\r\n\r\n(also, you can you triple ``` to format multiple lines of code)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35888\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35888\">No</a>\n"]}, {"number": 35887, "title": "[Windows] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED #7072", "body": "Closed/related issue\r\nhttps://github.com/tensorflow/tensorflow/issues/7072\r\n\r\nRunning tf 2.1 on windows 10 installed via pip\r\nPython 3.7.4\r\nCuda 10.1\r\ndnn 10.1 \r\nGPU: GeForce GTX 1650 \r\n\r\nWhen I run the tf canonical sample code\r\n```\r\ntf.debugging.set_log_device_placement(True)\r\na = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\r\nb = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\r\nc = tf.matmul(a, b)\r\nprint(c)``\r\n```\r\nI get no issues\r\n```\r\n2020-01-14 21:16:24.262673: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-01-14 21:16:26.104356: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-01-14 21:16:26.129817: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 1650 computeCapability: 7.5\r\ncoreClock: 1.245GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.34GiB/s\r\n2020-01-14 21:16:26.136690: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-01-14 21:16:26.149405: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-01-14 21:16:26.155714: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-01-14 21:16:26.160934: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-01-14 21:16:26.167186: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-01-14 21:16:26.175237: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-01-14 21:16:26.187187: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-01-14 21:16:26.189723: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-01-14 21:16:26.191974: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2020-01-14 21:16:26.200691: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 1650 computeCapability: 7.5\r\ncoreClock: 1.245GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.34GiB/s\r\n2020-01-14 21:16:26.206054: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-01-14 21:16:26.214181: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-01-14 21:16:26.216951: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-01-14 21:16:26.219549: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-01-14 21:16:26.227390: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-01-14 21:16:26.230551: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-01-14 21:16:26.233257: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-01-14 21:16:26.241289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-01-14 21:16:26.719313: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-01-14 21:16:26.722034: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0\r\n2020-01-14 21:16:26.724147: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N\r\n2020-01-14 21:16:26.726376: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2915 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2020-01-14 21:16:26.738893: I tensorflow/core/common_runtime/eager/execute.cc:573] Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2020-01-14 21:16:26.747089: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\ntf.Tensor(\r\n[[22. 28.]\r\n [49. 64.]], shape=(2, 2), dtype=float32)\r\n```\r\nHowever, when I run \r\n```import tensorflow as tf\r\n\r\nwith tf.device('/gpu:0'):\r\n    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\r\n    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\r\n    c = tf.matmul(a, b)\r\n\r\n\r\nwith tf.Session() as sess:\r\n    print (sess.run(c))\r\n```\r\nI get \r\n```\r\n2020-01-14 21:14:53.513065: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-01-14 21:14:55.360139: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-01-14 21:14:55.382691: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 1650 computeCapability: 7.5\r\ncoreClock: 1.245GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.34GiB/s\r\n2020-01-14 21:14:55.388200: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-01-14 21:14:55.400965: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-01-14 21:14:55.406969: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-01-14 21:14:55.412851: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-01-14 21:14:55.419095: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-01-14 21:14:55.426957: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-01-14 21:14:55.438634: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-01-14 21:14:55.441198: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-01-14 21:14:55.443479: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2020-01-14 21:14:55.452456: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 1650 computeCapability: 7.5\r\ncoreClock: 1.245GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.34GiB/s\r\n2020-01-14 21:14:55.457969: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-01-14 21:14:55.466455: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-01-14 21:14:55.469159: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-01-14 21:14:55.471860: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-01-14 21:14:55.480462: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-01-14 21:14:55.482601: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-01-14 21:14:55.484608: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-01-14 21:14:55.486769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-01-14 21:14:55.985284: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-01-14 21:14:55.987459: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0\r\n2020-01-14 21:14:55.989183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N\r\n2020-01-14 21:14:55.991385: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2915 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2020-01-14 21:14:56.213441: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-01-14 21:14:56.468795: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2020-01-14 21:14:56.471524: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2020-01-14 21:14:56.474410: W tensorflow/stream_executor/stream.cc:2041] attempting to perform BLAS operation using StreamExecutor without BLAS support\r\nTraceback (most recent call last):\r\n  File \".\\gpu.py\", line 6, in <module>\r\n    c = tf.linalg.matmul(a, b)\r\n  File \"C:\\Users\\shan\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\util\\dispatch.py\", line 180, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"C:\\Users\\shan\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_ops.py\", line 2798, in matmul\r\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\r\n  File \"C:\\Users\\shan\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_math_ops.py\", line 5616, in mat_mul\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"C:\\Users\\shan\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 6606, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(2, 3), b.shape=(3, 2), m=2, n=2, k=3 [Op:MatMul] name: MatMul/\r\n```\r\n\r\n", "comments": ["I should have read up on the new tf 2 syntax prior to post.\r\nThe following works \r\n```\r\nimport tensorflow as tf\r\n\r\n@tf.function\r\ndef mm():\r\n    with tf.device('/device:gpu:0'):\r\n        a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\r\n        b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\r\n        return tf.matmul(a, b)\r\nout = mm()\r\nprint(out)\r\n```\r\n>>\r\ntf.Tensor(\r\n[[22. 28.]\r\n [49. 64.]], shape=(2, 2), dtype=float32)", "With tensorflow>=2.0\r\n\r\nimport tensorflow as tf\r\nconfig = tf.compat.v1.ConfigProto(gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8)\r\n#device_count = {'GPU': 1}\r\n)\r\nconfig.gpu_options.allow_growth = True\r\nsession = tf.compat.v1.Session(config=config)\r\ntf.compat.v1.keras.backend.set_session(session)", "I still have the same problem, with ```CUBLAS_STATUS_ALLOC_FAILED```, in version ```2.5.0-dev20201121``` when calling ```fit``` on a custom-created VGG16 model with input size of 50x256x1 and output 128 as well as for... just **one data example for train and validation**!\r\n\r\nTried using the above code given by @SirJQ, doesn't help, nor any other comments from https://stackoverflow.com/questions/41117740/tensorflow-crashes-with-cublas-status-alloc-failed so far. \r\nConsole code:\r\n```2020-11-26 12:52:03.427034: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 0 with properties: \r\npciBusID: 0000:65:00.0 name: GeForce RTX 2060 SUPER computeCapability: 7.5\r\ncoreClock: 1.68GHz coreCount: 34 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2020-11-26 12:52:03.427489: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-11-26 12:52:03.427714: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-11-26 12:52:03.427941: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2020-11-26 12:52:03.428165: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2020-11-26 12:52:03.428382: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2020-11-26 12:52:03.428598: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2020-11-26 12:52:03.428821: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2020-11-26 12:52:03.429044: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2020-11-26 12:52:03.429297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1869] Adding visible gpu devices: 0\r\n2020-11-26 12:52:03.429528: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-11-26 12:52:03.429752: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1273]      0 \r\n2020-11-26 12:52:03.429893: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1286] 0:   N \r\n2020-11-26 12:52:03.430114: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1413] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6230 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2060 SUPER, pci bus id: 0000:65:00.0, compute capability: 7.5)\r\n2020-11-26 12:52:03.430559: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\nBackend Qt5Agg is interactive backend. Turning interactive mode on.\r\n2020-11-26 12:52:05.186170: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:127] None of the MLIR optimization passes are enabled (registered 2)\r\n2020-11-26 12:52:08.096615: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2020-11-26 12:52:08.695362: I tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Loaded cuDNN version 8005\r\n2020-11-26 12:52:09.280321: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 0\r\n2020-11-26 12:52:09.320120: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 0\r\n2020-11-26 12:52:09.351416: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-11-26 12:52:09.950764: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2020-11-26 12:52:09.980146: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2020-11-26 12:52:10.001555: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2020-11-26 12:52:10.028402: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2020-11-26 12:52:10.051630: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2020-11-26 12:52:10.081788: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2020-11-26 12:52:10.111923: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2020-11-26 12:52:10.156970: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2020-11-26 12:52:10.207538: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2020-11-26 12:52:10.209653: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2020-11-26 12:52:10.209861: W tensorflow/stream_executor/stream.cc:1455] attempting to perform BLAS operation using StreamExecutor without BLAS support\r\nTraceback (most recent call last):\r\n  File \"yyy\\Anaconda3\\envs\\tf_nightly\\lib\\contextlib.py\", line 131, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"yyy\\Anaconda3\\envs\\tf_nightly\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2825, in variable_creator_scope\r\n    yield\r\n  File \"yyy\\Anaconda3\\envs\\tf_nightly\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1132, in fit\r\n    tmp_logs = self.train_function(iterator)\r\n  File \"yyy\\Anaconda3\\envs\\tf_nightly\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 784, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"yyy\\Anaconda3\\envs\\tf_nightly\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 844, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"yyy\\Anaconda3\\envs\\tf_nightly\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2971, in __call__\r\n    return graph_function._call_flat(\r\n  File \"yyy\\Anaconda3\\envs\\tf_nightly\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1947, in _call_flat\r\n    return self._build_call_outputs(self._inference_function.call(\r\n  File \"yyy\\Anaconda3\\envs\\tf_nightly\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 556, in call\r\n    outputs = execute.execute(\r\n  File \"yyy\\Anaconda3\\envs\\tf_nightly\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 59, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.InternalError:  Blas xGEMM launch failed : a.shape=[1,1,4096], b.shape=[1,4096,4096], m=1, n=4096, k=4096\r\n\t [[node kjg4wg/fc1/MatMul (defined at FILENAME.py:52) ]] [Op:__inference_train_function_2067]\r\nFunction call stack:\r\ntrain_function\r\npython-BaseException```\r\n\r\nAny suggestions?", "> With tensorflow>=2.0\r\n> \r\n> import tensorflow as tf\r\n> config = tf.compat.v1.ConfigProto(gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8)\r\n> #device_count = {'GPU': 1}\r\n> )\r\n> config.gpu_options.allow_growth = True\r\n> session = tf.compat.v1.Session(config=config)\r\n> tf.compat.v1.keras.backend.set_session(session)\r\n\r\nit works. thanks."]}, {"number": 35886, "title": "add trainable property to AggregatingVariable", "body": "#35442 #35017\r\nWhen i tried to train a keras model using ParameterServerStrategy, i found the error message described in above issue. It says that 'trainable' property is not implemented.\r\nAnd i found AggregatingVariable which is a wrapper class used in ParameterServerStrategy doesn't override 'trainable' property. It should be fixed by overriding 'trainable' property.\r\nSo i simply fixed it by using unwrapped value's 'trainable' property.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35886) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35886) for more info**.\n\n<!-- ok -->"]}, {"number": 35885, "title": "Problem converting model to tensorflow lite (LSTM model)", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n**I have used google colab**\r\n\r\n- TensorFlow version (or github SHA if from source):\r\nTensorFlow 2.x selected.\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nConverterError                            Traceback (most recent call last)\r\n<ipython-input-73-6a5821dcb9ed> in <module>()\r\n      1 converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n----> 2 tflite_model = converter.convert()\r\n      3 \r\n      4 # Save the model to disk\r\n      5 #open(\"/content/gdrive/My Drive/detect_car_model.tflite\", \"wb\").write(tflite_model)\r\n\r\n2 frames\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    201       stdout = _try_convert_to_unicode(stdout)\r\n    202       stderr = _try_convert_to_unicode(stderr)\r\n--> 203       raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\n    204   finally:\r\n    205     # Must manually cleanup files.\r\n\r\nConverterError: See console for info.\r\n2020-01-15 04:01:36.499502: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\r\n2020-01-15 04:01:36.510161: E tensorflow/lite/toco/toco_tooling.cc:498] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONCATENATION, FULLY_CONNECTED, RESHAPE, REVERSE_V2, SOFTMAX, STRIDED_SLICE. Here is a list of operators for which you will need custom implementations: TensorListFromTensor, TensorListReserve, TensorListStack, While.\r\nTraceback (most recent call last):\r\n  File \"/tensorflow-2.1.0/python3.6/bin/toco_from_protos\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/tensorflow-2.1.0/python3.6/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 93, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/tensorflow-2.1.0/python3.6/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/tensorflow-2.1.0/python3.6/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/tensorflow-2.1.0/python3.6/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/tensorflow-2.1.0/python3.6/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 56, in execute\r\n    enable_mlir_converter)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONCATENATION, FULLY_CONNECTED, RESHAPE, REVERSE_V2, SOFTMAX, STRIDED_SLICE. Here is a list of operators for which you will need custom implementations: TensorListFromTensor, TensorListReserve, TensorListStack, While.\r\n\r\n\r\n```\r\n\r\n\r\n** info / logs**\r\n\r\nI have tried to convert the following model:\r\n\r\n```\r\n\r\nmodel = keras.Sequential()\r\nmodel.add(\r\n    keras.layers.Bidirectional(\r\n      keras.layers.LSTM(\r\n          units=128,\r\n          input_shape=[x_train.shape[1], x_train.shape[2]]\r\n      )\r\n    )\r\n)\r\nmodel.add(keras.layers.Dropout(rate=0.5))\r\nmodel.add(keras.layers.Dense(units=128, activation='relu'))\r\nmodel.add(keras.layers.Dense(y_train.shape[1], activation='softmax'))\r\n\r\nmodel.compile(\r\n  loss='categorical_crossentropy',\r\n  optimizer='adam',\r\n  metrics=['acc']\r\n)\r\n```", "comments": ["Can you try using the following flag `converter.experimental_new_converter = True` when converting the model?", "@suha-glal Did you follow @gargn's suggestion in converting the model? Please close the issue if it was resolved. If not, please share a standalone code to reproduce the issue. Thanks!", "Thanks for your help.\r\nI will try it soon and tell you the outcomes.\r\n\r\nI will close the issue for now."]}]