[{"number": 41368, "title": "twice embedding_lookup\u2018 result is the same as once embedding_lookup,but gradients not", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI have to use twice embedding_lookup to do the same thing as once embedding_lookup,but their gradients dont act the same\r\n\r\n`\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\na = tf.convert_to_tensor(np.random.random([2048,32]))\r\nb = tf.convert_to_tensor([1,2,2,3,4,4,5,1,2,3,4,5])\r\nsess = tf.Session()\r\nb_,idx = tf.unique(b)\r\nprint(sess.run(b_))\r\nw = tf.nn.embedding_lookup(a,b_)\r\nprint(sess.run(w))\r\n\r\nperm = tf.argsort(b)\r\naux = tf.gather_nd(b,tf.reshape(perm,[-1,1]))\r\nmask = tf.not_equal(aux[1:],aux[:-1])\r\npos = tf.boolean_mask(perm[1:],mask)\r\npos = tf.concat([[0],pos],axis=0)\r\n\r\nprint(sess.run(pos))\r\n\r\nw_2 = tf.nn.embedding_lookup(a,b)\r\nw_1 = tf.nn.embedding_lookup(w_2,pos)\r\n\r\nprint(sess.run(w_1))\r\n\r\n`\r\n\r\n![E5@B5_ XNA U83Q1BJ1 0XO](https://user-images.githubusercontent.com/22000530/87393434-c49a1580-c5e0-11ea-8cfa-9f755ad9818d.png)\r\n\r\n**Describe the expected behavior**\r\n\r\nTheir performance is the same,but when I use them in my models(all is the same except one is once embedding_lookup,one is twice embedding_lookup),they don't perform the same.\r\n\r\nuse once embedding_lookup:\r\n![MMZZQFLBEKQE8MJHKK8Y5FO](https://user-images.githubusercontent.com/22000530/87393621-0cb93800-c5e1-11ea-8da5-27f363fb7c0c.png)\r\n\r\nuse twice embedding_lookup:\r\n![X`K9D(FEM_HBM%17UO~5MRS](https://user-images.githubusercontent.com/22000530/87393625-0dea6500-c5e1-11ea-9f5f-28d36a9111fe.png)\r\nWe could see before training they perform the same,but after training, they perform different,I assume because their gradients are different,but I don't know how to fix them,by the way I have to use twice embedding_lookup\r\n\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@akafen \r\n\r\nRequest you to share colab link or simple standalone code with proper indentation to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "@ravikyram \r\n\r\nA simple standalone code is below :\r\n    import tensorflow as tf\r\n\r\n    from keras.layers.embeddings import Embedding\r\n    from lazy import lazy\r\n    import numpy as np\r\n    from keras.layers import Input\r\n    from keras.layers.core import Dense, Lambda,Reshape\r\n    from keras.models import Model\r\n    from keras.layers import Layer\r\n    from keras import backend as K\r\n    from keras import activations\r\n\r\n\r\n    class TiedEmbeddingsTransposed(Layer):\r\n    \r\n        def __init__(self, tied_to=None,\r\n                     activation=None,\r\n                     **kwargs):\r\n            super(TiedEmbeddingsTransposed, self).__init__(**kwargs)\r\n            self.tied_to = tied_to\r\n            if activation is not None:\r\n                self.activation = activations.get(activation)\r\n            else:\r\n                self.activation = None\r\n    \r\n        def build(self, input_shape):\r\n            self.transposed_weights = K.transpose(self.tied_to.weights[0])\r\n            #self.transposed_weights = K.l2_normalize(self.transposed_weights,axis=1)\r\n            super(TiedEmbeddingsTransposed, self).build(input_shape)\r\n    \r\n        def compute_mask(self, inputs, mask=None):\r\n            return mask\r\n    \r\n        def compute_output_shape(self, input_shape):\r\n            return input_shape[0], K.int_shape(self.tied_to.weights[0])[0]\r\n    \r\n        def call(self, inputs, mask=None):\r\n            #inputs = K.l2_normalize(inputs,axis=1)\r\n            output = K.dot(inputs, self.transposed_weights)\r\n            if self.activation is not None:\r\n                output = self.activation(output)\r\n            return output\r\n    \r\n    class Once_embedding_lookup_Layer(Layer):\r\n        def __init__(self,input,**kwargs):\r\n    \r\n            self.inputs = input\r\n            super(Once_embedding_lookup_Layer, self).__init__(**kwargs)\r\n    \r\n        def build(self, input_shape):\r\n            super(Once_embedding_lookup_Layer, self).build(input_shape)\r\n    \r\n    \r\n        def compute_loss(self, ys_true,video_embedding):\r\n            ys_true = tf.cast(ys_true,'int64')\r\n    \r\n            ys_true = tf.reshape(ys_true,[-1])\r\n            ys_true,idx = tf.unique(ys_true)\r\n    \r\n    \r\n            all_w = tf.nn.embedding_lookup(video_embedding,ys_true)\r\n    \r\n            logits = tf.matmul(self.inputs,all_w,transpose_b=True)\r\n    \r\n            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=idx, logits=logits)\r\n    \r\n            return loss\r\n    \r\n        def call(self, inputs, **kwargs):\r\n            ys_true = inputs[0]\r\n            video_embedding = inputs[1]\r\n            loss = self.compute_loss(ys_true,video_embedding)\r\n            self.add_loss(loss)\r\n            return loss\r\n    \r\n    class Twice_embedding_lookup_Layer(Layer):\r\n        def __init__(self,input,**kwargs):\r\n    \r\n            self.inputs = input\r\n            super(Twice_embedding_lookup_Layer, self).__init__(**kwargs)\r\n    \r\n        def build(self, input_shape):\r\n            super(Twice_embedding_lookup_Layer, self).build(input_shape)\r\n    \r\n    \r\n        def compute_loss(self, ys_true,video_embedding):\r\n            ys_true = tf.cast(ys_true,'int64')\r\n    \r\n            ys_true = tf.reshape(ys_true,[-1])\r\n    \r\n            perm = tf.argsort(ys_true)\r\n            aux = tf.gather_nd(ys_true, tf.reshape(perm, [-1, 1]))\r\n            mask = tf.not_equal(aux[1:], aux[:-1])\r\n            pos = tf.boolean_mask(perm[1:], mask)\r\n            pos = tf.concat([[0], pos], axis=0)\r\n    \r\n            all_w = tf.nn.embedding_lookup(video_embedding,ys_true)\r\n            all_w = tf.nn.embedding_lookup(all_w,pos)\r\n    \r\n            logits = tf.matmul(self.inputs,all_w,transpose_b=True)\r\n    \r\n            ys_true,idx = tf.unique(ys_true)\r\n    \r\n            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=idx, logits=logits)\r\n    \r\n            return loss\r\n    \r\n        def call(self, inputs, **kwargs):\r\n            ys_true = inputs[0]\r\n            video_embedding = inputs[1]\r\n            loss = self.compute_loss(ys_true,video_embedding)\r\n            self.add_loss(loss)\r\n            return loss\r\n    \r\n    \r\n    class Once_embedding_lookup_model(object):\r\n        def __init__(self,video_size,embedding_size):\r\n            self.video_size = video_size\r\n            self.embedding_size = embedding_size\r\n    \r\n        @lazy\r\n        def _video_embeddings(self):\r\n            return Embedding(self.video_size, self.embedding_size, weights=[np.ones([self.video_size,self.embedding_size])], mask_zero=False,\r\n                             name='video-embd')\r\n    \r\n        @lazy\r\n        def _user_state_model(self):\r\n            user_feature = Input(shape=(1,),name='user_feature')\r\n            user_state_features = Dense(256, activation='relu', name='user-state-dense-256')(\r\n                user_feature)\r\n            user_state_features = Dense(128, activation='relu', name='user-state-dense-128')(user_state_features)\r\n            user_state_features = Dense(self.embedding_size, activation='relu')(user_state_features)\r\n            return Model(inputs=user_feature, outputs=user_state_features)\r\n    \r\n        @lazy\r\n        def _video_embedding_model(self):\r\n            target_id_input = Input(shape=(1,),name='target_id_input')\r\n            target_id_embedding = self._video_embeddings(target_id_input)\r\n            target_id_embedding = Reshape((self.embedding_size,))(target_id_embedding)\r\n    \r\n            target_id_embedding = Dense(128,activation='relu',name='video_embedding_dense_128')(target_id_embedding)\r\n            target_id_embedding = Dense(self.embedding_size,activation='relu',name='video_embedding')(target_id_embedding)\r\n    \r\n            model = Model(inputs=target_id_input,outputs=target_id_embedding)\r\n            model.summary()\r\n            return model\r\n    \r\n        @lazy\r\n        def once_embedding_lookup_layer(self):\r\n            y_true = Input(shape=(1,),name='y_true')\r\n            inputs = self._user_state_model.outputs[0]\r\n            video_embedding = self._video_embedding_model.outputs[0]\r\n            loss = Once_embedding_lookup_Layer(input=inputs,name='once_embedding_lookup_loss')([y_true,video_embedding])\r\n    \r\n            predict_softmax = TiedEmbeddingsTransposed(tied_to=self._video_embeddings, activation='softmax',name='predict_softmax_once')(\r\n                self._user_state_model.outputs[0])\r\n    \r\n            model = Model(inputs=self._user_state_model.inputs + self._video_embedding_model.inputs + [y_true],\r\n                          outputs=[predict_softmax, loss],\r\n                          name='once_embedding_lookup_model')\r\n            model.summary()\r\n            return model\r\n    \r\n    class Twice_embedding_lookup_model(object):\r\n        def __init__(self,video_size,embedding_size):\r\n            self.video_size = video_size\r\n            self.embedding_size = embedding_size\r\n    \r\n        @lazy\r\n        def _video_embeddings(self):\r\n            return Embedding(self.video_size, self.embedding_size, weights=[np.ones([self.video_size,self.embedding_size])], mask_zero=False,\r\n                             name='video-embd-twice')\r\n    \r\n        @lazy\r\n        def _user_state_model(self):\r\n            user_feature = Input(shape=(1,),name='user_feature_twice')\r\n            user_state_features = Dense(256, activation='relu', name='user-state-dense-256_twice')(\r\n                user_feature)\r\n            user_state_features = Dense(128, activation='relu', name='user-state-dense-128_twice')(user_state_features)\r\n            user_state_features = Dense(self.embedding_size, activation='relu')(user_state_features)\r\n            return Model(inputs=user_feature, outputs=user_state_features)\r\n    \r\n        @lazy\r\n        def _video_embedding_model(self):\r\n            target_id_input = Input(shape=(1,),name='target_id_input_twice')\r\n            target_id_embedding = self._video_embeddings(target_id_input)\r\n            target_id_embedding = Reshape((self.embedding_size,))(target_id_embedding)\r\n    \r\n            target_id_embedding = Dense(128,activation='relu',name='video_embedding_dense_128_twice')(target_id_embedding)\r\n            target_id_embedding = Dense(self.embedding_size,activation='relu',name='video_embedding_twice')(target_id_embedding)\r\n    \r\n            model = Model(inputs=target_id_input,outputs=target_id_embedding)\r\n            model.summary()\r\n            return model\r\n    \r\n        @lazy\r\n        def twice_embedding_lookup_layer(self):\r\n            y_true = Input(shape=(1,),name='y_true')\r\n            inputs = self._user_state_model.outputs[0]\r\n            video_embedding = self._video_embedding_model.outputs[0]\r\n            loss = Twice_embedding_lookup_Layer(input=inputs,name='twice_embedding_lookup_loss')([y_true,video_embedding])\r\n    \r\n            predict_softmax = TiedEmbeddingsTransposed(tied_to=self._video_embeddings, activation='softmax',name='predict_softmax_twice')(\r\n                self._user_state_model.outputs[0])\r\n    \r\n            model = Model(inputs=self._user_state_model.inputs + self._video_embedding_model.inputs + [y_true],\r\n                          outputs=[predict_softmax, loss],\r\n                          name='twice_embedding_lookup_model')\r\n            model.summary()\r\n            return model\r\n    \r\n    def mean_pred(y_true,y_pred):\r\n        return K.mean(y_pred)\r\n    \r\n    from keras.metrics import sparse_top_k_categorical_accuracy\r\n    def top10(y_true,y_pred):\r\n        return sparse_top_k_categorical_accuracy(y_true, y_pred, 10)\r\n    \r\n    import os\r\n    os.environ['CUDA_VISIBLE_DEVICES'] = '1'\r\n    \r\n    if __name__ == '__main__':\r\n        once_model = Once_embedding_lookup_model(2048,32).once_embedding_lookup_layer\r\n        twice_model = Twice_embedding_lookup_model(2048,32).twice_embedding_lookup_layer\r\n        once_model.compile(\r\n            loss={\r\n                'predict_softmax_once': 'sparse_categorical_crossentropy',\r\n                \"once_embedding_lookup_loss\": lambda y_true, y_pred: y_pred,\r\n            },\r\n            loss_weights={\r\n                'predict_softmax_once': 0,\r\n                \"once_embedding_lookup_loss\": 1,\r\n            },\r\n            metrics={\r\n                \"predict_softmax_once\": top10,\r\n                'once_embedding_lookup_loss': mean_pred,\r\n            },\r\n            optimizer='adam')\r\n        twice_model.compile(\r\n            loss={\r\n                'predict_softmax_twice': 'sparse_categorical_crossentropy',\r\n                \"twice_embedding_lookup_loss\": lambda y_true, y_pred: y_pred,\r\n            },\r\n            loss_weights={\r\n                'predict_softmax_twice': 0,\r\n                \"twice_embedding_lookup_loss\": 1,\r\n            },\r\n            metrics={\r\n                \"predict_softmax_twice\": top10,\r\n                'twice_embedding_lookup_loss': mean_pred,\r\n            },\r\n            optimizer='adam')\r\n    \r\n        x = []\r\n        for i in range(128):\r\n            x.append(i)\r\n        x = np.array(x).astype('int64')\r\n    \r\n        y = []\r\n        for i in range(128,256):\r\n            y.append(i)\r\n        y = np.array(y).astype('int64')\r\n        inputs = []\r\n        inputs.append(x)\r\n        inputs.append(y)\r\n    \r\n        labels = []\r\n        for i in range(2):\r\n            for j in range(64):\r\n                labels.append(j)\r\n        labels = np.array(labels).astype('int64')\r\n        inputs.append(labels)\r\n    \r\n        predict_softmax_once, loss = once_model.predict(inputs, batch_size=128)\r\n        print('once',predict_softmax_once)\r\n    \r\n        predict_softmax_twice, loss = twice_model.predict(inputs, batch_size=128)\r\n        print('twice',predict_softmax_twice)\r\n    \r\n        for i in range(100):\r\n            loss_metrics = once_model.train_on_batch(inputs,[labels,labels])\r\n            loss_metrics = twice_model.train_on_batch(inputs,[labels,labels])\r\n    \r\n        predict_softmax_once, loss = once_model.predict(inputs, batch_size=128)\r\n        print('once',predict_softmax_once)\r\n    \r\n        predict_softmax_twice, loss = twice_model.predict(inputs, batch_size=128)\r\n        print('twice',predict_softmax_twice)", "@akafen \r\n\r\nI have tried in colab with TF 2.2.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/80a4bde009d50f43284f718457194a9b/untitled128.ipynb).You also see the same behavior?Thanks!", "@ravikyram \r\nYes, after some steps training, once embedding_lookup perform different to twice embedding_lookup,but they choose the same index as I said before,so I think their gradients are different", "Was able to replicate the isuue in TF 2.6.0-dev20210530,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/42307dda070f3933c3cdbc53c72bfee3/untitled135.ipynb#scrollTo=h0W4-1fPTNQ0)..Thanks !"]}, {"number": 41361, "title": "Keras model.compile(..., metrics=[\"accuracy\"]) no longer introspects loss function in TF 2.2", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS, Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): pip/binary\r\n- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b, 2.2.0\r\n- Python version: 3.7.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nThe documentation of [`tf.keras.Model.compile`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile) includes the following for the `metrics` parameter:\r\n\r\n> When you pass the strings 'accuracy' or 'acc', we convert this to one of tf.keras.metrics.BinaryAccuracy, tf.keras.metrics.CategoricalAccuracy, tf.keras.metrics.SparseCategoricalAccuracy based on the loss function used and the model output shape. We do a similar conversion for the strings 'crossentropy' and 'ce' as well. \r\n\r\nThe code in question only looks at the model shape, and ignores the loss function:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/keras/engine/compile_utils.py#L447-L453\r\n\r\nThis means that using `acc`/`accuracy` for a model doing binary classification with last dimension > 1 (e.g. binary predictions about multiple independent values for each batch element) will incorrectly use categorical accuracy, not binary accuracy. (That is, the `compile` invocation is equivalent to passing `metrics=[\"categorical_accuracy\"]`.)\r\n\r\n**Describe the expected behavior**\r\n\r\nThe behaviour stated by the documentation is to look at the loss function in addition to the output shape. That is, if the loss function is binary cross-entropy, `metrics=[\"accuracy\"]` should be equivalent to `metrics=[\"binary_accuracy\"]`. \r\n\r\nThis is behaviour of TF < 2.2, such as TF 2.1.1:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/3ffdb91f122f556a74a6e1efd2469bfe1063cb5c/tensorflow/python/keras/engine/training_utils.py#L1114-L1121\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```python\r\n#%pip install tensorflow==2.2.0\r\n#%pip install tensorflow==2.1.1\r\nimport tensorflow as tf\r\n\r\ninp = tf.keras.Input(3)\r\nmodel = tf.keras.Model(inp, inp)\r\n\r\nmodel.compile(loss=\"bce\", metrics=[\"acc\"])\r\nmodel.evaluate(tf.constant([[0.1, 0.6, 0.9]]), tf.constant([[0, 1, 1]]))\r\n\r\nif tf.version.VERSION == \"2.2.0\":\r\n    print(model.compiled_metrics.metrics[0]._fn.__name__)\r\nelse:\r\n    print(model._per_output_metrics[0][\"acc\"]._fn.__name__)\r\n```\r\n\r\nOutput\r\n\r\n- TF 2.2.0: `categorical_accuracy`\r\n- TF 2.1.1: `binary_accuracy`\r\n\r\nNotebook: https://gist.github.com/huonw/4a95b73e3d8a1c48a8b5fc5297d30772\r\nColab: https://colab.research.google.com/gist/huonw/4a95b73e3d8a1c48a8b5fc5297d30772\r\n\r\n\r\n**Other info / logs** N/A", "comments": ["I have tried in colab with TF 2.2 and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/cb15be043d402e051ffd25615f7a9291/untitled144.ipynb).Thanks!", "/cc @nikitamaia This bug is is still valid in TF nightly but the connected pull request was closed without any feedback. What we want to do here?", "Was able to replicate the issue in TF 2.7.0,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/90978bade8bf4a1016bfe3e82d0ebe70/untitled134.ipynb)..Thanks !", "> Was able to replicate the issue in TF 2.6.0-dev20210530,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/90978bade8bf4a1016bfe3e82d0ebe70/untitled134.ipynb)..Thanks !\n\nThere was a linked PR that was closed without any feedback.", "Can you check https://github.com/keras-team/keras/pull/16114"]}, {"number": 41360, "title": "Using HIP instead of CUDA (work on all graphics card)", "body": "Since half of the developers can use an AMD card or an old NVIDIA card, if that append, you cannot use the GPU with CUDA.\r\n\r\n**System information**\r\n- TensorFlow version: ALL\r\n- Are you willing to contribute it (Yes/No): YES\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nHIP is a C++ Runtime API and Kernel Language that allows developers to create portable applications for AMD and NVIDIA GPUs from single source code.\r\n\r\nKey features include:\r\n\r\nHIP is very thin and has little or no performance impact over coding directly in CUDA or hcc \"HC\" mode.\r\nHIP allows coding in a single-source C++ programming language including features such as templates, C++11 lambdas, classes, namespaces, and more.\r\nHIP allows developers to use the \"best\" development environment and tools on each target platform.\r\n\r\nThe HIPIFY tools automatically convert source from CUDA to HIP.\r\nDevelopers can specialize for the platform (CUDA or hcc) to tune for performance or handle tricky cases\r\nNew projects can be developed directly in the portable HIP C++ language and can run on either NVIDIA or AMD platforms. \r\n\r\nAdditionally, HIP provides porting tools which make it easy to port existing CUDA codes to the HIP layer, with no loss of performance as compared to the original CUDA application. HIP is not intended to be a drop-in replacement for CUDA, and developers should expect to do some manual coding and performance tuning work to complete the port.\r\n\r\n[https://github.com/ROCm-Developer-Tools/HIP]\r\n\r\n**Will this change the current api? How?** NO\r\n\r\n**Who will benefit with this feature?** ALL\r\n\r\n", "comments": ["This is great!", "You can already compile with HIP, see https://github.com/rocm-arch/tensorflow-rocm/blob/master/PKGBUILD as a reference installation guide.\r\n\r\nI believe this issue can be closed,"]}, {"number": 41325, "title": "Performance issue when calling Keras conv2D / gen_nn_ops.conv2d with different sizes", "body": "\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**:\r\nyes\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 18.04\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\nNA\r\n    happens on a mobile device**:\r\nNA\r\n-   **TensorFlow installed from (source or binary)**:  binary wheel via PyPI\r\n-   **TensorFlow version (use command below)**:\r\nv2.1.0-rc2-17-ge5bf8de 2.1.0;\r\nv2.2.0-0-g2b96f3662b 2.2.0\r\n-   **Python version**:\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:\r\nCUDA 10.0\r\n-   **GPU model and memory**:\r\nV100 32 GB\r\n\r\n\r\n### Describe the problem\r\nI'm suspecting time performance issue when running a model with different input sizes.\r\nthe first time of running a model produce tracing, but I created a model with tf.function with tensor spec and yet when changing the input size (even by 1) the performance decreases a lot.\r\nthe model is build with keras layer conv2d.\r\nwhen running the low level tensorflow API gen_nn_ops.conv2d received the same decrease of performance.\r\n\r\n**Describe the expected behavior**\r\nthe timing performance shouldn't decrease after the 1st time of calling the model with different size.\r\n\r\n\r\nKeras code:\r\n\r\n````\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom time import perf_counter\r\nfrom tensorflow.keras.layers import Conv2D\r\n\r\nfilters = 8\r\nkernel_size = (3, 3)\r\ninp_shape = (15, 15, filters)\r\n\r\nlayers = [Conv2D(filters=filters, kernel_size=kernel_size, input_shape=inp_shape)]\r\nlayers += [Conv2D(filters=filters, kernel_size=kernel_size) for _ in range(4)]\r\nmodel = tf.keras.Sequential(layers=layers)\r\n\r\n@tf.function(input_signature=[tf.TensorSpec([None, 15, 15, 8])])\r\ndef tf_func(inp):\r\n    print('graph tracing')\r\n    return model(inp)\r\n\r\ndef prepare_inp(n):\r\n    return np.random.rand(n, *inp_shape).astype(\"float32\")\r\n\r\nout = tf_func(prepare_inp(1)).numpy() # first call - very slow\r\nN = 30000\r\nfor n in range(N, N+6):\r\n    inp = prepare_inp(n)\r\n    elapsed = []\r\n    for _ in range(5):\r\n        _start = perf_counter()\r\n        out = tf_func(inp).numpy()\r\n        elapsed.append(perf_counter() - _start)\r\n    print(f'[{n:,} samples] running time (msec): ' + ' '.join(f'{1000*e:6.1f}' for e in elapsed))\r\n\r\n\"\"\"\r\nOutput:\r\ngraph tracing\r\n[30,000 samples] running time (msec):  466.6   77.4   63.0   63.5   77.9\r\n[30,001 samples] running time (msec):  389.0   64.0   49.6   38.8   63.4\r\n\r\nExpected something like:\r\ngraph tracing\r\n[30,000 samples] running time (msec):  466.6   77.4   63.0   63.5   77.9\r\n[30,001 samples] running time (msec):   60.0   64.0   49.6   38.8   63.4\r\n\"\"\"\r\n````\r\n\r\nlow level API code:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom time import perf_counter\r\n\r\nfilters = 8\r\nkernel_size = (3, 3)\r\norder = 'NHWC' # or 'NCHW'\r\nimg_shape = (15, 15)\r\n\r\ninp_shape = img_shape + (filters,) if order == 'NHWC' else (filters,) + img_shape\r\nkernel = tf.constant(np.random.rand(*kernel_size, filters, filters), dtype='float32')\r\n\r\nfrom tensorflow.python.ops import gen_nn_ops\r\n\r\n@tf.function(input_signature=[tf.TensorSpec([None, 15, 15, 8])])\r\ndef tf_func(inp):\r\n    print('graph tracing')\r\n    net = inp\r\n    for _ in range(5):\r\n        net = gen_nn_ops.conv2d(net, kernel, (1, 1, 1, 1), 'VALID', data_format=order)\r\n    return net\r\n\r\ndef prepare_inp(n):\r\n    return np.random.rand(n, *inp_shape).astype(\"float32\")\r\n\r\nout = tf_func(prepare_inp(1)).numpy() # first call - very slow\r\nN = 30010\r\nfor n in range(N, N+6):\r\n    inp = prepare_inp(n)\r\n    elapsed = []\r\n    for _ in range(5):\r\n        _start = perf_counter()\r\n        out = tf_func(inp).numpy()\r\n        elapsed.append(perf_counter() - _start)\r\n    print(f'[{n:,} samples] running time (msec): ' + ' '.join(f'{1000*e:6.1f}' for e in elapsed))\r\n\r\n\"\"\"\r\nOutput:\r\ngraph tracing\r\n[30,010 samples] running time (msec):  466.6   77.4   63.0   63.5   77.9\r\n[30,011 samples] running time (msec):  389.0   64.0   49.6   38.8   63.4\r\n\r\nExpected something like:\r\ngraph tracing\r\n[30,010 samples] running time (msec):  466.6   77.4   63.0   63.5   77.9\r\n[30,011 samples] running time (msec):   60.0   64.0   49.6   38.8   63.4\r\n\"\"\"\r\n```\r\n", "comments": ["Was able to reproduce the issue with TF v2.2, TF v2.3.0rc1 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/c78ab0058bd22aa0f588e6528f8d1c96/41325.ipynb). Thanks!", "I ran the gist, it looks like it runs with CPU in the gist but in my environment it runs with GPU.\r\nthe issue occurs only in GPU. what should I change so it will run in gist on the GPU?", "> I ran the gist, it looks like it runs with CPU in the gist but in my environment it runs with GPU.\r\n> the issue occurs only in GPU. what should I change so it will run in gist on the GPU?\r\n\r\n@farotem,\r\nPlease click on \"Runtime\" -> \"Change runtime type\"  and then select the preferred hardware accelerator.", "please add the label GPU since the issue is happening only on GPU", "@farotem,\r\nSorry for the delayed response. **`Performance`** seems to be improved when your code is executed with **`Tensorflow Version 2.5`**. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/742c2f26f882c0c82d2e398e1b362f18/41325.ipynb). Can you please confirm the same? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "@rmothukuru \r\nsorry for the late delay but it looks like it didn't improve with tensorflow 2.5.\r\nthe gist mechanism changed a bit.\r\nwhen running the gist for the 1st time the performance issue occurs. \r\nwhen calling **run cell** again with the same inputs shapes it doesn't.\r\nwhen calling run cell again with **different inputs shape** (N = 30010 for example) its happening again.\r\ncan you please re open it?", "@farotem Was able to reproduce on colab using latest TF v2.7.0 ,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/9c5499cae5f0894cb9c0f78ccbb66cea/41325.ipynb#scrollTo=_cfNkck1C88i)?Thanks! ", "Hi,\r\nI checked on tf 2.7 and the issue is still happening \r\n\r\ntf version = 2.7.0\r\ngraph tracing\r\n[29,990 samples] running time (msec): 1567.1  128.6  125.4  127.3  125.2\r\n[29,991 samples] running time (msec): 1377.4  125.5  125.9  124.9  124.5\r\n[29,992 samples] running time (msec): 1365.7  128.6  125.1  125.3  125.3\r\n[29,993 samples] running time (msec): 1377.8  125.7  126.0  124.9  124.3\r\n[29,994 samples] running time (msec): 1366.3  124.1  124.7  125.0  125.5\r\n[29,995 samples] running time (msec): 1380.6  126.2  125.6  124.1  124.9\r\n"]}, {"number": 41303, "title": "Improve ROCm's sqrt and rsqrt for std::complex.", "body": "Follow up from here: https://github.com/tensorflow/tensorflow/pull/41259/files#r452825851", "comments": ["m running this change on ROCm CI ... see following PR\r\nhttps://github.com/ROCmSoftwarePlatform/tensorflow-upstream/pull/1043\r\n\r\nwill update this PR once the run is complete", "/cc @ekuznetsov139 for awareness", "In what way is this an improvement? It is likely slower than before (more branches in impl_sqrt, more calls to sqrt in impl_sqrt). Can't tell if the accuracy is better. It may break unit tests if there is any loss of accuracy. Let's wait for CI results.\r\nAs a general note, no need to use impl_ prefixes when calling on float and double,\r\nimpl_rsqrt(norm(x))\r\nshould just be\r\nrsqrt(norm(x))", "all CI runs passed", "Sorry, my description should have been clearer.\r\n\r\nThis change adds handling of catastrophic cancellations to impl_sqrt.\r\n\r\nAnd the handling of catastrophic cancellations should be faster and work for half's as well.\r\n\r\nDoes that make sense?\r\n", "According to my tests, \"old\" impl_sqrt produces NaNs with inputs below the magnitude of ~10^-20. Your \"new\" impl_sqrt only starts to fail at 10^-23. Outside the extreme range, the two solutions have comparable accuracy levels, the new impl_sqrt is somewhat better and has a lower maximum relative error, the new impl_rsqrt is slightly worse.\r\n\r\nDo we care about half? There's no support for ```complex<half>``` anywhere in the code that I can see.\r\n\r\nThis approximation\r\nreturn T(0.5) * fabs(b) * impl_rsqrt(r);\r\n\r\nseems to be a bit off. To the lowest order in |v/r|, the correct value is \r\n\r\n0.5 |b|/sqrt(|v|) (1 - 1/8 |v/r|^2)\r\n\r\nand the formula above reduces to \r\n\r\n0.5 |b|/sqrt(|v|) (1 - 1/4 |v/r|^2)\r\n\r\nWhich does not matter for float, where, as long as |v/r| is less than 1e-5, the quantity in the brackets is exactly 1 either way, but it matters for double. (And even for float, the correct formula would allow one to raise the threshold and result in lower errors.)", "@chsigg This PR is in draft, any update on this? Please. Thanks!\r\n", "@chsigg This PR is in draft, any update on this? Please. Thanks!", "@chsigg This PR is in draft, any update on this? Please. Thanks!", "@chsigg This PR is in draft, any update on this? Please. Thanks!", "@chsigg This PR is in draft, any update on this? Please. Thanks!", "@chsigg This PR is in draft, any update on this? Please. Thanks!", "@chsigg This PR is in draft, any update on this? Please. Thanks!", "@chsigg This PR is in draft, any update on this? Please. Thanks!", "@chsigg This PR is in draft, any update on this? Please. Thanks!", "@chsigg This PR is in draft, any update on this? Please. Thanks!", "@chsigg  This PR is in draft, any update on this? Please. Thanks!", "@chsigg This PR is in draft, any update on this? Please. Thanks!", "@chsigg This PR is in draft, any update on this? Please. Thanks!", "@chsigg This PR is in draft, any update on this? Please. Thanks!", "@chsigg This PR is in draft, any update on this? Please. Thanks!"]}, {"number": 41280, "title": "Incorrect gradient for ctc_loss on GPU when using logit_length", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 9.12 (TF2.2 DeepLearning image on GCP)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Preinstalled\r\n- TensorFlow version (use command below): v2.2.0-0-g2b96f36 2.2.0-dlenv\r\n- Python version: 3.7.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: V10.1.243\r\n- GPU model and memory: Nvidia tesla P100\r\n\r\n**Describe the current behavior**\r\n\r\nI have experienced inconsistencies in the computation of the gradient of `tf.nn.ctc_loss` between the CPU and GPU implementations when the `logit_length` argument contains something else than `[num_frames]*batch_size`.\r\nMostly I observe that the gradient relative to `logits` for the GPU implementation does not contain zeros after the end of the sequence as given by `logit_length`. Whereas this is the case for the CPU implementation which seems to work correctly.\r\n\r\nI have noticed that the unit tests for this op do not test this case in particular (see https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/ctc_loss_op_test.py#L993).\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nuse_logits_lengths = True\r\n\r\nbatch_size = 8\r\nnum_labels = 27\r\nmax_labels_length = 32\r\nmax_logits_length = 128\r\n\r\nlabels = []\r\nlabels_lengths = []\r\nlogits = []\r\nlogits_lengths = []\r\nfor i in range(batch_size):\r\n    labels_lengths.append(tf.random.uniform([], 1, max_labels_length, tf.int32))\r\n    labels.extend(tf.random.uniform([labels_lengths[-1]], 0, num_labels-1, tf.int32))\r\n\r\n    # I multiply label_length by 2 to make sure there are enough frames\r\n    logits_lengths.append(tf.random.uniform([], labels_lengths[-1].numpy()*2, max_logits_length+1, tf.int32))\r\n\r\nlabels = tf.RaggedTensor.from_row_lengths(labels, labels_lengths).to_sparse()\r\nlabels_lengths = tf.concat(labels_lengths, 0)\r\nlogits = tf.random.uniform([batch_size, max_logits_length, num_labels])\r\nlogits_lengths = tf.concat(logits_lengths, 0)\r\nlogits_lengths_full = tf.constant([max_logits_length]*batch_size)\r\n\r\ndef ctc_compare_cpu_gpu(logits_lengths):\r\n    print(\"logits_lengths\", logits_lengths.numpy())\r\n\r\n    with tf.device(\"/gpu:0\"):\r\n        with tf.GradientTape() as t:\r\n            t.watch(logits)\r\n            gpu_loss = tf.nn.ctc_loss(labels, logits, labels_lengths, logits_lengths, logits_time_major=False, blank_index=-1)\r\n        gpu_grad = t.gradient(gpu_loss, [logits])[0]\r\n\r\n    with tf.device(\"/cpu:0\"):\r\n        with tf.GradientTape() as t:\r\n            t.watch(logits)\r\n            cpu_loss = tf.nn.ctc_loss(labels, logits, labels_lengths, logits_lengths, logits_time_major=False, blank_index=-1)\r\n        cpu_grad = t.gradient(cpu_loss, [logits])[0]\r\n\r\n    print(\"Max loss error\", tf.math.abs(gpu_loss - cpu_loss).numpy().max())\r\n    print(\"Max grad error\", tf.math.abs(gpu_grad - cpu_grad).numpy().max())\r\n    print()\r\n    return cpu_loss, gpu_loss, cpu_grad, gpu_grad\r\n\r\nctc_compare_cpu_gpu(logits_lengths_full)\r\nctc_compare_cpu_gpu(logits_lengths)\r\n```\r\nOutput:\r\n```\r\nlogits_lengths [128 128 128 128 128 128 128 128]\r\nMax loss error 0.00012207031\r\nMax grad error 0.00014734268\r\n\r\nlogits_lengths [ 70  86  22  74 112 121 103 123]\r\nMax loss error 6.1035156e-05\r\nMax grad error 0.9669469\r\n```\r\n", "comments": ["@pvanhaes \r\nPlease let us know if [this gist](https://colab.research.google.com/gist/Saduf2019/52034a25170bfaad071ac9aea0a7a07d/untitled273.ipynb) confirms your issue.", "@Saduf2019 \r\nIndeed this is displaying the same issue.", "@kaixih I believe you added the CTC loss GPU implementation in https://github.com/tensorflow/tensorflow/pull/32302, can you please take a look to see if this issue is related?", "@pvanhaes,\r\nCan you please respond to the above comment? Thanks! ", "As far as I can tell, yes? It seems to be related since the problem occurs only when using the cudnn ctc loss implementation.\r\nI looked into pytorch to see if they have the same issue. They have [test cases](https://github.com/pytorch/pytorch/blob/87242d2393119990ebe9043e854317f02536bdff/test/test_autograd.py#L7646) where logit_length is not uniform but have disabled them due to flakiness, however they seem to have correct gradients.", "I tried the script on V100 a couple of times and I can see the flakiness:\r\nRun 1:\r\n```\r\nMax loss error 0.00021362305\r\nMax grad error 0.00022548437\r\n\r\nlogits_lengths [  8 108  86  90  66  53 110  97]\r\nMax loss error 9.1552734e-05\r\nMax grad error 0.00022548437\r\n```\r\nRun X:\r\n```\r\nMax loss error 9.1552734e-05\r\nMax grad error 9.518862e-05\r\n\r\nlogits_lengths [ 61  88  79  14  42 112  95  60]\r\nMax loss error 6.1035156e-05\r\nMax grad error 0.55553436\r\n```\r\n\r\nLooking into it.", "I think the issue is the cudnn doesn't zero out the grads if it exceeds the sequence length. So, if the grads array happens to contain some large numbers, you will encounter the reported high error. One workaround is to explicitly apply the mask like below, where I use the sequence_mask() to generate a mask based on `logits_lengths` and then zero out the unwanted gradients.\r\n\r\nAlso, I will file a bug towards our cudnn team to fix this issue.\r\n```python\r\nimport tensorflow as tf\r\n\r\ntf.random.set_seed(1)\r\nuse_logits_lengths = True\r\n\r\nbatch_size = 8\r\nnum_labels = 27\r\nmax_labels_length = 32\r\nmax_logits_length = 128\r\n#batch_size = 4\r\n#num_labels = 6\r\n#max_labels_length = 32\r\n#max_logits_length = 64\r\n\r\nlabels = []\r\nlabels_lengths = []\r\nlogits = []\r\nlogits_lengths = []\r\nfor i in range(batch_size):\r\n    labels_lengths.append(tf.random.uniform([], 1, max_labels_length, tf.int32))\r\n    labels.extend(tf.random.uniform([labels_lengths[-1]], 0, num_labels-1, tf.int32))\r\n\r\n    # I multiply label_length by 2 to make sure there are enough frames\r\n    logits_lengths.append(tf.random.uniform([], labels_lengths[-1].numpy()*2, max_logits_length+1, tf.int32))\r\n\r\nlabels = tf.RaggedTensor.from_row_lengths(labels, labels_lengths).to_sparse()\r\nlabels_lengths = tf.concat(labels_lengths, 0)\r\n\r\nlogits_lengths = tf.concat(logits_lengths, 0)\r\nlogits_lengths_full = tf.constant([max_logits_length]*batch_size)\r\n\r\nlogits = tf.random.uniform([batch_size, max_logits_length, num_labels])\r\n\r\nlogit_mask = tf.sequence_mask(logits_lengths, max_logits_length,\r\n                              tf.dtypes.float32)\r\nlogit_mask = tf.expand_dims(logit_mask, axis=2)\r\n#print(\"XXX\", logit_mask)\r\n\r\ndef ctc_compare_cpu_gpu(logits_lengths, mask=None):\r\n\r\n    print(\"logits_lengths\", logits_lengths.numpy())\r\n    print(\"labels_lengths\", labels_lengths.numpy())\r\n\r\n    with tf.device(\"/gpu:0\"):\r\n        with tf.GradientTape() as t:\r\n            t.watch(logits)\r\n            gpu_loss = tf.nn.ctc_loss(labels, logits, labels_lengths, logits_lengths, logits_time_major=False, blank_index=-1)\r\n        gpu_grad = t.gradient(gpu_loss, [logits])[0]\r\n        if mask is not None:\r\n          gpu_grad = gpu_grad * mask\r\n\r\n    with tf.device(\"/cpu:0\"):\r\n        with tf.GradientTape() as t:\r\n            t.watch(logits)\r\n            cpu_loss = tf.nn.ctc_loss(labels, logits, labels_lengths, logits_lengths, logits_time_major=False, blank_index=-1)\r\n        cpu_grad = t.gradient(cpu_loss, [logits])[0]\r\n\r\n    print(\"Max loss error\", tf.math.abs(gpu_loss - cpu_loss).numpy().max())\r\n    print(\"Max grad error\", tf.math.abs(gpu_grad - cpu_grad).numpy().max())\r\n    return cpu_loss, gpu_loss, cpu_grad, gpu_grad\r\n\r\nctc_compare_cpu_gpu(logits_lengths_full)\r\n\r\nctc_compare_cpu_gpu(logits_lengths, mask=logit_mask)\r\n#ctc_compare_cpu_gpu(logits_lengths)\r\n\r\n\r\n```", "Hi @kaixih \nIndeed, I remember having tried that at the time of discovering this bug. However I still ended up using the cpu loss anyway due to a very unstable training. I wasnt able to explain why but I easily encountered NaN gradients and/or loss.", "Not sure the NaN issue is caused by those \"unused and not correctly initialized\" gradients output by cudnn backend. Do you mean you still hit the NaN issue even after manually applying the masks over gradients returned by the ctc loss call on GPU?", "What happened was that even when masking the gradients (using `tf.where` and not multiplication by zeros) I was unable to fully train a model and always ended up with NaN weights. Maybe I had too large gradients instead of NaNs, but in any case the behavior was different than with the CPU loss.\nI'll see if I get the chance to try that again soon.", "Thanks. It sounds like a numeric precision issue. If possible, could you also give a shot with \"TF_CUDNN_DETERMINISTIC=1\" which will force TF to use a deterministic CTC algorithm (However, this would require the label size under 256.). By default, TF uses a non-deterministic algorithm.", "Hi, I encountered the same problem where the CPU version of the ctc_loss works fine, but the GPU version gives the NAN. I've tried setting the TF_CUDNN_DETERMINISTIC=1 with bach_size=512 but the issue persists. However, setting batch size=128 or 256 seems to fix the issue.", "Hi @AveryLiu That is interesting. The deterministic algorithm shouldn't affect the batch size. Are you working on the variable sequence lengths or fixed sequence lengths? If it is the variable sequence lengths, maybe it is a fluke when batch size is 128 or 256. ", "> Hi @AveryLiu That is interesting. The deterministic algorithm shouldn't affect the batch size. Are you working on the variable sequence lengths or fixed sequence lengths? If it is the variable sequence lengths, maybe it is a fluke when batch size is 128 or 256.\r\n\r\nI'm using variable sequence length. Indeed setting batch_size to 128 or 256 does not solve the problem. It does not give me NANs, but the gradients seem incorrect and the loss stagnates (CPU version is ok). I am not sure how to apply the gradient masking mentioned above since I am using a fully capsulated Keras model. Another thing I observed is that the model can be trained with data where all label length is 1.", "Was able to replicate the issue in TF v2.5,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/91d1d1eb2aa4a9ec502df054c8e7586b/untitled132.ipynb)..Thanks !"]}, {"number": 41272, "title": "User defined color in tf.image.resize_with_crop_or_pad", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): tf 2.2.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nCurrently, the `tf.image.resize_with_crop_or_pad` ops pads with 0. It would be helpful to be able to give another value because in some application black is not the right color for padding (it messes with real data).\r\n\r\n**Will this change the current api? How?**\r\n\r\nAdding a kwarg padding_color Union[int, iterable] with:\r\n - int, a value to be used on all channels (currently 0)\r\n - iterable: with len(iterable) == number of channel to describe any rgb color\r\n\r\nExample of how to achieve the expected result currently:\r\n\r\n```\r\n        output_tensor = tf.image.resize_with_crop_or_pad(input_tensor, 450, 450)\r\n        padding = (1 - tf.image.resize_with_crop_or_pad(tf.ones_like(input_tensor), 450, 450)) * tf.constant(\r\n            [255, 0, 255], dtype=tf.uint8\r\n        )\r\n        output_tensor += padding\r\n```\r\n\r\n**Who will benefit with this feature?**\r\n\r\nUsers who need to pad images with another color because black is not suitable for their dataset\r\n\r\n**Any Other info.**\r\n", "comments": []}, {"number": 41256, "title": "Type inference using type annotations", "body": "Performs type inference using type annotations. \r\nA dictionary with variable names mapped to a list of possible types is added to the AST nodes.\r\n\r\n### Example\r\n```python\r\ndef f(x: tf.Tensor, a: int):\r\n  if y > 0:\r\n    y = x + y\r\n  return y\r\n```\r\nThe AST will be annotated with:\r\n`y = x + y`: \r\n```cs\r\n___pyct_anno={SCOPE: Scope{r=(x, y,), w=(y,)}, 'type_anno_read': {x: {tf.Tensor}, y: {int}}, 'type_anno_write': {y: {tf.Tensor}}}\r\n```\r\n\r\n`return y`: \r\n```cs\r\n___pyct_anno={SCOPE: Scope{r=(y,), w=()}, 'type_anno_read': {y: {tf.Tensor, int}}, 'type_anno_write': {}}\r\n```\r\n\r\nNote: This is experimental and does not infer types for variables assigned to a python object.\r\n", "comments": ["@rahul-kamat can you please check ubuntu sanity build failures ?", "The PR is still valid, but we've been delayed on the integration."]}, {"number": 41251, "title": "The `**kwargs` in `tf.keras.Model` does not accept any argument", "body": "The `**kwargs` in `tf.keras.Model` does not accept any argument.\r\n\r\nI am writing derivative class of `tf.keras.Model`, namely `Foo` class, which would like to create an instance of another class `Boo`. Hence I would like to pass arguments of `Boo`, say `booarg`, together to the derived class constructor. However, since `**kwargs` in `tf.keras.Model` does not accept any argument, I have to filter out arguments by myself, which is complicated, especially when the argument list of `Boo` is long. A toy code is attached.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nclass Boo():\r\n    def __init__(self, booarg1, booarg2):\r\n        print(\"Boo Class: \", booarg1)\r\n        print(\"Boo Class: \", booarg2)\r\n\r\nclass FooModel(tf.keras.Model):\r\n    def __init__(self, fooarg, **kwargs):\r\n        super(FooModel, self).__init__(**kwargs)\r\n        print(\"Foo Class: \", fooarg)\r\n        boo = Boo(**kwargs)\r\n\r\nfoo = FooModel(fooarg=1, booarg1=2, booarg2=3)\r\n```\r\n\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.8.2\r\n- GCC/Compiler version (if compiling from source): 9.3.0\r\n", "comments": ["@YingzhouLi \r\nPlease refer to this issue with similar error and let us know of it helps.\r\n[link](https://stackoverflow.com/questions/50830736/typeerror-keyword-argument-not-understood-data-format) [link1](url)", "@Saduf2019 Upgrading Keras does not solve the issue.", "@YingzhouLi,\r\nChanging the line `super(FooModel, self).__init__(**kwargs)` to `super().__init__()` seems to work.\r\nPlease check this [gist](https://colab.research.google.com/gist/amahendrakar/f8552954f37fb95561134076baeded24/41251.ipynb\r\n) for reference.\r\n\r\nAlso, take a look at this relevant StackOverflow [comment](https://stackoverflow.com/a/7629620). Thanks!", "@Saduf2019, that indeed works but not in the way I prefer.\r\nI would like to wrap another layer around `tf.Model` and some parameters in `**kwargs` would be passed to `tf.Model` class, like, optimizer, etc.", "Was able to replicate the issue in TF v2.7.0,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/6d580bee8537ba3fce97691e0514fd82/untitled131.ipynb)..Thanks ! ", "This real issue. I am also trying to extent the layer but it is not accepting anything in **kwargs with error TypeError: ('Keyword argument not understood:',"]}, {"number": 41225, "title": "Masked Pooling and Convolution", "body": "In geospatial domain often masked images are used due to various reasons. In some specific cases is not sufficient to fill the masked areas as zero and work with that image. Therefore it would be vvery helpful if it would be possible to perform convolution and pooling operations (specifically 2D pooling and convolution), optionally with a mask. ", "comments": ["@digital-idiot \r\n\r\nDo you have any use case that requires the feature you are interested in? Please feel free to submit a PR if you have use cases that supports that feature.Thanks!", "In general zeroing the masked values works (for example: [PixelRNN](https://arxiv.org/pdf/1601.06759.pdf)) but in some specific use cases if the subsequent pooling layer is average pooling then simply zeroing the values do not work anymore because zeroing the values results into wrong estimate of the mean. Moreover, there is no concrete way to distinguish between a masked pixel and a unmasked pixel with value zero. In many cases normalizing and shifting the value range is not advisable in case of geo-spatial images which further limits the option.\r\n\r\nThe following 2 paper shows currently how tedious it is to achieve masked convolution and pooling are:\r\nhttps://arxiv.org/pdf/1605.02226v3.pdf\r\nhttps://arxiv.org/pdf/2006.12486v1.pdf \r\n\r\n"]}, {"number": 41218, "title": "Differences in using tf.keras.Model.fit() API and writing training loop from scratch (simple regression model)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2\r\n- Python version: 3.6.3\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: nvidia gtx 1080 \r\n\r\n**Describe the current behavior**\r\nI'm trying to learn the new tensorflow 2 API using the following tutorials:\r\n[1] https://www.tensorflow.org/tutorials/keras/regression\r\n[2] https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch/\r\n\r\nCurrently, the regression model based on [1] works really well when I use the model.fit() API. \r\n\r\nLet's call this (A):\r\n~~~\r\ndef fit_model_a(model, dataset, normalize_fn):\r\n    model.compile(loss='mse',\r\n                    optimizer=tf.keras.optimizers.RMSprop(0.001),\r\n                    metrics=['mse'])\r\n\r\n    x_train = dataset.copy()\r\n    y_train = x_train.pop('MPG')\r\n    history = model.fit(\r\n        normalize_fn(x_train), y_train,\r\n        epochs=100, validation_split = 0.2, verbose=1)\r\n~~~\r\nUsing my own training loop however exhibits drastically different behavior.\r\n\r\nHere's the training function using my own loop (B):\r\n~~~\r\ndef fit_model_b(model, dataset, normalize_fn):\r\n    trainset = dataset.sample(frac=0.8,random_state=0)\r\n    devset = dataset.drop(trainset.index)\r\n\r\n    optimizer = tf.keras.optimizers.RMSprop(0.001)\r\n\r\n    bsize = 32\r\n    for ep in range(100):\r\n        trainset = trainset.sample(frac=1)\r\n\r\n        for i in range(0,len(trainset.index),bsize):\r\n            x_batch = trainset.iloc[i:i+bsize]\r\n            y_batch = x_batch.pop('MPG')\r\n            \r\n            with tf.GradientTape() as tape:\r\n                y_batch_pred = model(normalize_fn(x_batch), training=True)\r\n                loss = tf.keras.losses.MeanSquaredError()(y_batch.values, y_batch_pred)\r\n\r\n            gradients = tape.gradient(loss, model.trainable_weights)\r\n            optimizer.apply_gradients(zip(gradients, model.trainable_weights))\r\n        \r\n        print('{}\\tloss={:.2f}\\tfit-mse={:.2f}\\tdev-mse={:.2f}'.format(ep, loss.numpy(), \r\n                    eval_model(model, trainset, normalize_fn), eval_model(model, devset, normalize_fn)))\r\n~~~\r\n\r\nI've included a collab with the full source code at the bottom. Between the two, the models used are exactly the same (built using tf.keras.Sequential()), optimized with the same optimizer (RMSProp with LR=0.001), and trained on the same data from [1]. The training function (B) is meant to train for 100 epochs with a batch size of 32 to replicate the behavior of model.fit().  However, (B) is not fitting well on the data and by looking at some of the predictions, it looks like (B) is fixated on learning the average, while predictions by (A) is much more meaningful. How can this be?\r\n\r\nHere's the output of the model using (A):\r\n\r\n~~~\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ndense_15 (Dense)             (None, 64)                640       \r\n_________________________________________________________________\r\ndense_16 (Dense)             (None, 64)                4160      \r\n_________________________________________________________________\r\ndense_17 (Dense)             (None, 1)                 65        \r\n=================================================================\r\nTotal params: 4,865\r\nTrainable params: 4,865\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nEpoch 1/100\r\n8/8 [==============================] - 0s 13ms/step - loss: 573.9155 - mse: 573.9155 - val_loss: 577.1165 - val_mse: 577.1165\r\nEpoch 2/100\r\n8/8 [==============================] - 0s 6ms/step - loss: 535.4519 - mse: 535.4519 - val_loss: 535.2042 - val_mse: 535.2042\r\nEpoch 3/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 492.2293 - mse: 492.2293 - val_loss: 486.6287 - val_mse: 486.6287\r\nEpoch 4/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 441.6238 - mse: 441.6238 - val_loss: 429.2172 - val_mse: 429.2172\r\nEpoch 5/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 384.5562 - mse: 384.5562 - val_loss: 367.9710 - val_mse: 367.9710\r\nEpoch 6/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 324.1839 - mse: 324.1839 - val_loss: 304.0136 - val_mse: 304.0136\r\nEpoch 7/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 261.1829 - mse: 261.1829 - val_loss: 237.4727 - val_mse: 237.4727\r\nEpoch 8/100\r\n8/8 [==============================] - 0s 6ms/step - loss: 199.2595 - mse: 199.2595 - val_loss: 175.9138 - val_mse: 175.9138\r\nEpoch 9/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 143.4326 - mse: 143.4326 - val_loss: 122.1979 - val_mse: 122.1979\r\nEpoch 10/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 97.0918 - mse: 97.0918 - val_loss: 79.8414 - val_mse: 79.8414\r\nEpoch 11/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 61.4929 - mse: 61.4929 - val_loss: 48.1678 - val_mse: 48.1678\r\nEpoch 12/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 38.5452 - mse: 38.5452 - val_loss: 30.5858 - val_mse: 30.5858\r\nEpoch 13/100\r\n8/8 [==============================] - 0s 6ms/step - loss: 26.7722 - mse: 26.7722 - val_loss: 22.1917 - val_mse: 22.1917\r\nEpoch 14/100\r\n8/8 [==============================] - 0s 6ms/step - loss: 22.0821 - mse: 22.0821 - val_loss: 19.3342 - val_mse: 19.3342\r\nEpoch 15/100\r\n8/8 [==============================] - 0s 6ms/step - loss: 20.2049 - mse: 20.2049 - val_loss: 17.5354 - val_mse: 17.5354\r\nEpoch 16/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 19.3463 - mse: 19.3463 - val_loss: 17.0012 - val_mse: 17.0012\r\nEpoch 17/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 18.8869 - mse: 18.8869 - val_loss: 16.9589 - val_mse: 16.9589\r\nEpoch 18/100\r\n8/8 [==============================] - 0s 6ms/step - loss: 18.7929 - mse: 18.7929 - val_loss: 16.5268 - val_mse: 16.5268\r\nEpoch 19/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 18.6861 - mse: 18.6861 - val_loss: 16.2446 - val_mse: 16.2446\r\nEpoch 20/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 18.6174 - mse: 18.6174 - val_loss: 16.1443 - val_mse: 16.1443\r\nEpoch 21/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 17.9203 - mse: 17.9203 - val_loss: 16.8566 - val_mse: 16.8566\r\nEpoch 22/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 18.1555 - mse: 18.1555 - val_loss: 16.3021 - val_mse: 16.3021\r\nEpoch 23/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 18.0476 - mse: 18.0476 - val_loss: 15.5666 - val_mse: 15.5666\r\nEpoch 24/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 17.9181 - mse: 17.9181 - val_loss: 15.6645 - val_mse: 15.6645\r\nEpoch 25/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 18.1296 - mse: 18.1296 - val_loss: 15.3343 - val_mse: 15.3343\r\nEpoch 26/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 17.5297 - mse: 17.5297 - val_loss: 15.3144 - val_mse: 15.3144\r\nEpoch 27/100\r\n8/8 [==============================] - 0s 6ms/step - loss: 17.4787 - mse: 17.4787 - val_loss: 15.4818 - val_mse: 15.4818\r\nEpoch 28/100\r\n8/8 [==============================] - 0s 6ms/step - loss: 17.2343 - mse: 17.2343 - val_loss: 15.5582 - val_mse: 15.5582\r\nEpoch 29/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 17.3628 - mse: 17.3628 - val_loss: 15.0562 - val_mse: 15.0562\r\nEpoch 30/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 17.4319 - mse: 17.4319 - val_loss: 14.8126 - val_mse: 14.8126\r\nEpoch 31/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 16.8817 - mse: 16.8817 - val_loss: 14.8000 - val_mse: 14.8000\r\nEpoch 32/100\r\n8/8 [==============================] - 0s 6ms/step - loss: 17.0150 - mse: 17.0150 - val_loss: 14.7323 - val_mse: 14.7323\r\nEpoch 33/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 17.0183 - mse: 17.0183 - val_loss: 14.7309 - val_mse: 14.7309\r\nEpoch 34/100\r\n8/8 [==============================] - 0s 6ms/step - loss: 16.7561 - mse: 16.7561 - val_loss: 14.6972 - val_mse: 14.6972\r\nEpoch 35/100\r\n8/8 [==============================] - 0s 6ms/step - loss: 16.8762 - mse: 16.8762 - val_loss: 14.9364 - val_mse: 14.9364\r\nEpoch 36/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 16.7464 - mse: 16.7464 - val_loss: 14.4770 - val_mse: 14.4770\r\nEpoch 37/100\r\n8/8 [==============================] - 0s 7ms/step - loss: 16.9630 - mse: 16.9630 - val_loss: 14.8516 - val_mse: 14.8516\r\nEpoch 38/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 16.5634 - mse: 16.5634 - val_loss: 14.3185 - val_mse: 14.3185\r\nEpoch 39/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 16.2602 - mse: 16.2602 - val_loss: 14.5408 - val_mse: 14.5408\r\nEpoch 40/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 16.5903 - mse: 16.5903 - val_loss: 14.7360 - val_mse: 14.7360\r\nEpoch 41/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 16.2692 - mse: 16.2692 - val_loss: 14.1311 - val_mse: 14.1311\r\nEpoch 42/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 16.1788 - mse: 16.1788 - val_loss: 14.3953 - val_mse: 14.3953\r\nEpoch 43/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 16.2510 - mse: 16.2510 - val_loss: 14.2353 - val_mse: 14.2353\r\nEpoch 44/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 16.0959 - mse: 16.0959 - val_loss: 13.9634 - val_mse: 13.9634\r\nEpoch 45/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 16.2482 - mse: 16.2482 - val_loss: 13.9016 - val_mse: 13.9016\r\nEpoch 46/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 15.7698 - mse: 15.7698 - val_loss: 15.3176 - val_mse: 15.3176\r\nEpoch 47/100\r\n8/8 [==============================] - 0s 6ms/step - loss: 16.0930 - mse: 16.0930 - val_loss: 14.1587 - val_mse: 14.1587\r\nEpoch 48/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 16.1624 - mse: 16.1624 - val_loss: 14.0680 - val_mse: 14.0680\r\nEpoch 49/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 15.7494 - mse: 15.7494 - val_loss: 13.9287 - val_mse: 13.9287\r\nEpoch 50/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 15.5207 - mse: 15.5207 - val_loss: 13.7302 - val_mse: 13.7302\r\nEpoch 51/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 15.8989 - mse: 15.8989 - val_loss: 13.7902 - val_mse: 13.7902\r\nEpoch 52/100\r\n8/8 [==============================] - 0s 6ms/step - loss: 15.4386 - mse: 15.4386 - val_loss: 13.6323 - val_mse: 13.6323\r\nEpoch 53/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 15.5308 - mse: 15.5308 - val_loss: 13.7107 - val_mse: 13.7107\r\nEpoch 54/100\r\n8/8 [==============================] - 0s 6ms/step - loss: 15.2940 - mse: 15.2940 - val_loss: 13.5425 - val_mse: 13.5425\r\nEpoch 55/100\r\n8/8 [==============================] - 0s 6ms/step - loss: 15.2990 - mse: 15.2990 - val_loss: 13.6214 - val_mse: 13.6214\r\nEpoch 56/100\r\n8/8 [==============================] - 0s 6ms/step - loss: 15.5306 - mse: 15.5306 - val_loss: 13.7859 - val_mse: 13.7859\r\nEpoch 57/100\r\n8/8 [==============================] - 0s 6ms/step - loss: 15.2955 - mse: 15.2955 - val_loss: 13.4464 - val_mse: 13.4464\r\nEpoch 58/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 15.3422 - mse: 15.3422 - val_loss: 14.4806 - val_mse: 14.4806\r\nEpoch 59/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 15.2947 - mse: 15.2947 - val_loss: 13.3782 - val_mse: 13.3782\r\nEpoch 60/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 15.2226 - mse: 15.2226 - val_loss: 13.6523 - val_mse: 13.6523\r\nEpoch 61/100\r\n8/8 [==============================] - 0s 6ms/step - loss: 14.7474 - mse: 14.7474 - val_loss: 15.0304 - val_mse: 15.0304\r\nEpoch 62/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 14.9154 - mse: 14.9154 - val_loss: 13.6243 - val_mse: 13.6243\r\nEpoch 63/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 15.0523 - mse: 15.0523 - val_loss: 13.1219 - val_mse: 13.1219\r\nEpoch 64/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 15.0700 - mse: 15.0700 - val_loss: 13.0726 - val_mse: 13.0726\r\nEpoch 65/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 15.1012 - mse: 15.1012 - val_loss: 13.1141 - val_mse: 13.1141\r\nEpoch 66/100\r\n8/8 [==============================] - 0s 7ms/step - loss: 15.1817 - mse: 15.1817 - val_loss: 12.9964 - val_mse: 12.9964\r\nEpoch 67/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 14.7159 - mse: 14.7159 - val_loss: 13.1849 - val_mse: 13.1849\r\nEpoch 68/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 15.0554 - mse: 15.0554 - val_loss: 12.9389 - val_mse: 12.9389\r\nEpoch 69/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 14.6790 - mse: 14.6790 - val_loss: 13.2163 - val_mse: 13.2163\r\nEpoch 70/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 14.4835 - mse: 14.4835 - val_loss: 12.9312 - val_mse: 12.9312\r\nEpoch 71/100\r\n8/8 [==============================] - 0s 6ms/step - loss: 14.4713 - mse: 14.4713 - val_loss: 12.9020 - val_mse: 12.9020\r\nEpoch 72/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 14.5055 - mse: 14.5055 - val_loss: 12.8103 - val_mse: 12.8103\r\nEpoch 73/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 14.6565 - mse: 14.6565 - val_loss: 12.7457 - val_mse: 12.7457\r\nEpoch 74/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 14.4647 - mse: 14.4647 - val_loss: 12.7685 - val_mse: 12.7685\r\nEpoch 75/100\r\n8/8 [==============================] - 0s 6ms/step - loss: 14.2886 - mse: 14.2886 - val_loss: 12.7231 - val_mse: 12.7231\r\nEpoch 76/100\r\n8/8 [==============================] - 0s 6ms/step - loss: 14.2142 - mse: 14.2142 - val_loss: 12.6104 - val_mse: 12.6104\r\nEpoch 77/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 14.3990 - mse: 14.3990 - val_loss: 12.9287 - val_mse: 12.9287\r\nEpoch 78/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 14.4393 - mse: 14.4393 - val_loss: 13.0988 - val_mse: 13.0988\r\nEpoch 79/100\r\n8/8 [==============================] - 0s 7ms/step - loss: 14.2155 - mse: 14.2155 - val_loss: 12.4750 - val_mse: 12.4750\r\nEpoch 80/100\r\n8/8 [==============================] - 0s 6ms/step - loss: 14.0817 - mse: 14.0817 - val_loss: 13.0547 - val_mse: 13.0547\r\nEpoch 81/100\r\n8/8 [==============================] - 0s 6ms/step - loss: 14.1034 - mse: 14.1034 - val_loss: 12.5163 - val_mse: 12.5163\r\nEpoch 82/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 13.9492 - mse: 13.9492 - val_loss: 12.4928 - val_mse: 12.4928\r\nEpoch 83/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 14.0635 - mse: 14.0635 - val_loss: 12.6222 - val_mse: 12.6222\r\nEpoch 84/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 13.9243 - mse: 13.9243 - val_loss: 12.3200 - val_mse: 12.3200\r\nEpoch 85/100\r\n8/8 [==============================] - 0s 6ms/step - loss: 14.0242 - mse: 14.0242 - val_loss: 12.2624 - val_mse: 12.2624\r\nEpoch 86/100\r\n8/8 [==============================] - 0s 6ms/step - loss: 13.9066 - mse: 13.9066 - val_loss: 12.3266 - val_mse: 12.3266\r\nEpoch 87/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 13.6502 - mse: 13.6502 - val_loss: 12.2916 - val_mse: 12.2916\r\nEpoch 88/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 14.1173 - mse: 14.1173 - val_loss: 12.7280 - val_mse: 12.7280\r\nEpoch 89/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 13.4772 - mse: 13.4772 - val_loss: 12.1312 - val_mse: 12.1312\r\nEpoch 90/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 13.5305 - mse: 13.5305 - val_loss: 12.6813 - val_mse: 12.6813\r\nEpoch 91/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 13.4792 - mse: 13.4792 - val_loss: 12.7643 - val_mse: 12.7643\r\nEpoch 92/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 12.7118 - mse: 12.7118 - val_loss: 13.1304 - val_mse: 13.1304\r\nEpoch 93/100\r\n8/8 [==============================] - 0s 6ms/step - loss: 13.4823 - mse: 13.4823 - val_loss: 12.6384 - val_mse: 12.6384\r\nEpoch 94/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 13.3819 - mse: 13.3819 - val_loss: 13.0808 - val_mse: 13.0808\r\nEpoch 95/100\r\n8/8 [==============================] - 0s 6ms/step - loss: 13.3348 - mse: 13.3348 - val_loss: 12.1661 - val_mse: 12.1661\r\nEpoch 96/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 13.1636 - mse: 13.1636 - val_loss: 11.8638 - val_mse: 11.8638\r\nEpoch 97/100\r\n8/8 [==============================] - 0s 6ms/step - loss: 13.0808 - mse: 13.0808 - val_loss: 12.3383 - val_mse: 12.3383\r\nEpoch 98/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 13.2087 - mse: 13.2087 - val_loss: 11.7358 - val_mse: 11.7358\r\nEpoch 99/100\r\n8/8 [==============================] - 0s 6ms/step - loss: 13.1870 - mse: 13.1870 - val_loss: 11.9876 - val_mse: 11.9876\r\nEpoch 100/100\r\n8/8 [==============================] - 0s 5ms/step - loss: 13.2171 - mse: 13.2171 - val_loss: 12.1973 - val_mse: 12.1973\r\n\r\nMSE ON TRAINING:  12.762719337215518\r\n\r\nPREDICTIONS\r\n>> actual=15.0, predicted=13.902673721313477\r\n>> actual=10.0, predicted=11.325315475463867\r\n>> actual=9.0, predicted=11.17175579071045\r\n>> actual=25.0, predicted=25.51102638244629\r\n>> actual=19.0, predicted=20.770906448364258\r\n>> actual=14.0, predicted=13.679293632507324\r\n>> actual=14.0, predicted=14.169425964355469\r\n>> actual=13.0, predicted=13.453120231628418\r\n>> actual=18.0, predicted=20.06334114074707\r\n>> actual=35.0, predicted=32.093849182128906\r\n>> actual=25.0, predicted=28.19291877746582\r\n>> actual=19.0, predicted=26.57512092590332\r\n>> actual=13.0, predicted=15.338750839233398\r\n>> actual=28.0, predicted=28.23664093017578\r\n>> actual=13.0, predicted=13.387893676757812\r\n>> actual=14.0, predicted=15.006385803222656\r\n>> actual=15.0, predicted=14.883852005004883\r\n>> actual=13.0, predicted=13.88344955444336\r\n>> actual=18.0, predicted=20.642784118652344\r\n>> actual=12.0, predicted=12.577205657958984\r\nMSE ON TESTING: 8.60193713820663\r\n~~~\r\n\r\nHere's the output of the model with (B):\r\n\r\n~~~\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ndense_18 (Dense)             (None, 64)                640       \r\n_________________________________________________________________\r\ndense_19 (Dense)             (None, 64)                4160      \r\n_________________________________________________________________\r\ndense_20 (Dense)             (None, 1)                 65        \r\n=================================================================\r\nTotal params: 4,865\r\nTrainable params: 4,865\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nep=0\tloss=451.15\tfit-mse=562.54\tdev-mse=605.75\r\nep=1\tloss=634.75\tfit-mse=528.33\tdev-mse=570.50\r\nep=2\tloss=578.77\tfit-mse=489.81\tdev-mse=530.83\r\nep=3\tloss=383.15\tfit-mse=445.30\tdev-mse=485.17\r\nep=4\tloss=330.32\tfit-mse=394.18\tdev-mse=433.01\r\nep=5\tloss=350.10\tfit-mse=338.05\tdev-mse=375.80\r\nep=6\tloss=275.47\tfit-mse=280.20\tdev-mse=316.84\r\nep=7\tloss=216.04\tfit-mse=224.03\tdev-mse=259.55\r\nep=8\tloss=219.84\tfit-mse=171.60\tdev-mse=205.86\r\nep=9\tloss=169.93\tfit-mse=128.87\tdev-mse=161.59\r\nep=10\tloss=86.35\tfit-mse=98.62\tdev-mse=129.46\r\nep=11\tloss=67.55\tfit-mse=81.66\tdev-mse=110.51\r\nep=12\tloss=63.47\tfit-mse=70.28\tdev-mse=96.27\r\nep=13\tloss=40.30\tfit-mse=64.55\tdev-mse=88.48\r\nep=14\tloss=44.24\tfit-mse=60.42\tdev-mse=82.77\r\nep=15\tloss=55.45\tfit-mse=58.35\tdev-mse=79.49\r\nep=16\tloss=66.44\tfit-mse=57.02\tdev-mse=77.70\r\nep=17\tloss=49.05\tfit-mse=55.90\tdev-mse=75.52\r\nep=18\tloss=96.35\tfit-mse=54.94\tdev-mse=74.29\r\nep=19\tloss=46.29\tfit-mse=54.72\tdev-mse=74.27\r\nep=20\tloss=76.34\tfit-mse=55.96\tdev-mse=76.21\r\nep=21\tloss=54.97\tfit-mse=56.31\tdev-mse=76.22\r\nep=22\tloss=54.00\tfit-mse=54.82\tdev-mse=74.10\r\nep=23\tloss=65.32\tfit-mse=55.19\tdev-mse=74.64\r\nep=24\tloss=53.25\tfit-mse=56.81\tdev-mse=77.45\r\nep=25\tloss=40.72\tfit-mse=56.05\tdev-mse=75.60\r\nep=26\tloss=46.65\tfit-mse=55.25\tdev-mse=74.81\r\nep=27\tloss=57.01\tfit-mse=55.16\tdev-mse=74.98\r\nep=28\tloss=64.03\tfit-mse=55.09\tdev-mse=74.56\r\nep=29\tloss=68.08\tfit-mse=55.22\tdev-mse=75.21\r\nep=30\tloss=69.12\tfit-mse=53.94\tdev-mse=73.48\r\nep=31\tloss=42.17\tfit-mse=55.08\tdev-mse=75.19\r\nep=32\tloss=39.22\tfit-mse=56.67\tdev-mse=77.53\r\nep=33\tloss=76.51\tfit-mse=56.36\tdev-mse=76.03\r\nep=34\tloss=70.53\tfit-mse=55.41\tdev-mse=75.23\r\nep=35\tloss=36.58\tfit-mse=54.01\tdev-mse=73.56\r\nep=36\tloss=65.90\tfit-mse=53.55\tdev-mse=72.95\r\nep=37\tloss=59.30\tfit-mse=53.91\tdev-mse=73.55\r\nep=38\tloss=61.93\tfit-mse=54.09\tdev-mse=73.31\r\nep=39\tloss=66.33\tfit-mse=54.25\tdev-mse=73.54\r\nep=40\tloss=72.43\tfit-mse=55.24\tdev-mse=75.45\r\nep=41\tloss=37.08\tfit-mse=55.69\tdev-mse=76.03\r\nep=42\tloss=47.11\tfit-mse=55.91\tdev-mse=75.43\r\nep=43\tloss=42.63\tfit-mse=54.87\tdev-mse=75.01\r\nep=44\tloss=54.36\tfit-mse=55.95\tdev-mse=76.42\r\nep=45\tloss=26.99\tfit-mse=55.46\tdev-mse=75.88\r\nep=46\tloss=76.64\tfit-mse=55.95\tdev-mse=75.45\r\nep=47\tloss=48.25\tfit-mse=56.55\tdev-mse=76.82\r\nep=48\tloss=54.42\tfit-mse=55.08\tdev-mse=74.42\r\nep=49\tloss=73.48\tfit-mse=55.05\tdev-mse=74.62\r\nep=50\tloss=43.22\tfit-mse=55.49\tdev-mse=75.26\r\nep=51\tloss=54.03\tfit-mse=54.87\tdev-mse=74.38\r\nep=52\tloss=66.74\tfit-mse=54.54\tdev-mse=73.11\r\nep=53\tloss=65.90\tfit-mse=55.99\tdev-mse=75.91\r\nep=54\tloss=58.84\tfit-mse=56.37\tdev-mse=75.97\r\nep=55\tloss=49.63\tfit-mse=55.29\tdev-mse=74.15\r\nep=56\tloss=48.78\tfit-mse=55.60\tdev-mse=74.83\r\nep=57\tloss=54.61\tfit-mse=54.00\tdev-mse=72.63\r\nep=58\tloss=47.21\tfit-mse=55.10\tdev-mse=74.48\r\nep=59\tloss=80.70\tfit-mse=54.28\tdev-mse=72.64\r\nep=60\tloss=66.65\tfit-mse=55.22\tdev-mse=74.33\r\nep=61\tloss=82.05\tfit-mse=53.16\tdev-mse=70.96\r\nep=62\tloss=51.54\tfit-mse=54.64\tdev-mse=73.51\r\nep=63\tloss=48.69\tfit-mse=56.23\tdev-mse=76.05\r\nep=64\tloss=92.59\tfit-mse=55.95\tdev-mse=74.36\r\nep=65\tloss=62.16\tfit-mse=55.92\tdev-mse=75.03\r\nep=66\tloss=46.01\tfit-mse=55.84\tdev-mse=75.34\r\nep=67\tloss=50.22\tfit-mse=56.25\tdev-mse=76.24\r\nep=68\tloss=92.09\tfit-mse=55.56\tdev-mse=73.48\r\nep=69\tloss=53.25\tfit-mse=55.99\tdev-mse=75.80\r\nep=70\tloss=71.81\tfit-mse=55.77\tdev-mse=75.19\r\nep=71\tloss=66.39\tfit-mse=55.28\tdev-mse=74.20\r\nep=72\tloss=54.23\tfit-mse=54.80\tdev-mse=74.16\r\nep=73\tloss=54.83\tfit-mse=55.40\tdev-mse=74.64\r\nep=74\tloss=40.68\tfit-mse=55.03\tdev-mse=73.95\r\nep=75\tloss=81.32\tfit-mse=54.96\tdev-mse=73.06\r\nep=76\tloss=50.36\tfit-mse=55.94\tdev-mse=75.51\r\nep=77\tloss=64.00\tfit-mse=56.08\tdev-mse=75.65\r\nep=78\tloss=70.95\tfit-mse=54.84\tdev-mse=74.51\r\nep=79\tloss=64.96\tfit-mse=53.87\tdev-mse=72.72\r\nep=80\tloss=56.27\tfit-mse=54.93\tdev-mse=74.10\r\nep=81\tloss=61.17\tfit-mse=55.11\tdev-mse=74.21\r\nep=82\tloss=60.23\tfit-mse=57.36\tdev-mse=77.07\r\nep=83\tloss=41.64\tfit-mse=55.80\tdev-mse=75.71\r\nep=84\tloss=38.13\tfit-mse=55.96\tdev-mse=75.40\r\nep=85\tloss=58.91\tfit-mse=57.40\tdev-mse=77.60\r\nep=86\tloss=47.32\tfit-mse=56.12\tdev-mse=75.42\r\nep=87\tloss=77.51\tfit-mse=55.13\tdev-mse=74.06\r\nep=88\tloss=75.37\tfit-mse=53.90\tdev-mse=72.67\r\nep=89\tloss=49.26\tfit-mse=55.25\tdev-mse=74.08\r\nep=90\tloss=91.33\tfit-mse=52.48\tdev-mse=70.72\r\nep=91\tloss=38.65\tfit-mse=52.77\tdev-mse=71.15\r\nep=92\tloss=43.26\tfit-mse=54.84\tdev-mse=73.87\r\nep=93\tloss=59.81\tfit-mse=55.12\tdev-mse=73.99\r\nep=94\tloss=60.65\tfit-mse=54.58\tdev-mse=73.89\r\nep=95\tloss=49.32\tfit-mse=54.81\tdev-mse=73.37\r\nep=96\tloss=37.07\tfit-mse=55.97\tdev-mse=75.03\r\nep=97\tloss=51.49\tfit-mse=55.03\tdev-mse=73.78\r\nep=98\tloss=32.92\tfit-mse=55.53\tdev-mse=75.13\r\nep=99\tloss=79.23\tfit-mse=54.59\tdev-mse=72.64\r\n\r\nMSE ON TRAINING:  58.21192624872478\r\n\r\nPREDICTIONS\r\n>> actual=15.0, predicted=22.958749771118164\r\n>> actual=10.0, predicted=23.622652053833008\r\n>> actual=9.0, predicted=23.67362403869629\r\n>> actual=25.0, predicted=23.90199851989746\r\n>> actual=19.0, predicted=23.206247329711914\r\n>> actual=14.0, predicted=23.894834518432617\r\n>> actual=14.0, predicted=23.627328872680664\r\n>> actual=13.0, predicted=24.387636184692383\r\n>> actual=18.0, predicted=23.90337562561035\r\n>> actual=35.0, predicted=24.2421932220459\r\n>> actual=25.0, predicted=24.03383445739746\r\n>> actual=19.0, predicted=24.37618064880371\r\n>> actual=13.0, predicted=23.93202781677246\r\n>> actual=28.0, predicted=24.228940963745117\r\n>> actual=13.0, predicted=23.033733367919922\r\n>> actual=14.0, predicted=23.603410720825195\r\n>> actual=15.0, predicted=23.061141967773438\r\n>> actual=13.0, predicted=23.7595157623291\r\n>> actual=18.0, predicted=23.451452255249023\r\n>> actual=12.0, predicted=23.622989654541016\r\nMSE ON TESTING: 63.039009816852854\r\n~~~\r\n\r\n**Describe the expected behavior**\r\n\r\nThe output of these models should be similar, or at least within the same ballpark.\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1DZk1DHuN7MlgkS56-Zgsceg0OkWJ3WEI?usp=sharing\r\n", "comments": ["@tttr222 \r\n\r\nI have tried in colab with TF version 2.2 and was able to reproduce the issue.However i am seeing same output with model(A) but i am seeing the error message with model(B).Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/b0def2b01f5def4c2c0ed12a7799a877/untitled98.ipynb).Thanks!", "Thanks for looking into this. I've revised the colab code, the issue is reproducible on tf-night-gpu now. \r\n\r\nPlease see [this colab](https://colab.research.google.com/drive/1DZk1DHuN7MlgkS56-Zgsceg0OkWJ3WEI?usp=sharing) for the revised version. ", "@tttr222 Can you please check [this issue](https://github.com/tensorflow/tensorflow/issues/32895) which is very similar to you but a classification problem. Check the code given in the end and update it according to your regression. Hope it helps. Thanks!", "Hey, I took a look at the the other bug report you mentioned. If I understand it correctly, the comments at the end suggest that decorating the training loop with @tf.function may lead to learning issues. However, I am not using @tf.function at all in model (B), where the problem occurs. Thanks", "@tttr222 Sorry, I gave you wrong link last time. Can you please update your code based on the [gist](https://colab.research.google.com/gist/NLP-ZY/3762cb58f789d36511f812edabe129aa/untitled0.ipynb) provided in [this issue](https://github.com/tensorflow/tensorflow/issues/35533#issuecomment-626128762).\r\n\r\nNeed to update and reset states as mentioned in that issue. For further question, please post them in Stackoverflow as there is a large community to support this kind of questions. GitHub is mainly for bugs and performance related issues. Thanks!", "Hi @jvishnuvardhan , the problem looks similar but I'm not sure that it's the same. You would run reset_state() with an instance of tf.metrics.SparseCategoricalAccuracy() or similar, and it is for metric monitoring, right? \r\n\r\nThe code I posted does not use the built-in metric monitoring, and does not use anything from tf.metrics, so it seems that there is nothing to reset. Could you take a second look and let me know? Thanks for your help. ", "Was able to replicate the issue in TF v2.7.0,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/c027b09dc419cba749e324238dd5e008/untitled130.ipynb)..Thanks !", "Hi, is there any news about this? I have an even stranger issue. I have a simple face detector SSD model, and training with Model.fit vs. GradientTape not only behaves different. The Model.fit version simple doesn't work, validation loss doesn't decrease. On the other hand the GradientTape version predicts pretty well the faces after only 20 epochs. You can check it out here: https://colab.research.google.com/drive/1LvahE1CBuvvxAi7PT0gqcSrFYPDBEJmY?usp=sharing", "any updates on this issue?"]}, {"number": 41216, "title": "Spectral Normalization", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): TF 2.2\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nNeed to support Spectral Normalization as a tf.keras.layers.Wrapper to wrap dense and convolution layers.\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\nSpectral Normalization (https://arxiv.org/abs/1802.05957) is widely used for training generative adversarial networks. Many recent research use it and it is supported in pytorch ([torch.nn.utils.spectral_norm](https://pytorch.org/docs/master/generated/torch.nn.utils.spectral_norm.html)) for a long time now, but still no supported in tensorflow.\r\n\r\n**Any Other info.**\r\nThe main difficulty in implementing Spectral Normalization is that it needs to update the layer kernel (e.g. the kernel of tf.keras.layers.Conv2D) using tf.Variable.assign() which is not permitted within a Replica context (e.g. building a convolution layer under tf.distribute.MirroredStrategy).\r\n", "comments": ["can we just treat spectral normalization as one layer in the graph? so we don't need to worry about modifying the previous layers", "> can we just treat spectral normalization as one layer in the graph? so we don't need to worry about modifying the previous layers\r\n\r\nThis is what I've done for now. I've extended tf.keras.layers.Conv2D and tf.keras.layers.Dense directly. This way I need to store the value of a single variable used for normalization, and this variable is created with `aggregation=tf.VariableAggregation.ONLY_FIRST_REPLICA`, since its value doesn't depend on different minibatches to different replicas."]}, {"number": 41189, "title": "Autograph applied to Keras Custom Loss during Eager Execution", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, x64\r\n- TensorFlow installed from (source or binary): binary (pip)\r\n- TensorFlow version (use command below): TF 2.4.0.dev20200707\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: 10.1 / 7.6\r\n- GPU model and memory: Bug appears on several computers with different GPU\r\n\r\n\r\n**Describe the current behavior**\r\nTensorflow applies AutoGraph to keras custom loss even in eager execution, meaning that we can't debug the loss anymore (unless using tf.print). This did not happen in previous versions of Tensorflow.\r\nNotice that it both happens when run_eagerly is set to True in model.compile() and when tf.config.run_functions_eagerly is set to True.\r\n\r\n**Describe the expected behavior**\r\nWhen run_eagerly=True is passed to the model during compilation, we should expect Tensorflow to run eagerly in the loss function.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\n# Custom Model. Autograph is not applied in eager execution so debugging is possible.\r\nclass CustomModel(keras.models.Model):\r\n\tdef __init__(self):\r\n\t\tsuper(CustomModel, self).__init__()\r\n\t\tself.layer = tf.keras.layers.Dense(3) # Can debug here\r\n\r\n\tdef call(self, inputs, training=None, mask=None):\r\n\t\tx = self.layer(inputs) # Can debug here\r\n\t\treturn x\r\n\r\n# Custom Loss. AutoGraph is applied in eager execution so debugging is impossible.\r\nclass CustomLoss(keras.losses.Loss):\r\n\tdef call(self, y_true, y_pred):\r\n\t\tx = tf.reduce_mean(tf.abs(y_pred-y_true)) # Cannot debug here\r\n\t\treturn x\r\n\r\nif __name__ == '__main__':\r\n\tdata = np.random.random((1000, 3)).astype(np.float32)\r\n\r\n\tmodel = CustomModel()\r\n\r\n\tmodel.compile(loss=CustomLoss(), run_eagerly=True)\r\n\tmodel.fit(x=data, y=data, batch_size=32)\r\n```\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@MrManiaLive \r\nI ran the code shared, loss in [tf 2.2 is as per this gist](https://colab.research.google.com/gist/Saduf2019/fe531dd75ea910a954ac8525a6f2073e/untitled267.ipynb), and loss in [tf_nightly is as per this gist](https://colab.research.google.com/gist/Saduf2019/9dcccd4deaaf83c5e03076e16a9bff79/untitled268.ipynb).\r\nPlease let us know if this confirms your issue.", "@MrManiaLive can you also explain how you can tell that it's not running eagerly? With tf nightly I am able to print `tf.executing_eagerly()` within the loss function, which returns as `True`. Some more information on what debugging you are not able to do now would be useful.", "> @MrManiaLive can you also explain how you can tell that it's not running eagerly? With tf nightly I am able to print `tf.executing_eagerly()` within the loss function, which returns as `True`. Some more information on what debugging you are not able to do now would be useful.\r\n\r\nWithin Pycharm, when I put a breakpoint inside the call method of the custom loss function, the program does not stop (whereas it does in the custom model). I though this meant that the loss function was autographed. I am left with tf.print() for debugging.\r\n\r\nNotice that I am using the latest version of tf-nightly-gpu (tf 2.4). I remember not having this issue with tf 2.2.", "I'm not very familiar with Pycharm so I cannot speak to that, but I don't see any signs that the loss function is not running eagerly when using the nightly gist provided by @Saduf2019 . I can also print `x.numpy` within the loss function and that works fine when `run_eagerly=True`\r\n", "You're right. I tried to print x.numpy in the call method and it worked. I also printed tf.executing_eagerly() and it returned True. However, I still can't stop in the call method within Pycharm.\r\n\r\nI am investigating the code.\r\nIn the `__call__` method of the parent class `Loss` `autograph.tf_convert()` is called.\r\n\r\nAnd I have `ctx.status` equal to `ag_ctx.Status.UNSPECIFIED`, which creates a wrapper_factory.\r\nIt then calls `converted_call `in the `api.py` file and runs these lines:\r\n\r\n  ```\r\ntry:\r\n    program_ctx = converter.ProgramContext(\r\n        options=options, autograph_module=tf_inspect.getmodule(converted_call))\r\n    converted_f = conversion.convert(target_entity, program_ctx)\r\n    if logging.has_verbosity(2):\r\n      _log_callargs(converted_f, effective_args, kwargs)\r\n  except Exception as e:  # pylint:disable=broad-except\r\n    logging.log(1, 'Error transforming entity %s', target_entity, exc_info=True)\r\n    if is_autograph_strict_conversion_mode():\r\n      raise\r\n    return _fall_back_unconverted(f, args, kwargs, options, e)\r\n\r\n  with StackTraceMapper(converted_f), tf_stack.CurrentModuleFilter():\r\n    try:\r\n      if kwargs is not None:\r\n        result = converted_f(*effective_args, **kwargs)\r\n      else:\r\n        result = converted_f(*effective_args)\r\n    except Exception as e:\r\n      _attach_metadata(e, converted_f)\r\n      raise\r\n```\r\n\r\nWhile `target_entity `is my `CustomLoss.call`, `converted_f `is a `<function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x000001F5A2D6B558>`\r\n\r\nDo you think this could be the source of my problem?", "Hi @MrManiaLive, sorry for the delay. I'm not an expert with Pycharm, you might want to submit a request on [official Pycharm forum](https://youtrack.jetbrains.com/issues/PY?_ga=2.9618933.1109719345.1596474586-855270654.1596474586)\r\nIf you are still unable to figure it out at that point, please update this thread and we can investigate if this is an issue on the TF side.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41189\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41189\">No</a>\n", "I'm encountering the same issue.  PyCharm breakpoints are recognized in TF2.2 but some breakpoints are not recognized in TF2.3.  Here is my minimal working example of the problem.   Although I have not explicitly tested different Python IDEs (e.g., VSCode), I believe this problem will occur in those other IDE with interactive debuggers.\r\n```\r\nimport tensorflow as tf\r\n\r\ntf.config.experimental_run_functions_eagerly(True)\r\n\r\n\r\nclass MyLoss(tf.keras.losses.Loss):\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n    def call(self, y_true, y_pred):\r\n        mae = tf.abs(y_true - y_pred)     # set breakpoint here, debugger stops for tf2.2 only\r\n                                          # breakpoint not recognized in tf2.3\r\n        return mae\r\n\r\n\r\n@tf.function\r\ndef test_debug_breakpoint(x, y):\r\n    mae = tf.abs(x - y)        # set breakpoint here, debugger stops for both tf2.2 and tf2.3\r\n    return tf.reduce_mean(mae)\r\n\r\n\r\nprint(\"tf version:\", tf.__version__)\r\n\r\nt1 = tf.random.normal([10, 5], dtype=tf.float32)\r\nt2 = tf.random.normal([10, 5], dtype=tf.float32)\r\n\r\n# test user function\r\nans = test_debug_breakpoint(t1, t2)\r\nprint('test_debug_breakpoint:',ans.numpy())\r\n\r\n# test custom Loss function\r\nmy_loss = MyLoss()\r\nans = my_loss(t1, t2)\r\nprint('custom loss function:', ans.numpy())\r\n\r\n```\r\n\r\nWith the help of a colleague found reference to this experimental function: [`tf.autograph.experimental.do_not_convert()`](https://www.tensorflow.org/api_docs/python/tf/autograph/experimental/do_not_convert).  By using this  decorator as shown below, the unrecognized breakpoint is now recognized by the PyCharm debugger.\r\n```\r\nclass MyLoss(tf.keras.losses.Loss):\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n    @tf.autograph.experimental.do_not_convert   # breakpoint now recognized with this decorator\r\n    def call(self, y_true, y_pred):\r\n        mae = tf.abs(y_true - y_pred)     # set breakpoint here, debugger stops for tf2.2\r\n                                          # debugger stops at this breakpoint in tf2.3\r\n        return mae\r\n```\r\n\r\nIn TF 2.2, it was possible to recognize  breakpoints, regardless of location, with only the use of this function: [`tf.config.experimental_run_functions_eagerly(True)`](https://www.tensorflow.org/versions/r2.2/api_docs/python/tf/config/experimental_run_functions_eagerly)\r\n\r\nIn TF 2.3, it appears I need to know use both `tf.config.run_functions_eagerly(True)` and the `tf.autograph.experimental.do_not_convert()` functions to allow PyCharm to recognize all breakpoints.  From my perspective having to use `tf.autograph.experimental.do_not_convert()` is redundant. \r\n\r\n* Is there documentation on how to use the two functions?\r\n* Is there guidance on how we can use interactive debuggers with TF 2.3?\r\n\r\nAny guidance will be appreciated.\r\n", "Hi @jimthompson5802 thanks for providing the code and also a workaround. I am reopening this issue for further investigation.", "This looks like a bug. When calling `tf.config.run_functions_eagerly(True)`, AutoGraph should not be called at all. We can confirm this by calling `tf.autograph.set_verbosity(3, True)`, which results in this log entry:\r\n\r\n```\r\nINFO:tensorflow:Converted call: <bound method MyLoss.call of <__main__.MyLoss object at 0x7f6bfdead198>>\r\n```\r\n\r\nThat log entry is unexpected.\r\n\r\n@tomerk @omalleyt12 this looks like a regression, could you investigate the Keras calling site? The behavior of `call` should be consistent with that of `test_debug_breakpoint`.\r\n\r\n@jimthompson5802 another workaround could be to decorate `call` with `@tf.function`, although that might have other unintended side effects.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "I have a PR I'm working on to fix this", "Just checking in.  Any progress on fixing the issue?", "Was able to replicate the issue in TF v2.5,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/daeec88c51ccca7e2f08880c612caa0d/untitled129.ipynb)..Thanks !", "I am facing the same issue. Is this resolved?"]}, {"number": 41169, "title": "failed to query event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure tensorflow/stream_executor/dnn.cc:613] CUDNN_STATUS_INTERNAL_ERROR", "body": "\r\nI am unable to train my model. I get the below error after around the 100th epoch or so (randomly). Sometimes it fails on the 500th or so epoch. There are a few times I won't get this error.\r\n\r\nPython Version: 3.7\r\nTesorflow Version: 2.3\r\nTensorflow Installed: From binary.\r\nOS: Windows 10\r\nGPU Card: Nvidia Titan V\r\nCUDA: 10.1\r\nCUDNN: 7.6.5.32\r\n\r\n: failed to query event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n2020-07-07 17:15:37.551112: E tensorflow/stream_executor/dnn.cc:613] CUDNN_STATUS_INTERNAL_ERROR\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1867): 'cudnnRNNForwardTraining( cudnn.handle(), rnn_desc.handle(), model_dims.max_seq_length, input_desc.handles(), input_data.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), rnn_desc.params_handle(), params.opaque(), output_desc.handles(), output_data->opaque(), output_h_desc.handle(), output_h_data->opaque(), output_c_desc.handle(), output_c_data->opaque(), workspace.opaque(), workspace.size(), reserve_space.opaque(), reserve_space.size())'\r\n2020-07-07 17:15:37.551176: F tensorflow/stream_executor/cuda/cuda_dnn.cc:189] Check failed: status == CUDNN_STATUS_SUCCESS (7 vs. 0)Failed to set cuDNN stream.\r\n\r\n\r\n\r\n<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@nectario,\r\nCould you please provide the complete code to reproduce the issue reported here?\r\n\r\nAlso, please take a look at [this comment](https://github.com/tensorflow/tensorflow/issues/35950#issuecomment-577427083) from a similar issue and let us know if it helps. Thanks!", "I can't post the code since it's a proprietary model. It seems this is related to NVIDA drivers. This happens RANDOMNLY. Last night it happened on epoch 2500 for example.", "My model is a 6 layer LSTM.", "I really do not want to downgrade my NVIDIA GPU driver in order for this issue to go away. It's becoming impossible to train a model till completion because of this issue.", "> I can't post the code since it's a proprietary model.\r\n\r\n@nectario,\r\nAs an alternative, is it possible for you to provide a working example which can mimic the error? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Yes, I will soon be posting sample code that recreates this issue.", "This is an extremely serious issue. It prevents me from training a model till the end.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Hi,\r\n\r\nHere is the code that recreates this issue. I was able to make the error happen twice, although it happens randomly.\r\n\r\nFirst time it happened at epoch **1548** \r\nSecond time it happened at epoch **524**\r\n\r\n**To Download:**\r\n[http://www.nektron.com/cudnn_error.zip](url)\r\n\r\n**To Run:**\r\nunzip cudnn_error.zip\r\n`python CUDNN_Error.py`\r\n\r\nI have **tensorflow 2.3** \r\nGPU is the **Nvidia Titan V**\r\nI am running this on **Windows 10**\r\n\r\n**Note:** you need to have it run for 3000 epochs and it will happen as some point.\r\n\r\n", "Please note, in the code you will see a Callback that deliberately tries to load data \"on_epoch_end\"  just to add a bit of a delay that mimics my own callbacks.", "@nectario,\r\nI was able to run the code without any issues with TF v2.3. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/0c5687807b21b53df5942c83c4fb5e62/41169.ipynb).\r\n\r\nCould you please try setting a hard limit on the total GPU memory as show in [this guide](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) and check if it works. Thanks!", "@amahendrakar Thank you. I will try your suggestion. Looking at how you ran it, I don't believe this is repeatable in a server GPU like the V100 or any other ones. It really needs to run in a consumer GPU like the Titan V, and in a PC desktop environment. As of now, I can never finish training so it reaches the last epoch. I think it's related to the Nvidia driver.\r\n\r\nAlso, this gets triggered quicker if I am watching a YouTube video as I am training. I started getting this only after Nvidia updated their drivers.\r\n\r\nHere is a person who also experienced a similar issue. Check this link:\r\nhttps://forums.developer.nvidia.com/t/crash-on-training-cuda-error-launch-failed/76055\r\n\r\nBut it seems it was hardware memory issue which needed a GPU replacement?\r\n", "If someone can run the above in their local desktop environment with the latest Nvidia gpu drivers would be great.", "I keep getting this issue, in different variations. Very rarely am I able to complete training. It's extremely frustrating...\r\n```\r\n2020-09-01 17:05:07.819801: E tensorflow/stream_executor/dnn.cc:616] CUDNN_STATUS_EXECUTION_FAILED\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1892): 'cudnnRNNForwardTraining( cudnn.handle(), rnn_desc.handle(),\r\n model_dims.max_seq_length, input_desc.handles(), input_data.opaque(), input_h_desc.handle(), input_h_data.opaque(), \r\ninput_c_desc.handle(), input_c_data.opaque(), rnn_desc.params_handle(), params.opaque(), output_desc.handles(), output_data-\r\n>opaque(), output_h_desc.handle(), output_h_data->opaque(), output_c_desc.handle(), output_c_data->opaque(), \r\nworkspace.opaque(), workspace.size(), reserve_space.opaque(), reserve_space.size())'\r\n\r\n2020-09-01 17:05:07.819965: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query \r\nevent: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n\r\n2020-09-01 17:05:07.861316: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at cudnn_rnn_ops.cc:1517 : \r\nInternal: Failed to call ThenRnnForward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , \r\n[num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 300, 150, 1, 20, 620, 150]\r\n\r\n2020-09-01 17:05:07.873534: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:220] Unexpected Event status: 1\r\n\r\n```\r\n", "Is this some kind of hardware issue I am facing with my Titan V? Today I was only able to have one successful run.", "I have made the following observation:\r\nWhen my room temperature is 73F or below, the problem does not seem to happen. Generally, on average, my room temperature is 78F. \r\nCould this be an overheating issue of the GPU?", "I think I may have finally resolved this issue. It seems I had two CUDA versions installed: 10.1, and 10.2. My installation was pointing to the 10.2 version. After uninstalling all the other other versions and having only 10.1 I was able to finally train more than once without issues. I will run a couple more tests. Bottom line: After uninstalling all my previous CUDA/CUDNN versions and reinstalling the correct ones, it seems to have resolved the issue.", "I had this same issue and read all the comments here. I recognized that a pip install software I made updated my tensorflow to 2.3 which is not even a stable build that conda supports. But my tensorflow-gpu version stayed at 1.14. \r\nI realized this and forces both my tensorflow and tensorflow-gpu versions to 2.1. This solved my issue. ", "Thank you. Apparently this issue is still not gone for me. It is extremely frustrating. I cannot train till completion. TensorFlow is becoming unusable for me... Even using Python 3.8 with a fresh install I still get this...", "Thank you too, I have been running for 20 minutes without a crash so far. Try Python 3.7 with tensorflow 2.1 and tensorflow-gpu 2.1 :) \r\n", "I hate downgrades, but I will give it a shot!", "Good luck! ", "@nectario Confirming, got an error with CUDNN_Error.py at epoch 1683. Python 3.8, TF 2.3.1, CUDA 10.2, RTX 2070S.\r\n\r\nAlthough a bit different error:\r\n```\r\n2020-10-28 11:31:23.136550: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n2020-10-28 11:31:23.136714: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:220] Unexpected Event status: 1\r\n2020-10-28 11:31:23.136712: E tensorflow/stream_executor/dnn.cc:616] CUDNN_STATUS_EXECUTION_FAILED\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1892): 'cudnnRNNForwardTraining( cudnn.handle(), rnn_desc.handle(), model_dims.max_seq_length, input_desc.handles(), input_data.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), rnn_desc.params_handle(), params.opaque(), output_desc.handles(), output_data->opaque(), output_h_desc.handle(), output_h_data->opaque(), output_c_desc.handle(), output_c_data->opaque(), workspace.opaque(), workspace.size(), reserve_space.opaque(), reserve_space.size())'\r\n```\r\nBy  the way, switching between CUDA versions in Win is easy without uninstall&reinstall. NVIDIA drivers are backwards compatible with CUDA 10.x and 11.x, so one can always have the latest drivers installed. Just have to make sure to install CUDA & CuDNN separately. nvidia-smi reported CUDA version output is \"irrelevant\". To switch, I'm using env cmds:\r\n```\r\nSET CUDA_PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\r\nSET PATH=%CUDA_PATH%\\bin;%PATH%\r\nSET PATH=%CUDA_PATH%\\extras\\CUPTI\\libx64;%PATH%\r\nSET PATH=C:\\cudnn-7.6.5\\10.1\\bin;%PATH%;\r\n```\r\n\r\nTraining the same example with CUDA 10.1 in progress.\r\n\r\nIf I get time, will try again the CUDA 10.2 run with `SET CUDA_LAUNCH_BLOCKING=1` to see if there's any extra bits coming out.", "Wow, yeap, the error message would also change a bit sometimes for me. A few times I am able to complete an entire training run out of pure luck, but 90% of the time it fails. It can fail on epochs 50, 300, 800, 1100 -- it can be anywhere really. \r\n\r\nLet me know how the 10.1 training goes.\r\n\r\nThanks", "CUDA 10.1 training succeeded. Might be worth waiting a few weeks for the TF 2.4 release with CUDA 11 support and would be fun to try again. Don't know if it's going to be CUDA 11.0 or 11.1.\r\n\r\nTo be fair, TensorFlow >=2.1.0 officially supports only CUDA 10.1, in that sense works as expected.\r\n\r\n Btw, my GPU utilization was quite low with this training (batch size was optimal for max gpu memory use), so the error was not coming from power consumption and definitely not from overheating.", "Thank you. For me it happens with CUDA 10.1. Oh well... At least I now feel confident it is not a hardware issue.", "@nectario I did run the same model training on the latest TF 2.4 release candidate (r2.4 branch at 541fd26bd37) with CUDA 11.0 and CuDNN 8.0.4.30. For some reason this same TF code did not compile for CUDA 11.1.\r\n\r\nUnfortunately failed again at epoch 2720. CUDA utilization perfectly fine, ~80%, GPU temperature ~70C.\r\n\r\n```\r\n2020-10-29 09:41:11.513784: E tensorflow/stream_executor/dnn.cc:616] CUDNN_STATUS_EXECUTION_FAILED\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1968): 'cudnnRNNBackwardData( cudnn.handle(), rnn_desc.handle(), model_dims.max_seq_length, output_desc.handles(), output_data.opaque(), output_desc.handles(), output_backprop_data.opaque(), output_h_desc.handle(), output_h_backprop_data.opaque(), output_c_desc.handle(), output_c_backprop_data.opaque(), rnn_desc.params_handle(), params.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), input_desc.handles(), input_backprop_data->opaque(), input_h_desc.handle(), input_h_backprop_data->opaque(), input_c_desc.handle(), input_c_backprop_data->opaque(), workspace.opaque(), workspace.size(), reserve_space_data->opaque(), reserve_space_data->size())'\r\n2020-10-29 09:41:11.513855: F tensorflow/stream_executor/cuda/cuda_dnn.cc:189] Check failed: status == CUDNN_STATUS_SUCCESS (7 vs. 0)Failed to set cuDNN stream.\r\n2020-10-29 09:41:11.514110: F .\\tensorflow/core/kernels/conv_2d_gpu.h:1019] Non-OK-status: GpuLaunchKernel( SwapDimension1And2InTensor3UsingTiles<T, kNumThreads, kTileSize, kTileSize, conjugate>, total_tiles_count, kNumThreads, 0, d.stream(), input, input_dims, output) status: Internal: unspecified launch failure\r\n```\r\n", "I see. It is interesting that your failures occur close to the 3000 epochs. In my case, it almost always happens below 1200.", "This really needs to get fixed, LSTM's can't be run with Cuda now.", "Absolutely. It is even preventing another issue I reported to be debugged because of this issue. It's this one #45676\r\n", "This issue belongs in a range of issues related to LSTMs and CUDA/CUDNN. Each time the error is a bit different but same effect.", "Probably NVIDIA needs to be involved also for this to be fixed.", "When I train on AWS on their V100s, this never happens. Seems to be happening mainly with the consumer based graphics cards.", "> This issue belongs in a range of issues related to LSTMs and CUDA/CUDNN. Each time the error is a bit different but same effect.\r\n\r\nYeah, I get all of them, sometimes this Cuda error, sometimes the rnnbackward or rnnforward error. The strange thing is that it only occurred after I had swapped out my motherboard and CPU and reinstalled my windows. \r\n\r\nI downloaded the same drivers as before, did a complete reinstall of my conda environment, but somehow I got this issue in the process. ", "I know!! I have been living with this for way too long. More than six months... Do you get this with 2.4?", "With 2.4 it seems to have made it better, but training got slower.", "> I know!! I have been living with this for way too long. More than six months... Do you get this with 2.4?\r\n\r\nYep :( I had bought a couple of months ago a RTX 3080, was using TF 2.4 RC0 up to the final release with no issues. Could run LSTM's without a problem, but after swapping my cpu and motherboard and reinstalling windows, I can't seem to get around this problem.\r\n\r\nI even reinstalled windows another time, but still getting the issue. So maybe it's also hardware related...", "Yeah. I suspect it's a combination of gfx driver/tensorflow/cuda/cudnn. Was the gfx driver updated around that time?\r\n", "This issue needs tensorflow heavyweights to look into it! The credibility of the entire framework is in line! I am this close to completely switching frameworks!", "> Yeah. I suspect it's a combination of gfx driver/tensorflow/cuda/cudnn. Was the gfx driver updated around that time?\r\n\r\nNo, I simply reinstalled all the same drivers so that everything was in the same condition.", "Ooh, I see...", "It's surely seems there may be some hardware issue because it never happens with the V100s.", "This requires TensorFlow and NVIDA to look into it...", "> It's surely seems there may be some hardware issue because it never happens with the V100s.\r\n\r\nProbably a combination of multiple things. It is strange that it did not occur for me before though.\r\n\r\n> This requires TensorFlow and NVIDA to look into it...\r\n\r\nDefinitely, but have they have even acknowledged this issue yet? It seems that Tensorflow contributors keep closing all these tickets.\r\n\r\nThe only thing which allows me to train on LSTMs now is to just train on cpu. I am currently also trying to disable eager mode with the gpu, so that it stops using the Cudnn kernel. Perhaps that is a good enough fix for now...", "I believe it's been acknowledged but this issue is unique as it's hard to reproduce. If they try to reproduce this on any cloud machine it will never happen. It requires these consumer based graphics cards. Like I have a Titan V or an RTX 3080 which you have. It's good that we are keeping this thread \"warm\"\r\n\r\nI hope someone from TensorFlow can update us here if they are investigating it on what is causing it.\r\n\r\nMy resolution for now: I reduced the number of epochs. When it fails, I load the last saved weights and continue on.\r\n", "> I believe it's been acknowledged but this issue is unique as it's hard to reproduce. If they try to reproduce this on any cloud machine it will never happen. It requires these consumer based graphics cards. Like I have a Titan V or an RTX 3080 which you have. It's good that we are keeping this thread \"warm\"\r\n> \r\n> I hope someone from TensorFlow can update us here if they are investigating it on what is causing it.\r\n> \r\n> My resolution for now: I reduced the number of epochs. When it fails, I load the last saved weights and continue on.\r\n\r\nAh I see, let's hope they are looking into it and they are able to reproduce it. I'll try if just disabling the cudnn lstm kernel is enough. I'll give an update if it works! ", "@nectario So disabling eager execution helps, because it's not using the cudnn kernel for lstm anymore.", "@nectario I'm curious, which cudnn versions did you test with when running with the TF 2.4 final? There's now 8.0.3, 8.0.4.30 and 8.0.5.39, all having CUDA 11.0 support. I presume the same crash still happened with the 2.4 as well, not just the slowdown issue?", "> @nectario I'm curious, which cudnn versions did you test with when running with the TF 2.4 final? There's now 8.0.3, 8.0.4.30 and 8.0.5.39, all having CUDA 11.0 support. I presume the same crash still happened with the 2.4 as well, not just the slowdown issue?\r\n\r\nI tried all of those versions, but no avail. All of them showed the same behaviour. ", "> @nectario I'm curious, which cudnn versions did you test with when running with the TF 2.4 final? There's now 8.0.3, 8.0.4.30 and 8.0.5.39, all having CUDA 11.0 support. I presume the same crash still happened with the 2.4 as well, not just the slowdown issue?\n\nI initially tested with the latest versions 8.0.5.39 and then went back to the advertised versions.\ufffc Same issue. \n", "@nectario I ran your cudnn_error.zip through my setup and at least the 1st run worked fine. Although, I did do a couple of optimizations to speed it up.\r\n\r\nStarted with ~487ms/step, 3s epoch.\r\nAdded to the beginning of the CUDNN_Error.py to enable mixed precision and changed all LSTM units from 150 to 152 to be more friendly for the mixed precision (n%8=0)\r\n```\r\nfrom tensorflow.keras.mixed_precision import experimental as mixed_precision\r\npolicy = mixed_precision.Policy('mixed_float16')\r\nmixed_precision.set_policy(policy)\r\n```\r\n\r\nWent down to ~280ms/step. After enabling mixed precision I could also double the batch size from 620 to 1240 and went further down to ~440ms/step (for previous batch size normalization: 220ms/step). Each epoch down to 1s.\r\n\r\nThere might be room for XLA-based optimizations, but I'm getting `SubProcess ended with return code: 0` from XLA.\r\n\r\nWin10, Graphics driver 460.89, CUDA 11.0.2, cudnn-8.0.5.39.", "@ahtik Thank you! I will try those changes, except the batch change. The batch was carefully selected to yield better metrics. A bigger batch does not necessarily give me better metric performance.\r\n\r\nI wonder if this explains the drop in performance in 2.4?", "> @nectario I ran your cudnn_error.zip through my setup and at least the 1st run worked fine. Although, I did do a couple of optimizations to speed it up.\r\n> \r\n> Started with ~487ms/step, 3s epoch.\r\n> Added to the beginning of the CUDNN_Error.py to enable mixed precision and changed all LSTM units from 150 to 152 to be more friendly for the mixed precision (n%8=0)\r\n> \r\n> ```\r\n> from tensorflow.keras.mixed_precision import experimental as mixed_precision\r\n> policy = mixed_precision.Policy('mixed_float16')\r\n> mixed_precision.set_policy(policy)\r\n> ```\r\n> \r\n> Went down to ~280ms/step. After enabling mixed precision I could also double the batch size from 620 to 1240 and went further down to ~440ms/step (for previous batch size normalization: 220ms/step). Each epoch down to 1s.\r\n> \r\n> There might be room for XLA-based optimizations, but I'm getting `SubProcess ended with return code: 0` from XLA.\r\n> \r\n> Win10, Graphics driver 460.89, CUDA 11.0.2, cudnn-8.0.5.39.\r\n\r\n@nikitamaia I wonder if the above solution is what is needed to fix the slow performance for #45676?\r\n", "@nikitamaia @ahtik \r\n\r\nAdding your code @ahtik at the start, dropped my per epoch time to 1 second!\r\n\r\nfrom tensorflow.keras.mixed_precision import experimental as mixed_precision\r\npolicy = mixed_precision.Policy('mixed_float16')\r\nmixed_precision.set_policy(policy)", "@nectario To be honest, I suspect the 2.3 vs 2.4 slowdown in #45676 difference is still somewhere else because mixed precision has never been enabled by default, so it's an extra improvement on top of whatever else is going on.", "> @nectario To be honest, I suspect the 2.3 vs 2.4 slowdown in #45676 difference is still somewhere else because mixed precision has never been enabled by default, so it's an extra improvement on top of whatever else is going on.\r\n\r\nGot it. ", "@nectario TF2.3 and Cuda 10.1 and cudnn 7.6.5 don't cause any crashes with the cudnn accelerated LSTMs for me, but the only downside with an RTX 3080 is that the initial loading takes up to 15 min! \ud83d\udc4e So also not really a solution. I even tried to install older versions of windows, but also no success. Something within cudnn 8+ and cuda 11+ is causing this.", "@ion-elgreco Could you share your minimim LSTM code that is crashing with \nTF2.4? I was no longer able to reproduce the crash with the setup described \nabove with RTX2070S. I did enable mixed precision to speed it up, so maybe \ncudnn didn't get enough stress load... How long it takes in your case to \ncrash it?\n", "> @ion-elgreco Could you share your minimim LSTM code that is crashing with TF2.4? I was no longer able to reproduce the crash with the setup described above with RTX2070S. I did enable mixed precision to speed it up, so maybe cudnn didn't get enough stress load... How long it takes in your case to crash it?\r\n\r\n```\r\ndef build_base(model_name, units):\r\n    model = Sequential(name=model_name)\r\n    model.add(layers.InputLayer(input_shape=(X_train.shape[1], X_train.shape[2])))\r\n    model.add(layers.LSTM(units, return_sequences=True,))\r\n    model.add(layers.Dropout(0.4))\r\n    model.add(layers.LayerNormalization())\r\n    model.add(layers.Dense(7, activation=\"softmax\", name=\"Dense_Output\"))\r\n    model.compile(\r\n        optimizer=\"rmsprop\",\r\n        loss=CategoricalCrossentropy(),\r\n        metrics=[\"accuracy\"]\r\n    )\r\n    return model\r\n\r\nhistory = NN.fit(\r\n                X_train,\r\n                y_train,\r\n                batch_size=batchsize,\r\n                sample_weight=train_samples_weights[:train_div],\r\n                validation_data=(X_val, y_val),\r\n                epochs=5,\r\n                verbose=1,\r\n                shuffle=True,\r\n            )\r\n```\r\n\r\n\r\nIt will already crash with 5 units after 3 epochs...\r\n\r\nPS\r\nI have tried all cuda 11+ with all cudnn 8+ versions. Nothing helps.", "@ion-elgreco Can you provide the full code together with batchsize etc hyper parameters, train & val datasets etc. Otherwise, it's a bit too much effort to reproduce.\r\n\r\nDoes it also crash after adding this to the beginning of the file?\r\n```\r\nfrom tensorflow.keras.mixed_precision import experimental as mixed_precision\r\npolicy = mixed_precision.Policy('mixed_float16')\r\nmixed_precision.set_policy(policy)\r\n```\r\nIn case of mixed precision, also worth testing with 8 units instead of 5 for better performance.", "> @ion-elgreco Can you provide the full code together with batchsize etc hyper parameters, train & val datasets etc. Otherwise, it's a bit too much effort to reproduce.\r\n> \r\n> Does it also crash after adding this to the beginning of the file?\r\n> \r\n> ```\r\n> from tensorflow.keras.mixed_precision import experimental as mixed_precision\r\n> policy = mixed_precision.Policy('mixed_float16')\r\n> mixed_precision.set_policy(policy)\r\n> ```\r\n\r\nThis is the code: https://github.com/ion-elgreco/thesis-FER/blob/main/models.py\r\n\r\nHowever, I cannot share the datasets because of a legal agreement. The thing is, I hadn't changed a bit to this code. It worked fine before on my i7 7700k + rtx 3080, but after upgrading to a 5800x processor and having to reinstall windows, I keep getting this issue. I even tried reinstalling windows, 5 times..\r\n\r\nI already tried the mixed_float16 policy, but this makes everything slower in addition it still crashes.", "@ion-elgreco I'm not aware of a good reason how the training could become slower after enabling mixed precision... Initial graph building, yes.\r\n\r\nDo I understand correctly that you're also running TF2.3 right now inside this new desktop with a Ryzen CPU, and just switching between active CUDA version? So the TF2.3 is not crashing and TF2.4 does with the **exact** same hw and software conf.\r\n\r\nCouldn't rule out the HW issue as well, this one is from pytorch, but still relevant. https://discuss.pytorch.org/t/runtimeerror-cudnn-status-internal-error-when-l-run-the-program-for-a-second-time/2960/4\r\n\r\nAs there can be many causes for CUDNN_STATUS_INTERNAL_ERROR, the only pragmatic way to help here, is to also provide the dataset (processed enough to not cause the legal issues), that would be as small and compact as possible to reproduce the issue with TF2.4. Also include the affected version info etc. Might be worth opening as a separate issue, as the cause seems to be different from the one from @nectario.\r\n\r\nI noticed a comment of using conda. To be 100% clean, I'd definitely try to reproduce this also with a clean venv install with pip. Smth like\r\n```\r\ncd projdir\r\n\\python38\\python.exe -m venv .venv\r\n.\\venv\\Scripts\\activate or .\\venv\\Scripts\\Activate.ps1 for powershell\r\npip install -U pip\r\npip install tensorflow\r\npython -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])));print(tf.config.list_physical_devices('GPU'))\"\r\npython yourtrainging.py\r\n```", "> @ion-elgreco I'm not aware of a good reason how the training could become slower after enabling mixed precision... Initial graph building, yes.\r\n> \r\n> Do I understand correctly that you're also running TF2.3 right now inside this new desktop with a Ryzen CPU, and just switching between active CUDA version? So the TF2.3 is not crashing and TF2.4 does with the **exact** same hw and software conf.\r\n> \r\n> Couldn't rule out the HW issue as well, this one is from pytorch, but still relevant. https://discuss.pytorch.org/t/runtimeerror-cudnn-status-internal-error-when-l-run-the-program-for-a-second-time/2960/4\r\n> \r\n> As there can be many causes for CUDNN_STATUS_INTERNAL_ERROR, the only pragmatic way to help here, is to also provide the dataset (processed enough to not cause the legal issues), that would be as small and compact as possible to reproduce the issue with TF2.4. Also include the affected version info etc. Might be worth opening as a separate issue, as the cause seems to be different from the one from @nectario.\r\n> \r\n> I noticed a comment of using conda. To be 100% clean, I'd definitely try to reproduce this also with a clean venv install with pip. Smth like\r\n> \r\n> ```\r\n> cd projdir\r\n> \\python38\\python.exe -m venv .venv\r\n> .\\venv\\Scripts\\activate or .\\venv\\Scripts\\Activate.ps1 for powershell\r\n> pip install -U pip\r\n> pip install tensorflow\r\n> python -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])));print(tf.config.list_physical_devices('GPU'))\"\r\n> python yourtrainging.py\r\n> ```\r\n\r\nI have two environments, my main one with TF 2.4 with Cuda 11 and Cudnn 8.0.5, and TF 2.3 with Cuda 10.1 with Cudnn 7.6.5. After running TF 2.3 longer it also seems to crash also, so I was mistaken that it only was with Cuda 11+ and Cudnn 8+.\r\n\r\nI am not only getting CUDNN_STATUS_INTERNAL_ERROR, I also get:\r\n\r\n```\r\nFailed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 4608, 100, 1, 60, 32, 100] \r\n\t [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\r\n\t [[PartitionedCall]] [Op:__inference_train_function_1127558]\r\n```\r\n\r\nAnd some other cudnn crashes, they all happen at random and the error code is each time random.\r\n\r\nI'll try in a clean venv environment later today.", "> @ion-elgreco I'm not aware of a good reason how the training could become slower after enabling mixed precision... Initial graph building, yes.\r\n> \r\n> Do I understand correctly that you're also running TF2.3 right now inside this new desktop with a Ryzen CPU, and just switching between active CUDA version? So the TF2.3 is not crashing and TF2.4 does with the **exact** same hw and software conf.\r\n> \r\n> Couldn't rule out the HW issue as well, this one is from pytorch, but still relevant. https://discuss.pytorch.org/t/runtimeerror-cudnn-status-internal-error-when-l-run-the-program-for-a-second-time/2960/4\r\n> \r\n> As there can be many causes for CUDNN_STATUS_INTERNAL_ERROR, the only pragmatic way to help here, is to also provide the dataset (processed enough to not cause the legal issues), that would be as small and compact as possible to reproduce the issue with TF2.4. Also include the affected version info etc. Might be worth opening as a separate issue, as the cause seems to be different from the one from @nectario.\r\n> \r\n> I noticed a comment of using conda. To be 100% clean, I'd definitely try to reproduce this also with a clean venv install with pip. Smth like\r\n> \r\n> ```\r\n> cd projdir\r\n> \\python38\\python.exe -m venv .venv\r\n> .\\venv\\Scripts\\activate or .\\venv\\Scripts\\Activate.ps1 for powershell\r\n> pip install -U pip\r\n> pip install tensorflow\r\n> python -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])));print(tf.config.list_physical_devices('GPU'))\"\r\n> python yourtrainging.py\r\n> ```\r\n\r\nThat is interesting. It could be definitely the case that the 5800x is causing it, since that's the only I swapped (besides motherboard and ram). I am going to try if turning off xmp profiles for my ram also has an effect. Ram is not the cause, only hardware related thing left is CPU, but I don't have any spare ryzen cpu's laying around so I cant actually check this. ", "@ion-elgreco It could also help if you have a non-Ryzen GPU setup somewhere that can be used for this private dataset training making sure it works there with both TF2.3 and TF2.4.", "> @ion-elgreco It could also help if you have a non-Ryzen GPU setup somewhere that can be used for this private dataset training making sure it works there with both TF2.3 and TF2.4.\r\n\r\nI had an i7-7700k a week ago, with TF2.3 and TF2.4 my code worked. I cant switch back to Intel because then I would have to lend components from someone. ", "One interesting thing that is happening right now in regards to this issue:\r\nAfter upgrading to TensorFlow 2.4.0, and ugrading my CUDA to 11.0 and 8.02 then downgrading to TensorFlow 2.3.1 and downgrading my CUDA to the version supported by TF 2.3.1, the issue seems to have stopped happening.\r\n\r\nI have trained twice already with 1500 epochs, and no failures so far....", "My NVIDIA driver is also different. I am using the studio version rather than the gaming version.", "It is a bit slower. I remember running this at 1sec per epoch.", "> It is a bit slower. I remember running this at 1sec per epoch.\r\n\r\nStudio and game ready drivers are the same if the versions are the same", "> > It is a bit slower. I remember running this at 1sec per epoch.\r\n> \r\n> Studio and game ready drivers are the same if the versions are the same\r\n\r\nI see. It's strange, issue not happening now.", "> > > It is a bit slower. I remember running this at 1sec per epoch.\r\n> > \r\n> > \r\n> > Studio and game ready drivers are the same if the versions are the same\r\n> \r\n> I see. It's strange, issue not happening now.\r\n\r\nI have tried either version of studio and game-ready. Tried all of them which are available for the RTX 3080, but the issue persists. ", "This is my training output wen it starts:\r\n\r\n`2020-12-28 13:59:46.246094: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n2020-12-28 13:59:46.275929: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:83:00.0 name: TITAN V computeCapability: 7.0\r\ncoreClock: 1.455GHz coreCount: 80 deviceMemorySize: 12.00GiB deviceMemoryBandwidth: 607.97GiB/s\r\n2020-12-28 13:59:46.276253: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-12-28 13:59:46.288071: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-12-28 13:59:46.294217: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-12-28 13:59:46.296723: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-12-28 13:59:46.305299: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-12-28 13:59:46.311567: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2020-12-28 13:59:46.346196: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-12-28 13:59:46.346475: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-12-28 13:59:46.362025: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x20e513b8d80 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-12-28 13:59:46.362291: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-12-28 13:59:46.362731: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:83:00.0 name: TITAN V computeCapability: 7.0\r\ncoreClock: 1.455GHz coreCount: 80 deviceMemorySize: 12.00GiB deviceMemoryBandwidth: 607.97GiB/s\r\n2020-12-28 13:59:46.363085: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-12-28 13:59:46.363276: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-12-28 13:59:46.363465: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-12-28 13:59:46.363648: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-12-28 13:59:46.363841: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-12-28 13:59:46.364033: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2020-12-28 13:59:46.364218: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-12-28 13:59:46.364439: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-12-28 13:59:47.265842: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-12-28 13:59:47.266092: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-12-28 13:59:47.266217: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2020-12-28 13:59:47.266667: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10245 MB memory) -> physical GPU (device: 0, name: TITAN V, pci bus id: 0000:83:00.0, compute capability: 7.0)\r\n2020-12-28 13:59:47.271449: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x20ea8d5c4c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-12-28 13:59:47.271684: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): TITAN V, Compute Capability 7.0`\r\n", "> This is my training output wen it starts:\r\n> \r\n> \r\n\r\nSo it's working now with the exact same model and data? Because somehow I can run LSTM fine with some small text data set, but not with my big image dataset.", "> > This is my training output wen it starts:\r\n> \r\n> So it's working now with the exact same model and data? Because somehow I can run LSTM fine with some small text data set, but not with my big image dataset.\r\n\r\nIt's the same large data although slightly different (some content is different).", "The difference right now is that I can control training based to cutoff dates. My cutoff date right now is at Sept 30th.", "The main difference is the NVIDIA driver. It was different before. I am now using: 460.89 (Studio Version)\r\n", "My existing training would almost always fail. I am on the third run right now.", "I found the cause of cuda crashes... it is the RMSprop optimizer. With all other leaners, I can run the LSTM fine, this is mad...", "Wow! What about adam? That is what I have.", "> Wow! What about adam? That is what I have.\r\n\r\nOh never mind.. With Adam it will crash simply not as fast as with RMSprop. Everytime I think I have found the problem, but eventually, it will still crash. This has got to be the most frustrating thing I have encountered with tensorflow.", "Ok I have some good news :) I was messing around with WSL2 to simply run tensorflow-gpu in Linux. So I had to install the cuda capable driver 465.12 (developer) driver, which now does not cause anymore crashes in Windows.\r\n\r\nPS\r\nIn case someone has experience with WSL2, I am struggling to get it to recognize my GPU.", "Wow, awesome! So it's a driver issue! \r\n\r\nFunny you are messing with WSL2 -- I was doing the same a couple of days back. Haven't gotten too deep into it. Maybe this will help:\r\nhttps://ubuntu.com/blog/getting-started-with-cuda-on-ubuntu-on-wsl-2", "> Wow, awesome! So it's a driver issue!\r\n> \r\n> Funny you are messing with WSL2 -- I was doing the same a couple of days back. Haven't gotten too deep into it. Maybe this will help:\r\n> https://ubuntu.com/blog/getting-started-with-cuda-on-ubuntu-on-wsl-2\r\n\r\nYeah I found that guide already, but Ubuntu with WSL2 won't recognize my gpu. Even though  I have installed the right driver.", "Same for me. 2.4 was core dumping and I had to install 2.3.1 but it was CPU version. If I get it to work with the GPU I will let you know.  \r\n\r\nBut man, after all these months, finally a solution with the above problem! Today is the first time I can train with no issue for hours!", "> Same for me. 2.4 was core dumping and I had to install 2.3.1 but it was CPU version. If I get it to work with the GPU I will let you know.\r\n> \r\n> But man, after all these months, finally a solution with the above problem! Today is the first time I can train with no issue for hours!\r\n\r\nYeah it's nice to see TF working properly again! If you got GPU working in WSL Lett me know :)  I don't know where I read it but it seems like they want to push GPU support more towards WSL.", "You got it! That would be great.", "> You got it! That would be great.\r\n\r\nI think I have found the cause. I am not on a windows insider build. I am on:\r\n\r\n```\r\nEdition\tWindows 10 Pro\r\nVersion\t20H2\r\nInstalled on\t\u200e20/\u200e12/\u200e2020\r\nOS build\t19042.685\r\nExperience\tWindows Feature Experience Pack 120.2212.551.0\r\n```\r\n\r\nBut according to the cuda docs I should be on **20145 or higher** and for tensorflow directml 20150 or higher to be able to use cuda in wsl2.\r\n\r\n\r\n", "Ah, great. I just initiated the enrollment on the Windows Insider Build. This is great.", "I know you found a fix, but another fix i found is changing the settings so they do not align with the cudnn requirements:\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM i.e. adding recurrent dropout - which can be very useful.\r\nIf you change these settings it forces it to use the generic GPU kernel and seems to work even if at a performance cost.", "> I know you found a fix, but another fix i found is changing the settings so they do not align with the cudnn requirements:\r\n> https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM i.e. adding recurrent dropout - which can be very useful.\r\n> If you change these settings it forces it to use the generic GPU kernel and seems to work even if at a performance cost.\r\n\r\nThat is not a fix though, because you are not using the Cudnn kernel. It's a nice trick though, I used it too in the mean while by disabling eager execution", "Sorry, this solution will not work for me. I need the speed increase that the CUDNN optimization provides due to the large number of epochs.", "Have you tried and compared the speed? From what you said you are using a lot of RNN -- my trick doesn't hurt performance too much for one RNN layer as the rest of the layers (CNN, Dense) still use CUDNN. With many RNN layers maybe not. Another option is to use ConvLSTM2D with appropriately resized inputs. That also worked for me.", "> > You got it! That would be great.\r\n> \r\n> I think I have found the cause. I am not on a windows insider build. I am on:\r\n> \r\n> ```\r\n> Edition\tWindows 10 Pro\r\n> Version\t20H2\r\n> Installed on\t\u200e20/\u200e12/\u200e2020\r\n> OS build\t19042.685\r\n> Experience\tWindows Feature Experience Pack 120.2212.551.0\r\n> ```\r\n> \r\n> But according to the cuda docs I should be on **20145 or higher** and for tensorflow directml 20150 or higher to be able to use cuda in wsl2.\r\n\r\n@ion-elgreco \r\n\r\nI finally got TensorFlow with CUDA/CUDNN to work WSL 2 (without using Docker containers). Just a few things:\r\n\r\n1. I had to use TF 2.3.1 as 2.4 would core dump.\r\n2. It's actually slower: It takes 6 seconds per epoch versus 2 seconds on Windows 10.\r\n\r\nIf the speed issue can be resolved hands down this would be the only environment I will use!", "> 1. core dump.\r\n\r\nCould you maybe explain in detail, which OS build, driver versions etc you needed to get it to work?", "> > 1. core dump.\r\n> \r\n> Could you maybe explain in detail, which OS build, driver versions etc you needed to get it to work?\r\n\r\nHere is the info:\r\n\r\n**Build:** 10.0.21277 Build 21277\r\n**Nvidia Driver Version:** 460.89 (Studio)\r\n\r\nAs for the commands, I ran a bunch but I think these made it work:\r\n\r\n```\r\nwget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64/nvidia-machine-learning-repo-ubuntu2004_1.0.0-1_amd64.deb\r\n\r\nsudo apt install ./nvidia-machine-learning-repo-ubuntu2004_1.0.0-1_amd64.deb\r\n\r\nsudo apt install nvidia-cuda-toolkit\r\n\r\ndownload cudnn 7.65 for ubuntu 18.04 (there is no 20.04 version and you need to login and manually download it):\r\n\r\nhttps://developer.nvidia.com/compute/machine-learning/cudnn/secure/7.6.5.32/Production/10.1_20191031/Ubuntu18_04-x64/libcudnn7_7.6.5.32-1%2Bcuda10.1_amd64.deb\r\n\r\nsudo apt install ./libcudnn7_7.6.5.32-1%2Bcuda10.1_amd64.deb\r\n\r\n\r\nsudo apt install nvidia-driver-440\r\n\r\ntype nvidia-smi to verify\r\n\r\nMake sure you install tensorflow 3.1\r\n```\r\n\r\n", "@ion-elgreco \r\nI was trying many things for a long time not realizing the issue was with TF 2.4.0", "> > > 1. core dump.\r\n> > \r\n> > \r\n> > Could you maybe explain in detail, which OS build, driver versions etc you needed to get it to work?\r\n> \r\n> Here is the info:\r\n> \r\n> **Build:** 10.0.21277 Build 21277\r\n> **Nvidia Driver Version:** 460.89 (Studio)\r\n> \r\n> As for the commands, I ran a bunch but I think these made it work:\r\n> \r\n> ```\r\n> wget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64/nvidia-machine-learning-repo-ubuntu2004_1.0.0-1_amd64.deb\r\n> \r\n> sudo apt install ./nvidia-machine-learning-repo-ubuntu2004_1.0.0-1_amd64.deb\r\n> \r\n> sudo apt install nvidia-cuda-toolkit\r\n> \r\n> download cudnn 7.65 for ubuntu 18.04 (there is no 20.04 version and you need to login and manually download it):\r\n> \r\n> https://developer.nvidia.com/compute/machine-learning/cudnn/secure/7.6.5.32/Production/10.1_20191031/Ubuntu18_04-x64/libcudnn7_7.6.5.32-1%2Bcuda10.1_amd64.deb\r\n> \r\n> sudo apt install ./libcudnn7_7.6.5.32-1%2Bcuda10.1_amd64.deb\r\n> \r\n> \r\n> sudo apt install nvidia-driver-440\r\n> \r\n> type nvidia-smi to verify\r\n> \r\n> Make sure you install tensorflow 3.1\r\n> ```\r\n\r\nHmm interesting, because you are installing the Nvidia driver inside the WSL ubuntu env. According to env to properly use your GPU is to install the driver only on your main OS and not inside the WSL ubuntu. Maybe that's why it's slower?\r\n\r\nI'll give your commands a try later this week! Thanks.", "@ion-elgreco\r\n\r\n\r\n\r\n\r\n> > > > 1. core dump.\r\n> > > \r\n> > > \r\n> > > Could you maybe explain in detail, which OS build, driver versions etc you needed to get it to work?\r\n> > \r\n> > \r\n> > Here is the info:\r\n> > **Build:** 10.0.21277 Build 21277\r\n> > **Nvidia Driver Version:** 460.89 (Studio)\r\n> > As for the commands, I ran a bunch but I think these made it work:\r\n> > ```\r\n> > wget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64/nvidia-machine-learning-repo-ubuntu2004_1.0.0-1_amd64.deb\r\n> > \r\n> > sudo apt install ./nvidia-machine-learning-repo-ubuntu2004_1.0.0-1_amd64.deb\r\n> > \r\n> > sudo apt install nvidia-cuda-toolkit\r\n> > \r\n> > download cudnn 7.65 for ubuntu 18.04 (there is no 20.04 version and you need to login and manually download it):\r\n> > \r\n> > https://developer.nvidia.com/compute/machine-learning/cudnn/secure/7.6.5.32/Production/10.1_20191031/Ubuntu18_04-x64/libcudnn7_7.6.5.32-1%2Bcuda10.1_amd64.deb\r\n> > \r\n> > sudo apt install ./libcudnn7_7.6.5.32-1%2Bcuda10.1_amd64.deb\r\n> > \r\n> > \r\n> > sudo apt install nvidia-driver-440\r\n> > \r\n> > type nvidia-smi to verify\r\n> > \r\n> > Make sure you install tensorflow 3.1\r\n> > ```\r\n> \r\n> Hmm interesting, because you are installing the Nvidia driver inside the WSL ubuntu env. According to env to properly use your GPU is to install the driver only on your main OS and not inside the WSL ubuntu. Maybe that's why it's slower?\r\n> \r\n> I'll give your commands a try later this week! Thanks.\r\n\r\nMaybe you are right. I was having a ton of issues before that. I will try uninstalling it and see what happens.", "> @ion-elgreco\r\n> \r\n> > > > > 1. core dump.\r\n> > > > \r\n> > > > \r\n> > > > Could you maybe explain in detail, which OS build, driver versions etc you needed to get it to work?\r\n> > > \r\n> > > \r\n> > > Here is the info:\r\n> > > **Build:** 10.0.21277 Build 21277\r\n> > > **Nvidia Driver Version:** 460.89 (Studio)\r\n> > > As for the commands, I ran a bunch but I think these made it work:\r\n> > > ```\r\n> > > wget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64/nvidia-machine-learning-repo-ubuntu2004_1.0.0-1_amd64.deb\r\n> > > \r\n> > > sudo apt install ./nvidia-machine-learning-repo-ubuntu2004_1.0.0-1_amd64.deb\r\n> > > \r\n> > > sudo apt install nvidia-cuda-toolkit\r\n> > > \r\n> > > download cudnn 7.65 for ubuntu 18.04 (there is no 20.04 version and you need to login and manually download it):\r\n> > > \r\n> > > https://developer.nvidia.com/compute/machine-learning/cudnn/secure/7.6.5.32/Production/10.1_20191031/Ubuntu18_04-x64/libcudnn7_7.6.5.32-1%2Bcuda10.1_amd64.deb\r\n> > > \r\n> > > sudo apt install ./libcudnn7_7.6.5.32-1%2Bcuda10.1_amd64.deb\r\n> > > \r\n> > > \r\n> > > sudo apt install nvidia-driver-440\r\n> > > \r\n> > > type nvidia-smi to verify\r\n> > > \r\n> > > Make sure you install tensorflow 3.1\r\n> > > ```\r\n> > \r\n> > \r\n> > Hmm interesting, because you are installing the Nvidia driver inside the WSL ubuntu env. According to env to properly use your GPU is to install the driver only on your main OS and not inside the WSL ubuntu. Maybe that's why it's slower?\r\n> > I'll give your commands a try later this week! Thanks.\r\n> \r\n> Maybe you are right. I was having a ton of issues before that. I will try uninstalling it and see what happens.\r\n\r\nOnly the developer driver version is supported according to Nvidia. https://developer.nvidia.com/cuda/wsl/download", "> > @ion-elgreco\r\n> > > > > > 1. core dump.\r\n> > > > > \r\n> > > > > \r\n> > > > > Could you maybe explain in detail, which OS build, driver versions etc you needed to get it to work?\r\n> > > > \r\n> > > > \r\n> > > > Here is the info:\r\n> > > > **Build:** 10.0.21277 Build 21277\r\n> > > > **Nvidia Driver Version:** 460.89 (Studio)\r\n> > > > As for the commands, I ran a bunch but I think these made it work:\r\n> > > > ```\r\n> > > > wget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64/nvidia-machine-learning-repo-ubuntu2004_1.0.0-1_amd64.deb\r\n> > > > \r\n> > > > sudo apt install ./nvidia-machine-learning-repo-ubuntu2004_1.0.0-1_amd64.deb\r\n> > > > \r\n> > > > sudo apt install nvidia-cuda-toolkit\r\n> > > > \r\n> > > > download cudnn 7.65 for ubuntu 18.04 (there is no 20.04 version and you need to login and manually download it):\r\n> > > > \r\n> > > > https://developer.nvidia.com/compute/machine-learning/cudnn/secure/7.6.5.32/Production/10.1_20191031/Ubuntu18_04-x64/libcudnn7_7.6.5.32-1%2Bcuda10.1_amd64.deb\r\n> > > > \r\n> > > > sudo apt install ./libcudnn7_7.6.5.32-1%2Bcuda10.1_amd64.deb\r\n> > > > \r\n> > > > \r\n> > > > sudo apt install nvidia-driver-440\r\n> > > > \r\n> > > > type nvidia-smi to verify\r\n> > > > \r\n> > > > Make sure you install tensorflow 3.1\r\n> > > > ```\r\n> > > \r\n> > > \r\n> > > Hmm interesting, because you are installing the Nvidia driver inside the WSL ubuntu env. According to env to properly use your GPU is to install the driver only on your main OS and not inside the WSL ubuntu. Maybe that's why it's slower?\r\n> > > I'll give your commands a try later this week! Thanks.\r\n> > \r\n> > \r\n> > Maybe you are right. I was having a ton of issues before that. I will try uninstalling it and see what happens.\r\n> \r\n> Only the developer driver version is supported according to Nvidia. https://developer.nvidia.com/cuda/wsl/download\r\n\r\nActually, I had the correct driver version installed (the one in the link you sent). I confused the new driver update message with the version installed. I am still at 6 seconds even after uninstalling nvidia-440...", "> > > @ion-elgreco\r\n> > > > > > > 1. core dump.\r\n> > > > > > \r\n> > > > > > \r\n> > > > > > Could you maybe explain in detail, which OS build, driver versions etc you needed to get it to work?\r\n> > > > > \r\n> > > > > \r\n> > > > > Here is the info:\r\n> > > > > **Build:** 10.0.21277 Build 21277\r\n> > > > > **Nvidia Driver Version:** 460.89 (Studio)\r\n> > > > > As for the commands, I ran a bunch but I think these made it work:\r\n> > > > > ```\r\n> > > > > wget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64/nvidia-machine-learning-repo-ubuntu2004_1.0.0-1_amd64.deb\r\n> > > > > \r\n> > > > > sudo apt install ./nvidia-machine-learning-repo-ubuntu2004_1.0.0-1_amd64.deb\r\n> > > > > \r\n> > > > > sudo apt install nvidia-cuda-toolkit\r\n> > > > > \r\n> > > > > download cudnn 7.65 for ubuntu 18.04 (there is no 20.04 version and you need to login and manually download it):\r\n> > > > > \r\n> > > > > https://developer.nvidia.com/compute/machine-learning/cudnn/secure/7.6.5.32/Production/10.1_20191031/Ubuntu18_04-x64/libcudnn7_7.6.5.32-1%2Bcuda10.1_amd64.deb\r\n> > > > > \r\n> > > > > sudo apt install ./libcudnn7_7.6.5.32-1%2Bcuda10.1_amd64.deb\r\n> > > > > \r\n> > > > > \r\n> > > > > sudo apt install nvidia-driver-440\r\n> > > > > \r\n> > > > > type nvidia-smi to verify\r\n> > > > > \r\n> > > > > Make sure you install tensorflow 3.1\r\n> > > > > ```\r\n> > > > \r\n> > > > \r\n> > > > Hmm interesting, because you are installing the Nvidia driver inside the WSL ubuntu env. According to env to properly use your GPU is to install the driver only on your main OS and not inside the WSL ubuntu. Maybe that's why it's slower?\r\n> > > > I'll give your commands a try later this week! Thanks.\r\n> > > \r\n> > > \r\n> > > Maybe you are right. I was having a ton of issues before that. I will try uninstalling it and see what happens.\r\n> > \r\n> > \r\n> > Only the developer driver version is supported according to Nvidia. https://developer.nvidia.com/cuda/wsl/download\r\n> \r\n> Actually, I had the correct driver version installed (the one in the link you sent). I confused the new driver update message with the version installed. I am still at 6 seconds even after uninstalling nvidia-440...\r\n\r\nIt's probably the slow IO with WSL. Do you need to load a lot of files?", "@nectario There's an interesting observation and a fix regarding the TDR setting causing issues, MAYBE it's also relevant to your CUDA_ERROR_LAUNCH_FAILED case. https://github.com/tensorflow/tensorflow/issues/45987#issuecomment-774401485", "> @nectario There's an interesting observation and a fix regarding the TDR setting causing issues, MAYBE it's also relevant to your CUDA_ERROR_LAUNCH_FAILED case. [#45987 (comment)](https://github.com/tensorflow/tensorflow/issues/45987#issuecomment-774401485)\r\n\r\n@ahtik Thank you for bringing this to our attention! I just applied the fix. Let's see...!\r\n", "were you able to resolve the error?  I am getting the same thing currently.", "@ericvoots Yes. It has not happened ever since I upgraded my driver. I also applied the above fix. Although this issue was already resolved with the driver upgrade.", "@ion-elgreco, upgrading to the driver version 465.12 (developer)  solved this problem for me as well with Win10/TF2.2/Cuda 10.1.  Wonder whether this version in off the master release path and when a supported release driver will work.  I did not try the latest suggested driver to check.  Just happy I'm running again. One gotcha is that for some reason nvidia-smi.exe does not display GPU status with this driver, making it hard to monitor for batch size tweaking. Many thanks.", "> @ericvoots Yes. It has not happened ever since I upgraded my driver. I also applied the above fix. Although this issue was already resolved with the driver upgrade.\r\n\r\nDo you mean upgrade the drive version not the cuda version? this Bcuda10.1_amd64.deb link is not working and I think make sure tensorflow 3.1 is installed is not correct.\r\n\r\n```\r\ndownload cudnn 7.65 for ubuntu 18.04 (there is no 20.04 version and you need to login and manually download it):\r\n\r\nhttps://developer.nvidia.com/compute/machine-learning/cudnn/secure/7.6.5.32/Production/10.1_20191031/Ubuntu18_04-x64/libcudnn7_7.6.5.32-1%2Bcuda10.1_amd64.deb\r\n\r\nsudo apt install ./libcudnn7_7.6.5.32-1%2Bcuda10.1_amd64.deb\r\n\r\n\r\nsudo apt install nvidia-driver-440\r\n\r\ntype nvidia-smi to verify\r\n\r\nMake sure you install tensorflow 3.1\r\n\r\n```\r\n\r\nI have the same problem, I think it's because the versions of cuda and cudnn have been messed up.\r\n\r\nMy tensorflow is 2.4.1\r\nCUDA Version: 11.2\r\nDriver Version: 460.39\r\n\r\nI installed cudnn by ```sudo dpkg -i libcudnn8_8.1.0.77-1+cuda11.2_amd64.deb``` and ```sudo dpkg -i libcudnn8-dev_8.1.0.77-1+cuda11.2_amd64.deb```\r\n\r\nBut I cannot find the correct output via\r\n```\r\ncat /usr/include/cudnn.h | grep CUDNN_MAJOR -A 2\r\n```\r\n", "@nectario Could you please let us know if this is still an issue ? Thanks!", "> @nectario Could you please let us know if this is still an issue ? Thanks!\r\n\r\n@sushreebarsa Yes, this still happens. I tested it with TensorFlow 2.6 and 2.7rc1.\r\n\r\nI am starting to suspect that it may be related to the CUDNN version installed. I have CUDNN 8.2. Which also is the case with the Docker images. Please advise what CUDA/CUDNN version I should retest it with.", "@nectario Could please check the [tested build configurations](https://www.tensorflow.org/install/source#gpu) and let us know if it helps? Thank you! ", "> @nectario Could please check the [tested build configurations](https://www.tensorflow.org/install/source#gpu) and let us know if it helps? Thank you!\r\n\r\n@sushreebarsa I am testing right now with CUDNN 8.1.1 and CUDA 11.0. I will report back shortly.", "> > @nectario Could please check the [tested build configurations](https://www.tensorflow.org/install/source#gpu) and let us know if it helps? Thank you!\r\n\r\n\r\n@sushreebarsa There seems now to be some clarity as to when this issue happens. First, I tested it with CUDNN 8.2 and the issue happened. Then I switched to CUDNN 8.1.1 and the issue **did not** happen. So using CUDNN 8.1.1 (or 8.1) resolves this issue.\r\n \r\nBut this is a problem. Because when I use my models in the cloud like AWS, CUDNN 8.2 is by default installed. I don't have control to change the version of CUDNN, hence, the issue will keep happening.\r\n", "@sushreebarsa The other thing to keep in mind, is that this issue happens with the specific model architecture. If I change the model architecture significantly, it also stops happening. For example, switching to a ConvLSTM and removing the bidirectional layers, the issue did not happen but was taking longer per epoch.", "@sushreebarsa Also, to avoid this happening in the cloud, I am using TensorFlow 2.4.2 and the issue does not happen with this version.", "Is this issue addressed with the 2.7 release?"]}, {"number": 41161, "title": "Inconsistent regularization loss computation when used with pretrained models. ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Google Colab Environment**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**\r\n- TensorFlow installed from (source or binary): **Binary (Colab Pre-installed)**\r\n- TensorFlow version (use command below): **2.4.0-nightly**\r\n- Python version: **3.6.9**\r\n- Bazel version (if compiling from source): **N/A**\r\n- GCC/Compiler version (if compiling from source): **N/A**\r\n- CUDA/cuDNN version: **N/A (Colab CPU Environment used)**\r\n- GPU model and memory: **N/A**\r\n\r\n**Describe the current behavior**\r\nI am trying to use MobileNetV2 pretrained model for a simple classification task. Here I want to add regularization loss for MobileNetV2 - base model along with added FC layers. I have conducted 3 experiments as per attached colab notebook, where I get different results for the computation of regularization loss. The MobileNetV2 regularizer loss is computed twice in the model. And I dont know why it happens.\r\n\r\n**Describe the expected behavior**\r\nThe computation of the loss in all the experiments should be same.\r\n\r\n**Standalone code to reproduce the issue**\r\nColab Link : https://colab.research.google.com/drive/1CeKMIAq_g0AOKdakupKIeUjEhkL_TtI1?usp=sharing\r\n\r\n**Other info / logs** \r\n1. I have tried the same on TF2.2 but got the same behaviour.\r\n2. I have got the reference of this: [issue-37511](https://github.com/tensorflow/tensorflow/issues/37511#issuecomment-599173876 ) for adding regularization loss into pretrained model.", "comments": ["Was able to reproduce the issue with [TF v2.2](https://colab.research.google.com/gist/amahendrakar/b930aa726bbb49453829e891f0dad42b/41161-2-2.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/82ab859e8adb6a7eb87f70e49c840e01/41161.ipynb). Please find the attached gist .Thanks!", "Was able to reproduce  the issue with TF 2.5 and Nightly versions.Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/5ed96954861babbb70def2f6ac5fc7e8/41161-2-2.ipynb#scrollTo=JfUG_vigEQLl).Thanks!"]}, {"number": 41152, "title": "The order of weights is changed for array of layers inside sub classed model", "body": "\r\nThe order of moving variance and moving mean weights(BatchNormalization layer) changes when array of layers is created inside sub classed model\r\n\r\n````python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import *\r\nprint(tf.__version__)\r\n\r\nclass A(tf.keras.layers.Layer):\r\n    def  __init__(self,  name):\r\n      super(A, self).__init__(name=name)\r\n      self.conv = Conv2D(32, 3)\r\n      self.bn = BatchNormalization()  \r\n    def call(self,  x):\r\n      x = self.conv(x)\r\n      x = self.bn(x)\r\n      return x\r\n\r\nclass B(tf.keras.Model):\r\n    def __init__(self):\r\n        super(B, self).__init__()\r\n        self.blocks = []\r\n        for i in range(3):\r\n          self.blocks.append(A(f'a{i}'))    \r\n    \r\n    def call(self, x):\r\n        for block in self.blocks:\r\n          x = block(x)\r\n        return x\r\n\r\nb = B()\r\nb.build((1, 128, 128, 3))\r\nprint([weight.name for weight in b.weights])\r\n\r\n#The output is as below\r\n'a0/conv2d/kernel:0',\r\n 'a0/conv2d/bias:0',\r\n 'a0/batch_normalization/gamma:0',\r\n 'a0/batch_normalization/beta:0',\r\n 'a1/conv2d_1/kernel:0',\r\n 'a1/conv2d_1/bias:0',\r\n 'a1/batch_normalization_1/gamma:0',\r\n 'a1/batch_normalization_1/beta:0',\r\n 'a2/conv2d_2/kernel:0',\r\n 'a2/conv2d_2/bias:0',\r\n 'a2/batch_normalization_2/gamma:0',\r\n 'a2/batch_normalization_2/beta:0',\r\n 'a0/batch_normalization/moving_mean:0',\r\n 'a0/batch_normalization/moving_variance:0',\r\n 'a1/batch_normalization_1/moving_mean:0',\r\n 'a1/batch_normalization_1/moving_variance:0',\r\n 'a2/batch_normalization_2/moving_mean:0',\r\n 'a2/batch_normalization_2/moving_variance:0'", "comments": ["I am able to replicate the issue, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/ed97214ddbbded31a4e0b6b68b3cdba4/untitled266.ipynb).", "Was able to reproduce the issue in TF 2.5. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/3f8c9268584e2ac10fbe7c731fa963d2/untitled87.ipynb)."]}, {"number": 41141, "title": "Predict is slow on first call when using variable batch_size", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary (via pip)\r\n- TensorFlow version (use command below): v2.2.0-0-g2b96f3662b 2.2.0 (and tf-nightly: v1.12.1-35161-gd659eb9c0d 2.3.0-dev20200625)\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 10.1 \r\n- GPU model and memory: Tesla P100 / 16GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n- When I use a variable batch_size (<=32) the first prediction for a new size is slow. The following predictions for the same shape are fast. The interesting thing is for batch sizes > 32 there is no slowdown:\r\n![tf-nightly](https://user-images.githubusercontent.com/864213/86664759-ea268e00-bfe6-11ea-93fc-d646bce6fb64.png)\r\n\r\n\r\n\r\n**Describe the expected behavior**\r\n- I expect that calls will take the same time for execution\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1er9s3e4GgyMevU7MC6gOkpscsJ3j0Nr-?usp=sharing\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nThere is a similar issue with the variable input shape: https://github.com/tensorflow/tensorflow/issues/39458", "comments": ["I have tried in colab with TF version 2.2,nightly version and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/5110443db05e1c5f7ce1fe588537e53a/untitled91.ipynb).Thanks!", "@bayandin,\r\nSorry for the delayed response. Could reproduce the issue with **`Tensorflow Version 2.5`** Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/2228c5e1ad558afa39adc77b5d59153d/untitled91.ipynb). \r\n\r\nCan you please let us know the use-case where **`variable`** **`batch_size`** will be applicable? Thanks!", "Hey @rmothukuru, thanks for the response! \r\n\r\n> Can you please let us know the use-case where **`variable`** **`batch_size`** will be applicable?\r\n\r\nA service collects requests from multiple clients into a batch. Then it runs inference on this batch. Since the number of requests/clients is variable, we end up with different batch size. The idea behind collecting requests into a batch is a sequential inference of N requests take more time than running these N requests in a batch (for some N).", "Just an observation, the `model.predict` does not process the whole input in a single batch, it instead splits it into batches itself. The default batch_size is 32, which explains the drop in 32. (For larger inputs, they get split into two batches, one of size 32 and the other 1-32, so these are already \"used\" batch sizes, so the second half of the first measurement is also \"fast\" because of reusing large batch sizes.)\r\n\r\nIf you want to process single batches, use `.predict_on_batch`, which does not split the data; for that method, the first batch is slow until the batch size 64.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "I believe this issue still reproducibly in version 2.5 as per @rmothukuru comment https://github.com/tensorflow/tensorflow/issues/41141#issuecomment-853778917", "I could reproduce the issue with TF 2.7 .Please, find the gist [here](https://colab.research.google.com/gist/kumariko/b43baff89735a177f7944c09974cbb36/untitled91.ipynb#scrollTo=fEf-HVQKpBQi).Thanks!"]}, {"number": 41131, "title": "Support strings as a first-class input/output data type in Swift", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.2.0\r\n- Are you willing to contribute it (Yes/No): Maybe if time/company policy permits.\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nSupport strings as a first-class input/output data type in Swift. Currently only the several bool/int/float types are supported, per https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/swift/Sources/Tensor.swift\r\n\r\n**Will this change the current api? How?**\r\nYes on the Swift language bindings\r\n\r\n**Who will benefit with this feature?**\r\niOS/Swift developers\r\n\r\n**Any Other info.**\r\nAsked on the community group, and was redirected here to file this feature request. CC: @jdduke ", "comments": ["Thanks for the request, this feature is on our roadmap and we hope to make progress on it this quarter."]}, {"number": 41129, "title": "Grappler errors on LSTM jacobian when `experimental_use_pfor=False`", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): `Yes`\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Ubuntu 16.04`\r\n- TensorFlow installed from (source or binary): `Binary`\r\n- TensorFlow version (use command below): `v1.12.1-35711-g8202ae9d5c 2.4.0-dev20200705`\r\n- Python version: `3.7`\r\n- CUDA/cuDNN version:\r\n```\r\n$ conda list | grep cud\r\ncudatoolkit               10.1.243             h6bb024c_0\r\ncudnn                     7.6.5                cuda10.1_0\r\n```\r\n- GPU model and memory: `Nvidia GeForce GTX 1080 Ti`\r\n\r\n**Describe the current behavior**\r\nWhen running with gpu, grappler errors when computing `GradientTape.jacobian(..., experimental_use_pfor=False)` through LSTM. The code below runs, but outputs error messages to stdout. In some cases, similar code seems to be slower on gpu than cpu, even though the `GradientTape.gradient` is much faster on gpu than cpu.\r\n\r\n**Describe the expected behavior**\r\nCode runs without grappler errors.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\nbatch_size, sequence_length = 3, 5\r\n\r\nx_input = tf.keras.layers.Input(\r\n    shape=(sequence_length, 1),\r\n    name='input',\r\n    dtype=tf.float32)\r\n\r\nmask_input = tf.keras.layers.Input(\r\n    shape=(sequence_length, ),\r\n    name='mask',\r\n    dtype=tf.bool)\r\n\r\n\r\nout = tf.keras.layers.LSTM(\r\n    units=256,\r\n    return_sequences=True,\r\n    return_state=False,\r\n)(x_input, mask=mask_input)\r\nhidden_layer_sizes = (256,  256)\r\nfor hidden_layer_size in hidden_layer_sizes:\r\n    out = tf.keras.layers.Dense(hidden_layer_size, activation='relu')(out)\r\nout = tf.keras.layers.Dense(1, activation='linear')(out)\r\nmodel = tf.keras.Model((x_input, mask_input), out)\r\n\r\nx = tf.random.uniform(\r\n    (batch_size, sequence_length, x_input.shape[-1]),\r\n    dtype=x_input.dtype)\r\n\r\nmask = tf.sequence_mask(\r\n    tf.random.uniform(\r\n        (batch_size, ), minval=0, maxval=sequence_length, dtype=tf.int32),\r\n    maxlen=sequence_length,\r\n)[..., ::-1]\r\n\r\n\r\n@tf.function(experimental_relax_shapes=True)\r\ndef compute_jacobian(x):\r\n    y_true = tf.zeros(batch_size)\r\n    with tf.GradientTape() as tape:\r\n        y = model((x, mask))\r\n        y = tf.reduce_sum(y, axis=1)\r\n        loss = tf.losses.MSE(y_pred=y, y_true=y_true)\r\n\r\n    jacobian = tape.jacobian(\r\n        loss,\r\n        model.trainable_variables,\r\n        parallel_iterations=10,\r\n        experimental_use_pfor=False)\r\n\r\n    return jacobian\r\n\r\n_ = compute_jacobian(x)\r\n```\r\n\r\n**Other info / logs**\r\n```\r\n$ export CUDA_VISIBLE_DEVICES=1 && python ./test_lstm_jacobian.py\r\n...\r\n2020-07-06 18:19:52.907774: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:581] function_optimizer failed: Invalid argument: Input 0 of node while/enter/_25 was passed bool from functional_1/lstm/PartitionedCall:5 incompatible with expected int32.\r\n2020-07-06 18:19:52.954497: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:581] layout failed: Out of range: src_output = 26, but num_outputs is only 26\r\n2020-07-06 18:19:52.999724: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:581] function_optimizer failed: Invalid argument: Input 0 of node while/enter/_25 was passed bool from functional_1/lstm/PartitionedCall:5 incompatible with expected int32.\r\n2020-07-06 18:19:53.074385: W tensorflow/core/common_runtime/process_function_library_runtime.cc:773] Ignoring multi-device function optimization failure: Invalid argument: Input 0 of node while/enter/_25 was passed bool from functional_1/lstm/PartitionedCall:5 incompatible with expected int32.\r\n...\r\n```", "comments": ["@hartikainen \r\n\r\nI have tried in colab with TF nightly versions and i am not seeing any error.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/3722c2bef5b3c2dd4158e8dfe0e1ce5e/untitled.ipynb).Please, help me to reproduce the issue so it helps me in debugging.Thanks!", "@ravikyram, thanks for looking into this! How do I check the cuda drivers that are being used by tensorflow in the colab above? I'm using `cudatoolkit==10.1.243` and `cudnn==7.6.5`. Is it possible that these are different? Also, could the different GPU (`Tesla T4` on colab vs. `GTX 1080 Ti` on my end) make difference here?\r\n", "@ravikyram, looking into this again, I do actually see the same errors in the colab. Clicking `Runtime -> View runtime logs` shows the following:\r\n> Jul 7, 2020, 3:12:20 PM | WARNING | 2020-07-07 12:12:20.047368: W tensorflow/core/common_runtime/process_function_library_runtime.cc:773] Ignoring multi-device function optimization failure: Invalid argument: Input 0 of node while/enter/_25 was passed bool from functional_5/lstm_2/PartitionedCall:5 incompatible with expected int32.\r\nJul 7, 2020, 3:12:20 PM | WARNING | 2020-07-07 12:12:20.003220: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:581] function_optimizer failed: Invalid argument: Input 0 of node while/enter/_25 was passed bool from functional_5/lstm_2/PartitionedCall:5 incompatible with expected int32.\r\nJul 7, 2020, 3:12:19 PM | WARNING | 2020-07-07 12:12:19.976969: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:581] layout failed: Out of range: src_output = 26, but num_outputs is only 26\r\nJul 7, 2020, 3:12:19 PM | WARNING | 2020-07-07 12:12:19.953217: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:581] function_optimizer failed: Invalid argument: Input 0 of node while/enter/_25 was passed bool from functional_5/lstm_2/PartitionedCall:5 incompatible with expected int32.\r\nJul 7, 2020, 3:12:07 PM | INFO | Adapting to protocol v5.1 for kernel fe733757-dd0f-411c-a530-edf26d581989", "Sorry. Grappler is not a mobile component. Please find owner(s) of the grappler.", "Issue still exists in TF 2.5 as well  and I can see as warnings in  logs. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/d4362e264996d6c54a5bbb2f6b87938a/untitled39.ipynb#scrollTo=bgS959wM5Hs9&uniqifier=3).Thanks!", "Issue still exists in both TF2.3 and TF2.6, I also use the LSTM network like this:\r\n\r\n**tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(d_model, return_sequences=True))**\r\n\r\nit seems that this error is caused by this network, how to resolve it? Thanks  a lot!\r\n\r\n\r\n\r\n2021-10-22 16:15:26.548440: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\r\n2021-10-22 16:15:58.724411: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:801] function_optimizer failed: Invalid argument: Node 'Func/StatefulPartitionedCall/conformer_encoder/StatefulPartitionedCall/extract_feature/StatefulPartitionedCall/waveform_encoder/StatefulPartitionedCall/galr_block/StatefulPartitionedCall/local_rnn/StatefulPartitionedCall/output/_15813': Connecting to invalid output 30 of source node StatefulPartitionedCall/conformer_encoder/StatefulPartitionedCall/extract_feature/StatefulPartitionedCall/waveform_encoder/StatefulPartitionedCall/galr_block/StatefulPartitionedCall/local_rnn/StatefulPartitionedCall/bidirectional/forward_lstm/PartitionedCall which has 30 outputs.\r\n2021-10-22 16:16:00.574578: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:801] shape_optimizer failed: Out of range: src_output = 30, but num_outputs is only 30\r\n2021-10-22 16:16:01.556781: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:801] layout failed: Out of range: src_output = 30, but num_outputs is only 30\r\n", "@hartikainen I tried to run the code on colab using TF v2.7.0 and didn't  face the error reported.Could you please find the attached gist [here](https://colab.research.google.com/gist/sushreebarsa/26770d316d6ae751a4cfa4dd4f9998cb/41129.ipynb) and let us know if it helps?Thanks!", "Hey @sushreebarsa. I briefly tested this on my end as well and everything seems to be good. Thanks a lot for the fix \ud83d\ude4f ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41129\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41129\">No</a>\n", "Apologies, looking this a bit more carefully, I'm still seeing grappler errors from the above code:\r\n```\r\npython ./tests/test_tf_lstm_grappler.py\r\n2022-01-24 10:03:14.491930: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2022-01-24 10:03:26.419096: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 954 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:1a:00.0, compute capability: 7.5\r\n2022-01-24 10:03:26.421037: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 954 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:1b:00.0, compute capability: 7.5\r\n2022-01-24 10:03:26.423187: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 9643 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:60:00.0, compute capability: 7.5\r\n2022-01-24 10:03:26.424719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 954 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:61:00.0, compute capability: 7.5\r\n2022-01-24 10:03:26.426683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 6934 MB memory:  -> device: 4, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:b1:00.0, compute capability: 7.5\r\n2022-01-24 10:03:26.428275: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 954 MB memory:  -> device: 5, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:b2:00.0, compute capability: 7.5\r\n2022-01-24 10:03:26.430053: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:6 with 2177 MB memory:  -> device: 6, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:da:00.0, compute capability: 7.5\r\n2022-01-24 10:03:26.431923: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:7 with 9643 MB memory:  -> device: 7, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:db:00.0, compute capability: 7.5\r\n2022-01-24 10:03:28.250137: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:812] function_optimizer failed: INVALID_ARGUMENT: Input 0 of node while/enter/_25 was passed bool from model/lstm/PartitionedCall:5 incompatible with expected int32.\r\n2022-01-24 10:03:28.283472: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:812] layout failed: OUT_OF_RANGE: src_output = 27, but num_outputs is only 27\r\n2022-01-24 10:03:28.318619: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:812] tfg_optimizer{} failed: INVALID_ARGUMENT: Input 0 of node while/enter/_25 was passed bool from model/lstm/PartitionedCall:5 incompatible with expected int32.\r\n        when importing GraphDef to MLIR module in GrapplerHook\r\n2022-01-24 10:03:28.329970: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:812] function_optimizer failed: INVALID_ARGUMENT: Input 0 of node while/enter/_25 was passed bool from model/lstm/PartitionedCall:5 incompatible with expected int32.\r\n2022-01-24 10:03:28.383351: W tensorflow/core/common_runtime/process_function_library_runtime.cc:866] Ignoring multi-device function optimization failure: INVALID_ARGUMENT: Input 0 of node while/enter/_25 was passed bool from model/lstm/PartitionedCall:5 incompatible with expected int32.\r\n```\r\n\r\n@sushreebarsa, this also happens with [your notebook](https://colab.research.google.com/gist/sushreebarsa/26770d316d6ae751a4cfa4dd4f9998cb/41129.ipynb) when changing the runtime to use GPU (`Runtime -> Change runtime type -> Hardware Accelerator = GPU`).", "@hartikainen Thank you for the update!\r\nCould you please have a look at the [gist](https://colab.research.google.com/gist/sushreebarsa/26770d316d6ae751a4cfa4dd4f9998cb/41129.ipynb#scrollTo=YXIUTGpZJWVB) on GPU runtime as well?Thanks!", "Hey @sushreebarsa. Just tested on the GPU colab and it still fails. You might need to refresh the runtime logs after running the code.\r\n\r\nSee the error log from the [GPU Colab](https://colab.research.google.com/gist/sushreebarsa/26770d316d6ae751a4cfa4dd4f9998cb/41129.ipynb#scrollTo=YXIUTGpZJWVB) below:\r\n<details>\r\n  <summary>(Collapsed)</summary>\r\n\r\n```\r\nJan 25, 2022, 11:29:38 AM | WARNING | 2022-01-25  11:29:38.658496: W  tensorflow/core/common_runtime/process_function_library_runtime.cc:866]  Ignoring multi-device function optimization failure: INVALID_ARGUMENT:  Input 0 of node while/enter/_25 was passed bool from  model_1/lstm_1/PartitionedCall:5 incompatible with expected int32.\r\nJan 25, 2022, 11:29:38 AM | WARNING | 2022-01-25  11:29:38.610777: E  tensorflow/core/grappler/optimizers/meta_optimizer.cc:812]  function_optimizer failed: INVALID_ARGUMENT: Input 0 of node  while/enter/_25 was passed bool from model_1/lstm_1/PartitionedCall:5  incompatible with expected int32.\r\nJan 25, 2022, 11:29:38 AM | WARNING | when importing GraphDef to MLIR module in GrapplerHook\r\nJan 25, 2022, 11:29:38 AM | WARNING | 2022-01-25  11:29:38.600701: E  tensorflow/core/grappler/optimizers/meta_optimizer.cc:812]  tfg_optimizer{} failed: INVALID_ARGUMENT: Input 0 of node  while/enter/_25 was passed bool from model_1/lstm_1/PartitionedCall:5  incompatible with expected int32.\r\nJan 25, 2022, 11:29:38 AM | WARNING | 2022-01-25  11:29:38.574004: E  tensorflow/core/grappler/optimizers/meta_optimizer.cc:812] layout  failed: OUT_OF_RANGE: src_output = 27, but num_outputs is only 27\r\nJan 25, 2022, 11:29:38 AM | WARNING | 2022-01-25  11:29:38.550275: E  tensorflow/core/grappler/optimizers/meta_optimizer.cc:812]  function_optimizer failed: INVALID_ARGUMENT: Input 0 of node  while/enter/_25 was passed bool from model_1/lstm_1/PartitionedCall:5  incompatible with expected int32.\r\nJan 25, 2022, 11:28:56 AM | WARNING | 2022-01-25  11:28:56.007342: W  tensorflow/core/common_runtime/process_function_library_runtime.cc:866]  Ignoring multi-device function optimization failure: INVALID_ARGUMENT:  Input 0 of node while/enter/_25 was passed bool from  model/lstm/PartitionedCall:5 incompatible with expected int32.\r\nJan 25, 2022, 11:28:55 AM | WARNING | 2022-01-25  11:28:55.807796: E  tensorflow/core/grappler/optimizers/meta_optimizer.cc:812]  function_optimizer failed: INVALID_ARGUMENT: Input 0 of node  while/enter/_25 was passed bool from model/lstm/PartitionedCall:5  incompatible with expected int32.\r\nJan 25, 2022, 11:28:55 AM | WARNING | when importing GraphDef to MLIR module in GrapplerHook\r\nJan 25, 2022, 11:28:55 AM | WARNING | 2022-01-25  11:28:55.770614: E  tensorflow/core/grappler/optimizers/meta_optimizer.cc:812]  tfg_optimizer{} failed: INVALID_ARGUMENT: Input 0 of node  while/enter/_25 was passed bool from model/lstm/PartitionedCall:5  incompatible with expected int32.\r\nJan 25, 2022, 11:28:55 AM | WARNING | 2022-01-25  11:28:55.657697: E  tensorflow/core/grappler/optimizers/meta_optimizer.cc:812] layout  failed: OUT_OF_RANGE: src_output = 27, but num_outputs is only 27\r\nJan 25, 2022, 11:28:55 AM | WARNING | 2022-01-25  11:28:55.601893: E  tensorflow/core/grappler/optimizers/meta_optimizer.cc:812]  function_optimizer failed: INVALID_ARGUMENT: Input 0 of node  while/enter/_25 was passed bool from model/lstm/PartitionedCall:5  incompatible with expected int32.\r\nJan 25, 2022, 11:28:49 AM | WARNING | 2022-01-25  11:28:49.566799: W  tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding  allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment  variable is set. Original config value was 0.\r\nJan 25, 2022, 11:28:39 AM | INFO | Adapting to protocol v5.1 for kernel 66de4e16-db78-47f5-8419-106c9fdb1741\r\n```\r\n\r\n</details>"]}, {"number": 41123, "title": "tf.keras.layers.Permute documentation is misleading, error messages even more so", "body": "\r\n## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/Permute\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nPermute layer is quite picky about its dims argument despite docs clearly saying \r\n\r\n`dims: Tuple of integers. Permutation pattern, does not include the samples dimension. Indexing starts at 1. For instance, (2, 1) permutes the first and second dimensions of the input. `\r\n\r\n It does not actually follow from this documentation that the dims input MUST have all the dimensions clearly listed without misses -  as per check actually present in the code:\r\n``` python\r\n   if sorted(dims) != list(range(1, len(dims) + 1)):\r\n      raise ValueError(\r\n          'Invalid permutation `dims` for Permute Layer: %s. '\r\n          'The set of indices in `dims` must be consecutive and start from 1.' %\r\n          (dims,))\r\n```\r\nThe next hurdle is that the error message thrown by runtime is kind of inconsistent:\r\n\r\n``` python\r\nkeras.layers.Permute((3, 2), input_shape=[30, 6, 8], name=f\"Permute_layer\")\r\n>>> Invalid permutation `dims` for Permute Layer: (3, 2). The set of indices in `dims` must be consecutive and start from 1.\r\n```\r\nIt does not say in the doc that the dims must start with 1, it just says indexing starts with 1! Ok now the user knows it must start with 1, but how does one actually get dimensions 2 and 3 swapped? At that point it's just a mess from there on.\r\n\r\n### Suggest following changes:\r\nto the docs: please make example use 3D tensor, rather than 2D. Then it would be clear that all dims must be listed, even if they are not to be permuted, e.g.\r\n``` python\r\ntmp = keras.layers.Permute((1, 3, 2), name=f\"Permute_input\")(tmp)\r\n```\r\n\r\nAnother note that docs do not make is that the len(ndim) must match the  input_shape dimensions, which is however checked by the __init__, again possibly conflicting with the doc that says that input_shape can be whatever (clearly not whatever, it is coupled with ndim argument.\r\n\r\n\r\n", "comments": ["Hey @MarkDaoust , is the issue stil open or have you resolved it?? Can you assign the issue to me please.", "This page hasn't changed. You can give it a shot. The source for this page is here:\r\n\r\nhttps://github.com/keras-team/keras/blob/master/keras/layers/core/permute.py\r\n\r\nIt might be sufficient to say that `dims` expects a ***permutation*** of the dimensions.", "Hey @MarkDaoust , thanks for assigning me this issue. Soo as stated in the issue, I just need to use a 3-D tensor example instead of a 2-D one right?? And also state that len(ndim) must match the input_shape dimensions right?? I'm just clarifying on this cause I'm new to this, soo need your help. Thanks", "Yes, that sounds right.", "Okay I shall make a pull-request on it. Thanks", "Hey @MarkDaoust , I have understood the issue but the problem comes out here that what should I write in documentation?? How to change the documentation such that points are covered in that?? Please advice me some context and I shall definitely resolve this. Thanks", "Hey @MarkDaoust , are you there to help ??", "Just putting a 3D example on the api-page would be a good start.\r\n\r\n> Invalid permutation `dims` for Permute Layer: %s. The set of indices in `dims` must be consecutive and start from 1.\r\n\r\nFor the error message maybe something like this would help:\r\n\r\n```\r\nInvalid permutation `dims` for Permute Layer: %s. `dims` must be a permutation (reordering) of the input's axis ids, excluding the batch-dimension. To transpose a batch of tensors with shape `[batch, n, m]` pass `Permute([2,1])`.\r\n```\r\n\r\nAnd copy that to the description of the 'dims' argument."]}, {"number": 41118, "title": "Debug .dll build with CUDA support on Windows fails to link", "body": "**System information**\r\n- OS Platform and Distribution: Windows 10 Enterprise 1809\r\n- TensorFlow installed from: source\r\n- TensorFlow version: v2.2.0\r\n- Python version: 3.5.6\r\n- Bazel version (if compiling from source): 2.0.0\r\n- Compiler version (if compiling from source): MSVC 14.26.28801\r\n- CUDA/cuDNN version: CUDA 10.1 / cuDNN 7.6.5.32\r\n- GPU model and memory: NVIDIA Titan Xp\r\n\r\n\r\nAfter [some issues](https://github.com/tensorflow/tensorflow/issues/39905), a \"default\" build for the Tensorflow dll on my finally machine succeeded (i.e. with a command not unlike `bazel build --jobs=8 --define=no_tensorflow_py_deps=true -c opt //tensorflow:tensorflow.dll`).\r\n\r\nNow I tried to build the same dll in debug mode: `bazel build --jobs=8 --define=no_tensorflow_py_deps=true` **-c opt -c dbg** `--config=c++17 //tensorflow:tensorflow.dll`.\r\n\r\nThis causes several issues, which are at least somewhat solvable, thought probably still not great:\r\n\r\n1. Errors of the kind \"error C4716: 'xla::HloInstruction::unique_indices': must return a value\" -> solved by adding `--copt=/wd4716` to build:windows in .bazelrc\r\n2. It cannot compile with multiple jobs (get multiple - as many as --jobs - error windows with messages like \"Debug errror! Program ...\\x64_windows-dbg\\bin\\external\\llvm-project\\llvm-tblgen: abort() has been called\", and messages in the terminal like \"Assertion failed: !Ptr && !DeleterFn && !Next && \"Partially initialized ManagedStatic!?\", file external/llvm-project/llvm/lib/Support/ManagedStatic.cpp, line 51\". \r\n\r\nThe latter actually does not go away once it shows up - i.e. a repeat of the build command (with 1 or multiple jobs, does not matter) does not go any further, but adding `--define=with_xla_support=false` to the build command helped (I used to add it before because of the issue 1 above, then found out that that problem is not just inside xla::* but also arises for some other code in the repo, and found another workaround there). ~~Not sure if xla is to blame here, maybe adding the flag just triggered some re-do in the build chain...~~  **UPDATE:** without the  `--define=with_xla_support=false` option, this appears on a clean one-job build as well.\r\n\r\n3. Another issue of the similar kind: Getting errors like \"'C:\\users\\mikhail.startsev\\_bazel_mikhail.startsev\\3watukzj\\execroot\\org_tensorflow\\vc140.pdb'; if multiple CL.EXE write to the same .PDB file, please use /FS\"\" -> adding `--copt=/FS` **does not help**, no idea why, still had to run with `--jobs=1` to make it work.\r\n\r\n\r\nAnd _after all that_, it fails to link with **a lot** of undefined externals and a ton warnings to boot. I attach a full subcommand log (with `-s --verbose_failures`): [link_log.txt](https://github.com/tensorflow/tensorflow/files/4877528/link_log.txt)\r\n\r\nExamples of unresolved symbols:\r\n\r\n- void __cdecl **tensorflow::ConcatGPU**<unsigned __int64>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<unsigned __int64 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<unsigned __int64 const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<unsigned __int64 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<unsigned __int64 const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<unsigned __int64,2,1,__int64>,16,struct Eigen::MakePointer> *)\r\n- public: void __cdecl **tensorflow::functor::SpaceToDepthOpFunctor**<struct Eigen::GpuDevice,bool,1>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<bool const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<bool,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$SpaceToDepthOpFunctor@UGpuDevice@Eigen@@_N$00@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CB_N$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@_N$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,bool>::Compute(class tensorflow::OpKernelContext *)\r\n- public: void __cdecl **tensorflow::functor::DenseUpdate**<struct Eigen::GpuDevice,class tensorflow::Variant,2>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant,1,1,__int64>,16,struct Eigen::MakePointer>,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant const ,1,1,__int64>,16,struct Eigen::MakePointer>)\r\n\r\n\r\nWarnings seem to be of 2 kinds:\r\n\r\n1. libdebug_ops_gpu.lo(debug_ops_gpu.cu.o) : **warning LNK4099: PDB 'vc140.pdb' was not found** with 'libdebug_ops_gpu.lo(debug_ops_gpu.cu.o)' or at 'C:\\users\\mikhail.startsev\\_bazel_mikhail.startsev\\3watukzj\\execroot\\org_tensorflow\\bazel-out\\x64_windows-dbg\\bin\\tensorflow\\vc140.pdb'; linking object as if no debug info\r\n2. LINK : warning LNK4286: symbol '??0Status@tensorflow@@QEAA@W4Code@error@1@Vstring_view@absl@@@Z (public: __cdecl tensorflow::Status::Status(enum tensorflow::error::Code,class absl::string_view))' defined in 'libstatus.a(status.o)' is imported by 'libenv.a(env.o)'\r\n\r\nThere are plenty of both to go around...\r\n\r\nWill try to building from scratch with 1 job from the very beginning, thought that might take a while. \r\n\r\nAny help is appreciated :)\r\n\r\n", "comments": ["Finished the clean run of the following: `bazel --output_user_root=D:\\SMV\\build build -s --verbose_failures` **--jobs=1** `--define=no_tensorflow_py_deps=true -c opt -c dbg --define=with_xla_support=false //tensorflow:tensorflow.dll`, see attched log: [link_log_2.txt](https://github.com/tensorflow/tensorflow/files/4883436/link_log_2.txt). The warning and error types look the same to me...\r\n", "I do not think we have ever successfully built a debug binary for TF on windows.\r\nI can take a look at the linker failures, but I cannot promise a result, as I tried to do this a while ago, and miserably failed myself.", "@gunan Thanks to taking a look! But without a debug build, how does one build the standalone app in debug mode? I tried just using the non-debug .dll and .lib files, and got the weirdest runtime errors about heap corruption when vectors of TF-related objects go out of scope (i.e. in the `vector<tensorflow::something>::~vector<tensorflow::something>` call). Is this a sort of known side-effect, or should I reproduce it again and make a new issue about _that_?\r\n\r\nUPDATE: reproduced the error from before, the error was actually caused in the destructor of   \t`std::vector<google::protobuf::FieldDescriptor const *>`, or of `std::unique_ptr<tensorflow::GraphDef>`, which actually inherits from protobuf::Message...\r\n", "@gunan Okay, the linking problem is narrowing down. (1) If I build a no-CUDA-support dll, only the errors related to tensorflow::functor::SpaceToDepthOpFunctor and tensorflow::functor::DepthToSpaceOpFunctor remain, so that's something. \r\n(2) I think I pinpointed where this comes from, but not sure, whether this is something that is on a logical level \"wrong\".  I am right now talking about just 4 files in tensorflow/core/kernels: [depthtospace_op.h](https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/core/kernels/depthtospace_op.h), [depthtospace_op.cc](https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/core/kernels/depthtospace_op.cc), [spacetodepth_op.h](https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/core/kernels/spacetodepth_op.h), and [spacetodepth_op.cc](https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/core/kernels/spacetodepth_op.cc).\r\n\r\nWhile compiling a **CPU** version of tensorflow.dll, all of the undefined external symbols are `SpaceToDepthOpFunctor` or `DepthToSpaceOpFunctor` templated with the first argument of `Eigen::GpuDevice`, because the `SpaceToDepthOp` class (note, no `Functor` at the end), even if with the first template argument of `Eigen::ThreadPoolDevice`, which is the typedef for **`CPU`**`Device`, manages to require a GPUDevice in the arguments of `SpaceToDepthOpFunctor` that it uses externally. \r\n\r\nMore specifically, I am talking about lines [115-130](https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/core/kernels/depthtospace_op.cc#L115) in depthtospace_op.cc and lines [129-149](https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/core/kernels/spacetodepth_op.cc#L129) in spacetodepth_op.cc.\r\n\r\nEven though `if (std::is_same<Device, GPUDevice>::value)` would not actually evaluate to true for any _actaully_ instantiated `DepthToSpaceOp` class template parametrization (for all the missing symbols, they are parameterized with  `Eigen::ThreadPoolDevice`), this `if` **is not a compile-time operation**, so the class _technically_ requires the GPUDevice-template-parametrized functors! If I comment out the code that would not be executed anyway, the linking of the dll step does not crash with the errors that these symbols are not found, at least not yet (it used to stop after about 10 minutes, has been running for about 7 hours now....)\r\n\r\n....which is actually a question in its own right... Should it take quite so long? I think this is actually all due to collecting the right .pdb file. I think at some point I tried to pass /DEBUG:NONE to linker for the .dll linking step, and just toss in _some_ tensorflow.pdf while it was compiling to make sure bazel's alarms don't go off in the expectation of the .pdb, and the step then completed successfully much quicker. I will give it a night to keep linking, in case it is actually doing something useful, and then try this trick again tomorrow morning...\r\n\r\nRegarding the linking issue causes themselves, this seems to be like something solvable by partial template specialization for the CPUDevice?...\r\n\r\n\r\n**UPD after 22.5 hours of \"linking tensorflow.dll\"**: still nothing...", "**Update 2**: I might have been wrong about the non-compile-time nature of std::is_same, but at least inside of that `if` it definitely seems like it will end up as `if(true)` or `if(false)` for each instantiated class, with the code of the `if` still being linked and compiled. This actually would explain the debug/release difference: in release this `if(false)` body would be optimized away.\r\n\r\nI made a hopefully generic fix for the issue by specializing the class for the GPUDevice type (the only case where the \"infamous\" if would be actually evaluated), where only the appropriate code is kept. The generic class template definition is then left with all the original code but without the `if` block that would be evaluated to `if(false)` in the non-specialized case anyway. I managed to build the .dll (thought with a sleight of hand to circumvent the uber-lengthy creation of the tensorflow.pdb) in the end! and so far only for the no-CUDA case. \r\n\r\nI will first test (1) the integration of the dll with other code and (2) release build with CUDA, then probably submit a merge request, if it all works. CUDA-enabled debug dll will likely still be a problem (I got other linking errors with it, too), but we'll see.", "@MikhailStartsev,\r\nAny updates regarding this issue? Please feel free to close the issue if resolved. Thanks!", "@amahendrakar The merge request that was merged did not solve the issue (that is, it solved one linker issue but caused another similar one). I [mentioned](https://github.com/tensorflow/tensorflow/pull/42307#issuecomment-673954495) this in the merge thread before it was accepted, but it still got approved... \r\n\r\nThe initially proposed solution (i.e. up to commit 287d074, this being the main change diff: https://github.com/tensorflow/tensorflow/pull/42307/commits/132e8af2e90446a6f856b9ac3d793611517a5d36) links when building without CUDA, but it involved code duplication, which I'm not very sure how to get around here: One function of a templated class needs to be specialized for the GPUDevice as its template parameter; the rest of the functions can stay exactly the same. My initial solution specialized the whole class while duplicating the code for the unchanged functions.\r\n\r\nI was planning to take a look whether I can link the dll for the CUDA-case as well within the next couple of weeks, and thought of making the code less duplicating at the same time.", "> I was planning to take a look whether I can link the dll for the CUDA-case as well within the next couple of weeks, and thought of making the code less duplicating at the same time.\r\n\r\n@MikhailStartsev,\r\nAny updates? Did you get a chance to work on this? Thanks!", "@amahendrakar Thank you for following up on this, but unfortunately other issues took priority, as the absence of a CUDA-enabled debug DLL turned out not to be as critical, and building just a TF DLL in debug mode was resolved by my initial proposed change.\r\n\r\nIn this light, I propose to wrap up this thread by integrating the changes that make the DLL link under Windows _without_ CUDA, and if I ever come manage to build a CUDA-enabled version, I will submit a pull request to this thread later.\r\n\r\nTo this end, could I ask you or someone else to take a look at the initially proposed changes that _do_ work - the main changes seen in this commit: https://github.com/tensorflow/tensorflow/commit/132e8af2e90446a6f856b9ac3d793611517a5d36 - and suggest a specific way around code duplication, which was the main point of the [pull request discussion](https://github.com/tensorflow/tensorflow/pull/42307#discussion_r470412920)), which would be acceptable for you? I am not that deep into the TF codebase, just saw the problem that can be solved, but am not sure what refactoring would be most appropriate here. In the pull request discussion @sanjoy suggested, as far as I remember now, taking all but the operator() out of the struct, but I am not sure what else would need to be changed in this case..."]}, {"number": 41081, "title": "Zero AUROC when validation set is only comprised of positive labels", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Tested on Ubuntu 18.04 and MacOS Catalina 10.15.5\r\n- TensorFlow installed from (source or binary): installed from Conda (tensorflow-gpu) on Ubuntu and from pip on MacOS\r\n- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version: 3.7.6 (on both machines)\r\n- CUDA/cuDNN version: 11.0 (on Ubuntu machine)\r\n- GPU model and memory: 4 NVIDIA V100 GPUs\r\n\r\n**Describe the current behavior**\r\nThe AUROC is zero when there are no negatives in the validation set. This may lead users to believe that the model is not performing correctly, while it is just an error related to a misuse of the AUROC metric.\r\n\r\n**Describe the expected behavior**\r\nThe AUROC should be NaN when no negatives are present in the validation set. This way the user is aware that there is something wrong with the metric that is being used, with the possibility to add a warning since there is a 0/0 somewhere in the computation of the AUROC.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nfrom tensorflow.keras.metrics import AUC\r\nmetric = AUC()\r\nmetric.update_state([1, 1, 1], [1, 0.5, 0.3])\r\nmetric.result()\r\n```\r\n\r\nI believe that similar issues also happen for other metrics, as uncanny values were displayed also for Recall and Precision, but I have not identified a simple reproducible example.", "comments": ["Was able to reproduce the issue with TF v2.2, TF v2.3.0rc0 and TF-nightly (i.e. v2.4.0-dev20200705). Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/ffbc2e4b0e234b2e54122693e6cd4439/41081.ipynb). Thanks!", "Was able to reproduce the issue in TF 2.5. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/e722f9dff30466812d7138f4d38fe83e/copy-of-41081.ipynb) for reference."]}, {"number": 41073, "title": "tf.feature_column.shared_embeddings cannot be saved", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX 10.15.5\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CPU only\r\n- GPU model and memory: CPU only\r\n\r\n**Describe the current behavior**\r\n![\u622a\u5716 2020-07-04 \u4e0a\u534810 29 16](https://user-images.githubusercontent.com/4080524/86503500-3a340380-bde1-11ea-823c-66f2d55f715c.jpg)\r\n\r\n**Describe the expected behavior**\r\nshould save the model successfully\r\n\r\n**Standalone code to reproduce the issue**\r\n```py\r\nimport tensorflow as tf\r\nfrom tensorflow import feature_column\r\ntf.compat.v1.disable_eager_execution()\r\n\r\n# Define categorical colunm for our text feature, which is preprocessed into sequence of tokens\r\ntext_column = feature_column.sequence_categorical_column_with_vocabulary_list(key='text', vocabulary_list=list(['asd', 'asdf']))\r\n\r\nmax_length = 6\r\nsequence_feature_layer_inputs = {}\r\nsequence_feature_layer_inputs['text'] = tf.keras.Input(\r\n    shape=(max_length,), name='text', dtype=tf.string)\r\n\r\ntext_embedding = feature_column.shared_embeddings([text_column], dimension=64)\r\n\r\n# below is ok to save\r\n# text_embedding = feature_column.embedding_column(text_column, dimension=8)\r\n\r\n# Define SequenceFeatures layer to pass feature_columns into Keras model\r\nsequence_feature_layer = tf.keras.experimental.SequenceFeatures(text_embedding)\r\n\r\n# note here that SequenceFeatures layer produce tuple of two tensors as output. We need just first to pass next.\r\nsequence_feature_layer_outputs, _ = sequence_feature_layer(\r\n    sequence_feature_layer_inputs)\r\nx = tf.keras.layers.Conv1D(8, 4)(sequence_feature_layer_outputs)\r\nx = tf.keras.layers.MaxPooling1D(2)(x)\r\nx = tf.keras.layers.Dense(256, activation='relu')(x)\r\nx = tf.keras.layers.Dropout(0.2)(x)\r\nx = tf.keras.layers.GlobalAveragePooling1D()(x)\r\n# This example supposes binary classification, as labels are 0 or 1\r\nx = tf.keras.layers.Dense(1, activation='sigmoid')(x)\r\n\r\nmodel = tf.keras.models.Model(\r\n    inputs=[v for v in sequence_feature_layer_inputs.values()], outputs=x)\r\n\r\nmodel.summary()\r\n\r\n# This example supposes binary classification, as labels are 0 or 1\r\nmodel.compile(optimizer='adam',\r\n              loss='binary_crossentropy',\r\n              metrics=['accuracy']\r\n              #run_eagerly=True\r\n              )\r\n\r\nmodel.save('model.h5')\r\n```\r\n", "comments": ["@haifengkao \r\nI am able to replicate the issue faced, please find the [gist here.](https://colab.research.google.com/gist/Saduf2019/961444f26a7df634919fb5aad51a826c/untitled257.ipynb)\r\n", "Yes, I can replicate on my computer as well. I feel that the whole feature column stuff are underdeveloped. \r\nWill this issue be solved?\r\n", "Any updates on this? Been a while and issue persists for me as well. ", "Use Keras Preprocessing Layers instead of feature columns", "@tanzhenyu, I'm using the [TensorFlow Ranking ](https://github.com/tensorflow/ranking) library, which is dependent on feature_columns as input to transform the features into a format suitable for Learn to Rank modeling. Since feature_columns have not been deprecated, I would hope this bug would be fixed. \r\n", "> @tanzhenyu, I'm using the [TensorFlow Ranking ](https://github.com/tensorflow/ranking) library, which is dependent on feature_columns as input to transform the features into a format suitable for Learn to Rank modeling. Since feature_columns have not been deprecated, I would hope this bug would be fixed.\r\n\r\n@ramakumar1729 Has TFR's support for KPL been opened sourced?", "@davidcereal have you been able to find a solution?\r\n", "@VikashPeddakota999, unfortunately not yet. ", "Was able to reproduce the issue in TF 2.5 version. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/81555574e109c37d3c3a693869dd4f2b/untitled86.ipynb).Thanks!", "> Was able to reproduce the issue in TF 2.5 version. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/81555574e109c37d3c3a693869dd4f2b/untitled86.ipynb).Thanks!\r\nIf you only want to use shared_embedding, you checkout my gist, I do a copy of yours and modify it.\r\n-------------\r\nTLDR: \r\n    use `tf.Graph().as_default():` and use saved_model instead of h5\r\nhttps://gist.github.com/austinzh/361f7758b56dfa41b538c9b0c16feeb5"]}, {"number": 41053, "title": "Add a method to save and load the optimizer.", "body": "In TensorFlow 2.2 there is the capability to save a model with its optimizer.\r\nThis is fine, but I run in situations where I want to save the weights and the optimizer separately (conflating training, metrics architecture and weights in a single model object is a bit confusing too imho).\r\nIf I try to pickle the optimizer and then reload it, it looks like the contents of the the loaded optimizer are identical to the previous one, but when training the losses are different.\r\n\r\nI tested this by creating a model, training it for 4 epochs, collecting losses and its predictions, then training a model for 2 epochs, saving weights and pickling the optimizer, reloading it and training for 2 more epochs. I  reset the random seed where appropriate.\r\nWhat happens is that, if the optimizer is stateless like SGD, the predictions and all losses are identical (so you can pickle and unpickle a stateless optimizer, good!), but if I try to use a stateful optimizer like Adam,  losses and predictions differ (although all the variables are loaded correclty, as seen from manual inspection).\r\nThis suggests that there is something missing somewhere in my code that is not obvious. A save and load mechanism for the optimizer that takes care of whatever else is needed would be greatly appreciated.\r\n\r\nHere's my code for you to replicate the issue.\r\n\r\n```python\r\nimport pickle\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import Model\r\nfrom tensorflow.keras.layers import Dense\r\nfrom tensorflow.keras.losses import MeanSquaredError\r\nfrom tensorflow.keras.optimizers import SGD\r\nfrom tensorflow.keras.optimizers import Adam\r\n\r\n#optimizer_type = SGD  # this works\r\noptimizer_type = Adam  # this does not work\r\n\r\nnp.random.seed(42)\r\n\r\nx = np.array(range(100), dtype=np.float32).reshape(-1, 1)\r\nx_batched = np.split(x, 2)\r\ny = 2 * x + 1 + np.random.normal(size=x.shape[0]).reshape(-1, 1)\r\ny_batched = np.split(y, 2)\r\n\r\nloss_fn = MeanSquaredError()\r\n\r\n\r\nclass MyModel(Model):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.d1 = Dense(5, activation='relu', dtype=tf.float32)\r\n        self.d2 = Dense(1, dtype=tf.float32)\r\n\r\n    def call(self, inputs, **kwargs):\r\n        return self.d2(self.d1(inputs))\r\n\r\n\r\ndef train_step(model, loss_fn, optimizer, x, y):\r\n    with tf.GradientTape() as tape:\r\n        logits = model(x, training=True)\r\n        loss_value = loss_fn(y, logits)\r\n    grads = tape.gradient(loss_value, model.trainable_weights)\r\n    optimizer.apply_gradients(zip(grads, model.trainable_weights))\r\n    return loss_value\r\n\r\n\r\ndef optimize(model, loss_fn, optimizer, epochs):\r\n    losses = []\r\n    for epoch in range(epochs):\r\n        for x_batch, y_batch in zip(x_batched, y_batched):\r\n            loss_value = train_step(model, loss_fn, optimizer, x_batch,\r\n                                    y_batch)\r\n            losses.append(loss_value.numpy())\r\n    return losses\r\n\r\n\r\n# train 4 epochs\r\n\r\ntf.random.set_seed(42)\r\nmodel = MyModel()\r\noptimizer = optimizer_type(learning_rate=1e-3)\r\nlosses = optimize(model, loss_fn, optimizer, epochs=4)\r\npred = model(x_batched[0])\r\n\r\n# train 2 epochs\r\n\r\ntf.random.set_seed(42)\r\nmodel = MyModel()\r\noptimizer = optimizer_type(learning_rate=1e-3)\r\nlosses2 = optimize(model, loss_fn, optimizer, epochs=2)\r\nmodel.save_weights('weights')\r\nwith open('opt.pkl', 'wb') as f:\r\n    pickle.dump(optimizer, f)\r\n\r\n# load and train 2 more epochs\r\n\r\nmodel = MyModel()\r\nmodel.load_weights('weights')\r\nwith open('opt.pkl', 'rb') as f:\r\n    optimizer = pickle.load(f)\r\nlosses2.extend(optimize(model, loss_fn, optimizer, epochs=2))\r\npred2 = model(x_batched[0])\r\n\r\nprint(losses)\r\nprint(losses2)\r\nprint(pred)\r\nprint(pred2)\r\n\r\nassert losses == losses2\r\nassert np.all(pred == pred2)\r\n\r\n```", "comments": ["Was able to reproduce the issue. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/b82a2f337af07c3adbee0dd7ce547ab8/41053.ipynb). Thanks!", "Hello! Does somebody know what the status of this issue is? I'm running into exactly this problem.", "Hi,\r\n\r\nThe issue here is that you only pickle the optimizer's instantiation parameters (which, by the way, are better accessed through the `get_config` / `from_config` API), not its stateful variables. Those can be accessed _via_ the `get_weights` / `set_weights` API.\r\n\r\nThat being said, there is still the problem of re-creating the weights in a restored optimizer before being able to assign them values. _I.e._ it is possible to do `wgt = model.optimizer.get_weights()` after some iterations and to reset the optimizer's states using `model.optimizer.set_wights(wgt)` after some iterations, but no to assign those weights (save for the number of past iterations) right at instantiation time, which is indeed quite an issue.", "@pandrey-fr thanks for your insight, I understand what you are saying. I believe a function pair that saves both init and stateful parameters and is able to reload them would be greatly appreciated.", "Any news on this feature? I cannot save the full model for different reasons and I must save the weights + the optimizer state (in my case SGD with decay) to continue the training process starting from a checkpoint.\r\nI cannot figure out how to overcome the problem of saving the optimizer state without saving the whole model\r\n", "I'd very much like to have this capability as well. @goldiegadde any info? @k-w-w ?"]}, {"number": 41046, "title": "TF Lite Interpreter: Segmentation Fault", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary, nightly\r\n- TensorFlow version (or github SHA if from source): 2.5.0-dev20200629\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport sentencepiece as spm\r\n\r\nmodel_path = \"./save/use/use.tflite\"\r\nspm_path = \"./save/use/assets/universal_encoder_8k_spm.model\"\r\n\r\nsp = spm.SentencePieceProcessor()\r\nsp.Load(spm_path)\r\n\r\ndef process_to_IDs_in_sparse_format(sp, sentences):\r\n    # An utility method that processes sentences with the sentence piece processor\r\n    # 'sp' and returns the results in tf.SparseTensor-similar format:\r\n    # (values, indices, dense_shape)\r\n    ids = [sp.EncodeAsIds(x) for x in sentences]\r\n    max_len = max(len(x) for x in ids)\r\n    dense_shape = (len(ids), max_len)\r\n    values = [item for sublist in ids for item in sublist]\r\n    indices = [[row, col] for row in range(len(ids)) for col in range(len(ids[row]))]\r\n    values = np.array(values, dtype=np.int64)\r\n    indices = np.array(indices, dtype=np.int64)\r\n    dense_shape = np.array(dense_shape, dtype=np.int64)\r\n    return (values, indices, dense_shape)\r\n\r\nsentences = [\"The quick brown fox jumps over the lazy dog.\"]\r\nvalues, indices, dense_shape = process_to_IDs_in_sparse_format(sp, sentences)\r\n\r\n# Load TFLite model and allocate tensors.\r\ninterpreter = tf.lite.Interpreter(model_path=model_path)\r\n\r\ninput_details = interpreter.get_input_details()\r\n# 0: indicies, 2: values, 3: dense shape\r\ninterpreter.resize_tensor_input(input_details[0][\"index\"], indices.shape)\r\ninterpreter.resize_tensor_input(input_details[1][\"index\"], values.shape)\r\ninterpreter.allocate_tensors()\r\n\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\ninterpreter.set_tensor(input_details[0][\"index\"], indices)\r\ninterpreter.set_tensor(input_details[1][\"index\"], values)\r\ninterpreter.set_tensor(input_details[2][\"index\"], dense_shape)\r\ninterpreter.invoke()\r\np = interpreter.get_tensor(output_details[0][\"index\"])\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n2020-07-02 17:08:47.449467: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\nINFO: Created TensorFlow Lite delegate for select TF ops.\r\n2020-07-02 17:08:48.181872: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-07-02 17:08:48.207514: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2799925000 Hz\r\n2020-07-02 17:08:48.209674: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x558142710b70 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-07-02 17:08:48.209733: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-07-02 17:08:48.219289: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2020-07-02 17:08:48.272890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-02 17:08:48.273257: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5581427d67a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-07-02 17:08:48.273274: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1060, Compute Capability 6.1\r\n2020-07-02 17:08:48.273430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-02 17:08:48.273715: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1060 computeCapability: 6.1\r\ncoreClock: 1.6705GHz coreCount: 10 deviceMemorySize: 5.94GiB deviceMemoryBandwidth: 178.99GiB/s\r\n2020-07-02 17:08:48.273753: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-07-02 17:08:48.275067: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-07-02 17:08:48.276322: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-07-02 17:08:48.276612: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-07-02 17:08:48.278014: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-07-02 17:08:48.278841: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-07-02 17:08:48.281714: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-07-02 17:08:48.281885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-02 17:08:48.282357: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-02 17:08:48.282658: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-07-02 17:08:48.282721: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-07-02 17:08:48.625586: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-07-02 17:08:48.625634: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-07-02 17:08:48.625640: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2020-07-02 17:08:48.625841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-02 17:08:48.626194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-02 17:08:48.626500: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4986 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nINFO: TfLiteFlexDelegate delegate: 5 nodes delegated out of 256 nodes with 1 partitions.\r\n\r\nSegmentation fault (core dumped)\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\n# Model: Univeral Sentence Encoder Lite 2\r\nimport tensorflow.compat.v1 as tf\r\ntf.disable_v2_behavior()\r\ntf.enable_resource_variables()\r\nimport tensorflow_hub as hub\r\n\r\n\r\nmodule = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-lite/2\")\r\ninput_placeholder = tf.sparse_placeholder(tf.int64, shape=[None, None])\r\nencodings = module(inputs=dict(values=input_placeholder.values,\r\n                               indices=input_placeholder.indices,\r\n                               dense_shape=input_placeholder.dense_shape))\r\nwith tf.Session() as session:\r\n    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\r\n    output_dir = \"./save/use\"\r\n    tf.saved_model.simple_save(session, output_dir,\r\n                               inputs={\"text\": input_placeholder}, outputs={\"embedding\": encodings},\r\n                               legacy_init_op=tf.tables_initializer()\r\n                               )\r\n\r\n# Convert to TFLite\r\nimport tensorflow as tf\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(\"./save/use\")\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\nconverter.experimental_new_converter = True\r\ntflite_model = converter.convert()\r\nopen(\"./save/use/use.tflite\", \"wb\").write(tflite_model)\r\n```\r\n\r\n**Failure details**\r\n\r\n- This may be caused by the delegate ops (tf.AddV2, tf.ListDiff), but any clarity on the seg fault or how to fix it would be much appreciated.\r\n\r\n**Any other info / logs**\r\n\r\n\r\n\r\n", "comments": ["Sorry. We do not support e2e hash table use cases like the TF hub model you're using right now. Team is working on supporting hash table cases.\r\n\r\nPlease check out the following page if you can manually modify the hash table op usage in your use cases. \r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/kernels/hashtable", "@vinchg \r\nPlease update as per above comment.", "Thanks for the fast response. I'll attempt to modify the hash table op as you suggested.\r\n\r\nWould you happen to have a time frame on how soon hash table use cases may be supported?", "Progress is ongoing. Will leave a comment when a milestone is achieved.", "@vinchg \r\nCould you please check on the latest version of tf and let us know of this si still an issue.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 41035, "title": "FP32 model performance is better than quant model using TFLite on raspi 3 for mobilenetv2 ", "body": "Hi,\r\n\r\nI have checked out the the tensorflow master and built the tflite library on raspi3. When I use the label_image.cpp and pass in the  mobilenet_v2_1.0_224.tflite and execute , its performance is better than mobilenet_v2_1.0_224_quant.tflite model .  Ideally quant model performance should better than FP32. Below are the numbers that are observed  \r\nFor FP32 -332 ms\r\nFor quant  model - 538 ms.\r\n\r\nIs this a known issue? Could you check the same on your side and clarify.\r\n\r\nNote: I do not see the same issue with mobilenetv1_1.0_224.tflite model. In mobilenetv1 quant model performs better than FP32.\r\n\r\nThank you,\r\nPraveen.\r\n\r\n", "comments": ["Adding @petewarden for raspberry pi performance."]}, {"number": 41034, "title": "Error while saving a model with one layer having a list of one ragged item", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): 2.3.rc0\r\n- TensorFlow version (use command below):\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nIf you try to save a model which has a layer that takes only one ragged input in a list you can save it but not load it. \r\n\r\n**Describe the expected behavior**\r\nTo be able to load it back. \r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n````\r\nimport tensorflow as tf\r\nclass Test(tf.keras.layers.Layer):\r\n  def call(self, inputs):\r\n    return inputs[0][0][0]\r\n\r\na = tf.keras.layers.Input(ragged=True, shape=(None,))\r\nm = tf.keras.models.Model(a, Test()([a]))\r\nm.save('test')\r\ntf.keras.models.load_model('test')\r\n`````\r\n> ValueError: Could not find matching function to call loaded from the SavedModel. Got:\r\n  Positional arguments (1 total):\r\n    * tf.RaggedTensor(values=Tensor(\"inputs:0\", shape=(None,), dtype=float32), row_splits=Tensor(\"inputs_1:0\", shape=(None,), dtype=int64))\r\n  Keyword arguments: {}\r\nExpected these arguments to match one of the following 1 option(s):\r\nOption 1:\r\n  Positional arguments (1 total):\r\n    * [RaggedTensorSpec(TensorShape([None, None]), tf.float32, 1, tf.int64)]\r\n  Keyword arguments: {}\r\n\r\n\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nLink to gist: https://colab.research.google.com/gist/tanguycdls/cfbed31d313c247d2dc9d12bc19148cc/untitled20.ipynb", "comments": ["@tanguycdls \r\nI ran the code shared and face a different error, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/7d4d902cbe7cecdf4281ee077d2d7bf2/untitled251.ipynb).", "@Saduf2019 I think you imported keras from keras.layers.input and not tf.keras.\r\n Input has a keyword argument ragged described here: https://www.tensorflow.org/api_docs/python/tf/keras/Input ", "@tanguycdls \r\nCan you please share a executable gist with the error faced.", "> @tanguycdls\r\n> Can you please share a executable gist with the error faced.\r\n\r\nYes Link to gist: https://colab.research.google.com/gist/tanguycdls/cfbed31d313c247d2dc9d12bc19148cc/untitled20.ipynb\r\nDoes it work? it was on the issue. ", "I am able to replicate this issue, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/4f689f0495d0753ad9155eb624b22477/untitled264.ipynb).", "Issue is still happening in TF2.4rc2 :(", "Still an issue in TF2.5.rc1", "Issue still exist in TF 2.7.0 stable version. Please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/a5105f1c44aeedcceae11b13df5d8b93/untitled86.ipynb#scrollTo=AejPutv2vOnY).Thanks!"]}, {"number": 41019, "title": "'fit' takes a dataset with increasing batch size", "body": "**System information**\r\n- TensorFlow version (you are using): 2.2\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI have a custom model to train a text classifier. So far (tensorflow 2.1) we overwrote all important methods of `tf.keras.models.Model`, e.g. `fit`, `predict` etc. One of the reasons for that was that we wanted to adapt the batch size of our dataset every epoch. So, during the first epoch we would have, for example, a batch size of 16, in the next iteration the batch size would increase to 20, and so on. Until we reach an upper limit.\r\n\r\nHere is the method that generates our tf.Dataset. We would call this method every epoch to get a dataset with the desired batch size.\r\n```\r\ndef as_tf_dataset(\r\n    self, batch_size: int, batch_strategy: Text = SEQUENCE, shuffle: bool = False\r\n) -> tf.data.Dataset:\r\n    \"\"\"Create tf dataset.\"\"\"\r\n\r\n    shapes, types = self._get_shapes_types()\r\n\r\n    return tf.data.Dataset.from_generator(\r\n        lambda batch_size_: self._gen_batch(batch_size_, batch_strategy, shuffle),\r\n        output_types=types,\r\n        output_shapes=shapes,\r\n        args=([batch_size]),\r\n    )\r\n```\r\n\r\nWith Tensorflow 2.2. it is possible to just overwrite `train_step` and `test_step` instead of `fit` etc (we followed this [guide](https://keras.io/guides/customizing_what_happens_in_fit/)). This would simplify our code quite a bit and would allow us to use callbacks. \r\nHowever, when we would overwriting `train_step` instead of `fit` we would loose the possibility to control the dataset, e.g. adapt the batch size of the dataset every epoch. `fit` just takes a dataset and with the help of a `DataHandler` it create a new iterator over that dataset every epoch. \r\n\r\nI am not sure how to solve this, but maybe we could allow users to define a custom `DataHandler` that takes care of the dataset generation. \r\n\r\n**Who will benefit with this feature?**\r\nAny users that use a `tf.keras.models.Model`.\r\n", "comments": ["I found a way to make this work. We can implement a custom data generator that will take care of the increasing batch size depending on the current epoch. This can be passed on to `fit`. However, the data adapter used inside `fit` assumes the number of steps is the same across all epochs, which is not true in case of an increasing batch size. \r\nTo make that work, we need to add the line \r\n```\r\nself._inferred_steps = self._infer_steps(None, self._dataset)\r\n``` \r\nafter the following line:\r\nhttps://github.com/tensorflow/tensorflow/blob/fcc4b966f1265f466e82617020af93670141b009/tensorflow/python/keras/engine/data_adapter.py#L1143\r\nThis ensure that not only the dataset is updated, but also the steps per epoch.\r\n\r\nWill create a PR for this.", "@nikitamaia Can we assign to @tabergma ?"]}, {"number": 41009, "title": "\u201cLayer is not connected\u201d issue while accessing intermediate layer from custom callback if model is built by sub-classing", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 18.04.2 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n- TensorFlow installed from (source or binary):\r\nBinary\r\n- TensorFlow version (use command below):\r\nv2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version:\r\nPython 3.7.3\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nCUDA 10.2\r\n- GPU model and memory:\r\nNVIDIA TITAN X (Pascal), ~12GB\r\n\r\n**Describe the current behavior**\r\nI've a simple model and need access of intermediate layers within a custom callback to get intermediate predictions. If I build the model by sub-classing, I get the error `AttributeError: Layer dense is not connected`.\r\n\r\n**Describe the expected behavior**\r\nIt shouldn't cause any error and be able to get predictions using intermediate layers.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nX = np.ones((8,16))\r\ny = np.sum(X, axis=1)\r\n\r\nclass CustomCallback(tf.keras.callbacks.Callback):\r\n    def on_epoch_end(self, epoch, logs=None):\r\n        get_output = tf.keras.backend.function(\r\n            inputs = self.model.layers[0].input,\r\n            outputs = self.model.layers[1].output\r\n        )\r\n        print(\"\\nLayer output: \", get_output(X))\r\n\r\nclass Model(tf.keras.Model):\r\n    def build(self, input_shape):\r\n        self.dense1 = tf.keras.layers.Dense(units=32)\r\n        self.dense2 = tf.keras.layers.Dense(units=1)\r\n        \r\n    def call(self, input_tensor):\r\n        x = self.dense1(input_tensor)\r\n        x = self.dense2(x)\r\n        return x\r\n\r\nmodel = Model()\r\nmodel.compile(optimizer='adam',loss='mean_squared_error', metrics='accuracy')\r\nmodel.fit(X,y, epochs=2, callbacks=[CustomCallback()])\r\n```\r\n\r\n**Other info / logs** \r\nTraceback:\r\n```---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-3-dd6e118e08d6> in <module>\r\n     11 model = Model()\r\n     12 model.compile(optimizer='adam',loss='mean_squared_error', metrics='accuracy')\r\n---> 13 model.fit(X,y, epochs=2, callbacks=[CustomCallback()])\r\n\r\n/opt/anaconda3/envs/brats/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)\r\n     64   def _method_wrapper(self, *args, **kwargs):\r\n     65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\r\n---> 66       return method(self, *args, **kwargs)\r\n     67 \r\n     68     # Running inside `run_distribute_coordinator` already.\r\n\r\n/opt/anaconda3/envs/brats/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n    874           epoch_logs.update(val_logs)\r\n    875 \r\n--> 876         callbacks.on_epoch_end(epoch, epoch_logs)\r\n    877         if self.stop_training:\r\n    878           break\r\n\r\n/opt/anaconda3/envs/brats/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py in on_epoch_end(self, epoch, logs)\r\n    363     logs = self._process_logs(logs)\r\n    364     for callback in self.callbacks:\r\n--> 365       callback.on_epoch_end(epoch, logs)\r\n    366 \r\n    367   def on_train_batch_begin(self, batch, logs=None):\r\n\r\n<ipython-input-2-a1f33c1e2e52> in on_epoch_end(self, epoch, logs)\r\n      8     def on_epoch_end(self, epoch, logs=None):\r\n      9         get_output = tf.keras.backend.function(\r\n---> 10             inputs = self.model.layers[0].input,\r\n     11             outputs = self.model.layers[1].output\r\n     12         )\r\n\r\n/opt/anaconda3/envs/brats/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in input(self)\r\n   1806     if not self._inbound_nodes:\r\n   1807       raise AttributeError('Layer ' + self.name +\r\n-> 1808                            ' is not connected, no input to return.')\r\n   1809     return self._get_node_attribute_at_index(0, 'input_tensors', 'input')\r\n   1810 \r\n\r\nAttributeError: Layer dense is not connected, no input to return.\r\n```\r\nIf I build the model using functional API as shown below, it works fine:\r\n```\r\ninitial = tf.keras.layers.Input((16,))\r\nx = tf.keras.layers.Dense(units=32)(initial)\r\nfinal = tf.keras.layers.Dense(units=1)(x)\r\n\r\nmodel = tf.keras.Model(initial, final)\r\nmodel.compile(optimizer='adam',loss='mean_squared_error', metrics='accuracy')\r\nmodel.fit(X,y, epochs=2, callbacks=[CustomCallback()])\r\n```\r\n[Here's](https://stackoverflow.com/q/62668398/2679778) the stackoverflow question I created on the same issue.", "comments": ["I have tried in colab with TF versions 2.2, 2.3-rc0,nightly version(`2.4.0-dev20200701`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/fc81290bfd1c18d31a40e7c007da118e/untitled71.ipynb).Thanks!", "Is there any update on this ? This is an extremely important use case", "Run into similar issue trying to access intermediate layers in #46605, apparently you can't do it with subclassed model. Which is weird since it is just accessing subpart of the computational graph (build with submodel / functional api).", "Was able to reproduce the issue using TF 2.7.0. Please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/7e4cc0da370919e38867ad3f891ce2fa/untitled509.ipynb).Thanks!"]}, {"number": 40997, "title": "Keras optional Input with default value", "body": "**System information**\r\n- TensorFlow version (you are using): 2.3.rc0\r\n- Are you willing to contribute it (Yes/No): yes\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nFor one of my models i have multiple inputs with some of them only used at inference time and with a default value to 1 during training. \r\nTo support that i currently create two models with the functional api, one with those optional inputs and one connected to a constant but it's quite complex to put in place. \r\n\r\n**Will this change the current api? How?**\r\nWe could add a new parameter to Input(default_value=) we would need to pass the shape and type as well and make sure the default matches the condition. \r\nIt exists in Tensorflow here: https://www.tensorflow.org/api_docs/python/tf/compat/v1/placeholder_with_default\r\n\r\n**Who will benefit with this feature?**\r\nUsers with models with a large number of inputs with some features not used during either inference or training.\r\n\r\n**Any Other info.**\r\n", "comments": ["@tanguycdls \r\n\r\nDo you have any use case that requires the feature you are interested in? Please feel free to submit a PR if you have use cases that supports that feature.Thanks!", "> @tanguycdls\r\n> \r\n> Do you have any use case that requires the feature you are interested in? Please feel free to submit a PR if you have use cases that supports that feature.Thanks!\r\n\r\nHi Thanks for your response. \r\nMy use case is for an embedding with https://www.tensorflow.org/api_docs/python/tf/nn/safe_embedding_lookup_sparse that we implemented as a Keras layer taking a ragged input. \r\n\r\nDuring training all the weights are set to 1 so we dont even feed them to the function while at inference we want to control the weight of each contribution so we feed it also with a ragged of same shape (same len of len). \r\n\r\n```\r\nclass Embedd(tf.keras.layers.Embedding):\r\n    def __init__(self, input_dim, output_dim, combiner='sum', **kwargs):\r\n        self.output_dim = output_dim\r\n        self.combiner = combiner\r\n        super().__init__(input_dim, output_dim, **kwargs)\r\n\r\n    def get_config(self):\r\n        config = super().get_config()\r\n        return {**config, 'combiner': self.combiner}\r\n\r\n    def call(self, inputs):\r\n        out = list(inp.to_sparse() for inp in inputs)\r\n        if len(out) == 1:\r\n          sparse_ids, weight = out[0], None\r\n        else:\r\n          sparse_ids, weight = out\r\n        return tf.nn.safe_embedding_lookup_sparse(self.embeddings,\r\n                                                  sparse_ids=sparse_ids,\r\n                                                  sparse_weights=weight,\r\n                                                  combiner=self.combiner)\r\n\r\n    def build(self, inputs):\r\n        super().build(inputs[0])\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        batch_size = input_shape[0][0]\r\n        return tf.TensorShape((batch_size, self.output_dim))\r\n````\r\n\r\nI would be happy to do a PR however i have no idea how i could do that without two distinct functional models!", "+1 to this feature request. This is a very important feature. It is common to see feature missing when we feed data to the model for both training and inference. We prefer setting the default value of a input tensor *inside* the model not outside, and now with Keras API we need to either setting up default value in our dataset parser, or use some very tricky workarounds such as always take in sparse inputs and convert to dense to set up default value.", "TF 1.x has this feature, it's very useful for end-to-end production.", "I found out about tf.OptionalSpec: https://www.tensorflow.org/api_docs/python/tf/OptionalSpec that could work for that usage. But I dont think it's currently compatible with Keras. "]}]