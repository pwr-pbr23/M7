[{"number": 9765, "title": "Add skeleton/example XLA plugin device", "body": "I wonder if this would be a useful thing to have in the repo?", "comments": ["Can one of the admins verify this patch?", "Can one of the admins verify this patch?", "Hi David - thanks for the contribution!\r\n\r\nCould you elaborate a little bit on the motivation for landing this? Is it much different from, say, the existing CPU backend? If we just want to make new backends easier to write, would it not be simpler to beef up the docs a bit for what it takes to write a new backend?\r\n\r\nAll these backends naturally share quite a bit of boilerplate, as your PR demonstrates. Keeping this boilerplate working in the face of occasional API changes is burdensome, and any in-tree backend adds to the burden. This is why, although I'm not *opposed* to having such an example backend in-tree, I do want to hear some more about the motivation and potential advantages of having one.\r\n\r\n\r\n", "The CPU backend isn't really a skeleton, and depending on your jumping off point it is too complex of an example.\r\n\r\nI was really just sharing the minimal example that I generated when starting my own backend.  Someone on the google group was having trouble with a minimal working example.\r\n\r\nbut I am happy to abandon it.  it was really just a point thing.\r\n\r\nfrom a purely selfish point of view, having a working example that closely matches our own backend implementation will tend to prevent any API changes that cause significant work for us.  for instance, it would be a massive pain if someone decided to have tf2xla produce an llvm structure instead of Hlo objects.\r\n", "incidentally - although this is the wrong place to ask this question... i have an Hlo optimization stage that looks for 'big' ops (conv, dot) and outlines them, and downstream elementwise ops (up to a max computation parameter size).  would you like this upstreamed in the main service directory, or shall I keep it private?\r\n", "@DavidNorman actually we're in the process of creating a much simpler backend, based on the [HLO evaluator](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/hlo_evaluator.h) -- the evaluator isn't 100% feature complete yet, but it will eventually be. Having the evaluator as an XLA backend could serve both has as an example for writing a new simple backend, and be useful for testing XLA and the evaluator itself. @kayzhu has more details.\r\n\r\nWe've been busy adding more features to the evaluator but hadn't started wrapping it in an actual backend yet. If you'd like to contribute, would you consider sending a PR that does that?\r\n\r\n\r\n", "yeah - i don't think it would be too hard to drop into my example.  if the evaluator will, in itself,  handle calls and maps, then i suspect it is as simple as copying the hlo module for the 'compile' stage, and calling the evaluator at execution time.  copying buffers in and out from the stream is the same as my existing code.", "@DavidNorman Thanks for your interest and contribution!\r\n\r\nAs @eliben mentioned, we are working on getting the evaluator more feature-complete and wrap it up as a reference backend. Support for a few more simple ops are on the way, but I haven't worked on calls and maps yet.\r\n\r\nIf you are interested in contributing to the reference backend, I'm more than happy to review incoming PRs.", "Oops - forgot to squash the merge.  Now opened a new one:\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/10031\r\n"]}, {"number": 9764, "title": "how to call AttentionWrapper?", "body": "I am trying to write a simple seq2seq model with attention. But  it gets the following error: \r\n\r\n```\r\nattn_cell = tf.contrib.seq2seq.AttentionWrapper(\r\nAttributeError: 'module' object has no attribute 'AttentionWrapper'\r\n```\r\n How should I call AttentionWrapper?\r\n\r\n Here is my code:\r\n\r\n```\r\n    T=1000\r\n    N=100\r\n    input = tf.placeholder(tf.float32, shape=(N, T, 512), name=\"input_matrix\")\r\n    seq_lengths = tf.placeholder(tf.int32, shape=(N), name=\"input_lengths\")\r\n\r\n    cell= MultiRNNCell([DeviceWrapper(ResidualWrapper(LSTMCell(num_units=512)),device='/gpu:%d' %(i+1)) for i in range(2)])\r\n    encoder_outputs, encoder_final_state = tf.nn.dynamic_rnn(cell,input, parallel_iterations=32, swap_memory=True, dtype=tf.float32)\r\n\r\n    # Attention Mechanisms. Bahdanau is additive style attention\r\n    attn_mech = tf.contrib.seq2seq.BahdanauAttention(\r\n        num_units = 100, # depth of query mechanism\r\n        memory = encoder_outputs, # hidden states to attend (output of RNN)\r\n        #memory_sequence_length= T,#tf.sequence_mask(seq_lengths, T), # masks false memories\r\n        normalize=False, # normalize energy term\r\n        name='BahdanauAttention')\r\n\r\n    cell_out= MultiRNNCell([DeviceWrapper(ResidualWrapper(LSTMCell(num_units=512)),device='/gpu:%d' %(i+1)) for i in range(2)])\r\n        # Attention Wrapper: adds the attention mechanism to the cell\r\n\r\n    # Attention Wrapper: adds the attention mechanism to the cell\r\n    attn_cell = tf.contrib.seq2seq.AttentionWrapper(\r\n        cell = cell,# Instance of RNNCell\r\n        attention_mechanism = attn_mech, # Instance of AttentionMechanism\r\n        attention_size = 100, # Int, depth of attention (output) tensor\r\n        attention_history=False, # whether to store history in final output\r\n        name=\"attention_wrapper\")\r\n \r\n    # TrainingHelper does no sampling, only uses inputs\r\n    helper = tf.contrib.seq2seq.TrainingHelper(\r\n        inputs = x, # decoder inputs\r\n        sequence_length = seq_len_dec, # decoder input length\r\n        name = \"decoder_training_helper\")\r\n \r\n    # Decoder setup\r\n    decoder = tf.contrib.seq2seq.BasicDecoder(\r\n              cell = attn_cell,\r\n              helper = helper, # A Helper instance\r\n              initial_state = encoder_final_state, # initial state of decoder\r\n              output_layer = None) # instance of tf.layers.Layer, like Dense\r\n \r\n    # Perform dynamic decoding with decoder object\r\n    outputs, final_state = tf.contrib.seq2seq.dynamic_decode(decoder)\r\n```", "comments": ["You are likely running a version of tensorflow that is too old. Please upgrade and let us know.", "Thank you for your response. But I am using tf 1.1.0 and still get this error.", "Are you running from `HEAD`?", "I think it's still called `DynamicAttentionWrapper` in TF 1.1.", "Correct, as @ales-t suggested, in version 1.1 there is no `AttentionWrapper`:\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/contrib/seq2seq/__init__.py#L39", "I'm struggling to figure out how `AttentionWrapper` interacts with `MultiRNNCell` in TensorFlow 1.2. Could anyone explain or link me to an example? @gunan @ales-t @drpngx \r\n\r\nAlso, for anyone else coming along, TensorFlow 1.2 replaces `DynamicAttentionWrapper` with `AttentionWrapper`.", "In 1.3 also its AttentionWrapper instead of DynamicAttentionWrapper", "I am also struggling with below error:\r\n`AttributeError: module 'tensorflow.contrib.seq2seq' has no attribute 'DynamicAttentionWrapper'`\r\n\r\nI'm using tensorflow `1.8.0`", "@rajendraarora16 even iam facing the same issue can anyone help ?\r\n", "I am facing the same issue... Please someone help to rectify it. I also install Tensorflow addons, yet its not working", "I'm experiencing the same issue. I've tried `tf.contrib.seq2seq.AttentionWrapper` and `tf.contrib.seq2seq.DynamicAttentionWrapper` and both gave the same AttributeError. \r\n"]}, {"number": 9763, "title": "Android libandroid_tensorflow_inference - \"No OpKernel was registered to support Op 'FloorMod' with these attrs\" when using tf.nn.atrous_conv2d", "body": "hi, everyone,\r\n\r\nWe are trying to load a pb file inside Android. The pb file is generated with python 2.7.0 and Tensorflow 1.0.1.   We have this following error:\r\n\r\n```\r\nexception triggered java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'FloorMod' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n                                                                    <no registered kernels>                                                                  \r\n                                                                  \t [[Node: WCon/atrous_conv2d/mod = FloorMod[T=DT_INT32, _device=\"/device:CPU:0\"](WCon/atrous_conv2d/add_1, WCon/atrous_conv2d/mod/y)]]\r\n```\r\n\r\nThe network definition is using tf.nn.atrous_conv2d(i, k, dilation_rate, padding=padding)\r\n\r\nAnd our freezing graph code is also pretty standard.\r\n\r\nWe have no problem running this pb files for inference with python under Tensorflow 1.0.1\r\n\r\nBut when running on Android phone, we will see the above error, I tried several nightly build TF Android Inference library including the latest from https://ci.tensorflow.org/view/Nightly/job/nightly-android/    The result is the same\r\n\r\nMy feeling is that the inference on Tensorflow python and Tensorflow Android has a huge gap?\r\n\r\nThanks for any ideas!\r\n\r\n\r\n", "comments": ["Duplicate of https://github.com/tensorflow/tensorflow/issues/9476.", "This is still an issue for the TensorFlow AAR on maven ('org.tensorflow:tensorflow-android:1.3.0'), which makes it kind of.. useless."]}, {"number": 9762, "title": "Bazel CI / TensorFlow: fix more errors", "body": "In this commit:\r\n\r\n1. Keep just the first character of the python\r\nversion checker script's output. This is a simple\r\nway to strip \\n and \\r and we only need the major\r\nversion anyway. The string is later used in\r\nwriting the .bazelrc file where these characters\r\nwould cause problems.\r\n\r\n2. Export environment variables in\r\nrun_configure_for_cpu_build, use their default\r\nvalues\r\n\r\n3. Allow a user-defined value for BAZEL_SH\r\n\r\n4. Fix python_configure.bzl:\r\n\r\n4.a. Fix the symlink creation on Windows: create a\r\nsingle junction pointing to the src_dir. This is a\r\ntrick to get around the problem that on Windows we\r\ncan't create symlinks. By creating this symlink\r\nand declaring the same outputs as on Linux/Darwin,\r\nwe make it look like that the genrule successfully\r\nproduced these files. This only works in a\r\nsingle-machine execution environment.\r\n\r\n4.b. Get rid of the \"select\" in\r\n@local_config_python//:numpy_include and\r\npython_include. The results of `find` and `dir`\r\nonly make sense on Linux/Darwin and Windows\r\nrespectively, so there's no point in generating\r\nthe non-Windows genrule on Windows and the\r\nWindows-specific one on non-Windows, because all\r\nthe paths would be wrong. So instead of that,\r\ngenerate just the genrule corresponding to the\r\nplatform.\r\n\r\n4.c. Write the genrule.cmd's commands to scripts\r\nand execute those instead. This avoids the\r\ndifficulties with escaping backslashes and quotes.\r\nBackslashes are necessary on Windows because paths\r\nwith forward slashes aren't generally accepted,\r\ne.g. \"cmd.exe /c foo/bar\" doesn't work because\r\nWindows believes foo is an executable and /bar is\r\nits switch:\r\n\r\n    c:\\tempdir>type bar.cmd\r\n    @echo this is bar.cmd, %%1=(%1)\r\n\r\n    c:\\tempdir>cmd.exe /c bar/baz\r\n    this is bar.cmd, %1=(/baz)\r\n\r\n5. simplify the \"is_windows\" function in\r\n./configure\r\n\r\nFixes https://github.com/bazelbuild/bazel/issues/2892", "comments": ["@tensorflow-jenkins test this please", "Can one of the admins verify this patch?", "Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "@meteorcloudy could you take another look?", "@laszlocsomor I tested your change on my local machine and found a few bugs.\r\nFirst, dest_dir is not the complete output path, we need to add `$(@D)` as prefix.\r\nSecond, we cannot just create a junction to the include directory. Because Bazel cleans up all the output files which means all the files in the original directory will be deleted. So I think the safest way is to copy all the headers on Windows. Can you apply the following patch?\r\n\r\n```\r\n$ git diff third_party/py/python_configure.bzl\r\ndiff --git a/third_party/py/python_configure.bzl b/third_party/py/python_configure.bzl\r\nindex ed576a25d..525f4acc6 100644\r\n--- a/third_party/py/python_configure.bzl\r\n+++ b/third_party/py/python_configure.bzl\r\n@@ -122,8 +122,10 @@ def _symlink_genrule_for_dir(repository_ctx, src_dir, dest_dir, genrule_name):\r\n         executable = True,\r\n         content = \"\\n\".join([\r\n             '@rem Auto-generated by //third_party/py/python_configure.bzl',\r\n-            '@mklink /J \"%s\" \"%s\"' % (dest_dir, src_dir)]))\r\n-    cmd = \"cmd.exe /c $$(echo $(location %s) | sed 's,/,\\\\\\\\,g')\" % cmd_script\r\n+           'set DEST_DIR=%%1\\\\%s\\\\' % dest_dir,\r\n+           'rmdir /s /q %DEST_DIR%',\r\n+            'xcopy /e /q /y \"%s\" \"%%DEST_DIR%%\"' % src_dir]))\r\n+    cmd = \"cmd.exe /c \\\\\\\"$$(echo $(location %s) $(@D) | sed 's/\\//\\\\\\\\\\\\\\\\/g')\\\\\\\"\" % cmd_script\r\n   else:\r\n     cmd_script = \"%s-cmd.sh\" % genrule_name\r\n     repository_ctx.file(\r\n```", "> Second, we cannot just create a junction to the include directory. Because Bazel cleans up all the output files which means all the files in the original directory will be deleted. \r\n\r\nOuch. Heavy facepalm. Great catch Yun, thanks, also for the patch; I'll apply it now.", "Applied your fix, plus added \"/s\" to xcopy, as advised by the docs: https://technet.microsoft.com/en-us/library/cc771254(v=ws.11).aspx", "@laszlocsomor Thanks, the change looks good to me!", "@mrry would you please review this?", "It looks fine to me, but I'm not familiar enough with Skylark to know if it's correct. Can we somehow kick off Bazel/Windows and Bazel/Windows/GPU presubmits to make sure that things work?", "@mrry I think that's a great idea! @gunan Do we have plan to enable it?", "@mrry For this change, I tested on my local machine, it should fix the local_python_config breakage.\r\nI also found some other issues, will send PR to fix them.", "This is a matter of capacity, and test times.\r\nI do not want to slow down development for everyone at the moment, but it is always possible to manually run bazel presubmits.\r\n\r\nJenkins, test this please.", "Running bazel windows presubmit here:\r\nhttp://ci.tensorflow.org/job/tensorflow-pr-win-bazel/5", "@gunan @mrry who can review / approve this?", "In our CI this PR is failing.\r\nhttp://ci.tensorflow.org/job/tensorflow-pr-win-bazel/6/console\r\n\r\n@meteorcloudy @laszlocsomor could you take a look?", "```\r\n13:00:07 INFO: From Executing genrule @local_config_python//:numpy_include:\r\n13:00:07 26 File(s) copied\r\n13:00:07 INFO: From Executing genrule @local_config_python//:python_include:\r\n13:00:07 100 File(s) copied\r\n```\r\nThis indicates the local_python_config problem is fixed.\r\n\r\nThe new error we got:\r\n```\r\n13:00:10 ERROR: missing input file '@local_jdk//:bin/jar'\r\n```\r\nis because on Windows it should be `@local_jdk//:bin/jar.exe` which is already fixed at https://github.com/bazelbuild/bazel/commit/c77eac95ffdee4f127e20266c41cb92cec2a8956.\r\nBut the fix is not in 0.4.5. We need to update Bazel to 0.5.0 once it's released.\r\n", "Thanks @meteorcloudy for investigating!"]}, {"number": 9761, "title": "Can't launch tensorboard", "body": "When i try to launch tensorboard i get this error message : \r\n\r\ntensorboard --logdir=/home/mejdi/Desktop/CNN/logs Traceback (most recent call last): File \"/usr/local/bin/tensorboard\", line 7, in from tensorflow.tensorboard.tensorboard import main File \"/usr/local/lib/python3.5/dist-packages/tensorflow/tensorboard/tensorboard.py\", line 33, in from tensorflow.tensorboard.backend import application File \"/usr/local/lib/python3.5/dist-packages/tensorflow/tensorboard/backend/application.py\", line 47, in from tensorflow.tensorboard.plugins.projector import projector_plugin File \"/usr/local/lib/python3.5/dist-packages/tensorflow/tensorboard/plugins/projector/projector_plugin.py\", line 28, in from tensorflow.contrib.tensorboard.plugins.projector import PROJECTOR_FILENAME File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/init.py\", line 37, in from tensorflow.contrib import keras File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/keras/init.py\", line 26, in from tensorflow.contrib.keras.api.keras import * File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/keras/api/keras/init.py\", line 25, in from tensorflow.contrib.keras.api.keras import activations File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/keras/api/keras/activations/init.py\", line 22, in from tensorflow.contrib.keras.python.keras.activations import elu File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/keras/python/keras/init.py\", line 21, in from tensorflow.contrib.keras.python.keras import activations File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/keras/python/keras/activations.py\", line 23, in from tensorflow.contrib.keras.python.keras import backend as K File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/keras/python/keras/backend.py\", line 3601, in assert _image_data_format in {'channels_last', 'channels_first'} AssertionError\r\n\r\nWhat's wrong ?", "comments": ["I fixed it by changing image_data_format in the .keras/keras.json file from \"tf\" to \"channels_last\"", "So is it  working now? @mejdidallel ", "Yeah it's working now ! Thanks"]}, {"number": 9760, "title": "Add a mechanism for adding an XLA plugin to the build", "body": "I have used the same type of scheme as was used for adding a device to the plugin\r\nunit tests.\r\n", "comments": ["Can one of the admins verify this patch?", "Can one of the admins verify this patch?", "hi,\r\n\r\n@vrv \r\n\r\ni wonder if someone could consider this.  it is a smallish change.  i'm trying to get my repo back into alignment with the public one.\r\n", "Hey David, our rotation person was supposed to do this last week, but didn't. Cc'ing our current rotation person @rmlarsen", "(sorry for the delay)", "no prob.   cheers :)\r\n", "@tensorflow-jenkins test this please", "@DavidNorman Please fix the buildifier errors:\r\n\r\n`=== Sanity check step 3 of 8: do_buildifier (buildifier check) ===\r\n\r\nRunning do_buildifier on 268 files\r\n\r\ntensorflow/compiler/jit/BUILD # label\r\ntensorflow/compiler/xla/BUILD # listsort unsafesort sort:cc_header_only_library.deps\r\n\r\nbuildifier took 0 s\r\n\r\nFAIL: buildifier found errors and/or warnings in above BUILD files.\r\nbuildifier suggested the following changes:\r\n45c45\r\n<         \"//tensorflow/compiler/plugin:plugin\",\r\n---\r\n>         \"//tensorflow/compiler/plugin\",\r\n52,54d51\r\n<         \"//tensorflow/core:framework_headers_lib\",\r\n<         \"//tensorflow/core:stream_executor_headers_lib\",\r\n<         \"//tensorflow/compiler/xla:xla_proto\",\r\n55a53\r\n>         \"//tensorflow/compiler/xla:xla_proto\",\r\n58a57,58\r\n>         \"//tensorflow/core:framework_headers_lib\",\r\n>         \"//tensorflow/core:stream_executor_headers_lib\",\r\nPlease fix manually or run buildifier <file> to auto-fix.`\r\n\r\n", "Thanks for the review.   I decided to leave the name of the headers only libraries \"xxx_headers_lib\", to be consistent with things like \"framework_headers_lib\" and \"stream_executor_headers_lib\". \r\n\r\nIncidentally, I have been talking to the bazel people who are working on allowing bazel to be able to describe a shared library target that depends on a mixture of other shared objects and static libraries.   this should help remove the need for these header only targets.  not sure when that is coming though.\r\n\r\nanyway - these particular ones are needed for more than just static library hiding - they also expose the API without creating circular dependencies, and allow access to some headers that are part of non-public libraries.\r\n\r\ni am not creating an independent shared object for my plugin at the moment.  protobuf doesn't work for the case where 2 shared objects both link to it and are loaded into the same address space.\r\n", "@tensorflow-jenkins test this please", "@DavidNorman could you please address the BUILD file formatting issues: \r\n\r\nhttps://ci.tensorflow.org/job/tensorflow-pull-requests-sanity/4290/consoleFull", "@rmlarsen Done :)", "@tensorflow-jenkins test this please", "Thanks for the contribution!"]}, {"number": 9759, "title": "[XLA] Add ability to run the XLA unit tests against a different device", "body": "This is a squashed version of https://github.com/tensorflow/tensorflow/pull/9409\r\n", "comments": ["Can one of the admins verify this patch?", "Can one of the admins verify this patch?", "Hi all.  I wonder if you might consider this patch again?  Just trying to clear the many diffs between my version of TF and the public one.\r\n", "@hawkinsp could you give this another look? Thanks.", "@hawkinsp hi.  I wonder if this can be merged now?\r\n", "Jenkins, test this please"]}, {"number": 9758, "title": "I use the bazel to generate the android so library on mac, but i met an issue :'ERROR: /Users/nankaiming/Desktop/tensorflow-master/tensorflow/core/kernels/BUILD:3869:1: C++ compilation of rule '//tensorflow/core/kernels:android_tensorflow_kernels' failed: Process exited with status 1 [sandboxed].'", "body": "the gcc version on my mac is\r\n![image](https://cloud.githubusercontent.com/assets/18348154/25809650/3b40c944-3440-11e7-9235-ff6293194a6f.png)\r\nthe log of the process is \r\n![image](https://cloud.githubusercontent.com/assets/18348154/25809685/5cacd0aa-3440-11e7-9a2e-263a7120d762.png)\r\nthe WORKSPACE i edited is \r\n![image](https://cloud.githubusercontent.com/assets/18348154/25809807/c00e6c9e-3440-11e7-9424-76187b008c4d.png)\r\n\r\n\r\n", "comments": ["Android NDK r14b which you're probably using contains Clang, not GCC. Make sure you're using Android NDK r12b, or you'll get narrowing errors like this due to Clang (this is documented in recent versions, but it looks like you have an older version of TF)."]}, {"number": 9757, "title": "Large page fault causes slow performance while using gpu", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary, using `pip install tensorflow-gpu==1.0.1`\r\n- **TensorFlow version (use command below)**:\r\n1.0.1\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n8.0/5.1\r\n- **GPU model and memory**:\r\nnvidia gtx1080, 8g\r\n\r\n### Describe the problem\r\nI have observed a large amount of page fault while running the provided sample code on gpu, and this causes a serious performance drawdown. \r\n\r\nThe key parts of the output of `/usr/bin/time -v python sample.py` are:\r\n\r\n    System time (seconds): 7.28  \r\n    Percent of CPU this job got: 85%  \r\n    Elapsed (wall clock) time (h:mm:ss or m:ss): 0:22.41 \r\n    Minor (reclaiming a frame) page faults: 684695  \r\n    Involuntary context switches: 164 \r\n    File system inputs: 0  \r\n    File system outputs: 8  \r\n\r\nThere are 684k page faults,  and the `gpu-volatile usage` is only about 30%. \r\n\r\nI am very hesitating to ask for help here, because on another system with exact os, software and gpu, this issue does not appears, I have posted on stackoverflow to compare two systems [here](http://stackoverflow.com/questions/43842731/two-exactly-same-systems-have-very-different-performances-when-running-tensorflo) \r\nIs that possible that tensorflow handles different hardwares differently? It looks to me that the gpu-cpu I/O may have caused this issue, and I suspect that I need to configure my hardware settings somewhere, but don't know how.\r\n\r\nThings I have tried:\r\n\r\n1. Upgrade BIOS to the latest version and reset default settings.\r\n2. Call Asus(my motherboard and gpu vendor) customer service for help.\r\n3. Inject LD_PRELOAD=\"/usr/lib/libtcmalloc.so\" to .bashrc file.\r\n \r\n### Source code / logs\r\n\r\nHere is the sample code I used to test\r\n\r\n    import tensorflow as tf\r\n    import numpy as np\r\n    from tqdm import trange\r\n  \r\n    np.random.seed(111)\r\n    h,w = 3000, 2000\r\n    steps = 1000\r\n\r\n    x = tf.placeholder(dtype=tf.float32, shape=[h, w], name='x')\r\n    t = tf.constant(np.random.random(size=[w, w]), dtype=tf.float32)\r\n    m = tf.matmul(x,t)\r\n\r\n    x0 = np.random.random(size=[h, w])\r\n    sess = tf.Session()\r\n    for i in trange(steps):\r\n        x0 = sess.run(m, feed_dict={x: x0})\r\n\r\nThe attachment contains: `Nvidia-smi` output, `/usr/bin/time -v` output, hardware specs in html format, chrome trace timeline.\r\n[sysB.zip](https://github.com/tensorflow/tensorflow/files/983550/sysB.zip)\r\n\r\n\r\n\r\n", "comments": ["CC: @tfboyd ", "Just to provide another information, my memory Bank Locator is `node 1` reported by using `dmidecode --type memory` , whereas for a issue free computer it is `bank 1`. Dont know if its making any differences. [Here is my post on Superuser discussing this.](https://superuser.com/questions/1208443/what-is-ram-bank-locator/1208473#1208473)", "@luxiaolei you are using `feed_dict` which means that tensorflow copies array from Python to TensorFlow runtime at each step.\r\n\r\nThere's a way to avoid the copy in latest TF version if your data is aligned. Can you try latest TensorFlow nightly + align the data to 64 bits using recipe from https://github.com/tensorflow/tensorflow/issues/9690#issuecomment-299521465 to see if this speeds things up?", "@yaroslavvb Sorry for the late reply. I upgraded Tensorflow to version 1.1.0-rc2, ran the sample code with and without align the first input to 64 bits, both observed a 2.3 times speed boost! WOW, what a huge boost! I have noticed that the output of `sess.run` is 64 bits aligned, whereas in version 1.0.1 is not.\r\n\r\nMany thanks ! Really saved my day! My problem solved, closing the issue.", "If you need me to do more tests, I am very glad to:)", "@alextp  -- looks like a happy customer of your copy-free numpy arrays :)", "Minor nitpick in case anyone references this thread in the future: the speedup came by aligning with 64 bytes, not 64 bits (=8 bytes). \r\nThe ``EIGEN_MAX_ALIGN`` variable depends on the vector architecture with which Eigen was compiled. As far as I know, AVX-512 (= 64 bytes) is the largest vector size (and largest value for ``EIGEN_MAX_ALIGN``) in the wild.", "why does version 1.2 still have many page faults?\r\n\r\n![image](https://user-images.githubusercontent.com/3445470/57821998-343a2800-77c5-11e9-96a1-6182b232a75d.png)\r\n", "Version 1.2 is way too old."]}, {"number": 9756, "title": "Accessing Array based on subscript?", "body": "How I get the result using tensorflow?\r\n```\r\nimport numpy as np\r\na = np.random.rand(2,10)\r\nb = np.random.randint(0,10,(4,2))\r\nc = a[:,b]\r\n```\r\nThis code can be implemented by Nympy and Theano. But how to do that on tensorflow?\r\n```\r\nind = np.random.randint(0,10,(4,))\r\nx = K.placeholder(shape = (2,10),dtype = 'int64')\r\ny = x[:,ind]\r\nf = K.function([x],[y])\r\n\r\nx_ = np.random.rand(2,10)\r\ny_ = f([x_])[0]\r\nprint(x_)\r\nprint(y_)\r\n```", "comments": ["I have something like that in my code:\r\n```\r\n  train_indices = tf.random_uniform(\r\n      [BATCH_SIZE], 0, num_train_samples, tf.int64)\r\n  train_data_node = tf.gather(tf_train_data, train_indices)\r\n  train_labels_node = tf.gather(tf_train_labels, train_indices)\r\n```\r\n\r\nThis question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 9755, "title": "Bazel unable to build on Jetson TX1 with --config=cuda option?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04 LTS\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.1 (latest master)\r\n- **Bazel version (if compiling from source)**: 0.4.5\r\n- **CUDA/cuDNN version**: 8.0/5.1\r\n- **GPU model and memory**: Tegra X1\r\n- **Exact command to reproduce**:\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package \r\n\r\n\r\n### Describe the problem\r\nUsing the bazel build command above, I am unable to find a valid toolchain to support the aarch64 architecture in the Jetson TX1. However, by removing the `--config=cuda` option, the error is gone, although I am still unable to finish building the pip file (probably due to the fact that I configured TF for GPU using `./configure`). \r\n\r\nNote: I changed the bazel files as seen in this guide: http://zhiyisun.github.io/2017/02/15/Running-Google-Machine-Learning-Library-Tensorflow-On-ARM-64-bit-Platform.html , in order to get bazel to build for aarch64.\r\n\r\nIs there anything more that I have to change? using the suggested command of `bazel build -c opt --copt=\"-funsafe-math-optimizations\" --copt=\"-ftree-vectorize\" --copt=\"-fomit-frame-pointer\" --verbose_failures tensorflow/tools/pip_package:build_pip_package` I get the same error too.\r\n\r\nI saw this issue over here: https://github.com/bazelbuild/bazel/issues/1855\r\nand it says it has something related to the cuda crosstool. Is there a way to fix this to configure TensorFlow-GPU on the Jetson TX1?\r\n\r\nThank you for your help. :D\r\n\r\n### Source code / logs\r\n\r\nHere is the error I got:\r\n\r\n```\r\nubuntu@tegra-ubuntu:~/tensorflow$ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.build/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\r\nERROR: No toolchain found for cpu 'aarch64'. Valid cpus are: [\r\n  k8,\r\n  piii,\r\n  arm,\r\n  darwin,\r\n  ppc,\r\n].\r\nINFO: Elapsed time: 1.589s\r\n\r\n```", "comments": ["@jart Do you know what to do here?", "Do you think if I compile tensorflow for the Jetson on my laptop, this will work? Meaning to say is this just a build issue, or will TF not run properly even when successfully built because of the architecture issue?\r\n\r\nIf cross-compiling on a laptop will work, is there a guide on how to do this? I have tried the instructions in https://developer.ubuntu.com/en/snappy/guides/cross-build/ but I can't seem to run the commands to completion. Further, the architecture mentioned in the ubuntu guide is ARM64 and not aarch64, so there might be a difference. ", "@petewarden Are we likely to be Jetson compatible?", "This appears to be a duplicate of #9788. The solution appears to be upgrading Bazel.", "Or more correctly, the other issue is a duplicate of this one, since your issue was filed first.", "Do you have any solution for that error?\r\n\r\n`ERROR: No default_toolchain found for cpu 'arm64-v8a'. Valid cpus are: [\r\n  k8,\r\n  armeabi-v7a,\r\n  x64_windows,\r\n  x64_windows_msvc,\r\n  x64_windows_msys,\r\n  s390x,\r\n  ios_x86_64,\r\n]\r\nINFO: Elapsed time: 1.323s\r\n`\r\n\r\nI can't find solution to build it properly for arm64-v8a\r\n", "I have same problem as: \r\n\r\n```\r\nERROR: No default_toolchain found for cpu 'aarch64'. Valid cpus are: [\r\n k8, \r\n piii,\r\n arm,\r\n darwin,\r\n ppc,\r\n x64_windows,\r\n]\r\n```\r\n\r\n\r\nI solved this by  @apolotsk 's method at https://github.com/tensorflow/tensorflow/issues/21852#issuecomment-424718576  \r\n\r\n```\r\nadd\r\n\r\n\r\ndefault_toolchain {\r\n  cpu: \"aarch64\"\r\n  toolchain_identifier: \"local_linux\"\r\n}\r\n\r\nto ./third_party/gpus/crosstool/CROSSTOOL.tpl.\r\n```\r\n\r\n"]}, {"number": 9754, "title": "TensorFlow weight initialization taking 99% of total run time", "body": "### System information\r\n- **Have I written custom code**: No (but custom data)\r\n- **OS Platform and Distribution**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: pip\r\n- **TensorFlow version (use command below)**: tensorflow-gpu (1.0.0 and 1.1) (Python 3.5)\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 8.0\r\n- **GPU model and memory**: GTX 1080\r\n- **Exact command to reproduce**: \r\n\r\n### Describe the problem\r\nI noticed that one of my networks was running slow and nvidia-smi was reporting only around ~10% GPU usage. After running the profiler, I saw that `TruncatedNormal` process was taking the vast majority of running time (see photo).\r\n\r\n[Profiler report](https://i.stack.imgur.com/Ymup2.png)\r\n\r\n### Source code / logs\r\nWeight declaration function (from MNIST tutorial):\r\n```\r\n\r\ndef weight_variable(shape):\r\n    initial = tf.truncated_normal(shape, stddev=0.1)\r\n    return tf.Variable(initial)\r\n```\r\nCode in action:\r\n```\r\n # First Layer\r\n    with tf.name_scope('input'):\r\n        x = tf.placeholder(tf.float32, [None, Nvars])\r\n    w1 = weight_variable([Nvars, 8])\r\n    b1 = bias_variable([8])\r\n    y1 = tf.nn.relu(tf.matmul(x, w1) + b1)\r\n```\r\nPlease note this question was also asked on StackOverflow. [Link](http://stackoverflow.com/q/43793066)", "comments": ["Maybe you could try the latest version 1.1.0?", "Just updated to 1.1 and unfortunately the sluggish performance persists.", "@arinbjornk what's the shape of the variable?  \r\n\r\nAlso, maybe give this a try to see if it boosts performance:\r\n```python\r\nwith tf.variable_scope(\"foo\"):\r\n  v = tf.get_variable(\r\n      'v', [N, 8], dtype=tf.float32,\r\n      initializer=tf.truncated_normal_initializer(stddev=0.1))\r\n```", "Note that executing kernel for first time can be slow (up to a minute if you have video card with capability that TF didn't get compiled for). Try running it several times.\r\n\r\nAlso, doubling the number of GPUs seems to double PTX compilation time, so make sure you prewarm each GPU", "Oh yeah, I concur with @yaroslavvb.  I'd drop the first ~3 runs, and average over the subsequent tens of runs.", "@concretevitamin The input is [examples=100000, Nvars=2500], could the 'problem' be that, since the input is small and the network is simple, the GPU time (matmul, relu, etc.) is insignificant? I used your replacement code, but, alas, there was no change.\r\n\r\n@yaroslavvb I don't see any significant difference between many repeated runs. I also have a CNN (using different data) that I tried running in-between and that one had good performance.\r\n\r\n[Here](https://gist.github.com/arinbjornk/49e942c077fbc6cfdaa3a59c0df30926) is a larger section of the code, in case it helps.", "On my laptop I can generate about 200MB worth of truncated normal randoms in half a second. For your original issue of truncated normal taking 400ms, the question is -- how big is it, and how long you were expecting it to take.\r\n\r\nSession.run overhead can be 100 usec. If your random normal taking 400ms like in the original timeline then GPU time is definitely significant", "It is generating 20k (2500x8) randoms. In the MNIST example code, one of the layers has 3M weights and that runs a lot faster.\r\n\r\nI should have included the whole timeline output in the original post. [Here is the timeline](https://gist.github.com/arinbjornk/d83015a51d00a207d0ae20e37f280e12#file-slow_net_timeline-json-L479). The line of interest is 479. For comparison, the Mul ops are executing suspiciously quickly.", "So if calling `truncated_normal_initializer` to generate 20k randoms takes 400ms per call, and this timing doesn't go down after a few iterations, it would be a bug. A small self-contained repro case would be useful here, something that isolates the issue to `truncated_normal_initializer` (you can call it directly with `sess.run(tf.truncated_normal((n,)))`", "I can't replicate the issue in an isolated case. The first run of `truncated_normal_initializer` is slower as expected but subsequent runs are very fast. I suspect the issue lies elsewhere.\r\n\r\nWe can close this issue until I have more detailed information."]}, {"number": 9753, "title": "Much time cost from worker to ps server", "body": "I try to test the distribute speed, with one machine of 4 GPUs. I calculate one batch on each GPU, and add the four batch on the worker's cpu , and then average the four batch on ps CPU and update the parameters, I saved the timeline file, but there are no operations from workers CPU to ps CPU with a long time span in the timeline, I don`t know where the time gone? because of data transfer? but this is test in only one machine. the timeline is as follows, and the right up operation is average four data batch and updata parameters\r\n![webwxgetmsgimg](https://cloud.githubusercontent.com/assets/16236576/25800351/87f2acb4-341b-11e7-86a1-bfb888405d72.jpeg)\r\n\r\n\r\n\r\n\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 9752, "title": "contrib/verbs causes compile error on master", "body": "Recently merged RDMA-supporting feature is based on r1.0, and incompatible with the latest master.\r\n\r\n> ERROR: /data/chuangchen/workspace/tensorflow/tensorflow/contrib/verbs/BUILD:104:1: C++ compilation of rule '//tensorflow/contrib/verbs:rdma_rendezvous_mgr' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter ... (remaining 151 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\ntensorflow/contrib/verbs/rdma_rendezvous_mgr.cc: In constructor 'tensorflow::RdmaRemoteRendezvous::RdmaRemoteRendezvous(const tensorflow::WorkerEnv*, tensorflow::int64, tensorflow::RdmaMgr*)':\r\ntensorflow/contrib/verbs/rdma_rendezvous_mgr.cc:34:35: error: 'worker_name' was not declared in this scope\r\n       : BaseRemoteRendezvous(env, worker_name, step_id, true),\r\n                                   ^\r\ntensorflow/contrib/verbs/rdma_rendezvous_mgr.cc: In constructor 'tensorflow::RdmaRendezvousMgr::RdmaRendezvousMgr(const tensorflow::WorkerEnv*, const string&, tensorflow::WorkerCacheInterface*)':\r\ntensorflow/contrib/verbs/rdma_rendezvous_mgr.cc:139:41: error: no matching function for call to 'tensorflow::BaseRendezvousMgr::BaseRendezvousMgr(const tensorflow::WorkerEnv*&, const string&)'\r\n     : BaseRendezvousMgr(env, worker_name) {}\r\n                                         ^\r\ntensorflow/contrib/verbs/rdma_rendezvous_mgr.cc:139:41: note: candidate is:\r\nIn file included from ./tensorflow/contrib/verbs/rdma_rendezvous_mgr.h:22:0,\r\n                 from tensorflow/contrib/verbs/rdma_rendezvous_mgr.cc:18:\r\n./tensorflow/core/distributed_runtime/base_rendezvous_mgr.h:62:12: note: tensorflow::BaseRendezvousMgr::BaseRendezvousMgr(const tensorflow::WorkerEnv*)\r\n   explicit BaseRendezvousMgr(const WorkerEnv* worker_env);\r\n            ^\r\n./tensorflow/core/distributed_runtime/base_rendezvous_mgr.h:62:12: note:   candidate expects 1 argument, 2 provided\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\n", "comments": ["It's broken by [this commit](https://github.com/tensorflow/tensorflow/commit/f28935a7d280b6ba75fe93fe35783d87b9cc2ec9).\r\nThis is at least the second time of compile error. There should be a pre-commit check?", "@yifeif should we modify one of our configurations to configure with rdma?", "Is someone working on this?", "Is it solved ? I meet the same issue", "@DogNick I made a local fix. I can provide a PR.", "@llhe Nice!", "Thanks @llhe \r\n+1 on one configuration with RDMA check (we usually check the master with VERBS support but not every day)."]}, {"number": 9751, "title": "Make non-standard cudnn paths work again", "body": "This corrects #9634 for me.", "comments": ["Can one of the admins verify this patch?", "Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "Hmm this doesn't appear to be the right fix. It breaks the test builds with:\r\n\r\n tensorflow/stream_executor/cuda/cuda_dnn.cc:43:24: fatal error: cuda/cudnn.h: No such file or directory\r\n #include \"cuda/cudnn.h\"\r\n\r\n", "@sjperkins Can you take another look?", "Hey @rmlarsen. I had a quick look, but no luck. Apologies, I don't have more time  to spend on this at present :-(", "I was able to fix this a different way without changing the relative include path in `tensorflow/stream_executor/cuda/cuda_dnn.cc`:\r\n```diff --git a/third_party/gpus/cuda_configure.bzl b/third_party/gpus/cuda_configure.bzl\r\nindex 6994db0..dad0d83 100644\r\n--- a/third_party/gpus/cuda_configure.bzl\r\n+++ b/third_party/gpus/cuda_configure.bzl\r\n@@ -875,8 +875,8 @@ def _create_cuda_repository(repository_ctx):\r\n   included_files = _read_dir(repository_ctx, cuda_include_path).replace(\r\n       cuda_include_path, '').splitlines()\r\n   if '/cudnn.h' not in included_files:\r\n-    genrules.append(_symlink_genrule_for_dir(repository_ctx, None, \"\",\r\n-        \"cudnn-include\", [cudnn_header_dir + \"/cudnn.h\"], [\"include/cudnn.h\"]))\r\n+    genrules.append(_symlink_genrule_for_dir(repository_ctx, cudnn_header_dir,\r\n+        \"include\", \"cudnn-include\"))\r\n   else:\r\n     genrules.append(\r\n             'filegroup(\\n' +\r\n```\r\n\r\nWill continue testing to decide whether to open an alternative PR", "@jthestness yeah, that looks better. Feel free to submit an alternative PR.", "Closing in favour of alternative solutions. I'm not going to have time to look at this soon."]}, {"number": 9750, "title": "building error, tensorflow r1.0, with bazel 0.4.5, Ubuntu 16.04.1 LTS armv7 board", "body": "\r\n\r\nUpdate from #9632, @andydavis1 suggest upgrading to 1.1, but it seems to be worse:\r\n\r\n````\r\nodroid@odroid:~/local_DT_project/tensorflow_git$ git checkout r1.1\r\nBranch r1.1 set up to track remote branch r1.1 from origin.\r\nSwitched to a new branch 'r1.1'\r\nodroid@odroid:~/local_DT_project/tensorflow_git$ ./configure\r\nPlease specify the location of python. [Default is /usr/bin/python]:\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]:\r\nDo you wish to use jemalloc as the malloc implementation? [Y/n]\r\njemalloc enabled\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N]\r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N]\r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N]\r\nNo XLA support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  /usr/local/lib/python2.7/dist-packages\r\n  /usr/lib/python2.7/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\r\n\r\nUsing python library path: /usr/local/lib/python2.7/dist-packages\r\nDo you wish to build TensorFlow with OpenCL support? [y/N]\r\nNo OpenCL support will be enabled for TensorFlow \r\nDo you wish to build TensorFlow with CUDA support? [y/N]\r\nNo CUDA support will be enabled for TensorFlow   \r\nConfiguration finished\r\n......................................................................................................\r\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\r\n.....................................................................................................\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_dialog//': Error downloading [https://github.com/polymerelements/paper-dialog/archive/v1.0.4.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_dialog/v1.0.4.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_form_element_behavior//': Error downloading [https://github.com/polymerelements/iron-form-element-behavior/archive/v1.0.6.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_form_element_behavior/v1.0.6.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_collapse//': Error downloading [https://github.com/polymerelements/iron-collapse/archive/v1.0.8.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_collapse/v1.0.8.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_fit_behavior//': Error downloading [https://github.com/polymerelements/iron-fit-behavior/archive/v1.2.5.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_fit_behavior/v1.2.5.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_flex_layout//': Error downloading [https://github.com/polymerelements/iron-flex-layout/archive/v1.3.0.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_flex_layout/v1.3.0.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_dropdown//': Error downloading [https://github.com/polymerelements/iron-dropdown/archive/v1.4.0.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_dropdown/v1.4.0.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_checked_element_behavior//': Error downloading [https://github.com/polymerelements/iron-checked-element-behavior/archive/v1.0.4.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_checked_element_behavior/v1.0.4.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_behaviors//': Error downloading [https://github.com/polymerelements/iron-behaviors/archive/v1.0.17.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_behaviors/v1.0.17.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_autogrow_textarea//': Error downloading [https://github.com/polymerelements/iron-autogrow-textarea/archive/v1.0.12.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_autogrow_textarea/v1.0.12.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.  \r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_ajax//': Error downloading [https://github.com/polymerelements/iron-ajax/archive/v1.2.0.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_ajax/v1.2.0.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@dagre//': Error downloading [https://github.com/cpettitt/dagre/archive/v0.7.4.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/dagre/v0.7.4.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@font_roboto//': Error downloading [https://github.com/polymerelements/font-roboto/archive/v1.0.1.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/font_roboto/v1.0.1.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_a11y_keys_behavior//': Error downloading [https://github.com/polymerelements/iron-a11y-keys-behavior/archive/v1.1.8.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_a11y_keys_behavior/v1.1.8.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'. \r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_a11y_announcer//': Error downloading [https://github.com/polymerelements/iron-a11y-announcer/archive/v1.0.5.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_a11y_announcer/v1.0.5.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@graphlib//': Error downloading [https://github.com/cpettitt/graphlib/archive/v1.0.7.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/graphlib/v1.0.7.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@es6_promise//': Error downloading [https://github.com/components/es6-promise/archive/v2.1.0.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/es6_promise/v2.1.0.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@d3//': Error downloading [https://github.com/mbostock-bower/d3-bower/archive/v3.5.15.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/d3/v3.5.15.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@three_js_orbitcontrols_js//file': Error downloading [https://raw.githubusercontent.com/mrdoob/three.js/r77/examples/js/controls/OrbitControls.js] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/three_js_orbitcontrols_js/OrbitControls.js: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@web_animations_js//': Error downloading [https://github.com/web-animations/web-animations-js/archive/2.2.1.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/web_animations_js/2.2.1.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@weblas_weblas_js//file': Error downloading [https://raw.githubusercontent.com/waylonflinn/weblas/v0.9.0/dist/weblas.js] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/weblas_weblas_js/weblas.js: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@webcomponentsjs//': Error downloading [https://github.com/webcomponents/webcomponentsjs/archive/v0.7.22.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/webcomponentsjs/v0.7.22.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@three_js_three_min_js//file': Error downloading [https://raw.githubusercontent.com/mrdoob/three.js/r77/build/three.min.js] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/three_js_three_min_js/three.min.js: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@promise_polyfill//': Error downloading [https://github.com/polymerlabs/promise-polyfill/archive/v1.0.0.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/promise_polyfill/v1.0.0.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@polymer//': Error downloading [https://github.com/polymer/polymer/archive/v1.7.0.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/polymer/v1.7.0.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@plottable//': Error downloading [https://github.com/palantir/plottable/archive/v1.16.1.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/plottable/v1.16.1.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_tooltip//': Error downloading [https://github.com/polymerelements/paper-tooltip/archive/v1.1.2.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_tooltip/v1.1.2.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_toolbar//': Error downloading [https://github.com/polymerelements/paper-toolbar/archive/v1.1.4.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_toolbar/v1.1.4.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_toggle_button//': Error downloading [https://github.com/polymerelements/paper-toggle-button/archive/v1.2.0.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_toggle_button/v1.2.0.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_toast//': Error downloading [https://github.com/polymerelements/paper-toast/archive/v1.3.0.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_toast/v1.3.0.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_tabs//': Error downloading [https://github.com/polymerelements/paper-tabs/archive/v1.7.0.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_tabs/v1.7.0.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_styles//': Error downloading [https://github.com/polymerelements/paper-styles/archive/v1.1.4.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_styles/v1.1.4.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_toggle_button//': Error downloading [https://github.com/polymerelements/paper-toggle-button/archive/v1.2.0.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_toggle_button/v1.2.0.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_toast//': Error downloading [https://github.com/polymerelements/paper-toast/archive/v1.3.0.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_toast/v1.3.0.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_tabs//': Error downloading [https://github.com/polymerelements/paper-tabs/archive/v1.7.0.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_tabs/v1.7.0.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_styles//': Error downloading [https://github.com/polymerelements/paper-styles/archive/v1.1.4.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_styles/v1.1.4.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_spinner//': Error downloading [https://github.com/polymerelements/paper-spinner/archive/v1.1.1.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_spinner/v1.1.1.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_slider//': Error downloading [https://github.com/polymerelements/paper-slider/archive/v1.0.10.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_slider/v1.0.10.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_ripple//': Error downloading [https://github.com/polymerelements/paper-ripple/archive/v1.0.5.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_ripple/v1.0.5.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_radio_group//': Error downloading [https://github.com/polymerelements/paper-radio-group/archive/v1.0.9.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_radio_group/v1.0.9.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_radio_button//': Error downloading [https://github.com/polymerelements/paper-radio-button/archive/v1.1.2.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_radio_button/v1.1.2.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_progress//': Error downloading [https://github.com/polymerelements/paper-progress/archive/v1.0.9.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_progress/v1.0.9.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_menu_button//': Error downloading [https://github.com/polymerelements/paper-menu-button/archive/v1.5.1.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_menu_button/v1.5.1.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_menu//': Error downloading [https://github.com/polymerelements/paper-menu/archive/v1.2.2.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_menu/v1.2.2.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_material//': Error downloading [https://github.com/polymerelements/paper-material/archive/v1.0.6.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_material/v1.0.6.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_listbox//': Error downloading [https://github.com/polymerelements/paper-listbox/archive/v1.1.2.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_listbox/v1.1.2.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_item//': Error downloading [https://github.com/polymerelements/paper-item/archive/v1.1.4.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_item/v1.1.4.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_input//': Error downloading [https://github.com/polymerelements/paper-input/archive/v1.1.18.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_input/v1.1.18.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_icon_button//': Error downloading [https://github.com/polymerelements/paper-icon-button/archive/v1.1.3.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_icon_button/v1.1.3.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_header_panel//': Error downloading [https://github.com/polymerelements/paper-header-panel/archive/v1.1.4.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_header_panel/v1.1.4.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_dropdown_menu//': Error downloading [https://github.com/polymerelements/paper-dropdown-menu/archive/v1.4.0.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_dropdown_menu/v1.4.0.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_dialog_behavior//': Error downloading [https://github.com/polymerelements/paper-dialog-behavior/archive/v1.2.5.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_dialog_behavior/v1.2.5.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_dialog//': Error downloading [https://github.com/polymerelements/paper-dialog/archive/v1.0.4.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_dialog/v1.0.4.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_checkbox//': Error downloading [https://github.com/polymerelements/paper-checkbox/archive/v1.4.0.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_checkbox/v1.4.0.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_button//': Error downloading [https://github.com/polymerelements/paper-button/archive/v1.0.11.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_button/v1.0.11.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_behaviors//': Error downloading [https://github.com/polymerelements/paper-behaviors/archive/v1.0.12.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_behaviors/v1.0.12.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@numericjs_numeric_min_js//file': Error downloading [https://cdnjs.cloudflare.com/ajax/libs/numeric/1.2.6/numeric.min.js] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/numericjs_numeric_min_js/numeric.min.js: sun.security.validator.ValidatorException: PKIX path validation failed: java.security.cert.CertPathValidatorException: signature check failed and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@neon_animation//': Error downloading [https://github.com/polymerelements/neon-animation/archive/v1.2.2.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/neon_animation/v1.2.2.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@lodash//': Error downloading [https://github.com/lodash/lodash/archive/3.8.0.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/lodash/3.8.0.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_validatable_behavior//': Error downloading [https://github.com/polymerelements/iron-validatable-behavior/archive/v1.1.1.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_validatable_behavior/v1.1.1.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_selector//': Error downloading [https://github.com/polymerelements/iron-selector/archive/v1.5.2.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_selector/v1.5.2.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_scroll_target_behavior//': Error downloading [https://github.com/polymerelements/iron-scroll-target-behavior/archive/v1.0.3.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_scroll_target_behavior/v1.0.3.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_resizable_behavior//': Error downloading [https://github.com/polymerelements/iron-resizable-behavior/archive/v1.0.3.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_resizable_behavior/v1.0.3.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'. \r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_range_behavior//': Error downloading [https://github.com/polymerelements/iron-range-behavior/archive/v1.0.4.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_range_behavior/v1.0.4.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_overlay_behavior//': Error downloading [https://github.com/polymerelements/iron-overlay-behavior/archive/v1.10.1.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_overlay_behavior/v1.10.1.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_meta//': Error downloading [https://github.com/polymerelements/iron-meta/archive/v1.1.1.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_meta/v1.1.1.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_menu_behavior//': Error downloading [https://github.com/polymerelements/iron-menu-behavior/archive/v1.1.10.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_menu_behavior/v1.1.10.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_list//': Error downloading [https://github.com/polymerelements/iron-list/archive/v1.3.9.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_list/v1.3.9.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_input//': Error downloading [https://github.com/polymerelements/iron-input/archive/1.0.10.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_input/1.0.10.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_iconset_svg//': Error downloading [https://github.com/polymerelements/iron-iconset-svg/archive/v1.1.0.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_iconset_svg/v1.1.0.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_icons//': Error downloading [https://github.com/polymerelements/iron-icons/archive/v1.1.3.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_icons/v1.1.3.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_icon//': Error downloading [https://github.com/polymerelements/iron-icon/archive/v1.0.11.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_icon/v1.0.11.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: Evaluation of query \"deps(((//tensorflow/... - //tensorflow/contrib/nccl/...) - //tensorflow/examples/android/...))\" failed: errors were encountered while computing transitive closure.\r\n````\r\n\r\nAny help will be appreciated.", "comments": ["Can you run `bazel fetch ...`?", "Thanks @drpngx for your interest.\r\n\r\nI have the same kind of error.\r\n\r\n````\r\nodroid@odroid:~/local_DT_project/tensorflow_git$ bazel fetch //tensorflow/tensorboard/bower:bower\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@weblas_weblas_js//file': Error downloading [https://raw.githubusercontent.com/waylonflinn/weblas/v0.9.0/dist/weblas.js] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/weblas_weblas_js/weblas.js: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@d3//': Error downloading [https://github.com/mbostock-bower/d3-bower/archive/v3.5.15.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/d3/v3.5.15.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@weblas_weblas_js//file': Error downloading [https://raw.githubusercontent.com/waylonflinn/weblas/v0.9.0/dist/weblas.js] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/weblas_weblas_js/weblas.js: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@webcomponentsjs//': Error downloading [https://github.com/webcomponents/webcomponentsj\r\n.........\r\n`````\r\nand so on.\r\nWhat should I do?\r\nIs it possible to download manually or something else?", "@dandelionmane for the TensorBoard expertise.  @av8ramit  for the release expertise.", "Yes please upgrade to a more recent version of TensorFlow. In early January we added redundant mirrored URLs to our download configs. The new configuration is 99.9% reliable. It works even if GitHub goes down. Let us know if it still doesn't work."]}, {"number": 9749, "title": "tensorflow install for go", "body": "bojuzideMacBook-Pro:~ manman$ go get github.com/tensorflow/tensorflow/tensorflow/go github.com/tensorflow/tensorflow/tensorflow/go\r\nld: library not found for -ltensorflow\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nbojuzideMacBook-Pro:~ manman$ \r\n ", "comments": ["What is the fix for this - I've tried multiple approaches to installing libtensorflow including brew but no luck", "@angadn: you most likely forgot to:\r\n\r\n```bash\r\nexport LIBRARY_PATH=$LIBRARY_PATH:/usr/local\r\n```\r\n\r\nAfter [Installing TensorFlow on Mac OS X](https://www.tensorflow.org/install/install_mac) go through steps 2 and 3 of [Installing TensorFlow for Go](https://www.tensorflow.org/install/install_go).", "only thing i found that solved this same error for me was \r\n`brew install libtensorflow`", "@oziee Thank you this worked for me after a lot of other things didn't"]}, {"number": 9748, "title": "Segmentation fault on help(tf.train.SequenceExample)", "body": "I was trying to try and test the tf.train.SequenceExample class, but it wasn't in the official docs and using help crashed Python. Does anyone know how to use it or how to solve this issue?", "comments": ["Please provide a simple repro case and we can look into it.\r\n\r\nAlso, which version/OS/etc.", "My Tensorflow version is **1.1.0-rc2** (GPU, Python 3.5) on Ubuntu 16.04.\r\n\r\nSome information from `platform.uname()`:\r\nsystem='Linux', \r\nrelease='4.4.0-72-generic', \r\nversion='#93-Ubuntu SMP Fri Mar 31 14:07:41 UTC 2017',\r\nmachine='x86_64',\r\nprocessor='x86_64'\r\n\r\n```python\r\nimport tensorflow as tf\r\nhelp(tf.train.SequenceExample)\r\n```\r\nSegmentation fault (core dumped)\r\n\r\nThe same problem happened when I tried to call the related classes such as `help(tf.train.Feature)`. \r\nAlthough I eventually figured out how to use `tf.train.SequenceExample`, this remains an issue to other users that are not familiar with it. ", "@charlesnicholson could it be related to decorators?", "Sounds like a reasonable starting point, I'll take a look.", "@drpngx It doesn't seem like a tf_decorator issue.\r\n\r\nCompare\r\nhttps://github.com/tensorflow/tensorflow/tree/v1.1.0-rc2/tensorflow/python/util \r\nand\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/util\r\n\r\nSee that tf_decorator exists in 'master' but not in 1.1.0-rc2. I don't think the decorator work had landed when 1.1.0-rc2 was cut.", "@JeremyCCHsu is it just `help(tf.train.*)` or anything else? Can you call `tf.train.*`, say `get_global_step` for instance? Does `help(tf.*)` fail for anything else?", "To be more specific, segmentation fault only happened when I called `help(*)` with the following `tf.train.Example`-related classes:\r\n\r\n```python\r\n# tf.train.Example\r\n# tf.train.SequenceExample\r\n#\r\n#   tf.train.Features\r\n#   tf.train.FeatureLists\r\n#\r\n#       tf.train.Feature    \r\n#       tf.train.FeatureList\r\n#\r\n#         tf.train.Int64List\r\n#         tf.train.FloatList\r\n#         tf.train.BytesList\r\n```\r\n\r\nIn contrast, `help(*)` with the following worked normally.\r\n```python\r\ntf.train.AdamOptimizer\r\ntf.train.Coordinator\r\ntf.train.Saver\r\ntf.train.Supervisor\r\n\r\ntf.train.batch\r\ntf.train.queue_runner\r\n```\r\n", "It looks like it's a protobuf issue. We're working on that.\r\n\r\nIt should also crash with `tf.GraphDef`.\r\n\r\nI could not repro on my system, though. It could be because you have loaded another protobuf via `load_op_library`, or something to do with protobuf on your system.\r\n\r\nI'm closing in favor of #9525.", "(Helping to close this - please use #9525 for follow-ups.)"]}, {"number": 9747, "title": "'Tensor' object has no attribute 'initializer' after import from meta graph", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution**: Darwin Austins-MBP 16.5.0 Darwin Kernel Version 16.5.0: Fri Mar  3 16:52:33 PST 2017; root:xnu-3789.51.2~3/RELEASE_X86_64 x86_64\r\nMac OS X 10.12.4\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:v1.1.0-rc0-61-g1ec6ed5 1.1.0\r\n- **Bazel version (if compiling from source)**:0.4.5\r\n- **CUDA/cuDNN version**:None\r\n- **GPU model and memory**:None\r\n- **Exact command to reproduce**: Ref to Codes\r\n\r\n### tensorflow import \r\n    tf.VERSION = 1.1.0\r\n    tf.GIT_VERSION = v1.1.0-rc0-61-g1ec6ed5\r\n    tf.COMPILER_VERSION = v1.1.0-rc0-61-g1ec6ed5\r\n    Sanity check: array([1], dtype=int32)\r\n\r\n\r\n\r\n### Describe the problem\r\nAfter export and import a meta graph with uninitialized local variables,\r\nYou can not inittialize them with sess.run(tf. local_variables_initializer()), cause\r\nTF do not register variable's proto function with key 'LOCAL_VARIABLES' and when \r\nexport meta graph to protobuf, source code can not find to_proto function from repository.\r\n\r\n\r\n### Source code / logs\r\n```python\r\nimport tensorflow as tf\r\n\r\ngraph = tf.Graph()\r\nwith graph.as_default():\r\n    x = tf.Variable(1, collections=[tf.GraphKeys.LOCAL_VARIABLES])\r\n    y = tf.Variable(1)\r\n    z = x + y\r\norigin_meta_graph = tf.train.export_meta_graph(graph=graph)\r\nnew_graph = tf.Graph()\r\nwith new_graph.as_default():\r\n    tf.train.import_meta_graph(origin_meta_graph)\r\n    init = tf.local_variables_initializer()\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n```\r\n\r\n```bash\r\nTraceback (most recent call last):\r\n  File \"/Users/austin/workspace/aip/3rd/tensorflow/tensorflow/test.py\", line 12, in <module>\r\n    init = tf.local_variables_initializer()\r\n  File \"/Users/austin/workspace/aip/3rd/tensorflow/venv/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 1184, in local_variables_initializer\r\n    return variables_initializer(local_variables())\r\n  File \"/Users/austin/workspace/aip/3rd/tensorflow/venv/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 1149, in variables_initializer\r\n    return control_flow_ops.group(*[v.initializer for v in var_list], name=name)\r\n  File \"/Users/austin/workspace/aip/3rd/tensorflow/venv/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 1149, in <listcomp>\r\n    return control_flow_ops.group(*[v.initializer for v in var_list], name=name)\r\nAttributeError: 'Tensor' object has no attribute 'initializer'\r\n```\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ngraph = tf.Graph()\r\nwith graph.as_default():\r\n    x = tf.Variable(1, collections=[tf.GraphKeys.LOCAL_VARIABLES])\r\n    y = tf.Variable(1)\r\n    z = x + y\r\norigin_meta_graph = tf.train.export_meta_graph(graph=graph)\r\nnew_graph = tf.Graph()\r\nwith new_graph.as_default():\r\n    tf.train.import_meta_graph(origin_meta_graph)\r\nprint(graph.get_collection(tf.GraphKeys.LOCAL_VARIABLES))\r\nprint(new_graph.get_collection(tf.GraphKeys.LOCAL_VARIABLES))\r\n```\r\n\r\n```bash\r\n[<tf.Variable 'Variable:0' shape=() dtype=int32_ref>]\r\n[<tf.Tensor 'Variable:0' shape=() dtype=int32_ref>]\r\n```\r\nAs it show above, in origin graph local_variable collection is a list of **tf.Variable**\r\nbut in the new graph, is a list of **tf.Tensor**\r\n\r\n### Work around\r\nAdd following registration in your model core\r\nOR Ref to this [PR](https://github.com/tensorflow/tensorflow/pull/9674)\r\n\r\n```python\r\nfrom tensorflow.core.framework import variable_pb2\r\nfrom tensorflow.python.framework import ops\r\nfrom tensorflow.python.ops import variables\r\nfrom tensorflow.python.framework.ops import register_proto_function\r\n\r\nregister_proto_function(\r\n    ops.GraphKeys.LOCAL_VARIABLES,\r\n    proto_type=variable_pb2.VariableDef,\r\n    to_proto=variables.Variable.to_proto,\r\n    from_proto=variables.Variable.from_proto)\r\n```", "comments": ["`tf.local_varliabe_initializer()` might not be the right spelling.", "@josh11b Looks like metagraph import isn't creating Variable objects.  Any ideas why?", "I'm just facing the same problem. To be more precise, I'm using [tf.metrics.accuracy()](https://www.tensorflow.org/api_docs/python/tf/metrics/accuracy) in my training script, which uses local count variables internally.\r\n\r\n```\r\n_, accuracy_ = tf.metrics.accuracy(labels=y_ph, predictions=tf.argmax(model_y, axis=1))\r\ntf.add_to_collection('accuracy', accuracy_)\r\n```\r\n\r\nAfter saving and restoring the graph using [tf.train.import_meta_graph()](https://www.tensorflow.org/api_docs/python/tf/train/import_meta_graph), I'm not able so reuse the accuracy metric, because the required call to [tf.initialize_local_variables()](https://www.tensorflow.org/versions/master/api_docs/python/tf/initialize_local_variables) fails as desribed  @austinzh.\r\n\r\n```\r\nwith tf.Session() as sess:\r\n    new_saver = tf.train.import_meta_graph('checkpoint/model.ckpt.meta')\r\n    new_saver.restore(sess, 'checkpoint/model.ckpt')\r\n    # ...\r\n    accuracy_ = tf.get_collection('accuracy')[0]\r\n    sess.run(tf.local_variables_initializer())\r\n    accuracy = sess.run(accuracy_, feed_dict={...})\r\n```\r\n\r\n\r\nis there already any progress on this?\r\n\r\n**EDIT:**\r\nOS: Ubuntu 16.04\r\nTF-Version: r1.1 (native pip, Python 2.7) CPU only", "Same case for me. I'm using tf.metrics.accuracy() in my training process.\r\n```\r\nself.accuracy_metrics, self.update_accuracy_metrics = tf.metrics.accuracy(self.label,\r\n                                                                                  self.one_hot,\r\n                                                                                  name='accuracy')\r\n```\r\n\r\nWhen I try to restore the saved model:\r\n```\r\nnew_saver = tf.train.import_meta_graph(save_path + '.meta')\r\n    new_saver.restore(sess, save_path)\r\n    graph = tf.get_default_graph()\r\n```\r\n and use the function accuracy again:\r\n```\r\naccuracy_update = graph.get_operation_by_name('accuracy/update_op').outputs[0]\r\n\r\ninput_data, output_data = validator.read_batch(config['READ_BUFFER_SIZE'], sys.stdin,\r\n                                                       config['SEPARATOR'], config['OUTPUT_SIZE'])\r\ninput_data = input_data.reshape(input_data.shape[0], config['SEQ_LENGTH'], config['N_FEATURES'])\r\noutput_data = output_data.reshape(output_data.shape[0], config['OUTPUT_SIZE'])\r\n\r\naccuracy = sess.run(accuracy_update, {\"data:0\": input_data, \"target:0\": output_data,\r\n                                                         \"keep_prob:0\": config['KEEP_PROB']})\r\nprint(accuracy)\r\n```\r\nit fails:\r\n\r\n> Traceback (most recent call last):\r\n>   File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\", line 1022, in _do_call\r\n>     return fn(*args)\r\n>   File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\", line 1004, in _run_fn\r\n>     status, run_metadata)\r\n>   File \"/usr/lib/python3.4/contextlib.py\", line 66, in __exit__\r\n>     next(self.gen)\r\n>   File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n>     pywrap_tensorflow.TF_GetCode(status))\r\n> tensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value accuracy/count_1\r\n> \t [[Node: accuracy/AssignAdd_3 = AssignAdd[T=DT_FLOAT, _class=[\"loc:@accuracy/count_1\"], use_locking=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](accuracy/count_1, accuracy/ToFloat_2)]]\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"main.py\", line 70, in <module>\r\n>     main()\r\n>   File \"main.py\", line 46, in main\r\n>     {\"data:0\":input_data.reshape(200, 10, 16), \"label_aux:0\": output_data, \"keep_prob:0\": config['KEEP_PROB']})\r\n>   File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\", line 767, in run\r\n>     run_metadata_ptr)\r\n>   File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\", line 965, in _run\r\n>     feed_dict_string, options, run_metadata)\r\n>   File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\", line 1015, in _do_run\r\n>     target_list, options, run_metadata)\r\n>   File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\", line 1035, in _do_call\r\n>     raise type(e)(node_def, op, message)\r\n> tensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value accuracy/count_1\r\n> \t [[Node: accuracy/AssignAdd_3 = AssignAdd[T=DT_FLOAT, _class=[\"loc:@accuracy/count_1\"], use_locking=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](accuracy/count_1, accuracy/ToFloat_2)]]\r\n> \r\n> Caused by op 'accuracy/AssignAdd_3', defined at:\r\n>   File \"main.py\", line 70, in <module>\r\n>     main()\r\n>   File \"main.py\", line 43, in main\r\n>     accuracy_metrics, update_accuracy_metrics = tf.metrics.accuracy(label_operation, one_hot)\r\n>   File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/metrics_impl.py\", line 332, in accuracy\r\n>     updates_collections, name or 'accuracy')\r\n>   File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/metrics_impl.py\", line 267, in mean\r\n>     update_count_op = state_ops.assign_add(count, num_values)\r\n>   File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/gen_state_ops.py\", line 75, in assign_add\r\n>     use_locking=use_locking, name=name)\r\n>   File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\r\n>     op_def=op_def)\r\n>   File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/ops.py\", line 2327, in create_op\r\n>     original_op=self._default_original_op, op_def=op_def)\r\n>   File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/ops.py\", line 1226, in __init__\r\n>     self._traceback = _extract_stack()\r\n> \r\n> FailedPreconditionError (see above for traceback): Attempting to use uninitialized value accuracy/count_1\r\n> \t [[Node: accuracy/AssignAdd_3 = AssignAdd[T=DT_FLOAT, _class=[\"loc:@accuracy/count_1\"], use_locking=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](accuracy/count_1, accuracy/ToFloat_2)]]\r\n> \r\n> ", "@sherrym Any ideas?", "@bsautermeister any solution for this problem? I face the same problem as yours. ", "@AlbertXiebnu Unfortunately no. But I have not check if this has been fixed in TF 1.2. I try to check that in the next days...", "@bsautermeister I have tried in TF1.2, seems that still not work.", "Same issue here; for me, it seems to be related to https://github.com/tensorflow/tensorflow/issues/1045. When I set num_epochs to a value other than None in the file where I save the model, I will get the \"'Tensor' object has no attribute 'initializer'\" error in the file where I restore the model.\r\n\r\n(TF 1.2, Python 3.6.1+)", "Same issue here, while restoring a graph and using string input producer to evaluate model , first I get \r\n\"Attempting to use uninitialized value input_producer/limit_epochs/epochs \t [[Node: input_producer/limit_epochs/CountUpTo = CountUpTo[T=DT_INT64, _class=[\"loc:@input_producer/limit_epochs/epochs\"], limit=1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_producer/limit_epochs/epochs)]]\"\r\n\r\nIf to fix above issue I run tf.local_variable_intializer() then I get  \"'Tensor' object has no attribute 'initializer' error on (TF 1.2, Python 3.6.1+)", "I did manage to find a work around, upon restoring i removed queue_runners and local_variables from restored graph and it worked \r\nget_default_graph().clear_collection(\"queue_runners\")\r\n      get_default_graph().clear_collection(\"local_variables\")", "@gauravsindhwani , can you still run the code successfully after you remove these two collections since they are actually part of the graph. I try your way but find the program is stuck after restoring.", "@gauravsindhwani, this approach seams working ok for me. I use streaming_accuracy and it seems create many local variable \"accuracy\", which are no longer used. So remove local_variables seems to be a good way to hack it.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activityand the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "@suharshs has been working on MetaGraphDef related stuff, and might have some ideas.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It works well on TF 1.4.1, it seems this issue have been fixed by https://github.com/tensorflow/tensorflow/commit/2d1823f7be0d65d4ee28b7c6ea2acaac01f1db61 . Thanks.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly."]}, {"number": 9746, "title": "#8263-Update recurrent.md", "body": "Attempts to add some clarity to tutorial.\r\nAdds example to aid in understanding.\r\n\r\nFix #8263.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@ebrevdo looks good to me, but I'd like an authoritative opinion.\r\n\r\n@CarbonComputed Can you wrap the text at 80 characters?", "Can one of the admins verify this patch?", "@ebrevdo this looks ready to go, do you agree?", "Will look tomorrow.\n\nOn May 16, 2017 10:10 AM, \"Rasmus Munk Larsen\" <notifications@github.com>\nwrote:\n\n> Assigned #9746 <https://github.com/tensorflow/tensorflow/pull/9746> to\n> @ebrevdo <https://github.com/ebrevdo>.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/9746#event-1084552459>, or mute\n> the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim99KmBOzghl6xQB9tXra4pENH_tYks5r6diagaJpZM4NTgsz>\n> .\n>\n", "@tensorflow-jenkins test this please"]}, {"number": 9745, "title": "Feature request: distance between each pair of the two collections of inputs. Analog of scipy.spatial.distance.cdist", "body": "is it possible to add a function to calculate a distance between each pair of the two collections of inputs. Scipy has it implemented in [distance.sdist](https://docs.scipy.org/doc/scipy/reference/generated/generated/scipy.spatial.distance.cdist.html) and supports many distance metrics.\r\n\r\nIs it possible to have this function in TF? I have made an implementation for [Eucledian distance](http://stackoverflow.com/a/43839605/1090562) but have no idea how to implement it for other metrics.\r\n", "comments": ["This seems a bit baroque for inclusion in TensorFlow.  Implementing the other metrics seems pretty easy, but I don't see the motivation for the whole list in core.", "It is needed in the neural style transfer when calculating style loss.\r\nIs there any new updates on this?", "We would use this in molecular dynamics @girving ... possible to reopen?\r\n\r\nA naive implementation with linear algebra works but often maxes out GPU memory. It would be ideal to have an implementation of this which breaks big problems into chunks", "ref: https://github.com/tensorflow/tensorflow/issues/30659"]}, {"number": 9744, "title": "Adding a random seed parameter for tensorflow.layer.dense", "body": "Hi,\r\nI noticed that the `tensorflow.layer.dense` wrapper does not have a `seed` parameter, and I was wondering what you think about adding one. E.g., this seed would be used to initialize the random weights and bias units (if they are not initialized to zero).\r\n\r\nRelated to the points above,  it is currently also not clear how the weights are initialized. I.e., what distribution the random numbers are drawn from (if not zero). Does someone have some info on this? -- maybe this could be added to the API docs. \r\n\r\nWhat do you think? (I am happy to contribute a random seed implementation and a doc update if that's desirable.)", "comments": ["Short answer to the seed request: it is unlikely this will be accepted, since TensorFlow's universal convention has been to use \"initializers\" instead of a simple seed.  For example, see [init_ops.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/init_ops.py).\r\n\r\nThat said, we should perhaps document the default inits used in `dense()` docstring.  @fchollet - mind taking a look?", "The way to get deterministic weight initializations in `dense` is to pass a `kernel_initializer` argument set to an initializer (instance of a class from `init_ops`) that has a fixed `seed`.\r\n\r\nIf `kernel_initializer` is not set, we use the global default weight initializer, which is `glorot_uniform`. But I think it would be good to have it be explicitly present in the `dense` signature instead of relying on an implicit global default.", "Thanks for the clarification. So the following would be the current default behavior based on the global default weight initializer?\r\n\r\n```python\r\nimport tensorflow\r\nfrom tensorflow.python.ops.init_ops import glorot_uniform_initializer\r\n\r\ntensorflow.layers.dense(..., kernel_initializer=glorot_uniform_initializer())\r\n```\r\n\r\nI am happy to put together a PR to add a note to the docstrings of `Dense` and `dense`. When I understand correctly, you are suggesting to use `glorot_uniform_initializer` directly in `Dense`, i.e., via sth like\r\n\r\n```python\r\nif kernel_initializer is None:\r\n    self.kernel_initializer = glorot_uniform_initializer()\r\nelse:\r\n    self.kernel_initializer = kernel_initializer\r\n```\r\n\r\nNot sure what the current TensorFlow convention is; would it be okay to do these checks & assignments in the class `__init__`s directly?", "This code snippet in `__init__` would change neither the behavior of the layer nor its docstring and signature.  For the time being I would simply mention the default initializer in the docstring.", "Okay, will open a PR tomorrow.\r\n\r\n> This code snippet in __init__ would change neither the behavior of the layer nor its docstring\r\n\r\nI saw that in `Dense`'s `__init__` and the `dense` function, the bias initializer is set manually as well:\r\n\r\n```python\r\ndef dense(\r\n    inputs, units,\r\n    activation=None,\r\n    use_bias=True,\r\n    kernel_initializer=None,\r\n    bias_initializer=init_ops.zeros_initializer(),\r\n    kernel_regularizer=None,\r\n    bias_regularizer=None,\r\n    activity_regularizer=None,\r\n    trainable=True,\r\n    name=None,\r\n    reuse=None):\r\n```\r\n\r\nI am curious why setting `kernel_initializer=init_ops.glorot_uniform_initializer()` there wouldn't have an effect? I couldn't find anything in `Dense` and `dense` that would overwrite this setting ...", "> I am curious why setting kernel_initializer=init_ops.glorot_uniform_initializer() there wouldn't have an effect?\r\n\r\nWe should avoid instantiating anything in a class constructor signature, because the instance becomes global to all instances of the class. There are no immediate side effects in this case but it is bad practice and potentially dangerous for the user. For the same reason `bias_initializer=init_ops.zeros_initializer()`  should be avoided, but it is there for legacy reasons.\r\n\r\nWe should instantiate the initializers in the `__init__` of the class (so as to create new objects for every layer instance rather than a shared object everywhere). Keras achieves this by using default `initializer` argument values that are serial, i.e. that are simply the names of the initializers, which are then instantiated in the `__init__`.\r\n", "Ah thanks, makes sense now!\r\n\r\n> This code snippet in __init__ would change neither the behavior of the layer nor its docstring and signature. For the time being I would simply mention the default initializer in the docstring.\r\n\r\nYeah, previously, I think I misinterpreted this. I thought you meant that it wouldn't have an effect setting something in `__init__` but when I understand correctly now, you meant that it already uses glorot_uniform, so that this setting this in `__init__` wouldn't change anything compared to the current behavior. Only reason I thought this might be useful is that it might not be clear for someone looking at the code what the default weight initializer is, but yeah, I think leaving it as is and documenting it in the docstring should be fine.", "Closing since specifying the initializer is the right solution."]}, {"number": 9743, "title": "Use hash url to memorize the currently opened tab.", "body": "We already do this internally. This enables it for the open-source version. See issue: #4354\r\nChange: 153167734\r\n\r\nCreating pull request against r1.1 because it will fix https://github.com/tensorflow/tensorflow/issues/9285", "comments": ["Jenkins, test this please.", "Jenkins, test this please.", "Can one of the admins verify this patch?", "Jenkins, test this please.\r\n"]}, {"number": 9742, "title": "Optimized compiled Tensorflow uses almost 2x more memory than non-optimized binary", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Both\r\n- **TensorFlow version (use command below)**: ('v1.0.0-65-g4763edf-dirty', '1.0.1') (compiled at the same git commit of the binary version)\r\n- **Bazel version (if compiling from source)**: 0.4.5\r\n- **CUDA/cuDNN version**: N/A (CPU only)\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\n\r\n```python\r\nimport pdb\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndef get_layer(input_size, output_size, name):\r\n    # W_val is loaded from a file using numpy.load\r\n    W_val = np.random.normal(scale=0.1, size=(input_size, output_size)).astype(np.float32)\r\n    W = tf.get_variable(name='W_{}'.format(name), shape=(input_size, output_size),\r\n                        initializer=tf.constant_initializer(value=W_val, dtype=tf.float32),\r\n                        dtype=tf.float32)\r\n    b = tf.get_variable(name='b_{}'.format(name), shape=(output_size,),\r\n                        initializer=tf.constant_initializer(value=0.1, dtype=tf.float32),\r\n                        dtype=tf.float32)\r\n    return W, b\r\n\r\nsessions = []\r\nfor i in range(3):\r\n    g = tf.Graph()\r\n    with g.as_default():\r\n        W1, b1 = get_layer(158238, 900, '1')\r\n        W2, b2 = get_layer(900, 1000, '2')\r\n        W3, b3 = get_layer(1000, 1, '3')\r\n\r\n        init = tf.global_variables_initializer()\r\n    session = tf.Session(graph=g)\r\n    session.run(init)\r\n    print 'Loaded {}'.format(i)\r\n    sessions.append(session)\r\n\r\npdb.set_trace()\r\n```\r\n\r\n### Describe the problem\r\nAfter running the code snippet under the non-optimized binary Tensorflow installation, the used memory is ~6GB. However, when the same snippet is run with Tensorflow compiled with `-c opt --copt=-march=native` directives, the memory usage is ~11GB (1.8x larger).\r\n\r\nNote that there's no memory-usage difference when I use `tf.truncated_normal(stddev=0.1...)` instead of `tf.constant_initializer` with a numpy array.\r\n\r\nNot sure if this is a bug, or a side-effect of the optimized version or if there's something I can do to optimize memory usage?\r\n\r\n### Source code / logs\r\nI can attach chrome traces if necessary.", "comments": ["@tfboyd is there anyone who could take a look?", "Could you try to disable jemalloc?", "Which CPU are you using?  If we repro, I would like to compile with the same optimizations.  Thank you.  I ask because if I repo with AVX but the issue is with AVX2 for example I would not see it.  ", "Haven't tried with jemalloc disabled yet but to answer the question about CPU it's an Amazon ec2 instance with Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz.", "Just re-compiled with `TF_NEED_JEMALLOC` environment variable set to 0, memory usage is the same (i.e. equal to the compiled version with jemalloc enabled).", "What if you run with tcmalloc?\n\nsudo apt-get install google-perftools\nexport LD_PRELOAD=\"/usr/lib/libtcmalloc.so.4\"\n\n\nOn Mon, May 8, 2017 at 7:53 AM, Fayez <notifications@github.com> wrote:\n\n> Just re-compiled with TF_NEED_JEMALLOC environment variable set to 0,\n> memory usage is the same (i.e. equal to the compiled version with jemalloc\n> enabled).\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9742#issuecomment-299889130>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AYoxVP-OdUbvKvzohzA4BcwrxC0AgK-mks5r3yxmgaJpZM4NTSLO>\n> .\n>\n", "Tested with tcmalloc on both jemalloc enabled and disabled versions, memory usage is the same as well. Here's the output log with tcmalloc:\r\n\r\n```\r\n~# LD_PRELOAD=\"/usr/lib/libtcmalloc.so.4\" python test.py\r\ntcmalloc: large alloc 1139318784 bytes == 0x3488000 @  0x7f268db1fd1b 0x7f268be9835c 0x7f268becfe73 0x7f268bed2b19 0x7f268bf4b1e8 0x7f263b52ece5 0x7f263b571c00 0x4cada2 0x4c9d8f 0x4c2765 0x4c2509 0x4f1def 0x4ec652 0x4eae31 0x49e14a 0x7f268d527830 0x49d9d9 (nil)\r\nLoaded 0\r\nLoaded 1\r\nLoaded 2\r\n```", "Great! Could you follow the [pprof instructions](http://goog-perftools.sourceforge.net/doc/heap_profiler.html) to understand what does the big allocations?\r\n", "I ran the profiler tool against the pip version and the compiled version. First, here's the printed messages for each:\r\n\r\n### Pip version\r\n```\r\n~# LD_PRELOAD=\"/usr/lib/libtcmalloc.so.4\" HEAPPROFILE=/root/tf_profile python test.py\r\nStarting tracking the heap\r\nStarting tracking the heap\r\ntcmalloc: large alloc 1139318784 bytes == 0x4326000 @  0x7fa0319cfd1b 0x7fa02fa7835c 0x7fa02faafe73 0x7fa02fab2b19 0x7fa02fb2b1e8 0x7f9fdef7ece5 0x7f9fdefc1c00 0x4cada2 0x4c9d8f 0x4c2765 0x4c2509 0x4f1def 0x4ec652 0x4eae31 0x49e14a 0x7fa0313d7830 0x49d9d9 (nil)\r\nDumping heap profile to /root/tf_profile.0001.heap (1387 MB allocated cumulatively, 1105 MB currently in use)\r\nDumping heap profile to /root/tf_profile.0002.heap (1649 MB currently in use)\r\nDumping heap profile to /root/tf_profile.0003.heap (3016 MB allocated cumulatively, 1649 MB currently in use)\r\nDumping heap profile to /root/tf_profile.0004.heap (2192 MB currently in use)\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nDumping heap profile to /root/tf_profile.0005.heap (4676 MB allocated cumulatively, 1657 MB currently in use)\r\nDumping heap profile to /root/tf_profile.0006.heap (5769 MB allocated cumulatively, 1657 MB currently in use)\r\nDumping heap profile to /root/tf_profile.0007.heap (Exiting, 561 MB in use)\r\n```\r\n\r\n### Compiled version\r\n```\r\n~# LD_PRELOAD=\"/usr/lib/libtcmalloc.so.4\" HEAPPROFILE=/root/tf_profile python test.py\r\nStarting tracking the heap\r\nStarting tracking the heap\r\ntcmalloc: large alloc 1139318784 bytes == 0x3c70000 @  0x7f4c16a35d1b 0x7f4c14ade35c 0x7f4c14b15e73 0x7f4c14b18b19 0x7f4c14b911e8 0x7f4bc3fd4ce5 0x7f4bc4017c00 0x4cada2 0x4c9d8f 0x4c2765 0x4c2509 0x4f1def 0x4ec652 0x4eae31 0x49e14a 0x7f4c1643d830 0x49d9d9 (nil)\r\nDumping heap profile to /root/tf_profile.0001.heap (1390 MB allocated cumulatively, 1105 MB currently in use)\r\nDumping heap profile to /root/tf_profile.0002.heap (1649 MB currently in use)\r\nDumping heap profile to /root/tf_profile.0003.heap (3020 MB allocated cumulatively, 1649 MB currently in use)\r\nDumping heap profile to /root/tf_profile.0004.heap (2192 MB currently in use)\r\nDumping heap profile to /root/tf_profile.0005.heap (4680 MB allocated cumulatively, 1657 MB currently in use)\r\nDumping heap profile to /root/tf_profile.0006.heap (5777 MB allocated cumulatively, 2754 MB currently in use)\r\nDumping heap profile to /root/tf_profile.0007.heap (6867 MB allocated cumulatively, 2751 MB currently in use)\r\nDumping heap profile to /root/tf_profile.0008.heap (7960 MB allocated cumulatively, 2751 MB currently in use)\r\nDumping heap profile to /root/tf_profile.0009.heap (Exiting, 561 MB in use)\r\n```\r\nUp until heap 4 in both cases, memory usage is the same and both graphs (generated by `google-pprof` tool) are exactly the same based on a quick look.\r\n\r\nAt heap 5, the memory usage is still the same but in the compiled version's case I see some parallelization in the graph where there are 2 nodes with ~540MB each instead of 1 node with 1090MB in the pip version's case.\r\n\r\nHeap 6 is where the memory usage doubles in the compiled version's case. In the pip version there's 1 big unnamed node with 1637.6MB. However, in the compiled version there are 3 nodes with ~500MB each (`initcmath` + 2 other unnamed nodes) + a fourth large node called `tensorflow PartialRunSetupRequest MergePartialFromCodedStream` with 1093.8MB usage. I didn't see this particular node in any of the heap files of the pip-version. I can attach a screenshot but it becomes unreadable when I zoom out enough to view the whole thing. Let me know if you'd like me to attach the heap files.\r\n\r\nIn heap 7 of the compiled version, that same node appears with double the memory as well (2183.7MB).\r\n\r\nI used a simplified version of my original snippet to generate the heap files:\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndef get_layer(input_size, output_size, name):\r\n    # supposedly loaded from a saved file\r\n    W_val = np.random.normal(scale=0.1, size=(input_size, output_size)).astype(np.float32)\r\n    W = tf.get_variable(name='W_{}'.format(name), shape=(input_size, output_size),\r\n                        initializer=tf.constant_initializer(value=W_val, dtype=tf.float32),\r\n                        dtype=tf.float32)\r\n    b = tf.get_variable(name='b_{}'.format(name), shape=(output_size,),\r\n                        initializer=tf.constant_initializer(value=0.1, dtype=tf.float32),\r\n                        dtype=tf.float32)\r\n    return W, b\r\n\r\ng = tf.Graph()\r\nwith g.as_default():\r\n    W1, b1 = get_layer(158238, 900, '1')\r\n    W2, b2 = get_layer(900, 1000, '2')\r\n    W3, b3 = get_layer(1000, 1, '3')\r\n    init = tf.global_variables_initializer()\r\nsession = tf.Session(graph=g)\r\nsession.run(init)\r\nsession.close()\r\n```\r\n\r\n", "Could it be that one of the version is using Python implementation of\nprotobuf instead of cpp?\n\nTry this\npython -c \"from google.protobuf.internal import api_implementation;\nprint(api_implementation._default_implementation_type)\"\n\nIt should print \"cpp\"\n\nOn Mon, May 8, 2017 at 12:30 PM, Fayez <notifications@github.com> wrote:\n\n> I ran the profiler tool against the pip version and the compiled version.\n> First, here's the printed messages for each:\n> Pip version\n>\n> ~# LD_PRELOAD=\"/usr/lib/libtcmalloc.so.4\" HEAPPROFILE=/root/tf_profile python test.py\n> Starting tracking the heap\n> Starting tracking the heap\n> tcmalloc: large alloc 1139318784 bytes == 0x4326000 @  0x7fa0319cfd1b 0x7fa02fa7835c 0x7fa02faafe73 0x7fa02fab2b19 0x7fa02fb2b1e8 0x7f9fdef7ece5 0x7f9fdefc1c00 0x4cada2 0x4c9d8f 0x4c2765 0x4c2509 0x4f1def 0x4ec652 0x4eae31 0x49e14a 0x7fa0313d7830 0x49d9d9 (nil)\n> Dumping heap profile to /root/tf_profile.0001.heap (1387 MB allocated cumulatively, 1105 MB currently in use)\n> Dumping heap profile to /root/tf_profile.0002.heap (1649 MB currently in use)\n> Dumping heap profile to /root/tf_profile.0003.heap (3016 MB allocated cumulatively, 1649 MB currently in use)\n> Dumping heap profile to /root/tf_profile.0004.heap (2192 MB currently in use)\n> W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\n> W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n> W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n> W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n> W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n> W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n> Dumping heap profile to /root/tf_profile.0005.heap (4676 MB allocated cumulatively, 1657 MB currently in use)\n> Dumping heap profile to /root/tf_profile.0006.heap (5769 MB allocated cumulatively, 1657 MB currently in use)\n> Dumping heap profile to /root/tf_profile.0007.heap (Exiting, 561 MB in use)\n>\n> Compiled version\n>\n> ~# LD_PRELOAD=\"/usr/lib/libtcmalloc.so.4\" HEAPPROFILE=/root/tf_profile python test.py\n> Starting tracking the heap\n> Starting tracking the heap\n> tcmalloc: large alloc 1139318784 bytes == 0x3c70000 @  0x7f4c16a35d1b 0x7f4c14ade35c 0x7f4c14b15e73 0x7f4c14b18b19 0x7f4c14b911e8 0x7f4bc3fd4ce5 0x7f4bc4017c00 0x4cada2 0x4c9d8f 0x4c2765 0x4c2509 0x4f1def 0x4ec652 0x4eae31 0x49e14a 0x7f4c1643d830 0x49d9d9 (nil)\n> Dumping heap profile to /root/tf_profile.0001.heap (1390 MB allocated cumulatively, 1105 MB currently in use)\n> Dumping heap profile to /root/tf_profile.0002.heap (1649 MB currently in use)\n> Dumping heap profile to /root/tf_profile.0003.heap (3020 MB allocated cumulatively, 1649 MB currently in use)\n> Dumping heap profile to /root/tf_profile.0004.heap (2192 MB currently in use)\n> Dumping heap profile to /root/tf_profile.0005.heap (4680 MB allocated cumulatively, 1657 MB currently in use)\n> Dumping heap profile to /root/tf_profile.0006.heap (5777 MB allocated cumulatively, 2754 MB currently in use)\n> Dumping heap profile to /root/tf_profile.0007.heap (6867 MB allocated cumulatively, 2751 MB currently in use)\n> Dumping heap profile to /root/tf_profile.0008.heap (7960 MB allocated cumulatively, 2751 MB currently in use)\n> Dumping heap profile to /root/tf_profile.0009.heap (Exiting, 561 MB in use)\n>\n> Up until heap 4 in both cases, memory usage is the same and both graphs\n> (generated by google-pprof tool) are exactly the same based on a quick\n> look.\n>\n> At heap 5, the memory usage is still the same but in the compiled\n> version's case I see some parallelization in the graph where there are 2\n> nodes with ~540MB each instead of 1 node with 1090MB in the pip version's\n> case.\n>\n> Heap 6 is where the memory usage doubles in the compiled version's case.\n> In the pip version there's 1 big unnamed node with 1637.6MB. However, in\n> the compiled version there are 3 nodes with ~500MB each (initcmath + 2\n> other unnamed nodes) + a fourth large node called tensorflow\n> PartialRunSetupRequest MergePartialFromCodedStream with 1093.8MB usage. I\n> didn't see this particular node in any of the heap files of the\n> pip-version. I can attach a screenshot but it becomes unreadable when I\n> zoom out enough to view the whole thing. Let me know if you'd like me to\n> attach the heap files.\n>\n> In heap 7 of the compiled version, that same node appears with double the\n> memory as well (2183.7MB).\n>\n> I used a simplified version of my original snippet to generate the heap\n> files:\n>\n> import numpy as npimport tensorflow as tf\n> def get_layer(input_size, output_size, name):\n>     # supposedly loaded from a saved file\n>     W_val = np.random.normal(scale=0.1, size=(input_size, output_size)).astype(np.float32)\n>     W = tf.get_variable(name='W_{}'.format(name), shape=(input_size, output_size),\n>                         initializer=tf.constant_initializer(value=W_val, dtype=tf.float32),\n>                         dtype=tf.float32)\n>     b = tf.get_variable(name='b_{}'.format(name), shape=(output_size,),\n>                         initializer=tf.constant_initializer(value=0.1, dtype=tf.float32),\n>                         dtype=tf.float32)\n>     return W, b\n>\n> g = tf.Graph()with g.as_default():\n>     W1, b1 = get_layer(158238, 900, '1')\n>     W2, b2 = get_layer(900, 1000, '2')\n>     W3, b3 = get_layer(1000, 1, '3')\n>     init = tf.global_variables_initializer()\n> session = tf.Session(graph=g)\n> session.run(init)\n> session.close()\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9742#issuecomment-299966164>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AYoxVHu9SJzrRAiqslcLQwpRRaD2Ku51ks5r321OgaJpZM4NTSLO>\n> .\n>\n", "I ran the command in both environments (compiled and pip version - Docker containers) and the output is \"cpp\"", "After seeing #9823 (not sure if related to this), I tried the memory_util script suggested by @yaroslavvb but the output is pretty much the same for both environments:\r\n\r\n### Binary\r\n```\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nI tensorflow/core/common_runtime/local_device.cc:40] Local device intra op parallelism threads: 32\r\nI tensorflow/core/common_runtime/direct_session.cc:83] Direct session inter op parallelism threads: 32\r\n        0                Unknown (from Proto)(1-cpu)   569656800   569656800 cpu\r\n        1                Unknown (from Proto)(1-cpu)  -569656800           0 cpu\r\n        2               W_1/Initializer/Const(2-cpu)   569656800   569656800 cpu\r\n        3                Unknown (from Proto)(3-cpu)        3600   569660400 cpu\r\n        4                Unknown (from Proto)(3-cpu)       -3600   569656800 cpu\r\n        5               b_1/Initializer/Const(4-cpu)        3600   569660400 cpu\r\n        6                Unknown (from Proto)(5-cpu)     3600000   573260400 cpu\r\n        7                Unknown (from Proto)(5-cpu)    -3600000   569660400 cpu\r\n        8               W_2/Initializer/Const(6-cpu)     3600000   573260400 cpu\r\n        9                Unknown (from Proto)(7-cpu)        4000   573264400 cpu\r\n       10                Unknown (from Proto)(7-cpu)       -4000   573260400 cpu\r\n       11               b_2/Initializer/Const(8-cpu)        4000   573264400 cpu\r\n       12                Unknown (from Proto)(9-cpu)        4000   573268400 cpu\r\n       13                Unknown (from Proto)(9-cpu)       -4000   573264400 cpu\r\n       14              W_3/Initializer/Const(10-cpu)        4000   573268400 cpu\r\n       25               Unknown (from Proto)(13-cpu)   569656800  1142925200 cpu\r\n       26               Unknown (from Proto)(13-cpu)  -569656800   573268400 cpu\r\n       28               Unknown (from Proto)(14-cpu)        3600   573272000 cpu\r\n       29               Unknown (from Proto)(14-cpu)       -3600   573268400 cpu\r\n       31                         b_1/Assign(16-cpu)        3600   573272000 cpu\r\n       32                         W_1/Assign(17-cpu)   569656800  1142928800 cpu\r\n       34               Unknown (from Proto)(15-cpu)     3600000  1146528800 cpu\r\n       35               Unknown (from Proto)(15-cpu)    -3600000  1142928800 cpu\r\n       37               Unknown (from Proto)(18-cpu)        4000  1142932800 cpu\r\n       38               Unknown (from Proto)(18-cpu)       -4000  1142928800 cpu\r\n       40               Unknown (from Proto)(19-cpu)        4000  1142932800 cpu\r\n       41               Unknown (from Proto)(19-cpu)       -4000  1142928800 cpu\r\n       48                         W_2/Assign(22-cpu)     3600000  1146528800 cpu\r\n       49                         b_2/Assign(23-cpu)        4000  1146532800 cpu\r\n       52                         W_3/Assign(24-cpu)        4000  1146536800 cpu\r\nI tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 2 allocator_name: \"cpu\" }\r\nI tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 4 allocator_name: \"cpu\" }\r\nI tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 6 allocator_name: \"cpu\" }\r\nI tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 8 allocator_name: \"cpu\" }\r\nI tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 10 allocator_name: \"cpu\" }\r\nI tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 12 allocator_name: \"cpu\" }\r\nI tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 17 allocator_name: \"cpu\" }\r\nI tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 16 allocator_name: \"cpu\" }\r\nI tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 23 allocator_name: \"cpu\" }\r\nI tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 21 allocator_name: \"cpu\" }\r\nI tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 22 allocator_name: \"cpu\" }\r\nI tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 24 allocator_name: \"cpu\" }\r\n```\r\n\r\n### Compiled\r\n```\r\nI tensorflow/core/common_runtime/local_device.cc:40] Local device intra op parallelism threads: 32\r\nI tensorflow/core/common_runtime/direct_session.cc:83] Direct session inter op parallelism threads: 32\r\n        0                Unknown (from Proto)(1-cpu)   569656800   569656800 cpu\r\n        1                Unknown (from Proto)(1-cpu)  -569656800           0 cpu\r\n        2               W_1/Initializer/Const(2-cpu)   569656800   569656800 cpu\r\n        3                Unknown (from Proto)(3-cpu)        3600   569660400 cpu\r\n        4                Unknown (from Proto)(3-cpu)       -3600   569656800 cpu\r\n        5               b_1/Initializer/Const(4-cpu)        3600   569660400 cpu\r\n        6                Unknown (from Proto)(5-cpu)     3600000   573260400 cpu\r\n        7                Unknown (from Proto)(5-cpu)    -3600000   569660400 cpu\r\n        8               W_2/Initializer/Const(6-cpu)     3600000   573260400 cpu\r\n        9                Unknown (from Proto)(7-cpu)        4000   573264400 cpu\r\n       10                Unknown (from Proto)(7-cpu)       -4000   573260400 cpu\r\n       11               b_2/Initializer/Const(8-cpu)        4000   573264400 cpu\r\n       12                Unknown (from Proto)(9-cpu)        4000   573268400 cpu\r\n       13                Unknown (from Proto)(9-cpu)       -4000   573264400 cpu\r\n       14              W_3/Initializer/Const(10-cpu)        4000   573268400 cpu\r\n       25               Unknown (from Proto)(13-cpu)   569656800  1142925200 cpu\r\n       26               Unknown (from Proto)(13-cpu)  -569656800   573268400 cpu\r\n       28               Unknown (from Proto)(14-cpu)        3600   573272000 cpu\r\n       29               Unknown (from Proto)(14-cpu)       -3600   573268400 cpu\r\n       31                         b_1/Assign(16-cpu)        3600   573272000 cpu\r\n       32                         W_1/Assign(17-cpu)   569656800  1142928800 cpu\r\n       34               Unknown (from Proto)(15-cpu)     3600000  1146528800 cpu\r\n       35               Unknown (from Proto)(15-cpu)    -3600000  1142928800 cpu\r\n       37               Unknown (from Proto)(18-cpu)        4000  1142932800 cpu\r\n       38               Unknown (from Proto)(18-cpu)       -4000  1142928800 cpu\r\n       40               Unknown (from Proto)(19-cpu)        4000  1142932800 cpu\r\n       41               Unknown (from Proto)(19-cpu)       -4000  1142928800 cpu\r\n       48                         W_2/Assign(22-cpu)     3600000  1146528800 cpu\r\n       49                         b_2/Assign(23-cpu)        4000  1146532800 cpu\r\n       51                         W_3/Assign(24-cpu)        4000  1146536800 cpu\r\nI tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 2 allocator_name: \"cpu\" }\r\nI tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 4 allocator_name: \"cpu\" }\r\nI tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 6 allocator_name: \"cpu\" }\r\nI tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 8 allocator_name: \"cpu\" }\r\nI tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 10 allocator_name: \"cpu\" }\r\nI tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 12 allocator_name: \"cpu\" }\r\nI tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 21 allocator_name: \"cpu\" }\r\nI tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 24 allocator_name: \"cpu\" }\r\nI tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 22 allocator_name: \"cpu\" }\r\nI tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 17 allocator_name: \"cpu\" }\r\nI tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 23 allocator_name: \"cpu\" }\r\nI tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 16 allocator_name: \"cpu\" }\r\n```\r\nThis is the script I used for testing:\r\n```python\r\nimport memory_util\r\nmemory_util.vlog(1)\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\ndef get_layer(input_size, output_size, name):\r\n    # supposedly loaded from a saved file\r\n    W_val = np.random.normal(scale=0.1, size=(input_size, output_size)).astype(np.float32)\r\n    W = tf.get_variable(name='W_{}'.format(name), shape=(input_size, output_size),\r\n                        initializer=tf.constant_initializer(value=W_val, dtype=tf.float32),\r\n                        dtype=tf.float32)\r\n    b = tf.get_variable(name='b_{}'.format(name), shape=(output_size,),\r\n                        initializer=tf.constant_initializer(value=0.1, dtype=tf.float32),\r\n                        dtype=tf.float32)\r\n    return W, b\r\n\r\n\r\ng = tf.Graph()\r\nwith g.as_default():\r\n    W1, b1 = get_layer(158238, 900, '1')\r\n    W2, b2 = get_layer(900, 1000, '2')\r\n    W3, b3 = get_layer(1000, 1, '3')\r\n\r\n    init = tf.global_variables_initializer()\r\nsession = tf.Session(graph=g)\r\nwith memory_util.capture_stderr() as stderr:\r\n    session.run(init)\r\nmemory_util.print_memory_timeline(stderr, ignore_less_than_bytes=1000)\r\n```\r\n\r\nAre there any updates on this issue? Has anyone been able to reproduce it?", "`memory_util` only captures memory allocated by TF runtime, so this suggests the problem is elsewhere. Earlier you said that the extra memory comes from memory allocated by `MergePartialFromCodedStream` which could suggests that extra memory was allocated by protobuf library.\r\n\r\nNote that you are initializing your layers with huge constants. Those constants get serialized into GraphDef protobufs. There might be a bug somewhere which causes memory allocated by this serialization not to be freed.\r\n\r\nAs a work-around, you could try not using constant nodes to initialize your variables. You can initialize them directly from numpy arrays by using tf.placeholder and providing value through feed_dict, for examples see https://github.com/tensorflow/tensorflow/issues/9821", "@yaroslavvb wow, the difference is huge. Initializing the variables now takes less than 3GB of memory, that's 2x less than what the binary version was taking and 4x less than the compiled one (using `tf.constant_initializer`). That's a huge difference! (edit: It's also way faster)\r\n\r\nHowever, the placeholder initialization workaround is unpractical in my case as we have wrapper classes that do the loading for us which we just initialize using `global_variables_initializer` so it'll require some design changes which isn't very feasible right now.\r\n\r\nI understand this is an issue with `protobuf` but how come when loading the variables in the compiled version of tensorflow we get so much more memory usage? Even though it's the same `protobuf` version used in both cases? Also should I file an issue with the `protobuf` team regarding this or are there ways to tune it?\r\n\r\nIn case someone needs it, here's the code I used:\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndef get_layer(input_size, output_size, name):\r\n    # supposedly loaded from a saved file\r\n    W_pl = tf.placeholder(tf.float32, (input_size, output_size))\r\n    W_val = np.random.normal(scale=0.1, size=(input_size, output_size)).astype(np.float32)\r\n    W = tf.get_variable(name='W_{}'.format(name),\r\n                        initializer=W_pl,\r\n                        dtype=tf.float32)\r\n    b = tf.get_variable(name='b_{}'.format(name), shape=(output_size,),\r\n                        initializer=tf.constant_initializer(value=0.1, dtype=tf.float32),\r\n                        dtype=tf.float32)\r\n    return (W_pl, W_val), W, b\r\n\r\nsessions = []\r\nfor i in range(3):\r\n    g = tf.Graph()\r\n    with g.as_default():\r\n        W1_pl, W1, b1 = get_layer(158238, 900, '1')\r\n        W2_pl, W2, b2 = get_layer(900, 1000, '2')\r\n        W3_pl, W3, b3 = get_layer(1000, 1, '3')\r\n        init = tf.global_variables_initializer()\r\n    session = tf.Session(graph=g)\r\n    session.run(init, feed_dict=dict([W1_pl, W2_pl, W3_pl]))\r\n    print 'Loaded {}'.format(i)\r\n    sessions.append(session)\r\n```", "BTW, putting huge constants into Graph has performance implications besides higher memory, accessing data is slower too. Also there's a 2GB limit which includes graph inlined constants, once you hit it, you won't be able to add any more operations to the graph.\r\n\r\nI get around design issues by keeping global `init_dict` with links to all numpy arrays and passing it to `global_variables_initializer`.\r\n\r\nIf you really wanted to find out why there's a difference you could try to narrow problem further. \r\nFor instance, does it happen with Python protobufs as well or just with cpp?\r\n\r\nTo find out which protobuf you are running:\r\npython -c \"from google.protobuf.internal import api_implementation; print(api_implementation._default_implementation_type)\"\r\n\r\nDoes it happen when the constant in question are getting read or is just the serialization step leaking memory? You can create large constant + small constant, and do \"sess.run(small_constant)\"", "Thanks for the suggestion @yaroslavvb, sounds fair enough (and worth it given the significant memory/performance improvement).\r\n\r\nNot sure it's worth it to look into the difference between the environments given that I found the main bottleneck now which is in the protobuf serialization. For what it's worth the protobuf implementation type in my case is cpp.\r\n\r\nAs for your last question, I already tested with `tf.random_normal_initializer` instead of `tf.constant_initializer` with a numpy array and there's no issue at all.", "It sounds as if the right thing to do is follow up with protobuf to investigate further, so I am closing this for now but please reopen if there are more TF issues."]}, {"number": 9741, "title": "run own model on android", "body": "**question:**\r\ni run tensorflow SSD model on android. The android studio didn't throw error. But i can't detect anything.I test my pb file, it's seems right. I don't know why. Anyone who can help me? Thanks!!\r\n\r\n**code:**\r\nbecause the model is based on tensorflow demo. so i just paste own code.\r\n\r\n```\r\npublic List<Recognition> recognizeImage(final Bitmap bitmap) {\r\n    final SplitTimer timer = new SplitTimer(\"recognizeImage\");\r\n\r\n    // Log this method so that it can be analyzed with systrace.\r\n    Trace.beginSection(\"recognizeImage\");\r\n\r\n    Trace.beginSection(\"preprocessBitmap\");\r\n    // Preprocess the image data from 0-255 int to normalized float based\r\n    // on the provided parameters.\r\n    bitmap.getPixels(intValues, 0, bitmap.getWidth(), 0, 0, bitmap.getWidth(), bitmap.getHeight());\r\n\r\n    for (int i = 0; i < intValues.length; ++i) {\r\n      floatValues[i * 3 + 0] = ((intValues[i] >> 16) & 0xFF) / 255.0f;\r\n      floatValues[i * 3 + 1] = ((intValues[i] >> 8) & 0xFF) / 255.0f;\r\n      floatValues[i * 3 + 2] = (intValues[i] & 0xFF) / 255.0f;\r\n    }\r\n    Trace.endSection(); // preprocessBitmap\r\n\r\n    // Copy the input data into TensorFlow.\r\n    Trace.beginSection(\"fillNodeFloat\");\r\n    inferenceInterface.fillNodeFloat(\r\n            inputName, new int[] {1, inputSize, inputSize, 3}, floatValues);\r\n    Trace.endSection();\r\n\r\n    timer.endSplit(\"ready for inference\");\r\n\r\n    // Run the inference call.\r\n    Trace.beginSection(\"runInference\");\r\n    final int resultCode = inferenceInterface.runInference(outputNames);\r\n    if (resultCode != 0) {\r\n      throw new RuntimeException(\"Bad result code from inference: \" + resultCode);\r\n    }\r\n    Trace.endSection();\r\n\r\n    timer.endSplit(\"ran inference\");\r\n\r\n    // Copy the output Tensor back into the output array.\r\n    Trace.beginSection(\"readNodeFloat\");\r\n    //\u63a8\u7406\u5f97\u5230\u7684\u7ed3\u679c\r\n\r\n    final float[] outputs=new float[feat_shapes[0][0] * feat_shapes[0][1] * (anchor_sizes[0].length+anchor_ratios[0].length) * 4+\r\n                                          feat_shapes[1][0] * feat_shapes[1][1] * (anchor_sizes[1].length+anchor_ratios[1].length) * 4+\r\n                                          feat_shapes[2][0] * feat_shapes[2][1] * (anchor_sizes[2].length+anchor_ratios[2].length) * 4+\r\n                                          feat_shapes[3][0] * feat_shapes[3][1] * (anchor_sizes[3].length+anchor_ratios[3].length) * 4+\r\n                                          feat_shapes[4][0] * feat_shapes[4][1] * (anchor_sizes[4].length+anchor_ratios[4].length) * 4+\r\n                                          feat_shapes[5][0] * feat_shapes[5][1] * (anchor_sizes[5].length+anchor_ratios[5].length) * 4+\r\n                                          feat_shapes[0][0] * feat_shapes[0][1] * (anchor_sizes[0].length+anchor_ratios[0].length) * NUM_CLASSES+\r\n                                          feat_shapes[1][0] * feat_shapes[1][1] * (anchor_sizes[1].length+anchor_ratios[1].length) * NUM_CLASSES+\r\n                                          feat_shapes[2][0] * feat_shapes[2][1] * (anchor_sizes[2].length+anchor_ratios[2].length) * NUM_CLASSES+\r\n                                          feat_shapes[3][0] * feat_shapes[3][1] * (anchor_sizes[3].length+anchor_ratios[3].length) * NUM_CLASSES+\r\n                                          feat_shapes[4][0] * feat_shapes[4][1] * (anchor_sizes[4].length+anchor_ratios[4].length) * NUM_CLASSES+\r\n                                          feat_shapes[5][0] * feat_shapes[5][1] * (anchor_sizes[5].length+anchor_ratios[5].length) * NUM_CLASSES];\r\n\r\n    inferenceInterface.readNodeFloat(outputNames[0], outputs);\r\n    LOGGER.i(\"recognizeImage: \"+outputs);\r\n    Trace.endSection();\r\n\r\n    // Find the best detections.\r\n    final PriorityQueue<Recognition> pq =\r\n            new PriorityQueue<Recognition>(\r\n                    1,\r\n                    new Comparator<Recognition>() {\r\n                      @Override\r\n                      public int compare(final Recognition lhs, final Recognition rhs) {\r\n                        // Intentionally reversed to put high confidence at the head of the queue.\r\n                        return Float.compare(rhs.getConfidence(), lhs.getConfidence());\r\n                      }\r\n                    });\r\n\r\n    int num=0,offest=0;\r\n    float cx,cy,cw,ch,xpos,ypos,h,w;\r\n    for(int i=0;i<6;i++)\r\n    {\r\n      for(int y=0;y<feat_shapes[i][0];y++)\r\n      {\r\n        for(int x=0;x<feat_shapes[i][1];x++)\r\n        {\r\n          ypos=(y+anchor_offset)*anchor_steps[i]/img_shape[0];\r\n          xpos=(x+anchor_offset)*anchor_steps[i]/img_shape[1];\r\n          //\u9884\u6d4b\u6846\r\n          for(int l=0;l<(anchor_ratios[i].length+2);l++)\r\n          {\r\n            if(l==0)\r\n            {\r\n              h=anchor_sizes[i][0]/img_shape[0];\r\n              w=anchor_sizes[i][0]/img_shape[1];\r\n            }\r\n            else if(l==1)\r\n            {\r\n              h=Float.parseFloat(String.valueOf(Math.sqrt(anchor_sizes[i][0]*anchor_sizes[i][1])/img_shape[0]));\r\n              w=Float.parseFloat(String.valueOf(Math.sqrt(anchor_sizes[i][0]*anchor_sizes[i][1])/img_shape[1]));\r\n            }\r\n            else\r\n            {\r\n              h=Float.parseFloat(String.valueOf(anchor_sizes[i][0]/img_shape[0]/Math.sqrt(anchor_ratios[i][l-2])));\r\n              w=Float.parseFloat(String.valueOf(anchor_sizes[i][0]/img_shape[1]/Math.sqrt(anchor_ratios[i][l-2])));\r\n            }\r\n            //\u89e3\u7801\r\n            cx=outputs[offest]*w*prior_scaling[0]+xpos;\r\n            offest++;\r\n            cy=outputs[offest]*h*prior_scaling[1]+ypos;\r\n            offest++;\r\n            cw=Float.parseFloat(String.valueOf(w*Math.exp(outputs[offest]*prior_scaling[2])));\r\n            offest++;\r\n            ch=Float.parseFloat(String.valueOf(h*Math.exp(outputs[offest]*prior_scaling[3])));\r\n            offest++;\r\n            final RectF rect =\r\n                    new RectF(\r\n                            Math.max(0, cx - cw / 2),\r\n                            Math.max(0, cy - ch / 2),\r\n                            Math.min(1, cx + cw / 2),\r\n                            Math.min(1, cy + ch / 2));\r\n            //\u6311\u9009\u5927\u4e8eselect_threshold\u7684\u6846\r\n            offest++;\r\n            for(int m=1;m<NUM_CLASSES;m++)\r\n            {\r\n              if(outputs[offest]>select_threshold)\r\n              {\r\n                LOGGER.i(\r\n                        \"%s (%d) %f %s\", LABELS[m], m, outputs[offest], rect);\r\n                pq.add(new Recognition(\"\" + num, LABELS[m], outputs[offest], rect));\r\n                num++;\r\n              }\r\n              offest++;\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n    timer.endSplit(\"decoded results\");\r\n\r\n    final ArrayList<Recognition> sort = new ArrayList<Recognition>();\r\n    for (int i = 0; i < Math.min(pq.size(), top_k); ++i) {\r\n      sort.add(pq.poll());\r\n    }\r\n    timer.endSplit(\"sorted results\");\r\n\r\n    final ArrayList<Recognition> nms = new ArrayList<Recognition>();\r\n    float []keep_bboxes=new float[sort.size()];\r\n    for(int i=0;i<sort.size();i++)\r\n    {\r\n      keep_bboxes[i]=1;\r\n    }\r\n    float overlap;\r\n    for(int i=0;i<sort.size();i++)\r\n    {\r\n      if(keep_bboxes[i]==1)\r\n      {\r\n        for(int j=i+1;j<sort.size();j++)\r\n        {\r\n          overlap=bboxes_jaccard(sort.get(i).getLocation(),sort.get(j).getLocation());\r\n          if(overlap<nms_threshold||sort.get(i).getTitle()!=sort.get(j).getTitle())\r\n          {\r\n            nms.add(sort.get(i));\r\n          }\r\n          else\r\n            keep_bboxes[j]=0;\r\n        }\r\n      }\r\n    }\r\n    timer.endSplit(\"nms results\");\r\n\r\n    final ArrayList<Recognition> recognitions = new ArrayList<Recognition>();\r\n    for (int i = 0; i < Math.min(nms.size(), MAX_RESULTS); ++i) {\r\n      recognitions.add(nms.get(i));\r\n    }\r\n    Trace.endSection(); // \"recognizeImage\"\r\n\r\n    timer.endSplit(\"processed results\");\r\n\r\n    return recognitions;\r\n  }\r\n```", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 9740, "title": "Fix tf.meshgrid documentation", "body": "The original documentation has the wrong output regarding the default indexing xy.\r\n\r\n    x = [1, 2, 3]\r\n    y = [4, 5, 6]\r\n    X, Y = tf.meshgrid(x, y)\r\n    print(X.eval())\r\n    print(Y.eval())\r\n\r\n    [[1 2 3]\r\n     [1 2 3]\r\n     [1 2 3]]\r\n    [[4 4 4]\r\n     [5 5 5]\r\n     [6 6 6]]", "comments": ["Can one of the admins verify this patch?", "cc @ibab to confirm this is a doc bug and not an impl bug!", "This is a doc bug.\nI must have forgotten to update the example when we made `xy` the default\nto bring it in line with numpy.\n\n@thinxer: Thanks!\n\nOn Sun, May 7, 2017, 19:18 Vijay Vasudevan <notifications@github.com> wrote:\n\n> cc @ibab <https://github.com/ibab> to confirm this is a doc bug and not\n> an impl bug!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/9740#issuecomment-299720472>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AA2Wo3UU0QK_fSLDmqN_0-vQRqNKPPbGks5r3fzrgaJpZM4NTMvK>\n> .\n>\n"]}, {"number": 9739, "title": "404 Error When Installing 1.1.0 GPU Python 3.x version", "body": "~~~\r\nHTTP error 404 while getting https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-1.1.0-py3-none-any.whl\r\n  Could not install requirement tensorflow-gpu==1.1.0 from https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-1.1.0-py3-none-any.whl because of error 404 Client Error: Not Found for url: https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-1.1.0-py3-none-any.whl\r\nCould not install requirement tensorflow-gpu==1.1.0 from https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-1.1.0-py3-none-any.whl because of HTTP error 404 Client Error: Not Found for url: https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-1.1.0-py3-none-any.whl for URL https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-1.1.0-py3-none-any.whl\r\n~~~\r\nRan same command to successfully install version 1.0.0 GPU py3. I thought this would be the best place to bring it to the Google team's attention, but let me know if not!", "comments": ["@yifeif either there is a problem with the URL or the [doc](https://www.tensorflow.org/install/install_mac#the_url_of_the_tensorflow_python_package) should be changed.", "Looks like the link is working now. Let us know if this is still an issue. Thanks!"]}, {"number": 9738, "title": "OpenCL support for Window", "body": "Scheduled to be compatible with OpenCL for Window\r\nCurrently bazel and cmake can not build on OpenCL on Windows\r\nIs there no plan to support OpenCL on Windows?\r\nIf so, when will the period be?", "comments": ["@benoitsteiner and @lukeiwanski may be able to comment.", "Hi @twatasf7 \r\nWe are planning on releasing windows support soon ( cannot give an exact date yet )", "@lukeiwanski \r\nThat's great. Thank you", "Hi @lukeiwanski \r\nAny updates on that? Thanks!"]}, {"number": 9737, "title": "Why\uff1f", "body": "I have successfully install the tensorflow-gpu-1.0.0 in windows 10, but when I train a model , it appears this error:\r\nInternalError: Failed launching MaxPoolForwardNoMask", "comments": ["Ask this on [Stack Overflow](http://stackoverflow.com/) with the tag _tensorflow_ and you'll receive better support. Also, you'll want to provide code to reproduce the error.", "@wgmg165 like @carlthome says this list is mainly for issues to bring to attention of developers of tensorflow, and stackoverflow is best for support. If you think it's a bug, you could provide a way to reproduce this error in latest version (tf 1.1) and provide more info", "I have the same problem with\r\nubuntu 16.04\r\ntensorflow-gpu 1.3\r\n\r\n`InternalError (see above for traceback): Failed launching MaxPoolForwardNoMask`\r\n`[[Node: pool1 = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 2, 2, 1], \r\n\t\t\tpadding=\"VALID\", strides=[1, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]\r\n\t\t\t(conv1_2/conv1_2)]]`\r\n        ` [[Node: bbox_pred/bbox_pred/_105 = _Recv[client_terminated=false, \r\n\t\t\trecv_device=\"/job:localhost/replica:0/task:0/cpu:0\", \r\n\t\t\tsend_device=\"/job:localhost/replica:0/task:0/gpu:0\", \r\n\t\t\tsend_device_incarnation=1, \r\n\t\t\ttensor_name=\"edge_353_bbox_pred/bbox_pred\", \r\n\t\t\ttensor_type=DT_FLOAT, \r\n\t\t\t_device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]`\r\n\r\nAny idea? Did you resolve this problem?"]}, {"number": 9736, "title": "Fix missing dash for cxxopt build option", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}]