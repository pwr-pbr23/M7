[{"number": 5934, "title": "Errors on AWS with 8 NVIDIA K80 cards.", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nconvnet-benchmarks/tensorflow$ python benchmark_alexnet.py \r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so.8.0 locally\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties: \r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 0000:00:17.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.11GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x31a4790\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 1 with properties: \r\n\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0 1 2 3 4 5 6 7 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y Y Y Y Y Y Y Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 1:   Y Y Y Y Y Y Y Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 2:   Y Y Y Y Y Y Y Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 3:   Y Y Y Y Y Y Y Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 4:   Y Y Y Y Y Y Y Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 5:   Y Y Y Y Y Y Y Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 6:   Y Y Y Y Y Y Y Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 7:   Y Y Y Y Y Y Y Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:17.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla K80, pci bus id: 0000:00:18.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:2) -> (device: 2, name: Tesla K80, pci bus id: 0000:00:19.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:3) -> (device: 3, name: Tesla K80, pci bus id: 0000:00:1a.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:4) -> (device: 4, name: Tesla K80, pci bus id: 0000:00:1b.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:5) -> (device: 5, name: Tesla K80, pci bus id: 0000:00:1c.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:6) -> (device: 6, name: Tesla K80, pci bus id: 0000:00:1d.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:7) -> (device: 7, name: Tesla K80, pci bus id: 0000:00:1e.0)\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1110] failed to synchronize the stop event: CUDA_ERROR_MISALIGNED_ADDRESS\r\nE tensorflow/stream_executor/cuda/cuda_timer.cc:54] Internal: error destroying CUDA event in context 0x34d6b30: CUDA_ERROR_MISALIGNED_ADDRESS\r\nE tensorflow/stream_executor/cuda/cuda_timer.cc:59] Internal: error destroying CUDA event in context 0x34d6b30: CUDA_ERROR_MISALIGNED_ADDRESS\r\nF tensorflow/stream_executor/cuda/cuda_dnn.cc:1979] failed to enqueue convolution on stream: CUDNN_STATUS_EXECUTION_FAILED\r\nAborted (core dumped)\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nDISTRIB_ID=Ubuntu\r\nDISTRIB_RELEASE=16.04\r\nDISTRIB_CODENAME=xenial\r\nDISTRIB_DESCRIPTION=\"Ubuntu 16.04.1 LTS\"\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\nCUDA 8.0 (from: http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-repo-ubuntu1604_8.0.44-1_amd64.deb)\r\ncudnn:5.1\r\n\r\nIf installed from source, provide \r\n1. https://github.com/tensorflow/tensorflow/archive/v0.11.0.tar.gz\r\n\r\n2. The output of `bazel version`\r\nBuild label: 0.3.2\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Fri Oct 7 17:25:10 2016 (1475861110)\r\nBuild timestamp: 1475861110\r\nBuild timestamp as int: 1475861110\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\npython https://github.com/soumith/convnet-benchmarks/blob/master/tensorflow/benchmark_alexnet.py\r\n\r\n### What other attempted solutions have you tried?\r\nreinstall tensorflow and reconfigure the environment.\r\nIf only has 1 K80 card, it is success.\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["Were you able to get any more of a stack trace at the failure?  I haven't seen this CUDA_ERROR_MISALIGNED_ADDRESS error before, and I'm wondering where the program is executing when it arises.  You say the error does not arise using only one k80.  What if you use only 2?\r\n", "@poxvoculi I am sorry that on AWS I am only able to use either 8 or 1 GPU on a instance. and on K520 there's no such problem.\r\nRegarding the stack trace, I have already delete the instance because it is very expensive. What stack trace do you need and how can I get it? I am new to tensorflow.", "@zheng-xq what do you think?", "@Hui-Zhi you can set the environment variable `CUDA_VISIBLE_DEVICES` to set which GPUs are available for tensorflow to use. For example if you set `CUDA_VISIBLE_DEVICES=0,1` in your environment before you run your code, only two GPUs will be used by TF.", "@gunan No, I did not.", "@tfboyd, can you take a look at this?", "This issue is a little stale, but I should be on AWS this week and it should be easy enough to run the code from:\r\n\r\n`python https://github.com/soumith/convnet-benchmarks/blob/master/tensorflow/benchmark_alexnet.py`\r\n\r\nI will be using TF 1.0 so it will not be apples to apples but knowing it runs fine on TF 1.0 will be nice.  ", "I tested this on a p2.8xLarge with TF 1.0 (hash:c7fca9e) as follows:\r\n\r\n`python benchmark_alexnet.py`\r\nand\r\n`CUDA_VISIBLE_DEVICES=0 python benchmark_alexnet.py`\r\n\r\nIt worked fine for both cases.  Unless I am reading the code incorrectly (which happens), this [benchmark](https://github.com/soumith/convnet-benchmarks/blob/master/tensorflow/benchmark_alexnet.py) is not multi-GPU so the result is testing single GPU performance for AlexNet.  My results just because I have them:\r\n\r\nK80\r\n2017-02-21 22:05:39.664444: Forward across 100 steps, 0.064 +/- 0.006 sec / batch\r\n2017-02-21 22:06:01.289066: Forward-backward across 100 steps, 0.186 +/- 0.019 sec / batch\r\n\r\nLocal GTX 1080 (also my display GPU)\r\n2017-02-21 14:24:55.096074: Forward across 100 steps, 0.023 +/- 0.003 sec / batch\r\n2017-02-21 14:25:03.292397: Forward-backward across 100 steps, 0.070 +/- 0.007 sec / batch\r\n\r\np.s. I did have to install future as well `sudo pip install future`\r\n\r\n"]}, {"number": 5933, "title": "http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz failing to download", "body": "sidra@ironheart:/tmp$ wget http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz\r\n--2016-11-29 14:23:24--  http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz\r\nResolving download.tensorflow.org (download.tensorflow.org)... 216.58.208.240, 2a00:1450:4007:80e::2010\r\nConnecting to download.tensorflow.org (download.tensorflow.org)|216.58.208.240|:80... connected.\r\nHTTP request sent, awaiting response... 200 OK\r\nLength: 88931400 (85M) [application/x-compressed-tar]\r\nSaving to: \u2018inception-2015-12-05.tgz\u2019\r\n\r\ninception-2015-12-05.tgz                             0%[                                                                                                                 ]  15.34K  --.-KB/s    in 0.1s    \r\n\r\n2016-11-29 14:23:29 (113 KB/s) - Read error at byte 15709/88931400 (Connection reset by peer). Retrying.\r\n\r\n--2016-11-29 14:23:30--  (try: 2)  http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz\r\nConnecting to download.tensorflow.org (download.tensorflow.org)|216.58.208.240|:80... connected.\r\nHTTP request sent, awaiting response... 206 Partial Content\r\nLength: 88931400 (85M), 88915691 (85M) remaining [application/x-compressed-tar]\r\nSaving to: \u2018inception-2015-12-05.tgz\u2019\r\n\r\ninception-2015-12-05.tgz                             0%[                                                                                                                 ]  30.62K  94.9KB/s    in 0.2s    \r\n\r\n2016-11-29 14:23:31 (94.9 KB/s) - Read error at byte 31359/88931400 (Connection reset by peer). Retrying.\r\n\r\n--2016-11-29 14:23:33--  (try: 3)  http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz\r\nConnecting to download.tensorflow.org (download.tensorflow.org)|216.58.208.240|:80... connected.\r\nHTTP request sent, awaiting response... 206 Partial Content\r\nLength: 88931400 (85M), 88900041 (85M) remaining [application/x-compressed-tar]\r\nSaving to: \u2018inception-2015-12-05.tgz\u2019\r\n\r\ninception-2015-12-05.tgz                             0%[                                                                                                                 ]  45.91K  --.-KB/s    in 0.1s    \r\n\r\n2016-11-29 14:23:34 (113 KB/s) - Read error at byte 47009/88931400 (Connection reset by peer). Retrying.\r\n\r\n", "comments": ["Will be fixed by a Bazel change being pushed soon.", "This has been fixed by https://github.com/bazelbuild/bazel/commit/ed7ced0018dc5c5ebd6fc8afc7158037ac1df00d. Try using Bazel at HEAD. A new Bazel will be cut in a few days. As soon as that happens, the TensorFlow repository will be updated to use multiple mirrors for each external file."]}, {"number": 5932, "title": "W tensorflow/core/framework/op_kernel.cc:975] Invalid argument: Received a label value of 255 which is outside the valid range of [0, 10).", "body": "I use cifar10 model training my dataset,which is made by tf.python_io.TFRecordWriter() .\r\nI am sure labels in dataset are right,because it's running ok on Ubuntu PC but  get Error Invalid argument: Received a label value of 255 which is outside the valid range of [0, 10)  on Mac and GPU Server.\r\n\r\nW tensorflow/core/framework/op_kernel.cc:975] Invalid argument: Received a label value of 255 which is outside the valid range of [0, 10).  Label values: 65 229 184 161 102 117 112 160 66 93 107 117 131 122 129 132 113 163 149 130 75 52 109 84 161 165 99 203 82 42 57 179 155 63 126 49 172 50 144 224 152 220 164 82 195 169 171 125 107 127 70 60 93 115 165 143 78 116 60 153 113 62 89 175 125 90 85 178 167 200 133 168 125 92 62 93 166 141 98 172 102 103 72 179 138 108 49 176 46 70 55 101 141 144 107 126 60 146 108 77 125 59 58 109 171 107 63 151 55 93 172 131 52 128 75 167 255 97 108 171 113 130 77 166 150 132 62 196\r\nTraceback (most recent call last):\r\n  File \"/Users/yangk/cifar10/cifar10_train.py\", line 137, in <module>\r\n    tf.app.run()\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/platform/app.py\", line 43, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"/Users/yangk/cifar10/cifar10_train.py\", line 133, in main\r\n    train()\r\n  File \"/Users/yangk/cifar10/cifar10_train.py\", line 103, in train\r\n    _, loss_value = sess.run([train_op, loss])\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/client/session.py\", line 766, in run\r\n    run_metadata_ptr)\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/client/session.py\", line 964, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Received a label value of 255 which is outside the valid range of [0, 10).  Label values: 65 229 184 161 102 117 112 160 66 93 107 117 131 122 129 132 113 163 149 130 75 52 109 84 161 165 99 203 82 42 57 179 155 63 126 49 172 50 144 224 152 220 164 82 195 169 171 125 107 127 70 60 93 115 165 143 78 116 60 153 113 62 89 175 125 90 85 178 167 200 133 168 125 92 62 93 166 141 98 172 102 103 72 179 138 108 49 176 46 70 55 101 141 144 107 126 60 146 108 77 125 59 58 109 171 107 63 151 55 93 172 131 52 128 75 167 255 97 108 171 113 130 77 166 150 132 62 196\r\n\t [[Node: cross_entropy_per_example/cross_entropy_per_example = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT64, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](softmax_linear/softmax_linear, Cast_4)]]\r\n\r\nCaused by op u'cross_entropy_per_example/cross_entropy_per_example', defined at:\r\n  File \"/Users/yangk/cifar10/cifar10_train.py\", line 137, in <module>\r\n    tf.app.run()\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/platform/app.py\", line 43, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"/Users/yangk/cifar10/cifar10_train.py\", line 133, in main\r\n    train()\r\n  File \"/Users/yangk/cifar10/cifar10_train.py\", line 76, in train\r\n    loss = cifar10.loss(logits, labels)\r\n  File \"/Users/yangk/cifar10/cifar10.py\", line 289, in loss\r\n    logits, labels, name='cross_entropy_per_example')\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 1537, in sparse_softmax_cross_entropy_with_logits\r\n    precise_logits, labels, name=name)\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 2378, in _sparse_softmax_cross_entropy_with_logits\r\n    features=features, labels=labels, name=name)\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\r\n    op_def=op_def)\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/framework/ops.py\", line 2371, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/framework/ops.py\", line 1258, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): Received a label value of 255 which is outside the valid range of [0, 10).  Label values: 65 229 184 161 102 117 112 160 66 93 107 117 131 122 129 132 113 163 149 130 75 52 109 84 161 165 99 203 82 42 57 179 155 63 126 49 172 50 144 224 152 220 164 82 195 169 171 125 107 127 70 60 93 115 165 143 78 116 60 153 113 62 89 175 125 90 85 178 167 200 133 168 125 92 62 93 166 141 98 172 102 103 72 179 138 108 49 176 46 70 55 101 141 144 107 126 60 146 108 77 125 59 58 109 171 107 63 151 55 93 172 131 52 128 75 167 255 97 108 171 113 130 77 166 150 132 62 196\r\n\t [[Node: cross_entropy_per_example/cross_entropy_per_example = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT64, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](softmax_linear/softmax_linear, Cast_4)]]", "comments": ["Please include all the information requested in the new issue template. ", "duplicate of #5963", "If anyone else runs into this issue, and you're certain it isn't a logic error, see my answer here:\r\nhttps://stackoverflow.com/a/67491390/13924556\r\n\r\nTook me a minute to figure it out."]}, {"number": 5931, "title": "what is tag id or master commit id for 0.11rc2 version", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["You can check out the code for 0.11.0rc2 using:\r\nhttps://github.com/tensorflow/tensorflow/tree/v0.11.0rc2\r\n\r\nYou can find out which tag each release uses through the \"Releases\" page, by first going to:\r\nhttps://github.com/tensorflow/tensorflow\r\nThen clicking on \"16 releases\" right below the code, issues and pull requests tabs.\r\n"]}, {"number": 5930, "title": "base class order for tensorflow distributions", "body": "this is a feature request for `tf.contrib.distributions`. on master (i.e., beyond version r0.11), tensorflow distributions require that the first parent class is Distribution.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distributions/python/ops/distribution.py#L136-L138\r\n\r\nthis is a problem for our work with random variables in edward, which requires that random variable classes are first and Distribution is second:\r\n\r\nhttps://github.com/blei-lab/edward/blob/master/edward/models/random_variable.py#L35\r\nhttps://github.com/blei-lab/edward/blob/master/edward/models/random_variables.py#L33\r\n\r\ni chatted with @ebrevdo and @jvdillon about this a few weeks ago. they said it's possible to change. (i'm adding a GitHub issue to formalize the request.) can i help in any way? it is a high priority to fix for edward as it breaks all random variable support.", "comments": ["Testing the fix now.\n\nOn Tue, Nov 29, 2016 at 12:15 AM, Dustin Tran <notifications@github.com>\nwrote:\n\n> this is a feature request for tf.contrib.distributions. on master (i.e.,\n> beyond version r0.11), tensorflow distributions require that the first\n> parent class is Distribution.\n>\n> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/\n> distributions/python/ops/distribution.py#L136-L138\n>\n> this is a problem for our work with random variables in edward, which\n> requires that random variable classes are first and Distribution is second:\n>\n> https://github.com/blei-lab/edward/blob/master/edward/\n> models/random_variable.py#L35\n> https://github.com/blei-lab/edward/blob/master/edward/\n> models/random_variables.py#L33\n>\n> i chatted with @ebrevdo <https://github.com/ebrevdo> and @jvdillon\n> <https://github.com/jvdillon> about this a few weeks ago. they said it's\n> possible to change. (i'm adding a GitHub issue to formalize the request.)\n> can i help in any way? it is a high priority to fix for edward as it breaks\n> all random variable support.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/5930>, or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimxqRsIrvAnfKQwW9XAW3X_3FMeYDks5rC98UgaJpZM4K-rr1>\n> .\n>\n", "It works.", "Could you please merge https://github.com/tensorflow/tensorflow/commit/77cfa97ac784212130e97bc6fa9e6f8acfc86c08 to `r0.12` branch as well?", "Any news? Should we wait until 1.0 release? @yifeif ", "This is fixed at head and will be in the next rc\n\nOn Dec 28, 2016 3:15 AM, \"G\u00f6k\u00e7en Eraslan\" <notifications@github.com> wrote:\n\n> Any news? Should we wait until 1.0 release?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/5930#issuecomment-269463278>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimyWA0vMUhyTTqWWnwPNHB9fOEWQgks5rMkTDgaJpZM4K-rr1>\n> .\n>\n", "excuse me if it's the wrong place to ask. About when do you intend to do Tensorflow-0.13rc ?", "This should be fixed in the RC.", "just to confirm, this hasn't been fixed in any of the <1.0.0 releases yet, and is only fixed in the 1.0.0-alpha release.", "Looks like\n\nOn Jan 18, 2017 10:15 AM, \"Dustin Tran\" <notifications@github.com> wrote:\n\n> just to confirm, this hasn't been fixed in any of the <1.0.0 releases yet,\n> and is only fixed in the 1.0.0-alpha release.\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/5930#issuecomment-273555338>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim0mjc7H-BxFdyWuc3dK5huUaA5CZks5rTla5gaJpZM4K-rr1>\n> .\n>\n"]}, {"number": 5929, "title": "checkpoints file's max_to_keep not working", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\ncannot find any related post in the web\r\n\r\n### Environment info\r\nOperating System:\r\n\r\ncentos6.5   , tensorflow 0.11.0 without gpu(cpu only)\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n```\r\ntf.train.Saver(max_to_keep = 5)\r\nthen\r\nsaver.save(sess, checkpoint_file, global_step=step)\r\n```\r\n### What other attempted solutions have you tried?\r\n***IMPORTANT*** \r\nI run the same code in my laptop, it's right with only 5 latest checkponits\r\n**but** in the centos server , it generate unlimited checkponits files and the disk almost full\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n\r\nno error log", "comments": ["update:\r\nI try the tensorflow 0.12rc   , same problem", "From your two experiments the problem seems possibly centos6.5 specific, but not TF version specific.  The max_to_keep feature is implemented in this file:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/saver.py\r\nin a function called _MaybeDeleteOldCheckpoints.  It would be interesting to know whether this function is being called correctly, and if so whether the old file recognition or deletion is failing.  Add some print statements?\r\n\r\nYou might look first in your logs to see whether there are any messages about this.", "I found that when run to save.py line 1157, the \"checkpoint_prefix\" variable has double slash like \"checkpoint/adagrad-lr0.01-fs200-b10-u128.64.64.32//checkpoint.ckpt-6700.index\"\r\n**simply replace \"//\" with \"/\" and it's worked**\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/saver.py#L1157\r\n", "maybe a bug about file_io?", "@concretevitamin: can the checkpoint_prefix path component be improved?", "@ericyue: what's your `checkpoint_path`, when passed to `save()`?  We'd need to figure out whether this is related to the user-passed argument or something inside (e.g. file_io).", "checkpoint_path is something like \"./checkpoint/model_xxxx//checkpoint.ckpt\"\r\nI noticed that the double slash is passed by myself.\r\nbut , why this got error on centos , mac works right? ", "My guess is a quirk in the file system.  Since the issue has been identified I'm going to close this.", "@concretevitamin  @poxvoculi  can you help to solve this? also checkpoint ploblem https://github.com/tensorflow/tensorflow/issues/6326", "I ran into the same thing with the same fix using tf 1.2. I noticed that in file_io.py get_matching_files does a string comparison to check files, and I'd put good odds on this (or something similar) being the culprit.", "Also have same problems. More and more big checkpoints easily cause the resource exhausted problems, can someone look into it, thanks a lot!", "@jowettcz I don't understand what \"same problems\" is.  This issue was closed a while ago as a quirk of centos handling // in pathnames differently.  ", "@poxvoculi I'm not running centos (ubuntu 16.04) and I ran into this.  \r\n\r\n(Also I don't know how I un-assigned @concretevitamin ?)", "@kesinger I think @concretevitamin is no longer working at Google, but maybe he's still responding to issues on GitHub.  Can you confirm whether, like @ericyue, you are passing a pathname with an unusual construction, like //, that can be avoided?\r\n\r\nI see that a significant change to get_matching_files in file_io.py was submitted in May 2017.  Are you using a TF version that has it?", "I observe similar behaviour: at some random points in time saver starts to consistently ignore max_to_keep parameter and does not delete older checkpoints.\r\n\r\nI wish I could provide more info.", "Same here.\r\n\r\nIn \r\n`\r\nsaver = tf.train.Saver(max_to_keep=1)\r\n`\r\n...\r\n`\r\nsaver.save(sess, result_path + \"/model.ckpt\", global_step=save_num, write_meta_graph=False)\r\n`\r\nThe '/' before model.ckpt makes the difference whether my 1 Terrabyte quota is exhausted over night or not. Both on Ubuntu 14 and Scientific Linux 7."]}, {"number": 5928, "title": "Support distributed aggregation in embedding_lookup_sparse", "body": "The current code just gather the slice (co-located with the partition) and send back to the caller for the further aggregation with the specified combiner. In some cases, making the aggregation co-located with the variable partition can reduce latency (both computing time and transfer time).\r\n\r\nFor example, the embedding weights has shape `10000000 * 1024` with `10` partitions. The batch size is `256`. And the average number non-zero value of each feature tensor is `100`. Without aggregation, the transfer size out of each partition is around `256 * 100/10 * 1024 * 4 = 10MB`. If we make the aggregation first, the data size can be reduced to around `256 * 1024 * 4 = 1MB`. There are two benefits for the latency, first is the reduced transfer size, the second is partial of the aggregation is done in parallel.\r\n\r\nIt's suggested to add an option with this feature default to false (whether to use this feature depends on the devices speed up ratio and transfer bandwidth).", "comments": ["I'll work on this.", "before [dynamic_stitch](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/embedding_ops.py#L183), each partition's results should first be normalized, then multiply weights, then aggregate. code in `embedding_ops.py` needs great changes."]}, {"number": 5927, "title": "Cannot able to install Tensorflow r0.12 in Windows 10 - HTTP error 404", "body": "I was following the documentation of the [tensorflow](https://www.tensorflow.org/versions/r0.12/get_started/os_setup.html#pip-installation-on-windows) with respect to installing tensorflow in windows 10.\r\n\r\nWhen I execute pip command I'm getting following error\r\n\r\n**pip install --upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-0.12.0rc0-cp35-cp35m-win_amd64.whl**\r\n\r\nCollecting tensorflow==0.12.0rc0 from https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-0.12.0rc0-cp35-cp35m-win_amd64.whl\r\n  HTTP error 404 while getting https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-0.12.0rc0-cp35-cp35m-win_amd64.whl\r\n  Could not install requirement tensorflow==0.12.0rc0 from https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-0.12.0rc0-cp35-cp35m-win_amd64.whl because of error 404 Client Error: Not Found for url: https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-0.12.0rc0-cp35-cp35m-win_amd64.whl\r\nCould not install requirement tensorflow==0.12.0rc0 from https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-0.12.0rc0-cp35-cp35m-win_amd64.whl because of HTTP error 404 Client Error: Not Found for url: https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-0.12.0rc0-cp35-cp35m-win_amd64.whl for URL https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-0.12.0rc0-cp35-cp35m-win_amd64.whl", "comments": ["Oops! This file is now in place, so the installation should work if you try again. Please re-open the issue if you continue to have problems!"]}, {"number": 5926, "title": "Replace parser with FLAGS", "body": "", "comments": ["Can one of the admins verify this patch?", "Sorry, we are trying to move away from flags library"]}, {"number": 5925, "title": "Should we use cudnn v5.0 or v5.1?", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System: osx Sierra\r\n\r\nInstalled version of CUDA and cuDNN:\r\n\r\n- cuda v8.0,\r\n- cudnn v5.0\r\n \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\n```\r\nlib/libcuda.1.dylib     lib/libcudart.8.0.dylib lib/libcudnn.5.dylib\r\nlib/libcuda.dylib       lib/libcudart.dylib     lib/libcudnn.dylib\r\nlib/libcudadevrt.a      lib/libcudart_static.a  lib/libcudnn_static.a\r\n```\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed: https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow-0.11.0-py3-none-any.whl\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\n```\r\npython3 -c \"import tensorflow; print(tensorflow.__version__)\"\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.1.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.dylib locally\r\n0.11.0rc2\r\n```\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n```\r\ncd /usr/local/lib/python3.5/site-packages/tensorflow/models/image/mnist\r\npython3 convolutional.py\r\n````\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n**Install cudnn v5.1**\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n\r\nThe code sample failed, complaining about cudnn and tensorflow version mismatch. I lost the original output. It said something like the tensorflow is build with cudnn 51000x, but the loaded cudnn lib is 50000.\r\n\r\nThen, I installed cudnn v5.1, and the message is gone, and the sample code runs.\r\n\r\nI followed the installation instruction on https://www.tensorflow.org/versions/master/get_started/os_setup.html#pip-installation. I think either the instruction needs update to tell people to install cudnn v5.1; or something mistakenly build tensorflow with cudnn v5.1 and released it.\r\n", "comments": ["Thanks for the detailed report.  I believe TF version 0.12 introduced support for cudnn v5.1 with cuda 8.0.  It may be required on OSX.  ", "In MacOSX, 0.11 (final, non-rc) and 0.12 both require CuDNN 5.1 if you are using our prebuilt packages.\r\nin 0.11 rc versions, we noticed a problem in our builder machines where there were discrepancies in mac cudnn versions, so please install cudnn 5.1, then upgrade to either 0.11.0 (final) or 0.12.0rc0."]}, {"number": 5924, "title": "PLZ help.thx.tensorflow constatnly fails to download some packages!", "body": "On a KNL server, I tried to install Tensorflow by compiling way. I'm not root so I manually set the env in my directory.  And there is no network on the server, so I use SSH FORWARD.\r\nHere is the problem I've met:\r\n\r\n\r\nERROR: /home/guest/tensorflow/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_resizable_behavior//': Error downloading from https://github.com/polymerelements/iron-resizable-behavior/archive/v1.0.3.tar.gz to /home/guest/.cache/bazel/_bazel_guest/ac941aabf1be9475583be832ae449128/external/iron_resizable_behavior: Error downloading https://github.com/polymerelements/iron-resizable-behavior/archive/v1.0.3.tar.gz to /home/guest/.cache/bazel/_bazel_guest/ac941aabf1be9475583be832ae449128/external/iron_resizable_behavior/v1.0.3.tar.gz: Timed out connecting to https://github.com/polymerelements/iron-resizable-behavior/archive/v1.0.3.tar.gz : Read timed out and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/guest/tensorflow/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_resizable_behavior//': Error downloading from https://github.com/polymerelements/iron-resizable-behavior/archive/v1.0.3.tar.gz to /home/guest/.cache/bazel/_bazel_guest/ac941aabf1be9475583be832ae449128/external/iron_resizable_behavior: Error downloading https://github.com/polymerelements/iron-resizable-behavior/archive/v1.0.3.tar.gz to /home/guest/.cache/bazel/_bazel_guest/ac941aabf1be9475583be832ae449128/external/iron_resizable_behavior/v1.0.3.tar.gz: Timed out connecting to https://github.com/polymerelements/iron-resizable-behavior/archive/v1.0.3.tar.gz : Read timed out and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: Evaluation of query \"deps((//tensorflow/... union @bazel_tools//tools/jdk:toolchain))\" failed: errors were encountered while computing transitive closure.\r\n", "comments": ["A fix to Bazel is going to be pushed soon. It's going to be amazing.", "This has been fixed by https://github.com/bazelbuild/bazel/commit/ed7ced0018dc5c5ebd6fc8afc7158037ac1df00d. Try using Bazel at HEAD. A new Bazel will be cut in a few days. As soon as that happens, the TensorFlow repository will be updated to use multiple mirrors for each external file."]}, {"number": 5923, "title": "Merge pull request #1 from tensorflow/master", "body": "Update", "comments": ["Can one of the admins verify this patch?"]}, {"number": 5922, "title": "the sample code \"translate.py\" does not train a correct model", "body": "After v0.10, the seq2seq sample code, translate.py, although runnable, but does not give correct output.\r\nI trained a default model using downloaded EN->FR data set, the trained system cannot even translate basic words like: I, you, he and she.\r\nDoes someone know why?\r\n\r\n\r\nBelow are my system info:\r\n    Operating System:\r\n    Ubuntu 14.04.5 LTS\r\n\r\n    Installed version of CUDA and cuDNN:\r\n    /usr/local/cuda-8.0\r\n    cudnn-8.0-linux-x64-v5.1.tgz\r\n\r\n    It is installed from binary pip package\r\n\r\n    A link to the pip package you installed:\r\n    https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl\r\n\r\n    The output from python -c \"import tensorflow; print(tensorflow.version)\"\r\n    xuancong@wxc-i2r:~/projects/tf-rnnlm$ python -c \"import tensorflow; print(tensorflow.version)\"\r\n    I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\r\n    I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\r\n    I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\r\n    I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\r\n    I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\r\n    0.11.0\r\n\r\n", "comments": ["This sort of question is better asked on stackoverflow, assuming that it's user error and not a completely broken model.  In order to try to make that distinction, it would be helpful if you would clarify what happened when you trained the model, so we can determine whether training failed, or inference from a correct model is not working.\r\n\r\nDid the training run the expected number of steps without error?  Did the loss decrease as expected?  Did you run evaluations on the training set and a hold-out set and see the expected increases in accuracy during training?    If the training failed, you need to figure out why.\r\n\r\nIf training succeed, then the problem must lie in inference.  Again, it would be helpful to know exactly how you invoked inference and what diagnostic messages might have occurred.", "@poxvoculi Thanks for your reply!\r\nSince I am new to NN, there is something I would like to clarify with you.\r\nSince our server does not have enough graphics memory, I reduced the embedding size from 1024 to 512 to fit into memory.\r\nSince training took forever, the one I tested is at about:\r\nglobal step 150200 learning rate 0.1262 step-time 0.51 perplexity 6.82\r\n  eval: bucket 0 perplexity 3.91                                      \r\n  eval: bucket 1 perplexity 5.67                                      \r\n  eval: bucket 2 perplexity 6.33                                      \r\n  eval: bucket 3 perplexity 11.45                                     \r\n\r\nAll other parameters I use the default.\r\nSo does everything look normal?", "Those are some critical details.  By reducing the embedding size, you reduced its ability to distinguish between words dramatically.  Also, you may not have trained long enough.  The tutorial suggests you should train for a full epoch (340,000 steps at batch 64) before expecting the model to work...with the original embedding size.  Nonetheless, the perplexity scores suggest that it learned something. \r\n\r\nOne rationality test would be to retry queries from your eval set on the inference version you say cannot translate basic terms.  Do you get the same overall perplexity score as at the end of training?  If not, if its much worse, then you've somehow set up the inference model incorrectly.  If so, then either your sample test queries are not good (try full sentences, not single words), or the model isn't capable enough or trained long enough.\r\n\r\nI'm not an expert on translation models.  Perhaps someone else reading this thread has more insight.", "Closing due to lack of activity."]}, {"number": 5921, "title": "IOS - No OpKernel was registered to support Op \u2018SpaceToBatchND' with these attrs", "body": "I am trying to use `tf.nn.atrous_conv2d` to solve a segmentation problem. It works on my mac but when I saved this graph using `convert_variables_to_constants` and `tf.train.write_graph` then read it in using C++ API trying to get it running on my iPad, I got this error:\r\n\r\n```\r\nCould not create TensorFlow Graph: Invalid argument: No OpKernel was registered to support Op 'SpaceToBatchND' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[Node: CCC/SpaceToBatchND = SpaceToBatchND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tpaddings=DT_INT32](BBB, CCC/SpaceToBatchND/block_shape, CCC/SpaceToBatchND/paddings)]]\r\nF /workplace/testTensorFlow/testTensorFlowXcode/Classes/test_tensorflow.mm:162] test_tensorflow.mm : Couldn't load model: Invalid argument: No OpKernel was registered to support Op 'SpaceToBatchND' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[Node: CCC/SpaceToBatchND = SpaceToBatchND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tpaddings=DT_INT32](BBB, CCC/SpaceToBatchND/block_shape, CCC/SpaceToBatchND/paddings)]]\r\n```\r\n\r\n## What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/1269\r\nhttps://github.com/tensorflow/tensorflow/issues/2680\r\nhttps://github.com/tensorflow/tensorflow/issues/2929\r\n\r\n## If possible, provide a minimal reproducible example\r\n\r\nIn Python:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework.graph_util import convert_variables_to_constants\r\n\r\nflt = tf.Variable(tf.random_normal([3, 3, 3, 3], stddev=0.35), name='AAA')\r\ninputs = tf.placeholder(shape=(1, 224, 224, 3), dtype=tf.float32, name='BBB')\r\nconv = tf.nn.atrous_conv2d(value=inputs, filters=flt, rate=2, padding='SAME', name='CCC')\r\n\r\nsess = tf.Session(config=config)\r\ninit = tf.initialize_all_variables()\r\nsess.run(init)\r\n\r\nwith sess.as_default():\r\n    minimal_graph = convert_variables_to_constants(sess, sess.graph_def, [\"CCC\"])\r\n    tf.train.write_graph(minimal_graph, '.', 'rand_init_min_cpu.pb', as_text=False)\r\n```\r\n\r\nI can load and run graphs not using atrous_conv2d correctly, so I think my C++ code is ok.\r\n \r\n## What other attempted solutions have you tried?\r\n\r\nFollowing the related issues above, I tried to add SpaceToBatchND kernel to \r\n\r\n`tensorflow/contrib/makefile/tf_op_files.txt`\r\n\r\nI found SpaceToBatchND in array_ops.cc (which has already been included in `tf_op_files.txt`)\r\nand maybe in these two files:\r\n\r\n`tensorflow/core/kernels/spacetobatch_op.cc`\r\n`tensorflow/core/kernels/batchtospace_op.cc`\r\n\r\nSo I added them to `tf_op_files.txt`, but then when I `./build_all_ios.sh`, I got these errors (short version, I can post them all out if you need):\r\n\r\n```\r\nUndefined symbols for architecture armv7:     \r\n\"tensorflow::functor::SpaceToBatchFunctor<Eigen::ThreadPoolDevice, float, 4, $\r\nrue>::operator()(Eigen::ThreadPoolDevice const&, Eigen::TensorMap<Eigen::Tensor$\r\nfloat, 6, 1, int>, 16, Eigen::MakePointer>, long long const*, long long const*, \r\nEigen::TensorMap<Eigen::Tensor<float const, 6, 1, int>, 16, Eigen::MakePointer>$\r\n\", referenced from: \r\n\r\n...\r\n\r\nvoid tensorflow::(anonymous namespace)::SpaceToBatchOpCompute<Eigen::Threa\r\ndPoolDevice, int>(tensorflow::OpKernelContext*, tensorflow::Tensor const&, tenso\r\nrflow::Tensor const&, tensorflow::Tensor const&) in libtensorflow-core-armv7.a(s\r\npacetobatch_op.o)\r\nld: symbol(s) not found for architecture armv7\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nmake: *** [workingspace/tensorflow/tensorflow/contrib/makefile/gen/b\r\nin/ios_ARMV7/benchmark] Error 1\r\n+ '[' 2 -ne 0 ']'\r\n+ echo 'armv7 compilation failed.'\r\narmv7 compilation failed.\r\n```\r\n\r\n## Environment info\r\n\r\nOperating System:\r\nmac: OSX 10.11.6\r\niPad: IOS 10.1.1\r\n\r\nInstalled version of CUDA and cuDNN:\r\nNone\r\n\r\nSource:\r\ncommit d6b25985ac219a6e58d186a2beb74d5e8d9e4533\r\n\r\nbazel version:                                     \r\n.................................................\r\nBuild label: 0.4.0-homebrew\r\nBuild target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Nov 2 19:15:37 2016 (1478114137)\r\nBuild timestamp: 1478114137\r\nBuild timestamp as int: 1478114137\r\n", "comments": ["So, any luck on solving this?\r\n\r\nWe are currently trying to use a new model without atrous_conv2d but the result is not as good. Maybe we haven't fine tune the new model yet but using atrous_conv2d would be a big time saver.\r\n\r\nAny quick fixes or ideas would be appreciated, thanks.", "@PhilipZeratul: I worked around this by adding the following to `tf_op_files.txt`:\r\n\r\n```\r\ntensorflow/core/kernels/spacetobatch_op.cc\r\ntensorflow/core/kernels/spacetobatch_functor.cc\r\n```\r\nI've also replaced `__ANDROID_TYPES_SLIM__` with `__ANDROID_TYPES_FULL__` everywhere in the Makefile, but admittedly I am not sure whether this is needed.\r\n\r\nFor a couple of other flavours of the same error I also had to add\r\n```\r\ntensorflow/core/kernels/cwise_op_mod.cc\r\ntensorflow/core/kernels/batchtospace_op.cc\r\n```\r\nThis is along the lines of the suggestions in issue [4863](https://github.com/tensorflow/tensorflow/issues/4863)\r\n\r\nI hope this can help you (or someone else) through the fun times of compiling for iOS.", "@schackv: Thank you for your response.\r\n\r\nIt has been a while since we met this problem so we kind of manually implemented  atrous-conv using normal conv and achieved our goal with some efficiency sacrifice.\r\n\r\nWe are currently working on another project so I can not test this now, so I'm going to close this issue but still thank you very much."]}, {"number": 5920, "title": "Make activation function support sparse tensors", "body": "Usually, the input features need to be normalized, for example, when the feature is *duration of app usage* in *wide and deep* model. Usually you are not clear which normalization function is best for your model when you generate the features. So in the model, an activation function(for example softsign) can be applied to the input first. Currently sparse tensor is not supported. This change can be trivial.\r\n", "comments": ["I don't know what you say. Because, I can't understand sentence that is 'Currently sparse tensor is not supported.'\r\n\r\nYou can see website.\r\nhttps://www.tensorflow.org/versions/r0.12/api_docs/python/sparse_ops/sparse_tensor_representation#SparseTensor\r\n\r\nCan you please comment for me?\r\n\r\nThank you.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 5919, "title": "strip_unused should remove Switch Node with Constant Input", "body": "`strip_unused` does not appear to remove a `Switch` node with constant input.", "comments": ["I am thinking the solution looks something like this:\r\n```python\r\nnodes = {}\r\nfor node in temp_graph_def.node:\r\n    nodes[node.name] = node\r\n\r\nfor node in temp_graph_def.node:\r\n    if node.op == 'Switch':\r\n        pred = nodes[node.input[1]]\r\n        if pred.op == 'Const' and pred.attr['value'].tensor.bool_val[0] is False:\r\n            pass\r\n```", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "I think this is still relevant and \"active\" in that it was referenced by https://github.com/tensorflow/tensorflow/issues/8404.", "This workaround works for me.\r\nhttps://stackoverflow.com/a/43627334", "@cancan101 Happy to reopen if you're planning to submit a PR.  Otherwise it looks like this contributions welcome issue is being ignored.", "The new approach to modify the graph is the [Graph Transform Tool](https://github.com/tensorflow/tensorflow/issues/8404#issuecomment-286778560).", "Hey guys! I've been having the same problem trying to make this work on android and I have used some of the solutions from above but none seem to work...here are some of the errors I get back from the various methods of trying to solve this problem...Any Help would be appreciated! \r\n\r\nThe models work when I run it on python but doesn't when I run them on android.\r\n\r\nIm using the SSD_Mobilenet model from tensorflow object detection api.\r\n\r\n**This is from the optimize for inference function:** \r\nNodeDef expected inputs '' do not match 1 inputs specified; Op<name=Const; signature= -> output:dtype; attr=value:tensor; attr=dtype:type>; NodeDef: Preprocessor/map/while/add/y = Const[dtype=DT_INT32, value=Tensor<type: int32 shape: [] values: 1>](Preprocessor/map/while/Switch:1)\r\n\r\n**This is from renaming Switch to Identify:**\r\nNot a valid TensorFlow Graph serialization: Node 'Preprocessor/map/while/add/y': Connecting to invalid output 1 of source node Preprocessor/map/while/Switch which has 1 outputs\r\n\r\n**This last one loads the model but crashes when processing the inputs:**\r\njava.lang.IllegalArgumentException: No OpKernel was registered to support Op 'Switch' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n                                                                    device='GPU'; T in [DT_STRING]\r\n                                                                    device='GPU'; T in [DT_BOOL]\r\n                                                                    device='GPU'; T in [DT_INT32]\r\n                                                                    device='GPU'; T in [DT_FLOAT]\r\n                                                                    device='CPU'; T in [DT_FLOAT]\r\n                                                                    device='CPU'; T in [DT_INT32]\r\n                                                                  \r\n                                                                  \t [[Node: Postprocessor/BatchMultiClassNonMaxSuppression/PadOrClipBoxList/cond/Switch = Switch[T=DT_BOOL](Postprocessor/BatchMultiClassNonMaxSuppression/PadOrClipBoxList/Greater, Postprocessor/BatchMultiClassNonMaxSuppression/PadOrClipBoxList/Greater)]]\r\n                                                                      \r\nAny ideas guys? I've also tried to recompile tensorflow with the changes from [https://stackoverflow.com/questions/40855271/no-opkernel-was-registered-to-support-op-switch-with-these-attrs-on-ios/43627334#43627334](url) but bazel crashes while compiling :( \r\n\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 5918, "title": "add convert_placeholders_to_constants to tensorflow", "body": "Add `convert_placeholders_to_constants` which permanently injects a constant to fill in the value of a Placeholder.\r\n\r\nRepost from [SO](http://stackoverflow.com/questions/40852729/permanently-inject-constant-into-tensorflow-graph-for-inference/40852855). /CC @mrry ", "comments": ["Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Hi \r\nI have a graph with 3 inputs. For a particular application, two of the inputs are constants. \r\nBy applying the Graph Transform Tools, I want to replace those input nodes with the constant values. \r\n\r\nCan you help with this thing? When running the graph transform with ignoring the two inputs, I get the following error:\r\n\r\n`fold_constants: Ignoring error You must feed a value for placeholder tensor 'model_1/keep_probabilty' with dtype float [[Node: model_1/keep_probabilty = Placeholder[dtype=DT_FLOAT, shape=<unknown>]()]]`\r\n"]}, {"number": 5917, "title": "Broken link in \"A Tool Developer's Guide...\" for graph_metrics.py", "body": "In [this page](https://www.tensorflow.org/versions/master/how_tos/tool_developers/index.html), \r\ngraph_metrics.py link broken. graph_metrics.py is deleted by [this commit](https://github.com/tensorflow/tensorflow/commit/d8c94dba8f53626077b21f90df309577827a0f18#diff-2839c4fb2e611a1359978388f55f3583).", "comments": ["Thanks for the report.  Internal bug opened, but community fix welcome also.", "Is anyone working on this internally? I can work on this. If yes, then should I just remove that line or replace it with a link to some other file?", "So far as I know, no one is working on it yet.  @danmane ?"]}, {"number": 5916, "title": "Update release note for tensorboard", "body": "", "comments": []}, {"number": 5915, "title": "Update release note for tensorboard.", "body": "", "comments": []}, {"number": 5914, "title": "No documentation for sending input parameters of request to TensorFlow Serving ", "body": "I am running the sample iris program in TensorFlow Serving. Since it is a TF.Learn model, I am exporting the model using the following `classifier.export(export_dir=model_dir,signature_fn=my_classification_signature_fn)`\r\n\r\nand the signature_fn is defined as shown below:\r\n\r\n```\r\ndef my_classification_signature_fn(examples, unused_features, predictions):\r\n  \"\"\"Creates classification signature from given examples and predictions.\r\n  Args:\r\n    examples: `Tensor`.\r\n    unused_features: `dict` of `Tensor`s.\r\n    predictions: `Tensor` or dict of tensors that contains the classes tensor\r\n      as in {'classes': `Tensor`}.\r\n  Returns:\r\n    Tuple of default classification signature and empty named signatures.\r\n  Raises:\r\n    ValueError: If examples is `None`.\r\n  \"\"\"\r\n  if examples is None:\r\n    raise ValueError('examples cannot be None when using this signature fn.')\r\n\r\n  if isinstance(predictions, dict):\r\n    default_signature = exporter.classification_signature(\r\n        examples, classes_tensor=predictions['classes'])\r\n\r\n  else:\r\n\r\n    default_signature = exporter.classification_signature(\r\n        examples, classes_tensor=predictions)\r\n  named_graph_signatures={\r\n        'inputs': exporter.generic_signature({'x_values': examples}),\r\n        'outputs': exporter.generic_signature({'preds': predictions})}    \r\n  return default_signature, named_graph_signatures\r\n```\r\nThe model gets successfully exported using the following piece of code.\r\n\r\nI have created a client which makes real-time predictions using TensorFlow Serving.\r\n\r\nThe following is the code for the client:\r\n\r\n```\r\nflags.DEFINE_string(\"model_dir\", \"/tmp/iris_model_dir\", \"Base directory for output models.\")\r\ntf.app.flags.DEFINE_integer('concurrency', 1,\r\n                            'maximum number of concurrent inference requests')\r\ntf.app.flags.DEFINE_string('server', '', 'PredictionService host:port')\r\n\r\n#connection\r\nhost, port = FLAGS.server.split(':')\r\nchannel = implementations.insecure_channel(host, int(port))\r\nstub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\r\n\r\n\r\n# Classify two new flower samples.\r\nnew_samples = np.array([5.8, 3.1, 5.0, 1.7], dtype=float)\r\n\r\nrequest = predict_pb2.PredictRequest()\r\nrequest.model_spec.name = 'iris'\r\n\r\nrequest.inputs[\"x_values\"].CopyFrom(\r\n        tf.contrib.util.make_tensor_proto(new_samples))\r\n\r\nresult = stub.Predict(request, 10.0)  # 10 secs timeout\r\n```\r\n\r\nHowever, on making the predictions, the following error is displayed:\r\n\r\n`grpc.framework.interfaces.face.face.AbortionError: AbortionError(code=StatusCode.INTERNAL, details=\"Output 0 of type double does not match declared output type string for node _recv_input_example_tensor_0 = _Recv[client_terminated=true, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=2016246895612781641, tensor_name=\"input_example_tensor:0\", tensor_type=DT_STRING, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()\")`\r\n\r\nHere is the entire stack trace.\r\n![image](https://cloud.githubusercontent.com/assets/17990840/20688020/fac6285a-b58c-11e6-8a93-1fed6d78b8de.png)\r\n\r\nThe iris model is defined in the following manner:\r\n\r\n```\r\n# Specify that all features have real-value data\r\nfeature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=4)]\r\n\r\n# Build 3 layer DNN with 10, 20, 10 units respectively.\r\nclassifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,\r\n                                            hidden_units=[10, 20, 10],\r\n                                            n_classes=3, model_dir=model_dir)\r\n\r\n# Fit model.\r\nclassifier.fit(x=training_set.data, \r\n               y=training_set.target, \r\n               steps=2000)\r\n```\r\n\r\nCan someone provide some documentation for creating the client program which sends the request to TensorFlow Serving. ", "comments": ["Can someone help me with this?\r\n", "Generally, this kind of question is best asked on stackoverflow.  This forum is for bug reports.\r\n\r\nI'm not an expert on this topic.  Have you had success using TensorFlow Serving with any other models?  That is, have you tried (and succeeded) with these tutorials?\r\n\r\nhttps://tensorflow.github.io/serving/serving_basic.html\r\nhttps://tensorflow.github.io/serving/serving_advanced.html\r\n\r\n", "@poxvoculi: I have already asked the question on stack overflow, however, I haven't received any help. And I have also tried running the basic models (mnist and inception) and they seem to work fine. However, there is no documentation for actually creating a client program for any other algorithms, for instance, the models build using Tf.Learn API. Which is why I raised a concern here so that there could be some documentation for the same. ", "@martinwicke could you reassign to someone who might provide documentation?", "Can you please ask this question on tensorflow/serving?"]}, {"number": 5913, "title": "strip_unused assumes all placeholders are the same type", "body": "[`strip_unused`](https://github.com/tensorflow/tensorflow/blob/5657d0dee8d87f4594b3e5902ed3e3ca8d6dfc0a/tensorflow/python/tools/strip_unused.py) assumes that all of the placeholders are of the same type which may not be the case. This method is also used by [`optimize_for_inference`](https://github.com/tensorflow/tensorflow/blob/5657d0dee8d87f4594b3e5902ed3e3ca8d6dfc0a/tensorflow/python/tools/optimize_for_inference_lib.py#L82-L85) which leads to errors when the models is loaded:\r\n```\r\n msg std::__1::string \"Input 0 of node dropout6/cond/Switch was passed float from Placeholder_2:0 incompatible with expected bool.\" \r\n```\r\n\r\nI suggest having an option to introspect the graph to determine type.", "comments": ["For example why is this line:\r\n```python\r\n      placeholder_node.attr[\"dtype\"].CopyFrom(tf.AttrValue(\r\n          type=placeholder_type_enum))\r\n```\r\nnot?:\r\n```python\r\n      placeholder_node.attr[\"dtype\"].CopyFrom(\r\n          node.attr[\"dtype\"]\r\n      )\r\n```", "This is definitely a problem for me. My X placeholder is of type `int32` and my dropout keep probability is of type `float32`, so this is unusable for me.", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you.", "this still looks to be relevant.", "@petewarden do you have any plans to fix this? If not, please unassign yourself and mark contributions welcome (if appropriate).", "The new graph transform approach fixes this issue:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms/#strip_unused_nodes\r\n\r\nWe need to update the documentation and code to mark the old strip_unused.py script as deprecated.", "Thanks, I'll close this then.", "@petewarden Is `optimize_for_inference` deprecated as well? I see no mention regarding that not even in `strip_unused.py`, so I'm wondering :/"]}, {"number": 5912, "title": "tf.nn.crelu Static shape not defined", "body": "Currently (version 0.11.0) the static shape is not defined for crelu. Minimal example:\r\n\r\n    f = tf.random_normal([50, 5, 7, 10])\r\n\r\n    f2 = tf.nn.crelu(f)\r\n    print(f2.get_shape().as_list())  # [None, None, None, None]\r\n\r\n    f3 = tf.nn.relu(f)\r\n    print(f3.get_shape().as_list()) # [50, 5, 7, 10]\r\n\r\n\r\n", "comments": ["Fix applied internally at Google.  Should roll out with a subsequent release (after 0.12)."]}, {"number": 5911, "title": "Fix loading of libhdfs.so when installed in non-standard directory", "body": "Some Hadoop distributions do not install libhdfs.so in $HADOOP_HDFS_HOME/lib/native. For example, Cloudera Hadoop installs libhdfs.so in /opt/cloudera/parcels/CDH/lib64/libhdfs.so.\r\n\r\nThis patch allows the dynamic loader to search for libhdfs.so if it fails to find it in $HADOOP_HDFS_HOME/lib/native", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please"]}, {"number": 5910, "title": "Transposed Atrous Convolutions", "body": "Hi there,\r\n\r\nI'm looking for atrous (dilated) transposed convolutions, but did not find much.\r\nThere is a brief section in the [`tf.nn.atrous_conv2d`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/nn.md#tfnnatrous_conv2dvalue-filters-rate-padding-namenone-atrous_conv2d) doc\r\n\r\n> Used in conjunction with bilinear interpolation, it offers an alternative to conv2d_transpose in dense prediction tasks such as semantic image segmentation, optical flow computation, or depth estimation.\r\n\r\nBut it is not quite clear how to proceed from there, it looks like tensorflow does not implement the transposed atrous convolution, yet?", "comments": ["@gpapan is there need or plans for a transposed atrous convolution?", "Hi Paul,\nI replied to the thread at\nhttps://github.com/tensorflow/tensorflow/issues/4668\nand will look into the FR this week.\n\nOn Mon, Nov 28, 2016 at 2:26 PM, Paul Tucker <notifications@github.com>\nwrote:\n\n> @gpapan <https://github.com/gpapan> is there need or plans for a\n> transposed atrous convolution?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/5910#issuecomment-263414160>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AF8Y_Suaw-bortcAYvUttDVkOvlux00iks5rC1UFgaJpZM4K-EZA>\n> .\n>\n\n\n\n-- \nBest,\nGeorge\n", "Looks like this is a duplicate of #4668.", "Thanks for the quick reply!"]}, {"number": 5909, "title": "Revert \"Removed unnecessary classproperty decorator\"", "body": "Reverts tensorflow/tensorflow#5510 because it fails legacy usage `GraphKeys.VARIABLES` reported #5877.", "comments": ["Hmm, looks like flakes?\r\n@tensorflow-jenkins test this please.", "@tensorflow-jenkins test this please.", "This issue is starting to show up in [Stack Overflow questions](http://stackoverflow.com/q/40975371/3574081).\r\n\r\n@gunan Can we cherry-pick this into r0.12?", "I will create a cherrypick once this is merged.\r\nJenkins, test this please.\r\n", "Jenkins, test this please"]}, {"number": 5908, "title": "Add undocumented pre-required tool for building iOS library", "body": "'libtool' is not mentioned for building iOS library, which will cause 'build_all_ios.sh' failure if it is missing.", "comments": ["Can one of the admins verify this patch?", "Cool, thanks!"]}, {"number": 5907, "title": "FIFOQueue speed problem ", "body": "Hi I am testing the speed of FIFOQueue. I create a queue of Tensor with type of tf.float32 and shape of (576, 3, 220, 220). Then I push a tensor to the queue, followed by pop the tensor from the queue.\r\nThe speed of single node version( sess = tf.Session())  and distributed version of tensorflow (even in single machine single process scenario, sess = tf.Session(server.target) ) differs much. \r\nPushing and poping take about 0.2s in single node version while 4s in  distributed version.\r\nI know that using distributed version of tensorflow will encounter some proto serialize overhead, but the overhead seems too much.\r\n\r\nThe main code is like this (I also give a link to full source code below, which can be used to reproduce the problem)\r\n```\r\n    raw_shape = [576, 3, 220, 220]\r\n    shape = tf.TensorShape(raw_shape)\r\n    if FLAGS.cluster:    \r\n        server = tf.train.Server.create_local_server()\r\n        sess = tf.Session(server.target)\r\n    else:\r\n        sess = tf.Session()\r\n    with tf.device('/cpu:0'): \r\n        q = tf.FIFOQueue(10, tf.float32, shape)\r\n        rand_data = tf.zeros(shape)\r\n        init_op = tf.initialize_local_variables()\r\n        sess.run(init_op)\r\n        result = q.dequeue()\r\n        x = tf.placeholder(tf.float32, shape, 'data')\r\n        enqueue_op = q.enqueue(x)\r\n        while True:\r\n            log('pushing')\r\n            sess.run(enqueue_op, feed_dict = {x: np.zeros(raw_shape)})\r\n            log('push done')\r\n            sess.run(result)\r\n            log('pop done')\r\n```\r\n\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/3009 \r\nBut this issue is more related to threading, not same as mine case.\r\n\r\n### Environment info\r\nOperating System:\r\nUbuntu 14.04\r\nCPU E5-2643 v3 @ 3.40GHz\r\n\r\nInstalled version of CUDA and cuDNN: \r\nNone (I used CUDA_VISIBLE_DEVICES='')\r\n\r\n\r\n\r\nIf installed from source, provide \r\n1. The commit hash (`git rev-parse HEAD`)\r\nd6b25985ac219a6e58d186a2beb74d5e8d9e4533\r\n2. The output of `bazel version`\r\nBuild label: 0.4.0\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Nov 2 17:54:14 2016 (1478109254)\r\nBuild timestamp: 1478109254\r\nBuild timestamp as int: 1478109254\r\n\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nhttps://gist.github.com/gowithqi/6bcb1dc50facfd992639f09e9af463a8\r\n\r\nLog\r\nCUDA_VISIBLE_DEVICES='' python test_queue_release.py --cluster=true\r\n```\r\n2016-11-28 16:18:13.767 pushing\r\n2016-11-28 16:18:13.998 push done\r\n2016-11-28 16:18:14.112 pop done\r\n2016-11-28 16:18:14.112 pushing\r\n2016-11-28 16:18:14.337 push done\r\n2016-11-28 16:18:14.501 pop done\r\n2016-11-28 16:18:14.502 pushing\r\n2016-11-28 16:18:14.746 push done\r\n2016-11-28 16:18:14.916 pop done\r\n2016-11-28 16:18:14.916 pushing\r\n2016-11-28 16:18:15.155 push done\r\n2016-11-28 16:18:15.269 pop done\r\n```\r\n\r\nCUDA_VISIBLE_DEVICES='' python test_queue_release.py --cluster=true\r\n```\r\n2016-11-28 16:17:00.218 pushing\r\n2016-11-28 16:17:03.470 push done\r\n2016-11-28 16:17:07.395 pop done\r\n2016-11-28 16:17:07.395 pushing\r\n2016-11-28 16:17:10.908 push done\r\n2016-11-28 16:17:14.732 pop done\r\n2016-11-28 16:17:14.733 pushing\r\n2016-11-28 16:17:17.878 push done\r\n2016-11-28 16:17:21.788 pop done\r\n```\r\n", "comments": ["Thanks for the very detailed report.  Using it I was able to reproduce your problem and confirm that the speed difference is in fact due to protocol buffer construction and parsing.  At a detailed level, constructing and then parsing a protobuf involves a number of operations that are done separately on each atomic element of the data structure, in this case float32s.  As an experiment you might try changing float32 to int64 and halving one of the dimensions.  The number of bytes is the same, but now the protobuf handling speed should be decreased.   In this test case, for the purpose of higher performance it would be convenient if one could alias the Tensor as a single binary buffer of a particular length, then reinterpret that buffer as the properly typed Tensor on the other side of the grpc call.  Protobufs would support this kind of (reckless) type aliasing, but TensorFlow does not.", "Thank you for the explanation. I still wonder some questions now.\r\n1 If I want to implement the \" alias the Tensor as a single binary buffer of a particular length\" in the Tensorflow code,  can u tell me which code in the TF codebase should I modify(hack)? \r\n2 While transferring big data via FIFOQueue is slow because of serializing overhead, I also tested the multi machine cluster distributed training speed, it seems that the speedup of multi machine training is almost linear, which indicates the data transferring overhead is small comparably. So my question is why transferring hundreds of MB of data by FIFOQueue is slow while not that slow by PS, both of which need proto serializing.", "Good questions :)\r\n\r\nSecond one first: Why is there not a comparable slowdown due to protobuf serialization when talking to a parameter server shard?  Because we optimized the way in which Tensors are protobuf encoded-decoded at the RPC interface for the RecvTensor method.  A numeric Tensor is backed by a dense memory buffer of same-size binary fields.  See core/framework/tensor.proto for how as a protobuf a Tensor can be coded as a large collection of small items, or as a relatively untyped tensor_content byte array.  See the TensorResponse class and supporting code in core/distributed_runtime/tensor_coding.cc and distributed_runtime/rpc/grpc_tensor_coding.cc.  Whenever possible, when building the protobuf for sending a Tensor via RPC, we use a grpc::ByteBuffer to hold the content, rather than as a protobuf of lots of little items.\r\n\r\nBack to your first question, using the tensor_content field of tensor.proto, or the TensorResponse interface defined by tensor_coding.h would be the way to go.  I think the issue you're running into with FIFOQUEUE with the distributed interface is that maybe the RPCs involved are not the optimized RecvTensor method, but some other RPC that doesn't have efficient coding.\r\n", "@gowithqi I've noticed same kind of problem, not limited to FIFOQueue -- specifically, when Python client requests data from another TensorFlow worker, the data is transferred at a rate of 50-200MB/s even if all the workers are local. The bottleneck seems to be protobuf decoding which happens on a single core. (more cores = smaller cores = worse performance)\r\n\r\nSelf-contained benchmark that reproduces it is [here](https://github.com/tensorflow/tensorflow/issues/4498#issuecomment-248483967\r\n).\r\n\r\nSo, in your case, your buffer is 576x3x220x220x4=334MB, so I would expect a single dequeue to take 3 seconds.\r\n\r\nregarding on why \"dequeue\" transfer was faster than \"ps\" transfer -- there are two kinds of transfers:\r\n1. Between TensorFlow processes (two C runtimes)\r\n2. Between C runtime and Python runtime of the same process\r\n\r\nSo you could think of 4 different kind of scenarios on 2 dimensions:\r\n1. is data transferred between devices in single process, or between multiple processes\r\n2. result fetched in Python client vs remains in C runtime\r\n\r\nSo for single process mode on one machine, the speed I saw was 24GB/second (no Python fetch) vs 4GB/second (Python fetch), and for distributed version I saw 1 GB/second (no Python fetch) vs. 60 MB/second (Python fetch)\r\n\r\nThe way to run without Python transfer in your example is to do `sess.run(dequeue_op.op)` instead of `sess.run(dequeue_op)`", "BTW, Marc O'Connoranalyzed this slowness (gRPC transfers bottlenecking at 80-200MB/second when you add large vector of 1's in one worker to variable on another local worker and fetch result into Python on original worker) and found following break-down.\r\n\r\nOut of 1 second:\r\n1.\tNothing happens (0.2s +- 0.05): main process waits in grpc_competion_queue_pick, other threads wait in std::this_thread::__sleep_for(!)\r\n2.\tWorkers active (0.6s +-0.05 plus up to 0.2s overlap with next phase). 90% of time split time between:\r\n    a.\tDoRecvTensor (7%), all spent in Tensor::AsProtoTensorContent calling std::string::_M_replace_safe.\r\n    b.\tgrpc_completion_queue_next(26%), split between post_parse_locked calling gpr_unref (11%) and memmove_ssse3_back (15%)\r\n    c.\tDoRunGraph (34%) calling Tensor::AsProtoField (11% all spent in RepeatedField::Reserve), RunGraphResponse::ByteSize (7%) and grpc::Serialize (15%)\r\n    d.\tgrpc::Deserialize(23%)\r\n3.\tMaster merges data (0.2s +- 0.05s): All main thread time in TensorProto::MergePartialFromCodedStream, split evenly between self time, RepeatedField::Reserve and CodedInputStream::BytesUntilLimit. This partially overlaps with the previous phase.\u2028\r\n", "@gowithqi does avoiding the Python transfer as @yaroslavvb suggested address your problem?", "Closing because we haven't heard back from the OP in a while. @yaroslavvb has offered good guidance here, and he also shared some helpful knowledge regarding queues in https://github.com/tensorflow/tensorflow/issues/6845. If you're still encountering problems @gowithqi and believe there's a performance bug in TensorFlow, let us know, and I'll re-open."]}, {"number": 5906, "title": "No file exists called preprocess.py", "body": "It has been quite some time without this file existing, if there is actually a plan to work on this functionality in  Slim, then feel free to close this issue. ", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.\r\n"]}, {"number": 5905, "title": "Tensorflow - ValueError: Cannot feed value of shape", "body": "I have 12 input integer features. Output and labels is 1 or 0. I examines MNIST example from tensorflow website.\r\nHere is my code:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\ndef init_weights(shape):\r\n    return tf.Variable(tf.random_normal(shape, stddev=0.01))\r\ndef model(X, w):\r\n    return tf.matmul(X, w) # notice we use the same model as linear regression, this is because there is a baked in cost function which performs softmax and cross entropy\r\ntrain_data1  = \"./data/xx.csv\"\r\ntest_data1 = \"./data/xxx.csv\"\r\ntrain_label1 = \"./data/xxl.csv\"\r\ntest_label1 = \"./data/xx.csv\"\r\ntrain_data = np.genfromtxt(train_data1, delimiter=',')\r\ntrain_label = np.genfromtxt(train_label1, delimiter=',').astype(int)\r\ntest_data = np.genfromtxt(test_data1, delimiter=',')\r\ntest_label = np.genfromtxt(test_label1, delimiter=',').astype(int)\r\n# Load datasets.\r\ntrX, trY, teX, teY = train_data,train_label, test_data, test_label\r\nX = tf.placeholder(\"float\", [None, 12]) # create symbolic variables\r\nY = tf.placeholder(\"float\", [None, 2])\r\n\r\nw = init_weights([12, 2]) \r\npy_x = model(X, w)\r\n\r\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(py_x, Y)) # compute mean cross entropy (softmax is applied internally)\r\ntrain_op = tf.train.GradientDescentOptimizer(0.05).minimize(cost) # construct optimizer\r\npredict_op = tf.argmax(py_x, 1) # at predict time, evaluate the argmax of the logistic regression\r\n\r\n# Launch the graph in a session\r\nwith tf.Session() as sess:\r\n    # you need to initialize all variables\r\n    tf.initialize_all_variables().run()\r\n\r\n    for i in range(100):\r\n         #for (x, y) in zip(train_X, train_Y):\r\n             #sess.run(optimizer, feed_dict={X: trX, Y: trY})\r\n        for start, end in zip(range(0, len(trX), 128), range(128, len(trX)+1, 128)):\r\n            sess.run(train_op, feed_dict={X: trX[start:end], Y: trY[start:end]})\r\n        print(i, np.mean(np.argmax(teY, axis=1) ==\r\n                         sess.run(predict_op, feed_dict={X: teX})))\r\n\r\n```\r\n\r\n**But I run upper code, I get an error. The compiler says that:**\r\n```\r\nTraceback (most recent call last):\r\n  File \"LRexample.py\", line 74, in <module>\r\n    sess.run(train_op, feed_dict={X: trX[start:end], Y: trY[start:end]})\r\nValueError: Cannot feed value of shape (128,) for Tensor u'Placeholder_1:0', which has shape '(?, 2)\r\n```\r\n\r\nHow can I handle this error?", "comments": ["I solved this problem.\r\nfirstly,I transformed load data into :\r\n```\r\ntrain_data = np.genfromtxt(train_data1, delimiter=',')\r\ntrain_label = np.transpose(train_label1, delimiter=',')\r\ntest_data = np.genfromtxt(test_data1, delimiter=',')\r\ntest_label = np.transpose(test_label1, delimiter=',')\r\n```\r\nThen,transformed  trX, trY, teX, teY data into:\r\n```\r\n# convert the data\r\ntrX, trY, teX, teY = train_data,train_label, test_data, test_label\r\ntemp = trY.shape\r\ntrY = trY.reshape(temp[0], 1)\r\ntrY = np.concatenate((1-trY, trY), axis=1)\r\ntemp = teY.shape\r\nteY = teY.reshape(temp[0], 1)\r\nteY = np.concatenate((1-teY, teY), axis=1)\r\n\r\n```\r\nFinally,I transformed launching the graph in a session  into:\r\n\r\n```\r\nwith tf.Session() as sess:\r\n    # you need to initialize all variables\r\n    tf.initialize_all_variables().run()\r\n\r\n    for i in range(100):\r\n            sess.run(train_op, feed_dict={X:  trX, Y: trY})        \r\n            print(i, np.mean(np.argmax(teY, axis=1) == sess.run(predict_op, feed_dict={X: teX})))\r\n```\r\n\r\nThat's all."]}]