[{"number": 14974, "title": "Support passing layer instances to produce attentional hidden states", "body": "This PR closes #14972.\r\n  ", "comments": ["Can one of the admins verify this patch?", "@ebrevdo WDYT?", "Backwards compatibility with the simpler parameter would be great.\n\nOn Thu, Dec 28, 2017, 1:59 AM Guillaume Klein <notifications@github.com>\nwrote:\n\n> *@guillaumekln* commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\n> <https://github.com/tensorflow/tensorflow/pull/14974#discussion_r158921668>\n> :\n>\n> >        self._attention_layers = tuple(\n>            layers_core.Dense(\n> -              attention_layer_size,\n>\n> That makes sense. Do you still want to keep compatibility with the\n> existing behavior? It seems cleaner and more powerful to only allow passing\n> a list of Layers. I suppose you suggest to add a new option and raise an\n> exception when both are set.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/14974#discussion_r158921668>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim78ISHmBZmPy4Xzy3KVplGY1L8wuks5tE2aHgaJpZM4Qu6ii>\n> .\n>\n", "When directly passing layers, the output size is not known until `build(input_shape)` is called on the layer instance. Some parts of `AttentionWrapper` assume that the output size is defined at construction time which would not be the case here. Any thoughts?", "The `Dense` layer has a special hidden function, [_compute_output_shape](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/core.py#L169), which can be called before `build`.  Theoretically, all layer objects should have it (in practice, any other Layers may have to add it separately).  We use this function in `tf.contrib.seq2seq` to infer the output static shape.", "Thanks, I used this method in the latest commit. Please have a second look.", "@ebrevdo can you take another look, please?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ebrevdo: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hi @ebrevdo, the check \"layer_sizes is none != layers is None\" is already in the code. See https://github.com/tensorflow/tensorflow/pull/14974/files#diff-fb6bbbf892e4a4722fe40b8abd0b60bfR1194.\r\n\r\nI also noticed that `compute_output_shape` is now public. The PR is updated accordingly.", "Nagging Assignee @ebrevdo: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Sorry for bumping this thread. Is there something else I need to do?"]}, {"number": 14973, "title": "Tensor as index to other tensor not supported?", "body": "I have 2 tensors (one is constant and one is placeholder), lets say:\r\nA = [0.1,0.2,0.3,0.4,0.5,0.6,0.7]\r\nB = [[1,2,4],[5,0],[3]]\r\nI want to build a tensor C like this:\r\nC= [[0.2,0.3,0.5],[0.6,0.1],[0.4]] which is a tensor in the same size of B and every element in C is equal to element in A indexed by B elements.\r\n\r\nis there any way to do that?\r\n\r\nThanks!", "comments": ["I think you're looking for one of these:\r\nhttps://www.tensorflow.org/versions/master/api_docs/python/tf/gather\r\nhttps://www.tensorflow.org/versions/master/api_docs/python/tf/gather_nd\r\n\r\nFor future reference, this question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 14972, "title": "Support optional activation function to produce attentional hidden states", "body": "*This is a feature request.*\r\n\r\nWhen using an `AttentionWrapper` with the `attention_layer_size` argument set, the cell output and the context are combined and fed through a dense layer to produce an attentional hidden state.\r\n\r\nHowever, it is also common to apply an activation function on this layer output (typically *tanh*). This is for example described in:\r\n\r\n> Minh-Thang Luong, Hieu Pham, Christopher D. Manning. \"Effective Approaches to Attention-based Neural Machine Translation.\" EMNLP 2015.  https://arxiv.org/abs/1508.04025\r\n\r\nI would be happy to contribute. Do we want to add a new argument `attention_layer_activation=None` to the `AttentionWrapper` constructor?", "comments": ["Thanks for your interest in contributing. @ebrevdo, could you discuss with and advise @guillaumekln on creating this feature. ", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ebrevdo: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ebrevdo: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ebrevdo: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 14971, "title": "Fix Wrong Rendering of Code Example", "body": "Code example was being rendered as plain text. This commit fixes it.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!\n\nOn 29-Nov-2017 3:06 PM, \"googlebot\" <notifications@github.com> wrote:\n\n> Thanks for your pull request. It looks like this may be your first\n> contribution to a Google open source project. Before we can look at your\n> pull request, you'll need to sign a Contributor License Agreement (CLA).\n>\n> \ud83d\udcdd *Please visit https://cla.developers.google.com/\n> <https://cla.developers.google.com/> to sign.*\n>\n> Once you've signed, please reply here (e.g. I signed it!) and we'll\n> verify. Thanks.\n> ------------------------------\n>\n>    - If you've already signed a CLA, it's possible we don't have your\n>    GitHub username or you're using a different email address. Check your\n>    existing CLA data <https://cla.developers.google.com/clas> and verify\n>    that your email is set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>    - If your company signed a CLA, they designated a Point of Contact who\n>    decides which employees are authorized to participate. You may need to\n>    contact the Point of Contact for your company and ask to be added to the\n>    group of authorized contributors. If you don't know who your Point of\n>    Contact is, direct the project maintainer to go/cla#troubleshoot.\n>    - In order to pass this check, please resolve this problem and have\n>    the pull request author add another comment and the bot will run again.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/14971#issuecomment-347803521>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AJ6nhmfcXp0cIkSbu5tgy_7cDWoviJboks5s7SWbgaJpZM4QuqWH>\n> .\n>\n", "CLAs look good, thanks!\n\n<!-- ok -->", "The builds are failing. Do I need to change anything ??", "Seems like it couldn't start the build. Let's try one more time. \r\n\r\nSaving the link for this run for future reference https://source.cloud.google.com/results/invocation/93164fdd-a76a-45c8-a156-4b9c62e39096/log", "I don't think the builds are running.\r\n`ERROR: no such package '@bazel_toolchains//configs/debian8_clang/0.2.0/bazel_0.7.0': BUILD file not found on package path`"]}, {"number": 14970, "title": "AttributeError: module 'tensorflow' has no attribute 'session'", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 (64bit)\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**: 1.4\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nWhy in tensorflow 1.4 when running validating command in sess = tf.session() Line gives this error? is Session attribute changed in 1.4 version of tensorflow compare to previous versions Like 0.12?\r\n\r\n### Source code / logs\r\n>>> import tensorflow as tf\r\n>>> hello = tf.constant ('hello, tensorflow!')\r\n>>> sess = tf.session ()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: module 'tensorflow' has no attribute 'session'", "comments": ["`tf.Session` has an upper-case `S`. ", "So do I ...\r\nand my code is:\r\n>>> sess = tf.Session()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: module 'tensorflow' has no attribute 'Session'", "I'm still facing the problem despite using S in session.", "import tensorflow as tf\r\n#Session used in tensorflow 1.* \r\n#when you use tensorflow 2.* you can try the following codes\r\n\r\ng = tf.Graph()\r\nwith g.as_default():\r\n    a = tf.constant(3.14)\r\n    b = tf.constant(10)\r\n    c = tf.constant(\"Hello world!\")\r\n\r\nwith tf.compat.v1.Session(graph=g) as sess:\r\n    print(sess.run(a))\r\n    print(sess.run(b))\r\n    print(sess.run(c))\r\n\r\n"]}, {"number": 14969, "title": "remplace .npy weight file from a checkpoint", "body": "Hi,\r\nI went modify this line of code (loading weights from pretrained mode):\r\nweights_dict = np.load(self.WEIGHTS_PATH, encoding = 'bytes').item()\r\n\r\nwhere WEIGHTS_PATH is a an npy file WEIGHTS_PATH = 'bvlc_alexnet.npy'\r\n\r\nby my own weights saved in checkpoint.\r\n\r\nThanks a lot", "comments": []}, {"number": 14968, "title": "Question about tf.contrib.image.rotate", "body": "Good Morning :) -\r\nis it possible to parse the result from `tf.contrib.image.rotate` to a tensor to use it in `t.cast` ?\r\n\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 14967, "title": "Fix decode_bmp crash by adding length check before reading the data in buffer", "body": "This fix tries to address the issue raised in #14959 where the bmp content length was not checked before reading the buffer. As a result, decode_bmp might trigger a crash if the content of bmp is incomplete.\r\n\r\nThis fix fixes the issue by adding the needed check before reading the data.\r\n\r\nAdditional test cases have been added.\r\n\r\nThis fix fixes #14959.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 14966, "title": "why the size of filter is 3*3 in inceptionv1.py?", "body": "https://github.com/tensorflow/tensorflow/blob/b5df90f91cde6eb12af9cbe818bd2cf4a9bcc687/tensorflow/contrib/slim/python/slim/nets/inception_v1.py#L103\r\naccording to the paper,  the size of filter in the scope of \"InceptionV1/Mixed_3b/Branch_2\"  should be 5*5.\r\nwhy is 3*3 in the script?", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 14965, "title": "compile tensorflow with the source code-----ERROR", "body": "when i compile tensorflow with the source code,an ERROR appear. If you know how to solve this problem,please help me. Thank you.\r\n\r\nERROR: /home/hy003/tensorflow/tensorflow/python/BUILD:4508:1 C++ compilation of rule '//tensorflow/python:framework/fast_tensor_util.so' failed(Exit 1). bazel-out/local-opt/genfiles/tensorflow/python/framework/fast_tensor_util.cpp: In function 'PyObject* __pyx_f_5numpy_PyDataType_SHAPE(PyArray_Descr*)': bazel-out/local/genfiles/tensorflow/python/framework/fastz_tensor_util.cpp:5500:48: error 'PyDataType_HASSUBARRAY' was not declared in this scope  __pyx_t_1 = (PyDataType_HASSUBARRAY(__pyx_v_d != 0))  ", "comments": ["Which version of Python, which commit hash, and which configuration options? And which platform are you building on?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly."]}, {"number": 14964, "title": "Has anybody ever written a gradient for the operator 'SparseReduceSumSparse' ?", "body": "I could only find a gradient for `SparseReduceSum`.\r\n\r\n```\r\n@ops.RegisterGradient(\"SparseReduceSum\")\r\ndef _SparseReduceSumGrad(op, out_grad):\r\n  \"\"\"Similar to gradient for the Sum Op (i.e. tf.reduce_sum()).\"\"\"\r\n  sp_indices = op.inputs[0]\r\n  sp_shape = op.inputs[2]\r\n  output_shape_kept_dims = math_ops.reduced_shape(sp_shape, op.inputs[3])\r\n  out_grad_reshaped = array_ops.reshape(out_grad, output_shape_kept_dims)\r\n  scale = sp_shape // math_ops.to_int64(output_shape_kept_dims)\r\n  # (sparse_indices, sparse_values, sparse_shape, reduction_axes)\r\n  return (None, array_ops.gather_nd(out_grad_reshaped, sp_indices // scale),\r\n          None, None)\r\n```\r\n\r\nIt seems uneasy to write a similar gradient for operator `SparseReduceSumSparse`.\r\nCould anyone give me a hand? Thanks.", "comments": ["Hi, @chaihua483 . `SparseReduceSumSparse` behaves the same way with `SparseReduceSum`, so their gradient implementations are almost same. However, since `gather_nd` doesn't support `SparseTensor`, I cannot find an efficient way to gather values from its `out_grad[1]`. \r\n\r\n@aselle  The only one solution I can think of is to implement the gradient op in C++ side. Is it worthwhile to do that? \r\n\r\nAbove all, I suggest you to use `SparseReduceSum` if you need gradient calculation. Please correct me if I'm wrong.", "@facaiy That's right, it might be replaced by `SparseReduceSum ` in small datasets. However, I had to use `SparseReduceSumSparse` with a 3D sparse tensor in my case because it seems still too large if the result 2D matrix is a dense one. I did have implemented a C++ version(#15184 ) of the gradient of `SparseReduceSumSparse` and it works well in my code. On the other hand, there exists an operation with no gradient implemented which seems not very perfect. ", "Nice work! However I'm afraid that your new operator is quite like `tf.gather_nd`, right? Do you have an interest in #1950?", "@facaiy Well not exactly, `sparse_tile_like` is much easier rather than `tf.gather_nd` on `SparseTensor`.  Implementing all features included in `tf.gather_nd` seems much harder. \r\n\r\nI would be pleased to have a try when an idea comes to my mind. ", "cc @suharshs who I think might be interested in the issue. How about implementing the gradient operator in c++ side? ", "+1 for support this", "Closing as this has been resolved."]}, {"number": 14963, "title": "Documentation of tf.nn.raw_rnn is confusing", "body": "The documentation for the tf.nn.raw_rnn displays the following line as the pseudocode for the function:\r\n<br>\r\n`(finished, next_input, initial_state, _, loop_state) = loop_fn(\r\n    time=time, cell_output=None, cell_state=None, loop_state=None)`\r\n<br>\r\nThis is the first time the loop_fn is called by the raw_rnn interface. Here, the '_' for the emit_output returned by the loop_fn is a **_do not care_** for the function. This misleads the user into believing that returning `None` for the first time as the `emit_output` is OK.\r\n\r\nHowever, the actual code has the line:\r\n<br>\r\n`(elements_finished, next_input, initial_state, emit_structure,\r\n     init_loop_state) = loop_fn(\r\n         time, None, None, None)  # time, cell_output, cell_state, loop_state`\r\n<br>\r\nThe code uses the `emit_structure` from the `emit_output` that then determines the shape of the emitted output to be aggregated in the `emit_ta` array. \r\nSo, even for the first time the `loop_fn` needs to output a mock tensor for setting the shape of the output values.\r\n\r\nThe documentation needs to include this as going through the framework implementation for figuring this out is too tiresome. \r\n\r\nCheers!", "comments": ["Thanks for filing the clear issue @akanimax!  Would you like to submit a PR to improve the documentation?\r\n\r\nFYI @ebrevdo.", "Sure! It'd be a privilege.", "I see that the documentation and the the pseudo code has been updated here https://www.tensorflow.org/api_docs/python/tf/compat/v1/nn/raw_rnn.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 14962, "title": "Tensorflow Conv model crashes on GPU with zero size batch", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, I have created two CNN models\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: I installed tensorflow via pip install (Python 3)\r\n- **TensorFlow version (use command below)**: 1.4.0\r\n- **Python version**: 3\r\n- **Bazel version (if compiling from source)**: \r\n- **GCC/Compiler version (if compiling from source)**: \r\n- **CUDA/cuDNN version**: 8.0/6\r\n- **GPU model and memory**: NVIDIA Tesla k80 totalMemory: 11.17GiB freeMemory: 11.09GiB\r\n- **Exact command to reproduce**: See below:\r\n\r\n### Below is the log:\r\n\r\n> keep_dims is deprecated, use keepdims instead\r\n> _________________________________________________________________\r\n> Layer (type)                 Output Shape              Param #   \r\n> =================================================================\r\n> input_1 (InputLayer)         (None, 7, 264)            0         \r\n> _________________________________________________________________\r\n> reshape_1 (Reshape)          (None, 7, 264, 1)         0         \r\n> _________________________________________________________________\r\n> conv2d_1 (Conv2D)            (None, 7, 264, 64)        16960     \r\n> _________________________________________________________________\r\n> max_pooling2d_1 (MaxPooling2 (None, 7, 132, 64)        0         \r\n> _________________________________________________________________\r\n> flatten_1 (Flatten)          (None, 59136)             0         \r\n> _________________________________________________________________\r\n> dense_1 (Dense)              (None, 1024)              60556288  \r\n> _________________________________________________________________\r\n> dropout_1 (Dropout)          (None, 1024)              0         \r\n> _________________________________________________________________\r\n> dense_2 (Dense)              (None, 512)               524800    \r\n> _________________________________________________________________\r\n> dropout_2 (Dropout)          (None, 512)               0         \r\n> _________________________________________________________________\r\n> dense_3 (Dense)              (None, 88)                45144     \r\n> =================================================================\r\n> Total params: 61,143,192\r\n> Trainable params: 61,143,192\r\n> Non-trainable params: 0\r\n> _________________________________________________________________\r\n> /home/hpnhxxwn/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/engine/training.py:2057: UserWarning: Using a generator with `use_multiprocessing=True` and multiple worker\r\n> s may duplicate your data. Please consider using the`keras.utils.Sequence class.\r\n>   UserWarning('Using a generator with `use_multiprocessing=True`'\r\n> ld: learning rate is now 0.01\r\n> 2017-11-29 04:58:20.964015: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX A\r\n> VX2 FMA\r\n> 2017-11-29 04:58:21.094051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:900] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUM\r\n> A node, so returning NUMA node zero\r\n> 2017-11-29 04:58:21.094719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1062] Found device 0 with properties: \r\n> name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\n> pciBusID: 0000:00:04.0\r\n> totalMemory: 11.17GiB freeMemory: 11.09GiB\r\n> 2017-11-29 04:58:21.094744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1152] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0\r\n> , compute capability: 3.7)\r\n> Epoch 1/1000\r\n> 5180/6944 [=====================>........] - ETA: 3:43 - loss: 0.1382 - acc: 0.9637 - mean_absolute_error: 0.0715 - sparse_categorical_accuracy: 7.1640e-05switching to  ['AkPnStgb']\r\n> 5212/6944 [=====================>........] - ETA: 3:39 - loss: 0.1384 - acc: 0.9636 - mean_absolute_error: 0.0715 - sparse_categorical_accuracy: 7.1200e-052017-11-29 05:09:22.210897: F\r\n>  tensorflow/stream_executor/cuda/cuda_dnn.cc:444] could not convert BatchDescriptor {count: 0 feature_map_count: 64 spatial: 7 264  value_min: 0.000000 value_max: 0.000000 layout: Batc\r\n> hDepthYX} to cudnn tensor descriptor: CUDNN_STATUS_BAD_PARAM\r\n\r\n\r\n\r\n\r\n### Below is the model. \r\n```\r\ndef baseline_model():\r\n    inputs = Input(shape=input_shape)\r\n    reshape = Reshape(input_shape_channels)(inputs)\r\n    #normal convnet layer (have to do one initially to get 64 channels)\r\n    conv1 = Conv2D(50,(5,25),activation='tanh')(reshape)\r\n    do1 = Dropout(0.5)(conv1)\r\n    pool1 = MaxPooling2D(pool_size=(1,3))(do1)\r\n    conv2 = Conv2D(50,(3,5),activation='tanh')(pool1)\r\n    do2 = Dropout(0.5)(conv2)\r\n    pool2 = MaxPooling2D(pool_size=(1,3))(do2)\r\n    flattened = Flatten()(pool2)\r\n    fc1 = Dense(1000, activation='sigmoid')(flattened)\r\n    do3 = Dropout(0.5)(fc1)\r\n    fc2 = Dense(200, activation='sigmoid')(do3)\r\n    do4 = Dropout(0.5)(fc2)\r\n    outputs = Dense(note_range, activation='sigmoid')(do4)\r\n    model = Model(inputs=inputs, outputs=outputs)\r\n    return model\r\n````\r\n\r\nI have found other github issue created for the same issue, not so far seems like there is no fix yet. I heard the workaround is to use tf.cond, can someone show me how to use it in such case. ", "comments": ["Duplicate of #14657", "Closing as duplicate (for the feature request aspect). For help using tf.cond refer to the documentation or post a question on StackOverflow."]}, {"number": 14961, "title": "Clarifying checkpoint is not a file", "body": "I would like to clarify checkpoint is not a file. I also changed some wording which may lead readers to use a physical file name in their code.", "comments": ["Can one of the admins verify this patch?"]}, {"number": 14960, "title": "Method of stabilizing prediction box", "body": "I found that when I used other methods to detect objects, the prediction box was not stable. However, in the demo you provided, I found that the detection box is very stable. Why, can you give me the code to stabilize the detection box?", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 14959, "title": "[BUG]Out-of-Bounds Read in DecodeBmpOp class(tensorflow/core/kernels/decode_bmp_op.cc)", "body": "------------------------\r\n\r\n### System information\r\n- **The following is output of tf_env_collect.sh**:\r\n== cat /etc/issue ===============================================\r\nLinux ubuntu 4.4.0-31-generic #50~14.04.1-Ubuntu SMP Wed Jul 13 01:07:32 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"14.04.5 LTS, Trusty Tahr\"\r\nVERSION_ID=\"14.04\"\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4\r\nCopyright (C) 2013 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux ubuntu 4.4.0-31-generic #50~14.04.1-Ubuntu SMP Wed Jul 13 01:07:32 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.3)\r\nprotobuf (3.4.0)\r\ntensorflow (1.4.0)\r\ntensorflow-tensorboard (0.4.0rc2)\r\n\r\n== check for virtualenv =========================================\r\nTrue\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.4.0\r\ntf.GIT_VERSION = v1.4.0-rc1-11-g130a514\r\ntf.COMPILER_VERSION = v1.4.0-rc1-11-g130a514\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\ntensorflow/tools/tf_env_collect.sh: line 105: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n                                                              \r\n\r\n### Describe the problem\r\nThe DecodeBmpOp class is the decoder of bmp file. The class is in \r\n tensorflow/core/kernels/decode_bmp_op.cc. When dealling with a bmp file, the decoder doesn't invalidate the meta info of bmp file, such as header_size, width, height. It causes a Out-of-Bound Read in DecodeBmpOp::Decode func or DecodeBmpOp ::Compute func. If given an evil bmp file, the program using this API will crash.\r\n\r\n### Source code / logs\r\n\r\n- **Here is the crash call stack of program**:\r\nThe bmp file  in the attachment causes a crash.\r\n\r\nProgram received signal SIGSEGV, Segmentation fault.\r\n0x0000000004200bc0 in tensorflow::DecodeBmpOp::Decode (this=this@entry=0x602400032a40, input=input@entry=0x601c000214ce \"33\", output=0x7fffcefcf800 \"\", width=width@entry=2336, height=height@entry=61727, channels=channels@entry=3, top_down=top_down@entry=false) at tensorflow/core/kernels/decode_bmp_op.cc:122\r\nProgram received signal SIGSEGV (fault address 0x601c19caaa10)\r\npwndbg> bt\r\n#0  0x0000000004200bc0 in tensorflow::DecodeBmpOp::Decode (this=this@entry=0x602400032a40, input=input@entry=0x601c000214ce \"33\", output=0x7fffcefcf800 \"\", width=width@entry=2336, height=height@entry=61727, channels=channels@entry=3, top_down=top_down@entry=false) at tensorflow/core/kernels/decode_bmp_op.cc:122\r\n#1  0x0000000004202d3b in tensorflow::DecodeBmpOp::Compute (this=0x602400032a40, context=<optimized out>) at tensorflow/core/kernels/decode_bmp_op.cc:88\r\n#2  0x00007ffff3ed8880 in tensorflow::ThreadPoolDevice::Compute (this=<optimized out>, op_kernel=0x602400032a40, context=0x7fffffff8320) at tensorflow/core/common_runtime/threadpool_device.cc:59\r\n#3  0x00007ffff3d47110 in tensorflow::(anonymous namespace)::ExecutorState::Process (this=<optimized out>, tagged_node=..., scheduled_usec=<optimized out>) at tensorflow/core/common_runtime/executor.cc:1652\r\n#4  0x00007ffff3d4cc0c in operator() (__closure=<optimized out>) at tensorflow/core/common_runtime/executor.cc:2055\r\n#5  std::_Function_handler<void(), tensorflow::(anonymous namespace)::ExecutorState::ScheduleReady(const TaggedNodeSeq&, tensorflow::(anonymous namespace)::ExecutorState::TaggedNodeReadyQueue*)::__lambda3>::_M_invoke(const std::_Any_data &) (__functor=...) at /usr/include/c++/4.8/functional:2071\r\n#6  0x00007ffff3db2351 in operator() (this=0x7fffffff8790) at /usr/include/c++/4.8/functional:2471\r\n#7  operator() (__closure=<optimized out>, c=...) at tensorflow/core/common_runtime/graph_runner.cc:146\r\n#8  std::_Function_handler<void(std::function<void()>), tensorflow::GraphRunner::Run(tensorflow::Graph*, tensorflow::FunctionLibraryRuntime*, const NamedTensorList&, const std::vector<std::basic_string<char> >&, std::vector<tensorflow::Tensor>*)::__lambda2>::_M_invoke(const std::_Any_data &, std::function<void()>) (__functor=..., __args#0=...) at /usr/include/c++/4.8/functional:2071\r\n#9  0x00007ffff3ce0258 in operator() (__args#0=..., this=<optimized out>) at /usr/include/c++/4.8/functional:2471\r\n#10 tensorflow::(anonymous namespace)::ExecutorState::ScheduleReady (this=0x6026000fd1a0, ready=..., inline_ready=0x0) at tensorflow/core/common_runtime/executor.cc:2055\r\n#11 0x00007ffff3d00a0c in ScheduleReady (inline_ready=0x0, ready=..., this=<optimized out>) at tensorflow/core/common_runtime/executor.cc:2046\r\n#12 RunAsync (done=<error reading variable: access outside bounds of object referenced via synthetic pointer>, this=<optimized out>) at tensorflow/core/common_runtime/executor.cc:1439\r\n#13 tensorflow::(anonymous namespace)::ExecutorImpl::RunAsync (this=this@entry=0x602400032f40, args=..., done=...) at tensorflow/core/common_runtime/executor.cc:2564\r\n#14 0x00007ffff3db999f in Run (args=..., this=0x602400032f40) at ./tensorflow/core/common_runtime/executor.h:117\r\n#15 tensorflow::GraphRunner::Run (this=this@entry=0x6004002d08f0, graph=graph@entry=0x603a00003140, function_library=function_library@entry=0x602400033800, inputs=std::vector of length 0, capacity 0, output_names=std::vector of length 1, capacity 1 = {...}, outputs=outputs@entry=0x7fffffffa810) at tensorflow/core/common_runtime/graph_runner.cc:174\r\n#16 0x00007ffff3c4f36d in tensorflow::ConstantFold (opts=..., function_library=function_library@entry=0x602400033800, env=env@entry=0x60060000e140, partition_device=partition_device@entry=0x6024000395c0, graph=graph@entry=0x603a00003480, was_mutated=was_mutated@entry=0x7fffffffb260) at tensorflow/core/common_runtime/constant_folding.cc:603\r\n#17 0x00007ffff3db02bd in tensorflow::GraphOptimizer::Optimize (this=this@entry=0x7fffffffbe90, runtime=runtime@entry=0x602400033800, env=0x60060000e140, device=0x6024000395c0, graph=graph@entry=0x60060038d2f0, shape_map=shape_map@entry=0x0) at tensorflow/core/common_runtime/graph_optimizer.cc:66\r\n#18 0x000000000ad04984 in tensorflow::DirectSession::GetOrCreateExecutors (this=this@entry=0x604000007080, inputs=..., outputs=..., target_nodes=..., executors_and_keys=executors_and_keys@entry=0x7fffffffc4f0, run_state_args=run_state_args@entry=0x7fffffffc730) at tensorflow/core/common_runtime/direct_session.cc:1208\r\n#19 0x000000000ad0d0f7 in tensorflow::DirectSession::Run (this=<optimized out>, run_options=..., inputs=std::vector of length 0, capacity 0, output_names=std::vector of length 1, capacity 1 = {...}, target_nodes=std::vector of length 0, capacity 0, outputs=0x0, run_metadata=0x0) at tensorflow/core/common_runtime/direct_session.cc:472\r\n#20 0x000000000ad6fa95 in tensorflow::ClientSession::Run (this=this@entry=0x7fffffffdc30, run_options=..., inputs=std::unordered_map with 0 elements, fetch_outputs=std::vector of length 1, capacity 1 = {...}, run_outputs=std::vector of length 0, capacity 0, outputs=outputs@entry=0x0, run_metadata=run_metadata@entry=0x0) at tensorflow/cc/client/client_session.cc:127\r\n#21 0x000000000ad74b6d in Run (outputs=0x0, run_outputs=std::vector of length 0, capacity 0, fetch_outputs=std::vector of length 1, capacity 1 = {...}, inputs=std::unordered_map with 0 elements, this=0x7fffffffdc30) at tensorflow/cc/client/client_session.cc:90\r\n#22 tensorflow::ClientSession::Run (this=this@entry=0x7fffffffdc30, fetch_outputs=std::vector of length 1, capacity 1 = {...}, outputs=outputs@entry=0x0) at tensorflow/cc/client/client_session.cc:76\r\n#23 0x000000000048042a in main (argc=1, argc@entry=2, argv=argv@entry=0x7fffffffe2e8) at tensorflow/examples/decode_image/main.cc:99\r\n#24 0x00007ffff03c1f45 in __libc_start_main (main=0x47e4b0 <main(int, char**)>, argc=2, argv=0x7fffffffe2e8, init=<optimized out>, fini=<optimized out>, rtld_fini=<optimized out>, stack_end=0x7fffffffe2d8) at libc-start.c:287\r\n#25 0x0000000000737eca in _start ()\r\n\r\n- **source code of c++ program**:\r\n#include <fstream>\r\n#include <utility>\r\n#include <vector>\r\n\r\n#include \"tensorflow/cc/ops/const_op.h\"\r\n#include \"tensorflow/cc/ops/image_ops.h\"\r\n#include \"tensorflow/cc/ops/standard_ops.h\"\r\n#include \"tensorflow/core/framework/graph.pb.h\"\r\n#include \"tensorflow/core/framework/tensor.h\"\r\n#include \"tensorflow/core/graph/default_device.h\"\r\n#include \"tensorflow/core/graph/graph_def_builder.h\"\r\n#include \"tensorflow/core/lib/core/errors.h\"\r\n#include \"tensorflow/core/lib/core/stringpiece.h\"\r\n#include \"tensorflow/core/lib/core/threadpool.h\"\r\n#include \"tensorflow/core/lib/io/path.h\"\r\n#include \"tensorflow/core/lib/strings/stringprintf.h\"\r\n#include \"tensorflow/core/platform/env.h\"\r\n#include \"tensorflow/core/platform/init_main.h\"\r\n#include \"tensorflow/core/platform/logging.h\"\r\n#include \"tensorflow/core/platform/types.h\"\r\n#include \"tensorflow/core/public/session.h\"\r\n#include \"tensorflow/core/util/command_line_flags.h\"\r\n#include \"tensorflow/cc/client/client_session.h\"\r\n\r\n// These are all common classes it's handy to reference with no namespace.\r\nusing tensorflow::Flag;\r\nusing tensorflow::Tensor;\r\nusing tensorflow::Status;\r\nusing tensorflow::string;\r\nusing tensorflow::int32;\r\n\r\nint main(int argc, char* argv[]) {\r\n  string image = \"tensorflow/examples/label_image/data/grace_hopper.jpg\";\r\n  std::vector<Flag> flag_list = {\r\n      Flag(\"image\", &image, \"image to be processed\"),\r\n  };\r\n  string usage = tensorflow::Flags::Usage(argv[0], flag_list);\r\n  const bool parse_result = tensorflow::Flags::Parse(&argc, argv, flag_list);\r\n  if (!parse_result) {\r\n    LOG(ERROR) << usage;\r\n    return -1;\r\n  }\r\n\r\n  // We need to call this to set up global state for TensorFlow.\r\n  tensorflow::port::InitMain(argv[0], &argc, &argv);\r\n  if (argc > 1) {\r\n    LOG(ERROR) << \"Unknown argument \" << argv[1] << \"\\n\" << usage;\r\n    return -1;\r\n  }\r\n\r\n        using namespace ::tensorflow::ops;\r\n\r\n  auto root = tensorflow::Scope::NewRootScope();\r\n  tensorflow::Output file_reader = ReadFile(root.WithOpName(\"input_image\"), image);\r\n        tensorflow::Output gif_reader = DecodeGif(root.WithOpName(\"gif_reader\"), file_reader);\r\n        tensorflow::Output bmp_reader = DecodeBmp(root.WithOpName(\"bmp_reader\"), file_reader);\r\n        tensorflow::Output jpeg_reader = DecodeJpeg(root.WithOpName(\"jpeg_reader\"), file_reader);\r\n        tensorflow::Output png_reader = DecodePng(root.WithOpName(\"png_reader\"), file_reader);\r\n\r\n        std::vector<tensorflow::Tensor> outputs;\r\n        tensorflow::ClientSession session(root);\r\n  session.Run({gif_reader}, nullptr);\r\n  session.Run({bmp_reader}, nullptr);\r\n  session.Run({jpeg_reader}, nullptr);\r\n  session.Run({png_reader}, nullptr);\r\n\r\n  return 0;\r\n}\r\n\r\n- **source code of python program**:\r\nimport argparse\r\nimport tensorflow as tf\r\n\r\nif __name__ == \"__main__\":\r\n        file_name = \"tensorflow/examples/label_image/data/grace_hopper.jpg\"\r\n        parser = argparse.ArgumentParser()\r\n        parser.add_argument(\"--image\", help=\"image to be processed\")\r\n        args = parser.parse_args()\r\n        if args.image:\r\n            file_name = args.image\r\n        file_reader = tf.read_file(file_name, \"file_reader\")\r\n        image_reader = tf.image.decode_bmp(file_reader, name='bmp_reader')\r\n        sess = tf.Session()\r\n        sess.run(image_reader)\r\n\r\n[evil.zip](https://github.com/tensorflow/tensorflow/files/1512398/evil.zip)\r\n\r\n", "comments": ["Added a PR #14967 to fix the crash.", "Thanks @yongtang!  I'll close this out once your PR gets merged.", "Duplicate of #"]}, {"number": 14958, "title": "Make tf_upgrade.py dependency free", "body": "Nothing else references the ast_edits, so it will make tf_upgrade.py much\r\neasier to use if it's just absorbed. This change fixes #11217 where a whole\r\nbunch of folks encountered difficulties for this very reason.", "comments": ["@tensorflow-jenkins test this please", "@jart, please take a look at this test failure:\r\nhttps://ci.tensorflow.org/job/tensorflow-pull-requests-gpu/7887/consoleFull\r\n\r\n> ERROR: missing input file '//tensorflow/tools/compatibility:ast_edits.py'.\r\n> ERROR: /workspace/tensorflow/tools/compatibility/BUILD:20:1: //tensorflow/tools/compatibility:tf_upgrade_test: missing input file '//tensorflow/tools/compatibility:ast_edits.py'.\r\n> INFO: Building complete.\r\n> ERROR: /workspace/tensorflow/tools/compatibility/BUILD:20:1 1 input file(s) do not exist.\r\n\r\nIt looks like a BUILD rule needs to be updated.", "@jart is this still current?", "Yes. Build issue should be addressed. If tests pass please merge.", "Jenkins, test this please.", "Looks like there's some error here\r\n\r\n```\r\n==================== Test output for //tensorflow/tools/compatibility:tf_upgrade_test:\r\nTraceback (most recent call last):\r\n  File \"/tmp/botexec/bazel-out/k8-py3-opt/bin/tensorflow/tools/compatibility/tf_upgrade_test.runfiles/org_tensorflow/tensorflow/tools/compatibility/tf_upgrade_test.py\", line 25, in <module>\r\n    from tensorflow.tools.compatibility import ast_edits\r\nImportError: cannot import name 'ast_edits'\r\n```", "Apologies for that one. I made sure to run that test locally before pushing again.", "Thanks!"]}, {"number": 14957, "title": "tf.profiler reports 0B memory usage", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.5.0-dev20171103\r\n- **Python version**: 3.5\r\n- **Exact command to reproduce**:\r\n\r\n``` python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ngraph = tf.Graph()\r\nwith graph.as_default(), tf.device(\"/cpu:0\"):\r\n    a = tf.constant(np.ones((1000, 1000)))\r\n    b = tf.constant(np.ones((1000, 1000)))\r\n    c = a * b\r\n\r\nwith tf.Session(graph=graph) as sess:\r\n    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\n    run_metadata = tf.RunMetadata()\r\n    sess.run(c, options=run_options, run_metadata=run_metadata)\r\n\r\n    options = tf.profiler.ProfileOptionBuilder.time_and_memory()\r\n    options[\"min_bytes\"] = 0\r\n    options[\"select\"] = (\"bytes\", \"peak_bytes\", \"output_bytes\",\r\n                         \"residual_bytes\")\r\n    tf.profiler.profile(graph, run_meta=run_metadata, cmd=\"scope\",\r\n                        options=options)\r\n```\r\n\r\n### Describe the problem\r\nThe above script gives output\r\n```\r\n==================Model Analysis Report======================\r\nnode name | requested bytes | peak bytes | residual bytes | output bytes\r\n_TFProfRoot (--/0B, --/0B, --/0B, --/8.00MB)\r\n  mul (0B/0B, 0B/0B, 0B/0B, 8.00MB/8.00MB)\r\n  _retval_mul_0_0 (0B/0B, 0B/0B, 0B/0B, 0B/0B)\r\n```\r\n`requested bytes` and `peak bytes` are both reported as 0, whereas I would have thought they would be 8MB (based on the description of these measures [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/options.md)).  \r\n\r\nI think the memory is not being recorded in the `RunMetadata` the way the profiler expects.  For example, if we look at \r\n``` python\r\n    mul_stats = run_metadata.step_stats.dev_stats[0].node_stats[1]\r\n\r\n    print(\"total_bytes\", [x.total_bytes for x in mul_stats.memory])   # --> [0]\r\n    print(\"persistent_mem\", mul_stats.memory_stats.host_persistent_memory_size)  # --> 8000000\r\n```\r\n`\"total_bytes\"` is what the profiler reports as \"requested bytes\" above.  It seems like that data isn't being updated in the `run_metadata`.  The correct value is in `memory_stats.host_persistent_memory_size`, but that value isn't available in the profiler output.  And according to the [profiler proto](https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/core/profiler/tfprof_log.proto) that value is supposed to represent the bytes allocated to persistent objects (like Variables), even though none of the values in the example are persistent.  So I'm not sure if this is an issue with `tf.profiler`, or with how memory information is stored in `RunMetadata`.", "comments": ["@panyx0718 might have some ideas here.", "Thanks for reporting it and doing the analysis.\r\n\r\nThe constant op memory is recorded in \"memory_stats\" but not in \"memory\". And profiler\r\nmissed that. I sent out a fix for it.\r\n\r\n Some clean up is needed for the TensorFlow internal memory tracing. Here are some thing I noted down during my code digging:\r\n  // 1. OpKernelConstruction::allocate_xxx is not traced. Below, we only\r\n  //    discuss OpKernelContext-related allocations.\r\n  // 2. allocate_output calls allocate_tensor, which is properly tracked in\r\n  //    'NodeExecStats.memory'.\r\n  // 3. allocate_temp is only tracked through record_xxx_temp. It appears\r\n  //    in 'NodeExecStats.memory_stats'.\r\n  // 4. allocate_persistent calls allocate_tensor, which is properly tracked\r\n  //    in 'NodeExecStats.memory'. However, there is no way to count it as\r\n  //    persistent now.\r\n  // 5. record_xxx_persistent is called when allocate_persistent\r\n  //    is not used. Hence tracks some complementary bytes. It appears in\r\n  //    'NodeExecStats.memory_stats'. It's suspicious. But we should\r\n  //    use it now since it covers constant op.", "I'm not sure if this is covered under your cases above, but I found another situation where 0B memory usage is reported that seems to have a different cause:\r\n``` python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ngraph = tf.Graph()\r\nwith graph.as_default(), tf.device(\"/cpu:0\"):\r\n    a = tf.placeholder(shape=(1000, 1000), dtype=tf.float64)\r\n\r\n    b = tf.placeholder(shape=(1,), dtype=tf.float64)\r\n    d = tf.scatter_nd([[0, 0]], b, (1000, 1000))\r\n\r\n    c = a * d\r\n\r\nwith tf.Session(graph=graph) as sess:\r\n    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\n    run_metadata = tf.RunMetadata()\r\n    sess.run(c, feed_dict={a: np.ones((1000, 1000)), b: np.ones((1,))},\r\n             options=run_options, run_metadata=run_metadata)\r\n\r\n    options = tf.profiler.ProfileOptionBuilder.time_and_memory()\r\n    options[\"min_bytes\"] = 0\r\n    options[\"min_micros\"] = 0\r\n    options[\"select\"] = (\"bytes\", \"peak_bytes\", \"output_bytes\",\r\n                         \"residual_bytes\")\r\n    tf.profiler.profile(graph, run_meta=run_metadata, cmd=\"scope\",\r\n                        options=options)\r\n```\r\ngives output\r\n```\r\n==================Model Analysis Report======================\r\nnode name | requested bytes | peak bytes | residual bytes | output bytes\r\n_TFProfRoot (--/8.00MB, --/8.00MB, --/8.00MB, --/24.00MB)\r\n  ScatterNd (8.00MB/8.00MB, 8.00MB/8.00MB, 8.00MB/8.00MB, 8.00MB/8.00MB)\r\n    ScatterNd/indices (0B/0B, 0B/0B, 0B/0B, 8B/8B)\r\n    ScatterNd/shape (0B/0B, 0B/0B, 0B/0B, 8B/8B)\r\n  mul (0B/0B, 0B/0B, 0B/0B, 8.00MB/8.00MB)\r\n  _arg_Placeholder_1_0_1 (0B/0B, 0B/0B, 0B/0B, 8B/8B)\r\n  _arg_Placeholder_0_0 (0B/0B, 0B/0B, 0B/0B, 8.00MB/8.00MB)\r\n  _retval_mul_0_0 (0B/0B, 0B/0B, 0B/0B, 0B/0B)\r\n  Placeholder (0B/0B, 0B/0B, 0B/0B, 0B/0B)\r\n  Placeholder_1 (0B/0B, 0B/0B, 0B/0B, 0B/0B)\r\n```\r\nAgain I'd expect `mul` to report 8MB memory usage, but it reports 0B.  But if we look at the node stats this time\r\n```\r\n    node_stats {\r\n      node_name: \"mul\"\r\n      all_start_micros: 1512146558031874\r\n      op_start_rel_micros: 2\r\n      op_end_rel_micros: 832\r\n      all_end_rel_micros: 843\r\n      memory {\r\n        allocator_name: \"cpu\"\r\n      }\r\n      output {\r\n        tensor_description {\r\n          dtype: DT_DOUBLE\r\n          shape {\r\n            dim {\r\n              size: 1000\r\n            }\r\n            dim {\r\n              size: 1000\r\n            }\r\n          }\r\n          allocation_description {\r\n            requested_bytes: 8000000\r\n            allocated_bytes: 8000000\r\n            allocator_name: \"cpu\"\r\n            allocation_id: 1\r\n            ptr: 2245408854112\r\n          }\r\n        }\r\n      }\r\n      timeline_label: \"mul = Mul(_arg_Placeholder_0_0, ScatterNd)\"\r\n      scheduled_micros: 1512146558031856\r\n      memory_stats {\r\n      }\r\n    }\r\n```\r\nwe can see that the memory usage isn't reported in `memory` or `memory_stats`.\r\n\r\nOn a different note, the Placeholders also report 0B memory usage, but I'm not sure if that is expected or not (not sure how memory allocation is handled for placeholders).", "For the first issue you reported, I believe it's covered, since I reproduced your case in a unittest.\r\n\r\nFor the second issue about placeholder, I'm not sure yet. As I documented above, TensorFlow doesn't trace all allocations yet, for example, in your case, since the tensor is from a np, I suspect the tensor is created from proto directly, in stead of going through OpContext.allocate_xxx.\r\nI'll look into it later. Since most allocation is dynamically allocated through OpContext.allocate_xxx, the current trace should have high coverage. While the rest of the allocation, it might not be easily traced due to the way they are implemented now.\r\n\r\n", "One more case; I'm not sure if the profiler takes into account allocations that happen within a loop.  E.g., this code\r\n\r\n``` python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nsteps = 1000\r\ntranspose = True\r\n\r\ngraph = tf.Graph()\r\nwith graph.as_default(), tf.device(\"/cpu:0\"):\r\n    p = tf.TensorArray(tf.float64, size=steps)\r\n    a = tf.placeholder(shape=(1000, 1000), dtype=tf.float64)\r\n    n = tf.placeholder(shape=(), dtype=tf.int32)\r\n\r\n    def loop_body(x, y, z):\r\n        if transpose:\r\n            y = tf.transpose(y)\r\n\r\n        z = z.write(x, y[0])\r\n\r\n        return x + 1, y, z\r\n\r\n    _, _, c = tf.while_loop(\r\n        lambda x, y, z: x < n, loop_body,\r\n        [tf.constant(0), a, p])\r\n\r\n    c = c.stack()\r\n\r\nwith tf.Session(graph=graph) as sess:\r\n    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\n    run_metadata = tf.RunMetadata()\r\n    feed = {n: steps, a: np.zeros((1000, 1000))}\r\n\r\n\r\n    def sess_run():\r\n        sess.run(c, feed_dict=feed,\r\n                 options=run_options, run_metadata=run_metadata\r\n                 )\r\n\r\n\r\n    sess_run()\r\n\r\n    options = tf.profiler.ProfileOptionBuilder.time_and_memory()\r\n    options[\"min_bytes\"] = 0\r\n    options[\"min_micros\"] = 0\r\n    options[\"select\"] = (\"bytes\", \"peak_bytes\", \"output_bytes\",\r\n                         \"residual_bytes\")\r\n    tf.profiler.profile(graph, run_meta=run_metadata, cmd=\"scope\",\r\n                        options=options)\r\n```\r\nshows memory usage that increases linearly with the number of loop `steps`\r\n\r\n![mem_usage](https://user-images.githubusercontent.com/1952220/33625352-46a52048-d9c5-11e7-9f32-e046527f3398.png)\r\n\r\nBut if we look at the profiler output, the reported memory usage of the `while` loop is constant regardless of the value of `steps` (and much less than the true memory usage):\r\n```\r\n==================Model Analysis Report======================\r\nnode name | requested bytes | peak bytes | residual bytes | output bytes\r\n_TFProfRoot (--/24.00MB, --/24.00MB, --/24.00MB, --/72.00MB)\r\n  while (0B/16.00MB, 0B/16.00MB, 0B/16.00MB, 0B/56.00MB)\r\n    while/transpose_1 (8.00MB/8.00MB, 8.00MB/8.00MB, 8.00MB/8.00MB, 8.00MB/8.00MB)\r\n      while/transpose_1/sub (0B/0B, 0B/0B, 0B/0B, 4B/4B)\r\n        while/transpose_1/sub/y (0B/0B, 0B/0B, 0B/0B, 0B/0B)\r\n      while/transpose_1/Rank (4B/4B, 4B/4B, 4B/4B, 4B/4B)\r\n      while/transpose_1/Range (8B/8B, 8B/8B, 8B/8B, 8B/8B)\r\n        while/transpose_1/Range/delta (0B/0B, 0B/0B, 0B/0B, 0B/0B)\r\n        while/transpose_1/Range/start (0B/0B, 0B/0B, 0B/0B, 0B/0B)\r\n      while/transpose_1/sub_1 (0B/0B, 0B/0B, 0B/0B, 8B/8B)\r\n    while/transpose (8.00MB/8.00MB, 8.00MB/8.00MB, 8.00MB/8.00MB, 8.00MB/8.00MB)\r\n      while/transpose/sub (4B/4B, 4B/4B, 4B/4B, 4B/8B)\r\n        while/transpose/sub/y (0B/0B, 0B/0B, 0B/0B, 4B/4B)\r\n      while/transpose/Range (8B/8B, 8B/8B, 8B/8B, 8B/12B)\r\n        while/transpose/Range/start (0B/0B, 0B/0B, 0B/0B, 4B/4B)\r\n        while/transpose/Range/delta (0B/0B, 0B/0B, 0B/0B, 0B/0B)\r\n      while/transpose/sub_1 (0B/0B, 0B/0B, 0B/0B, 8B/8B)\r\n      while/transpose/Rank (4B/4B, 4B/4B, 4B/4B, 4B/4B)\r\n    while/strided_slice (0B/0B, 0B/0B, 0B/0B, 8.00MB/8.00MB)\r\n      while/strided_slice/stack (0B/0B, 0B/0B, 0B/0B, 4B/4B)\r\n      while/strided_slice/stack_1 (0B/0B, 0B/0B, 0B/0B, 4B/4B)\r\n      while/strided_slice/stack_2 (0B/0B, 0B/0B, 0B/0B, 0B/0B)\r\n    while/TensorArrayWrite (0B/0B, 0B/0B, 0B/0B, 0B/140B)\r\n      while/TensorArrayWrite/TensorArrayWriteV3 (0B/0B, 0B/0B, 0B/0B, 4B/140B)\r\n        while/TensorArrayWrite/TensorArrayWriteV3/Enter (0B/0B, 0B/0B, 0B/0B, 136B/136B)\r\n    while/Less (1B/1B, 1B/1B, 1B/1B, 1B/5B)\r\n      while/Less/Enter (0B/0B, 0B/0B, 0B/0B, 4B/4B)\r\n    while/add (4B/4B, 4B/4B, 4B/4B, 4B/4B)\r\n      while/add/y (0B/0B, 0B/0B, 0B/0B, 0B/0B)\r\n    while/Merge_1 (4B/4B, 4B/4B, 4B/4B, 8.00MB/8.00MB)\r\n    while/NextIteration_1 (0B/0B, 0B/0B, 0B/0B, 8.00MB/8.00MB)\r\n    while/NextIteration (0B/0B, 0B/0B, 0B/0B, 4B/4B)\r\n    while/Merge_2 (4B/4B, 4B/4B, 4B/4B, 8B/8B)\r\n    while/Merge (4B/4B, 4B/4B, 4B/4B, 8B/8B)\r\n    while/LoopCond (0B/0B, 0B/0B, 0B/0B, 1B/1B)\r\n    while/Switch_2 (0B/0B, 0B/0B, 0B/0B, 4B/4B)\r\n    while/Switch (0B/0B, 0B/0B, 0B/0B, 4B/4B)\r\n    while/NextIteration_2 (0B/0B, 0B/0B, 0B/0B, 4B/4B)\r\n    while/Identity (0B/0B, 0B/0B, 0B/0B, 4B/4B)\r\n    while/Switch_1 (0B/0B, 0B/0B, 0B/0B, 8.00MB/8.00MB)\r\n    while/Enter_1 (0B/0B, 0B/0B, 0B/0B, 8.00MB/8.00MB)\r\n    while/Enter (0B/0B, 0B/0B, 0B/0B, 4B/4B)\r\n    while/Enter_2 (0B/0B, 0B/0B, 0B/0B, 4B/4B)\r\n    while/Exit_2 (0B/0B, 0B/0B, 0B/0B, 4B/4B)\r\n    while/Exit (0B/0B, 0B/0B, 0B/0B, 0B/0B)\r\n    while/Exit_1 (0B/0B, 0B/0B, 0B/0B, 0B/0B)\r\n    while/Identity_1 (0B/0B, 0B/0B, 0B/0B, 0B/0B)\r\n    while/Identity_2 (0B/0B, 0B/0B, 0B/0B, 0B/0B)\r\n  TensorArrayStack (0B/8.00MB, 0B/8.00MB, 0B/8.00MB, 0B/8.00MB)\r\n    TensorArrayStack/TensorArrayGatherV3 (8.00MB/8.00MB, 8.00MB/8.00MB, 8.00MB/8.00MB, 8.00MB/8.00MB)\r\n    TensorArrayStack/range (4.00KB/4.00KB, 4.00KB/4.00KB, 4.00KB/4.00KB, 4.00KB/4.01KB)\r\n      TensorArrayStack/range/start (0B/0B, 0B/0B, 0B/0B, 4B/4B)\r\n      TensorArrayStack/range/delta (0B/0B, 0B/0B, 0B/0B, 4B/4B)\r\n    TensorArrayStack/TensorArraySizeV3 (4B/4B, 4B/4B, 4B/4B, 4B/4B)\r\n  TensorArray (204B/204B, 204B/204B, 204B/204B, 140B/144B)\r\n    TensorArray/size (0B/0B, 0B/0B, 0B/0B, 4B/4B)\r\n  _arg_Placeholder_0_0 (0B/0B, 0B/0B, 0B/0B, 8.00MB/8.00MB)\r\n  _arg_Placeholder_1_0_1 (0B/0B, 0B/0B, 0B/0B, 4B/4B)\r\n  _retval_TensorArrayStack (0B/0B, 0B/0B, 0B/0B, 0B/0B)\r\n    _retval_TensorArrayStack/TensorArrayGatherV3_0_0 (0B/0B, 0B/0B, 0B/0B, 0B/0B)\r\n  Const (0B/0B, 0B/0B, 0B/0B, 4B/4B)\r\n  Placeholder (0B/0B, 0B/0B, 0B/0B, 0B/0B)\r\n  Placeholder_1 (0B/0B, 0B/0B, 0B/0B, 0B/0B)\r\n  init (0B/0B, 0B/0B, 0B/0B, 0B/0B)\r\n```\r\n\r\n", "Nice catch. I got a fix to add memory tracking for loop, will submit it soon. \r\nNote: it only works for GPU now so you need to place it on gpu.\r\n\r\n![selection_003](https://user-images.githubusercontent.com/2887803/33742189-064faa0c-db5c-11e7-8487-e92648165c50.png)\r\n", "drive-by comment, some scripts to parse out peak memory from run_metadata are [here](https://github.com/yaroslavvb/chain_constant_memory/blob/master/mem_util_test.py)", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Let me know if there is any other issue"]}, {"number": 14956, "title": "traceback error", "body": "Traceback (most recent call last):\r\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\r\n    \"__main__\", fname, loader, pkg_name)\r\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\r\n    exec cod in run_globals\r\n  File \"/usr/lib/python2.7/py_compile.py\", line 181, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/lib/python2.7/py_compile.py\", line 173, in main\r\n    compile(filename, doraise=True)\r\n  File \"/usr/lib/python2.7/py_compile.py\", line 106, in compile\r\n    with open(file, 'U') as f:\r\nIOError: [Errno 2] No such file or directory: ''\r\n[Finished in 0.7s with exit code 1]\r\n[shell_cmd: python -m py_compile \"\"]\r\n[dir: /opt/sublime_text]\r\n[path: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/lib/jvm/java-8-oracle/bin:/usr/lib/jvm/java-8-oracle/db/bin:/usr/lib/jvm/java-8-oracle/jre/bin]c", "comments": ["I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you."]}, {"number": 14955, "title": "add support for quantized ops on windows", "body": "Quantized ops support on windows. Added some simple python unit test to make sure it works.\r\nGotcha: I need to use the latest version of gemmlowp in gemmlowp.cmake which is not on mirror.bazel.build so I use github.com directly for now.\r\nThis should provide the same functionality as tf on linux. Performance could be better because msvc can't make use of the inline asm in gemmlowp (compiling gemmlowp with clang would show much better results).", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please.", "yeah, this was in the making for some time. thanks for helping with the gemmlowp.\r\nLooked at the ci failure - don't think it was my change.", "Yes, I think those failures are unrelated. Thanks again!\r\n\r\n@tensorflow-jenkins test this please.", "Jenkins, test this please.", "@tensorflow-jenkins test this please"]}, {"number": 14954, "title": "enable get the 'mathematical' gradient of output w.r.t neural network parameters", "body": "In some situation, it is necessary to get the 'mathematical' gradient of output w.r.t neural network parameters. \r\n\r\nFor example, suppose I have a neural network `out = f(s)`, where `s` is a batch of input with shape`[None, dim_s]`, while out is a scaler, `f` is simply a MLP. With `tf.gradient(out, tf.trainable_variables())` I can get gradient of out w.r.t neural network parameters of f, which is a list of gradient. Now, I have two different batch of `s`: `s1` and `s2`, then we can get two different the above gradients `G1` and `G2`. It seems that it is impossible to compute cosine between `G1` and `G2` using current tensorflow? Do I need to flatten both gradients first? Do `G1` and `G2` are the usual gradient in math? ", "comments": ["It's really difficult to follow what you are asking. TensorFlow computes gradients w.r.t. to scalar functions. I.e. usually you do tf.gradients on a loss which is a scalar (and if there is a batch, it will be some reduction across that batch too). If you give it a vector function it implicitly sums it first.   I'd suggest you setup an example with concrete things and make sure it is giving what you expect.\r\n", "Hi, @aselle Thanks. I have changed problem to scaler output to simplifier statement.\r\n\r\nAn example: \r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nbatch_1=np.random.normal(0,1, [2, 3])\r\nbatch_2=np.random.normal(0,1, [2, 3])\r\nx = tf.placeholder(tf.float32, shape=(None, 3))\r\n\r\npredictions = tf.layers.dense(x, 1, tf.tanh,\r\n                              kernel_initializer=tf.random_normal_initializer(\r\n                                  stddev=np.sqrt(1 / 100)))\r\n\r\nopt = tf.train.GradientDescentOptimizer(learning_rate=0.01)\r\ngradient_step = opt.compute_gradients(predictions, tf.trainable_variables())\r\n\r\nsess=tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\ngradients_1 = sess.run(gradient_step, feed_dict={x: batch_1})\r\ngradients_2 = sess.run(gradient_step, feed_dict={x: batch_2})\r\n```\r\n\r\n\r\nThen both `gradients_1` and `gradients_2` will look something like:\r\n\r\n> [(array([[-0.80507326],\r\n>        [-1.30997419],\r\n>        [-1.71417713]], dtype=float32), array([[-0.07164487],\r\n>        [-0.05090996],\r\n>        [ 0.01569027]], dtype=float32)), (array([ 1.99042296], dtype=float32), array([ 0.], dtype=float32))]\r\n\r\nwith shape as following:\r\n> gradients_1[0][0].shape\r\n(3, 1)\r\n> gradients_1[0][1].shape\r\n(3, 1)\r\n> gradients_1[1][1].shape\r\n(1,)\r\n> gradients_1[1][0].shape\r\n(1,)\r\n\r\nGiven the gradient is of such kind of shape, I think there is a gap between tensorflow gradient and the usual mathematical gradient(which usually is a vector or matrix), so it makes some applications, such as compute the cosine between `gradients_1` and `gradients_2` seems intractable.\r\n\r\n\r\n\r\n", "Hi, @Erichliu00 . \r\n\r\nYou build the function: `prediction = y = tanh(w x + b)` and calculate gradients as\r\n\r\n```python\r\ngradient_step = opt.compute_gradients(predictions, tf.trainable_variables())\r\n```\r\n\r\nhere `tf.trainable_variables()` is `[w, b]`, so we get: `dy / dw` and `dy / db`, the corresponding shapes are (3, 1) and (1,). I think everything works well.\r\n\r\nSince you mentioned usual mathematical gradient, I guess that you might expect `dy / dx`, like:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nbatch_1=np.random.normal(0,1, [2, 3])\r\nbatch_2=np.random.normal(0,1, [2, 3])\r\nx = tf.placeholder(tf.float32, shape=(None, 3))\r\n\r\npredictions = tf.layers.dense(x, 1, tf.tanh,\r\n                              kernel_initializer=tf.random_normal_initializer(\r\n                                  stddev=np.sqrt(1 / 100)))\r\n\r\ngrad_by_x = tf.gradients(predictions, x)[0]\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    gradients_1 = sess.run(grad_by_x, feed_dict={x: batch_1})\r\n    gradients_2 = sess.run(grad_by_x, feed_dict={x: batch_2})\r\n    print(\"gradients_1: shape: {}, value:\\n{}\".format(gradients_1.shape, gradients_1))\r\n    print(\"gradients_2: shape: {}, value:\\n{}\".format(gradients_2.shape, gradients_2))\r\n```\r\n\r\n```bash\r\n~/Downloads \u276f\u276f\u276f python test.py\r\ngradients_1: shape: (2, 3), value:\r\n[[-0.00683434  0.2317003  -0.04384441]\r\n [-0.0068151   0.23104796 -0.04372097]]\r\ngradients_2: shape: (2, 3), value:\r\n[[-0.00593388  0.20117244 -0.03806766]\r\n [-0.00684012  0.23189643 -0.04388152]]\r\n```\r\n", "Hi @facaiy , thanks.  Indeed I want to get the gradient of output `predictions ` w.r.t neural network parameters `tf.trainable_variables `, however, my concern is that `tf.trainable_variables ` will contains some variables with shape like `3x2` then the gradient w.r.t such variable denoted as `g1` and `g2` will be of the same shape `3x2`, then my concerns are: \r\n   1. are the gradient `g1` and `g2` averaged over batch input or summed over batch, mathematically, seems we should get a averaged gradient. \r\n   \r\n   2.  how can we compute cosine between `g1` and `g2` given their shapes are not a vector.\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nbatch_1=np.random.normal(0,1, [2, 3])\r\nbatch_2=np.random.normal(0,1, [2, 3])\r\nx = tf.placeholder(tf.float32, shape=(None, 3))\r\n\r\nout = tf.layers.dense(x, 2, tf.tanh,\r\n                              kernel_initializer=tf.random_normal_initializer(\r\n                                  stddev=np.sqrt(1 / 100)))\r\npredictions = tf.layers.dense(out, 1, tf.tanh,\r\n                              kernel_initializer=tf.random_normal_initializer(\r\n                                  stddev=np.sqrt(1 / 100)))\r\n\r\nopt = tf.train.GradientDescentOptimizer(learning_rate=0.01)\r\ngradient_step = opt.compute_gradients(predictions, tf.trainable_variables())\r\n\r\nsess=tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\ngradients_1 = sess.run(gradient_step, feed_dict={x: batch_1})\r\ngradients_2 = sess.run(gradient_step, feed_dict={x: batch_2})\r\n```", "gradients_1 and gradients_2 are the same shape. So just multiply them elementwise and sum the elements. You want to see if the gradients agree in direction i.e. the cosine, right? That should work, but perhaps you could give more background on what you are trying to accomplish. I think you might want to ask on Stack Overflow, as this seems to be more of a usage question rather than a bug in tensorflow.\r\n", "Yes, I want to see if the gradients agree in direction.  But there is a gap between gradients of tf and math(plz see the above two points.)", "Sure, so just do\r\n```\r\nsum([np.dot(i,j) for i,j in zip(gradients_1, gradients_2)])\r\n```\r\nand that will be the cosine. The point is tf.gradients gives you an array entry for the gradient with respect to each discrete variable instead of a flattened one, because the optimizer needs to apply the gradients to each object in turn. .", "Hi @aselle , thanks, you are right, I should consider gradient of each variable to compute cosine and also should not expect a flatten gradient since variable has shape that is not flattened.  However, I still believe that if gradient is not the usual flattened gradient, then some computation may not applicable.\r\nFor example, suppose we have a neural network parameter `x` with shape `[2,4]`, then the gradient of output scaler w.r.t it is of the same shape `[2,4]`, then given four different batches of input, we can have four values of `x`, say `xi= np.random.normal(0,1, [2,4])` for i = 0,1,2,3,  how can we compute the variance of `x` given four observations?"]}, {"number": 14953, "title": "Tensor roll op implementation", "body": "Closes #10761\r\nAdded a tf.manip.roll op that works similarly to numpy's np.roll. This was a feature requested in #10761 and was marked as contributions welcome.\r\n\r\n### Usage:\r\n\r\nRolls the elements of a tensor by the offsets of `shift` along the dimensions\r\nof `axis`. Elements that roll passed the last position will wrap around\r\nto the first.\r\nFor example:\r\n```\r\n# 't' is [0, 1, 2, 3, 4]\r\nroll(t, shift=2, axis=0) ==> [3, 4, 0, 1, 2]\r\n# shifting along multiple dimensions\r\n# 't' is [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]\r\nroll(t, shift=[1, -2], axis=[0, 1]) ==> [[7, 8, 9, 5, 6], [2, 3, 4, 0, 1]]\r\n# shifting along the same axis multiple times\r\n# 't' is [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]\r\nroll(t, shift=[2, -3], axis=[1, 1]) ==> [[1, 2, 3, 4, 0], [6, 7, 8, 9, 5]]\r\n```\r\nshift: `shift[i]` specifies the number of places by which elements are shifted\r\n  along the dimension specified by `axis[i]`. Negative shifts will roll the\r\n  elements in the opposite direction.\r\naxis: `axis[i]` specifies the dimension that the shift `shift[i]` should occur.\r\n  if the same axis is referenced more than once, the total shift for that axis\r\n  will be the sum of all the shifts that belong to that axis.\r\noutput: Has the same shape and size as the input. The elements are shifted by\r\n  the offsets of `shift` along the dimensions of `axis`.\r\n\r\n### Unit tests:\r\n```\r\n[==========] Running 24 tests from 1 test case.\r\n[----------] Global test environment set-up.\r\n[----------] 24 tests from RollOpTest\r\n[ RUN      ] RollOpTest.ScalarIndices\r\n[       OK ] RollOpTest.ScalarIndices (5 ms)\r\n[ RUN      ] RollOpTest.ScalarIndices_NoMemcpy\r\n[       OK ] RollOpTest.ScalarIndices_NoMemcpy (0 ms)\r\n[ RUN      ] RollOpTest.ScalarIndices_Complex\r\n[       OK ] RollOpTest.ScalarIndices_Complex (0 ms)\r\n[ RUN      ] RollOpTest.Simple_TwoD32\r\n[       OK ] RollOpTest.Simple_TwoD32 (0 ms)\r\n[ RUN      ] RollOpTest.Simple_TwoD32_NoMemcpy\r\n[       OK ] RollOpTest.Simple_TwoD32_NoMemcpy (0 ms)\r\n[ RUN      ] RollOpTest.Simple_ThreeD32\r\n[       OK ] RollOpTest.Simple_ThreeD32 (1 ms)\r\n[ RUN      ] RollOpTest.Simple_ThreeD32_NoMemcpy\r\n[       OK ] RollOpTest.Simple_ThreeD32_NoMemcpy (0 ms)\r\n[ RUN      ] RollOpTest.Simple_TwoD64\r\n[       OK ] RollOpTest.Simple_TwoD64 (0 ms)\r\n[ RUN      ] RollOpTest.Simple_TwoD64_NoMemcpy\r\n[       OK ] RollOpTest.Simple_TwoD64_NoMemcpy (0 ms)\r\n[ RUN      ] RollOpTest.Simple_ThreeD64\r\n[       OK ] RollOpTest.Simple_ThreeD64 (0 ms)\r\n[ RUN      ] RollOpTest.Simple_ThreeD64_NoMemcpy\r\n[       OK ] RollOpTest.Simple_ThreeD64_NoMemcpy (0 ms)\r\n[ RUN      ] RollOpTest.ZeroShift_ThreeD32\r\n[       OK ] RollOpTest.ZeroShift_ThreeD32 (0 ms)\r\n[ RUN      ] RollOpTest.ZeroShift_ThreeD32_NoMemcpy\r\n[       OK ] RollOpTest.ZeroShift_ThreeD32_NoMemcpy (0 ms)\r\n[ RUN      ] RollOpTest.ZeroSize_ThreeD32\r\n[       OK ] RollOpTest.ZeroSize_ThreeD32 (0 ms)\r\n[ RUN      ] RollOpTest.ZeroSize_ThreeD32_NoMemcpy\r\n[       OK ] RollOpTest.ZeroSize_ThreeD32_NoMemcpy (0 ms)\r\n[ RUN      ] RollOpTest.OneSize_ThreeD32\r\n[       OK ] RollOpTest.OneSize_ThreeD32 (1 ms)\r\n[ RUN      ] RollOpTest.OneSize_ThreeD32_NoMemcpy\r\n[       OK ] RollOpTest.OneSize_ThreeD32_NoMemcpy (0 ms)\r\n[ RUN      ] RollOpTest.DuplicateShifts_TwoD32\r\n[       OK ] RollOpTest.DuplicateShifts_TwoD32 (0 ms)\r\n[ RUN      ] RollOpTest.DuplicateShifts_TwoD32_NoMemcpy\r\n[       OK ] RollOpTest.DuplicateShifts_TwoD32_NoMemcpy (0 ms)\r\n[ RUN      ] RollOpTest.Error_InputMustBeVectorOrHigher\r\n[       OK ] RollOpTest.Error_InputMustBeVectorOrHigher (0 ms)\r\n[ RUN      ] RollOpTest.Error_AxisMustBeScalarOrVector\r\n[       OK ] RollOpTest.Error_AxisMustBeScalarOrVector (0 ms)\r\n[ RUN      ] RollOpTest.Error_ShiftMustBeScalarOrVector\r\n[       OK ] RollOpTest.Error_ShiftMustBeScalarOrVector (0 ms)\r\n[ RUN      ] RollOpTest.Error_ShiftAndAxisMustBeSameSize\r\n[       OK ] RollOpTest.Error_ShiftAndAxisMustBeSameSize (0 ms)\r\n[ RUN      ] RollOpTest.Error_AxisOutOfRange\r\n[       OK ] RollOpTest.Error_AxisOutOfRange (0 ms)\r\n[----------] 24 tests from RollOpTest (7 ms total)\r\n\r\n[----------] Global test environment tear-down\r\n[==========] 24 tests from 1 test case ran. (8 ms total)\r\n[  PASSED  ] 24 tests.\r\n```\r\n\r\n```\r\n//tensorflow/python/kernel_tests:manip_ops_test                          PASSED in 0.9s\r\n```\r\n\r\n### Benchmarks:\r\n```\r\nRunning main() from test_main.cc\r\nBenchmark                     Time(ns) Iterations\r\n-------------------------------------------------\r\nBM_cpu_roll_outer/256/256        46739      16065\t 5608.7MB/s 1402.2M items/s\r\nBM_cpu_roll_outer/512/512       124247       5135\t 8439.5MB/s 2109.9M items/s\r\nBM_cpu_roll_outer/1024/1024     638245       1149\t 6571.6MB/s 1642.9M items/s\r\nBM_cpu_roll_outer/2048/2048    5261260        100\t 3188.8MB/s 797.2M items/s\r\nBM_cpu_roll_all/256/256         135631       5353\t 1932.8MB/s 483.2M items/s\r\nBM_cpu_roll_all/512/512         286927       2263\t 3654.5MB/s 913.6M items/s\r\nBM_cpu_roll_all/1024/1024       981417        727\t 4273.7MB/s 1068.4M items/s\r\nBM_cpu_roll_all/2048/2048      4549074        149\t 3688.1MB/s 922.0M items/s\r\n```\r\n\r\nI had made a pull request for this before but accidentally closed it", "comments": ["Can one of the admins verify this patch?", "@yzhwang Thanks for the review! Just added the changes!", "@yzhwang Could you please take another look?", "Thanks for the review! Just added the last changes!", "@rmlarsen I think I fixed the failures. Can we run the checks again?", "@rmlarsen @yzhwang Ok this should do it. Sorry can we run the checks again?", "@asimshankar thanks. Sorry I missed that you had already done an API review on this!"]}, {"number": 14952, "title": "Branch 177191521", "body": "", "comments": []}, {"number": 14951, "title": "Document S3 `S3_REGION` constant", "body": "This gave me a lot of grief recently as the AWS Region for S3 is controlled by an undocumented constant `S3_REGION` which defaults to `us-east-1` unless defined on the system-level. Extra frustrating as it departs from the common naming practice: `AWS_REGION`:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/79422ab39b5fe0e1491abb8deabc7ecb5fd9f3a2/tensorflow/core/platform/s3/s3_file_system.cc#L52\r\n\r\nIf somebody could suggest where to document this, I'd be happy to contribute. ", "comments": ["@jhseu, do you think we should support both and just fallback or just document it?\r\n", "Yeah, if there's a common practice here, we should use that and deprecate the other environment variable in TF 2.0.", "The S3_REGION was introduced in PR #11089. Sorry about the confusion on AWS_REGION vs. S3_REGION.  I will create a PR to add AWS_REGION support shortly.", "Added a PR #14984 for the fix. Please take a look."]}, {"number": 14950, "title": "Incorrect GPU memory usage ", "body": "    with tf.device('/gpu:0') :\r\n    \twith tf.variable_scope('network1') :\r\n    \t\tx = tf.get_variable('inp1',shape = [256,50,50,3],dtype = tf.float32)\r\n    \t\tw1 = tf.get_variable('weight1',shape = [1,1,3,256],dtype = tf.float32)\r\n    \t\ty1 = tf.nn.conv2d(x, w1, strides=[1, 1, 1, 1], padding='VALID')\r\n    \t\tw1 = tf.get_variable('weight2',shape = [1,1,256,256],dtype = tf.float32)\r\n    \t\ty1 = tf.nn.conv2d(y1, w1, strides=[1, 1, 1, 1], padding='VALID')\r\n    \t\tw1 = tf.get_variable('weight3',shape = [1,1,256,1],dtype = tf.float32)\r\n    \t\ty1 = tf.nn.conv2d(y1, w1, strides=[1, 1, 1, 1], padding='VALID')\r\n    \t\r\n    \r\n    with tf.device('/gpu:0') :\r\n    \twith tf.variable_scope('network2') :\r\n    \t\tx1 = tf.get_variable('inp2',shape = [1,3],dtype = tf.float32)\r\n    \t\tw2 = tf.get_variable('weight4',shape = [3,8192],dtype = tf.float32)\r\n    \t\ty2 = tf.matmul(x1,w2)\r\n    \t\tw2 = tf.get_variable('weight5',shape = [8192,8192],dtype = tf.float32)\r\n    \t\ty2 = tf.matmul(y2,w2)\r\n    \t\tw2 = tf.get_variable('weight6',shape = [8192,1],dtype = tf.float32)\r\n    \t\ty2 = tf.matmul(y2,w2)\r\n\r\nIf I have the above model graph, then running it on a GPU and checking memory usage via nvidia-smi shows usage of 4200MB . However when running only the network1 and commenting out network2, nvidia-smi shows usage of 2200 MB. With only network2, it shows usage of 700MB.I have already set gpu_options.allow_growth = True\r\n\r\nHow is then the total graph using 4200 MB when it should have used only 2900(2200 + 700)MB ?\r\n\r\nI am using Ubuntu 14.04, python version - 2.7 , tensorflow version - 1.2", "comments": ["You are probably not accounting for intermediates implied by the various operations? You can't just assume memory usage is going to be the total memory usage of the variables.", "I don't have any other operation apart from the ones in the above graph. So when only network1 shows GPU usage of 2200 MB it does include all the intermediates , similarly with network2.", "I have another example showing the above concern\r\n\r\n<img width=\"1205\" alt=\"screen shot 2017-11-30 at 1 10 37 am\" src=\"https://user-images.githubusercontent.com/12389081/33395462-96c2a630-d56b-11e7-83a3-d2670caa90b2.png\">\r\n\r\nIn the graph defined in the image if I run it on a single GPU , the node network1 shows usage of 1.2 GB whereas if I run the left and right half of graphs on seperate GPU's the node network1 shows usage of 2 GB.\r\n \r\n", "Well it could be that even though you have allow_growth on it, the allocator graphs a larger chunk that it needs. This is often done by memory allocators for efficiency and to reduce fragmentation. @zheng-xq, might be able to provide more insight.\r\n", "This is a stale issue. Please check the issue with latest TensorFlow. If the issue still persists in the newer version of TF, please feel free to reopen it by providing details about the issue and a standalone code to reproduce the issue. Thanks!"]}, {"number": 14949, "title": "Add user friendly error checking on download_dependencies.sh", "body": "- Both on tflite and tensorflow makefiles.\r\n- Check it is run on root.", "comments": ["@tensorflow-jenkins test this please"]}, {"number": 14948, "title": "Setup.py on CentOS7 pywrap_tensorflow_internal Error", "body": "System information\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux CentOS7\r\nTensorFlow installed from (source or binary):\r\nSetup.py\r\nTensorFlow version (use command below):\r\n1.3\r\nPython version:\r\n2.7\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\n4.8.5\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\nExact command to reproduce:\r\nimport tensorflow as tf\r\nYou can collect some of this information using our environment capture script:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\ntf_env_collect.sh output:\r\n\r\n== cat /etc/issue ===============================================\r\nLinux rs-control1 3.10.0-327.36.1.el7.x86_64 #1 SMP Sun Sep 18 13:04:29 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"7 (Core)\"\r\nVERSION_ID=\"7\"\r\nCENTOS_MANTISBT_PROJECT_VERSION=\"7\"\r\nREDHAT_SUPPORT_PRODUCT_VERSION=\"7\"\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-16)\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions. There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n== uname -a =====================================================\r\nLinux rs-control1 3.10.0-327.36.1.el7.x86_64 #1 SMP Sun Sep 18 13:04:29 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\nTraceback (most recent call last):\r\nFile \"\", line 1, in \r\nFile \"/usr/local/python-tgt-201701/lib/python2.7/site-packages/tensorflow/init.py\", line 24, in \r\nfrom tensorflow.python import *\r\nFile \"/usr/local/python-tgt-201701/lib/python2.7/site-packages/tensorflow/python/init.py\", line 49, in \r\nfrom tensorflow.python import pywrap_tensorflow\r\nFile \"/usr/local/python-tgt-201701/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 52, in \r\nraise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\nFile \"/usr/local/python-tgt-201701/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in \r\nfrom tensorflow.python.pywrap_tensorflow_internal import *\r\nImportError: No module named pywrap_tensorflow_internal\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions. Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\nDescribe the problem\r\n\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nInstalling using setup.py works until trying to import tensorflow, causing an Import Error for pywrap_tensorflow_internal\r\n\r\nDependencies I believe are met and using setup.py because I'm building this into an rpm to deploy to different nodes and clusters.\r\n\r\nSource code / logs\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nSimply running setup.py and then import tensorflow as tf produces Import Error:\r\nTraceback (most recent call last):\r\nFile \"/tmp/check_tf.py\", line 1, in \r\nimport tensorflow as tf;\r\nFile \"/usr/local/python-tgt-201701/lib/python2.7/site-packages/tensorflow/init.py\", line 24, in \r\nfrom tensorflow.python import *\r\nFile \"/usr/local/python-tgt-201701/lib/python2.7/site-packages/tensorflow/python/init.py\", line 49, in \r\nfrom tensorflow.python import pywrap_tensorflow\r\nFile \"/usr/local/python-tgt-201701/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 52, in \r\nraise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\nFile \"/usr/local/python-tgt-201701/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in \r\nfrom tensorflow.python.pywrap_tensorflow_internal import *\r\nImportError: No module named pywrap_tensorflow_internal", "comments": ["Sorry, what version did you install. Did you use pip? I don't know exactly what you mean by installing with setup.py. This doesn't match our usual install instructions.", "I installed TF version 1.3 with Python 2.7. Usually just running python on the setup.py file will install the python package, but for some reason running python on Tensorflow's setup.py will install it but it won't create the pywrap_tensorflow_internal file. As far as I've noticed this is the only file missing and causes an error when trying to do anything with TF.", "I'm still confused. Where did you get a setup.py? From the source code? If you download from the web site you get a .whl  which can be installed with pip. If you install from source you sually run build_pip_package which again gives you a .whl. Can you give me the exact steps to reproduce. Did you uncompress the whl with unzip or something? ", "Yes, I got the setup.py file from the source code. Specifically: /tensorflow/tools/pip_package/setup.py\r\nAnd unfortunately the environments I'm deploying Tensorflow to are cut off from the internet due to a firewall so building it through an rpm for linux was the route I'm going. Being able to run \"python setup.py\" should install Tensorflow and it does except for one file, causing it to break once trying to import tensorflow in the python shell. Since it produces an import error on \"import tensorflow as tf\" it means it was able to install by just using Tensorflow's setup.py file. I've attempted to install by compiling from sources and using the .whl file, but neither seemed to work as well as just using setup.py.", "Well pywrap_tensorflow_internal is the entire c++ backend of TensorFlow. I.e. it's not just one file, it's the backend that runs everything. Cou'll at the very least need to run build_pip_package as\r\nbazel run tensorflow/tools/pip_package:build_pip_package \r\nBefore you do that edit the build_pip_package command to output to a specific directory  (i.e. replace\r\n```\r\n  TMPDIR=$(mktemp -d -t tmp.XXXXXXXXXX)\r\n```\r\nwith some specific directory. Then you should have a directory with a working setup.py. Maybe this is what you are doing. But at least from there you can look if the pywrap_tensorflow_internal. However, this installation path is not officially supported, so I really have no way to test any of this on CentOS7.", "I'm also getting the same error with a pip install of the tf_nightly-none-linux.whl on a similar, but separate environment (CentOS7). With error 'Failed to load the native TensorFlow runtime.'", "Closing this issue due to staleness. Please use the latest version of TensorFlow and build again.\r\nFeel free to report any issues you encounter with latest TensorFlow. Thanks!"]}, {"number": 14947, "title": "Tensorflow minimize operation runs into shape mismatch with StridedSliceGrad", "body": "System Info:\r\n   - custom code\r\n   - Linux Ubuntu 16.04\r\n   - Tensorflow 1.4.0 installed with pip install tensorflow-gpu\r\n   - python 2.7\r\n   - CuDa 8\r\n   - terminal command just calls my script with \"python GAN.py\"\r\n\r\nSource code:\r\n\r\ninput_points = tf.placeholder(shape=[batch_size, 2048, 3], dtype=tf.float32)\r\nlatent_vector = encoder(input_points, encoder_weights, encoder_biases)                       \r\nreconstructed_points = decoder(latent_vector, decoder_weights, decoder_biases)               \r\n                                                                                                 \r\nAE_loss = chamfer_distance(input_points, reconstructed_points, batch_size)                   \r\n                                                                                        \r\nAE_optimizer = tf.train.AdamOptimizer(learning_rate=0.0005, beta1=0.9)                       \r\nAE_minimizer = AE_optimizer.minimize(AE_loss)\r\n\r\n\r\n\r\nContext: The input points and reconstructed points are the same shape. After the call to chamfer_distance(), AE_loss becomes a tf.constant.\r\n\r\n\r\n\r\nUpon calling AE_minimizer, I receive the following error log:\r\nTraceback (most recent call last):\r\n  File \"GAN.py\", line 168, in <module>\r\n    main()\r\n  File \"GAN.py\", line 162, in main\r\n    AE_minimizer = AE_optimizer.minimize(AE_loss, var_list=AE_parameters)\r\n  File \"/home/adraganov/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 343, in minimize\r\n    grad_loss=grad_loss)\r\n  File \"/home/adraganov/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 414, in compute_gradients\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"/home/adraganov/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py\", line 581, in gradients\r\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n  File \"/home/adraganov/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py\", line 353, in _MaybeCompile\r\n    return grad_fn()  # Exit early\r\n  File \"/home/adraganov/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py\", line 581, in <lambda>\r\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n  File \"/home/adraganov/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/array_grad.py\", line 245, in _StridedSliceGrad\r\n    shrink_axis_mask=op.get_attr(\"shrink_axis_mask\")), None, None, None\r\n  File \"/home/adraganov/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 5572, in strided_slice_grad\r\n    shrink_axis_mask=shrink_axis_mask, name=name)\r\n  File \"/home/adraganov/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 546, in _apply_op_helper\r\n    inferred_from[input_arg.type_attr]))\r\nTypeError: Input 'begin' of 'StridedSliceGrad' Op has type int64 that does not match type int32 of argument 'shape'.\r\n\r\n\r\n\r\nI have seen traffic regarding problems with type mismatches with input 'begin', such as in Issue 11380. I have, however, seen nothing online for this specific problem, where the fields 'begin' and 'shape' have different types.", "comments": ["I found what the issue was, but am curious why the trace error didn't specify. The chamfer_distance loss metric had an arg_min operation, which was then being processed by minimize incorrectly, as it is non-differentiable. Does minimize not check for differentiability in the graph it is working with?", "@ebrevdo or @andydavis1 might know whether `tf.train.AdamOptimizer(loss)` is supposed to return a better error, if `loss` is non-differentiable.", "As I known, the gradient op is not registered for `argmax` and `argmin`, since there not practical implementation, see #14931 and #2177 .  I just wonder whether we should mark them by `ops.NotDifferentiable` or `REGISTER_OP_NO_GRADIENT`. So the gradients are always zero when those ops are involved. ", "@aselle Can you comment on the StrideSliceGrad?", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "I believe ops without a gradient (like argmax) are treated as if their gradient is zeros (@mrry can you confirm?) I don't see how that could cause the error @Andrew-Draganov is getting though.\r\n\r\n@Andrew-Draganov, can you post a self-contained example reproducing the problem?", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Perhaps this is solved, or just no longer of interest.  ", "I define op with argmax, but my loss function is differentiable. When I use python 3.6 ,it works without error.When I transfer the same code to the server(python 2.7)\uff0cit report an error \"TypeError: Input 'begin' of 'StridedSliceGrad' Op has type int64 that does not match type int32 of argument 'shape'\".Maybe it is a bug of tensorflow?", "Oh, my problem solved! If your python is 2.7,try it!\r\n index = tf.to_int32(tf.argmax(xxx))"]}, {"number": 14946, "title": "Getting the given error while installing gpu version of tensorflow", "body": "Exception:\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Shivam\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\pip\\basecommand.py\", line 215, in main\r\n    status = self.run(options, args)\r\n  File \"C:\\Users\\Shivam\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\pip\\commands\\install.py\", line 342, in run\r\n    prefix=options.prefix_path,\r\n  File \"C:\\Users\\Shivam\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\pip\\req\\req_set.py\", line 784, in install\r\n    **kwargs\r\n  File \"C:\\Users\\Shivam\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\pip\\req\\req_install.py\", line 851, in install\r\n    self.move_wheel_files(self.source_dir, root=root, prefix=prefix)\r\n  File \"C:\\Users\\Shivam\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\pip\\req\\req_install.py\", line 1064, in move_wheel_files\r\n    isolated=self.isolated,\r\n  File \"C:\\Users\\Shivam\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\pip\\wheel.py\", line 345, in move_wheel_files\r\n    clobber(source, lib_dir, True)\r\n  File \"C:\\Users\\Shivam\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\pip\\wheel.py\", line 323, in clobber\r\n    shutil.copyfile(srcfile, destfile)\r\n  File \"C:\\Users\\Shivam\\Anaconda2\\envs\\tensorflow\\lib\\shutil.py\", line 121, in copyfile\r\n    with open(dst, 'wb') as fdst:\r\nPermissionError: [Errno 13] Permission denied: 'C:\\\\Users\\\\Shivam\\\\Anaconda2\\\\envs\\\\tensorflow\\\\Lib\\\\site-packages\\\\numpy\\\\core\\\\multiarray.cp35-win_amd64.pyd'", "comments": ["Also on doing import tensorflow I am getting the following\r\n\r\nimport tensorflow\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-1-d6579f534729>\", line 1, in <module>\r\n    import tensorflow\r\n\r\n  File \"C:\\Users\\Shivam\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n\r\n  File \"C:\\Users\\Shivam\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n\r\n  File \"C:\\Users\\Shivam\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 30, in <module>\r\n    self_check.preload_check()\r\n\r\n  File \"C:\\Users\\Shivam\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\platform\\self_check.py\", line 97, in preload_check\r\n    % (build_info.cudnn_dll_name, build_info.cudnn_version_number))\r\n\r\nImportError: Could not find 'cudnn64_6.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Note that installing cuDNN is a separate step from installing CUDA, and this DLL is often found in a different directory from the CUDA DLLs. You may install the necessary DLL by downloading cuDNN 6 from this URL: https://developer.nvidia.com/cudnn", "The error suggests this. Did you try finding cudnn and putting it in your path?\r\n```\r\nImportError: Could not find 'cudnn64_6.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Note that installing cuDNN is a separate step from installing CUDA, and this DLL is often found in a different directory from the CUDA DLLs. You may install the necessary DLL by downloading cuDNN 6 from this URL: https://developer.nvidia.com/cudnn\r\n```", "if you not get the cudnn64_6.dll, then use below method\r\n\r\nyou should rename the cudnn64_7.dll or cudnn64_8.dll as cudnn64_6.dll in the below path of your system\r\n\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\bin\r\n\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "I'm also getting this error on Windows 10 for Tensorflow GPU 1.4.0 version:\r\n\r\nTensorflow version is correct:\r\n```\r\n>pip show tensorflow-gpu\r\nName: tensorflow-gpu\r\nVersion: 1.4.0\r\n```\r\n\r\nPython/Anaconda versions are correct:\r\n```\r\n>conda info\r\nconda version : 4.4.3\r\npython version : 3.5.0.final.0\r\n```\r\n\r\ncuDNN64_6.dll exists:\r\n```\r\n>where cudnn64_6.dll\r\nC:\\Program Files\\cuDNN6\\cuda\\bin\\cudnn64_6.dll\r\n```\r\n\r\nNVidia CUDA 8.0 is installed.\r\n\r\nI also have full Visual Studio 2015 as well as 2017 with C++ installed (so all runtimes are included).\r\n\r\nSo everything looks good and still I'm getting:\r\n```\r\nImportError: DLL load failed: The specified module could not be found.\r\n```\r\n\r\n", "Ok... I figured out the cause for `ImportError: DLL load failed` and also figured out the way to debug this systematically. I have seen this error occuring because either cuDNN or CUDA toolkit DLLs being of wrong version or found in Anaconda folder instead of NVidia's folder. I've written a blog post on how to debug this error:\r\n\r\nhttp://shitalshah.com/p/debugging-tensorflow-dll-importerror/\r\n\r\nHope this helps.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 145 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 14945, "title": "FreeBSD compatibility", "body": "After 1.2.0 tensorflow broke on FreeBSD, provided changes makes it build again.\r\n\r\nAlso, when nsync has merged this PR:\r\nhttps://github.com/google/nsync/pull/3\r\n\r\nThe nsync dependency needs to be bumped.", "comments": ["Can one of the admins verify this patch?", "@gunan any chance you could give me answers to the two questions so we could move this forward and get it merged? :)", "@gunan should be good to go now :)", "@gunan i've now changed the resolver error to return an empty string instead as requested", "Jenkins, test this please.", "Jenkins, test this please."]}]