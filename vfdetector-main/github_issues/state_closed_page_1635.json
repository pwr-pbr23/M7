[{"number": 3858, "title": "Fix reduce_prod gradient for scalar reduction indices params", "body": "The gradient of `reduce_prod` currently can't deal with scalars being passed as the `reduction_indices` parameter.\nI've added a `reshape` to fix this, and also added a test case for `reduce_prod` and other reduction ops.\nThis fixes #3815.\n", "comments": ["Can one of the admins verify this patch?\n", "Jenkins, test this please.\n", "test this please\n", "@tensorflow-jenkins test this please\n", "The ci test failure seems to be unrelated. Could someone restart Jenkins?\n", "@tensorflow-jenkins test this please.\n"]}, {"number": 3857, "title": "(Eigen thirdparty) Can't compile fixed point matrix matrix multiply", "body": "I can't get the Eigen fixed point backend to compile a simple matrix matrix multiply with Intel AVX2.\nAnyone got a working example? I think the PacketMathAVX2.h is either missing something or I need to tell Eigen to align the matrices somehow. But I'm at a loss.\n\nHere is a simple C++ code that highlights the issue. \n\n```\n#include <Eigen/Dense>\n#define EIGEN_VECTORIZE_AVX\n#define EIGEN_VECTORIZE_AVX2\n#include <unsupported/Eigen/CXX11/Tensor>\n#include <unsupported/Eigen/CXX11/FixedPoint>\n\nint main(int argc, char* argv[])\n{\n    Eigen::QInt8 dbg((float)1.5), dbg2((float)2.5), dbg3;\n    Eigen::Matrix<Eigen::QInt32,32,32>one,two,three;\n\n    dbg3 = dbg + dbg2;\n    dbg3 = dbg * dbg2;\n    dbg3 = dbg - dbg2; //OK\n    three = one * two;   //Does not compile\n    return 0;\n}\n```\n### Environment info\n\nOperating System: Ubuntu 14.04 lts, Eigen3.3-beta2 gcc 4.9.3 cmake 3.2.2\n\nInstalled version of CUDA and cuDNN: \nlibcudart.so.7.5.18\n1. The commit hash:  da41c02\n2. The output of `bazel version` NA\n### Steps to reproduce\n1. Install Eigen3.3-beta2\n2. Merge tensorflow/thirdparty/eigen3/unsupported/Eigen with Eigen3.3-beta2root/unsupported/Eigen\n3. build the example code listing following these steps https://eigen.tuxfamily.org/dox/GettingStarted.html  \n### What have you tried?\n1. building without AVX defines \n2. confirmed that non-matrix algebra works ok\n### Logs or other output that would be helpful\n\nattached\n\n[tensoreigenlog.txt](https://github.com/tensorflow/tensorflow/files/421290/tensoreigenlog.txt)\n", "comments": ["I was informed that this is an unsupported useage of tensorflow. Here is a snippet that was sent which does compile but uses tensors instead of Eigen::MatrixEigen::Qint32,X,X. Many thanks to @benoitsteiner  :+1: \n\n\"I managed to get the following code to compile by calling gcc -I . -mavx2  test.cpp -std=c++11  -o test_contract:\"\n\n```\n#include <Eigen/Dense>\n#define EIGEN_VECTORIZE_AVX\n#define EIGEN_VECTORIZE_AVX2\n#include <unsupported/Eigen/CXX11/Tensor>\n#include <unsupported/Eigen/CXX11/FixedPoint>\n\nint main(int argc, char* argv[])\n{\n  Eigen::QInt8 dbg((float)1.5), dbg2((float)2.5), dbg3;\n  Eigen::Tensor<Eigen::QInt8,2> one;\n  Eigen::Tensor<Eigen::QUInt8,2> two;\n\n  Eigen::Tensor<Eigen::QInt32,2> three;\n  Eigen::Tensor<Eigen::QInt8,2> four;\n\n  dbg3 = dbg + dbg2;\n  dbg3 = dbg * dbg2;\n  dbg3 = dbg - dbg2;\n\n  Eigen::array<Eigen::Tensor<Eigen::QInt32,2>::DimensionPair, 1> contract_dims;\n  contract_dims[0] = Eigen::Tensor<Eigen::QInt32,2>::DimensionPair(1, 0);\n  three = one.contract(two, contract_dims);\n\n  // still doesn't work.\n  //    three = one.contract(four, contract_dims);\n\n    return 0;\n}\n```\n"]}, {"number": 3856, "title": "Need Protobuf pip wheel package for Python 3.4, which is missing the setup doc", "body": "Current documentation only gives `protobuf-3.0.0b2.post2-cp35-none-any.whl` but nothing for `python 3.4`.\n### Environment info\n\nOperating System: Mac OS X El Capitan 10.11.6\n\nInstalled version of CUDA and cuDNN: None (Intel Iris Graphics 6100)\n\nIf installed from binary pip package, provide:\nTF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.10.0rc0-py3-none-any.whl\n#### pip package you installed: `tensorflow-0.10.0rc0-py3-none-any.whl`\n### Steps to reproduce\n1. Follow the Mac [setup steps here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md)\n2. Installation completed fine.  \n3. Protobuf's package is the pure python one, which is VERY SLOW. The [setup guide](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#protobuf-library-related-issues) only gives a pip wheel package for Python 3.5\n### What steps you have tried\n\n``` bash\n$ pip3 install --upgrade https://storage.googleapis.com/tensorflow/mac/protobuf-3.0.0b2.post2-cp35-none-any.whl\nprotobuf-3.0.0b2.post2-cp35-none-any.whl is not a supported wheel on this platform.\n\n$ pip3 install --upgrade https://storage.googleapis.com/tensorflow/mac/protobuf-3.0.0b2.post2-cp34-none-any.whl\nCollecting protobuf==3.0.0b2.post2 from https://storage.googleapis.com/tensorflow/mac/protobuf-3.0.0b2.post2-cp34-none-any.whl\n  HTTP error 404 while getting https://storage.googleapis.com/tensorflow/mac/protobuf-3.0.0b2.post2-cp34-none-any.whl\n  Could not install requirement protobuf==3.0.0b2.post2 from https://storage.googleapis.com/tensorflow/mac/protobuf-3.0.0b2.post2-cp34-none-any.whl because of error 404 Client Error: Not Found for url: https://storage.googleapis.com/tensorflow/mac/protobuf-3.0.0b2.post2-cp34-none-any.whl\nCould not install requirement protobuf==3.0.0b2.post2 from https://storage.googleapis.com/tensorflow/mac/protobuf-3.0.0b2.post2-cp34-none-any.whl because of HTTP error 404 Client Error: Not Found for url: https://storage.googleapis.com/tensorflow/mac/protobuf-3.0.0b2.post2-cp34-none-any.whl for URL https://storage.googleapis.com/tensorflow/mac/protobuf-3.0.0b2.post2-cp34-none-any.whl\n\n```\n", "comments": ["Ok, it seems that the same wheel package is compatible with python 3.4. Simply rename the wheel package `protobuf-3.0.0b2.post2-cp35-none-any.whl` to `protobuf-3.0.0b2.post2-cp34-none-any.whl` or `protobuf-3.0.0b2.post2-cp3-none-any.whl` would work. \n\nSo can we update the docs to drop the extra **5** in the wheel package filename so that it would work for both python 3.4 and 3.5?\n\n``` bash\n$ wget https://storage.googleapis.com/tensorflow/mac/protobuf-3.0.0b2.post2-cp35-none-any.whl\n--2016-08-16 16:55:00--  https://storage.googleapis.com/tensorflow/mac/protobuf-3.0.0b2.post2-cp35-none-any.whl\nResolving storage.googleapis.com... \n173.194.68.128, 2607:f8b0:400d:c0c::80\nConnecting to storage.googleapis.com|173.194.68.128|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 422170 (412K) [application/octet-stream]\nSaving to: \u2018protobuf-3.0.0b2.post2-cp35-none-any.whl\u2019\n\nprotobuf-3.0.0b2.post2-cp35-none-any.whl                                                   100%\n\n2016-08-16 16:55:05 (5.37 MB/s) - \u2018protobuf-3.0.0b2.post2-cp35-none-any.whl\u2019 saved [422170/422170]\n\n$ mv protobuf-3.0.0b2.post2-cp3{5,4}-none-any.whl\n$ pip install --upgrade protobuf-3.0.0b2.post2-cp34-none-any.whl \nProcessing ./protobuf-3.0.0b2.post2-cp34-none-any.whl\nRequirement already up-to-date: six>=1.9 in /Users/ye/.envs/ai/lib/python3.4/site-packages (from protobuf==3.0.0b2.post2)\nRequirement already up-to-date: setuptools in /Users/ye/.envs/ai/lib/python3.4/site-packages (from protobuf==3.0.0b2.post2)\nInstalling collected packages: protobuf\n  Found existing installation: protobuf 3.0.0b2\n    Uninstalling protobuf-3.0.0b2:\n      Successfully uninstalled protobuf-3.0.0b2\nSuccessfully installed protobuf-3.0.0b2.post2\n\n```\n", "Except that Jenkins continuous build only tests things for a single Python\n3 version, so there's no telling if some new change makes it incompatible\nwith 3.4\n\nOn Tue, Aug 16, 2016 at 2:09 PM, Ye Wang notifications@github.com wrote:\n\n> Ok, it seems that the same wheel package is compatible with python 3.4.\n> Simply rename the wheel package protobuf-3.0.0b2.post2-cp35-none-any.whl\n> to protobuf-3.0.0b2.post2-cp34-none-any.whl or protobuf-3.0.0b2.post2-cp3-\n> none-any.whl would work.\n> \n> So can we update the docs to drop the extra _5_ in the wheel package\n> filename so that it would work for both python 3.4 and 3.5?\n> \n> $ wget https://storage.googleapis.com/tensorflow/mac/protobuf-3.0.0b2.post2-cp35-none-any.whl\n> --2016-08-16 https://storage.googleapis.com/tensorflow/mac/protobuf-3.0.0b2.post2-cp35-none-any.whl--2016-08-16 16:55:00--  https://storage.googleapis.com/tensorflow/mac/protobuf-3.0.0b2.post2-cp35-none-any.whl\n> Resolving storage.googleapis.com...\n> 173.194.68.128, 2607:f8b0:400d:c0c::80\n> Connecting to storage.googleapis.com|173.194.68.128|:443... connected.\n> HTTP request sent, awaiting response... 200 OK\n> Length: 422170 (412K) [application/octet-stream]\n> Saving to: \u2018protobuf-3.0.0b2.post2-cp35-none-any.whl\u2019\n> \n> protobuf-3.0.0b2.post2-cp35-none-any.whl                                                   100%\n> \n> 2016-08-16 16:55:05 (5.37 MB/s) - \u2018protobuf-3.0.0b2.post2-cp35-none-any.whl\u2019 saved [422170/422170]\n> \n> $ mv protobuf-3.0.0b2.post2-cp3{5,4}-none-any.whl\n> $ pip install --upgrade protobuf-3.0.0b2.post2-cp34-none-any.whl\n> Processing ./protobuf-3.0.0b2.post2-cp34-none-any.whl\n> Requirement already up-to-date: six>=1.9 in /Users/ye/.envs/ai/lib/python3.4/site-packages (from protobuf==3.0.0b2.post2)\n> Requirement already up-to-date: setuptools in /Users/ye/.envs/ai/lib/python3.4/site-packages (from protobuf==3.0.0b2.post2)\n> Installing collected packages: protobuf\n>   Found existing installation: protobuf 3.0.0b2\n>     Uninstalling protobuf-3.0.0b2:\n>       Successfully uninstalled protobuf-3.0.0b2\n> Successfully installed protobuf-3.0.0b2.post2\n> $ ipython\n> Python 3.4.3 (default, Jun 10 2015, 19:56:14)\n> Type \"copyright\", \"credits\" or \"license\" for more information.\n> \n> IPython 5.1.0 -- An enhanced Interactive Python.?         -> Introduction and overview of IPython's features.%quickref -> Quick reference.help      -> Python's own help system.\n> object?   -> Details about 'object', use 'object??' for extra details.\n> \n> In [1]: from google import protobuf\n> \n> In [2]: protobuf.**version**\n> Out[2]: '3.0.0b2.post2'\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/3856#issuecomment-240239696,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AABaHKSZwecCNoxVQVCZVKfRddWzTgsWks5qgibxgaJpZM4Jl1l3\n> .\n", "@yaroslavvb so no love for python 3.4 folks?!\n", "Have you tested with `protobuf-3.0.0b2.post2-cp3-none-any.whl`? If that works, I'd be happy to rename the file and adjust the docs.\n", "@martinwicke no I have not tried it yet. Can you tell me where to get it? (I've google'd, nothing came up relevant for `protobuf-3.0.0b2.post2-cp3-none-any.whl`) \n", "I meant, rename `protobuf-3.0.0b2.post2-cp35-none-any.whl` to `protobuf-3.0.0b2.post2-cp3-none-any.whl` to see if that works.\n", "@martinwicke Oh ok. In that case, it worked, as I demo'd [15 days ago](#issuecomment-240239696).\n", "@gunan when you rebuild the protobuf whl, can you rename it and adjust the docs accordingly?\n", "New wheel files built, uploaded. PR #4271 will update the docs. \n", "How can you verify that you actually have a C++ version of the protobuf library installed?\n\n```\npython -c \"from google.protobuf.internal import api_implementation; print(api_implementation._default_implementation_type)\"\n```\n\nstill says `python`\n", "@hholst80 good point and you are right, mine says `python` as well.\n\n``` bash\n$ workon ai\n(ai) $ pip freeze\nappnope==0.1.0\ncycler==0.10.0\ndecorator==4.0.10\nipython==5.1.0\nipython-genutils==0.1.0\nmatplotlib==1.5.1\nnumpy==1.11.1\npandas==0.18.1\npexpect==4.2.0\npickleshare==0.7.4\nprompt-toolkit==1.0.6\nprotobuf==3.0.0b2.post2\nptyprocess==0.5.1\nPygments==2.1.3\npyparsing==2.1.8\npython-dateutil==2.5.3\npytz==2016.6.1\nrequests==2.11.1\nscikit-learn==0.17.1\nscipy==0.18.0\nsimplegeneric==0.8.1\nsix==1.10.0\ntensorflow==0.10.0rc0\nTheano==0.8.2\ntraitlets==4.2.2\nwcwidth==0.1.7\n(ai) $ python -c \"from google.protobuf.internal import api_implementation; print(api_implementation._default_implementation_type)\"\npython\n(ai) $ python -V\nPython 3.4.3\n```\n", "Unfortunately, just renaming the `.whl` file seems insufficient. You also have to rename `<venv_path>/lib/python3.4/site-packages/google/protobuf/internal/_api_implementation.cpython-35m-darwin.so` to `<venv_path>/lib/python3.4/site-packages/google/protobuf/internal/_api_implementation.so`. @gunan, perhaps it's worth building a 3.4 specific protobuf package.\n", "Bonus, we found out that our proto packages were not optimized when building. Now we are.\nI have built and uploaded all the following packages, this time with correct python versions:\nhttps://storage.googleapis.com/tensorflow/mac/cpu/protobuf-3.0.0-cp35-cp35m-macosx_10_11_x86_64.whl\nhttps://storage.googleapis.com/tensorflow/mac/cpu/protobuf-3.0.0-cp27-cp27m-macosx_10_11_x86_64.whl\n\nhttps://storage.googleapis.com/tensorflow/linux/cpu/protobuf-3.0.0-cp27-none-linux_x86_64.whl\nhttps://storage.googleapis.com/tensorflow/linux/cpu/protobuf-3.0.0-cp34-cp34m-linux_x86_64.whl\nhttps://storage.googleapis.com/tensorflow/linux/cpu/protobuf-3.0.0-cp35-cp35m-linux_x86_64.whl\n\nBut there is a problem building the py3.4 version on mac. I am exploring what is going on.\n\nIn the meantime, could you try using the py3.5 version by renaming, and see if it works?\n", "Yes, will try 3.5 whl out again today. \n\nRe bonus: Nice! :)\n\nSent from my iPhone\n\n> On Sep 14, 2016, at 6:20 PM, gunan notifications@github.com wrote:\n> \n> Bonus, we found out that our proto packages were not optimized when building. Now we are.\n> I have built and uploaded all the following packages, this time with correct python versions:\n> https://storage.googleapis.com/tensorflow/mac/cpu/protobuf-3.0.0-cp35-cp35m-macosx_10_11_x86_64.whl\n> https://storage.googleapis.com/tensorflow/mac/cpu/protobuf-3.0.0-cp27-cp27m-macosx_10_11_x86_64.whl\n> \n> https://storage.googleapis.com/tensorflow/linux/cpu/protobuf-3.0.0-cp27-none-linux_x86_64.whl\n> https://storage.googleapis.com/tensorflow/linux/cpu/protobuf-3.0.0-cp34-cp34m-linux_x86_64.whl\n> https://storage.googleapis.com/tensorflow/linux/cpu/protobuf-3.0.0-cp35-cp35m-linux_x86_64.whl\n> \n> But there is a problem building the py3.4 version on mac. I am exploring what is going on.\n> \n> In the meantime, could you try using the py3.5 version by renaming, and see if it works?\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n", "@gunan sorry for the delay. Renaming the Python 3.5 wheel package will install okay but it's not using the optimized C++ version but rather the Python version, see below. \n\nLooking forward to get that Python 3.4 version!\n\n``` bash\n$ wget https://storage.googleapis.com/tensorflow/mac/cpu/protobuf-3.0.0-cp35-cp35m-macosx_10_11_x86_64.whl\n--2016-09-19 17:44:31--  https://storage.googleapis.com/tensorflow/mac/cpu/protobuf-3.0.0-cp35-cp35m-macosx_10_11_x86_64.whl\nResolving storage.googleapis.com... 209.85.232.128, 2607:f8b0:400d:c0d::80\nConnecting to storage.googleapis.com|209.85.232.128|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 952473 (930K) [application/octet-stream]\nSaving to: \u2018protobuf-3.0.0-cp35-cp35m-macosx_10_11_x86_64.whl\u2019\n\nprotobuf-3.0.0-cp35-cp35m-macosx_10_11_x86_64.whl                                          100%[========================================================================================================================================================================================================================================>] 930.15K  4.52MB/s    in 0.2s    \n\n2016-09-19 17:44:32 (4.52 MB/s) - \u2018protobuf-3.0.0-cp35-cp35m-macosx_10_11_x86_64.whl\u2019 saved [952473/952473]\n\n$ pip install protobuf-3.0.0-cp35-cp35m-macosx_10_11_x86_64.whl\nprotobuf-3.0.0-cp35-cp35m-macosx_10_11_x86_64.whl is not a supported wheel on this platform.\n\n$ mv protobuf-3.0.0-cp35-cp35m-macosx_10_11_x86_64.whl protobuf-3.0.0-cp34-cp34m-macosx_10_11_x86_64.whl\n$ pip install ./protobuf-3.0.0-cp34-cp34m-macosx_10_11_x86_64.whl\nProcessing ./protobuf-3.0.0-cp34-cp34m-macosx_10_11_x86_64.whl\nRequirement already satisfied (use --upgrade to upgrade): six>=1.9 in /Users/ye/.envs/ai/lib/python3.4/site-packages (from protobuf==3.0.0)\nRequirement already satisfied (use --upgrade to upgrade): setuptools in /Users/ye/.envs/ai/lib/python3.4/site-packages (from protobuf==3.0.0)\nInstalling collected packages: protobuf\nSuccessfully installed protobuf-3.0.0\n\n$ python -c \"from google.protobuf.internal import api_implementation; print(api_implementation._default_implementation_type)\"\npython\n```\n", "I am trying, but I do not have access to a macosx machine with python34 installed.\nFunny, I have no idea how to proceed.\n", "Can we publish the instructions how to build the custom protobuf whl? Then\nYe (and anyone else) can rebuild it on whatever system they like.\nOn Wed, Sep 28, 2016 at 01:52 gunan notifications@github.com wrote:\n\n> I am trying, but I do not have access to a macosx machine with python34\n> installed.\n> Funny, I have no idea how to proceed.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/3856#issuecomment-250077922,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAjO_TFkeDx3se4DtmPKeaH1r6zKPhL8ks5qugCfgaJpZM4Jl1l3\n> .\n", "That's a good idea :)\nLet me share them here first:\n\nAll the instructions are in line with here, please see the page for all the prerequisites:\nhttps://github.com/google/protobuf/blob/master/src/README.md\n\nThen I tried to run the following commands:\ngit clone https://github.com/google/protobuf.git\ncd protobuf\n./autogen.sh\nCXXFLAGS=\"-fPIC -g -O2\" ./configure\nmake -j12\nexport PROTOC=$PWD/src/protoc\ncd python\npython3.4 setup.py bdist_wheel --cpp_implementation --compile_static_extension\ncd dist\n\nThe wheel file will be in the \"dist\" directory.\n", "Let's publish this, possibly inline in the os_setup.md document, to be\nreorganized soon to be more friendly to beginners.\n\nOn Wed, Sep 28, 2016 at 11:43 AM, gunan notifications@github.com wrote:\n\n> That's a good idea :)\n> Let me share them here first:\n> \n> All the instructions are in line with here, please see the page for all\n> the prerequisites:\n> https://github.com/google/protobuf/blob/master/src/README.md\n> \n> Then I tried to run the following commands:\n> git clone https://github.com/google/protobuf.git\n> cd protobuf\n> ./autogen.sh\n> CXXFLAGS=\"-fPIC -g -O2\" ./configure\n> make -j12\n> export PROTOC=$PWD/src/protoc\n> cd python\n> python3.4 setup.py bdist_wheel --cpp_implementation\n> --compile_static_extension\n> cd dist\n> \n> The wheel file will be in the \"dist\" directory.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/3856#issuecomment-250260474,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAjO_UtG4dotPDglRsZNxRjL7odyrffxks5qurVjgaJpZM4Jl1l3\n> .\n", "@martinwicke @gunan great thanks. I will give the instruction a run today.\n", "Following @gunan 's guide step by step to build `protobuf-3.0.2-cp34-cp34m-macosx_10_10_x86_64.whl` wheel package locally on my Mac, and it works like a charm!\n\nThank you guys!\n\n``` bash\n$ pip install protobuf-3.0.2-cp34-cp34m-macosx_10_10_x86_64.whl \nProcessing ./protobuf-3.0.2-cp34-cp34m-macosx_10_10_x86_64.whl\nRequirement already satisfied (use --upgrade to upgrade): six>=1.9 in /Users/ye/.envs/ai/lib/python3.4/site-packages (from protobuf==3.0.2)\nRequirement already satisfied (use --upgrade to upgrade): setuptools in /Users/ye/.envs/ai/lib/python3.4/site-packages (from protobuf==3.0.2)\nInstalling collected packages: protobuf\nSuccessfully installed protobuf-3.0.2\n\n$ pip freeze | grep protobuf\nprotobuf==3.0.2\n\n$ python -c \"from google.protobuf.internal import api_implementation; print(api_implementation._default_implementation_type)\"\ncpp\n```\n", "As the #4642 is merged, I am closing this issue.\n"]}, {"number": 3855, "title": "[SOLVED] How to save TensorForest Model?", "body": "Thanks for reading. \n\nI am trying to save a `TensorForestEstimatorModel` and am not sure how to do so. \n\nI run:\n\n``` python\nimport tensorflow as tf\nimport tensorflow.contrib.learn\n\nhparams = tf.contrib.tensor_forest.python.tensor_forest.ForestHParams(\n        num_trees=3, max_nodes=1000, num_classes=3, num_features=4)\nclassifier = tf.contrib.learn.TensorForestEstimator(hparams)\n\niris = tf.contrib.learn.datasets.load_iris()\ndata = iris.data.astype(np.float32)\ntarget = iris.target.astype(np.float32)\n\nclassifier.fit(x=data, y=target, steps=100)\n```\n\nAnd everything works great but I see no method to save the model. That is if I run:\n\n``` python\nclassifier.save('/tmp/tf_examples/my_model_1/')\n```\n\nI get the error:\n\n```\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n<ipython-input-62-0079ac3a857d> in <module>()\n----> 1 classifier.save('/tmp/tf_examples/my_model_1/')\n\nAttributeError: 'TensorForestEstimator' object has no attribute 'save'\n```\n\nThanks so much!\n### Environment info\n\nOperating System: OS X 10.11.6\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n   sudo pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.10.0rc0-py2-none-any.whl\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n   0.10.0rc0\n", "comments": ["It appears the old estimator has a save function. @martinwicke should know more about the plan.\n", "Update on this issue:\n\nWhen I create the model ``, I get the warning:\n\n```\nWARNING:tensorflow:Using temporary folder as model directory: /var/folders/k7/x_ddt3gn57z7x452wglt9brh0000gn/T/tmpM2yh_z\n```\n\nAnd when I look in that folder I find the following files/folders:\n- `checkpoint`\n- `eval`\n- `events.out.tfevents.1471375449.ip-192-168-100-142.ec2.internal`\n- `graph.pbtxt`\n- `model.ckpt-100-00000-of-00001`\n- `model.ckpt-100.meta`\n\nAnd so if I try and run `skflow.TensorFlowEstimator.restore('/var/folders/k7/x_ddt3gn57z7x452wglt9brh0000gn/T/tmpM2yh_z')`, if get the error:\n\n```\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-73-128bb2217974> in <module>()\n----> 1 skflow.TensorFlowEstimator.restore('/var/folders/k7/x_ddt3gn57z7x452wglt9brh0000gn/T/tmpM2yh_z')\n\n/Users/brad/anaconda/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/base.pyc in restore(cls, path, config)\n    324     model_def_filename = os.path.join(path, 'model.def')\n    325     if not os.path.exists(model_def_filename):\n--> 326       raise ValueError(\"Restore folder doesn't contain model definition.\")\n    327     # list of parameters that are allowed to be reconfigured\n    328     reconfigurable_params = ['_config']\n\nValueError: Restore folder doesn't contain model definition.\n```\n\nSo it looks like there is a model in there, but it's not recognizing it. \n", "Another update that solved my problem. Posting it here for posterity. \n\nIf during the model definition I add the `model_dir` parameter, this loads the model and I can do inference\n\n``` python\nimport tensorflow as tf\nimport tensorflow.contrib.learn as skflow\n\nhparams = tf.contrib.tensor_forest.python.tensor_forest.ForestHParams(\n        num_trees=10, max_nodes=1000, num_classes=2, num_features=5)\nclassifier = skflow.TensorForestEstimator(hparams, model_dir='/var/folders/k7/x_ddt3gn57z7x452wglt9brh0000gn/T/tmpEJhoS9')\n\nclassifier.evaluate(x=X_test, y=y_test, steps=100)\n```\n\nAnd everything works as expected. \n", "Thanks for sharing."]}, {"number": 3854, "title": "Peculiar behavior with tf.self_adjoint_eig", "body": "Hi, \n\nI've come across a really peculiar bug that I think is actually a sign of a much larger problem, but what that is, I have no idea.  The following code:\n\n```\nx = np.random.randn(10, 10)\n\nx = (x+x.T)/2\n\nX = tf.placeholder(dtype=tf.float32, shape=[10, 10])\n\nE, V = tf.self_adjoint_eig(X)\n\nwith tf.Session() as sess:\n    e, v = sess.run([E, V], feed_dict={X: x})\n\n    print(e.shape)\n```\n\nthrows the error:\n\n```\nraise TypeError(\"'Tensor' object is not iterable.\")\n```\n\nbut curiously:\n\n```\nx = np.random.randn(10, 10)\n\nx = (x+x.T)/2\n\nX = tf.placeholder(dtype=tf.float32, shape=[10, 10])\n\nE = tf.self_adjoint_eig(X)\n\nwith tf.Session() as sess:\n    e = sess.run(E, feed_dict={X: x})\n\n    print(e.shape)\n```\n\nreturns (11, 10), and I have no idea how that's even possible, since the docs say that tf.self_adjoint_eig returns two tensors.  While investigating this, I discovered that a lot of my install (from pip) differs substantially from what's documented here even though I have the latest version (tensorflow==0.10.0rc0 from pip freeze).  In particular, tf.self_adjoint_eig is not defined in linalg_ops.py, and I can't find where it's being called from.  Any ideas?\n\nThanks,\nShawn\n\n**Update:**\n\nI just found it where it's called from.  According to the comment in gen_linalg_ops.py:\n\n```\n  The result is a M+1 x M matrix whose first row is the eigenvalues, and\n  subsequent rows are eigenvectors.\n```\n\nThis is different than the API docs.  One or the other (from a usefulness standpoint, probably the code) should be changed.\n\n**Further Update:**\n\nThis actually might be an install problem as mentioned above.  The code here calls self_adjoint_eig_v2 which doesn't exist in the latest install.  Why is that?\n", "comments": ["Hi @shawnjhenry . 0.10.0rc0 has the older implementation where self_adjoint_eig returns a single value. The documentation you are reading online is the live version with this particular change:\n\n# \n\nChanges {batch_}self_adjoint_eig ops to return eigenvalues and eigenv\u2026\n\u2026ectors as separate outputs. This is implemented as a new set of kernels SelfAdjointEigV2 and BatchSelfAdjointEigV2. {batch_}self_adjoint_eig become Python wrappers that call the new kernels.\n\nAdds new ops self_adjoint_eigvals and batch_self_adjoint_eigvals that compute the eigenvalues but not the eigenvectors, which is faster and uses less memory than computing both.\n\nNOTICE: This changes the public API and TensorFlow models calling tf.self_adjoint_eig or tf.batch_self_adjoint_eig from Python will most likely break. Models saved as serialized graphs should be unaffected.\n\nThis CL also cleans up the python shape inference functions in linalg_ops.py a bit.\n\n# Change: 129390113\n\nSo you code will work as you expect with the next release.\n", "Adding @rmlarsen for further comments.\n", "Thanks!  Any idea when the next release will be?\n", "BTW, it may be available under nightlies --\nhttps://github.com/tensorflow/tensorflow#installation\n\nOn Tue, Aug 16, 2016 at 1:06 PM, Shawn Henry notifications@github.com\nwrote:\n\n> Thanks! Any idea when the next release will be?\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/3854#issuecomment-240221340,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AABaHBcyouv4IT-Hk0pylUo2QbbSVbj8ks5qghgogaJpZM4Jlwwv\n> .\n", "It is.  Thanks for the suggestion!\n", "Sounds like it's resolved? I will close the issue then. Thanks!\n"]}, {"number": 3853, "title": "Options for sparse gradients?", "body": "Does `compute_gradients()` support outputs in sparse form? I'm working on a large-scale linear model with sparse inputs. It's quite inefficient to use gradients as dense `Tensor`, especially in distributed training.\n\nIt seems that `apply_gradients()` of some optimisers supports gradient inputs as `IndexedSlices` type. But currently I've not found a clear way to convert gradients into this form. \n\nI've try sth like below to get X indices from mini-batch inputs, and then feed into IndexedSlices with gradient slices. But it doesn't seem to be good because `tf.unique` does not make `x_indices` sorted.\n\n```\nnew_grads_and_vars = []\nx_indices, _ = tf.unique(tf.reshape(tf.slice(X.indices, [0, 1], [-1, 1]), shape=[-1]))  # <- not sorted\nfor grad, var in grads_and_vars:\n   grad_vals = tf.gather(grad, indices=x_indices)\n   grad_slice = tf.IndexedSlices(indices=x_indices, values=grad_vals)\n   new_grads_and_vars.append((grad_slice, var))\ngrads_and_vars = new_grads_and_vars\n```\n\nAny ideas? Thx.\n", "comments": ["@concretevitamin please comment.\n", "Using `tf.IndexedSlices` in optimizers is already a pattern to handle sparse gradients -- for example, see the gradient of `tf.gather()`.  I suspect the issue with `tf.unique()` not sorting can be resolved by either (1) using some other combination of existing ops, or (2) writing your own op/kernel to handle the desired.\n\nClosing this for now.  If you have further questions (not bugs), I suspect StackOverflow will yield more promptly answers.\n", "@concretevitamin Does the indices in IndexSlices need to be sorted when apply gradients? "]}, {"number": 3852, "title": "cifar10_eval.py Error", "body": "python cifar10_eval.py\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX TITAN X\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\npciBusID 0000:01:00.0\nTotal memory: 12.00GiB\nFree memory: 281.36MiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:839] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:01:00.0)\nW tensorflow/core/common_runtime/bfc_allocator.cc:213] Ran out of memory trying to allocate 68.02MiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\n", "comments": ["281MB is probably not enough to run CIFAR10.\n\nCould you please run nvidia-smi to see what other binaries might be using up memory?\n\nSherry\n", "Thanks sherry. Here is what got and maybe I need to suspend the training  biniaries some how. Any suggestions?\n+------------------------------------------------------+  \n| NVIDIA-SMI 352.63     Driver Version: 352.63         |  \n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce GTX TIT...  Off  | 0000:01:00.0      On |                  N/A |\n| 28%   68C    P2    79W / 250W |  11891MiB / 12284MiB |      1%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0      1189    G   /usr/bin/X                                     178MiB |\n|    0      2090    G   compiz                                         123MiB |\n|    0      2313    G   /usr/lib/firefox/firefox                         2MiB |\n|    0      2586    C   python                                         111MiB |\n|    0      2686    C   .../dan/anaconda2/envs/tensorflow/bin/python   111MiB |\n|    0     18361    G   itksnap                                          8MiB |\n|    0     26483    G   MATLAB                                          63MiB |\n|    0     32609    C   python                                       11257MiB |\n+-----------------------------------------------------------------------------+\n\ufffd\n", "Maybe close a few of those applications before trying to run cifar?\n", "after closing, some of the apps works fine but strange error\n\npciBusID 0000:01:00.0\nTotal memory: 12.00GiB\nFree memory: 11.49GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:839] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:01:00.0)\n2016-08-16 20:54:12.681518: precision @ 1 = 0.866\nE tensorflow/core/client/tensor_c_api.cc:485] Enqueue operation was cancelled\n     [[Node: input_producer/input_producer_EnqueueMany = QueueEnqueueMany[Tcomponents=[DT_STRING], _class=[\"loc:@input_producer\"], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_producer, input_producer/RandomShuffle)]]\u0000\n", "The observed message is expected when we close the queue. Is eval working for you?\n", "Yes. Thank you\n"]}, {"number": 3851, "title": "CSV decode error", "body": "I am using Tensorflow 0.9.0. I have CUDA Driver Version = 7.5 and CUDNN 4 on Ubuntu 14.04\nThis maybe related to another issue I found [here](https://github.com/tensorflow/tensorflow/issues/633)\n\nI have a simple csv file which has a single line like this\n\n```\n\"field with\nnewline\",0\n```\n\nwhere the newline has been added by pressing Enter key in vim on Ubuntu.\nI am able to read this file in pandas using the read_csv function where the text field is shown as containing a single \\n character.\n\nBut when I try to read it in tensorflow, I get the following error:\n`tensorflow.python.framework.errors.InvalidArgumentError: Quoted field has to end with quote followed by delim or end`\n\nMy tensorflow code to read the csv uses this function to read a single row.\n\n```\ndef read_single_example(filename_queue, skip_header_lines, record_defaults, feature_index, label_index):\n    reader = tf.TextLineReader(skip_header_lines=skip_header_lines)\n    key, value = reader.read(filename_queue)\n    record = tf.decode_csv(\n            value,\n            record_defaults=record_defaults)\n    features, label = record[feature_index], record[label_index]\n    return features, label\n```\n\nIf I read using pandas and replace all newlines with spaces, the tensorflow code is able to parse the csv successfully.\n\nBut it will be really helpful if newlines can be handled within the Tensorflow CSV pipeline itself.\n", "comments": ["I tried this on Tensorflow 0.10 and the issue is still there. Please advise on possible ways to solve the problem.\n", "@jmchen-g, it looks like you wrote the CSVReader original. It seems like it is possible that each row of a csv may need to consume an arbitrary number of lines, because embedded unescaped newlines are allowed in the format.  https://tools.ietf.org/html/rfc4180.  Section 2 talks about CRLF's even being allowed.\n", "Isn't the use of TextLineReader the problem?\n", "Good point... to expand on @vrv's suggestion... He pointed out ot me that maybe WholeFileReader could possibly work. Can you try that and let us know if that solves your problem?\n", "I tried this in Tensorflow 0.10. I changed to use WholeFileReader instead of TextLineReader.\nI get the following error `tensorflow.python.framework.errors.InvalidArgumentError: Unquoted fields cannot have quotes/CRLFs inside`\n\nAlso I have another question about using a WholeFileReader. If the input csv file is large wouldn't WholeFileReader bring all of the data into memory, causing an out of memory error? \nAlso there would be no way to skip header lines if I use a WholeFileReader right?\n", "Yeah WholeFileReader is not a great solution for large files, but it at least allowed testing whether the CSVReader was robust to CRLFs. In any case it seems like a bug in the csv reader. Assigning to @jmchen-g.\n", "This is not a bug but intended behavior. In decode_csv, the input is expected to be a tensor of string/record, each element string is expected to be a record. Inside the record, it is already supported that you can have '\\n' in a string field. It is expected that some pre-processing is done to satisfy this requirement. \n\nIf we don't have such constraint, the example below will have a problem to decide if 5 is from the second or third record:\n\"  \n    1,2,3, (first record)\n    ,4,  (second)\n    5,6,7 (third)\n\"\n", "@jmchen-g, That reasoning  makes sense from a simplicity of implementation stand point, but then we are technically not adhering to the RFC.\nhttps://tools.ietf.org/html/rfc4180\nSection 2  item 6 specifically mentions this case. If we aren't going to change it, I suggest we just add this deviation from the standard in the documentation to make it easier for users to know to do the preprocessing. It is also true that CSV is a very de facto standard anyways and there are many variants.\n", "hi @aselle and @jmchen-g , I think I understand that if some preprocessing is done to extract records from the file and then the record is fed to decode_csv it will work.\n\nI didn't understand why there is ambiguity in the example you have provided. I was thinking a \\n marks end of record unless it is inside a quoted field. Is that not true? If it is true, then there are three records in your example as there is no quoting. \n\nCurrently there is no CSVReader which can read a single csv record for decode_csv. I think all that is required is to wrap TextLineReader, and read till a \\n, determine if we are within a quoted field. Will counting number of double quotes and seeing if it is odd suffice to validate we are inside a quoted field? \n\nThen if we are inside quoted field read more lines until number of quotes becomes even. When that happens return the record.\n\nI don't know how to write this class and replace TextLineReader with this. Can you guide me to write it?\n", "@aselle We already allow section 2 item 6 case in the current implementation. The only issue here is that the current op expect a tensor of records, each of which is a record. \n\nYou will need to either preprocess the csv to generate the tensor of records or have a new op that takes the whole file and process on the fly. It is better to just convert it directly instead of composing tensor of records and do another traversal of it in this op.\n", "I hit this issue too. I was hoping to be able to use the in-graph loading of data to simplify a multi-GPU pipeline.\n\nIf `decode_csv()` really needs just one record then the reader needs to itself do some parsing of the CSV just to see when a record ends, which seems a bit counter to the current separation of reader/decoder. Using `WholeFileReader` would work for me (I can just split up the inputs into multiple files) and I was thinking of having a go making `decode_csv()` handle multiple input/output records.\n\nAn alternative would be to have something like a `CSVLineReader` that just has a simple \"am I inside a quoted field\" check and is otherwise like `TextLineReader`. Sounds from the comments above that this might be a more welcome addition?\n", "@darrengarvey I suppose you could go ahead and have a `CSVLineReader`. Note that this will complicate things when we allow splittable files. ", "@drpngx - Can you give any more information on what you mean by \"splittable files\"?\r\n\r\nIn the end I've been going more in the direction of writing (a much more complicated) input function that feeds this data into a tf.learn `Estimator` using multiple threads in Python that feed data into a queue. This is basically done but seemingly very complicated to get right.", "\"Splittable file\" is hadoop parlance for files that can be split for parallel processing. An example of that is a simple file, say, that has fixed-length records. Each thread can skip to an offset and read from there. An example of a non-skippable file is stream-compressed file.", "I experienced this error when moving my code to windows ( running tensorflow in Anaconda3). In my case, I also work with csv data created by myself.  However, the error occurs only when I create the csv data running my code in Windows, so it is definitely related to the way the OS ends each line of the file. The question is, what extra consideration should I make in order to create the file  in Windows that does not have this problem?  To create the file  I use the csv library available in python.", "@slothkong please ask this question on stackoverflow.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Came to this issue by way of an [old stackoverflow question](https://stackoverflow.com/questions/38996904/tensorflow-csv-decode-error/51883105#51883105). For the interested parties, this issue has been fixed in tf.data with [`tf.contrib.data.CsvDataset` ](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/data/python/ops/readers.py#L515).", "Here's the updated link for [`tf.data.experimental.CsvDataset`](https://www.tensorflow.org/api_docs/python/tf/data/experimental/CsvDataset)\r\n\r\nbecause [`tf.io.decode_csv`](https://www.tensorflow.org/api_docs/python/tf/io/decode_csv) sometimes will not work at all."]}, {"number": 3850, "title": "real gradients of complex-valued functions", "body": "I am experiencing _unexpected_ behaviour when taking gradients of a complex-valued function parametrised with real variables. It looks like the gradient with respect to the complex part of the function is (erroneously) zero.\n\nExample:\n\n```\n> x = tf.constant(0.0)    # real-valued parameters of a complex-valued function\n> y = tf.constant(0.0)\n> z = tf.complex(x, y)    # the complex-valued function\n\n> session.run(tf.gradients(z, x), {x:1.0, y:2.0})    # value of x and y don't matter\n[1.0]    # good, expected\n> session.run(tf.gradients(z, y), {x:1.0, y:2.0})\n[0.0]    # bad, expecting 1j\n```\n\nIs the result being silently cast to real before being returned, or am I misusing `tf.gradients` somehow?\n\nUsing tensorflow version 0.9.0 with GPU on RedHat 4.8.6-4.\n", "comments": ["This is expected behavior.  TensorFlow gradients are always mathematically of the form `dL/dx`, where `L` is a scalar _real_ loss tensor.  If you ask for the gradient of something (`z` in this case) that isn't already scalar real, it assumes the gradients `dL/dz` are 1.  Since the imaginary part of 1 is zero, the chain rule gives zero in your second case.\n\nNote that scalar and real here are essentially the same restriction.\n", "I've opened an issue on this some time ago here: https://github.com/tensorflow/tensorflow/issues/2818.\nMaybe it would be a good idea to not allow taking the derivative of a complex expression, to avoid confusion?\n"]}, {"number": 3849, "title": "I like add some functions in deep MNIST but I get the error as tensorflow.python.pywrap_tensorflow.StatusNotOK: Invalid argument: Node 'multiplelayer error/initial_value': Node name contains invalid characters [[Node: multiplelayer error = Variablecontainer=\"\", dtype=DT_INT32, shape=[], shared_name=\"\"]]?", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System:\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n2. The output of `bazel version`\n### Steps to reproduce\n\n1.\n2.\n3.\n### What have you tried?\n\n1.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n", "comments": ["Could you please follow the above instruction when submitting a bug report?\n", "Automatically closing due to lack of recent activity. Please open a new issue that follows the template if your problem persists.\n"]}, {"number": 3848, "title": "Using epochs in tf.train.slice_input_producer gives uninitialized value when combined with batching", "body": "### Environment info\n\nMacOS X 10.11.4 no CUDA\nTensorflow 0.10.0rc0, installed precompiled pip\n### Problem\n\nI have my test data saved as `XXX_input.png` and `XXX_cb.png` (for label) and tried to use the\npython code below to create the batched input tensors:\n\n``` python\ndef _load_png_file(path, name):\n    file_content = tf.read_file(path)\n    png = tf.image.decode_png(file_content, channels=3)\n    png = tf.slice(png, [0, 0, 0], [ 128, 128, 3])\n    return tf.image.convert_image_dtype(png, dtype=tf.float32, name=name)\n\ndef load_data(path):\n    if not tf.gfile.Exists(path):\n        raise ValueError(\"Datadir {} don't exists!\".format(path))\n    photo_files = []                                                                \n    cb_files = []\n    for file_name in os.listdir(path):                                                  \n        if file_name.endswith(\"_input.png\"):\n            file_path = os.path.join(path, file_name)                                        \n            photo_files.append(file_path)\n            cb_files.append(file_path.replace(\"_input.png\", \"_cb.png\"))\n\n    photos = tf.convert_to_tensor(photo_files, dtype=tf.string)\n    cbs = tf.convert_to_tensor(cb_files, dtype=tf.string)\n    input_queue = tf.train.slice_input_producer([photos, cbs],          \n            num_epochs=FLAGS.num_epochs, # <-- delete this line to make it work\n            shuffle=True)\n\n    photo = _load_png_file(input_queue[0], \"photo\")\n    cb = _load_png_file(input_queue[1], \"cb\")\n    capacity = FLAGS.min_after_dequeue + 3 * FLAGS.batch_size                       \n    photo_batch, cb_batch = tf.train.batch([photo, cb],\n            capacity=capacity,                                                             \n            batch_size=FLAGS.batch_size)\n    return photo_batch, cb_batch\n```\n\nWhen I try to use it it will generate an error as below. When I remove the `num_epochs` argument to `tf.train.slice_input_producer` everything works as expected (net converge etc) so I don't there is something wrong with the rest of the code.\n\n```\nE tensorflow/core/client/tensor_c_api.cc:485] Attempting to use uninit[221/1918]\nlue input_producer/input_producer/fraction_of_32_full/limit_epochs/epochs\n         [[Node: input_producer/input_producer/fraction_of_32_full/limit_epochs/\nCountUpTo = CountUpTo[T=DT_INT64, _class=[\"loc:@input_producer/input_producer/fraction_of_32_full/limit_epochs/epochs\"], limit=2, _device=\"/job:localhost/replic\na:0/task:0/cpu:0\"](input_producer/input_producer/fraction_of_32_full/limit_epochs/epochs)]]\nERROR:tensorflow:Exception in QueueRunner: Attempting to use uninitialized value\n input_producer/input_producer/fraction_of_32_full/limit_epochs/epochs\n         [[Node: input_producer/input_producer/fraction_of_32_full/limit_epochs/CountUpTo = CountUpTo[T=DT_INT64, _class=[\"loc:@input_producer/input_producer/fr\naction_of_32_full/limit_epochs/epochs\"], limit=2, _device=\"/job:localhost/replic\na:0/task:0/cpu:0\"](input_producer/input_producer/fraction_of_32_full/limit_epoch\ns/epochs)]]\nCaused by op u'input_producer/input_producer/fraction_of_32_full/limit_epochs/Co\nuntUpTo', defined at:\n  File \"train.py\", line 94, in <module>\n    tf.app.run()\n  File \"/Users/kalle/Development/tensorflow_cbnet/.env/lib/python2.7/site-packag\nes/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"train.py\", line 91, in main\n    train()\n  File \"train.py\", line 30, in train\n    x, y = cbnet.load_data(\"/Users/kalle/Development/nn_data/train\")\n  File \"/Users/kalle/Development/tensorflow_cbnet/cbnet.py\", line 42, in load_data\n    shuffle=True)\n  File \"/Users/kalle/Development/tensorflow_cbnet/.env/lib/python2.7/site-packag\nes/tensorflow/python/training/input.py\", line 266, in slice_input_producer\n    shared_name=shared_name)\n  File \"/Users/kalle/Development/tensorflow_cbnet/.env/lib/python2.7/site-packages/tensorflow/python/training/input.py\", line 223, in range_input_producer\n    shared_name, name, \"fraction_of_%d_full\" % capacity)\n  File \"/Users/kalle/Development/tensorflow_cbnet/.env/lib/python2.7/site-packag\nes/tensorflow/python/training/input.py\", line 133, in input_producer\n    input_tensor = limit_epochs(input_tensor, num_epochs)\n  File \"/Users/kalle/Development/tensorflow_cbnet/.env/lib/python2.7/site-packages/tensorflow/python/training/input.py\", line 84, in limit_epochs\n    counter = epochs.count_up_to(num_epochs)\n  File \"/Users/kalle/Development/tensorflow_cbnet/.env/lib/python2.7/site-packag\nes/tensorflow/python/ops/variables.py\", line 577, in count_up_to\n    return state_ops.count_up_to(self._variable, limit=limit)\n  File \"/Users/kalle/Development/tensorflow_cbnet/.env/lib/python2.7/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 127, in count_up_to\n    result = _op_def_lib.apply_op(\"CountUpTo\", ref=ref, limit=limit, name=name)\n  File \"/Users/kalle/Development/tensorflow_cbnet/.env/lib/python2.7/site-packag\nes/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/Users/kalle/Development/tensorflow_cbnet/.env/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2310, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/kalle/Development/tensorflow_cbnet/.env/lib/python2.7/site-packag\nes/tensorflow/python/framework/ops.py\", line 1232, in __init__\n    self._traceback = _extract_stack()\n```\n", "comments": ["You're probably missing a `session.run(tf.initialize_local_variables())`.\n", "No, I'm doing that. As I said, I can train the net if I remove the `num_epochs` argument to `tf.train.slice_input_producer` and handle the epochs \"manually\" in the train loop.\n", "The error says that \"input_producer/input_producer/\nfraction_of_32_full/limit_epochs/epochs\" has not been initialized, which\nshould be initialized by running \"tf.initialize_local_variables()\"\n\nCan you check that this variable is present the local variables collection (\n[v.name for v in tf.get_collection(tf.GraphKeys.LOCAL_VARIABLES)]) or the\nregular collection ([v.name for v in\ntf.get_collection(tf.GraphKeys.LOCAL_VARIABLES)])\n\nOn Tue, Aug 16, 2016 at 11:40 AM, Kalle Svensson notifications@github.com\nwrote:\n\n> No, I'm doing that. As I said, I can train the net if I remove the\n> num_epochs argument to tf.train.slice_input_producer and handle the\n> epochs \"manually\" in the train loop.\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/3848#issuecomment-240142503,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AABaHLXW1atOgaA5RE9drINF1Ls_skbiks5qgdnqgaJpZM4JlVJg\n> .\n", "@kalle Note that `initialize_*local*_variables` is not included in `initialize_all_variables`. If you remove the `num_epochs` argument, your graph shouldn't have any local variables, which should be why it works in this case. If you are really running `initialize_local_variables`, then something else seems to be going on.\n", "Aha, @fwalch was right. I thought all variables were initialized when I ran `initialize_all_variables()` so I missed the local initialization. My bad.\n\nJust curious. Under what circumstance do you _not_ want to initialize the local variables? \n", "> Just curious. Under what circumstance do you not want to initialize the local variables?\n\nI'm not sure, but maybe when the training was initiated, but interrupted for some reason; and now you want to restart from the point you left instead of going back to epoch 1..?\n", "@lucasdavid Thanks for chiming in. But if you want to continue the training you wouldn't run `initialize_all_variables()` either?  :-)\n\nI think TensorFlow should have three functions:\n- `initialize_all_variables()` that really took care of _all_ variables.\n- `initialize_global_variables()` and `initialize_local_variables()` that did the more specific initializations.\n\nIf you are doing something clever/unusual that only require initialization of one kind of variable you are probably pretty skilled with TF and would know there are two different kind of variables.\n", "Okay, maybe my example wasn't really a good. I agree that a function called `initialize_all_variables` which does not initialize all variables seems counter intuitive, but there's probably a reason for that. \n\nAlso, following your organization (in which `all_variables` means all variables), you could easily achieve any initialization without the \"global_variables\" collection:\n\n``` py\n    with tf.Session() as s:\n        # All of them.\n        s.run(tf.initialize_all_variables())\n\n        # Local ones.\n        s.run(tf.initialize_local_variables())\n\n        # \"Global\" ones.\n        global_variables = set(tf.all_variables()) -set(tf.local_variables())\n        s.run(tf.initialize_variables(global_variables))\n```\n", "I ran into the same issue as @kalle, where setting num_epochs gave me the \"Attempting to use un-initialized value\" error. And like @kalle, I was initializing all variables like I am supposed to. \n\nTo fix it, I tried running tf.initialize_local_variables, but with num_epochs set, I still get the same error. I even tried including the global variable initialization, if only to cover my bases, and that didn't help either.\n\nI can control the number of epochs through a For loop and catch the exception using Try-Except when the loop ends. But it would be more elegant to not have to catch that exception in the first place.\n", "+1 Seeing the same problem with a very simple test program:\n\n```\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\n\nN_EXAMPLES = 100\nBATCH_SIZE = 4\nN_EPOCHS = 3\n\nsteps_per_epoch = N_EXAMPLES / BATCH_SIZE\n\nwith tf.Graph().as_default():\n    init_op = tf.group(tf.initialize_all_variables(), tf.initialize_local_variables())\n\n    a = tf.convert_to_tensor(np.arange(N_EXAMPLES))\n    b = tf.convert_to_tensor(np.arange(N_EXAMPLES))\n\n    aa, bb = tf.train.slice_input_producer([a, b], shuffle=True, seed=1, num_epochs=N_EPOCHS)\n    batch = tf.train.batch([aa, bb], batch_size=BATCH_SIZE)\n\n    s = tf.Session()\n\n    s.run(init_op)\n\n    coord = tf.train.Coordinator()\n    threads = tf.train.start_queue_runners(sess=s, coord=coord)\n\n    try:\n        step = 0\n        epoch = 0\n        batch_within_epoch = 0\n        while not coord.should_stop() and epoch < N_EPOCHS:\n            xa, xb = s.run(batch)\n            step += 1\n            print(str(step), 'aa=', str(xa), 'bb=', str(xb))\n            batch_within_epoch += 1\n            if batch_within_epoch % steps_per_epoch == 0:\n                print('------')\n                epoch += 1\n                batch_within_epoch = 0\n    except tf.errors.OutOfRangeError:\n        print('out of range')\n    finally:\n        coord.request_stop()\n        print('requesting stop')\n```\n\nWorks fine if the `num_epochs` parameter to `slice_input_producer` is removed.\n", "@rgobbel I think you just need to move the creation of the init ops below the creation of `slice_input_producer` to fix that.\n", "Thanks! That fixed it. Turns out my real model has other issues, based on my obviously incorrect understanding of how queues and batches work. \n"]}, {"number": 3847, "title": "Java compilation in rule '//tensorflow/examples/android:tensorflow_demo' failed", "body": "### Issue\n\n`$ bazel build //tensorflow/examples/android:tensorflow_demo` fails with the following error:\n\n```\nERROR: /home/milan/tools/tensorflow/tensorflow/tensorflow/examples/android/BUILD:47:1: Java compilation in rule '//tensorflow/examples/android:tensorflow_demo' failed: java failed: error executing command external/local_jdk/bin/java -Xbootclasspath/p:external/bazel_tools/third_party/java/jdk/langtools/javac.jar -XX:+TieredCompilation '-XX:TieredStopAtLevel=1' -jar ... (remaining 2 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\ntensorflow/examples/android/src/org/tensorflow/demo/CameraActivity.java:50: error: method does not override or implement a method from a supertype\n\n```\n\n(Verbose output below)\n### Environment info\n\nOperating System: Ubuntu 14.04 LTS 64-bit\n\nInstalled version of CUDA and cuDNN: CUDA 7.5.18, CUDNN 5.0.5.  (Not essential for the issue)\n\nTF installed from source, commit hash f646559f5d7f15f2403c472d1b031a3fbc981fa1\n\nBazel version 0.3.1 installed from repository.\n\nAndroid settings in WORKSPACE:\n\n```\nandroid_sdk_repository(\n    name = \"androidsdk\",\n    api_level = 21,\n    build_tools_version = \"24.0.0\",\n    # Replace with path to Android SDK on your system\n    path = \"/home/milan/tools/Android/Sdk/\",\n)\n\nandroid_ndk_repository(\n    name=\"androidndk\",\n    path=\"/home/milan/tools/Android/Sdk/ndk-bundle/\",\n    api_level=21)\n```\n### Steps to reproduce\n1. (Build TF from branch r0.10)\n2. `bazel build //tensorflow/examples/android:tensorflow_demo --verbose_failures`\n### What have you tried?\n1. Originally, I had the same issue with Bazel 0.3.0. `bazel clean` and rebuilding didn't help.\n2. `sudo apt-get upgrade bazel` to Bazel 0.3.1, `bazel clean` and rebuilding didn't help.\n### Logs or other output that would be helpful\n\n```\n(tfsource) milan@stroj:~/tools/tensorflow/tensorflow$ bazel build //tensorflow/examples/android:tensorflow_demo --verbose_failures\nWARNING: Bazel Android NDK crosstools are based on Android NDK revision 11. The revision of the Android NDK given in android_ndk_repository rule 'androidndk' is '12.1.2977051'.\nWARNING: /home/milan/tools/tensorflow/tensorflow/tensorflow/core/BUILD:638:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/debug:debug_graph_utils.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\n .. a number of similar warnings, skipping ..\nWARNING: /home/milan/tools/tensorflow/tensorflow/tensorflow/core/BUILD:638:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_loss_util.h' directly. You should either move the file to this package or depend on an appropriate rule there.\nINFO: Found 1 target...\nERROR: /home/milan/tools/tensorflow/tensorflow/tensorflow/examples/android/BUILD:47:1: Java compilation in rule '//tensorflow/examples/android:tensorflow_demo' failed: java failed: error executing command \n  (cd /home/milan/.cache/bazel/_bazel_milan/2849cb88d02d6f4c5e9ffcff64d240c6/execroot/tensorflow && \\\n  exec env - \\\n    LC_CTYPE=en_US.UTF-8 \\\n  external/local_jdk/bin/java -Xbootclasspath/p:external/bazel_tools/third_party/java/jdk/langtools/javac.jar -XX:+TieredCompilation '-XX:TieredStopAtLevel=1' -jar external/bazel_tools/tools/jdk/JavaBuilder_deploy.jar @bazel-out/local-fastbuild/bin/tensorflow/examples/android/libtensorflow_demo.jar-2.params): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\ntensorflow/examples/android/src/org/tensorflow/demo/CameraActivity.java:50: error: method does not override or implement a method from a supertype\n  @Override\n  ^\ntensorflow/examples/android/src/org/tensorflow/demo/CameraActivity.java:66: error: cannot find symbol\n    if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.M) {\n                                                    ^\n  symbol:   variable M\n  location: class VERSION_CODES\ntensorflow/examples/android/src/org/tensorflow/demo/CameraActivity.java:67: error: cannot find symbol\n      return checkSelfPermission(PERMISSION_CAMERA) == PackageManager.PERMISSION_GRANTED && checkSelfPermission(PERMISSION_STORAGE) == PackageManager.PERMISSION_GRANTED;\n             ^\n  symbol:   method checkSelfPermission(String)\n  location: class CameraActivity\ntensorflow/examples/android/src/org/tensorflow/demo/CameraActivity.java:67: error: cannot find symbol\n      return checkSelfPermission(PERMISSION_CAMERA) == PackageManager.PERMISSION_GRANTED && checkSelfPermission(PERMISSION_STORAGE) == PackageManager.PERMISSION_GRANTED;\n                                                                                            ^\n  symbol:   method checkSelfPermission(String)\n  location: class CameraActivity\ntensorflow/examples/android/src/org/tensorflow/demo/CameraActivity.java:74: error: cannot find symbol\n    if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.M) {\n                                                    ^\n  symbol:   variable M\n  location: class VERSION_CODES\ntensorflow/examples/android/src/org/tensorflow/demo/CameraActivity.java:75: error: cannot find symbol\n      if (shouldShowRequestPermissionRationale(PERMISSION_CAMERA) || shouldShowRequestPermissionRationale(PERMISSION_STORAGE)) {\n          ^\n  symbol:   method shouldShowRequestPermissionRationale(String)\n  location: class CameraActivity\ntensorflow/examples/android/src/org/tensorflow/demo/CameraActivity.java:75: error: cannot find symbol\n      if (shouldShowRequestPermissionRationale(PERMISSION_CAMERA) || shouldShowRequestPermissionRationale(PERMISSION_STORAGE)) {\n                                                                     ^\n  symbol:   method shouldShowRequestPermissionRationale(String)\n  location: class CameraActivity\ntensorflow/examples/android/src/org/tensorflow/demo/CameraActivity.java:78: error: cannot find symbol\n      requestPermissions(new String[] {PERMISSION_CAMERA, PERMISSION_STORAGE}, PERMISSIONS_REQUEST);\n      ^\n  symbol:   method requestPermissions(String[],int)\n  location: class CameraActivity\n8 errors\nTarget //tensorflow/examples/android:tensorflow_demo failed to build\nINFO: Elapsed time: 3.327s, Critical Path: 2.99s\n```\n", "comments": ["@sulc You need to build at API level 23 for the added API permission calls, however the app will run on a level 21+ device by doing runtime detection.\n\nJust set:\n`api_level = 23,`\nin your WORKSPACE file.\n", "@andrewharp Could I use api_level 22 ? ", "@iamblue The app needs to be compiled with API 23+, but you can run it on an API 21+ device.\r\n\r\nIt is possible to modify it to use the camera 1 API (#8736) and remove the permission checks, in which case you could compile and run it on a lower API level."]}, {"number": 3846, "title": " 'module' object has no attribute 'sparse_column_with_keys'", "body": "i try to run the example  came with the linear model tutorial but i get the error below\n attributeerror :  'module' object has no attribute 'sparse_column_with_keys' in the\ngender = tf.contrib.layers.sparse_column_with_keys(column_name=\"gender\", keys=[\"female\",\"male\"])\n\nmy environmenent : i run tensorflow with docker on Windows 10\n\noutput when running:\n\nTraceback (most recent call last):\n  File \"tensor3.py\", line 54, in <module>\n    gender = tf.contrib.layers.sparse_column_with_keys(column_name=\"gender\", keys=[\"female\", \"male\"])\nAttributeError: 'module' object has no attribute 'sparse_column_with_keys'\n", "comments": ["@jokisa , can you let me know which TensorFlow docker image you are using?\n", "I use the grc.io/tensorflow/tensorflow :la test  image.\n\nLe 16 ao\u00fbt 2016 20:45, \"caisq\" notifications@github.com a \u00e9crit :\n\n> @jokisa , can you let me know which TensorFlow docker image you are using?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n", "@jokisa The tag you are using is pointing to an older version. Can you try this image instead? b.gcr.io/tensorflow/tensorflow:r0.9 \n", "hi,\n\nsorry for my late response . I was in a vacation.\nI have just try the image that you have suggested  .\n\nthings work fine now.\n\nthank you for your help\n\n2016-08-17 2:43 GMT+02:00 caisq notifications@github.com:\n\n> @jokisa https://github.com/jokisa The tag you are using is pointing to\n> an older version. Can you try this image instead? b.gcr.io/tensorflow/\n> tensorflow:r0.9\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/3846#issuecomment-240282206,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AD18WU8n5QlhqOMij8huH99vsJNP0ynLks5qglkygaJpZM4JlRa0\n> .\n"]}, {"number": 3845, "title": "Build fails Tensorflow 0.9, with cuda 8.0 on Ubuntu 16.04, ppc64le", "body": "Tensorflow 0.9, with cuda 8.0 fails to build on Ubuntu 16.04 linux ppc64le. Without cuda, it does build successfully. Is this NVIDIA driver issue? Or some eigen issue. Kindly help.\n\nThe error I get is as below -\nexternal/eigen_archive/eigen-eigen-d02e6a705c30/unsupported/Eigen/CXX11/src/Tensor/TensorBroadcasting.h(192): internal error: assertion failed at: \"/dvs/p4/build/sw/rel/gpu_drv/r361/r361_00/drivers/compiler/edg/EDG_4.10/src/folding.c\", line 9819\n\n1 catastrophic error detected in the compilation of \"/tmp/tmpxft_00020696_00000000-9_cwise_op_gpu_select.cu.compute_52.cpp1.ii\".\nCompilation aborted.\nAborted\nERROR: /home/nishidha/pkgbuild/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:973:1: output 'tensorflow/core/kernels/_objs/cwise_op_gpu/tensorflow/core/kernels/cwise_op_gpu_select.cu.pic.o' was not created.\nERROR: /home/nishidha/pkgbuild/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:973:1: not all outputs were created.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\n### Environment info\n\nOperating System: Ubuntu 16.04, ppc64le\nNVIDIA Driver: 361.78\n\nInstalled version of CUDA and cuDNN: Cuda 8.0 with cudnn 5.1\n$ ls -s /usr/local/cuda/lib64\ntotal 1239696\n 48080 libcublas_device.a        0 libcufftw.so.8.0            0 libnppc.so.8.0         20320 libnppig.so.8.0.35    10864 libnpps_static.a\n     0 libcublas.so            556 libcufftw.so.8.0.35       536 libnppc.so.8.0.35          0 libnppim.so               0 libnvblas.so\n     0 libcublas.so.8.0         48 libcufftw_static.a         28 libnppc_static.a           0 libnppim.so.8.0           0 libnvblas.so.8.0\n 37884 libcublas.so.8.0.35       0 libcuinj64.so               0 libnppial.so            4296 libnppim.so.8.0.35      584 libnvblas.so.8.0.35\n 43936 libcublas_static.a        0 libcuinj64.so.8.0           0 libnppial.so.8.0           0 libnppi.so                0 libnvgraph.so\n   552 libcudadevrt.a         5572 libcuinj64.so.8.0.35     9684 libnppial.so.8.0.35        0 libnppi.so.8.0            0 libnvgraph.so.8.0\n     0 libcudart.so             36 libculibos.a                0 libnppicc.so          105316 libnppi.so.8.0.35      4568 libnvgraph.so.8.0.35\n     0 libcudart.so.8.0          0 libcurand.so                0 libnppicc.so.8.0      131588 libnppi_static.a       6824 libnvgraph_static.a\n   464 libcudart.so.8.0.35       0 libcurand.so.8.0         3864 libnppicc.so.8.0.35        0 libnppist.so              0 libnvrtc-builtins.so\n   936 libcudart_static.a    57768 libcurand.so.8.0.35         0 libnppicom.so              0 libnppist.so.8.0          0 libnvrtc-builtins.so.8.0\n     4 libcudnn5.0.5         57820 libcurand_static.a          0 libnppicom.so.8.0      13860 libnppist.so.8.0.35    9400 libnvrtc-builtins.so.8.0.35\n     0 libcudnn.so               0 libcusolver.so           1104 libnppicom.so.8.0.35       0 libnppisu.so              0 libnvrtc.so\n     0 libcudnn.so.5             0 libcusolver.so.8.0          0 libnppidei.so              0 libnppisu.so.8.0          0 libnvrtc.so.8.0\n 76296 libcudnn.so.5.0.5     50916 libcusolver.so.8.0.35       0 libnppidei.so.8.0        528 libnppisu.so.8.0.35   19380 libnvrtc.so.8.0.34\n 67748 libcudnn_static.a     21528 libcusolver_static.a     6912 libnppidei.so.8.0.35       0 libnppitc.so              0 libnvToolsExt.so\n     0 libcufft.so               0 libcusparse.so              0 libnppif.so                0 libnppitc.so.8.0          0 libnvToolsExt.so.1\n     0 libcufft.so.8.0           0 libcusparse.so.8.0          0 libnppif.so.8.0         2916 libnppitc.so.8.0.35      44 libnvToolsExt.so.1.0.0\n143232 libcufft.so.8.0.35    42080 libcusparse.so.8.0.35   46544 libnppif.so.8.0.35         0 libnpps.so                4 stubs\n126244 libcufft_static.a     50268 libcusparse_static.a        0 libnppig.so                0 libnpps.so.8.0\n     0 libcufftw.so              0 libnppc.so                  0 libnppig.so.8.0         8564 libnpps.so.8.0.35\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`) - v0.9.0 (and cherry-pick two commits - 2c3385523f00ea7de97ec16a7c060b39b834a7f5 and ea9e00a630f91a459dd5858cb22e8cd1a666ba4e)\n2. The output of `bazel version` - 0.2.0\n### Steps to reproduce\n1. ./configure - Enable GPU support, Cloud Platform support, and rest all with default\n2. bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\n### What have you tried?\n1. Building without GPU support works.\n", "comments": ["This looks like a nvidia bug on powerpc.\n", "@sherrym , Thanks. Yes, it looks so. I tried by downgrading nvidia driver to 361.62 (cuda 8.0.27) and building Tensorflow 0.9, and it worked. Just one workaround is need to get rid of cuda 8 and gcc 5.4 compatibility issue. I'd to hand-edit /usr/local/cuda/include/host_config.h file to remove that #error that pops up if we've gcc greater 5.3. We've filed a NVIDIA bug for the same.\n", "Appears not to be a TF issue.  Please reopen if this is not the case.\n", "The same happens when building TensorFlow 1.0 on Tegra (Nvidia Jetson TX1) and cuda 8.0.34 (so for me, it's sort of related to #851). Patching away the `#error` as @npanpaliya described (I had to do this trick once with Caffe) did not help for me; in fact, some of the cuda sources do _seem_ to get compiled. I tried both the 1.0 tag as well as master for a while now and nothing seems to work.\r\n\r\nHere's the system info:\r\n\r\n```\r\nLinux tegra-ubuntu 3.10.96-tegra #1 SMP PREEMPT Wed Nov 9 19:42:57 PST 2016 aarch64 aarch64 aarch64 GNU/Linux\r\n\r\nDistributor ID: Ubuntu\r\nDescription:    Ubuntu 16.04.2 LTS\r\nRelease:        16.04\r\nCodename:       xenial\r\n```\r\n\r\nI started the build using\r\n\r\n```\r\nbazel build --verbose_failures --local_resources 2048,.5,1.0 --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\nAt one point it was complaining abount `cwise_op_gpu_select.cu.pic.o`, during the last try it apparently chose to fail at `softmax_op_gpu.cu.pic.o`:\r\n```\r\nINFO: From Compiling tensorflow/core/common_runtime/gpu/gpu_tracer.cc:\r\ntensorflow/core/common_runtime/gpu/gpu_tracer.cc:82:13: warning: 'const char* {anonymous}::getActivityOverheadKindString(CUpti_ActivityOverheadKind)' defined but not used [-Wunused-function]\r\n const char *getActivityOverheadKindString(CUpti_ActivityOverheadKind kind) {\r\n             ^\r\nINFO: From Compiling tensorflow/core/distributed_runtime/rpc/grpc_worker_service_impl.cc:\r\ntensorflow/core/distributed_runtime/rpc/grpc_worker_service_impl.cc: In function 'const char* tensorflow::GrpcWorkerMethodName(tensorflow::GrpcWorkerMethod)':\r\ntensorflow/core/distributed_runtime/rpc/grpc_worker_service_impl.cc:50:1: warning: control reaches end of non-void function [-Wreturn-type]\r\n }\r\n ^\r\nINFO: From Compiling tensorflow/core/debug/debug_io_utils.cc:\r\ntensorflow/core/debug/debug_io_utils.cc:306:15: warning: 'tensorflow::Status tensorflow::CloseDebugURL(const string&)' defined but not used [-Wunused-function]\r\n static Status CloseDebugURL(const string& debug_url) { return Status::OK(); }\r\n               ^\r\nINFO: From Compiling tensorflow/core/kernels/softmax_op_gpu.cu.cc:\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorBroadcasting.h(275): internal error: assertion failed at: \"/dvs/p4/build/sw/rel/gpu_drv/r361/r361_00/drivers/compiler/edg/EDG_4.10/src/folding.c\", line 9819\r\n\r\n\r\n1 catastrophic error detected in the compilation of \"/tmp/tmpxft_00003c5c_00000000-9_softmax_op_gpu.cu.compute_52.cpp1.ii\".\r\nCompilation aborted.\r\nAborted\r\nERROR: /mnt/tensorflow/tensorflow/core/kernels/BUILD:2531:1: output 'tensorflow/core/kernels/_objs/softmax_op_gpu/tensorflow/core/kernels/softmax_op_gpu.cu.pic.o' was not created.\r\nERROR: /mnt/tensorflow/tensorflow/core/kernels/BUILD:2531:1: not all outputs were created or valid.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 6367.284s, Critical Path: 5141.64s\r\n```"]}, {"number": 3844, "title": "Missing documentation: Distribution Dashboard", "body": "The new _Distributions Dashboard_ lacks written documentation:\n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorboard/README.md\n", "comments": ["We're considering removing the distribution dashboard entirely, because the histogram dashboard is a superior view of the same information. The new distribution dashboard is the same as the old histogram dashboard. \n\nWill try to decide this soon (ideally before r0.10).\n", "I'd like to throw in my two cents for _not_ removing the distribution dashboard. I think for examining changes over time, it remains superior to the histogram dashboard. If it were possible to restrict the temporal window in the histogram dashboard, i.e. a way to slide back and forth through the exposed temporal range, that would help a great deal, but as it is, the distribution dashboard provides something that the histogram dashboard does not.\n", "We're discussing adding a third \"rotation\" to the histogram dashboard, which would be pretty similar to the distribution dashboard. We won't take out the distribution dashboard unless we can solve the same use case in the histogram dashboard.\n", "Is this still current?", "Closing due to inactivity. Feel free to re-open if you would like us to look again.", "This issue still exists. \r\n\r\nThe distribution tab is still there and there is no documentation. And the above discussion just does not clarify how histograms are superior versions of gradient sparsities ??"]}, {"number": 3843, "title": "Can't invoke TF_NewSession with C API of tensorflow.", "body": "Operating System:Ubuntu 16.04 LTS\nSteps to reproduce\n1.bazel build  -c opt //tensorflow/c:c_api\n\n2.gcc -g c_api_test.cc -I. -L. ./libc_api.a ./libcore_cpu_internal.lo ./libframework_internal.lo ./libproto_text.a ./liblib_internal.a ./libprotos_all_cc.a ./libprotobuf.a ./liblib_proto_parsing.a ./libprotobuf_lite.a   ./libarray_grad.lo ./libarray_ops_op_lib.lo ./libcandidate_sampling_ops_op_lib.lo ./libcontrol_flow_ops_op_lib.lo  ./libctc_ops_op_lib.lo ./libdata_flow_ops_op_lib.lo ./libdirect_session_internal.lo ./libexample_parser_configuration.lo  ./libfunctional_grad.lo ./libfunctional_ops_op_lib.lo ./libfunction_ops_op_lib.lo ./libgpu_runtime.lo ./libimage_ops_op_lib.lo ./libio_ops_op_lib.lo ./liblinalg_ops_op_lib.lo ./liblogging_ops_op_lib.lo ./libmath_grad.lo ./libmath_ops_op_lib.lo ./libnn_grad.lo ./libnn_ops_op_lib.lo ./libno_op_op_lib.lo ./libparsing_ops_op_lib.lo ./librandom_ops_op_lib.lo ./libsendrecv_ops_op_lib.lo ./libsparse_ops_op_lib.lo ./libstate_ops_op_lib.lo ./libstring_ops_op_lib.lo ./libtraining_ops_op_lib.lo ./libuser_ops_op_lib.lo  -lstdc++ -lpthread -lm -lz -ldl -lgcc_s -lc\nthe static library 'lo' and 'a' above within the directory 'bazel-bin'\n\nc_api_test.cc src:\n# include \"./c_api.h\"\n# include<stdio.h>\n# include<string.h>\n\nint main(int argc, char _argv[]){\n  printf(\"---- -------start\");\n  TF_Status_ s = TF_NewStatus();\n  TF_SessionOptions\\* opt = TF_NewSessionOptions();\n  TF_Session\\* session = TF_NewSession(opt, s);\n  TF_DeleteSessionOptions(opt);\n  TF_CloseSession(session,s);\n  TF_DeleteStatus(s);\n\n  printf(\"---- -------end\");\n  return 1;\n}\n\nbut always get the wrong messages as below:\nE tensorflow/core/common_runtime/session.cc:69] Not found: No session factory registered for the given session options: {target: \"\" config: } Registered factories are {}.\n", "comments": ["Could you please attach the c_api.h file you use, and complete .cc file (the file as pasted clearly will not compile).\n\nThanks.\n", "c_api_test.cc src:\n#include \"./c_api.h\"\n#include<stdio.h>\n#include<string.h>\n\nint main(int argc, char _argv[]){\n  printf(\"---- -------start\");\n  TF_Status_ s = TF_NewStatus();\n  TF_SessionOptions\\* opt = TF_NewSessionOptions();\n  TF_Session\\* session = TF_NewSession(opt, s); \n  TF_DeleteSessionOptions(opt);\n  TF_CloseSession(session,s);\n  TF_DeleteStatus(s);\n\n  printf(\"---- -------end\");\n  return 1;\n}\n\nc_api.h file is within tensorflow/c directory :\n/\\* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n```\nhttp://www.apache.org/licenses/LICENSE-2.0\n```\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#ifndef TENSORFLOW_C_C_API_H_\n#define TENSORFLOW_C_C_API_H_\n\n#include <stddef.h>\n#include <stdint.h>\n\n// --------------------------------------------------------------------------\n// C API for TensorFlow.\n//\n// The API leans towards simplicity and uniformity instead of convenience\n// since most usage will be by language specific wrappers.\n//\n// Conventions:\n// \\* We use the prefix TF_ for everything in the API.\n// \\* Objects are always passed around as pointers to opaque structs\n//   and these structs are allocated/deallocated via the API.\n// \\* TF_Status holds error information.  It is an object type\n//   and therefore is passed around as a pointer to an opaque\n//   struct as mentioned above.\n// \\* Every call that has a TF_Status\\* argument clears it on success\n//   and fills it with error info on failure.\n//\n// Questions left to address:\n// \\* Might at some point need a way for callers to provide their own Env.\n// \\* Maybe add TF_TensorShape that encapsulates dimension info.\n//\n// Design decisions made:\n// \\* Backing store for tensor memory has an associated deallocation\n//   function.  This deallocation function will point to client code\n//   for tensors populated by the client.  So the client can do things\n//   like shadowing a numpy array.\n// \\* We do not provide TF_OK since it is not strictly necessary and we\n//   are not optimizing for convenience.\n// \\* We make assumption that one session has one graph.  This should be\n//   fine since we have the ability to run sub-graphs.\n// \\* We could allow NULL for some arguments (e.g., NULL options arg).\n//   However since convenience is not a primary goal, we don't do this.\n// \\* Devices are not in this API.  Instead, they are created/used internally\n//   and the API just provides high level controls over the number of\n//   devices of each type.\n\n#ifdef __cplusplus\nextern \"C\" {\n#endif\n\n// --------------------------------------------------------------------------\n// TF_DataType holds the type for a scalar value.  E.g., one slot in a tensor.\n// The enum values here are identical to corresponding values in types.proto.\ntypedef enum {\n  TF_FLOAT = 1,\n  TF_DOUBLE = 2,\n  TF_INT32 = 3,  // Int32 tensors are always in 'host' memory.\n  TF_UINT8 = 4,\n  TF_INT16 = 5,\n  TF_INT8 = 6,\n  TF_STRING = 7,\n  TF_COMPLEX64 = 8,  // Single-precision complex\n  TF_COMPLEX = 8,    // Old identifier kept for API backwards compatibility\n  TF_INT64 = 9,\n  TF_BOOL = 10,\n  TF_QINT8 = 11,     // Quantized int8\n  TF_QUINT8 = 12,    // Quantized uint8\n  TF_QINT32 = 13,    // Quantized int32\n  TF_BFLOAT16 = 14,  // Float32 truncated to 16 bits.  Only for cast ops.\n  TF_QINT16 = 15,    // Quantized int16\n  TF_QUINT16 = 16,   // Quantized uint16\n  TF_UINT16 = 17,\n  TF_COMPLEX128 = 18,  // Double-precision complex\n  TF_HALF = 19,\n} TF_DataType;\n\n// --------------------------------------------------------------------------\n// TF_Code holds an error code.  The enum values here are identical to\n// corresponding values in error_codes.proto.\ntypedef enum {\n  TF_OK = 0,\n  TF_CANCELLED = 1,\n  TF_UNKNOWN = 2,\n  TF_INVALID_ARGUMENT = 3,\n  TF_DEADLINE_EXCEEDED = 4,\n  TF_NOT_FOUND = 5,\n  TF_ALREADY_EXISTS = 6,\n  TF_PERMISSION_DENIED = 7,\n  TF_UNAUTHENTICATED = 16,\n  TF_RESOURCE_EXHAUSTED = 8,\n  TF_FAILED_PRECONDITION = 9,\n  TF_ABORTED = 10,\n  TF_OUT_OF_RANGE = 11,\n  TF_UNIMPLEMENTED = 12,\n  TF_INTERNAL = 13,\n  TF_UNAVAILABLE = 14,\n  TF_DATA_LOSS = 15,\n} TF_Code;\n\n// --------------------------------------------------------------------------\n// TF_Status holds error information.  It either has an OK code, or\n// else an error code with an associated error message.\ntypedef struct TF_Status TF_Status;\n\n// Return a new status object.\nextern TF_Status\\* TF_NewStatus();\n\n// Delete a previously created status object.\nextern void TF_DeleteStatus(TF_Status*);\n\n// Record <code, msg> in _s.  Any previous information is lost.\n// A common use is to clear a status: TF_SetStatus(s, TF_OK, \"\");\nextern void TF_SetStatus(TF_Status_ s, TF_Code code, const char\\* msg);\n\n// Return the code record in _s.\nextern TF_Code TF_GetCode(const TF_Status_ s);\n\n// Return a pointer to the (null-terminated) error message in _s.  The\n// return value points to memory that is only usable until the next\n// mutation to *s.  Always returns an empty string if TF_GetCode(s) is\n// TF_OK.\nextern const char_ TF_Message(const TF_Status\\* s);\n\n// --------------------------------------------------------------------------\n// TF_Buffer holds a pointer to a block of data and its associated length.\n// Typically, the data consists of a serialized protocol buffer, but other data\n// may also be held in a buffer.\n//\n// By default, TF_Buffer itself does not do any memory management of the\n// pointed-to block.  If need be, users of this struct should specify how to\n// deallocate the block by setting the `data_deallocator` function pointer.\ntypedef struct {\n  const void\\* data;\n  size_t length;\n  void (_data_deallocator)(void_ data, size_t length);\n} TF_Buffer;\n\n// Makes a copy of the input and sets an appropriate deallocator.  Useful for\n// passing in read-only, input protobufs.\nextern TF_Buffer\\* TF_NewBufferFromString(const void\\* proto, size_t proto_len);\n\n// Useful for passing _out_ a protobuf.\nextern TF_Buffer\\* TF_NewBuffer();\n\nextern void TF_DeleteBuffer(TF_Buffer*);\n\nextern TF_Buffer TF_GetBuffer(TF_Buffer\\* buffer);\n\n// --------------------------------------------------------------------------\n// TF_Tensor holds a multi-dimensional array of elements of a single data type.\n// For all types other than TF_STRING, the data buffer stores elements\n// in row major order.  E.g. if data is treated as a vector of TF_DataType:\n//\n//   element 0:   index (0, ..., 0)\n//   element 1:   index (0, ..., 1)\n//   ...\n//\n// The format for TF_STRING tensors is:\n//   start_offset: array[uint64]\n//   data:         byte[...]\n//\n//   String length is encoded (varint?) starting at data[start_offset[i]]\n//   String contents follow immediately after string length.\n\ntypedef struct TF_Tensor TF_Tensor;\n\n// Return a new tensor that holds the bytes data[0,len-1].\n//\n// The data will be deallocated by a subsequent call to TF_DeleteTensor via:\n//      (_deallocator)(data, len, deallocator_arg)\n// Clients must provide a custom deallocator function so they can pass in\n// memory managed by something like numpy.\nextern TF_Tensor_ TF_NewTensor(TF_DataType, const int64_t\\* dims, int num_dims,\n                               void\\* data, size_t len,\n                               void (_deallocator)(void_ data, size_t len,\n                                                   void\\* arg),\n                               void\\* deallocator_arg);\n\n// Destroy a tensor.\nextern void TF_DeleteTensor(TF_Tensor*);\n\n// Return the type of a tensor element.\nextern TF_DataType TF_TensorType(const TF_Tensor*);\n\n// Return the number of dimensions that the tensor has.\nextern int TF_NumDims(const TF_Tensor*);\n\n// Return the length of the tensor in the \"dim_index\" dimension.\n// REQUIRES: 0 <= dim_index < TF_NumDims(tensor)\nextern int64_t TF_Dim(const TF_Tensor\\* tensor, int dim_index);\n\n// Return the size of the underlying data in bytes.\nextern size_t TF_TensorByteSize(const TF_Tensor*);\n\n// Return a pointer to the underlying data buffer.\nextern void\\* TF_TensorData(const TF_Tensor*);\n\n// --------------------------------------------------------------------------\n// TF_SessionOptions holds options that can be passed during session creation.\ntypedef struct TF_SessionOptions TF_SessionOptions;\n\n// Return a new options object.\nextern TF_SessionOptions\\* TF_NewSessionOptions();\n\n// Set the target in TF_SessionOptions.options.\n// target can be empty, a single entry, or a comma separated list of entries.\n// Each entry is in one of the following formats :\n// \"local\"\n// ip:port\n// host:port\nextern void TF_SetTarget(TF_SessionOptions\\* options, const char\\* target);\n\n// Set the config in TF_SessionOptions.options.\n// config should be a serialized tensorflow.ConfigProto proto.\n// If config was not parsed successfully as a ConfigProto, record the\n// error information in _status.\nextern void TF_SetConfig(TF_SessionOptions_ options, const void\\* proto,\n                         size_t proto_len, TF_Status\\* status);\n\n// Destroy an options object.\nextern void TF_DeleteSessionOptions(TF_SessionOptions*);\n\n// TODO(jeff,sanjay):\n// - export functions to set Config fields\n\n// --------------------------------------------------------------------------\n// The new graph construction API, still under development.\n\n// Represents a computation graph.  Graphs may be shared between sessions.\n// Graphs are thread-safe when used as directed below.\ntypedef struct TF_Graph TF_Graph;\n\n// Return a new graph object.\nextern TF_Graph\\* TF_NewGraph();\n\n// Destroy an options object.  Graph will be deleted once no more\n// TFSessionWithGraph's are referencing it.\nextern void TF_DeleteGraph(TF_Graph*);\n\n// Node being built. The underlying graph must outlive this.\ntypedef struct TF_NodeDescription TF_NodeDescription;\n\n// Node that has been added to the graph. Valid until the graph is\n// deleted -- in particular adding a new node to the graph does not\n// invalidate old TF_Node\\* pointers.\ntypedef struct TF_Node TF_Node;\n\n// Represents a specific input or output of a node, e.g. to specify the\n// specific output to pass as an input to an op.\ntypedef struct TF_Port {\n  TF_Node\\* node;\n  int index;  // Specifies the index of the input or output within node.\n} TF_Port;\n\n// Node will only be added to _graph when TF_FinishNode() is called\n// (assuming TF_FinishNode() does not return an error).  *graph must\n// not be deleted until after TF_FinishNode() is called.\nextern TF_NodeDescription_ TF_NewNode(TF_Graph\\* graph, const char\\* op_type,\n                                      const char\\* node_name);\n\n// Specify the device for `desc`.  Defaults to empty, meaning unconstrained.\nextern void TF_SetDevice(TF_NodeDescription\\* desc, const char\\* device);\n\n// The calls to TF_AddInput and TF_AddInputList must match (in number,\n// order, and type) the op declaration.  For example, the \"Concat\" op\n// has registration:\n//   REGISTER_OP(\"Concat\")\n//       .Input(\"concat_dim: int32\")\n//       .Input(\"values: N \\* T\")\n//       .Output(\"output: T\")\n//       .Attr(\"N: int >= 2\")\n//       .Attr(\"T: type\");\n// that defines two inputs, \"concat_dim\" and \"values\" (in that order).\n// You must use TF_AddInput() for the first input (since it takes a\n// single tensor), and TF_AddInputList() for the second input (since\n// it takes a list, even if you were to pass a list with a single\n// tensor), as in:\n//   TF_NodeDescription\\* desc = TF_NewNode(graph, \"Concat\", \"c\");\n//   TF_Port concat_dim_input = {...};\n//   TF_AddInput(desc, concat_dim_input);\n//   TF_Port values_inputs[5] = {{...}, ..., {...}};\n//   TF_AddInputList(desc, 5, values_inputs);\n\n// For inputs that take a single tensor.\nextern void TF_AddInput(TF_NodeDescription\\* desc, TF_Port input);\n\n// For inputs that take a list of tensors.\n// inputs must point to TF_Port[num_inputs].\nextern void TF_AddInputList(TF_NodeDescription\\* desc, const TF_Port\\* inputs,\n                            int num_inputs);\n\n// Call once per control input to `desc`.\nextern void TF_AddControlInput(TF_NodeDescription\\* desc, TF_Node\\* input);\n\n// Call some TF_SetAttr*() function for every attr that is not\n// inferred from an input and doesn't have a default value you wish to\n// keep.\n\n// `value` must point to a string of length `length` bytes.\nextern void TF_SetAttrString(TF_NodeDescription\\* desc, const char\\* attr_name,\n                             const void\\* value, int length);\n// `values` and `lengths` both must have lengths `num_values`.\n// `values[i]` must point to a string of length `lengths[i]` bytes.\nextern void TF_SetAttrStringList(TF_NodeDescription\\* desc,\n                                 const char\\* attr_name,\n                                 const void\\* const\\* values, const int\\* lengths,\n                                 int num_values);\nextern void TF_SetAttrInt(TF_NodeDescription\\* desc, const char\\* attr_name,\n                          int64_t value);\nextern void TF_SetAttrIntList(TF_NodeDescription\\* desc, const char\\* attr_name,\n                              const int64_t\\* values, int num_values);\nextern void TF_SetAttrFloat(TF_NodeDescription\\* desc, const char\\* attr_name,\n                            float value);\nextern void TF_SetAttrFloatList(TF_NodeDescription\\* desc, const char\\* attr_name,\n                                const float\\* values, int num_values);\nextern void TF_SetAttrBool(TF_NodeDescription\\* desc, const char\\* attr_name,\n                           unsigned char value);\nextern void TF_SetAttrBoolList(TF_NodeDescription\\* desc, const char\\* attr_name,\n                               const unsigned char\\* values, int num_values);\nextern void TF_SetAttrType(TF_NodeDescription\\* desc, const char\\* attr_name,\n                           TF_DataType value);\nextern void TF_SetAttrTypeList(TF_NodeDescription\\* desc, const char\\* attr_name,\n                               const TF_DataType\\* values, int num_values);\n\n// Set `num_dims` to -1 to represent \"unknown rank\".  Otherwise,\n// `dims` points to an array of length `num_dims`.  `dims[i]` must be\n// >= -1, with -1 meaning \"unknown dimension\".\nextern void TF_SetAttrShape(TF_NodeDescription\\* desc, const char\\* attr_name,\n                            const int64_t\\* dims, int num_dims);\n// `dims` and `num_dims` must point to arrays of length `num_shapes`.\n// Set `num_dims[i]` to -1 to represent \"unknown rank\".  Otherwise,\n// `dims[i]` points to an array of length `num_dims[i]`.  `dims[i][j]`\n// must be >= -1, with -1 meaning \"unknown dimension\".\nextern void TF_SetAttrShapeList(TF_NodeDescription\\* desc, const char\\* attr_name,\n                                const int64_t\\* const\\* dims, const int\\* num_dims,\n                                int num_shapes);\n// `proto` must point to an array of `proto_len` bytes representing a\n// binary-serialized TensorShapeProto.\nextern void TF_SetAttrTensorShapeProto(TF_NodeDescription\\* desc,\n                                       const char\\* attr_name, void\\* proto,\n                                       int proto_len, TF_Status\\* status);\n// `protos` and `proto_lens` must point to arrays of length `num_shapes`.\n// `protos[i]` must point to an array of `proto_lens[i]` bytes\n// representing a binary-serialized TensorShapeProto.\nextern void TF_SetAttrTensorShapeProtoList(TF_NodeDescription\\* desc,\n                                           const char\\* attr_name,\n                                           const void\\* const\\* protos,\n                                           const int\\* proto_lens,\n                                           int num_shapes, TF_Status\\* status);\n\n// This functions takes ownership of _value (the\n// implementation will eventually call TF_DeleteTensor).\nextern void TF_SetAttrTensor(TF_NodeDescription_ desc, const char\\* attr_name,\n                             TF_Tensor\\* value, TF_Status\\* status);\n// This functions takes ownership of values[0]..values[num_values-1](the\n// implementation will eventually call TF_DeleteTensor on each).\nextern void TF_SetAttrTensorList(TF_NodeDescription\\* desc,\n                                 const char\\* attr_name,\n                                 TF_Tensor\\* const\\* values, int num_values,\n                                 TF_Status\\* status);\n\n// `proto` should point to a sequence of bytes of length `proto_len`\n// representing a binary serialization of an AttrValue protocol\n// buffer.\nextern void TF_SetAttrToAttrValueProto(TF_NodeDescription\\* desc,\n                                       const char\\* attr_name, const void\\* proto,\n                                       size_t proto_len, TF_Status\\* status);\n\n// If this function succeeds:\n//   \\* _status is set to an OK value,\n//   \\* a TF_Node is added to the graph,\n//   \\* a non-null value pointing to the added node is returned --\n//     this value is valid until the underlying graph is deleted.\n// Otherwise:\n//   \\* *status is set to a non-OK value,\n//   \\* the graph is not modified,\n//   \\* a null value is returned.\n// In either case, it deletes `desc`.\nextern TF_Node_ TF_FinishNode(TF_NodeDescription\\* desc, TF_Status\\* status);\n\n// TF_Node functions.  Nodes are immutable once created, so these are all\n// query functions.\n\nextern const char\\* TF_NodeName(TF_Node\\* node);\nextern const char\\* TF_NodeOpType(TF_Node\\* node);\nextern const char\\* TF_NodeDevice(TF_Node\\* node);\n\nextern int TF_NodeNumOutputs(TF_Node\\* node);\nextern TF_DataType TF_NodeOutputType(TF_Port node_out);\nextern int TF_NodeOutputListLength(TF_Node\\* node, const char\\* arg_name,\n                                   TF_Status\\* status);\n\nextern int TF_NodeNumInputs(TF_Node\\* node);\nextern TF_DataType TF_NodeInputType(TF_Port node_in);\nextern int TF_NodeInputListLength(TF_Node\\* node, const char\\* arg_name,\n                                  TF_Status\\* status);\n\n// In this code:\n//   TF_Port producer = TF_NodeInput(consumer);\n// There is an edge from producer.node's output (given by\n// producer.index) to consumer.node's input (given by consumer.index).\nextern TF_Port TF_NodeInput(TF_Port node_in);\n\n// Get the number of current consumers of a node's output.  Note that\n// this number can change when new nodes are added to the graph.\nextern int TF_NodeOutputNumConsumers(TF_Port node_out);\n\n// Get list of all current consumers of a node's output.  consumers\n// must point to an array of length at least max_consumers (ideally\n// set to TF_NodeOutputNumConsumer(node_out)).  Beware that a\n// concurrent modification of the graph can increase the number of\n// consumers of a node.  Returns the number of output consumers\n// (should match TF_NodeOutputNumConsumers(node_out)).\nextern int TF_NodeOutputConsumers(TF_Port node_out, TF_Port\\* consumers,\n                                  int max_consumers);\n\n// Get the number of control inputs to a node.\nextern int TF_NodeNumControlInputs(TF_Node\\* node);\n\n// Get list of all control inputs to a node.  control_inputs must\n// point to an array of length max_control_inputs (ideally set to\n// TF_NodeNumControlInputs(node)).  Returns the number of control\n// inputs (should match TF_NodeNumControlInputs(node)).\nextern int TF_NodeGetControlInputs(TF_Node\\* node, TF_Node*\\* control_inputs,\n                                   int max_control_inputs);\n\n// Get the number of nodes that have _node as a control inputs.\n// Note that this number can change when new nodes are added to the\n// graph.\nextern int TF_NodeNumControlOutputs(TF_Node_ node);\n\n// Get the list of nodes that have _node as a control input.\n// control_outputs must point to an array of length at least\n// max_control_outputs (ideally set to\n// TF_NodeNumControlOutputs(node)). Beware that a concurrent\n// modification of the graph can increase the number of control\n// outputs.  Returns the number of control outputs (should match\n// TF_NodeNumControlOutputs(node)).\nextern int TF_NodeGetControlOutputs(TF_Node_ node, TF_Node*\\* control_outputs,\n                                    int max_control_outputs);\n\n// Sets `output_attr_value` to the binary-serialized AttrValue proto\n// representation of the value of the `attr_name` attr of `node`.\nextern void TF_NodeGetAttrValueProto(TF_Node\\* node, const char\\* attr_name,\n                                     TF_Buffer\\* output_attr_value,\n                                     TF_Status\\* status);\n\n// Returns the node in the graph with `node_name`. Returns nullptr if\n// no node found.\nextern TF_Node\\* TF_GraphNodeByName(TF_Graph\\* graph, const char\\* node_name);\n\n// Iterate through the nodes of a graph.  To use:\n// size_t pos = 0;\n// TF_Node\\* node;\n// while ((node = TF_GraphNextNode(graph, &pos)) != nullptr) {\n//   DoSomethingWithNode(node);\n// }\nextern TF_Node\\* TF_GraphNextNode(TF_Graph\\* graph, size_t\\* pos);\n\n// Note: The following two functions may fail on very large protos in the\n// future.\n\nextern void TF_GraphToGraphDef(TF_Graph\\* graph, TF_Buffer\\* output_graph_def,\n                               TF_Status\\* status);\n\nextern void TF_NodeToNodeDef(TF_Node\\* node, TF_Buffer\\* output_node_def,\n                             TF_Status\\* status);\n\n// TODO(josh11b): Query attrs for a Node.\n\n// TODO(cwhipkey): Query shape for node outputs.\n\n// TODO(josh11b,mrry): Import GraphDef into TF_Graph.\n\n// TODO(andydavis): Function to add gradients to a graph.\n\n// TODO(josh11b): Register OpDef, available to all nodes added\n// to this graph.\n\n// The following two may both benefit from a subgraph-definition API\n// that re-uses most of the graph-definition API.\n// TODO(andydavis): Add functions to a graph.\n// TODO(yuanbyu): Add while loop to graph.\n\n// --------------------------------------------------------------------------\n// The new session API that uses TF_Graph*.  The intent is this will\n// replace the TF_ExtendGraph() API.\n\n// TODO(josh11b): Rename this TF_Session once we delete the old API.\ntypedef struct TF_SessionWithGraph TF_SessionWithGraph;\n\n// Return a new execution session with the associated graph, or NULL\n// on error.  _graph must be a valid graph (not deleted or nullptr).\n// This function will prevent the graph from being deleted until\n// TF_DeleteSessionWithGraph() is called.  Does not take ownership of opts.\n// TODO(josh11b): Rename this TF_NewSession() once we delete the old API.\nextern TF_SessionWithGraph_ TF_NewSessionWithGraph(\n    TF_Graph\\* graph, const TF_SessionOptions\\* opts, TF_Status\\* status);\n\n// Close a session. This contacts any other processes associated with this\n// session, if applicable. This may not be called after\n// TF_DeleteSessionWithGraph().\n// TODO(josh11b): Rename this TF_CloseSession() once we delete the old API.\nextern void TF_CloseSessionWithGraph(TF_SessionWithGraph_, TF_Status_ status);\n\n// Destroy a session object.  Even if error information is recorded in\n// _status, this call discards all local resources associated with the\n// session.  The session may not be used during or after this call\n// (and the session drops its reference to the corresponding graph).\n// TODO(josh11b): Rename this TF_DeleteSession() once we delete the old API.\nextern void TF_DeleteSessionWithGraph(TF_SessionWithGraph_, TF_Status\\* status);\n\n// See TF_Run() below.\nextern void TF_SessionRun(TF_SessionWithGraph\\* session,\n                          // RunOptions\n                          const TF_Buffer\\* run_options,\n                          // Input tensors\n                          const TF_Port\\* inputs, TF_Tensor\\* const\\* input_values,\n                          int ninputs,\n                          // Output tensors\n                          const TF_Port\\* outputs, TF_Tensor*\\* output_values,\n                          int noutputs,\n                          // Target nodes\n                          const TF_Node\\* const\\* target_nodes, int ntargets,\n                          // RunMetadata\n                          TF_Buffer\\* run_metadata,\n                          // Output status\n                          TF_Status*);\n\n// See TF_PRunSetup() below.\nextern void TF_SessionPRunSetup(TF_SessionWithGraph_,\n                                // Input names\n                                const TF_Port_ inputs, int ninputs,\n                                // Output names\n                                const TF_Port\\* outputs, int noutputs,\n                                // Target nodes\n                                const TF_Node\\* const\\* target_nodes,\n                                int ntargets,\n                                // Output handle\n                                const char*\\* handle,\n                                // Output status\n                                TF_Status*);\n\n// See TF_PRun() below.\nextern void TF_SessionPRun(TF_SessionWithGraph_, const char_ handle,\n                           // Input tensors\n                           const TF_Port\\* inputs,\n                           TF_Tensor\\* const\\* input_values, int ninputs,\n                           // Output tensors\n                           const TF_Port\\* outputs, TF_Tensor*\\* output_values,\n                           int noutputs,\n                           // Target nodes\n                           const TF_Node\\* const\\* target_nodes, int ntargets,\n                           // Output status\n                           TF_Status*);\n\n// --------------------------------------------------------------------------\n// The deprecated session API.  Please switch to the above instead of\n// TF_ExtendGraph().  TF_Session manages a single graph and execution.\n\ntypedef struct TF_Session TF_Session;\n\n// Return a new execution session, or NULL on error.\nextern TF_Session\\* TF_NewSession(const TF_SessionOptions_, TF_Status_ status);\n\n// Close a session.\nextern void TF_CloseSession(TF_Session_, TF_Status_ status);\n\n// Destroy a session.  Even if error information is recorded in _status,\n// this call discards all resources associated with the session.\nextern void TF_DeleteSession(TF_Session_, TF_Status\\* status);\n\n// Closes all existing sessions connected to the `target` specified in the\n// `SessionOptions`, and frees shared resources in `containers` on `target'.\n// If no containers are provided, all containers are cleared.\nextern void TF_Reset(const TF_SessionOptions\\* opt, const char*\\* containers,\n                     int ncontainers, TF_Status\\* status);\n\n// Treat the bytes proto[0,proto_len-1] as a serialized GraphDef and\n// add the nodes in that GraphDef to the graph for the session.\nextern void TF_ExtendGraph(TF_Session_, const void_ proto, size_t proto_len,\n                           TF_Status*);\n\n// Run the graph associated with the session starting with the\n// supplied inputs (inputs[0,ninputs-1]).  Regardless of success or\n// failure, inputs[] become the property of the implementation (the\n// implementation will eventually call TF_DeleteTensor on each input).\n//\n// Any NULL and non-NULL value combinations for (`run_options`,\n// `run_metadata`) are valid.\n//\n//    - `run_options` may be NULL, in which case it will be ignored; or\n//      non-NULL, in which case it must point to a `TF_Buffer` containing the\n//      serialized representation of a `RunOptions` protocol buffer.\n//    - `run_metadata` may be NULL, in which case it will be ignored; or\n//      non-NULL, in which case it must point to an empty, freshly allocated\n//      `TF_Buffer` that may be updated to contain the serialized representation\n//      of a `RunMetadata` protocol buffer.\n//\n// The caller retains the ownership of `run_options` and/or `run_metadata` (when\n// not NULL) and should manually call TF_DeleteBuffer on them.\n//\n// On success, the tensors corresponding to output_names[0,noutputs-1]\n// are placed in outputs[], and these outputs[] become the property\n// of the caller (the caller must eventually call TF_DeleteTensor on\n// them).\n//\n// On failure, outputs[] contains NULLs.\nextern void TF_Run(TF_Session_,\n                   // RunOptions\n                   const TF_Buffer_ run_options,\n                   // Input tensors\n                   const char*\\* input_names, TF_Tensor*\\* inputs, int ninputs,\n                   // Output tensors\n                   const char*\\* output_tensor_names, TF_Tensor*\\* outputs,\n                   int noutputs,\n                   // Target nodes\n                   const char*\\* target_node_names, int ntargets,\n                   // RunMetadata\n                   TF_Buffer\\* run_metadata,\n                   // Output status\n                   TF_Status*);\n\n// Set up the graph with the intended feeds and fetches for a sequence\n// of partial run calls.\n//\n// On success, returns a handle that is used for subsequent PRun calls.\n//\n// On failure, out_status contains a tensorflow::Status with an error\n// message.\n// NOTE: This is EXPERIMENTAL and subject to change.\nextern void TF_PRunSetup(TF_Session_,\n                         // Input names\n                         const char_\\* input_names, int ninputs,\n                         // Output names\n                         const char*\\* output_tensor_names, int noutputs,\n                         // Target nodes\n                         const char*\\* target_node_names, int ntargets,\n                         // Output handle\n                         const char*\\* handle,\n                         // Output status\n                         TF_Status*);\n\n// Continue to run the graph with additional feeds and fetches. The\n// execution state is uniquely identified by the handle.\n// NOTE: This is EXPERIMENTAL and subject to change.\nextern void TF_PRun(TF_Session_, const char_ handle,\n                    // Input tensors\n                    const char*\\* input_names, TF_Tensor*\\* inputs, int ninputs,\n                    // Output tensors\n                    const char*\\* output_tensor_names, TF_Tensor*\\* outputs,\n                    int noutputs,\n                    // Target nodes\n                    const char*\\* target_node_names, int ntargets,\n                    // Output status\n                    TF_Status*);\n\n// --------------------------------------------------------------------------\n// Load plugins containing custom ops and kernels\n\n// TF_Library holds information about dynamically loaded TensorFlow plugins.\ntypedef struct TF_Library TF_Library;\n\n// Load the library specified by library_filename and register the ops and\n// kernels present in that library.\n//\n// Pass \"library_filename\" to a platform-specific mechanism for dynamically\n// loading a library. The rules for determining the exact location of the\n// library are platform-specific and are not documented here.\n// Expects the symbols \"RegisterOps\", \"RegisterKernels\", and \"GetOpList\", to be\n// defined in the library.\n//\n// On success, place OK in status and return the newly created library handle.\n// The caller owns the library handle.\n//\n// On failure, place an error status in status and return NULL.\nextern TF_Library\\* TF_LoadLibrary(const char\\* library_filename,\n                                  TF_Status\\* status);\n\n// Get the OpList of OpDefs defined in the library pointed by lib_handle.\n//\n// Returns a TF_Buffer. The memory pointed to by the result is owned by\n// lib_handle. The data in the buffer will be the serialized OpList proto for\n// ops defined in the library.\nextern TF_Buffer TF_GetOpList(TF_Library\\* lib_handle);\n\n#ifdef __cplusplus\n} /\\* end extern \"C\" */\n#endif\n\n#endif  // TENSORFLOW_C_C_API_H_\n", "#include \"./c_api.h\"\n#include<stdio.h>\n#include<string.h>\n", "stdio.h\nstring.h\n", "There is a unit test for the c api in   tensorflow/c/c_api_test.cc\nmake sure you can run that e.g.\n\n$ bazel run tensorflow/c/c_api_test\n\nThen you can try copying the contents of that into your file one by one until you achieve what you are trying to do.  \n\nIn the reproduce instructions you could also just specify the git sha hash that you checked out from i.e.\n\ngit clone <tensorflow url>\ngit checkout <sha hash>\nbazel build ...\n\nP.S. when including large files, please use github's attach function (attach files by dragging and dropping in e.g. Chrome).\n", "Closing due to lack of recent activity. Please reopen if more information becomes available.\n"]}, {"number": 3842, "title": "can't deal with nii data using tensorflow ", "body": "Hi all\uff1a\n    I'd like to use tensorflow to resize nii data(This kind of data is using in medical for example ct fmri and so on).Any information is appreciate .\n", "comments": ["- http://nifti.nimh.nih.gov/nifti-1\n- https://www.tensorflow.org/versions/r0.10/how_tos/new_data_formats/index.html#custom-data-readers\n", "@hholst80  I don't know if another way to replace it. Actually, I'd like to use other python library to resize nii data,not tensorflow. But I don't if it's ok.\n", "Closing this, as this is not a feature request or enhancement request. Please ask this question on StackOverflow.\n"]}, {"number": 3841, "title": "This rule is missing dependency declarations error on build.", "body": "I am using Ubuntu 16.04 with Cuda 8.0.  I am building for pyhon3.5 with a GTX 1080 GPU.  I get the following error.\n\n```\nERROR: /home/chase/Desktop/tensorflow-master/tensorflow/core/kernels/BUILD:1529:1: undeclared inclusion(s) in rule '//tensorflow/core/kernels:depth_space_ops_gpu':\nthis rule is missing dependency declarations for the following files included by 'tensorflow/core/kernels/spacetodepth_op_gpu.cu.cc':\n  '/usr/local/cuda-8.0/include/cuda_runtime.h'\n  '/usr/local/cuda-8.0/include/host_config.h'\n  '/usr/local/cuda-8.0/include/builtin_types.h'\n  '/usr/local/cuda-8.0/include/device_types.h'\n  '/usr/local/cuda-8.0/include/host_defines.h'\n  '/usr/local/cuda-8.0/include/driver_types.h'\n  '/usr/local/cuda-8.0/include/surface_types.h'\n  '/usr/local/cuda-8.0/include/texture_types.h'\n  '/usr/local/cuda-8.0/include/vector_types.h'\n  '/usr/local/cuda-8.0/include/library_types.h'\n  '/usr/local/cuda-8.0/include/channel_descriptor.h'\n  '/usr/local/cuda-8.0/include/cuda_runtime_api.h'\n  '/usr/local/cuda-8.0/include/cuda_device_runtime_api.h'\n  '/usr/local/cuda-8.0/include/driver_functions.h'\n  '/usr/local/cuda-8.0/include/vector_functions.h'\n  '/usr/local/cuda-8.0/include/vector_functions.hpp'\n  '/usr/local/cuda-8.0/include/common_functions.h'\n  '/usr/local/cuda-8.0/include/math_functions.h'\n  '/usr/local/cuda-8.0/include/math_functions.hpp'\n  '/usr/local/cuda-8.0/include/math_functions_dbl_ptx3.h'\n  '/usr/local/cuda-8.0/include/math_functions_dbl_ptx3.hpp'\n  '/usr/local/cuda-8.0/include/cuda_surface_types.h'\n  '/usr/local/cuda-8.0/include/cuda_texture_types.h'\n  '/usr/local/cuda-8.0/include/device_functions.h'\n  '/usr/local/cuda-8.0/include/device_functions.hpp'\n  '/usr/local/cuda-8.0/include/device_atomic_functions.h'\n  '/usr/local/cuda-8.0/include/device_atomic_functions.hpp'\n  '/usr/local/cuda-8.0/include/device_double_functions.h'\n  '/usr/local/cuda-8.0/include/device_double_functions.hpp'\n  '/usr/local/cuda-8.0/include/sm_20_atomic_functions.h'\n  '/usr/local/cuda-8.0/include/sm_20_atomic_functions.hpp'\n  '/usr/local/cuda-8.0/include/sm_32_atomic_functions.h'\n  '/usr/local/cuda-8.0/include/sm_32_atomic_functions.hpp'\n  '/usr/local/cuda-8.0/include/sm_35_atomic_functions.h'\n  '/usr/local/cuda-8.0/include/sm_60_atomic_functions.h'\n  '/usr/local/cuda-8.0/include/sm_60_atomic_functions.hpp'\n  '/usr/local/cuda-8.0/include/sm_20_intrinsics.h'\n  '/usr/local/cuda-8.0/include/sm_20_intrinsics.hpp'\n  '/usr/local/cuda-8.0/include/sm_30_intrinsics.h'\n  '/usr/local/cuda-8.0/include/sm_30_intrinsics.hpp'\n  '/usr/local/cuda-8.0/include/sm_32_intrinsics.h'\n  '/usr/local/cuda-8.0/include/sm_32_intrinsics.hpp'\n  '/usr/local/cuda-8.0/include/sm_35_intrinsics.h'\n  '/usr/local/cuda-8.0/include/surface_functions.h'\n  '/usr/local/cuda-8.0/include/texture_fetch_functions.h'\n  '/usr/local/cuda-8.0/include/texture_indirect_functions.h'\n  '/usr/local/cuda-8.0/include/surface_indirect_functions.h'\n  '/usr/local/cuda-8.0/include/device_launch_parameters.h'\n  '/usr/local/cuda-8.0/include/cuda_fp16.h'\n  '/usr/local/cuda-8.0/include/math_constants.h'\n  '/usr/local/cuda-8.0/include/curand_kernel.h'\n  '/usr/local/cuda-8.0/include/curand.h'\n  '/usr/local/cuda-8.0/include/curand_discrete.h'\n  '/usr/local/cuda-8.0/include/curand_precalc.h'\n  '/usr/local/cuda-8.0/include/curand_mrg32k3a.h'\n  '/usr/local/cuda-8.0/include/curand_mtgp32_kernel.h'\n  '/usr/local/cuda-8.0/include/cuda.h'\n  '/usr/local/cuda-8.0/include/curand_mtgp32.h'\n  '/usr/local/cuda-8.0/include/curand_philox4x32_x.h'\n  '/usr/local/cuda-8.0/include/curand_globals.h'\n  '/usr/local/cuda-8.0/include/curand_uniform.h'\n  '/usr/local/cuda-8.0/include/curand_normal.h'\n  '/usr/local/cuda-8.0/include/curand_normal_static.h'\n  '/usr/local/cuda-8.0/include/curand_lognormal.h'\n  '/usr/local/cuda-8.0/include/curand_poisson.h'\n  '/usr/local/cuda-8.0/include/curand_discrete2.h'.\nnvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\nnvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 243.911s, Critical Path: 154.80s\n```\n", "comments": ["I have the same exact problem. Using Ubuntu 14.04, CUDA 7.5, python 2.7, and Titan X. This is with 0.10 RC.\n", "https://github.com/tensorflow/tensorflow/issues/3431#issuecomment-234131699\n", "@yaroslavvb yes that works thank you!\n"]}, {"number": 3840, "title": "missing return statement at end of non-void function", "body": "Since TensorFlow uses C++11 I think it would be good style to fix the errors such as:\n\n```\nframework/allocator.h(155): warning: missing return statement at end of non-void function \"tensorflow::Allocator::RequestedSize\"\n```\n\nThe code is not harmful in this instance but the compiler would have to parse and trust comments:\n\n``` cpp\n// CHECK dies with a fatal error if condition is not true.  It is *not*\n// controlled by NDEBUG, so the check will be executed regardless of\n// compilation mode.\n```\n\nA simple patch for the problem would be:\n\n``` cpp\n  virtual size_t RequestedSize(void* ptr) {\n    CHECK(false) << \"allocator doesn't track sizes\";\n  }\n```\n\n-->\n\n``` cpp\n  virtual size_t RequestedSize  [[ noreturn ]] (void* ptr) {\n    CHECK(false) << \"allocator doesn't track sizes\";\n  }\n```\n\nReference: http://www.stroustrup.com/C++11FAQ.html#attributes\n", "comments": ["Please submit a PR. Thanks.\n", "Is it necessary to put the `[[ noreturn ]]` on the method that invokes the `CHECK` macro, or would it be possible to add an attribute once that works for _all_ invokers of `CHECK(false)`?\n\nFor example [`LogMessageFatal::~LogMessageFatal()`](https://github.com/tensorflow/tensorflow/blob/d42facc3cc9611f0c9722c81551a7404a0bd3f6b/tensorflow/core/platform/default/logging.h#L54) has the `TF_ATTRIBUTE_NORETURN` annotation, so I'm not sure why that's not enough to inhibit the warning (assuming the compiler constant-folds the `false`).\n", "The standards compliant attribute needs to be placed before the function argument list. There might be a way to use compiler specific attributes just before the function body as in the case with `~LogMessageFatal()` above.\n", "@mrry I would just start by compiling fresh and saving the compiler log and grep through and fix all instances of `warning: missing return statement at end of non-void function` spotted by the compiler. I don't think there are too many of them. I might do such a pass one day at work when I am waiting for some results from the cluster. \n\n(There are lots and lots of stupid compiler warnings that I would like to fix. If warnings are enabled and then just ignored, kittens dies, etc.)\n", "@hholst80 It would be great to clean up those warnings - I definitely agree! And even better if all of them can be resolved by a single judiciously placed annotation :).\n", "I think adding the `[[ noreturn ]]` attribute is better than providing compiler specific directives. Should the attribute be wrapped in a MACRO like `TF_ATTRIBUTE_NOTRETURN` like `TF_FUNC_NORETURN(FuncName) (args)` ?\n", "I started taking a shot at cleaning up the Warnings, the no returns and the unsigned/signed comparisons alerts. https://github.com/tensorflow/tensorflow/compare/master...Mistobaan:feature/cleanup_warnings-3840  \nearly feedback is welcome.\n", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you."]}, {"number": 3839, "title": "TensorFlow works with python, Jupyter notebook, but not iPython", "body": "Operating System: OS X EL Capitan 10.11.5\n\nI installed TensorFlow with Anaconda following https://www.tensorflow.org/versions/r0.10/get_started/index.html\n\nSpecifically, I created a `tensorflow` environment, and used `pip` to install the one with **GPU support**.\n\nAfter activating the environment, I am able to use TensowFlow in `python` and `jupyter notebook` (actually, I have to _turn off the hard ware acceleration of the Chrome browser_ to run TensorFlow correctly), but not in `ipython`:\n\n`which ipython`\n~/anaconda/envs/tensorflow/bin/ipython\n\n`ipython --version`\n5.1.0\n\n`ipython -c \"import tensorflow; print(tensorflow.__version__)\"`\n...\nImportError: dlopen(~/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so, 10): Library not loaded: @rpath/libcudart.7.5.dylib\n  Referenced from: ~/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n  Reason: image not found\n\n`ls -l /Developer/NVIDIA/CUDA-7.5/lib/libcud*`\n\n-rw-r--r-- 1 root wheel 563K Apr 13 02:02 \"/Developer/NVIDIA/CUDA-7.5/lib/libcudart_static.a\"\nlrwxr-xr-x 1 root wheel   19 Apr 13 02:02 \"/Developer/NVIDIA/CUDA-7.5/lib/libcudart.dylib\" -> \"libcudart.7.5.dylib\"\n-rwxr-xr-x 1 root wheel 272K Apr 13 02:02 \"/Developer/NVIDIA/CUDA-7.5/lib/libcudart.7.5.dylib\"\n-rw-r--r-- 1 root wheel 299K Apr 13 02:02 \"/Developer/NVIDIA/CUDA-7.5/lib/libcudadevrt.a\"\n\n`ls -l /usr/local/cuda/lib/`\ntotal 165M\n-rwxr-xr-x 1 root wheel 8.1K Apr 13 02:02 \"libcuda.dylib\"\nlrwxr-xr-x 1 root wheel   36 Apr 13 02:03 \"stubs\" -> \"/Developer/NVIDIA/CUDA-7.5/lib/stubs\"\n...\n-rw-r--r-- 1 root wheel  53M Aug 15 21:17 \"libcudnn_static.a\"\n-rwxr-xr-x 1 root wheel  56M Aug 15 21:17 \"libcudnn.dylib\"\n-rwxr-xr-x 1 root wheel  56M Aug 15 21:17 \"libcudnn.5.dylib\"\nlrwxr-xr-x 1 root wheel   13 Aug 15 21:29 \"libcuda.1.dylib\" -> \"libcuda.dylib\"\n\n`which python`\n~/anaconda/envs/tensorflow/bin/python\n\n`python -c \"import tensorflow; print(tensorflow.__version__)\"`\n\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.1.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.dylib locally\n0.10.0rc0\n", "comments": ["Just to make sure, you're not starting `ipython` from within the TensorFlow source directory are you?\n", "I am starting `ipython` from the home directory, and changing the starting directory does not seem to matter.\n", "http://stackoverflow.com/questions/38768620/installing-a-package-in-conda-environment-but-only-works-in-python-not-ipython/38771575#38771575\n\nOn Tue, Aug 16, 2016 at 5:29 AM, Fujun Du notifications@github.com wrote:\n\n> I am starting ipython from the home directory, and changing the starting\n> directory does not seem to matter.\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/3839#issuecomment-240052369,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AABaHEe3Er6yo3rGUOhqP7I-h28tnzwBks5qgYMEgaJpZM4JlGXd\n> .\n", "That was the way I installed ipython; and the error is \"image not found\", not \"there is no module tensorflow\".\n", "Oh right, my bad. Is your DYLD_LIBRARY_PATH/LD_LIBRARY_PATH and current directory the same for when you try in python vs ipython? The brute-force approach is to run under dtruss to see where it's looking for the file. Also there's a related issue of that image being not found due to System Integrity Protection in El Capitan, but there should be another message starting with `dyld: warning` in the logs\n", "`echo $DYLD_LIBRARY_PATH`\n/usr/local/cuda/lib:/Developer/NVIDIA/CUDA-7.5/lib:\n`echo $LD_LIBRARY_PATH`\n/usr/local/cuda/lib:/Developer/NVIDIA/CUDA-7.5/lib:\n(There may be some redundancy here in these PATHs.)\n\nI do not see any `dyld: warning` when doing import tensorflow.\n\n`dtruss` does not seem to work in my case due to a `codesign` problem (never used that before; does that require an apple developer membership?).\n", "You can also set DYLD_PRINT_ENV and DYLD_PRINT_LIBRARIES env variables (see man dyld).  One thing to note is that on El Capitan DYLD_LIBRARY_PATH is not inherited for protected binaries. See e.g.\nhttp://apple.stackexchange.com/questions/215030/el-capitan-make-check-dyld-library-path\nSo if you are in protected mode that might be causing it to be ignored.\n", "Automatically closing due to lack of recent activity. Please reopen if further information becomes available.\n"]}, {"number": 3838, "title": "Tensorflow: cannot install custom user_op for roi_pooling", "body": "I experimented with a user-op for ROI_pooling in this fork: https://github.com/yuxng/tensorflow/, \n\nI tried several ways to make it work:\n\nAttempt 1:\n\n1) uninstalled tensorflow\n2) placed the roi_pooling files (roi_pooling_op.cc, roi_pooling_op_gpu.cu.cc, roi_pooling_op_gpu.h, roi_pooling_op_grad.py, roi_pooling_op_test.py) in //tensorflow/core/user_ops/ (without BUILD so that they get incorporated with the install)\n3) reinstalled tensorflow using bazel with bazel build -c opt --config=cuda ...\n\nI then get this error with attempt 1: \n\nERROR: /home/fishdrop/tensorflow/tensorflow/cc/BUILD:116:1: Linking of rule '//tensorflow/cc:ops/user_ops_gen_cc' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -o bazel-out/host/bin/tensorflow/cc/ops/user_ops_gen_cc ... (remaining 41 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nbazel-out/host/bin/tensorflow/core/libuser_ops_op_lib.lo(roi_pooling_op.o): In function `RoiPoolGradOp<Eigen::GpuDevice, float>::Compute(tensorflow::OpKernelContext*)':\nroi_pooling_op.cc:(.text._ZN13RoiPoolGradOpIN5Eigen9GpuDeviceEfE7ComputeEPN10tensorflow15OpKernelContextE[_ZN13RoiPoolGradOpIN5Eigen9GpuDeviceEfE7ComputeEPN10tensorflow15OpKernelContextE]+0x3ea): undefined reference to`ROIPoolBackwardLaucher(float const_, float, int, int, int, int, int, int, int, float const_, float_, int const_, Eigen::GpuDevice const&)'\nbazel-out/host/bin/tensorflow/core/libuser_ops_op_lib.lo(roi_pooling_op.o): In function `RoiPoolOp<Eigen::GpuDevice, float>::Compute(tensorflow::OpKernelContext*)':\nroi_pooling_op.cc:(.text._ZN9RoiPoolOpIN5Eigen9GpuDeviceEfE7ComputeEPN10tensorflow15OpKernelContextE[_ZN9RoiPoolOpIN5Eigen9GpuDeviceEfE7ComputeEPN10tensorflow15OpKernelContextE]+0x392): undefined reference to`ROIPoolForwardLaucher(float const_, float, int, int, int, int, int, int, float const_, float_, int_, Eigen::GpuDevice const&)'\ncollect2: error: ld returned 1 exit status\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nERROR: /home/fishdrop/tensorflow/tensorflow/cc/BUILD:116:1 Linking of rule '//tensorflow/cc:ops/user_ops_gen_cc' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -o bazel-out/host/bin/tensorflow/cc/ops/user_ops_gen_cc ... (remaining 41 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\n\nAttempt 2:\n\n1) re-install tensorflow\n2) place files (with BUILD) in //tensorflow/core/user_ops/\n3) use bazel build -c opt //tensorflow/core/user_ops:roi_pooling.so\n\nThis successfully built roi_pooling.so, but when I tried to load the module with tf.load_op_library, I get this error: \n\ntensorflow.python.framework.errors.NotFoundError: /home/fishdrop/tensorflow/bazel-bin/tensorflow/core/user_ops/roi_pooling.so: undefined symbol: _Z21ROIPoolForwardLaucherPKffiiiiiiS0_PfPiRKN5Eigen9GpuDeviceE\n\nThe error seems to be related with Eigen, but I read in this Link: http://stackoverflow.com/questions/38618960/tensorflow-how-to-insert-custom-input-to-existing-graph, that the user_op is supposed to work.. \n\nI am using Ubuntu 14.04 LTS, and I tried these procedures in both Tensorflow 0.10 and Tensorflow 0.08, and got the same errors.. Anyone who could help?\n", "comments": ["In your roi_polling.cc file, all GPU related code must be encapsulated by #if GOOGLE_CUDA #endif.\n\nTo make debugging easier, would you please do the following:\n1. Remove all GPU code, include removing them from the build file.\n2. Build with -c opt only and see if you can load the .so successfully.\n\nIf that works, then add the GPU code.\n\nPlease following instructions here https://www.tensorflow.org/versions/r0.10/how_tos/adding_an_op/index.html\nstep by step.\n\nSherry\n", "thanks for the tip, i just found out that the error occured when I built the user_op without GPU support. \n\nI reinstalled tensorflow v 0.10, with GPU support, and placed all user_op files inside //tensorflow/core/user_ops. \n\nIf I compile the user_op using bazel build -c opt --config=cuda //tensorflow/core/user_ops:roi_pooling.so (the addition of --config=cuda isn't in the tensorflow user_op documentation), the user_op now works.. I guess this issue could now be closed\n"]}, {"number": 3837, "title": "Concurrent access lock", "body": "sess.run() is called concurrently by multiple threads in word2vec. Set use_locking=True in GradientDescentOptimizer to prohibit concurrent parameter read and update.\n", "comments": ["Can one of the admins verify this patch?\n", "That may be by design (locking makes things slower)\n", "Yes, I agree that lock will make things slower. But the program without lock sometimes has behavior that is unrepeatable. IMHO, the lock is not needed when the sess.run is only called inside a single thread but for multithread program, it's better to add it.\n", "Typical application of TensorFlow is non-deterministic even in single-threaded .run because of non-deterministic ops such as reduce_sum on GPU. But I've seen cases when locking was justified because it made training faster (ie, each iteration was slower, but bigger cost decrease)\n", "We'll keep this as is, but you are free to add locking as you see fit into your own code.\n"]}, {"number": 3836, "title": "Correct typo in word2vec.py", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Jenkins, test this please.\n"]}, {"number": 3835, "title": "Some types of fetches in tf.Session.run are not supported and it behaves strange in container", "body": "There're two issues I have found when giving different type of fetches in `tf.Session.run()`.\n\nThe [doc](https://www.tensorflow.org/versions/master/api_docs/python/client.html) says that we can pass the nested list or tuple to `run()` but it doesn't work.\n\n```\ntf.Session.run(fetches, feed_dict=None, options=None, run_metadata=None)\n\nRuns operations and evaluates tensors in fetches.\n\nThis method runs one \"step\" of TensorFlow computation, by running the necessary graph fragment to execute every Operation and evaluate every Tensor in fetches, substituting the values in feed_dict for the corresponding input values.\n\nThe fetches argument may be a single graph element, or an arbitrarily nested list, tuple, namedtuple, or dict containing graph elements at its leaves. A graph element can be one of the following types:\n```\n\nIt's easy to test with this script.\n\n```\nimport tensorflow as tf\n\na = tf.placeholder(\"float\")\nb = tf.placeholder(\"float\")\nadd_op = tf.add(a, b)\nmul_op = tf.mul(a, b)\n\noutput_array = [[add_op, mul_op]]\n\nwith tf.Session() as sess:\n    output_array = sess.run(output_array, feed_dict={a: 1, b: 2})\n    print(output_array)\n```\n\nAnd I found it works if I pass the `array` or `dict`. But what's strange is that the `dict` doesn't work in docker container which is also easy to reproduce by creating the tensorflow container to run the same script. You can find my test code and error log below.\n\n![screen shot 2016-08-16 at 11 52 31](https://cloud.githubusercontent.com/assets/2715000/17687714/3e4a83a8-63aa-11e6-98be-e20a7099f77b.png)\n### Environment info\n\nOperating System: Ubuntu 16.04\n\nIf installed from binary pip package, provide:\nTensorFlow 0.8\n", "comments": ["Try a newer version of tf\n", "I run the `tensorflow/tensorflow:0.10.0rc0` and all `list`, `nested list`, `dict` and `tuple` work like a charm. The `dict` works because my local TensorFlow is `0.9.0` and the document seems really clear.\n\nThanks for your guide @vrv and this feature is really important for our inference services.\n\nBTW, the `tensorflow/tensorflow:latest` is not \"latest\" at all.\n"]}, {"number": 3834, "title": "cannot connect to https://www.tensorflow.org/", "body": "hi, i cannot connect to the home site of tensorflow for long. Can you fix this? My location is China, i can visit Google , tweet,facebook and so on,but no access to tensorflow ,Neither   can my friends do.\n", "comments": ["Not really an issue. I can access https://www.tensorflow.org/ in China.\n\nYou should check your host file or configuration of proxy.\n", "When i ping www.tensorflow.org ,it responses like below\n\n```\n Ping ghs-migration.l.google.com [74.125.203.214]\n.....\nlosss 100%\n```\n", "Sounds like a routing problem rather than an issue with _The Great Firewall_. Have you tried restarting both your computer and router?\n", "Yes, this problem last for about two months .during this period ,i have never gotten access to www.tensorflow.org.\n", "What is the result from a traceroute from your computer to www.tensorflow.org?\n", "Here is the output of my routing:\n\n```\nTracing route to ghs-migration.l.google.com [74.125.204.214]   over a maximum of 30 hops\n   ---------------------------------------------------\n1    12 ms    16 ms     1 ms  192.168.1.8\n2    23 ms     5 ms     8 ms  11.61.0.55\n3     5 ms     6 ms     7 ms  124.74.57.1\n4     9 ms    22 ms     3 ms  11.95.41.129\n5     9 ms     5 ms     7 ms  11.95.120.110\n6     9 ms     7 ms    13 ms  202.97.35.154\n7     *        *        *     request out of time\u3002\n8     *        *        *     request out of time\u3002\n9     *        *        *     request out of time\u3002\n0\n\n\n```\n", "I am not an expert but it looks like your traffic is still left in mainland China (your Ping time should be much higher also as www.tensorflow.org is hosted in the USA). Contact either your ISP or CHINANET.\n\n```\n$ whois 202.97.35.154\n% [whois.apnic.net]\n% Whois data copyright terms    http://www.apnic.net/db/dbcopyright.html\n\n% Information related to '202.97.32.0 - 202.97.63.255'\n\ninetnum:        202.97.32.0 - 202.97.63.255\nnetname:        CHINANET-BB\ndescr:          CHINANET backbone network\ndescr:          Data Communication Division\ndescr:          China Telecom\ncountry:        CN\nadmin-c:        CH93-AP\ntech-c:         CH93-AP\nmnt-by:         APNIC-HM\nmnt-lower:      MAINT-CHINANET\nstatus:         ALLOCATED PORTABLE\nsource:         APNIC\nmnt-irt:        IRT-CHINANET-CN\nchanged:        hostmaster@ns.chinanet.cn.net 20000801\nchanged:        hm-changed@apnic.net 20041214\n\nirt:            IRT-CHINANET-CN\naddress:        No.31 ,jingrong street,beijing\naddress:        100032\ne-mail:         anti-spam@ns.chinanet.cn.net\nabuse-mailbox:  anti-spam@ns.chinanet.cn.net\nadmin-c:        CH93-AP\ntech-c:         CH93-AP\nauth:           # Filtered\nmnt-by:         MAINT-CHINANET\nchanged:        anti-spam@ns.chinanet.cn.net 20101115\nsource:         APNIC\n\nperson:         Chinanet Hostmaster\nnic-hdl:        CH93-AP\ne-mail:         anti-spam@ns.chinanet.cn.net\naddress:        No.31 ,jingrong street,beijing\naddress:        100032\nphone:          +86-10-58501724\nfax-no:         +86-10-58501724\ncountry:        CN\nchanged:        dingsy@cndata.com 20070416\nchanged:        zhengzm@gsta.com 20140227\nmnt-by:         MAINT-CHINANET\nsource:         APNIC\n\n% This query was served by the APNIC Whois Service version 1.69.1-APNICv1r0 (UNDEFINED)\n\n```\n", "it is caused by great firewall. however, It is not totally blocked and I can visit it occasionally. I suggest you to use vpn or proxy. \n\n\u53d1\u81ea\u6211\u7684 iPhone\n\n> \u5728 2016\u5e748\u670816\u65e5\uff0c\u4e0a\u534810:45\uff0cxia notifications@github.com \u5199\u9053\uff1a\n> \n> hi, i cannot connect to the home site of tensorflow for long. Can you fix this?\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n", "I don't understand why did it try to visit  ghs-migration.l.google.com first. To bypass  the GFW, i changed my hosts file,there are many name to ip map,but no item about tensorflow.\n", "Wait what? Tracing route to 11.1.0.1? That resolves to DoD. I can understand if traffic to the Department of Defense (in the USA) is blocked by GFW. The name resolver seems to be random. Maybe it is a GFW issue anyway. In any case its not a bug in tensorflow.org. :-)\n", "It's `ghs-migration.l.google.com [74.125.204.214]` not `11.1.0.1` ,to transfer the content to english, i copy sth wrong from other site. \nAll right ,it's not a bug of tensorflow, but sth annoying , some of my friends get the same problem.\n", "It seems `www.tensorflow.org`  had been blocked by GFW.  I can't understand the logic behind this! \n", "I'm guessing the problem is that tensorflow.org resolves to \"\nghs-migration.l.google.com\" and google.com domains are blocked\n\nOn Tue, Aug 16, 2016 at 4:15 AM, xia notifications@github.com wrote:\n\n> It seems www.tensorflow.org had been blocked by GFW. I can't understand\n> the logic behind this!\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/3834#issuecomment-240033589,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AABaHFA-kj7lzeoghMiYMdJGrCa6jegpks5qgXGFgaJpZM4Jk-bb\n> .\n", "Unfortunately this might not be something we can easily resolve. Community contribution welcome. :)\n", "add this to the hosts file,can fix this.\n\n```\n#TensorFlow start\n64.233.188.121  www.tensorflow.org\n#TensorFlow end\n```\n", "great, thanks to @shartoo \n", "your IP address is on tensorflow's server blacklist go to\n\nhttp://www.ipvoid.com\n", "Thanks for your answer, @shartoo. It really works. By the way, I'am Chinese too.", "@shartoo I type `64.233.188.121` in browser,but it cant be accessed.Does that way you supposed still could work or not?", "thanks to @shartoo , it's worked.", "that's really helpful, thanks to @shartoo ", "@chengshuyi Yes, it does.", "@fangpin  Yes,I also found this question, how do you solve the problem\uff1f", "@SiriusHsh  I fount it's the VPN settings problem which cause this situation. For example, I use shadowsocks on mac, it will be all ok when I use the global mode instead of auto mode. So maybe it works for you too. By the way, I use shadowsocks and chrome with extension swithchyomega on ubuntu.", "it works, thanks to @shartoo ", "amazing, when I use the global mode instead of auto mode, it shows the correct page.  I also use shadowsocks and Mac. thx, @fangpin ", "I think it's a proxy problem. You can export PAC file from some proxy tools such as SwitchyOmega. Then overwrite PAC file of shadowsocks. ", "@fangpin @SiriusHsh \u662f\u7684\uff0c\u90a3\u4e2aPAC\u6587\u4ef6\u91cc\u53ea\u5199\u4e86  ||tensorflow.org \u52a0\u4e00\u4e2a  .tensorflow.org \u5c31\u597d\u4e86", "when i add the host to my host file, i can get response when i ping www.tensorflow.org.\r\nHowever, i still fail to access the web via browser.\r\n\r\n```\r\nPING www.tensorflow.org (64.233.188.121): 56 data bytes\r\n64 bytes from 64.233.188.121: icmp_seq=0 ttl=245 time=194.245 ms\r\n64 bytes from 64.233.188.121: icmp_seq=1 ttl=245 time=198.178 ms\r\n```", "@SiriusHsh it works! cool", "> @SiriusHsh I fount it's the VPN settings problem which cause this situation. For example, I use shadowsocks on mac, it will be all ok when I use the global mode instead of auto mode. So maybe it works for you too. By the way, I use shadowsocks and chrome with extension swithchyomega on ubuntu.\r\n\r\nthat works for me too, on my old win7 desktop, thanks!!", "> @SiriusHsh I fount it's the VPN settings problem which cause this situation. For example, I use shadowsocks on mac, it will be all ok when I use the global mode instead of auto mode. So maybe it works for you too. By the way, I use shadowsocks and chrome with extension swithchyomega on ubuntu.\r\n\r\n", "@SiriusHsh Thanks dear same issue with me and I am also using shadowsocks on windows 10. By changing the configuration setting from PAC to Global, it solved my issue very thanks.", "> @SiriusHsh Thanks dear same issue with me and I am also using shadowsocks on windows 10. By changing the configuration setting from PAC to Global, it solved my issue very thanks.\r\n\r\nThank you! I get it!"]}, {"number": 3833, "title": "/home/myubuntu/tensorflow/tensorflow/examples/android/BUILD:47:1: Processing Android resources for //tensorflow/examples/android:tensorflow_demo failed: linux-sandbox failed: error executing command ", "body": "ubuntu 14.04 \nbazel build give the error:\n/home/myubuntu/tensorflow/tensorflow/examples/android/BUILD:47:1: Processing Android resources for //tensorflow/examples/android:tensorflow_demo failed: linux-sandbox failed: error executing command \n", "comments": ["the error of detail is \nERROR: /home/zwb/tensorflow/tensorflow/examples/android/BUILD:47:1: Processing Android resources for //tensorflow/examples/android:tensorflow_demo failed: linux-sandbox failed: error executing command \n  (cd /home/zwb/.cache/bazel/_bazel_zwb/0c95949bd08d065b0b8ae2967e52ee42/execroot/tensorflow && \\\n  exec env - \\\n  /home/zwb/.cache/bazel/_bazel_zwb/0c95949bd08d065b0b8ae2967e52ee42/execroot/tensorflow/_bin/linux-sandbox @/home/zwb/.cache/bazel/_bazel_zwb/0c95949bd08d065b0b8ae2967e52ee42/execroot/tensorflow/bazel-sandbox/1406bb67-c527-4333-9864-896c2a7476b6-0.params -- bazel-out/host/bin/external/bazel_tools/tools/android/resources_processor --buildToolsVersion 24.0.1 --aapt bazel-out/host/bin/external/androidsdk/aapt_binary --annotationJar external/androidsdk/tools/support/annotations.jar --androidJar external/androidsdk/platforms/android-24/android.jar --primaryData tensorflow/examples/android/res::tensorflow/examples/android/AndroidManifest.xml --rOutput bazel-out/local-fastbuild/bin/tensorflow/examples/android/tensorflow_demo_symbols/R.txt --srcJarOutput bazel-out/local-fastbuild/bin/tensorflow/examples/android/tensorflow_demo.srcjar --proguardOutput bazel-out/local-fastbuild/bin/tensorflow/examples/android/proguard/tensorflow_demo/_tensorflow_demo_proguard.cfg --mainDexProguardOutput bazel-out/local-fastbuild/bin/tensorflow/examples/android/proguard/tensorflow_demo/main_dex_tensorflow_demo_proguard.cfg --manifestOutput bazel-out/local-fastbuild/bin/tensorflow/examples/android/tensorflow_demo_processed_manifest/AndroidManifest.xml --resourcesOutput bazel-out/local-fastbuild/bin/tensorflow/examples/android/tensorflow_demo_files/resource_files.zip --packagePath bazel-out/local-fastbuild/bin/tensorflow/examples/android/tensorflow_demo.ap_ --debug --packageForR org.tensorflow.demo).\nError: bazel-out/host/bin/external/androidsdk/aapt_binary: line 4: /home/zwb/.cache/bazel/_bazel_zwb/0c95949bd08d065b0b8ae2967e52ee42/execroot/tensorflow/bazel-out/host/bin/external/androidsdk/aapt_binary.runfiles/androidsdk/build-tools/24.0.1/aapt: Permission denied\nError: bazel-out/host/bin/external/androidsdk/aapt_binary: line 4: /home/zwb/.cache/bazel/_bazel_zwb/0c95949bd08d065b0b8ae2967e52ee42/execroot/tensorflow/bazel-out/host/bin/external/androidsdk/aapt_binary.runfiles/androidsdk/build-tools/24.0.1/aapt: Permission denied\nError: bazel-out/host/bin/external/androidsdk/aapt_binary: line 4: /home/zwb/.cache/bazel/_bazel_zwb/0c95949bd08d065b0b8ae2967e52ee42/execroot/tensorflow/bazel-out/host/bin/external/androidsdk/aapt_binary.runfiles/androidsdk/build-tools/24.0.1/aapt: Permission denied\nError: bazel-out/host/bin/external/androidsdk/aapt_binary: line 4: /home/zwb/.cache/bazel/_bazel_zwb/0c95949bd08d065b0b8ae2967e52ee42/execroot/tensorflow/bazel-out/host/bin/external/androidsdk/aapt_binary.runfiles/androidsdk/build-tools/24.0.1/aapt: Permission denied\nError: bazel-out/host/bin/external/androidsdk/aapt_binary: line 4: /home/zwb/.cache/bazel/_bazel_zwb/0c95949bd08d065b0b8ae2967e52ee42/execroot/tensorflow/bazel-out/host/bin/external/androidsdk/aapt_binary.runfiles/androidsdk/build-tools/24.0.1/aapt: Permission denied\nAug 16, 2016 2:54:25 PM com.google.devtools.build.android.AndroidResourceProcessingAction main\nSEVERE: Error during merging resources\nError: Failed to run command:\n    bazel-out/host/bin/external/androidsdk/aapt_binary s -i /tmp/android_resources_tmp6549436154600677783/tmp-deduplicated/tensorflow/examples/android/res/drawable-hdpi/ic_launcher.png -o /tmp/android_resources_tmp6549436154600677783/merged_resources/drawable-hdpi-v4/ic_launcher.png\nError Code:\n    1\nOutput:\n    bazel-out/host/bin/external/androidsdk/aapt_binary: line 4: /home/zwb/.cache/bazel/_bazel_zwb/0c95949bd08d065b0b8ae2967e52ee42/execroot/tensorflow/bazel-out/host/bin/external/androidsdk/aapt_binary.runfiles/androidsdk/build-tools/24.0.1/aapt: Permission denied\n\n```\nat com.android.ide.common.res2.MergeWriter.end(MergeWriter.java:54)\nat com.android.ide.common.res2.MergedResourceWriter.end(MergedResourceWriter.java:113)\nat com.android.ide.common.res2.DataMerger.mergeData(DataMerger.java:291)\nat com.android.ide.common.res2.ResourceMerger.mergeData(ResourceMerger.java:48)\nat com.google.devtools.build.android.AndroidResourceProcessor.mergeData(AndroidResourceProcessor.java:932)\nat com.google.devtools.build.android.AndroidResourceProcessingAction.main(AndroidResourceProcessingAction.java:257)\n```\n\nCaused by: com.android.ide.common.internal.LoggedErrorException: Failed to run command:\n    bazel-out/host/bin/external/androidsdk/aapt_binary s -i /tmp/android_resources_tmp6549436154600677783/tmp-deduplicated/tensorflow/examples/android/res/drawable-hdpi/ic_launcher.png -o /tmp/android_resources_tmp6549436154600677783/merged_resources/drawable-hdpi-v4/ic_launcher.png\nError Code:\n    1\nOutput:\n    bazel-out/host/bin/external/androidsdk/aapt_binary: line 4: /home/zwb/.cache/bazel/_bazel_zwb/0c95949bd08d065b0b8ae2967e52ee42/execroot/tensorflow/bazel-out/host/bin/external/androidsdk/aapt_binary.runfiles/androidsdk/build-tools/24.0.1/aapt: Permission denied\n\n```\nat com.android.ide.common.internal.CommandLineRunner.runCmdLine(CommandLineRunner.java:123)\nat com.android.ide.common.internal.CommandLineRunner.runCmdLine(CommandLineRunner.java:96)\nat com.android.ide.common.internal.AaptCruncher.crunchPng(AaptCruncher.java:58)\nat com.android.ide.common.res2.MergedResourceWriter$1.call(MergedResourceWriter.java:188)\nat com.android.ide.common.res2.MergedResourceWriter$1.call(MergedResourceWriter.java:139)\nat java.util.concurrent.FutureTask.run(FutureTask.java:266)\nat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\nat java.util.concurrent.FutureTask.run(FutureTask.java:266)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\nat java.lang.Thread.run(Thread.java:745)\n```\n\nException in thread \"main\" Error: Failed to run command:\n    bazel-out/host/bin/external/androidsdk/aapt_binary s -i /tmp/android_resources_tmp6549436154600677783/tmp-deduplicated/tensorflow/examples/android/res/drawable-hdpi/ic_launcher.png -o /tmp/android_resources_tmp6549436154600677783/merged_resources/drawable-hdpi-v4/ic_launcher.png\nError Code:\n    1\nOutput:\n    bazel-out/host/bin/external/androidsdk/aapt_binary: line 4: /home/zwb/.cache/bazel/_bazel_zwb/0c95949bd08d065b0b8ae2967e52ee42/execroot/tensorflow/bazel-out/host/bin/external/androidsdk/aapt_binary.runfiles/androidsdk/build-tools/24.0.1/aapt: Permission denied\n\n```\nat com.android.ide.common.res2.MergeWriter.end(MergeWriter.java:54)\nat com.android.ide.common.res2.MergedResourceWriter.end(MergedResourceWriter.java:113)\nat com.android.ide.common.res2.DataMerger.mergeData(DataMerger.java:291)\nat com.android.ide.common.res2.ResourceMerger.mergeData(ResourceMerger.java:48)\nat com.google.devtools.build.android.AndroidResourceProcessor.mergeData(AndroidResourceProcessor.java:932)\nat com.google.devtools.build.android.AndroidResourceProcessingAction.main(AndroidResourceProcessingAction.java:257)\n```\n\nCaused by: com.android.ide.common.internal.LoggedErrorException: Failed to run command:\n    bazel-out/host/bin/external/androidsdk/aapt_binary s -i /tmp/android_resources_tmp6549436154600677783/tmp-deduplicated/tensorflow/examples/android/res/drawable-hdpi/ic_launcher.png -o /tmp/android_resources_tmp6549436154600677783/merged_resources/drawable-hdpi-v4/ic_launcher.png\nError Code:\n    1\nOutput:\n    bazel-out/host/bin/external/androidsdk/aapt_binary: line 4: /home/zwb/.cache/bazel/_bazel_zwb/0c95949bd08d065b0b8ae2967e52ee42/execroot/tensorflow/bazel-out/host/bin/external/androidsdk/aapt_binary.runfiles/androidsdk/build-tools/24.0.1/aapt: Permission denied\n\n```\nat com.android.ide.common.internal.CommandLineRunner.runCmdLine(CommandLineRunner.java:123)\nat com.android.ide.common.internal.CommandLineRunner.runCmdLine(CommandLineRunner.java:96)\nat com.android.ide.common.internal.AaptCruncher.crunchPng(AaptCruncher.java:58)\nat com.android.ide.common.res2.MergedResourceWriter$1.call(MergedResourceWriter.java:188)\nat com.android.ide.common.res2.MergedResourceWriter$1.call(MergedResourceWriter.java:139)\nat java.util.concurrent.FutureTask.run(FutureTask.java:266)\nat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\nat java.util.concurrent.FutureTask.run(FutureTask.java:266)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\nat java.lang.Thread.run(Thread.java:745)\n```\n\nTarget //tensorflow/examples/android:tensorflow_demo failed to build\nINFO: Elapsed time: 3.245s, Critical Path: 1.61s\n", "Closing due to inactivity. Feel free to open a new github issue if the problem still persists in recent versions."]}, {"number": 3832, "title": "Branch 130325850", "body": "", "comments": []}, {"number": 3831, "title": "Update Eigen version to fix CUDA 8 compilation error (second attempt)", "body": "The last attempt (#3818) at fixing the compilation errors in didn't work, because bitbucket turned the merged PR into an empty commit: https://bitbucket.org/eigen/eigen/commits/5fea06b2707993a43c5b31105b8224266ecc2c8f.\n\nThis PR now definitely updates Eigen to a version that includes the fixes.\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n"]}, {"number": 3830, "title": "Add a new target including the C api.", "body": "This is to fix issue #3814.\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please.\n", "@tensorflow-jenkins test this please\n"]}, {"number": 3829, "title": "Tensorboard not showing any curves", "body": "No curves appear in Tensorboard 25 when I click on a heading to open one.\nMaybe related to #3782\n### Environment info\n\nOperating System: Ubuntu 14.04\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`): 008bcaea38815f46804fc3f56492f4dd93837a56\n2. The output of `bazel version` 0.3.0\n", "comments": ["When you say no curves appear, do you mean that no charts appear, or that the charts appear without curves?\n\nI suspect its a duplicate of #3750 \n", "Yes it's the same issue; sorry didn't see it was already reported.\n"]}]