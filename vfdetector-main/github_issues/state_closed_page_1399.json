[{"number": 11087, "title": "[feature] Option to Selectively Disable Certain Graph Optimizations", "body": "repost from: https://github.com/tensorflow/tensorflow/commit/999b794c137d12d73adbf41dcbe9383a0cd94769#commitcomment-22789669 (CC @petewarden).\r\n\r\nOn iOS when using a graph that has been \"memory mapped\", you want to disable constant folding to prevent the optimization pass from copying all of the weight data. The current way to do so is to set optimization [level `L0` which disables all optimizations](https://github.com/tensorflow/tensorflow/blob/12f033df4c8fa3feb88ce936eb1581eaa92b303e/tensorflow/core/protobuf/config.proto#L101-L102). Ideally there is a way to keep Common subexpression elimination and function inlining, but prevent constant folding.", "comments": ["It looks like the optimization level is actually the logical OR of the Boolean flags in the OptimizerOptions and the optimization level. See the code at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/graph_optimizer.cc#L27-L32\r\n\r\nSo you can get the semantics you want with something like:\r\n```\r\nopt_opts = options.config.mutable_graph_options()->mutable_optimizer_options();\r\nopt_opts->set_opt_level(::tensorflow::OptimizerOptions::L0);\r\nopt_opts->set_do_common_subexpression_elimination(true);\r\nopt_opts->set_do_function_inlining(true);\r\n```"]}, {"number": 11086, "title": "KMeansClustering fail with Assertion Error", "body": "I try to use KMeansClustering on letter recognition dataset (http://archive.ics.uci.edu/ml/datasets/Letter+Recognition). I use first 16000 rows. I use this code:\r\n\r\nimport tensorflow as tf\r\nkmeans = tf.contrib.learn.KMeansClustering(10)\r\ntraining_set = pd.read_csv(\"letter_train.csv\", header=None, dtype=np.float64)\r\nFEATURES = [\"V\" + str(i) for i in range(16)]\r\ntraining_set.columns = [\"V\" + str(i) for i in range(17)]\r\nLABEL = \"V\" + str(16)\r\ndef input_fn(data_set):\r\n    feature_cols = {k: tf.constant(data_set[k].values)\r\n                    for k in FEATURES}\r\n    labels = tf.constant(data_set[LABEL].values)\r\n    return feature_cols, labels\r\n\r\nkmeans = tf.contrib.learn.KMeansClustering(10).fit(input_fn=lambda: input_fn(training_set), steps=100)\r\n\r\nBut I have following error:\r\n\r\nAssertionError: Tensor(\"Const_16:0\", shape=(16000,), dtype=float64)\r\n\r\nFor target Variable, I have <tf.Tensor 'Const_7883:0' shape=(16000,) dtype=float64>)\r\n\r\ninput_fn is built like here: https://www.tensorflow.org/get_started/input_fn\r\nHow to make KMeans work?\r\n\r\n", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "### System information\r\nOS: RedHat_7.2_x86_64\r\nCPU Model: Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz\r\nTensorFlow installed with pip install tensorflow-gpu\r\n\r\n[nvidia-smi.txt](https://github.com/tensorflow/tensorflow/files/1127376/nvidia-smi.txt) - output of nvidia-smi -q\r\n\r\nPython 3.5.2 :: Intel Corporation is used\r\nCUDA version: 8.0\r\n\r\nTensorFlow Version: v1.0.0-65-g4763edf-dirty 1.0.1\r\n[kmeans_rep.txt](https://github.com/tensorflow/tensorflow/files/1127391/kmeans_rep.txt) - script to use\r\nUse first 16000 rows from dataset (http://archive.ics.uci.edu/ml/datasets/Letter+Recognition). Name file as letter_train.csv and put it in same folder as script.\r\n\r\n\r\n\r\n", "I tried to repro, but the call to read_csv with dtype float64 failed because of the first column which is characters. If you give more context (or link to the full log) for your error it may be easier to debug.\r\n\r\n--------------------------\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-1-ea7c4a9a280b> in <module>()\r\n      4 kmeans = tf.contrib.learn.KMeansClustering(10)\r\n      5 \r\n----> 6 training_set = pd.read_csv(\"/tmp/letter_train.csv\", header=None, dtype=np.float64)\r\n      7 \r\n      8 FEATURES = [\"V\" + str(i) for i in range(16)]\r\n\r\npandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\r\n    644                     skip_blank_lines=skip_blank_lines)\r\n    645 \r\n--> 646         return _read(filepath_or_buffer, kwds)\r\n    647 \r\n    648     parser_f.__name__ = name\r\n\r\npandas/io/parsers.py in _read(filepath_or_buffer, kwds)\r\n    399         return parser\r\n    400 \r\n--> 401     data = parser.read()\r\n    402     parser.close()\r\n    403     return data\r\n\r\npandas/io/parsers.py in read(self, nrows)\r\n    937                 raise ValueError('skipfooter not supported for iteration')\r\n    938 \r\n--> 939         ret = self._engine.read(nrows)\r\n    940 \r\n    941         if self.options.get('as_recarray'):\r\n\r\npandas/io/parsers.py in read(self, nrows)\r\n   1506     def read(self, nrows=None):\r\n   1507         try:\r\n-> 1508             data = self._reader.read(nrows)\r\n   1509         except StopIteration:\r\n   1510             if self._first_chunk:\r\n\r\npandas/parser.so in pandas.parser.TextReader.read (pandas/parser.c:9977)()\r\n\r\npandas/parser.so in pandas.parser.TextReader._read_low_memory (third_party/py/pandas/parser.c:10235)()\r\n\r\npandas/parser.so in pandas.parser.TextReader._read_rows (pandas/parser.c:11254)()\r\n\r\npandas/parser.so in pandas.parser.TextReader._convert_column_data (third_party/py/pandas/parser.c:12629)()\r\n\r\npandas/parser.so in pandas.parser.TextReader._convert_tokens (pandas/parser.c:13876)()\r\n\r\nValueError: could not convert string to float: C\r\n\r\n", "Hi!\r\nSorry - I forgot that I've pre-processed this dataset. I used all columns except first one for clustering. \r\n\r\nWith first column removed form this dataset, I recieve the error.", "A link to the full log would still be helpful.", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!"]}, {"number": 11084, "title": "Fix import in documentation documentation", "body": "", "comments": ["Ignoring unrelated failure."]}, {"number": 11083, "title": "Fix #10823 moving vlog to higher level", "body": "This PR contains fix for moving VLOG level to higher level as specified in the issue.\r\nVLOG is moved to level 3 as suggested in the comments.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please."]}, {"number": 11082, "title": "Only use weakref.finalize from backports in Python < 3.4", "body": "### System information\r\n- Arch Linux (up-to-date)\r\n- TensorFlow installed from binary (with pacman, Arch package manager)\r\n- TensorFlow version: 1.2.0\r\n- No GPU or CUDA\r\n\r\n### Describe the problem\r\n[tensorflow/python/util/tf_should_use.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/util/tf_should_use.py) is doing `from backports import weakref` (introduced by    cf238e1f2f68309822e1adb3f86dd439c0b87441), though I guess this is only useful for Python 2. In Python 3, we could simply `import weakref`. This bug has been observed by other people and reported on the [Arch Linux bug tracker](https://bugs.archlinux.org/task/54606).\r\n\r\n### Traceback\r\n```\r\npython -c \"import tensorflow\"\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 63, in <module>\r\n    from tensorflow.python.framework.framework_lib import *\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/framework/framework_lib.py\", line 100, in <module>\r\n    from tensorflow.python.framework.subscribe import subscribe\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/framework/subscribe.py\", line 26, in <module>\r\n    from tensorflow.python.ops import variables\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 26, in <module>\r\n    from tensorflow.python.ops import control_flow_ops\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 70, in <module>\r\n    from tensorflow.python.ops import tensor_array_ops\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 33, in <module>\r\n    from tensorflow.python.util import tf_should_use\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\", line 28, in <module>\r\n    from backports import weakref  # pylint: disable=g-bad-import-order\r\nModuleNotFoundError: No module named 'backports'\r\n```", "comments": ["In [tensorflow/blob/master/tensorflow/python/util/tf_should_use.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/util/tf_should_use.py) the function `weakref.finalize` is used. This is [new since Python 3.4](https://docs.python.org/3/library/weakref.html#weakref.finalize). Thus, it is not a distinction between Python 2 and Python 3 here. But clearly `backports.weakref`should not be a dependency for Python >= 3.4\r\n\r\nPossible solution is to check if the `finalize` function exists in `weakref` module and import `backports.weakref` otherwise. Importing `weakref` should be safe, as it [exists since Python 2.1](https://docs.python.org/2/library/weakref.html):\r\n\r\n```python\r\nimport weakref\r\nif not hasattr(weakref, 'finalize'):    \r\n    from backports import weakref\r\n```\r\nOr alternatively\r\n\r\n```python\r\ntry:\r\n    from weakref import finalize\r\nexcept ImportError:\r\n    from backports.weakref import finalize\r\n```\r\nas `finalize` is (currently) the only used member from `weakref` module in tf_should_use.\r\n\r\n@mdeff Maybe you want to change the title of the issue to something like *\"Only use weakref.finalize from backports in Python < 3.4\"*\r\n", "@ebrevdo What do you think? I could also add a PR for this.", "I just opened PR #11174 and preferred @spinnau's second option because it's \"easier to ask for forgiveness than permission\"."]}, {"number": 11081, "title": "Problem with operating system allocated ports in distributed Tensorflow", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes (simple test case to reproduce)\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Reproduced on Ubuntu 16.04.1 and MacOS Sierra (10.12.5).\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: v1.2.0-rc2-21-g12f033d 1.2.0\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: See source code below\r\n\r\n### Describe the problem\r\n\r\nWhen using operating system allocated ports (specifying port zero in the cluster spec), distributed Tensorflow seems to wait forever with the message \"CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\" when initializing variables on the parameter server.\r\n\r\nThe same code works fine with explicitly allocated ports.  \r\n\r\nSee below for code to reproduce the issue.\r\n\r\n### Source code / logs\r\n\r\nThis code works fine:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ncluster = tf.train.ClusterSpec({\"ps\": [\"localhost:65062\"], \"worker\": [\"localhost:65063\"]})\r\n\r\nps = tf.train.Server(cluster, job_name=\"ps\", task_index=0)\r\nworker = tf.train.Server(cluster, job_name=\"worker\", task_index=0)\r\n\r\nprint(\"PS: {0}\".format(ps.target))\r\nprint(\"Worker: {0}\".format(worker.target))\r\n\r\nwith tf.Session(worker.target) as sess:\r\n\r\n    with tf.device(\"/job:ps/task:0\"):\r\n        W = tf.Variable(tf.zeros([784, 10]))\r\n        b = tf.Variable(tf.zeros([10]))\r\n\r\n    init = tf.global_variables_initializer()\r\n\r\n    print(\"RUNNING SESSION\")\r\n    sess.run(init)\r\n    print(\"SESSION FINISHED\")\r\n```\r\n\r\nIt gets to the end and prints \"SESSION FINISHED\", producing the following output:\r\n\r\n```\r\n2017-06-27 10:12:58.482841: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:65062}\r\n2017-06-27 10:12:58.482857: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:65063}\r\n2017-06-27 10:12:58.483156: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:65062\r\n2017-06-27 10:12:58.493057: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:65062}\r\n2017-06-27 10:12:58.493077: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:65063}\r\n2017-06-27 10:12:58.493263: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:65063\r\nPS: b'grpc://localhost:65062'\r\nWorker: b'grpc://localhost:65063'\r\nRUNNING SESSION\r\n2017-06-27 10:12:58.525303: I tensorflow/core/distributed_runtime/master_session.cc:999] Start master session 78091edd7d24288b with config: \r\n\r\nSESSION FINISHED\r\n```\r\n\r\nHowever, this code does not work:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ncluster = tf.train.ClusterSpec({\"ps\": [\"localhost:0\"], \"worker\": [\"localhost:0\"]})\r\n\r\nps = tf.train.Server(cluster, job_name=\"ps\", task_index=0)\r\nworker = tf.train.Server(cluster, job_name=\"worker\", task_index=0)\r\n\r\nprint(\"PS: {0}\".format(ps.target))\r\nprint(\"Worker: {0}\".format(worker.target))\r\n\r\nwith tf.Session(worker.target) as sess:\r\n\r\n    with tf.device(\"/job:ps/task:0\"):\r\n        W = tf.Variable(tf.zeros([784, 10]))\r\n        b = tf.Variable(tf.zeros([10]))\r\n\r\n    init = tf.global_variables_initializer()\r\n\r\n    print(\"RUNNING SESSION\")\r\n    sess.run(init)\r\n    print(\"SESSION FINISHED\")\r\n```\r\n\r\nThe only difference in the above code is that we let the operating system allocate ports by specifying zero as the port number, rather than explicitly allocating them.\r\n\r\nThis code does not reach the end, but instead prints the message \"CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\" repeatedly. The following output is produced:\r\n\r\n```\r\n2017-06-27 10:11:31.238753: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:65062}\r\n2017-06-27 10:11:31.238770: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:0}\r\n2017-06-27 10:11:31.239114: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:65062\r\n2017-06-27 10:11:31.247859: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:0}\r\n2017-06-27 10:11:31.247877: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:65063}\r\n2017-06-27 10:11:31.248059: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:65063\r\nPS: b'grpc://localhost:65062'\r\nWorker: b'grpc://localhost:65063'\r\nRUNNING SESSION\r\n2017-06-27 10:11:41.283559: I tensorflow/core/distributed_runtime/master.cc:209] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n2017-06-27 10:11:51.287739: I tensorflow/core/distributed_runtime/master.cc:209] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n2017-06-27 10:12:01.290028: I tensorflow/core/distributed_runtime/master.cc:209] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n2017-06-27 10:12:11.290560: I tensorflow/core/distributed_runtime/master.cc:209] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n2017-06-27 10:12:21.292900: I tensorflow/core/distributed_runtime/master.cc:209] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n``` \r\n\r\nNote the messages ```Initialize GrpcChannelCache for job worker -> {0 -> localhost:0}``` above, which may indicate the source of the problem.", "comments": ["@saeta can you comment or redirect? Thanks!", "Hi @nfergu,\r\n\r\nYou're exactly on the right track with regards to what the issue is. The GrpcChannelCache needs the actual ports of the TF servers in order to correctly connect. The way to do this is by using [ClusterSpec propagation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/client/session_clusterspec_prop_test.py#L56) a new feature in TF 1.2.\r\n\r\nYour script would look like the following:\r\n\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.core.protobuf import cluster_pb2 as cluster\r\n\r\ntmp_cluster = tf.train.ClusterSpec({\"tmp\": [\"localhost:0\"]})\r\n\r\nps = tf.train.Server(tmp_cluster, job_name=\"tmp\", task_index=0)\r\nworker = tf.train.Server(tmp_cluster, job_name=\"tmp\", task_index=0)\r\n\r\nprint(\"PS: {0}\".format(ps.target))\r\nprint(\"Worker: {0}\".format(worker.target))\r\n\r\ncluster_def = cluster.ClusterDef()\r\nps_job = cluster_def.job.add()\r\nps_job.name = 'ps'\r\nps_job.tasks[0] = ps.target[len('grpc://'):]  # strip off `grpc://` prefix\r\n\r\nworker_job = cluster_def.job.add()\r\nworker_job.name = 'worker'\r\nworker_job.tasks[0] = worker.target[len('grpc://'):]\r\n\r\nconfig = tf.ConfigProto(cluster_def=cluster_def)\r\n\r\nwith tf.Session(worker.target, config=config) as sess:\r\n\r\n  with tf.device(\"/job:ps/task:0\"):\r\n    W = tf.Variable(tf.zeros([784, 10]))\r\n    b = tf.Variable(tf.zeros([10]))\r\n\r\n    init = tf.global_variables_initializer()\r\n\r\n    print(\"RUNNING SESSION\")\r\n    sess.run(init)\r\n    print(\"SESSION FINISHED\")\r\n\r\n```\r\n\r\nThe output of this on my machine is:\r\n\r\n```\r\n2017-06-29 18:38:45.168720: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-29 18:38:45.168744: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-29 18:38:45.168750: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-29 18:38:45.168754: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-29 18:38:45.168758: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-29 18:38:45.197904: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job tmp -> {0 -> localhost:35403}\r\n2017-06-29 18:38:45.208088: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:35403\r\n2017-06-29 18:38:45.241186: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job tmp -> {0 -> localhost:38776}\r\n2017-06-29 18:38:45.243309: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:38776\r\nPS: grpc://localhost:35403\r\nWorker: grpc://localhost:38776\r\nRUNNING SESSION\r\n2017-06-29 18:38:45.268510: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:35403}\r\n2017-06-29 18:38:45.268564: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:38776}\r\n2017-06-29 18:38:45.280314: I tensorflow/core/distributed_runtime/master_session.cc:999] Start master session 928a851d8c8eae3d with config: \r\ncluster_def {\r\n  job {\r\n    name: \"ps\"\r\n    tasks {\r\n      key: 0\r\n      value: \"localhost:35403\"\r\n    }\r\n  }\r\n  job {\r\n    name: \"worker\"\r\n    tasks {\r\n      key: 0\r\n      value: \"localhost:38776\"\r\n    }\r\n  }\r\n}\r\n\r\n2017-06-29 18:38:45.280573: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:35403}\r\n2017-06-29 18:38:45.280599: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:38776}\r\n2017-06-29 18:38:45.280790: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:35403}\r\n2017-06-29 18:38:45.280829: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:38776}\r\nSESSION FINISHED\r\n```\r\n\r\nNote: ClusterSpec propagation is a new feature I added in the latest TF release. We haven't yet updated the documentation as we have some ongoing work to make it much easier to construct the ClusterDef's. (Hence the awkward import.) Hope this helps!", "Great, thanks @saeta ", "Hi I'm confused about how to use this figure. I want to run distributed tensorflow on several machines for which I have ssh access to. What port number should I specify for my different workers? It seems everytime I called cluster.ClusterDef() it just gives me some port number on localhost, instead of the port number of other machines I want."]}, {"number": 11080, "title": "Feature Request\uff1a delete key in MutableHashTable", "body": "A 'delete' method in contrib.lookup.MutableHashTable would be useful when there are too many <key, value> pairs and we can only handle the recent ones! \r\nIs there any possibility that the delete method will be implemented? Thanks. @vrv @ebrevdo  ", "comments": ["add delete(key) in MutableHashTable? need to add API in LookupInterface and implement in MutableHashTableOfTensors and register the OP.", "@joshua-xia So delete(key) is possible in MutableHashTable?  What I do now for expediency  is to close the session and then restart. Very time-consuming when the hash table is large. ", "This requires some updates in the API and it's in the works right now.", "@ysuematsu That's great! ", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly."]}, {"number": 11079, "title": "Fix some typos in doc strings of Conv3D of keras.", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Jenkins, test this please."]}, {"number": 11078, "title": "error running object_detection samples TypeError: unorderable types: tuple() < str()", "body": "I cannot run any of the examples from the new object detection api. I followed the documented \"prepare inputs\" and \"run locally\", but when I run the below command I get the following error.\r\n\r\n`python3 train.py --pipeline_config_path=/home/chris/tensorflow/models/object_detection/samples/configs/ssd_inception_v2_pets.config --train_dir=.\r\n`\r\n`Traceback (most recent call last):\r\n  File \"train.py\", line 199, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"train.py\", line 195, in main\r\n    worker_job_name, is_chief, FLAGS.train_dir)\r\n  File \"/home/chris/tensorflow/models/object_detection/trainer.py\", line 184, in train\r\n    data_augmentation_options)\r\n  File \"/home/chris/tensorflow/models/object_detection/trainer.py\", line 77, in _create_input_queue\r\n    prefetch_queue_capacity=prefetch_queue_capacity)\r\n  File \"/home/chris/tensorflow/models/object_detection/core/batcher.py\", line 93, in __init__\r\n    num_threads=num_batch_queue_threads)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/input.py\", line 924, in batch\r\n    name=name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/input.py\", line 698, in _batch\r\n    tensor_list = _as_tensor_list(tensors)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/input.py\", line 386, in _as_tensor_list\r\n    return [tensors[k] for k in sorted(tensors)]\r\nTypeError: unorderable types: tuple() < str()\r\n`\r\n\r\nI get the feeling this error is because it isn't processing the record file correctly, or something along those lines is wrong. However, as far as I can tell, I followed everything correctly. I edited the paths in the config file and triple checked that they point to real files.", "comments": ["I am also having this exact issue on python 3 with tensorflow for GPU. I might be making a mistake somewhere along as well, though.", "To add to this, I found that their trainer_test.py code also fails because of this issue, leading me to think there's more going on here:\r\n```\r\npython trainer_test.py \r\n\r\n======================================================================\r\nERROR: test_configure_trainer_and_train_two_steps (__main__.TrainerTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"trainer_test.py\", line 201, in test_configure_trainer_and_train_two_steps\r\n    train_dir=train_dir)\r\n  File \"/home/alex/workspace/models-master/object_detection/trainer.py\", line 184, in train\r\n    data_augmentation_options)\r\n  File \"/home/alex/workspace/models-master/object_detection/trainer.py\", line 77, in _create_input_queue\r\n    prefetch_queue_capacity=prefetch_queue_capacity)\r\n  File \"/home/alex/workspace/models-master/object_detection/core/batcher.py\", line 94, in __init__\r\n    num_threads=num_batch_queue_threads)\r\n  File \"/home/alex/.virtualenvs/tens2/lib/python3.5/site-packages/tensorflow/python/training/input.py\", line 920, in batch\r\n    name=name)\r\n  File \"/home/alex/.virtualenvs/tens2/lib/python3.5/site-packages/tensorflow/python/training/input.py\", line 698, in _batch\r\n    tensor_list = _as_tensor_list(tensors)\r\n  File \"/home/alex/.virtualenvs/tens2/lib/python3.5/site-packages/tensorflow/python/training/input.py\", line 385, in _as_tensor_list\r\n    return [tensors[k] for k in sorted(tensors)]\r\nTypeError: unorderable types: tuple() < str()\r\n\r\n----------------------------------------------------------------------\r\nRan 2 tests in 0.019s\r\n\r\nFAILED (errors=1)```", "I used pascal voc 2007 with python3.5, it had the same problem as you mentioned. However, I can run the the same model in python2.7 correctly. Maybe it is the problem of python version compatibility.", "I'm getting the same error with Python 3.6.1. How can we fix this ?", "@rajatmonga  You can try python 2.7 to train your model, if you are hurry. It works.", "Hi guys, I was able to fix this with info from [this](https://github.com/tensorflow/models/pull/1610) thread. Hopefully this gets fixed soon, but it'll work for now.", "I did this too, but for python 3 you need items() instead of iteritems()"]}, {"number": 11077, "title": "something wrong with attentionwrapper?", "body": "I am trying to write a seq2seq model with attention. But it gets the following error:\r\n\r\n    cell_inputs = self._cell_input_fn(inputs, state.attention)\r\nAttributeError: 'tuple' object has no attribute 'attention'\r\n\r\nIt seems the state which initialize by the encoder state that does't has the attribute 'attention'.\r\nHow should I call AttentionWrapper?\r\n\r\nHere is my code:\r\n\r\n        cell_list = []\r\n        for layer_i in xrange(hps.num_layers):\r\n            cell_list.append( tf.contrib.rnn.LSTMCell(hps.num_hidden) )\r\n            with tf.variable_scope('encoder%d'%layer_i):\r\n                cell_list.append( tf.contrib.rnn.LSTMCell(hps.num_hidden) )\r\n        enc_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\r\n\r\n        encoder_outputs, encoder_state = tf.contrib.rnn.static_rnn(enc_cell, enc_inp, dtype=dtype)\r\n\r\n        cell_list = []\r\n        for layer_i in xrange(hps.num_layers):\r\n            with tf.variable_scope('decoder%d'%layer_i):\r\n                cell_list.append( tf.contrib.rnn.LSTMCell(hps.num_hidden) )\r\n        dec_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\r\n\r\n\r\n        new_enc_out = tf.reshape(encoder_outputs, [hps.batch_size, 30, 256])\r\n        attn_mech = tf.contrib.seq2seq.BahdanauAttention(\r\n                num_units = 128, # depth of query mechanism\r\n                memory = new_enc_out, # hidden states to attend (output of RNN)\r\n                normalize=False, # normalize energy term\r\n                name='BahdanauAttention')\r\n\r\n        attn_cell = tf.contrib.seq2seq.AttentionWrapper(\r\n                cell = dec_cell,# Instance of RNNCell\r\n                attention_mechanism = attn_mech, # Instance of AttentionMechanism\r\n                name=\"attention_wrapper\")\r\n\r\n        if hps.mode==\"train\":\r\n            # TrainingHelper does no sampling, only uses inputs\r\n            helper = tf.contrib.seq2seq.TrainingHelper(\r\n                    inputs = dec_inp[:decoder_size], # decoder inputs\r\n                    sequence_length = [1]*decoder_size, # decoder input length\r\n                    name = \"decoder_training_helper\")\r\n\r\n            decoder = tf.contrib.seq2seq.BasicDecoder(\r\n                    cell = attn_cell,\r\n                    helper = helper, # A Helper instance\r\n                    initial_state = encoder_state, # initial state of decoder\r\n                    output_layer = None) # instance of tf.layers.Layer, like Dense\r\n\r\n        decoder_outputs, final_state, _ = tf.contrib.seq2seq.dynamic_decode(decoder)", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "You provide wrong initial state while constructing the decoder in: \r\n\r\n```\r\ndecoder = tf.contrib.seq2seq.BasicDecoder(\r\n                cell = attn_cell,\r\n                helper = helper, # A Helper instance\r\n                initial_state = encoder_state, # initial state of decoder\r\n                output_layer = None) # instance of tf.layers.Layer, like Dense\r\n)\r\n```\r\n`encoder_state` is not an instance of the AttentionWrapperState,  and naturally it has no attribute named \"attention\". You should provide an instance of AttentionWrapperState as initial_state, such as `attn_cell.zero_state(...)`.", "You can use  `initial_state = wrapper.zero_state(...).clone(cell_state=encoder_state)` if you need to pass the encoder state initially.\r\n", "@madhurcodes  I am trying to pass the encoder state into the decoder using the clone, \r\nhowever I got an error `AttributeError: 'tuple' object has no attribute 'clone'` Could you please help. Thanks\r\nA part of my code is as below; \r\n```\r\ndef lstm_cell():\r\n    cell = tf.nn.rnn_cell.LSTMCell(128)\r\n    return cell\r\nencoder_cell_fw = tf.nn.rnn_cell.MultiRNNCell([lstm_cell() for _ in range(num_layers)])\r\nencoder_cell_bw = tf.nn.rnn_cell.MultiRNNCell([lstm_cell() for _ in range(num_layers)])\r\nenc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(encoder_cell_fw, encoder_cell_bw, enc_input,  dype=tf.float32, time_major=False) \r\natt_fn = BahdanauAttention(128, enc_output)\r\ndecoder_cell = tf.nn.rnn_cell.MultiRNNCell(\r\n                                [tf.contrib.seq2seq.AttentionWrapper(lstm_cell(), att_fn) for _ in range(num_layers)])\r\ndec_init_state = decoder_cell.zero_state(5, tf.float32).clone(cell_state=enc_state)"]}, {"number": 11076, "title": "Is it possible to add a feature to tf.Print , so that it will be capable to save tensor value to file?", "body": "Since the while_loop operation is hard to debug, usually we use `tf.Print` to visual tensors in the loop. but `tf.Print` only print a summarization of a tensor. Sometimes I need to get all the value in a tensor, and it is not possible to use `sess.run()` to eval the intermediate tensor value in a loop unless use `TensorArray`. any body would add a **save-to-file** option in `tf.Print` .", "comments": ["Redirect all log out to file", "Would it redirect the whole tensor to a file, or it would just output the summary? ", "@JerrikEph If you don't mind having long lines in your primary log output, you can increase the size of the tensor summaries with the `summarize` argument of `tf.Print`.  You can also use the `message` argument of `tf.Print` to prepend an easy-to-grep string to every tensor it outputs. For example, `tf.Print( ... , message='BIGTENSOR', summarize=1000000, ...)` will print up to 1 million entries of each input tensor and will prepend each tensor with the string \"BIGTENSOR\". You could then copy all those tensors into a separate file by searching for lines containing the string \"BIGTENSOR\" in the original log file; or you could filter those big tensors out by grepping for lines that don't contain that string.", "These solutions don't really work though when what you want is an easily readable CSV file.", "Tensorflow version 2.0.0 of `tf.print` now support the **save-to-file** feature."]}, {"number": 11075, "title": "doc: update the conv1 shape.", "body": "Update the conv1 shape from [5, 5, 32, 32] to [5, 5, 1, 32]. (`1` means gray image.)\r\n\r\nAlthough this is irrelevant with the key content in the chapter, one might be confused when looking at this detail.", "comments": ["Can one of the admins verify this patch?", "@MarkDaoust WDYT?", "It's reasonable for the example model to start with a 1 channel image.\r\n\r\nBut this `variable_scope.md` doc was just merged into `variables.md`, so accepting this now would just set us up for a merge conflict when the sync happens.\r\n\r\nIf you want to update `variables.md` after the sync, that's fine with me.", "Got it. Thank you. I'll update this PR after sync.", "rebased.", "Jenkins, test this please."]}, {"number": 11074, "title": "Adding Swift Language Bindings.", "body": "https://github.com/PerfectlySoft/Perfect-TensorFlow\r\nSupports Swift 3.1/4.0 on both macOS / Ubuntu Linux, with full documentation and example.", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "Can one of the admins verify this patch?", "Just signed the CLA, please check it out, Thank you!\r\n", "@RockfordWei I can't see the CLA in our database. It's associated with email. Could you check again?\r\n\r\nThanks.", "@drpngx Yes, I did sign the document and the email address is 100% matched with my google developer account. The CLA is terrible - not editable, actually, so I had to sign both documents for individual and on behalf my employer and I can review them in any moment online. Do you need my envelope id?", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please."]}, {"number": 11073, "title": "Building from source MAC OS: invalid command 'bdist_wheel'", "body": "I'm having a problem when building tensorflow from source on MAC OS:\r\n\r\n```\r\ntensorflow$ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\nMon 26 Jun 2017 21:40:51 MDT : === Using tmpdir: /var/folders/64/q2l17vhn5c75ym7yv5gv1flw0000gn/T/tmp.XXXXXXXXXX.0Xadn5ti\r\n~/tensorflow/bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles ~/tensorflow\r\n~/tensorflow\r\n/var/folders/64/q2l17vhn5c75ym7yv5gv1flw0000gn/T/tmp.XXXXXXXXXX.0Xadn5ti ~/tensorflow\r\nMon 26 Jun 2017 21:40:56 MDT : === Building wheel\r\nusage: setup.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\r\n   or: setup.py --help [cmd1 cmd2 ...]\r\n   or: setup.py --help-commands\r\n   or: setup.py cmd --help\r\n\r\nerror: invalid command 'bdist_wheel'\r\n```\r\nMy python version:\r\n```\r\n>python --version\r\nPython 2.7.10\r\n\r\n>python3 --version\r\nPython 3.6.2rc1\r\n\r\n>pip3 list\r\nbackports.weakref (1.0rc1)\r\nbleach (1.5.0)\r\nhtml5lib (0.9999999)\r\nMarkdown (2.2.0)\r\nnumpy (1.13.0)\r\npip (9.0.1)\r\nprotobuf (3.3.0)\r\nsetuptools (36.0.1)\r\nsix (1.10.0)\r\ntensorflow (1.2.0)\r\nvirtualenv (15.1.0)\r\nWerkzeug (0.12.2)\r\nwheel (0.29.0)\r\n\r\n```\r\n\r\n\r\nI tried `pip install wheel`, `pip3 install wheel`, `pyenv global 3.6.2` which didn't work.\r\n```\r\n>pip3 install wheel\r\nRequirement already satisfied: wheel in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages\r\n\r\n>pyenv virtualenv 3.6.2\r\nRequirement already satisfied: setuptools in /Users/Jason/.pyenv/versions/3.6.0/envs/3.6.2/lib/python3.6/site-packages\r\nRequirement already satisfied: pip in /Users/Jason/.pyenv/versions/3.6.0/envs/3.6.2/lib/python3.6/site-packages\r\n\r\n>pyenv virtualenv 3.6.2 tensorflow\r\npyenv-virtualenv: `/Users/Jason/.pyenv/versions/tensorflow' already exists.\r\n```\r\n\r\nDoes anyone encounter this problem or know how to fix it?\r\nThank you!", "comments": ["Same problem here on Gentoo Linux, python 3.4.5.", "Looks like the bazel script somehow escaped my virtualenv (again: #10529)\r\n\r\nI installed wheel on my system and it worked.\r\n\r\n@TethysSun: check which python version is used for the command. Also go into the tmp directory and try the command:\r\n```bash\r\npython setup.py --help-commands\r\n```\r\n\r\nIt should list beside other things:\r\n```\r\n(...)\r\nExtra commands:\r\n  (...)\r\n  bdist_wheel       create a wheel distribution\r\n```\r\n\r\nGood luck.\r\n", "@ribx I tried `python setup.py --help-commands` under the tmp directory, and got this:\r\n```\r\n(...)\r\nExtra commands:\r\n  saveopts          save supplied options to setup.cfg or other config file\r\n  develop           install package in 'development mode'\r\n  bdist_mpkg        create a Mac OS X mpkg distribution for Installer.app\r\n  upload_docs       Upload documentation to PyPI\r\n  test              run unit tests after in-place build\r\n  setopt            set an option in setup.cfg or another config file\r\n  install_egg_info  Install an .egg-info directory for the package\r\n  rotate            delete older distributions, keeping N newest files\r\n  egg_info          create a distribution's .egg-info directory\r\n  py2app            create a Mac OS X application or plugin from Python scripts\r\n  alias             define a shortcut to invoke one or more commands\r\n  easy_install      Find/get/install Python packages\r\n  bdist_egg         create an \"egg\" distribution\r\n\r\nusage: setup.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\r\n   or: setup.py --help [cmd1 cmd2 ...]\r\n   or: setup.py --help-commands\r\n   or: setup.py cmd --help\r\n```\r\nI didn't found `bdist_wheel`.\r\n\r\nMy tensorflow is installed under `/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow`\r\nThis means  \r\n```\r\npython3 --version\r\nPython 3.6.2rc1\r\n```\r\nis used for tensorflow, right? I tried install wheel with both `pip install wheel` and `pip3 install wheel`, but it shows wheel is already there.\r\n\r\nWhat should I do now...", "@TethysSun: sounds for me like a problem with your environment. I am sorry, but I cannot help you with OSX.\r\n\r\nJust in case, try to uninstall and reinstall wheel. Also check which python variable you used when you run \"./configure\". Maybe you want to change it to the python3 binary.", "@ribx thanks. I uninstalled python3 and removed tensorflow, then re-installed them again. It worked.\r\n", "Looks like you got it working. Thanks for reporting back."]}, {"number": 11072, "title": "how use tensorflow distributed use mpi version ??", "body": "how use tensorflow distributed use mpi version ??how use tensorflow distributed use mpi version ??how use tensorflow distributed use mpi version ??\r\n\r\nhow use tensorflow distributed use mpi version ??how use tensorflow distributed use mpi version ??", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 11071, "title": "non-chief worker stuck in distributed SYNC mode for graph with two optimizers", "body": "Seems distributed tensorflow cannot train graph with two optimizers in sync mode (one for local update, the other for ps update)\r\n\r\nThere are three parts in my graph: \r\n1. each worker has its own copy of vars as the ps server, but are defined with local variable `collections=[tf.GraphKeys.LOCAL_VARIABLES]`, each worker has its own forward-backward loop based on the local vars and its own optimizer\r\n2. (local variable - ps variable) as the gradient and apply to ps variable with SyncReplicasOptimizer.apply_gradients\r\n3. broadcast the ps variable to the local variable\r\n\r\nThe three parts are run in this way: run subgraph 1 several times, then run subgraph 2 in distributed sync mode to update ps params and then run subgraph 3\r\n\r\n### Source code / logs\r\n```\r\n    if args.job_name == 'ps':\r\n        server.join()\r\n    elif args.job_name == 'worker':\r\n        is_chief = (args.task_index == 0)\r\n        num_gpus = len(worker_spec)\r\n\r\n        ps_device = '/job:ps/cpu:0'\r\n        worker_device = '/job:worker/task:%d/gpu:0' % args.task_index\r\n        with tf.device(\r\n                tf.train.replica_device_setter(cluster=cluster, ps_device=ps_device, worker_device=worker_device)):\r\n            global_step = tf.Variable(args.start_step, name='global_step', trainable=False)\r\n\r\n            print 'building ps params'\r\n            ps_tparams = init_tparams()\r\n\r\n            print 'building local params'\r\n            with tf.device(worker_device):\r\n                worker_tparams = init_tparams(is_local=True)  # define variable in collection tf.GraphKeys.LOCAL_VARIABLES\r\n\r\n            print 'building graph'\r\n\r\n            print '-- local update'\r\n            x, x_mask, y, y_mask, cost = build_graph(worker_tparams, config)\r\n            opt = tf.train.MomentumOptimizer(config.lr, config.mr)\r\n            updates = worker_tparams\r\n            grads = tf.gradients(cost, updates, colocate_gradients_with_ops=True)\r\n            clipped_grads, _ = tf.clip_by_global_norm(grads, config.clip_grads)\r\n            train_op = opt.apply_gradients(zip(clipped_grads, updates))\r\n\r\n            print '-- reduce average'\r\n            ps_updates = ps_tparams\r\n            avg_grads = [tf.sub(var, ps_var) for var, ps_var in zip(updates, ps_updates)]\r\n            bopt = tf.train.MomentumOptimizer(config.blr, config.bmr, use_nesterov=True)\r\n            bopt = tf.train.SyncReplicasOptimizerV2(bopt, replicas_to_aggregate=num_gpus, total_num_replicas=num_gpus)\r\n            update_op = bopt.apply_gradients(zip(avg_grads, ps_updates), global_step=global_step)\r\n\r\n            print '-- broadcast'\r\n            broadcast_ops = []\r\n            for kk, pp in ps_tparams.items():\r\n                broadcast_ops.append(worker_tparams[kk].assign(pp).op)\r\n\r\n            # Others related to sync mode\r\n            chief_queue_runner = bopt.get_chief_queue_runner()\r\n            sync_init_op = bopt.get_init_tokens_op()\r\n\r\n            sv = tf.train.Supervisor(\r\n                is_chief=is_chief,\r\n                logdir=config.ckp,\r\n                init_op=tf.global_variables_initializer(),\r\n                local_init_op=tf.local_variables_initializer(),\r\n                global_step=global_step)\r\n\r\n            if is_chief:\r\n                print('Worker %d: Initializing session' % args.task_index)\r\n            else:\r\n                print('Worker %d: Waiting for session to be initialized' % args.task_index)\r\n\r\n            sess_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False,\r\n                                         device_filters=['/job:ps', '/job:worker/task:%d' % args.task_index])\r\n            with sv.managed_session(server.target, config=sess_config) as sess:\r\n                print('Worker %d: Session initialization completed' % args.task_index)\r\n\r\n                if is_chief:\r\n                    # Chief worker will start the chief queue runner and call the init op\r\n                    sess.run(sync_init_op)\r\n                    sv.start_queue_runners(sess, [chief_queue_runner])\r\n```\r\nThe non-chief works stuck at `sv.managed_session` and showing below message again and again:\r\n`I tensorflow/core/distributed_runtime/master_session.cc:993] Start master session a4de1cec7011a62d with config:`\r\nThe code can run successfully when there is no local optimizer.\r\n\r\n### System information\r\n- Linux Ubuntu 14.04\r\n- CUDA 8.0\r\n- tf 0.12.0-rc1\r\n- GPU: GeForce GTX 1080", "comments": ["@mrry can you comment? Is this a TF bug?", "I don't think it's a bug, although without a self-contained example I can't say for sure.\r\n\r\nThe most likely cause of `sv.managed_session()` hanging is because `sv.prepare_or_wait_for_session()`  is blocking inside [`SessionManager.wait_for_session()`](https://github.com/tensorflow/tensorflow/blob/90b2a38a1f7cd7cde0f012249407574e97583096/tensorflow/python/training/session_manager.py#L363). The non-chief workers will block in here until the chief initializes all of the variables (and hence the `ready_op` returns `True`). It's possible that, because you have a slightly unusual variable configuration, the `ready_op` never returns `True`. You might need to customize the `ready_op` (or possibly the `init_op`) when constructing the `tf.train.Supervisor` to work for your configuration.", "Thanks @mrry for your reply. What do you mean by \"slightly unusual variable configuration\"? Is there any reference of the customized `ready_op` (or `init_op`) you mentioned?", "The \"slightly unusual\" part is this line:\r\n\r\n```python\r\n            print 'building local params'\r\n            with tf.device(worker_device):\r\n                worker_tparams = init_tparams(is_local=True)  # define variable in collection tf.GraphKeys.LOCAL_VARIABLES\r\n```\r\n\r\n...which *should* work, but it might conflict with some of the assumptions in the `tf.train.SyncReplicasOptimizer` code.\r\n\r\nI'd expect [this log statement](https://github.com/tensorflow/tensorflow/blob/9a4a736ae45a1cc88db472354c5129853ec9fb50/tensorflow/python/training/session_manager.py#L417) to provide useful information in the non-chief replicas. Does it print anything?", "No, there is no \"Waiting for model to be ready.\" info printed. Just \"Start master session *** with config:\" hang there over and over again.\r\nAnd, the problem fixed after I remove the local optimizer below and write my own by following [this](https://stackoverflow.com/questions/39167070/implementing-gradient-descent-in-tensorflow-instead-of-using-the-one-provided-wi).\r\nold:\r\n```\r\n            print '-- local update'\r\n            x, x_mask, y, y_mask, cost = build_graph(worker_tparams, config)\r\n            opt = tf.train.MomentumOptimizer(config.lr, config.mr)\r\n            updates = worker_tparams\r\n            grads = tf.gradients(cost, updates, colocate_gradients_with_ops=True)\r\n            clipped_grads, _ = tf.clip_by_global_norm(grads, config.clip_grads)\r\n            train_op = opt.apply_gradients(zip(clipped_grads, updates))\r\n```\r\nnew:\r\n```\r\n            print '-- local update'\r\n            local_train_ops = []\r\n            vars = worker_tparams\r\n            x, x_mask, y, y_mask, cost = build_graph(worker_tparams, config)\r\n            grads = tf.gradients(cost, vars, colocate_gradients_with_ops=True)\r\n            clipped_grads, _ = tf.clip_by_global_norm(grads, config.clip_grads)\r\n            for var, grad in zip(vars, clipped_grads):\r\n                local_train_ops.append(var.assign_sub(grad * tf.convert_to_tensor(config.lr)))  # var -= lr * grad, GradientDescentOptimizer instead of MomentumOptimizer\r\n            train_op = tf.group(*local_train_ops)\r\n```\r\nBut new problem occurs, the \"sync\" mode seems not stable. The loss will increase dramatically suddenly after several global updates.", "@hellolovetiger I just ran into a similar as you - repeating \"Start master session xxx with config:\" messages for all non chief workers. I'm curious if you ever found a solution.\r\n\r\nBtw I suspect the reason why your training loss increases after the change you posted above is that when you call apply_gradient() it applies the gradient in the negative direction. I guess TF assumes you will be minimizing a loss function. This tripped me up once when I actually wanted to maximize a function.\r\n\r\n*edited", "@adamklec , the issue is gone after I replaced the optimizer for local update with my own implementation. This may not apply to your situation. You may want to read the details above. \r\nThanks for you kind reminder of the \"loss increases\" issue. Yes, you are right. I got the wrong direction when incorporating the gradients from the workers.", "@hellolovetiger Actually I did just that last night! I replaced SyncReplicasOptimizer with my own gradient accumulator and the problem went away. Cheers!", "Cool!", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @mrry: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @mrry: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @mrry: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I'm going to close this issue, because it seems like a workaround was found, and the eventual plan is to drop support for `tf.train.SyncReplicasOptimizer`, replacing it with a more robust implementation in `DistributionStrategy`."]}, {"number": 11070, "title": "deadloop on replaying kernel when profiling DL network with nvprof ", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes, use models from Keras ,e.g. VGG16\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary\r\n- **TensorFlow version (use command below)**:\r\n1.1.0\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n8.0/5.1\r\n- **GPU model and memory**:\r\nNvidia K40, 10G\r\n- **Exact command to reproduce**:\r\nnvprof -metrics flop_sp_efficiency python train_vgg\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nThe profiling process will end with a deadloop in replaying some kernels with prompt:\r\n\"Replaying kernel \"cgemm_sm35_ldg_tn_64x8x64x16x16\"\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@poxvoculi, do we support nvprof?", "I believe it's possible to use nvprof with TF, but I haven't used it in a long time.  Probably @zheng-xq is better informed.", "I hit the same issue - worked around by moving out the actual CUDA kernel into a PyCUDA wrapper and profiling that along with captured launch parameters. I think what's happening is there's some startup overhead in TensorFlow that ends up getting captured by the CUDA profiler unnecessarily (and there's a mountain of it). Another thing to try could be to instrument using the CUDA profiler start/stop API *inside* tensorflow code, because if you use the API from python it has to be inside a session. Something else you could try is to make py_func ops using Python CUDA profiler start/stop API (an idea I just came up with right now, haven't tried it yet)\r\n\r\n\r\n\r\n```\r\n# add to shell: export LD_LIBRARY_PATH=/usr/local/cuda-8.0/extras/CUPTI/lib64/\r\n\r\n\r\nimport ctypes\r\n\r\n_cudart = ctypes.CDLL('libcudart.so')\r\n\r\ndef cu_prof_start():\r\n    ret = _cudart.cudaProfilerStart()\r\n    if ret != 0:\r\n        raise Exception('cudaProfilerStart() returned %d' % ret)\r\n\r\n\r\ndef cu_prof_stop():\r\n    ret = _cudart.cudaProfilerStop()\r\n    if ret != 0:\r\n        raise Exception('cudaProfilerStop() returned %d' % ret)\r\n```\r\n", "Have anyone know how to use the nvprof in tensorflow?", "Nagging Assignee @robieta: It has been 65 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@linrio Hi, please go through [this link ](https://docs.nvidia.com/cuda/profiler-users-guide/index.html)which gives a picture on using nvprof.\r\n\r\n@zheng-xq  Hi, could you please look into this ?\r\n\r\n", "Does this issue still persist with the latest tensorflow version ? If so, please open a new issue providing all the information asked in[ this template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thank you !"]}, {"number": 11069, "title": "Fix for contrib.layers test that was raising an IndexError", "body": "Following the discussion at https://github.com/tensorflow/tensorflow/pull/11049\r\n\r\nRunning the ```testSparsePartialFlatten``` test would occasionally raise the following error:\r\n\r\n```======================================================================\r\nERROR: testSparsePartialFlatten (__main__.PartialFlattenTest)\r\nTest `_inner_flatten` on `SparseTensor`s.\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/bin/bazel_pip/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/bazel_pip/tensorflow/contrib/layers/python/layers/layers_test.py\", line 1434, in testSparsePartialFlatten\r\n    expected_indices, expected_values, _ = _sparsify(reshaped_random_)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/bin/bazel_pip/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/bazel_pip/tensorflow/contrib/layers/python/layers/layers_test.py\", line 1399, in _sparsify\r\n    values = array[non_zero]\r\nIndexError: index 3 is out of bounds for axis 1 with size 3\r\n```\r\n\r\nThis seems to be due to inconsistent behaviour from numpy.nonzero when operating on an array of over 4 dimensions. Reducing the size of the array in the test seems to eliminate the problem.\r\n\r\nNote: As the problem would only arise in some test runs, and not others. It's important that this commit be tested several times to ensure elimination of the issue.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Many thanks @jalammar !\r\n\r\nJust to reference & summarize the numpy issue:\r\nhttps://github.com/numpy/numpy/issues/9304\r\nIt has been fixed upstream."]}, {"number": 11068, "title": "Global Variables of graph not loaded on different computer", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:l\r\n- MacOSx :\r\n- **TensorFlow installed from binary** (pip install):\r\n- Tensorflow version 1.1.0\r\n\r\n### Describe the problem\r\nI have been having issues trying to load a model from a checkpoint file. When trying to access any of the global variables created in our tensorflow model (for example `print sess.run(W)`, we get an error \r\n\r\n> NameError: global name 'W' is not defined\r\n\r\nWe expect W and other global variables to be present, but none are. Thanks for the help.\r\n\r\n### Source code / logs\r\n\r\n```\r\n        init_op = tf.global_variables_initializer()\r\n        with tf.Session() as sess:\r\n            sess.run(init_op)\r\n            saver = tf.train.import_meta_graph('FinalTensorflowModel/model.ckpt.meta')\r\n            saver.restore(sess,\"FinalTensorflowModel/model.ckpt\")\r\n            print sess.run(W)\r\n            for idx, row in df.iterrows():\r\n                profitability = tf.matmul(row,W) + b\r\n                print sess.run(profitability)\r\n```", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 11067, "title": "Feature Request: How to Access Attention Weights of Attention Wrapper", "body": "OS: macOS Sierra version 10.12.5\r\nTensorFlow Version: v1.2.0-rc2-21-g12f033d 1.2.0\r\n\r\nThis is a two-part request related to `tensorflow.contrib.seq2seq`. I would like the ability to visualize the attention weights of the `AttentionWrapper`, but I'm hampered by the lack of examples and I'm struggling to infer the input for `BahdanauAttention`'s `__call__` method's argument `previous_alignments`.\r\n\r\nFirst, could someone clarify how to access the attention weights?\r\n\r\nSecond, would it be possible to add some tool that visualizes the attention weights (possibly to TensorBoard)?", "comments": ["Thanks for the issue, but this question looks like a feature request that belongs in tensorboard \r\nhttp://github.com/tensorflow/tensorboard/issues\r\nThanks!\r\n", "@aselle , perhaps the second question belongs to TensorBoard, but I don't think that the first does at all. Even if TensorBoard had the visualization capability I'm looking for, if I wanted to add a summary to visualize the attention weights, I wouldn't know how to access the weights to add them as summaries. I think that either an example or additional documentation would be useful and relevant to TensorFlow, not TensorBoard.", "@aselle If you have time, maybe you could answer [the question](https://stackoverflow.com/questions/44613173/how-to-visualize-attention-weights-from-attentionwrapper) on Stack Overflow? It's clear I'm not the only person struggling to access the AttentionWrapper weights.", "@ebrevdo , you were helpful on an earlier thread. If you have the time, could you please provide an answer to my first question of how to access the attention mechanism's weights?", "For anyone else wondering, you can access the alignments by setting `alignment_history=True` in `AttentionWrapper`. So if I have an object called `model` with my decoder's final state as a member variable, I can access the alignments as follows:\r\n\r\n`\r\n    alignments = sess.run([model.decoder_final_states[0].alignment_history.stack()],\r\n                          feed_dict={whatever})\r\n`", "Your approach is the right one.  We don't enable alignments history by\ndefault because:\n\n1.  It requires extra memory\n2.  It's not always possible, i.e. when using a beam search decoder.\n\nOn Jun 27, 2017 9:59 AM, \"Rylan Schaeffer\" <notifications@github.com> wrote:\n\n> @ebrevdo <https://github.com/ebrevdo> , you were helpful on an earlier\n> thread. If you have the time, could you please provide an answer to my\n> first question of how to access the attention mechanism's weights?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11067#issuecomment-311420504>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim_DZWf-z4I6VuuBQEv9utxrueGygks5sITT3gaJpZM4OF1_h>\n> .\n>\n", "@ebrevdo , can you clarify what `_keys` are for `AttentionWrapper`?\r\n\r\nAlso, to confirm, the `normalizer` referred to by `AttentionWrapper`'s `call` method's `Step 4: Calculate the alignments by passing the score through the normalizer` is the attention mechanism's `probability_fn`?\r\n\r\n", "@ebrevdo , I'm having a strange problem with my Sequence to Sequence model's alignment history.\r\n\r\nMy model's alignment values are initially uniformly distributed (~0.002, for encoder outputs of 500 steps), which makes sense. However, the alignment values remain roughly the same (~0.002) even after training, despite the fact that my model's accuracy climbs from chance (25%) to 100%. My problem is structured in a way that the only way to do well at it is for the decoder to learn to pay attention.\r\n\r\nI have no idea what might be causing this - has anyone experienced anything similar to this? Alternatively, does anyone have suggestions for debugging this issue?\r\n\r\nEdit: I'm posting this on [Stack Overflow](https://stackoverflow.com/questions/45285167/uniformly-distributed-alignment-weights-for-seq-to-seq-model). @aselle , if you know the answer, or know someone who knows, I'd appreciate it!", "@oahziur , do you have an idea of what might be causing my problem?", "I thought I'd add more information and code, in case that can help someone help me.\r\n\r\nAs background, both my inputs and labeled outputs at each time step are vectors of shape `(4, )`. I run my encoder for 500 steps i.e. inputs have shape `(minibatch size, 500, 4)`, and my decoder runs for approximately 40-41 steps i.e. final output has shape `(minibatch size, 41, 4)`. Each output label depends roughly on 12 sequential inputs, so for example, the first output depends on inputs 1-12, the second output depends on inputs 13-24, etc. I don't use embeddings.\r\n\r\nI reduced my model to a single layer encoder, single layer decoder to eliminate any mistake I might be making with multi-layered architectures. The encoder is a bidirectional RNN.\r\n\r\nAt the start of training, my `alignment_history` has roughly random uniform weights. Its shape is (41, minibatch size, 500) (although I could transpose it from time-major to batch-major). `alignment_history` will have values between 0.001739 and 0.002241, which makes sense - randomly initialized attention should be around 1/500 = 0.002. Additionally, my model performs at chance (25% classification accuracy).\r\n\r\nDuring training, my model converges to 100% classification accuracy on both training and validation data, as shown below.\r\n\r\n<img width=\"894\" alt=\"screen shot 2017-07-29 at 12 01 29 pm\" src=\"https://user-images.githubusercontent.com/8942987/28747525-c4975280-7455-11e7-8cfa-57680188c16f.png\">\r\n\r\nThe model never sees the same training data twice, so I'm 99% confident that the model isn't memorizing the training data. However, after training, the values of alignment_history effectively haven't changed; the values now look randomly chosen from between 0.00185 and 0.00219.\r\n\r\nMy code is relatively straightforward. I have a class encapsulating my model. One method instantiates a RNN cell:\r\n\r\n    @staticmethod\r\n    def _create_lstm_cell(cell_size):\r\n        \"\"\"\r\n        Creates a RNN cell. If lstm_or_gru is True (default), create a Layer\r\n        Normalized LSTM cell (if layer_norm is True (default); otherwise,\r\n        create a vanilla LSTM cell. If lstm_or_gru is False, create a Gated\r\n        Recurrent Unit cell.\r\n        \"\"\"\r\n\r\n        if tf.flags.FLAGS.lstm_or_gru:\r\n            if tf.flags.FLAGS.layer_norm:\r\n                return LayerNormBasicLSTMCell(cell_size)\r\n            else:\r\n                return BasicLSTMCell(cell_size)\r\n        else:\r\n            return GRUCell(cell_size)\r\n\r\nI have one method for building the encoder:\r\n\r\n    def _define_encoder(self):\r\n        \"\"\"\r\n        Construct an encoder RNN using a bidirectional layer.\r\n        \"\"\"\r\n\r\n        with tf.variable_scope('define_encoder'):\r\n\r\n            encoder_outputs, encoder_final_states = bidirectional_dynamic_rnn(\r\n                cell_fw=self._create_lstm_cell(ENCODER_SINGLE_DIRECTION_SIZE),\r\n                cell_bw=self._create_lstm_cell(ENCODER_SINGLE_DIRECTION_SIZE),\r\n                inputs=self.x,\r\n                dtype=tf.float32,\r\n                sequence_length=self.x_lengths,\r\n                time_major=False  # default\r\n            )\r\n\r\n            # concatenate forward and backwards encoder outputs\r\n            encoder_outputs = tf.concat(encoder_outputs, axis=-1)\r\n\r\n            # concatenate forward and backwards cell states\r\n            new_c = tf.concat([encoder_final_states[0].c, encoder_final_states[1].c], axis=1)\r\n            new_h = tf.concat([encoder_final_states[0].h, encoder_final_states[1].h], axis=1)\r\n            encoder_final_states = (LSTMStateTuple(c=new_c, h=new_h),)\r\n\r\n        return encoder_outputs, encoder_final_states\r\n\r\nI similarly have another method for building the decoder:\r\n\r\n    def _define_decoder(self, encoder_outputs, encoder_final_states):\r\n        \"\"\"\r\n        Construct a decoder complete with an attention mechanism. The encoder's\r\n        final states will be used as the decoder's initial states.\r\n        \"\"\"\r\n\r\n \r\n\r\n        with tf.variable_scope('define_decoder'):\r\n            # instantiate attention mechanism\r\n            attention_mechanism = BahdanauAttention(num_units=DECODER_SIZE,\r\n                                                    memory=encoder_outputs,\r\n                                                    normalize=True)\r\n\r\n            # wrap LSTM cell with attention mechanism\r\n            attention_cell = AttentionWrapper(cell=self._create_lstm_cell(cell_size=DECODER_SIZE),\r\n                                              attention_mechanism=attention_mechanism,\r\n                                              # output_attention=False,  # doesn't seem to affect alignments\r\n                                              alignment_history=True,\r\n                                              attention_layer_size=DECODER_SIZE)  # arbitrarily chosen\r\n\r\n            # create initial attention state of zeros everywhere\r\n            decoder_initial_state = attention_cell.zero_state(batch_size=tf.flags.FLAGS.batch_size, dtype=tf.float32).clone(cell_state=encoder_final_states[0])\r\n\r\n\r\n            # TODO: switch this out at inference time\r\n            training_helper = TrainingHelper(inputs=self.y,  # feed in ground truth\r\n                                             sequence_length=self.y_lengths)  # feed in sequence lengths\r\n\r\n            decoder = BasicDecoder(cell=attention_cell,\r\n                                   helper=training_helper,\r\n                                   initial_state=decoder_initial_state\r\n                                   )\r\n\r\n            # run decoder over input sequence\r\n            decoder_outputs, decoder_final_states, decoder_final_sequence_lengths = dynamic_decode(\r\n                decoder=decoder,\r\n                maximum_iterations=41,\r\n                impute_finished=True)\r\n\r\n            decoder_outputs = decoder_outputs[0]\r\n            decoder_final_states = (decoder_final_states,)\r\n\r\n        return decoder_outputs, decoder_final_states\r\n\r\nI use both of these methods, and then project the output of the decoder to the same dimensionality as my labels.\r\n\r\n    def _add_inference(self):\r\n        \"\"\"\r\n        Create a Sequence-to-Sequence model using a bidirectional encoder and an\r\n        attention mechanism-wrapped decoder.\r\n        \r\n        The outputs of the decoder need to be projected to a lower dimensional\r\n        space i.e. from DECODER_SIZE to 4.\r\n        \"\"\"\r\n\r\n        with tf.variable_scope('add_inference'):\r\n            encoder_outputs, encoder_final_states = self._define_encoder()\r\n            decoder_outputs, decoder_final_states = self._define_decoder(encoder_outputs, encoder_final_states)\r\n\r\n            weights = tf.Variable(tf.truncated_normal(shape=[DECODER_SIZE, 4]))\r\n            bias = tf.Variable(tf.truncated_normal(shape=[4]))\r\n            logits = tf.tensordot(decoder_outputs, weights, axes=[[2], [0]]) + bias  # 2nd dimension of decoder outputs, 0th dimension of weights\r\n\r\n        return encoder_final_states, decoder_final_states, logits\r\n\r\n", "Most of my code was written before the NMT tutorial was released, so I read the code and then stepped through it, but I can't find any glaring differences. I do have a couple of additional questions.\r\n\r\n1) I have two hypotheses. One is that I'm incorrectly accessing my model's alignments, and the other is that I'm screwing something up in a much more significant way. Just to eliminate the first as a possibility, the correct way to access the decoder's alignments is through setting alignment_history=True in AttentionWrapper and then examining the values in decoder_final_states[0].alignment_history.stack(). Is this correct?\r\n\r\n2) How is the attention mechanism's num_units chosen? Is the attention mechanism's number of units required to match the number of units in the RNN cell as well as the number of units in the AttentionWrapper, or is that not necessary?\r\n\r\n3) I'm confused by the terminology used regarding memory, queries and keys. Memory and keys are both defined in English as \"the set of source hidden states\", but mathematically they're defined differently i.e. memory is W_2\\overline{h}_s for Bahdanau Attention, but the keys are W_1h_t for Bahdanau Attention. My guess is that the tutorial means to say that the query h_t is converted into a key using W_1, and that key is then compared against keys generated from the encoder's hidden states i.e. W\\overline{h}_s. Is this correct, or am I misunderstanding something?", "@lmthang , if you could help, I'd really appreciate it!"]}, {"number": 11066, "title": "Can we disable tensorflow's theading?", "body": "I'm running tensorflow in a simulator that does not support mult-threading. Is it possible to disable tensorflow's multi-threading?", "comments": ["No. It is possible to use a single executor thread and a single eigen thread, but the API as designed is likely to deadlock if you have no threads.", "Can you elaborate on how to use a single executor thread and a single eigen thread? I tracked down the error and the deepest failure point i can find is \"impl_.reset(new ThreadPool::Impl(env, thread_options, \"tf_\" + name, num_threads, low_latency_hint));\"", "The ConfigProto you pass to the first session you build in a process lets you specify how many dispatch and compute threads to use.", "I forgot to mention I'm using C++., Do you know how to do that in C++? Also to confirm, you mean it is impossible to have no thread, at least two treads, one executor and one eigen are needed, right? ", "In C++ you can also use a configproto in sessionoptions: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/public/session_options.h\r\n\r\nThe minimum I know you can have is 3 threads: the two you mentioned and the one which is calling tf code.", "I changed num_threads at line 116 in threadpool.cc to 0 and I bypassed the threading error but of course when I do sesson.run() I got pointer pointing to invalid address.", "0 is not supported, as I said above. It'll only work if each number of\nthreads is at least 1.\n\nOn Tue, Jun 27, 2017 at 7:38 AM, kyo744 <notifications@github.com> wrote:\n\n> I changed num_threads at line 116 in threadpool.cc to 0 and I bypassed the\n> threading error but of course when I do sesson.run() I got pointer pointing\n> invalid address.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11066#issuecomment-311378304>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxd2pat4f0M-Kw1QTvGnssMqzz2Z1ks5sIRP2gaJpZM4OFtTa>\n> .\n>\n\n\n\n-- \n - Alex\n", "try this:\r\n```\r\ntensorflow::SessionOptions GetSessionOptions()\r\n{\r\n\ttensorflow::SessionOptions sessionOptions;\r\n\ttensorflow::ConfigProto& config = sessionOptions.config;\r\n\tconfig.set_intra_op_parallelism_threads(1);\r\n\tconfig.set_inter_op_parallelism_threads(1);\r\n\treturn sessionOptions;\r\n}\r\n```\r\n```\r\nstd::unique_ptr<tensorflow::Session> session;\r\nsession.reset(tensorflow::NewSession(GetSessionOptions()));\r\n```", "I've tried that. Any non zero value would cause my simulator to crash.", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 11065, "title": "Unable to install TensorFlow properly on windows for python. Please help!", "body": "I am trying to install TensorFlow on Windows 10 for Python 3.5 64 bit. It says \"successfully installed\" but when I test it using \"import tensorflow as tf\" command in the command prompt, it shows a long error saying \"DLL load failed\" and \"failed to load native tensorflow runtime\".\r\n\r\nI had tried installing it earlier but it didn't work. When I uninstalled from cmd: pip uninstall tensorflow, it said \"successfully uninstalled\". But when I try reinstalling, it says, \"requirement already satisfied\" and gives the same long error (DLL load failed etc.) when i type \"import tensorflow as tf\". This doesn't make sense. Please help.\r\n\r\nPlease see images.\r\n![image](https://user-images.githubusercontent.com/26795925/27552208-0fd2058e-5ac4-11e7-82d1-c3e16bc74261.png)\r\n\r\n![image](https://user-images.githubusercontent.com/26795925/27552227-1e0670f4-5ac4-11e7-9ee8-60969c0a91b9.png)\r\n", "comments": ["Try running the script here to diagnose DLL problems with TensorFlow on Windows: https://gist.github.com/mrry/ee5dbcfdd045fa48a27d56664411d41c", "Hi,\r\n          In order to get tensor flow working with `Windows 10 `with old version of `Python,cUDNN,Nvidia.` Classical solution link will be [https://github.com/tensorflow/tensorflow/issues/10994](https://github.com/tensorflow/tensorflow/issues/10994)", "Hi\r\nDid you solve the problem of installing TensorFlow on Windows 10 for Python 3.5 64 bit?\r\nI got exactly same problem like you.\r\nCan you advise me to solve that problem with your experience?\r\nI am teaching java in school but a novice user in Python and Deep learning. Now I am studying those.\r\nSo please guide me detailly how to solve tensorflow installing problems in Windows.\r\nThank you", "Hi,\r\n      Yes, It works. Please follow this thread and this link  : [Windows_10 install](https://stackoverflow.com/questions/42011070/on-windows-running-import-tensorflow-generates-no-module-named-pywrap-tenso)", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "https://youtu.be/OiPjSnKAmSI", "@ruthyaneth Now,it's far more stable.You can do it far more easily with nightly builds.", "After searching a lot and trying to install and reinstall Python, i found the solution was very simple\r\n\r\nuse the following for windows\r\n\r\npython -m pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.12.0-py3-none-any.whl\r\n\r\nchange to following on mac\r\n\r\npython3 -m pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.12.0-py3-none-any.whl\r\n\r\nfor Anaconda use corresponding conda", "Thanks, it worked for me\r\n", "i am solve this problem \r\nyou type  this follow this process \r\nright click to windows button and open windows power shell\r\nand write this    pip install tensorflow==2.3.0\r\nok and give time to his computer to download this\r\nand check the tensorflow to using import command \r\nyou type import tensorflow\r\nand computer gives you no errors \r\nyou did it your tensorflow successfuly installed thank you ..."]}, {"number": 11064, "title": "Unsupported CPU features cause runtime errors.", "body": "I'm having runtime errors when running Tensorflow programs. I tracked down the problem and the source seems to be that the shared object I have compiled includes CPU features that are not supported by my processor. That causes some pointers pointing to invalid addresses. Right now there are 37 CPU feature defined in tensorflow/core/platform/cpu_info.h. My question is how to not include some of those when compiling Tensorflow's source code.", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 11063, "title": "TensorFlow 1.2.0 depends on a 5 year old release of the Markdown package", "body": "### System information\r\n\r\n- This is an installation problem\r\n- Windows 10 64 bit\r\n- Tensorflow installed from pip\r\n- Version 1.2.0\r\n- Bazel version: None\r\n- CUDA v8 CuDNN v6\r\n- GTX980Ti and Pascal TITAN X\r\n- python -m pip install tensorflow-gpu==1.2.0\r\n\r\n### Describe the problem\r\n\r\nTensorFlow 1.2.0 has added a new dependency on the markdown==2.2.0 package. This package is being actively maintained, and the latest version of it is 2.6.8. However, Tensorflow 1.2.0 has a strict dependency on version 2.2.0, which was released July 2012; almost 5 years ago. As such, the package no longer installs cleanly on all modern distributions of python.\r\n\r\nI am attempting to use the Windows python-3.6.1-embed-amd64 release, but when attempting to install either Tensorflow==1.2.0 or markdown==2.2.0 on this environment, installation fails due to a syntax issue in the Markdown package. I haven't delved deep into the issue, but have included log output from trying to use pip to install tensorflow on this system.\r\n\r\nHowever, the most recent version of markdown 2.6.8 installs fine under this environment.\r\n\r\nIs there a good reason for why Tensorflow specifically requires the old version? Can the dependency be upgraded to a later version that will support the embedded 3.6.1 Windows release of python?\r\n\r\nSurely taking on a legacy dependency like this is not ideal, and should hopefully be fixed.\r\n\r\n### Source code / logs\r\n\r\n```\r\nC:\\Users\\Valdemar\\Downloads\\python-3.6.1-embed-amd64>python -m pip install tensorflow==1.2.0\r\nCollecting tensorflow==1.2.0\r\n  Using cached tensorflow-1.2.0-cp36-cp36m-win_amd64.whl\r\nRequirement already satisfied: six>=1.10.0 in c:\\users\\valdemar\\downloads\\python-3.6.1-embed-amd64\\lib\\site-packages (from tensorflow==1.2.0)\r\nRequirement already satisfied: protobuf>=3.2.0 in c:\\users\\valdemar\\downloads\\python-3.6.1-embed-amd64\\lib\\site-packages (from tensorflow==1.2.0)\r\nCollecting numpy>=1.11.0 (from tensorflow==1.2.0)\r\n  Using cached numpy-1.13.0-cp36-none-win_amd64.whl\r\nRequirement already satisfied: html5lib==0.9999999 in c:\\users\\valdemar\\downloads\\python-3.6.1-embed-amd64\\lib\\site-packages (from tensorflow==1.2.0)\r\nCollecting werkzeug>=0.11.10 (from tensorflow==1.2.0)\r\n  Using cached Werkzeug-0.12.2-py2.py3-none-any.whl\r\nCollecting markdown==2.2.0 (from tensorflow==1.2.0)\r\n  Using cached Markdown-2.2.0.tar.gz\r\nCollecting backports.weakref==1.0rc1 (from tensorflow==1.2.0)\r\n  Using cached backports.weakref-1.0rc1-py3-none-any.whl\r\nRequirement already satisfied: bleach==1.5.0 in c:\\users\\valdemar\\downloads\\python-3.6.1-embed-amd64\\lib\\site-packages (from tensorflow==1.2.0)\r\nRequirement already satisfied: wheel>=0.26 in c:\\users\\valdemar\\downloads\\python-3.6.1-embed-amd64\\lib\\site-packages (from tensorflow==1.2.0)\r\nRequirement already satisfied: setuptools in c:\\users\\valdemar\\downloads\\python-3.6.1-embed-amd64\\lib\\site-packages (from protobuf>=3.2.0->tensorflow==1.2.0)\r\nBuilding wheels for collected packages: markdown\r\n  Running setup.py bdist_wheel for markdown ... error\r\n  Complete output from command C:\\Users\\Valdemar\\Downloads\\python-3.6.1-embed-amd64\\python.exe -u -c \"import setuptools, tokenize;__file__='C:\\\\Users\\\\Valdemar\\\\AppData\\\\Local\\\\Temp\\\\pip-build-9rmoawj8\\\\markdown\\\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d C:\\Users\\Valdemar\\AppData\\Local\\Temp\\tmpeowqyarkpip-wheel- --python-tag cp36:\r\n  running bdist_wheel\r\n  running build\r\n  running build_py\r\n  creating build\r\n  creating build\\lib\r\n  creating build\\lib\\markdown\r\n  copying markdown\\blockparser.py -> build\\lib\\markdown\r\n  copying markdown\\blockprocessors.py -> build\\lib\\markdown\r\n  copying markdown\\etree_loader.py -> build\\lib\\markdown\r\n  copying markdown\\inlinepatterns.py -> build\\lib\\markdown\r\n  copying markdown\\odict.py -> build\\lib\\markdown\r\n  copying markdown\\postprocessors.py -> build\\lib\\markdown\r\n  copying markdown\\preprocessors.py -> build\\lib\\markdown\r\n  copying markdown\\serializers.py -> build\\lib\\markdown\r\n  copying markdown\\treeprocessors.py -> build\\lib\\markdown\r\n  copying markdown\\util.py -> build\\lib\\markdown\r\n  copying markdown\\__init__.py -> build\\lib\\markdown\r\n  copying markdown\\__main__.py -> build\\lib\\markdown\r\n  creating build\\lib\\markdown\\extensions\r\n  copying markdown\\extensions\\abbr.py -> build\\lib\\markdown\\extensions\r\n  copying markdown\\extensions\\attr_list.py -> build\\lib\\markdown\\extensions\r\n  copying markdown\\extensions\\codehilite.py -> build\\lib\\markdown\\extensions\r\n  copying markdown\\extensions\\def_list.py -> build\\lib\\markdown\\extensions\r\n  copying markdown\\extensions\\extra.py -> build\\lib\\markdown\\extensions\r\n  copying markdown\\extensions\\fenced_code.py -> build\\lib\\markdown\\extensions\r\n  copying markdown\\extensions\\footnotes.py -> build\\lib\\markdown\\extensions\r\n  copying markdown\\extensions\\headerid.py -> build\\lib\\markdown\\extensions\r\n  copying markdown\\extensions\\html_tidy.py -> build\\lib\\markdown\\extensions\r\n  copying markdown\\extensions\\meta.py -> build\\lib\\markdown\\extensions\r\n  copying markdown\\extensions\\nl2br.py -> build\\lib\\markdown\\extensions\r\n  copying markdown\\extensions\\rss.py -> build\\lib\\markdown\\extensions\r\n  copying markdown\\extensions\\sane_lists.py -> build\\lib\\markdown\\extensions\r\n  copying markdown\\extensions\\smart_strong.py -> build\\lib\\markdown\\extensions\r\n  copying markdown\\extensions\\tables.py -> build\\lib\\markdown\\extensions\r\n  copying markdown\\extensions\\toc.py -> build\\lib\\markdown\\extensions\r\n  copying markdown\\extensions\\wikilinks.py -> build\\lib\\markdown\\extensions\r\n  copying markdown\\extensions\\__init__.py -> build\\lib\\markdown\\extensions\r\n  running build_scripts\r\n  creating build\\scripts-3.6\r\n  copying and adjusting bin\\markdown_py -> build\\scripts-3.6\r\n  running build_docs\r\n  Traceback (most recent call last):\r\n    File \"<string>\", line 1, in <module>\r\n    File \"C:\\Users\\Valdemar\\AppData\\Local\\Temp\\pip-build-9rmoawj8\\markdown\\setup.py\", line 208, in <module>\r\n      setup(**data)\r\n    File \"distutils\\core.py\", line 148, in setup\r\n    File \"distutils\\dist.py\", line 955, in run_commands\r\n    File \"distutils\\dist.py\", line 974, in run_command\r\n    File \"C:\\Users\\Valdemar\\Downloads\\python-3.6.1-embed-amd64\\lib\\site-packages\\wheel\\bdist_wheel.py\", line 179, in run\r\n      self.run_command('build')\r\n    File \"distutils\\cmd.py\", line 313, in run_command\r\n    File \"distutils\\dist.py\", line 974, in run_command\r\n    File \"distutils\\command\\build.py\", line 135, in run\r\n    File \"distutils\\cmd.py\", line 313, in run_command\r\n    File \"distutils\\dist.py\", line 974, in run_command\r\n    File \"C:\\Users\\Valdemar\\AppData\\Local\\Temp\\pip-build-9rmoawj8\\markdown\\setup.py\", line 125, in run\r\n      import markdown\r\n    File \"build\\lib\\markdown\\__init__.py\", line 213\r\n      except AttributeError, e:\r\n                           ^\r\n  SyntaxError: invalid syntax\r\n\r\n  ----------------------------------------\r\n  Failed building wheel for markdown\r\n  Running setup.py clean for markdown\r\nFailed to build markdown\r\nInstalling collected packages: numpy, werkzeug, markdown, backports.weakref, tensorflow\r\n  Running setup.py install for markdown ... error\r\n    Complete output from command C:\\Users\\Valdemar\\Downloads\\python-3.6.1-embed-amd64\\python.exe -u -c \"import setuptools, tokenize;__file__='C:\\\\Users\\\\Valdemar\\\\AppData\\\\Local\\\\Temp\\\\pip-build-9rmoawj8\\\\markdown\\\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record C:\\Users\\Valdemar\\AppData\\Local\\Temp\\pip-6zhxvaeg-record\\install-record.txt --single-version-externally-managed --compile:\r\n    running install\r\n    running build\r\n    running build_py\r\n    creating build\r\n    creating build\\lib\r\n    creating build\\lib\\markdown\r\n    copying markdown\\blockparser.py -> build\\lib\\markdown\r\n    copying markdown\\blockprocessors.py -> build\\lib\\markdown\r\n    copying markdown\\etree_loader.py -> build\\lib\\markdown\r\n    copying markdown\\inlinepatterns.py -> build\\lib\\markdown\r\n    copying markdown\\odict.py -> build\\lib\\markdown\r\n    copying markdown\\postprocessors.py -> build\\lib\\markdown\r\n    copying markdown\\preprocessors.py -> build\\lib\\markdown\r\n    copying markdown\\serializers.py -> build\\lib\\markdown\r\n    copying markdown\\treeprocessors.py -> build\\lib\\markdown\r\n    copying markdown\\util.py -> build\\lib\\markdown\r\n    copying markdown\\__init__.py -> build\\lib\\markdown\r\n    copying markdown\\__main__.py -> build\\lib\\markdown\r\n    creating build\\lib\\markdown\\extensions\r\n    copying markdown\\extensions\\abbr.py -> build\\lib\\markdown\\extensions\r\n    copying markdown\\extensions\\attr_list.py -> build\\lib\\markdown\\extensions\r\n    copying markdown\\extensions\\codehilite.py -> build\\lib\\markdown\\extensions\r\n    copying markdown\\extensions\\def_list.py -> build\\lib\\markdown\\extensions\r\n    copying markdown\\extensions\\extra.py -> build\\lib\\markdown\\extensions\r\n    copying markdown\\extensions\\fenced_code.py -> build\\lib\\markdown\\extensions\r\n    copying markdown\\extensions\\footnotes.py -> build\\lib\\markdown\\extensions\r\n    copying markdown\\extensions\\headerid.py -> build\\lib\\markdown\\extensions\r\n    copying markdown\\extensions\\html_tidy.py -> build\\lib\\markdown\\extensions\r\n    copying markdown\\extensions\\meta.py -> build\\lib\\markdown\\extensions\r\n    copying markdown\\extensions\\nl2br.py -> build\\lib\\markdown\\extensions\r\n    copying markdown\\extensions\\rss.py -> build\\lib\\markdown\\extensions\r\n    copying markdown\\extensions\\sane_lists.py -> build\\lib\\markdown\\extensions\r\n    copying markdown\\extensions\\smart_strong.py -> build\\lib\\markdown\\extensions\r\n    copying markdown\\extensions\\tables.py -> build\\lib\\markdown\\extensions\r\n    copying markdown\\extensions\\toc.py -> build\\lib\\markdown\\extensions\r\n    copying markdown\\extensions\\wikilinks.py -> build\\lib\\markdown\\extensions\r\n    copying markdown\\extensions\\__init__.py -> build\\lib\\markdown\\extensions\r\n    running build_scripts\r\n    creating build\\scripts-3.6\r\n    copying and adjusting bin\\markdown_py -> build\\scripts-3.6\r\n    running build_docs\r\n    Traceback (most recent call last):\r\n      File \"<string>\", line 1, in <module>\r\n      File \"C:\\Users\\Valdemar\\AppData\\Local\\Temp\\pip-build-9rmoawj8\\markdown\\setup.py\", line 208, in <module>\r\n        setup(**data)\r\n      File \"distutils\\core.py\", line 148, in setup\r\n      File \"distutils\\dist.py\", line 955, in run_commands\r\n      File \"distutils\\dist.py\", line 974, in run_command\r\n      File \"C:\\Users\\Valdemar\\Downloads\\python-3.6.1-embed-amd64\\lib\\site-packages\\setuptools\\command\\install.py\", line 61, in run\r\n        return orig.install.run(self)\r\n      File \"distutils\\command\\install.py\", line 545, in run\r\n      File \"distutils\\cmd.py\", line 313, in run_command\r\n      File \"distutils\\dist.py\", line 974, in run_command\r\n      File \"distutils\\command\\build.py\", line 135, in run\r\n      File \"distutils\\cmd.py\", line 313, in run_command\r\n      File \"distutils\\dist.py\", line 974, in run_command\r\n      File \"C:\\Users\\Valdemar\\AppData\\Local\\Temp\\pip-build-9rmoawj8\\markdown\\setup.py\", line 125, in run\r\n        import markdown\r\n      File \"build\\lib\\markdown\\__init__.py\", line 213\r\n        except AttributeError, e:\r\n                             ^\r\n    SyntaxError: invalid syntax\r\n\r\n    ----------------------------------------\r\nCommand \"C:\\Users\\Valdemar\\Downloads\\python-3.6.1-embed-amd64\\python.exe -u -c \"import setuptools, tokenize;__file__='C:\\\\Users\\\\Valdemar\\\\AppData\\\\Local\\\\Temp\\\\pip-build-9rmoawj8\\\\markdown\\\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record C:\\Users\\Valdemar\\AppData\\Local\\Temp\\pip-6zhxvaeg-record\\install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in C:\\Users\\Valdemar\\AppData\\Local\\Temp\\pip-build-9rmoawj8\\markdown\\\r\n\r\nC:\\Users\\Valdemar\\Downloads\\python-3.6.1-embed-amd64>\r\n```", "comments": ["@gunan, PTAL.", "markdown is a dependency of tensorboard. Tensorboard has moved to a new repository, and at head should be handled differently.\r\n@dandelionmane @jart \r\nWhat do you recommend for 1.2 release?", "Indeed, as of 1.3 and onwards TF won't have a markdown dependency, just TensorBoard. However, that won't directly fix the issue, as it's failing during pip install - and TensorFlow will list TB as a dependency, meaning that pip install will still fail. We need to fix https://github.com/tensorflow/tensorboard/issues/43.\r\n\r\nFor the 1.2 release - if you want to make a patch release, it wouldn't be hard to make a new pip package with the strict version pinning removed. Just change the https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/tools/pip_package/setup.py#L40 to '>= 2.6.8'. Unit tests cover all the markdown functionality we need, so regressions are not too big a concern.", "@av8ramit I think we should make a patch release with this.\r\nCould you make the change in release branch, and release 1.2.1?", "@gunan sure sounds good, I'll get started.", "1.2.1 was released and fixes this issue: https://github.com/tensorflow/tensorflow/commit/8695d596385e754159af5a1f0ae0676163bc73bb#diff-739fc4f018f5288972ae5826b15c36e7"]}, {"number": 11062, "title": "C lib for windows with GPU support", "body": "I know that there is a windows c library built for cpu https://github.com/tensorflow/tensorflow/issues/10817.\r\n\r\nHowever, I could not find one with gpu support. I've tried https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-windows-x86_64-1.2.0.zip, but obvious it does not exist. \r\n\r\nIs it possible to have one?\r\n\r\nThank you!", "comments": ["Or if anybody can tell me how to do that myself..Just like the official cpu version...?", "Hi @tcmxx , it was built by bazel.  Please try to follow these 2 guides:\r\n\r\nhttps://docs.bazel.build/versions/master/install.html\r\nhttps://www.tensorflow.org/install/install_sources\r\n\r\nAnd \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/windows/libtensorflow_cpu.sh\r\n\r\n\r\n", "@snnn Thanks\uff01\r\nJust tried it and it worked! There is already the function to configure GPU build in the scripts and I only need to modify a little of it to make it work."]}, {"number": 11061, "title": "what's the different between tf.sub(a,b) and a-b, if i can Subsitute each other", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 11060, "title": "windows 10 /Anaconda/ python 3.6.1/ Tensroflow import error", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: \r\n- **TensorFlow version (use command below)**: \r\n- **Bazel version (if compiling from source)**: \r\n- **CUDA/cuDNN version**:  v8.0 / cudnn-8.0-windows10-x64-v6.0\r\n- **GPU model and memory**:  NVIDIA Geforce GTX 1060 6GB\r\n- **Exact command to reproduce**: \r\n\r\n(D:\\Anaconda3) C:\\Users\\user>python\r\n>>> import tensorflow as tf\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"D:\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 978, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 950, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 648, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 560, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 205, in _call_with_frames_removed\r\nImportError: DLL load failed: \uc9c0\uc815\ub41c \ubaa8\ub4c8\uc744 \ucc3e\uc744 \uc218 \uc5c6\uc2b5\ub2c8\ub2e4.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"D:\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"D:\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 978, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 950, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 648, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 560, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 205, in _call_with_frames_removed\r\nImportError: DLL load failed: \uc9c0\uc815\ub41c \ubaa8\ub4c8\uc744 \ucc3e\uc744 \uc218 \uc5c6\uc2b5\ub2c8\ub2e4.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"D:\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nHere is the errors came up.\r\nI just started python and kinda novice to programming.. \r\nCan you guys get me any help..?\r\n", "comments": ["Try running the script here to diagnose DLL problems with TensorFlow on Windows: https://gist.github.com/mrry/ee5dbcfdd045fa48a27d56664411d41c", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 11059, "title": "bugfix: summaries variable was undefined.", "body": "The definition of var `summaries` was missing in the first code example.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please.", "Hmm... Looks like an unrelated flake.\r\n\r\nJenkins, test this please.", "Not sure why the build was aborted. Trying again.\r\n\r\nJenkins, test this please."]}, {"number": 11058, "title": "terminate called after throwing an instance of 'std::out_of_range' error when call made to tf.contrib.tensor_forest.random_forest.TensorForestEstimator.predict()", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nInstalled with pip install tensorflow-gpu\r\n- **TensorFlow version (use command below)**:\r\n('v1.2.0-rc2-21-g12f033d', '1.2.0')\r\n- **Bazel version (if compiling from source)**:\r\nn/a\r\n- **CUDA/cuDNN version**:\r\n5.1.10 for CUDA 8.0\r\n- **GPU model and memory**:\r\nname: GeForce GTX 1050 Ti\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.493\r\npciBusID 0000:01:00.0\r\nTotal memory: 3.94GiB\r\nFree memory: 3.46GiB\r\n- **Exact command to reproduce**:\r\npython RandomForestTrainer.py ?? I'd be happy to upload my code\r\nYou can collect some of this information using our environment capture script:\r\nCollecting system information...\r\n2017-06-26 02:27:23.087673: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-26 02:27:23.087713: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-26 02:27:23.087720: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-26 02:27:23.087727: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-26 02:27:23.394076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: \r\nname: GeForce GTX 1050 Ti\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.493\r\npciBusID 0000:01:00.0\r\nTotal memory: 3.94GiB\r\nFree memory: 3.46GiB\r\n2017-06-26 02:27:23.394119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 \r\n2017-06-26 02:27:23.394126: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y \r\n2017-06-26 02:27:23.394144: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0)\r\nWrote environment to tf_env.txt. You can review the contents of that file.\r\nand use it to populate the fields in the github issue template.\r\n\r\ncat tf_env.txt\r\n\r\n== cat /etc/issue ===============================================\r\nLinux Desktop 4.4.0-79-generic #100-Ubuntu SMP Wed May 17 19:58:14 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.2 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux Desktop 4.4.0-79-generic #100-Ubuntu SMP Wed May 17 19:58:14 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.12.1)\r\nprotobuf (3.3.0)\r\ntensorflow-gpu (1.2.0)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.2.0\r\ntf.GIT_VERSION = v1.2.0-rc2-21-g12f033d\r\ntf.COMPILER_VERSION = v1.2.0-rc2-21-g12f033d\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /usr/local/cuda/lib64/\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nMon Jun 26 02:27:24 2017       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.66                 Driver Version: 375.66                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 105...  Off  | 0000:01:00.0      On |                  N/A |\r\n|  0%   45C    P0    36W / 120W |    448MiB /  4031MiB |      1%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    0      1164    G   /usr/lib/xorg/Xorg                             258MiB |\r\n|    0     15045    G   ...el-token=7CE624E5F1863243374CB7B5F4C7B81C    72MiB |\r\n|    0     20044    G   /usr/lib/xorg/Xorg                              41MiB |\r\n|    0     25088    G   compiz                                          38MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\r\n\r\n### Describe the problem\r\nI created and trained a classifier with the tf.contrib.tensor_forest.random_forest.TensorForestEstimator class, but when I try to use the predict() method I get the following error:\r\n\r\n2017-06-26 02:30:13.003812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0)\r\nterminate called after throwing an instance of 'std::out_of_range'\r\nterminate called recursively\r\nterminate called recursively\r\nAborted (core dumped)\r\n\r\n\r\n\r\n\r\n", "comments": ["I'd be happy to send the program that generated this if it helps as I don't see a way to post it here.  Sorry if I'm doing something wrong; I've never filed a bug like this before.", "Can we close this or can you send a TF 2.x standalone minimal example or Colab to reproduce this?", "@kessler9 ,\r\nWe see that you are using older version of tensorflow (1.x) which is not actively supported. We recommend that you upgrade to latest stable version of tensorflow 2.6.0 and let us know if the issue still persists in newer versions .Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/11058\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/11058\">No</a>\n"]}, {"number": 11057, "title": "Update documentation regarding 'shape' parameters", "body": "These functions are documented to take lists, but perfectly\r\n  accept tuples. Explicitly documented this case.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed the CLA!", "CLAs look good, thanks!\n\n<!-- ok -->", "Thanks @stephenjfox!"]}]