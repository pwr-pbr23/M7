[{"number": 17456, "title": "TensorForestEstimator with TensorFlow v1.4 \"No attribute model_fn\"", "body": "Setting up a TensorForestEstimator on CloudML running TensorFlow version 1.4. \r\nThe setup is identical to a working DNNClassifier estimator (of course, swapping the parameters within the Random Forest), however, an error occurs when the TensorForestEstimator has its model_fn called.\r\nThis seems to be a bug, since the model_fn attribute is defined during initialization [https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/contrib/tensor_forest/client/random_forest.py](url)\r\n\r\nHere's the relevant parts of the code:\r\n`  # Build an estimator.\r\n    \r\ngraph_builder_class = tensor_forest.RandomForestGraphs\r\n    \r\nparams = tf.contrib.tensor_forest.python.tensor_forest.ForestHParams(num_classes=10, num_features=100, num_trees=200, max_nodes=1000)\r\n    rfClf=random_forest.TensorForestEstimator(params=params,feature_columns=TF_INPUT_COLUMNS,config=config,graph_builder_class = graph_builder_class,keys_column='key2')\r\n    \r\nrfClf2 = tf.contrib.estimator.forward_features(rfClf, ['key1',key2', 'key3'])`\r\n\r\n\r\nBelow is the error traceback I get from the CloudML runner.\r\n\r\n> Traceback (most recent call last): File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main \"__main__\", fname, loader, pkg_name) File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code exec code in run_globals File \"/root/.local/lib/python2.7/site-packages/RF_Model/RF_task_tf14.py\", line 201, in <module> run_experiment(hparams) File \"/root/.local/lib/python2.7/site-packages/RF_Model/RF_task_tf14.py\", line 88, in run_experiment eval_spec) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/training.py\", line 464, in train_and_evaluate getattr(executor, task_to_run)() File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/training.py\", line 562, in run_master self._start_distributed_training(saving_listeners=saving_listeners) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/training.py\", line 687, in _start_distributed_training saving_listeners=saving_listeners) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 302, in train loss = self._train_model(input_fn, hooks, saving_listeners) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 711, in _train_model features, labels, model_fn_lib.ModeKeys.TRAIN, self.config) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 694, in _call_model_fn model_fn_results = self._model_fn(features=features, **kwargs) File \"/root/.local/lib/python2.7/site-packages/RF_Model/model1.py\", line 286, in _model_fn features=features, labels=labels, mode=mode, config=estimator.config) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 694, in _call_model_fn model_fn_results = self._model_fn(features=features, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/estimator/python/estimator/extenders.py\", line 222, in new_model_fn spec = estimator.model_fn(features, labels, mode, config) AttributeError: 'TensorForestEstimator' object has no attribute 'model_fn'\r\n\r\nI'm attempting to use tf.contrib.estimator.forward_features to past a list of keys through to the prediction output (this is reflected in the traceback when new_model_fn tries to call the TensorForestEstimator's \"model_fn attribute\") [https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/contrib/estimator/python/estimator/extenders.py](url)\r\n\r\nIt's also probably worth mentioning that the model_fn call within contrib.estimator.forward_features passes 4 parameters, whereas the model function defined within TensorForestEstimator only accepts 3 (see line 392 in the first link). \r\n\r\n-OS Platform: (N/A - CloudML (likely Debian workers?))\r\n-Tensorflow running v1.4\r\n-Relevant code mentioned above.\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 17455, "title": "Don't use NCHW or NHCW in tf.layers.conv1d", "body": "Fixes deprecation warning when using tf.layers.conv1d.", "comments": ["This PR was initially closed in #17338 because @protoget thought that:\r\n\r\n> Internally we still need NCHW or NWHC, the data_format is later passed to gen_nn_ops.conv2d.\r\n\r\nBut I've confirmed that this is actually not true: `tf.nn.conv1d` explicitly converts `NCW` and `NWC` back to `NCHW` and `NWHC`:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/6fdb9ad1baf7686a75f9e660178f7ac595e7fc2e/tensorflow/python/ops/nn_ops.py#L2428-L2435\r\n\r\nand has explicitly deprecated `NCHW` and NWHC` as arguments:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/b07680459a88224fce83daa7b3b70bcc62b9c896/tensorflow/python/ops/nn_ops.py#L2366-L2375.\r\n", "@reedwm Done!", "@reedwm @protoget Is there anything else I need to do here? I'm hoping that this can get merged in for the 1.7 release (if only for the aesthetic value of not having deprecation warnings in my code :stuck_out_tongue_winking_eye:).", "Sorry for the delay.\r\n\r\n@gunan, will this be included in 1.7?", "No, 1.7 is almost final.\r\nAnd since this is only warnings, we will probably not delay the final release for this cherrypick."]}, {"number": 17454, "title": "einsum: allow capital letter indices", "body": "Small tweak to allow capital letters in ``einsum``, as in ``numpy``. This is occasionally a problem when more than 26 indices are needed.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to `go/cla#troubleshoot`.\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "CLA signed", "CLAs look good, thanks!\n\n<!-- ok -->", "A similar fix has already be merged. Closing this pull request"]}, {"number": 17453, "title": "Fix broken link pointing to vulnerability reporting/SECURITY.md", "body": "The vulnerability reporting (SECURITY.md) has been moved to top level\r\ndirectory, this fix fixes the broken link inside tensorflow/docs_src/community/welcome.md\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 17452, "title": "Deprecation of the LXD PPAs", "body": "LXD PPAs are deprecated at the end of 2017 and need to use\r\nthe official backports in the Ubuntu archive.\r\n\r\nSigned-off-by: Yihong Wang <yh.wang@ibm.com>", "comments": ["@yhwang There is another PR (#17416) that has been pending which addresses this issue.", "@yongtang  thanks, close this now"]}, {"number": 17451, "title": "Page crash when up load a well fromat tsv(files contain 300K line of vector(300 dim))", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Nagging Assignee @tatatodd: It has been 201 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 17450, "title": "Updating the cuda compute info and avx info for Windows.", "body": "", "comments": ["Feel free to request nit changes."]}, {"number": 17449, "title": "Branch 187892975", "body": "", "comments": []}, {"number": 17448, "title": "Update CUDA capability to 3.5 when install from binary in documentation", "body": "This fix tries to address the issue raised in #17445. The minimal CUDA capability is 3.5 for Linux install from binary, though the documentation only specifies 3.0.\r\n\r\nThis fix updates the docs to 3.5 for binary install.\r\n\r\nThis fix fixes #17445.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n", "comments": ["Already working on a fix: https://github.com/tensorflow/tensorflow/pull/17450"]}, {"number": 17447, "title": "Tensorflow Usage with the C++ API, not working", "body": "I am trying to go through the tutorial that provided on the TensorFlow website [(LINK).](https://www.tensorflow.org/tutorials/image_recognition)\r\n\r\nIm in ```Usage with the C++ API``` part. It says: \r\nYou can download the archive containing the GraphDef that defines the model like this (running from the root directory of the TensorFlow repository):\r\n\r\nand then it provide the following command:\r\n```\r\ncurl -L \"https://storage.googleapis.com/download.tensorflow.org/models/inception_v3_2016_08_28_frozen.pb.tar.gz\" |\r\n  tar -C tensorflow/examples/label_image/data -xz\r\n```\r\n\r\nI tried to run this command in my linux terminal but it is givving me error.\r\nCan you tell me how and where I should run this command?\r\n\r\nThanks\r\n\r\n\r\nUpdates:\r\n\r\nHave I written custom code: NO I used the code provided on the tensorflow website\r\nOS Platform and Distribution: Linux, ubuntu 16.04\r\nTensorFlow installed from: pip\r\nTensorFlow version: 1.6\r\nBazel version: was not able to installed \r\nCUDA/cuDNN version: cuda 9.0 cudnn7.0\r\nGPU model and memory:  Quadro M2200 \r\nExact command to reproduce", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "I did provide the information in the first comment right after you asked.\r\nI gave up on trying it...", "Hello,\r\n\r\nI had similar issues but after some trying, I ran it in my virtual environment in the tensorflow directory and it worked.\r\n\r\nHope this helps,\r\nAaron"]}, {"number": 17446, "title": "tf.image.resize_bilinear outputs weird when scale from [256, 256] to [96, 96]", "body": "I have a three band .tif image of size [256, 256, 3], I tried to scale to [128, 128], the output look fine. However, when I tried to scale to [96, 96] the output is only rgb noises.\r\nAnyone has the same experience?\r\nimage = tf.image.resize_images(image, [96, 96])\r\n![image](https://user-images.githubusercontent.com/22228307/36994810-d6ba3cd8-2077-11e8-8e07-e2cc7595df79.png)\r\n\r\n![image](https://user-images.githubusercontent.com/22228307/36994793-c4bd5a10-2077-11e8-9b36-bfb7510270fb.png)\r\n", "comments": ["just solved this problem from #10169", "Nagging Assignee @shivaniag: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 17445, "title": "Minimum Cuda capability is 3.5? But, 3.0 stated on site", "body": "I'm able to run the hello world examples, but the following warning (or error) is printed. So, I can't make use of my gpu? While this maybe a simple correction on the web page, is there anyway I can get a version that allows me to run with a Cuda 3.0 card?\r\n\r\nOS: Ubuntu 16.04\r\nGPU: K2000M\r\n\r\nOn the linux installation page, the minimum capability is written as 3.0. But, when I try to run hello world on a cuda 3.0 card, the following is printed:\r\n\r\nname: Quadro K2000M major: 3 minor: 0 memoryClockRate(GHz): 0.745\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 1.95GiB freeMemory: 977.81MiB\r\n2018-03-05 13:43:54.533246: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1283] Ignoring visible gpu device (device: 0, name: Quadro K2000M, pci bus id: 0000:01:00.0, compute capability: 3.0) with Cuda compute capability 3.0. The minimum required Cuda capability is 3.5.", "comments": ["I think minimal capability is 3.5 for binary install. It might still be possible to support 3.0 when building from source.\r\n\r\nCreate a PR #17448 for the doc fix.\r\n", "Ok, is this a regression? As per this thread this should be available?https://github.com/tensorflow/tensorflow/issues/25", "I think in the thread its mentioned that you might have to change some lines in common_runtime/gpu/gpu_device.cc for this to work. By default minimum is 3.5", "It can be compiled from source with CUDA capability 3.0. It is working for me this way.", "@mackiem did you have to make the changes suggested by @rohan100jain before building from source?", "No. I believe basel configured it for me without having to touch the source.\n\nOn Thu, May 10, 2018, 7:13 PM JoshuaC3 <notifications@github.com> wrote:\n\n> @mackiem <https://github.com/mackiem> did you have to make the changes\n> suggested by @rohan100jain <https://github.com/rohan100jain> before\n> building from source?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/17445#issuecomment-388212553>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABsR4TTovkhJvWb2wHx-bYL8xrKj5KVIks5txMmngaJpZM4Sc1Yr>\n> .\n>\n", "Hello i have a **CUDA Compute Capability of 2.1**  is there a chance i am able to change the source code to support my GPU lower capability? Or the tensorflow's algo need a specific capability ?\r\n\r\nCause CPU take way too long:  been training the model for more than 48h so far on **CPU**  \r\nLikely i will be dying from old age before i can evaluate the inception model for my needs\r\n\r\n- 19%-30% validation accuracy \r\n- 1.5 m training step\r\n- model : inceptions-v3 transfert learning & retraining of classification layer\r\n- dataset 6GB\r\n- 130 classes, ... \r\n- GPU:Intel(R) Xeon(R) CPU E3-1220 V2 @ 3.10GHz\r\n", "Quite sure, 2.1 is too old for a lot of functionality and acceleration. Sorry."]}, {"number": 17444, "title": "getting error while running my model: -    \"tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 17443, "title": "Update the documentation of `softmax_cross_entropy`", "body": "This fix updates the documentation of `softmax_cross_entropy`,\r\nand removed the shape restrictions of `onehot_labels` and `logits`.\r\nThey only needs to be of the same shape, not necessary `[batch_size, num_classes]`.\r\n\r\nThis fix fixes #16263.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 17442, "title": "gradient back propagation through image transforms", "body": "As we know from the description of tf.contrib.image.transform(images, transforms): \"Note that gradients are not backpropagated into transformation parameters\".\r\n\r\nHere, I would like to know whether gradients can be correctly backpropagated into images?\r\n\r\nIf yes, whether it is automatic?\r\n\r\nDo I need to explicitly define the gradient using gradient_map_override?\r\n\r\nAny suggestions would be helpful. Thanks\r\n\r\nHave I written custom code: N/A\r\nOS Platform and Distribution: Ubuntu 15.04\r\nTensorFlow installed from: pip \r\nTensorFlow version: 1.6.0\r\nBazel version: N/A\r\nCUDA/cuDNN version: 9.0\r\nGPU model and memory: 24G Quadro P6000\r\nExact command to reproduce: N/A\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "I have the same problem, have you solved? Or the gradient couldn't back propagation  into transformation parameters?\r\nAny suggestions would be helpful. Thanks"]}, {"number": 17441, "title": "Illegal instruction (core dumped)", "body": "karthick@karthick-Aspire-4739Z:~$ python3\r\nPython 3.5.2 (default, Nov 23 2017, 16:37:01) \r\n[GCC 5.4.0 20160609] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n**Illegal instruction (core dumped)**\r\nkarthick@karthick-Aspire-4739Z:~$ python\r\nPython 2.7.12 (default, Dec  4 2017, 14:50:18) \r\n[GCC 5.4.0 20160609] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  **File \"<stdin>\", line 1, in <module>\r\nImportError: No module named tensorflow**\r\n>>> exit()\r\n\r\n\r\nhow can i solve this problem but i install tensorflow using native pip on both pip3 and pip on ubuntu 16.04", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "https://github.com/tensorflow/tensorflow/issues/17411 \r\n\r\nshows installing tensoflow 1.5 (vs 1.6) fixed this issue.\r\nresolved issue for me also"]}, {"number": 17440, "title": "Can't \"adb install\" TfLiteCameraDemo.", "body": "I built it using bazel and got a 5.1MB TfLiteCameraDemo.apk, but it can't be installed. The console just got stuck as executing the \"adb install ...\"\r\nYour pre-build one is 23.1MB and can be installed without any problem.\r\n\r\nError messages:\r\n\r\nTerminal -\r\nadb: failed to install bazel-bin/tensorflow/contrib/lite/java/demo/app/src/main/TfLiteCameraDemo.apk: Failure [INSTALL_FAILED_NO_MATCHING_ABIS: Failed to extract native libraries, res=-113]\r\n\r\nLogcat -\r\n**03-05 14:23:22.186 1643-1684/system_process E/PackageInstaller: Commit of session 1384727845 failed: Failed to extract native libraries, res=-113**\r\n\r\nMy environment\r\n\r\nAndroid Studio 3.2 Canary 4(android virtual device : Pixel 2 XL API 27)\r\nbazel 0.11.0\r\ntensorflow master(no custom modification)\r\nstand-alone ndk-r14b, sdk within android-studio 3..2\r\nMacOS High Sierra 10.13.3\r\n\r\nMy step\r\n\r\nbazel build --cxxopt=--std=c++11 //tensorflow/contrib/lite/java/demo/app/src/main:TfLiteCameraDemo --config=android_arm64 --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a\r\n\r\nadb install -r bazel-bin/tensorflow/contrib/lite/java/demo/app/src/main/TfLiteCameraDemo.apk", "comments": ["Does it give sone kind of error message?\n\nOn Mon, 5 Mar 2018, 15:37 VincentLin78, <notifications@github.com> wrote:\n\n> I built it using bazel and got a 5.1MB TfLiteCameraDemo.apk, but it can't\n> be installed. The console just got stuck as executing the \"adb install ...\"\n> Your pre-build one is 23.1MB and can be installed without any problem.\n>\n> My environment\n>\n> Android Studio 3.2 Canary 4(android virtual device : Pixel 2 XL API 27)\n> bazel 0.11.0\n> tensorflow master(no custom modification)\n> stand-alone ndk-r14b, sdk within android-studio 3..2\n> MacOS High Sierra 10.13.3\n>\n> My step\n>\n> bazel build --cxxopt=--std=c++11\n> //tensorflow/contrib/lite/java/demo/app/src/main:TfLiteCameraDemo\n> --config=android_arm64 --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a\n>\n> adb install -r\n> bazel-bin/tensorflow/contrib/lite/java/demo/app/src/main/TfLiteCameraDemo.apk\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/17440>, or mute the\n> thread\n> <https://github.com/notifications/unsubscribe-auth/AFjlg_E72_UxICMYDkdWadbEp-16KHsAks5tbU26gaJpZM4ScY3P>\n> .\n>\n", "Nagging Assignee @bignamehyp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I'm closing this one. Please reopen if you still see errors."]}, {"number": 17439, "title": "tf.variable initial value hangs in a cycle for ever", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you."]}, {"number": 17438, "title": "Add optimizers with decoupled weight decay.", "body": "This pull request implements decoupled weight decay as described in 'Fixing Weight Decay Regularization' by  Loshchilov & Hutter [https://arxiv.org/abs/1711.05101](https://arxiv.org/abs/1711.05101).\r\n\r\nThis paper shows that for adaptive gradient algorithms, the implemented method regularizes variables with large gradients more than L2 regularization would and that this  yields better training loss and generalization error.\r\n\r\nFor SGD variants, this simplifies hyperparameter search since it decouples the settings of weight decay and learning rate, which is nicely visualized in Fig. 2 in the paper: \r\n![adamw](https://user-images.githubusercontent.com/16101605/36976180-f2fcbf92-207c-11e8-879e-4677381d892d.png)\r\n\r\nThis implementation explicitly adds the optimizers described in the paper (`AdamW` and `MomentumW`) to tf.contrib.opt and provides a factory function `extend_with_decoupled_weight_decay` that can be used to create a new optimizer class with decoupled weight decay.\r\n\r\nCloses #15237.", "comments": ["I'll be travelling for the next 4 weeks, so I probably won't be able to answer comments immediately ;)", "I just tried using AdamW as implemented above, please correct me if I am wrong, but I think it breaks tf.layers.batch_norm during inference (when training=False)", "Thanks for trying this out! :)\r\nI tested this implementation adapting the [official resnet](https://github.com/tensorflow/models/blob/master/official/resnet/cifar10_main.py) implementation and it worked for me, with and without decaying the batch norm variables.\r\nI suspect that your training diverged as the optimizer is not active in the forward pass (assuming you don't do inference on the backward pass).\r\n- Did you adapt the weight decay rate? Most likely it should be lower compared to l2 loss regularization. \r\n- Do you decay all variables, i.e. including the batch norm variables? Often it's useful to exclude the batch norm vars from weight decay.\r\n- Do you schedule the weight decay similar to the learning rate? I.e., when lowering the learning rate, do you also lower the weight decay?", "Thanks for your reply.\n\nI am using the cosine annealing with warm restarts learning rate scheduler.\nI use the normalized weight decay as specified in the paper. Furthermore,\nwhen I replace training=False to training=True at inference time the whole\nthing just works. So I suspect that the batch norm global statistics don't\nget updated somehow, but I will take a closer look to see what exactly is\noff. :)\n\nOn 28 March 2018 at 03:59, Phil <notifications@github.com> wrote:\n\n> Thanks for trying this out! :)\n> I tested this implementation adapting the official resnet\n> <https://github.com/tensorflow/models/blob/master/official/resnet/cifar10_main.py>\n> implementation and it worked for me, with and without decaying the batch\n> norm variables.\n> I suspect that your training diverged as the optimizer is not active in\n> the forward pass (assuming you don't do inference on the backward pass).\n>\n>    - Did you adapt the weight decay rate? Most likely it should be lower\n>    compared to l2 loss regularization.\n>    - Do you decay all variables, i.e. including the batch norm variables?\n>    Often it's useful to exclude the batch norm vars from weight decay.\n>    - Do you schedule the weight decay similar to the learning rate? I.e.,\n>    when lowering the learning rate, do you also lower the weight decay?\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/17438#issuecomment-376744340>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AKSuNsPDPuC1KT1EW4Tz4ohSa83vSsU7ks5tivytgaJpZM4ScPo0>\n> .\n>\n", "@andrewharp Sorry for manually pulling you in, I thought it might make sense as the tensorflow butler assigned you to triage the last few pull request and this PR fell off the radar. Could you triage this PR? Thanks a lot!", "Following the discussion in #15237 I re-checked the apply functions for other optimizers.\r\nThis implementation uses the fact that the `apply_` functions already get the precomputed gradient and Adam + Momentum optimizers don't compute values based on var, so the pre-decay implementation is equivalent to the algorithm described in the paper for these optimizers.\r\nHowever, I just checked the apply functions of other optimizers and e.g. Ftrl computes factors based on var, so decoupling decay for such optimizers would need a custom op.\r\nMy question now is: Do you favor limiting the decoupled implementation to Adam + Momentum optimizers, i.e., remove the extend_with-decoupled_weight_decay function or do you think adding a warning in the documentation is enough, still allowing to create e.g., AdadeltaW, AdagradW, RMSPropW with one line of code?", "@PhilJd I think the hacking method is feasible (and looks concise and flexible), however I have no idea of whether it is the best solution. I might prefer to implement the window decay for each optimizer awkwardly. Anyway, thanks for your nice work :) . Let's wait for the reply from tensorflower @alextp ", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm working on sparse updates but I'm rather busy due to upcoming ICRA.", "@alextp \r\nI finally found the time to update this pull request, sorry it took so long! The last commit adds sparse updates as well as a note on compatible optimizers.", "I'm not sure what causes this error:\r\n`RuntimeError: The following files are missing load(\"//tensorflow:tensorflow.bzl\", \"py_test\").`\r\n`py_test` is included at the top of the BUILD file.\r\n\r\nThe `do_buildifier` error is now fixed (wrong order).", "Thanks a lot for the great work @PhilJd !\r\nI've been trying it a bit and have been getting very chaotic results so far.\r\nI'm a bit lost with the weight decay parameter. Compared to the original paper, is your weight_decay parameter corresponding to their normalized or regular weight decay?\r\nAny insights on what kind of values we should use?", "@pierremac: The weight decay parameter (let's call it `w` here) is the parameter that is multiplied with the variable before subtracting it, i.e., an update step looks roughly like this:\r\n`var = var - grad - (w * var)`\r\nIt's not possible to compute the normalized weight decay within the optimizer as this depends on your specific dataset. \r\nTo now create AdamWR as in the paper, you need to compute the decay manually by multiplying your initial decay (ideally the normalized version) with your learning rate schedule, e.g.,\r\n```\r\nLR = ... # set your learning rate here\r\nW_NORM = ...  # set your weight decay value here\r\nglobal_step = tf.train.get_or_create_global_step()\r\nschedule = tf.train.cosine_decay_restarts(1, global_step,\r\n                                          first_decay_steps=?, t_mul=2.0,\r\n                                          m_mul=1.0, alpha=0.0)\r\nlr = LR * schedule\r\nweight_decay = W_NORM * sqrt(batch_size / num_training_samples * num_epochs) * schedule\r\n\r\nloss = ...\r\noptimizer = tf.contrib.opt.AdamwOptimizer(weight_decay, lr)\r\ntrain_op = optimizer.minimize(loss)\r\n\r\n```\r\n\r\nRegarding values for LR and W_NORM, this really depends on your model. W_NORM usually should be smaller than LR as otherwise regularization is stronger than the update step. The nice thing about decoupled weight decay is that you can tune learning rate and weight decay independent. So I'd suggest to first set decay to 0, do hyperparameter optimization for the learning rate and once you have found the\r\nbest performing learning rate you keep that fixed and start to optimize the decay parameter.\r\n\r\nHope that helps a bit ;)", "Will this change be added to v1.9.0 release? I do want this feature!", "I believe it missed 1.9, it will be in 1.10. ", "@PhilJd . I tried the code,and got the error:  \r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/opt/python/training/weight_decay_optimizers.py\", line 179, in _apply_sparse\r\n    decay_op = self._decay_weights_sparse_op(var, grad.indices, scatter_add)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/opt/python/training/weight_decay_optimizers.py\", line 163, in _decay_weights_sparse_op\r\n    self._use_locking)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/state_ops.py\", line 405, in scatter_add\r\n    use_locking=use_locking, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_state_ops.py\", line 591, in scatter_add\r\n    use_locking=use_locking, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3195, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): Must have updates.shape = indices.shape + params.shape[1:] or updates.shape = [], got updates.shape [4000000,64], indices.shape [20612], params.shape [4000000,64]", "@yym-ustc Do you have any small example to reproduce your error? And which tf-version do you use?", "@PhilJd  I use the lastest tensorflow code.", "@PhilJd  I used an embedding vector(4000000 * 64). When tf updated gradient of the embedding vector,  the above error happened.", "@yym-ustc  Could you share a small code snippet demonstrating your error?", "@PhilJd It's ok to use the AdamOptimizer, but when change to use AdaWOptimizer, I got the error.\r\nweights_var = tf.trainable_variables()\r\ngradients = tf.gradients(cost, weights_var)\r\n#optimizer = tf.train.AdamOptimizer(learning_rate=deep_learning_rate)\r\nAdamWOptimizer = tf.contrib.opt.extend_with_decoupled_weight_decay(tf.train.AdamOptimizer)\r\noptimizer = AdamWOptimizer(weight_decay=weight_decay, learning_rate=deep_learning_rate)\r\ntrain_op = optimizer.apply_gradients(zip(gradients, weights_var))", "Do you have a small, self-contained example that I can run to reproduce the bug?\r\nE.g., without knowing what your weights_var looks like (I suppose it's sparse, but resource/not resource etc) I can't really debug the problem.\r\nThanks!"]}, {"number": 17437, "title": "Bazel build missing dependencies error with MPI", "body": "OS: SLES12\r\nPython version: 3.6\r\nBazel version: Build label: 0.11.0- (@non-git)\r\ngcc version 7.2.0 (GCC)\r\nNo GPU\r\nNo CUDA\r\n\r\nWith MPI enabled in configure everything else disabled.\r\n\r\nWhen I run the command:  bazel build --config=mkl --copt=\"-DEIGEN_USE_VML\" --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.1 --copt=-msse4.2 -s -c opt //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\n\r\nI get the error below, I guess I could modify the build file to include the additional dependencies:\r\n\r\nERROR: /home/hpc/pr28fa/di72giz/TENSORFLOW/tensorflow/tensorflow/contrib/mpi_collectives/BUILD:40:1: undeclared inclusion(s) in rule '//tensorflow/contrib/mpi_collectives:python/ops/_mpi_ops.so':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc':\r\n  '/home/hpc/pr28fa/di72giz/TENSORFLOW/tensorflow/tensorflow/stream_executor/lib/statusor.h'\r\n  '/home/hpc/pr28fa/di72giz/TENSORFLOW/tensorflow/tensorflow/stream_executor/platform/port.h'\r\n  '/home/hpc/pr28fa/di72giz/TENSORFLOW/tensorflow/tensorflow/stream_executor/lib/error.h'\r\n  '/home/hpc/pr28fa/di72giz/TENSORFLOW/tensorflow/tensorflow/stream_executor/lib/status.h'\r\n  '/home/hpc/pr28fa/di72giz/TENSORFLOW/tensorflow/tensorflow/stream_executor/lib/stringpiece.h'\r\n  '/home/hpc/pr28fa/di72giz/TENSORFLOW/tensorflow/tensorflow/stream_executor/platform/logging.h'\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:128:6: warning: 'bool tensorflow::contrib::mpi_collectives::{anonymous}::IsGPUDevice() [with T = Eigen::GpuDevice]' defined but not used [-Wunused-function]\r\n bool IsGPUDevice<GPUDevice>() {\r\n      ^~~~~~~~~~~~~~~~~~~~~~\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "OS Platform and Distribution: OS: SLES12\r\nTensorFlow: installed from Github 05-Mar-2018\r\nTensorFlow version: 1.6\r\nCUDA/cuDNN version: N/A CPU only\r\nGPU model and memory: N/A CPU only\r\nExact command to reproduce: bazel build --config=mkl --copt=\"-DEIGEN_USE_VML\" --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.1 --copt=-msse4.2 -s -c opt //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\n\r\nconfigure:\r\nAnaconda Python 3.6\r\nMPI enable\r\nEvery other option disabled", "Can you please try the patch below?\r\n\r\n--- a/tensorflow/contrib/mpi_collectives/BUILD\r\n+++ b/tensorflow/contrib/mpi_collectives/BUILD\r\n@@ -53,6 +53,7 @@ tf_custom_op_library(\r\n         \":mpi_defines\",\r\n         \":mpi_message_proto_cc\",\r\n         \"//third_party/mpi\",\r\n(+)        \"//tensorflow/stream_executor\",\r\n     ],\r\n )\r\n \r\ndiff --git a/tensorflow/tensorflow.bzl b/tensorflow/tensorflow.bzl\r\nindex 23d11c88ed..12512ae6df 100644\r\n--- a/tensorflow/tensorflow.bzl\r\n+++ b/tensorflow/tensorflow.bzl\r\n@@ -1247,7 +1247,7 @@ def tf_custom_op_library(name, srcs=[], gpu_srcs=[], deps=[], linkopts=[]):\r\n       deps=deps + if_cuda(cuda_deps),\r\n       disallowed_deps=[\r\n           clean_dep(\"//tensorflow/core:framework\"),\r\n(-)          clean_dep(\"//tensorflow/core:lib\")\r\n(+)#          clean_dep(\"//tensorflow/core:lib\")\r\n       ])\r\n   tf_cc_shared_object(\r\n       name=name,\r\n\r\n\r\nCredit goes to https://github.com/aburden5 in the comments of a similar issue: https://github.com/baidu-research/tensorflow-allreduce/issues/5 \r\n", "Thanks the patch fixed the build errors.", "I would just like to thank @wei-v-wang for the patch. I have the same type of error but the patch successfully fixed it. The error log is attached for documentation purpose.\r\n\r\n```\r\nINFO: From ProtoCompile tensorflow/contrib/boosted_trees/proto/learner.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/contrib/decision_trees/proto/generic_tree_model_extensions.pb.h:\r\nbazel-out/k8-py2-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nbazel-out/k8-py2-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\n\r\n                                                                             ^\r\nINFO: From ProtoCompile tensorflow/contrib/mpi_collectives/mpi_message.pb.h:\r\nbazel-out/k8-py2-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nbazel-out/k8-py2-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nbazel-out/k8-py2-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nbazel-out/k8-py2-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nERROR: /sasdata/ra/user/$USER/tmp/brew-pkgs/tensorflow/tensorflow/contrib/mpi_collectives/BUILD:40:1: undeclared inclusion(s) in rule '//tensorflow/contrib/mpi_coll                                                                                                                                                     ectives:python/ops/_mpi_ops.so':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc':\r\n  'tensorflow/stream_executor/lib/statusor.h'\r\n  'tensorflow/stream_executor/lib/status.h'\r\n  'tensorflow/stream_executor/lib/error.h'\r\n  'tensorflow/stream_executor/platform/logging.h'\r\n  'tensorflow/stream_executor/platform/port.h'\r\n  'tensorflow/stream_executor/lib/statusor_internals.h'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 16471.364s, Critical Path: 304.28s\r\nINFO: 12036 processes: 12036 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n"]}, {"number": 17436, "title": "TF 1.6 build from source fails on ppc64le", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.4\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.6.0\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.11.0\r\n- **GCC/Compiler version (if compiling from source)**:  gcc (Ubuntu/IBM 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\n- **CUDA/cuDNN version**: CUDA 9.0 / CuDNN 7.0\r\n- **GPU model and memory**: Tesla P100-SXM2-16GB\r\n- **Exact command to reproduce**: bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\n\r\n### Describe the problem\r\nWhen building TF 1.6 from source on ppc64 we see the following error (bug):\r\n\r\n```gcc: error: unrecognized command line option '-march=native'```\r\n\r\nTensorFlow 1.6 build on ppc64le does not work, because on ppc64le we would need ```-mcpu=native```.\r\n\r\n### Source code / logs\r\n\r\nSeems like there is some handling for ppc64le in configure.py , however this commit https://github.com/tensorflow/tensorflow/commit/c9885ea7a73ade2d3f8e4712c3a14d9da72462b8 seems to set ```-march=native``` instead of ```-mcpu=native``` for ppc64le ...\r\n\r\n\r\nWould be great if someone could have a look at it - Thank you very much!", "comments": ["/CC @aselle, can you take a look?", "You can use `-mcpu=native -mtune=native` instead of `-march=native`.", "The build of r1.7 is working on pcc64le.\r\nsee:\r\nhttps://github.com/tensorflow/tensorflow/compare/r1.6...r1.7#diff-ade1d3e4b7c35655f854151d899df62bR497\r\n\r\nnot sure if you want to close this issue or have a look at @asispatra suggestion (```-mtune=native```)?", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 45 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 60 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 60 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 76 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 91 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 106 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Issue is obsolete."]}, {"number": 17435, "title": " input_layer = tf.reshape(features[\"x\"], [-1, 28, 28, 1]) NameError: name 'features' is not defined", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you."]}, {"number": 17434, "title": "Fix typo", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to `go/cla#troubleshoot`.\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", " @googlebot I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 17433, "title": "Running model failed:Not found:FeedInputs:unable to find feed output ", "body": "My platform is :win10 with GPU,visual studio 2015.\r\nApplication:tracking object\r\nMy code as following:\r\n\tauto place1 = Placeholder(root.WithOpName(\"Placeholder_1\"), tensorflow::DataType::DT_UINT8);\r\n\tauto place2 = Placeholder(root.WithOpName(\"Placeholder_2\"), tensorflow::DataType::DT_FLOAT);\r\n\tauto place3 = Placeholder(root.WithOpName(\"Placeholder_3\"), tensorflow::DataType::DT_INT32);\r\n\r\n        unsigned unsigned char data1[6];\r\n\tauto mapped_place1_ = Eigen::TensorMap<Eigen::Tensor<unsigned char, 4, Eigen::RowMajor>>\r\n\t\t(MergeMatFloat.data, 2 , outputSize, outputSize, 3);\r\n\tauto eigen_place1_ = Eigen::Tensor<unsigned char, 4, Eigen::RowMajor>(mapped_place1_);\r\n\tTensor place1_(tensorflow::DT_UINT8, tensorflow::TensorShape({ 2,outputSize,outputSize,3 }));\r\n\tplace1_.tensor<unsigned char, 4  > () = eigen_place1_;\r\n\r\n\tfloat data2[2048] = {0};\r\n\tauto mapped_place2_ = Eigen::TensorMap<Eigen::Tensor<float, 2, Eigen::RowMajor>>\r\n\t\t(&data2[0], 4,2);\r\n\tauto eigen_place2_ = Eigen::Tensor<float, 2, Eigen::RowMajor>(mapped_place2_);\r\n\tTensor place2_(tensorflow::DT_FLOAT, tensorflow::TensorShape({ 4, 2 }));\r\n\tplace2_.tensor<float, 2>() = eigen_place2_;\r\n\r\n\tstd::vector<int> data3 = {1};\r\n\tauto mapped_place3_ = Eigen::TensorMap<Eigen::Tensor<int, 1, Eigen::RowMajor>>\r\n\t\t(&data3[0], 1);\r\n\tauto eigen_place3_ = Eigen::Tensor<int, 1, Eigen::RowMajor>(mapped_place3_);\r\n\tTensor place3_(tensorflow::DT_INT32, tensorflow::TensorShape({ 1}));\r\n\tplace3_.tensor<int, 1>() = eigen_place3_;\r\n\r\n        .....\r\n        std::vector<Tensor> outputs;\r\n\tStatus run_status = session->Run({ { \"Placeholder_1\",place1_ } },\r\n\t{ output_layer }, {}, &outputs);\r\n\r\nwhich give message:\r\nRunning model failed:Not found:FeedInputs:unable to find feed output Placeholder_1\r\nwhat should i do?\r\n\r\nmy frozen.pb is like this\uff08looks strange,i don't know what node shold i select as the input\uff09:\r\n  Placeholder\r\nPlaceholder_1\r\nPlaceholder_2\r\nPlaceholder_3\r\nfifo_queue\r\nfifo_queue_EnqueueMany\r\nfifo_queue_DequeueMany/n\r\nfifo_queue_DequeueMany\r\nReshape/shape\r\nReshape\r\nReshape_1/shape\r\nReshape_1\r\nsub/y\r\nsub\r\nre3/conv1/W_conv/Initializer/random_uniform/shape\r\n\u3002\u3002\u3002\r\ngradients/re3\r\n\u3002\u3002\u3002\r\nAdam/beta1\r\n\u3002\u3002\u3002\r\nlosses/total_loss/tags\r\n\u3002\u3002\u3002\r\nre3_1/conv1/summaries/W_conv/Rank\r\n\u3002\u3002\u3002\r\nsave/Assign_47\r\n\u3002\u3002\u3002\r\ntest/robustness/tags\r\n\u3002\u3002\u3002\r\n\r\nwho know this\uff0cappreciation\uff01\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I have resolved it!Thanks for ur response,the reason is from the module(which is not right)", "Could you please elaborate what was your exact problem? Thanks!", "I have solved it.Thanks! @tuzand \r\nI just didn't know which node is the right output node for rnn,Now i have found it.\r\nIt's \uff1a \r\n \"re3/lstm1/rnn/while/Exit_3\";\r\n\"re3/lstm1/rnn/while/Exit_4\";\r\n\"re3/lstm2/rnn/while/Exit_3\";\r\n\"re3/lstm2/rnn/while/Exit_4\";\r\n"]}, {"number": 17432, "title": "\"tensorflow/core/platform/posix/net.cc:75:17: error: invalid 'asm': invalid operand for code 'w'    actual_port = ntohs(addr.sin_port);\" Error While Cross Compiling for Raspberry Pi 3 with the given script \"build_raspberry_pi.sh\"", "body": "### System information\r\n- ****Have I written custom code (as opposed to using a stock example script provided in TensorFlow)****: No, Using the provided Script(build_raspberry_pi.sh)\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16.04LTS\r\n\r\n- **TensorFlow installed from (source or binary)**: Source\r\n\r\n- **TensorFlow version (use command below)**: 1.6.0\r\n\r\n- **Python version**:  2.7\r\n\r\n- **Bazel version (if compiling from source)**: 0.8.0\r\n\r\n- **GCC/Compiler version (if compiling from source)**: arm-linux-gnueabihf-gcc 4.9\r\n\r\n- **CUDA/cuDNN version**:No\r\n\r\n- **GPU model and memory**: NO\r\n\r\n- **Exact command to reproduce**: ./tensorflow/tools/ci_build/pi/build_raspberry_pi.sh\r\n\r\n### Describe the problem\r\nI'm trying to cross compile tensorflow for Raspberry Pi 3 and I'm following the below steps,\r\n\r\n**1:- Cloning Tensorflow 1.6.0 \r\n\r\n2:- Installing Bazel 0.8.0\r\n\r\n3:- Running the script ./tensorflow/tools/ci_build/pi/build_raspberry_pi.sh**\r\n\r\n**Getting Error:-** \r\n\"tensorflow/core/platform/posix/net.cc:60:19: error: invalid 'asm': invalid operand for code 'w'\r\n   addr.sin_port = htons(static_cast<uint16_t>(*port));\r\n                   ^\r\ntensorflow/core/platform/posix/net.cc:75:17: error: invalid 'asm': invalid operand for code 'w'\r\n   actual_port = ntohs(addr.sin_port);\"\r\n**Note:-**\r\n**\"error: invalid 'asm': invalid operand for code 'w'\", Chances are, if you got this error then you were cross-compiling, probably from an x86 or x86_64 host to an ARM target. The error most likely occurred when you used, directly or indirectly, a network byte order translation function from the C library like htons(), or ntohl().**\r\n\r\n### Source code / logs\r\nGNU C++ (crosstool-NG crosstool-ng-1.22.0-88-g8460611) version 4.9.3 (arm-linux-gnueabihf)\r\n\tcompiled by GNU C version 4.8.4, GMP version 6.0.0, MPFR version 3.1.3, MPC version 1.0.3\r\nGGC heuristics: --param ggc-min-expand=100 --param ggc-min-heapsize=131072\r\nCompiler executable checksum: 5490cb547d5e63dcf8255bc3ba4b9a59\r\nIn file included from /usr/include/bits/byteswap.h:35:0,\r\n                 from /usr/include/endian.h:60,\r\n                 from /usr/include/bits/waitstatus.h:64,\r\n                 from /usr/include/stdlib.h:42,\r\n                 from /home/baladev/.cache/bazel/_bazel_baladev/da0e175f87e998fe3d550279550cec2c/external/arm_compiler/bin/../lib/gcc/arm-linux-gnueabihf/4.9.3/../../../../arm-linux-gnueabihf/include/c++/4.9.3/cstdlib:72,\r\n                 from # tensorflow/core/platform/posix/net.cc:19:\r\ntensorflow/core/platform/posix/net.cc: In function 'bool tensorflow::internal::{anonymous}::IsPortAvailable(int*, bool)':\r\ntensorflow/core/platform/posix/net.cc:60:19: error: invalid 'asm': invalid operand for code 'w'\r\n   addr.sin_port = htons(static_cast<uint16_t>(*port));\r\n                   ^\r\ntensorflow/core/platform/posix/net.cc:75:17: error: invalid 'asm': invalid operand for code 'w'\r\n   actual_port = ntohs(addr.sin_port);\r\n                 ^\r\n\r\n", "comments": ["@samjabrahams Please Help", "This web page seems to be the source of your quote: https://nerdland.net/unstumping-the-internet/invalid-operand-for-code-w/\r\n\r\nDid you check the headers that are being referenced by your build, as specified in that note?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 17431, "title": "docker image for 1.6.0 is missing CUPTI libraries", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04, 4.4.0-104-generic\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version**: v1.6.0-0-gd2e24b6 1.6.0\r\n- **Bazel version**: N/A\r\n- **CUDA/cuDNN version**: 9.0.176 (from docker image)\r\n- **GPU model and memory**: GeForce GTX TITAN X\r\n- **Exact command to reproduce**: (see below)\r\n\r\n### Describe the Error\r\n\r\nCUPTI library is missing in docker image `tensorflow/tensorflow:1.6.0-gpu-py3`. This is required when using `trace_level=tf.RunOptions.FULL_TRACE` and `run_metadata` in session.run.\r\n\r\n### Source code / logs\r\nTo reproduce:\r\n<!-- language: lang-py -->\r\n    # run.py\r\n    import tensorflow as tf\r\n\r\n    with tf.Session().as_default() as sess:\r\n        test_op = tf.no_op()\r\n\r\n        run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\n        run_metadata = tf.RunMetadata()\r\n        sess.run(test_op, options=run_options, run_metadata=run_metadata)\r\n\r\nAnd run:\r\n`> nvidia-docker run --volume /path/to/run.py:/run.py --rm -it tensorflow/tensorflow:1.6.0-gpu-py3 python /run.py`\r\n\r\nThe error message is:\r\n<!-- language: lang-none -->\r\n    2018-03-05 00:00:00.000000: I tensorflow/stream_executor/dso_loader.cc:141] Couldn't open CUDA library libcupti.so.9.0. LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\n    2018-03-05 00:00:00.000000: F ./tensorflow/stream_executor/lib/statusor.h:212] Non-OK-status: status_ status: Failed precondition: could not dlopen DSO: libcupti.so.9.0; dlerror: libcupti.so.9.0: cannot open shared object file: No such file or directory\r\n\r\nAdditional information from within the docker image:\r\n\r\n<!-- language: lang-bash -->\r\n    > echo $LD_LIBRARY_PATH\r\n    /usr/local/cuda/extras/CUPTI/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\n    > ll /usr/local/cuda/extras/CUPTI/lib64\r\n    ls: cannot access '/usr/local/cuda/extras/CUPTI/lib64': No such file or directory\r\n    > find / -name libcupti*\r\n    (not found)\r\n\r\nThe library seems to be missing in this build. Worked with tensorflow/tensorflow:1.4.0-gpu-py3.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@tensorflowbutler: Updated accordingly. They are all not relevant, still filled them in.", "In case anyone needs a workaround: Go to https://developer.nvidia.com/cuda-90-download-archive, Select: Linux, x86_64, Ubuntu, 16.04, deb (local). Download base installer and all patches.\r\n\r\nBuild a new docker image with Dockerfile:\r\n```Dockerfile\r\nFROM tensorflow/tensorflow:1.6.0-gpu-py3\r\nADD cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64.deb /tmp\r\nADD cuda-repo-ubuntu1604-9-0-local-cublas-performance-update_1.0-1_amd64.deb /tmp\r\nADD cuda-repo-ubuntu1604-9-0-local-cublas-performance-update-2_1.0-1_amd64.deb /tmp\r\n# Does not require any internet connection\r\nRUN dpkg -i /tmp/cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64.deb && \\\r\n dpkg -i /tmp/cuda-repo-ubuntu1604-9-0-local-cublas-performance-update_1.0-1_amd64.deb && \\\r\n dpkg -i /tmp/cuda-repo-ubuntu1604-9-0-local-cublas-performance-update-2_1.0-1_amd64.deb && \\\r\n apt-key add /var/cuda-repo-9-0-local/7fa2af80.pub && \\\r\n apt-get update -o Dir::Etc::sourcelist=\"sources.list.d/cuda-9-0-local.list\" -o Dir::Etc::sourceparts=\"-\" -o APT::Get::List-Cleanup=\"0\" && \\\r\n apt-get update -o Dir::Etc::sourcelist=\"sources.list.d/cuda-9-0-local-cublas-performance-update.list\" -o Dir::Etc::sourceparts=\"-\" -o APT::Get::List-Cleanup=\"0\" && \\\r\n apt-get update -o Dir::Etc::sourcelist=\"sources.list.d/cuda-9-0-local-cublas-performance-update-2.list\" -o Dir::Etc::sourceparts=\"-\" -o APT::Get::List-Cleanup=\"0\" && \\\r\n apt-get install -y cuda-libraries-9-0 cuda-command-line-tools-9-0 cuda-cublas-9-0\r\n```", "cupti is present in `tensorflow/tensorflow:1.6.0-devel-gpu-py3`. I think it makes sense.\r\n\r\nIn addition, in CUDA 9.0, CUPTI is not in its own package, so we would have to pull ~100MB of dependencies just to get CUPTI in the non-devel image.\r\nI suggest that we either keep CUPTI out of the non-devel image, or we wait until cuda 9.1+ to add it back since it finally has a dedicated package (`cuda-cupti-9-1`).\r\n\r\n@gunan ", "Thanks for your response. I see that cupti does not have a dedicated package f\u00fcr cuda 8.0 for Ubuntu 16.04 (https://packages.ubuntu.com/search?keywords=libcupti) and is not included in the main package. Still one might wish not to build tensorflow (thats what devel is for, right?), but still want to get run_metadata. libcupti.so is ~5.7MB, thus I see no issue to put it in the nondevel image.", "Unfortunately, CUPTI is a runtime dependency for certain modules in TF, so it may be best to pull in the extra 100MBs. We try to provide the non-devel images to be as a quickstart to TF, so it is better to make sure they have as little issues as possible.\r\nBut once cupti has its own package it looks like this will be much better, thanks for the information @flx42 \r\n", "I could bundle this fix with the change I suggested here: https://github.com/tensorflow/tensorflow/issues/17566#issuecomment-372851697"]}, {"number": 17430, "title": "the correct argument is \"--output_image\"", "body": "It looks like the correct argument name is \"--output_image\".  I got the following error when I ran it with \"--output_png\":\r\n```\r\nINFO: Running command line: bazel-bin/tensorflow/examples/wav_to_spectrogram/wav_to_spectrogram '--input_wav=/tmp/speech_dataset/happy/ab00c4b2_nohash_0.wav' '--output_png=/tmp/spectrogram.png'\r\n2018-03-04 23:47:22.532841: E tensorflow/examples/wav_to_spectrogram/main.cc:54] Unknown argument --output_png=/tmp/spectrogram.png\r\nusage: /private/var/tmp/_bazel_hakaydin/12fe9c53feea8848ab5cfab4ea3dcd47/execroot/org_tensorflow/bazel-out/darwin-py3-opt/bin/tensorflow/examples/wav_to_spectrogram/wav_to_spectrogram\r\nFlags:\r\n\t--input_wav=\"tensorflow/core/kernels/spectrogram_test_data/short_test_segment.wav\"\tstring\taudio file to load\r\n\t--window_size=256                \tint32\tfrequency sample window width\r\n\t--stride=128                     \tint32\thow far apart to place frequency windows\r\n\t--brightness=64.000000           \tfloat\tcontrols how bright the output image is\r\n\t--output_image=\"spectrogram.png\" \tstring\twhere to save the spectrogram image to\r\n\r\nERROR: Non-zero return code '255' from command: Process exited with status 255\r\n```", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to `go/cla#troubleshoot`.\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it.", "@tjeckleburg Can you check again? You might need to sign it with the email you made your commits in.", "Looks like CLA is still not signed.\r\nPlease re-send the PR, after modifying the commit author email to the one you signed the CLA with.\r\nI will close the PR for now."]}, {"number": 17429, "title": "How to get the predicted probabilities for image classification using tensor flow CNN", "body": "I am studying the tensor flow for CNN image classification following the official document(https://www.tensorflow.org/tutorials/layers) . The code in above link for image classification with tensorflow CNN, I want to get all the probabilities of each image labels in test data. I want to ask.\r\n\r\nHow to get the probabilities of predicting result for each image.\r\nHow to evaluate the result of predicted probabilities, in this example using accuracy for label classification. Thanks!\r\n\r\n\r\n```\r\n`import numpy as np\r\nimport tensorflow as tf\r\n\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\n\r\n\r\ndef cnn_model_fn(features, labels, mode):\r\n  \"\"\"Model function for CNN.\"\"\"\r\n  # Input Layer\r\n  # Reshape X to 4-D tensor: [batch_size, width, height, channels]\r\n  # MNIST images are 28x28 pixels, and have one color channel\r\n  input_layer = tf.reshape(features[\"x\"], [-1, 28, 28, 1])\r\n\r\n  # Convolutional Layer #1\r\n  # Computes 32 features using a 5x5 filter with ReLU activation.\r\n  # Padding is added to preserve width and height.\r\n  # Input Tensor Shape: [batch_size, 28, 28, 1]\r\n  # Output Tensor Shape: [batch_size, 28, 28, 32]\r\n  conv1 = tf.layers.conv2d(\r\n      inputs=input_layer,\r\n      filters=32,\r\n      kernel_size=[5, 5],\r\n      padding=\"same\",\r\n      activation=tf.nn.relu)\r\n\r\n  # Pooling Layer #1\r\n  # First max pooling layer with a 2x2 filter and stride of 2\r\n  # Input Tensor Shape: [batch_size, 28, 28, 32]\r\n  # Output Tensor Shape: [batch_size, 14, 14, 32]\r\n  pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\r\n\r\n  # Convolutional Layer #2\r\n  # Computes 64 features using a 5x5 filter.\r\n  # Padding is added to preserve width and height.\r\n  # Input Tensor Shape: [batch_size, 14, 14, 32]\r\n  # Output Tensor Shape: [batch_size, 14, 14, 64]\r\n  conv2 = tf.layers.conv2d(\r\n      inputs=pool1,\r\n      filters=64,\r\n      kernel_size=[5, 5],\r\n      padding=\"same\",\r\n      activation=tf.nn.relu)\r\n\r\n  # Pooling Layer #2\r\n  # Second max pooling layer with a 2x2 filter and stride of 2\r\n  # Input Tensor Shape: [batch_size, 14, 14, 64]\r\n  # Output Tensor Shape: [batch_size, 7, 7, 64]\r\n  pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\r\n\r\n  # Flatten tensor into a batch of vectors\r\n  # Input Tensor Shape: [batch_size, 7, 7, 64]\r\n  # Output Tensor Shape: [batch_size, 7 * 7 * 64]\r\n  pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\r\n\r\n  # Dense Layer\r\n  # Densely connected layer with 1024 neurons\r\n  # Input Tensor Shape: [batch_size, 7 * 7 * 64]\r\n  # Output Tensor Shape: [batch_size, 1024]\r\n  dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\r\n\r\n  # Add dropout operation; 0.6 probability that element will be kept\r\n  dropout = tf.layers.dropout(\r\n      inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\r\n\r\n  # Logits layer\r\n  # Input Tensor Shape: [batch_size, 1024]\r\n  # Output Tensor Shape: [batch_size, 10]\r\n  logits = tf.layers.dense(inputs=dropout, units=10)\r\n\r\n  predictions = {\r\n      # Generate predictions (for PREDICT and EVAL mode)\r\n      \"classes\": tf.argmax(input=logits, axis=1),\r\n      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\r\n      # `logging_hook`.\r\n      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\r\n  }\r\n  if mode == tf.estimator.ModeKeys.PREDICT:\r\n    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\r\n\r\n  # Calculate Loss (for both TRAIN and EVAL modes)\r\n  loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\r\n\r\n  # Configure the Training Op (for TRAIN mode)\r\n  if mode == tf.estimator.ModeKeys.TRAIN:\r\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\r\n    train_op = optimizer.minimize(\r\n        loss=loss,\r\n        global_step=tf.train.get_global_step())\r\n    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\r\n\r\n  # Add evaluation metrics (for EVAL mode)\r\n  eval_metric_ops = {\r\n      \"accuracy\": tf.metrics.accuracy(\r\n          labels=labels, predictions=predictions[\"classes\"])}\r\n  return tf.estimator.EstimatorSpec(\r\n      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\r\n\r\n\r\ndef main(unused_argv):\r\n  # Load training and eval data\r\n  mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\r\n  train_data = mnist.train.images  # Returns np.array\r\n  train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\r\n  eval_data = mnist.test.images  # Returns np.array\r\n  eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)\r\n\r\n  # Create the Estimator\r\n  mnist_classifier = tf.estimator.Estimator(\r\n      model_fn=cnn_model_fn, model_dir=\"/tmp/mnist_convnet_model\")\r\n\r\n  # Set up logging for predictions\r\n  # Log the values in the \"Softmax\" tensor with label \"probabilities\"\r\n  tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\r\n  logging_hook = tf.train.LoggingTensorHook(\r\n      tensors=tensors_to_log, every_n_iter=50)\r\n\r\n  # Train the model\r\n  train_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n      x={\"x\": train_data},\r\n      y=train_labels,\r\n      batch_size=100,\r\n      num_epochs=None,\r\n      shuffle=True)\r\n  mnist_classifier.train(\r\n      input_fn=train_input_fn,\r\n      steps=20000,\r\n      hooks=[logging_hook])\r\n\r\n  # Evaluate the model and print results\r\n  eval_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n      x={\"x\": eval_data},\r\n      y=eval_labels,\r\n      num_epochs=1,\r\n      shuffle=False)\r\n  eval_results = mnist_classifier.evaluate(input_fn=eval_input_fn)\r\n  print(eval_results)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n  tf.app.run()`\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 17428, "title": "Correct capitalization", "body": "", "comments": []}, {"number": 17427, "title": "Feature Request: Train Using Multiple GPUs with Tensorflow in a Tower-like Fashion for tensorflow 1.4", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nN/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 8.1\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary\r\n- **TensorFlow version (use command below)**:\r\n1.4\r\n- **Python version**: \r\n3.6\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **GCC/Compiler version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\n8.0\r\n- **GPU model and memory**:\r\n2 NVIDIA GeForce GTX 1070 (8 GB each)\r\n-**Exact command to reproduce**:\r\nN/A\r\n\r\n### Describe the problem\r\nI tried training my model with a large batch size that is supposed to be enough just for two GPUs but I encounter out of memory errors. I'm using the Estimator API and I was wondering if it handles multi-gpu training like in this [example](https://www.tensorflow.org/tutorials/deep_cnn#training_a_model_using_multiple_gpu_cards) using the cifar_10 dataset. If it doesn't, it would be nice to have the Estimator API handle multi-gpu training such that we can use larger batch sizes.\r\n\r\n### Source code / logs\r\nHere's the architecture of the model I trained.\r\n\r\n```\r\n{\r\n  \"network\":[\r\n    {\"layer_type\": \"input_layer\", \"name\": \"inputs\", \"shape\": [-1, 168, 168, 1]},\r\n    {\"layer_type\": \"l2_normalize\", \"axis\": [1, 2]},\r\n    {\"layer_type\": \"conv2d\", \"num_filters\": 16, \"kernel_size\": [3, 3]},\r\n    {\"layer_type\": \"max_pool2d\", \"pool_size\": [2, 2]},\r\n    {\"layer_type\": \"l2_normalize\", \"axis\": [1, 2]},\r\n    {\"layer_type\": \"conv2d\", \"num_filters\": 32, \"kernel_size\": [3, 3]},\r\n    {\"layer_type\": \"max_pool2d\", \"pool_size\": [2, 2]},\r\n    {\"layer_type\": \"l2_normalize\", \"axis\": [1, 2]},\r\n    {\"layer_type\": \"dropout\", \"keep_prob\": 0.5},\r\n    {\"layer_type\": \"conv2d\", \"num_filters\": 64, \"kernel_size\": [3, 3]},\r\n    {\"layer_type\": \"max_pool2d\", \"pool_size\": [2, 2]},\r\n    {\"layer_type\": \"l2_normalize\", \"axis\": [1, 2]},\r\n    {\"layer_type\": \"dropout\", \"keep_prob\": 0.5},\r\n    {\"layer_type\": \"collapse_to_rnn_dims\"},\r\n    {\"layer_type\": \"birnn\", \"num_hidden\": 128, \"cell_type\": \"LSTM\"},\r\n    {\"layer_type\": \"dropout\", \"keep_prob\": 0.5}\r\n  ],\r\n  \"output_layer\": \"ctc_decoder\",\r\n  \"loss\": \"ctc\",\r\n  \"metrics\": [\"label_error_rate\"],\r\n  \"learning_rate\": 0.001,\r\n  \"optimizer\": \"adam\"\r\n}\r\n```\r\n\r\nThe image dimensions are 168x168 px and the working batch size is 240 which I would like to double.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nExact command to reproduce", "It\u2019s not really relevant to the issue since this is a feature request.", "It's actually super easy to get data parallel, single-machine, multi-GPU training going with a `tf.estimator` by using the [replicate_model_fn](https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/estimator/replicate_model_fn) and [TowerOptimizer](https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/estimator/TowerOptimizer) decorators. Just make sure your batch size is divisible by the number of GPUs.", "Can this be supported in tensorflow 1.4 as well? ", "I created this pull request for that. https://github.com/tensorflow/tensorflow/pull/17462", "@selcouthlyBlue did you send a PR against master?", "I did send a PR against master. https://github.com/tensorflow/tensorflow/pull/17478 But it's not related to this request.", "Ok how about sending one for this request?\n\nOn Tue, Apr 3, 2018, 7:17 PM Jerome <notifications@github.com> wrote:\n\n> I did send a PR against master. #17478\n> <https://github.com/tensorflow/tensorflow/pull/17478> But it's not\n> related to this request.\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/17427#issuecomment-378457967>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_Sbce9LW47Zy2HhmDnUIIJgrDSMizDks5tlC0ygaJpZM4SbxHD>\n> .\n>\n", "This feature apparently exists in master already. I just want it to be supported in tensorflow 1.4 as well. Here's the pull request I made against r1.4 https://github.com/tensorflow/tensorflow/pull/17462", "Thanks for checking! Ok, we can close this then\n\nOn Tue, Apr 3, 2018, 7:41 PM Jerome <notifications@github.com> wrote:\n\n> This feature apparently exists in master already. I just want it to be\n> supported in tensorflow 1.4 as well. Here's the pull request I made against\n> r1.4 #17462 <https://github.com/tensorflow/tensorflow/pull/17462>\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/17427#issuecomment-378461684>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbSBu9dHjMlizMRT6n1awJB7UWQ3Xks5tlDLEgaJpZM4SbxHD>\n> .\n>\n", "So will the said feature be added to tensorflow 1.4? I did notice that my pull request for that branch was closed for the following reason:\r\n\r\n> TensorFlow development is on master rather than any of the release branches, and once there is a release branch, we only accept bugfixes for that branch, so we can't accept this right now.", "You need to send PRs directly against master. The release branches are for\nour use only. They contain the release bits. The release branches are cut\nfrom the master branch periodically, do if you get something into master,\nit will become part of the next release\n\nOn Tue, Apr 3, 2018, 7:52 PM Jerome <notifications@github.com> wrote:\n\n> So will the said feature be added to tensorflow 1.4? I did notice that my\n> pull request for that branch was closed for the following reason:\n>\n> TensorFlow development is on master rather than any of the release\n> branches, and once there is a release branch, we only accept bugfixes for\n> that branch, so we can't accept this right now.\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/17427#issuecomment-378463490>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbU-cdLbn9RtcVs8kI_86KbfD1Qpoks5tlDVxgaJpZM4SbxHD>\n> .\n>\n", "So every feature that gets added into master are periodically added into the release branches including the older ones? ", "No, we only cherry pick important bug fixes to the older releases\n\nOn Tue, Apr 3, 2018, 8:10 PM Jerome <notifications@github.com> wrote:\n\n> So every feature that gets added into master are periodically added into\n> the release branches including the older ones?\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/17427#issuecomment-378466130>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbVEGN-Sm0I4XW9-JOB_4DJqjUdn5ks5tlDmWgaJpZM4SbxHD>\n> .\n>\n", "Also when we cherry pick these but sizes, we pick them from master.\n\nOn Tue, Apr 3, 2018, 8:47 PM Patrick Nguyen <drpng@google.com> wrote:\n\n> No, we only cherry pick important bug fixes to the older releases\n>\n> On Tue, Apr 3, 2018, 8:10 PM Jerome <notifications@github.com> wrote:\n>\n>> So every feature that gets added into master are periodically added into\n>> the release branches including the older ones?\n>>\n>> \u2014\n>> You are receiving this because you were assigned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/17427#issuecomment-378466130>,\n>> or mute the thread\n>> <https://github.com/notifications/unsubscribe-auth/AT_SbVEGN-Sm0I4XW9-JOB_4DJqjUdn5ks5tlDmWgaJpZM4SbxHD>\n>> .\n>>\n>\n", "In that case, I'm closing this issue now."]}]