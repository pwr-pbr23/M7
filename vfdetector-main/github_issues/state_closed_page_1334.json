[{"number": 13077, "title": "Branch 168917534", "body": "", "comments": []}, {"number": 13076, "title": "unit test tools print all failure logs if failed", "body": "When a PR is tested failed, Jenkins just reports failure and tells where to check log. However, at least for me, I don't know whether I have the permission, and where to find the logs. So I have to reproduce the failed test on my own machine, but maintaining all kinds of environment is an unnecessary burden for each developer (cpu, gpu, py2, py3, window...)\r\n\r\n```bash\r\n//tensorflow/tools/test:check_futures_test                               PASSED in 1.3s\r\n//tensorflow/user_ops:ackermann_test                                     PASSED in 2.3s\r\n//tensorflow/user_ops:duplicate_op_test                                  PASSED in 2.2s\r\n//tensorflow/user_ops:invalid_op_test                                    PASSED in 2.1s\r\n//tensorflow/python/kernel_tests:matrix_solve_ls_op_test                TIMEOUT in 65.0s\r\n  /var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/testlogs/tensorflow/python/kernel_tests/matrix_solve_ls_op_test/test.log\r\n\r\nExecuted 1205 out of 1205 tests: 1204 tests pass and 1 fails locally.\r\n```\r\n\r\nIt will be more convenient to attach total failure content at the end for developers to debug, like reports of `py.test`:\r\n\r\n```bash\r\n$ pytest\r\n======= test session starts ========\r\nplatform linux -- Python 3.x.y, pytest-3.x.y, py-1.x.y, pluggy-0.x.y\r\nrootdir: $REGENDOC_TMPDIR, inifile:\r\ncollected 1 item\r\n\r\ntest_sample.py F\r\n\r\n======= FAILURES ========\r\n_______ test_answer ________\r\n\r\n    def test_answer():\r\n>       assert inc(3) == 5\r\nE       assert 4 == 5\r\nE        +  where 4 = inc(3)\r\n\r\ntest_sample.py:5: AssertionError\r\n======= 1 failed in 0.12 seconds ========\r\n```\r\n\r\n", "comments": ["@gunan, is there any way we can support a better workflow for accessing the failing logs. Perhaps with your new project?\r\n", "Next week we should have something\r\n", "Actually. in the full log, if you serarch in page for the failed test, you can find the full dump of the test logs.\r\nHowever, I agree that they are difficult to find.\r\nNext week, we will migrate to a better solution, which should solve this problem.", "Cool, since the issue will be fixed later, so I close it."]}, {"number": 13075, "title": "distributed Tensorflow assign device issue", "body": "I recently found an interesting issue on device assignment when I ran the following simple code `test.py`:\r\n\r\n```\r\nimport argparse\r\nimport tensorflow as tf\r\n\r\nCLUSTER_SPEC = {\"ps\": [\"localhost:2222\"],\r\n                \"worker\": [\"localhost:1111\", \"localhost:1112\"]}\r\n\r\n\r\ndef parse_command_arguments():\r\n    \"\"\" Set up and parse the command line arguments. \"\"\"\r\n    parser = argparse.ArgumentParser(\r\n        description=\"Parameters and Arguments for the Test.\")\r\n    parser.add_argument(\r\n        \"--job_name\", type=str, default=\"\", help=\"One of 'ps', 'worker'\")\r\n    parser.add_argument(\r\n        \"--task_index\", type=int, default=0, help=\"Index of task\")\r\n    return parser.parse_args()\r\n\r\n\r\ndef start_server(job_name, task_index, tf_config):\r\n    \"\"\" Create a server based on a cluster spec. \"\"\"\r\n    cluster = tf.train.ClusterSpec(CLUSTER_SPEC)\r\n    server = tf.train.Server(\r\n        cluster, config=tf_config, job_name=job_name, task_index=task_index)\r\n    return server, cluster\r\n\r\n\r\ndef model(cluster=None, worker_device=None):\r\n    \"\"\" Build up a simple estimator model. \"\"\"\r\n    with tf.device(tf.train.replica_device_setter(\r\n            worker_device=worker_device, cluster=cluster)):\r\n        W = tf.Variable([1.0], tf.float32)\r\n        b = tf.Variable([10.0], tf.float32)\r\n        x = tf.placeholder(tf.float32)\r\n        y = W * x + b\r\n    return W, b, x, y\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    arguments = parse_command_arguments()\r\n    job_name = arguments.job_name\r\n    task_index = arguments.task_index\r\n    # Set up tensorflow configuration.\r\n    tf_config = tf.ConfigProto(\r\n        allow_soft_placement=True, device_count={'GPU': 1},\r\n        log_device_placement=True)\r\n    # Start a server.\r\n    server, cluster = start_server(job_name, task_index, tf_config)\r\n\r\n    if job_name == \"ps\":\r\n        server.join()\r\n    else:\r\n        worker_device = \"/gpu:0\"\r\n        W, b, x, y = model(cluster, worker_device=worker_device)\r\n        is_chief = (arguments.task_index == 0) and (\r\n            arguments.job_name == 'worker')\r\n        sess = tf.train.MonitoredTrainingSession(\r\n             master=server.target, is_chief=is_chief, config=tf_config)\r\n        # Run the model.\r\n        step = 0\r\n        x_train = [1, 2, 3, 4]\r\n        y_train = [0, -1, -2, -3]\r\n        while not sess.should_stop() and step < 1000:\r\n            sess.run([y], {x: x_train, y: y_train})\r\n            step += 1\r\n```\r\nAs the code uses `tf.train.replica_device_setter()` with `worker_device=\"/gpu:0\"`, I imagine all tensorflow `Variable` ops go to the parameter server, while all other type of ops stay in the worker.\r\n\r\nHowever, if we run the above code:\r\n`$ python test.py --job_name ps --task_index 0`\r\n`$ python test.py --job_name worker --task_index 0`\r\n`$ python test.py --job_name worker --task_index 1`\r\nI notice that all tensorflow ops go to parameter server, for example in the worker log: \r\n`report_uninitialized_variables/boolean_mask/strided_slice/stack_2: (Const)/job:ps/replica:0/task:0/gpu:0`.\r\n\r\nBut if I change `worker_device=\"/gpu:0\"` in the code to `worker_device=\"/job:worker/task:0/gpu:0\"`, then the result is correct (all non-Variable ops go to worker). So does it mean there is a bug for `tf.train.replica_device_setter()` to automatically assign devices?\r\n\r\nP.S The above code runs on single GPU (TITAN X (Pascal)/PCIe/SSE2) with Ubuntu 16.04, Python 3.5 and Tensorflow v1.2", "comments": ["Hm, that seems strange, from the logic [here](https://github.com/tensorflow/tensorflow/blob/0dfb1825ea20e7d4da6c57e4ba3710547562d423/tensorflow/python/training/device_setter.py#L119) it seems anything that's not a Variable will be assigned to be on worker_device.\r\n\r\nSince only workers construct the graph, \"/gpu:0\" should resolve to worker's /gpu:0\r\nMaybe you could add some print's in device_setter.py or add `print(graph_def.SerializeToString()` [here](https://github.com/tensorflow/tensorflow/blob/581f59447ac138857f07169f9fe7b1c2c5687a6b/tensorflow/python/client/session.py#L1351) to see what \"device\" annotation is given to those ops", "@yaroslavvb wrote:\r\n> Since only workers construct the graph, \"/gpu:0\" should resolve to worker's /gpu:0\r\n\r\nThat's not how the placement algorithm works. You must include `\"/job:worker\"` in the device spec to guarantee that the op will be placed on a task in the `worker` job.\r\n\r\nIf you override the default `worker_device` argument (which is `\"/job:worker\"`) with `\"/gpu:0\"`, there's no guarantee that the task will be placed in any particular task.", "@yaroslavvb @mrry Thanks for the reply! That makes sense. But I still don't understand why all tensorflow ops go to the parameter server every time if we do `worker_device=\"/gpu:0\"`, is it intentional?\r\n\r\nThe other issue is I compared the performance for using `worker_device=\"/job:worker/task:0\"` vs. `worker_device=\"/gpu:0\"`, and I noticed that when `worker_device=\"/gpu:0\"` where all ops go to parameter server, the training performance is much better than using `worker_device=\"/job:worker/task:0\"` (I compared them in different code which real training is involved, and the training speed for using `worker_device=\"/gpu:0\"` is much much faster than using `worker_device=\"/job:worker/task:0\"`). Is it also what we should expect? Why does it happen?", "@mrry interesting .... the the following example is similar, is it also going to run the op on an arbitrary machine?\r\n\r\n```\r\nserver = tf.Server(...)\r\nsess = tf.Session(server.target)\r\nwith tf.device(\"/gpu:0\"):\r\n   sess.run(tf.random_uniform())\r\n```", "It sounds like short form (\"/gpu:0\") for distributed is not really supported, I'll close it since it doesn't seem to be a bug, but if someone figures out where the logic lives that determines where such ops are placed, it would be useful to update it here", "@RuofanKong When the operation runs in worker's GPU, the worker needs to fetch the variables from the PS by network. It is slower than the operation runs in PS because  the variables are stored in  PS. Maybe you can use the tcpdump to catch the data in network to prove what I guess. "]}, {"number": 13074, "title": "Revert accidentally changed android_test_proto_lib dep to protos_cc", "body": "android_test_proto_lib is a Google-internal target", "comments": ["Jenkins, test this please", "Jenkins, test this please.", "Jenkins, test this please."]}, {"number": 13073, "title": "Java/Android: Fix bug introduce in #12643", "body": "The contents of the graphDef file weren't actually being read into the\r\nbyte array.", "comments": ["@asimshankar, thanks for your PR! By analyzing the history of the files in this pull request, we identified @andrewharp, @tensorflower-gardener and @resec to be potential reviewers.", "Jenkins, test this please", "`fft_ops_test` is known failure internally, not sure what happened to `nn_grad_test`. Trying again.\r\n\r\nJenkins, test this please.", "Jenkins, test this please.", "I ended up pushing from internal. Thank you for the fix!"]}, {"number": 13072, "title": "Revert \"Updating default version for building from master source.\"", "body": "Reverts tensorflow/tensorflow#12970\r\n\r\nThis is breaking all our nightly links and our gcs links. After further inspection, we need to create a dynamically linked update_version process. For now I'll add the update_version logic combined with head links for all nightly packages. ", "comments": ["Jenkins, test this please.", "Android was a flake: http://ci.tensorflow.org/job/tensorflow-pull-requests-android/6742/\r\nLinux GPU is broken."]}, {"number": 13071, "title": "Unreachable input gradients", "body": "This PR checks if all the inputs are reachable from the outputs during AddSymbolicGradients.\r\n@skye would be the best person to review this task as she asked me to add a TODO during a previous PR.", "comments": ["Can one of the admins verify this patch?", "@theflofly, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @suharshs and @skye to be potential reviewers.", "It should be okay.", "Actually some lines exceeded 80 chars.", "Jenkins, test this please.", "@tensorflow-jenkins test this please"]}, {"number": 13070, "title": "Use AllClose instead of AllEqual in layers tests", "body": "- While simple implementations of convolution (e.g., gemm-based) will\r\n  produce exact results in these tests, general implementations\r\n  only guarantee floating-point precision.\r\n- (One of these tests was previously observed to fail with a max error\r\n  of 1e-7).", "comments": ["Can one of the admins verify this patch?", "@nluehr, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @jart and @zhangyaobit to be potential reviewers.", "Where are you seeing this error? If we introduce this, I'd like to set the tightest tolerance bounds so that we can track precision changes over time.", "@nluehr gentle ping?\r\n", "Sorry for the delay. We are no longer able to reproduce the test failures on the master branch using either cuda 9/cudnn 7 or cuda 8/cudnn 6. So I'll Close the PR."]}, {"number": 13069, "title": "Get stuck in the process of building from sources", "body": "## System information\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04 LTS\r\nTensorFlow installed from (source or binary): Source\r\nTensorFlow version (use command below): 1.3 CPU\r\nBazel version (if compiling from source): 5.0\r\nExact command to reproduce: \r\n\r\n`\r\nRUN tensorflow/tools/ci_build/builds/configured CPU \\\r\n    bazel build -c opt --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" \\\r\n        tensorflow/tools/pip_package:build_pip_package && \\\r\n    bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip && \\\r\n    pip --no-cache-dir install --upgrade /tmp/pip/tensorflow-*.whl \r\n`\r\n\r\n\r\n\r\n## Describe the problem\r\n\r\nI tried to build a docker image using the dockerfile provided by tensorflow repository, but every time I faced with the problem it got stuck in the process, repeating the output \r\n\r\n`\r\n____[2,615 / 3,437] Still waiting for 2 jobs to complete:\r\n      Running (standalone):\r\n        Compiling tensorflow/core/kernels/matrix_solve_ls_op.cc, 851 s\r\n        Compiling tensorflow/core/kernels/svd_op_double.cc, 839 s\r\n____[2,615 / 3,437] Still waiting for 2 jobs to complete:\r\n`\r\n\r\n\r\nThe following is the first output of this layer which may be helpful.\r\n\r\n`\r\nExtracting Bazel installation...\r\nYou have bazel 0.5.0 installed.\r\nFound possible Python library paths:\r\n  /usr/local/lib/python2.7/dist-packages\r\n  /usr/lib/python2.7/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\r\nUsing python library path: /usr/local/lib/python2.7/dist-packages\r\nNo MKL support will be enabled for TensorFlow\r\njemalloc enabled\r\nNo VERBS support will be enabled for TensorFlow\r\nNo OpenCL support will be enabled for TensorFlow\r\nMPI support will not be enabled for TensorFlow\r\nConfiguration finished\r\n/tensorflow\r\nINFO: Reading 'startup' options from /etc/bazel.bazelrc: --batch\r\nTF_BUILD_INFO = {container_type: \"cpu\", command: \"bazel build -c opt --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0 tensorfl\r\ns/pip_package:build_pip_package\", source_HEAD: \"b46340f40fe5e2ec9bfcd385b07cfb914055fb51\", source_remote_origin:\r\n//github.com/tensorflow/tensorflow.git\", OS: \"Linux\", kernel: \"4.9.41-moby\", architecture: \"x86_64\", processor: \"\r\n5550M APU with Radeon(tm) HD Graphics\", processor_count: \"2\", memory_total: \"2027780 kB\", swap_total: \"1048572 kB\r\nl_version: \"Build label: 0.5.0\", Java_version: \"1.8.0_131\", Python_version: \"2.7.12\", gpp_version: \"g++ (Ubuntu 5\r\nbuntu1~16.04.5) 5.4.0 20160609\", swig_version: \"\", NVIDIA_driver_version: \"\", CUDA_device_count: \"0\", CUDA_device\r\n \"\", CUDA_toolkit_version: \"\"}\r\n`", "comments": ["@caisq can you reproduce? I'm not sure how to run that docker command.\r\n\r\n@Luxios can you try waiting, say, an hour to see if it eventually finishes?", "@Luxios, did you try the approach outlined in this section? \r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/docker#rebuilding-the-containers", "@reedwm I've tried many times.\r\nMy dockerfile modified from yours\r\n`\r\nFROM ubuntu:latest\r\nMAINTAINER Luxios\r\nRUN cp /etc/apt/sources.list /etc/apt/sources.list_backup && \\\r\n    apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E084DAB9\r\nCOPY sources.list /etc/apt/\r\nRUN apt-get update && apt-get install -y --no-install-recommends --fix-missing \\\r\n        build-essential \\\r\n        curl \\\r\n        git \\\r\n        libcurl3-dev \\\r\n        libfreetype6-dev \\\r\n        libpng12-dev \\\r\n        libzmq3-dev \\\r\n        pkg-config \\\r\n        python-dev \\\r\n        rsync \\\r\n        software-properties-common \\\r\n        unzip \\\r\n        zip \\\r\n        zlib1g-dev \\\r\n        openjdk-8-jdk \\\r\n        openjdk-8-jre-headless \\\r\n        libopenblas-dev liblapack-dev libopencv-dev \\\r\n        polipo \\\r\n        vim \\\r\n        r-base \\\r\n        libssl-dev libcurl4-openssl-dev && \\\r\n    apt-get install x11vnc xvfb firefox -y\r\n    \r\nCOPY config /etc/polipo/config\r\nRUN curl -fSsL -O https://bootstrap.pypa.io/get-pip.py && \\\r\n    python get-pip.py && \\\r\n    rm get-pip.py && \\\r\n    pip --no-cache-dir install \\\r\n        Pillow \\\r\n        h5py \\\r\n        requests \\\r\n        ipykernel \\\r\n        jupyter \\\r\n        matplotlib \\\r\n        numpy \\\r\n        pandas \\\r\n        scipy \\\r\n        sklearn \\\r\n        shadowsocks \\\r\n        && \\\r\n    python -m ipykernel.kernelspec \r\n\r\n\r\nCOPY jupyter_notebook_config.py /root/.jupyter/\r\nCOPY run_jupyter.sh /\r\n\r\nRUN echo \"startup --batch\" >>/etc/bazel.bazelrc\r\nRUN echo \"build --spawn_strategy=standalone --genrule_strategy=standalone\" \\\r\n    >>/etc/bazel.bazelrc\r\nENV BAZEL_VERSION 0.5.0\r\nWORKDIR /\r\nCOPY shawdowsocks.json /\r\nRUN sslocal -c /shawdowsocks.json -d start && \\\r\n    /etc/init.d/polipo restart && \\\r\n    export http_proxy=\"http://127.0.0.1:8123/\" && \\\r\n    mkdir /bazel && \\\r\n    cd /bazel && \\\r\n    curl -H \"User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36\" -fSsL -O https://github.com/bazelbuild/bazel/releases/download/$BAZEL_VERSION/bazel-$BAZEL_VERSION-installer-linux-x86_64.sh && \\\r\n    curl -H \"User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36\" -fSsL -o /bazel/LICENSE.txt https://raw.githubusercontent.com/bazelbuild/bazel/master/LICENSE && \\\r\n    chmod +x bazel-*.sh && \\\r\n    ./bazel-$BAZEL_VERSION-installer-linux-x86_64.sh && \\\r\n    cd / && \\\r\n    rm -f /bazel/bazel-$BAZEL_VERSION-installer-linux-x86_64.sh\r\n\r\nRUN git clone https://github.com/tensorflow/tensorflow.git && \\\r\n    cd tensorflow && \\\r\n    git checkout r1.3\r\nWORKDIR /tensorflow\r\n\r\nENV CI_BUILD_PYTHON python\r\nRUN tensorflow/tools/ci_build/builds/configured CPU \\\r\n    bazel build -c opt --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" \\\r\n        tensorflow/tools/pip_package:build_pip_package && \\\r\n    bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip && \\\r\n    pip --no-cache-dir install --upgrade /tmp/pip/tensorflow-*.whl \r\n\r\nENV PATH /google-cloud-sdk/bin:$PATH\r\nRUN curl -O https://dl.google.com/dl/cloudsdk/channels/rapid/google-cloud-sdk.zip && \\\r\n    unzip google-cloud-sdk.zip && rm google-cloud-sdk.zip && \\\r\n    google-cloud-sdk/install.sh --usage-reporting=false --path-update=true --bash-completion=true --rc-path=/.bashrc --additional-components app-engine-python && \\\r\n    google-cloud-sdk/bin/gcloud config set --installation component_manager/disable_update_check true\r\n\r\nEXPOSE 6006\r\nEXPOSE 8888\r\nEXPOSE 5900\r\n\r\nWORKDIR /\r\nCMD [\"/bin/bash\"]\r\n\u00b7", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "@caisq any other advice here?", "@sherkwast This bug is kind of old. Are you still having this issue with the latest tensorflow code?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "No, I've solved it."]}, {"number": 13068, "title": "cuDNN 6 incompatible with Tensorflow 1.3 error", "body": "Any help is greatly appreciated! I've spent way too much time on this. \r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nIn tensorflow/stream_executor/cuda/cuda_gpu_executor.cc in function \"static int TryToReadNumaNode(conststring &pci_bus_id,intdevice_ordinal)\" added the following lines at the beginning of the function:\r\nLOG(INFO) << \"ARM has no NUMA node, hardcoding to return zero\";\r\nreturn 0;\r\nmodified workspace.bzl: set eigen_archive to use http://mirror.bazel.build/bitbucket.org/eigen/eigen/get/d781c1de9834.tar.gz because of an error that kept coming up related to the current version of eigen.\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n Linux Ubuntu 16.04\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n\r\n- **TensorFlow version (use command below)**:\r\n1.3.0\r\ncommand used: git checkout v1.3.0\r\n\r\n- **Python version**: \r\n2.7\r\n\r\n- **Bazel version (if compiling from source)**:\r\n0.4.5\r\n\r\n- **CUDA/cuDNN version**:\r\nCUDA version 8.0, CUDNN version 6.0.21\r\ncudnn was downloaded from the NVIDIA website using the cuDNN v6.0 Library for Linux version\r\n\r\n- **GPU model and memory**:\r\nNVIDIA Pascal GPU on the TX2\r\n\r\n- **Exact command to reproduce**:\r\nsudo bazel build -c opt --local_resources 3072,4.0,1.0 --verbose_failures --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\n- **Other*:\r\nUsing an 8GB swapfile\r\nCUDA install path: usr/local/cuda/\r\nCUDNN install paths:\r\n     include: /usr/include/\r\n     libs: /usr/lib/aarch64-linux-gnu/\r\n     I used the following command on the libs: sudo chmod a+r /usr/lib/aarch64-linux-gnu/libcudnn*\r\n\r\n\r\n### Describe the problem\r\nWhen I try to install tensorflow 1.3.0 I get the error listed below. It goes through pretty much the entire build and fails at the very end. Tensorflow 1.0 installs just fine on the TX2 using cuda 8 and cudnn 5.1(these lib are no longer on my machine so its not an issue with having 5.1 installed). I would use Tensoflow 1.0, but the network I am working with has a reliance on 1.3. \r\n\r\n### Source code / logs\r\nIERROR: /home/nvidia/tensorflow/tensorflow/python/BUILD:2762:1: Linking of rule '//tensorflow/python:_pywrap_tensorflow_internal.so' failed: link_dynamic_library.sh failed: error executing command \r\n  (cd /home/nvidia/.cache/bazel/_bazel_root/d2751a49dacf4cb14a513ec663770624/execroot/tensorflow && \\\r\n  exec env - \\\r\n    CUDA_TOOLKIT_PATH=/usr/local/cuda \\\r\n    CUDNN_INSTALL_PATH=/usr/local/cuda-8.0 \\\r\n    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \\\r\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin \\\r\n    PYTHON_BIN_PATH=/usr/bin/python \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \\\r\n    TF_CUDA_CLANG=0 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=6.2 \\\r\n    TF_CUDA_VERSION=8.0 \\\r\n    TF_CUDNN_VERSION=6.0.21 \\\r\n    TF_NEED_CUDA=1 \\\r\n    TF_NEED_OPENCL=0 \\\r\n  external/bazel_tools/tools/cpp/link_dynamic_library.sh no ignored ignored ignored external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -shared -o bazel-out/local_linux-opt/bin/tensorflow/python/_pywrap_tensorflow_internal.so -Lbazel-out/local_linux-opt/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccusolver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib -Lbazel-out/local_linux-opt/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib -Lbazel-out/local_linux-opt/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccuda_Udriver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib -Lbazel-out/local_linux-opt/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib -Lbazel-out/local_linux-opt/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccufft___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib -Lbazel-out/local_linux-opt/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccurand___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib -Lbazel-out/local_linux-opt/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib '-Wl,-rpath,$ORIGIN/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccusolver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib' '-Wl,-rpath,$ORIGIN/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib' '-Wl,-rpath,$ORIGIN/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccuda_Udriver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib' '-Wl,-rpath,$ORIGIN/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib' '-Wl,-rpath,$ORIGIN/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccufft___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib' '-Wl,-rpath,$ORIGIN/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccurand___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib' '-Wl,-rpath,$ORIGIN/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib' -Wl,--version-script tensorflow/tf_version_script.lds -Wl,-z,muldefs -Wl,-rpath,../local_config_cuda/cuda/lib64 -Wl,-rpath,../local_config_cuda/cuda/extras/CUPTI/lib64 -pthread -Wl,-no-as-needed -B/usr/bin/ -Wl,-z,relro,-z,now -no-canonical-prefixes -pass-exit-codes '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -Wl,--gc-sections -Wl,@bazel-out/local_linux-opt/bin/tensorflow/python/_pywrap_tensorflow_internal.so-2.params): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\n/usr/bin/ld: skipping incompatible bazel-out/local_linux-opt/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/libcudnn.so.6 when searching for -l:libcudnn.so.6\r\n/usr/bin/ld: skipping incompatible bazel-out/local_linux-opt/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/libcudnn.so.6 when searching for -l:libcudnn.so.6\r\n/usr/bin/ld: skipping incompatible /usr/lib/gcc/aarch64-linux-gnu/5/../../../aarch64-linux-gnu/libcudnn.so.6 when searching for -l:libcudnn.so.6\r\n/usr/bin/ld: skipping incompatible /usr/lib/gcc/aarch64-linux-gnu/5/../../../aarch64-linux-gnu/libcudnn.so.6 when searching for -l:libcudnn.so.6\r\n/usr/bin/ld: skipping incompatible /usr/lib/aarch64-linux-gnu/libcudnn.so.6 when searching for -l:libcudnn.so.6\r\n/usr/bin/ld: skipping incompatible /usr/lib/aarch64-linux-gnu/libcudnn.so.6 when searching for -l:libcudnn.so.6\r\n/usr/bin/ld: skipping incompatible //usr/lib/aarch64-linux-gnu/libcudnn.so.6 when searching for -l:libcudnn.so.6\r\n/usr/bin/ld: skipping incompatible //usr/lib/aarch64-linux-gnu/libcudnn.so.6 when searching for -l:libcudnn.so.6\r\n/usr/bin/ld: cannot find -l:libcudnn.so.6\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 5.977s, Critical Path: 4.15s\r\n", "comments": ["@gunan, have you ever seen this? I would basically suggest reinstalling cuda and cudnn in the more standard locations of /usr/local/cuda/..., but it appears it is finding them but you are getting these errors:\r\n```\r\n/usr/bin/ld: skipping incompatible /usr/lib/aarch64-linux-gnu/libcudnn.so.6 when searching for -l:libcudnn.so.6\r\n/usr/bin/ld: skipping incompatible /usr/lib/aarch64-linux-gnu/libcudnn.so.6 when searching for -l:libcudnn.so.6\r\n/usr/bin/ld: skipping incompatible //usr/lib/aarch64-linux-gnu/libcudnn.so.6 when searching for -l:libcudnn.so.6\r\n/usr/bin/ld: skipping incompatible //usr/lib/aarch64-linux-gnu/libcudnn.so.6 when searching for -l:libcudnn.so.6\r\n```", "I think that means the libraries you installed are either incompatible with your system, or the TF libraries incompatible with what you have. I dont have much insight, as we never tested on TX2.\r\n\r\n@petewarden in case he know something about TX* systems.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "I feel like this is due to some bazel/compiler options used. But I cannot be sure.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "could you try adding `--action_env=LD_LIBRARY=${LD_LIBRARY_PATH}` to your bazel command?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "After adding `--action_env=LD_LIBRARY=${LD_LIBRARY_PATH}` to my bazel command, same issue on my TX2...  the version of TF is 1.4\r\n\r\n\r\n```\r\nERROR: /home/nvidia/tensorflow/tensorflow/BUILD:573:1: Linking of rule '//tensorflow:libtensorflow_framework.so' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command\r\n  (cd /home/nvidia/.cache/bazel/_bazel_nvidia/d2751a49dacf4cb14a513ec663770624/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=/home/nvidia/cuda/lib64:/usr/local/cuda-8.0/lib64: \\\r\n    PATH=/usr/local/cuda-8.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\r\n    PWD=/proc/self/cwd \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -shared -o bazel-out/host/bin/tensorflow/libtensorflow_framework.so '-Wl,-rpath,$ORIGIN/../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib' '-Wl,-rpath,$ORIGIN/../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccuda_Udriver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib' '-Wl,-rpath,$ORIGIN/../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib' '-Wl,-rpath,$ORIGIN/../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccufft___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib' '-Wl,-rpath,$ORIGIN/../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccurand___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib' '-Wl,-rpath,$ORIGIN/../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib' -Lbazel-out/host/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib -Lbazel-out/host/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccuda_Udriver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib -Lbazel-out/host/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib -Lbazel-out/host/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccufft___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib -Lbazel-out/host/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccurand___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib -Lbazel-out/host/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib '-Wl,-rpath,$ORIGIN/' -pthread -Wl,-rpath,../local_config_cuda/cuda/lib64 -Wl,-rpath,../local_config_cuda/cuda/extras/CUPTI/lib64 -Wl,-no-as-needed -B/usr/bin/ -Wl,-z,relro,-z,now -no-canonical-prefixes -pass-exit-codes '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -Wl,--gc-sections -Wl,-S -Wl,@bazel-out/host/bin/tensorflow/libtensorflow_framework.so-2.params)\r\n/usr/bin/ld: skipping incompatible bazel-out/host/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/libcudnn.so.6 when searching for -l:libcudnn.so.6\r\n/usr/bin/ld: skipping incompatible bazel-out/host/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/libcudnn.so.6 when searching for -l:libcudnn.so.6\r\n/usr/bin/ld: cannot find -l:libcudnn.so.6\r\ncollect2: error: ld returned 1 exit status\r\n```", "Nagging Assignee @gunan: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @gunan: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @gunan: It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @gunan: It has been 59 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I think we are missing a CPU arg here, we may need to tell bazel that it needs to build for arm CPUs.\r\nCould you try adding flags:\r\n```\r\n--cpu=armeabi\r\n--crosstool_top=@local_config_arm_compiler//:toolchain\r\n```", "Nagging Assignee @gunan: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @gunan: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I encountered a similar issue.\r\nOS: CentOS\r\nTF source: r1.7\r\ngcc: 4.9.3\r\nCUDA: 9.0\r\ncudnn: 7.0.5\r\n\r\n```\r\nError:\r\nINFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded).\r\nINFO: Found 1 target...\r\nERROR: /media/disk1/fordata/web_server/project/xiaolun/workshop/tensorflow.gpu.1.7/tensorflow/BUILD:744:1: Linking of rule '//tensorflow:libtensorflow_framework.so' failed (Exit 1)\r\n/usr/bin/ld: skipping incompatible bazel-out/host/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/libcudnn.so.7 when searching for -l:libcudnn.so.7\r\n/usr/bin/ld: skipping incompatible bazel-out/host/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/libcudnn.so.7 when searching for -l:libcudnn.so.7\r\n/usr/bin/ld: skipping incompatible /usr/local/lib/libcudnn.so.7 when searching for -l:libcudnn.so.7\r\n/usr/bin/ld: skipping incompatible /usr/local/lib/libcudnn.so.7 when searching for -l:libcudnn.so.7\r\n/usr/bin/ld: skipping incompatible /lib/libcudnn.so.7 when searching for -l:libcudnn.so.7\r\n/usr/bin/ld: skipping incompatible /lib/libcudnn.so.7 when searching for -l:libcudnn.so.7\r\n/usr/bin/ld: skipping incompatible /usr/lib/libcudnn.so.7 when searching for -l:libcudnn.so.7\r\n/usr/bin/ld: skipping incompatible /usr/lib/libcudnn.so.7 when searching for -l:libcudnn.so.7\r\n/usr/bin/ld: cannot find -l:libcudnn.so.7\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 0.800s, Critical Path: 0.28s\r\nFAILED: Build did NOT complete successfully\r\n```\r\nSeems the bazel has found the libcudnn.so.7, but it thinks it's incompatible."]}, {"number": 13067, "title": "[Docs] tf.constant docstring: Whitespace fix", "body": "This should allow the ` ```python ` section to properly render on the API page\r\n    https://www.tensorflow.org/api_docs/python/tf/constant\r\nwhich (at the time of writing) displays the fences in plain-text.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Jenkins, test this please.", "Ignoring `fft_ops_test` timeout."]}, {"number": 13066, "title": "r1.3 branch - encountered error attempting build on Pi 2", "body": "OS is Ubuntu Mate 16.04 .. cannot quite decipher the problem yet.  Might try the Pi2 specific build line next.\r\n\r\n### System information\r\n```\r\n== cat /etc/issue ===============================================\r\nLinux mepi-desktop 4.4.38-v7+ #938 SMP Thu Dec 15 15:22:21 GMT 2016 armv7l armv7l armv7l GNU/Linux\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu/Linaro 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux mypi-desktop 4.4.38-v7+ #938 SMP Thu Dec 15 15:22:21 GMT 2016 armv7l armv7l armv7l GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.11.0)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nImportError: No module named tensorflow\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\ntf_env_collect.sh: 105: tf_env_collect.sh: nvidia-smi: not found\r\n\r\n== cuda libs  ===================================================\r\n\r\n```\r\n\r\n### Describe the problem\r\nAttempting to following compilation instructions for RPi, getting error on `make -f ...`\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/r1.3/tensorflow/contrib/makefile\r\n\r\n```\r\ntensorflow/contrib/makefile/download_dependencies.sh\r\nsudo apt-get install -y autoconf automake libtool gcc-4.8 g++-4.8\r\ncd tensorflow/contrib/makefile/downloads/protobuf/\r\n./autogen.sh\r\n./configure\r\nmake\r\nsudo make install\r\nsudo ldconfig  # refresh shared library cache\r\ncd ../../../../..\r\n\r\nmake -f tensorflow/contrib/makefile/Makefile HOST_OS=PI TARGET=PI OPTFLAGS=\"-Os\" CXX=g++-4.8\r\n```\r\n\r\n### Source code / logs\r\n```\r\nremote_fused_graph_execute_op.cc:(.text._ZN10tensorflow25RemoteFusedGraphExecuteOpC2EPNS_20OpKernelConstructionE[_ZN10tensorflow25RemoteFusedGraphExecuteOpC5EPNS_20OpKernelConstructionE]+0x198): undefined reference to `google::protobuf::internal::fixed_address_empty_string'\r\n/home/mypi/Dev/git/tensorflow/tensorflow/tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a(saved_tensor_slice.pb_text.o): In function `tensorflow::internal::ProtoParseFromScanner(tensorflow::strings::Scanner*, bool, bool, tensorflow::SavedSlice*)':\r\nsaved_tensor_slice.pb_text.cc:(.text+0x630): undefined reference to `google::protobuf::internal::fixed_address_empty_string'\r\n/home/mypi/Dev/git/tensorflow/tensorflow/tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a(saved_tensor_slice.pb_text.o): In function `tensorflow::internal::ProtoParseFromScanner(tensorflow::strings::Scanner*, bool, bool, tensorflow::SavedSliceMeta*)':\r\nsaved_tensor_slice.pb_text.cc:(.text+0x136c): undefined reference to `google::protobuf::internal::fixed_address_empty_string'\r\n/home/mypi/Dev/git/tensorflow/tensorflow/tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a(memmapped_file_system.pb_text.o): In function `tensorflow::internal::ProtoParseFromScanner(tensorflow::strings::Scanner*, bool, bool, tensorflow::MemmappedFileSystemDirectoryElement*)':\r\nmemmapped_file_system.pb_text.cc:(.text+0x354): undefined reference to `google::protobuf::internal::fixed_address_empty_string'\r\n/home/mypi/Dev/git/tensorflow/tensorflow/tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a(saver.pb_text.o): In function `tensorflow::internal::ProtoParseFromScanner(tensorflow::strings::Scanner*, bool, bool, tensorflow::SaverDef*)':\r\nsaver.pb_text.cc:(.text+0x698): undefined reference to `google::protobuf::internal::fixed_address_empty_string'\r\n/home/mypi/Dev/git/tensorflow/tensorflow/tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a(cluster.pb_text.o):cluster.pb_text.cc:(.text+0x6e4): more undefined references to `google::protobuf::internal::fixed_address_empty_string' follow\r\ncollect2: error: ld returned 1 exit status\r\ntensorflow/contrib/makefile/Makefile:552: recipe for target '/home/mypi/Dev/git/tensorflow/tensorflow/tensorflow/contrib/makefile/gen/bin/benchmark' failed\r\nmake: *** [/home/mypi/Dev/git/tensorflow/tensorflow/tensorflow/contrib/makefile/gen/bin/benchmark] Error 1\r\n```", "comments": ["To build tensorflow on Raspberry Pi, there's a guide for it by @samjabrahams \r\nhttps://github.com/samjabrahams/tensorflow-on-raspberry-pi/blob/master/GUIDE.md\r\n\r\nThere are links to compiled whls for tensorflow on his guide for python 2.7 and 3.4. I have also uploaded one for python 3.5 [here](https://github.com/akashjobanputra/tensorflow-on-raspberry-pi/releases/tag/v1.2.1)", "Will give these a run through, thank you.  Would a PR with the steps outlined in here be helpful?  Either in master or `master` & `r1.3` ?", "@EMCP, are you suggesting a PR for @samjabrahams' instructions or in TensorFlow somewhere?", "was waiting to get things working on my end, but yes.. \r\n\r\nrunning the instructions that are there now, on a RPi2, seemed to not quite get me there.. ", "The fact that you need to use a USB swap drive makes me think it may not make sense to add those instructions here..  but would be helpful to somehow highlight the way in which some people have shoehorned bazel onto the RPi...\r\n\r\nI am asking some questions on those instructions before I would want to write up the steps from there..\r\n\r\nMy gut is, the script we are leveraging in the official instructions of TF just needs to be updated / verified to work ", "Assigning @petewarden. You might find his page useful as well \r\nhttps://petewarden.com/2017/08/20/cross-compiling-tensorflow-for-the-raspberry-pi/\r\n", "This looks like it can work for me.... only problem is \r\n\r\n1) This compiles for python 2.7, I think I can get by but ideally would want to get a python 3.5 version\r\n\r\nto fix that I think I would need some time to modify this file in `master` branch to give it `/usr/bin/python3` instead of the default 2.x\r\n\r\n```\r\ntensorflow/tensorflow/tools/ci_build/pi/build_raspberry_pi.sh\r\n\r\n...\r\n# Make sure you have an up to date version of the Bazel build tool installed too.\r\n\r\nyes '' | ./configure # I'll need to change this I'd imagine\r\n...\r\n```\r\n\r\n2) the scripts cited in that blog post are not in the `r1.3` branch.. so I am guessing they're not really released yet.  adding these instructions then would not make sense for 1.3 branch", "In theory, you should just be able to set the following environment variables before building to get Python 3:\r\n```\r\nexport TF_BUILD_CONTAINER_TYPE=CPU\r\nexport TF_BUILD_PYTHON_VERSION=PYTHON3\r\nexport TF_BUILD_IS_OPT=OPT\r\nexport TF_BUILD_IS_PIP=PIP\r\nexport USER=\"${USER:-jenkins}\"\r\ntensorflow/tools/ci_build/ci_build.sh PI tensorflow/tools/ci_build/pi/build_raspberry_pi.sh\r\n```\r\nI haven't tried this myself though, so it may well be trickier than that.\r\n\r\nThe scripts will be part of the 1.4 release, along with some official 1.4 Pi binaries, but they're also available in the nightlies.\r\n", "<3 official 1.4 Pi Binaries , but I will give the settings a run and report back here.\r\n\r\nThank you\r\n\r\nAs for my original issue, I think once the binaries are out it will be less needed to fiddle with the documentation.. but I do wonder if that makefile needs an update in both `r1.3` and `master`"]}, {"number": 13065, "title": "maxpooling error while building tf_label_image_example", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n   No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n   Windows 10 (64 bit)\r\n- **TensorFlow installed from (source or binary)**:\r\n   Using CMake and instructions present at (tensorflow/tensorflow/contrib/cmake)\r\n- **Python version**: \r\n   3.6.1\r\n- **CUDA/cuDNN version**:\r\n   CUDA-8.0.61\r\n   cuDNN-5.1\r\n- **GPU model and memory**:\r\n   NVIDIA GeForce GTX TITAN X (382.05)\r\n- **Exact command to reproduce**:\r\n   MSBuild /p:Configuration=Release tf_label_image_example.vcxproj\r\n\r\n### Describe the problem\r\ni am facing this error while trying to build the project tf_label_image_example. I was able to build this project without GPU support, but, while building with GPU support, i am facing the following error.\r\n\r\n\"C:\\Users\\alalwani\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_label_image_example.vcxproj\" (default target) (1) ->\r\n(Link target) ->\r\nmaxpooling_op.obj : error LNK2001: unresolved external symbol \"public: bool __cdecl tensorflow::functor::MaxPoolForwardWithOptionalArgmax::operator()(struct Eigen::QInt8 const *,int,int,int,int,int,int,int,int,int\r\n,int,int,int,struct Eigen::QInt8 *,__int64 *,struct Eigen::GpuDevice const &)\" (??R?$MaxPoolForwardWithOptionalArgmax@UQInt8@Eigen@@@functor@tensorflow@@QEAA_NPEBUQInt8@Eigen@@hhhhhhhhhhhhpeau34@PEA_JAEBUGpuDevice@4@@z) [C:\\Users\\alalwa\r\nni\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_label_image_example.vcxproj]\r\nC:\\Users\\alalwani\\tensorflow\\tensorflow\\contrib\\cmake\\build\\Release\\tf_label_image_example.exe : fatal error LNK1120: 1 unresolved externals [C:\\Users\\alalwani\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_label_image_example.vcxproj]", "comments": ["@mrry can you take a look?", "The problem seems to come from c14550a38308a7f516e83f5c8e21748ad76bf972, which added a `DT_QINT8` kernel registration for the `\"MaxPool\"` op, but there isn't a corresponding functor implementation for that type.\r\n\r\nAt a guess, maybe the problem stems from [this `if (std::is_same<T, qint8>::value)` check](https://github.com/tensorflow/tensorflow/commit/c14550a38308a7f516e83f5c8e21748ad76bf972#diff-4b4334a33322a74e2afdb75d174a1d6fR405). Maybe MSVC isn't as aggressive at dead code elimination as GCC/Clang, and it's trying to instantiate the (unimplemented) template in the `else` branch, or elsewhere in that class?", "@mrry Will the problem be solved if i try and use older versions of tensorflow,i.e. the version before the commit you mentioned?", "@lalwaniabhi I tried r1.2 and it too failed, with error `MSBuild: 'error MSB6006: \u201ccmd.exe\u201d exited with code 1.'`\r\n", "Oh.\r\nand did you try working with the known good configurations given [here]?(https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/cmake).\r\nWill the problem be solved if we use the exact same versions of all the software mentioned in the 'Known good configurations'?\r\n@mrry @akashjobanputra ", "@lalwaniabhi No, not the exact versions of swig and cmake.\r\nThis is my configuration:\r\n**Microsoft Visual Studio Enterprise 2015 with Visual C++ 2015**\r\n**Python 3.5.4** (conda env)\r\n**swigwin-3.0.12**\r\n**NVidia CUDA Toolkit 8.0\r\nNVidia CUDNN 5.1 (Patch level 10)**\r\n**cmake 3.8.2**", "I have same error with MaxPoolForwardWithOptionalArgmax", "I installed tensorflow-gpu from `conda` for python3.5. It works, though not with AVX or AVX2 flags but works.", "@akashjobanputra  Could you share your CMAKE comand .bat file (assume your env is same as above and GPU version)?\r\nAnd have you succeeded \"MSBuild /p:Configuration=Release ALL_BUILD.vcxproj \" (GPU version)? ", "@spk921 No success with `MSBuild /p:Configuration=Release ALL_BUILD.vcxproj`. I'm facing same error on `MaxPoolForwardWithOptionalArgmax`\r\nSo I gave up building from source, and installed through `conda`", "@akashjobanputra But installing through conda does't give you the functionality of tensorflow in C++ and visual studio, right? Or is there a way of doing so?", "@lalwaniabhi AFAIK, it won't give functionality to C++.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly."]}, {"number": 13064, "title": "Variance update in tf.contrib.layer.batch_norm", "body": "Hi,\r\n\r\nIn the current implementation of batch norm, the variance to be used for update to moving variance is computed by centering the data around the current mean. The more proper way would be to compute the variance around the moving mean. With the current method, the moving variance will depend upon the batch size. With the updated method, the batch size will have less influence on the moving variance.\r\n\r\nThanks,\r\nMayank", "comments": ["@sguada this seems like a reasonable observation. Should we ask @mkabra for a PR?", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Sure let see a PR and evaluate it.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Contrib folder is deprecated and moved to tf addons , will be closing this request. Thank you"]}, {"number": 13063, "title": "XLA representation of batch normalization has changed since TF 1.3", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source (commit 1f19b8c)\r\n- **TensorFlow version (use command below)**: ('v1.3.0-rc1-2173-g36813d5', '1.4.0-dev')\r\n- **Python version**: Python 2.7.12\r\n- **Bazel version (if compiling from source)**: Build label: 0.5.3\r\n- **CUDA/cuDNN version**: no\r\n- **GPU model and memory**: no\r\n- **Exact command to reproduce**: cannot provide\r\n\r\n### Describe the problem\r\nI use Tensorflow compiled from sources (last commit 1f19b8c) with XLA JIT compilation enabled. My **inference** network uses batch normalization. When I visualize the HLO cluster dumped into the dot file with graphviz I see that batch normalization operation is translated differently in TF 1.3 and TF master. In the master branch the result is like this:\r\n![tf_1 3](https://user-images.githubusercontent.com/30651952/30482986-d53c9480-9a2d-11e7-8e8e-b5a48d5e95a2.png)\r\nBut in the 1.3 branch all these instructions were merged into a single broadcast with a constant parameter. Looks like the translation behavior has been changed by commit 7359fec. This introduces a kind of regression since these instructions can be pre-calculated because the input is constant (as it was done in 1.3).\r\n\r\n### Source code / logs\r\nIn order to reproduce the issue run the Inception V3 network from slim models zoo with variables converted to constants and marked with XLA compile flag.\r\n", "comments": ["/CC @yunxing", "Sorry just noticed this issue when doing a clean up of XLA issues. \r\n\r\nAre you saying 1.3 is the preferred behavior while master branch is a regression? I'm actually not sure why that worked in 1.3 at all : There was an assert that prevents fused_batchnorm inference from being used and 7359fec fixed that (https://github.com/tensorflow/tensorflow/commit/7359fec792e4efec1670a12332bb524a5608b215#diff-0edeb6b6a649db8138c4dd53d8ba0ed0L42).\r\n\r\nPlus, from the graph you sent out, constant folding should be able to precompute all the nodes, so I don't think there will be a perf regression. \r\n\r\n(Feel free to reopen the issue if you have more questions)"]}, {"number": 13062, "title": "inline assembly requires more registers than available while cross compiling tensroflow for arm cortex a15 using clang-3.8", "body": "I am trying to cross compile tensorflow for armv7a cortex a15 using clang-3.8 and I am using bazel 0.5.4 to build tensorflow.\r\n\r\nMy Environment details are, Ubuntu - 16.04 Clang - 3.8 bazel - 0.5.4 tensorflow source code version - 1.3 target cpu - cortex a15 compiler_flag: \"-O2\"\r\n\r\nThe command i am using is \r\nbazel build --copt=-Wno-c++11-narrowing --crosstool_top=//tools/arm_compiler:toolchain --cpu=armeabi-v7a --verbose_failures --sandbox_debug //tensorflow/tools/pip_package:build_pip_package\r\n\r\n\r\nThe error is external/gemmlowp/meta/transform_kernels_arm_32.h:5506:7: error: inline assembly requires more registers than available \"ldr r0, %[input_range_min]\\n\"\r\n\r\nThe compiler flags are configured as bellow in CROSSTOOL\r\n\r\n toolchain {\r\n      abi_version: \"clang_3.8\"\r\n      abi_libc_version: \"glibc_2.19\"\r\n      builtin_sysroot: \"\"\r\n      compiler: \"clang\"\r\n      host_system_name: \"armeabi-v7a\"\r\n      needsPic: true\r\n      supports_gold_linker: false\r\n      supports_incremental_linker: false\r\n      supports_fission: false\r\n      supports_interface_shared_objects: false\r\n      supports_normalizing_ar: true\r\n      supports_start_end_lib: false\r\n      supports_thin_archives: true\r\n      target_libc: \"glibc_2.19\"\r\n      target_cpu: \"armeabi-v7a\"\r\n      target_system_name: \"arm_a15\"\r\n      toolchain_identifier: \"clang_linux_armhf\"\r\n\r\n      tool_path { name: \"ar\" path: \"linaro_linux_gcc/arm-linux-gnueabihf-ar\" }\r\n      tool_path { name: \"compat-ld\" path: \"linaro_linux_gcc/arm-linux-gnueabihf-ld\" }\r\n      tool_path { name: \"cpp\" path: \"linaro_linux_gcc/clang_bin/clang\" }\r\n      tool_path { name: \"dwp\" path: \"linaro_linux_gcc/arm-linux-gnueabihf-dwp\" }\r\n      tool_path { name: \"gcc\" path: \"linaro_linux_gcc/clang_bin/clang\" }\r\n      tool_path { name: \"gcov\" path: \"arm-frc-linux-gnueabi/arm-frc-linux-gnueabi-gcov-4.9\" }\r\n      # C(++) compiles invoke the compiler (as that is the one knowing where\r\n      # to find libraries), but we provide LD so other rules can invoke the linker.\r\n      tool_path { name: \"ld\" path: \"linaro_linux_gcc/arm-linux-gnueabihf-ld\" }\r\n      tool_path { name: \"nm\" path: \"linaro_linux_gcc/arm-linux-gnueabihf-nm\" }\r\n      tool_path { name: \"objcopy\" path: \"linaro_linux_gcc/arm-linux-gnueabihf-objcopy\" }\r\n      objcopy_embed_flag: \"-I\"\r\n      objcopy_embed_flag: \"binary\"\r\n      tool_path { name: \"objdump\" path: \"linaro_linux_gcc/arm-linux-gnueabihf-objdump\" }\r\n      tool_path { name: \"strip\" path: \"linaro_linux_gcc/arm-linux-gnueabihf-strip\" }\r\n\r\n      compiler_flag: \"-target\"\r\n      compiler_flag: \"armv7a-arm-linux-gnueabihf\"\r\n      compiler_flag: \"--sysroot=external/linaro_linux_gcc_repo/arm-linux-gnueabihf/libc\"\r\n      compiler_flag: \"-mfloat-abi=hard\"\r\n      compiler_flag: \"-mcpu=cortex-a15\"\r\n      compiler_flag: \"-mfpu=neon-vfpv4\"\r\n\r\n      compiler_flag: \"-nostdinc\"\r\n      compiler_flag: \"-isystem\"\r\n      compiler_flag: \"/usr/lib/clang/3.8/include\"\r\n      compiler_flag: \"-isystem\"\r\n      compiler_flag: \"external/linaro_linux_gcc_repo/lib/gcc/arm-linux-gnueabihf/4.9.3/include\"\r\n      compiler_flag: \"-isystem\"\r\n      compiler_flag: \"external/linaro_linux_gcc_repo/arm-linux-gnueabihf/libc/usr/include\"\r\n      compiler_flag: \"-isystem\"\r\n      compiler_flag: \"external/linaro_linux_gcc_repo/lib/gcc/arm-linux-gnueabihf/4.9.3/include-fixed\"\r\n      compiler_flag: \"-isystem\"\r\n      compiler_flag: \"external/linaro_linux_gcc_repo/arm-linux-gnueabihf/libc/usr/include\"\r\n      cxx_flag: \"-isystem\"\r\n      cxx_flag: \"external/linaro_linux_gcc_repo/arm-linux-gnueabihf/include/c++/4.9.3/arm-linux-gnueabihf\"\r\n      cxx_flag: \"-isystem\"\r\n      cxx_flag: \"external/linaro_linux_gcc_repo/arm-linux-gnueabihf/include/c++/4.9.3\"\r\n      cxx_flag: \"-isystem\"\r\n      cxx_flag: \"external/linaro_linux_gcc_repo/include/c++/4.9.3/arm-linux-gnueabihf\"\r\n      cxx_flag: \"-isystem\"\r\n      cxx_flag: \"external/linaro_linux_gcc_repo/include/c++/4.9.3\"\r\n\r\n      cxx_builtin_include_directory: \"%package(@linaro_linux_gcc_repo//include)%\"\r\n      cxx_builtin_include_directory: \"%package(@linaro_linux_gcc_repo//arm-linux-gnueabihf/libc/usr/include)%\"\r\n      cxx_builtin_include_directory: \"%package(@linaro_linux_gcc_repo//arm-linux-gnueabihf/libc/usr/lib/include)%\"\r\n      cxx_builtin_include_directory: \"%package(@linaro_linux_gcc_repo//arm-linux-gnueabihf/libc/lib/gcc/arm-linux-gnueabihf/4.9.3/include-fixed)%\"\r\n      cxx_builtin_include_directory: \"%package(@linaro_linux_gcc_repo//include)%/c++/4.9.3\"\r\n      cxx_builtin_include_directory: \"%package(@linaro_linux_gcc_repo//arm-linux-gnueabihf/libc/lib/gcc/arm-linux-gnueabihf/4.9.3/include)%\"\r\n      cxx_builtin_include_directory: \"%package(@linaro_linux_gcc_repo//arm-linux-gnueabihf/libc/lib/gcc/arm-linux-gnueabihf/4.9.3/include-fixed)%\"\r\n      cxx_builtin_include_directory: \"%package(@linaro_linux_gcc_repo//lib/gcc/arm-linux-gnueabihf/4.9.3/include)%\"\r\n      cxx_builtin_include_directory: \"%package(@linaro_linux_gcc_repo//lib/gcc/arm-linux-gnueabihf/4.9.3/include-fixed)%\"\r\n      cxx_builtin_include_directory: \"%package(@linaro_linux_gcc_repo//arm-linux-gnueabihf/include)%/c++/4.9.3\"\r\n      cxx_builtin_include_directory: \"/usr/lib/clang/3.8/include\"\r\n      cxx_builtin_include_directory: \"/usr/lib/llvm-3.8/lib/clang/3.8.0/include\"\r\n      cxx_builtin_include_directory: \"/home/gopinathr/Documents/work/tf1.2/tensorflow/bazel-out/clang_linux_armhf-opt/genfiles/external/local_config_python/python_include/arm-linux-gnueabihf/python2.7\"\r\n\r\n      cxx_flag: \"-std=c++0x\"\r\n      cxx_flag: \"-std=c++11\"\r\n      linker_flag: \"-target\"\r\n      linker_flag: \"armv7a-arm-linux-gnueabihf\"\r\n      linker_flag: \"--sysroot=external/linaro_linux_gcc_repo/arm-linux-gnueabihf/libc\"\r\n      linker_flag: \"-lstdc++\"\r\n      linker_flag: \"-Ltools/arm_compiler/linaro_linux_gcc/clang_more_libs\"\r\n      linker_flag: \"-Lexternal/linaro_linux_gcc_repo/arm-linux-gnueabihf/lib\"\r\n      linker_flag: \"-Lexternal/linaro_linux_gcc_repo/arm-linux-gnueabihf/libc/lib\"\r\n      linker_flag: \"-Lexternal/linaro_linux_gcc_repo/arm-linux-gnueabihf/libc/usr/lib\"\r\n      linker_flag: \"-Bexternal/linaro_linux_gcc_repo/arm-linux-gnueabihf/bin\"\r\n      linker_flag: \"-Wl,--dynamic-linker=/lib/ld-linux-armhf.so.3\"\r\n\r\n      # Anticipated future default.\r\n      # This makes GCC and Clang do what we want when called through symlinks.\r\n      unfiltered_cxx_flag: \"-no-canonical-prefixes\"\r\n      linker_flag: \"-no-canonical-prefixes\"\r\n\r\n      # Make C++ compilation deterministic. Use linkstamping instead of these\r\n      # compiler symbols.\r\n      unfiltered_cxx_flag: \"-Wno-builtin-macro-redefined\"\r\n      unfiltered_cxx_flag: \"-D__DATE__=\\\"redacted\\\"\"\r\n      unfiltered_cxx_flag: \"-D__TIMESTAMP__=\\\"redacted\\\"\"\r\n      unfiltered_cxx_flag: \"-D__TIME__=\\\"redacted\\\"\"\r\n\r\n      # Security hardening on by default.\r\n      # Conservative choice; -D_FORTIFY_SOURCE=2 may be unsafe in some cases.\r\n      # We need to undef it before redefining it as some distributions now have\r\n      # it enabled by default.\r\n      compiler_flag: \"-U_FORTIFY_SOURCE\"\r\n      compiler_flag: \"-fstack-protector\"\r\n      compiler_flag: \"-fPIE\"\r\n      linker_flag: \"-pie\"\r\n      linker_flag: \"-Wl,-z,relro,-z,now\"\r\n\r\n      # Enable coloring even if there's no attached terminal. Bazel removes the\r\n      # escape sequences if --nocolor is specified.\r\n      compiler_flag: \"-fdiagnostics-color=always\"\r\n\r\n        # All warnings are enabled. Maybe enable -Werror as well?\r\n      compiler_flag: \"-Wall\"\r\n      # Enable a few more warnings that aren't part of -Wall.\r\n      compiler_flag: \"-Wunused-but-set-parameter\"\r\n      # But disable some that are problematic.\r\n      compiler_flag: \"-Wno-free-nonheap-object\" # has false positives\r\n\r\n      # Keep stack frames for debugging, even in opt mode.\r\n      compiler_flag: \"-fno-omit-frame-pointer\"\r\n\r\n      # Stamp the binary with a unique identifier.\r\n      linker_flag: \"-Wl,--build-id=md5\"\r\n      linker_flag: \"-Wl,--hash-style=gnu\"\r\n\r\n      compilation_mode_flags {\r\n        mode: DBG\r\n        # Enable debug symbols.\r\n        compiler_flag: \"-g\"\r\n      }\r\n      compilation_mode_flags {\r\n        mode: OPT\r\n\r\n        # No debug symbols.\r\n        # Maybe we should enable https://gcc.gnu.org/wiki/DebugFission for opt or\r\n        # even generally? However, that can't happen here, as it requires special\r\n        # handling in Bazel.\r\n        compiler_flag: \"-g0\"\r\n\r\n        # Conservative choice for -O\r\n        # -O3 can increase binary size and even slow down the resulting binaries.\r\n        # Profile first and / or use FDO if you need better performance than this.\r\n        compiler_flag: \"-O2\"\r\n\r\n        # Disable assertions\r\n        compiler_flag: \"-DNDEBUG\"\r\n\r\n        # Removal of unused code and data at link time (can this increase binary size in some cases?).\r\n        compiler_flag: \"-ffunction-sections\"\r\n        compiler_flag: \"-fdata-sections\"\r\n\r\n        #added by sachin\r\n        compiler_flag: \"-v\"\r\n\r\n        linker_flag: \"-Wl,--gc-sections\"\r\n      }\r\n    }\r\n\r\nThe file contents are external/gemmlowp/meta/transform_kernels_arm_32.h\r\n\r\ntemplate <>\r\ninline void Transform1DKernel<uint8_t, int32_t, BiasAdd<uint8_t>, 16,\r\n                              0>::Transform(const uint8_t* input,\r\n                                            const BiasAdd<uint8_t>& params,\r\n                                            int32_t* output) {\r\n#ifdef DEBUG\r\n#ifdef DEBUG_METAGEMM_VERBOSE\r\n  std::cout << __FILE__ << \"(\" << __LINE__\r\n            << \") BiasAdd<uint8_t><uint8_t, int32_t, BiasAdd<uint8_t>, 16, \"\r\n               \"0>::Transform()\"\r\n            << std::endl\r\n            << std::flush;\r\n#endif\r\n#endif\r\n  int params_rows_copy = params.rows;\r\n  asm volatile(\r\n      \"ldr r0, %[input_range_min]\\n\"\r\n      \"vdup.32 q8, r0\\n\"\r\n      \"ldr r0, %[input_range_scale]\\n\"\r\n      \"vdup.32 q9, r0\\n\"\r\n      \"ldr r0, %[bias_range_min]\\n\"\r\n      \"vdup.32 q10, r0\\n\"\r\n      \"ldr r0, %[bias_range_scale]\\n\"\r\n      \"vdup.32 q11, r0\\n\"\r\n      \"ldr r0, %[output_range_min]\\n\"\r\n      \"vdup.32 q12, r0\\n\"\r\n      \"ldr r0, %[one_over_output_range_scale]\\n\"\r\n      \"vdup.32 q13, r0\\n\"\r\n      \"ldr r0, %[output_range_offset]\\n\"\r\n      \"vdup.32 q14, r0\\n\"\r\n      \"1:\"\r\n      \"mov r0, %[count]\\n\"\r\n      \"mov r1, %[bias]\\n\"\r\n      \"2:\"\r\n      \"subs r0, r0, #16\\n\"\r\n\r\n      // BiasAdd::Transform\r\n      \"vld1.32 {d0, d1}, [%[input]]!\\n\"\r\n      \"vld1.32 {d8, d9}, [r1]!\\n\"\r\n      \"pld [%[input], #32]\\n\"\r\n      \"vmovl.u8 q1, d1\\n\"\r\n      \"vmovl.u8 q0, d0\\n\"\r\n      \"vmovl.u8 q5, d9\\n\"\r\n      \"vmovl.u8 q4, d8\\n\"\r\n      \"vmovl.s16 q3, d3\\n\"\r\n      \"vmovl.s16 q2, d2\\n\"\r\n      \"vmovl.s16 q7, d11\\n\"\r\n      \"vmovl.s16 q6, d10\\n\"\r\n      \"vmovl.s16 q1, d1\\n\"\r\n      \"vmovl.s16 q0, d0\\n\"\r\n      \"vmovl.s16 q5, d9\\n\"\r\n      \"vmovl.s16 q4, d8\\n\"\r\n      \"vcvt.f32.s32 q0, q0\\n\"\r\n      \"vcvt.f32.s32 q1, q1\\n\"\r\n      \"vcvt.f32.s32 q2, q2\\n\"\r\n      \"vcvt.f32.s32 q3, q3\\n\"\r\n      \"vcvt.f32.s32 q4, q4\\n\"\r\n      \"vcvt.f32.s32 q5, q5\\n\"\r\n      \"vcvt.f32.s32 q6, q6\\n\"\r\n      \"vcvt.f32.s32 q7, q7\\n\"\r\n      \"vmul.f32 q0, q0, q9\\n\"\r\n      \"vmul.f32 q1, q1, q9\\n\"\r\n      \"vmul.f32 q2, q2, q9\\n\"\r\n      \"vmul.f32 q3, q3, q9\\n\"\r\n      \"vmul.f32 q4, q4, q11\\n\"\r\n      \"vmul.f32 q5, q5, q11\\n\"\r\n      \"vmul.f32 q6, q6, q11\\n\"\r\n      \"vmul.f32 q7, q7, q11\\n\"\r\n      \"vadd.f32 q0, q0, q8\\n\"\r\n      \"vadd.f32 q1, q1, q8\\n\"\r\n      \"vadd.f32 q2, q2, q8\\n\"\r\n      \"vadd.f32 q3, q3, q8\\n\"\r\n      \"vadd.f32 q4, q4, q10\\n\"\r\n      \"vadd.f32 q5, q5, q10\\n\"\r\n      \"vadd.f32 q6, q6, q10\\n\"\r\n      \"vadd.f32 q7, q7, q10\\n\"\r\n      \"vadd.f32 q0, q0, q4\\n\"\r\n      \"vadd.f32 q1, q1, q5\\n\"\r\n      \"vadd.f32 q2, q2, q6\\n\"\r\n      \"vadd.f32 q3, q3, q7\\n\"\r\n      \"vsub.f32 q0, q0, q12\\n\"\r\n      \"vsub.f32 q1, q1, q12\\n\"\r\n      \"vsub.f32 q2, q2, q12\\n\"\r\n      \"vsub.f32 q3, q3, q12\\n\"\r\n      \"vmul.f32 q0, q0, q13\\n\"\r\n      \"vmul.f32 q1, q1, q13\\n\"\r\n      \"vmul.f32 q2, q2, q13\\n\"\r\n      \"vmul.f32 q3, q3, q13\\n\"\r\n      \"vadd.f32 q0, q0, q14\\n\"\r\n      \"vadd.f32 q1, q1, q14\\n\"\r\n      \"vadd.f32 q2, q2, q14\\n\"\r\n      \"vadd.f32 q3, q3, q14\\n\"\r\n      \"vcvt.s32.f32 q0, q0\\n\"\r\n      \"vcvt.s32.f32 q1, q1\\n\"\r\n      \"vcvt.s32.f32 q2, q2\\n\"\r\n      \"vcvt.s32.f32 q3, q3\\n\"\r\n\r\n      \"vst1.32 {d0, d1, d2, d3}, [%[output]]!\\n\"\r\n      \"vst1.32 {d4, d5, d6, d7}, [%[output]]!\\n\"\r\n      \"pld [%[output]]\\n\"\r\n      \"bne 2b\\n\"\r\n      \"subs %[rows], %[rows], #1\\n\"\r\n      \"bne 1b\\n\"\r\n      : [input] \"+r\"(input), [output] \"+r\"(output)\r\n      : [count] \"r\"(params.count), [rows] \"r\"(params_rows_copy),\r\n        [output_range_offset] \"m\"(params.output_range_offset),\r\n        [input_range_scale] \"m\"(params.input_range_scale),\r\n        [one_over_output_range_scale] \"m\"(params.one_over_output_range_scale),\r\n        [bias_range_min] \"m\"(params.bias_range_min),\r\n        [output_range_min] \"m\"(params.output_range_min),\r\n        [bias_range_scale] \"m\"(params.bias_range_scale),\r\n        [bias] \"r\"(params.bias), [input_range_min] \"m\"(params.input_range_min)\r\n      : \"r0\", \"r1\", \"d0\", \"d1\", \"d2\", \"d3\", \"d4\", \"d5\", \"d6\", \"d7\", \"d8\", \"d9\",\r\n        \"d10\", \"d11\", \"d12\", \"d13\", \"d14\", \"d15\", \"d16\", \"d17\", \"d18\", \"d19\",\r\n        \"d20\", \"d21\", \"d22\", \"d23\", \"d24\", \"d25\", \"d26\", \"d27\", \"d28\", \"d29\",\r\n        \"cc\", \"memory\");\r\n}\r\n\r\nDoes anyone can help me out with what is wrong??\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["I'd recommend filing bug with the gemmlowp, since this is not a TensorFlow specific bug. @maciekcc, do you have any other ideas?\r\n"]}, {"number": 13060, "title": "Pyinstaller with Tensorflow takes incorrect path for _checkpoint_ops.so file", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04, Pyinstaller\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.3.0\r\n- **Python version**: 2.7.6\r\n- **CUDA/cuDNN version**: CUDA 8.0 cuDNN 6.0\r\n- **GPU model and memory**: 3x nVidia GEForce 1080\r\n\r\n### Describe the problem\r\nAs Tensorflow's `load_op_library` finds paths according to the OS it is being run on, I believe this is a problem with Tensorflow in Pyinstaller environment. \r\n\r\nI am trying to make an executable of my Python code which uses Tensorflow . The executable gets generated correctly but when I try to run it, I get the following error:\r\n```\r\n\r\nTraceback (most recent call last):\r\n  File \"detection_init.py\", line 14, in <module>\r\n    import lib.tensorboxDetector as tensorboxDetector\r\n  File \"/usr/local/lib/python2.7/dist-packages/PyInstaller/loader/pyimod03_importers.py\", line 396, in load_module\r\n    exec(bytecode, module.__dict__)\r\n  File \"lib/tensorboxDetector.py\", line 26, in <module>\r\n    from lib.train import build_forward\r\n  File \"/usr/local/lib/python2.7/dist-packages/PyInstaller/loader/pyimod03_importers.py\", line 396, in load_module\r\n    exec(bytecode, module.__dict__)\r\n  File \"lib/train.py\", line 4, in <module>\r\n    import tensorflow.contrib.slim as slim\r\n  File \"/usr/local/lib/python2.7/dist-packages/PyInstaller/loader/pyimod03_importers.py\", line 396, in load_module\r\n    exec(bytecode, module.__dict__)\r\n  File \"tensorflow/contrib/__init__.py\", line 22, in <module>\r\n  File \"/usr/local/lib/python2.7/dist-packages/PyInstaller/loader/pyimod03_importers.py\", line 396, in load_module\r\n    exec(bytecode, module.__dict__)\r\n  File \"tensorflow/contrib/bayesflow/__init__.py\", line 24, in <module>\r\n  File \"/usr/local/lib/python2.7/dist-packages/PyInstaller/loader/pyimod03_importers.py\", line 396, in load_module\r\n    exec(bytecode, module.__dict__)\r\n  File \"tensorflow/contrib/bayesflow/python/ops/csiszar_divergence.py\", line 26, in <module>\r\n  File \"/usr/local/lib/python2.7/dist-packages/PyInstaller/loader/pyimod03_importers.py\", line 396, in load_module\r\n    exec(bytecode, module.__dict__)\r\n  File \"tensorflow/contrib/bayesflow/python/ops/csiszar_divergence_impl.py\", line 42, in <module>\r\n  File \"/usr/local/lib/python2.7/dist-packages/PyInstaller/loader/pyimod03_importers.py\", line 396, in load_module\r\n    exec(bytecode, module.__dict__)\r\n  File \"tensorflow/contrib/framework/__init__.py\", line 89, in <module>\r\n  File \"/usr/local/lib/python2.7/dist-packages/PyInstaller/loader/pyimod03_importers.py\", line 396, in load_module\r\n    exec(bytecode, module.__dict__)\r\n  File \"tensorflow/contrib/framework/python/ops/__init__.py\", line 24, in <module>\r\n  File \"/usr/local/lib/python2.7/dist-packages/PyInstaller/loader/pyimod03_importers.py\", line 396, in load_module\r\n    exec(bytecode, module.__dict__)\r\n  File \"tensorflow/contrib/framework/python/ops/checkpoint_ops.py\", line 32, in <module>\r\n  File \"tensorflow/contrib/util/loader.py\", line 55, in load_op_library\r\n  File \"tensorflow/python/framework/load_library.py\", line 64, in load_op_library\r\ntensorflow.python.framework.errors_impl.NotFoundError: tensorflow/contrib/util/tensorflow/contrib/framework/python/ops/_checkpoint_ops.so: cannot open shared object file: No such file or directory\r\n[11241] Failed to execute script detection_init\r\n```\r\nIf we look carefully, Pyinstaller is expecting the file `_checkpoint_ops.so` in directory `tensorflow/contrib/util/tensorflow/contrib/framework/python/ops/` but there's no directory like this. `_checkpoint_ops.so` is located at `tensorflow/contrib/framework/python/ops/`. How can this be fixed?", "comments": ["@sherrym do you have any suggestions, please?", "i am seeing a similar issue when and only when doing:\r\npip install tensorflow -t project_folder\r\nand then importing from the project_folder", "I think I am having a related issue. Has anyone been able to package tensorflow with pyinstaller on OSX?\r\n\r\nI opened a SO question.\r\n\r\nhttps://stackoverflow.com/questions/46369902/how-to-package-tensorflow-with-pyinstaller-on-macosx-what-is-tensorflow-interna\r\n\r\nSystem information\r\n\r\n![image](https://user-images.githubusercontent.com/1208492/30927530-982f1baa-a36d-11e7-874c-1f9c489ae203.png)\r\n\r\nTensorflow grabbing local libraries. Identical code on Windows 10 works.", "https://github.com/tensorflow/tensorflow/issues/13126", "@eyaler are you suggesting that packaging tensorflow may be effected by the relative paths? I can open a separate issue if my example wasn't a good fit here, but I think the pyinstaller challenge is larger than a PATH issue. Still trying to debug today, I will report back if adding to PYTHONPATH makes a difference in distributing as .exe", "Again, I can open a new issue, but my gut feeling is that this is the same underlying issue in tensorflow for distributing as an app as reported by other user above.\r\n\r\nhttps://github.com/pyinstaller/pyinstaller/issues/2883\r\n\r\nHere is our program. Let's just load tensorflow and have it print out some path info.\r\n\r\n```\r\ntf_check.py\r\n\r\n    import tensorflow\r\n    print(tensorflow.__file__)\r\n```\r\n\r\nRun with pyinstaller.\r\n\r\n```\r\n    Bens-MacBook-Pro:tests ben$ pyinstaller tf_check.py \r\n    66 INFO: PyInstaller: 3.3\r\n    66 INFO: Python: 2.7.10\r\n    78 INFO: Platform: Darwin-16.7.0-x86_64-i386-64bit\r\n    79 INFO: wrote /Users/ben/Documents/DeepMeerkat/tests/tf_check.spec\r\n    87 INFO: UPX is not available.\r\n```\r\n\r\nBuilds without a problem. Trying to run the tf_check under dist/tf_check/\r\n\r\nyields\r\n```\r\n    Last login: Thu Sep 28 12:54:34 on ttys000\r\n    Bens-MacBook-Pro:~ ben$ /Users/ben/Documents/DeepMeerkat/tests/dist/tf_check/tf_check ; exit;\r\n    Traceback (most recent call last):\r\n      File \"tf_check.py\", line 1, in <module>\r\n      File \"/Library/Python/2.7/site-packages/PyInstaller-3.3-py2.7.egg/PyInstaller/loader/pyimod03_importers.py\", line 396, in load_module\r\n        exec(bytecode, module.__dict__)\r\n      File \"tensorflow/__init__.py\", line 24, in <module>\r\n      File \"/Library/Python/2.7/site-packages/PyInstaller-3.3-py2.7.egg/PyInstaller/loader/pyimod03_importers.py\", line 396, in load_module\r\n        exec(bytecode, module.__dict__)\r\n      File \"tensorflow/python/__init__.py\", line 107, in <module>\r\n      File \"/Library/Python/2.7/site-packages/PyInstaller-3.3-py2.7.egg/PyInstaller/loader/pyimod03_importers.py\", line 396, in load_module\r\n        exec(bytecode, module.__dict__)\r\n      File \"tensorflow/python/platform/test.py\", line 60, in <module>\r\n      File \"/Library/Python/2.7/site-packages/PyInstaller-3.3-py2.7.egg/PyInstaller/loader/pyimod03_importers.py\", line 396, in load_module\r\n        exec(bytecode, module.__dict__)\r\n      File \"mock/__init__.py\", line 2, in <module>\r\n      File \"/Library/Python/2.7/site-packages/PyInstaller-3.3-py2.7.egg/PyInstaller/loader/pyimod03_importers.py\", line 396, in load_module\r\n        exec(bytecode, module.__dict__)\r\n      File \"mock/mock.py\", line 71, in <module>\r\n      File \"pbr/version.py\", line 461, in semantic_version\r\n      File \"pbr/version.py\", line 448, in _get_version_from_pkg_resources\r\n      File \"pbr/packaging.py\", line 755, in get_version\r\n    Exception: Versioning for this project requires either an sdist tarball, or access to an upstream git repository. It's also possible that there is a mismatch between the package name in setup.cfg and the argument given to pbr.version.VersionInfo. Project name mock was given, but was not able to be found.\r\n    [1916] Failed to execute script tf_check\r\n    logout\r\n    Saving session...\r\n    ...copying shared history...\r\n    ...saving history...truncating history files...\r\n    ...completed.\r\n    Deleting expired sessions...9 completed.\r\n    \r\n    [Process completed]\r\n```\r\n\r\nSome system information\r\n\r\n```\r\n    Python 2.7.10 (default, Feb  7 2017, 00:08:15) \r\n    [GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.34)] on darwin\r\n    Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n    >>> import tensorflow\r\n    >>> tensorflow.__version__\r\n    '1.3.0'\r\n```\r\n\r\nCan anyone show an example of packaging tensorflow on OSX?\r\n", "can we get any point here ? is this a problem of pyinstaller or tensorflow ?", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Is that still happening in the latest version? I haven't heard anything recently on that front.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "I'm getting the same error\r\n\r\ntensorflow.python.framework.errors_impl.NotFoundError: /home/bitberry/darkflow/dist/detect_from_image/tensorflow/contrib/coder/python/ops/_coder_ops.so: cannot open shared object file: No such file or directory\r\n[11102] Failed to execute script detect_from_image\r\n", "Is the file present? Can you try ldd on it?"]}, {"number": 13059, "title": "Clarifying relations of high level APIs", "body": "There are right now several high level APIs in TensorFlow (`tf.keras`, `tf.estimators`, `tf.layers`, `tf.contrib.learn`) that seem to be sometimes complementary and sometimes redundant. It is somewhat hard for me (and other parts of the internet I have searched) to understand which API to use for which purpose and how they relate to each other.\r\n\r\nFor me it would greatly help if it could be specified:\r\n* What is the intended high level API for the future? Even so if none of them will be discontinued, it would be nice one could just get pointed to one preferred. All I could find the documentation right now is: \r\n\"The higher level APIs are built on top of TensorFlow Core. These higher level APIs are typically easier to learn and use than TensorFlow Core. In addition, the higher level APIs make repetitive tasks easier and more consistent between different users. A high-level API like tf.estimator helps you manage data sets, estimators, training and inference.\"\r\n* How different modules from `contrib` relate to those in core tensorflow. E. g. it seems that `tf.estimators` is a portion of `tf.contrib.learn` that made it into core.\r\n* Why there are duplicate implementations like `tf.layers.contrib.conv2d` and `tf.layers.conv2d`?\r\n* If modules have complementary or duplicate functionality. E.g. `tf.layers` can be used with `tf.estimators` to define the model, but probably one does not want to mix `tf.layers` and `tf.keras` .\r\n\r\nIt would be great if that could be specified in the Overview section of the respective documentations, instead of just saying it is a high level API\r\n \r\n", "comments": ["cc @martinwicke ", "Here are a few general guidelines:\r\n\r\n- Everything in contrib is experimental. Some of it may move to core (tf.estimator is a piece of contrib.learn which moved to core), some of it may not. \r\n- There are strong API guarantees for things not in contrib, there are no such guarantees in contrib.\r\n- Nevertheless, we avoid removing things without notice, so this process leaves duplicates (often with slight differences) in contrib.\r\n- We will mark a lot of things in contrib to be explicitly deprecated in the coming months to make this clearer.\r\n- Consequently, if there is an option to use that's not in contrib, use it. It's safe, won't change, and won't be removed.\r\n- tf.layers in fact does work (mostly) with tf.keras. tf.keras is a full implementation of the Keras 2.0 spec, which is partly collected from other places in TensorFlow via simple aliases, and partly by extending functionality implemented elsewhere. If you rely on compatibility with Keras, use tf.keras. Mixing with other things in TensorFlow is generally ok, but some things may not work (e.g. tf.layers.Dense does not currently support serialization). All of TensorFlow can be used with tf.estimator.\r\n\r\nI know the current state of contrib is confusing, and we're working on cleaning it up.", "Hi, @martinwicke . I noticed that contrib supports more features than core. Say, `tf.contrib.learn.estimator` supports `feature_enginerring_fn`, while `tf.estimator` not. If `tf.contrib.learn.estimator` is deprecated, where do we find the missing features? ", "@facaiy `tf.contrib` is experimental API, things in contrib can disappear/get renamed/etc. IE, no guarantee that `feature_enginerring_fn` from contrib will exist in future versions of TF", "We decided not to further support feature_engineering_fn.\n\nWe experiment with designs in contrib. The things that work go to core. The\nthings that don't don't. feature_engineering_fn did not make the cut.\n\nIt's use case is unclear and it causes a bunch of gnarly problems.\n", "Thanks, @martinwicke . I understand `feature_engineering_fn` not a perfect solution. How about using pipeline? Will tensorflow plan to use pipeline to simply feature transform for training and predicting, like sklearn and spark?", "Check out github.com/tensorflow/transform. I think it may be what you want."]}, {"number": 13058, "title": "Fix typo in math_ops.py", "body": "Subscripts in documentation of `tensdordot` function were not enclosed in curly braces which led to only one of the three subscripts being lowered.", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Oh, this is against `1.3`, we only accept contributions in `master`."]}, {"number": 13057, "title": "check invalid string type for dest_nodes in extract_sub_graph", "body": "Fix #13047.\r\n\r\n### How to test\r\n\r\n+ [x] add an unit test.\r\n+ [ ] pass all tests.", "comments": ["Jenkins, test this please.", "@reedwm mind taking a look?", "How about taking a test? Thanks.", "Jenkins, test this please.", "This test failure seems unrelated.\r\n\r\n```bash\r\nrm: cannot remove '/workspace/tensorflow/contrib/makefile/gen/dep/tensorflow/contrib/makefile/downloads/fft2d/fftsg.d': Permission denied\r\nmake: *** [clean] Error 1\r\nBuild step 'Execute shell' marked build as failure\r\n[Set GitHub commit status (universal)] ERROR on repos [] (sha:7702103) with context:tensorflow-pull-requests-makefile\r\nUnable to get pull request builder trigger!!\r\n```", "Jenkins, test this please.", "Yay! All tests passed."]}, {"number": 13056, "title": "Update mnist beginners softmax variables", "body": "The usage of variable 'x' when describing the softmax function can be confusing to newcomers since 'x' is used immediately prior to represent the unweighted input. Continuing the usage of 'evidence' helps with continuity.", "comments": ["@nealwu what do you think?"]}, {"number": 13055, "title": "Add support of `drop_negatives` for `tf.unsorted_segment_sum`", "body": "This fix tries to address the issue raised in #478 by adding the support of `drop_negatives` for `tf.unsorted_segment_sum` so that it is possible to skip entries (when index = -1)\r\n\r\nThis fix fixes #478.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "@yongtang, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @tensorflower-gardener and @vrv to be potential reviewers.", "This will probably require an API review.\r\n\r\nAt a high level, could we use `tf.where` with the simple `reduce_sum`?", "Thanks @drpngx for the review. I addressed some of the comments. For using `tf.where` with `reduce_sum`, I am still playing with it.\r\n\r\nHowever, I noticed that at the moment, the behavior of CPU and GPU is actually different. On CPU, negative indices will through out an error while on GPU negative indices will be ignored silently.\r\n\r\nIn other words, on GPU the behavior of `tf.unsorted_segment_sum` is already `drop_negatives=True` even before this PR.\r\n\r\nI am wondering if it make sense to just adjust the behavior on CPU so that negatives will be dropped? \r\n\r\nThis will bring the behavior of GPU and CPU to the same without introducing another flag of `drop_negatives=True`.\r\n\r\nBelow is the output for GPU and CPU without this PR:\r\n```python\r\nPython 2.7.12 (default, Nov 19 2016, 06:48:10) \r\n[GCC 5.4.0 20160609] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> print tf.__version__\r\n1.3.0\r\n>>> with tf.device('/gpu:0'):\r\n...   v = tf.unsorted_segment_sum([1,2], [1, -1], 5)\r\nKeyboardInterrupt\r\n>>> \r\n>>> import numpy as np\r\n>>> with tf.device('/gpu:0'):\r\n... v = tf.unsorted_segment_sum(np.asarray([1,2], dtype=np., [1, -1], 5)\r\nKeyboardInterrupt\r\n>>> \r\n>>> \r\n>>> with tf.device('/gpu:0'):\r\n...   v = tf.unsorted_segment_sum([1.0, 2.0], [1, -1], 5)\r\n...   tf.Session().run(v)\r\n... \r\n....\r\narray([ 0.,  1.,  0.,  0.,  0.], dtype=float32)\r\n>>> with tf.device('/cpu:0'):\r\n...   v = tf.unsorted_segment_sum([1.0, 2.0], [1, -1], 5)\r\n...   tf.Session().run(v)\r\n... \r\n2017-09-19 21:26:50.961788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)\r\nTraceback (most recent call last):\r\n...\r\n...\r\nInvalidArgumentError (see above for traceback): segment_ids[1] = -1 is out of range [0, 5)\r\n\t [[Node: UnsortedSegmentSum_1 = UnsortedSegmentSum[T=DT_FLOAT, Tindices=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](UnsortedSegmentSum_1/data, UnsortedSegmentSum_1/segment_ids, UnsortedSegmentSum_1/num_segments)]]\r\n\r\n>>> \r\n```", "Requesting API review. To rephrase, see @yongtang comment: GPU and CPU behave differently, one dropping negatives and the other one doesn't. We can leave the behavior undefined until we use `drop_negatives`, or just decide on a common behavior (error out or silently skip).\r\n", "Since the behavior on GPU is `drop_negative=True`, and since (IIRC) the current behavior on CPU is an error, we are within our right to change the behavior on CPU to match GPU (`drop_negative=True`). I think this alone would address #478. \r\n\r\nI do not know whether we would still want the extra argument. Does anyone require `drop_negative=False` (which does not result in an error)?", "Thanks @drpngx @martinwicke for the review. I updated the PR so that now negative indices will be dropped silently, bring in behaviors of GPU and CPU closer.\r\n\r\nI do notice another discrepancy between GPU and CPU. In GPU the bad index (> num_segments) are ignored as well while in CPU greater than num_segments will results in error. Not sure the best way to handle this situation. Any suggestions about that?", "Can you edit the docs in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/math_ops.cc#L1681 to point out the negative-dropping behavior?", "Thanks @martinwicke. The PR has been updated with the documentation added in `math_ops.cc`.", "Jenkins, test this please.", "I tried to produce locally for the python 3 failure `//tensorflow/contrib/learn:dnn_linear_combined_test` but it works fine. Maybe it is transient?", "Jenkins, test this please.\n\nOn Sep 29, 2017 8:17 AM, \"Yong Tang\" <notifications@github.com> wrote:\n\n> I tried to produce locally for the python 3 failure\n> //tensorflow/contrib/learn:dnn_linear_combined_test but it works fine.\n> Maybe it is transient?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/13055#issuecomment-333154403>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbTCvXSammRotSl5bmYQAqDYccMzJks5snQofgaJpZM4PYen7>\n> .\n>\n", "Approval for API review.", "Thanks all for the review. The Windows build failure is likely to be  transient (didn't failure in the previous runs). I think a rerun might fix it.", "Jenkins, test this please.", "The `Linux CPU Tests Makefile` is the only test failing. It might be unrelated (also saw the same failure on other PRs).", "Yes, but I still want to see it run.\r\n\r\nJenkins, test this please."]}, {"number": 13054, "title": "R1.3", "body": "", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "Sorry, not sure what you're trying to do here."]}, {"number": 13053, "title": "Branch 168776302", "body": "", "comments": ["Thanks!\n\nOn Sep 14, 2017 7:30 PM, \"Shanqing Cai\" <notifications@github.com> wrote:\n\n> Merged #13053 <https://github.com/tensorflow/tensorflow/pull/13053>.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/13053#event-1250326474>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbVq_6xTSKwGPuoVIBU8dkFZALPOLks5sieFbgaJpZM4PYYqr>\n> .\n>\n"]}, {"number": 13052, "title": "Updating readme to show Windows binary support.", "body": "", "comments": ["@av8ramit, thanks for your PR! By analyzing the history of the files in this pull request, we identified @alanyee, @keveman and @vrv to be potential reviewers."]}, {"number": 13051, "title": "Revert \"EHN: csv supports missing value (#13008)\"", "body": "This reverts commit 36c5f218d00c4bdee296974b7a7e4d95da8a88a7.\r\n\r\nOriginal change needs to go through API review.", "comments": []}, {"number": 13050, "title": "MKL inference numbers are incorrect", "body": "In [Performance Guide -> Comparing Compiler Optimizations](https://www.tensorflow.org/performance/performance_guide#comparing_compiler_optimizations), the numbers shared for inference on InceptionV3 and ResNet-50 models are exactly the same.\r\n\r\nI'm assuming this is incorrect?", "comments": ["@tfboyd would you please take a look?", "You are correct.  In my effort to format them I must have copied and\r\npasted.  I will update and get someone to publish it again.  Thank you.\r\n \r\n@derekhh I want to add I am really sorry about that.  I know it seems crazy as how can a person copy the numbers wrong.  The review process to rework the guide was so long I think I just lost focus.  I will post the fresh numbers here incase you are interested and update the guide ASAP, likely this weekend but it will take a few days for someone to do the publish part.  \r\n", "The data you are seeing is InceptionV3 copied twice as an FYI until I get it fixed.  ", "@tfboyd - Thanks for the reply. I definitely understand that everyone can and will make mistakes. :) \r\n\r\nI really appreciate the great documentation and work TensorFlowers like you have brought to the community. ", "cl/169317026  (my tracking purposes)\r\n\r\nSorry about that and here are the updated numbers.  I also had some slightly newer numbers for inception3.  No real difference as a glance but I reran the numbers and did not update the document before I published.  Thank you for understanding.  It might take a little bit to update the site but you have the numbers here for now.  Sorry about the tables we have a slightly different Mark Down engine for the website.  \r\n\r\n#### Inference InceptionV3\r\n\r\n**Environment**\r\n\r\n*   Instance Type: AWS EC2 m4.xlarge\r\n*   CPU: Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz (Broadwell)\r\n*   Dataset: ImageNet\r\n*   TensorFlow Version: 1.2.0 RC2\r\n*   Test Script: [tf_cnn_benchmarks.py](https://github.com/tensorflow/benchmarks/blob/mkl_experiment/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py)\r\n\r\n**Batch Size: 1**\r\n\r\nCommand executed for the MKL test:\r\n\r\n```bash\r\npython tf_cnn_benchmarks.py --forward_only=True --device=cpu --mkl=True \\\r\n--kmp_blocktime=0 --nodistortions --model=inception3 --data_format=NCHW \\\r\n--batch_size=1 --num_inter_threads=1 --num_intra_threads=4 \\\r\n--data_dir=<path to ImageNet TFRecords>\r\n```\r\n\r\n| Optimization | Data Format | Images/Sec   | Intra threads | Inter Threads |\r\n:              :             : (step time)  :               :               :\r\n| ------------ | ----------- | ------------ | ------------- | ------------- |\r\n| AVX2         | NHWC        | 7.0 (142ms)  | 4             | 0             |\r\n| MKL          | NCHW        | 6.6 (152ms)  | 4             | 1             |\r\n| AVX          | NHWC        | 5.0 (202ms)  | 4             | 0             |\r\n| SSE3         | NHWC        | 2.8 (361ms)  | 4             | 0             |\r\n\r\n**Batch Size: 32**\r\n\r\nCommand executed for the MKL test:\r\n\r\n```bash\r\npython tf_cnn_benchmarks.py --forward_only=True --device=cpu --mkl=True \\\r\n--kmp_blocktime=0 --nodistortions --model=inception3 --data_format=NCHW \\\r\n--batch_size=32 --num_inter_threads=1 --num_intra_threads=4 \\\r\n--data_dir=<path to ImageNet TFRecords>\r\n```\r\n\r\n| Optimization | Data Format | Images/Sec    | Intra threads | Inter Threads |\r\n:              :             : (step time)   :               :               :\r\n| ------------ | ----------- | ------------- | ------------- | ------------- |\r\n| MKL          | NCHW        | 10.3          | 4             | 1             |\r\n:              :             : (3,104ms)     :               :               :\r\n| AVX2         | NHWC        | 7.5 (4,255ms) | 4             | 0             |\r\n| AVX          | NHWC        | 5.1 (6,275ms) | 4             | 0             |\r\n| SSE3         | NHWC        | 2.8 (11,428ms)| 4             | 0             |\r\n\r\n#### Inference ResNet-50\r\n\r\n**Environment**\r\n\r\n*   Instance Type: AWS EC2 m4.xlarge\r\n*   CPU: Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz (Broadwell)\r\n*   Dataset: ImageNet\r\n*   TensorFlow Version: 1.2.0 RC2\r\n*   Test Script: [tf_cnn_benchmarks.py](https://github.com/tensorflow/benchmarks/blob/mkl_experiment/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py)\r\n\r\n**Batch Size: 1**\r\n\r\nCommand executed for the MKL test:\r\n\r\n```bash\r\npython tf_cnn_benchmarks.py --forward_only=True --device=cpu --mkl=True \\\r\n--kmp_blocktime=0 --nodistortions --model=resnet50 --data_format=NCHW \\\r\n--batch_size=1 --num_inter_threads=1 --num_intra_threads=4 \\\r\n--data_dir=<path to ImageNet TFRecords>\r\n```\r\n\r\n| Optimization | Data Format | Images/Sec   | Intra threads | Inter Threads |\r\n:              :             : (step time)  :               :               :\r\n| ------------ | ----------- | ------------ | ------------- | ------------- |\r\n| AVX2         | NHWC        | 8.8 (113ms)  | 4             | 0             |\r\n| MKL          | NCHW        | 8.5 (120ms)  | 4             | 1             |\r\n| AVX          | NHWC        | 6.4 (157ms)  | 4             | 0             |\r\n| SSE3         | NHWC        | 3.7 (270ms)  | 4             | 0             |\r\n\r\n**Batch Size: 32**\r\n\r\nCommand executed for the MKL test:\r\n\r\n```bash\r\npython tf_cnn_benchmarks.py --forward_only=True --device=cpu --mkl=True \\\r\n--kmp_blocktime=0 --nodistortions --model=resnet50 --data_format=NCHW \\\r\n--batch_size=32 --num_inter_threads=1 --num_intra_threads=4 \\\r\n--data_dir=<path to ImageNet TFRecords>\r\n```\r\n\r\n| Optimization | Data Format | Images/Sec    | Intra threads | Inter Threads |\r\n:              :             : (step time)   :               :               :\r\n| ------------ | ----------- | ------------- | ------------- | ------------- |\r\n| MKL          | NCHW        | 12.4          | 4             | 1             |\r\n:              :             : (2,590ms)     :               :               :\r\n| AVX2         | NHWC        | 10.4 (3,079ms)| 4             | 0             |\r\n| AVX          | NHWC        | 7.3 (4,4416ms)| 4             | 0             |\r\n| SSE3         | NHWC        | 4.0 (8,054ms) | 4             | 0             |\r\n", "Thanks @tfboyd for the update! :)", "@tfboyd is this resolved?", "with 1.4.  It is so hard to publish content to the website I usually give up unless it is urgent.  The easy solution is to wait for a release.  So when 1.4 becomes root the fix will show up.  So roughly 30+ days after I fixed the content.  @gunan ", "There used to be an option to see \"Master\" version of the docs on the website, that was useful", "Master is still there if you go to versions.  The problem is master is not\nupdated by any type of job so it is really only updated when an rc\nhappens.  In a normal world I would have updated the content, waited MAYBE\n1 week verified it and marked this issue closed.  This is not related to\nGunhan at all.\n\nOn Sun, Oct 15, 2017 at 9:18 AM, Yaroslav Bulatov <notifications@github.com>\nwrote:\n\n> There used to be an option to see \"Master\" version of the docs on the\n> website, that was useful\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13050#issuecomment-336722475>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AWZestrmHlOJMViArazSK5YG0d7anT2Lks5ssjBxgaJpZM4PYOq2>\n> .\n>\n", "Feels like a life time.  Numbers are now on the default page due to 1.4 launching.  Closing and having a personal party and a bit sip of diet cherry coke.  cheers.  "]}, {"number": 13049, "title": "Optimize batch matrix transposition for narrow matrices.", "body": "This is an improved version of matrix transposition that specializes in the case when the matrices to be transposed are narrow. This is a preliminary implementation and I hope to get some feedback before further optimizing this implementation as it would likely entail great deal of efforts and patience.\r\n- This implementation modifies the general matrix transposition kernel in a minimal way. Essentially this implementation enables the rectangular tile shapes to be used for transposition. Before, only square tiles are allowed.\r\n- This implementation has a couple of specialized tile sizes and a very simple cost function is used to select among them during runtime. The logic behind this cost function is simple and will likely to be effective on a variety of platforms.\r\n- We tested this implementation on problem sizes of {32, 64, 128, 256, 512, 1024} X range(16, 2048, 32) X range(2, 16) as well as {32, 64, 128, 256, 512, 1024} X range(2, 16)  X range(16, 2048, 32) where these problem size dimensions can be interpreted as batch_size, matrix_height, matrix_width correspondingly.  The average speedup is 16.7%. This experiment includes 10752 data points and is plotted using excel to indicate for what sub-space of all problem size space do we see speedups over baseline/existing implementation. See [picture](https://imgur.com/a/MyVOf). In the picture, problem sizes where we do see speedups are colored red and otherwise white. The three dimensional problem size space is collapsed to two by serializing the batch size dimension and matrix height dimension.\r\n- As you can see from the current performance results, existing implementation is very good at dealing with large batches of tiny matrices. This is due to the fact that the baseline implementation uses a brute force (and nonetheless effective) way of dividing up workloads evenly which outperforms this commits. I may consider modify the cost function to use the baseline in these cases in the future.\r\n- Again, this is only a preliminary implementation, I'm happy to make significant changes.", "comments": ["Can one of the admins verify this patch?", "@tjingrant, thanks for your PR! By analyzing the history of the files in this pull request, we identified @vrv, @keveman and @zheng-xq to be potential reviewers.", "A few high-level thoughts:\r\n\r\n* What kind of comments are you looking for on this PR?  I typed up a bunch of stuff below, but I realize now you may be looking for an entirely different kind of feedback.  Sorry if below isn't helpful.\r\n\r\n* It seems like you have a benchmark for this.  Can you add it to the PR?  Otherwise it's going to be hard for anyone else to evaluate other changes they might want to make.\r\n\r\n* Similarly, can you add the detailed benchmark results (beyond just the picture) to the commit message?\r\n\r\n* This adds a *lot* of new kernels to compile, and that can have an effect on compilation speed and binary size, especially when, as is common, we have to compile the GPU code for multiple GPU archs.\r\n\r\n  I don't want to prematurely optimize or make this a PITA for you, especially if this ends up not being important.  Maybe a sufficient starting place would be: What's the change in compile time for this file?\r\n\r\n* It seems like you'd want to add some additional tests to cover new edge cases in this code?\r\n\r\n* In terms of formatting, please clang-format over this code so we don't have to argue about whitespace, and you'll want to revert some of the style changes, e.g. changing int template parameters from CamelCase to MACRO_CASE, adding (unnecessary parens) and so on.  A decent starting place is https://google.github.io/styleguide/cppguide.html, although that's unfortunately out of date.  :(\r\n\r\n  We prefer constants to macros, e.g. TILE_SIZE.  Also, I think you could rewrite the dispatch using templated functions rather than macros, which might be a nice simplification.\r\n\r\n```\r\ntemplate <int foo, int bar>\r\nbool DoTranspose(...) {\r\n  if (...) {\r\n    launch_kernel<<<...>>>();\r\n    return true;\r\n  }\r\n  ...\r\n  return false;\r\n}\r\n```\r\n\r\n* Please add comments to the code and commit message explaining what this patch does.", "Sorry for this late reply. I was organizing my benchmarks into a presentable form. \r\n- I'm looking for comments that address whether this PR clears or can be improved to clear all the basic design rationales behind Tensorflow (my last point of the PR message euphemistically states that if this PR fails to clear such criteria then I'm willing to let it go). Some of my concerns are:\r\n    - Method invocation is very very ugly.\r\n    - Maybe TF dev team values maintainability well over performance.\r\nAnd your comments are very helpful as they showed me what metrics you hope to evaluate this PR with apart from performance (compilation time, binary size).\r\n\r\n- I have uploaded my standalone benchmark [here](https://github.com/tjingrant/batch_matrix_transpose_benchmark) which includes a README for anyone interested in evaluating. Also a detailed performance evaluation report is included in the repo for data collected on K40 machine and P100 machine. Basically based on our observation, this PR on average gives 12% performance increase on K40 machine and 39% on P100. This benchmark also serves as a test suite as all results are cross-checked.\r\n- I will eventually integrate the standalone benchmark to TF but for now the standalone version seems better suited for evaluating compilation times and binary size. I will add the performance figures to the commit message once implementation stabilizes.\r\n- Compilation time is 5.6 seconds for float. Assuming this holds more or less the same for half and double this amounts to 16.8 seconds on our older 32 core Power8 machine with nvcc 8.0.61. Binary size is 2MB. I hope these figures provide reassurance.", "> Sorry for this late reply.\r\n\r\nNo problem.  I really appreciate what you're doing here.  This is an important kernel, and it hasn't been loved as much as it deserves.\r\n\r\n> Compilation time is 5.6 seconds for float. [...] Binary size is 2MB. I hope these figures provide reassurance.\r\n\r\nThanks you.  I think that should be fine.  If the compilation time becomes a problem, it sounds like we could reasonably easily split it up into three files, one for each data type (i.e. float/half/double).  But I also don't think it's necessary.", "@jlebar , can I have your comments on the latest commit? Some improvements include:\r\n- Using recursive template functions to search and launch appropriate kernels.\r\n- More comments to explain what's going on.\r\n- Fixed issues raised in the previous comments.\r\n\r\nI noticed that in `transpose_op_test.py`, there are already tests in place to check the cases I'm trying to optimize. Specifically:\r\n```\r\n  def test3DGPU(self):\r\n    # If no GPU available, skip the test\r\n    if not test.is_gpu_available(cuda_only=True):\r\n      return\r\n\r\n    datatypes = [np.int8, np.float16, np.float32, np.float64, np.complex128]\r\n    large_shapes = [[4, 1000, 3], [4, 1000, 8], [4, 1000, 13], [4, 3, 1000],\r\n                    [4, 8, 1000], [4, 13, 1000]] * 3\r\n    perms = [[0, 2, 1]] * 6 + [[2, 1, 0]] * 6 + [[1, 2, 0]] * 3 + [[2, 0, 1]\r\n                                                                  ] * 3\r\n```\r\nThe latest commit passes all the test cases present in `transpose_op_test.py`.", "(No problem that you're responding incrementally, but please ping me directly when you need my attention again so I don't accidentally leave you hanging.)", "Hi, @jlebar some more updates:\r\n- Compile 1 set of dispatchers for each data size instead of each data type.\r\n- Tried out your trick and got the tile size searching code to a much cleaner form.\r\n- Refactor by singling out the cost analysis for narrow matrices transposition.\r\n- Fixed misc issues raised.\r\n\r\nCan I have your comments? Again, thanks for being so responsive!", "And a comment to your WDYT question regarding your suggestion of using enable::if to reduce cases down to 2:\r\n\r\nI played around with your neat trick and was able to get a much cleaner presentation. However, fundamentally we can't get below three cases. The \"base case\" you mentioned is really the boundary case. Visualizing all the tile sizes combinations in a 2D space, we face three constraints that, when considered all together, would define the _valid tile size combinations_ subset: physical limit on thread numbers, physical limit on shared memory and further limit on the tile area (product of 2 tile sizes) to increase parallelism. I conveniently called the limit derived from these three constraints the _tile size possibility frontier_, drawing the name from the concept of [production possibility frontier](https://en.wikipedia.org/wiki/Production%E2%80%93possibility_frontier). I think this is a very intuitive name even without the parallel drawn.\r\n\r\nThus with this frontier established, we have three cases:\r\n\r\n- tile size combination lands in the interior of the frontier: we try to launch kernel and if request is not satisfied, we search for larger tile size combinations.\r\n- tile size combination lands on the frontier: we launch regardless of whether request is satisfied, no search performed.\r\n- tile size combination lands in the exterior of the frontier: even compiling such kernel would cause us trouble, thus specialize dispatcher to do nothing at all.", "Ping @jlebar , I have also included a performance note for all types on K40 for float type on P100. ", "Hi, sorry I haven't been able to get to this quickly.  I'll try to do the review by eow, things have just been a little crazy over here.", "@jlebar can I have your feedback? \r\n\r\nI think previously I was using clang-format from clang-4.0 installation and it was having trouble formatting kernel launches. Switching to 5.0 seems to solve this problem.\r\n\r\nThe latest commit fixes the issues raised except for the base case problem. I commented below your comment.", "@jlebar Fixed issues raised. I think the comment becomes outdated once the associated code changes in the new commit.\r\n\r\nThis latest commit does the specialization as discussed. ", "Hi @jlebar , thanks again for being so responsive. I was actually reviewing my last commit when your comments came in... ", "Ping @jlebar , @yzhwang,\r\nUnfortunately I think we've run into a conflict. By examining the code @yzhwang submitted, I think we have very similar ideas but the implementation @jlebar and I worked out here forms a super set of what @yzhwang has in his implementation. \r\n\r\nedit: I take the above statement back. @yzhwang seems to have batched sub-tiles within a single kernel launch. It is useful to learn the motivation behind such design.\r\n\r\nNotably from his comment:\r\n```\r\n// Each thread block operates on a single rectangle tile, where its width is\r\n// kTileLength (we currently set it to 64) and its height is small_dim,\r\n// We set the thread block's X dimension to be tile_num_per_block, and its Y\r\n// and Z to be one.\r\n```\r\nand \r\n```\r\n// When only one of the dimensions is smaller than kMinDimensionToUseTiles,\r\n// we use one block to process a rectangle region with the size of\r\n// kTileLength * small_dim. We found that when set kTileLength to 64 on\r\n// TitanX Maxwell GPU, it achieves the best performance.\r\n```\r\nA few things I would like to point out:\r\n- this PR specializes the kernel launch parameters for each type.\r\n- this PR also does not have a fixed long side len (in fact we have choices of {32, 64, 128...1024}).\r\n- this PR also has a very nicely designed kernel dispatcher (thanks to @jlebar ) that works to select among multiple pre-compiled kernels.\r\n- this PR has dynamically determined share memory sizes and therefore will be more shared-memory-efficient.\r\n- @yzhwang seems to have written another kernel but I simply generalized the SwapDimension1And2InTensor3UsingTiles kernel so it seems like my approach produces less maintenance overhead.\r\n- Similarly, can you explain why do you need two `__syncthreads()`? I think you are operating on a 1D sliding window in a tile to transpose the matrix. I suspect that this could hurt parallelism; also to achieve performance portability, I think we should make this into multiple blocks and let the hardware scheduler make the decision of how to schedule sub-tiles.\r\n- this PR performs exhaustive experiments on 2 server-grade GPUs including P100 and K40 and thus the performance results will be more reliable and trust-worthy for performance-critical scenarios.\r\n\r\nSo I think if I get these facts right, I would like to propose the following to resolve the conflicts:\r\n- Absorb suggestions from @yzhwang. Maybe there are reasons for having another kernel SwapDimension1And2InTensor3UsingTiles.\r\n- Acknowledge in any ways we see fit the contribution of @yzhwang.\r\n- Unfortunately, if the reasons I listed above are factually correct, I think the logical thing to do is to base everything in `conv_ops_gpu_3.cu.cc` on this PR and acknowledge @yzhwang 's contribution by applying his suggestions here and make explicit attributions in comment. \r\n- @yzhwang seems to have a more comprehensive test suite that should be used instead of mine.\r\n- I hope there could be a mechanism for people inside and outside Google to know what each other is working on... at least for the open source part of Tensorflow.\r\n\r\n**Most importantly**, I hope that this conflict can be resolved in a way that pays acknowledgement to every one involved in a fair way. It is genuinely awful see anyone's effort made in vain and I hope we can work towards preventing that from happening.", "> I hope there could be a mechanism for people inside and outside Google to know what each other is working on... at least for the open source part of Tensorflow.\r\n\r\nYeah, this is disappointing for me, too.\r\n\r\nI'm not sure what we should do to avoid this in the future -- @hawkinsp, do you have any thoughts?  Should I announce a big change like this on our internal TF mailing list so people know we're working on it?  I guess I figured this code hadn't been reworked in so long, the chances were small that we'd midair like this...", "Hi @tjingrant Thanks for your changes! I have been working on-and-off on this for a long time. The original submission got roll-backed since I spotted a bug for large batch size with one large dimension. Recently I fixed the bug and resubmitted the code.\r\nI agree we should have some mechanism letting people in the open source know what we are working on. Usually we submit smaller change lists with quicker cycles. I don't think our case is very common.\r\nI will now study your design and implementation and run some tests and benchmark internally. I will keep you updated on this.\r\nIf your implementation is a super set of my kernel and it always performs better, I'd be happy to assist you get this in.\r\nAgain, thanks for your work on this. ", "@yzhwang great to see you here! Now we can get the conversation starting and move forward! If you look into my implementation, beware of a bug introduced by the later commits that causes redundant kernel executions and somehow slipped through the tests because it doesn't interfere with the results.\r\n\r\nI also looked into your design and you certainly raise the baseline to a new height! As I put in my edit, I no longer believe that what you did is a subset of my implementation. Your specializations prove useful for later architectures (which I did not specifically target since from comments of your code, I inferred that K40 is what you care about and this implementation is indeed faster on K40 using your benchmarks) and I'm still in the process of figuring out which are the most important factors contributing to the better performance. That being said, it would be nice to take the best from both of our implementation and properly recognize these contributions.\r\n\r\nAlso, I suspect that you are trying to tackle the possibility that y dimension of the grid may overflow. I'm curious why not just use the x dimension of the grid as the general case does?", "Hi @tjingrant ! Using x dimension of the grid will simplify the implementation (we don't need to handle multiple batches per block any more. I put batch size to y dimension at first because I didn't realize we would have batch number larger than 65536, then when someone reported such bug, I tried to make as few changes as possible to fix that bug. Hence the current implementation. Also, the two _syncthreads() is likely redundant, currently it solves a bug for certain cases. I will root cause this and eventually figure out a better way.\r\nAs for benchmark, I will do it on all the platforms we care internally and report back. We can decide then how to proceed. I'm sure we can figure out a way to integrate our work into TensorFlow.", "@tjingrant Yes you are right. I know for gridDim.x we can go as far as 2^32-1, switching to it though requires more changes to the kernel and I currently don't have too much time to look at. Anyway, let me finish testing and benchmark on your PR first. We can always keep improving the kernel.", "@yzhwang thanks for your feedback, I realized that you did not intentionally put it on the y dim which explains. ", "Hi @tjingrant Sorry for the late reply. I spent some time understanding your implementation. I really like how it provides a unified version of doing transpose with tiling and shared memory. Thanks a lot for the work! After some internal discussion with my colleagues, we would like to integrate your implementation.\r\n\r\nSo I did some benchmarks using: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/transpose_benchmark.py and we found that comparing with our current implementation, the performance of your implementation is not always better. For float64, float32, int8, and float16, they all have some input shapes that showed regression with your implementation. I did the benchmark on a GTX 1080 card. That being said, it is likely due to that you are not tuning the consts in TileSizePossibilityFrontierCheck() for that specific architecture. I will try to benchmark this on more architectures this week and let you know.\r\n\r\nSince checking in your code as it is would cause some performance regression, going forward, we should work together to make sure that this unified tiling transpose implementation could provide the best performance on every architecture (or most architectures) we care about. I hope you could keep working on this with us. So my plan is to first figure out a better tuning strategy so that it could work better on both Pascal and K40/K80. Then we can improve it further for Volta in another PR. I know it might be hard for you to work on this given the hardware constraint, I will try my best to assist you with the benchmarks. Right now, it would be really nice if you could share some of your thoughts on tuning for different architectures. Thank you!", "@yzhwang Thanks for getting back to me! Glad to know you are interested in my implementation.\r\n\r\nI sure will be working closely with the TF team. But now I'm temporarily busy with other things our team is working on related to TF for this week so likely I'll get back to you with more substance some time next week. From my experience, the frontier indeed needs to be tuned. However the kernel also needs some tweaking. I noticed that on P100, cuda does not realize that I'm reading from contiguous memory locations possibly because of the thread ID relabeling process. With nvprof, the resulting shared memory store transactions are significantly higher. I'm not very sure about the exact cause of it yet because we have a custom cuda driver that may break things. \r\n\r\nRegarding the hardware constraint, I have access to all the machines you are interested in but the constraint is that I may have a slightly customized driver and therefore it would be useful for you to post your performance results so that we can cross-check.\r\n\r\nAgain thanks a lot for actively resolving the conflict!\r\n", "@yzhwang Hi, sorry for my digression, I'm more or less done with other works and will be working on tuning this patch now. As explained in my previous comment, can you share your performance results on the P100 machine so that I can cross-check whether my driver is interfering with the results? (It has been in the past).", "Hey @tjingrant ! Thank you! I've sent the performance comparison sheet to your gmail.", "@tjingrant @yzhang87 Any progress on this?", "@martinwicke hi, this is not stalled, it just takes time. Hopefully I'll get back with some news in about a week.", "@tjingrant Thanks for the work! Let me know if you need any help from my side too.", "@tjingrant any luck with this?", "Yes, I was able to identify the major culprit behind the performance degradation and I'm applying my fix. \r\n\r\nSpecifically, by examining the generated assembly code, I think the performance degradation compared to @yzhwang 's implementation comes down to these lines:\r\n```\r\nint ti = x / TileSizeJ;\r\nint tj = x % TileSizeJ;\r\n```\r\nI think due to these computations to obtain a 2D thread ids, the compiler is unable to recognize that I'm actually accessing contiguous memory regions within a warp. I'm trying to make it more obvious for the compiler to optimize for such memory access. I'll try to push some changes between today and tomorrow.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "Strange, there's some issue with e4659c4fcbf64b96c5f389519d3d87416bf8bc70. Could you check the email? This is only a two-line change so it can't be the previous comment.", "@drpngx what problems specifically? CI build failures are all Google internal CI failures; they seem to be caused by failure to start the build process.", "OK, the bot appears to have approved the change. Let's see.", "Jenkins, test this please.", "For some reason the Windows build didn't kick off, either transient or we ran out of executors. Trying again.\r\n\r\nJenkins, test this please.", "Jenkins, test this please."]}, {"number": 13048, "title": "Word2vec on GPU slower than CPU", "body": "\r\n### System information\r\n- **OS Platform and Distribution**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.3.0\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: 0.5.0\r\n- **CUDA/cuDNN version**: 8.0/6.0\r\n- **GPU model and memory**: NVIDIA GTX 1060 / 3GB\r\n- **Docker used:** yes\r\n- I picked up the code from the [word2vec example](https://github.com/tensorflow/models/tree/master/tutorials/embedding) on your official repo and made a few changes.The core code to train word2vec remains the same.\r\n\r\n### Describe the problem\r\nI have been working on benchmarking commonly used frameworks/libraries for unsupervised learning of word embeddings(word2vec). I am currently comparing tensorflow(cpu/gpu), gensim, deeplearning4j and the original c code on standard metrics like training time, peak memory usage and quality of learned vectors. Link to my [github repo](https://github.com/manneshiva/benchmark-word2vec-frameworks) (still working on it). I ran the benchmark on text8 corpus(plan to run it on a much larger corpus later for the true picture) which gave me strange results. \r\n- Tensorflow on GPU is much slower than CPU\r\n- Tensorflow is much slower than other frameworks\r\n\r\nIs this behavior expected? Would appreciate any inputs.\r\n### Source code / logs\r\nLink to [tensorflow code](https://github.com/manneshiva/benchmark-word2vec-frameworks/blob/master/nn_frameworks/tensorflow/word2vec.py)\r\nLink to [results](http://nbviewer.jupyter.org/github/manneshiva/benchmark-word2vec-frameworks/blob/8223b9ff4b37869e5aef36a909dec384e08f3a05/visualize_report.ipynb) of sample benchmark on text8 corpus\r\n", "comments": ["A few thoughts:\r\nOur tutorial code tends to be written to maximize clarity rather than to maximize performance. It's not surprising that tutorial code wouldn't necessarily run very efficiently, on either CPU or GPU. \r\n\r\nI don't see anything in the word2vec code that suggests that it's been optimized to work on a GPU. \r\nEmbeddings, by their nature, tend to emphasize fine-grained, random memory lookups. That plays much less to the strengths of the GPU. \r\n\r\nThis question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "@cy89 do you know of a more optimized implementation of word2vec in TensorFlow (less tutorial-ish)?", "Same problem. Word2vec cpu is 10 times faster than word2vec gpu. Yes, it is very surprising! But this is what I got. Both cpu and gpu are slow. Waste a lot of time studying the code and modifying the code.", "Hello,\r\n\r\nI have the same problem on TensorFlow 1.8 running word2vec_optimized.py on a system with Volta GPUs. \r\n\r\nRgds,\r\nFM"]}, {"number": 13047, "title": "tf.graph_util.extract_sub_graph should raise a better error message", "body": "Hi, \r\n\r\nI used `tf.graph_util.extract_sub_graph` to get a subgraph reaching a particular dest node. My dest_node is say `conv2d_transpose_7`. I by mistake gave it as a string in the second parameter. Python parses String too as a list and it raised an error `AssertionError: c is not in graph`. The `extract_sub_graph` should first check if the second parameter `isinstace` of a List so that it would be convenient to debug this kind of error in the future. A screenshot is attached.\r\n\r\nThanks, \r\n\r\n\r\n<img width=\"613\" alt=\"screenshot at sep 14 23-53-32\" src=\"https://user-images.githubusercontent.com/6195312/30446912-22e74a76-99a8-11e7-81b4-3479d2e49c09.png\">\r\n", "comments": ["Sounds reasonable, I can work on it. ", "Marking as contributions welcome since @facaiy is working on it"]}]