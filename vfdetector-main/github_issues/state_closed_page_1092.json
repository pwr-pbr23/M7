[{"number": 20508, "title": "[Update debugger.md] Fix error in sample code", "body": "Fix issue of \"function object is not subscriptable\" in sample code:\r\nnp.ones[10]->np.ones(10)", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 20507, "title": "avoid signed unsigned comparison warning", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Is this PR still alive? @walyong, please address the review comments, thanks.", "It has been 20 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'll close this for inactivity. Reopen a new PR if you want to continue this."]}, {"number": 20506, "title": "Pop from empty context_switches when take outputs of one estimator.predict as inputs of another", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: y \r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux  4.14.8-gentoo-r1\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: 3.5.5\r\n- **Bazel version (if compiling from source)**: n\r\n- **GCC/Compiler version (if compiling from source)**: n\r\n- **CUDA/cuDNN version**: 7.1.4\r\n- **GPU model and memory**:  GeForce GTX 1080 Ti / 11171MiB\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nCurrently I am working on making predictions of two models (outputs of model1 as inputs of model2) with large amount of inputs. I tried to use tf.data.Dataset.from_generator, while it seems there are some problems about context stack.\r\n\r\n### Source code / logs\r\nHere is my example\r\n``` python\r\nimport tensorflow as tf\r\n\r\ndef model_fn(features, labels, mode):\r\n    x = features[\"inputs\"]\r\n    W = tf.get_variable(\r\n        name=\"weight\",\r\n        shape=[1],\r\n        dtype=tf.float32,\r\n        initializer=tf.initializers.constant(value=0.0),\r\n        trainable=(mode == tf.estimator.ModeKeys.TRAIN),\r\n    )\r\n    b = tf.get_variable(\r\n        name=\"bias\",\r\n        shape=[1],\r\n        dtype=tf.float32,\r\n        initializer=tf.initializers.constant(value=0.0),\r\n        trainable=(mode == tf.estimator.ModeKeys.TRAIN),\r\n    )\r\n    hypothesis = W * x + b\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        y = labels[\"labels\"]\r\n        cost = tf.square(hypothesis - y)\r\n        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\r\n        train_op = optimizer.minimize(\r\n            loss=cost, global_step=tf.train.get_global_step())\r\n    else:\r\n        train_op = None\r\n        cost = None\r\n    return tf.estimator.EstimatorSpec(\r\n        mode=mode, predictions=hypothesis, loss=cost, train_op=train_op)\r\n\r\n\r\ndef get_input_fn(inputs, num_epochs=1):\r\n    def gen():\r\n        for item in inputs:\r\n            if isinstance(item, tuple) or isinstance(item, list):\r\n                yield item\r\n            else:\r\n                yield item, 0\r\n\r\n    def input_fn():\r\n        dataset = tf.data.Dataset.from_generator(\r\n            gen, (tf.float32, tf.float32),\r\n            (tf.TensorShape([]), tf.TensorShape([])))\r\n        dataset = dataset.repeat(num_epochs)\r\n        dataset = dataset.batch(1)\r\n        iterator = dataset.make_one_shot_iterator()\r\n        inputs, labels = iterator.get_next()\r\n        return {\"inputs\": inputs}, {\"labels\": labels}\r\n\r\n    return input_fn\r\n\r\n\r\nmodel1 = tf.estimator.Estimator(model_fn=model_fn, model_dir=\"model1\")\r\nmodel2 = tf.estimator.Estimator(model_fn=model_fn, model_dir=\"model2\")\r\n# model1.train(input_fn=get_input_fn([(1, 1), (2, 2), (3, 3)], num_epochs=100))\r\n# model2.train(input_fn=get_input_fn([(1, 2), (2, 4), (3, 6)], num_epochs=100))\r\n\r\n# ok\r\nfor item in model1.predict(input_fn=get_input_fn([1, 2, 3, 4])):\r\n    print(item)\r\n# ok\r\nfor item in model2.predict(input_fn=get_input_fn([1, 2, 3, 4])):\r\n    print(item)\r\n\r\nmodel1_output = model1.predict(input_fn=get_input_fn([1, 2, 3, 4]))\r\nmodel2_input_fn = get_input_fn(model1_output)\r\n# IndexError: pop from empty list\r\n# tensorflow/python/framework/ops.py\", line 5267, in get_controller\r\n#    context.context().context_switches.pop()\r\nfor item in model2.predict(model2_input_fn):\r\n    print(item)\r\n```\r\n\r\nError log\r\n```\r\nTraceback (most recent call last):\r\n\r\n  File \"/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/ops/script_ops.py\", line 157, in __call__\r\n    ret = func(*args)\r\n\r\n  File \"/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 382, in generator_py_func\r\n    values = next(generator_state.get_iterator(iterator_id))\r\n\r\n  File \"test.py\", line 36, in gen\r\n    for item in inputs:\r\n\r\n  File \"/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/estimator/estimator.py\", line 519, in predict\r\n    for key, value in six.iteritems(preds_evaluated)\r\n\r\n  File \"/usr/lib64/python3.5/contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n\r\n  File \"/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 5267, in get_controller\r\n    context.context().context_switches.pop()\r\n\r\n  File \"/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/eager/context.py\", line 136, in pop\r\n    self.stack.pop()\r\n\r\nIndexError: pop from empty list\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/client/session.py\", line 1322, in _do_call\r\n    return fn(*args)\r\n  File \"/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/client/session.py\", line 1307, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/client/session.py\", line 1409, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.UnknownError: IndexError: pop from empty list\r\nTraceback (most recent call last):\r\n\r\n  File \"/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/ops/script_ops.py\", line 157, in __call__\r\n    ret = func(*args)\r\n\r\n  File \"/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 382, in generator_py_func\r\n    values = next(generator_state.get_iterator(iterator_id))\r\n\r\n  File \"test.py\", line 36, in gen\r\n    for item in inputs:\r\n\r\n  File \"/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/estimator/estimator.py\", line 519, in predict\r\n    for key, value in six.iteritems(preds_evaluated)\r\n\r\n  File \"/usr/lib64/python3.5/contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n\r\n  File \"/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 5267, in get_controller\r\n    context.context().context_switches.pop()\r\n\r\n  File \"/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/eager/context.py\", line 136, in pop\r\n    self.stack.pop()\r\n\r\nIndexError: pop from empty list\r\n\r\n\r\n         [[Node: PyFunc = PyFunc[Tin=[DT_INT64], Tout=[DT_FLOAT, DT_FLOAT], token=\"pyfunc_7\"](arg0)]]\r\n         [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?]], output_types=[DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator)]]\r\n         [[Node: IteratorGetNext/_19 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_6_IteratorGetNext\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 72, in <module>\r\n    for item in model2.predict(model2_input_fn):\r\n  File \"/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/estimator/estimator.py\", line 509, in predict\r\n    preds_evaluated = mon_sess.run(predictions)\r\n  File \"/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/training/monitored_session.py\", line 567, in run\r\n    run_metadata=run_metadata)\r\n  File \"/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/training/monitored_session.py\", line 1043, in run\r\n    run_metadata=run_metadata)\r\n  File \"/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/training/monitored_session.py\", line 1134, in run\r\n    raise six.reraise(*original_exc_info)\r\n  File \"/usr/lib64/python3.5/site-packages/six.py\", line 693, in reraise\r\n    raise value\r\n  File \"/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/training/monitored_session.py\", line 1119, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/training/monitored_session.py\", line 1191, in run\r\n    run_metadata=run_metadata)\r\n  File \"/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/training/monitored_session.py\", line 971, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/client/session.py\", line 900, in run\r\n    run_metadata_ptr)\r\n  File \"/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/client/session.py\", line 1135, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/client/session.py\", line 1316, in _do_run\r\n    run_metadata)\r\n  File \"/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/client/session.py\", line 1335, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.UnknownError: IndexError: pop from empty list\r\nTraceback (most recent call last):\r\n\r\n  File \"/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/ops/script_ops.py\", line 157, in __call__\r\n    ret = func(*args)\r\n\r\n  File \"/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 382, in generator_py_func\r\n    values = next(generator_state.get_iterator(iterator_id))\r\n\r\n  File \"test.py\", line 36, in gen\r\n    for item in inputs:\r\n\r\n  File \"/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/estimator/estimator.py\", line 519, in predict\r\n    for key, value in six.iteritems(preds_evaluated)\r\n\r\n  File \"/usr/lib64/python3.5/contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n\r\n  File \"/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 5267, in get_controller\r\n    context.context().context_switches.pop()\r\n\r\n  File \"/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/eager/context.py\", line 136, in pop\r\n    self.stack.pop()\r\n\r\nIndexError: pop from empty list\r\n\r\n\r\n         [[Node: PyFunc = PyFunc[Tin=[DT_INT64], Tout=[DT_FLOAT, DT_FLOAT], token=\"pyfunc_7\"](arg0)]]\r\n         [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?]], output_types=[DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator)]]\r\n         [[Node: IteratorGetNext/_19 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_6_IteratorGetNext\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n\r\n```", "comments": ["@mrry, could you please take a look? (Are you the right person)?", "Thanks for the straightforward reproducer, @miacro. It looks like this might be a bug in the `Estimator.predict()` implementation (or at least something that deserves a better error message). In particular, if I changed the following lines of code:\r\n\r\n```python\r\nmodel1_output = model1.predict(input_fn=get_input_fn([1, 2, 3, 4]))\r\nmodel2_input_fn = get_input_fn(model1_output)\r\nfor item in model2.predict(model2_input_fn):\r\n    print(item)\r\n```\r\n\r\n...to the following:\r\n\r\n```python\r\nmodel1_output = model1.predict(input_fn=get_input_fn([1, 2, 3, 4]))\r\nmodel1_output = list(model1_output)\r\nmodel2_input_fn = get_input_fn(model1_output)\r\nfor item in model2.predict(model2_input_fn):\r\n    print(item)\r\n```\r\n\r\n...the program behaves correctly.\r\n\r\nI'll assign this to @ispirmustafa, since he made some changes to context handling in `Estimator.predict()` in 0385bfe0726ad9710bfcca145e19611e9e2391bb, and this may account for the error message about \"context switches\".", "Nagging Assignee @ispirmustafa: It has been 46 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ispirmustafa: It has been 61 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Estimator.predict yields predictions under a graph context. The way with handle graph context in TF make yielding an issue. For example following code has same error.\r\n\r\nI'll update the documentation of predict:\r\n\r\n```\r\ndef f1():\r\n  with tf.Graph().as_default():\r\n    yield tf.no_op()\r\n\r\ndef f2():\r\n  with tf.Graph().as_default():\r\n    yield tf.no_op()\r\n\r\ny1, y2 = f1(), f2()\r\n\r\nnext(y1)\r\nnext(y2)\r\nnext(y1)\r\n```", "Updated the documentation: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L484\r\n", "Hi, estimator.predict is to slow , i want to predict some text in my model for every 2 second.  estimator.predict() function allways load the model from latest checkpoint.  I want to load model only once in estimator.predict to get fast prediction."]}, {"number": 20505, "title": "[feature request] allclose", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.4\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 8.0\r\n- **GPU model and memory**: GTX 1070\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nAs far as I can see, tensorflow does not have a function like `allclose` in numpy [1]. It's not something very tricky to implement, but it would be very convenient if it's already included in tensorflow. See the code below for a possible implementation. The default values for `rtol` and `atol` are chosen as they are for the numpy function.\r\n\r\n[1] https://docs.scipy.org/doc/numpy/reference/generated/numpy.allclose.html\r\n\r\n### Source code / logs\r\n```\r\ndef allclose(x, y, rtol=1e-5, atol=1e-8):\r\n    return tf.reduce_all(tf.abs(x - y) <= tf.abs(y) * rtol + atol)\r\n```\r\n", "comments": ["@mrader1248,\r\nIt is implemented in [tf.experimental.numpy.allclose](https://www.tensorflow.org/api_docs/python/tf/experimental/numpy/allclose). Please confirm if we can close this issue. Thanks!"]}, {"number": 20504, "title": "freeze_graph failed with core dump", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux CentOS 7\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:0.11.1\r\n- **GCC/Compiler version (if compiling from source)**: 4.9.2\r\n- **CUDA/cuDNN version**: none\r\n- **GPU model and memory**: none\r\n- **Exact command to reproduce**:\r\n`bazel-bin/tensorflow/python/tools/freeze_graph --input_saved_model_dir ~/tmp/model/test.model --saved_model_tags=serve --output_node_names=Test/Sigmoid --output_graph ~/tmp/model/test_model.pb`\r\n\r\n### Describe the problem\r\nCore dump occurred when I tried to freeze a 7.6G saved model.\r\n\r\n    $ du -sh test.model/\r\n    7.6G\ttest.model/\r\n\r\n2018-07-03 16:15:26.895400: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX\r\nConverted 16 variables to const ops.\r\n\r\n[libprotobuf FATAL external/protobuf_archive/src/google/protobuf/wire_format.cc:830] CHECK failed: (output->ByteCount()) == (expected_endpoint): : Protocol message serialized to a size different from what was originally expected.  Perhaps it was modified by another thread during serialization?\r\nterminate called after throwing an instance of 'google::protobuf::FatalException'\r\n  what():  CHECK failed: (output->ByteCount()) == (expected_endpoint): : Protocol message serialized to a size different from what was originally expected.  Perhaps it was modified by another thread during serialization?\r\nAborted (core dumped)\r\n\r\n\r\n\r\n\r\n\r\n### Source code / logs\r\n```\r\n(gdb) bt\r\n#0  0x00007f7a6cd575f7 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56\r\n#1  0x00007f7a6cd58ce8 in __GI_abort () at abort.c:90\r\n#2  0x00007f7a61d356bd in __gnu_cxx::__verbose_terminate_handler () at ../../.././libstdc++-v3/libsupc++/vterminate.cc:95\r\n#3  0x00007f7a61d33726 in __cxxabiv1::__terminate (handler=<optimized out>) at ../../.././libstdc++-v3/libsupc++/eh_terminate.cc:47\r\n#4  0x00007f7a61d33771 in std::terminate () at ../../.././libstdc++-v3/libsupc++/eh_terminate.cc:57\r\n#5  0x00007f7a61d33988 in __cxxabiv1::__cxa_throw (obj=0xd10f070, tinfo=0x7f7a61aad2f0 <typeinfo for google::protobuf::FatalException>, dest=0x7f7a61a1e8f0 <google::protobuf::FatalException::~FatalException()>)\r\n    at ../../.././libstdc++-v3/libsupc++/eh_throw.cc:87\r\n#6  0x00007f7a61a1f71c in google::protobuf::internal::LogMessage::Finish() ()\r\n   from /home/user/.cache/bazel/_bazel_user/c06ab0b1cffa9a52f3e96df3f3fdd0d6/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so\r\n#7  0x00007f7a619ff402 in google::protobuf::internal::WireFormat::SerializeWithCachedSizes(google::protobuf::Message const&, int, google::protobuf::io::CodedOutputStream*) ()\r\n   from /home/user/.cache/bazel/_bazel_user/c06ab0b1cffa9a52f3e96df3f3fdd0d6/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so\r\n#8  0x00007f7a619fe0d8 in google::protobuf::internal::WireFormat::SerializeFieldWithCachedSizes(google::protobuf::FieldDescriptor const*, google::protobuf::Message const&, google::protobuf::io::CodedOutputStream*) ()\r\n   from /home/user/.cache/bazel/_bazel_user/c06ab0b1cffa9a52f3e96df3f3fdd0d6/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so\r\n#9  0x00007f7a619ff35f in google::protobuf::internal::WireFormat::SerializeWithCachedSizes(google::protobuf::Message const&, int, google::protobuf::io::CodedOutputStream*) ()\r\n   from /home/user/.cache/bazel/_bazel_user/c06ab0b1cffa9a52f3e96df3f3fdd0d6/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so\r\n#10 0x00007f7a619fe0d8 in google::protobuf::internal::WireFormat::SerializeFieldWithCachedSizes(google::protobuf::FieldDescriptor const*, google::protobuf::Message const&, google::protobuf::io::CodedOutputStream*) ()\r\n   from /home/user/.cache/bazel/_bazel_user/c06ab0b1cffa9a52f3e96df3f3fdd0d6/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so\r\n#11 0x00007f7a619ff35f in google::protobuf::internal::WireFormat::SerializeWithCachedSizes(google::protobuf::Message const&, int, google::protobuf::io::CodedOutputStream*) ()\r\n   from /home/user/.cache/bazel/_bazel_user/c06ab0b1cffa9a52f3e96df3f3fdd0d6/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so\r\n#12 0x00007f7a619fe0d8 in google::protobuf::internal::WireFormat::SerializeFieldWithCachedSizes(google::protobuf::FieldDescriptor const*, google::protobuf::Message const&, google::protobuf::io::CodedOutputStream*) ()\r\n   from /home/user/.cache/bazel/_bazel_user/c06ab0b1cffa9a52f3e96df3f3fdd0d6/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so\r\n#13 0x00007f7a619ff35f in google::protobuf::internal::WireFormat::SerializeWithCachedSizes(google::protobuf::Message const&, int, google::protobuf::io::CodedOutputStream*) ()\r\n   from /home/user/.cache/bazel/_bazel_user/c06ab0b1cffa9a52f3e96df3f3fdd0d6/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so\r\n#14 0x00007f7a619fe0d8 in google::protobuf::internal::WireFormat::SerializeFieldWithCachedSizes(google::protobuf::FieldDescriptor const*, google::protobuf::Message const&, google::protobuf::io::CodedOutputStream*) ()\r\n   from /home/user/.cache/bazel/_bazel_user/c06ab0b1cffa9a52f3e96df3f3fdd0d6/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so\r\n#15 0x00007f7a619ff35f in google::protobuf::internal::WireFormat::SerializeWithCachedSizes(google::protobuf::Message const&, int, google::protobuf::io::CodedOutputStream*) ()\r\n   from /home/user/.cache/bazel/_bazel_user/c06ab0b1cffa9a52f3e96df3f3fdd0d6/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so\r\n#16 0x00007f7a619fe0d8 in google::protobuf::internal::WireFormat::SerializeFieldWithCachedSizes(google::protobuf::FieldDescriptor const*, google::protobuf::Message const&, google::protobuf::io::CodedOutputStream*) ()\r\n   from /home/user/.cache/bazel/_bazel_user/c06ab0b1cffa9a52f3e96df3f3fdd0d6/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so\r\n#17 0x00007f7a619ff35f in google::protobuf::internal::WireFormat::SerializeWithCachedSizes(google::protobuf::Message const&, int, google::protobuf::io::CodedOutputStream*) ()\r\n   from /home/user/.cache/bazel/_bazel_user/c06ab0b1cffa9a52f3e96df3f3fdd0d6/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so\r\n#18 0x00007f7a619fe0d8 in google::protobuf::internal::WireFormat::SerializeFieldWithCachedSizes(google::protobuf::FieldDescriptor const*, google::protobuf::Message const&, google::protobuf::io::CodedOutputStream*) ()\r\n   from /home/user/.cache/bazel/_bazel_user/c06ab0b1cffa9a52f3e96df3f3fdd0d6/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so\r\n---Type <return> to continue, or q <return> to quit---\r\n#19 0x00007f7a619ff35f in google::protobuf::internal::WireFormat::SerializeWithCachedSizes(google::protobuf::Message const&, int, google::protobuf::io::CodedOutputStream*) ()\r\n   from /home/user/.cache/bazel/_bazel_user/c06ab0b1cffa9a52f3e96df3f3fdd0d6/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so\r\n#20 0x00007f7a618ccd40 in google::protobuf::python::cmessage::InternalSerializeToString(google::protobuf::python::CMessage*, _object*, _object*, bool) ()\r\n   from /home/user/.cache/bazel/_bazel_user/c06ab0b1cffa9a52f3e96df3f3fdd0d6/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so\r\n#21 0x00007f7a6dae8aa4 in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\r\n#22 0x00007f7a6daea0bd in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\r\n#23 0x00007f7a6dae876f in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\r\n#24 0x00007f7a6daea0bd in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\r\n#25 0x00007f7a6dae876f in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\r\n#26 0x00007f7a6daea0bd in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\r\n#27 0x00007f7a6dae876f in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\r\n#28 0x00007f7a6daea0bd in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\r\n#29 0x00007f7a6dae876f in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\r\n#30 0x00007f7a6daea0bd in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\r\n#31 0x00007f7a6dae876f in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\r\n#32 0x00007f7a6daea0bd in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\r\n#33 0x00007f7a6dae876f in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\r\n#34 0x00007f7a6daea0bd in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\r\n#35 0x00007f7a6daea1c2 in PyEval_EvalCode () from /lib64/libpython2.7.so.1.0\r\n#36 0x00007f7a6db035ff in run_mod () from /lib64/libpython2.7.so.1.0\r\n#37 0x00007f7a6db047be in PyRun_FileExFlags () from /lib64/libpython2.7.so.1.0\r\n#38 0x00007f7a6db05a49 in PyRun_SimpleFileExFlags () from /lib64/libpython2.7.so.1.0\r\n#39 0x00007f7a6db16b9f in Py_Main () from /lib64/libpython2.7.so.1.0\r\n#40 0x00007f7a6cd43b15 in __libc_start_main (main=0x4006f0 <main>, argc=8, ubp_av=0x7fffc3fae458, init=<optimized out>, fini=<optimized out>, rtld_fini=<optimized out>, stack_end=0x7fffc3fae448)\r\n    at libc-start.c:274\r\n#41 0x0000000000400721 in _start ()\r\n```", "comments": ["@tianyapiaozi @petewarden  Hello, I too am facing a similar issue when trying to freeze a 2.1G saved model. Any updates on how to solve this or how can it be prevented?", "Same noise here when I try to move to a really large dataset. Can't provide exact sizes, unfortunately. Rank amateur playing with a canned estimator.\r\n\r\n```\r\nINFO:tensorflow:Calling model_fn.\r\nWARNING:tensorflow:From /home/ec2-user/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/clip_ops.py:113: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nkeep_dims is deprecated, use keepdims instead\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\n[libprotobuf FATAL google/protobuf/wire_format.cc:830] CHECK failed: (output->ByteCount()) == (expected_endpoint): : Protocol message serialized to a size different from what was originally expected.  Perhaps it was modified by another thread during serialization?\r\nterminate called after throwing an instance of 'google::protobuf::FatalException'\r\n  what():  CHECK failed: (output->ByteCount()) == (expected_endpoint): : Protocol message serialized to a size different from what was originally expected.  Perhaps it was modified by another thread during serialization?\r\nAborted\r\n```", "Unfortunately we're not likely to be able to fix this soon, since protobuf has some built-in limits around a maximum size of 2GB. Closing as infeasible."]}, {"number": 20503, "title": "Andrewharp patch 1", "body": "", "comments": ["it is nice!", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I think this PR was likely a mistake. Please reopen a clean one if there's a patch in here."]}, {"number": 20502, "title": "TensorFlow master build failure on s390x", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: s390x Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: master\r\n- **Python version**:  2.7.12\r\n- **Bazel version (if compiling from source)**: 0.12.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: NA\r\n\r\n### Describe the problem\r\nTensorFlow Master build on s390x failed due to BoringSSL again.\r\nChanges causing build failure can be checked [here](https://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_CI/179/)\r\n\r\n@gunan , Could you please have a look?\r\n\r\n### Source code / logs\r\n```\r\nERROR: /home/jenkins/.cache/bazel/_bazel_jenkins/14d9bef57f8e4d2a0eef0de174c4144b/external/boringssl/BUILD:128:1: C++ compilation of rule '@boringssl//:ssl' failed (Exit 1)\r\nIn file included from external/boringssl/src/include/openssl/ssl.h:145:0,\r\n                 from external/boringssl/src/ssl/s3_lib.cc:149:\r\nexternal/boringssl/src/include/openssl/base.h:114:2: error: #error \"Unknown target CPU\"\r\n #error \"Unknown target CPU\"\r\n```\r\n", "comments": ["I think the root cause is https://github.com/tensorflow/tensorflow/commit/42c6133b93ef8eabb6c5cb00ab8bbe90a519dfc4", "@saeta, could you please have a look?", "Nagging Assignee @bignamehyp: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @bignamehyp: It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 20501, "title": "Enable crc32c acceleration for MSVC", "body": "Use `TestCPUFeature` instead of `__builtin_cpu_supports` to dynamically detect SSE4.2 support.\r\nThis will allow us to enable crc32c acceleration for MSVC and old Apple clang.", "comments": ["@martinwicke @meteorcloudy Can I get CI test for this PR, https://github.com/tensorflow/tensorflow/pull/20538, https://github.com/tensorflow/tensorflow/pull/20537 and https://github.com/tensorflow/tensorflow/pull/20541? They already get LGTM-ed.", "There were errors in the build, can you look at those?", "@rongjiecomputer, please take a look for the test failure and update the PR.", "Currently a bit busy with other matters, I will revisit this PR when I have the free time again.", "Nagging Assignee @martinwicke: It has been 17 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "np, let us know.", "I will close this PR for now -- please reopen once you have time to look at it. Thanks!"]}, {"number": 20500, "title": "problem while compiling as c++ lib (libtensorflow_cc.so)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**:  2.7.12\r\n- **Bazel version (if compiling from source)**: 0.14.1\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: 9.0/7.1.2\r\n- **GPU model and memory**: GTX850M 2GB\r\n- **Exact command to reproduce**:\r\nsudo apt-get purge eigen* && sudo apt-get install libeigen3-dev\r\n(move eigen3 to /usr/local/ )\r\n\r\ngit clone https://github.com/google/protobuf.git\r\n./autogen.sh && ./configure && make && sudo make install\r\n\r\ngit clone --recurse-submodules https://github.com/tensorflow/tensorflow.git -b r1.8\r\n./configure\r\nbazel build //tensorflow:libtensorflow_cc.so\r\n(make soft link to libs and includes)\r\n\r\n### Describe the problem\r\nfirstly, shows that lost lots of *.pb.h,\r\nand I generate files like 'protoc --cpp_out=./  tensorflow/core/protobuf/\\*.proto', so on other folders.\r\n\r\nafter that I got lots of error like image shows below:\r\n\r\n![2018-07-03 13-34-24](https://user-images.githubusercontent.com/12708080/42200462-ee7205a2-7ec5-11e8-9ae6-f77cbca15e7d.png)\r\n\r\nafter adding `-std=c++11` to `CFLAGS` in Makefile...\r\n\r\n![2018-07-03 13-36-55](https://user-images.githubusercontent.com/12708080/42200504-2ac3d666-7ec6-11e8-8b72-abfd1e592e7d.png)\r\n\r\ncomment the `template<>` line...\r\n\r\nand... now I cant understand that why highest() and lowest() can passed, but infinity() and quiet_NaN() failed. whats the difference between them in coding layer?\r\n![2018-07-03 14-22-52](https://user-images.githubusercontent.com/12708080/42201994-9738cc56-7ecc-11e8-881c-b4629e30a46d.png)\r\n\r\ncomment them too...\r\nnow it cant find c++5 files\r\n![2018-07-03 15-09-20](https://user-images.githubusercontent.com/12708080/42204043-24109112-7ed3-11e8-9ec0-6a1a078015a4.png)\r\n\r\n\r\nand here's my Makefile\r\n\r\n```\r\nGPU=1\r\nCUDNN=1\r\nCUDNN_HALF=0\r\nOPENCV=1\r\nAVX=0\r\nOPENMP=0\r\nLIBSO=1\r\nEXECT=1\r\n\r\n# set GPU=1 and CUDNN=1 to speedup on GPU\r\n# set CUDNN_HALF=1 to further speedup 3 x times (Mixed-precision using Tensor Cores) on GPU Tesla V100, Titan V, DGX-2\r\n# set AVX=1 and OPENMP=1 to speedup on CPU (if error occurs then set AVX=0)\r\n\r\nDEBUG=0\r\n\r\nARCH= -gencode arch=compute_50,code=[sm_50,compute_50]\r\n\r\nOS := $(shell uname)\r\n\r\n# Tesla V100\r\n# ARCH= -gencode arch=compute_70,code=[sm_70,compute_70]\r\n\r\n# GTX 1080, GTX 1070, GTX 1060, GTX 1050, GTX 1030, Titan Xp, Tesla P40, Tesla P4\r\n# ARCH= -gencode arch=compute_61,code=sm_61 -gencode arch=compute_61,code=compute_61\r\n\r\n# GP100/Tesla P100 \\96 DGX-1\r\n# ARCH= -gencode arch=compute_60,code=sm_60\r\n\r\n# For Jetson Tx1 uncomment:\r\n# ARCH= -gencode arch=compute_51,code=[sm_51,compute_51]\r\n\r\n# For Jetson Tx2 or Drive-PX2 uncomment:\r\n# ARCH= -gencode arch=compute_62,code=[sm_62,compute_62]\r\n\r\n\r\nVPATH=./darkSrc/ ./feature/ ./matching/ ./thirdPart/ ./thirdPart/munkres/ ./thirdPart/munkres/adapters/\r\nEXEC=deepsort\r\nOBJDIR=./obj/\r\n\r\nifeq ($(LIBSO), 1)\r\nLIBNAMESO=deepsort.so\r\nAPPNAMESO=uselib\r\nendif\r\n\r\nCC=gcc\r\nCPP=g++\r\nNVCC=/usr/local/cuda/bin/nvcc\r\nOPTS=-Ofast\r\nLDFLAGS= -lm -pthread\r\nCOMMON=\r\nCFLAGS=-w -std=c++11\r\n#-Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas\r\n\r\nifeq ($(DEBUG), 1)\r\nOPTS= -O0 -g\r\nelse\r\nifeq ($(AVX), 1)\r\nCFLAGS+= -ffp-contract=fast -mavx -msse4.1 -msse4a\r\nendif\r\nendif\r\n\r\nCFLAGS+=$(OPTS)\r\n\r\nifeq ($(OPENCV), 1)\r\nCOMMON+= -DOPENCV\r\nCFLAGS+= -DOPENCV\r\nLDFLAGS+= `pkg-config --libs tensorflow opencv`\r\nCOMMON+= `pkg-config --cflags tensorflow opencv`\r\nendif\r\n\r\nifeq ($(OPENMP), 1)\r\nCFLAGS+= -fopenmp\r\nLDFLAGS+= -lgomp\r\nendif\r\n\r\nifeq ($(GPU), 1)\r\nCOMMON+= -DGPU -I/usr/local/cuda/include/\r\nCFLAGS+= -DGPU\r\nifeq ($(OS),Darwin) #MAC\r\nLDFLAGS+= -L/usr/local/cuda/lib -lcuda -lcudart -lcublas -lcurand\r\nelse\r\nLDFLAGS+= -L/usr/local/cuda/lib64 -lcuda -lcudart -lcublas -lcurand\r\nendif\r\nendif\r\n\r\nifeq ($(CUDNN), 1)\r\nCOMMON+= -DCUDNN\r\nifeq ($(OS),Darwin) #MAC\r\nCFLAGS+= -DCUDNN -I/usr/local/cuda/include\r\nLDFLAGS+= -L/usr/local/cuda/lib -lcudnn\r\nelse\r\nCFLAGS+= -DCUDNN -I/usr/local/cudnn/include\r\nLDFLAGS+= -L/usr/local/cudnn/lib64 -lcudnn\r\nendif\r\nendif\r\n\r\nifeq ($(CUDNN_HALF), 1)\r\nCOMMON+= -DCUDNN_HALF\r\nCFLAGS+= -DCUDNN_HALF\r\nARCH+= -gencode arch=compute_70,code=[sm_70,compute_70]\r\nendif\r\n\r\nOBJ= activation_layer.o activations.o avgpool_layer.o batchnorm_layer.o blas.o box.o col2im.o connected_layer.o convolutional_layer.o cost_layer.o crnn_layer.o crop_layer.o cuda.o data.o deconvolutional_layer.o demo.o detection_layer.o dropout_layer.o gemm.o gettimeofday.o gru_layer.o im2col.o image.o layer.o list.o local_layer.o matrix.o maxpool_layer.o network.o normalization_layer.o option_list.o parser.o region_layer.o reorg_layer.o rnn_layer.o route_layer.o shortcut_layer.o softmax_layer.o tree.o utils.o\r\nifeq ($(GPU), 1)\r\nLDFLAGS+= -lstdc++\r\nOBJ+= convolutional_kernels.o activation_kernels.o im2col_kernels.o col2im_kernels.o blas_kernels.o crop_layer_kernels.o dropout_layer_kernels.o maxpool_layer_kernels.o network_kernels.o avgpool_layer_kernels.o\r\nendif\r\n\r\nOBJS = $(addprefix $(OBJDIR), $(OBJ))\r\nDEPS = $(wildcard darkSrc/*.h) Makefile\r\n\r\nall: obj backup results $(EXEC) $(LIBNAMESO) $(APPNAMESO)\r\n\r\nifeq ($(LIBSO), 1)\r\nCFLAGS+= -fPIC\r\n\r\n$(LIBNAMESO): $(OBJS) ./errmsg.hpp ./VideoTracker.hpp ./errmsg.cpp ./VideoTracker.cpp ./feature/dataType.hpp ./feature/FeatureTensor.hpp ./feature/model.hpp ./matching/kalmanfilter.hpp ./matching/linear_assignment.hpp ./matching/nn_matching.hpp ./matching/tracker.hpp ./matching/track.hpp ./feature/FeatureTensor.cpp ./feature/model.cpp ./matching/kalmanfilter.cpp ./matching/linear_assignment.cpp ./matching/nn_matching.cpp ./matching/track.cpp ./matching/tracker.cpp ./thirdPart/hungarianoper.cpp\r\n\t$(CPP) -shared -std=c++11 -fvisibility=hidden -DYOLODLL_EXPORTS $(COMMON) $(CFLAGS) $(OBJS) ./errmsg.cpp ./VideoTracker.cpp ./feature/FeatureTensor.cpp ./feature/model.cpp ./matching/kalmanfilter.cpp ./matching/linear_assignment.cpp ./matching/nn_matching.cpp ./matching/track.cpp ./matching/tracker.cpp ./thirdPart/hungarianoper.cpp -o $@ $(LDFLAGS)\r\n\r\n$(APPNAMESO): $(LIBNAMESO) ./*.hpp ./*/*.hpp ./main.cpp\r\n\t$(CPP) -std=c++11 $(COMMON) $(CFLAGS) -o $@  ./main.cpp $(LDFLAGS) -L ./ -l:$(LIBNAMESO)\r\nendif\r\n\r\nifeq ($(EXECT), 1)\r\n$(EXEC): $(OBJS)\r\n\t$(CPP) $(COMMON) $(CFLAGS) $^ -o $@ $(LDFLAGS)\r\nendif\r\n\r\n$(OBJDIR)%.o: %.c $(DEPS)\r\n\t$(CC) $(COMMON) $(CFLAGS) -c $< -o $@\r\n\r\n$(OBJDIR)%.o: %.cpp $(DEPS)\r\n\t$(CPP) $(COMMON) $(CFLAGS) -c $< -o $@\r\n\r\n$(OBJDIR)%.o: %.cu $(DEPS)\r\n\t$(NVCC) $(ARCH) $(COMMON) --compiler-options \"$(CFLAGS)\" -c $< -o $@\r\n\r\nobj:\r\n\tmkdir -p obj\r\nbackup:\r\n\tmkdir -p backup\r\nresults:\r\n\tmkdir -p results\r\n\r\n.PHONY: clean\r\n\r\nvalgrind:\r\n\tVALGRIND=\"valgrind --log-file=valgrind-%p.log\" $(MAKE)\r\n\r\nclean:\r\n\trm -rf $(OBJS) $(EXEC) $(LIBNAMESO) $(APPNAMESO)\r\n```\r\n", "comments": ["I have the same questions! Did anyone find solutions?", "In addition, I set up the tensorflow's environment in China. Because of GFW, some library wasn't the newest. Maybe that's the reason?", "I try to build libtensorflow_cc.so again. It didn't pass, and here's many new error.\r\n\r\n> \r\n\r\n> ~/tensorflow $ bazel build --config=monolithic --config=cuda -c opt --verbose_failures //tensorflow:libtensorflow_cc.so\r\n> WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\n> INFO: Analysed target //tensorflow:libtensorflow_cc.so (0 packages loaded).\r\n> INFO: Found 1 target...\r\n> INFO: From ProtoCompile tensorflow/core/grappler/costs/op_performance_data.pb.cc:\r\n> bazel-out/k8-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.\r\n> bazel-out/k8-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.\r\n> INFO: From ProtoCompile tensorflow/core/example/example.pb.cc:\r\n> bazel-out/k8-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.\r\n> ERROR: /home/aimhabo/tensorflow/tensorflow/core/BUILD:221:1: C++ compilation of rule '//tensorflow/core:protos_all_cc_impl' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n>   (cd /home/aimhabo/.cache/bazel/_bazel_aimhabo/94b6a27f9d9a278b4559106953d7303d/execroot/org_tensorflow && \\\r\n>   exec env - \\\r\n>     LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/home/aimhabo/nonmine-craft/distro/install/lib: \\\r\n>     PATH=/home/aimhabo/bin:/usr/local/cuda/bin:/home/aimhabo/nonmine-craft/distro/install/bin:/home/aimhabo/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \\\r\n>     PWD=/proc/self/cwd \\\r\n>   external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -MD -MF bazel-out/host/bin/tensorflow/core/_objs/protos_all_cc_impl/tensorflow/core/framework/iterator.pb.d '-frandom-seed=bazel-out/host/bin/tensorflow/core/_objs/protos_all_cc_impl/tensorflow/core/framework/iterator.pb.o' -iquote . -iquote bazel-out/host/genfiles -iquote external/protobuf_archive -iquote bazel-out/host/genfiles/external/protobuf_archive -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -isystem external/protobuf_archive/src -isystem bazel-out/host/genfiles/external/protobuf_archive/src -isystem bazel-out/host/bin/external/protobuf_archive/src -g0 -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -g0 -Wno-unknown-warning-option -Wno-unused-but-set-variable -Wno-sign-compare -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fno-canonical-system-headers -c bazel-out/host/genfiles/tensorflow/core/framework/iterator.pb.cc -o bazel-out/host/bin/tensorflow/core/_objs/protos_all_cc_impl/tensorflow/core/framework/iterator.pb.o)\r\n> In file included from bazel-out/host/genfiles/tensorflow/core/framework/iterator.pb.cc:4:0:\r\n> ./tensorflow/core/framework/iterator.pb.h:12:2: error: #error This file was generated by a newer version of protoc which is\r\n>  #error This file was generated by a newer version of protoc which is\r\n>   ^\r\n> ./tensorflow/core/framework/iterator.pb.h:13:2: error: #error incompatible with your Protocol Buffer headers. Please update\r\n>  #error incompatible with your Protocol Buffer headers.  Please update\r\n>   ^\r\n> ./tensorflow/core/framework/iterator.pb.h:14:2: error: #error your headers.\r\n>  #error your headers.\r\n>   ^\r\n> In file included from bazel-out/host/genfiles/tensorflow/core/framework/iterator.pb.cc:4:0:\r\n> ./tensorflow/core/framework/iterator.pb.h:215:41: error: 'CachedSize' in namespace 'google::protobuf::internal' does not name a type\r\n>    mutable ::google::protobuf::internal::CachedSize _cached_size_;\r\n>                                          ^\r\n> ./tensorflow/core/framework/iterator.pb.h: In member function 'virtual tensorflow::IteratorStateMetadata* tensorflow::IteratorStateMetadata::New() const':\r\n> ./tensorflow/core/framework/iterator.pb.h:114:12: error: 'CreateMaybeMessage' was not declared in this scope\r\n>      return CreateMaybeMessage<IteratorStateMetadata>(NULL);\r\n>             ^\r\n> ./tensorflow/core/framework/iterator.pb.h:114:52: error: expected primary-expression before '>' token\r\n>      return CreateMaybeMessage<IteratorStateMetadata>(NULL);\r\n>                                                     ^\r\n> ./tensorflow/core/framework/iterator.pb.h: In member function 'virtual tensorflow::IteratorStateMetadata* tensorflow::IteratorStateMetadata::New(google::protobuf::Arena*) const':\r\n> ./tensorflow/core/framework/iterator.pb.h:118:12: error: 'CreateMaybeMessage' was not declared in this scope\r\n>      return CreateMaybeMessage<IteratorStateMetadata>(arena);\r\n>             ^\r\n> ./tensorflow/core/framework/iterator.pb.h:118:52: error: expected primary-expression before '>' token\r\n>      return CreateMaybeMessage<IteratorStateMetadata>(arena);\r\n>                                                     ^\r\n> ./tensorflow/core/framework/iterator.pb.h: In member function 'virtual int tensorflow::IteratorStateMetadata::GetCachedSize() const':\r\n> ./tensorflow/core/framework/iterator.pb.h:134:44: error: '_cached_size_' was not declared in this scope\r\n>    int GetCachedSize() const final { return _cached_size_.Get(); }\r\n>                                             ^\r\n> bazel-out/host/genfiles/tensorflow/core/framework/iterator.pb.cc: In function 'void protobuf_tensorflow_2fcore_2fframework_2fiterator_2eproto::AddDescriptorsImpl()':\r\n> bazel-out/host/genfiles/tensorflow/core/framework/iterator.pb.cc:90:16: error: 'InitDefaults' was not declared in this scope\r\n>    InitDefaults();\r\n>                 ^\r\n> bazel-out/host/genfiles/tensorflow/core/framework/iterator.pb.cc: In copy constructor 'tensorflow::IteratorStateMetadata::IteratorStateMetadata(const tensorflow::IteratorStateMetadata&)':\r\n> bazel-out/host/genfiles/tensorflow/core/framework/iterator.pb.cc:147:7: error: class 'tensorflow::IteratorStateMetadata' does not have any field named '_cached_size_'\r\n>        _cached_size_(0) {\r\n>        ^\r\n> bazel-out/host/genfiles/tensorflow/core/framework/iterator.pb.cc: In member function 'void tensorflow::IteratorStateMetadata::SharedCtor()':\r\n> bazel-out/host/genfiles/tensorflow/core/framework/iterator.pb.cc:159:3: error: '_cached_size_' was not declared in this scope\r\n>    _cached_size_ = 0;\r\n>    ^\r\n> bazel-out/host/genfiles/tensorflow/core/framework/iterator.pb.cc: In member function 'virtual void tensorflow::IteratorStateMetadata::SetCachedSize(int) const':\r\n> bazel-out/host/genfiles/tensorflow/core/framework/iterator.pb.cc:180:3: error: '_cached_size_' was not declared in this scope\r\n>    _cached_size_ = size;\r\n>    ^\r\n> bazel-out/host/genfiles/tensorflow/core/framework/iterator.pb.cc: In member function 'virtual size_t tensorflow::IteratorStateMetadata::ByteSizeLong() const':\r\n> bazel-out/host/genfiles/tensorflow/core/framework/iterator.pb.cc:363:3: error: '_cached_size_' was not declared in this scope\r\n>    _cached_size_ = cached_size;\r\n>    ^\r\n> bazel-out/host/genfiles/tensorflow/core/framework/iterator.pb.cc: In member function 'void tensorflow::IteratorStateMetadata::InternalSwap(tensorflow::IteratorStateMetadata*)':\r\n> bazel-out/host/genfiles/tensorflow/core/framework/iterator.pb.cc:438:8: error: '_cached_size_' was not declared in this scope\r\n>    swap(_cached_size_, other->_cached_size_);\r\n>         ^\r\n> bazel-out/host/genfiles/tensorflow/core/framework/iterator.pb.cc:438:30: error: 'class tensorflow::IteratorStateMetadata' has no member named '_cached_size_'\r\n>    swap(_cached_size_, other->_cached_size_);\r\n>                               ^\r\n> At global scope:\r\n> cc1plus: warning: unrecognized command line option '-Wno-unknown-warning-option'\r\n> Target //tensorflow:libtensorflow_cc.so failed to build\r\n> INFO: Elapsed time: 3.442s, Critical Path: 2.25s\r\n> INFO: 4 processes, local.\r\n> FAILED: Build did NOT complete successfully\r\n> ", "@sd707589 My previous failure is caused by \" compling first, and then fill up \\*.pb.\\* \".\r\nso the right way is \"protoc\" for every \"\\*.pb.\\*\" file first, and then bazeling.\r\nThat's what principle is, but the re-bazeling did't pass yet", "@aimhabo I'm sorry, but I don't understand your most recent comment. Are you saying that you've resolved the issue but were waiting for bazel to complete? If so, did your compilation complete successfully? Or is something else still broken?", "@cy89 haven't yet.\r\nI bazeling tensorflow(r1.10) nomarly. and still 'cannot find xx.pb.h'\r\n![2018-07-22 11-43-36](https://user-images.githubusercontent.com/12708080/43042039-897fa27e-8da4-11e8-9c95-57518e80517e.png)\r\n\r\nand then, I run `tensorflow/tensorflow/contrib/makefile/build_all_linux.sh`, here comes \r\n> sh tensorflow/contrib/makefile/build_all_linux.sh \r\n> tensorflow/contrib/makefile/build_all_linux.sh: 22: tensorflow/contrib/makefile/build_all_linux.sh: Bad substitution\r\n> tensorflow/contrib/makefile/build_all_linux.sh: 25: tensorflow/contrib/makefile/build_all_linux.sh: source: not found\r\n\r\nand just try to `protoc`(3.6.x), `bazel clean`, `bazel build --config=monlithic :libtensorflow_cc.so`\r\n\r\n> INFO: From ProtoCompile tensorflow/core/grappler/costs/op_performance_data.pb.cc:\r\n> bazel-out/k8-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.\r\n> bazel-out/k8-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.\r\n> bazel-out/k8-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.\r\n> bazel-out/k8-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.\r\n> INFO: From ProtoCompile tensorflow/core/lib/core/error_codes.pb.cc:\r\n> bazel-out/k8-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.\r\n> ERROR: /home/aimhabo/tensorflow/tensorflow/core/debug/BUILD:64:1: C++ compilation of rule '//tensorflow/core/debug:debugger_event_metadata_proto_cc' failed (Exit 1)\r\n> In file included from bazel-out/k8-opt/genfiles/tensorflow/core/debug/debugger_event_metadata.pb.cc:4:0:\r\n> ./tensorflow/core/debug/debugger_event_metadata.pb.h:17:2: error: #error This file was generated by an older version of protoc which is\r\n>  #error This file was generated by an older version of protoc which is\r\n>   ^\r\n> ./tensorflow/core/debug/debugger_event_metadata.pb.h:18:2: error: #error incompatible with your Protocol Buffer headers. Please\r\n>  #error incompatible with your Protocol Buffer headers.  Please\r\n>   ^\r\n> ./tensorflow/core/debug/debugger_event_metadata.pb.h:19:2: error: #error regenerate this file with a newer version of protoc.\r\n>  #error regenerate this file with a newer version of protoc.\r\n>   ^\r\n> In file included from bazel-out/k8-opt/genfiles/tensorflow/core/debug/debugger_event_metadata.pb.cc:4:0:\r\n> ./tensorflow/core/debug/debugger_event_metadata.pb.h:103:10: error: 'PROTOBUF_CONSTEXPR' does not name a type\r\n>    static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =\r\n>           ^\r\n> bazel-out/k8-opt/genfiles/tensorflow/core/debug/debugger_event_metadata.pb.cc: In function 'void protobuf_tensorflow_2fcore_2fdebug_2fdebugger_5fevent_5fmetadata_2eproto::InitDefaultsDebuggerEventMetadata()':\r\n> bazel-out/k8-opt/genfiles/tensorflow/core/debug/debugger_event_metadata.pb.cc:36:47: error: 'void protobuf_tensorflow_2fcore_2fdebug_2fdebugger_5fevent_5fmetadata_2eproto::InitDefaultsDebuggerEventMetadata()' was declared 'extern' and later 'static' [-fpermissive]\r\n>  static void InitDefaultsDebuggerEventMetadata() {\r\n>                                                ^\r\n> In file included from bazel-out/k8-opt/genfiles/tensorflow/core/debug/debugger_event_metadata.pb.cc:4:0:\r\n> ./tensorflow/core/debug/debugger_event_metadata.pb.h:46:6: note: previous declaration of 'void protobuf_tensorflow_2fcore_2fdebug_2fdebugger_5fevent_5fmetadata_2eproto::InitDefaultsDebuggerEventMetadata()'\r\n>  void InitDefaultsDebuggerEventMetadata();\r\n>       ^\r\n> bazel-out/k8-opt/genfiles/tensorflow/core/debug/debugger_event_metadata.pb.cc: In function 'void protobuf_tensorflow_2fcore_2fdebug_2fdebugger_5fevent_5fmetadata_2eproto::InitDefaults()':\r\n> bazel-out/k8-opt/genfiles/tensorflow/core/debug/debugger_event_metadata.pb.cc:50:6: error: redefinition of 'void protobuf_tensorflow_2fcore_2fdebug_2fdebugger_5fevent_5fmetadata_2eproto::InitDefaults()'\r\n>  void InitDefaults() {\r\n>       ^\r\n> In file included from bazel-out/k8-opt/genfiles/tensorflow/core/debug/debugger_event_metadata.pb.cc:4:0:\r\n> ./tensorflow/core/debug/debugger_event_metadata.pb.h:47:13: note: 'void protobuf_tensorflow_2fcore_2fdebug_2fdebugger_5fevent_5fmetadata_2eproto::InitDefaults()' previously defined here\r\n>  inline void InitDefaults() {\r\n>              ^\r\n> bazel-out/k8-opt/genfiles/tensorflow/core/debug/debugger_event_metadata.pb.cc: In member function 'virtual void third_party::tensorflow::core::debug::DebuggerEventMetadata::SetCachedSize(int) const':\r\n> bazel-out/k8-opt/genfiles/tensorflow/core/debug/debugger_event_metadata.pb.cc:173:17: error: request for member 'Set' in '((const third_party::tensorflow::core::debug::DebuggerEventMetadata*)this)->third_party::tensorflow::core::debug::DebuggerEventMetadata::_cached_size_', which is of non-class type 'int'\r\n>    _cached_size_.Set(size);\r\n>                  ^\r\n> bazel-out/k8-opt/genfiles/tensorflow/core/debug/debugger_event_metadata.pb.cc: In static member function 'static const google::protobuf::Descriptor* third_party::tensorflow::core::debug::DebuggerEventMetadata::descriptor()':\r\n> bazel-out/k8-opt/genfiles/tensorflow/core/debug/debugger_event_metadata.pb.cc:177:106: error: 'kIndexInFileMessages' was not declared in this scope\r\n>    return ::protobuf_tensorflow_2fcore_2fdebug_2fdebugger_5fevent_5fmetadata_2eproto::file_level_metadata[kIndexInFileMessages].descriptor;\r\n>                                                                                                           ^\r\n> bazel-out/k8-opt/genfiles/tensorflow/core/debug/debugger_event_metadata.pb.cc: In member function 'virtual google::protobuf::Metadata third_party::tensorflow::core::debug::DebuggerEventMetadata::GetMetadata() const':\r\n> bazel-out/k8-opt/genfiles/tensorflow/core/debug/debugger_event_metadata.pb.cc:479:106: error: 'kIndexInFileMessages' was not declared in this scope\r\n>    return ::protobuf_tensorflow_2fcore_2fdebug_2fdebugger_5fevent_5fmetadata_2eproto::file_level_metadata[kIndexInFileMessages];\r\n>                                                                                                           ^\r\n> At global scope:\r\n> cc1plus: warning: unrecognized command line option '-Wno-unknown-warning-option'\r\n> Target //tensorflow:libtensorflow_cc.so failed to build\r\n> Use --verbose_failures to see the command lines of failed build steps.\r\n> INFO: Elapsed time: 374.035s, Critical Path: 14.86s\r\n> INFO: 1450 processes, local.\r\n> FAILED: Build did NOT complete successfully\r\n> \r\nIt seems that I use wrong protobuf again. I use protobuf 3.6.x now. what's the correct version?", "I have the same issue on 1.10.0 release. I don't see the problem with 1.9.0 release. My cuda version is 9.2, cuDNN is 7.1.4. Try both protobuf 3.5 and 3.6, same issues. Any solution?", "@aimhabo and @tho15 it looks to me like protobuf 3.6.1 is the most recent version: \r\n#define GOOGLE_PROTOBUF_MIN_PROTOC_VERSION 3006001\r\n\r\nCan you please try 3.6.1, just to see if you've run into some micro-versioning error?", "@cy89 I try all versions after 3.5.0 and got the same issue. The errors when using 3.5.0 are:\r\n\r\n/home/tho/Development/Tools/tensorflow-1.10.0/bazel-genfiles/tensorflow/core/protobuf/config.pb.h: In member function \u2018tensorflow::ConfigProto_Experimental* tensorflow::ConfigProto::mutable_experimental()\u2019:\r\n/home/tho/Development/Tools/tensorflow-1.10.0/bazel-genfiles/tensorflow/core/protobuf/config.pb.h:4172:15: error: \u2018CreateMaybeMessage\u2019 was not declared in this scope\r\n     auto* p = CreateMaybeMessage<::tensorflow::ConfigProto_Experimental>(GetArenaNoVirtual());\r\n               ^~~~~~~~~~~~~~~~~~\r\n/home/tho/Development/Tools/tensorflow-1.10.0/bazel-genfiles/tensorflow/core/protobuf/config.pb.h:4172:72: error: expected primary-expression before \u2018>\u2019 token\r\n  auto* p = CreateMaybeMessage<::tensorflow::ConfigProto_Experimental>(GetArenaNoVirtual());\r\n                                                                     ^\r\n/home/tho/Development/Tools/tensorflow-1.10.0/bazel-genfiles/tensorflow/core/protobuf/config.pb.h: In member function \u2018tensorflow::DebugOptions* tensorflow::RunOptions::release_debug_options()\u2019:\r\n/home/tho/Development/Tools/tensorflow-1.10.0/bazel-genfiles/tensorflow/core/protobuf/config.pb.h:4294:65: error: no matching function for call to \u2018DuplicateIfNonNull(tensorflow::DebugOptions*&)\u2019\r\n     temp = ::google::protobuf::internal::DuplicateIfNonNull(temp);\r\n\r\nThanks.", "@tho15 Try it 3.6.1 (delete protoc in every ../bin && git clone https://github.com/google/protobuf.git --recursive -b3.6.x)\r\nI haven't had time to test this yet.", "@aimhabo I have tested with 3.6.1 version, same issue. Let me know if you have tested it.", "Hey all,\r\nI have same error message within a slightly different scenario. Wrote a long description of the issue in [Stack Overflow](https://stackoverflow.com/questions/51965775/c-compiling-project-with-shared-object-libtensorflow-cc-so-failed).\r\nIt would be nice if someone can take a short look to help me out getting my project running.\r\n\r\nThank you for your support!", "@aimhabo can you please inspect or post the file that generated the \"#error This file was generated by an older version of protoc which is\" message? I'd expect the protobuf version in that number to be <3.6.1, which means that either that file is lying around from a previous build, or you still have mixed protobuf versions somewhere on your system.\r\n", "@cy89 yes I found that code, so I tried to delete all 'protoc' and reinstall https://github.com/protocolbuffers/protobuf . And then 'protoc --version' said its 3.6.1 now.\r\nI manually download and compile the installation-dependencies, according to the links in download_dependencies.sh, which show up when running sh directly, and comment out the relevant lines in download_dependencies.sh to avoid overwriting.\r\nProblems still is that \"1. some files protoc failed because of variables (I forgot what kind of mistake.); 2. bazel failed \"\r\n(Last attempt at the end of July)", "Nagging Assignee @cy89: It has been 45 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 20499, "title": "AttributeError: module 'tensorflow.python.training.checkpointable' has no attribute 'CheckpointableBase'", "body": "from __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nfrom matplotlib.patches import Ellipse\r\nimport seaborn as sns\r\n\r\nimport tensorflow as tf                            # importing Tensorflow\r\nimport tensorflow_probability as tfp               # and Tensorflow probability\r\nfrom tensorflow_probability import edward2 as ed   # Edwardlib extension\r\n\r\ntfd = tfp.distributions             # Basic probability distribution toolkit\r\ntfb = tfp.distributions.bijectors   # and their modifiers\r\n\r\n# Eager Execution\r\n# tfe = tf.contrib.eager\r\n# tfe.enable_eager_execution()\r\n\r\n%matplotlib inline\r\nplt.style.use(\"fivethirtyeight\")        # Styling plots like FiveThirtyEight\r\n\r\nimport warnings\r\nwarnings.filterwarnings('ignore')\r\n%config InlineBackend.figure_format=\"retina\" # improves resolution of plots\r\n\r\n> ### it's my source.\r\n> but i have a problem. I installed tensorflow and my tensorflow version is 1.8.0.\r\n> But that source has a error. I don't know what my source is error.\r\n> \r\n\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-6-86bc64cca59d> in <module>()\r\n      8 import seaborn as sns\r\n      9 \r\n---> 10 import tensorflow as tf                            # importing Tensorflow\r\n     11 import tensorflow_probability as tfp               # and Tensorflow probability\r\n     12 from tensorflow_probability import edward2 as ed   # Edwardlib extension\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py in <module>()\r\n     22 \r\n     23 # pylint: disable=g-bad-import-order\r\n---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n     25 # pylint: disable=wildcard-import\r\n     26 from tensorflow.tools.api.generator.api import *  # pylint: disable=redefined-builtin\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/__init__.py in <module>()\r\n     61 \r\n     62 # Framework\r\n---> 63 from tensorflow.python.framework.framework_lib import *  # pylint: disable=redefined-builtin\r\n     64 from tensorflow.python.framework.versions import *\r\n     65 from tensorflow.python.framework import errors\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/framework_lib.py in <module>()\r\n    102 from tensorflow.python.framework.random_seed import set_random_seed\r\n    103 from tensorflow.python.framework.sparse_tensor import convert_to_tensor_or_sparse_tensor\r\n--> 104 from tensorflow.python.framework.importer import import_graph_def\r\n    105 \r\n    106 # Utilities for working with Tensors\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/importer.py in <module>()\r\n     30 from tensorflow.python.framework import dtypes\r\n     31 from tensorflow.python.framework import errors\r\n---> 32 from tensorflow.python.framework import function\r\n     33 from tensorflow.python.framework import op_def_registry\r\n     34 from tensorflow.python.framework import ops\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/function.py in <module>()\r\n     34 from tensorflow.python.framework import ops\r\n     35 from tensorflow.python.ops import array_ops\r\n---> 36 from tensorflow.python.ops import resource_variable_ops\r\n     37 from tensorflow.python.ops import variable_scope as vs\r\n     38 from tensorflow.python.util import compat\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py in <module>()\r\n     33 from tensorflow.python.ops import gen_state_ops\r\n     34 from tensorflow.python.ops import math_ops\r\n---> 35 from tensorflow.python.ops import variables\r\n     36 # go/tf-wildcard-import\r\n     37 # pylint: disable=wildcard-import\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variables.py in <module>()\r\n     38 \r\n     39 @tf_export(\"Variable\")\r\n---> 40 class Variable(checkpointable.CheckpointableBase):\r\n     41   \"\"\"See the @{$variables$Variables How To} for a high level overview.\r\n     42 \r\n\r\nAttributeError: module 'tensorflow.python.training.checkpointable' has no attribute 'CheckpointableBase'\r\n\r\n\r\n> it's my error...\r\n> \r\n> help me pleazzzzzzzzzzzzzz\r\n> ", "comments": ["Sounds like a half-updated version of TensorFlow. Could you try removing TensorFlow entirely (e.g. `pip uninstall tf-nightly` or whichever package is installed), making sure `import tensorflow` fails, then reinstalling?", "Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Did the solution suggested by @allenlavoie work?", "Note that what likely amounts to the exact same issue (albeit on Windows) has been posted as a [question on StackOverflow](https://stackoverflow.com/q/51390692/5085211).", "Reinstalling tensorflow works. ", "Great! I will close the issue thanks.", "No reinstalling did not worked for me", "D:\\RasaChatBot_Workspace\\RasaChatBot\\main\\rasa_core_api>pip uninstall tensorflow\r\nUninstalling tensorflow-1.8.0:\r\n  Would remove:\r\n    c:\\users\\tmpl1672\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow-1.8.0.dist-info\\*\r\n    c:\\users\\tmpl1672\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\*\r\n    c:\\users\\tmpl1672\\appdata\\local\\programs\\python\\python35\\scripts\\freeze_graph.exe\r\n    c:\\users\\tmpl1672\\appdata\\local\\programs\\python\\python35\\scripts\\saved_model_cli.exe\r\n    c:\\users\\tmpl1672\\appdata\\local\\programs\\python\\python35\\scripts\\tensorboard.exe\r\n    c:\\users\\tmpl1672\\appdata\\local\\programs\\python\\python35\\scripts\\toco.exe\r\n    c:\\users\\tmpl1672\\appdata\\local\\programs\\python\\python35\\scripts\\toco_from_protos.exe\r\n  Would not remove (might be manually added):\r\n    c:\\users\\tmpl1672\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\tensor\\tensorsyclconverttodeviceexpression.h\r\n    .....\r\nProceed (y/n)? y\r\n  Successfully uninstalled tensorflow-1.8.0\r\n\r\nD:\\RasaChatBot_Workspace\\RasaChatBot\\main\\rasa_core_api>pip uninstall tensorboard\r\nUninstalling tensorboard-1.8.0:\r\n  Would remove:\r\n    c:\\users\\tmpl1672\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorboard-1.8.0.dist-info\\*\r\n    c:\\users\\tmpl1672\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorboard\\*\r\nProceed (y/n)? y\r\n  Successfully uninstalled tensorboard-1.8.0\r\n\r\nD:\\RasaChatBot_Workspace\\RasaChatBot\\main\\rasa_core_api>pip --no-cache-dir install tensorflow==1.8.0\r\nCollecting tensorflow==1.8.0\r\n  Downloading https://files.pythonhosted.org/packages/3f/bb/dd01844cf88d15264d92e12a8b89526e1d805c082b8e945b632d4a1989a4/tensorflow-1.8.0-cp35-cp35m-win_amd64.whl (34.4MB)\r\n    100% |\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6| 34.4MB 2.2MB/s\r\nRequirement already satisfied: wheel>=0.26 in c:\\users\\tmpl1672\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from tensorflow==1.8.0) (0.31.1)\r\nRequirement already satisfied: astor>=0.6.0 in c:\\users\\tmpl1672\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from tensorflow==1.8.0) (0.7.1)\r\nRequirement already satisfied: grpcio>=1.8.6 in c:\\users\\tmpl1672\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from tensorflow==1.8.0) (1.14.1)\r\nCollecting tensorboard<1.9.0,>=1.8.0 (from tensorflow==1.8.0)\r\n  Downloading https://files.pythonhosted.org/packages/59/a6/0ae6092b7542cfedba6b2a1c9b8dceaf278238c39484f3ba03b03f07803c/tensorboard-1.8.0-py3-none-any.whl (3.1MB)\r\n    100% |\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6| 3.1MB 1.6MB/s\r\nRequirement already satisfied: gast>=0.2.0 in c:\\users\\tmpl1672\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from tensorflow==1.8.0) (0.2.0)\r\nRequirement already satisfied: six>=1.10.0 in c:\\users\\tmpl1672\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from tensorflow==1.8.0) (1.11.0)\r\nRequirement already satisfied: protobuf>=3.4.0 in c:\\users\\tmpl1672\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from tensorflow==1.8.0) (3.6.1)\r\nRequirement already satisfied: absl-py>=0.1.6 in c:\\users\\tmpl1672\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from tensorflow==1.8.0) (0.4.0)\r\nRequirement already satisfied: numpy>=1.13.3 in c:\\users\\tmpl1672\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from tensorflow==1.8.0) (1.15.1)\r\nRequirement already satisfied: termcolor>=1.1.0 in c:\\users\\tmpl1672\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from tensorflow==1.8.0) (1.1.0)\r\nRequirement already satisfied: werkzeug>=0.11.10 in c:\\users\\tmpl1672\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow==1.8.0) (0.14.1)\r\nRequirement already satisfied: html5lib==0.9999999 in c:\\users\\tmpl1672\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow==1.8.0) (0.9999999)\r\nRequirement already satisfied: markdown>=2.6.8 in c:\\users\\tmpl1672\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow==1.8.0) (2.6.11)\r\nRequirement already satisfied: bleach==1.5.0 in c:\\users\\tmpl1672\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow==1.8.0) (1.5.0)\r\nRequirement already satisfied: setuptools in c:\\users\\tmpl1672\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from protobuf>=3.4.0->tensorflow==1.8.0) (40.2.0)\r\nInstalling collected packages: tensorboard, tensorflow\r\nSuccessfully installed tensorboard-1.8.0 tensorflow-1.8.0\r\n\r\n\r\nD:\\RasaChatBot_Workspace\\RasaChatBot\\main\\rasa_core_api>python train_bot.py\r\nUsing TensorFlow backend.\r\nTraceback (most recent call last):\r\n  File \"train_bot.py\", line 49, in <module>\r\n    train_bot()\r\n  File \"train_bot.py\", line 36, in train_bot\r\n    agent = Agent(\"domain.yml\", policies=[ MemoizationPolicy(), KerasPolicy()])#,\r\n  File \"C:\\Users\\tmpl1672\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\rasa_core\\policies\\keras_policy.py\", line 47, in __init__\r\n    if KerasPolicy.is_using_tensorflow() and not graph:\r\n  File \"C:\\Users\\tmpl1672\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\rasa_core\\policies\\keras_policy.py\", line 64, in is_using_tensorflow\r\n    from keras.backend import _BACKEND\r\n  File \"C:\\Users\\tmpl1672\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\keras\\__init__.py\", line 3, in <module>\r\n    from . import utils\r\n  File \"C:\\Users\\tmpl1672\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\keras\\utils\\__init__.py\", line 6, in <module>\r\n    from . import conv_utils\r\n  File \"C:\\Users\\tmpl1672\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\keras\\utils\\conv_utils.py\", line 9, in <module>\r\n    from .. import backend as K\r\n  File \"C:\\Users\\tmpl1672\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\keras\\backend\\__init__.py\", line 89, in <module>\r\n    from .tensorflow_backend import *\r\n  File \"C:\\Users\\tmpl1672\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 5, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\tmpl1672\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\tmpl1672\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 63, in <module>\r\n    from tensorflow.python.framework.framework_lib import *  # pylint: disable=redefined-builtin\r\n  File \"C:\\Users\\tmpl1672\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\framework_lib.py\", line 104, in <module>\r\n    from tensorflow.python.framework.importer import import_graph_def\r\n  File \"C:\\Users\\tmpl1672\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\importer.py\", line 32, in <module>\r\n    from tensorflow.python.framework import function\r\n  File \"C:\\Users\\tmpl1672\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\function.py\", line 36, in <module>\r\n    from tensorflow.python.ops import resource_variable_ops\r\n  File \"C:\\Users\\tmpl1672\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 35, in <module>\r\n    from tensorflow.python.ops import variables\r\n  File \"C:\\Users\\tmpl1672\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 40, in <module>\r\n    class Variable(checkpointable.CheckpointableBase):\r\nAttributeError: module 'tensorflow.python.training.checkpointable' has no attribute 'CheckpointableBase'", "@manslogic please try uninstalling, deleting the files, then installing again. The issue is stale files sticking around.", "Hello everyone, I have the same mistake, but it holds with python 3. \r\nSo I try these codes:\r\n\r\n`pip3 uninstall \"tensorflow==1.7.*\"`\r\nbefore reinstall\r\n\r\n```\r\npip3 install --upgrade \"tensorflow==1.7.*\"\r\n```\r\n\r\nIt worked for me,", "I was having a similar problem, but only when running with ipython, the solution was to reinstall ipython\r\n( I leave this comment just in case someone has the same issue)", "> @manslogic please try uninstalling, deleting the files, then installing again. The issue is stale files sticking around.\r\nThx a lot! This works for me. \r\nBTW, you need to delete the tensorflow directory, even its version is not the one you just installed."]}, {"number": 20498, "title": "Discrepency between build of  tensorflow in --config=mkl vs --config=cuda", "body": "I am building the tensorflow from source for cpu and gpu.\r\n\r\n\r\nFor GPU \r\nAs mentioned here https://www.tensorflow.org/install/install_sources\r\n--config = cuda \r\nIn this case cuda library link is happening through run time\r\n\r\n\r\nfor CPU\r\nAs mentioned here https://www.tensorflow.org/performance/performance_guide\r\n--config = mkl  \r\n\r\nIn this case if I have MKL-DNN installed its linking at compile time and generating fat binary. \r\n\r\nWhy there is discrepancy here or is my understanding incorrect ?\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Hi @gautamkmr  MKL-DNN is also linked at runtime, but different from cuDNN (where users need to install manually), MKL-DNN is automatically generated/put in TF build output directory. \r\n\r\nCan you please list the size you see in TF_output_dir/tensorflow/libtensorflow*.so? \r\nTensorFlow is probably not just one binary - which binary are you referring to? ", "Let me get back to you on this.", "@gautamkmr Thank you. To clarify on my previous comment: MKL-DNN currently depends on a library called MKLML, this library is dynamically linked to libtensorflow*.so. However, the MKL-DNN code is compiled and linked into .so static. ", "Nagging Assignee @robieta: It has been 105 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 20497, "title": "Fix int64 to int truncation in std::accumulate", "body": "From https://en.cppreference.com/w/cpp/algorithm/accumulate\r\n\r\n```cpp\r\ntemplate< class InputIt, class T, class BinaryOperation >\r\nT accumulate( InputIt first, InputIt last, T init,\r\n              BinaryOperation op );\r\n```\r\n\r\nReturn type of `std::accumulate` (and hence `{xla,absl}::c_accumulate`) is determined by the type of third parameter, not the lambda/function or the iterator.\r\n\r\nFor `std::accumulate(sizes_.begin(), sizes_.end(), /*int*=/1, std::multiplies<int64>())`, compiler will always cast the element of `sizes_` from `int64` to `int`, pass it to `std::multiplies<int64>()` which will cast it to `int64` for multiplication and cast the result back to `int`.\r\n\r\nWriting `1LL` instead of `1` will tell compiler the third parameter is 64-bit, making the function works as expected.\r\n\r\nPlease spread awareness about this to the internal team. `<algorithm>` and `<numeric>` template can be pretty counter-intuitive sometime.", "comments": ["Nagging Assignee @martinwicke: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 20496, "title": "Is between-graph mode in MultiWorkerMirroredStrategy being developed ?", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: master\r\n- **Python version**:  2.7\r\n- **Bazel version (if compiling from source)**: \r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n\r\nI found MultiWorkerMirroredStrategy has been supported in Estimator. However, it seems that only works with in-graph mode. And the comments here shows it will support between-graph mode in the future. \r\n\r\n`# TODO(yuefengz): support between-graph replication.`\r\n\r\nI want to know the progress of the between-graph development. Whether it is in progress or even has not started yet\uff1f If it has been in progress, when does it release in master branch\uff1f \r\n\r\nThank you.  \r\n\r\n", "comments": ["@yuefengz Could you please comment on our question? \r\nIn our in-house environment, we are also working on something alike MirroredStrategy, we just check to avoid duplicated work.", "@yangjunpro I guess you are working on a MPI-based multi-worker solution? We don't have plan to support MPI right now but we'll add framework support if you want to plug in MPI-based components. ", "@yuefengz Is the multi-worker solution related to protocols\uff1fWe are working for gRPC-based between-graph now. So I want to know the progress of yours to avoid unnecessary duplicated work.", "@wangsiyu We will try to make our between-graph solution independent of the network protocol. We will support gRPC-based solution first.", "Nagging Assignee @yuefengz: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@yuefengz Thanks for your answer. I still have two questions. The first is that I found the ParameterServerStrategy for between-graph in contrib directory. Does it only support Async-SGD\uff1fBecause there is no SyncReplicasOptimizer here.  Another is that multi-thread has been supported in DistributedStrategy. Does it for eager-mode\uff1fIn my point of view, multi-thread can only speed up graph-construction in graph-mode.  ", "@wangsiyu It supports both synchronous and asynchronous training depending on whether you use SyncReplicaOptimizer in the model function. Please note that we haven't officially released them since we haven't tested it with enough use cases.\r\n\r\nThe MirroredStrategy works with eager mode for multi-GPU but we haven't tuned its performance yet.\r\n\r\nWe use threads not just for eager mode. They also serve as a trick to call several `model_fn`s and add synchronization subgraphs between them.\r\n\r\nClosing this issue but still feel free to ask questions.", "@yuefengz I see. Another  question is about the simple_estimator_example.py. Does this `MirroredStrategy` case will do gradients reduction in training process\uff1fThis `model_fn` does not include any cross  tower ops and the `estimator.py` only serves for the loss reduction not for the gradients. So if I want to do sync training, should I add cross tower ops in `model_fn`? ", "@wangsiyu The `MirroredStrategy` is using synchronous training no matter whether it is multi-GPU or multi-worker (which will be ready soon). We've changed the `optimizer` code so that gradients reduction happens silently: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/optimizer.py#L557"]}, {"number": 20495, "title": "Better with -L for following link, and libudnn should be libcudnn", "body": "In my case, cuda in /usr/local is a link to /opt/cuda-9.0, so better with -L for following link here.\r\nThe libudnn here I think should be libcudnn.", "comments": ["Nagging Assignee @martinwicke: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 20494, "title": "Why dense layer cannot be speed up in tf.contrib.trt ?", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Ubuntu14.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**:  2.7.5\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9.0 / 7.0.5\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n\r\nWhen using tf.contrib.trt.create_inference_graph, I meet the following error, it seems that dense layer is not support because the input tensor is not rank 4 ? Why has this request on dense layer ?\r\n\r\n> subgraph conversion error for subgraph_index:9 due to: \"Unimplemented: Require 4 dimensional input. Got 2 dense/MatMul\" SKIPPING\r\n\r\n### Source code / logs\r\n\r\nNeed not to code, clear above ...\r\n", "comments": ["Nagging Assignee @tatatodd: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This is a duplicate of #20398. Closing in favor of that one."]}, {"number": 20493, "title": "Huge whl file for 1.9 compared to 1.6", "body": "After building  Tensorflow 1.9 and 1.6 for GPU, noticed that file sizes are 7x larger for 1.9.   \r\nTF 1.9 :  Ubuntu 16.04, Python 3.6.6,  Cuda 9.0 , Cudnn 7.0\r\nTF 1.6:   Ubuntu 16.04, Python 3.5.3   Cuda 9.0 , Cudnn 7.0\r\n\r\n-rw-rw-r-- 1     75302881 Jul  2 00:16 tensorflow-1.6.1-cp35-cp35m-linux_x86_64.whl\r\n-rw-rw-r-- 1   512505346 Jul  2 18:51 tensorflow-1.9.0rc2-cp36-cp36m-linux_x86_64.whl", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 20492, "title": "How to replace the model in TensorFlow Android Demo", "body": "I would like to replace the model of the TensorFlowAndroidDemo. There are two files in the asset folder: one is \"model.pb\", file and one \" label.txt\".\r\n\r\nNow I already have a pretrained pb model. How can I get the corresponding label. txt file?\r\n\r\nThanks", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 20491, "title": "Better with -L for following link, and libudnn should be libcudnn", "body": "Better with -L for following link, and libudnn should be libcudnn", "comments": []}, {"number": 20490, "title": "Branch 203039240", "body": "", "comments": []}, {"number": 20489, "title": "Match for -path instead of -name", "body": "", "comments": ["Thanks for the fix"]}, {"number": 20488, "title": "Text generation", "body": "Text Generation with Tensorflow Eager Execution", "comments": ["@alextp @MarkDaoust @lamberta @random-forests @asimshankar ", "Hi Yash, can you coordinate these notebooks with Josh for the 1.10 release?", "@lamberta I did ask Josh and he told me to put these here for now and then for 1.10, we can move it to wherever the folder is", "Now let's wait for the hold on merges to be lifted to merge this."]}, {"number": 20487, "title": "Validate variable dtype before restoring checkpoint", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: [Yes](https://stackoverflow.com/questions/51137417/wrong-output-for-restored-variable-in-tensorflow-graph)\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.10.0-dev20180620\r\n- **Python version**:  2.7\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 9.0 / 7\r\n- **GPU model and memory**: GeForce GTX 1080 / 8 Gb\r\n- **Exact command to reproduce**: Run the two scripts below in \"Source code/logs\"\r\n\r\n### Describe the problem\r\nA warning/error should be raised when a variable is loaded with a different dtype as that which was saved. \r\n\r\nIn the code below, the variable `global_step` is saved as a `tf.int32`, but treated as a `tf.float32` when restored. When this happens, the `global_step` variable appears to have a value of 7e-45 instead of the expected value of 5. \r\n\r\n### Source code / logs\r\n [Link to Stackoverflow Post with the code/question](https://stackoverflow.com/questions/51137417/wrong-output-for-restored-variable-in-tensorflow-graph)\r\n\r\n[Original post copied here]\r\n**Script that saves the checkpoints:**\r\n\r\n    import tensorflow as tf\r\n\r\n    a = tf.Variable(3.0, name='a')\r\n    b = tf.Variable(5.0, name='b')\r\n    \r\n    b = tf.assign_add(b, a)\r\n    \r\n    n_steps = 5\r\n    \r\n    global_step = tf.Variable(0, name='global_step', trainable=False)\r\n    \r\n    saver = tf.train.Saver()\r\n    \r\n    with tf.Session() as sess:\r\n    \r\n        sess.run(tf.global_variables_initializer())\r\n        \r\n        for step in range(n_steps):\r\n            print(sess.run(b))\r\n    \r\n            global_step.assign_add(1).eval()\r\n            print(global_step.eval())\r\n    \r\n            saver.save(sess, './my_test_model', global_step=global_step)\r\n\r\n**Script that restores checkpoint:**\r\n\r\n    import tensorflow as tf\r\n    \r\n    from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file\r\n    \r\n    # List ALL tensors.\r\n    print_tensors_in_checkpoint_file(tf.train.latest_checkpoint('./'), all_tensors=True, tensor_name='')\r\n    \r\n    tf.reset_default_graph()\r\n    \r\n    a = tf.get_variable('a', shape=[])\r\n    b = tf.get_variable('b', shape=[])\r\n    global_step = tf.get_variable('global_step', shape=[])\r\n    \r\n    saver = tf.train.Saver()\r\n    \r\n    with tf.Session() as sess:\r\n            \r\n        ckpt = tf.train.latest_checkpoint('./')\r\n        if ckpt:\r\n            print(ckpt)\r\n        \r\n            saver.restore(sess, ckpt)\r\n        \r\n        else:\r\n            print('Nothing restored')\r\n        \r\n        print(a.eval())\r\n        print(b.eval())\r\n        print(global_step.eval())\r\n\r\n**Output**\r\n```\r\ntensor_name:  a\r\n3.0\r\ntensor_name:  b\r\n20.0\r\ntensor_name:  global_step\r\n5\r\n./my_test_model-5\r\nINFO:tensorflow:Restoring parameters from ./my_test_model-5\r\n3.0\r\n20.0\r\n7e-45\r\n```\r\n\r\n", "comments": ["Nagging Assignee @drpngx: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Yes, some extra validation sounds like a good idea.", "@drpngx I create a PR #21078 to fix the issue, could you take a look? Thanks."]}, {"number": 20486, "title": "Reapplying #20254", "body": "PR #20430 contained 151 commits, one of which backleveled PR #20254.\r\n\r\nThis PR is to re-apply that change. Text of 20254:\r\nPR #20229 included a change to the Eigen version that failed to\r\ncompile on ppc64le. rmlarsen created a pull request in Eigen to\r\nfix the compile failure:\r\nhttps://bitbucket.org/eigen/eigen/pull-requests/410\r\n\r\nThis patch is to pick up the Eigen version of that patch.", "comments": []}, {"number": 20485, "title": "tflite label_image not showing any labels", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OSX 10.13.3\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: v1.8.0-0-g93bc2e2072 1.8.0\r\n- **Python version**: Python 3.6.0 :: Anaconda 4.3.1 (x86_64)\r\n- **Bazel version (if compiling from source)**: Build label: 0.14.1-homebrew\r\n- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 9.1.0 (clang-902.0.39.2)\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NVIDIA GeForce GT 650M 1 GB\r\n- **Exact command to reproduce**:  bazel-bin/tensorflow/contrib/lite/examples/label_image/label_image --tflite_model=../new_training_dir/retrainedMNetV1_7_8000.tflite --image=../tf_files/test_data_BITMAP/sidewalk/IMG_1913.bmp --labels=testLabels.txt\r\n\r\n\r\n### Describe the problem\r\nI have managed to compile the tflite label_image but when I use it with a retrained mobilenet I get the results without any labels next to it like following:\r\n\r\n### Source code / logs\r\n```\r\nLoaded model ../new_training_dir/retrainedMNetV1_7_8000.tflite\r\nresolved reporter\r\ninvoked\r\naverage time: 70.195 ms\r\n8.15717: 544\r\n6.8344: 732\r\n6.73561: 734\r\n6.2833: 423\r\n5.93283: 420\r\n```", "comments": ["interesting, does the labels.txt in this [zip](https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_v1_224_android_quant_2017_11_08.zip) work for you?", "It actually works:\r\n\r\n```\r\nLoaded model ../new_training_dir/retrainedMNetV1_7_8000.tflite\r\nresolved reporter\r\ninvoked\r\naverage time: 147.621 ms\r\n8.15717: 544 dumbbell\r\n6.8344: 732 plunger\r\n6.73561: 734 pole\r\n6.2833: 423 barbell\r\n5.93283: 420 Band Aid\r\n```\r\n\r\nBut why can't I test my model with the categories I retrained it for? And does it mean that the detection errors I have been experiencing on Android with this model (detecting almost always the same category, stackoverflow post: https://stackoverflow.com/questions/50938992/tensorflow-lite-high-loss-in-accuracy-after-converting-model-to-tflite) are likely to have the same cause? \r\n\r\nI have no problem using the label_image for my .pb graph with the right labels and the detections are accurate.", "@jbuisson1 probably your `testLabels.txt` is problematic. On macOS, it's supposed to be one line for each category (with lines ended with 0xa). See [the source code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/examples/label_image/label_image.cc#L50-L72) for how labels are read.", "Here is the [testLabels.txt](https://github.com/tensorflow/tensorflow/files/2173832/testLabels.txt) file I am using, it doesn't seem to be much different from the other one except I only have 2 categories (the ones I retrained the mobilenet with).\r\n\r\n```\r\ncat -vet testLabels.txt\r\nsidewalk$\r\ncrosswalk$\r\n```\r\n \r\n```\r\ncat -vet labels.txt\r\nbackground$\r\ntench$\r\ngoldfish$\r\ngreat white shark$\r\n...\r\nhen-of-the-woods$\r\nbolete$\r\near$\r\ntoilet tissue$\r\n```\r\n\r\nThis labels file works with the label_image for the .pb file, it's also the one I use for TFLite on Android and the TFLiteClassifier works well with it, the only problem is that now the recognitions are not accurate at all compared to the ones made by the .pb file as it recognizes every image as the same category (I don't know if this might be related).", "@jbuisson1 I don't get it. If you only have two categories, how come the results are showing that the highest one is category 544?", "Well I wasn't aware that was what these numbers meant, but your guess is as good as mine... Here is the .pb model [retrainedMNetV1_7_8000.pb.zip](https://github.com/tensorflow/tensorflow/files/2179821/retrainedMNetV1_7_8000.pb.zip) and the .tflite model [retrainedMNetV1_7_8000.tflite.zip](https://github.com/tensorflow/tensorflow/files/2179826/retrainedMNetV1_7_8000.tflite.zip)\r\n if you want to take a look (maybe I did something wrong in the conversion to .tflite or something) ", "I uploaded the models if it helps you see if anything is wrong on that end", "@jbuisson1: It's my fault (I wrote the original label_image for tf lite). The number of outputs and number of results are fixed numbers now (see https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/examples/label_image/label_image.cc#L216-L237). Quick solution: changing output_size and num_results should fix the problem. \r\n\r\nI'll send a PR to add command options and check the output tensor later.", "It works with this quick fix! Thank you!!!", "> @jbuisson1: It's my fault (I wrote the original label_image for tf lite). The number of outputs and number of results are fixed numbers now (see https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/examples/label_image/label_image.cc#L216-L237). Quick solution: changing output_size and num_results should fix the problem.\r\n> \r\n> I'll send a PR to add command options and check the output tensor later.\r\n@freedomtan @jbuisson1 @aselle \r\n\r\nI don't get it how exactly did you solve that? The link is dead, please give a hint.\r\n\r\nWhat are the \"output_size\" and \"num_results\", where exactly are they in the project and why exactly are there for?\r\n", "The same issue after deploying the Tensorflow lite example on mobile with my custom classes it's not showing any labels.", "@Karadarya this should be fixed by #20988 (a7fc381) already."]}, {"number": 20484, "title": "Unable to import tensorflow with tf-nightly-gpu", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: pip install tensorflow-nightly-gpu\r\n- **TensorFlow version (use command below)**: 1.10\r\n- **Python version**:  2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\nimport tensorflow as tf\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nttTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/dhingratul/.virtualenvs/tf/local/lib/python2.7/site-packages/tensorflow/__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/home/dhingratul/.virtualenvs/tf/local/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 81, in <module>\r\n    from tensorflow.python import keras\r\n  File \"/home/dhingratul/.virtualenvs/tf/local/lib/python2.7/site-packages/tensorflow/python/keras/__init__.py\", line 24, in <module>\r\n    from tensorflow.python.keras import activations\r\n  File \"/home/dhingratul/.virtualenvs/tf/local/lib/python2.7/site-packages/tensorflow/python/keras/activations/__init__.py\", line 22, in <module>\r\n    from tensorflow.python.keras._impl.keras.activations import elu\r\n  File \"/home/dhingratul/.virtualenvs/tf/local/lib/python2.7/site-packages/tensorflow/python/keras/_impl/keras/__init__.py\", line 21, in <module>\r\n    from tensorflow.python.keras._impl.keras import activations\r\n  File \"/home/dhingratul/.virtualenvs/tf/local/lib/python2.7/site-packages/tensorflow/python/keras/_impl/keras/activations.py\", line 23, in <module>\r\n    from tensorflow.python.keras._impl.keras import backend as K\r\n  File \"/home/dhingratul/.virtualenvs/tf/local/lib/python2.7/site-packages/tensorflow/python/keras/_impl/keras/backend.py\", line 38, in <module>\r\n    from tensorflow.python.layers import base as tf_base_layers\r\n  File \"/home/dhingratul/.virtualenvs/tf/local/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 25, in <module>\r\n    from tensorflow.python.keras.engine import base_layer\r\n  File \"/home/dhingratul/.virtualenvs/tf/local/lib/python2.7/site-packages/tensorflow/python/keras/engine/__init__.py\", line 21, in <module>\r\n    from tensorflow.python.keras.engine.base_layer import InputSpec\r\n  File \"/home/dhingratul/.virtualenvs/tf/local/lib/python2.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 33, in <module>\r\n    from tensorflow.python.keras import backend\r\n  File \"/home/dhingratul/.virtualenvs/tf/local/lib/python2.7/site-packages/tensorflow/python/keras/backend/__init__.py\", line 22, in <module>\r\n    from tensorflow.python.keras._impl.keras.backend import abs\r\nImportError: cannot import name abs\r\n", "comments": ["appears to be a duplicate of #19883"]}, {"number": 20483, "title": "Branch 202995903", "body": "", "comments": []}, {"number": 20482, "title": "Trouble replicating results using capi", "body": " I have written minimal code in both python and using the capi. This code loads a premade graph and runs a test image through it. I cannot seem to get the same results using the capi as I get running through python.\r\nLinux Ubuntu 16.04\r\nI have both installed tf and built the library using bazel.\r\nI haqve tried various versions but for this experiment am fixing on 1.9.\r\nPython 3.5.2\r\nbazel 0.13.0\r\ngcc 5.4\r\nno cuda\r\n\r\nI am trying to run the graph \"ssd_mobilenet_v1_android_export.pb\" given with the android mobile examples.\r\n\r\nI am using this test image https://github.com/pjreddie/darknet/raw/master/data/dog.jpg\r\n\r\nI have simple python code (code below), that loads the graph, feeds the image to it and reads back various tensors. This graph has an op at the end called \"num_detections\", I am reading the tensor at this point to check for success.\r\nWith the dog.jpg I get a \"3.0\" result. Other test images give appropriate results. \r\n\r\nI am repeating this experiment with the capi (code below), but the only result I ever get is zero. \r\nThe first obvious place I looked is my image input. The second graph op is a \"ToFloat\" operation. I have been using that to check the values of the pixel data going into the feed. They are identical. I checked the image is not flipped and the color channels are in the same order after decoding. I then started looking further up the graph, sampling values of the tensors at each op. I find they are mostly the same until they get to a long list of convolution ops where it seems the values slowly diverge - like they are the same the first couple of ops then start to show differences getting further and further apart.\r\n\r\nNot working capi code:\r\n\r\n````\r\n\r\n#include <png.hpp>\r\n#include <stdlib.h>\r\n#include <stdio.h>\r\n#include <tensorflow/c/c_api.h>\r\n\r\ntypedef unsigned char byte; \r\n\r\nconst int num_channels = 3;\r\n\r\nstruct Image {\r\n  byte* data;\r\n  int width, height;\r\n  int dataSize() {\r\n    return width * height * num_channels;\r\n  }\r\n};\r\n\r\nImage open_image(const char* filename) {\r\n\r\n  Image image;\r\n  int dataSize = image.width * image.height * num_channels;\r\n\r\n  png::image< png::rgb_pixel > png_import(filename);\r\n  image.width = png_import.get_width();\r\n  image.height = png_import.get_height();\r\n  image.data = (byte*)malloc(image.dataSize());\r\n  for(int y=0; y<image.height; y++) {\r\n    for(int x=0; x<image.width; x++) {\r\n      png::rgb_pixel pixel = png_import.get_pixel(x,y);\r\n      byte* data = &image.data[(y*image.width+x)*num_channels];\r\n      *data++ = pixel.red;\r\n      *data++ = pixel.green;\r\n      *data++ = pixel.blue;\r\n    }\r\n  }\r\n\r\n  printf(\"%s loaded, %dx%d\\n\", filename, image.width, image.height);\r\n  return image;\r\n}\r\n\r\nTF_Graph* open_graph(const char* filename, TF_Status* status) {\r\n\r\n  TF_Graph* graph = TF_NewGraph();\r\n\r\n  TF_ImportGraphDefOptions* opts = TF_NewImportGraphDefOptions();\r\n\r\n  FILE* fp = fopen(filename, \"r\");\r\n\r\n  fseek(fp, 0L, SEEK_END);\r\n  int file_size = ftell(fp);\r\n  rewind(fp);\r\n\r\n  char* graph_content = (char*)malloc(file_size);\r\n\r\n  fread(graph_content, file_size, 1, fp);\r\n  fclose(fp);\r\n\r\n  TF_Buffer tfBuffer;\r\n  tfBuffer.data = graph_content;\r\n  tfBuffer.length = file_size;\r\n  \r\n  TF_GraphImportGraphDef(graph, &tfBuffer, opts, status);\r\n\r\n  free(graph_content);\r\n\r\n  if(TF_GetCode(status) != TF_OK) {\r\n\r\n    printf(\"%s\\n\", TF_Message(status));\r\n    return 0;\r\n  }\r\n\r\n  printf(\"Successfully loaded graph\\n\");\r\n\r\n  return graph;\r\n}\r\nvoid TensorDeallocator(void* data, size_t len, void* arg)\r\n{\r\n\r\n}\r\n\r\n\r\nint main(int argi, char** argc) {\r\n\r\n  printf(\"TensorFlow C library version %s\\n\", TF_Version());\r\n\r\n  TF_Status* status = TF_NewStatus();\r\n  TF_Graph* graph = open_graph(argc[1], status);\r\n\r\n  Image image = open_image(argc[2]);\r\n\r\n  int64_t dims[] = {1, image.width, image.height, num_channels};\r\n  \r\n  TF_Tensor* image_tensor = TF_NewTensor(\r\n    TF_UINT8, dims, 4, \r\n    image.data, image.dataSize(), \r\n    TensorDeallocator, 0\r\n  );\r\n\r\n  TF_Operation* input_image_op = TF_GraphOperationByName(graph, \"image_tensor\");\r\n  if(!input_image_op) {\r\n    printf(\"Failed to find Op '%s'\", \"image_tensor\");\r\n    return 1;\r\n  }\r\n\r\n  TF_Output image_input;\r\n  image_input.oper = input_image_op;\r\n  image_input.index = 0;\r\n  TF_Output inputs[1] = {image_input};\r\n\r\n  TF_Operation* num_detection_op = TF_GraphOperationByName(graph, \"num_detections\");\r\n  if(!num_detection_op) {\r\n    printf(\"Failed to find Op '%s'\", \"num_detections\");\r\n    return 1;\r\n  }\r\n  TF_Output num_detection_output;\r\n  num_detection_output.oper = num_detection_op;\r\n  num_detection_output.index = 0;\r\n\r\n  TF_Output outputs[1] = {num_detection_output};\r\n  TF_Tensor* output_tensors[1];\r\n\r\n  TF_SessionOptions * options = TF_NewSessionOptions();\r\n  TF_Session* session = TF_NewSession( graph, options, status );\r\n\r\n  TF_SessionRun(session, 0, \r\n                &image_input, &image_tensor, 1,\r\n                outputs, output_tensors, 1,\r\n                0, 0,\r\n                0, status\r\n                );\r\n\r\n  if(TF_GetCode(status) != TF_OK) {\r\n\r\n    printf(\"%s\\n\", TF_Message(status));\r\n  }\r\n  else {\r\n      \r\n    printf(\"Ran successfully\\n\");\r\n\r\n    float* f = (float*)TF_TensorData(output_tensors[0]);\r\n\r\n    printf(\"TF data %f\\n\", f[0]);\r\n\r\n  }\r\n  \r\n  free(image.data);\r\n\r\n  return 0;\r\n}\r\n\r\n\r\n```\r\n\r\n\r\nWorking python code:\r\n\r\n\r\n```\r\nimport data_helpers\r\nimport sys\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\nfrom tensorflow.python.platform import gfile\r\nfrom tensorflow.python.util import compat\r\nfrom tensorflow.core.protobuf import saved_model_pb2\r\n\r\ndef read_graph(model_filename):\r\n\r\n    with gfile.FastGFile(model_filename, 'rb') as f:\r\n\r\n        graph_def = tf.GraphDef()\r\n        graph_def.ParseFromString(f.read())\r\n        g_in = tf.import_graph_def(graph_def)\r\n\r\n        return g_in\r\n\r\n\r\ndef printOps():\r\n\r\n    for op in tf.get_default_graph().get_operations():\r\n\r\n        print(op.name)\r\n\r\nif __name__ == \"__main__\":\r\n    \r\n    if len(sys.argv) < 3:\r\n        print(\"Usage: run_image_on_graph.py [input graph filepath] [output image filepath]\")\r\n        exit(1)\r\n\r\n    graph_file = sys.argv[1]\r\n    image_file = sys.argv[2]\r\n\r\n    image = data_helpers.import_image(image_file)\r\n\r\n    with tf.Session() as sess:\r\n\r\n        read_graph(graph_file)\r\n\r\n        graph = tf.get_default_graph()\r\n\r\n        image_tensor = graph.get_tensor_by_name(\"import/image_tensor:0\")\r\n        num_detections = graph.get_tensor_by_name(\"import/num_detections:0\")\r\n        detection_scores = graph.get_tensor_by_name(\"import/detection_scores:0\")\r\n        detection_boxes = graph.get_tensor_by_name(\"import/detection_boxes:0\")\r\n        debug = graph.get_tensor_by_name(\"import/ToFloat:0\")\r\n\r\n        image = np.array([data_helpers.import_image(image_file)])\r\n\r\n        print(\"sess = \" + str(sess.run([num_detections], feed_dict={\r\n            image_tensor: image})\r\n        ))\r\n      \r\n```\r\n   \r\n\r\nIs there anything obviously wrong with my approach?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "System information\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):yes, shown\r\nOS Platform and Distribution: Ubuntu 16.04.4 LTS\r\nTensorFlow installed from (source or binary):source\r\nTensorFlow version (use command below):1.9\r\nPython version: 3.5.2\r\nBazel version (if compiling from source):0.13.0\r\nGCC/Compiler version (if compiling from source):5.4.0\r\nCUDA/cuDNN version:N/A\r\nGPU model and memory:N/A\r\nExact command to reproduce: \r\n\r\ntf_test ssd_mobilenet_v1_android_export.pb dog.png", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "Hi, thanks for taking a look. It might be a bug. I'm just asking whether you are of the opinion that running the same graph with the same data should produce identical results between python and the capi. And if that is true is there something about the code above that is the reason that is not happening?."]}, {"number": 20481, "title": "tf.data.Dataset.from_tensor_slices incompatible with tuples?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04 and 18.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: `1.8.0`\r\n- **Python version**: `2.7`\r\n- **CUDA/cuDNN version**: 9.0 / 7.5 but also failing in CPU-only mode\r\n- **GPU model and memory**: P100\r\n- **Exact command to reproduce**:\r\n``` python\r\n>>> import tensorflow as tf\r\n>>> \r\n>>> data_list = 'tensors: A nested structure of tensors, each having the same size in the 0th dimension.'.split()\r\n>>> data_tuple = tuple(data_list)\r\n>>> print('list: {}'.format(data_list))\r\nlist: ['tensors:', 'A', 'nested', 'structure', 'of', 'tensors,', 'each', 'having', 'the', 'same', 'size', 'in', 'the', '0th', 'dimension.']\r\n>>> print('tuple: {}'.format(data_tuple))\r\ntuple: ('tensors:', 'A', 'nested', 'structure', 'of', 'tensors,', 'each', 'having', 'the', 'same', 'size', 'in', 'the', '0th', 'dimension.')\r\n>>> \r\n>>> ds_l = tf.data.Dataset.from_tensor_slices(data_list)\r\n>>> try:\r\n...     ds_t = tf.data.Dataset.from_tensor_slices(data_tuple)\r\n... except IndexError as e:\r\n...     print(e.message)\r\n... \r\nlist index out of range\r\n>>> ds_l = tf.data.Dataset.from_tensors(data_list)\r\n>>> ds_t = tf.data.Dataset.from_tensors(data_tuple)\r\n>>> \r\n>>> with tf.Session() as session:\r\n...     for ds in [ds_l, ds_t]:\r\n...         it = ds.make_one_shot_iterator().get_next()\r\n...         while True:\r\n...             try:\r\n...                 print(session.run(it))\r\n...             except tf.errors.OutOfRangeError:\r\n...                 break\r\n... \r\n['tensors:' 'A' 'nested' 'structure' 'of' 'tensors,' 'each' 'having' 'the'\r\n'same' 'size' 'in' 'the' '0th' 'dimension.']\r\n('tensors:', 'A', 'nested', 'structure', 'of', 'tensors,', 'each', 'having', 'the', 'same', 'size', 'in', 'the', '0th', 'dimension.')\r\n\r\n\r\n```\r\n\r\n### Describe the problem\r\nApparently one can not use `tf.data.Dataset.from_tensor_slices` with tuples. This is very counter-intuitive as they almost everywhere have the same behavior as lists.\r\nAlso using `tf.data.Dataset.from_tensors` is no option. Even though this seems to handle tuples properly, one only gets a single element instead of `n` elements. This is in alignment with the documentation but does not fulfill the same functionality as `tf.data.Dataset.from_tensor_slices`.\r\n\r\nAm I just using it wrong, is the documentation to ambiguous or should this be fixed?", "comments": ["This is working as intended: the `tf.data` API uses Python lists to signify values that should be converted implicitly to tensors, and Python tuples to signify values that should be interpreted as multiple components of a (potentially nested) structure."]}, {"number": 20480, "title": "Branch 202961895", "body": "", "comments": []}, {"number": 20479, "title": "bugfix: cleanup CondContext at exception", "body": "This fix is to solve a bug: if an exception occurred in tf.cond(),  the CondContext is left uncleaned. The bug causes unexpected problems, for instance, ```tf.Variable(tf.zeros(20))``` will not work!\r\n\r\n\r\n```python\r\n>>> import tensorflow as tf\r\n>>> \r\n>>> a = tf.constant(20)\r\n>>> c = tf.cond(a>1, lambda: a, lambda: b)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/tf/.local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 432, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/tf/.local/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2072, in cond\r\n    orig_res_f, res_f = context_f.BuildCondBranch(false_fn)\r\n  File \"/home/tf/.local/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1913, in BuildCondBranch\r\n    original_result = fn()\r\n  File \"<stdin>\", line 1, in <lambda>\r\nNameError: name 'b' is not defined\r\n>>> tf.get_default_graph()._get_control_flow_context()\r\n<tensorflow.python.ops.control_flow_ops.CondContext object at 0x7fefe2617630>\r\n>>> \r\n>>> tf.Variable(tf.zeros(20))\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/tf/.local/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 235, in __init__\r\n    constraint=constraint)\r\n  File \"/home/tf/.local/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 362, in _init_from_args\r\n    \"initializer.\" % name)\r\nValueError: Initializer for variable Variable/ is from inside a control-flow construct, such as a loop or conditional. When creating a variable inside a loop or conditional, use a lambda as the initializer.\r\n```", "comments": ["Nagging Assignee @martinwicke: It has been 16 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}]