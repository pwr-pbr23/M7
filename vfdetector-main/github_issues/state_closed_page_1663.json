[{"number": 3015, "title": "WIP: Reorganizes tutorial navigation", "body": "Reorganizes tutorial navigation.\n", "comments": ["Can one of the admins verify this patch?\n"]}, {"number": 3014, "title": "WIP: Reorganizes leftnav files.", "body": "Changes to be committed:\n    modified:   tensorflow/g3doc/tutorials/leftnav_files\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n"]}, {"number": 3013, "title": "WIP: Site navigation change", "body": "Reorganizes leftnav file for tutorials.\n", "comments": ["Can one of the admins verify this patch?\n"]}, {"number": 3012, "title": "Conv3d_transpose in TensorFlow", "body": "Feature request: Would it be possible to provide a conv3d_transpose, analogous to conv2d_transpose?\n", "comments": []}, {"number": 3011, "title": "R0.9 ver2", "body": "Reduces the test time from about 33 s to 24 s.\n", "comments": ["Will submit as CL internally.\n"]}, {"number": 3010, "title": "Added complex type support to ops for GPU", "body": "Initial `matmul` complex support for issue #2977 \n", "comments": ["Can one of the admins verify this patch?\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Hey, any updates on this?\n", "@vrv would be happy to get an initial review?\n", "nice, thanks.\n\n@tensorflow-jenkins test this please\n", "Hi @kashif -- I just submitted some changes internally that add complex support to tf.matmul and tf.batch_matmul so it looks like I've duplicated some work with you.\n\nI'm sorry for not noticing you were already working on this and will do a review of active PRs next time.\n\nThanks for taking care of tf.conf! That's been on my TODO-list for a while.\n", "@rryan no worries. should I close this issue? or is there something you can cherry pick?\n", "> @rryan no worries. should I close this issue? or is there something you can cherry pick?\n\nIMO the tf.conj change in this PR is still worth adding! (I'm not on the TF team, just a Googler using TF).\n", "@rryan ok cool I will wait for your changes and then merge the conflicts\n", "The changes are in https://github.com/tensorflow/tensorflow/commit/6c7681fbbcc3c244f3e406abc4ea1287fd717752 so you should be able to merge/rebase now.\n", "(friendly ping that you can rebase)\n", "@vrv can you kindly test? This pull request now only has support for `conj` on GPUs\n", "@tensorflow-jenkins test this please\n"]}, {"number": 3009, "title": "FIFOQueue: dequeue many operation very slow?", "body": "When training a relatively simple model (1-layer LSTM, 256 units) my Titan X GPU keeps spiking from 0% to 30% GPU utilization. Conclusion: somewhere in the pipeline there is a bottleneck which limits the GPU to be processing the training batches continuously. I use a FIFOQueue to which examples are being fed in one or more separate threads:\n\n``` python\nqueue = tf.FIFOQueue(\n     capacity=self.config.queue_capacity,\n     dtypes=[tf.float32, tf.float32],\n     shapes=[[30, 49, 512], [30]],\n     name=\"FIFOQueue\"\n)\n```\n\nFor the training operation I use `queue.dequeue_many` to get examples from the queue. As you can see the batch size is 64 examples. So in the end the input tensor is `64x30x49x512` of type `tf.float32`:\n\n``` python\n# Model inputs, either use the queue (training) or feed_dict (evaluation)\ninputs, targets = queue.dequeue_many(64)\n```\n\nTo find out why my code is running \"slow\" (i.e. spiking GPU allocation and no temperature increase) I use the `Timeline` object ([see here](http://stackoverflow.com/questions/34293714/tensorflow-can-i-measure-the-execution-time-of-individual-operations)) to measure execution times of individual operations. The results displayed below show the measurements for one training iteration at which point the queue was filled with more than 1000 examples. I have included screenshots for both GPU and CPU-only runs (forced with `export CUDA_VISIBLE_DEVICES=-1`. \n\nWhat strikes me from these results is that it takes a really long time to dequeue examples from the FIFOQueue. What is happening here...something wrong or is the dequeuing operation just very slow? Overall the dequeuing operation and sending the data to the GPU takes up half of the time of a training iteration. No wonder that the GPU utilization is spiking. Any help is welcome optimizing my training pipeline! As I understand correctly the examples are all queued in RAM, is there also a way to queue them ahead on GPU memory so when they are needed they do not have to be moved CPU => GPU?\n\nThis is tested on TensorFlow v9.0 build from sources about 1.5 week ago.\n\n**GPU running on Titan X**\n![gpu](http://i.imgur.com/eTZNPDM.png?1) \n\n**CPU running on Xeon CPU E5-2640**\n![cpu](http://i.imgur.com/mQkLH8h.png?1)\n", "comments": ["Thanks for including the timelines!  Nice to see people using that ;-)\n\nIt's hard to say very much about what's happening here without seeing the whole program.  I don't _believe_ that dequeueing 192MB is inherently that expensive.  (@mrry - does this sound right?)\n\nThere are a number of things which might be happening.  Obviously `DequeueMany` is a blocking operation and the time it appears to take in the timeline will include all the time from when it was issued until when the final element was enqueued into the FIFO.  This timeline will not show the other, potentially concurrent steps which are happening.\n\nAre you enqueueing all 64 required elements before calling the step which dequeues them? \nCan you contrive a test case where you put 64 elements into the queue in advance, and just run a step which does the dequeue?\n", "@prb12: At the very least, the DequeueMany op is going to do 64 separate Eigen slice assignments to build up the output batch. If the capacity is less than 64, it will also have the copies out of any corresponding EnqueueMany op arguments accounted to it. The most likely cause is that it's blocking, however, so it would be great if @tomrunia could do the contrived test that you're suggesting. Otherwise using a sampling profiler might reveal a suprisingly costly copy.\n", "Oke, I wrote a simple script (see below) in which the threads simple enqueue batches of `np.ones([64, 30, 49, 512], dtype=np.float32)` to the `FIFOQueue`. The main loop simple dequeues 64 examples and performs a simple `tf.square` of the input tensor. Before the main loop starts I make sure that the queue is filled with a sufficient number of examples. Again, the dequeue operation takes a \"long\" time to finish as you can see in the timelines below. \n\n**GPU running on Titan X**\n![gpu](http://i.imgur.com/VrKa51G.png)\n\n**CPU running on Xeon E5-2640**\n![cpu](http://i.imgur.com/VlG68ZX.png)\n\n``` python\nimport time\nimport numpy as np\nimport threading\n\nimport tensorflow as tf\nfrom tensorflow.python.client import timeline\n\n\ndef test_queue():\n\n    feature_input = tf.placeholder(tf.float32, shape=[None, 30, 49, 512], name=\"queue_inputs\")\n    target_intput = tf.placeholder(tf.float32, shape=[None, 30], name=\"queue_targets\")\n\n    queue = tf.FIFOQueue(\n        capacity=1000,\n        dtypes=[tf.float32, tf.float32],\n        shapes=[[30, 49, 512], [30]],\n        name=\"FIFOQueue\"\n    )\n\n    # Enqueue and dequeue operations\n    enqueue_op = queue.enqueue_many([feature_input, target_intput])\n    queue_inputs, queue_targets = queue.dequeue_many(64)\n\n    queue_size = queue.size()\n\n    # Very simple training operator, dequeue examples from the FIFOQueue and\n    # square the matrix elementwise. Just for testing of course.\n    square_op = tf.square(queue_inputs)\n\n    # Coordinator for threads\n    coord = tf.train.Coordinator()\n\n    # Initialize the TensorFlow session\n    gpu_options = tf.GPUOptions(\n        per_process_gpu_memory_fraction=0.75,\n    )\n\n    sess = tf.Session(config=tf.ConfigProto(\n        gpu_options=gpu_options,\n        log_device_placement=False,\n        allow_soft_placement=False\n    ))\n\n    def load_and_enqueue():\n        while True:\n            # Just feed random stuff to the queue\n            #features = np.random.rand(64, 30, 49, 512)\n            features = np.ones([64, 30, 49, 512], dtype=np.float32)\n            targets  = np.ones([64, 30], dtype=np.float32)\n\n            # Feed example to Tensorflow placeholder\n            feed_dict = {\n                feature_input: features,\n                target_intput: targets\n            }\n\n            # Push all the training examples to the queue\n            sess.run(enqueue_op, feed_dict=feed_dict)\n\n            if coord.should_stop():\n                break\n\n    # Start the threads\n    num_threads = 4\n    for i in range(num_threads):\n        t = threading.Thread(target=load_and_enqueue)\n        t.setDaemon(True)\n        t.start()\n\n    # Make sure the queueu is filled with some examples (n = 500)\n    num_samples_in_queue = 0\n    while num_samples_in_queue < 500:\n        num_samples_in_queue = sess.run(queue_size)\n        print(\"Initializing queue, current size = %i\" % num_samples_in_queue)\n        time.sleep(1)\n\n    # Initialize the session\n    init = tf.initialize_all_variables()\n    sess.run(init)\n\n    run_options  = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n    run_metadata = tf.RunMetadata()\n\n    for step in range(1000):\n\n        print(\"Step = %i, QueueSize = %i\" % (step, sess.run(queue_size)))\n\n        if step == 10:\n            # Perform a step with saving the results as timeline object\n            result = sess.run(square_op, options=run_options, run_metadata=run_metadata)\n\n            # Create the Timeline object, and write it to a json\n            tl = timeline.Timeline(run_metadata.step_stats)\n            ctf = tl.generate_chrome_trace_format()\n            with open('timeline.json', 'w') as f:\n                print(\"writing to timeline.json\")\n                f.write(ctf)\n\n        else:\n            # Perform a step without saving the results\n            result = sess.run(square_op)\n\n    # Ask for the threads to stop\n    coord.request_stop()\n    sess.close()\n\n\nif __name__ == \"__main__\":\n    test_queue()\n\n```\n", "@tomrunia Thanks for the clear and self-contained repro code. It really helps us to have good quality reports for performance issues like this (and I especially like having the timelines!) \n\nWe'll take a look into this.\n", "I spent some time looking into what's going on here...   \n\nAs I mentioned earlier, your input batch of  [64, 30, 49, 512] \\* tf.float32 equates to 192MB...  \n\nFirst, I established a baseline cost for memcpying a tensor of the size you are feeding.  A native C++ program calling memcpy in a tight loop between buffers of this size takes around **22ms** per iteration (single threaded).  The TensorFlow enqueue/dequeue operations actually use the _Eigen_ library to copy out slices of the tensors and this is likely to be **less** efficient than a flat memcpy since it needs to handle general shapes of n-D arrays.   Theoretically Eigen might be able to do this in parallel, but doesn't appear to be in this case (from looking at the CPU usage).\n\nBecause you are feeding in numpy arrays, and retrieving a large result, you incur a copy in each direction within the session.Run() call,  [here](https://github.com/tensorflow/tensorflow/blob/dfb71ea206eb9f61e5d97c9727caa1a6449e39cb/tensorflow/python/client/tf_session_helper.cc#L445) and [here](https://github.com/tensorflow/tensorflow/blob/dfb71ea206eb9f61e5d97c9727caa1a6449e39cb/tensorflow/python/client/tf_session_helper.cc#L525) for TensorFlow 0.9.  \n\nNote - this code path is slightly different in various versions of TensorFlow.  Also, if you ever move to the distributed runtime these 192MB tensors will get serialized as ProtoBufs and this is _very_ expensive.\n\nAlso, as @mmry says, some of the CPU time related to enqueue operations is most likely being accounted to the `DequeueMany` op due to [this](https://github.com/tensorflow/tensorflow/blob/r0.9/tensorflow/core/kernels/queue_base.cc#L336) code - once the queue is full, enqueue ops are blocked and wait in a list.  They can then get executed the next time a dequeue succeeds.\n\nNote that a queue with max size of 1000 elements is about **3GB**, so depending on your machine config you may be causing a lot of virtual memory (and allocator) pressure.  This _isn't_ happening on my machine, but you may want to check by running `htop` or `vmstat` while executing your program.  A smaller queue may be more sensible.\n\nIn general, you may be better off using one of the TensorFlow input ops which reads image data directly from the file sytem, and then do any preprocessing as part of the graph.  (as opposed to preprocessing in Python and feeding in the raw tensor data).\n\nFor reference, I've attached a [pprof](http://goog-perftools.sourceforge.net/doc/cpu_profiler.html) profile of your code (with a few relatively insignificant changes).  You can see that the bulk of the cycles are spent in `TF_Run_wrapper_helper` (doing memcpys of the numpy arrays), and under `QueueBase::FlushUnlocked` doing both the enqueue and dequeue copy ops via _Eigen_ (one of which ends up turning into a `memcpy`)\n\n**EDIT:**  You will see that there also seems to be a decent amount of time spent in libcuda (for which we have no symbols).  This is most likely due to the memory being used for host to device transfers not being \"pinned\".  This appears to cause the Nvidia driver to throw out the anchors and either copy the data to a DMA'able buffer or pin the relevant pages. Net result is that those transfers also take about the same time as a memcpy and consume a lot of CPU.  TensorFlow has heuristics which attempt to allocate tensors in pinned memory when they need to be DMA'd.  It may be the case that they are not working well here. @poxvoculi may know more?\n\nHope this helps .... \nPaul\n\n![profile](https://cloud.githubusercontent.com/assets/11547801/16394032/140cbc64-3c68-11e6-8fe3-6335de4ab229.png)\n", "Hi Paul, thanks a lot for your incredibly detailed analysis of what's happening here. This give me much more insight in what's is happening under the hood. I will rethink about the data processing (input) pipeline and see if I can optimize it here and there and decrease the queue size. Great work with the analysis!\n", "I was facing similar issues, copying large amounts of data from numpy to TF by using a FIFOQueue manually. I moved all the data loading and preprocessing into the graph (using https://www.tensorflow.org/versions/r0.9/how_tos/reading_data/index.html) and jpeg_decode, resize_images etc functionalities, and it sped up the training significantly.\n", "I've noticed memcpy's being mentioned in this and [other](https://github.com/tensorflow/tensorflow/issues/2919) issues related to performance. I'm guessing memcpy is used to avoid thread-safety issues, in particular to support operations that take references to tensors such as **assign**.\n\nHas any thought been given to introducing Copy-On-Write functionality to tensors within the graph? Numpy handles [this](http://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.flags.html#numpy.ndarray.flags) but obviously without the tricky thread safety issue.\n\nI ask because it seems wasteful in terms of I/O and memory to spend a memcpy feeding a numpy array onto a FIFOQueue and another memcpy (within a graph) copying the resultant tensor off the FIFOQueue. Ideally it would be nice to use FIFOQueue as a a pure synchronisation construct ala. go channels, python Queues rather a sychronised buffer. Clearly this needs to happen in cases where tensors cross device boundaries (GPU's/Network interfaces) but might there be scope for performance gains here?\n\nI also notice that memcpy's happen within slicing operations. Could this be replaced with a Eigen::TensorMap (homologous to a numpy view) for e.g.?\n", "I think @prb12 's really detailed analysis answers the question. There is always a tradeoff in using convenient and general operations for data manipulation as well as using numpy source data. We are continually working to improve performance. Automatic fusion of ops as alluded in the original paper's future work and issue #164 will provide generality and good performance.  \n\nClosing for now.\n", "@prb12 Thanks for your detailed analysis! I am facing similar issues (timeline attached). I'm using tensorflow read operations throughout, so I believe my problem is not the numpy<->TF data copy. However your comment on virtual memory/allocator pressure seems related to my problem. Here's a screenshot from `htop`, and also the `vmstat` output\n\n``` text\nprocs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----\n r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st\n 4  0 9744848 1865340 2888112 45542040    1    8   118    50    0    0 21 18 61  0  0\n```\n\n![screen shot 2016-07-25 at 1 24 57 pm](https://cloud.githubusercontent.com/assets/1893429/17116146/46c40090-526b-11e6-8071-ef45b0fdc881.png)\nIt seems my virtual memory use is 160G, do you think that might be the problem? I am not really familiar with these system level details, so I would greatly appreciate if you could elaborate on this problem. Thank you!\n\n![screen shot 2016-07-25 at 10 55 04 am](https://cloud.githubusercontent.com/assets/1893429/17116222/a385f1f8-526b-11e6-846f-53ce2140edf1.png)\n\n[Update]\nI tried running the same model with much lower queue capacity (2*batch_size, where batch_size is 288 and my images are 224x224x3) and min_after_dequeue=10). The virtual memory use on htop is still 160G, and I am again experiencing this random slowdown after 1000 iterations. I'm happy to provide any information/logs you might need to help us fix this problem! Here's my data reading code (just in case)\n\n``` python\n    example_list = [_read_image_from_disk(filename_queue, train) for _ in range(num_readers)]\n    if train:\n        example_batch, label_batch = tf.train.shuffle_batch_join(\n                example_list, batch_size=batch_size,\n                capacity=2 * batch_size,\n                min_after_dequeue=10)\n    else:\n        example_batch, label_batch = tf.train.batch_join(example_list, batch_size)\n\n```\n", "I have an additional observation that might help us debug this issue. The slowdown for me happens when my cache memory gets full. I tried clearing the cache using:\n\n``` bash\n$ sync && sudo echo 3 > /proc/sys/vm/drop_caches\n```\n\nThe training then runs perfectly until the cache gets ~3/4 full again (as seen by the yellow bars on `htop`), and then it gets slow. So far my solution is to clear cache (requires `sudo` though) and the training gets fast again. Hoping there's a better way to fix this!\n", "@rohitgirdhar clearing cache works for me, when i was training inception v3.\n", "@rohitgirdhar I'm not 100% sure I want to add to a closed issue, commenting on something which may not even be related to the original problem, but...\n\nFrom the `htop` output and your `drop_caches` observation it sounds to me like there may be some memory pressure caused by virtual address space fragmentation and high system buffer cache churn (reading large training datasets from the file system).  Can you see if your program is causing lots of page faults?  e.g. [like this](http://www.cyberciti.biz/faq/linux-command-to-see-major-minor-pagefaults/)\n\nInternally at google we almost always use the [TCMalloc](http://goog-perftools.sourceforge.net/doc/tcmalloc.html) memory allocator which appears to be much better for TensorFlow workloads (and most others at google!).  When running the open source TensorFlow build via a standard Python binary the only way to use TCMalloc would be to inject a different heap implementation into Python using `LD_PRELOAD`, e.g.\n`LD_PRELOAD=\"/usr/lib/libtcmalloc.so\" python myprogram.py`\n\nIt would be very useful if you could try running with TCMalloc and see if it makes any difference.\n", "The relevant issue is https://github.com/tensorflow/models/issues/170\n", "I think TCMalloc was the solution for me, as I no longer need to clear the cache to maintain the same training speed. Thanks @prb12 !\n", "Is there any progress on improving Numpy -> GPU throughput with Tensorflow?\n\nI have a similar benchmark to @tomrunia of passing a O(100MB) tensor from Numpy to GPU and returning a small slice of the tensor. This would ideally be bottlenecked by PCI-e bandwidth, but runs at ~1/10th of that rate. Additionally, I see very poor scaling when attempting to run this tasks over multiple GPUs (on a single node), while theoretically I should benefit from the additional PCI-e bandwidth. (all stats from Tensorflow 0.9 built from source).\n\n@prb12's description is very useful, but I'd like to understand more of what's going on when passing through feed_dict, enqueuing, dequeuing.\nMy understanding:\nWhen a Numpy tensor is passed through a feed_dict it is copied into a host side Eigen tensor.\nWhen a tensor is enqueued, it is added to a queue that lives on the host.\nWhen a tensor is dequeued on the GPU, a cudaMemcpy copies the tensor from host to device. This should run near PCI-e bandwidth.\n\nQuestions about my understanding:\nIs enqueueing a tensor done by reference or with a full copy (and memory allocation)?\nAre the memcpy's on feed_dict passing (and enqueue if there's a memcpy there) performed by a single thread for the full Tensorflow session? Even if the `Session.run` calls come from multiple Python threads?\n\nAre there plans to fix this, such as a queue that is resident on the GPU?\n", "@prb12 I tried TCMalloc on AlexNet with 1-4 Pascal-grade GPUs (Titan X, GP100). The queues are doing the threading for the CPU-side JPEG decoding, and since GPUs have very high throughput, threading of the CPU-side code is critical here. TCMalloc does speed things up by 20% for 1-4 threads (inter/intra threads, threads associated with custom queues, etc.), but there's a break-even point at 8 threads, and beyond that TCMalloc does more harm than good. At 20 threads (e,g, on a 20-core Intel Xeon E7-8870 v4), perf. is actually 40% down by using TCMalloc rather than vanilla malloc in such a heavily threaded environment. So, I doubt that TCMalloc is a universally beneficial solution.\n", "TCMalloc also seems to suffer from page faults, but it deals with them better for some reason. You can prominently see page fault activity in the kernel using `perf top -p <PID> -g`. Jemalloc gets completely horked on this when training on large datasets like ImageNet, though. 50+% drop in performance, and never recovers on its own. This is _really bad_ user experience. I would wager most TF users don't have the skills to diagnose or fix this. I'd much prefer something as straightforward as a data loader would work out of the box.", "As mentioned by @prb12 in the comment dated June 27, 2016,  based on my Cuda profiles using nvprof, I also see that that host to device transfers are not pinned in Tensor Flow v 1.2.1. Any ideas on when this will be fixed?", "it's probably better to open a separate issue to address the \"not pinned\" problem and link to relevant info, so that it could get triaged properly", "@rohitgirdhar , Could you paste your code about function _read_image_from_disk()? I have verify that parse_example is much faster than parse_single_example."]}, {"number": 3008, "title": "NotImplementedError for learn.TensorFlowEstimator.restore", "body": "I had saved a model using tensorflow.contrib.learn and am currently trying to restore it. However, I am getting a `NotImplementedError`.\n\n```\n---------------------------------------------------------------------------\nNotImplementedError                       Traceback (most recent call last)\n<ipython-input-9-2a884c20d327> in <module>()\n----> 1 gender_classifier = learn.TensorFlowEstimator.restore('gender_classifier_model/')\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/base.pyc in restore(cls, path, config)\n    336       custom_estimator = TensorFlowEstimator(model_fn=None, **model_def)\n    337       # pylint: disable=protected-access\n--> 338       custom_estimator._restore(path)\n    339       return custom_estimator\n    340 \n\n/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/base.pyc in _restore(self, path)\n    295       path: Path to checkpoints and other information.\n    296     \"\"\"\n--> 297     raise NotImplementedError\n    298 \n    299   @classmethod\n```\n\nI realized that this is indeed not implemented in the [latest commit](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/estimators/base.py).\n\nUntil this is implemented, are there any workarounds? I tried pickling the model but was not able to do so, as it is a Module object.\n", "comments": ["For workarounds, please ask this question on StackOverflow.  Please ask it there and tag it with the `tensorflow` tag.\n", "NotImplemented means that it never should be implemented? Or it will be implemented later? Or this is some wrong usage issue?\n", "To save and restore model just pass the same `model_dir` into Estimator.\n\nFor example:\n\n```\nest = LinearClassifier(model_dir='/tmp/dir1', ...)\nest.fit(...)\n...\nest2 = LinearClassifier(model_dir='/tmp/dir1', ...)\nest2.fit(..)\n```\n", "@ilblackdragon , for custom models, it doesn't seem to be working.(Like when using the TensorFlowEstimator).\nIs there any workaround?\n", "Got it to work with initializing classifier as classifier=Estimator(model_fn=function_name,model_dir=dir_path). If the original function is not specified, it fails.\nBut, then, I am running into[ this issue](https://github.com/tensorflow/tensorflow/issues/2797), as I am using a ConvNet(from mnist example of skflow).\n", "@rifatmahmud  Do you have an example where you got it to work?\n", "Is there  any solution for tf.contrib.learn.TensorFlowRNNRegressor? It seems that TensorFlowRNNRegressor dose not have parameter \"model_dir\". Please help me. Thanks!\n", "```\nclassifier = learn.TensorFlowRNNClassifier.restore('/opt/model')\n\n\n//anaconda/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/base.pyc in _restore(self, path)\n    418       path: Path to checkpoints and other information.\n    419     \"\"\"\n--> 420     raise NotImplementedError\n    421 \n    422   @classmethod\n\nNotImplementedError:\n```\n\nAny solution yet ? \n", "Exactly same problem here. The learn.TensorFlowEstimator class does not have model_dir as an argument, and its _restore function is just empty. Any one can help? Thank you very much!\n", "@rifatmahmud  In the work around you mentioned. That is using its base class, so do you mean using the base class to fit, save and restore? But that base class even doesn't have a predict_proba function... Thanks!\n", "I wasn't able to make it works, so I just started to use original tensowrflow without skflow wrapper. Seems like skflow wrapper is very fresh.\n", "Please don't use `TensorFlowEstimator` it's deprecated and will be removed in next version.\nUse `Estimator` for custom models and then you have `model_dir` to specify to load from existing saved checkpoint.\n", "@ilblackdragon thanks for your response. I already wrote a tensorflow graph myself and make it work. The `Estimator` class seems not having a model_type as an argument; i.e., I can't choose what kind of cell to pass in as my network's major component. And it does not have predict_proba function either. Thanks!\n", "@IsaacBanjo Not sure what do you mean by model_type? You can pass any parameter via `params` that you will receive in your model function. To get `predict_proba` just run `predict` on the probability predictions. \n\nE.g. it's just `est.predict(..., outputs=['probability'])['probability']` if your model function returns something like this `return {'probability': probs, 'class': tf.argmax(probs, axis=1)}, loss, train_op`\n", "@ilblackdragon I meant model_fn. Previously in my machine there is no such field. Now I see it at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/estimators/estimator.py. Thanks!\n", "@ilblackdragon When you say \"if your model function returns something like this return {'probability': probs, 'class': tf.argmax(probs, axis=1)}, loss, train_op\", can you give an example of what you define for probs? Thank you!"]}, {"number": 3007, "title": "Tensorflow with ubuntu 16.04 doesn't work", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System: Ubuntu 14.04\n\nInstalled version of CUDA and cuDNN: None\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide: NA\n1. Which pip package you installed. NA\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`. \n\nIf installed from sources, provide the commit hash:\n### Steps to reproduce\n1. I change the Dockerfile that you have provided to get ubuntu 16.04. i.e. FROM ubuntu:14.04 -> ubuntu:16.04\n2. Also changed to git checkout to 0.9\n3. It was for CPU only, so nothing else has changed regarding GPU setting.\n### What have you tried?\n1. I change the version back to 14.04, which worked flawlessly.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n\nWould you let me know if you provide tensorflow Dockerfile for ubuntu 16.04? Soon, I will connect this to Nvidia-docker as well and NVIDIA already provides the ubuntu 16.04 support.\n", "comments": []}, {"number": 3006, "title": "TensorBoard doesn't display Events under Safari", "body": "I've been attempting to view events in TensorBoard, using both the mnist_with_summaries.py tutorial, and a much simpler example from the \"Hello, TensorFlow!\" article (example code at the bottom of this page: https://www.oreilly.com/learning/hello-tensorflow). In both cases, I can see the Graph, but nothing under Events.\n### Environment info\n\nOperating System: OSX 10.11.5\nRunning Python 3.5.1 (installed from the DMG installer on python.org)\nInstalled tensorflow 0.9.0rc0 via pip3\n### Steps to reproduce\n1. Paste the following code into a file:\n\n``` python\nimport tensorflow as tf\n\nx = tf.constant(1.0, name='input')\nw = tf.Variable(0.8, name='weight')\ny = tf.mul(w, x, name='output')\ny_ = tf.constant(0.0, name='correct_value')\nloss = tf.pow(y - y_, 2, name='loss')\ntrain_step = tf.train.GradientDescentOptimizer(0.025).minimize(loss)\n\nfor value in [x, w, y, y_, loss]:\n    tf.scalar_summary(value.op.name, value)\n\nsummaries = tf.merge_all_summaries()\n\nsess = tf.Session()\nsummary_writer = tf.train.SummaryWriter('log_simple_stats', sess.graph)\n\nsess.run(tf.initialize_all_variables())\nfor i in range(100):\n    summary_writer.add_summary(sess.run(summaries), i)\n    sess.run(train_step)\n```\n1. Run the code, then after it is complete, run \"tensorboard --logdir=log_simple_stats/\"\n2. Go to localhost:6006, under Events, there are no Runs listed.\n   ### What have you tried?\n3. Adding a flush() call to the summary writer\n4. Copying the css and js directories under tensorboard/lib, from source (this had no effect, so I reverted the change)\n### Logs or other output that would be helpful\n\nThe log output looks like:\nLooking in log_simple_stats reveals:\n\n``` bash\n-rw-r--r--  1 jason  staff  19030 Jun 23 01:11 events.out.tfevents.1466669497.hostname.local\nMD5 (events.out.tfevents.1466669497.hostname.local) = f7032e6e643bbac6b0009411a02cb8db\n```\n\nAnd the logs from tensorboard:\n\n``` bash\n~ tensorboard --logdir=log_simple_stats/\nWARNING:tensorflow:Found more than one graph event per run. Overwriting the graph with the newest event.\nWARNING:tensorflow:IOError [Errno 2] No such file or directory: '/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/tensorboard/TAG' on path /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/tensorboard/TAG\nWARNING:tensorflow:Unable to read TensorBoard tag\nStarting TensorBoard  on port 6006\n(You can navigate to http://0.0.0.0:6006)\n127.0.0.1 - - [23/Jun/2016 01:11:11] \"GET / HTTP/1.1\" 200 -\n127.0.0.1 - - [23/Jun/2016 01:11:11] \"GET /lib/css/global.css HTTP/1.1\" 200 -\n127.0.0.1 - - [23/Jun/2016 01:11:11] \"GET /external/d3/d3.min.js HTTP/1.1\" 200 -\n127.0.0.1 - - [23/Jun/2016 01:11:11] \"GET /external/lodash/lodash.min.js HTTP/1.1\" 200 -\n...\n```\n", "comments": ["After re-reading the TensorBoard README again, I realized what the issue might be... I was using Safari instead of Chrome. Trying out Chrome 51, the Events tab displays properly now. TensorBoard wasn't working for me with Safari 9.1.1, and title updated to reflect.\n", "I think the pip package is outdated; the particular issue you describe (graphs tab works, events tab never works) is fixed in a newly-built pip package, and we will rebuild the pip package before release. However, in the local testing I'm doing, Safari is pretty spotty, I think because it doesn't play well with webcomponents/Polymer. We don't currently consider Safari a supported browser so I don't invest much in making sure that it's working. But both Chrome and FF are fine.\n\nIf you feel strongly that we should be supporting Safari please say so. (Especially if you are willing to do the work to improve support :) )\n", "I don't think missing Safari support is that big a deal (more just a nice-to-have), but it may help others if TB displayed a warning when loading inside of Safari, explaining it's not a supported browser (like a dismiss-able banner, or even on the command line if it detects a Safari user agent). I'm sure like me, others on the Mac will make the same mistake, since in the tutorials for TensorBoard there's no mention of browser incompatibilities. Even the first time I skimmed the TensorBoard README, I missed the browser compatibility warning.\n", "Just for the record, supporting all popular browsers is a big deal and must-to-have. Now tensorboard is working in safari after the update :)\r\n\r\nIn the long run, cross-browser support should stay."]}, {"number": 3005, "title": "Tensorflow Variables Copy Issue", "body": "I am using Tensorflow 0.8 to train the Deep Neural Networks. Currently, I encounter an issue that I want to define two exact same Neural Networks N1 and N2, and I train N1, during the training loop, I copy updated weights from N1 to N2 every 4 iterations. In fact, I know there is way using `tf.train.saver.save()` to save all N1 weights into a `.ckpt` file on Disk, and using `tf.train.saver.restore()` to load those weights from `.ckpt` file, which is equivalent to the copy functionality. However, this load/reload will impact the training speed, and I wonder if there are other more efficient ways to do the copy (For example, do in-memory copy, etc.). Thanks!\n", "comments": ["This is a question better suited for StackOverflow. Please ask it there and tag it with the `tensorflow` tag\n"]}, {"number": 3004, "title": "[iOS] Undefined symbols for architecture armv7 Error", "body": "While following the iOS readme installation instructions, the framework didn't compile for iOS. Below are the details.\n\nOperating System: OS X El Capitan 10.11.5\npip package: Mac OS X, CPU only, Python 2.7\nTensorFlow version: 0.9.0rc0\n### Steps\n\n1.\n\n``` bash\nsh tensorflow/contrib/makefile/download_dependencies.sh\n```\n\n2.\n\n``` bash\ncd tensorflow/contrib/makefile/downloads/protobuf/\n./autogen.sh\n./configure\nmake\nsudo make install\ncd ../../../../..\n```\n1. `sh tensorflow/contrib/makefile/compile_ios_protobuf.sh`\n2. `sh tensorflow/contrib/makefile/compile_ios_tensorflow.sh`\n### Log showing error\n\n```\nUndefined symbols for architecture armv7:\n  \"tensorflow::shape_inference::UnchangedShape(tensorflow::shape_inference::InferenceContext*)\", referenced from:\n      ___cxx_global_var_init.17 in libtensorflow-core-armv7.a(math_ops.o)\n      ___cxx_global_var_init.24 in libtensorflow-core-armv7.a(math_ops.o)\n      ___cxx_global_var_init.27 in libtensorflow-core-armv7.a(math_ops.o)\n      ___cxx_global_var_init.31 in libtensorflow-core-armv7.a(math_ops.o)\n      ___cxx_global_var_init.37 in libtensorflow-core-armv7.a(math_ops.o)\n      ___cxx_global_var_init.41 in libtensorflow-core-armv7.a(math_ops.o)\n      ___cxx_global_var_init.44 in libtensorflow-core-armv7.a(math_ops.o)\n      ...\nld: symbol(s) not found for architecture armv7\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\nmake: *** [/Users/Main/Documents/tensorflow/tensorflow/contrib/makefile/gen/bin/benchmark] Error 1\narmv7 compilation failed.\n```\n", "comments": ["I'm also experiencing exactly the same error.   The CameraExample sample app won't compile due to this missing reference.\n", "Sorry about the recent issues with iOS building. I've added the missing file, along with an Eigen patch, in #3075. Can you give this a try and let me know if you're still having issues?\n", "Thanks. Do I need to reinstall tensorflow from master?\n", "I would do a git clone https://github.com/tensorflow/tensorflow into a clean directory, and then try the tensorflow/contrib/makefile/build_all_ios.sh script again.\n", "new error while building the framework: \n\n``` bash\ngcc --std=c++11 -DIS_SLIM_BUILD  -miphoneos-version-min=9.2 -arch armv7 -D__thread= -Wno-c++11-narrowing -mno-thumb -DTF_LEAN_BINARY -D__ANDROID_TYPES_SLIM__ -DMIN_LOG_LEVEL=0 -fno-exceptions -isysroot /Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS9.3.sdk -I/usr/local/include -I. -I/Users/Main/Documents/tensorflow/tensorflow/contrib/makefile/downloads/ -I/Users/Main/Documents/tensorflow/tensorflow/contrib/makefile/downloads/eigen-eigen-334b1d428283 -I/Users/Main/Documents/tensorflow/tensorflow/contrib/makefile/gen/proto/ -I/Users/Main/Documents/tensorflow/tensorflow/contrib/makefile/gen/proto_text/ -I/Users/Main/Documents/tensorflow/tensorflow/contrib/makefile/gen/protobuf-host/include -c tensorflow/core/kernels/xent_op.cc -o /Users/Main/Documents/tensorflow/tensorflow/contrib/makefile/gen/obj/tensorflow/core/kernels/xent_op.o\nIn file included from tensorflow/core/kernels/xent_op.cc:22:\nIn file included from ./tensorflow/core/framework/op_kernel.h:22:\nIn file included from ./tensorflow/core/framework/allocator.h:25:\nIn file included from ./tensorflow/core/framework/type_traits.h:22:\n./tensorflow/core/framework/types.h:31:10: fatal error: 'tensorflow/core/framework/types.pb.h'\n      file not found\n#include \"tensorflow/core/framework/types.pb.h\"\n         ^\n1 error generated.\nmake: *** [/Users/Main/Documents/tensorflow/tensorflow/contrib/makefile/gen/obj/tensorflow/core/kernels/xent_op.o] Error 1\narmv7 compilation failed.\n```\n", "before running the make command, I ran `compile_ios_protobuf.sh`, which took quite a while and seemed to have succeeded.\n", "This is usually an error when running parallel builds (e.g. make -j 8), because the makefile dependencies weren't set up quite right. I've added the right dependencies to #3104 so once that's in this should be fixed.\n", "Worked! Thanks @petewarden.\n\n![img_0025](https://cloud.githubusercontent.com/assets/3888042/16468501/2610c65e-3e1a-11e6-9a83-84480171c7a1.PNG)\n", "Awesome to see, thanks for the update!\n", "hi, @petewarden @sitefeng \nI met a similar problem with the latest tensorflow source code, \n\nmy operation environment is:\nOperating System: OS X El Capitan 10.11.5\npip package: Mac OS X, CPU only, Python 2.7\n\nthe error message is \n\n```\nUndefined symbols for architecture x86_64:\n  \"google::protobuf::io::CodedInputStream::ReadVarintSizeAsIntFallback()\", referenced from:\n      google::protobuf::io::CodedInputStream::ReadVarintSizeAsInt(int*) in test_log.pb.o\n      google::protobuf::io::CodedInputStream::ReadVarintSizeAsInt(int*) in saved_tensor_slice.pb.o\n      google::protobuf::io::CodedInputStream::ReadVarintSizeAsInt(int*) in event.pb.o\n      google::protobuf::io::CodedInputStream::ReadVarintSizeAsInt(int*) in tensorflow_server.pb.o\n      google::protobuf::io::CodedInputStream::ReadVarintSizeAsInt(int*) in named_tensor.pb.o\n      google::protobuf::io::CodedInputStream::ReadVarintSizeAsInt(int*) in meta_graph.pb.o\n      google::protobuf::io::CodedInputStream::ReadVarintSizeAsInt(int*) in config.pb.o\n      ...\n  \"google::protobuf::MessageLite::InternalSerializeWithCachedSizesToArray(bool, unsigned char*) const\", referenced from:\n      vtable for google::protobuf::internal::MapEntryBase in test_log.pb.o\n      vtable for google::protobuf::internal::MapEntryBase in tensorflow_server.pb.o\n      vtable for google::protobuf::internal::MapEntryBase in meta_graph.pb.o\n      vtable for google::protobuf::internal::MapEntryBase in config.pb.o\n      vtable for google::protobuf::internal::MapEntryBase in graph.pb.o\n      vtable for google::protobuf::internal::MapEntryBase in function.pb.o\n      vtable for google::protobuf::internal::MapEntryBase in attr_value.pb.o\n      ...\nld: symbol(s) not found for architecture x86_64\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\nmake: *** [/Users/fengjian/my-work/machine-learning/tensorflow/tensorflow/contrib/makefile/gen/host_bin/proto_text] Error 1\n+ '[' 2 -ne 0 ']'\n+ echo 'armv7 compilation failed.'\narmv7 compilation failed.\n+ exit 1\n```\n\nany suggestion?\n", "@petewarden Any clue about this last issue posted by @fengjian0106 \n", "No, but I'm hoping that the changes we've made to the build_all_ios.sh script since July have fixed this.\n"]}, {"number": 3003, "title": "Update Eigen to new version with cumsum/cumprod fixes", "body": "Would it be okay to update the version of Eigen to one that contains the fixes needed for #2711 to get merged?\nThis PR changes the version to the one right after the fixes were merged.\n", "comments": ["Can one of the admins verify this patch?\n", "Eigen has now been updated at https://github.com/tensorflow/tensorflow/commit/bb401633a9a1e5cfc3b6f051e000d9739395d6db, so this isn't needed anymore.\n"]}, {"number": 3002, "title": "google.protobuf.message.DecodeError: Truncated message", "body": "I retrained a Inception model using my data and saved the label txt and pd file. I tried to run @wolffg \ncode lab file to load my trained network on my Raspberry Pi and test on another image. But I got this error below:\npi@raspberrypi:~/robotimages $ python retrain_test.py /home/pi/robotimages/Robot1/w/1.jpg\nTraceback (most recent call last):\n  File \"retrain_test.py\", line 15, in <module>\n    graph_def.ParseFromString(f.read())\n  File \"/usr/local/lib/python2.7/dist-packages/google/protobuf/message.py\", line 185, in ParseFromString\n    self.MergeFromString(serialized)\n  File \"/usr/local/lib/python2.7/dist-packages/google/protobuf/internal/python_message.py\", line 1091, in MergeFromString\n    if self._InternalParse(serialized, 0, length) != length:\n  File \"/usr/local/lib/python2.7/dist-packages/google/protobuf/internal/python_message.py\", line 1127, in InternalParse\n    pos = field_decoder(buffer, new_pos, end, self, field_dict)\n  File \"/usr/local/lib/python2.7/dist-packages/google/protobuf/internal/decoder.py\", line 610, in DecodeRepeatedField\n    raise _DecodeError('Truncated message.')\ngoogle.protobuf.message.DecodeError: Truncated message.\npi@raspberrypi:~/robotimages $ \n", "comments": ["That appears to be an issue with the model file loading. It's the sort of problem I'd expect to see if the model file was truncated or corrupt in some way. Can you check that you can load the model file on another machine?\n", "Thank you for your answer. Yes, it can somehow be loaded on my Desktop but\nhas another different error:\n\n(tensorflow)xu@xu-ThinkCentre-M72e:~/Robotimages $ python retrain_test.py\n/home/xu/Robotimages/Robot1/w/1.jpg\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA\nlibrary libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA\nlibrary libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA\nlibrary libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA\nlibrary libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA\nlibrary libcurand.so locally\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:900] successful NUMA\nnode read from SysFS had negative value (-1), but there must be at least\none NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with\nproperties:\nname: GeForce GTX 750 Ti\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.2545\npciBusID 0000:01:00.0\nTotal memory: 2.00GiB\nFree memory: 1.80GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow\ndevice (/gpu:0) -> (device: 0, name: GeForce GTX 750 Ti, pci bus id:\n0000:01:00.0)\nW tensorflow/core/kernels/batch_norm_op.cc:36] Op is deprecated. It will\ncease to work in GraphDef version 9. Use tf.nn.batch_normalization().\nW tensorflow/core/kernels/batch_norm_op.cc:36] Op is deprecated. It will\ncease to work in GraphDef version 9. Use tf.nn.batch_normalization().\nW tensorflow/core/kernels/batch_norm_op.cc:36] Op is deprecated. It will\ncease to work in GraphDef version 9. Use tf.nn.batch_normalization().\nE tensorflow/core/common_runtime/executor.cc:332] Executor failed to create\nkernel. Invalid argument: NodeDef mentions attr 'T' not in Op<name=MaxPool;\nsignature=input:float -> output:float; attr=ksize:list(int),min=4;\nattr=strides:list(int),min=4; attr=padding:string,allowed=[\"SAME\",\n\"VALID\"]; attr=data_format:string,default=\"NHWC\",allowed=[\"NHWC\", \"NCHW\"]>;\nNodeDef: pool = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 3, 3, 1],\npadding=\"VALID\", strides=[1, 2, 2, 1],\n_device=\"/job:localhost/replica:0/task:0/gpu:0\"](pool/control_dependency)\n     [[Node: pool = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 3, 3,\n1], padding=\"VALID\", strides=[1, 2, 2, 1],\n_device=\"/job:localhost/replica:0/task:0/gpu:0\"](pool/control_dependency)]]\nTraceback (most recent call last):\n  File \"retrain_test.py\", line 86, in <module>\n    run_inference_on_image()\n  File \"retrain_test.py\", line 69, in run_inference_on_image\n    {'DecodeJpeg/contents:0': image_data})\n  File\n\"/home/xu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py\",\nline 340, in run\n    run_metadata_ptr)\n  File\n\"/home/xu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py\",\nline 564, in _run\n    feed_dict_string, options, run_metadata)\n  File\n\"/home/xu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py\",\nline 637, in _do_run\n    target_list, options, run_metadata)\n  File\n\"/home/xu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py\",\nline 659, in _do_call\n    e.code)\ntensorflow.python.framework.errors.InvalidArgumentError: NodeDef mentions\nattr 'T' not in Op<name=MaxPool; signature=input:float -> output:float;\nattr=ksize:list(int),min=4; attr=strides:list(int),min=4;\nattr=padding:string,allowed=[\"SAME\", \"VALID\"];\nattr=data_format:string,default=\"NHWC\",allowed=[\"NHWC\", \"NCHW\"]>; NodeDef:\npool = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 3, 3, 1],\npadding=\"VALID\", strides=[1, 2, 2, 1],\n_device=\"/job:localhost/replica:0/task:0/gpu:0\"](pool/control_dependency)\n     [[Node: pool = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 3, 3,\n1], padding=\"VALID\", strides=[1, 2, 2, 1],\n_device=\"/job:localhost/replica:0/task:0/gpu:0\"](pool/control_dependency)]]\nCaused by op u'pool', defined at:\n  File \"retrain_test.py\", line 86, in <module>\n    run_inference_on_image()\n  File \"retrain_test.py\", line 63, in run_inference_on_image\n    create_graph()\n  File \"retrain_test.py\", line 50, in create_graph\n    _ = tf.import_graph_def(graph_def, name='')\n  File\n\"/home/xu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/importer.py\",\nline 240, in import_graph_def\n    op_def=op_def)\n  File\n\"/home/xu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\",\nline 2154, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File\n\"/home/xu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\",\nline 1154, in __init__\n    self._traceback = _extract_stack()\n\nOn Thu, Jun 30, 2016 at 5:28 AM, Pete Warden notifications@github.com\nwrote:\n\n> That appears to be an issue with the model file loading. It's the sort of\n> problem I'd expect to see if the model file was truncated or corrupt in\n> some way. Can you check that you can load the model file on another machine?\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_tensorflow_tensorflow_issues_3002-23issuecomment-2D229608560&d=CwMCaQ&c=imBPVzF25OnBgGmVOlcsiEgHoG1i6YHLR0Sj_gZ4adc&r=X3YsXL1AZsKHgsh28HFY8A&m=c--SsXNiQfW9z_fKFuiXF-CrOQ3gmfEZMAG2PHXf8L0&s=n1y4Ty2shfn26GvoNBwl9io5dgNEDmmHyLN07zMlcwM&e=,\n> or mute the thread\n> https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_notifications_unsubscribe_ALPsQIfmIg5JsobEbQzYQOltGSmrP0rBks5qQ4wlgaJpZM4I8VDi&d=CwMCaQ&c=imBPVzF25OnBgGmVOlcsiEgHoG1i6YHLR0Sj_gZ4adc&r=X3YsXL1AZsKHgsh28HFY8A&m=c--SsXNiQfW9z_fKFuiXF-CrOQ3gmfEZMAG2PHXf8L0&s=NVHp4g6a1glMwaWyab-SAbF3EuAlkjNRxob0_qaHlAc&e=\n> .\n", "By the way, if I use Bazel to load the same pb and txt file, it works well\non my desktop.\n\nOn Thursday, June 30, 2016, Xu Zhang xz70@duke.edu wrote:\n\n> Thank you for your answer. Yes, it can somehow be loaded on my Desktop but\n> has another different error:\n> \n> (tensorflow)xu@xu-ThinkCentre-M72e:~/Robotimages $ retrain_test.py\n> /home/xu/Robotimages/Robot1/w/1.jpg\n> retrain_test.py: command not found\n> (tensorflow)xu@xu-ThinkCentre-M72e:~/Robotimages $ python retrain_test.py\n> /home/xu/Robotimages/Robot1/w/1.jpg\n> I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA\n> library libcublas.so locally\n> I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA\n> library libcudnn.so locally\n> I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA\n> library libcufft.so locally\n> I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA\n> library libcuda.so.1 locally\n> I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA\n> library libcurand.so locally\n> I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:900] successful\n> NUMA node read from SysFS had negative value (-1), but there must be at\n> least one NUMA node, so returning NUMA node zero\n> I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with\n> properties:\n> name: GeForce GTX 750 Ti\n> major: 5 minor: 0 memoryClockRate (GHz) 1.2545\n> pciBusID 0000:01:00.0\n> Total memory: 2.00GiB\n> Free memory: 1.80GiB\n> I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0\n> I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating\n> TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 750 Ti, pci bus\n> id: 0000:01:00.0)\n> W tensorflow/core/kernels/batch_norm_op.cc:36] Op is deprecated. It will\n> cease to work in GraphDef version 9. Use tf.nn.batch_normalization().\n> W tensorflow/core/kernels/batch_norm_op.cc:36] Op is deprecated. It will\n> cease to work in GraphDef version 9. Use tf.nn.batch_normalization().\n> W tensorflow/core/kernels/batch_norm_op.cc:36] Op is deprecated. It will\n> cease to work in GraphDef version 9. Use tf.nn.batch_normalization().\n> E tensorflow/core/common_runtime/executor.cc:332] Executor failed to\n> create kernel. Invalid argument: NodeDef mentions attr 'T' not in\n> Op<name=MaxPool; signature=input:float -> output:float;\n> attr=ksize:list(int),min=4; attr=strides:list(int),min=4;\n> attr=padding:string,allowed=[\"SAME\", \"VALID\"];\n> attr=data_format:string,default=\"NHWC\",allowed=[\"NHWC\", \"NCHW\"]>; NodeDef:\n> pool = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 3, 3, 1],\n> padding=\"VALID\", strides=[1, 2, 2, 1],\n> _device=\"/job:localhost/replica:0/task:0/gpu:0\"](pool/control_dependency)\n>      [[Node: pool = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 3,\n> 3, 1], padding=\"VALID\", strides=[1, 2, 2, 1],\n> _device=\"/job:localhost/replica:0/task:0/gpu:0\"](pool/control_dependency)]]\n> Traceback (most recent call last):\n>   File \"retrain_test.py\", line 86, in <module>\n>     run_inference_on_image()\n>   File \"retrain_test.py\", line 69, in run_inference_on_image\n>     {'DecodeJpeg/contents:0': image_data})\n>   File\n> \"/home/xu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py\",\n> line 340, in run\n>     run_metadata_ptr)\n>   File\n> \"/home/xu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py\",\n> line 564, in _run\n>     feed_dict_string, options, run_metadata)\n>   File\n> \"/home/xu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py\",\n> line 637, in _do_run\n>     target_list, options, run_metadata)\n>   File\n> \"/home/xu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py\",\n> line 659, in _do_call\n>     e.code)\n> tensorflow.python.framework.errors.InvalidArgumentError: NodeDef mentions\n> attr 'T' not in Op<name=MaxPool; signature=input:float -> output:float;\n> attr=ksize:list(int),min=4; attr=strides:list(int),min=4;\n> attr=padding:string,allowed=[\"SAME\", \"VALID\"];\n> attr=data_format:string,default=\"NHWC\",allowed=[\"NHWC\", \"NCHW\"]>; NodeDef:\n> pool = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 3, 3, 1],\n> padding=\"VALID\", strides=[1, 2, 2, 1],\n> _device=\"/job:localhost/replica:0/task:0/gpu:0\"](pool/control_dependency)\n>      [[Node: pool = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 3,\n> 3, 1], padding=\"VALID\", strides=[1, 2, 2, 1],\n> _device=\"/job:localhost/replica:0/task:0/gpu:0\"](pool/control_dependency)]]\n> Caused by op u'pool', defined at:\n>   File \"retrain_test.py\", line 86, in <module>\n>     run_inference_on_image()\n>   File \"retrain_test.py\", line 63, in run_inference_on_image\n>     create_graph()\n>   File \"retrain_test.py\", line 50, in create_graph\n>     _ = tf.import_graph_def(graph_def, name='')\n>   File\n> \"/home/xu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/importer.py\",\n> line 240, in import_graph_def\n>     op_def=op_def)\n>   File\n> \"/home/xu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\",\n> line 2154, in create_op\n>     original_op=self._default_original_op, op_def=op_def)\n>   File\n> \"/home/xu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\",\n> line 1154, in __init__\n>     self._traceback = _extract_stack()\n> \n> On Thu, Jun 30, 2016 at 5:28 AM, Pete Warden <notifications@github.com\n> <javascript:_e(%7B%7D,'cvml','notifications@github.com');>> wrote:\n> \n> > That appears to be an issue with the model file loading. It's the sort of\n> > problem I'd expect to see if the model file was truncated or corrupt in\n> > some way. Can you check that you can load the model file on another machine?\n> > \n> > \u2014\n> > You are receiving this because you authored the thread.\n> > Reply to this email directly, view it on GitHub\n> > https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_tensorflow_tensorflow_issues_3002-23issuecomment-2D229608560&d=CwMCaQ&c=imBPVzF25OnBgGmVOlcsiEgHoG1i6YHLR0Sj_gZ4adc&r=X3YsXL1AZsKHgsh28HFY8A&m=c--SsXNiQfW9z_fKFuiXF-CrOQ3gmfEZMAG2PHXf8L0&s=n1y4Ty2shfn26GvoNBwl9io5dgNEDmmHyLN07zMlcwM&e=,\n> > or mute the thread\n> > https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_notifications_unsubscribe_ALPsQIfmIg5JsobEbQzYQOltGSmrP0rBks5qQ4wlgaJpZM4I8VDi&d=CwMCaQ&c=imBPVzF25OnBgGmVOlcsiEgHoG1i6YHLR0Sj_gZ4adc&r=X3YsXL1AZsKHgsh28HFY8A&m=c--SsXNiQfW9z_fKFuiXF-CrOQ3gmfEZMAG2PHXf8L0&s=NVHp4g6a1glMwaWyab-SAbF3EuAlkjNRxob0_qaHlAc&e=\n> > .\n", "For me, the retrained inception-v3 model loads properly as I am able to see all the different types of tensors including \"final_result\" and \"DecodeJpeg/contents.\" The model file doesn't seem corrupted. But I do get the exact same DecodeJpeg error as zxzhijia\n", "@ank286 It works for me now after I use tensorflow 0.9.0 rc0.\n", "@ank286 So please have a try and see if it works. \n", "I did a \"pip show tensorflow\" and I have 0.9.0rc0 running.\n\nI still get an error when trying to call import_graph_def(graph_def, name='') with a retrained Inception-v3 model for the flowers data.\n\n`W tensorflow/core/framework/op_def_util.cc:332] Op BatchNormWithGlobalNormalization is deprecated. It will cease to work in GraphDef version 9. Use tf.nn.batch_normalization().\nE tensorflow/core/common_runtime/executor.cc:334] Executor failed to create kernel. Invalid argument: NodeDef mentions attr 'T' not in Op<name=MaxPool; signature=input:float -> output:float; attr=ksize:list(int),min=4; attr=strides:list(int),min=4; attr=padding:string,allowed=[\"SAME\", \"VALID\"]; attr=data_format:string,default=\"NHWC\",allowed=[\"NHWC\", \"NCHW\"]>; NodeDef: pool = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 3, 3, 1], padding=\"VALID\", strides=[1, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](pool/control_dependency)\n         [[Node: pool = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 3, 3, 1], padding=\"VALID\", strides=[1, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](pool/control_dependency)]]\nTraceback (most recent call last):\n  File \"retrain_inference.py\", line 78, in <module>\n    run_inference_on_image()\n  File \"retrain_inference.py\", line 62, in run_inference_on_image\n    {'DecodeJpeg/contents:0': image_data})\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 372, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 636, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 708, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 728, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.InvalidArgumentError: NodeDef mentions attr 'T' not in Op<name=MaxPool; signature=input:float -> output:float; attr=ksize:list(int),min=4; attr=strides:list(int),min=4; attr=padding:string,allowed=[\"SAME\", \"VALID\"]; attr=data_format:string,default=\"NHWC\",allowed=[\"NHWC\", \"NCHW\"]>; NodeDef: pool = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 3, 3, 1], padding=\"VALID\", strides=[1, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](pool/control_dependency)\n         [[Node: pool = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 3, 3, 1], padding=\"VALID\", strides=[1, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](pool/control_dependency)]]\nCaused by op u'pool', defined at:\n  File \"retrain_inference.py\", line 78, in <module>\n    run_inference_on_image()\n  File \"retrain_inference.py\", line 50, in run_inference_on_image\n    create_inception_graph()\n  File \"retrain_inference.py\", line 30, in create_inception_graph\n    tf.import_graph_def(graph_def, name='')\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py\", line 274, in import_graph_def\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2260, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1230, in __init__\n    self._traceback = _extract_stack()\n\n`\n", "@ank286 , is the version which you downloaded from github also 0.9.0 rc0?\nWhat I did is installing using pip, and also downloaded the tensorflow\nsource code from github _with version 0.9.0 rc0_.\n\nOn Tue, Jul 12, 2016 at 10:39 PM, ank286 notifications@github.com wrote:\n\n> I did a \"pip show tensorflow\" and I have 0.9.0rc0 running.\n> \n> I still get an error when trying to call import_graph_def(graph_def,\n> name='') with a retrained Inception-v3 model for the flowers data.\n> \n> `W tensorflow/core/framework/op_def_util.cc:332] Op\n> BatchNormWithGlobalNormalization is deprecated. It will cease to work in\n> GraphDef version 9. Use tf.nn.batch_normalization().\n> E tensorflow/core/common_runtime/executor.cc:334] Executor failed to\n> create kernel. Invalid argument: NodeDef mentions attr 'T' not in Op\n> output:float; attr=ksize:list(int),min=4; attr=strides:list(int),min=4;\n> attr=padding:string,allowed=[\"SAME\", \"VALID\"];\n> attr=data_format:string,default=\"NHWC\",allowed=[\"NHWC\", \"NCHW\"]>; NodeDef:\n> pool = MaxPoolT=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 3, 3, 1],\n> padding=\"VALID\", strides=[1, 2, 2, 1],\n> _device=\"/job:localhost/replica:0/task:0/cpu:0\"\n> http://pool/control_dependency\n> [[Node: pool = MaxPoolT=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 3, 3, 1],\n> padding=\"VALID\", strides=[1, 2, 2, 1],\n> _device=\"/job:localhost/replica:0/task:0/cpu:0\"\n> http://pool/control_dependency]]\n> Traceback (most recent call last):\n> File \"retrain_inference.py\", line 78, in\n> run_inference_on_image()\n> File \"retrain_inference.py\", line 62, in run_inference_on_image\n> {'DecodeJpeg/contents:0': image_data})\n> File\n> \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\",\n> line 372, in run\n> run_metadata_ptr)\n> File\n> \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\",\n> line 636, in\n> \n> _run feed_dict_string, options, run_metadata) File\n> \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\",\n> line 708, in do_run target_list, options, run_metadata) File\n> \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\",\n> line 728, in _do_call raise type(e)(node_def, op, message)\n> tensorflow.python.framework.errors.InvalidArgumentError: NodeDef mentions\n> attr 'T' not in Op output:float; attr=ksize:list(int),min=4;\n> attr=strides:list(int),min=4; attr=padding:string,allowed=[\"SAME\",\n> \"VALID\"]; attr=data_format:string,default=\"NHWC\",allowed=[\"NHWC\", \"NCHW\"]>;\n> NodeDef: pool = MaxPoolT=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 3, 3, 1],\n> padding=\"VALID\", strides=[1, 2, 2, 1],\n> _device=\"/job:localhost/replica:0/task:0/cpu:0\"\n> http://pool/control_dependency [[Node: pool = MaxPoolT=DT_FLOAT,\n> data_format=\"NHWC\", ksize=[1, 3, 3, 1], padding=\"VALID\", strides=[1, 2, 2,\n> 1], _device=\"/job:localhost/replica:0/task:0/cpu:0\"\n> http://pool/control_dependency]] Caused by op u'pool', defined at: File\n> \"retrain_inference.py\", line 78, in run_inference_on_image() File\n> \"retrain_inference.py\", line 50, in run_inference_on_image\n> create_inception_graph() File \"retrain_inference.py\", line 30, in\n> create_inception_graph tf.import_graph_def(graph_def, name='') File\n> \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py\",\n> line 274, in import_graph_def op_def=op_def) File\n> \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\",\n> line 2260, in create_op original_op=self._default_original_op,\n> op_def=op_def) File\n> \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\",\n> line 1230, in __init_\n> self._traceback = _extract_stack()\n> \n> `\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_tensorflow_tensorflow_issues_3002-23issuecomment-2D232238549&d=CwMCaQ&c=imBPVzF25OnBgGmVOlcsiEgHoG1i6YHLR0Sj_gZ4adc&r=X3YsXL1AZsKHgsh28HFY8A&m=waoZCzIzFxoS22WucWKtHijU0IhTrqyDFduXCeF-LOc&s=FvXqaZp-8FfSFq1D_eScxaBgaWiCrKg2k2C0-lI6WSM&e=,\n> or mute the thread\n> https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_notifications_unsubscribe_ALPsQMZ3s79qrZklnaV4Xv8R-2DRqgJlckks5qVE-5FTgaJpZM4I8VDi&d=CwMCaQ&c=imBPVzF25OnBgGmVOlcsiEgHoG1i6YHLR0Sj_gZ4adc&r=X3YsXL1AZsKHgsh28HFY8A&m=waoZCzIzFxoS22WucWKtHijU0IhTrqyDFduXCeF-LOc&s=mUTeu2vyGqlaq982EcGbZIV0pdTDW7uu-jMupff-__Q&e=\n> .\n", "So you installed TensorFlow 0.9.0rc0 using `sudo pip2 install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.9.0rc0-cp27-none-linux_x86_64.whl`\n\nand you also compiled from source? Or you downloaded the source code for 0.9.0rc0 and put it in some folder, which folder? \n\nThank you\n", "Download it to any folder you wants and run\n\n```\n   cd /tensorflow/\n```\n\nbazel build -c opt --copt=-mavx tensorflow/examples/image_retraining:retrain\n\nOn Tuesday, July 12, 2016, ank286 notifications@github.com wrote:\n\n> So you installed TensorFlow 0.9.0rc0 using sudo pip2 install\n> https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.9.0rc0-cp27-none-linux_x86_64.whl\n> \n> and you also compiled from source? Or you downloaded the source code for\n> 0.9.0rc0 and put it in some folder, which folder?\n> \n> Thank you\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_tensorflow_tensorflow_issues_3002-23issuecomment-2D232239827&d=CwMCaQ&c=imBPVzF25OnBgGmVOlcsiEgHoG1i6YHLR0Sj_gZ4adc&r=X3YsXL1AZsKHgsh28HFY8A&m=0K6V8c4J_kMPX5wRSGLYVOtgrD8WeY0-MIbB6LU2VPc&s=hdlhWP6l9wLdF9UqxhbwFUwk4SrXLkMzAMrdZ-IjJZU&e=,\n> or mute the thread\n> https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_notifications_unsubscribe_ALPsQAXJ9xR0wWtvHX19Ss-2DdU87FKVRVks5qVFJLgaJpZM4I8VDi&d=CwMCaQ&c=imBPVzF25OnBgGmVOlcsiEgHoG1i6YHLR0Sj_gZ4adc&r=X3YsXL1AZsKHgsh28HFY8A&m=0K6V8c4J_kMPX5wRSGLYVOtgrD8WeY0-MIbB6LU2VPc&s=9q7KA2v1782-42bPBa5v551_kzclolZU5KN6WxpyfJk&e=\n> .\n", "Then run below in the same folder to retrain. For more detail, you can see\nTensorflow for poets.\n\nbazel-bin/tensorflow/examples/image_retraining/retrain \\\n--bottleneck_dir=/tf_files/bottlenecks \\\n--model_dir=/tf_files/inception \\\n--output_graph=/tf_files/retrained_graph.pb \\\n--output_labels=/tf_files/retrained_labels.txt \\\n--image_dir /tf_files/flower_photos\n\nOn Tuesday, July 12, 2016, Xu Zhang xz70@duke.edu wrote:\n\n> Download it to any folder you wants and run\n> \n> ```\n>    cd /tensorflow/\n> ```\n> \n> bazel build -c opt --copt=-mavx tensorflow/examples/image_retraining:retrain\n> \n> On Tuesday, July 12, 2016, ank286 <notifications@github.com\n> <javascript:_e(%7B%7D,'cvml','notifications@github.com');>> wrote:\n> \n> > So you installed TensorFlow 0.9.0rc0 using sudo pip2 install\n> > https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.9.0rc0-cp27-none-linux_x86_64.whl\n> > \n> > and you also compiled from source? Or you downloaded the source code for\n> > 0.9.0rc0 and put it in some folder, which folder?\n> > \n> > Thank you\n> > \n> > \u2014\n> > You are receiving this because you authored the thread.\n> > Reply to this email directly, view it on GitHub\n> > https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_tensorflow_tensorflow_issues_3002-23issuecomment-2D232239827&d=CwMCaQ&c=imBPVzF25OnBgGmVOlcsiEgHoG1i6YHLR0Sj_gZ4adc&r=X3YsXL1AZsKHgsh28HFY8A&m=0K6V8c4J_kMPX5wRSGLYVOtgrD8WeY0-MIbB6LU2VPc&s=hdlhWP6l9wLdF9UqxhbwFUwk4SrXLkMzAMrdZ-IjJZU&e=,\n> > or mute the thread\n> > https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_notifications_unsubscribe_ALPsQAXJ9xR0wWtvHX19Ss-2DdU87FKVRVks5qVFJLgaJpZM4I8VDi&d=CwMCaQ&c=imBPVzF25OnBgGmVOlcsiEgHoG1i6YHLR0Sj_gZ4adc&r=X3YsXL1AZsKHgsh28HFY8A&m=0K6V8c4J_kMPX5wRSGLYVOtgrD8WeY0-MIbB6LU2VPc&s=9q7KA2v1782-42bPBa5v551_kzclolZU5KN6WxpyfJk&e=\n> > .\n", "Okay, I am retraining now with 0.9.0rc0 and re-generating the .pb model file. I will report back if I can import this model successfully. \nThank you for all your help!\n", "No problem. Let us know if it works.\n\nOn Tuesday, July 12, 2016, ank286 notifications@github.com wrote:\n\n> Okay, I am retraining now with 0.9.0rc0 and re-generating the .pb model\n> file. I will report back if I can import this model successfully.\n> Thank you for all your help!\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_tensorflow_tensorflow_issues_3002-23issuecomment-2D232245205&d=CwMCaQ&c=imBPVzF25OnBgGmVOlcsiEgHoG1i6YHLR0Sj_gZ4adc&r=X3YsXL1AZsKHgsh28HFY8A&m=rgtmIn_hKsaFILmc1mKX5mcoHdqimopev7NIyMEocq0&s=phWZlBe8fYgK4Q2IqihjZWp39BFvLdxUFCDHKCYX09g&e=,\n> or mute the thread\n> https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_notifications_unsubscribe_ALPsQD8iOK2blYMaGkLze0v6uqIKJO7Rks5qVFnSgaJpZM4I8VDi&d=CwMCaQ&c=imBPVzF25OnBgGmVOlcsiEgHoG1i6YHLR0Sj_gZ4adc&r=X3YsXL1AZsKHgsh28HFY8A&m=rgtmIn_hKsaFILmc1mKX5mcoHdqimopev7NIyMEocq0&s=w3dHKeQ_QhDLpdlmH6bVriGumNMxxbtUVvOyAVsK6s4&e=\n> .\n"]}, {"number": 3001, "title": "Use Basic neural network subroutines (BNNS) on iOS", "body": "Take advantage of the [BNNS library that Apple announced for iOS](https://developer.apple.com/reference/accelerate/1912851-bnns).\n", "comments": ["Thanks for suggesting this. At this time we probably do not have the bandwidth to look into it immediately, so I am marking it as a contributions welcome. \n", "I want to work and enhance using BNNS on iOS. Can anybody suggest from where to start.\n", "These sample projects might help: \nhttps://developer.apple.com/library/content/samplecode/MPSCNNHelloWorld/Introduction/Intro.html#//apple_ref/doc/uid/TP40017482\nhttps://developer.apple.com/library/content/samplecode/MetalImageRecognition/Introduction/Intro.html#//apple_ref/doc/uid/TP40017385\nhttps://github.com/shu223/iOS-10-Sampler\n\nFor custom weights / own models, perhaps this might help: https://www.bignerdranch.com/blog/use-tensorflow-and-bnns-to-add-machine-learning-to-your-mac-or-ios-app/ \n", "I had to jump away from this for a while due to other obligations but would love to take a look again. Thanks for the links, hanleyweng!\n", "I demoed MNIST on BNNS with TensorFlow model here: https://github.com/paiv/mnist-bnns\r\n\r\nBNNS is somewhat limited in features. One should make performance comparison before committing.", "@paiv Wondering if you saw any issues like this: http://stackoverflow.com/questions/42641668/using-basic-neural-network-subroutines-bnn-accelerate", "Core ML might be an interesting abstraction to both this and MPS: https://github.com/tensorflow/tensorflow/issues/10468.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Can we close this as we have CORE ML delagate in TF lite?", "I guess we can close this request. Thank you"]}, {"number": 3000, "title": "Memory Leak Converting Numpy ndarry to Tensor", "body": "I'm working on a small convolution network to identify faces which takes png images and loads them into numpy arrays through PIL.  Once all the images are loaded, they are passed into the tensorflow model through `train_op.train(feed_dict={input:train_images})`.  The problem is that each iteration of the training loop copies a new batch from the total loaded images before passing the batch into train_op and that memory is never released.\n\nI've looked and looked and found several related issues to memory not being released but nothing that I thought really fit my problem or had a working solution.  I am aware that this process of pre-loading images is not ideal, but I have other obstacles to using a queue structure to read the images directly to tensors and I would like to address this as a memory leak.\n### Environment info\n\nOperating System: Ubuntu 14.04\nPython: 2.7.6\nTensorflow: 0.8.0\nCUDA: 7.5\ncuDNN: 4.0.7\n### Steps to reproduce\n\nBelow is a short python script that replicates the problem by loading an image (pass a filepath as argument) and passing it to `tf.image.random_flip_left_right()` while printing out the memory in use by on the computer.  \n\n```\nimport tensorflow as tf\nimport psutil\nimport numpy as np\nimport Image\nimport sys\nimport gc\n\ndef printMemUsed(discript):\n    print(\"%s:\\t%d\" % (discript, psutil.virtual_memory().used))\n\ndef main(file):\n    sess = tf.InteractiveSession()\n    im = Image.open(file)\n    arr = np.array(im)\n    printMemUsed(\"After Array Creation\")\n    arr = flipArr(arr)\n    printMemUsed(\"After Tensor Conversion\")\n    del arr\n    printMemUsed(\"After Array Deletion\")\n\ndef flipArr(arr):\n    tensor = tf.image.random_flip_left_right(arr)\n    arr = tensor.eval()\n    return arr  \n\nif __name__ == '__main__':\n    main(sys.argv[1])\n    printMemUsed(\"After Scope Lost\")\n    gc.collect()\n    printMemUsed(\"After gc Collect\")\n```\n\nFor me this program prints:\n\n> After Array Creation:   1196838912\n> After Tensor Conversion:        1273106432\n> After Array Deletion:   1273106432\n> After Scope Lost:       1273106432\n> After gc Collect:       1273106432\n### What have you tried?\n1. As shown in the example, gc.collect() and del do nothing to free the memory, it is not released when the variables pass out of scope (loop or method), only when the program exits, or in my case when I run out of RAM causing the machine to hang until manually restarted.\n2. Again I recognize that there are other (perhaps better) ways to get images into tensorflow, but at least for the start I'd like to address why the memory is not released.\n### Other Comments\n\nThe plot below shows memory usage over the first 5 training rounds of my full model.  The orange points mark the top of each loop.  The first jump is the creation of the batch, which passes all the images through `tf.image.random_flip_left_right()` as in the example above, the second jump from calling `train_op.train(feed_dict={input:train_images})`, passing in the newly created batch.\n\n![memory_leak_plot](https://cloud.githubusercontent.com/assets/10762452/16284315/75e71fec-3885-11e6-915e-a6d9f6610119.png)\n", "comments": ["The problem might be that you're copying the contents of `arr` into the global tensorflow graph each time you call `tf.image.random_flip_left_right(arr)` (as this automatically creates a constant op to hold the contents of `arr`).\nThe global graph persists until your script finishes.\nYou could try to create an op that computes the flipped image and then pass in the array using `feed_dict`, as in this modified version of your script:\n\n```\nimport tensorflow as tf\nimport psutil\nimport numpy as np\nimport Image\nimport sys\nimport gc\n\ndef printMemUsed(discript):\n    print(\"%s:\\t%d\" % (discript, psutil.virtual_memory().used))\n\ndef main(file):\n    sess = tf.InteractiveSession()\n    im = Image.open(file)\n    arr = np.array(im)\n    printMemUsed(\"After Array Creation\")\n    arr = flipArr(arr)\n    printMemUsed(\"After Tensor Conversion\")\n    del arr\n    printMemUsed(\"After Array Deletion\")\n\ndef flipArr(arr):\n    tf_arr = tf.placeholder(dtype=arr.dtype)\n    flipped = tf.image.random_flip_left_right(tf_arr)\n    arr = flipped.eval(feed_dict={tf_arr: arr})\n    return arr  \n\nif __name__ == '__main__':\n    main(sys.argv[1])\n    printMemUsed(\"After Scope Lost\")\n    gc.collect()\n    printMemUsed(\"After gc Collect\")\n```\n\nIt could be helpful to run `print(tf.get_default_graph().as_graph_def())` to check what ends up in your graph after each iteration.\n", "Wow that seemed to solved the problem.  Data is now being returned after the batch creation returns.   However at least at the beginning the batch method is not releasing as much as it consumed and train_op is still leaking, but the amount to which they do so decays out after the first few iterations to a (mostly) even use function.  The graph below shows 10 iterations.\n\n![memory_leak_plot_2](https://cloud.githubusercontent.com/assets/10762452/16287145/e0206aca-3895-11e6-944d-87586d853244.png)\n\nOver 400 iterations in use memory increased by less than 100MB, was formally more like 5GB.  Thank you, that was a silly thing for me to do wrong.\n", "Are you also using `psutil.virtual_memory().used` to produce the graph?\nNot sure if that would give you an accurate picture of the memory consumption.\n", "Yes the graph is generated from the output from `psutil.virtual_memory().used`.  The values printed match the behavior I observe in `top` and since I work on this computer remotely through ssh I'm confident that the tensorflow script is the only thing running and taking up meaningful quantities of memory.\n", "The comment by @lbab is correct - your original code was effectively adding all of the images to the graph as tensorflow constants!  (InteractiveSession probably increases the risk of this sort of mistake)  You could verify this by saving the graphdef to a file and opening it in an editor. \n\n@zmimlitz Note: virtual address space size is not usually a good way to measure memory consumption (depending on all sorts of low level implementation details of the heap, thread stacks and other things, you may end up with lots of virtual pages not backed by physical memory).  Resident set size is usually a much better indicator of what pages are actually being used.  We have noticed that the default linux heap implementation seems to have some quite bad behavior in this respect when running TensorFlow.\n\nFor problems like this, it is often informative to try running with the [TCMalloc](http://goog-perftools.sourceforge.net/doc/tcmalloc.html) heap, which a) performs much better in both space and cost overheads, and b) includes a very useful heap profiler.\n", "Thank you, I'll definitely look into TCMalloc for future memory monitoring.  In the meantime I am satisfied that the original problem I saw was the fault my implementation and has (regardless of viewing) been resolved by the change recommended by @ibab.  I don't seem to have permission to close this Issue but you are welcome to do so.  Thank you for the help.\n", "> I don't seem to have permission to close this Issue but you are welcome to do so. Thank you for the help.\n\nYou're welcome!\n"]}, {"number": 2999, "title": "Problem in  restore a previously saved model", "body": "I used tensorflow 0.9. I want save my model to be reused with that, I simply add tf.train.save() to save and restore my training variables.\n\nThis is my code:\n\n```\n`import tensorflow as tf\nimport input_data\nimport os\n\ncheckpoint_dir='./ckpt_dir/'\n\nmnist = input_data.read_data_sets(\"MNIST_data\", one_hot = True)\n\nx = tf.placeholder(tf.float32, shape = [None , 784])\ny_ = tf.placeholder(tf.float32, [None, 10])\n\nsess = tf.InteractiveSession()\n\ndef load_model(sess, saver, checkpoint_dir ):\n\nckpt = tf.train.get_checkpoint_state(checkpoint_dir)\nif ckpt and ckpt.model_checkpoint_path:\nprint(ckpt.model_checkpoint_path)\n\nsaver.restore(sess, ckpt.model_checkpoint_path)\n\nelse:\nif not os.path.exists(checkpoint_dir):\nos.makedirs(checkpoint_dir)\nsess.run(init)\nreturn\n\ndef weight_variable(shape):\ninitial = tf.truncated_normal(shape, stddev = 0.1)\nreturn tf.Variable(initial)\n\ndef bias_variable(shape):\ninitial = tf.constant(0.1, shape= shape)\nreturn tf.Variable(initial)\n\ndef conv2d(x, W):\nreturn tf.nn.conv2d(x, W, strides = [1, 1, 1, 1], padding = \"SAME\")\n\ndef max_pool_2x2(x):\nreturn tf.nn.max_pool(x, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1],\npadding = \"SAME\")\n\nW_conv1 = weight_variable([5, 5, 1, 32])\nb_conv1 = bias_variable([32])\n\nx_image = tf.reshape(x, [-1, 28, 28, 1])\n\n#\nh_conv1 = tf.nn.relu(conv2d(x_image, W_conv1))\nh_pool1 = max_pool_2x2(h_conv1)\n\n#\nW_conv2 = weight_variable([5, 5, 32, 64])\nb_conv2 = bias_variable([64])\n\nh_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2))\nh_pool2 = max_pool_2x2(h_conv2)\n\nW_fc1 = weight_variable([7764, 1024])\nb_fc1 = bias_variable([1024])\n\nh_pool2_flat = tf.reshape(h_pool2, [-1, 7764])\nh_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n\n#\nkeep_prob = tf.placeholder(tf.float32)\nh_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n\n#\nW_fc2 = weight_variable([1024, 10])\nb_fc2 = bias_variable([10])\n\ny_conv = tf.nn.softmax(tf.matmul(h_fc1_drop,W_fc2) +b_fc2)\n\n#\ncross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices = [1]))\ntrain_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n\ncorrect_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\ninit = tf.initialize_all_variables()\n\nsaver = tf.train.Saver()\n\nload_model(sess, saver, checkpoint_dir)\n\nfor i in range(1):\nbatch = mnist.train.next_batch(50)\nif i%10 == 0:\ntrain_accuracy = accuracy.eval(feed_dict = {x : batch[0] , y_ : batch[1], keep_prob : 1.0})\nprint(\"step %d, training accuracy %g\"%(i, train_accuracy))\n\ntrain_step.run(feed_dict = {x : batch[0], y_ : batch[1], keep_prob : 0.5})\nprint(\"test accuracy %g\"%accuracy.eval(feed_dict={\nx: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))\n\ntf.scalar_summary(\"accuracy\", accuracy)\n\nsaver.save(sess,checkpoint_dir+'model.ckpt')`\n```\n\nWhen I restore the checkpoint:\n\n`saver.restore(sess, ckpt.model_checkpoint_path)`\n\nthen arises this error:\n\n```\nTraceback (most recent call last):\n.\n.\n.\nNotFoundError: Tensor name \"global_step_7\" not found in checkpoint files ./ckpt_dir/model.ckpt-0\n[[Node: save_18/restore_slice_438 = RestoreSlicedt=DT_INT32, preferred_shard=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\nCaused by op 'save_18/restore_slice_438', defined at:\nFile \"/home/m/anaconda3/lib/python3.5/site-packages/spyderlib/widgets/externalshell/start_ipython_kernel.py\", line 205, in\nipythonkernel.start()\n.\n.\n.\nFile \"/home/m/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1224, in __init\nraise TypeError(\"Control input must be an Operation, \"\n```\n\nHow can I solve this problem?\n", "comments": ["Is this still and issue? Can you print out the contents of your check point directory (including the path name) during the run? Are you passing int different paths to saver.save and saver.restore? \n", "@andydavis1 ,\nI use anaconda. when I run this code  first time in spyder or ipython whit \"`run filename.py`\", it save the check point, but again when I run this code arises the error.\n\nBut when I close the spyder or ipython and open it and run the code it restore the check point correctly.\n\nAlso when I run in terminal \"`python filename.py`\" it always run and don't arises any error.\n", "It sounds like it works in the terminal, so perhaps you could re-ask this question on StackOverflow to see if people there who are familiar with \"spyder\" or \"ipython\" can help. Please ask it there and tag it with the `tensorflow` tag.\n", "Here i found the good guidance :  [How to restore checkpoint in TensorFlow inside ipython or anaconda](http://stackoverflow.com/questions/37858866/how-to-restore-checkpoint-in-tensorflow-inside-ipython-or-anaconda/38093736?noredirect=1#comment63638806_38093736)\n\n```\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\n#import data\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n\ndef main():\n    with tf.Graph().as_default():\n        sess = tf.InteractiveSession()\n\n        # Create the model\n\n        x = tf.placeholder(tf.float32, [None, 784])\n        y_ = tf.placeholder(tf.float32, [None, 10])\n\n        keep_prob = tf.placeholder(tf.float32)\n\n        y_conv = model(x, keep_prob)\n\n        # Define loss and optimizer\n        cross_entropy =  tf.reduce_mean(- tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices = [1]))\n        train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n        correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n\n        saver = tf.train.Saver()\n        sess.run(tf.initialize_all_variables())\n\n        ckpt = tf.train.get_checkpoint_state('./')\n\n        if ckpt and ckpt.model_checkpoint_path:\n            print(ckpt.model_checkpoint_path)\n            saver.restore(sess,  \"model2.ckpt\")\n\n        for i in range(1000):\n          batch = mnist.train.next_batch(50)\n          if i%100 == 0:\n            train_accuracy = accuracy.eval(feed_dict={\n                x:batch[0], y_: batch[1], keep_prob: 1.0})\n            print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n          train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n\n        save_path = saver.save(sess, \"model2.ckpt\")\n        print (\"Model saved in file: \", save_path)\n\n        sess.close()\n\ndef weight_variable(shape):\n  initial = tf.truncated_normal(shape, stddev=0.1)\n  return tf.Variable(initial)\n\ndef bias_variable(shape):\n  initial = tf.constant(0.1, shape=shape)\n  return tf.Variable(initial)\n\n\ndef conv2d(x, W):\n  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n\ndef max_pool_2x2(x):\n  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n                        strides=[1, 2, 2, 1], padding='SAME')\n\ndef model(x, keep_prob):\n    W_conv1 = weight_variable([5, 5, 1, 32])\n    b_conv1 = bias_variable([32])\n\n    x_image = tf.reshape(x, [-1,28,28,1])\n    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n    h_pool1 = max_pool_2x2(h_conv1)\n\n\n    W_conv2 = weight_variable([5, 5, 32, 64])\n    b_conv2 = bias_variable([64])\n\n    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n    h_pool2 = max_pool_2x2(h_conv2)\n\n    W_fc1 = weight_variable([7 * 7 * 64, 1024])\n    b_fc1 = bias_variable([1024])\n\n    h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n\n    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n\n    W_fc2 = weight_variable([1024, 10])\n    b_fc2 = bias_variable([10])\n\n    y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n\n    return y_conv\n\nif __name__ == \"__main__\":\n    main()\n```\n"]}, {"number": 2998, "title": "Enable tf.tanh() for SparseTensor", "body": "Enabled `tf.tanh()` for `SparseTensor`. Added tests and verified locally. This partially addresses #1828.\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n", "Please rebase your repo to resolve the merge conflicts.\n", "@tensorflow-jenkins test this please\n", "@siddharth-agrawal Could you try to rebase your repo again to fix the test error?\n", "@rmlarsen Done. Could you merge this in as soon as possible? I've already had to rebase twice.\n", "@tensorflow-jenkins test this please\n", "@rmlarsen The tests don't seem to be running\n", "@tensorflow-jenkins test this please\n", "@rmlarsen While working on [this PR](https://github.com/tensorflow/tensorflow/pull/3122), I noticed that I missed adding tests for `tf.tanh()` [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/cwise_ops_test.py#L219). I will send out a PR once the `tf.erf()` change is merged in (to avoid conflicts).\n", "@siddharth-agrawal Oops! should have caught that :-) Thanks for adding the tests.\n"]}, {"number": 2997, "title": "Branch 125575345", "body": "", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n"]}, {"number": 2996, "title": "tf.reduce_sum(a) is slow", "body": "For large vectors, cost of `tf.reduce_sum(a)` is dominated by cost of `range` that's called to construct list of reduction indices.\n\nSumming up 10k elements 1500 times, I get about 0.8 seconds spent in `_sum` while 2 seconds is spent in `range`. Also, transferring all those indices to sum probably slows things down. This only happens when static graph optimizations don't apply. Will update with reproducible benchmark in a bit\n", "comments": ["Here's the [benchmark](https://gist.github.com/yaroslavvb/1d358b155b0c9491b4c3d2a5570de42d). I seem to have done something strange to my environment because in official version, reduce_sum is pretty fast, hence closing this.\n\n```\n          numpy time 0.37053\n             tf time 0.18646\n       tf persistent 0.18634\n\n```\n"]}, {"number": 2995, "title": "Multiple (Bidirectional)LSTM layers leads to nan in loss when time step is large", "body": "I've tried to implement a deep LSTM network and tested it on an autoencoder task in recovering audio magnitude spectrogram. At first I set the number of frames in spectrogram (i.e. time step in LSTM) to be small, and both LSTM and BLSTM worked well. However, when the time step is large, both LSTM and BLSTM began to generate nan loss. To be more specific, when I used a time step of 20 frames, BLSTM worked well with 4 layers and can successfully recover the input; but when I increased it to 100, a 3 layer BLSTM network began to generate nan in two training steps.\n\nI think there might be some problem within the code for `rnn` and `bidirectional_rnn` function (maybe due to unrolling?), but I'm not sure if I did anything wrong.\n\nThe codes for (B)LSTM I use are:\n\n``` python\ndef LSTM(lstm_hidden, batch_size, X, chunk_size, name='LSTM', seq_len=None):\n    initializer = tf.random_uniform_initializer(-1, 1)\n\n    cell = tf.nn.rnn_cell.LSTMCell(lstm_hidden, initializer=initializer, \n                                   use_peepholes=True, state_is_tuple=True)\n\n    initial_state = cell.zero_state(batch_size, tf.float32)\n\n    if seq_len is not None:     \n        output, _ = tf.nn.rnn(cell, X, initial_state=initial_state,\n                                             sequence_length=seq_len, scope=name)\n    else:\n        output, _ = tf.nn.rnn(cell, X, initial_state=initial_state, scope=name)\n\n    return output\n```\n\n``` python\ndef BLSTM(lstm_hidden, batch_size, X, chunk_size, name='BLSTM', seq_len=None):\n    initializer = tf.random_uniform_initializer(-1, 1)\n\n    cell_fw = tf.nn.rnn_cell.LSTMCell(lstm_hidden, initializer=initializer, \n                                      use_peepholes=True, state_is_tuple=True)\n    cell_bw = tf.nn.rnn_cell.LSTMCell(lstm_hidden, initializer=initializer, \n                                      use_peepholes=True, state_is_tuple=True)\n\n    # initial states\n    initial_state_fw = cell_fw.zero_state(batch_size, tf.float32)\n    initial_state_bw = cell_bw.zero_state(batch_size, tf.float32)\n\n    # BLSTM\n    if seq_len is not None:     \n        output, _, _ = tf.nn.bidirectional_rnn(cell_fw, cell_bw, X, \n                                               initial_state_fw=initial_state_fw,\n                                               initial_state_bw=initial_state_bw, \n                                               sequence_length=seq_len, \n                                               scope=name)\n\n    else:\n        output, _, _ = tf.nn.bidirectional_rnn(cell_fw, cell_bw, X, \n                                               initial_state_fw=initial_state_fw,\n                                               initial_state_bw=initial_state_bw, \n                                               scope=name)\n    return output\n```\n\nand the loss function is the L2-loss between the recovered magnitude spectrogram and the input, so there might not be a divide-by-zero problem (anyway it works well when time step is small, so I think it's not due to the loss function).\n### Environment info\n\nOperating System: Ubuntu 14.04.3\nPython version: 2.7.6\nTensorflow version: r0.9\nCUDA version: 7.5\n", "comments": ["I am not sure if this is the problem, but have you tried clipping the gradients? LSTM layers tend to have exploding gradient problem with large sequences.\n", "@mts42000 Actually I've set the learning rate to be 0 for debugging, so this may not be the case. What frustrate me is that the same data, network structure and learning rate worked well in Theano.\n", "Well, finally I found out that the problem occurs because of `initializer = tf.random_uniform_initializer(-1, 1)`. Deleting it will make things begin to work. I saw that initializer [here](https://gist.github.com/kastnerkyle/90a6d0f6789e15c38e97) so I used it, but I still don't know why it doesn't make sense in my case. Anyway just in case someone meets the same problem like I do. \n", "I would like to know how the multi-layer BLSTM to achieve \uff1fthank you\n", "@gallupliu A simple skeleton is like:\n\n``` python\noutput = input\nfor n in range(num_layer):\n        lstm_fw = LSTMCell(n_hidden, state_is_tuple=True)\n        lstm_bw = LSTMCell(n_hidden, state_is_tuple=True)\n\n        _initial_state_fw = lstm_fw.zero_state(batch_size, tf.float32)\n        _initial_state_bw = lstm_bw.zero_state(batch_size, tf.float32)\n\n        output, _states = bidirectional_rnn(lstm_fw, lstm_bw, output, \n                                                  initial_state_fw=_initial_state_fw,\n                                                  initial_state_bw=_initial_state_bw, \n                                                  scope='BLSTM_'+str(n+1))\n        output_fw = output[0]\n        output_bw = output[1]\n        output = tf.concat(2, [output_fw, output_bw])\n```\n\nUsing `bidirectional_dynamic_rnn` would make you create the model faster.\n", "@ScartleRoy Could you please explain why you are concatenating the outputs from forward, and backward cells though ?\nI thought it should be (as per [Alex Graves, 2013):](http://www.cs.toronto.edu/~graves/asru_2013.pdf)\n\n<pre><code>\nfw_out = output[0]\nbw_out = output[1]\nfw_out = tf.reshape(fw_out,[-1,hidden_size])\nbw_out = tf.reshape(bw_out,[-1,hidden_size])\nW_fw   = tf.Variable(tf.truncated_normal(shape=[hidden_size,n_chars],\n                                         mean=0,stddev=0.1,dtype=tf.float32))\nW_bw = tf.Variable(tf.truncated_normal(shape=[hidden_size, n_chars],\n                                       mean=0, stddev=0.1, dtype=tf.float32))\nb_out = tf.constant(0.1,shape=[n_chars])\nlogits = tf.add(tf.add(tf.matmul(fw_out,W_fw), tf.matmul(bw_out,W_bw)),b_out)\n</code></pre>\n", "@rajkumarcm Oh, that's just the one we're currently in use (and it works). We don't know if this one (seems to be the one that Google likes?) will perform much better than that one, but I think you may choose either. Well, sorry that I don't know if there are any performance differences between them.\n", "@ScartleRoy Could you please show here how you compute the loss with the concatenated version of logits, and what was the outcome ? \n\nI have been training a Bidirectional LSTM network, aiming to reproduce result of network described in Alex Grave's \"Towards End-End Speech Recognition using Recurrent Neural Networks\", but the predicted transcription is nowhere near that of actual. After 4000 iterations (not epochs, mini-batch size:100 with total data: 47,000) I get the CTC loss reducing from 650 to roughly 90. I am finding it really hard to determine the hyperparameters for training such a network.\n\nAny suggestions would be greatly appreciated.\n", "@rajkumarcm I just simply feed the concatenated output into next layer. Sorry that I haven't read that paper (my interest is not in ASR) so that I don't know if the decrease of loss is good or not. But I think you can start with 10% data to see if you can overfit (and determine the size of the network, tune the parameters, etc.). Or maybe choose another learning algorithm (preferably with momentum), start from a large learning rate and gradually decrease it. Those are all very common ways to debug a network.\n", "@ScartleRoy Thanks\uff0cI has master to use the\n function bidirectional_rnn to achieve one layer,But I don\u2018t know to  achieve multiple layer\uff1fmay\nbe I'm wrong understand blstm\n", "@rajkumarcm I am confused about the shape of W_fw you mentioned above: [hidden_size,n_chars],what's n_chars in the second dimension? Is it the max length of the batch of sequences?", "@vino5211 Apologies for the late reply. With regards to your question, the second dimension (n_chars) actually refers to n classes ( number of characters + an additional class for blank node ). Hope this helps."]}, {"number": 2994, "title": "cifar10_multi_gpu_train.py - unintended loss reporting", "body": "# cifar10_multi_gpu_train.py\n\nAt this [line](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/cifar10/cifar10_multi_gpu_train.py#L180), every loss for each tower in the multi GPU is calculated\n\nHowever, these losses are not averaged, and it seems like the loss from the last GPU is used to return [loss](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/cifar10/cifar10_multi_gpu_train.py#L246).\n\nIs this on purpose (if yes, why?) or is it a bug in the code?\n", "comments": ["This is a question better suited for StackOverflow. Please ask it there and tag it with the `tensorflow` tag.\n"]}, {"number": 2993, "title": "Added Xcode version check and logging utilities for kernel errors", "body": "", "comments": ["@ebrevdo Can you take a look at the LogAllRegisteredKernels() function I added to tensorflow/core/framework/op_kernel.cc? I think it's trivial, but it's pretty useful for debugging missing kernels on mobile builds where we strip a lot out.\n", "Jenkins, test this please.\n", "Failing tests with python3?\n", "Jenkins, test this please.\n", "@ebrevdo is this ready to be merged?\n", "Thanks for the review! Since the changes were fairly small, but I have a high-priority iOS fix to get in, I went ahead and merged.\n"]}, {"number": 2992, "title": "Added Xcode version check, and logging utilities for kernel errors", "body": "", "comments": []}, {"number": 2991, "title": "Error running tf.nn.atrous_conv2d on TF 0.9.0rc0", "body": "Getting error running tf.nn.atrous_conv2d on TensorFlow 0.9.0rc0. The same code runs successfully on TensorFlow 0.8.0 installed from the source.\n### Environment info\n\nOperating System: Ubuntu 14.04\n\nInstalled version of CUDA and cuDNN:  None\n\nIf installed from binary pip package, provide:\n1. https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.9.0rc0-cp35-cp35m-linux_x86_64.whl\n2. 0.9.0rc0\n### Steps to reproduce\n1. Install TF from the package\n2. Call tf.nn.atrous_conv2d from the client code (see code here - https://github.com/nmayorov/ufcnn/blob/master/ufcnn/ufcnn.py)\n3. Get TypeError: **int** returned non-int (type NoneType)\n### What have you tried?\n1. Installation of 0.8.0 from the source helps\n### Logs or other output that would be helpful\n\nTraceback (most recent call last):\n  File \"UFCNN_functional.py\", line 2050, in <module>\n    x, y_hat, *_ = construct_ufcnn(n_inputs=2, n_outputs=y_train.shape[2], n_levels=4, n_filters=150)\n  File \"/notebook/UFCNN/ufcnn/ufcnn/ufcnn.py\", line 227, in construct_ufcnn\n    x = tf.nn.relu(conv(x, w, b, filter_length, dilation))\n  File \"/notebook/UFCNN/ufcnn/ufcnn/ufcnn.py\", line 15, in conv\n    x = tf.nn.atrous_conv2d(x, w, dilation, padding='VALID')\n  File \"/root/miniconda2/envs/keras/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\", line 169, in atrous_conv2d\n    in_width = int(value_shape[2])\nTypeError: **int** returned non-int (type NoneType)\n", "comments": ["Looks like you are passing in the `value` parameter a placeholder with a shape which does not specify the width and height dimensions [here](https://github.com/nmayorov/ufcnn/blob/master/ufcnn/ufcnn.py#L238).   The `atrous_conv2d` op (at least in 0.9) requires the `in_height` and `in_width` dimensions to be defined: \nhttps://github.com/tensorflow/tensorflow/blob/r0.9/tensorflow/python/ops/nn_ops.py#L169\nYou appear to be able to leave the batch dimension unspecified.\n\nThe handling of these parameters was improved in the following commit: https://github.com/tensorflow/tensorflow/commit/3a744a417178646712e9cda0a85655d5d8968e19\n\nIf this is a problem for you, could you please try against a newer build and see if this fixes your problem?\n", "@prb12 Thank you for your response, we'll take a look.\n", "All these is not surprise for me, later versions of TF support variable size dimensions in `atrous_conv2d` and I never had any problem with it. This issue can be close.\n", "@nmayorov Are you sure that it will not re-occure in 0.9.0 release?\n", "Well, it depends on what will be included in 0.9.0 release. At the moment I see that the 0.9 branch doesn't contain this fix. A bit of inconvenience, but what can we do.\n\nThe solution on our side is to create our ufcnn with a fixed length of signals (which sort of contradicts sequence-to-sequence paradigm). But it is better to discuss it not here.\n", "OK, closed.\n"]}, {"number": 2990, "title": "inconsistent variable initialization behavior for callables / ops", "body": "When a variable is initialized with a callable, running the initializer of the variable has no effect after it is run once:\n\n``` python\nimport tensorflow as tf\nimport scipy\n\nsession = tf.InteractiveSession()\nretrand = lambda: scipy.random.random(size=[2,2])\nvar = tf.Variable(retrand, dtype=tf.float32)\nsession.run(var.initializer)\nvar.eval()\n# prints array([[ 0.73060566,  0.26469722],\n#               [ 0.63376802,  0.64898247]], dtype=float32)\nsession.run(var.initializer)\nvar.eval()\n# prints array([[ 0.73060566,  0.26469722],\n#              [ 0.63376802,  0.64898247]], dtype=float32)\n# (same as first call)\n```\n\nIf a variable is initialized with a tensorflow init op, then rerunning the initializer changes the variable state:\n\n``` python\nvar = tf.Variable(tf.random_normal([2,2]))\nsession.run(var.initializer)\nvar.eval()\n#prints \n#array([[ 0.61732173,  0.14423341],\n#       [ 0.3965871 , -0.98214936]], dtype=float32)\nsession.run(var.initializer)\nvar.eval()\n# prints \n# array([[ 0.48240849,  0.26547143],\n#      [ 1.18776596, -0.12901327]], dtype=float32)\n```\n\nWhy do variables initialized with a callable only call the callable once, even if they are initialized repeatedly, when variables initialized with an init op rerun the op on each run of the initializer? Is there any way to have a variable initialized with a callable call it each time the variable's initializer is run?\n### Environment info\n\nOperating System:  Mac OSX\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`): None\nIf installed from binary pip package, provide: \n1. Which pip package you installed.\nhttps://storage.googleapis.com/tensorflow/mac/tensorflow-0.9.0rc0-py2-none-any.whl\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n0.9.0rc0\n", "comments": ["> Is there any way to have a variable initialized with a callable call it each time the variable's initializer is run?\n\nYou can define the initializer as a `PyFunc` op.\nNote that you need to manually specify the output shape of your op with `tensor.set_shape` before you can use it to initialize a variable.\n\n``` python\nimport tensorflow as tf\nimport numpy as np\nimport scipy\n\nsession = tf.InteractiveSession()\n\ndef get_random():\n    return scipy.random.random(size=[2,2]).astype(np.float32)\n\ninit = tf.py_func(get_random, [], [tf.float32])[0]\ninit.set_shape((2, 2))\nvar = tf.Variable(init)\nsession.run(var.initializer)\nprint(var.eval())\n#[[ 0.97525001  0.68293589]\n# [ 0.59959763  0.14780547]]\nsession.run(var.initializer)\nprint(var.eval())\n#[[ 0.7729072   0.84212685]\n# [ 0.74777764  0.97365493]]\n```\n", "Thanks!\n\nOn Wed, Jun 22, 2016 at 3:47 PM, Igor Babuschkin notifications@github.com\nwrote:\n\n> Is there any way to have a variable initialized with a callable call it\n> each time the variable's initializer is run?\n> \n> You can define the initializer as a PyFunc op.\n> Note that you need to manually specify the output shape of your op with\n> tensor.set_shape before you can use it to initialize a variable.\n> \n> import tensorflow as tfimport numpy as npimport scipy\n> \n> session = tf.InteractiveSession()\n> def get_random():\n>     return scipy.random.random(size=[2,2]).astype(np.float32)\n> \n> init = tf.py_func(get_random, [], [tf.float32])[0]\n> init.set_shape((2, 2))\n> var = tf.Variable(init)\n> session.run(var.initializer)print(var.eval())#[[ 0.97525001  0.68293589]# [ 0.59959763  0.14780547]]\n> session.run(var.initializer)print(var.eval())#[[ 0.7729072   0.84212685]# [ 0.74777764  0.97365493]]\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2990#issuecomment-227900152,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/ACpC4M2MCb7ZvaD_jlqiYIAfJvEdvdhyks5qObuSgaJpZM4I7bnS\n> .\n\n## \n\nDanny Goldstein\nhttp://astro.berkeley.edu/~dgold/\n"]}, {"number": 2989, "title": "Import pre-trained model to MATLAB?", "body": "Hi,\n\nI am training my own CNN model using tensorflow.\n\nI wonder if there is a way to import a (pre-)trained model (in tensorflow format, .ckpt) into MATLAB/Simulink or Caffe?\n\nThank you in advance.\n", "comments": ["This is a question better suited for StackOverflow. Please ask it there and tag it with the `tensorflow` tag.\n"]}, {"number": 2988, "title": "Branch 125511710", "body": "", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "ignore the bad cla check, it seems to fail sometimes on large lists of commits\n\n@willnorris\n", "Jenkins, test this please.\n", "Jenkins, test this please.\n", "Jenkins is overloaded, and it was running 2 builds for this PR. I cancelled one of those, but the latest started one is still running.\nThe one run I cancelled had these two tests failing:\nhttp://ci.tensorflow.org/job/tensorflow-pull-requests-mac/991/console\nhttp://ci.tensorflow.org/job/tensorflow-pull-requests-cpu-python3/818/console\n"]}, {"number": 2987, "title": "Random slow downs during training.", "body": "Usually the speed of each training batch is somewhat consistent for me when using tensorflow.  However, I am currently training an RNN.  For the first 3 epochs each batch of 128 samples took about 1 second.  Now on the third epoch my times have become inconsistent.  The batches will randomly become very slow.  I can hear the fan on my GTX Titian spin down indicating that during these slowdowns it is not using the compute resources on the GPU.  \n\nI am timing it like this so I don't think anything else in my code could be affecting it.\n\n```\n            begin_time = time.time()\n            loss, ts = sess.run([cost, train_step], feed_dict = {input_tensor: x_train, expected_output: y_train, keep_prob: 0.8})\n            end_time = time.time()\n```\n\nHere is the output with timing information.\n\n```\nEpoch 3        Batch 858      Loss 172.072438250   Last Loss 206.985626221   Time 1.432 \nEpoch 3        Batch 859      Loss 172.067967827   Last Loss 168.227874756   Time 1.419 \nEpoch 3        Batch 860      Loss 172.057925642   Last Loss 163.421646118   Time 1.447 \nEpoch 3        Batch 861      Loss 172.056937587   Last Loss 171.206222534   Time 1.339 \nEpoch 3        Batch 862      Loss 172.051488565   Last Loss 167.354431152   Time 1.285 \nEpoch 3        Batch 863      Loss 172.016926642   Last Loss 142.189987183   Time 1.310 \nEpoch 3        Batch 864      Loss 172.011091517   Last Loss 166.969543457   Time 1.291 \nEpoch 3        Batch 865      Loss 172.011779468   Last Loss 172.606857300   Time 1.317 \nEpoch 3        Batch 866      Loss 172.004100742   Last Loss 165.354324341   Time 1.307 \nEpoch 3        Batch 867      Loss 172.008801860   Last Loss 176.084671021   Time 1.372 \nEpoch 3        Batch 868      Loss 172.032961320   Last Loss 193.003372192   Time 1.298 \nEpoch 3        Batch 869      Loss 172.036121868   Last Loss 174.782638550   Time 1.310 \nEpoch 3        Batch 870      Loss 172.044807513   Last Loss 179.601318359   Time 1.429 \nEpoch 3        Batch 871      Loss 172.066208551   Last Loss 190.706512451   Time 1.311 \nEpoch 3        Batch 872      Loss 172.052568940   Last Loss 160.158828735   Time 2.614 \nEpoch 3        Batch 873      Loss 172.032694941   Last Loss 154.682693481   Time 2.208 \nEpoch 3        Batch 874      Loss 172.040674805   Last Loss 179.015075684   Time 3.138 \nEpoch 3        Batch 875      Loss 172.034015673   Last Loss 166.207275391   Time 1.750 \nEpoch 3        Batch 876      Loss 172.029174909   Last Loss 167.788665771   Time 1.955 \nEpoch 3        Batch 877      Loss 172.042762183   Last Loss 183.958801270   Time 2.576 \nEpoch 3        Batch 878      Loss 172.028727179   Last Loss 159.705993652   Time 3.130 \nEpoch 3        Batch 879      Loss 172.035255710   Last Loss 177.773834229   Time 2.622 \nEpoch 3        Batch 880      Loss 172.043415273   Last Loss 179.223831177   Time 1.581 \nEpoch 3        Batch 881      Loss 172.040243793   Last Loss 169.246170044   Time 3.093 \nEpoch 3        Batch 882      Loss 172.017778805   Last Loss 152.203659058   Time 2.470 \nEpoch 3        Batch 883      Loss 172.018503957   Last Loss 172.658813477   Time 2.540 \nEpoch 3        Batch 884      Loss 172.054275064   Last Loss 203.675933838   Time 2.597 \nEpoch 3        Batch 885      Loss 172.029123448   Last Loss 149.769943237   Time 2.915 \nEpoch 3        Batch 886      Loss 172.013957027   Last Loss 158.576507568   Time 3.095 \nEpoch 3        Batch 887      Loss 171.994215424   Last Loss 154.483413696   Time 2.250 \nEpoch 3        Batch 888      Loss 171.998252946   Last Loss 175.583572388   Time 2.997 \nEpoch 3        Batch 889      Loss 171.974352959   Last Loss 150.727264404   Time 3.417 \nEpoch 3        Batch 890      Loss 171.968166587   Last Loss 166.462295532   Time 2.290 \nEpoch 3        Batch 891      Loss 172.011782095   Last Loss 210.873199463   Time 1.358 \nEpoch 3        Batch 892      Loss 172.013166695   Last Loss 173.248229980   Time 0.910 \nEpoch 3        Batch 893      Loss 172.016952293   Last Loss 175.397491455   Time 0.893 \nEpoch 3        Batch 894      Loss 172.015184030   Last Loss 170.434356689   Time 0.986 \nEpoch 3        Batch 895      Loss 172.001527531   Last Loss 159.778961182   Time 1.000 \nEpoch 3        Batch 896      Loss 172.008338426   Last Loss 178.110900879   Time 0.999 \nEpoch 3        Batch 897      Loss 172.015577083   Last Loss 178.508651733   Time 1.699 \nEpoch 3        Batch 898      Loss 172.076713689   Last Loss 226.977386475   Time 1.570 \nEpoch 3        Batch 899      Loss 172.043461711   Last Loss 142.149932861   Time 1.699 \nEpoch 3        Batch 900      Loss 172.069320933   Last Loss 195.342620850   Time 1.678 \nEpoch 3        Batch 901      Loss 172.063626562   Last Loss 166.932998657   Time 1.685 \nEpoch 3        Batch 902      Loss 172.060249506   Last Loss 169.014144897   Time 1.467 \nEpoch 3        Batch 903      Loss 172.068654811   Last Loss 179.658645630   Time 3.737 \nEpoch 3        Batch 904      Loss 172.061790508   Last Loss 165.856460571   Time 3.705 \nEpoch 3        Batch 905      Loss 172.078748707   Last Loss 187.425918579   Time 3.376 \nEpoch 3        Batch 906      Loss 172.065538518   Last Loss 160.097106934   Time 4.606 \nEpoch 3        Batch 907      Loss 172.040349112   Last Loss 149.193557739   Time 4.513 \nEpoch 3        Batch 908      Loss 172.078197416   Last Loss 206.444458008   Time 2.491 \nEpoch 3        Batch 909      Loss 172.081030156   Last Loss 174.655990601   Time 4.244 \n```\n\nHere is what the output normally looks like most of the time when this issue is not occurring.\n\n```\nEpoch 1        Batch 213      Loss 262.820208541   Last Loss 189.761398315   Time 0.978 \nEpoch 1        Batch 214      Loss 262.570083973   Last Loss 209.043426514   Time 0.984 \nEpoch 1        Batch 215      Loss 262.265294534   Last Loss 196.735565186   Time 0.985 \nEpoch 1        Batch 216      Loss 261.973181588   Last Loss 198.876785278   Time 0.988 \nEpoch 1        Batch 217      Loss 261.685996729   Last Loss 199.366882324   Time 0.989 \nEpoch 1        Batch 218      Loss 261.385472058   Last Loss 195.871093750   Time 0.981 \nEpoch 1        Batch 219      Loss 261.200773135   Last Loss 220.751708984   Time 0.983 \nEpoch 1        Batch 220      Loss 260.912834202   Last Loss 197.566268921   Time 0.982 \nEpoch 1        Batch 221      Loss 260.757691254   Last Loss 226.471099854   Time 0.981 \nEpoch 1        Batch 222      Loss 260.588221186   Last Loss 222.965866089   Time 0.984 \nEpoch 1        Batch 223      Loss 260.272108146   Last Loss 189.778900146   Time 0.992 \nEpoch 1        Batch 224      Loss 259.962260607   Last Loss 190.556411743   Time 0.981 \nEpoch 1        Batch 225      Loss 259.509933337   Last Loss 157.736297607   Time 0.985 \nEpoch 1        Batch 226      Loss 259.273305347   Last Loss 205.795379639   Time 0.982 \nEpoch 1        Batch 227      Loss 259.062388972   Last Loss 211.184371948   Time 0.991 \nEpoch 1        Batch 228      Loss 258.901376783   Last Loss 222.190597534   Time 0.984 \nEpoch 1        Batch 229      Loss 258.643836975   Last Loss 199.667221069   Time 0.986 \nEpoch 1        Batch 230      Loss 258.344224079   Last Loss 189.433258057   Time 0.979 \n\n```\n\nAfter a while it will make it through the slow phase and return to being 1s per batch.  I am using Ubuntu 16.04 with CUDNN 5 and Cuda 8.  This problem is intermittent and I have no real way to reproduce it.\n", "comments": ["I encountered the same problem.  \nThe log is as following:\n2016-06-23 12:13:58.846196: step 2430, loss = 8.52 (460.8 examples/sec; 1.111 sec/batch)\n2016-06-23 12:14:10.184638: step 2440, loss = 8.49 (474.2 examples/sec; 1.080 sec/batch)\n2016-06-23 12:14:22.017317: step 2450, loss = 8.61 (457.5 examples/sec; 1.119 sec/batch)\n2016-06-23 12:14:33.098411: step 2460, loss = 8.50 (462.9 examples/sec; 1.106 sec/batch)\n2016-06-23 12:14:44.626485: step 2470, loss = 8.53 (463.6 examples/sec; 1.105 sec/batch)\n2016-06-23 12:14:55.997127: step 2480, loss = 8.63 (469.0 examples/sec; 1.092 sec/batch)\n2016-06-23 12:15:07.829877: step 2490, loss = 8.54 (447.0 examples/sec; 1.145 sec/batch)\n2016-06-23 12:15:19.500701: step 2500, loss = 8.59 (336.6 examples/sec; 1.521 sec/batch)\n2016-06-23 12:15:32.871750: step 2510, loss = 8.53 (471.3 examples/sec; 1.086 sec/batch)\n2016-06-23 12:15:44.725588: step 2520, loss = 8.44 (478.0 examples/sec; 1.071 sec/batch)\n2016-06-23 12:15:56.119878: step 2530, loss = 8.44 (477.7 examples/sec; 1.072 sec/batch)\n2016-06-23 12:16:07.449835: step 2540, loss = 8.48 (396.2 examples/sec; 1.292 sec/batch)\n2016-06-23 12:16:18.885702: step 2550, loss = 8.47 (438.6 examples/sec; 1.167 sec/batch)\n2016-06-23 12:16:30.225739: step 2560, loss = 8.45 (483.1 examples/sec; 1.060 sec/batch)\n2016-06-23 12:16:42.241132: step 2570, loss = 8.32 (445.8 examples/sec; 1.148 sec/batch)\n2016-06-23 12:16:53.850374: step 2580, loss = 8.49 (453.8 examples/sec; 1.128 sec/batch)\n2016-06-23 12:17:07.085051: step 2590, loss = 8.43 (290.5 examples/sec; 1.762 sec/batch)\n2016-06-23 12:17:20.120950: step 2600, loss = 8.46 (256.8 examples/sec; 1.994 sec/batch)\n2016-06-23 12:17:39.631906: step 2610, loss = 8.31 (241.3 examples/sec; 2.122 sec/batch)\n2016-06-23 12:18:00.403822: step 2620, loss = 8.28 (279.3 examples/sec; 1.833 sec/batch)\n2016-06-23 12:18:18.883428: step 2630, loss = 8.33 (306.9 examples/sec; 1.668 sec/batch)\n2016-06-23 12:18:38.437799: step 2640, loss = 8.45 (265.5 examples/sec; 1.928 sec/batch)\n2016-06-23 12:18:58.108157: step 2650, loss = 8.55 (260.9 examples/sec; 1.963 sec/batch)\n2016-06-23 12:19:18.003289: step 2660, loss = 8.62 (362.8 examples/sec; 1.411 sec/batch)\n2016-06-23 12:19:39.784456: step 2670, loss = 8.35 (192.5 examples/sec; 2.660 sec/batch)\n2016-06-23 12:20:03.160854: step 2680, loss = 8.43 (244.8 examples/sec; 2.092 sec/batch)\n2016-06-23 12:20:25.787582: step 2690, loss = 8.60 (216.9 examples/sec; 2.361 sec/batch)\n2016-06-23 12:20:51.217995: step 2700, loss = 8.62 (182.7 examples/sec; 2.802 sec/batch)\n2016-06-23 12:21:22.977688: step 2710, loss = 8.53 (154.7 examples/sec; 3.309 sec/batch)\n2016-06-23 12:21:54.205844: step 2720, loss = 8.54 (142.1 examples/sec; 3.604 sec/batch)\n2016-06-23 12:22:23.399541: step 2730, loss = 8.42 (193.0 examples/sec; 2.653 sec/batch)\n2016-06-23 12:23:00.475763: step 2740, loss = 8.28 (186.7 examples/sec; 2.743 sec/batch)\n2016-06-23 12:23:29.253763: step 2750, loss = 8.38 (167.2 examples/sec; 3.062 sec/batch)\n2016-06-23 12:23:56.726199: step 2760, loss = 8.61 (197.4 examples/sec; 2.594 sec/batch)\n2016-06-23 12:24:26.043562: step 2770, loss = 8.51 (144.0 examples/sec; 3.555 sec/batch)\n\nBoth the status of CPU and IO are normal. It can be reproduced in my machine. \n", "I switched my aggregation method in my optimizer from EXPIRMENTAL_TREE to EXPERIMENTAL_ACCUMULATE_N.  I have not seen the issue since but I am not sure if I have fixed it since it is intermittent and I have not done much training since making that change.\n", "Can you please follow the steps referenced in this github issue to produce a timeline which shows the slowdown?: https://github.com/tensorflow/tensorflow/issues/3009\n", "I produced the timeline which shows the same situation with #3009 . The operation \"QueueDequeueMany\" takes a long time and a lot of virtual memory are allocated.\n", "@chasep255 Are you also seing the slow operation as \"DequeMany\"? If so, I may mark this as a duplicate of #3009 \n", "I did not run any tests.  I have not encountered the issue again since \npositing about this problem.  I think it has to do with the \nEXPERIMENTAL_TREE aggregation method since it went away one I stopped \nusing this.\n\nOn 06/28/2016 11:55 AM, andydavis1 wrote:\n\n> @chasep255 https://github.com/chasep255 Are you also seing the slow \n> operation as \"DequeMany\"? If so, I may mark this as a duplicate of \n> #3009 https://github.com/tensorflow/tensorflow/issues/3009\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub \n> https://github.com/tensorflow/tensorflow/issues/2987#issuecomment-229093723, \n> or mute the thread \n> https://github.com/notifications/unsubscribe/APDnFabcNVCI_7EXyAq-0T4PJsaAMyFLks5qQUPlgaJpZM4I7Wtl.\n", "Ok thanks. I'll close this issue out. It looks like @FangxiangFeng's issue may be an instance of #3009.\n", "I'm facing the same problem. I'm running the inception model fine-tuning on my data. Here's a part of the log being printed. I'm using all TensorFlow data read operations (so no copy from numpy to TF, as I'm not using `DequeueMany` manually, but through `shuffle_batch_join`). I'll try to provide the timeline when I have a chance to re-run the training. The machine resources otherwise seem fine (CPU/memory is not choked, the prefetch queues logs show >70% full) \n\n``` txt\n2016-07-23 14:34:15.784645: step 2720, loss = 2.63 (68.5 examples/sec; 3.740 sec/batch)                                                                                                                                                                                                                                                                         [224/1046]\n2016-07-23 14:34:54.449992: step 2730, loss = 2.23 (65.7 examples/sec; 3.897 sec/batch)\n2016-07-23 14:35:30.115364: step 2740, loss = 2.11 (66.2 examples/sec; 3.867 sec/batch)\n2016-07-23 14:36:06.048820: step 2750, loss = 2.14 (64.8 examples/sec; 3.953 sec/batch)\n2016-07-23 14:36:43.302534: step 2760, loss = 2.18 (71.4 examples/sec; 3.585 sec/batch)\n2016-07-23 14:37:21.960443: step 2770, loss = 2.26 (74.2 examples/sec; 3.451 sec/batch)\n2016-07-23 14:37:59.039025: step 2780, loss = 2.04 (67.2 examples/sec; 3.808 sec/batch)\n2016-07-23 14:38:39.144252: step 2790, loss = 2.45 (59.2 examples/sec; 4.321 sec/batch)\n2016-07-23 14:39:17.613073: step 2800, loss = 2.18 (81.0 examples/sec; 3.162 sec/batch)\n2016-07-23 14:39:56.705607: step 2810, loss = 2.55 (66.5 examples/sec; 3.847 sec/batch)\n2016-07-23 14:40:32.612656: step 2820, loss = 2.38 (78.8 examples/sec; 3.247 sec/batch)\n2016-07-23 14:41:09.194193: step 2830, loss = 2.47 (67.1 examples/sec; 3.814 sec/batch)\n2016-07-23 14:41:44.523209: step 2840, loss = 2.30 (77.5 examples/sec; 3.303 sec/batch)\n2016-07-23 14:42:21.437528: step 2850, loss = 2.27 (64.8 examples/sec; 3.949 sec/batch)\n2016-07-23 14:42:55.936083: step 2860, loss = 2.22 (80.3 examples/sec; 3.189 sec/batch)\n2016-07-23 14:43:32.151591: step 2870, loss = 2.35 (60.1 examples/sec; 4.259 sec/batch)\n2016-07-23 14:44:07.513355: step 2880, loss = 2.38 (65.1 examples/sec; 3.931 sec/batch)\n2016-07-23 14:44:42.496976: step 2890, loss = 1.99 (67.7 examples/sec; 3.781 sec/batch)\n2016-07-23 14:45:20.375483: step 2900, loss = 2.67 (50.2 examples/sec; 5.100 sec/batch)\n2016-07-23 14:46:00.563096: step 2910, loss = 2.23 (62.5 examples/sec; 4.093 sec/batch)\n2016-07-23 14:46:39.610138: step 2920, loss = 2.19 (80.1 examples/sec; 3.197 sec/batch)\n2016-07-23 14:47:19.147337: step 2930, loss = 2.31 (51.6 examples/sec; 4.957 sec/batch)\n2016-07-23 14:48:03.449487: step 2940, loss = 2.27 (50.8 examples/sec; 5.041 sec/batch)\n2016-07-23 14:48:47.893646: step 2950, loss = 2.29 (52.0 examples/sec; 4.919 sec/batch)\n2016-07-23 14:49:27.023626: step 2960, loss = 2.27 (73.7 examples/sec; 3.475 sec/batch)\n2016-07-23 14:50:11.125284: step 2970, loss = 2.08 (65.4 examples/sec; 3.914 sec/batch)\n2016-07-23 14:50:51.295322: step 2980, loss = 2.19 (50.7 examples/sec; 5.053 sec/batch)\n2016-07-23 14:51:33.655334: step 2990, loss = 2.30 (66.0 examples/sec; 3.879 sec/batch)\n2016-07-23 14:52:10.323608: step 3000, loss = 2.25 (68.8 examples/sec; 3.723 sec/batch)\n2016-07-23 14:52:52.609913: step 3010, loss = 2.04 (64.8 examples/sec; 3.949 sec/batch)\n2016-07-23 14:53:31.631134: step 3020, loss = 2.31 (54.5 examples/sec; 4.696 sec/batch)\n2016-07-23 14:54:13.547641: step 3030, loss = 2.33 (55.1 examples/sec; 4.643 sec/batch)\n2016-07-23 14:55:00.528403: step 3040, loss = 2.29 (49.1 examples/sec; 5.217 sec/batch)\n2016-07-23 14:55:45.258737: step 3050, loss = 2.17 (51.4 examples/sec; 4.979 sec/batch)\n2016-07-23 14:56:31.159313: step 3060, loss = 2.29 (51.5 examples/sec; 4.968 sec/batch)\n2016-07-23 14:57:15.148195: step 3070, loss = 2.27 (51.8 examples/sec; 4.946 sec/batch)\n2016-07-23 14:57:54.451044: step 3080, loss = 2.40 (68.8 examples/sec; 3.724 sec/batch)\n2016-07-23 14:58:32.325357: step 3090, loss = 1.93 (58.4 examples/sec; 4.380 sec/batch)\n2016-07-23 14:59:06.746820: step 3100, loss = 2.02 (88.0 examples/sec; 2.910 sec/batch)\n2016-07-23 14:59:39.746600: step 3110, loss = 2.20 (86.2 examples/sec; 2.970 sec/batch)\n2016-07-23 15:00:10.409635: step 3120, loss = 2.19 (79.7 examples/sec; 3.213 sec/batch)\n2016-07-23 15:00:41.756720: step 3130, loss = 2.28 (83.6 examples/sec; 3.062 sec/batch)\n2016-07-23 15:01:12.757209: step 3140, loss = 2.26 (85.6 examples/sec; 2.989 sec/batch)\n2016-07-23 15:01:43.993791: step 3150, loss = 2.23 (79.6 examples/sec; 3.214 sec/batch)\n2016-07-23 15:02:15.348264: step 3160, loss = 2.04 (82.8 examples/sec; 3.090 sec/batch)\n2016-07-23 15:02:46.752311: step 3170, loss = 2.26 (83.7 examples/sec; 3.058 sec/batch)\n2016-07-23 15:03:18.581683: step 3180, loss = 1.92 (78.9 examples/sec; 3.243 sec/batch)\n2016-07-23 15:03:50.599771: step 3190, loss = 2.38 (80.9 examples/sec; 3.166 sec/batch)\n2016-07-23 15:04:22.723497: step 3200, loss = 2.00 (78.4 examples/sec; 3.266 sec/batch)\n2016-07-23 15:05:16.565673: step 3210, loss = 1.99 (25.1 examples/sec; 10.188 sec/batch)\n2016-07-23 15:06:37.336032: step 3220, loss = 2.34 (28.4 examples/sec; 9.013 sec/batch)\n2016-07-23 15:08:03.531096: step 3230, loss = 2.10 (26.1 examples/sec; 9.817 sec/batch)\n2016-07-23 15:09:26.177609: step 3240, loss = 2.34 (29.7 examples/sec; 8.613 sec/batch)\n2016-07-23 15:10:48.697512: step 3250, loss = 2.06 (25.0 examples/sec; 10.229 sec/batch)\n2016-07-23 15:12:14.720998: step 3260, loss = 2.16 (26.0 examples/sec; 9.861 sec/batch)\n2016-07-23 15:13:42.700315: step 3270, loss = 2.06 (25.6 examples/sec; 10.000 sec/batch)\n2016-07-23 15:15:07.717132: step 3280, loss = 2.00 (26.9 examples/sec; 9.524 sec/batch)\n2016-07-23 15:16:29.206475: step 3290, loss = 2.15 (71.1 examples/sec; 3.600 sec/batch)\n2016-07-23 15:17:56.483761: step 3300, loss = 2.17 (25.3 examples/sec; 10.102 sec/batch)\n2016-07-23 15:19:32.355597: step 3310, loss = 2.26 (24.7 examples/sec; 10.352 sec/batch)\n2016-07-23 15:20:58.064109: step 3320, loss = 2.18 (25.8 examples/sec; 9.907 sec/batch)\n2016-07-23 15:22:18.608871: step 3330, loss = 2.02 (27.9 examples/sec; 9.183 sec/batch)\n2016-07-23 15:23:39.227969: step 3340, loss = 2.17 (26.8 examples/sec; 9.567 sec/batch)\n2016-07-23 15:25:03.643470: step 3350, loss = 2.01 (25.7 examples/sec; 9.976 sec/batch)\n2016-07-23 15:26:25.196536: step 3360, loss = 2.08 (33.1 examples/sec; 7.728 sec/batch)\n2016-07-23 15:27:50.458838: step 3370, loss = 2.01 (26.9 examples/sec; 9.533 sec/batch)\n2016-07-23 15:29:12.861687: step 3380, loss = 2.07 (66.9 examples/sec; 3.827 sec/batch)\n2016-07-23 15:30:31.386601: step 3390, loss = 1.95 (71.9 examples/sec; 3.559 sec/batch)\n2016-07-23 15:31:58.734378: step 3400, loss = 2.37 (25.6 examples/sec; 10.011 sec/batch)\n2016-07-23 15:33:33.894047: step 3410, loss = 2.06 (26.7 examples/sec; 9.581 sec/batch)\n2016-07-23 15:34:54.087397: step 3420, loss = 1.99 (27.5 examples/sec; 9.311 sec/batch)\n2016-07-23 15:36:11.707716: step 3430, loss = 2.15 (69.0 examples/sec; 3.710 sec/batch)\n2016-07-23 15:37:31.881528: step 3440, loss = 2.20 (46.0 examples/sec; 5.570 sec/batch)\n2016-07-23 15:38:55.509458: step 3450, loss = 2.08 (25.5 examples/sec; 10.041 sec/batch)\n2016-07-23 15:40:16.870401: step 3460, loss = 2.35 (26.5 examples/sec; 9.647 sec/batch)\n2016-07-23 15:41:40.889863: step 3470, loss = 2.23 (27.3 examples/sec; 9.389 sec/batch)\n2016-07-23 15:43:06.156092: step 3480, loss = 2.08 (27.3 examples/sec; 9.385 sec/batch)\n2016-07-23 15:44:25.849125: step 3490, loss = 2.04 (65.0 examples/sec; 3.939 sec/batch)\n2016-07-23 15:45:46.423841: step 3500, loss = 2.04 (55.5 examples/sec; 4.616 sec/batch)\n2016-07-23 15:47:24.597796: step 3510, loss = 2.13 (25.8 examples/sec; 9.938 sec/batch)\n2016-07-23 15:48:50.303574: step 3520, loss = 1.99 (25.7 examples/sec; 9.968 sec/batch)\n2016-07-23 15:50:13.826881: step 3530, loss = 2.03 (31.7 examples/sec; 8.069 sec/batch)\n2016-07-23 15:51:27.094435: step 3540, loss = 1.93 (63.6 examples/sec; 4.026 sec/batch)\n2016-07-23 15:52:46.522184: step 3550, loss = 2.13 (70.3 examples/sec; 3.639 sec/batch)\n2016-07-23 15:54:10.492599: step 3560, loss = 2.17 (25.6 examples/sec; 10.000 sec/batch)\n2016-07-23 15:55:32.567000: step 3570, loss = 1.93 (27.0 examples/sec; 9.465 sec/batch)\n2016-07-23 15:56:58.073701: step 3580, loss = 2.21 (25.5 examples/sec; 10.058 sec/batch)\n2016-07-23 15:58:21.894001: step 3590, loss = 2.14 (31.5 examples/sec; 8.129 sec/batch)\n```\n", "Just to add my 2 cents; I'm running into a similar issue. My setup includes 4 Titan X GPUs and plenty of CPU/RAM. I'm training two models at a time (each using two GPUs). One model has been training for a long time with consistent times per step. I then start up the second model and things look good for about 1500-3000 steps and both models have consistent training times per step. After a while both models slow down considerably with sporadic training times. I then kill my second model and the first one goes back to fast, consistent time per step. Can't figure out what the problem is, and both my CPU and RAM are not fully utilized.\n", "There was similar sounding issue here\nhttps://github.com/tensorflow/tensorflow/issues/3009#issuecomment-236260459\nCan you see if enabling tcmalloc\nhttp://goog-perftools.sourceforge.net/doc/tcmalloc.html solves it?\n\nOn Wed, Sep 14, 2016 at 4:23 PM, jstaker7 notifications@github.com wrote:\n\n> Just to add my 2 cents; I'm running into a similar issue. My setup\n> includes 4 Titan X GPUs and plenty of CPU/RAM. I'm training two models at a\n> time (each using two GPUs). One model has been training for a long time\n> with consistent times per step. I then start up the second model and things\n> look good for about 1500-3000 steps and both models have consistent\n> training times per step. After a while both models slow down considerably\n> with sporadic training times. I then kill my second model and the first one\n> goes back to fast, consistent time per step. Can't figure out what the\n> problem is, and both my CPU and RAM are not fully utilized.\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2987#issuecomment-247187237,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AABaHAselrJKyaPFPtfK2gVnWB_r--pTks5qqIHjgaJpZM4I7Wtl\n> .\n", "Update: Tried clearing the cache and that didn't help (didn't appear to be under pressure anyway). Also, I noticed that CPU usage starts at ~8% when the second model gets started, but gradually climbs to about 50%, at which time training drastically slows down for both cases. Not sure why CPU usage would increase overtime like that with a second model running, and why there would be a slow down at only 50% load. Killing the second model brings the load back down to 8%.\n\nAnother observation: high CPU usage allocated to User during normal operation, and goes close to zero while the Sys usage shoots up when the slowdown hits. Wondering now if my problem is actually unrelated to this ticket (sorry for the distraction) and could be a hardware issue on my end. I'll keep digging.\n", "@jstaker7 I'm having the exact same problem. None of the suggestions in linked threads are helping. Did you find a solution?", "Can you try running under tf 1.0 alpha? (I believe it uses jemalloc which\ncan help)\n\nOn Jan 21, 2017 6:36 AM, \"Jeremy Howard\" <notifications@github.com> wrote:\n\n> @jstaker7 <https://github.com/jstaker7> I'm having the exact same\n> problem. None of the suggestions in linked threads are helping. Did you\n> find a solution?\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/2987#issuecomment-274265429>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AABaHMxUMvOo7ShnPXj0zjbS93kqzJejks5rUhfYgaJpZM4I7Wtl>\n> .\n>\n", "Sure I'll try that.", "Was this ever truly resolved? I'm observing the same problem. Training slows down linearly with time to the point of it basically becoming impossible to use tensorflow.", "It was resolved. Perhaps you didn't finalize the graph"]}, {"number": 2986, "title": "ImportError: libcudart.so.7.5: cannot open shared object file: No such file or directory", "body": "- GPU: GTX 1080\n- GPU driver: 367.27\n- OS: Ubuntu 16.04\n- CUDA: 8.0 rc\n- cudnn: 8.0\n\nWhen I`import tensorflow`I got the error in the header. Looks like tensorflow doesn't support one (or more) or those?\n", "comments": ["You have to install from sources right now to use any library that's still in release candidate mode.\n", "Try `sudo ldconfig /usr/local/cuda/lib64`\n", "It says `sbin/ldconfig.real: /usr/local/cuda/lib64/libcudnn.so.5 is not a symbolic link`\n@talpay\n", "There is a easy way.\n\n```\ncd /usr/local/cuda/lib64/\ncp libcudnn.so.8.0.5 libcudnn.so.5\n```\n"]}]