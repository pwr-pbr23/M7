[{"number": 21363, "title": "tf.clip_by_global_norm() returns zero tensors if the norm is infinity", "body": "I think this should return NaN tensors instead of zero tensors to signal an error. Users expect `tf.clip_by_global_norm()` to return tensors unaltered or with global norm matching the threshold passed into the function. This invariant is violated by returning zero tensors.\r\n\r\nThis is caused by `1.0 / use_norm` which is zero for an infinite norm in [clip_ops.py](https://github.com/tensorflow/tensorflow/blob/25c197e02393bd44f50079945409009dd4d434f8/tensorflow/python/ops/clip_ops.py#L257):\r\n\r\n```python\r\nscale = clip_norm * math_ops.minimum(\r\n        1.0 / use_norm,\r\n        constant_op.constant(1.0, dtype=use_norm.dtype) / clip_norm)\r\n```\r\n\r\nBoilerplate:\r\n\r\n- Have I written custom code: No\r\n- OS Platform and Distribution: N/A\r\n- TensorFlow installed from: N/A\r\n- TensorFlow version: master\r\n- Bazel version: N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n- Exact command to reproduce: N/A\r\n- Mobile device: N/A", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Done", "Hi, which norm do you mean the infinity is in, use_norm or clip_norm?", "The infinity is in `use_norm`, which is computed internally if not provided by the user.", "Thanks. Sounds reasonable to me. I create the PR #21428 to fix it.", "Thanks @facaiy! I marked this as \"Contributions Welcome\" since you've prepared a PR for it."]}, {"number": 21362, "title": "WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".", "body": "Version 1.10.0-rc1\r\nbazel-0.15.0", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "I got this during the FreeBSD port build on FreeBSD 11.2. But now I think that this is a bazel bug. Please feel free to close.\r\n\r\nFYI: The other report I posted today is actually a show-stopper: https://github.com/tensorflow/tensorflow/issues/21366", "try invoking the command in your \"tensorflow\" directory. I was getting the same error when i was in different directory.\r\n\r\n\r\n", "Nagging Assignee @tatatodd: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing this issue for now.", "I'm still seeing this with TensorFlow 1.12.0. Why was this closed while this problem hasn't been fixed?", "Still facing this issue in 1.13.1", "I am facing this issue too.\r\nwhen i am installing tensorflow from source https://www.tensorflow.org/install/source \r\non running bellow line i get error\r\n./configure\r\nExtracting Bazel installation...\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.23.0 installed.\r\nPlease downgrade your bazel installation to version 0.21.0 or lower to build TensorFlow! To downgrade: download the installer for the old version (from https://github.com/bazelbuild/bazel/releases) then run the installer.\r\n\r\nHave I written custom code\r\nOS Platform and Distribution = Ubuntu 18.04 LTS\r\nTensorFlow installed from = source\r\nTensorFlow version = could not able to install  because of this error\r\nBazel version = tried 0.21 and 0.23\r\nCUDA/cuDNN version = NA\r\nGPU model and memory = NA\r\nExact command to reproduce = NA\r\nMobile device = NA\r\n"]}, {"number": 21361, "title": "[tf.keras] Fit Generator & TensorBoard Callback with Stateful Metrics", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04/OSX\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.9.0/1.10.0rc1\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **GCC/Compiler version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: n/a\r\n- **Exact command to reproduce**: n/a\r\n\r\n### Describe the problem\r\nWhen using ``fit_generator`` with ``TensorBoard`` callback and a ``StatefulMetric`` inside ``tf.keras``:\r\n\r\n```\r\n  File \"/anaconda/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\", line 841, in on_epoch_end\r\n    summary_value.simple_value = value.item()\r\nAttributeError: 'float' object has no attribute 'item'\r\n```\r\n**This issue was fixed in regular Keras** (https://github.com/keras-team/keras/pull/10673), but has not been reviewed (https://github.com/tensorflow/tensorflow/pull/21071) yet in TensorFlow\r\n\r\n### Source code / logs\r\n\r\nHeres a dummy example of fit_generator with a dummy stateful metric that demonstrates the error:\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.keras.layers import Input, Dense\r\nfrom tensorflow.python.keras.models import Model\r\nfrom tensorflow.python.ops import state_ops\r\n\r\nimport numpy as np\r\n\r\nclass BatchCounter(tf.keras.layers.Layer):\r\n\r\n        def __init__(self, name=\"batch_counter\", **kwargs):\r\n            super(BatchCounter, self).__init__(name=name, **kwargs)\r\n            self.stateful = True\r\n            self.batches = tf.keras.backend.variable(value=0, dtype=\"int32\")\r\n\r\n        def reset_states(self):\r\n            tf.keras.backend.set_value(self.batches, 0)\r\n\r\n        def __call__(self, y_true, y_pred):\r\n            current_batches = self.batches * 1\r\n            self.add_update(\r\n              state_ops.assign_add(self.batches,\r\n                                   tf.keras.backend.variable(value=1, dtype=\"int32\")))\r\n            return current_batches + 1\r\n\r\nclass DummyGenerator(object):\r\n    \"\"\" Dummy data generator. \"\"\"\r\n\r\n    def run(self):\r\n        while True:\r\n            yield np.ones((10, 1)), np.zeros((10, 1))\r\n\r\ntrain_gen = DummyGenerator()\r\nval_gen = DummyGenerator()\r\n\r\n# Dummy model\r\ninputs = Input(shape=(1,))\r\noutputs = Dense(1)(inputs)\r\nmodel = Model(inputs=inputs, outputs=outputs)\r\nmodel.compile(loss=\"mse\", optimizer=\"adam\", metrics=[BatchCounter()])\r\n\r\nmodel.fit_generator(\r\n    train_gen.run(),\r\n    steps_per_epoch=5,\r\n    epochs=200,\r\n    validation_data=val_gen.run(),\r\n    validation_steps=5,\r\n    callbacks=[tf.keras.callbacks.TensorBoard()])\r\n```\r\n", "comments": ["@fchollet ", "Nagging Assignee @jart: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Looks like this was fixed in this commit:\r\nhttps://github.com/tensorflow/tensorflow/commit/306dc604991c7f5fa45622a8c8236f59039d2c6a", "Hi,\r\nI am trying the below command but it is giving the mentioned error. Please help me out.\r\n\r\nCode : history = model.fit_generator(train_generator, steps_per_epoch = 8,\r\n                             epochs = 15, verbose = 1, callback = [tensorboard])\r\n\r\nError: fit_generator() got an unexpected keyword argument 'callback'"]}, {"number": 21360, "title": "Minor doc enhancement in CheckpointSaverHook", "body": "", "comments": []}, {"number": 21359, "title": "Object detection: Unable to get mAP", "body": "Right now I'm facing a problem which is unable to get the mAP value from my trained model.\r\nI run below commands parallelly to monitor the mAP of my model.\r\n\r\nTrain.py\r\n`python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/faster_rcnn_resnet101_kitti.config --num_clones=2 --ps_tasks=1`\r\n\r\nEval.py\r\n`CUDA_VISIBLE_DEVICES=\"\" python eval.py --logtostderr --pipeline_config_path=training/faster_rcnn_resnet101_kitti.config --checkpoint_dir=training/ --eval_dir=images/test `\r\n\r\nTensorboard\r\n`tensorboard --logdir=imag/test`\r\n\r\nHowever in the tensorboard there is no scalar tab. There is only Images and Graphs tab. My ultimate goal is to calculate the detection accuracy of my model.\r\nPlease help. \r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@alvinxiii would you please fill out the template?\r\n\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "It has been 35 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 21358, "title": "[Bug]Using CUDA_VISIBLE_DEVICES for Specified GPU", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow):NA\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- **TensorFlow installed from (source or binary):binary\r\n- **TensorFlow version (use command below):1.0.0\r\n- **Python version:3.5\r\n- **Bazel version (if compiling from source)**:NA\r\n- **GCC/Compiler version (if compiling from source)**:NA\r\n- **CUDA/cuDNN version**:8.0\r\n- **GPU model and memory**:GTX1080,8G\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI am trying to train two same structure segmentation model,so i build two tensorflow graphs on two GPU.But,when i use CUDA_VISIBLE_DEVICES specify GPU, i find there exist bug.so I'm trying to do some experiment\uff1aCUDA_VISIBLE_DEVICES  is 0,can detect GPU:0,but set the value is 1,can't detect GPU:1,only detec GPU:0,but set the value is 0,1,GPU:0,GPU:1 all can detected..the result is confused.\r\n\r\n### Source code / logs\r\ni attach the result.\r\n![1](https://user-images.githubusercontent.com/28638716/43631453-d1eca760-9735-11e8-89da-63eb36502e04.PNG)\r\n![2](https://user-images.githubusercontent.com/28638716/43631455-d31a2220-9735-11e8-8ac6-bdad3ee7ebbe.PNG)\r\n![3](https://user-images.githubusercontent.com/28638716/43631457-d51375c2-9735-11e8-9191-db17eddd16e5.PNG)\r\n\r\nanyone know how to do it?", "comments": ["Even though TensorFlow says it's using GPU0, it's very likely that it is actually using GPU1. The GPU numbers TensorFlow shows are relative. Run `nvidia-smi` to check which GPU TF is actually using.", "Yeah,i do another experiment that is running the python script with CUDA_VISIBLE_DEVICES is 0 and 1,at same time also running the nvidia-smi commend.There are different results between nvidia-smi and tensorflow's device_lib.I set CUDA_VISIBLE_DEVICES  is 1,and nvidia-smi also show the GPU1 is working,but tensorflow show the GPU0 is working,maybe there is a bug.\r\n![2](https://user-images.githubusercontent.com/28638716/44074290-97d6ac4a-9fca-11e8-95bf-630590c908fe.PNG)\r\n![1](https://user-images.githubusercontent.com/28638716/44074295-99d8999a-9fca-11e8-95f2-fc2d52b8798f.PNG)", "Nagging Assignee @zheng-xq: It has been 32 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "The `CUDA_VISIBILE_DEVICES` environment variable controls the GPUs that are visible to TensorFlow (or really any CUDA library).\r\n\r\nThe names of devices in TensorFlow (`\"/gpu:0\") are assigned in serial order based on what is visible. Thus, when you set `CUDA_VISIBLE_DEVICES=1`, then the TensorFlow device name \"/gpu:0\" corresponds to it (since it is referring to the one and only device visible to TensorFlow). As mentioned above, you should be able to see the physical device where things are happening via `nvidia-smi`.\r\n\r\nI'm tempted to close this out (since when `CUDA_VISIBLE_DEVICES` is set, then no CUDA library client  can know of the existence of non-visible devices anyway, or so I think).\r\n\r\nFeel free to reopen if I'm misunderstood.\r\nThanks."]}, {"number": 21356, "title": "register float64 GPU kernel for Conv3d", "body": "Fix #21295.", "comments": ["Pinging review", "Thank you. Any update?", "@yifeif Hi, yifei. Do you have any update for the PR? Thank you.", "Looks like the PR failed some internal tests. @josh11b is the failure something that can be fixed in the PR?", "@josh11b Hi, what can I do for the PR? ", "Nagging Assignee @case540: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "ping @josh11b \r\n\r\n@case540 @yifeif could you give a hand? Thanks.", "I'm sorry, I'm not sure what the internal test failures are, I don't know how to help.", "Hi Josh, the change and test results from previous import are at cl/208243530. We might need to do anther import.", "Thank you, @yifeif @josh11b . Is it necessary to merge / rebase the latest master branch?", "This appears to be failing the test. Triggering again.", "Thanks, I'll check the log of failed test later.", "```\r\n2018-10-15 18:28:30.260444: E tensorflow/core/common_runtime/executor.cc:624] Executor failed to create kernel. Invalid argument: Conv3DBackpropInputOpV2 only supports NDHWC on the CPU.\r\n```\r\nseems to be triggering.", "@drpngx @josh11b Sorry for the delay. The environment was broken in my working station last month. I have fixed the failure on GPU Python3. And the failed test on GPU CC seems unrelated for me.\r\n\r\nCould you take a look again? Thanks.", "@drpngx Hi, I merged the latest master branch to resolve code conflict. Those failures seems unrelated?", "OK, let's try to test internally.", "Thank you. Please keep me updated :-)", "Thank all for your help :-)"]}, {"number": 21355, "title": "\"ValueError: If initializer is a constant, do not specify shape.\" when calling dynamic_rnn", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:NA\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Debian 4.9.65\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:NA\r\n- **TensorFlow version (use command below)**:1.8.0\r\n- **Python version**:2.7.14\r\n- **Bazel version (if compiling from source)**:NA\r\n- **GCC/Compiler version (if compiling from source)**:NA\r\n- **CUDA/cuDNN version**:9.1\r\n- **GPU model and memory**:GTX 1080Ti, 11G\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI am trying to implement a LSTM encoder with attention mechanism, the embedding matrix of input sentences with shape [batch_size, seq_length, embedding_size] is passed into a dynamic lstm layer. I'm trying to use the dynamic_rnn wrapper on a BasicLSTMCell. However, an error raised and I can't find any solution about the problem. \r\n\r\n### Source code / logs\r\nThe below codes are used to build the graph:\r\n```\r\nwith tf.device('/cpu:0'), tf.variable_scope('embedding', reuse=tf.AUTO_REUSE):\r\n    emb_w = tf.get_variable('emb_w', shape=[self.vocab_size, self.embedding_size], \r\ninitializer=tf.random_uniform_initializer())\r\n    embedded_chars = tf.nn.embedding_lookup(emb_w, input_x)\r\nwith tf.variable_scope('lstm', reuse=tf.AUTO_REUSE):\r\n    self.lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(self.rnn_hidden_size)\r\n    self.lstm_out, _ = tf.nn.dynamic_rnn(self.lstm_cell, inputs=embedded_chars, dtype=tf.float32)\r\n```\r\nAnd I got such error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"src/tf_simnet.py\", line 252, in <module>\r\n    tf.app.run()\r\n  File \"/data00/home/wupeihao/anaconda4/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 126, in run\r\n    _sys.exit(main(argv))\r\n  File \"src/tf_simnet.py\", line 243, in main\r\n    train(args, config)\r\n  File \"src/tf_simnet.py\", line 46, in train\r\n    model = SimNet(config, args.encoder_type)\r\n  File \"/data00/home/zhaodongdi/workspace/lab-speech/SimNet/src/model.py\", line 137, in __init__\r\n    self.query_encoder = LSTMEncoder(self.query, config)\r\n  File \"/data00/home/zhaodongdi/workspace/lab-speech/SimNet/src/model.py\", line 95, in __init__\r\n    self.lstm_out, _ = tf.nn.dynamic_rnn(self.lstm_cell, inputs=embedded_chars, dtype=tf.float32)\r\n  File \"/data00/home/wupeihao/anaconda4/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 627, in dynamic_rnn\r\n    dtype=dtype)\r\n  File \"/data00/home/wupeihao/anaconda4/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 824, in _dynamic_rnn_loop\r\n    swap_memory=swap_memory)\r\n  File \"/data00/home/wupeihao/anaconda4/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 3224, in while_loop\r\n    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\n  File \"/data00/home/wupeihao/anaconda4/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2956, in BuildLoop\r\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n  File \"/data00/home/wupeihao/anaconda4/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2893, in _BuildLoop\r\n    body_result = body(*packed_vars_for_body)\r\n  File \"/data00/home/wupeihao/anaconda4/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 3194, in <lambda>\r\n    body = lambda i, lv: (i + 1, orig_body(*lv))\r\n  File \"/data00/home/wupeihao/anaconda4/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 795, in _time_step\r\n    (output, new_state) = call_cell()\r\n  File \"/data00/home/wupeihao/anaconda4/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 781, in <lambda>\r\n    call_cell = lambda: cell(input_t, state)\r\n  File \"/data00/home/wupeihao/anaconda4/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 339, in __call__\r\n    *args, **kwargs)\r\n  File \"/data00/home/wupeihao/anaconda4/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 699, in __call__\r\n    self.build(input_shapes)\r\n  File \"/data00/home/wupeihao/anaconda4/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 588, in build\r\n    shape=[input_depth + h_depth, 4 * self._num_units])\r\n  File \"/data00/home/wupeihao/anaconda4/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 546, in add_variable\r\n    partitioner=partitioner)\r\n  File \"/data00/home/wupeihao/anaconda4/lib/python2.7/site-packages/tensorflow/python/training/checkpointable.py\", line 436, in _add_variable_with_custom_getter\r\n    **kwargs_for_getter)\r\n  File \"/data00/home/wupeihao/anaconda4/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1317, in get_variable\r\n    constraint=constraint)\r\n  File \"/data00/home/wupeihao/anaconda4/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1079, in get_variable\r\n    constraint=constraint)\r\n  File \"/data00/home/wupeihao/anaconda4/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 425, in get_variable\r\n    constraint=constraint)\r\n  File \"/data00/home/wupeihao/anaconda4/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 394, in _true_getter\r\n    use_resource=use_resource, constraint=constraint)\r\n  File \"/data00/home/wupeihao/anaconda4/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 718, in _get_single_variable\r\n    raise ValueError(\"If initializer is a constant, do not specify shape.\")\r\nValueError: If initializer is a constant, do not specify shape.\r\n```\r\nAnyone knows how to solve this problem? Thanks a lot.", "comments": []}, {"number": 21354, "title": "Support complex type for tf.Print (Fix issue #21250)", "body": "Fix issue #21250 \r\n\r\nThe following tests passed:\r\n- On mac, rebuilding the python package and executing the test case from the issue above\r\n- With Docker and TensorFlow's CI scripts:\u00a0`//tensorflow/core:framework_tensor_test`\u00a0&\u00a0`//tensorflow/core:lib_strings_strcat_test`\r\n\r\nClang-format was used to verify the C++ coding style.\r\n\r\nNote that it is my first pull request in this project, so let me know if anything is missing.\r\n", "comments": ["It has been 49 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 21353, "title": "Support complex types for tf.Print", "body": "Fix issue #21250.\r\n\r\nThe following tests passed:\r\n- On mac, rebuilding the python package and executing the test case from the issue above\r\n- With Docker and TensorFlow's CI scripts: `//tensorflow/core:framework_tensor_test` & `//tensorflow/core:lib_strings_strcat_test`\r\n\r\nClang-format was used to verify the C++ coding style.\r\n\r\nNote that it is my first pull request in this project, so let me know if anything is missing.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "Sorry, I used the wrong email for the commits, I will send a new one"]}, {"number": 21352, "title": "tf.keras.model layer shows nothing", "body": "tf.keras.Model.layers shows nothing?\r\n\r\n![2018-08-03_115300](https://user-images.githubusercontent.com/26696338/43623464-e6dc594e-9713-11e8-8711-c403f6b20995.gif)\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 21351, "title": "The Chinese version tutorial miss the png image", "body": "Hello,\r\n\r\nI find the page https://www.tensorflow.org/get_started/eager in Chinese version miss the png image, which the link is https://www.tensorflow.org/get_started/eager_files/output_30_0.png. The position is under the \"\u53ef\u89c6\u5316\u635f\u5931\u51fd\u6570\u968f\u65f6\u95f4\u63a8\u79fb\u800c\u53d8\u5316\u7684\u60c5\u51b5\" part.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Thanks @Freshield -- @MarkDaoust , @lamberta -- who is the right person for fixing this?", "BTW, Here is the image which I create follow the tutorial, maybe you can\nuse it\n\nBest Regards\n\nYANG YU\n\nKarmel Allison <notifications@github.com> \u4e8e2018\u5e748\u67084\u65e5\u5468\u516d \u4e0a\u53486:37\u5199\u9053\uff1a\n\n> Thanks @Freshield <https://github.com/Freshield> -- @MarkDaoust\n> <https://github.com/MarkDaoust> , @lamberta <https://github.com/lamberta>\n> -- who is the right person for fixing this?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/21351#issuecomment-410393118>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AOdqPgV7tMWMK03dkuGjnmYdRaA3xJXcks5uNNCcgaJpZM4VtUnn>\n> .\n>\n", "I see. This is a problem with how we name the images.\r\n\r\nThe .ipynb --> .md converter names the image files after the cell index.\r\nAnd there's lag between updating english versions, and any other languages.\r\n\r\nSo if the cell number changes and we republish the image, it moves and breaks the other languages.\r\n\r\nA better solution will be to use the cell['metadata']['id'] for the image paths, then they will be somewhat more robust.", "@lamberta just published new versions that have the images fixed!\r\n\r\nIt's not a permanent/automatic fix.\r\n\r\nI think the real solution is to take advantage of: https://github.com/jupyter/nbconvert/issues/671\r\n", "Nagging Assignees @lamberta, @MarkDaoust: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This is fixed: https://tensorflow.google.cn/tutorials/eager/custom_training_walkthrough?hl=zh-cn\r\n\r\nSometimes there will be issues when jobs are out for translation, but it should settle down. And our latency for CN is going down"]}, {"number": 21350, "title": "add throttle_step to Evalspec and  support estimator evaluation trigger with throttle_step", "body": "try to implement throttle_step settings discussed in #17650\uff0cusing throttle_step to trigger evaluation can be more robust to different GPU load\u3002", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it", "CLAs look good, thanks!\n\n<!-- ok -->", "@xiejw  could you review this PR?", "@wenmengzhou can you please resolve conflicts", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 21347, "title": "Boosted Trees Regressor - incompatible tensor shapes", "body": "Hi, I used the boosted trees regressor estimator and it gave results on the first run. Cleared my checkpoints  to rerun the model but the program then gave the following value error. Think that this may be a bug because the value error was not raised during my first run, and I am not sure how to fix this error.\r\n\r\n````\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 191, in <module>\r\ntf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/.../.../.local/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n_sys.exit(main(argv))\r\n  File \"main.py\", line 170, in main\r\nexperiment.train_and_evaluate()\r\n  File \"/.../.../.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/experiment.py\", line 671, in train_and_evaluate\r\nself.train(delay_secs=0)\r\n  File \"/.../.../.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/experiment.py\", line 389, in train\r\nsaving_listeners=self._saving_listeners)\r\n  File \"/.../.../.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/experiment.py\", line 881, in _call_train\r\nsaving_listeners=saving_listeners)\r\n  File \"/.../.../.local/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 366, in train\r\nloss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/.../.../.local/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1119, in _train_model\r\nreturn self._train_model_default(input_fn, hooks, saving_listeners)\r\n File \"/.../.../.local/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1132, in _train_model_default\r\nfeatures, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\r\n  File \"/.../.../.local/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1107, in _call_model_fn\r\nmodel_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"/.../.../.local/lib/python3.6/site-packages/tensorflow/python/estimator/canned/boosted_trees.py\", line 929, in _model_fn\r\nn_batches_per_layer, config)\r\n  File \"/.../.../.local/lib/python3.6/site-packages/tensorflow/python/estimator/canned/boosted_trees.py\", line 650, in _bt_model_fn\r\nlogits=logits)\r\nFile \"/.../.../.local/lib/python3.6/site-packages/tensorflow/python/estimator/canned/head.py\", line 239, in create_estimator_spec\r\nregularization_losses))\r\n  File \"/.../.../.local/lib/python3.6/site-packages/tensorflow/python/estimator/canned/head.py\", line 1506, in _create_tpu_estimator_spec\r\ntrain_op = train_op_fn(regularized_training_loss)\r\n  File \"/.../.../.local/lib/python3.6/site-packages/tensorflow/python/estimator/canned/boosted_trees.py\", line 611, in _train_op_fn\r\narray_ops.stack(stats_summaries, axis=0), stamp_token)\r\n  File \"/.../.../.local/lib/python3.6/site-packages/tensorflow/python/ops/data_flow_ops.py\", line 1286, in apply_grad\r\ngrad.get_shape().assert_is_compatible_with(self._shape)\r\n  File \"/.../.../.local/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\", line 847, in assert_is_compatible_with\r\nraise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\r\nValueError: Shapes (0,) and (0, 63, 2, 2) are incompatible\r\n````\r\n\r\nFor reference, these are my feature columns.\r\n\r\n```\r\na = tf.feature_column.categorical_column_with_hash_bucket(\"a\", hash_bucket_size=1000)\r\nb = tf.feature_column.numeric_column(\"b\")\r\nb_bucket = tf.feature_column.bucketized_column(b,  boundaries=np.arange(0,2644,100).tolist())\r\nc= tf.feature_column.numeric_column(\"c\")\r\nc_bucket = tf.feature_column.bucketized_column(c,  boundaries=np.arange(0,210,10).tolist())\r\nd = tf.feature_column.numeric_column(\"d\")\r\nd_bucket = tf.feature_column.bucketized_column(d,  boundaries=np.arange(0,245,10).tolist())\r\ne = tf.feature_column.categorical_column_with_vocabulary_list(key=\"e\", vocabulary_list =[\"l\", \"m\", \"h\"])\r\ncolumns = [a, b_bucket, c_bucket, d_bucket, e]\r\nfeature_columns = [tf.feature_column.indicator_column(x) for x in columns]\r\n```", "comments": ["Did you remove all the files in your model directory or just the checkpoint files ?", "I removed the checkpoint files and any potential cache files.", "@nataliaponomareva - Can you take a look?", "Can you show your config for the boosted trees estimator? Did you pass in the feature columns? This first 0 in (0, 63, 2, 2) indicates that you have 0 feature columns", "Hi Natalia, thanks for your reply. This is my code snippet.\r\n\r\n```\r\nfeature_columns = build_model_columns()\r\n#the build_model_columns() function returns the feature_columns from the previous code snippet.\r\nrun_config = tf.estimator.RunConfig(tf_random_seed=42, save_checkpoints_steps=5000)\r\n \r\nif model_type == 'boosted_trees':\r\n        nn = tf.estimator.BoostedTreesRegressor(\r\n            model_dir=FLAGS.model_dir,\r\n            feature_columns = feature_columns,\r\n            n_batches_per_layer = 1000,\r\n            learning_rate = 0.1,\r\n            n_trees = 100,\r\n            max_depth = 6,\r\n            config=run_config)\r\n\r\ntrain_spec=tf.estimator.TrainSpec(input_fn=lambda: input_fn(FLAGS.train_data, FLAGS.train_epochs, True, FLAGS.batch_size))\r\neval_spec=tf.estimator.EvalSpec(input_fn=lambda: input_fn(FLAGS.test_data, 1, True, FLAGS.batch_size),steps=1000)\r\ntf.estimator.train_and_evaluate(nn, train_spec, eval_spec)\r\n```\r\n\r\nI think I passed the feature columns correctly..?", "Btw when you say you cleared the checkpoints - it means you cleared the whole model dir right? Not just checkpoint files? Can you try on a completely new folder\r\nCan you print out the length of feature columns list after u do the build_model_columns()", "Yep I cleared the whole model directory. and tried it on a completely new folder. Same error still exists.\r\nLength of feature columns list printed is 5.", "Ah, sorry this is weird\r\nfeature_columns = [tf.feature_column.indicator_column(x) for x in columns]\r\nYou seem to be creating an indicator over bucketized columns as well. This is not needed. Also, to narrow down\r\n1) Can you first try just with bucketized feature columns b_bucket, d_bucket (no indicator over them)\r\n2) Then add the categorical with an indicator", "Nagging Assignee @nataliaponomareva: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "have not heard back, closing"]}, {"number": 21346, "title": "[Intel MKL] Upgrade to Bazel 0.15.0.", "body": "@gunan Can you approve this? This will fix out nightly builds.", "comments": []}, {"number": 21345, "title": "import_graph_def ERROR when NodeDef contains NameAttrList (func) message defined in attr_value.proto - makes the graph unusable / unloadable in TF 1.8 or 1.9 (works in 1.7). ValueError: NodeDef mentions attr '<my_attr>' not in Op<>", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a\r\n- **TensorFlow installed from (source or binary)**: binary \r\n- **TensorFlow version (use command below)**: v1.9.0-0-g25c197e023 1.9.0\r\n- **Python version**: Python 3.5.2\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **GCC/Compiler version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: n/a\r\n- **Exact command to reproduce**: See below.\r\n\r\n### Describe the problem\r\nOur simplified code `modify.py` below does the following: \r\n\r\n- Loads an existing TF graph from `graph1.pb` into `graph_def = tf.GraphDef()`\r\n- To each node within the `graph_def`, adds user-defined attributes using `NameAttrList func` protobuf message in [attr_value.proto](https://github.com/tensorflow/tensorflow/blob/25c197e02393bd44f50079945409009dd4d434f8/tensorflow/core/framework/attr_value.proto#L40-L44).\r\n- Saves out the modified `graph_def` to `graph2.pb`\r\n- Loads this new `graph2.pb` and imports to default graph --> Works in TF 1.7, fails in TF 1.8/1.9\r\n\r\nThis flow works fine in TF 1.7. However due to an added constraint in TF 1.8 or 1.9, when importing a graph containing nodes with user-defined attributes, it fails as follows (full error log included below):\r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: NodeDef mentions attr 'my_attr' not in Op<name=VariableV2; signature= -> ref:Ref(dtype); attr=shape:shape; attr=dtype:type; attr=container:string,default=\"\"; attr=shared_name:string,default=\"\"; is_stateful=true>; \r\nNodeDef: import/loss3_classifier/biases = VariableV2[_class=[\"loc:@import/loss3_classifier/biases\"], _output_shapes=[[1000]], container=\"\", dtype=DT_FLOAT, my_attr=attr_name[data1=8, data2=2], shape=[1000], shared_name=\"\"](). \r\n(Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\r\n```\r\n\r\nThis is not an issue with the GraphDef interpreting binary mismatching with GraphDef-generating binary, since the same binary is used to generate and interpret. I believe this is failing because the check to ensure all attrs within the NodeDef are valid doesn't account for user-defined attributes given using `NameAttrList func` message in [attr_value.proto](https://github.com/tensorflow/tensorflow/blob/25c197e02393bd44f50079945409009dd4d434f8/tensorflow/core/framework/attr_value.proto#L40-L44). \r\n\r\nThis is breaking some of our production code written in TF1.7, when migrating to TF1.9, since the graph is no longer loadable.\r\n\r\n### Attachments\r\n[graphs_attachment.zip](https://github.com/tensorflow/tensorflow/files/2254600/graphs_attachment.zip)\r\nContains:\r\n- graph1.pb\r\n- graph2.pb\r\n- tensorboard events file for graph1.pb\r\n- tensorboard events file for graph2.pb (can only generate using TF1.7, fails with TF1.8/1.9)\r\n- modify.py script\r\n\r\n### Source code / logs\r\n#### modify.py\r\n```python\r\nimport tensorflow as tf\r\n\r\ndef add_attr_to_tf_node(node, key, name, data):\r\n  \"\"\" Add a dictionary 'data' to a TF graph node,\r\n  with optional label 'name'. Key is 'key'.\r\n  Each element of 'data' should be a tf.AttrValue() instance\r\n  e.g.  a1 = tf.AttrValue(i=8)\r\n        a2 = tf.AttrValue(i=2)\r\n        attrs = {\"data1\": a1, \"data2\": a2}\r\n  \"\"\"\r\n  if name is None:\r\n    node.attr[key].CopyFrom(tf.AttrValue(func={\"attr\": data}))\r\n  else:\r\n    node.attr[key].CopyFrom(tf.AttrValue(func={\"name\": name, \"attr\": data}))\r\n\r\n# Create empty graph_def\r\ngraph_def = tf.GraphDef()\r\n\r\n# Parse/Load from graph1.pb\r\nwith tf.gfile.GFile('graph1.pb', 'rb') as f:\r\n  graph_def.ParseFromString(f.read())\r\n\r\n# Add user attributes to each node in graph_def\r\nfor node in graph_def.node:\r\n  a1 = tf.AttrValue(i=8)\r\n  a2 = tf.AttrValue(i=2)\r\n  attrs = {\"data1\": a1, \"data2\": a2}\r\n  add_attr_to_tf_node(node, 'my_attr', 'attr_name', attrs)\r\n\r\n# Serialize/Save modified graph_def to graph2.pb\r\nwith tf.gfile.GFile('graph2.pb', 'wb') as f:\r\n  f.write(graph_def.SerializeToString())\r\n\r\n# Parse/Load from graph2.pb\r\nwith tf.gfile.GFile('graph2.pb', 'rb') as f:\r\n  graph_def.ParseFromString(f.read())\r\n\r\n# Import graph_def to defalt graph --> WORKS IN TF-1.7, ERROR WITH TF-1.8/1.9\r\ntf.import_graph_def(graph_def)\r\n```\r\n#### TensorFlow 1.7:\r\nNo issues. \r\n\r\n#### TensorFlow 1.8/1.9 (ERROR LOG):\r\n```\r\nTraceback (most recent call last):\r\n  File \"/scratch/tensorflow_1p8/lib/python3.5/site-packages/tensorflow/python/framework/importer.py\", line 489, in import_graph_def\r\n    graph._c_graph, serialized, options)  # pylint: disable=protected-access\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: NodeDef mentions attr 'my_attr' not in Op<name=VariableV2; signature= -> ref:Ref(dtype); attr=shape:shape; attr=dtype:type; attr=container:string,default=\"\"; attr=shared_name:string,default=\"\"; is_stateful=true>; NodeDef: import/loss3_classifier/biases = VariableV2[_class=[\"loc:@import/loss3_classifier/biases\"], _output_shapes=[[1000]], container=\"\", dtype=DT_FLOAT, my_attr=attr_name[data1=8, data2=2], shape=[1000], shared_name=\"\"](). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"modify.py\", line 37, in <module>\r\n    tf.import_graph_def(graph_def)\r\n  File \"/scratch/tensorflow_1p8/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py\", line 432, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/scratch/tensorflow_1p8/lib/python3.5/site-packages/tensorflow/python/framework/importer.py\", line 493, in import_graph_def\r\n    raise ValueError(str(e))\r\nValueError: NodeDef mentions attr 'my_attr' not in Op<name=VariableV2; signature= -> ref:Ref(dtype); attr=shape:shape; attr=dtype:type; attr=container:string,default=\"\"; attr=shared_name:string,default=\"\"; is_stateful=true>; NodeDef: import/loss3_classifier/biases = VariableV2[_class=[\"loc:@import/loss3_classifier/biases\"], _output_shapes=[[1000]], container=\"\", dtype=DT_FLOAT, my_attr=attr_name[data1=8, data2=2], shape=[1000], shared_name=\"\"](). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\r\n```", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "Thanks @angersson for checking on this. I think this can be classified as a bug, since it is breaking the code that is supposed to otherwise run, and does run normally on an earlier version (1.7). The details from the issue template are all populated. Please try to re-create it at your end to see how this actually affects the forward compatibility of the codebase written for 1.7. \r\n\r\nAs long as the code in `modify.py` looks reasonable to do (allowed by TF design), the fact that it breaks importing the graph in TF1.9 and not in TF1.7 speaks to the nature of this issue.\r\n\r\nThe real issue is this: \r\nIn earlier versions, importing a graph def had no checks for the node attributes and if they're valid. It was added in TF1.8 onwards, I believe to ensure no invalid attributes are present when importing the graph def. However it should indeed allow room for a user-attribute, which can be added through the `NameAttrList func` in the [attr_value.proto](https://github.com/tensorflow/tensorflow/blob/25c197e02393bd44f50079945409009dd4d434f8/tensorflow/core/framework/attr_value.proto#L40-L44), which is a valid TF definition for a node attribute. \r\n\r\nPlease let me know if you need help re-creating the issue at your end."]}, {"number": 21344, "title": "Fail to print Global Step as specified in documentation", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra version 10.13.6\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: v1.8.0-0-g93bc2e2072 1.8.0\r\n- **Python version**: 3.6.5\r\n- **Bazel version**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Mobile device**: N/A\r\n- **Exact command to reproduce**:\r\n\r\n```\r\nglobal_step_tensor = tf.Variable(10, trainable=False, name='global_step')\r\nsess = tf.Session()\r\nprint('global_step: %s' % tf.train.global_step(sess, global_step_tensor))\r\n```\r\n\r\nAs found in: [https://www.tensorflow.org/api_docs/python/tf/train/global_step](https://www.tensorflow.org/api_docs/python/tf/train/global_step)\r\n\r\n### Describe the problem\r\nI am testing the command to print the global_step and fails \r\n\r\n### Source code / logs\r\n\r\n> 2018-08-02 13:14:17.524910: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n> Traceback (most recent call last):\r\n>   File \"/*my project*/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1322, in _do_call\r\n>     return fn(*args)\r\n>   File \"/*my project*/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1307, in _run_fn\r\n>     options, feed_dict, fetch_list, target_list, run_metadata)\r\n>   File \"/*my project*/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1409, in _call_tf_sessionrun\r\n>     run_metadata)\r\n> tensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value global_step\r\n> \t [[Node: _retval_global_step_0_0 = _Retval[T=DT_INT32, index=0, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](global_step)]]\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"/*my project*/\", line *, in <module>\r\n>     print('global_step: %s' % tf.train.global_step(sess, global_step_tensor))\r\n>   File \"/*my project*/venv/lib/python3.6/site-packages/tensorflow/python/training/training_util.py\", line 67, in global_step\r\n>     return int(sess.run(global_step_tensor))\r\n>   File \"/*my project*/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 900, in run\r\n>     run_metadata_ptr)\r\n>   File \"/*my project*/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1135, in _run\r\n>     feed_dict_tensor, options, run_metadata)\r\n>   File \"/*my project*/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1316, in _do_run\r\n>     run_metadata)\r\n>   File \"/*my project*/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1335, in _do_call\r\n>     raise type(e)(node_def, op, message)\r\n> tensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value global_step\r\n> \t [[Node: _retval_global_step_0_0 = _Retval[T=DT_INT32, index=0, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](global_step)]]", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nMobile device", "Thanks for the notice, will update the docs. It should read something like:\r\n\r\n```python\r\nglobal_step_tensor = tf.Variable(10, trainable=False, name='global_step')\r\nsess = tf.Session()\r\n# Initialize the variable\r\nsess.run(global_step_tensor.initializer)\r\nprint('global_step: %s' % tf.train.global_step(sess, global_step_tensor))\r\n```"]}, {"number": 21343, "title": "Tensorboard cuts off tops of graphs", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: False\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: \r\n```\r\n\u276f lsb_release -a\r\nNo LSB modules are available.\r\nDistributor ID:\tUbuntu\r\nDescription:\tUbuntu 16.04.3 LTS\r\nRelease:\t16.04\r\nCodename:\txenial\r\n```\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: False\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nObviously, this won't help you, but I'll include for the record:\r\n`tensorboard --logdir=.runs/tensorboard/multi-task-2d/continuous-goals/ --port=6002`\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\n```\r\n\u276f cat tf_env.txt\r\n\r\n== cat /etc/issue ===============================================\r\nLinux rldl6 4.4.0-130-generic #156-Ubuntu SMP Thu Jun 14 08:53:28 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux rldl6 4.4.0-130-generic #156-Ubuntu SMP Thu Jun 14 08:53:28 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy             1.14.0      \r\nprotobuf          3.5.2.post1 \r\ntensorflow        1.8.0       \r\n\r\n== check for virtualenv =========================================\r\nTrue\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.8.0\r\ntf.GIT_VERSION = v1.8.0-0-g93bc2e2072\r\ntf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072\r\nSanity check: array([1], dtype=int32)\r\n\r\n== cat /etc/issue ===============================================\r\nLinux rldl6 4.4.0-130-generic #156-Ubuntu SMP Thu Jun 14 08:53:28 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.3 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux rldl6 4.4.0-130-generic #156-Ubuntu SMP Thu Jun 14 08:53:28 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy             1.14.0      \r\nprotobuf          3.5.2.post1 \r\ntensorflow        1.8.0       \r\n\r\n== check for virtualenv =========================================\r\nTrue\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.8.0\r\ntf.GIT_VERSION = v1.8.0-0-g93bc2e2072\r\ntf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /home/ethanbro/.mujoco/mjpro150/bin:/usr/local/cuda-9.1/bin::/usr/lib/nvidia-390\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nThu Aug  2 10:42:05 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 390.12                 Driver Version: 390.12                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 108...  Off  | 00000000:02:00.0 Off |                  N/A |\r\n| 23%   23C    P8     8W / 250W |    789MiB / 11178MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce GTX 108...  Off  | 00000000:03:00.0 Off |                  N/A |\r\n| 23%   22C    P8     8W / 250W |      1MiB / 11178MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  GeForce GTX 108...  Off  | 00000000:82:00.0 Off |                  N/A |\r\n| 23%   20C    P8     8W / 250W |      1MiB / 11178MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  GeForce GTX 108...  Off  | 00000000:83:00.0 Off |                  N/A |\r\n| 23%   18C    P8     8W / 250W |      1MiB / 11178MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0     21306      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |\r\n|    0     21321      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |\r\n|    0     21356      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |\r\n|    0     21374      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |\r\n|    0     21380      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |\r\n|    0     21382      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |\r\n|    0     21391      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |\r\n|    0     21402      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |\r\n|    0     21412      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |\r\n|    0     21421      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |\r\n|    0     21425      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |\r\n|    0     21426      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |\r\n|    0     21430      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |\r\n|    0     21434      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |\r\n|    0     21435      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |\r\n|    0     21439      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |\r\n|    0     21442      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |\r\n|    0     21446      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |\r\n|    0     21447      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |\r\n|    0     21449      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |\r\n|    0     21450      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |\r\n|    0     21451      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |\r\n|    0     21453      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |\r\n|    0     21454      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |\r\n|    0     21456      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |\r\n|    0     21457      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |\r\n|    0     21460      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |\r\n|    0     21472      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |\r\n|    0     21474      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |\r\n|    0     21479      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |\r\n|    0     21480      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |\r\n|    0     21484      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |\r\n|    0     21498      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |\r\n|    0     21519      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |\r\n|    0     21529      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |\r\n|    0     21540      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-9.1/targets/x86_64-linux/lib/libcudart.so.9.1.85\r\n/usr/local/cuda-9.1/targets/x86_64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-9.1/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-9.1/doc/man/man7/libcudart.7\r\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61\r\n/usr/local/cuda-8.0/lib64/libcudart_static.a\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\r\n```\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nThis is a minor issue, but tensorflow has a tendency to cut off the tops of peaking graph lines after clicking the \"Fit domain to data\" button:\r\n<img width=\"313\" alt=\"screen shot 2018-08-02 at 10 46 20 am\" src=\"https://user-images.githubusercontent.com/10344742/43591539-9523af3a-9641-11e8-9d8d-f7149f07fc04.png\">\r\n\r\nOf course, these are the lines I usually care the most about. I find myself constantly setting smoothing to .9, clicking the \"Fit domain to data\" button, and then setting smoothing back to .99. Is this avoidable somehow?\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Please submit TensorBoard issues in TB issues: https://github.com/tensorflow/tensorboard/issues"]}, {"number": 21342, "title": "install TF using Anaconda", "body": "Hi\r\n\r\nUsing \"Use pip in Anaconda\" installation method to install TF on ubuntu 16.04 system with Intel Xeon E5645 CPU, I test installation which give a error message \"Illegal instruction (core dumped)\". **Why?**\r\n\r\nBut when I use conda in Anaconda to install TF directly (\"conda install tensorflow\"), I can test installation successfully. **Can this installation method install TF-CPU and TF-GPU versions at the same time\uff1f**\r\n\r\nAlan", "comments": ["Alan tf-gpu and tf-cpu are completely different to install and some items the tf-gpu is default when downloading.\nPlease check that you can import tensorflow as\n- import tensorflow as tf\nIt might happen that you install tf-gpu and your pc does not support it. So uninstall the tensorflow and download CPU version from tensorflow website using pip/pip3 in anaconda command line or from the anaconda website to import tensorflow special to your pc configuration. ", "Try this https://anaconda.org/conda-forge/tensorflow", "The issue is about your CPU model, not the installation. closing."]}, {"number": 21341, "title": " Does tensorflow  support Xeon E5645 CPU ?", "body": "Hi,\r\n\r\nDoes tensorflow version r1.9 support Intel Xeon E5645 CPU?\r\n\r\nThanks !\r\nAlan", "comments": ["https://ark.intel.com/products/48768/Intel-Xeon-Processor-E5645-12M-Cache-2_40-GHz-5_86-GTs-Intel-QPI\r\nYour CPU does not have AVX instruction set.\r\nIf you install from certain community maintained packages, or build from sources, yes.\r\nIf you install from official binaries, no\r\n\r\nClosing as a duplicate of https://github.com/tensorflow/tensorflow/issues/19584\r\n"]}, {"number": 21340, "title": "tf.fold{r,l} no longer works on sequence of tensors", "body": "### System information\r\n- **Have I written custom code**: Yes\r\n- **OS Platform and Distribution**: Ubuntu 16.04\r\n- **Mobile device**: N/A\r\n- **TensorFlow installed from**: binary\r\n- **TensorFlow version (use command below)**: `v1.9.0-0-g25c197e023`\r\n- **Python version**: 3.5.2\r\n- **Bazel version**: N/A\r\n- **GCC/Compiler version**: N/A\r\n- **CUDA/cuDNN version**: 9.0\r\n- **GPU model and memory**: GTX 1060 (6GB)\r\n- **Exact command to reproduce**: see below\r\n\r\n### Describe the problem\r\n\r\nStarting from TensorFlow 1.9, it appears that `tf.fold{r,l}` functions no longer work on a list of tensors. The code snippet provided below works fine in 1.8 but not in 1.9 or 1.10-rc1. Do you confirm the bug? Thanks. \r\n\r\n### Source code / logs\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\na = tf.constant([1, 2])\r\nb = tf.constant([3, 4])\r\nc = tf.foldl(lambda a, x: a * x, [a, b])\r\n\r\nwith tf.Session() as sess:\r\n    print(sess.run(c))\r\n```\r\n\r\nshould print `[3, 8]` but fails with:\r\n\r\n```text\r\nTraceback (most recent call last):\r\n  File \"test/foldl.py\", line 5, in <module>\r\n    c = tf.foldl(lambda a, x: a * x, [a, b])\r\n  File \"<python>/envpy3/lib/python3.5/site-packages/tensorflow/python/ops/functional_ops.py\", line 145, in foldl\r\n    swap_memory=swap_memory)\r\n  File \"<python>/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 3209, in while_loop\r\n    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\n  File \"<python>/envpy3/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2941, in BuildLoop\r\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n  File \"<python>/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2878, in _BuildLoop\r\n    body_result = body(*packed_vars_for_body)\r\n  File \"<python>/lib/python3.5/site-packages/tensorflow/python/ops/functional_ops.py\", line 138, in compute\r\n    a = fn(a, elem_i)\r\n  File \"test/foldl.py\", line 5, in <lambda>\r\n    c = tf.foldl(lambda a, x: a * x, [a, b])\r\nTypeError: can't multiply sequence by non-int of type 'list'\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nTensorFlow installed from", "Thanks for the report @guillaumekln -- it doesn't appear that those particular functions have changed recently, at least not since ~April when support for nested structures was added, but it's easy to imagine that changes in while loops or similar had an unexpected effect. @guptapriya , could this be related to changes you made on 07/03 [here](https://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/python/ops/control_flow_ops.py#L3209)?", "Hey @karmel, you're referring to this change right: https://github.com/tensorflow/tensorflow/commit/bbde9a6c47fedc35a03c90b85b9f301d643f6680#diff-76ab9dafbe12c20ddc3769c6b108986c\r\n\r\nThis change doesn't exist in 1.9 from what I can tell (the file you linked to is the 1.9 version right?). It doesn't have the changes from that commit. Also, that change should actually have no effect unless the new `return_same_structure` argument was used. So even if this change were in 1.9, it shouldn't change existing behavior.", "According to an automated `git bisect`, the commit 9e1d93d28fe30171de3f6838028eeadb44b0d6fd is the culprit. It was first released as part of 1.9.", "@ebrevdo , @lukaszkaiser -- looks like that was someone working with you on adding support for nested tensors? Do you have context here, or know who would?", "We found the associated internal change & author, will loop back.  Will see if this is a backwards breaking change.", "OK; this isn't quite a backwards breaking change because you're relying on tf auto-magic to perform a tf.stack() for you.\r\n\r\nThe [version 1.8 documentation](https://www.tensorflow.org/versions/r1.8/api_docs/python/tf/foldl) explicitly states that it expects a tensor or tensor-like object, and a list is not one of these.  A tensor or a numpy array is.  Now that we've added support for nested structures, `[a, b]` is treated as a list of 2 tensors; and the foldl body sees a list of 2 tensors at each iteration.\r\n\r\nWhat you want to do is pass a single tensor:\r\n```\r\nc = tf.foldl(lambda a, x: a * x, tf.stack([a, b]))\r\n```", "Thanks @ebrevdo. After re-reading the documentation, I can accept that my input was not covered by the API. However, the `fold{l,r}` examples from the documentation are no longer working on 1.9 because of this change:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nelems = [1, 2, 3, 4, 5, 6]\r\ns = tf.foldl(lambda a, x: a + x, elems)\r\n\r\nwith tf.Session() as sess:\r\n    print(sess.run(s))\r\n```\r\n\r\nprints `21` on 1.8 but fails on 1.9 with:\r\n\r\n```text\r\nTraceback (most recent call last):\r\n  File \"test/foldl.py\", line 4, in <module>\r\n    s = tf.foldl(lambda a, x: a + x, elems)\r\n  File \"<python>/lib/python3.5/site-packages/tensorflow/python/ops/functional_ops.py\", line 125, in foldl\r\n    n = elems_flat[0].shape[0].value or array_ops.shape(elems_flat[0])[0]\r\n  File \"<python>/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py\", line 615, in __getitem__\r\n    return self._dims[key]\r\nIndexError: list index out of range\r\n```\r\n\r\nThey should either be updated or this change be considered as breaking."]}, {"number": 21339, "title": "Error in impoerting tensorflow 1.9", "body": "Hello!\r\nI had installed tensorflow 1.5 for using g2p_seq2seq. But for some error using this, I decided to upgrade my tensorflow version to 1.9. when I use pip install --upgrade tensorflow everything is ok and I have Successfully installed tensorboard-1.9.0 tensorflow-1.9.0 in my CMD, but when I want to import tensorflow I have below error : \r\n\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\ida\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 14, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\ida\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 658, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 571, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\ida\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\ida\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\ida\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 16, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\ida\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<pyshell#0>\", line 1, in <module>\r\n    import tensorflow\r\n  File \"C:\\Users\\ida\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\ida\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\ida\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\ida\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 14, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\ida\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 658, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 571, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\ida\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\ida\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\ida\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 16, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\ida\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nwhat should I do now???\r\nI really need to help.\r\n\r\nmy platform is windows 10, my python version is 3.6.5 and I'm using cpu tensorflow.\r\n\r\nThanks in advance.\r\n", "comments": ["You can try to uninstall tensorflow and reinstall tensorflow CPU version from tensorflow website.", "I've done it, but I've gotten the same error.", "A few users have claimed using conda instead of pip install helped with Windows python3.5 issues. Can you try that?", "No I've never worked with it.\r\nI uninstalled 1.9.0 version of tensorflow and installed 1.5.0 version of it to use the older verson of g2p_seq2seq.", "Still There is issue you have downgrade installation to tensorflow 1.5.\r\nis there any proper solution ?", "I tried to debug this or a related issue a little more: https://github.com/tensorflow/tensorflow/issues/27035#issuecomment-479392652", "@VBR1016, Sorry for the late response. Is this still an issue for you?\r\n\r\nCould you please update TensorFlow to the latest stable version v.2.6 and let us know if you are facing the same error. For installation you can refer [this link](https://www.tensorflow.org/install). Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/21339\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/21339\">No</a>\n"]}, {"number": 21338, "title": "Max pooling cause error on empty batch", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 3.10.0-693.2.2.el7.x86_64\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: Python 2.7.14 :: Anaconda\r\n- **Bazel version (if compiling from source)**: None\r\n- **GCC/Compiler version (if compiling from source)**: None\r\n- **CUDA/cuDNN version**: cuda==9.0, cudnn==7.0.4\r\n- **GPU model and memory**: None\r\n- **Exact command to reproduce**: See below\r\n\r\n### Describe the problem\r\nWhen batch_size is 0, max pooling operation seems to produce an unhandled cudaError_t status. It may cause subsequent operations fail with odd error message. That is extremely difficult to debug.\r\n\r\n(This corner case bothers us, where we first extract some bounding boxes and then run traditional convolution operations on areas specified by them. The above error occurs in case that no bounding boxes are detected thus batch_size becomes 0. However, the python exception will be randomly thrown at following operation or following session run steps)\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nx = tf.placeholder(dtype=tf.float32, shape=[None, 4, 4, 1])\r\npool_op = tf.nn.pool(x, pooling_type=\"MAX\", window_shape=[2, 2], strides=[1, 1], padding=\"SAME\")\r\n\r\ny = tf.placeholder(dtype=tf.float32, shape=[None])\r\nother_op = tf.where(tf.equal(y, 1.0))\r\n\r\nnormal_data = np.zeros([1, 4, 4, 1], dtype=\"float32\")\r\nempty_data = np.zeros([0, 4, 4, 1], dtype=\"float32\")\r\n\r\n# cudaError is thread local, limit thread pool size to make it easy to reproduce\r\nconfig = tf.ConfigProto()\r\nconfig.inter_op_parallelism_threads = 1\r\nwith tf.Session(config=config) as sess:\r\n    # run other_op success\r\n    print sess.run(other_op, {y: [1.0, 2.0, 3.0, 4.0]})  # [[0]]\r\n\r\n    # run pooling on datas success\r\n    print sess.run(pool_op, {x: normal_data}).shape  # (1, 4, 4, 1)\r\n    print sess.run(pool_op, {x: empty_data}).shape  # (0, 4, 4, 1)\r\n\r\n    # run other_op now failed\r\n    print sess.run(other_op, {y: [1.0, 2.0, 3.0, 4.0]})  # err\r\n``` \r\n\r\nAbove code report error:\r\ntensorflow.python.framework.errors_impl.InternalError: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true / nonzero indices.  temp_storage_bytes: 1, status: invalid configuration argument\r\n\r\n\"invalid configuration argument\" seems to be message return by cudaGetError, which indicates a failed kernel launch due to zero or too large number of block threads.\r\n\r\n### Source code / logs\r\n![image](https://user-images.githubusercontent.com/7600935/43579974-6dd4967a-9686-11e8-9b22-8288159d155c.png)\r\n\r\n", "comments": ["Interesting findings! The error disappears when you use NCHW data format.\r\nNCHW maxpooling will use cudnn and I've fixed the empty-input case for cudnn in https://github.com/tensorflow/tensorflow/pull/15264/files#diff-13381a722607d8496555bfd0e84c19ba.\r\n\r\nHowever the NHWC code path uses a custom cuda kernel which does not explicitly handle empty input tensor.", "Adding an if-else somewhere in the op to check empty inputs can solve this issue. But before that I think it's worth fixing the unit test framework first:\r\n\r\nThere is actually a test that should've triggered this error: https://github.com/tensorflow/tensorflow/blob/1a13c4f2a0b4491ae3003ff0a400d5d8cb521c4a/tensorflow/python/kernel_tests/pooling_ops_test.py#L570-L578. Maybe the unit tests should check cuda error after each session run.\r\n", "Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nMobile device", "@ppwwyyxx  Thanks for your nice notes! The error disappears after I change to NCWH.\r\n\r\nI go through some test cases and find that most of them just do a session.run() and check result with function like assertAllClose(). And rare case of cudaGetLastError() or cudaPeekAtLastError() are used directly in tf repo (Does it mean kernel error will somewhat \"broadcast\" into execution engine?). \r\n\r\nIn WhereOp case, the error seems to finally get detected in nvidia cub lib https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/where_op_gpu.cu.h#L21\r\n\r\nAny idea about mechanism to do such kind of checking in python? \r\n#", "I don't think the error can be checked from Python. I hope the TF team will find a way to fix the tests soon. It will be even better if there is a way to identify ops that do not support empty inputs. Similar error probably exists in many places. \r\n\r\nI reference a horovod issue above which may be related (I'm also using empty inputs when I met that issue, but not using NHWC maxpool). Others have seen similar issues, for example: https://github.com/tensorpack/tensorpack/issues/760, https://github.com/CharlesShang/FastMaskRCNN/issues/159, https://github.com/tensorflow/tensorflow/issues/16035, https://github.com/tensorflow/serving/issues/627. All of these issues have not been clearly resolved, and they are all object detection use cases where empty inputs appear quite often. So it sounds like they are all related to the bug you found.", "Thanks @ppwwyyxx! I marked this as \"Contributions Welcome\" since you're looking at it.", "@angersson No I'm not. I can fix the pooling ops but I expect TF team to find out why the tests did not detect such failure.", "At least in synchronous execution on gpu devices, whether the computation is buggy or not seems to be checked by OpContext's status, which requires OP developers to follow some error handling conventions, or else errors may \"leak\".\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/gpu_device.cc#L506-L508\r\n\r\nI test to do some additional check here and recompile, do get nonzero cuda error code (9). Wonder why such kind of post op check not done currently.\r\n\r\n\r\n  ", "To summarize, what happened is:\r\n1. cuda kernel launch `mykernel<<<0, ...>>>` will do nothing (exactly what we need) but set cuda error\r\n2. the error is not checked after the kernel launch.\r\n3. the error may appear as an error of other ops (if the other op happen to check cuda error)\r\n\r\nI think it's a quite serious issue: \r\n1. other ops (in addition to pooling) may have the same problem.\r\n2. it causes very misleading error messages\r\n3. the unit test framework now fails to detect it\r\nSpamming XQ @zheng-xq and Toby @tfboyd to escalate.", "ping @tfboyd and @zheng-xq ", "ping @tfboyd and @zheng-xq \r\nbtw, in general I think a clear bug should not be marked \"contribution welcome\"", "@tfboyd ", "@asimshankar   Can you review this with the changes to the team?  I was not sure which github was Alek.  This was marked as contributions welcome and that should be reviewed.", "Thanks Yuxin, I've approved your fix PR.\r\n\r\nI will investigate what we can do about checking CUDA errors where they happen. The correct way to prevent CUDA errors from tunneling to other places is to check cudaGetLastError after each kernel launch. Most kernel launches don't currently do that. We are considering adding infrastructure code that all kernel launches would go through, which would allow for a central place to fix this (among other things related to launching kernels directly through the CUDA runtime). Until this is in place, it might be worthwhile to just add those checks where they are missing.", "Thanks @chsigg!", "Hi @wrongtest ! Closing this issue as it has been resolved in [2.8 version](https://colab.sandbox.google.com/gist/mohantym/4f5243a90826ba1f60e832d9b1699081/github_21338.ipynb)."]}, {"number": 21337, "title": "Problem with softmax on Windows OS", "body": "------------------------\r\n### System information\r\nWindows 10 x64 pro 17314.365, i5 6600k, Z170(clevo), GTX1070(notebook), 16G DDR4 2400 Dual Channel .\r\nTensorflow 1.9.0 with AVX2 simd, CUDA9.2.148, Cudnn7.1.4, py3.6.6 , VS2017 15.7(with cmake), CP/SM6.1\r\nTensorflow 1.9.0 offical, CUDA9.0, Cudnn7.05, py3.6.6\r\n\r\n### Describe the problem\r\nsoftmax_cross_entropy_with_logits or softmax_cross_entropy_with_logits_v2 can not be computed on GPU, but sparse_softmax_cross_entropy_with_logits can be computed on GPU. I could add with device cpu to solve the problem. But it seems simlar to https://github.com/tensorflow/models/issues/2803.\r\nThe problem may be with softmax kernel, and allow_soft_placement will also solve? \r\n\r\n\r\n### Source code / logs\r\nhttps://github.com/golbin/TensorFlow-Multi-GPUs/blob/master/many-GPUs-MNIST.py\r\n`import datetime\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nfrom tensorflow.python.client import device_lib\r\n\r\n\r\ndef check_available_gpus():\r\n    local_devices = device_lib.list_local_devices()\r\n    gpu_names = [x.name for x in local_devices if x.device_type == 'GPU']\r\n    gpu_num = len(gpu_names)\r\n\r\n    print('{0} GPUs are detected : {1}'.format(gpu_num, gpu_names))\r\n\r\n    return gpu_num\r\n\r\n\r\ndef model(X, reuse=False):\r\n    with tf.variable_scope('L1', reuse=reuse):\r\n        L1 = tf.layers.conv2d(X, 64, [3, 3], reuse=reuse)\r\n        L1 = tf.layers.max_pooling2d(L1, [2, 2], [2, 2])\r\n        L1 = tf.layers.dropout(L1, 0.7, True)\r\n\r\n    with tf.variable_scope('L2', reuse=reuse):\r\n        L2 = tf.layers.conv2d(L1, 128, [3, 3], reuse=reuse)\r\n        L2 = tf.layers.max_pooling2d(L2, [2, 2], [2, 2])\r\n        L2 = tf.layers.dropout(L2, 0.7, True)\r\n\r\n    with tf.variable_scope('L2-1', reuse=reuse):\r\n        L2_1 = tf.layers.conv2d(L2, 128, [3, 3], reuse=reuse)\r\n        L2_1 = tf.layers.max_pooling2d(L2_1, [2, 2], [2, 2])\r\n        L2_1 = tf.layers.dropout(L2_1, 0.7, True)\r\n\r\n    with tf.variable_scope('L3', reuse=reuse):\r\n        L3 = tf.contrib.layers.flatten(L2_1)\r\n        L3 = tf.layers.dense(L3, 1024, activation=tf.nn.relu)\r\n        L3 = tf.layers.dropout(L3, 0.5, True)\r\n\r\n    with tf.variable_scope('L4', reuse=reuse):\r\n        L4 = tf.layers.dense(L3, 256, activation=tf.nn.relu)\r\n\r\n    with tf.variable_scope('LF', reuse=reuse):\r\n        LF = tf.layers.dense(L4, 10, activation=None)\r\n\r\n    return LF\r\n\r\n\r\nif __name__ == '__main__':\r\n    # need to change learning rates and batch size by number of GPU\r\n    batch_size = 10000\r\n    learning_rate = 0.001\r\n    total_epoch = 10\r\n\r\n    gpu_num = check_available_gpus()\r\n\r\n    X = tf.placeholder(tf.float32, [None, 28, 28, 1])\r\n    Y = tf.placeholder(tf.float32, [None, 10])\r\n\r\n    losses = []\r\n    X_A = tf.split(X, int(gpu_num))\r\n    Y_A = tf.split(Y, int(gpu_num))\r\n\r\n    '''\r\n    Multi GPUs Usage\r\n    Results on P40\r\n     * Single GPU computation time: 0:00:22.252533\r\n     * 2 GPU computation time: 0:00:12.632623\r\n     * 4 GPU computation time: 0:00:11.083071\r\n     * 8 GPU computation time: 0:00:11.990167\r\n     \r\n    Need to change batch size and learning rates\r\n         for training more efficiently\r\n    \r\n    Reference: https://research.fb.com/wp-content/uploads/2017/06/imagenet1kin1h5.pdf\r\n    '''\r\n    for gpu_id in range(int(gpu_num)):\r\n        with tf.device(tf.DeviceSpec(device_type=\"GPU\", device_index=gpu_id)):\r\n            with tf.variable_scope(tf.get_variable_scope(), reuse=(gpu_id > 0)):\r\n                cost = tf.nn.softmax_cross_entropy_with_logits(\r\n                                logits=model(X_A[gpu_id], gpu_id > 0),\r\n                                labels=Y_A[gpu_id])\r\n                losses.append(cost)\r\n\r\n    loss = tf.reduce_mean(tf.concat(losses, axis=0))\r\n\r\n    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(\r\n        loss, colocate_gradients_with_ops=True)  # Important!\r\n\r\n    init = tf.global_variables_initializer()\r\n    sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\r\n    sess.run(init)\r\n\r\n    mnist = input_data.read_data_sets('/tmp/tensorflow/mnist/input_data', one_hot=True)\r\n    total_batch = int(mnist.train.num_examples/batch_size)\r\n    print(\"total: %s, %s, %s\" % (mnist.train.num_examples, total_batch, batch_size))\r\n\r\n    start_time = datetime.datetime.now()\r\n\r\n    for epoch in range(total_epoch):\r\n        total_cost = 0\r\n\r\n        for i in range(total_batch):\r\n            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\r\n            batch_xs = batch_xs.reshape(-1, 28, 28, 1)\r\n            _, cost_val = sess.run([optimizer, loss],\r\n                                   feed_dict={X: batch_xs,\r\n                                              Y: batch_ys})\r\n            total_cost += cost_val\r\n\r\n        print(\"total cost : %s\" % total_cost)\r\n\r\n    print(\"--- Training time : {0} seconds /w {1} GPUs ---\".format(\r\n        datetime.datetime.now() - start_time, gpu_num))`\r\n\r\nLogs:\r\n\r\n`F:\\Anaconda3\\envs\\tf\\python.exe F:/ODT/Leaf/leafmg.py\r\n1.9.0\r\n2018-08-02 17:25:58.253241: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1392] Found device 0 with properties: \r\nname: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.645\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 8.00GiB freeMemory: 6.62GiB\r\n2018-08-02 17:25:58.253526: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1471] Adding visible gpu devices: 0\r\n2018-08-02 17:26:43.721732: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-08-02 17:26:43.721924: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:958]      0 \r\n2018-08-02 17:26:43.722057: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:971] 0:   N \r\n2018-08-02 17:26:43.722281: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1084] Created TensorFlow device (/device:GPU:0 with 6394 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n1 GPUs are detected : ['/device:GPU:0']\r\nWARNING:tensorflow:From F:/ODT/Leaf/leafmg.py:81: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\n\r\nFuture major versions of TensorFlow will allow gradients to flow\r\ninto the labels input on backprop by default.\r\n\r\nSee @{tf.nn.softmax_cross_entropy_with_logits_v2}.\r\n\r\n2018-08-02 17:26:44.358873: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1471] Adding visible gpu devices: 0\r\n2018-08-02 17:26:44.359084: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-08-02 17:26:44.359266: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:958]      0 \r\n2018-08-02 17:26:44.359392: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:971] 0:   N \r\n2018-08-02 17:26:44.359563: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6394 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nTraceback (most recent call last):\r\n  File \"F:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1322, in _do_call\r\n    return fn(*args)\r\n  File \"F:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1305, in _run_fn\r\n    self._extend_graph()\r\n  File \"F:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1340, in _extend_graph\r\n    tf_session.ExtendSession(self._session)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'softmax_cross_entropy_with_logits_sg': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\nColocation Debug Info:\r\nColocation group had the following types and devices: \r\nSoftmaxCrossEntropyWithLogits: GPU CPU \r\nIdentity: GPU CPU \r\nZerosLike: CPU \r\nConst: GPU CPU \r\nExpandDims: GPU CPU \r\nNeg: GPU CPU \r\nMul: GPU CPU \r\nLogSoftmax: CPU \r\n\r\nColocation members and user-requested devices:\r\n  softmax_cross_entropy_with_logits_sg (SoftmaxCrossEntropyWithLogits) /device:GPU:0\r\n  gradients/zeros_like (ZerosLike) /device:GPU:0\r\n  gradients/softmax_cross_entropy_with_logits_sg_grad/ExpandDims/dim (Const) /device:GPU:0\r\n  gradients/softmax_cross_entropy_with_logits_sg_grad/ExpandDims (ExpandDims) /device:GPU:0\r\n  gradients/softmax_cross_entropy_with_logits_sg_grad/mul (Mul) /device:GPU:0\r\n  gradients/softmax_cross_entropy_with_logits_sg_grad/LogSoftmax (LogSoftmax) /device:GPU:0\r\n  gradients/softmax_cross_entropy_with_logits_sg_grad/Neg (Neg) /device:GPU:0\r\n  gradients/softmax_cross_entropy_with_logits_sg_grad/ExpandDims_1/dim (Const) /device:GPU:0\r\n  gradients/softmax_cross_entropy_with_logits_sg_grad/ExpandDims_1 (ExpandDims) /device:GPU:0\r\n  gradients/softmax_cross_entropy_with_logits_sg_grad/mul_1 (Mul) /device:GPU:0\r\n  gradients/softmax_cross_entropy_with_logits_sg_grad/tuple/control_dependency (Identity) /device:GPU:0\r\n  gradients/softmax_cross_entropy_with_logits_sg_grad/tuple/control_dependency_1 (Identity) /device:GPU:0\r\n\r\nRegistered kernels:\r\n  device='CPU'; T in [DT_HALF]\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='CPU'; T in [DT_DOUBLE]\r\n  device='GPU'; T in [DT_HALF]\r\n  device='GPU'; T in [DT_FLOAT]\r\n  device='GPU'; T in [DT_DOUBLE]\r\n\r\n\t [[Node: softmax_cross_entropy_with_logits_sg = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/device:GPU:0\"](softmax_cross_entropy_with_logits_sg/Reshape, softmax_cross_entropy_with_logits_sg/Reshape_1)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"F:/ODT/Leaf/leafmg.py\", line 91, in <module>\r\n    sess.run(init)\r\n  File \"F:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 900, in run\r\n    run_metadata_ptr)\r\n  File \"F:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1135, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"F:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1316, in _do_run\r\n    run_metadata)\r\n  File \"F:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1335, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'softmax_cross_entropy_with_logits_sg': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\nColocation Debug Info:\r\nColocation group had the following types and devices: \r\nSoftmaxCrossEntropyWithLogits: GPU CPU \r\nIdentity: GPU CPU \r\nZerosLike: CPU \r\nConst: GPU CPU \r\nExpandDims: GPU CPU \r\nNeg: GPU CPU \r\nMul: GPU CPU \r\nLogSoftmax: CPU \r\n\r\nColocation members and user-requested devices:\r\n  softmax_cross_entropy_with_logits_sg (SoftmaxCrossEntropyWithLogits) /device:GPU:0\r\n  gradients/zeros_like (ZerosLike) /device:GPU:0\r\n  gradients/softmax_cross_entropy_with_logits_sg_grad/ExpandDims/dim (Const) /device:GPU:0\r\n  gradients/softmax_cross_entropy_with_logits_sg_grad/ExpandDims (ExpandDims) /device:GPU:0\r\n  gradients/softmax_cross_entropy_with_logits_sg_grad/mul (Mul) /device:GPU:0\r\n  gradients/softmax_cross_entropy_with_logits_sg_grad/LogSoftmax (LogSoftmax) /device:GPU:0\r\n  gradients/softmax_cross_entropy_with_logits_sg_grad/Neg (Neg) /device:GPU:0\r\n  gradients/softmax_cross_entropy_with_logits_sg_grad/ExpandDims_1/dim (Const) /device:GPU:0\r\n  gradients/softmax_cross_entropy_with_logits_sg_grad/ExpandDims_1 (ExpandDims) /device:GPU:0\r\n  gradients/softmax_cross_entropy_with_logits_sg_grad/mul_1 (Mul) /device:GPU:0\r\n  gradients/softmax_cross_entropy_with_logits_sg_grad/tuple/control_dependency (Identity) /device:GPU:0\r\n  gradients/softmax_cross_entropy_with_logits_sg_grad/tuple/control_dependency_1 (Identity) /device:GPU:0\r\n\r\nRegistered kernels:\r\n  device='CPU'; T in [DT_HALF]\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='CPU'; T in [DT_DOUBLE]\r\n  device='GPU'; T in [DT_HALF]\r\n  device='GPU'; T in [DT_FLOAT]\r\n  device='GPU'; T in [DT_DOUBLE]\r\n\r\n\t [[Node: softmax_cross_entropy_with_logits_sg = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/device:GPU:0\"](softmax_cross_entropy_with_logits_sg/Reshape, softmax_cross_entropy_with_logits_sg/Reshape_1)]]\r\n\r\nCaused by op 'softmax_cross_entropy_with_logits_sg', defined at:\r\n  File \"F:/ODT/Leaf/leafmg.py\", line 81, in <module>\r\n    labels=Y_A[gpu_id])\r\n  File \"F:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 250, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"F:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1968, in softmax_cross_entropy_with_logits\r\n    labels=labels, logits=logits, dim=dim, name=name)\r\n  File \"F:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1879, in softmax_cross_entropy_with_logits_v2\r\n    precise_logits, labels, name=name)\r\n  File \"F:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 7738, in softmax_cross_entropy_with_logits\r\n    name=name)\r\n  File \"F:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"F:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3414, in create_op\r\n    op_def=op_def)\r\n  File \"F:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1740, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): Cannot assign a device for operation 'softmax_cross_entropy_with_logits_sg': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\nColocation Debug Info:\r\nColocation group had the following types and devices: \r\nSoftmaxCrossEntropyWithLogits: GPU CPU \r\nIdentity: GPU CPU \r\nZerosLike: CPU \r\nConst: GPU CPU \r\nExpandDims: GPU CPU \r\nNeg: GPU CPU \r\nMul: GPU CPU \r\nLogSoftmax: CPU \r\n\r\nColocation members and user-requested devices:\r\n  softmax_cross_entropy_with_logits_sg (SoftmaxCrossEntropyWithLogits) /device:GPU:0\r\n  gradients/zeros_like (ZerosLike) /device:GPU:0\r\n  gradients/softmax_cross_entropy_with_logits_sg_grad/ExpandDims/dim (Const) /device:GPU:0\r\n  gradients/softmax_cross_entropy_with_logits_sg_grad/ExpandDims (ExpandDims) /device:GPU:0\r\n  gradients/softmax_cross_entropy_with_logits_sg_grad/mul (Mul) /device:GPU:0\r\n  gradients/softmax_cross_entropy_with_logits_sg_grad/LogSoftmax (LogSoftmax) /device:GPU:0\r\n  gradients/softmax_cross_entropy_with_logits_sg_grad/Neg (Neg) /device:GPU:0\r\n  gradients/softmax_cross_entropy_with_logits_sg_grad/ExpandDims_1/dim (Const) /device:GPU:0\r\n  gradients/softmax_cross_entropy_with_logits_sg_grad/ExpandDims_1 (ExpandDims) /device:GPU:0\r\n  gradients/softmax_cross_entropy_with_logits_sg_grad/mul_1 (Mul) /device:GPU:0\r\n  gradients/softmax_cross_entropy_with_logits_sg_grad/tuple/control_dependency (Identity) /device:GPU:0\r\n  gradients/softmax_cross_entropy_with_logits_sg_grad/tuple/control_dependency_1 (Identity) /device:GPU:0\r\n\r\nRegistered kernels:\r\n  device='CPU'; T in [DT_HALF]\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='CPU'; T in [DT_DOUBLE]\r\n  device='GPU'; T in [DT_HALF]\r\n  device='GPU'; T in [DT_FLOAT]\r\n  device='GPU'; T in [DT_DOUBLE]\r\n\r\n\t [[Node: softmax_cross_entropy_with_logits_sg = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/device:GPU:0\"](softmax_cross_entropy_with_logits_sg/Reshape, softmax_cross_entropy_with_logits_sg/Reshape_1)]]\r\n\r\n\r\nProcess finished with exit code 1\r\n`\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Have I written custom code\r\nYes\r\nOS Platform and Distribution\r\nWindows 10 x64 pro 17314.365, i5 6600k, Z170(clevo), \r\nTensorflow 1.9.0 with AVX2 simd, CUDA9.2.148, Cudnn7.1.4, py3.6.6 , VS2017 15.7(with cmake), CP/SM6.1\r\nTensorflow 1.9.0 offical, CUDA9.0, Cudnn7.05, py3.6.6\r\nTensorFlow installed from\r\nOffical and from source\r\nTensorFlow version\r\n1.9.0\r\nBazel version\r\nNull(with cmake)\r\nCUDA/cuDNN version\r\nTensorflow 1.9.0 with AVX2 simd, CUDA9.2.148, Cudnn7.1.4, py3.6.6 , VS2017 15.7(with cmake), CP/SM6.1\r\nTensorflow 1.9.0 offical, CUDA9.0, Cudnn7.05, py3.6.6\r\nGPU model and memory\r\nGTX1070(notebook) 16G DDR4 2400 Dual Channel \r\n\r\nTest on Desktop PC\uff0c7900x 16G D4 2666x6 TTxpx4 also failed.\r\n", "@Windaway It looks like you are using an older Version of Tensorflow which is out of support window. Many bugs have been fixed in the latest version. Can you please execute your code using Latest Version (2.6.0) and let us know if the issue still persists? Try to use tf.losses.sparse_softmax_cross_entropy or tf.nn.sparse_softmax_cross_entropy_with_logits and Could you please have a look at the [migration doc](https://www.tensorflow.org/guide/migrate) from 1.x to 2.x , Please let us know if it helps?Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/21337\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/21337\">No</a>\n"]}, {"number": 21336, "title": "Internal error: Cannot allocate memory for the interpreter: tensorflow/contrib/lite/kernels/conv.cc:191 input->dims->size != 4 (1 != 4)Node 0 failed to prepare", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Hi! I don't see any information that can help us reproduce the problem. It looks like you are applying the 2D convolution on a 1D input though.", "Ping", "Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Same issue encountered when I tried to quantized the model given by the example [TensorFlow for Poets 2: TFLite Android](https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2-tflite/#2). If no quantization the tf-lite works, but after quantization (using tf-nightly), the model only works in python env (though the result is not accurate). However, when deploying to android, it throws the same exception. \r\n\r\nbeginning of crash\r\n09-20 03:05:24.821 7759-7759/android.example.com.tflitecamerademo E/AndroidRuntime: FATAL EXCEPTION: main\r\n    Process: android.example.com.tflitecamerademo, PID: 7759\r\n    java.lang.RuntimeException: Unable to start activity ComponentInfo{android.example.com.tflitecamerademo/com.example.android.tflitecamerademo.CameraActivity}: java.lang.NullPointerException: Internal error: Cannot allocate memory for the interpreter: tensorflow/contrib/lite/kernels/conv.cc:201 filter->type != data_type (3 != 1)Node 4 failed to prepare.\r\n    \r\n        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2665)\r\n        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2726)\r\n        at android.app.ActivityThread.-wrap12(ActivityThread.java)\r\n        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1477)\r\n        at android.os.Handler.dispatchMessage(Handler.java:102)\r\n        at android.os.Looper.loop(Looper.java:154)\r\n        at android.app.ActivityThread.main(ActivityThread.java:6119)\r\n        at java.lang.reflect.Method.invoke(Native Method)\r\n        at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:886)\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:776)\r\n     Caused by: java.lang.NullPointerException: Internal error: Cannot allocate memory for the interpreter: tensorflow/contrib/lite/kernels/conv.cc:201 filter->type != data_type (3 != 1)Node 4 failed to prepare.\r\n    \r\n        at org.tensorflow.lite.NativeInterpreterWrapper.createInterpreter(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:75)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:54)\r\n        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:114)\r\n        at com.example.android.tflitecamerademo.ImageClassifier.<init>(ImageClassifier.java:97)\r\n        at com.example.android.tflitecamerademo.Camera2BasicFragment.onActivityCreated(Camera2BasicFragment.java:299)\r\n        at android.app.Fragment.performActivityCreated(Fragment.java:2362)\r\n        at android.app.FragmentManagerImpl.moveToState(FragmentManager.java:1014)\r\n        at android.app.FragmentManagerImpl.moveToState(FragmentManager.java:1171)\r\n        at android.app.BackStackRecord.run(BackStackRecord.java:816)\r\n        at android.app.FragmentManagerImpl.execPendingActions(FragmentManager.java:1578)\r\n        at android.app.FragmentController.execPendingActions(FragmentController.java:371)\r\n        at android.app.Activity.performStart(Activity.java:6695)\r\n        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2628)\r\n        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2726)\u00a0\r\n        at android.app.ActivityThread.-wrap12(ActivityThread.java)\u00a0\r\n        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1477)\u00a0\r\n        at android.os.Handler.dispatchMessage(Handler.java:102)\u00a0\r\n        at android.os.Looper.loop(Looper.java:154)\u00a0\r\n        at android.app.ActivityThread.main(ActivityThread.java:6119)\u00a0\r\n        at java.lang.reflect.Method.invoke(Native Method)\u00a0\r\n        at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:886)\u00a0\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:776)\u00a0\r\n\r\n[assets.zip](https://github.com/tensorflow/tensorflow/files/2399485/assets.zip)\r\n\r\n", "Same error as the @minizon. There is some solution?", "@damhurmuller Maybe you can check this [issue](https://github.com/tensorflow/tensorflow/issues/22535).", "Thanks @minizon. I solved the problem using tflite 0.0.0-nightly in android. Now I have that [issue](https://github.com/tensorflow/tensorflow/issues/23759) and I didn't find any solution yet."]}, {"number": 21334, "title": "\"could not initialize a memory descriptor\" error using tensorflow on windows using CMake", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 (build 17134) \r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.9\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: 0.15\r\n- **GCC/Compiler version (if compiling from source)**: CMake > VS 2017\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: `Session::run()`\r\n\r\n### Describe the problem\r\n\r\nI've built tensorflow (r1.9, using the CMake tools) linked with MKL (v2018 U3) and mkl-dnn (v0.15). I'm running windows 10 (build 17134) on an Intel Core i7-7820HQ CPU. \r\n\r\nI've built mkl-dnn from source and it's test are passing. However, a C++ project that loads a pre-trained tensorflow graph and passes an image for inference gives the following error when calling `Session::run(...)` :\r\n```\r\nW d:\\dev\\tensorflow\\tensorflow\\core\\framework\\op_kernel.cc:1318] OP_REQUIRES failed at mkl_conv_ops.cc:888 : Aborted: Operation received an exception:Status: 3, message: could not initialize a memory descriptor, in file d:\\dev\\tensorflow\\tensorflow\\core\\kernels\\mkl_conv_ops.cc:886\r\nAborted: Operation received an exception:Status: 3, message: could not initialize a memory descriptor, in file d:\\dev\\tensorflow\\tensorflow\\core\\kernels\\mkl_conv_ops.cc:886\r\n         [[Node: conv1/BiasAdd = _MklConv2DWithBias[T=DT_FLOAT, _kernel=\"MklOp\", data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](conv1_pad/Pad, conv1/kernel, conv1/bias, DMT/_0, DMT/_1, DMT/_2)]]\r\n```\r\nThe same code (with the same pre-trained model) does work on a linux machine (Intel(R) Xeon(R) CPU E5-2673 v3, running on Microsoft Azure), also with TF built with mkl-dnn.\r\n\r\n### Source code / logs\r\n\r\nInstall MKL 2018 U3 and build MKL-DNN v0.15 from source.\r\n\r\nBuild TF r1.9 with the following flags:\r\n```\r\ntensorflow_BUILD_ALL_KERNERS=ON\r\ntensorflow_BUILD_CONTRIB_KERNELS=ON\r\ntensorflow_BUILD_SHARED_LIB=ON\r\ntensorflow_ENABLE_GPU=OFF\r\ntensorflow_ENABLE_GRPC_SUPPORT=ON\r\ntensorflow_ENABLE_JEMALLOC_SUPPORT=OFF\r\ntensorflow_ENABLE_MKLDNN_SUPPORT=ON\r\ntensorflow_ENABLE_MKL_SUPPORT=ON\r\ntensorflow_ENABLE_POSITION_INDEPENDENT_CODE=ON\r\ntensorflow_ENABLE_SNAPPY_SUPPORT=ON\r\ntensorflow_OPTIMIZE_FOR_NATIVE_ARCH=ON\r\ntensorflow_WIN_CPU_SIMD_OPTIONS=ON\r\n```\r\n\r\nBuild passes and generates `tensorflow.dll`. Use keras to save a pre-trained Resnet50 model and attempt inference from C++ (linked with `tensorflow.dll`) as follows:\r\n```\r\nSession* session;\r\nStatus status = NewSession(SessionOptions(), &session);\r\n\r\nGraphDef graph_def;\r\nstatus = ReadBinaryProto(Env::Default(), \"../graph/resnet50.pb\", &graph_def);\r\nstatus = session->Create(graph_def);\r\n\r\nMat img = imread(\"../elephant.png\"), imgFloat, imgResized;\r\nimg.convertTo(imgFloat, CV_32FC3);\r\n\r\nTensor tInput(DT_FLOAT, TensorShape({ 1, 224, 224, 3 }));\r\nauto input_tensor_mapped = tInput.tensor<float, 4>();\r\n\r\nMat imgTensor(224, 224, CV_32FC3);\r\nresize(imgFloat, imgTensor, Size(224, 224));\r\nimgTensor -= Scalar(103.939, 116.779, 123.68);\r\n\t\r\nauto source_data = imgTensor.ptr<float>(0);\r\nint width = imgTensor.cols, height = imgTensor.rows, depth=3;\r\nfor (int y = 0; y < height; ++y) {\r\n\tfor (int x = 0; x < width; ++x) {\r\n\t\tfor (int c = 0; c < depth; ++c) {\r\n\t\t\tconst float* source_value = source_data + (3*width*y) + (3*x) + (2-c);\r\n\t\t\tinput_tensor_mapped(0, y, x, c) = *source_value;\r\n\t\t}\r\n\t}\r\n}\r\n\r\nstd::vector<std::pair<string, tensorflow::Tensor>> inputs = {\r\n\t{ \"input_1\", tInput },\r\n};\r\n\r\nstd::vector<tensorflow::Tensor> outputs;\r\nstatus = session->Run(inputs, { \"fc1000/Softmax\" }, {}, &outputs);\r\nif (!status.ok()) {\r\n\tstd::cout << status.ToString() << \"\\n\";\r\n\treturn 1;\r\n}\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nExact command to reproduce", "Exact command to reproduce: `Session::run()`", "Hello,\r\n\r\nThis issue is still occuring despite (as usual) TF team closed the bug without a fix:\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/17494\r\n\r\nPeople are complaining from a wide range of tensorflow versions, up to 1.11.", "@syagev Is this still an issue ? We see that you are using old version of tensorflow  1.x which is not actively supported, We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions .Please open a new issue in case you face any errors, we will get you the right help . Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 21333, "title": "the grappler module is not accessible?", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["I wannt to use the newly open-source RL placer but I have no idea how I can use it.", "Is there documentation Yao? please respond to the original poster @zhangyaobit. Thanks!", "The code has some documentation here: https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/python/grappler/graph_placer.py\r\n\r\n@chrisbyd can you try follow the example here: \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/grappler/graph_placer_test.py", "Nagging Assignee @zhangyaobit: It has been 59 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "The link provided by @zhangyaobit should work."]}, {"number": 21332, "title": "TensorFlow 1.9.0 fails on aarch64 platforms", "body": "Building TensorFlow 1.9.0 from source fails on aarch64 platforms (NVIDIA TX1 and Linaro HiKey960) due to using an inappropriate compiler flag (`-mfpu=neon`):\r\n```\r\nERROR: /home/anton/CK_TOOLS/lib-tensorflow-src-cpu-1.9-compiler.python-2.7.12-linux-64/src/tensorflow/contrib/lite/kernels/internal/BUILD:359:1: C++ compilation of rule '//tensorflow/contrib/lite/kernels/internal:tensor_utils' failed (Exit 1)\r\ngcc: error: unrecognized command line option '-mfpu=neon'\r\n```\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A (a build problem)\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04 (JetPack 3.1)\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.9.0\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: 0.15.2\r\n- **GCC/Compiler version (if compiling from source)**: 5.5.0\r\n- **CUDA/cuDNN version**: N/A (CPU ony)\r\n- **GPU model and memory**: N/A (CPU only)\r\n- **Exact command to reproduce**: Install dependencies and build via [CK-TensorFlow](github.com/ctuning/ck-tensorflow):\r\n```\r\n$ sudo apt install liblapack-dev libatlas-dev\r\n$ sudo pip install enum34 mock pillow wheel absl-py scipy ck\r\n$ ck pull repo:ck-tensorflow\r\n$ ck install ck-env:package:tool-bazel-0.15.2-linux\r\n$ ck install package:lib-tensorflow-1.9.0-src-cpu --env.CK_HOST_CPU_NUMBER_OF_PROCESSORS=1\r\n```\r\n**NB:** Restricting the number of building processes to 1 is necessary to prevent running out of memory on the NVIDIA TX1 platform (4 GB and no swap enabled) or similar.\r\n\r\n### Describe the problem\r\n\r\nBuilding TensorFlow 1.9.0 fails on aarch64 platforms (NVIDIA TX1 and Linaro HiKey960). Similar instructions for TensorFlow 1.7.0 with Bazel 0.11.1 worked well:\r\n```\r\n$ ck install package:lib-tensorflow-1.7.0-src-cpu --env.CK_HOST_CPU_NUMBER_OF_PROCESSORS=1\r\n$ ck install package:lib-tensorflow-1.7.0-src-cpu-xla --env.CK_HOST_CPU_NUMBER_OF_PROCESSORS=1\r\n\r\n$ ck show env --tags=tensorflow,v1.7\r\nEnv UID:         Target OS: Bits: Name:                                       Version: Tags:\r\n\r\ned191cc45dda7ee4   linux-64    64 TensorFlow library (from sources, cpu, xla) 1.7      64bits,bazel,channel-stable,host-os-linux-64,lib,needs-bazel,needs-bazel-0.11.1,target-os-linux-64,tensorflow,tensorflow-cpu,v1,v1.7,v1.7.0,vcpu,vsrc,vxla\r\ned191cc45dda7ee3   linux-64    64 TensorFlow library (from sources, cpu)      1.7      64bits,bazel,channel-stable,host-os-linux-64,lib,needs-bazel,needs-bazel-0.11.1,target-os-linux-64,tensorflow,tensorflow-cpu,v1,v1.7,v1.7.0,vcpu,vsrc\r\n```\r\nThey also worked for TensorFlow 1.8.0 with Bazel 0.13.0 after [including a patch](\r\nhttps://github.com/tensorflow/tensorflow/issues/18643#issuecomment-386245419\r\n) into the CK package:\r\n\r\n```\r\n$ sudo apt install liblapack-dev libatlas-dev\r\n$ sudo pip install enum34 mock pillow wheel absl-py scipy ck\r\n$ ck pull repo:ck-tensorflow\r\n$ ck install ck-env:package:tool-bazel-0.13.0-linux\r\n$ ck install package:lib-tensorflow-1.8.0-src-cpu [--env.CK_HOST_CPU_NUMBER_OF_PROCESSORS=1]\r\n```\r\n\r\n### Source code / logs\r\nAfter about 10 hours, the build failed:\r\n```\r\nERROR: /home/anton/CK_TOOLS/lib-tensorflow-src-cpu-1.9-compiler.python-2.7.12-linux-64/src/tensorflow/contrib/lite/kernels/internal/BUILD:359:1: C++ compilation of rule '//tensorflow/contrib/lite/kernels/internal:tensor_utils' failed (Exit 1)\r\ngcc: error: unrecognized command line option '-mfpu=neon'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\n```", "comments": ["I have a PR (https://github.com/tensorflow/tensorflow/pull/16175) for this.", "Thanks @freedomtan!", "@freedomtan's patch has helped me to make progress but only to find out about another failure to build TF 1.9 on aarch64 platforms:\r\n```\r\nERROR: /home/anton/CK_TOOLS/lib-tensorflow-src-cpu-1.9-compiler.python-2.7.12-linux-64/src/tensorflow/contrib/lite/kernels/BUILD:128:1: C++ compilation of rule '//tensorflow/contrib/lite/kernels:builtin_ops' failed (Exit 1)\r\nIn file included from ./tensorflow/contrib/lite/kernels/internal/optimized/depthwiseconv_uint8.h:21:0,\r\n                 from tensorflow/contrib/lite/kernels/depthwise_conv.cc:26:\r\n./tensorflow/contrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:80:43: error: expected primary-expression before ',' token\r\n static_assert(offsetof(DepthwiseConvParams, input_depth) ==\r\n                                           ^\r\n./tensorflow/contrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:80:45: error: 'input_depth' was not declared in this scope\r\n static_assert(offsetof(DepthwiseConvParams, input_depth) ==\r\n                                             ^\r\n./tensorflow/contrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:80:56: error: 'offsetof' was not declared in this scope\r\n static_assert(offsetof(DepthwiseConvParams, input_depth) ==\r\n                                                        ^\r\n...\r\n```\r\nNow, peeking into `tensorflow/contrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h` revealed the following case of conditional compilation excluding practically all of the file (full of inline assembly):\r\n```\r\n// Enable for arm64 except for the Nvidia Linux 4 Tegra (L4T) running on\r\n// Jetson TX-2. This compiler does not support the offsetof() macro.\r\n#if defined(__aarch64__) && !defined(GOOGLE_L4T)\r\n```\r\nNow, that's at the same time both too specific (why only TX2, not TX1, HiKey960 or any other target?) and not specific (which compiler? gcc-5?)", "There are 20 occurrences of `offsetof` under `src/tensorflow`:\r\n```\r\n$ grep offsetof * -R \r\ncontrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:// Jetson TX-2. This compiler does not support the offsetof() macro.\r\ncontrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:static_assert(offsetof(DepthwiseConvParams, input_depth) ==\r\ncontrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:static_assert(offsetof(DepthwiseConvParams, input_row_size) ==\r\ncontrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:static_assert(offsetof(DepthwiseConvParams, output_depth) ==\r\ncontrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:static_assert(offsetof(DepthwiseConvParams, output_row_size) ==\r\ncontrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:static_assert(offsetof(DepthwiseConvParams, input_offset) ==\r\ncontrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:static_assert(offsetof(DepthwiseConvParams, output_offset) ==\r\ncontrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:static_assert(offsetof(DepthwiseConvParams, filter_offset) ==\r\ncontrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:static_assert(offsetof(DepthwiseConvParams, output_multiplier) ==\r\ncontrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:static_assert(offsetof(DepthwiseConvParams, output_activation_min) ==\r\ncontrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:static_assert(offsetof(DepthwiseConvParams, output_activation_max) ==\r\ncontrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:static_assert(offsetof(DepthwiseConvParams, output_shift) ==\r\ncontrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:static_assert(offsetof(DepthwiseConvParams, input_width) ==\r\ncontrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:static_assert(offsetof(DepthwiseConvParams, input_height) ==\r\ncontrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:static_assert(offsetof(DepthwiseConvParams, output_width) ==\r\ncontrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:static_assert(offsetof(DepthwiseConvParams, output_height) ==\r\ncontrib/lite/kernels/internal/optimized/depthwiseconv_uint8.h:// Jetson TX-2. This compiler does not support the offsetof() macro.\r\npython/eager/pywrap_tensor.cc:    offsetof(EagerTensor, weakreflist), /* tp_weaklistoffset */\r\nstream_executor/cuda/cuda_helpers.h:static_assert(offsetof(cuComplex, x) == 0,\r\nstream_executor/cuda/cuda_helpers.h:static_assert(offsetof(cuDoubleComplex, x) == 0,\r\n```\r\nI suspect this feature has crept into TF via TFLite, and requires compiler support (e.g. [gcc-7.1+](https://github.com/gcc-mirror/gcc/blob/gcc-7_1_0-release/gcc/ginclude/stddef.h#L416-L417)).\r\n\r\nThe same `grep` for TF 1.8 and 1.7 returns only two occurrences:\r\n```\r\nanton@tegra-ubuntu:~/CK_TOOLS/lib-tensorflow-src-cpu-1.8-linux-64/src/tensorflow$ grep offsetof * -R \r\nstream_executor/cuda/cuda_helpers.h:static_assert(offsetof(cuComplex, x) == 0,\r\nstream_executor/cuda/cuda_helpers.h:static_assert(offsetof(cuDoubleComplex, x) == 0,\r\nanton@tegra-ubuntu:~/CK_TOOLS/lib-tensorflow-src-cpu-1.7-linux-64/src/tensorflow$ grep offsetof * -R \r\nstream_executor/cuda/cuda_helpers.h:static_assert(offsetof(cuComplex, x) == 0,\r\nstream_executor/cuda/cuda_helpers.h:static_assert(offsetof(cuDoubleComplex, x) == 0,\r\n```", "However, I have the same failure on HiKey960 with gcc-7.2. (Unless, gcc-6 was used instead.)", "The following patch (commenting out all lines mentioning `offsetof` in `depthwiseconv_uint8_3x3_filter.h`):\r\n```\r\nanton@hikey962:~/CK_REPOS/ck-tensorflow$ cat package/lib-tensorflow-1.9.0-src-cpu/patch.linux/offsetof-off.patch\r\ndiff --git a/tensorflow/contrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h b/tensorflow/contrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h\r\nindex 8cd72239e9..a7c4cf8f76 100644\r\n--- a/tensorflow/contrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h\r\n+++ b/tensorflow/contrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h\r\n@@ -76,7 +76,7 @@ struct DepthwiseConvParams {\r\n #define OFFSET_INPUT_HEIGHT 64\r\n #define OFFSET_OUTPUT_WIDTH 68\r\n #define OFFSET_OUTPUT_HEIGHT 72\r\n-\r\n+#if 0\r\n static_assert(offsetof(DepthwiseConvParams, input_depth) ==\r\n                   OFFSET_INPUT_DEPTH, \"\");\r\n static_assert(offsetof(DepthwiseConvParams, input_row_size) ==\r\n@@ -107,7 +107,7 @@ static_assert(offsetof(DepthwiseConvParams, output_width) ==\r\n                   OFFSET_OUTPUT_WIDTH, \"\");\r\n static_assert(offsetof(DepthwiseConvParams, output_height) ==\r\n                   OFFSET_OUTPUT_HEIGHT, \"\");\r\n-\r\n+#endif\r\n template <int32 kDepth, int32 kStrideWidth, int32 kStrideHeight>\r\n struct DepthwiseConvWindow {};\r\n```\r\nallowed me to complete the build on HiKey960:\r\n```\r\nanton@hikey962:~/CK_REPOS/ck-tensorflow/package/lib-tensorflow-1.9.0-src-cpu$ ck install \\\r\n--env.CK_HOST_CPU_NUMBER_OF_PROCESSORS=2 --rebuild\r\n...\r\nEnvironment entry added (89bab7b66c62ba7e)!\r\n\r\nRecording CK configuration to /home/anton/CK_TOOLS/lib-tensorflow-src-cpu-1.9-compiler.python-2.7.13-linux-64/ck-install.json ...\r\n\r\nInstallation path: /home/anton/CK_TOOLS/lib-tensorflow-src-cpu-1.9-compiler.python-2.7.13-linux-64\r\n```\r\n\r\nOf course, it cannot be considered a proper fix, but I've committed it as a workaround to `ck-tensorflow:package:lib-tensorflow-1.9.0-src-cpu`. One can test it as follows:\r\n```\r\n$ ck install package:lib-tensorflow-1.9.0-src-cpu --env.CK_HOST_CPU_NUMBER_OF_PROCESSORS=1\r\n```\r\n(See full CK instructions above.)", "Just a note that the above worked with Python 2.7 only. With Python 3.5 it failed:\r\n```\r\nINFO: From Compiling tensorflow/python/framework/fast_tensor_util.cpp:\r\nIn file included from bazel-out/arm-opt/genfiles/external/local_config_python/numpy_include/numpy/ndarraytypes.h:1821:0,\r\n                 from bazel-out/arm-opt/genfiles/external/local_config_python/numpy_include/numpy/ndarrayobject.h:18,\r\n                 from bazel-out/arm-opt/genfiles/external/local_config_python/numpy_include/numpy/arrayobject.h:4,\r\n                 from bazel-out/arm-opt/genfiles/tensorflow/python/framework/fast_tensor_util.cpp:531:\r\nbazel-out/arm-opt/genfiles/external/local_config_python/numpy_include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning \"Using deprecated NumPy API, disable it by \" \"#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-Wcpp]\r\n #warning \"Using deprecated NumPy API, disable it by \" \\\r\n  ^\r\nERROR: /home/anton/CK_TOOLS/lib-tensorflow-src-cpu-1.9-compiler.python-3.5.2-linux-64/src/tensorflow/BUILD:541:1: Executing genrule //tensorflow:python_api_gen failed (Exit 1)\r\nTraceback (most recent call last):\r\n  File \"/home/anton/.cache/bazel/_bazel_anton/6d1d19b4e763685a94d885534baf7158/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/tools/api/generator/create_python_api.py\", line 27, in <module>\r\n    from tensorflow.python.util import tf_decorator\r\n  File \"/home/anton/.cache/bazel/_bazel_anton/6d1d19b4e763685a94d885534baf7158/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 52, in <module>\r\n    from tensorflow.core.framework.graph_pb2 import *\r\n  File \"/home/anton/.cache/bazel/_bazel_anton/6d1d19b4e763685a94d885534baf7158/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/core/framework/graph_pb2.py\", line 6, in <module>\r\n    from google.protobuf import descriptor as _descriptor\r\n  File \"/usr/local/lib/python3.5/dist-packages/google/protobuf/__init__.py\", line 37, in <module>\r\n    __import__('pkg_resources').declare_namespace(__name__)\r\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 2927, in <module>\r\n    @_call_aside\r\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 2913, in _call_aside\r\n    f(*args, **kwargs)\r\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 2952, in _initialize_master_working_set\r\n    add_activation_listener(lambda dist: dist.activate())\r\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 956, in subscribe\r\n    callback(dist)\r\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 2952, in <lambda>\r\n    add_activation_listener(lambda dist: dist.activate())\r\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 2515, in activate\r\n    declare_namespace(pkg)\r\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 2097, in declare_namespace\r\n    _handle_ns(packageName, path_item)\r\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 2047, in _handle_ns\r\n    _rebuild_mod_path(path, packageName, module)\r\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 2066, in _rebuild_mod_path\r\n    orig_path.sort(key=position_in_sys_path)\r\nAttributeError: '_NamespacePath' object has no attribute 'sort'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 36844.569s, Critical Path: 170.55s\r\nINFO: 4722 processes: 4722 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\nWhat's weird is that I used NumPy 1.11 with Python 2, and NumPy 1.15 with Python 3.", "@psyhtest FYR, I was able to build pip package for Python 3.6 last week, so mostly it's problem of your environment?", "@freedomtan I can hardly call it \"mine\" :). It's a standard TX1 environment, and TX1 is a very popular board. \r\n\r\nIt's very disheartening to see that the TensorFlow community enables support for aarch64 platforms only one platform at a time (notably, only Raspberry Pi is properly supported). ", "@psyhtest I guess no Google TensorFlow guys are really working on AArch64 linux platforms. That's why nobody reviewed the PR I sent in in Jan. Yes, RPI 3s are on ARMv8 SoC, but Raspbian for RPI 3 is actually running AArch32 kernel and user space. I started building TensorFlow on ARM64 Debian env because I wanted to have better debugging and profiling env than Android AArch64 devices.\r\n\r\nThe Debian rootfs I am using is from deboostrap arm64 [1], so I think I am using the \"authentic\" standard Debian env. I know TX1 is quite popular, but I don't have access it. If there is no TensorFlow guys working on it. I think you should ask NVIDIA to support it or \"use the source\" and \"scratch your own itch\".\r\n\r\n[1] https://wiki.debian.org/Arm64Port#Debootstrap_arm64\r\n[2] http://catb.org/jargon/html/U/UTSL.html", "Thanks @freedomtan. That perfectly matches my understanding.\r\n\r\n> I think you should ask NVIDIA to support it or \"use the source\" and \"scratch your own itch\".\r\n\r\nAs far as I know, NVIDIA do not recommend using TensorFlow on the TXs because it's slow and consumes too much memory. Google suggest to use TFLite now, but it seems to share the same build problems. Given the situation, I'd switch to an alternative framework e.g. MXNet or Caffe2, and recommend our community to do the same. If my response makes Google guys angry (and thus reluctant to fix this problem), it will just prove the point :)).", "Did you manage to get a proper solution? I am experiencing the same problem on 1.10 (DrivePX2)", "Is there any update @case540 ?", "Hi, nvidia has wheels for different tensorflow versions (up to 1.11) and JetPack and Python versions here (should work for TX1 and TX2 but havn't tried): https://devtalk.nvidia.com/default/topic/1031300/jetson-tx2/tensorflow-1-11-0-wheel-with-jetpack-3-3/1\r\nAnother approach is building tensorflow with makefile (no python and GPU support), that worked for me on the TX1", "Is this still failing with the latest TensorFlow source/build?", "Sorry I never got around to fixing this.\r\n\r\n@jdduke : this should still be failing on the specific platforms that Anton and others mentioned. \r\n- Looks like after freedomtan@'s PR, the build will work on his/her platform\r\n- From Anton's comments, we need to find the right compile-time flags to ignore offsetof for TX1 and other listed platforms he mentioned, and then find a followup fix for Python 3.5.\r\n\r\n\r\n", "@alanchiao - are there any resource blockers for this work? I have server-class machines at @packethost from @worksonarm that can be had (by anyone involved here) for additional testing and CI.", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 21331, "title": "Inconsistent behavior for decode_bmp() with channels specification", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 3.10.0-693.2.2.el7.x86_64\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: Python 2.7.14 :: Anaconda\r\n- **Bazel version (if compiling from source)**: None\r\n- **GCC/Compiler version (if compiling from source)**: None\r\n- **CUDA/cuDNN version**: None\r\n- **GPU model and memory**: None\r\n- **Exact command to reproduce**: See below\r\n\r\n### Describe the problem\r\nWe rely on tensorflow.image.decode_xxx() apis to load images. We expect a fixed channel size for decoded images and below code works:\r\n\r\n```python\r\nimport tensorflow as tf\r\ndata = tf.placeholder(dtype=tf.string, shape=())\r\nimage = tf.image.decode_jpeg(data, channels=3)\r\n```\r\n\r\nHowever, it does not work for tensorflow.image.decode_bmp() for every images. For example, a bmp image with channel model \"L\":\r\n[my_image.bmp.zip](https://github.com/tensorflow/tensorflow/files/2252717/my_image.bmp.zip)\r\n\r\n\r\n```python\r\nimport tensorflow as tf\r\ndata = tf.placeholder(dtype=tf.string, shape=())\r\n\r\nimage_no_spec = tf.image.decode_bmp(data)\r\nimage = tf.image.decode_bmp(data, channels=3)\r\n\r\nmy_image = open(\"my_image.bmp\", \"rb\").read()\r\nwith tf.Session() as sess:\r\n    res = sess.run(image_no_spec, {data: my_image})\r\n    print \"Decode shape: \", res.shape   # (666, 1000, 1)\r\n\r\n    res = sess.run(image, {data: my_image})\r\n    print \"Decode shape: \", res.shape  # err\r\n```\r\n\r\nSince tensorflow.image.decode_image() is a wrapper for decode_xxx() api(s), it may benefits if we can specify channels for all formats safely.\r\n\r\n### Source code / logs\r\n\r\nDecode shape:  (666, 1000, 1)\r\nTraceback (most recent call last):\r\n  File \"a.py\", line 12, in <module>\r\n    res = sess.run(image, {data: my_image})\r\n  File \"/home/work/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 905, in run\r\n    run_metadata_ptr)\r\n  File \"/home/work/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1137, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/work/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1355, in _do_run\r\n    options, run_metadata)\r\n  File \"/home/work/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1374, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: channels attribute 3 does not match bits per pixel from file 1\r\n\t [[Node: DecodeBmp_1 = DecodeBmp[channels=3, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_Placeholder_0_0)]]\r\n\r\nCaused by op u'DecodeBmp_1', defined at:\r\n  File \"a.py\", line 5, in <module>\r\n    image = tf.image.decode_bmp(data, channels=3)\r\n  File \"/home/work/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_image_ops.py\", line 513, in decode_bmp\r\n    \"DecodeBmp\", contents=contents, channels=channels, name=name)\r\n  File \"/home/work/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/work/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3271, in create_op\r\n    op_def=op_def)\r\n  File \"/home/work/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1650, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): channels attribute 3 does not match bits per pixel from file 1\r\n\t [[Node: DecodeBmp_1 = DecodeBmp[channels=3, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_Placeholder_0_0)]]\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nMobile device", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "I think this is working as intended.  You are feeding in a bmp image that has only 1 channel and telling the operation that it should expect 3 channels. \r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/image/decode_image the documentation says that channels=None or 0 means 'automatically figure it out'.  Otherwise channels is used as a specification to enforce.\r\n\r\nDecode with channels=0 and then use something like grayscale_to_rgb if you need to guarantee that all images are 3 channels as output.", "@vrv  Thanks for your nice notes :)\r\n\r\nI understand that you mean the semantic of specified channels is \"the operation expect (for eg) 3 channels input\", rather than \"the operation will produce 3 channels output\".\r\n\r\nI think it is ok but my problem is I do not find any TensorFlow api to convert decoded images with different channels to (for eg) RGB format. I have to depend on external lib like opencv or pillow to deal with all kind of images, or I have to guarantee that all input images are of the same format.", "Try: https://www.tensorflow.org/api_docs/python/tf/image/grayscale_to_rgb\r\n\r\nIf your BMP is 1 channel, then you need an API like that to convert it.  \r\n\r\nSomething I've done in the past is to conditionally convert the image to 3 channels by using that API if the output is 1 channel, as a post-processing step.\r\n\r\n    image = tf.cond(tf.shape(image)[2] == 1, lambda: tf.image.grayscale_to_rgb(image), lambda: image)", "Thanks @vrv . it may work if we \"reshape\" the tensor after `decode_bmp`. But the interface/usage/meaning of `decode_jpeg` and `decode_jpeg` are a little different which may lead to confusion for the developers."]}, {"number": 21330, "title": "[keras]The model size is 250MB, but it takes up 800MB of memory after loading", "body": "The keras model(h5) size is 250MB, but it takes up 800MB of memory after loading\r\n\r\ndoes anyone can explain the reason?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "It has been 30 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "bytes problem. I got it."]}]