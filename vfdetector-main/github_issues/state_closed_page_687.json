[{"number": 32982, "title": "TF2 Stubs for intellisense; current way tf2 imports modules does not support intellisense", "body": "**System information**\r\n- TensorFlow version (you are using): 2.0.0\r\n- Are you willing to contribute it (Yes/No): Happy to help out\r\n\r\n**Describe the feature and the current behavior/state.**\r\nIntellisense does not work because of the way tf2 has import statements.\r\n\r\n![image](https://user-images.githubusercontent.com/9828683/66023599-f7f83180-e4f1-11e9-83d0-fe931c4486f6.png)\r\n\r\n\r\n**Will this change the current api? How?**\r\n\r\nAdd stubs.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nVscode users\r\n\r\n**Any Other info.**\r\n\r\nDiscussed here: \r\nhttps://github.com/microsoft/python-language-server/issues/818#issuecomment-537143000", "comments": ["@RensDimmendaal,\r\nCould you please elaborate the issue. Thanks!", "Sure :-) Please note that I'm not an expert on this issue. I'm a tensorflow user who is inconvenienced by the issue and I thought I'd pass it along from the vscode thread to tensorflow when I saw that it was not something vscode could deal with by itself.\r\n\r\nThe end goal is intellisense support for tensorflow in the popular ide VSCode.\r\nI believe the reason TF2 does not work with intellisense is that it's imports are \"indirect\" such as in the tensorflow.data.__init__. \r\n\r\n```\r\n\"\"`tf.data.Dataset` API for input pipelines.\r\nSee [Importing Data](https://tensorflow.org/guide/data) for an overview.\r\n\"\"\"\r\n\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\n# pylint: disable=unused-import\r\nfrom tensorflow.python.data import experimental\r\nfrom tensorflow.python.data.ops.dataset_ops import Dataset\r\nfrom tensorflow.python.data.ops.dataset_ops import make_initializable_iterator\r\nfrom tensorflow.python.data.ops.dataset_ops import make_one_shot_iterator\r\nfrom tensorflow.python.data.ops.iterator_ops import Iterator\r\nfrom tensorflow.python.data.ops.readers import FixedLengthRecordDataset\r\nfrom tensorflow.python.data.ops.readers import TextLineDataset\r\nfrom tensorflow.python.data.ops.readers import TFRecordDataset\r\n# pylint: enable=unused-import\r\n```\r\n\r\nAs discsussed in the vs code thread (https://github.com/microsoft/python-language-server/issues/818#issuecomment-537143000) one way around this would be by using stubs.\r\n\r\nI guess this would be similar to pyspark stubs: https://pypi.org/project/pyspark-stubs/ and can be created with mypy's stubgen https://github.com/python/mypy/blob/master/mypy/stubgen.py.", "Also see https://github.com/microsoft/python-language-server/issues/818#issuecomment-511495313, in which @alextp specifically asks @pkch about adding stubs for TF.\r\n\r\nNote that this issue affects does not only affect VS Code, but also PyCharm and Atom. I'm guessing it'll also affect most Vim and Emacs plugins providing Python intellisense functionality using static type analysis.", "It is extremely frustrating discovering the new Tensorflow 2.0 APIs without proper intellisense support in my IDEs. Please fix this!", "Tensorflow 1.15 has the same problem.", "I've tried a simple solution for current situation (tf: 2.0, vscode:1.39.1) : \r\n\r\n1. Go to dir `/python3/site-packages/` and change the name of `/tensorflow/` to `/tensorflow_back`, then change the name of  `/tensorflow_core/` to `/tensorflow/` (only use core api)\r\n\r\n2. Go to file `/tensorflow/__init__.py` (which was in `/tensorflow_core/` ), add the following codes:\r\n`from .python.keras.api._v2 import keras`\r\n`from tensorflow_estimator.python.estimator.api._v2 import estimator`\r\n\r\nMy workspace looks like this:\r\n![results](https://user-images.githubusercontent.com/36128896/66695234-a457c600-ecf1-11e9-8d00-128625851c0b.jpg)\r\n\r\nSo it works!\r\n\r\n---\r\nupdate: \r\n- As @coolzhao and @thomasjo mentioned, the core issue is `lazy import` used in tensorflow, which confuses IntelliSense (and many other autocomplete tools). So the temporary hacks above likewise works on all editors. \r\n- Thanks ro @aKzenT, there is another option for vscode which doesn't need to modify the original tensorflow package. \r\n\r\n", "@Mulns Thanks for this solution! I found a way to use your solution without having to modify the `site-packages` folder.\r\n\r\nJust copy the \"tensorflow_core\" folder to another directory (e.g. `C:\\stubs`) and then rename it there to \"tensorflow\" (`C:\\stubs\\tensorflow`). Adjust the `__init__.py` file as described above.\r\n\r\nIn your Visual Studio Code open your user `settings.json` and add the following setting:\r\n`    \"python.autoComplete.extraPaths\": [\"C:\\\\stubs\"],\r\n`\r\n\r\nVS Code will use the specified path for auto-completion while your application will run with the original unmodified library.", "@aKzenT Thanks for the solution, it's better to let the original TensorFlow remain untouched. But actually it's not necessary to add those two lines in the `__init__.py` if you add an autoComplete.extraPath in setting.json. I have tried that with the `__init__.py` unchanged, and it works too.\r\n\r\nAnd I think what confuses IntelliSense is that when we `import tensorflow`(the proxy), it leads to tensorflow_core(the real one) in `./tensorflow/__init__.py`, but when you look into the `./tensorflow_core/__init__.py`, it goes back to the proxy name tensorflow. Which I think may confuse the IntelliSense.\r\n\r\nSo after we add an autoComplete.extraPath with a real tensorflow(the core but named as tensorflow), the IntelliSense can finally figure out the path to search.\r\n\r\n_I currently find the solution of extraPath only works on the Windows 10, not working on macOS, don't know why._", "@RensDimmendaal, Looks like the issue is resolved. Can you please let us know if you are happy to close if no issue persists. Thanks!", "@gadagashwini This issue is in no way resolved. \r\n\r\nThe proposed workarounds listed above are only temporary hacks, nothing else. The underlying problem either needs to be resolved by changing/removing the dynamic module loading bits introduced in TF 2.0, or someone (arguably the TF team) needs to provide us with type stubs ([PEP 484](https://www.python.org/dev/peps/pep-0484/)).", "@coolzhao @aKzenT Thanks for your solution. \r\n\r\n@gadagashwini \r\nThe issue should still remain open as this is a problem that everyone is encountering. Tensorflow intellisense is not working as it should.", "@RensDimmendaal maybe the title should be changed, because as @thomasjo it affects other editors, including PyCharm. The above hack by @Mulns will likewise works for all editors.", "@aKzenT I found an even better solution: A symlink which keeps everything up to date:\r\n\r\nStep by step solution:\r\n- Find your tensorflow_core installation (the python package). This is e.g. in `~/.local/lib/python3.6/site-packages`, any other `site-packages` folder you might use (e.g. from virtualenv, pyenv,...)\r\n- Create a folder to use for IDE navigation, e.g. `~/.local/virtual-site-packages`\r\n- Create a symlink in that folder called `tensorflow` to your `tensorflow_core` package (mind the name difference, this is intentional!)\r\n- Add the path created in the 2nd step to `python.autoComplete.extraPaths` in VSC (use the full path, i.e. replace your username)\r\n\r\nExample for me using pyenv with Python 3.6.9:\r\n\r\n```\r\nmkdir ~/.local/virtual-site-packages\r\nln -s ~/.pyenv/versions/3.6.9/lib/python3.6/site-packages/tensorflow_core ~/.local/virtual-site-packages/tensorflow\r\n```\r\nThen use \r\n```\r\n \"python.autoComplete.extraPaths\": [\r\n        \"/home/<USER>/.local/virtual-site-packages\"\r\n    ],\r\n```", "> @aKzenT I found an even better solution: A symlink which keeps everything up to date:\r\n> \r\n> ```\r\n> mkdir ~/.local/virtual-site-packages\r\n> ln -s ~/.pyenv/versions/3.6.9/lib/python3.6/site-packages/tensorflow_core ~/.local/virtual-site-packages/tensorflow\r\n> ```\r\n> \r\n> Then use\r\n> \r\n> ```\r\n>  \"python.autoComplete.extraPaths\": [\r\n>         \"/home/<USER>/.local/virtual-site-packages\"\r\n>     ],\r\n> ```\r\n\r\nThis workaround also works for tensorflow 1.15, thank all you guys", "> @aKzenT I found an even better solution: A symlink which keeps everything up to date:\r\n> \r\n> ```\r\n> mkdir ~/.local/virtual-site-packages\r\n> ln -s ~/.pyenv/versions/3.6.9/lib/python3.6/site-packages/tensorflow_core ~/.local/virtual-site-packages/tensorflow\r\n> ```\r\n> \r\n> Then use\r\n> \r\n> ```\r\n>  \"python.autoComplete.extraPaths\": [\r\n>         \"/home/<USER>/.local/virtual-site-packages\"\r\n>     ],\r\n> ```\r\n\r\nSomehow this is not working for me on Ubuntu 18.04", "Did you adjust your path to tensorflow in the `ln` command? This is not a C&P solution but needs to be adapted", "> Did you adjust your path to tensorflow in the `ln` command? This is not a C&P solution but needs to be adapted\r\n\r\nI only followed your steps. What else need to be adjust? \r\n\r\n**Update**: Oh it doesn't work because my import were like this: `from tensorflow.keras import layers`. The example from above works just fine.", "I just changed the imports to their source location at tensorflow_core. Looks stable to use intra-release and can be switch after either Tensorflow or Code improve the situation?\r\n\r\n`from tensorflow_core.python.keras.api import keras`", "> from tensorflow_core.python.keras.api import keras\r\n\r\nThis won't work. At some point you get double-imports and crashes because TF modules (and others) `import tensorflow`, which aliases `tensorflow_core` without Python realizing -> double load -> boom", "PyCharm 2019.3 EAP version worked! \r\nhttps://www.jetbrains.com/pycharm/nextversion/?_ga=2.221720863.623821984.1573808626-1607002874.1573808626", "> > from tensorflow_core.python.keras.api import keras\r\n> \r\n> This won't work. At some point you get double-imports and crashes because TF modules (and others) `import tensorflow`, which aliases `tensorflow_core` without Python realizing -> double load -> boom\r\n\r\nCurrently I am just using tensorflow.keras and tensorflow root modules which work fine, but I will keep this in mind. Good to know, but I would like to avoid the fallback since it makes sharing the conda environment impossible without tweaking post-install.", "> \r\n> \r\n> > Did you adjust your path to tensorflow in the `ln` command? This is not a C&P solution but needs to be adapted\r\n> \r\n> I only followed your steps. What else need to be adjust?\r\n> \r\n> **Update**: Oh it doesn't work because my import were like this: `from tensorflow.keras import layers`. The example from above works just fine.\r\n\r\n@minhduc0711 Could you please elaborate on what you use ? \r\n\r\nIs it \r\n```python\r\nfrom tensorflow import keras\r\ninput = keras.layers.Input(...)\r\n```\r\nwithout the possibility to do `from tensorflow.keras import layers` to just use \r\n```python\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\ninput = layers.Input(...)\r\n```\r\n? (Beginner here, just tipping my toe in grasping Python import mechanisms)", "Any updates?", "Many issues have already been fixed before (for e.g. new PyCharms version was working well, tensorflow/tensorflow#34629 from @lgeiger fixed some important cases).\r\nAfter this commit, everything should be fixed (it removed tensorflow_core/ directory, now we just have tensorflow/ directory in the pip package so structure is simpler):\r\nhttps://github.com/tensorflow/tensorflow/commit/5c00e793c61860bbf26778cd4704313e867645be#diff-62ec18ff8bdd93adaff55160f27a7e09\r\n\r\nRe-open this or a new bug if something still doesn't work in the new nightlies.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32982\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32982\">No</a>\n", "@annarev I've tried Tensorflow 2.2.0rc0 which includes 5c00e793c61860bbf26778cd4704313e867645be but Intellisense in VS Code still doesn't work, while 1.13.0 works perfectly fine.", "Can you give an example of an import you are using that doesn't work?\r\n(just to clarify, I found some cases after closing this bug that still don't work. I think these are less frequently used ones. But I want to make sure if you are asking about a common case that should work)\r\n\r\nImports that should work (I just checked them in VS Code):\r\n`import tensorflow as tf`\r\n`from tensorflow import keras`\r\n`from tensorflow.keras import losses`", "This import cannot be resolved.\r\n`from tensorflow.keras.utils import multi_gpu_model, plot_model, to_categorical`\r\n\r\nBy the way,\r\n`import tensorflow as tf` works\r\n`from tensorflow import keras` works\r\n`from tensorflow.keras import losses` doesn't work for me. It can't find information of `losses` object.\r\n", "The top one `from tensorflow.keras.utils import multi_gpu_model, plot_model, to_categorical` is a deeper import that indeed doesn't work now (it also doesn't work for me in 1.13.0).\r\n\r\nWhich VS Code version are you using? I tested `from tensorflow.keras import losses` in two versions of VS Code (1.14.1 and 1.43.0). It works in both for 2.2.0rc0 but not 1.13.0.", "@annarev I'm using VS Code 1.43.1 on macOS. It's somehow working now in a very weird way.\r\n\r\nIf I have this single import in a file\r\n```python\r\nimport tensorflow as tf\r\n\r\n# Try to type tf.keras.layers.Conv1D here\r\n```\r\nI can use `tf.keras.` auto-suggestion normally.\r\n\r\nBut if I add a line of keras import from tensorflow\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\n# Try to type tf.keras.layers.Conv1D here\r\n```\r\nI can't use `tf.keras.` auto-suggestion anymore. I've tested on multiple conda environments which installed Tensorflow 2.2.0rc0 and had the same result.\r\n\r\nAnother thing is that I can't find suggestions for anything after `import tensorflow.keras.` statement.", "Nevermind....I installed the nightly version of tensorflow and it solved the problem...so maybe in the next versions it will be fixed.\r\n\r\n`pip3 install tf-nightly`\r\n\r\nps: i deleted the previous comment, this problem is not related to Windows Defender", "In this case, can we close the issue as it has been solved?", "@redeaglekiller were you able to test with 2.2.0-rc3 and see if your issue is resolved ? ", "It works in `tf-nightly==2.2.0.dev20200422`, closing this issue.\r\n\r\n![image](https://user-images.githubusercontent.com/9828683/80068282-a7086300-853f-11ea-8352-37d61630b738.png)\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32982\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32982\">No</a>\n", "Interesting how things seem to get resolved without actual testing being involved.   Intellisense is still broken on vscode, 1.45.1, tf-nightly-2.3.0.dev20200516.\r\n\r\nIntellisense error : {\r\n\t\"resource\": \"./code/encoder_mlp.py\",\r\n\t\"owner\": \"_generated_diagnostic_collection_name_#3\",\r\n\t\"code\": \"unresolved-import\",\r\n\t\"severity\": 4,\r\n\t\"message\": \"unresolved import 'tensorflow.keras'\",\r\n\t\"source\": \"Python\",\r\n\t\"startLineNumber\": 9,\r\n\t\"startColumn\": 6,\r\n\t\"endLineNumber\": 9,\r\n\t\"endColumn\": 22\r\n}", "@realdanielbyrne that looks like a recent breakage. Please open a new issue.", "I am using `deoplete-jedi` and `tensorflow=1.15.0`. Changing `PYTHONPATH` works.\r\n```\r\nmkdir $HOME/.local/share/jedi-extra\r\nln -s $HOME/miniconda3/lib/python3.7/site-packages/tensorflow_core $HOME/.local/share/jedi-extra/tensorflow\r\nexport PYTHONPATH=$HOME/.local/share/jedi-extra\r\n```"]}, {"number": 32981, "title": "Accessing validation data within a custom callback ??", "body": "**Describe the current behavior**\r\nHi, how can I access validation data within a custom callback ?\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\n\r\nimport os\r\nimport matplotlib.pyplot as plt\r\ntry:\r\n  # %tensorflow_version only exists in Colab.\r\n  %tensorflow_version 2.x\r\nexcept Exception:\r\n  pass\r\nimport tensorflow as tf\r\nimport datetime\r\n\r\n\r\n\r\nprint(\"TensorFlow version: {}\".format(tf.__version__))\r\nprint(\"Eager execution: {}\".format(tf.executing_eagerly()))\r\n\r\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D\r\nfrom tensorflow.keras import Model\r\n\r\nmnist = tf.keras.datasets.mnist\r\n\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\n\r\nmodel = tf.keras.models.Sequential([\r\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n  tf.keras.layers.Dense(128, activation='relu'),\r\n  tf.keras.layers.Dropout(0.2),\r\n  tf.keras.layers.Dense(10, activation='softmax')\r\n])\r\n\r\nclass MyCustomCallback(tf.keras.callbacks.Callback):\r\n    def on_epoch_end(self, epoch, logs=None):\r\n        print('Hello')\r\n        print(self.validation_data[0].shape)\r\n  \r\ndef customLoss3(y_true, y_pred):\r\n    return tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred))\r\n  \r\n#model.compile(optimizer='adam', loss=customLoss3, metrics=['accuracy'])\r\n\r\nmodel.compile(optimizer='adam', loss=customLoss3, metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\r\n\r\nmodel.fit(x_train, y_train, epochs=5, validation_split=0.2, callbacks=[MyCustomCallback()])\r\n\r\nmodel.evaluate(x_test, y_test)\r\n\r\n```\r\n**Other info / logs**\r\nTypeError: 'NoneType' object is not subscriptable\r\n", "comments": ["@hagianga21 Please find my [github gist](https://colab.sandbox.google.com/gist/gowthamkpr/5d9c7f1420dc42d19c2460a9eae6ac65/untitled.ipynb) here where I have written a custom callback and try to apply it to your issue and it should work. Thanks!", "How can we access the self.model._targets[0] and self.model.outputs[0] inside a tf.keras Custom Callback at on_batch_end?  @fchollet , @gowthamkpr\r\ntensorflow 2, eager execution enabled.\r\nI want to calculate the confusion matrix and the code I use inside the custom callback is:\r\n    \r\n```\r\n    @tf.function\r\n    def get_batch_conf_matrix(self):\r\n        pass\r\n        batch_conf_matrix = tf.math.confusion_matrix(\r\n                    labels=tf.reshape(\r\n                        tf.argmax(input=self.model._targets[0], axis=-1), [-1]\r\n                    ),\r\n                    predictions=tf.reshape(\r\n                        tf.argmax(input=self.model.outputs[0], axis=-1), [-1]\r\n                    ),\r\n                    num_classes=len(self.label_mapper.classes_),\r\n                    dtype=tf.dtypes.int64,\r\n                    name=None,\r\n                    weights=None,\r\n                )\r\n        return batch_conf_matrix\r\n\r\n\r\n    def on_batch_end(self, batch, logs=None):\r\n\r\n        batch_conf_matrix = self.get_batch_conf_matrix()\r\n        self.conf_mat_gpu.append(tf.keras.backend.eval(batch_conf_matrix))\r\n```", "Hi\r\nI am also facing this issue while defining my own custom callback. I raised it in a wrong prf earlier this morning . Quoting the reference here [(https://github.com/keras-team/keras/issues/10472.)]\r\n\r\nHere is the test case description to reproduce the issue:\r\n`I am executing the code on google colab and tf version is 2.2.0-rc2 , i have built a custom callback for calculating the F1 score and AUC score.\r\nI am getting the same error with fit. I am not using fit_generators as data augmentation in not necessary in my code.\r\n\r\n`from tensorflow.python.util.tf_export import keras_export\r\n@keras_export('keras.callbacks.Callback')\r\nclass MyCustomCallback(tf.keras.callbacks.Callback):\r\ndef on_train_begin(self, logs={}):\r\nself._data = []\r\n\r\ndef on_epoch_end(self, epoch, logs={}):\r\nprint(type(self.validation_data[0]))\r\nprint(type(self.validation_data[1]))\r\nX_val, y_val = self.validation_data[0], self.validation_data[1]\r\ny_predict = np.asarray(model.predict(X_val))\r\n\r\ny_val = np.argmax(y_val, axis=1)\r\ny_predict = np.argmax(y_predict, axis=1)\r\n\r\nself._data.append({\r\n  'val_microF1Score': f1_score(y_val, y_pred, average='micro'),\r\n  'val_rocauc': roc_auc_score(y_val, y_predict),\r\n  })\r\n\r\nprint('The f1 score {:7.2f} and auc {:7.2f}  for epoch {} .'.format( logs['val_microF1Score'], logs['val_rocauc'],epoch))\r\nreturn\r\ndef get_data(self):\r\nreturn self._data\r\n\r\nmycallback = MyCustomCallback()`\r\nI am invoking fit with these arguments \r\n\r\n `history = model.fit(X_train, y_train, epochs=nb_epoch,batch_size=batch_size,validation_split=0.2,validation_data = (X_cv,y_cv),callbacks=[mycallback])`\r\n\r\n\r\nCould you please suggest any workaround temporary for this as my work is held up due to this\r\n\r\nThank you", "> @hagianga21 Please find my [github gist](https://colab.sandbox.google.com/gist/gowthamkpr/5d9c7f1420dc42d19c2460a9eae6ac65/untitled.ipynb) here where I have written a custom callback and try to apply it to your issue and it should work. Thanks!\r\n\r\n@hagianga21,\r\nIs this still an issue? Could you please check @gowthamkpr's comment above? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 32980, "title": "Why are the Adam implementations in tensorflow.python.keras.optimizers and tensorflow.keras.optimizers different in tensorflow2.0stable? ", "body": "\r\nWhy are the Adam implementations in tensorflow.python.keras.optimizers and tensorflow.keras.optimizers different? ", "comments": ["@jiawei6636 Can you create a standalone code to reproduce the issue? I tried running a model with both the optimizers you mentioned. `optimizer=tf.keras.optimizers.Adam()` works as expected. But, `optimizer=tf.python.keras.optimizers.Adam()` throws an error as follows. [gist is here](https://colab.sandbox.google.com/gist/jvishnuvardhan/779336aedee841bbebdbd06521033076/untitled533.ipynb) for your reference. Thanks!\r\n\r\n```\r\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\r\n11493376/11490434 [==============================] - 0s 0us/step\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-2-680887cceef6> in <module>()\r\n     12 ])\r\n     13 \r\n---> 14 optimizer=tf.python.keras.optimizers.Adam()\r\n     15 model.compile(optimizer='adam',\r\n     16               loss='sparse_categorical_crossentropy',\r\n\r\nAttributeError: module 'tensorflow' has no attribute 'python'\r\n```", "> @jiawei6636 Can you create a standalone code to reproduce the issue? I tried running a model with both the optimizers you mentioned. `optimizer=tf.keras.optimizers.Adam()` works as expected. But, `optimizer=tf.python.keras.optimizers.Adam()` throws an error as follows. [gist is here](https://colab.sandbox.google.com/gist/jvishnuvardhan/779336aedee841bbebdbd06521033076/untitled533.ipynb) for your reference. Thanks!\r\n> \r\n> ```\r\n> Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\r\n> 11493376/11490434 [==============================] - 0s 0us/step\r\n> ---------------------------------------------------------------------------\r\n> AttributeError                            Traceback (most recent call last)\r\n> <ipython-input-2-680887cceef6> in <module>()\r\n>      12 ])\r\n>      13 \r\n> ---> 14 optimizer=tf.python.keras.optimizers.Adam()\r\n>      15 model.compile(optimizer='adam',\r\n>      16               loss='sparse_categorical_crossentropy',\r\n> \r\n> AttributeError: module 'tensorflow' has no attribute 'python'\r\n> ```\r\n\r\n`from tensorflow.python.keras.optimizers import Adam `", "@jiawei6636 Anything under `tf.python.*` is private, intended for development only, rather than public use. Importing from tensorflow.python or any other modules (including import tensorflow_core...) is not supported, and can break unannounced. For full response on similar issue, you can check [here](https://github.com/tensorflow/tensorflow/issues/33075#issuecomment-539070546).\r\n\r\nI am closing the issue as it was resolved. Please feel free to open it if the issue persists again. Thanks!"]}, {"number": 32979, "title": "Fix typo in release note", "body": "This fixes a small typo in the release notes in the r2.0 branch with the assumption that the change will propagate into the master branch and/or later releases from the r2.0 branch.", "comments": ["These release notes have now been copied back into master. I will now close and re-submit this pull request against master."]}, {"number": 32978, "title": "Fix the Consistency of shape example", "body": "", "comments": []}, {"number": 32977, "title": "tensorflow2.0 can't find CRF", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):tensorflow2.0\r\n- Are you willing to contribute it (Yes/No):\r\nI don't know how to achieve it well, if need me, I can do as much as I can\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nNo named entity identification module was found\r\n\r\n**Will this change the current api? How?**\r\nThere is no named entity recognition API\r\n\r\n**Who will benefit with this feature?**\r\nA technician who studies named entity recognition and relationship extraction\r\n\r\n**Any Other info.**\r\n", "comments": ["Try this https://github.com/tensorflow/addons/commit/47e687725eb4d65e45498e67a9026177fda684dd", "@MQ99 As you can see in the comment [here](https://github.com/tensorflow/tensorflow/issues/26167#issuecomment-517431612), CRF has been ported to Tensorflow 2.0. Closing this issue as it has been resolved"]}, {"number": 32976, "title": "Remove unneeded Kineis dependency libraries", "body": "In TensorFlow 1.x Kinesis used to be in tf.contrib. Now as\r\ntf.contrib has been moved out it makes sense to remove Kinesis\r\nform the dependency libraries in aws/BUILD, to reduce build time with C++ (slightly)\r\nNote AWS itself is still very much needed as s3 is still supported\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 32975, "title": "tf.io.gfile.GFile does not work with Python zipfile", "body": "**System information**\r\n\r\n-  Have I written custom code: Yes\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: v1.14.0-rc1-22-gaf24dc9 1.14.0\r\n- Python version: 3.6\r\n\r\n**Describe the current behavior**\r\n`tf.io.gfile.GFile` does not work correctly with the Python built in `zipfile` module. When writing to a ZipFile where `file=GFile()` the resulting ZIP file is corrupt.\r\n\r\n**Describe the expected behavior**\r\nZipFiles written with `file=GFile()` should be not be corrupt and equal to those written with `file=<other_type_fd>`.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\nimport zipfile\r\nimport filecmp\r\n\r\n\r\nnormal_fd = open(\"normal.zip\", \"wb\")\r\nnormal2_fd = open(\"normal2.zip\", \"wb\")\r\nopen(\"gfile.zip\", \"w\").close()  # \"touch\" file so that it exists, issue #32090\r\ngfile_fd = tf.io.gfile.GFile(\"gfile.zip\", \"w+b\")  # need +, issue #32122\r\n\r\nfor fd in (normal_fd, normal2_fd, gfile_fd):\r\n    with zipfile.ZipFile(file=fd, \"w\") as zipfd:\r\n        with zipfd.open(\"test.txt\", \"w\") as fid:\r\n            fid.write(\"Hello, World!\".encode())\r\n    fd.close()\r\n\r\nprint(\"Normal zips equal?\", filecmp.cmp(\"normal.zip\", \"normal2.zip\"))\r\nprint(\"GFile zip equal normal?\", filecmp.cmp(\"normal.zip\", \"gfile.zip\"))\r\n\r\n```", "comments": ["Issue replicating for TF-1.14, kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/c9c779a326c8e55b7ad6f50b726774cf/32975.ipynb) of colab.Thanks!", "Will handle this after refactoring entailed by tensorflow/community#101", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32975\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32975\">No</a>\n"]}, {"number": 32974, "title": "MKL DNN: Fix issues with hadoopFileSystem load error message", "body": "If Hadoop File system is not installed, still TF tries to load it. However, if cannot load it, then it emits error message. \r\nHere in this PR, the error message is removed, since the loading is tried speculatively and, thus, if fails to load, it should not emit any error message.", "comments": ["I think @jhseu is a better reviewer for this.", "Hi @jhseu if you some quick time, can you please take a look at this small PR. It crashes some unit tests in CPUs.", "Hi @jhseu if you have some quick time, can you please take a look at this small PR. It fixes some crashes in unit test.", "@superbobry This PR wants to remove one of the error warning messages added by a PR you reviewed a few weeks ago (https://github.com/tensorflow/tensorflow/pull/32649). Could you please help take a look? Thank you!", "Thanks for the heads up @penpornk! ", "Hadoop use lazy load for now.There is no need to remove error log.Please revert the PR. #32885\r\n@superbobry @penpornk @jhseu @ashah-abq "]}, {"number": 32973, "title": "Add 2 space local rule in python style guide.", "body": "Tensorflow use 2 spaces but google python style guide is 4 space. \r\nIt would be kind to tell the beginner about this.\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32973) for more info**.\n\n<!-- need_sender_cla -->", "@godaji can you please sign the CLA ?", "@godaji gentle ping to sign CLA. Thanks!", "I don't think it's necessary to update this. Developers already see Python code using 2 spaces and Python is very sensitive to spacing so if one were to use 4 spaces they would get errors."]}, {"number": 32972, "title": "tf.io.gfile.mkdir does not throw exception if path exists", "body": "**System information**\r\n\r\n-  Have I written custom code: Yes\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: v1.14.0-rc1-22-gaf24dc9 1.14.0\r\n- Python version: 3.6\r\n\r\n**Describe the current behavior**\r\n`tf.io.gfile.mkdir()` on Linux does not throw an exception if the path exists. This can cause problems if the path is a file, as you would assume that the path is a directory after calling `mkdir()`. See the code below for an example.\r\n\r\n**Describe the expected behavior**\r\n`mkdir()` should throw an exception if the path exists, like the Python built in [`os.mkdir()`](https://docs.python.org/3/library/os.html#os.mkdir) does.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ntest_path = \"test\"\r\n\r\n# Create a file at <test_path>\r\nwith open(test_path, \"w\") as f:\r\n    f.write(\"Hello, World!\")\r\n\r\n# No error when creating dir\r\ntf.io.gfile.mkdir(test_path)\r\nwith tf.io.gfile.GFile(test_path + \"/test_file.txt\", \"w\") as f:\r\n    # .write throws exception since the <test_path> directory does not exist\r\n    f.write(\"Hello\")\r\n```", "comments": ["Issue replicating for TF-1.14, kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/3d836d2cc2a89f8618022f9095d42363/32972.ipynb) of colab.Thanks!", "This is a known bug in C++ filesystem support too. I will fix this after https://github.com/tensorflow/community/pull/101", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32972\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32972\">No</a>\n"]}, {"number": 32971, "title": "Clean up DatasetParams and refactor multiple DatasetOpTests", "body": "This PR \r\n- cleans up `DatasetParams` a little bit \r\n  - `op_name()` -> `dataset_type()`; \r\n  - remove `CreateFactory()`\r\n- refactors `InterleaveDatasetOpTest`, `MapDefunOpTest`, `OptimizeDatasetOpTest`, and `PaddedBatchDatasetOpTest`.", "comments": ["@aaudiber @rthadur This PR seems to be blocked by the internal tests for a while. Could you please help retrigger the internal tests?"]}, {"number": 32970, "title": "java.nio.BufferOverflowException TensorFlowYoloDetector", "body": "Hello, \r\nI am using Yolov2-tiny to detect a single class a running this on this example app. Because I only need a single class, I have been experimenting by reducing layers on yolov2-tiny and the filter sizes.  After a second reduction of the model, I have ran into the error \"java.nio.BufferOverflowException\". \r\n\r\n**Full Error**\r\nE/AndroidRuntime: FATAL EXCEPTION: inference\r\n    Process: org.tensorflow.demo, PID: 32033\r\n    java.nio.BufferOverflowException\r\n        at java.nio.HeapFloatBuffer.put(HeapFloatBuffer.java:180)\r\n        at org.tensorflow.Tensor.writeTo(Tensor.java:488)\r\n        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.fetch(TensorFlowInferenceInterface.java:488)\r\n        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.fetch(TensorFlowInferenceInterface.java:442)\r\n        at org.tensorflow.demo.TensorFlowYoloDetector.recognizeImage(TensorFlowYoloDetector.java:168)\r\n        at org.tensorflow.demo.DetectorActivity$3.run(DetectorActivity.java:330)\r\n        at android.os.Handler.handleCallback(Handler.java:873)\r\n        at android.os.Handler.dispatchMessage(Handler.java:99)\r\n        at android.os.Looper.loop(Looper.java:214)\r\n        at android.os.HandlerThread.run(HandlerThread.java:65)\r\nI/Process: Sending signal. PID: 32033 SIG: 9\r\n\r\n**Here is my setup** \r\n\r\n- **Hardware**: Samsung S8\r\n- **System**: Windows\r\n-**Models**: Trained inside Linux VM and converted to .pb file using darknet.\r\n- **Preview Size**: Changed from 640x480 to 1920x1080\r\n\r\n**Models**\r\n\r\n- **Trial 1**\r\n     - **Model**: Yolov2-tiny (no modifications)\r\n     - **Model size**: 43MB\r\n     -**Inference Time**: ~600ms\r\n     -**Status**: Works with no issues\r\n\r\n- **Trial 2**\r\n     - **Model**: Yolov2-tiny (Removed a convolutional layer and scaled down filters)\r\n     - **Model size**: 10MB\r\n     -**Inference Time**: ~400ms\r\n     -**Status**: Works with no issues\r\n\r\n- **Trial 3**\r\n     - **Model**: Yolov2-tiny (removed 2 convolutional layers and scaled down filters)\r\n     - **Model size**: 2.7MB\r\n     -**Inference Time**: None\r\n     -**Status**: Throws the exception status above\r\n     -**Note**: This version works if I lower the parameter YOLO_BLOCK_SIZE in \r\n                      DetectorActivitty.java from 32 to 16. The problem with this is that it gives me an \r\n                      inference time of 600MS and that negates the purpose of scaling down the model\r\n\r\nAll three of these model work well when used with darknet and were converted using darkflow in the exact same environment and commands. \r\n\r\nAs mentioned by  @UsamaIslam in #12649 , he fixed this issue by changing the MAX_RESULTS parameter when using fast rcnn with this example app. I tried doing the same, but it didn't work. I don't fully understand this exception so I need help understanding why this fails with **Trial 3** and not the other two trials. \r\n\r\nI would appreciate any suggestions on how I could make this work. \r\n", "comments": ["Is there a reason you cannot use TensorFlow Lite? See also https://blog.francium.tech/real-time-object-detection-on-mobile-with-flutter-tensorflow-lite-and-yolo-android-part-a0042c9b62c6. Note that the TensorFlow Mobile library/bindings are deprecated.", "@jdduke thank you for the reply. I originally tried to use TF LIte. I tried to build the sample application in https://blog.francium.tech/real-time-object-detection-on-mobile-with-flutter-tensorflow-lite-and-yolo-android-part-a0042c9b62c6 a couple of times, but I could not make it work. Ran into multiple issues when using flutter and I found no solution for the problems online. I used a different flutter example with TF Lite and yolo2, but this one did not feature detection on a video feed. Because I need to make progress on other parts of the project I didn't have time to figure out how to figure out how to add this.\r\n\r\nIf you could refer me to a project that implements yolo2-tiny in TF Lite with real time detection that would be a best case scenario. ", "I see. In theory you should be able to swap out your use of `TensorFlowInferenceInterface` with the standard TensorFlow Lite `Interpreter`, assuming you have the .tflite model available. Can you provide a link to the .tflite model you tried using? Note that we're also working on YoloV3 compat (see https://github.com/YunYang1994/tensorflow-yolov3/pull/312).", "Hello @jdduke, \r\nI will try changing the the app to use TF lite interpreter. I'll give an update once I do. \r\n\r\nThank you,\r\nMiguel", "Tei has made Yolo-v3 work. \r\n\r\nHi Miguel, is it possible to attach the problematic model for us to debug? thanks", "@mkulisic \r\nIs this still an issue.", "@Saduf2019 I eventually managed to solve the problem.", "@mkulisic \r\nPlease move this to closed status if resolved.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32970\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32970\">No</a>\n"]}, {"number": 32969, "title": "Improve docs for cmdline", "body": "Minor improvements to make docs a bit more beautiful - #TFDocsSprint ", "comments": []}, {"number": 32968, "title": "Point directly to guide without alpha", "body": "", "comments": ["> Thanks! This is correct. I'm working on a larger PR that will update tensorflow/tensorflow\r\n> \r\n> (Though redirects will forward this along)\r\n\r\nyeah, it works fine, it's just that i was re-reading docs and decided to submit this small change in place.", "similar changes are pushed by ` db6be9c ` , so closing this PR, thank you ", "> db6be9c\r\n\r\n@rthadur I think your reference is wrong, since mentioned commit is not addressing it - https://github.com/tensorflow/tensorflow/commit/db6be9c", "Looks like the fix landed in https://github.com/tensorflow/tensorflow/commit/4f1336086675aaf747703ad26b4a01743f8ed85c\r\nThanks"]}, {"number": 32967, "title": "TensorFlow 2.0 doesn't detect the GPU when rerunning the same program", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): /\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: /\r\n- TensorFlow installed from (source or binary): /\r\n- TensorFlow version (use command below): tensorflow-gpu==2.0.0\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): /\r\n- GCC/Compiler version (if compiling from source): /\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: GeForce RTX 2080 Ti, ~11GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nWhen running a tensorflow program for the first time (in a fresh docker instance) it detects the GPU as expected. When running the same program again (after stopping the previous run) it doesn't detect any GPU devices.\r\nThis works as expected in tensorflow-gpu==2.0.0rc1 however.\r\n\r\n**Describe the expected behavior**\r\nTensorflow should always detect all attached GPUs, even on subsequent runs.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\nimport tensorflow as tf\r\n(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()\r\ntrain_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\r\ntrain_images = (train_images - 127.5) / 127.5\r\ntrain_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(100).batch(32)\r\nfrom tensorflow.python.client import device_lib \r\nprint(device_lib.list_local_devices())\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nDevices detected by tensorflow on the first run:\r\n```\r\n[name: \"/device:CPU:0\"\r\ndevice_type: \"CPU\"\r\nmemory_limit: 268435456\r\nlocality {\r\n}\r\nincarnation: 2693156956587076928\r\n, name: \"/device:XLA_CPU:0\"\r\ndevice_type: \"XLA_CPU\"\r\nmemory_limit: 17179869184\r\nlocality {\r\n}\r\nincarnation: 7803384591284859902\r\nphysical_device_desc: \"device: XLA_CPU device\"\r\n, name: \"/device:XLA_GPU:0\"\r\ndevice_type: \"XLA_GPU\"\r\nmemory_limit: 17179869184\r\nlocality {\r\n}\r\nincarnation: 8169967461022885217\r\nphysical_device_desc: \"device: XLA_GPU device\"\r\n, name: \"/device:GPU:0\"\r\ndevice_type: \"GPU\"\r\nmemory_limit: 10742700442\r\nlocality {\r\n  bus_id: 1\r\n  links {\r\n  }\r\n}\r\nincarnation: 643882925305723598\r\nphysical_device_desc: \"device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:0b:00.0, compute capability: 7.5\"\r\n]\r\n```\r\n\r\nDevices detected by tensorflow on the second run:\r\n```\r\n[name: \"/device:CPU:0\"\r\ndevice_type: \"CPU\"\r\nmemory_limit: 268435456\r\nlocality {\r\n}\r\nincarnation: 13203723707335455513\r\n, name: \"/device:XLA_CPU:0\"\r\ndevice_type: \"XLA_CPU\"\r\nmemory_limit: 17179869184\r\nlocality {\r\n}\r\nincarnation: 6024229839119595332\r\nphysical_device_desc: \"device: XLA_CPU device\"\r\n]\r\n```\r\n", "comments": ["Update:\r\nit works as expected when installing `tensorflow-gpu==2.0.0rc1` and also `tensorflow-gpu==2.0.0`. It doesn't work with `tensorflow-gpu` (without the version specification).\r\nThis might be related to [this previous issue](https://github.com/tensorflow/tensorflow/issues/12388) where `pip install tensorflow-gpu` installed the tensorflow cpu version as well which messed something up.", "@schmidtdominik  \r\nI tried with installing `!pip install tensorflow-gpu` on colab it works as expected.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/78d3958f0e88fe2a3b28f73c04137ed6/untitled239.ipynb).\r\nCan we close this issue since it looks like it is resolved. Please, let us know if i am mistaken. Thanks!", "I originally ran it in a Jupyter notebook in a nightly-gpu-py3 docker instance\r\nBut now I'm having difficulty to reproduce it so I guess it's resolved, thanks", "> Update:\r\n> it works as expected when installing `tensorflow-gpu==2.0.0rc1` and also `tensorflow-gpu==2.0.0`. It doesn't work with `tensorflow-gpu` (without the version specification).\r\n> This might be related to [this previous issue](https://github.com/tensorflow/tensorflow/issues/12388) where `pip install tensorflow-gpu` installed the tensorflow cpu version as well which messed something up.\r\n\r\nHey man!! you are a lifesaver, I wasted like 2/3 days to figure out this.. then finally your trick worked.. thanks a lot. ", "> Update:\r\n> it works as expected when installing `tensorflow-gpu==2.0.0rc1` and also `tensorflow-gpu==2.0.0`. It doesn't work with `tensorflow-gpu` (without the version specification).\r\n> This might be related to [this previous issue](https://github.com/tensorflow/tensorflow/issues/12388) where `pip install tensorflow-gpu` installed the tensorflow cpu version as well which messed something up.\r\n\r\nConfirmed as working. I've been trying different solutions. But, this one worked. Thank you.", "`pip install tf-nightly-gpu` work for me", "My confusion is [Install TensorFlow 2](https://www.tensorflow.org/install) says\r\n\r\n> TensorFlow 2 packages are available\r\n>\r\n> - `tensorflow` \u2014Latest stable release with CPU and GPU support (Ubuntu and Windows)\r\n> - `tf-nightly` \u2014Preview build (unstable). Ubuntu and Windows include GPU support.\r\n>\r\n> For TensorFlow 1.x, CPU and GPU packages are separate:\r\n\r\nAccording to that, I think `pip install tensorflow` will support GPU. I try to execute the following code provided by [Use a GPU | TensorFlow Core](https://www.tensorflow.org/guide/gpu):\r\n\r\n```python\r\nimport tensorflow as tf\r\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\n```\r\n\r\nIt gives me `Num GPUs Available:  0`. Also `tf.test.is_gpu_available()` is False.\r\n\r\nThen I try to remove the preciously installed tensorflow by `pip unintall tensorflow` and install gpu version like `pip install tf-nightly-gpu`.\r\n\r\nNow the result is `Num GPUs Available:  4`.\r\n\r\nWhy the doc says `Latest stable release of tensorflow 2 has CPU and GPU support (Ubuntu and Windows)`?\r\n\r\n\r\n"]}, {"number": 32966, "title": "DepthwiseConv2D missing dilation_rate argument (& higher performance)", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): (yes)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): ongoing, reference github source https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/layers/convolutional.py#L1686-L1877\r\n- TensorFlow version (use command below): TF2 rc\r\n- Python version: (3)\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nConv, Conv2D, and backend.depthwise_conv2d support dilation_rate, but DepthwiseConv2D does not. \r\n\r\n**Describe the expected behavior**\r\n\r\nDepthwiseConv2D needs the dilation_rate argument. It is supported by underlying code and mentioned by the DepthwiseConv2D documentation, but is missing from DepthwiseConv2D. \r\n\r\n**Note that while the DepthwiseConv2D  as below works OK, performance of the forward inference code is significantly less than what it should be. DepthwiseConv2D with dilation is a very useful operation for full-resolution feature matching with a low operation count, so improving the execution code would be valuable.**\r\n\r\n**Other info / logs**\r\n\r\nProposed working revised version (sorry, not adept with github).\r\n\r\n\r\n```\r\n\r\n@keras_export('keras.layers.DepthwiseConv2D')\r\nclass DepthwiseConv2D(Conv2D):\r\n  \"\"\"Depthwise separable 2D convolution.\r\n  Depthwise Separable convolutions consists in performing\r\n  just the first step in a depthwise spatial convolution\r\n  (which acts on each input channel separately).\r\n  The `depth_multiplier` argument controls how many\r\n  output channels are generated per input channel in the depthwise step.\r\n  Arguments:\r\n    kernel_size: An integer or tuple/list of 2 integers, specifying the\r\n      height and width of the 2D convolution window.\r\n      Can be a single integer to specify the same value for\r\n      all spatial dimensions.\r\n    strides: An integer or tuple/list of 2 integers,\r\n      specifying the strides of the convolution along the height and width.\r\n      Can be a single integer to specify the same value for\r\n      all spatial dimensions.\r\n      Specifying any stride value != 1 is incompatible with specifying\r\n      any `dilation_rate` value != 1.\r\n    padding: one of `'valid'` or `'same'` (case-insensitive).\r\n    depth_multiplier: The number of depthwise convolution output channels\r\n      for each input channel.\r\n      The total number of depthwise convolution output\r\n      channels will be equal to `filters_in * depth_multiplier`.\r\n    data_format: A string,\r\n      one of `channels_last` (default) or `channels_first`.\r\n      The ordering of the dimensions in the inputs.\r\n      `channels_last` corresponds to inputs with shape\r\n      `(batch, height, width, channels)` while `channels_first`\r\n      corresponds to inputs with shape\r\n      `(batch, channels, height, width)`.\r\n      It defaults to the `image_data_format` value found in your\r\n      Keras config file at `~/.keras/keras.json`.\r\n      If you never set it, then it will be 'channels_last'.\r\n    dilation_rate: an integer or tuple/list of 2 integers, specifying\r\n      the dilation rate to use for dilated convolution.\r\n      Can be a single integer to specify the same value for\r\n      all spatial dimensions.\r\n      Currently, specifying any `dilation_rate` value != 1 is\r\n      incompatible with specifying any stride value != 1.\r\n    activation: Activation function to use.\r\n      If you don't specify anything, no activation is applied\r\n      (ie. 'linear' activation: `a(x) = x`).\r\n    use_bias: Boolean, whether the layer uses a bias vector.\r\n    depthwise_initializer: Initializer for the depthwise kernel matrix.\r\n    bias_initializer: Initializer for the bias vector.\r\n    depthwise_regularizer: Regularizer function applied to\r\n      the depthwise kernel matrix.\r\n    bias_regularizer: Regularizer function applied to the bias vector.\r\n    activity_regularizer: Regularizer function applied to\r\n      the output of the layer (its 'activation').\r\n    depthwise_constraint: Constraint function applied to\r\n      the depthwise kernel matrix.\r\n    bias_constraint: Constraint function applied to the bias vector.\r\n  Input shape:\r\n    4D tensor with shape:\r\n    `[batch, channels, rows, cols]` if data_format='channels_first'\r\n    or 4D tensor with shape:\r\n    `[batch, rows, cols, channels]` if data_format='channels_last'.\r\n  Output shape:\r\n    4D tensor with shape:\r\n    `[batch, filters, new_rows, new_cols]` if data_format='channels_first'\r\n    or 4D tensor with shape:\r\n    `[batch, new_rows, new_cols, filters]` if data_format='channels_last'.\r\n    `rows` and `cols` values might have changed due to padding.\r\n  \"\"\"\r\n\r\n  def __init__(self,\r\n               kernel_size,\r\n               strides=(1, 1),\r\n               padding='valid',\r\n               depth_multiplier=1,\r\n               data_format=None,\r\n               activation=None,\r\n               use_bias=True,\r\n               dilation_rate=(1,1),\r\n               depthwise_initializer='glorot_uniform',\r\n               bias_initializer='zeros',\r\n               depthwise_regularizer=None,\r\n               bias_regularizer=None,\r\n               activity_regularizer=None,\r\n               depthwise_constraint=None,\r\n               bias_constraint=None,\r\n               **kwargs):\r\n    super(DepthwiseConv2D, self).__init__(\r\n        filters=None,\r\n        kernel_size=kernel_size,\r\n        strides=strides,\r\n        padding=padding,\r\n        data_format=data_format,\r\n        dilation_rate=dilation_rate,\r\n        activation=activation,\r\n        use_bias=use_bias,\r\n        bias_regularizer=bias_regularizer,\r\n        activity_regularizer=activity_regularizer,\r\n        bias_constraint=bias_constraint,\r\n        **kwargs)\r\n    self.depth_multiplier = depth_multiplier\r\n    self.depthwise_initializer = initializers.get(depthwise_initializer)\r\n    self.depthwise_regularizer = regularizers.get(depthwise_regularizer)\r\n    self.depthwise_constraint = constraints.get(depthwise_constraint)\r\n    self.bias_initializer = initializers.get(bias_initializer)\r\n\r\n  def build(self, input_shape):\r\n    if len(input_shape) < 4:\r\n      raise ValueError('Inputs to `DepthwiseConv2D` should have rank 4. '\r\n                       'Received input shape:', str(input_shape))\r\n    input_shape = tensor_shape.TensorShape(input_shape)\r\n    if self.data_format == 'channels_first':\r\n      channel_axis = 1\r\n    else:\r\n      channel_axis = 3\r\n    if input_shape.dims[channel_axis].value is None:\r\n      raise ValueError('The channel dimension of the inputs to '\r\n                       '`DepthwiseConv2D` '\r\n                       'should be defined. Found `None`.')\r\n    input_dim = int(input_shape[channel_axis])\r\n    depthwise_kernel_shape = (self.kernel_size[0],\r\n                              self.kernel_size[1],\r\n                              input_dim,\r\n                              self.depth_multiplier)\r\n\r\n    self.depthwise_kernel = self.add_weight(\r\n        shape=depthwise_kernel_shape,\r\n        initializer=self.depthwise_initializer,\r\n        name='depthwise_kernel',\r\n        regularizer=self.depthwise_regularizer,\r\n        constraint=self.depthwise_constraint)\r\n\r\n    if self.use_bias:\r\n      self.bias = self.add_weight(shape=(input_dim * self.depth_multiplier,),\r\n                                  initializer=self.bias_initializer,\r\n                                  name='bias',\r\n                                  regularizer=self.bias_regularizer,\r\n                                  constraint=self.bias_constraint)\r\n    else:\r\n      self.bias = None\r\n    # Set input spec.\r\n    self.input_spec = InputSpec(ndim=4, axes={channel_axis: input_dim})\r\n    self.built = True\r\n\r\n  def call(self, inputs):\r\n    outputs = backend.depthwise_conv2d(\r\n        inputs,\r\n        self.depthwise_kernel,\r\n        strides=self.strides,\r\n        padding=self.padding,\r\n        dilation_rate=self.dilation_rate,\r\n        data_format=self.data_format)\r\n\r\n    if self.use_bias:\r\n      outputs = backend.bias_add(\r\n          outputs,\r\n          self.bias,\r\n          data_format=self.data_format)\r\n\r\n    if self.activation is not None:\r\n      return self.activation(outputs)\r\n\r\n    return outputs\r\n\r\n  @tf_utils.shape_type_conversion\r\n  def compute_output_shape(self, input_shape):\r\n    if self.data_format == 'channels_first':\r\n      rows = input_shape[2]\r\n      cols = input_shape[3]\r\n      out_filters = input_shape[1] * self.depth_multiplier\r\n    elif self.data_format == 'channels_last':\r\n      rows = input_shape[1]\r\n      cols = input_shape[2]\r\n      out_filters = input_shape[3] * self.depth_multiplier\r\n\r\n    rows = conv_utils.conv_output_length(rows, self.kernel_size[0],\r\n                                         padding=self.padding,\r\n                                         stride=self.strides[0],\r\n                                         dilation=self.dilation_rate[0])\r\n    cols = conv_utils.conv_output_length(cols, self.kernel_size[1],\r\n                                         padding=self.padding,\r\n                                         stride=self.strides[1],\r\n                                         dilation=self.dilation_rate[1])\r\n    if self.data_format == 'channels_first':\r\n      return (input_shape[0], out_filters, rows, cols)\r\n    elif self.data_format == 'channels_last':\r\n      return (input_shape[0], rows, cols, out_filters)\r\n\r\n  def get_config(self):\r\n    config = super(DepthwiseConv2D, self).get_config()\r\n    config.pop('filters')\r\n    config.pop('kernel_initializer')\r\n    config.pop('kernel_regularizer')\r\n    config.pop('kernel_constraint')\r\n    config['depth_multiplier'] = self.depth_multiplier\r\n    config['depthwise_initializer'] = initializers.serialize(\r\n        self.depthwise_initializer)\r\n    config['depthwise_regularizer'] = regularizers.serialize(\r\n        self.depthwise_regularizer)\r\n    config['depthwise_constraint'] = constraints.serialize(\r\n        self.depthwise_constraint)\r\n    return config\r\n\r\n\r\n```", "comments": ["is there any solution?thanks.", "This is [fixed](https://github.com/tensorflow/tensorflow/commit/5f2e159a58d1ef3414b2c34339266449574d8f94).\r\nFor performance issue, please file another bug. I don't see this as a Keras issue, the performance needs to be investigated at either op level or kernel level.", "Thanks for adding this :+1:"]}, {"number": 32965, "title": "Freezing a graph using the C API", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\n\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n\r\n\r\n\r\n\r\nHi everyone,\r\n\r\nIs it possible to freeze a graph and save the frozen version using the C API after loading the graph's definition file (.pb) and restoring its variables created in Python ?\r\n\r\nA code snippet of the \"freezing\" step would be greatly appreciated!\r\n\r\nThanks :)\r\n\r\n\r\n", "comments": ["@adaber,\r\nCan you please check [this link](https://towardsdatascience.com/creating-a-tensorflow-cnn-in-c-part-3-freezing-the-model-and-augmenting-it-59a07c7c4ec6) and let us know if it helps. If that link doesn't help, can you please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. \r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "Hi, rmothukuru\r\n\r\nFirstly, thank you for the response. It is very appreciated :)\r\n\r\nMy environment:\r\n\r\n-Operating system: Windows 10\r\n-TensorFlow version: 1.13.1 (C library: libtensorflow-gpu-windows-x86_64-1.13.1)\r\n-Visual Studio version: 2017\r\n\r\nI had checked out that link before and while it has a ton of useful information it is the C++ API, not the C API. For example, the author uses the \"FreezeSavedModel\" method that doesn't exist in the C API.\r\n\r\nI've built C++ code (using the TF **C** API!) for loading and running a model created in Python but I would like to be able to train a model in C++ (using the TF **C** API!) and freeze it before saving it to the harddrive. I know how to train a model but don't know how to freeze it using the TF C API.\r\n\r\nI have tried to find any information regarding this online but couldn't find any. So, my question is about freezing a model but using the C API not the C++ API. Any pointers or pseudo code would help.\r\n\r\nThanks again for your time, rmothukuru!\r\n\r\n\r\n", "There's no way to freeze it using only the C API. Also freezing is unsupported going forward. @gargn to confirm.", "Hi, alextp\r\n\r\nThank you for your response. I have tried to Google keywords such as \"tensorflow model freezing being unsupported in the future\" and didn't get any hits. Will the model file format be different in the future ? Will the network architecture and tensor values be saved to a single file by default ?\r\n\r\nIt'd be great if you could provide a bit more information if you have any at this point.\r\n\r\nThanks!", "The primary supported file format in 2.0 is [SavedModel](https://www.tensorflow.org/guide/saved_model). While GraphDef is a part of the SavedModel, it is no longer an independently supported format."]}, {"number": 32964, "title": "Automatic gradient for numpy functions/external functions", "body": "I am thinking of the following scenarios and want to see if TensorFlow could do Automatic Differentiation for them:\r\n\r\n1. There would be a numpy function that accepts numpy array(s) as input(s) and numpy array(s) as output(s);\r\n\r\n2. There would be an external executable that accepts numpy array(s) as input(s) and numpy array(s) as output(s).\r\n\r\nI know TensorFlow has `tf.numpy_function` (with the v2 context, and equivalently, it was `tf.py_func` in the v1 context) can \"convert\" a numpy function to a tensor function and inject it into the graph but unfortunately it cannot support Automatic Differentiation; also, `tf.py_function` must accept a tensor function and tensor inputs to inject the function into the graph and fortunately it can support Automatic Differentiation. I cannot think of a way to inject an external executable to be part of the graph and possibly get its differentiation.\r\n\r\nIf there is no way to achieve scenario 1 and/or 2, is there any workaround?", "comments": ["Hi,\r\n\r\nWhat you are asking would require some kind of AutoGraph for numpy functions, _i.e._ a generic translator of \"numpy functions\" into tensorflow operations, which would perhaps be possible to implement but would require a lot of effort, and would probably induce some heavy overhead (AutoGraph is already creating some significant overhead on some cases, and it is attached to a set of Python control flows way more limited than the set of numpy operations - and it already represents a massive and honestly quite impressive work).\r\n\r\nAs a matter of fact, what `tf.numpy_function` does is (benefitting from Eager execution) that it extracts EagerTensors' values as numpy arrays, and then simply runs these arrays through the wrapped function and converts the outputs to Tensors again. So, it just allows pipelining numpy functions in the process, but does not do any code translation into tensorflow operations.\r\n\r\nIn my humble opinion, your question is interesting in that it touches to common frustrations one may encounter regarding the possibility to re-use existing numpy-based code, but I do not believe it can or will be solved, at least not on the short run (who knows, perhaps there is work on it). The only alternative I can think of to the solution I described would be to add up some automatic differentiation in numpy, and then make it compatible with tensorflow (enabling a cross-framework automatic differentiation), but I would not really bet on that happening (at least not soon, but again I do not have an omniscient knowledge of what people might be working on).\r\n\r\nThe best \"workaround\" you have is to actually re-implement the numpy-based functions you want to use in tensorflow, which can be tedious for large pieces of code but should be achievable as most numpy operations have a tensorflow equivalent (and AutoGraph could allow to leave some python control flows as-is).", "@zhulingchen,\r\nSorry for the delayed response. Can you please refer the Documentation for [Gradients Calculation for Numpy Arrays](https://www.tensorflow.org/guide/tf_numpy#gradients_and_jacobians_tfgradienttape) and let us know if this is what you are looking for? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 32963, "title": "tf.io.gfile.mkdir restricts directory mode/(permissions)", "body": "**System information**\r\n\r\n-  Have I written custom code: Yes\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: v1.14.0-rc1-22-gaf24dc9 1.14.0\r\n- Python version: 3.6\r\n\r\n**Describe the current behavior**\r\nDirectories created with `tf.io.gfile.mkdir()` on Linux does not have the w mode bit set for group and other even if allowed by the umask and ACL.\r\n\r\n**Describe the expected behavior**\r\nDirectories created with `tf.io.gfile.mkdir` should have the maximum permissions allowed by the umask and ACL, which is the way `os.mkdir()` in Python works.\r\nI think this behavior is caused by the fact that [TF always calls mkdir with mode 0755](https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/core/platform/posix/posix_file_system.cc#L281) while  [Python calls mkdir with mode 511 (= 777 in octal)](https://github.com/python/cpython/blob/3.7/Modules/clinic/posixmodule.c.h#L1094) if no mode is given .\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\nimport stat\r\nimport os\r\n\r\nos.umask(0000)\r\n\r\ntf_dir = \"test1\"\r\nos_dir = \"test2\"\r\n\r\ntf.io.gfile.mkdir(tf_dir)\r\ntf_mode = os.stat(tf_dir).st_mode\r\n\r\nos.mkdir(os_dir)\r\nos_mode = os.stat(os_dir).st_mode\r\n\r\nif (tf_mode != os_mode):\r\n    print(\"File mode differs:\")\r\n    print(\"TF: {}, OS: {}\".format(stat.filemode(tf_mode), stat.filemode(os_mode)))\r\n\r\n```", "comments": ["Added a PR #33312 for the fix.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32963\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32963\">No</a>\n"]}, {"number": 32962, "title": "Remove left over code", "body": "Remove left over code since the removal of XLAFusionOptimizer in e833b6c98b53d7e06a619d0b5329d7ffb2135b64\r\n\r\n@sanjoy ", "comments": ["I forgot to remove some other lines. So I amended the commit above to remove all leftover lines.", "4 CI failed, but I do not think the error is related to my PR.\r\nAnd the Ubuntu CC didn't run. Any idea what is going on?"]}, {"number": 32961, "title": "Update EagerSessionTest.java", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32961) for more info**.\n\n<!-- need_sender_cla -->", "@ShehanLiyanaarachchi thanks for the PR , i see you removed the comments , is there another changes you are missing ?\r\nAlso please sign CLA.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 32960, "title": "Keras Nadam optmizer generates error when using MirroredStrategy", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18\r\n- TensorFlow installed from (source or binary): Tensorflow 2\r\n- TensorFlow version (use command below) : Tensorflow 2.0.0\r\n- Python version: Python 3.7\r\n- CUDA/cuDNN version: 10\r\n- GPU model and memory: 2 x Nvidia 1080 TI\r\n\r\n**Describe the current behavior**\r\n\r\nThe training crashes with an error( ValueError: You must specify an aggregation method to update a MirroredVariable in Replica Context.) If the model is compiled with the optimizer Nadam (tf.keras.optimizers.Nadam) along with a MirroredStrategy.\r\n\r\n**Describe the expected behavior**\r\n Expect to be able to train with any optimizer from Keras' options.\r\n\r\n", "comments": ["In order to expedite the trouble-shooting process, please provide a minimal standalone code to reproduce the issue reported here. Thanks!", "```\r\nimport tensorflow_datasets as tfds\r\nimport tensorflow as tf\r\ntfds.disable_progress_bar()\r\n\r\nimport os\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\r\n\r\ndatasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)\r\n\r\nmnist_train, mnist_test = datasets['train'], datasets['test']\r\n\r\nmirrored_strategy = tf.distribute.MirroredStrategy()\r\n\r\nprint('Number of devices: {}'.format(mirrored_strategy.num_replicas_in_sync))\r\n\r\n# You can also do info.splits.total_num_examples to get the total\r\n# number of examples in the dataset.\r\n\r\nnum_train_examples = info.splits['train'].num_examples\r\nnum_test_examples = info.splits['test'].num_examples\r\n\r\nBUFFER_SIZE = 100\r\n\r\nBATCH_SIZE_PER_REPLICA = 64\r\nBATCH_SIZE = BATCH_SIZE_PER_REPLICA * mirrored_strategy.num_replicas_in_sync\r\n\r\ndef scale(image, label):\r\n    image = tf.cast(image, tf.float32)\r\n    image /= 255\r\n \r\n    return image, label\r\n\r\ntrain_dataset = mnist_train.map(scale).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\r\neval_dataset = mnist_test.map(scale).batch(BATCH_SIZE)\r\n\r\nwith mirrored_strategy.scope():\r\n    model = tf.keras.Sequential([\r\n        tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\r\n        tf.keras.layers.MaxPooling2D(),\r\n        tf.keras.layers.Flatten(),\r\n        tf.keras.layers.Dense(64, activation='relu'),\r\n        tf.keras.layers.Dense(10, activation='softmax')\r\n    ])\r\n\r\n    model.compile(loss='sparse_categorical_crossentropy',\r\n                  optimizer=tf.keras.optimizers.Nadam(),\r\n                  metrics=['accuracy'])\r\n    model.summary()\r\n\r\nmodel.fit(train_dataset, epochs=5)", "I have tried on colab with TF version 2.0.0-rc2 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/7dbb1f1dd1ecc4a0fba0cc6792bdfb98/untitled247.ipynb). However i am able to reproduce the issue with   `optimizer=tf.keras.optimizers.Nadam` and not seeing any issue if we use `optimizer=tf.keras.optimizers.Adam`.Thanks!", "Hi this issue if fixed [here](https://github.com/tensorflow/tensorflow/commit/1a1c0b6980f1648fe674de3e3b471e78a59143e1), which unfortunately hasn't made its way into the latest release.  It should be there in the next release and already in the nightlies.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32960\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32960\">No</a>\n"]}, {"number": 32959, "title": "Tensorflow to CoreML with tf-coreml: `Retval[26]` error", "body": "**System information**\r\n- Have I written custom code: a mix of both\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary): `pip install tensorflow-gpu`\r\n- TensorFlow version: 1.14.0\r\n- Python version: 3.6.6\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: NVIDIA-SMI 418.87.00 / Driver Version: 418.87.00\r\n\r\n**Describe the current behavior**\r\nI have a multi-input network that uses a `tf.bool` `tf.placeholder` to manage how batch normalization is executed in training and validation / testing.\r\nI\u2019ve been trying to convert this trained model to `CoreML` via `tf-coreml` library with no success, with below error:\r\n`tensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[26] does not have value`\r\nI've also encountered: `NotImplementedError: Unsupported Ops of type: Switch,Merg`\r\n\r\nI understand that this error states that there is a certain node that\u2019s missing a value so the converter can execute the model. I also understand this error is connected to control flow operations (linked to the batch normalization method creating operations like `Switch` and `Merge`). The [source code](https://github.com/tensorflow/tensorflow/blob/1c7993f7c597745e3639b9918c3b3ae2165d4af2/tensorflow/python/kernel_tests/control_flow_ops_py_test.py#L244) shows this:\r\n\r\n```def testSwitchDeadBranch(self):\r\n    with self.cached_session():\r\n      data = constant_op.constant([1, 2, 3, 4, 5, 6], name=\"data\")\r\n      ports = ops.convert_to_tensor(True, name=\"ports\")\r\n      switch_op = control_flow_ops.switch(data, ports)\r\n      dead_branch = array_ops.identity(switch_op[0])\r\n\r\n      with self.assertRaisesWithPredicateMatch(\r\n          errors_impl.InvalidArgumentError,\r\n          lambda e: \"Retval[0] does not have value\" in str(e)):\r\n        self.evaluate(dead_branch)\r\n```\r\nNote that my error is `Retval[26]` (I\u2019ve gotten [24], etc.), not `Retval[0]`. I\u2019m assuming it tests the `Switch` \u201cdead branch\u201d, which should be the non-used branch for inference. The code also does the same with `Merge` \u201cdead branch\u201d.\r\n\r\nIs there any detail I\u2019m missing that may be causing this error (not the first error I\u2019ve faced during conversion, of course)? The way the inference is done? The way batch normalization is implemented? The way the model is saved? \r\n\r\nWhat I\u2019ve done so far:\r\n- I know `tf.layers.batch_normalization` creates operations `Switch` and `Merge`, which are not CoreML compatible\r\n- I\u2019ve tried converting to `Tensorflow Lite` with similar issues\r\n- I\u2019ve follow `Facenet` (this model uses the same `tf.bool` logic for training, validation, testing) conversion process with no success\r\n- I\u2019ve tried the [`GraphTransforms`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md) library\r\n- I\u2019ve tried scripts to remove / modify the control flow \r\n- I\u2019ve created separate graphs to avoid the extra ops with no success\r\n\r\n_Note: I\u2019ve abstracted big part of the code and network to post this issue._\r\n\r\n**Code to reproduce the issue**\r\n\r\nThis is how batch normalization is implemented (within a convolution block):\r\n\r\n```\r\ntraining = tf.placeholder(tf.bool, shape = (), name = 'training')\r\n\r\ndef conv_layer(input, kernelSize, nFilters, poolSize, stride, input_channels = 1, name = 'conv'):\r\n        with tf.name_scope(name):\r\n        shape = [kernelSize, kernelSize, input_channels, nFilters]\r\n        weights = new_weights(shape = shape)        biases = new_biases(length = nFilters)\r\n        conv = tf.nn.conv2d(input, weights, strides = [1, 2, 2, 1], padding = 'SAME', name = 'convL')\r\n        conv += biases\r\n        pool = tf.reduce_max(conv, reduction_indices=[3], keep_dims=True, name = 'pool') \r\n       pool = tf.nn.max_pool(conv, ksize = [1, poolSize, poolSize, 1], strides = shape, padding = 'SAME')\r\n        bnorm = tf.layers.batch_normalization(pool, training = training, center = True, scale = True, fused = False, reuse= False)\r\n        act = tf.nn.relu(bnorm)\r\n        return act\r\n```\r\n\r\nBelow is the code to train and save the model.\r\n\r\n```\r\nsaver = tf.train.Saver()\r\n\r\n    with tf.Session(config = config) as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        sess.run(tf.local_variables_initializer())\r\n        sess.run(init_train_op)\r\n\r\n        for epoch in range(MAX_EPOCHS):\r\n            \r\n            for step in range(10):\r\n                \r\n                l, _, se = sess.run(\r\n                    [loss, train_op, mean_squared_error],\r\n                     feed_dict = {training: True})\r\n\r\n            print('\\nRunning validation operation...')\r\n\r\n            sess.run(init_val_op)\r\n            for _ in range(10):\r\n                val_out, val_l, val_se = sess.run(\r\n                    [out, val_loss, val_mean_squared_error],\r\n                    feed_dict = {training: False})\r\n         \r\n            sess.run(init_train_op) # switch back to training set\r\n\r\n        #Save model\r\n        print('Saving Model...\\n')\r\n        saver.save(sess, join(saveDir, './model_saver_validation'.format(modelIndex)), write_meta_graph = True)\r\n```\r\n\r\nBelow is the code to load, update inputs, perform inference, and freeze the model.\r\n\r\n```\r\n# Dummy data for inference\r\nb = np.zeros((1, 80, 160, 1), np.float32)\r\nill = np.ones((1,3), np.float32)\r\nis_train = False\r\n\r\ndef freeze():\r\n    with tf.Graph().as_default():\r\n        with tf.Session() as sess:\r\n            bIn = tf.placeholder(dtype=tf.float32, shape=[\r\n                             1, 80, 160, 1], name='bIn')\r\n            illumIn = tf.placeholder(dtype=tf.float32, shape=[\r\n                                     1, 3], name='illumIn')\r\n            training = tf.placeholder(tf.bool, shape=(), name = 'training')\r\n\r\n            # Load the model metagraph and checkpoint\r\n            meta_file = meta_graph #.meta file from saver.save()\r\n            ckpt_file = checkpoint_file #checkpoint file\r\n\r\n            # Load graph to redirect inputs from iterator to expected inputs\r\n            saver = tf.train.import_meta_graph(meta_file, input_map={\r\n                'IteratorGetNext:0': bIn,\r\n                'IteratorGetNext:3': illumIn,\r\n                'training:0': training},  clear_devices = True)\r\n            \r\n            tf.get_default_session().run(tf.global_variables_initializer())\r\n            tf.get_default_session().run(tf.local_variables_initializer())\r\n            saver.restore(tf.get_default_session(), ckpt_file)\r\n            \r\n            pred = tf.get_default_graph().get_tensor_by_name('Out:0')\r\n            \r\n            tf.get_default_session().run(pred, feed_dict={'bIn:0': b, 'poseIn:0': po, 'training:0': is_train})\r\n\r\n            # Retrieve the protobuf graph definition and fix the batch norm nodes\r\n            input_graph_def = sess.graph.as_graph_def()\r\n\r\n            # Freeze the graph def\r\n            output_graph_def = freeze_graph_def(\r\n                sess, input_graph_def, output_node_names)\r\n\r\n        # Serialize and dump the output graph to the filesystem\r\n        with tf.gfile.GFile(frozen_graph, 'wb') as f:\r\n            f.write(output_graph_def.SerializeToString())\r\n\r\nfreeze()\r\n```\r\n\r\nBelow is the code to convert to CoreML.\r\n\r\n```\r\ntfcoreml.convert(\r\n    tf_model_path=frozen_graph,\r\n    mlmodel_path='./coreml_model.mlmodel',\r\n    output_feature_names=['Out:0'],\r\n    input_name_shape_dict={\r\n        'bIn:0': [1, 80, 160, 1],\r\n        'illumIn:0': [1, 3], \r\n        'training:0': []})\r\n```\r\n\r\n**Other info / logs**\r\nBelow is the error thrown by `tf-coreml`.\r\n```\r\nLoading the TF graph...\r\nGraph Loaded.\r\nCollecting all the 'Const' ops from the graph, by running it....\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1356, in _do_call\r\n    return fn(*args)\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1341, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1429, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[26] does not have value\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"tf2opencv.py\", line 392, in <module>\r\n    'illumIn:0': [1, 3], 'poseIn:0': [1, 16], 'training:0': []})\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tfcoreml/_tf_coreml_converter.py\", line 586, in convert\r\n    custom_conversion_functions=custom_conversion_functions)\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tfcoreml/_tf_coreml_converter.py\", line 243, in _convert_pb_to_mlmodel\r\n    tensors_evaluated = sess.run(tensors, feed_dict=input_feed_dict)\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 950, in run\r\n    run_metadata_ptr)\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1173, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1350, in _do_run\r\n    run_metadata)\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1370, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[26] does not have value\r\n```\r\n", "comments": ["@danvargg \r\n\r\nCan you please help us with the simple stand alone code to reproduce the issue in our environment.Thanks!", "@ravikyram thanks for the response, I'll post a code between tomorrow and Monday.", "Hi, \r\n\r\nHere's a minimal stand alone code.\r\n\r\nhttps://gist.github.com/danvargg/d5258167e27744d071f7bbb00b329e24", "@danvargg \r\n\r\nLooks like this is not related to tensorflow issue.Can you please confirm.Thanks!", "Hi,\r\n\r\nYes, sorry for not updating earlier. I am handling the issue in [`tf-coreml`](https://github.com/tf-coreml/tf-coreml/issues/342).\r\n\r\nThanks!", "@danvargg \r\nCan we close this issue here and track the issue in [tf-coreml](https://github.com/tf-coreml/tf-coreml/issues/342) ,as this issue is more related to tf-coreml. Please confirm.Thanks!", "Yes, confirmed! Please close this issue."]}, {"number": 32958, "title": "[r1.15-CherryPick]: RebatchDataset performance fix", "body": "- [tf.data] Add a new `RebatchDatasetV2` op that does rebatching (instead of rebatching via graph rewrites) for performance and correctness. \r\n\r\nPiperOrigin-RevId: 268544896", "comments": ["Note that this changes the behavior of some tf.data.Dataset + distribution strategy code; as such, it may break things that rely on the old (less correct) behavior. I'm not sure what all code exists in 1.15 that this might affect, but thought I should give a heads up, given @tfboyd's comment here:\r\n https://github.com/tensorflow/tensorflow/pull/32245#issuecomment-536658498", "Will defer to @goldiegadde as to whether this should be cherry picked."]}, {"number": 32957, "title": "Unability to reach some modules (feature_column and data) with previous working code", "body": "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.6\r\n- TensorFlow installed from (source or binary): pip install tensorflow==2.0.0-beta1\r\n- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d382ca 2.0.0\r\n- Python version: v3.6.7:6ec5cf24b7, Oct 20 2018, 03:02:14\r\n\r\n**Describe the current behavior**\r\n\r\n`from tensorflow.feature_column import numeric_column`and `from tensorflow.data import Dataset` raise `ModuleNotFoundError: No module named 'tensorflow.feature_column'`and `ModuleNotFoundError: No module named 'tensorflow.data'`.\r\n\r\n The workaround `from tensorflow.python.data import Dataset` works but `from tensorflow.python.feature_column import numeric_column` fails and raises `ImportError: cannot import name 'numeric_column'`. To import `numeric_column` (or other functions of the module) I have found this workaround: `from tensorflow.compat.v2.feature_column import numeric_column`.\r\n", "comments": ["@durandg12 ,\r\nThanks for reporting the issue, Can you share a simple and standalone code to reproduce the issue?", "The two import lines are standalone code to reproduce the issue. \r\n\r\nAlso, this is an issue with the released version of tf2, not the beta0. ", "@durandg12 ,\r\nYou can try using like this \r\n```\r\nfrom tensorflow import feature_column\r\nfeature= feature_column.numeric_column\r\n```\r\nand also \r\n```\r\nfrom tensorflow import data\r\nData=data.Dataset\r\n```\r\nThanks!", "@durandg12 ,\r\nAny update on the issue ?Thanks!", "Since there is no new version of TF 2.0 since I posted the issue, it is obviously still here.\r\n\r\nAs you can see in my first post, I had already found some workarounds. My workarounds even success to load `numeric_column` and `Dataset` in the namespace.\r\n\r\nBut it bothers me that I have to change all my previous codes to work around a bug.\r\n\r\nEdit : sorry for the closing/reopening of the issue,  I think I missclicked.\r\n\r\n", "@durandg12 Can you please try this so that you don't need to change much of your old code. Another point is anything under `tf.python.*` is private, intended for development only, rather than public use. Importing from tensorflow.python or any other modules (including import tensorflow_core...) is not supported, and can break unannounced. So, it is suggested not to use anything with tf.python.*.\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import data\r\nDataset = data.Dataset \r\n\r\nfrom tensorflow import feature_column\r\nnumeric_column = feature_column.numeric_column\r\n```\r\n", "Your piece of code works. Nonetheless it seems strange to have to define those kind of aliases at the beginning of each file.\r\n\r\nFurthermore I don't get why my issue is not considered a bug and have the flair \"type:support\" instead. I mean, is it really intended that\r\n```\r\nimport tensorflow\r\n\r\nfc = tensorflow.feature_column.numeric_column('test')\r\n```\r\nworks well,\r\n```\r\nfrom tensorflow import feature_column\r\n\r\nfc = feature_column.numeric_column('test')\r\n```\r\nalso works, but then\r\n```\r\nfrom tensorflow.feature_column import numeric_column\r\n```\r\nthrows me `ModuleNotFoundError: No module named 'tensorflow.feature_column'`? Given that the two previous blocks of code work, I am tempted to assume that there indeed exists a module named `tensorflow.feature_column`. This result is pretty counterintuitive and unnatural. And everything worked until at least the tf2.0.0-rc0 so I don't get the change.\r\n", "That assumption is wrong actually. Consider this example:\r\n\r\n```console\r\nmihaimaruseac@ankh:/tmp/py$ tree\r\n.\r\n\u2514\u2500\u2500 tensorflow\r\n    \u251c\u2500\u2500 __init__.py\r\n    \u2514\u2500\u2500 test.py\r\n```\r\n\r\nWhere `test.py` is:\r\n\r\n```python\r\nclass C:\r\n  def numeric_column(self, s):\r\n    print(s)\r\n\r\nfeature_column=C()\r\n```\r\n\r\nand `__init__.py` is:\r\n\r\n```python\r\nfrom test import feature_column\r\n```\r\n\r\nIn this case:\r\n\r\n```console\r\nmihaimaruseac@ankh:/tmp/py$ python\r\nPython 2.7.16 (default, Apr  6 2019, 01:42:57) \r\n[GCC 7.3.0] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\n>>> tensorflow.feature_column.numeric_column('test')\r\ntest\r\n>>> \r\nmihaimaruseac@ankh:/tmp/py$ python\r\nPython 2.7.16 (default, Apr  6 2019, 01:42:57) \r\n[GCC 7.3.0] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> from tensorflow import feature_column\r\n>>> feature_column.numeric_column('test')\r\ntest\r\n>>> \r\nmihaimaruseac@ankh:/tmp/py$ python\r\nPython 2.7.16 (default, Apr  6 2019, 01:42:57) \r\n[GCC 7.3.0] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> from tensorflow.feature_column import numeric_column\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nImportError: No module named feature_column\r\n>>> \r\n```\r\n\r\nProving that the assumption is wrong.\r\n\r\nIn our case, some of the modules have names attached to them as attributes, not as other modules. This is to provide machinery for deprecation notices (in 1.x), lazy loading, virtual pip, API generation, etc. \r\n\r\nIn fact, the only supported API usage is\r\n\r\n```\r\nimport tensorflow as tf\r\ntf.get.here.all.the.attributes.you.want\r\n```\r\n\r\nAny other API usage can break at any time and fixing it is a best effort job.\r\n", "> In fact, the only supported API usage is\r\n> \r\n> ```\r\n> import tensorflow as tf\r\n> tf.get.here.all.the.attributes.you.want\r\n> ```\r\n> \r\n> Any other API usage can break at any time and fixing it is a best effort job.\r\n\r\nIt is good to know, thank you. But it makes me wonder why other API usages are displayed on a lot of [tensorflow tutorials](https://www.tensorflow.org/tutorials/), e.g. [this basic one on keras](https://www.tensorflow.org/tutorials/keras/classification) or [this quickstart for advanced user](https://www.tensorflow.org/tutorials/quickstart/advanced).\r\n\r\nThanks also for the counterexample on my assumption about modules, I have still lot of things to learn about Python.", "> ```\r\n> from tensorflow.feature_column import numeric_column\r\n> ```\r\n> \r\n> throws me `ModuleNotFoundError: No module named 'tensorflow.feature_column'`?\r\n\r\n@durandg12,\r\nI was able to reproduce the error with [TF v2.0](https://colab.research.google.com/gist/amahendrakar/925cce6dd8e7c8bb568d6aa3511a9c55/32957-2-0.ipynb). However, the issue seems to be fixed with [TF v2.2](https://colab.research.google.com/gist/amahendrakar/6ae2570cee8a170c81fc88ed33318e4a/32957.ipynb).\r\n\r\nPlease check the linked Colab gist and let us know if the issue is resolved. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@amahendrakar I have checked the Colab gists and the issue seems solved indeed, I have also checked that I can import `Dataset` with `from tensorflow.data import Dataset`.\r\n\r\nI never got an answer as to why the only supported API usage would be \r\n```\r\nimport tensorflow as tf\r\ntf.get.here.all.the.attributes.you.want\r\n```\r\nwhen all official tutorials and guides deviate from it.", "@durandg12,\r\n> I never got an answer as to why the only supported API usage would be\r\n\r\nRegarding this please check @mihaimaruseac's [comment](https://github.com/tensorflow/tensorflow/issues/32957#issuecomment-543819065) in this thread and [this](https://github.com/tensorflow/tensorflow/issues/33075#issuecomment-539070546) similar query from another issue. Thanks!", "The comments you are pointing me out do not answer why the \"bad usage\" of not importing from the root is still widely used in official guides and tutorials, [here for example](https://www.tensorflow.org/tutorials/quickstart/advanced).\r\n\r\nI think that keeping such inconsistencies in the guides is likely to mislead users, for no benefit.", "There is a difference between importing from Keras/Estimator (which are separate projects included into TF) and importing from private APIs in TF.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 32956, "title": "Custom Loss in tensorflow 2.0", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nHi, I try to write a custom loss in tensorflow 2.0 RC. But the result of customLoss2 is not equal to sparse_categorical_crossentropy. How can I write it the right way? You can easily copy and run the code in Colab.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\nimport os\r\nimport matplotlib.pyplot as plt\r\nimport tensorflow as tf\r\nprint(\"TensorFlow version: {}\".format(tf.__version__))\r\nprint(\"Eager execution: {}\".format(tf.executing_eagerly()))\r\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D\r\nfrom tensorflow.keras import Model\r\n\r\nmnist = tf.keras.datasets.mnist\r\n\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\nmodel = tf.keras.models.Sequential([\r\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n  tf.keras.layers.Dense(128, activation='relu'),\r\n  tf.keras.layers.Dropout(0.2),\r\n  tf.keras.layers.Dense(10, activation='softmax')\r\n])\r\ndef customLoss2(y_true, y_pred):\r\n    return tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=True))\r\n  \r\ndef customLoss3(y_true, y_pred):\r\n    return tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred))\r\n  \r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy', customLoss2])\r\n\r\nmodel.fit(x_train, y_train, epochs=5)\r\n\r\nmodel.evaluate(x_test, y_test)\r\n```\r\n\r\n**Other info / logs**\r\nTensorFlow version: 2.0.0-rc2\r\nEager execution: True\r\nTrain on 60000 samples\r\nEpoch 1/5\r\n60000/60000 [==============================] - 5s 85us/sample - loss: 0.3025 - accuracy: 0.9114 - customLoss2: 1.6056\r\nEpoch 2/5\r\n60000/60000 [==============================] - 5s 84us/sample - loss: 0.1438 - accuracy: 0.9574 - customLoss2: 1.5303\r\nEpoch 3/5\r\n60000/60000 [==============================] - 5s 79us/sample - loss: 0.1063 - accuracy: 0.9677 - customLoss2: 1.5126\r\nEpoch 4/5\r\n60000/60000 [==============================] - 5s 78us/sample - loss: 0.0889 - accuracy: 0.9730 - customLoss2: 1.5038\r\nEpoch 5/5\r\n60000/60000 [==============================] - 5s 79us/sample - loss: 0.0738 - accuracy: 0.9769 - customLoss2: 1.4975\r\n10000/1 [=====] - 1s 91us/sample - loss: 0.0398 - accuracy: 0.9763 - customLoss2: 1.4932\r\n[0.07908454576081131, 0.9763, 1.4931514]\r\n", "comments": []}, {"number": 32955, "title": "[INTEL MKL] Fix debug_grappler unit test failure", "body": " Fix a bug with debug_grappler, which is related to MklAddN. ", "comments": ["Hi, \r\nThis PR has been approved for a while but not merged.\r\nPlease let me know if I need to do anything.  Because this PR will reduce one MKL test failure, I would like it to be included in TF 2.1.  \r\nThanks,\r\nGZ", "@rthadur Can you please take a look why this PR still hasn't been pulled internally?", "@caisq sure trying again to pull this in, @gzmkl thanks for waiting.", "Thanks!"]}, {"number": 32954, "title": "Keras 2.2.4 Leaks Memory when using Tensorflow 2.0.0", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows and Ubuntu 19.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 7.6.0\r\n- GPU model and memory: Quadro RTX 5000 16Gb\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nAs I fit a custom model using the Keras API. I monitor the memory usage via Task Manager and I see that every .fit() call the memory increases until it eventually crashes the script with no warning whatsoever. It starts off with allocated 5gb of memory and by the time it crashes it as exceeded 16gb of memory.\r\n\r\n**Describe the expected behavior**\r\nI expect the memory to not continuously increase\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```python\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nclass Generator:\r\n    def __init__(self, latent_dim=5, seq_length=30, batch_size=28, hidden_size=100, num_generated_features=1):\r\n        self.latent_dim = latent_dim\r\n        self.seq_length = seq_length\r\n        self.batch_size = batch_size\r\n        self.hidden_size = hidden_size\r\n        self.num_generated_features = num_generated_features\r\n\r\n        # self.model = tf.keras.models.Sequential([\r\n        #     LSTM(self.hidden_size, input_shape=(self.seq_length, self.latent_dim), return_sequences=True),\r\n        #     tf.keras.layers.Dense(1, input_shape=[None, self.hidden_size]),\r\n        #     tf.keras.layers.Activation('tanh'),\r\n        #     Reshape(target_shape=(self.batch_size, self.seq_length, self.num_generated_features))\r\n        # ])\r\n        self.model = tf.keras.models.Sequential([\r\n            tf.keras.layers.LSTM(self.hidden_size, input_shape=(\r\n                self.seq_length, self.latent_dim), return_sequences=True, name='g_lstm1'),\r\n            tf.keras.layers.LSTM(\r\n                self.hidden_size, return_sequences=True, recurrent_dropout=0.4, name='g_lstm2'),\r\n            tf.keras.layers.LSTM(1, return_sequences=True, name='g_lstm3')\r\n        ], name='generator')\r\n\r\n\r\nclass Discriminator:\r\n    def __init__(self, input_shape, hidden_size=100):\r\n        self.model = tf.keras.models.Sequential([\r\n            tf.keras.layers.LSTM(\r\n                hidden_size, input_shape=input_shape, return_sequences=True, name='d_lstm'),\r\n            tf.keras.layers.LSTM(\r\n                hidden_size, return_sequences=True, name='d_lstm2', recurrent_dropout=0.4),\r\n            tf.keras.layers.Dense(1, activation='linear', name='d_output')\r\n        ], name='discriminator')\r\n\r\n        self.model.compile(\r\n            loss=self.d_loss, optimizer=tf.keras.optimizers.SGD(lr=0.1), metrics=['acc'])\r\n\r\n    def d_loss(self, y_true, y_pred):\r\n        loss = tf.keras.losses.binary_crossentropy(\r\n            y_true, y_pred, from_logits=True)\r\n        return loss\r\n\r\n\r\nclass GAN:\r\n    real_loss = []\r\n    fake_loss = []\r\n    def __init__(self, *args, **kwargs):\r\n\r\n        self.generator = Generator(*args, **kwargs)\r\n        gen_output = (self.generator.seq_length,\r\n                      self.generator.num_generated_features)\r\n        self.discriminator = Discriminator(input_shape=gen_output)\r\n        self.discriminator.model.trainable = False\r\n\r\n        self.batch_size = self.generator.batch_size\r\n        self.seq_length = self.generator.seq_length\r\n\r\n        self.model = tf.keras.models.Sequential([\r\n            self.generator.model,\r\n            self.discriminator.model\r\n        ], name='gan')\r\n\r\n        self.model.compile(\r\n            loss=self.gan_loss, optimizer=tf.keras.optimizers.SGD(lr=0.1), metrics=['acc'])\r\n\r\n    def train(self, epochs, n_eval, d_train_steps=5, load_weights=False, metric='loss'):\r\n        for epoch in range(epochs):\r\n            start = time.time()\r\n\r\n            for step in range(steps_over_data):\r\n                tmp_r, tmp_f = [], []\r\n\r\n                for _ in range(d_train_steps):\r\n\r\n                    x_r, y_r = self.generator.real_samples()\r\n                    x_f, y_f = self.generator.fake_samples()\r\n\r\n                    real = self.discriminator.model.fit(\r\n                        x_r, y_r, epochs=1, batch_size=self.batch_size, verbose=0, shuffle=True).history\r\n                    fake = self.discriminator.model.fit(\r\n                        x_f, y_f, epochs=1, batch_size=self.batch_size, verbose=0, shuffle=True).history\r\n\r\n                    tmp_r.append(real[metric])\r\n                    tmp_f.append(fake[metric])\r\n\r\n            self.real_loss.append(np.mean(tmp_r))\r\n            self.fake_loss.append(np.mean(tmp_f))\r\n\r\n            x_gan = self.generator.sample_latent_space()\r\n            y_gan = np.ones((self.batch_size, self.seq_length,\r\n                             self.generator.num_generated_features)).astype(np.float32)\r\n\r\n            self.model.fit(\r\n                x_gan, y_gan, batch_size=self.batch_size, epochs=1, verbose=0)\r\n\r\nif __name__ == '__main__':\r\n    gan = GAN(latent_dim=5, seq_length=30, batch_size=128)\r\n    gan.discriminator.model.summary()\r\n    gan.load_weights()\r\n\r\n    # crashes around epoch ~35\r\n    gan.train(epochs=40, n_eval=1, d_train_steps=3,\r\n              load_weights=True, metric='loss')\r\n\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I can't run the provided example because `gan_loss` isn't defined; could you update the code?", "Just an update: I found similiar issues about keras leaking memory on the net. I still couldn't find exactly what was wrong, but it seemed that the .fit() method doesn't like to be called in loop, this somehow creates things. I was able to partially subdue this problem by calling `tf.keras.backend.clear_session()` however that only slowed the memory leaked, it didn't stop it.\r\n\r\nI managed to fix this problem by replacing .fit() with .train_on_batch() method. Now my model is stable and I can run for n epochs without it crashing. I hope this helped some", "I have similar errors. I have GAN model to generate fake images but after few epochs, program crash due to out of memory error. Error happened at the line of \r\n`g_loss = model.train_on_batch(x, y)`\r\n\r\nMy intuition is when Generator create an image from noise (size is 400) and output of generator is image size of (batch_size, 128, 128, 3).  It is allocating gpu memory in each _**train_on_batch**_ call. After few epochs, it crash due to not enough GPU memories. \r\n\r\nBelow is code to create noise of size 400. \r\n`np.random.multivariate_normal(np.zeros(400), np.eye(400) * 400 / np.sqrt(400), batch_size)`\r\n\r\nMay be I am wrong in understanding the issue. How can I resolve this issue?\r\n", "Hi @humayun, I am having exactly the same isuse, have you been able to find a solution or a workaround ?", "This is due to Tensorflow 2.0 version. Just install new version tensorflow 2.1 and problem go away."]}, {"number": 32952, "title": "ImportError: No module named 'tensorflow_core.estimator'", "body": "Hi, today I updated tensorflow. While trying to run tensorboard, I get the following error:\r\n\r\n> ImportError: No module named 'tensorflow_core.estimator'\r\n\r\n/home/niko/venv_tf2/bin/python /home/niko/workspace/pupil-detection-v2/tensorflow/diagnose_tensorboard.py\r\n### Diagnostics\r\n\r\n<details>\r\n<summary>Diagnostics output</summary>\r\n\r\n``````\r\n--- check: autoidentify\r\nINFO: diagnose_tensorboard.py version 4725c70c7ed724e2d1b9ba5618d7c30b957ee8a4\r\n\r\n--- check: general\r\nINFO: sys.version_info: sys.version_info(major=3, minor=5, micro=2, releaselevel='final', serial=0)\r\nINFO: os.name: posix\r\nINFO: os.uname(): posix.uname_result(sysname='Linux', nodename='helix', release='4.4.0-148-generic', version='#174-Ubuntu SMP Tue May 7 12:20:14 UTC 2019', machine='x86_64')\r\nINFO: sys.getwindowsversion(): N/A\r\n\r\n--- check: package_management\r\nINFO: has conda-meta: False\r\nINFO: $VIRTUAL_ENV: '/home/niko/venv_tf2'\r\n\r\n--- check: installed_packages\r\nINFO: installed: tensorboard==2.0.0\r\nINFO: installed: tensorflow-gpu==2.0.0\r\nINFO: installed: tensorflow-estimator==2.0.0\r\n\r\n--- check: tensorboard_python_version\r\nINFO: tensorboard.version.VERSION: '2.0.0'\r\n\r\n--- check: tensorflow_python_version\r\nINFO: tensorflow.__version__: '2.0.0'\r\nINFO: tensorflow.__git_version__: 'v2.0.0-rc2-26-g64c3d38'\r\n\r\n--- check: tensorboard_binary_path\r\nINFO: which tensorboard: b'/home/niko/venv_tf2/bin/tensorboard\\n'\r\n\r\n--- check: readable_fqdn\r\nINFO: socket.getfqdn(): 'helix'\r\n\r\n--- check: stat_tensorboardinfo\r\nINFO: directory: /tmp/.tensorboard-info\r\nINFO: os.stat(...): os.stat_result(st_mode=16895, st_ino=2356525, st_dev=2066, st_nlink=2, st_uid=1000, st_gid=1000, st_size=4096, st_atime=1569914603, st_mtime=1569939157, st_ctime=1569939157)\r\nINFO: mode: 0o40777\r\n\r\n--- check: source_trees_without_genfiles\r\nINFO: tensorboard_roots (1): ['/home/niko/venv_tf2/lib/python3.5/site-packages']; bad_roots (0): []\r\n\r\n--- check: full_pip_freeze\r\nINFO: pip freeze --all:\r\nabsl-py==0.8.0\r\nastor==0.8.0\r\nattrs==19.1.0\r\nbackcall==0.1.0\r\nbackports.weakref==1.0.post1\r\nbleach==3.1.0\r\ncertifi==2019.6.16\r\nchardet==3.0.4\r\ncolorama==0.4.1\r\ncycler==0.10.0\r\ndecorator==4.4.0\r\ndefusedxml==0.6.0\r\nentrypoints==0.3\r\nenum34==1.1.6\r\ngast==0.2.2\r\ngitdb2==2.0.5\r\nGitPython==3.0.2\r\ngoogle-pasta==0.1.7\r\ngrpcio==1.23.0\r\nh5py==2.9.0\r\nidna==2.8\r\nipykernel==5.1.2\r\nipython==7.7.0\r\nipython-genutils==0.2.0\r\njedi==0.15.1\r\nJinja2==2.10.1\r\njson5==0.8.5\r\njsonschema==3.0.2\r\njupyter-client==5.3.1\r\njupyter-contrib-core==0.3.3\r\njupyter-core==4.5.0\r\njupyter-nbextensions-configurator==0.4.1\r\njupyterlab==1.0.9\r\njupyterlab-git==0.8.1\r\njupyterlab-server==1.0.6\r\nKeras==2.2.5\r\nKeras-Applications==1.0.8\r\nKeras-Preprocessing==1.1.0\r\nkeras-rectified-adam==0.10.0\r\nkiwisolver==1.1.0\r\nMarkdown==3.1.1\r\nMarkupSafe==1.1.1\r\nmatplotlib==3.0.3\r\nmistune==0.8.4\r\nnbconvert==5.6.0\r\nnbdime==1.1.0\r\nnbformat==4.4.0\r\nnotebook==6.0.1\r\nnumpy==1.17.1\r\nopencv-python==4.1.0.25\r\nopt-einsum==3.0.1\r\npandas==0.24.2\r\npandocfilters==1.4.2\r\nparso==0.5.1\r\npexpect==4.7.0\r\npickleshare==0.7.5\r\nPillow==6.1.0\r\npip==19.2.3\r\npkg-resources==0.0.0\r\nprometheus-client==0.7.1\r\nprompt-toolkit==2.0.9\r\nprotobuf==3.9.1\r\nptyprocess==0.6.0\r\nPygments==2.4.2\r\npyparsing==2.4.2\r\npyrsistent==0.15.4\r\npython-dateutil==2.8.0\r\npytz==2019.2\r\nPyYAML==5.1.2\r\npyzmq==18.1.0\r\nrequests==2.22.0\r\nscipy==1.3.1\r\nSend2Trash==1.5.0\r\nsetuptools==41.2.0\r\nsix==1.12.0\r\nsmmap2==2.0.5\r\ntensorboard==2.0.0\r\ntensorflow-estimator==2.0.0\r\ntensorflow-gpu==2.0.0\r\ntermcolor==1.1.0\r\nterminado==0.8.2\r\ntestpath==0.4.2\r\ntornado==6.0.3\r\ntraitlets==4.3.2\r\nurllib3==1.25.3\r\nwcwidth==0.1.7\r\nwebencodings==0.5.1\r\nWerkzeug==0.15.5\r\nwheel==0.33.6\r\nwrapt==1.11.2\r\n\r\n``````\r\n\r\n</details>\r\n\r\n### Next steps\r\n\r\nNo action items identified. Please copy ALL of the above output,\r\nincluding the lines containing only backticks, into your GitHub issue\r\nor comment. Be sure to redact any sensitive information.\r\n\r\nProcess finished with exit code 0\r\n\r\n", "comments": ["Can you post the versions of tensorflow, estimator and tensorboard you have?\r\n\r\nAre you in a virtual env? what operating system? Can you please fill in the issue template?", "I had the same issue, fixed it by uninstalling everything tensorflow related (also `pip uninstall tensorflow-estimator tensorboard tb-nightly tf-estimator-nightly`). A clean install afterwards fixed it.", "@aaronpries , I uninstalled tensorboard, reinstalled estimator and tensorflow and it works fine now. I guess the issue can be closed.", "@nikogamulin I came across the same error. What did you install and uninstall and reinstall can you clarify?\r\n", "Thanks, that solved my problem!"]}]