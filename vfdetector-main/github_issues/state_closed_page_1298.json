[{"number": 14175, "title": "typo fixed in CODE_OF_CONDUCT.md ", "body": "The spelling of behaviour is a non-American variant. For consistency, consider replacing it with the\r\nAmerican English spelling.\r\n\r\nthe word `behaviour` in located at line 16 as `behavior`, so why typing it in two ways?", "comments": ["Can one of the admins verify this patch?"]}, {"number": 14174, "title": "macOS build failed", "body": "macOS 10.13\r\nCUDA9.0 CUDNN7.0\r\nbazel 0.7.0\r\n\r\n\r\n```\r\ntensorflow/core/kernels/split_lib_gpu.cu.cc(122): error: specified alignment (4) is different from alignment (2) specified on a previous declaration\r\n          detected during:\r\n            instantiation of \"void tensorflow::split_v_kernel<T,IntType,useSmem>(const T *, tensorflow::CudaDeviceArrayStruct<IntType, 8>, IntType, IntType, tensorflow::CudaDeviceArrayStruct<T *, 8>) [with T=float, IntType=tensorflow::int32, useSmem=true]\" \r\n(231): here\r\n            instantiation of \"void tensorflow::SplitVOpGPULaunch<T, IntType>::Run(const Eigen::GpuDevice &, __nv_bool, const T *, int, int, const tensorflow::CudaDeviceArrayStruct<IntType, 8> &, const tensorflow::CudaDeviceArrayStruct<T *, 8> &) [with T=float, IntType=tensorflow::int32]\" \r\n(251): here\r\n\r\ntensorflow/core/kernels/split_lib_gpu.cu.cc(122): error: specified alignment (4) is different from alignment (2) specified on a previous declaration\r\n          detected during:\r\n            instantiation of \"void tensorflow::split_v_kernel<T,IntType,useSmem>(const T *, tensorflow::CudaDeviceArrayStruct<IntType, 8>, IntType, IntType, tensorflow::CudaDeviceArrayStruct<T *, 8>) [with T=float, IntType=tensorflow::int32, useSmem=false]\" \r\n(236): here\r\n            instantiation of \"void tensorflow::SplitVOpGPULaunch<T, IntType>::Run(const Eigen::GpuDevice &, __nv_bool, const T *, int, int, const tensorflow::CudaDeviceArrayStruct<IntType, 8> &, const tensorflow::CudaDeviceArrayStruct<T *, 8> &) [with T=float, IntType=tensorflow::int32]\" \r\n(251): here\r\n\r\ntensorflow/core/kernels/split_lib_gpu.cu.cc(122): error: specified alignment (4) is different from alignment (2) specified on a previous declaration\r\n          detected during:\r\n            instantiation of \"void tensorflow::split_v_kernel<T,IntType,useSmem>(const T *, tensorflow::CudaDeviceArrayStruct<IntType, 8>, IntType, IntType, tensorflow::CudaDeviceArrayStruct<T *, 8>) [with T=float, IntType=tensorflow::int64, useSmem=true]\" \r\n(231): here\r\n            instantiation of \"void tensorflow::SplitVOpGPULaunch<T, IntType>::Run(const Eigen::GpuDevice &, __nv_bool, const T *, int, int, const tensorflow::CudaDeviceArrayStruct<IntType, 8> &, const tensorflow::CudaDeviceArrayStruct<T *, 8> &) [with T=float, IntType=tensorflow::int64]\" \r\n(251): here\r\n\r\ntensorflow/core/kernels/split_lib_gpu.cu.cc(122): error: specified alignment (4) is different from alignment (2) specified on a previous declaration\r\n          detected during:\r\n            instantiation of \"void tensorflow::split_v_kernel<T,IntType,useSmem>(const T *, tensorflow::CudaDeviceArrayStruct<IntType, 8>, IntType, IntType, tensorflow::CudaDeviceArrayStruct<T *, 8>) [with T=float, IntType=tensorflow::int64, useSmem=false]\" \r\n(236): here\r\n            instantiation of \"void tensorflow::SplitVOpGPULaunch<T, IntType>::Run(const Eigen::GpuDevice &, __nv_bool, const T *, int, int, const tensorflow::CudaDeviceArrayStruct<IntType, 8> &, const tensorflow::CudaDeviceArrayStruct<T *, 8> &) [with T=float, IntType=tensorflow::int64]\" \r\n(251): here\r\n\r\ntensorflow/core/kernels/split_lib_gpu.cu.cc(122): error: specified alignment (8) is different from alignment (2) specified on a previous declaration\r\n          detected during:\r\n            instantiation of \"void tensorflow::split_v_kernel<T,IntType,useSmem>(const T *, tensorflow::CudaDeviceArrayStruct<IntType, 8>, IntType, IntType, tensorflow::CudaDeviceArrayStruct<T *, 8>) [with T=double, IntType=tensorflow::int32, useSmem=true]\" \r\n(231): here\r\n            instantiation of \"void tensorflow::SplitVOpGPULaunch<T, IntType>::Run(const Eigen::GpuDevice &, __nv_bool, const T *, int, int, const tensorflow::CudaDeviceArrayStruct<IntType, 8> &, const tensorflow::CudaDeviceArrayStruct<T *, 8> &) [with T=double, IntType=tensorflow::int32]\" \r\n(251): here\r\n\r\ntensorflow/core/kernels/split_lib_gpu.cu.cc(122): error: specified alignment (8) is different from alignment (2) specified on a previous declaration\r\n          detected during:\r\n            instantiation of \"void tensorflow::split_v_kernel<T,IntType,useSmem>(const T *, tensorflow::CudaDeviceArrayStruct<IntType, 8>, IntType, IntType, tensorflow::CudaDeviceArrayStruct<T *, 8>) [with T=double, IntType=tensorflow::int32, useSmem=false]\" \r\n(236): here\r\n            instantiation of \"void tensorflow::SplitVOpGPULaunch<T, IntType>::Run(const Eigen::GpuDevice &, __nv_bool, const T *, int, int, const tensorflow::CudaDeviceArrayStruct<IntType, 8> &, const tensorflow::CudaDeviceArrayStruct<T *, 8> &) [with T=double, IntType=tensorflow::int32]\" \r\n(251): here\r\n\r\ntensorflow/core/kernels/split_lib_gpu.cu.cc(122): error: specified alignment (8) is different from alignment (2) specified on a previous declaration\r\n          detected during:\r\n            instantiation of \"void tensorflow::split_v_kernel<T,IntType,useSmem>(const T *, tensorflow::CudaDeviceArrayStruct<IntType, 8>, IntType, IntType, tensorflow::CudaDeviceArrayStruct<T *, 8>) [with T=double, IntType=tensorflow::int64, useSmem=true]\" \r\n(231): here\r\n            instantiation of \"void tensorflow::SplitVOpGPULaunch<T, IntType>::Run(const Eigen::GpuDevice &, __nv_bool, const T *, int, int, const tensorflow::CudaDeviceArrayStruct<IntType, 8> &, const tensorflow::CudaDeviceArrayStruct<T *, 8> &) [with T=double, IntType=tensorflow::int64]\" \r\n(251): here\r\n\r\ntensorflow/core/kernels/split_lib_gpu.cu.cc(122): error: specified alignment (8) is different from alignment (2) specified on a previous declaration\r\n          detected during:\r\n            instantiation of \"void tensorflow::split_v_kernel<T,IntType,useSmem>(const T *, tensorflow::CudaDeviceArrayStruct<IntType, 8>, IntType, IntType, tensorflow::CudaDeviceArrayStruct<T *, 8>) [with T=double, IntType=tensorflow::int64, useSmem=false]\" \r\n(236): here\r\n            instantiation of \"void tensorflow::SplitVOpGPULaunch<T, IntType>::Run(const Eigen::GpuDevice &, __nv_bool, const T *, int, int, const tensorflow::CudaDeviceArrayStruct<IntType, 8> &, const tensorflow::CudaDeviceArrayStruct<T *, 8> &) [with T=double, IntType=tensorflow::int64]\" \r\n(251): here\r\n\r\ntensorflow/core/kernels/split_lib_gpu.cu.cc(122): error: specified alignment (8) is different from alignment (2) specified on a previous declaration\r\n          detected during:\r\n            instantiation of \"void tensorflow::split_v_kernel<T,IntType,useSmem>(const T *, tensorflow::CudaDeviceArrayStruct<IntType, 8>, IntType, IntType, tensorflow::CudaDeviceArrayStruct<T *, 8>) [with T=tensorflow::complex64, IntType=tensorflow::int32, useSmem=true]\" \r\n(231): here\r\n            instantiation of \"void tensorflow::SplitVOpGPULaunch<T, IntType>::Run(const Eigen::GpuDevice &, __nv_bool, const T *, int, int, const tensorflow::CudaDeviceArrayStruct<IntType, 8> &, const tensorflow::CudaDeviceArrayStruct<T *, 8> &) [with T=tensorflow::complex64, IntType=tensorflow::int32]\" \r\n(252): here\r\n\r\ntensorflow/core/kernels/split_lib_gpu.cu.cc(122): error: specified alignment (8) is different from alignment (2) specified on a previous declaration\r\n          detected during:\r\n            instantiation of \"void tensorflow::split_v_kernel<T,IntType,useSmem>(const T *, tensorflow::CudaDeviceArrayStruct<IntType, 8>, IntType, IntType, tensorflow::CudaDeviceArrayStruct<T *, 8>) [with T=tensorflow::complex64, IntType=tensorflow::int32, useSmem=false]\" \r\n(236): here\r\n            instantiation of \"void tensorflow::SplitVOpGPULaunch<T, IntType>::Run(const Eigen::GpuDevice &, __nv_bool, const T *, int, int, const tensorflow::CudaDeviceArrayStruct<IntType, 8> &, const tensorflow::CudaDeviceArrayStruct<T *, 8> &) [with T=tensorflow::complex64, IntType=tensorflow::int32]\" \r\n(252): here\r\n\r\ntensorflow/core/kernels/split_lib_gpu.cu.cc(122): error: specified alignment (8) is different from alignment (2) specified on a previous declaration\r\n          detected during:\r\n            instantiation of \"void tensorflow::split_v_kernel<T,IntType,useSmem>(const T *, tensorflow::CudaDeviceArrayStruct<IntType, 8>, IntType, IntType, tensorflow::CudaDeviceArrayStruct<T *, 8>) [with T=tensorflow::complex64, IntType=tensorflow::int64, useSmem=true]\" \r\n(231): here\r\n            instantiation of \"void tensorflow::SplitVOpGPULaunch<T, IntType>::Run(const Eigen::GpuDevice &, __nv_bool, const T *, int, int, const tensorflow::CudaDeviceArrayStruct<IntType, 8> &, const tensorflow::CudaDeviceArrayStruct<T *, 8> &) [with T=tensorflow::complex64, IntType=tensorflow::int64]\" \r\n(252): here\r\n\r\ntensorflow/core/kernels/split_lib_gpu.cu.cc(122): error: specified alignment (8) is different from alignment (2) specified on a previous declaration\r\n          detected during:\r\n            instantiation of \"void tensorflow::split_v_kernel<T,IntType,useSmem>(const T *, tensorflow::CudaDeviceArrayStruct<IntType, 8>, IntType, IntType, tensorflow::CudaDeviceArrayStruct<T *, 8>) [with T=tensorflow::complex64, IntType=tensorflow::int64, useSmem=false]\" \r\n(236): here\r\n            instantiation of \"void tensorflow::SplitVOpGPULaunch<T, IntType>::Run(const Eigen::GpuDevice &, __nv_bool, const T *, int, int, const tensorflow::CudaDeviceArrayStruct<IntType, 8> &, const tensorflow::CudaDeviceArrayStruct<T *, 8> &) [with T=tensorflow::complex64, IntType=tensorflow::int64]\" \r\n(252): here\r\n\r\ntensorflow/core/kernels/split_lib_gpu.cu.cc(122): error: specified alignment (16) is different from alignment (2) specified on a previous declaration\r\n          detected during:\r\n            instantiation of \"void tensorflow::split_v_kernel<T,IntType,useSmem>(const T *, tensorflow::CudaDeviceArrayStruct<IntType, 8>, IntType, IntType, tensorflow::CudaDeviceArrayStruct<T *, 8>) [with T=tensorflow::complex128, IntType=tensorflow::int32, useSmem=true]\" \r\n(231): here\r\n            instantiation of \"void tensorflow::SplitVOpGPULaunch<T, IntType>::Run(const Eigen::GpuDevice &, __nv_bool, const T *, int, int, const tensorflow::CudaDeviceArrayStruct<IntType, 8> &, const tensorflow::CudaDeviceArrayStruct<T *, 8> &) [with T=tensorflow::complex128, IntType=tensorflow::int32]\" \r\n(253): here\r\n\r\ntensorflow/core/kernels/split_lib_gpu.cu.cc(122): error: specified alignment (16) is different from alignment (2) specified on a previous declaration\r\n          detected during:\r\n            instantiation of \"void tensorflow::split_v_kernel<T,IntType,useSmem>(const T *, tensorflow::CudaDeviceArrayStruct<IntType, 8>, IntType, IntType, tensorflow::CudaDeviceArrayStruct<T *, 8>) [with T=tensorflow::complex128, IntType=tensorflow::int32, useSmem=false]\" \r\n(236): here\r\n            instantiation of \"void tensorflow::SplitVOpGPULaunch<T, IntType>::Run(const Eigen::GpuDevice &, __nv_bool, const T *, int, int, const tensorflow::CudaDeviceArrayStruct<IntType, 8> &, const tensorflow::CudaDeviceArrayStruct<T *, 8> &) [with T=tensorflow::complex128, IntType=tensorflow::int32]\" \r\n(253): here\r\n\r\ntensorflow/core/kernels/split_lib_gpu.cu.cc(122): error: specified alignment (16) is different from alignment (2) specified on a previous declaration\r\n          detected during:\r\n            instantiation of \"void tensorflow::split_v_kernel<T,IntType,useSmem>(const T *, tensorflow::CudaDeviceArrayStruct<IntType, 8>, IntType, IntType, tensorflow::CudaDeviceArrayStruct<T *, 8>) [with T=tensorflow::complex128, IntType=tensorflow::int64, useSmem=true]\" \r\n(231): here\r\n            instantiation of \"void tensorflow::SplitVOpGPULaunch<T, IntType>::Run(const Eigen::GpuDevice &, __nv_bool, const T *, int, int, const tensorflow::CudaDeviceArrayStruct<IntType, 8> &, const tensorflow::CudaDeviceArrayStruct<T *, 8> &) [with T=tensorflow::complex128, IntType=tensorflow::int64]\" \r\n(253): here\r\n\r\ntensorflow/core/kernels/split_lib_gpu.cu.cc(122): error: specified alignment (16) is different from alignment (2) specified on a previous declaration\r\n          detected during:\r\n            instantiation of \"void tensorflow::split_v_kernel<T,IntType,useSmem>(const T *, tensorflow::CudaDeviceArrayStruct<IntType, 8>, IntType, IntType, tensorflow::CudaDeviceArrayStruct<T *, 8>) [with T=tensorflow::complex128, IntType=tensorflow::int64, useSmem=false]\" \r\n(236): here\r\n            instantiation of \"void tensorflow::SplitVOpGPULaunch<T, IntType>::Run(const Eigen::GpuDevice &, __nv_bool, const T *, int, int, const tensorflow::CudaDeviceArrayStruct<IntType, 8> &, const tensorflow::CudaDeviceArrayStruct<T *, 8> &) [with T=tensorflow::complex128, IntType=tensorflow::int64]\" \r\n(253): here\r\n\r\n16 errors detected in the compilation of \"/var/folders/2f/891g9y691gzcy23blt644z6m0000gn/T//tmpxft_00002e92_00000000-6_split_lib_gpu.cu.cpp1.ii\".\r\nERROR: /Users/odin/local/tensorflow/tensorflow/core/kernels/BUILD:387:1: output 'tensorflow/core/kernels/_objs/split_lib_gpu/tensorflow/core/kernels/split_lib_gpu.cu.pic.o' was not created.\r\nERROR: /Users/odin/local/tensorflow/tensorflow/core/kernels/BUILD:387:1: not all outputs were created or valid.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\n```", "comments": ["Same issue here. ", "Same issue here too", "same here", "\ud83d\udc4d ", "same here", "finally build success!!\r\n\r\nGTX 1080\r\n10.13.1\r\nCUDA9.0 CUDNN7.0\r\nbazel 0.7.0\r\n\r\nthe point is delete text '__align__(sizeof(T))'\r\n\r\nconcat_lib_gpu.impl.cu.cc\r\ndepthwise_conv_op_gpu.cu.cc\r\nsplite_lib_gpu.cu.cc\r\n\r\nextern __shared__ __align__(sizeof(T)) unsigned char smem[];\r\n->   extern __shared__ unsigned char smem[];\r\n\r\n\r\n<img width=\"1543\" alt=\"2017-11-06 11 10 08\" src=\"https://user-images.githubusercontent.com/485090/32445438-09cb2a9c-c349-11e7-9f86-67aade41b64a.png\">\r\n\r\n", "@chaoswith what Xcode command line tool version are you using?\r\n", "We only promise compatibility with XCode 7.2 at this time. See the [source install docs](https://www.tensorflow.org/install/install_sources). CC: @gunan \r\n\r\nPTAL @ekelsen We're getting a compiler error about memory alignment, in recent versions of clang, on some GPU sources you worked on.\r\n\r\nIt also seems like [PyTorch](https://github.com/pytorch/pytorch/issues/2692) and a bunch of other projects have recently encountered this same issues. Maybe the compiler got stricter and has helped us identify a suboptimality.\r\n\r\n@chaoswith While removing `__align__` might be a successful workaround, it's probably not a fix we could use in the long run.", "Build success! Thank you @chaoswith ", "The GPU build on macOS is completely community supported now, as there are no \"Officially supported\" systems released by apple that have NVIDIA GPUs.\r\n\r\nBut looks like this issue was resolved anyway?", "I finally got CUDA working on High Sierra, CUDA 9, and cuDNN 7, by downgrading to Xcode 8.3.3, removing the \"align\"s, doing the LD_LIBRARY_PATH modification, and removing \"-lgomp\" from the CUDA makefile. Pain in the butt, but it works!", "I have listed all the steps to build tensorflow 1.4 with CUDA 9 at here https://gist.github.com/smitshilu/53cf9ff0fd6cdb64cca69a7e2827ed0f", "@kanwei I also came across the `ld: library not found for -lgomp` error. Can you please let me know where I should remove the  \"-lgomp\" ?", "@lightingghost inside the file [`third_party/gpus/cuda/BUILD.tpl` line 112](https://github.com/tensorflow/tensorflow/blob/master/third_party/gpus/cuda/BUILD.tpl#L112)", "@norman-thomas Thanks for helping.", "It's not clear to me that the align directive is needed here; might be worth testing internally.", "Closing as the issue is resolved.", "I'm curious why this was closed as \"resolved.\" The issue definitely still exists as of today.  Looking above, there are some serious constructive suggestions for resolving it.", "@elbamos Let's send a pull request with a patch for that?", "The issue described above definitely exists in two source files, still. But I still haven\u2019t gotten a clean install after fixing them. The PR should come from someone who has. If I get there I\u2019ll try to do it, but I don\u2019t know that I will get there.\n\n> On Dec 18, 2018, at 12:03 AM, Samuel Audet <notifications@github.com> wrote:\n> \n> @elbamos Let's send a pull request with a patch for that?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n", "@elbamos Try these commands:\r\nhttps://github.com/bytedeco/javacpp-presets/blob/master/tensorflow/cppbuild.sh#L128\r\nIt builds fine for me: https://travis-ci.org/bytedeco/javacpp-presets/jobs/468862064", "Didn't work for me - I'm stuck on `./tensorflow/contrib/nccl/kernels/nccl_manager.h:30:10: fatal error: 'third_party/nccl/nccl.h' file not found` That's pretty unrelated to this Issue so this isn't the place to discuss it.", "@elbamos clone this (git clone [https://github.com/NVIDIA/nccl.git](https://github.com/NVIDIA/nccl.git)) repo and copy all the files from mccl/src to /tensorflow/third_party/.", "> @elbamos clone this (git clone https://github.com/NVIDIA/nccl.git) repo and copy all the files from mccl/src to /tensorflow/third_party/.\r\n\r\nThanks. It works but need to compile the library before using it for Tensorflow building.\r\n", "@saudet I was ultimately able to get it compiled with your commands. However, the problem arises that then the library paths need to be set in the environment before running python.  This makes it hard to run with jupyter, among other issues.  ", "@saudet I believe the issue is that instead of changing the PATH and DYLD_LIBRARY_PATH at compile time, you need to add the relevant directories to the directives given to the linker...  I used to know the details of this, some years ago.\r\n\r\nHere's some additional information:\r\n```\r\notool -L ./libtensorflow_framework.so\r\n./libtensorflow_framework.so:\r\n\t@rpath/libtensorflow_framework.so (compatibility version 0.0.0, current version 0.0.0)\r\n\t/usr/local/cuda/lib/libcuda.dylib (compatibility version 1.1.0, current version 10.0.130)\r\n\t@rpath/libcudnn.7.dylib (compatibility version 0.0.0, current version 7.3.1)\r\n\t@rpath/libcudart.10.0.dylib (compatibility version 0.0.0, current version 10.0.130)\r\n\t/usr/lib/libSystem.B.dylib (compatibility version 1.0.0, current version 1252.50.4)\r\n\t/System/Library/Frameworks/IOKit.framework/Versions/A/IOKit (compatibility version 1.0.0, current version 275.0.0)\r\n\t/usr/lib/libc++.1.dylib (compatibility version 1.0.0, current version 400.9.0)\r\n\r\notool -L ./_pywrap_tensorflow_internal.so\r\n./_pywrap_tensorflow_internal.so:\r\n\t@rpath/_pywrap_tensorflow_internal.so (compatibility version 0.0.0, current version 0.0.0)\r\n\t@rpath/libtensorflow_framework.so (compatibility version 0.0.0, current version 0.0.0)\r\n\t@rpath/libcublas.10.0.dylib (compatibility version 0.0.0, current version 10.0.130)\r\n\t@rpath/libcusolver.10.0.dylib (compatibility version 0.0.0, current version 10.0.130)\r\n\t@rpath/libcudart.10.0.dylib (compatibility version 0.0.0, current version 10.0.130)\r\n\t/usr/lib/libSystem.B.dylib (compatibility version 1.0.0, current version 1252.50.4)\r\n\t/System/Library/Frameworks/CoreFoundation.framework/Versions/A/CoreFoundation (compatibility version 150.0.0, current version 1454.93.0)\r\n\t/System/Library/Frameworks/Security.framework/Versions/A/Security (compatibility version 1.0.0, current version 58286.70.9)\r\n\t/System/Library/Frameworks/IOKit.framework/Versions/A/IOKit (compatibility version 1.0.0, current version 275.0.0)\r\n\t/usr/lib/libc++.1.dylib (compatibility version 1.0.0, current version 400.9.0)\r\n```\r\n\r\nSo, you can see the problem is the `@rpath` on libcudnn and libcudart.  \r\n\r\nOne way of changing this would be to run `install_name_tool` as \r\n```\r\ninstall_name_tool -change \"@rpath/libcudnn.7.dylib\" \"/usr/local/cuda/lib/libcudnn.7.dylib\" libtensorflow_framework.so\r\ninstall_name_tool -change \"@rpath/libcudart.10.0.dylib\" \"/usr/local/cuda/lib/libcudart.10.0.dylib\" libtensorflow_framework.so\r\n```\r\netc. \r\n\r\nThis *does* work when applied to all 5 cuda library references in the two libraries.  (I suspect it might only need to be done with the `_pywrap_tensorflow_internal.so`, but I haven't checked. \r\n\r\nThere's a comment here: https://github.com/tensorflow/tensorflow/issues/6729#issuecomment-284106845  with an alternative approach.\r\n\r\nHowever, I believe the *correct* thing to do, is to provide proper directives to the linker, and *not* set the environment variables at compile time. \r\n\r\n", "I'm pretty sure the CUDA paths are in the rpath. Check `otool -l`, I'm sure\nthey will show up there.\n", "I just checked, and they're definitely not there.  (If they were there, then we wouldn't have this issue.)\r\n\r\nFor both of them I do see:\r\n\r\n```\r\nLoad command 26\r\n          cmd LC_RPATH\r\n      cmdsize 48\r\n         path ../local_config_cuda/cuda/lib (offset 12)\r\nLoad command 27\r\n          cmd LC_RPATH\r\n      cmdsize 56\r\n         path ../local_config_cuda/cuda/extras/CUPTI/lib (offset 12)\r\n```\r\n\r\nSo it looks like an install tool directive that is improperly replacing a variable. \r\n"]}, {"number": 14173, "title": "using batchnorm in conv2d discard the bias", "body": "Hi!\r\nI observed that whenever I applied batch normalization to conv2d, the bias variable are not created!\r\nversion : Tensorflow 1.3\r\n\r\n```\r\nimport tensorflow\r\n\r\nimage = tf.placeholder(tf.float32, [None, 14, 14, 1024])\r\n\r\ntf.contrib.layers.conv2d(image,\r\n                                 num_outputs=128,\r\n                                 kernel_size=[3,3],\r\n                                 normalizer_fn=tf.layers.batch_normalization,\r\n                                 normalizer_params={\"training\": False, \"reuse\": False},\r\n                                 activation_fn=tf.nn.relu,\r\n                                 reuse=False,\r\n                                 scope=\"conv1\")\r\n                                 \r\ntf.contrib.layers.conv2d(image,\r\n                                 num_outputs=128,\r\n                                 kernel_size=[3,3],\r\n                                 #normalizer_fn=tf.layers.batch_normalization,\r\n                                 #normalizer_params={\"training\": False, \"reuse\": False},\r\n                                 #activation_fn=tf.nn.relu,\r\n                                 reuse=False,\r\n                                 scope=\"conv2\")\r\n                                 \r\nfor v in tf.trainable_variables():\r\n\tprint(v)\r\n```\r\noutput:\r\n\r\n<tf.Variable 'conv1/weights:0' shape=(3, 3, 1024, 128) dtype=float32_ref>\r\n<tf.Variable 'conv1/batch_normalization/beta:0' shape=(128,) dtype=float32_ref>\r\n<tf.Variable 'conv1/batch_normalization/gamma:0' shape=(128,) dtype=float32_ref>\r\n<tf.Variable 'conv2/weights:0' shape=(3, 3, 1024, 128) dtype=float32_ref>\r\n**<tf.Variable 'conv2/biases:0' shape=(128,) dtype=float32_ref>**\r\n\r\nno bias for conv1 !\r\n\r\nQuestion: Is it a bug or a feature :)\r\n\r\n[edit] I also observed the same behavior with mlp.\r\n\r\nThank you very much!\r\n\r\nFlorian\r\n\r\n", "comments": ["Nevermind... it is a feature...\r\nNormalization function to use instead of `biases`. If\r\n      `normalizer_fn` is provided then `biases_initializer` and\r\n      `biases_regularizer` are ignored and `biases` are not created nor added.\r\n      default set to None for no normalizer function", "The batch_normalization already included the addition of the bias term.\r\ncheck out this link https://stackoverflow.com/questions/46256747/can-not-use-both-bias-and-batch-normalization-in-convolution-layers."]}, {"number": 14172, "title": "Eliminate ambiguity", "body": "[batch_size] + shape means the sum of two matrix", "comments": ["Can one of the admins verify this patch?"]}, {"number": 14171, "title": "Memory leak in conv2d()", "body": "### System information\r\n\r\nPython: 3.5.2\r\nTensorFlow version 1.3.0 via pip, 1.3.1 via source, 1.4.0-rc1 from source (confirmed the problem on all three of them)\r\nBazel: 0.7.0\r\nUbuntu 16.04\r\nGPU: GeForce GTX 1080 with 8GB\r\nNvidia drivers: 384.90\r\nCUDA: 8.0\r\nCuDNN: 6.0\r\nOptimizer: Adam with default parameters\r\n\r\n### Problem Description\r\n\r\nI will post here some evidence that makes me think that there's a major leak in `conv_2d()`.\r\nUnfortunately I can't provide code and data replicate this as my employer doesn't allow me to as both are confidential, but hopefully the information i will provide will make it possible for you to replicate it.\r\n\r\nThe setting:\r\nI'm training a word cnn on text data on GPU. The length of the sequence of text is 256, the dimension of the embeddings is 256, I have 4 convolutional layers in parallel with different sizes (2, 3, 4 and 5 x 256). The inputs to the conv layers are `batch_size x 256 x 256 x 1`.\r\nIn my code I run a full epoch of training, then I evaluate on the training set, the validation set and the test set, then I start a new epoch.\r\n\r\nThe problem:\r\nDepending on the batch_size the memory needed by TensorFlow to compute his operations changes, but it is in the order of few hundreds MB. At the last batch of training, the memory consumption increases dramatically, as happens at the last batch of evaluation on validation set and the last batch of evaluation on the test set.\r\nAs _traini_set_size mod batch_size is not 0 (the same happens for validation and test set too), the last batch has a different dimension with respect to all others batches. For instance in my case with a batch_size of 100, the last batch of the validation set is of size 25.\r\nWhat happens is that, in the `conv2d()` function TF allocates a lot of memory if the batch size is not the same that was used so far. In my case, that operation goes from needing 50 MB to 3.18 GB. As it seems too much, I suspect there's a memory leak somehow.\r\n\r\nI'm attaching 2 screenshots taken from TensorBoard. What they show is the same node in the second last batch of the validation set and the last batch of the validation set. The memory consumption is in the node state on the right of the image. I can share the log directory if needed as it contains the graph of the model and the memory information at all steps of training and evaluation.\r\n\r\n![eval_vali_step_8](https://user-images.githubusercontent.com/349256/32309260-99a53ab4-bf47-11e7-96d0-a429180f0e1c.png)\r\n![eval_vali_step_9](https://user-images.githubusercontent.com/349256/32309259-99895934-bf47-11e7-8b3f-08306321cb8e.png)\r\n\r\nThe weird thing is that when batch_size is 256 or 512 it doesn't happen, but with batch_size 100 or 128 or 200 (the other 3 I tested) it happens. testing on another bigger dataset, even with 256 as the batch size the supposed memory leak happens, but in that case TF tries to allocate around 33 GB of ram and goes out of memory. My temporary workaround is just throwing away the last batch of the train, validation and test set. Doing so the memory consumption keeps constant.\r\n\r\nHopefully this is enough information for investigate the problem, otherwise feel free to request me additional information, even if, as I said, unfortunately I can't provide code and data.\r\n\r\n", "comments": ["If you found a leak in conv2d, that would be a very high impact find, because it's such a commonly used feature. While it's less likely there would be a major issue, such a report is always worthy of consideration.\r\n\r\nIs it possible that your shape, filter, stride, sizeof(dtype), etc. are all sort of multiplying together into that huge amount of memory? I've personally always found it to be interesting how quickly tiny integer parameters can turn into all the RAM that exists on my system. I wouldn't be surprised if the same thing has happened to every one of my colleagues.\r\n\r\nIf you think that's unlikely to be the case, then we would probably need a minimal reproducible example in order to investigate further.", "Hey jart, thanks for you answer. (I use you fabulous lib, it\u2019s amazing!)\r\n\r\nI spent 4 days debugging this issue in my code and I ruled out all other possibilities. I looked at the datapoints in the last batches one by one and also reshuffled all the datasets to be sure it was not a data related issue. Unfortunately no, it doesn\u2019t seem to me that it is an issue related with loosing track of the sizes of the tensors, also because the last batch is actually smaller than the other batches, so the operation should actually use less memory and not more.\r\n\r\nI realize that `conv2d()` is second only to `add` and `matmul` in terms of usage, so if no one encountered this issue before it could be that it happens to me because of a weird combination of os+python+gpu+nvidia software version, but I believe it\u2019s worth investigating nonetheless.\r\n\r\nI can try to extract a subset of my codebase and generate some synthetic data with the same dimensions of the offending dataset, but it will require me some time. I wouldn\u2019t be able to do it before the weekend.", "If you liked Fabulous, check out [hiptext](https://github.com/jart/hiptext) which is what came next. It can [render Gangnam Style](https://www.youtube.com/watch?v=sYMczHySmYU) with quasi 512-color unicode half blocks in Terminal.app. @keroserene and I used it to make [rickrollrc](https://github.com/keroserene/rickrollrc). @tombh [plugged hiptext into X11](https://github.com/tombh/texttop) so you can use Google Chrome in your terminal.\r\n\r\nI trust your judgement and appreciate the time you've invested so far. We'll be keeping this one open until at least the weekend. It's worth noting that at Google we have policies for situations like this, where if it's under a certain number of lines, it's less of a concern\u2014especially if it helps the community. So a minimal reproducible example is likely to keep everyone happy. Just be sure to check with your employer.", "Here's a python3 script to reproduce the issue (it's a bit long but I tried to be as faithful to my original setting even if I'm not using a single line of code from my employer):\r\n\r\n```python\r\nimport math\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n# parameters\r\nskip_last = False\r\n\r\ntrain_set_size = 7113\r\nvali_set_size = 925\r\ntest_set_size = 1962\r\n\r\ntext_length = 256\r\nvocab_size = 20000\r\nnum_classes = 500\r\n\r\nepochs = 2\r\nbatch_size = 128\r\n\r\n# create artificial dataset\r\nx_train = np.random.randint(0, vocab_size, [train_set_size, text_length])\r\ny_train = np.random.randint(0, num_classes, [train_set_size])\r\nx_vali = np.random.randint(0, vocab_size, [vali_set_size, text_length])\r\ny_vali = np.random.randint(0, num_classes, [vali_set_size])\r\nx_test = np.random.randint(0, vocab_size, [test_set_size, text_length])\r\ny_test = np.random.randint(0, num_classes, [test_set_size])\r\n\r\n\r\n# Building model\r\ndef fc_layer(inputs, in_count, out_count, act=\"relu\"):\r\n    weights = tf.get_variable(\"weights\", [in_count, out_count])\r\n    bias = tf.get_variable(\"bias\", [out_count])\r\n    layer = tf.matmul(inputs, weights) + bias\r\n    if act:\r\n        return getattr(tf.nn, act)(layer)\r\n    else:\r\n        return layer\r\n\r\n\r\ndef conv_layer(inputs, kernel_shape, bias_shape, stride=1, act=\"relu\", padding='VALID', dropout=None, is_training=True):\r\n    weights = tf.get_variable(\"weights\", kernel_shape)\r\n    conv = tf.nn.conv2d(inputs, weights,\r\n                        strides=[1, stride, stride, 1], padding=padding)\r\n    layer = conv\r\n    if act:\r\n        layer = getattr(tf.nn, act)(layer)\r\n    if dropout is not None:\r\n        layer = tf.layers.dropout(layer, rate=dropout, training=is_training)\r\n    return layer\r\n\r\n\r\ndef word_cnn(input_text,\r\n             vocab_size,\r\n             dropout,\r\n             embedding_size=256,\r\n             filter_sizes=(2, 3, 4, 5),\r\n             num_filters=(256, 256, 256, 256),\r\n             fully_connected_sizes=(512, 256),\r\n             regularize_layers=(False, True, True),\r\n             embeddings_oon_cpu=False,\r\n             is_training=True,\r\n             name=None,\r\n             **kwargs):\r\n    assert len(filter_sizes) == len(num_filters)\r\n    assert len(regularize_layers) == len(fully_connected_sizes) + 1\r\n\r\n    sequence_length = input_text.get_shape()[-1]\r\n\r\n    with tf.variable_scope(\"word_cnn_{}\".format(name)):\r\n\r\n        # ================ Word Embeddings ================\r\n        if embeddings_oon_cpu:\r\n            with tf.device('/cpu:0'):\r\n                word_embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\r\n                                              name='word_embeddings')\r\n                initial_input = tf.nn.embedding_lookup(word_embeddings, input_text, name='embeddings_lookup')\r\n        else:\r\n            word_embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\r\n                                          name='word_embeddings')\r\n            initial_input = tf.nn.embedding_lookup(word_embeddings, input_text, name='embeddings_lookup')\r\n        curr_input = tf.expand_dims(initial_input, -1)\r\n\r\n        # ================ Conv Layers ================\r\n        parallel_conv_layers = []\r\n        for i in range(len(filter_sizes)):\r\n            curr_filter_size = filter_sizes[i]\r\n            curr_num_filters = num_filters[i]\r\n            with tf.variable_scope(\"conv_{}\".format(curr_filter_size)):\r\n                filter_shape = [curr_filter_size, embedding_size, 1, curr_num_filters]\r\n                layer_output = conv_layer(curr_input, filter_shape, [curr_num_filters],\r\n                                          is_training=is_training, dropout=None)\r\n                layer_output = tf.nn.max_pool(\r\n                    layer_output,\r\n                    ksize=[1, sequence_length - curr_filter_size + 1, 1, 1],\r\n                    strides=[1, 1, 1, 1],\r\n                    padding='VALID',\r\n                    name=\"pool_{}\".format(curr_filter_size))\r\n                parallel_conv_layers.append(layer_output)\r\n\r\n        # Flatten to vector\r\n        num_filters_total = sum(num_filters)\r\n        hidden = tf.reshape(tf.concat(parallel_conv_layers, 3), [-1, num_filters_total], name='flatten')\r\n        hidden_size = num_filters_total\r\n\r\n        # ================ Fully Connected Layers ================\r\n        num_fully_connected_layers = len(fully_connected_sizes)\r\n        for i in range(num_fully_connected_layers):\r\n            fully_connected_size = fully_connected_sizes[i]\r\n            with tf.variable_scope(\"fc_{}\".format(i)):\r\n                if dropout is not None:\r\n                    with tf.name_scope(\"dropout\"):\r\n                        hidden = tf.layers.dropout(hidden, rate=dropout, training=is_training)\r\n\r\n                # Fully Connected Layer\r\n                hidden = fc_layer(hidden, hidden_size, fully_connected_size)\r\n                hidden_size = fully_connected_size\r\n\r\n    return hidden, hidden_size\r\n\r\n\r\nis_training = tf.placeholder(tf.bool, [], name='is_training')\r\nglobal_step = tf.Variable(0, trainable=False)\r\ndropout = tf.placeholder(tf.float32, name=\"dropout\")\r\n\r\ninput_placeholder = tf.placeholder(tf.int32, [None, text_length], name=\"input_placeholder\")\r\nlabel_placeholder = tf.placeholder(tf.int64, [None], name=\"label_placeholder\")\r\n\r\nhidden, hidden_size = word_cnn(input_placeholder, vocab_size, dropout=dropout, is_training=is_training, name=\"text\")\r\nsoftmax_w = tf.get_variable(\"softmax_weights\", [hidden_size, num_classes])\r\nsoftmax_b = tf.get_variable(\"softmax_bias\", [num_classes])\r\nlogits = tf.matmul(hidden, softmax_w) + softmax_b\r\n\r\nprobabilities = tf.nn.softmax(logits, name=\"probabilities\")\r\npredictions = tf.argmax(logits, 1, name=\"predictions\")\r\nwith tf.device('/cpu:0'):\r\n    top_k_predictions = tf.nn.top_k(logits, k=3, sorted=True, name=\"top_k_predictions\")\r\n\r\ncorrect_predictions = tf.equal(predictions, label_placeholder, name=\"correct_predictions\")\r\nwith tf.device('/cpu:0'):\r\n    hits_at_k = tf.nn.in_top_k(logits, label_placeholder, 3, name=\"hits_at_k\")\r\n\r\nonehot_labels = tf.one_hot(label_placeholder, num_classes, name=\"onehot_labels\")\r\nloss = tf.losses.softmax_cross_entropy(onehot_labels=onehot_labels, logits=logits)\r\n\r\noptimize = tf.train.AdamOptimizer(0.0025).minimize(loss)\r\n\r\n\r\n# training\r\ndef convert_size(size_bytes):\r\n    if size_bytes == 0:\r\n        return \"0B\"\r\n    size_name = (\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\")\r\n    i = int(math.floor(math.log(size_bytes, 1024)))\r\n    p = math.pow(1024, i)\r\n    s = round(size_bytes / p, 2)\r\n    return \"{} {}\".format(s, size_name[i])\r\n\r\n\r\nclass Batcher(object):\r\n    def __init__(self, dataset, batch_size, skip_last=False):\r\n        self.dataset = dataset\r\n        self.batch_size = batch_size\r\n        self.total_size = min(map(len, dataset.values()))\r\n        self.skip_last = skip_last\r\n        self.index = 0\r\n\r\n    def next_batch(self):\r\n        if self.last_batch():\r\n            self.index = 0\r\n        sub_batch = {}\r\n        for key in self.dataset:\r\n            sub_batch[key] = self.dataset[key][self.index:self.index + self.batch_size]\r\n        self.index += self.batch_size\r\n        return sub_batch\r\n\r\n    def last_batch(self):\r\n        return self.index >= self.total_size or (self.skip_last and self.index + self.batch_size >= self.total_size)\r\n\r\n\r\ndef batch_eval(session, dataset, batch_size, epoch, name):\r\n    batcher = Batcher(dataset, batch_size, skip_last)\r\n    tot_size = batcher.total_size\r\n    losses = []\r\n    correct_preds = []\r\n    hits_at_ks = []\r\n    step = 0\r\n    while not batcher.last_batch():\r\n        batch = batcher.next_batch()\r\n        loss_val, correct_predictions_val, hits_at_k_val = session.run(\r\n            [loss, correct_predictions, hits_at_k],\r\n            feed_dict={input_placeholder: batch['x'], label_placeholder: batch['y'], dropout: 0.3, is_training: True}\r\n        )\r\n        losses.append(loss_val)\r\n        correct_preds.append(correct_predictions_val)\r\n        hits_at_ks.append(hits_at_k_val)\r\n        print('epoch', epoch, name, 'eval step', step)\r\n        mbiu = session.run(tf.contrib.memory_stats.MaxBytesInUse())\r\n        print('mbiu', convert_size(mbiu))\r\n        step += 1\r\n    losses = np.array(losses)\r\n    correct_preds = np.concatenate(correct_preds)\r\n    hits_at_ks = np.concatenate(hits_at_ks)\r\n    return losses.sum() / tot_size, correct_preds.sum() / tot_size, hits_at_ks.sum() / tot_size\r\n\r\n\r\nwith tf.Session() as session:\r\n    tf.global_variables_initializer().run()\r\n\r\n    epoch = 0\r\n    step = 0\r\n    train_batcher = Batcher({'x': x_train, 'y': y_train}, batch_size, skip_last)\r\n\r\n    while epoch < epochs:\r\n        if step == 0:\r\n            print(\"Epoch {}\".format(epoch + 1))\r\n        batch = train_batcher.next_batch()\r\n        _, loss_val = session.run(\r\n            [optimize, loss],\r\n            feed_dict={input_placeholder: batch['x'], label_placeholder: batch['y'], dropout: 0.3, is_training: True}\r\n        )\r\n        print('epoch', epoch + 1, 'step', step + 1, 'loss', loss_val)\r\n        mbiu = session.run(tf.contrib.memory_stats.MaxBytesInUse())\r\n        print('mbiu', convert_size(mbiu))\r\n        step += 1\r\n\r\n        if train_batcher.last_batch():\r\n            epoch += 1\r\n            step = 0\r\n\r\n            train_loss, train_acc, train_hits = batch_eval(session, {'x': x_train, 'y': y_train}, batch_size, epoch,\r\n                                                           'train')\r\n            print('train_loss', train_loss, 'train_acc', train_acc, 'train_hits', train_hits)\r\n\r\n            vali_loss, vali_acc, vali_hits = batch_eval(session, {'x': x_vali, 'y': y_vali}, batch_size, epoch, 'vali')\r\n            print('vali_loss', vali_loss, 'vali_acc', vali_acc, 'vali_hits', vali_hits)\r\n\r\n            test_loss, test_acc, test_hits = batch_eval(session, {'x': x_test, 'y': y_test}, batch_size, epoch, 'test')\r\n            print('test_loss', test_loss, 'test_acc', test_acc, 'test_hits', test_hits)\r\n\r\n```\r\n\r\nYou can play with `skip_last`. Setting it to `False` instructs the batcher to create the last batch with size less than `batch_size` if `dataset_size % batch_size > 0`. In this case you can see the leak happening in the last batch of the validation set evaluation. Memory consuption goes suddenly from 461.72 MB to 7.41 GB. See a log here:\r\n\r\n```\r\n2017-11-02 21:18:28.606838: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:\r\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\r\npciBusID: 0000:04:00.0\r\ntotalMemory: 7.92GiB freeMemory: 7.80GiB\r\n2017-11-02 21:18:28.606864: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:04:00.0, compute capability: 6.1)\r\nEpoch 1\r\nepoch 1 step 1 loss 6.22924\r\nmbiu 461.85 MB\r\nepoch 1 step 2 loss 6.27471\r\nmbiu 461.85 MB\r\nepoch 1 step 3 loss 6.37816\r\nmbiu 461.85 MB\r\n...\r\nepoch 1 step 55 loss 6.21585\r\nmbiu 461.85 MB\r\nepoch 1 step 56 loss 6.24239\r\nmbiu 461.85 MB\r\nepoch 1 train eval step 0\r\nmbiu 461.85 MB\r\nepoch 1 train eval step 1\r\nmbiu 461.85 MB\r\nepoch 1 train eval step 2\r\nmbiu 461.85 MB\r\nepoch 1 train eval step 3\r\nmbiu 461.85 MB\r\n...\r\nepoch 1 train eval step 53\r\nmbiu 461.85 MB\r\nepoch 1 train eval step 54\r\nmbiu 461.85 MB\r\nepoch 1 train eval step 55\r\nmbiu 461.85 MB\r\ntrain_loss 0.048836645724 train_acc 0.00281175312808 train_hits 0.00955996063546\r\nepoch 1 vali eval step 0\r\nmbiu 461.85 MB\r\nepoch 1 vali eval step 1\r\nmbiu 461.85 MB\r\nepoch 1 vali eval step 2\r\nmbiu 461.85 MB\r\nepoch 1 vali eval step 3\r\nmbiu 461.85 MB\r\nepoch 1 vali eval step 4\r\nmbiu 461.85 MB\r\nepoch 1 vali eval step 5\r\nmbiu 461.85 MB\r\nepoch 1 vali eval step 6\r\nmbiu 461.85 MB\r\nepoch 1 vali eval step 7\r\nmbiu 7.41 GB\r\nvali_loss 0.0538858032227 vali_acc 0.00108108108108 vali_hits 0.00216216216216\r\nepoch 1 test eval step 0\r\nmbiu 7.41 GB\r\nepoch 1 test eval step 1\r\nmbiu 7.41 GB\r\n...\r\nepoch 1 test eval step 13\r\nmbiu 7.41 GB\r\nepoch 1 test eval step 14\r\nmbiu 7.41 GB\r\nepoch 1 test eval step 15\r\nmbiu 7.41 GB\r\ntest_loss 0.0507940473177 test_acc 0.00254841997961 test_hits 0.00917431192661\r\nEpoch 2\r\nepoch 2 step 1 loss 6.18541\r\nmbiu 7.41 GB\r\nepoch 2 step 2 loss 6.19003\r\nmbiu 7.41 GB\r\nepoch 2 step 3 loss 6.18152\r\nmbiu 7.41 GB\r\n...\r\nepoch 2 test eval step 14\r\nmbiu 7.41 GB\r\nepoch 2 test eval step 15\r\nmbiu 7.41 GB\r\ntest_loss 0.0509010027186 test_acc 0.00254841997961 test_hits 0.00560652395515\r\n```\r\n\r\nIf you set `skip_last` to `True` the last betch size with a different size from the usual `batch_size` will be skipped. As you can see from this log the memory consuption stay constant at 461.72 MB from beginning to the end.\r\n\r\n```\r\n2017-11-02 21:14:45.784362: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:\r\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\r\npciBusID: 0000:04:00.0\r\ntotalMemory: 7.92GiB freeMemory: 7.80GiB\r\n2017-11-02 21:14:45.784395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:04:00.0, compute capability: 6.1)\r\nEpoch 1\r\nepoch 1 step 1 loss 6.22736\r\nmbiu 461.72 MB\r\nepoch 1 step 2 loss 6.27873\r\nmbiu 461.72 MB\r\nepoch 1 step 3 loss 6.32521\r\nmbiu 461.72 MB\r\nepoch 1 step 4 loss 6.34492\r\nmbiu 461.72 MB\r\nepoch 1 step 5 loss 6.33192\r\nmbiu 461.72 MB\r\nepoch 1 step 6 loss 6.27755\r\nmbiu 461.72 MB\r\n...\r\nepoch 2 test eval step 11\r\nmbiu 461.78 MB\r\nepoch 2 test eval step 12\r\nmbiu 461.78 MB\r\nepoch 2 test eval step 13\r\nmbiu 461.78 MB\r\nepoch 2 test eval step 14\r\nmbiu 461.78 MB\r\ntest_loss 0.0475411449125 test_acc 0.00101936799185 test_hits 0.00407747196738\r\n```\r\n\r\nIn the case of this example, the leaked memory is still within the 8 GB limit of my GPU so it doesn't go out of memory, but I have a bigger dataset and using it TF at the last batch of validation tries to allocate 33 GB and goes out of memory. To replicate that setting you should set the parameters at the beginning of the script in this way:\r\n\r\n```\r\ntrain_set_size = 2896257\r\nvali_set_size = 97713\r\ntest_set_size = 97702\r\n```\r\n\r\nHope this is enough.", "I just tested it on v1.4.0 and the problem is still there.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "I'm currently trying to run your code on a Windows 10 machine, with 2 NVidia GeForce GTX 1070.\r\nThe [memory_stats](https://github.com/tensorflow/tensorflow/issues/15412) op is not yet available on windows, so I used the following strategy to get an estimate of used VRAM, could you please comment if this approach is correct ?\r\n\r\nI have removed the mem_stats op, and allowed growth of GPU memory allocation with\r\n\r\n```\r\ngpu_options = tf.GPUOptions(allow_growth=True)\r\nwith tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as session:\r\n```\r\n\r\nThen using a tool like GPU-Z, I'm plotting GPU usage. Of course this gives me the memory allocated by TF, not the amount it is truly using (which should be less than that I guess).\r\n\r\nThe GPU `Memory Used` is at 352 MB at rest, after starting the script it climbs up to 1623MB (if you remove the initial 352MB, that leaves approximately ~ 1.2GB allocated by TF).\r\n\r\n![image](https://user-images.githubusercontent.com/1294805/34879708-68927fe6-f7ae-11e7-83d7-869f988c4edb.png)\r\n\r\nThe script has not yet reached the last batch. How long does it take approximately to complete ?\r\n\r\n### Edit\r\nIt's now in epoch 2 and VRAM is still at 1623MB\r\n\r\n![image](https://user-images.githubusercontent.com/1294805/34881545-251cef84-f7b4-11e7-96d0-f368b284217f.png)\r\n", "Yours is a totally different setting: different OS, different drivers, different card, different way of calculating ram, so I don't really know if that's telling anything. My best guess is that the reason for the leak i am experiencing has to do with CUDA / CuDNN for linux, so if that's true testing on Windows wouldn't help debugging unfortunately.\r\nAlso, 1.2GB seems a lot of ram for this, in my case, when it doesn't leak, the script uses just 461MB. I have no idea why this is the case unfortunately. A tensorflower would be helpful here :)", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Closing due to inactivity. If still a problem let us know and I'll reopen.", "Confirm that it's still an issue. Tested it with TensorFlow 1.5 with python 3.5 on Ubuntu 16.04 with Nvidia driver 390.12 cuda 9.1 and cuDNN 7. The leak is still there, even if now instead of taking 7G of VRAM it takes only 3.8G.\r\n\r\n```\r\n...\r\nepoch 1 vali eval step 6\r\nmbiu 472.89 MB\r\nepoch 1 vali eval step 7\r\nmbiu 3.8 GB\r\n...\r\n```\r\n\r\nIt really seems like an urgent thing to fix to me, conv2d is likely the most used function after add and multiply.", "I was hoping to see more signal boost from the community by now. But due to the seriousness of the concerns being raised here, I'd ask @zheng-xq please take another look?", "Unfortunately no one has the bandwidth to look at this right now. @w4nderlust, if you're able, paring down the script you provided to a shorter repro would be extremely helpful. I'm gonna mark contributions welcome for now since we're not actively working on it.", "Hey @skye, here's a much shorter version of the script, only 90 lines:\r\n\r\n```python\r\nimport math\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n# parameters\r\nskip_last = False\r\n\r\nsequence_length = 256\r\nvocab_size = 20000\r\nnum_classes = 500\r\ntrain_set_size = 925\r\nepochs = 2\r\nbatch_size = 128\r\n\r\nembedding_size = 256\r\nfilter_size = 5\r\nnum_filters = 256\r\n\r\n# create artificial dataset\r\nx_train = np.random.rand(train_set_size, sequence_length, embedding_size, 1)\r\ny_train = np.random.randint(0, num_classes, [train_set_size])\r\n\r\n# Model\r\ninput_placeholder = tf.placeholder(tf.float32, [None, sequence_length, embedding_size, 1])\r\nlabel_placeholder = tf.placeholder(tf.int64, [None])\r\nfilter_shape = [filter_size, embedding_size, 1, num_filters]\r\nweights = tf.get_variable(\"conv_w\", filter_shape)\r\nconv = tf.nn.conv2d(input_placeholder, weights, [1, 1, 1, 1], 'VALID')\r\nconv = tf.nn.relu(conv)\r\npooled = tf.nn.max_pool(conv, [1, sequence_length - filter_size + 1, 1, 1], [1, 1, 1, 1], 'VALID')\r\nhidden = tf.reshape(pooled, [-1, num_filters])\r\nsoftmax_w = tf.get_variable(\"softmax_w\", [num_filters, num_classes])\r\nlogits = tf.matmul(hidden, softmax_w)\r\nprobabilities = tf.nn.softmax(logits)\r\npredictions = tf.argmax(logits, 1)\r\ncorrect_predictions = tf.equal(predictions, label_placeholder)\r\nloss = tf.losses.sparse_softmax_cross_entropy(labels=label_placeholder, logits=logits)\r\noptimize = tf.train.AdamOptimizer(0.0025).minimize(loss)\r\n\r\n# training\r\ndef convert_size(size_bytes):\r\n    if size_bytes == 0:\r\n        return \"0B\"\r\n    size_name = (\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\")\r\n    i = int(math.floor(math.log(size_bytes, 1024)))\r\n    p = math.pow(1024, i)\r\n    s = round(size_bytes / p, 2)\r\n    return \"{} {}\".format(s, size_name[i])\r\n\r\nclass Batcher(object):\r\n    def __init__(self, x, y, batch_size, skip_last=False):\r\n        assert len(x) == len(y)\r\n        self.x = x\r\n        self.y = y\r\n        self.batch_size = batch_size\r\n        self.total_size = len(x)\r\n        self.skip_last = skip_last\r\n        self.index = 0\r\n\r\n    def next_batch(self):\r\n        if self.last_batch():\r\n            self.index = 0\r\n        x_batch = self.x[self.index:self.index + self.batch_size]\r\n        y_batch = self.y[self.index:self.index + self.batch_size]\r\n        self.index += self.batch_size\r\n        return x_batch, y_batch\r\n\r\n    def last_batch(self):\r\n        return self.index >= self.total_size or (self.skip_last and self.index + self.batch_size >= self.total_size)\r\n\r\nwith tf.Session() as session:\r\n    tf.global_variables_initializer().run()\r\n    epoch = 0\r\n    step = 0\r\n    batcher = Batcher(x_train, y_train, batch_size, skip_last)\r\n    while epoch < epochs:\r\n        batch_x, batch_y = batcher.next_batch()\r\n        _, loss_val = session.run(\r\n            [optimize, loss],\r\n            feed_dict={input_placeholder: batch_x, label_placeholder: batch_y}\r\n        )\r\n        mbiu = session.run(tf.contrib.memory_stats.MaxBytesInUse())\r\n        print('epoch: {:1d} step: {:2d} loss: {:.4f} mbiu: {}'.format(\r\n            epoch + 1, step + 1, loss_val, convert_size(mbiu)))\r\n        if batcher.last_batch():\r\n            epoch += 1\r\n            step = 0\r\n        else:\r\n            step += 1\r\n```\r\n\r\nHere is the output I'm getting from it:\r\n```\r\nepoch: 1 step:  1 loss: 6.2207 mbiu: 260.71 MB\r\nepoch: 1 step:  2 loss: 6.6852 mbiu: 260.71 MB\r\nepoch: 1 step:  3 loss: 6.5704 mbiu: 260.71 MB\r\nepoch: 1 step:  4 loss: 6.4245 mbiu: 260.71 MB\r\nepoch: 1 step:  5 loss: 6.2299 mbiu: 260.71 MB\r\nepoch: 1 step:  6 loss: 6.2337 mbiu: 260.71 MB\r\nepoch: 1 step:  7 loss: 6.2340 mbiu: 260.71 MB\r\nepoch: 1 step:  8 loss: 6.2334 mbiu: 3.7 GB\r\nepoch: 2 step:  1 loss: 6.2051 mbiu: 3.7 GB\r\nepoch: 2 step:  2 loss: 6.1781 mbiu: 3.7 GB\r\nepoch: 2 step:  3 loss: 6.1723 mbiu: 3.7 GB\r\nepoch: 2 step:  4 loss: 6.1483 mbiu: 3.7 GB\r\nepoch: 2 step:  5 loss: 6.1409 mbiu: 3.7 GB\r\nepoch: 2 step:  6 loss: 6.2119 mbiu: 3.7 GB\r\nepoch: 2 step:  7 loss: 6.2550 mbiu: 3.7 GB\r\nepoch: 2 step:  8 loss: 6.1255 mbiu: 3.7 GB\r\n```\r\n\r\nAs you can see the memory usage increases A LOT, more than 10x the regular usage, at the end of the epoch, when a sample with a different batch size is pushed through the graph. If I set `skip_last` to `True`, meaning the last and shorter batch is skipped, the memory usage keeps constant. This suggests there's a memory leak in `conv2d()` when dealing with batch sizes that are different than the initial one.\r\nAs this scenario is ubiquitous, it seems to me that this should be a pretty critical thing to fix rather than wait for community contributions.", "@reedwm can you take a look?", "Here is a simpler example the reproduces the problem\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nsequence_length = 256\r\nembedding_size = 256\r\nfilter_size = 5\r\nnum_filters = 256\r\n\r\n# Model\r\ninput_placeholder = tf.placeholder(tf.float32, [None, sequence_length, embedding_size, 1])\r\nlabel_placeholder = tf.placeholder(tf.int64, [None])\r\nfilter_shape = [filter_size, embedding_size, 1, num_filters]\r\nweights = tf.get_variable(\"conv_w\", filter_shape)\r\nconv = tf.nn.conv2d(input_placeholder, weights, [1, 1, 1, 1], 'VALID')\r\n\r\nwith tf.Session() as session:\r\n  tf.global_variables_initializer().run()\r\n  sizes = [128] * 2 + [29] + [128] * 2\r\n  for size in sizes:\r\n    batch_x = np.zeros([size, sequence_length, embedding_size, 1])\r\n    batch_y = np.zeros([size])\r\n    loss_val = session.run(\r\n        conv.op,\r\n        feed_dict={input_placeholder: batch_x, label_placeholder: batch_y}\r\n    )\r\n    mbiu, biu = session.run([tf.contrib.memory_stats.MaxBytesInUse(),\r\n                             tf.contrib.memory_stats.BytesInUse()])\r\n    print('mbiu: {} MB, bui: {} MB'.format(mbiu / 2**20, biu / 2**20))\r\n```\r\n\r\nThe output is:\r\n\r\n```\r\nmbiu: 255 MB, bui: 1 MB\r\nmbiu: 255 MB, bui: 1 MB\r\nmbiu: 3766 MB, bui: 1 MB\r\nmbiu: 3766 MB, bui: 1 MB\r\nmbiu: 3766 MB, bui: 1 MB\r\n```\r\n\r\nThis is no memory leak, because the current bytes-in-use always goes back to 1MB after the session finishes running. But it is weird how running a single step with a batch-size of 29 causes future steps with a batch size of 128 to use more memory.\r\n\r\n@yzhwang can you take a look? My post has all the info of the root problem, so I don't think you need to read the previous posts. Is this an issue with autotune?", "Actually I just realized that MaxBytesInUse stays at 3766 MB, since it is never reset, so once it reaches 3766 MB it will never go down.\r\n\r\nSo, it seems the issue is that a batch size of 29 uses more memory than a batch size of 128. My guess is that with a batch size of 29, the faster algorithm is different than with a batch size of 128. The batch-size-29 algorithm probably uses a lot more memory than the batch-size-128 algorithm, which explains the difference\r\n\r\n@yzhwang can you confirm?", "Yes, I tested with TensorFlow 1.5.0. By turning off autotune with this code:\r\n`os.environ[\"TF_CUDNN_USE_AUTOTUNE\"] = \"0\"`\r\npeak memory drops to 97.5MB, otherwise I get the same result as @reedwm .\r\n\r\nThe reason for this is that currently, when we use autotune to pick the best algorithm, we use running time as the only metrics. We should start considering memory consumption too. This is on my todo list, and I will update the autotune metrics but it won't happen within weeks.", "@yzhwang, thanks for the info. Closing this issue, since considering memory consumption is a separate issue.", "Thank you guys for shedding light on the issue. In my specific case, this additional usage of memory for a smaller batch size was making the model go out of memory, not immediately but at the end of the epoch. So if I may have a suggestion. Now I know that setting TF_CUDNN_USE_AUTOTUNE I can keep the memory usage constant.\r\nSo may I suggest adding an option to ConfigProto? Setting it the user would know that he can rely on constant memory consumption and he can tune the size of the model and the batch size accordingly, without weird surprises at the end of the epoch that will result in out of memory errors.", "I am suspecting something weird is happening. With autotuning, it is not supposed to use much more memory than if autotune is disabled\r\n\r\n1. During the measurement phase, if an algorithm failed to allocate memory, it is not a fatal error. TF just removes that algorithm from candidacy.\r\n2. During the execution phase, even if the op uses scratch memory, it almost immediately releases for later. Only concurrent kernels might be impacted by this. But even with that, other ops should retry and as if no extra memory was used.\r\n\r\nSomething might not be following this protocol. Disabling autotune has major performance impact. Also it should have too much memory overhead. So let's root cause this first. \r\n\r\nReopening the issue and ask @yzhwang to take a look.", "@w4nderlust We had a discussion offline. So as @zheng-xq just mentioned, autotune could use a large chunk of memory in two cases. In the measurement phase, I think he has explained it very well why there shouldn't be an OOM or any other failure. Just to provide you more info as to help you understand what happens in the execution phase:\r\nThere will be two algorithms recorded in autotune: the primary one which uses scratch memory, a secondary one which uses zero scratch memory, and regarding cuDNN's documentation, as long as the input shape and other parameters are legal, there should always be a default algorithm without scratch memory working fine.\r\nSuppose autotune find a best performing algorithm to use but in the execution phase it fails to allocate the scratch memory, we get a warning, but not a fatal error:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_dnn.cc#L2400\r\nThen in the following few lines, we will fall back to that default algorithm that uses no scratch memory. If an error happens there, it means one of two things:\r\n1) there is a cuDNN bug, since the default algorithm doesn't work for a legal set of parameters;\r\n2) we somehow fail to set the default algorithm for a legal set of parameters that should have a default algorithm.\r\nIn either case, we would be happy to fix that for you.\r\n\r\nMy suggestion for you is to try more batch size for validation, and see if there will be any error happening during the execution phase of conv. Note that if an OOM happens before the execution, it is the intended behavior as it would imply that TF doesn't have enough memory to even hold the input tensor. If you see error during the execution phase of conv, please report back and we are committed to fix that for you.\r\n\r\nAlso note that MaxBytesInUse() is a watermark. So it doesn't reflect the current memory in use, it only reflects the largest memory in use ever. So it could be a large one as long as one of the internal algorithm needs a large chunk of scratch memory.", "@yzhwang thank you really much for your detailed explanation and for taking the time to write it. I will test if the OOM that I was getting originally still happens now (with updated TF and CUDA/cuDNN) and in case will provide some code to replicate it. Thank you again.", "Nagging Assignee @yzhwang: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @yzhwang: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @yzhwang: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @yzhwang: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @yzhwang: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @yzhwang: It has been 95 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @yzhwang: It has been 111 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to inactivity.", "I am facing the same issue and somehow google has brought me here...\r\nI saw OOM error when trying to apply Conv2D even the input and output size is well below the ram i have on GPU.\r\nI also see setting autotune has reduced the peak memory by a bit.\r\n\r\nAny more recent advice? Many Thanks!\r\n\r\nSome info:\r\ntensorflow: 1.14 \r\nGPU:Titan XP \r\ncuda: 9.0", "> I am facing the same issue and somehow google has brought me here...\r\n> I saw OOM error when trying to apply Conv2D even the input and output size is well below the ram i have on GPU.\r\n> I also see setting autotune has reduced the peak memory by a bit.\r\n> \r\n> Any more recent advice? Many Thanks!\r\n> \r\n> Some info:\r\n> tensorflow: 1.14\r\n> GPU:Titan XP\r\n> cuda: 9.0\r\n\r\nI saw the same issue today with tf2.0. Before the conv2d operator the usage was like 0.3gb and right after I get the result from the operator, the gpu memory usage 4.8gb constantly. \r\nCUDA:10"]}, {"number": 14170, "title": "List of functions could be improved with \"const std::string&\" or \"std::string&\" instead \"std::string\"", "body": "After a quick scan in the latest tensorflow \"master\" branch, here is the list of functions which could improve passing parameter by \"std::string\":\r\n\r\nAfter a quick scan in the latest tensorflow \"master\" branch, here is the list of functions which could improve passing parameter by \"std::string\":\r\n\r\n1. c/c_api_function.cc:  static string Normalize(string name);\r\n   Suggestion: string& name;\r\n2. c/c_api_function.cc:string NodeNameMapping::Normalize(string name) {\r\n   Suggestion: string& name\r\n#\r\n3. compiler/jit/graph_to_functiondef.cc:  string NormalizeHelper(string name) const;\r\n   Suggestion: string& name\r\n4. compiler/jit/graph_to_functiondef.cc:  string UniquifyHelper(string name);\r\n   Suggestion: const string&\r\n5. compiler/jit/graph_to_functiondef.cc:string NodeNameMapping::NormalizeHelper(string name) const {\r\n   Suggestion: string& name\r\n6. compiler/jit/graph_to_functiondef.cc:string NodeNameMapping::UniquifyHelper(string name) {\r\n   Suggestion: const string&\r\n7. compiler/tf2xla/dump_graph.cc:string MakeUniquePath(string name) {\r\n   Suggestion: string& name\r\n8. compiler/xla/service/llvm_ir/llvm_util.cc:string IrName(string a) {\r\n    Suggestion: string& a\r\n9. compiler/xla/service/llvm_ir/llvm_util.cc:string SanitizeFunctionName(string function_name) {\r\n    Suggestion: string& function_name\r\n10. compiler/xla/service/llvm_ir/llvm_util.h:string IrName(string a);\r\n    Suggestion: string& a\r\n11. compiler/xla/service/llvm_ir/llvm_util.h:string SanitizeFunctionName(string function_name);\r\n    Suggestion: string& function_name\r\n12. compiler/xla/util.cc:string SanitizeFileName(string file_name) {\r\n    Suggestion: string& file_name\r\n13. compiler/xla/util.h:string SanitizeFileName(string file_name);\r\n    Suggestion: string& file_name\r\n#\r\n14. contrib/verbs/rdma.cc:RdmaBuffer::RdmaBuffer(RdmaChannel* channel, string name)\r\n    Suggestion: const string& name\r\n15. contrib/verbs/rdma.cc:RdmaAckBuffer::RdmaAckBuffer(RdmaChannel* channel, string name)\r\n    Suggestion: const string& name\r\n16. contrib/verbs/rdma.cc:RdmaMessageBuffer::RdmaMessageBuffer(RdmaChannel* channel, string name)\r\n    Suggestion: const string& name\r\n17. contrib/verbs/rdma.cc:RdmaTensorBuffer::RdmaTensorBuffer(RdmaChannel* channel, string name)\r\n    Suggestion: const string& name\r\n18. contrib/verbs/rdma.h:  explicit RdmaBuffer(RdmaChannel* channel, string name);\r\n    Suggestion: const string& name\r\n19. contrib/verbs/rdma.h:  explicit RdmaAckBuffer(RdmaChannel* channel, string name);\r\n    Suggestion: const string& name\r\n20. contrib/verbs/rdma.h:  explicit RdmaMessageBuffer(RdmaChannel* channel, string name);\r\n    Suggestion: const string& name\r\n21. contrib/verbs/rdma.h:  explicit RdmaTensorBuffer(RdmaChannel* channel, string name);\r\n    Suggestion: const string& name\r\n#\r\n22. core/common_runtime/step_stats_collector.cc:static int ExtractGpuWithStreamAll(string device_name) {\r\n    Suggestion: string& device_name\r\n23. core/common_runtime/step_stats_collector.cc:static int ExtractGpuWithoutStream(string device_name) {\r\n    Suggestion: string& device_name\r\n24. core/kernels/ops_util.cc:string SanitizeThreadSuffix(string suffix) {\r\n    Suggestion: const string& suffix\r\n25. core/kernels/ops_util.h:string SanitizeThreadSuffix(string suffix);\r\n    Suggestion: const string& suffix\r\n26. core/kernels/xsmm_conv2d.cc:static void chk_libxsmm_err(libxsmm_dnn_err_t status, string msg) {\r\n    Suggestion: const string& msg\r\n#\r\n27. stream_executor/platform.cc:PlatformKind PlatformKindFromString(string kind) {\r\n    Suggestion: const string& kind\r\n28. stream_executor/platform.h:PlatformKind PlatformKindFromString(string platform_string);\r\n    Suggestion: const string& platform_string\r\n", "comments": ["@orpillar I'm a big fan of cleanup work. A lot of the stuff on this list checks out, like `for (string name : events) {` tracing.cc. I support refactoring those unidiomatic usages of the C++ language. However this list needs more curation. I looked at (1) which points to [NodeNameMapping::Normalize](https://github.com/tensorflow/tensorflow/blob/4f39566/tensorflow/c/c_api_function.cc#L75) and this function passes by value because it mutates the value. Could you help us narrow things down?", "@jart You are right. The first function **NodeNameMapping::Normalize** needs to have \"**&**\" added, there is no need to add \"const\". I updated the issue description, will list with more details soon.", "Updated the list", "@orpillar Passing a non-const reference would change the API and isn't allowed by the [Google C++ Style Guide](http://google.github.io/styleguide/cppguide.html#Reference_Arguments).", "@jart Thanks for providing the Google C++ Style Guide. The guide does mention the rule. Please feel free to close the issue if not applicable.", "Well the issue is applicable, it's simply a question of curation. We appreciate that you took the time to suggest an improvement strategy. If you decide later that you want to take on this cleanup work, please send us a pull request, which I'd be happy to review."]}, {"number": 14169, "title": "Tensorflow predicts odd results with insufficient GPU memory", "body": "Hi all, \r\n\r\nI am currently having a problem that, when my code tries to initialize two or more predict instances in a GPU with insufficient memory, instead of throwing an OOM exception, the instances are initialized normally. However, when I try to predict an image with these instances, they produce weird results like [1.0, 0.0, 0.0, \u2026].\r\n\r\nIs this a bug when two or more sessions try to race for limited amount of GPU memory? \r\n\r\nPlease see below for my system information and the exact steps to reproduce the problem. I would very much appreciate it if you could take a look.\r\n\r\nCheers, \r\nVincent\r\n\r\nSystem information\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution: CentOS Linux release 7.2.1511\r\n- TensorFlow installed from (source or binary): official binaries\r\n- TensorFlow version (use command below): 1.3.0/ 1.2.0/ 1.1.0\r\n- Python version: 2.7.5\r\n- CUDA/cuDNN version: CUDA 8.0.61, CUDNN 6.0\r\n- GPU model and memory: 1080 Ti (11172MB)\r\n- Exact steps to reproduce:\r\n1. Download and uncompress the file below with two scripts;\r\n2. Download the official inception_v1 imagenet pretrained model  from http://download.tensorflow.org/models/inception_v1_2016_08_28.tar.gz\r\n3. Try to occupy most of the memory in the GPU to run on - the exact amount of memory to occupy needs to be carefully tuned to reproduce the problem, in my case, I use pycuda to allocate 10720 out of the 11172MB of my 1080 Ti. \r\n4. Run test_monitor.py in two separate command prompts, we should see weird results within a few minutes (the scripts stop when encounter such results).\r\n\r\n[reproduce_code.zip](https://github.com/tensorflow/tensorflow/files/1436321/reproduce_code.zip)\r\n", "comments": ["Thanks for the instructions to reproduce.\r\n\r\n@yzhwang and @zheng-xq have been investigating some issues with memory allocations and multiple processes and might have some more details here.", "Hi @vincentfung13 . Thanks for reporting the issue. We have encountered similar issues internally when two or more processes are running TensorFlow with limited GPU memory. In our case, a simple const tensor or random tensor creation will cause garbage values instead of any OOM error when one process has already taken most GPU memory.\r\nThe short answer to the issue is that: it is due to CUDA runtime silently failing to initialize with insufficient GPU memory. It will cause all the CUDA calls afterwards to produce incorrect results, but not fail with errors. We currently have a workaround which is to initialize CUDA runtime before any of our large GPU memory allocation. It should go in within a week. I'm also investigating the reason for the silent failure. I will provide update when I have something to share with you.", "Hi @yzhwang @zheng-xq, any updates on this issue? Thanks. :)", "Hi @vincentfung13 I've made a commit that should solve the problem: https://github.com/tensorflow/tensorflow/commit/4535bd5df4d077072a8f207146bf4cd051971237 Now each process that runs a TF program will try to initialize CUDA runtime before the giant device memory allocation, and if the remaining memory is insufficient, there should be an error for session creation. Please verify if this solves the problem. Thanks!", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "I believe the problem has been resolved after the commit 4535bd5, closing this for now."]}, {"number": 14168, "title": "fix typo", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 14167, "title": "TensorFlow for NVIDIA Tegra devices with CUDA support", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "@andrewharp could you take a look?", "@drpngx Hi. You asked me \"@lihanchen could you pull rebase and push again? That makes it easier to review. Thanks!\"\r\nDo you mean to combine all the commits into one?", "@lihanchen there were many commits so I thought it was a merge, but it wasn't. I have deleted that comment. You're good!", "Jenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please.", "@zheng-xq The failures seem to be unrelated, can we go ahead and merge this in?", "If the test are failing at head, and can be fixed soon, maybe wait until that. But if it takes too long, it's okay to merge. Thanks!", "Jenkins, test this please."]}, {"number": 14166, "title": "Documentation fix - clarifying sum in softmax function.", "body": "This notational fix makes it more clear that you're summing over the classes in the denominator of the softmax function.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@nsuh any update? Marking as stalled.", "@nsuh closing for now, feel free to reopen if you get a chance to address @martinwicke comments."]}, {"number": 14164, "title": "tfe.Network created inside variable_scopes", "body": "This is a feature request. \r\nCurrently `tfe.Network` cannot be created under variable scopes. This is particularly useful to still have when composing very complicated models. ", "comments": ["@asimshankar Our friend is\u2026 eagerly awaiting your response to his feedback on tfe.Network + variable scopes.", "FYI @alextp @allenlavoie @josh11b @agarwal-ashish \r\n\r\nCould you elaborate a bit on what you're trying and why :)? An example would help.\r\n(I'm guessing you're filing this feature request because of [this exception](https://github.com/tensorflow/tensorflow/blob/d7cffe9/tensorflow/contrib/eager/python/network.py#L301))\r\n\r\nWe'd really like to understand your use case better before thinking about removing this restriction. Perhaps there is a more idiomatic alternative to structuring your examples that could be leveraged instead?", "So, I wanted to use a `tfe.Network` which defines, for instance, the Normalizing Flow of an Autoencoder. I have another network for the inference network. Now at this point, you can technically just have another net on top where these are part of, which I don't. Nevertheless, since I use the VAE inside an RL algorithm, that last bit is scoped, as so far in most code people and me included have used more of the functional interface. I understand that you can pretty much change any place where you used a scope to having a class, however, I was hoping that we can start the transition from using our old models to using `tfe.Network` without having to refactor 100% of our code. However, for that to happen we do need scopes to work otherwise the only option for changing is refactor everything is it just crashes. \r\n\r\nAnd yes that is the exception. I think this is useful for people trying to move into the suggested framework, but still have old code using scopes, which we don't want to recode all. Hope that gives some motive for removing the restriction. Could you maybe as well elaborate a bit more on why the restriction is there so we can try to understand what could be the issues of removing it?", "So it sounds like you'd be fine with variable scopes having no effect on Network variable naming, as long as they don't crash? (not a proposal, just a clarifying question)\r\n\r\nThe error is there because we'd like variables in a Network to have a name which matches the names of the Networks/Layers it belongs to, and it wasn't clear that adding special naming for when the Network is in a variable scope was worth it.\r\n\r\nThe likely approach is to keep the parent scope as part of the Network name. Anonymous networks (named/numbered based on their class name) would be numbered so that they're unique within the parent scope (rather than at the graph level). We'd strip off the parent scope prefix as part of the Network name when writing checkpoints, but shared layers would have their full scope prefix in the checkpoint (which might be a bit fragile).", "I think yes as long as they don't crash I if variable scopes have no affect at least at what I have in mind atm that would work. \r\n\r\nI can't say I'm a 100% sure which is the best solution, but I do think what you are saying - prefixing the name of the network by the parent scope - pretty much like the others to be a sound solution. This probably is fine even for anonymous networks to have it and is numbered.  For the checkpoints, I don't think many users would care either way as long as it provides consistent saving and loading of models."]}, {"number": 14163, "title": "The tensorflow doesn't work suddenly", "body": "Hi,\r\n\r\nI am so confused that the tensorflow cannot use today.  It is perfectly run yesterday and the days before.  I am sure that the coding are right (It can run on other computers and before on my computer). I tried reinstall the graphics driver, cuda and cudnn but did not work. Here is the problem when I run the program:\r\n\r\n2017-11-01 14:07:43.309592: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-11-01 14:07:43.309643: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-11-01 14:07:43.309660: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-11-01 14:07:43.309674: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-11-01 14:07:43.309693: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-11-01 14:07:43.453723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-11-01 14:07:43.454015: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: \r\nname: GeForce GTX 1080\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.835\r\npciBusID 0000:01:00.0\r\nTotal memory: 7.92GiB\r\nFree memory: 7.62GiB\r\n2017-11-01 14:07:43.454027: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 \r\n2017-11-01 14:07:43.454030: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y \r\n2017-11-01 14:07:43.454051: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)\r\n========Recognition=========\r\n2017-11-01 14:07:44.790433: E tensorflow/stream_executor/cuda/cuda_dnn.cc:359] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2017-11-01 14:07:44.790460: E tensorflow/stream_executor/cuda/cuda_dnn.cc:326] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\n2017-11-01 14:07:44.790467: F tensorflow/core/kernels/conv_ops.cc:671] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms) \r\nAborted (core dumped)\r\n\r\nThanks so much!", "comments": ["Could you try the recommendation here?\r\nhttps://github.com/tensorflow/tensorflow/issues/6698#issuecomment-327154461\r\n\r\nPlease make sure to check for duplicate issues and their resolutions before filing new issues.", "@gunan thanks. That really works"]}, {"number": 14162, "title": "[Feature Request] tf.resume_gradient", "body": "tf.stop_gradient provides a convenient way for various uses. However, sometimes, it may be useful to resume the gradient passing after some conditions are met. \r\n\r\nFor example, in Faster-RCNN algorithm, the RPN layers' gradients are stopped because they may cause unstable results in early rounds. However, this indicates that these layers' weights would be kept the initial random values forever. If we could resume the gradients back propagation after several steps, the results may be better. Therefore, I believe that the resume_gradient function would be useful. Thank you.", "comments": ["`tf.where(condition, x, tf.stop_gradient(x))`", "@ybsave Does @ppwwyyxx's suggestion satisfy your needs? (Thank you by the way @ppwwyyxx)", "@ppwwyyxx Thank you. I think this is OK for my use.", "Hello, I just made some simple tests and find that the tf.where may not satisfy my need. I want the gradient of a loss to stop back propagation in early rounds, but continue the BP in later rounds. \r\n\r\nAs loss is usually defined before a session is really created, if I use codes like\r\n    loss = tf.where(math_ops.greater_equal(global_step, STEP_THRESHOLD), loss, tf.stop_gradient(loss))\r\n\r\nI found that the gradient still passes when global_step < STEP_THRESHOLD. This does not meet my requirement. \r\n\r\nIs there anyway to do this (using global_step to control \"stop_gradient\")? Or a new resume_gradient feature is still needed? Thank you.", "> I found that the gradient still passes when global_step > STEP_THRESHOLD. This does not meet my requirement.\r\n\r\nThis is exactly what you wrote. Gradient doesn't pass only when step < STEP_THRESHOLD.", "@ppwwyyxx I am sorry for my typo. I have corrected it. The gradient still pass when global_step < STEP_THRESHOLD.", "You'll need to give a small repro then, for bugs like this. At least it works in my test:\r\n```python\r\nx = tf.get_variable('a', shape=[5])\r\ny = x * 2\r\nloss = tf.reduce_mean(y)\r\ngs = tf.get_variable('global_step', shape=[], dtype=tf.int64)\r\ngs_inc = tf.assign_add(gs, 1)\r\nloss = tf.where(tf.greater(gs_inc, 2), loss, tf.stop_gradient(loss))\r\ngrad = tf.gradients(loss, x)\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    for i in range(5):\r\n        print(sess.run([grad, gs_inc]))\r\n```\r\n```\r\n[[array([ 0.,  0.,  0.,  0.,  0.], dtype=float32)], 1]\r\n[[array([ 0.,  0.,  0.,  0.,  0.], dtype=float32)], 2]\r\n[[array([ 0.40000001,  0.40000001,  0.40000001,  0.40000001,  0.40000001], dtype=float32)], 3]\r\n[[array([ 0.40000001,  0.40000001,  0.40000001,  0.40000001,  0.40000001], dtype=float32)], 4]\r\n[[array([ 0.40000001,  0.40000001,  0.40000001,  0.40000001,  0.40000001], dtype=float32)], 5]\r\n```", "@ppwwyyxx Thank you so much for your quick response. To clarify my question, I am using slim.learning.train to perform the training, where the global_step increment was performed inside the function.\r\n\r\nHowever, when I define my loss, it is defined before any session is created. Thus, when I use the global_step variable, I cannot get the correct value as tf.train.global_step requires a session as input. If I just use tf.where(tf.greater(global_step, 2), loss, tf.stop_gradient(loss)) instead of your codes like tf.where(tf.greater(gs_inc, 2), loss, tf.stop_gradient(loss)), I cannot get the correct global_step. Therefore, I wonder whether there is any method to perform the tf.where for defining the loss without using operators like \"gs_inc = tf.assign_add(gs, 1)\"?\r\n\r\nI am currently testing it in a big program. I will try to write a small program to show the problem soon. I appreciate your kind help.", "Though I don't think I fully understand what's your problem, but you should not use `tf.train.global_step` for this. `tf.train.get_or_create_global_step` will give you the global step __variable__ (instead of the value) and that's what you need. But still I'm not sure whether it works with high-level wrappers such as slim which takes control of the increment.", "Hello, I made a simple test as below\r\n\r\n\t  TRAIN_STEP_NUM = 5\r\n\t  x_train = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\r\n\t  y_train = [0, -1, -2, -3, -4, -5, -6, -7, -8, -9]\r\n\r\n\t  dataset = tf.data.Dataset.from_tensor_slices(\r\n\t\t  (tf.cast(x_train, tf.float32), tf.cast(y_train, tf.float32)))\r\n\r\n\t  with tf.Graph().as_default():\r\n\t\titerator = dataset.batch(1).make_one_shot_iterator()\r\n\t\tx, y = iterator.get_next()\r\n\t\t\r\n\t\tW = tf.Variable([.3], dtype=tf.float32)\r\n\t\tb = tf.Variable([-.3], dtype=tf.float32)\r\n\t\tlinear_model = W*x + b\r\n\t\tloss = tf.reduce_sum(tf.square(linear_model - y))\r\n\r\n\t\tloss = tf.where(\r\n\t\t  tf.greater(tf.train.get_or_create_global_step(), TRAIN_STEP_NUM),\r\n\t\t\t\t\t loss, tf.stop_gradient(loss))\r\n\t\tslim.losses.add_loss(loss)\r\n\t  \r\n\t\toptimizer = tf.train.GradientDescentOptimizer(0.01)\r\n\t\ttrain_op = slim.learning.create_train_op(loss, optimizer)\r\n\r\n\t\tslim.learning.train(train_op, './tmp',\r\n\t\t\t\t\t\t\tlog_every_n_steps=1,\r\n\t\t\t\t\t\t\tnumber_of_steps=10)\r\n\r\nThe outputs are:\r\n\r\n\tINFO:tensorflow:Starting Session.\r\n\tINFO:tensorflow:global_step/sec: inf\r\n\tINFO:tensorflow:Saving checkpoint to path ./tmp\\model.ckpt\r\n\tINFO:tensorflow:Starting Queues.\r\n\tINFO:tensorflow:global step 1: loss = 0.0000 (0.016 sec/step)\r\n\tINFO:tensorflow:global step 2: loss = 1.6900 (0.000 sec/step)\r\n\tINFO:tensorflow:global step 3: loss = 6.7600 (0.000 sec/step)\r\n\tINFO:tensorflow:global step 4: loss = 15.2100 (0.000 sec/step)\r\n\tINFO:tensorflow:global step 5: loss = 27.0400 (0.016 sec/step)\r\n\tINFO:tensorflow:global step 6: loss = 42.2500 (0.000 sec/step)\r\n\tINFO:tensorflow:global step 7: loss = 60.8400 (0.000 sec/step)\r\n\tINFO:tensorflow:global step 8: loss = 0.0433 (0.000 sec/step)\r\n\tINFO:tensorflow:global step 9: loss = 0.0126 (0.000 sec/step)\r\n\tINFO:tensorflow:global step 10: loss = 0.0068 (0.000 sec/step)\r\n\tINFO:tensorflow:Stopping Training.\r\n\tINFO:tensorflow:Finished training! Saving model to disk.\r\n\r\nIf I disabled the \"tf.where\" line, then the outputs are:\r\n\r\n\tINFO:tensorflow:Starting Session.\r\n\tINFO:tensorflow:global_step/sec: inf\r\n\tINFO:tensorflow:Saving checkpoint to path ./tmp\\model.ckpt\r\n\tINFO:tensorflow:Starting Queues.\r\n\tINFO:tensorflow:global step 1: loss = 0.0000 (0.016 sec/step)\r\n\tINFO:tensorflow:global step 2: loss = 1.6900 (0.000 sec/step)\r\n\tINFO:tensorflow:global step 3: loss = 5.8467 (0.000 sec/step)\r\n\tINFO:tensorflow:global step 4: loss = 9.2253 (0.000 sec/step)\r\n\tINFO:tensorflow:global step 5: loss = 8.2057 (0.000 sec/step)\r\n\tINFO:tensorflow:global step 6: loss = 3.7965 (0.000 sec/step)\r\n\tINFO:tensorflow:global step 7: loss = 0.7162 (0.000 sec/step)\r\n\tINFO:tensorflow:global step 8: loss = 0.0489 (0.000 sec/step)\r\n\tINFO:tensorflow:global step 9: loss = 0.0143 (0.000 sec/step)\r\n\tINFO:tensorflow:global step 10: loss = 0.0077 (0.000 sec/step)\r\n\tINFO:tensorflow:Stopping Training.\r\n\tINFO:tensorflow:Finished training! Saving model to disk.\r\n\r\nIt seems that the stop_gradient did not freeze the updating of parameters, leading to loss change; however, it did influence somewhat unexpected, making the loss different. What should I do to freeze the updating in 5 steps? Thank you.", "The loss is changing because you're sending different data in each iteration. You snippet doesn't show any bugs.", "@ppwwyyxx  Thank you for your kind help. After using the same data, the program now performs as expected. Really appreciate!"]}, {"number": 14161, "title": "Initial add of docs for Tensorflow on Mobile.", "body": "Cherry-pick of mobile docs from master onto r1.4\r\n\r\n```\r\ngit cherry-pick 2ba52985657e8189a19f1be52448b8268ccd879a\r\n```\r\n\r\nPiperOrigin-RevId: 173980290", "comments": ["Can one of the admins verify this patch?"]}, {"number": 14160, "title": "Fix a broken link in `Threading and Queues` docs", "body": "The link to Datasets in `Threading and Queues` seems\r\nto be broken. This fix attempts to fix the broken link.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 14159, "title": "Make sure to set _GLIBCXX_USE_CXX11_ABI=0 if it's not defined", "body": "This is necessary to make sure we can compile TensorFlow with gcc4 and compile custom operator with gcc5.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please", "@jhseu, @gunan, thanks for the quick turnaround!"]}, {"number": 14158, "title": "Android - No OpKernel was registered to support Op 'Cos' with these attrs.  Registered devices: [CPU], Registered kernels:                                                                        <no registered kernels>                                                                                                                                            \t [[Node: stft/hann_window/Cos = Cos[T=DT_FLOAT, _device=\"/device:CPU:0\"](stft/hann_window/truediv)]]", "body": "I've exported a model to Android that uses : \r\nstfts = tf.contrib.signal.stft(transwav, frame_length=2048, frame_step=512,\r\n                             fft_length=2048,window_fn=functools.partial(tf.contrib.signal.hann_window, periodic=False), pad_end=True)\r\nthe model works properly in pyhton\r\nbut when i load it in Android using the downloaded compiled tensorflow - compile 'org.tensorflow:tensorflow-android:+'\r\nI get this error - \r\nFATAL EXCEPTION: main\r\n                                                                     Process: org.tensorflow.demo, PID: 25836\r\n                                                                     java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'Cos' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n                                                                       <no registered kernels> \t [[Node: stft/hann_window/Cos = Cos[T=DT_FLOAT, _device=\"/device:CPU:0\"](stft/hann_window/truediv)]]\r\n\r\nwhich comes from the hann_window\r\n\r\nany recommended work around?\r\n", "comments": ["This is a very common issue with Android development. You need to make sure all the kernels you need are in your build. Please check [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) and search Google for similar issues.", "Please note that C++ kernels use a registration pattern.", "I have recompiled libtensorflow_inference.so with print_selective_registration_header\r\nand i see in the ops_to_register.h file the Cos OP - \r\n\r\n...\r\nconstexpr inline bool ShouldRegisterOp(const char op[]) {\r\n  return false\r\n     || isequal(op, \"Add\")\r\n     || isequal(op, \"BatchToSpaceND\")\r\n     || isequal(op, \"CTCBeamSearchDecoder\")\r\n     || isequal(op, \"Cast\")\r\n     || isequal(op, \"Complex\")\r\n     || isequal(op, \"ComplexAbs\")\r\n     || isequal(op, \"ConcatV2\")\r\n     || isequal(op, \"Const\")\r\n     || isequal(op, \"Conv2D\")\r\n     **|| isequal(op, \"Cos\")**\r\n     || isequal(op, \"DecodeWav\")\r\n     || isequal(op, \"Exp\")\r\n     || isequal(op, \"ExpandDims\")\r\n     || isequal(op, \"Fill\")...\r\nthere for it should be compiled into the libtensorflow_inference.so\r\n\r\nbut i still get the same error message when trying to run the model in my android app", "@eli99999 Please let me know if [this](https://github.com/tensorflow/tensorflow/issues/3549#issuecomment-339125196) help.", "@bobeo thanks,\r\nI looked into it, and it seems that there is  a problem with the bazel compilation\r\ni changed the models a few time and then did the ops_to_register.h process, \r\neach time the ops_to_register.h file changed but the libtensorflow_inference.so  does not.\r\nI tried all sort of bazel clean and bazel dump, and not chaned the resulted  libtensorflow_inference.so\r\nso i think that either there is some issue with the bazel caching or that the  bug is in the print_selective_registration_header process\r\n\r\ni'm working on ubuntu 16.04\r\nbazel 0.7.0\r\ntf 1.4\r\n\r\n", "I've answered a similar question on StackOverflow: https://stackoverflow.com/questions/49116833/no-opkernel-was-registered-to-support-op-cos-when-running-inference-on-android\r\n\r\nApparently, you might need to \"hack\" the BUILD file, according to one of the TensorFlow developers: https://github.com/tensorflow/tensorflow/issues/11804#issuecomment-318415228"]}, {"number": 14157, "title": "build with tensorflow_cc with cmake.", "body": "I found https://github.com/FloopCZ/tensorflow_cc/ \r\nas a good solution.\r\nshould somebody add it to official repository", "comments": ["That sounds wonderful and I hope the community finds it useful.\r\n\r\nFrom what I understand, [`bazel` is used](https://github.com/FloopCZ/tensorflow_cc/blob/master/tensorflow_cc/cmake/build_tensorflow.sh#L41) to build `libtensorflow_cc.so`, and what's there packages that up with a set of header files.\r\n\r\nEventually, we would like to distribute the shared library and header files in a tarball that can be used by any build system, instead of having to maintain support for different build systems. We'd also like to tighten the API a bit, to reduce the number of internal header files that need to be exposed.\r\n\r\nIf anyone is interested in contributing to that, I'd suggest following along on #2412.\r\nThanks!\r\n\r\n(FYI @skye)", "@asimshankar Yes, that is exactly what is done in there. However, it should be noted that if https://github.com/FloopCZ/tensorflow_cc/issues/8 is resolved (add support for CPack), you will be able to easily build a tarball, installers and even deb/rpm packages independent of the build system (FYI @skye).", "@FloopCZ Where should I place the **example** folder or its contents. From which location should I run those last commands(**3rd point under Usage**) in your [repository](https://github.com/FloopCZ/tensorflow_cc/) ?", "Lets continue at https://github.com/FloopCZ/tensorflow_cc/issues/112#issuecomment-461413277."]}, {"number": 14156, "title": "Implement shape constraints for `Cross`", "body": "This fix implements shape constrainsts for `Cross`, as was specified in the TODO:\r\n```\r\n    // * Both inputs have the same shape.\r\n    // * Input rank >= 1.\r\n    // * input_shape[-1] == 3.\r\n```\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please!", "@tensorflow-jenkins test this please"]}, {"number": 14155, "title": "time to restore saved model increases over time ", "body": "### System information\r\n- Manjaro Linux\r\n- TensorFlow installed with pip:\r\n- TensorFlow version: v1.3.0-rc2-20-g0787eee 1.3.0:\r\n- Python version: 3.6.2 \r\n- CUDA/cuDNN version: 8.0.61-3 / 7.0.3-1\r\n- GPU model and memory: GeForce GTX 1080 (8 GB)\r\n\r\n\r\n### Describe the problem\r\nExecuting the code below in a loop slows down linearly with\r\nnumber of iteration as can be seen in the figures below.\r\n(The use case being a long running app where different models\r\nare loaded (multiple times) depending on user inputs) \r\n\r\n```python\r\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.25)\r\nconfig = tf.ConfigProto(gpu_options=gpu_options)\r\n\r\nwith tf.Session(config=config) as sess:\r\n    saver = tf.train.import_meta_graph(modelname)\r\n    saver.restore(sess, tf.train.latest_checkpoint(\"./\"))\r\n```\r\n\r\n### Source code / logs\r\nReproducible code and plots of the time increase can be found here:\r\n[https://www.dropbox.com/s/rgbqp9z3m5vnr9d/test.zip?dl=1](url)\r\n\r\nTo reproduce do the following from withing the extracted zip archive:\r\n```\r\npython train_model.py\r\npython test_a.py\r\n```\r\n\r\n![import_metagraph](https://user-images.githubusercontent.com/7083141/32281142-4b2c3eec-bf1e-11e7-85f0-5da5af5e4135.png)\r\n\r\n![saver_restore](https://user-images.githubusercontent.com/7083141/32281147-4f1244d4-bf1e-11e7-8126-28eb01df3c8a.png)", "comments": ["I really love it when people take the time to make these plots to demonstrate performance issues. Thank you. The dropbox link seems broken. Is there any chance you could just copy and paste the whole script into a comment here?", "It is not a single script unfortunately. The link above works for me if I copy/paste it into my browsers address bar. When clicking on it it seems dead, as you said. Can you try the copy/paste approach with the link?", "OK that works. But this is so much code. Is there any way you could make this more digestible? Also is there any chance the growth is due to writing a whole bunch of files being written and latest_checkpoint needs to crawl the whole directory?", "Ok, condensed it down to one script. There is only one checkpoint. So no dir crawling.\r\n\r\n\r\n```python\r\nimport time\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\ndef rebuild_model(modelname):\r\n    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = 0.25)\r\n    config = tf.ConfigProto(gpu_options=gpu_options)\r\n    with tf.Session(config=config) as sess:\r\n        saver = tf.train.import_meta_graph(modelname + \".meta\")\r\n        saver.restore(sess, tf.train.latest_checkpoint(\"./\"))\r\n        a = 1 + np.random.randn()\r\n        \r\n\r\nnVars = 10 \r\nnExamples = 100\r\nnClasses = 2\r\nnEpochs = 100 \r\n\r\nX = np.random.randn(nExamples, nVars)\r\ny = np.random.randint(0, nClasses, nExamples)\r\n\r\nnHidden1 = 50 \r\nnHidden2 = 20\r\n\r\n\r\n# Build model\r\ngraph = tf.Graph()\r\nwith graph.as_default():\r\n    x_tf = tf.placeholder(tf.float32, shape=(None, nVars), name=\"x_tf\")\r\n    y_tf = tf.one_hot(y, nClasses, name=\"y_tf\")\r\n\r\n    w1 = tf.Variable(tf.random_normal([nVars, nHidden1]))\r\n    b1 = tf.Variable(tf.zeros(nHidden1))\r\n    a1 = tf.nn.relu(tf.matmul(x_tf, w1) + b1)\r\n\r\n    w2 = tf.Variable(tf.random_normal([nHidden1, nHidden2]))\r\n    b2 = tf.Variable(tf.zeros(nHidden2))\r\n    a2 = tf.nn.relu(tf.matmul(a1, w2) + b2)\r\n\r\n    w_out = tf.Variable(tf.random_normal([nHidden2, nClasses]))\r\n    b_out = tf.Variable(tf.zeros(nClasses))\r\n    logits = tf.add(tf.matmul(a2, w_out), b_out, name=\"logits\")\r\n    yhat = tf.nn.softmax(logits, name=\"yhat\")\r\n\r\n\r\n    loss = tf.reduce_mean(\r\n           tf.nn.softmax_cross_entropy_with_logits(\r\n           labels=y_tf, logits=logits))\r\n\r\n    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\r\n    saver = tf.train.Saver()\r\n\r\n# Train model\r\nwith tf.Session(graph=graph) as sess:\r\n    tf.global_variables_initializer().run()\r\n\r\n    for step in range(nEpochs+1):\r\n        _, current_loss, prediction, logi = sess.run([optimizer, loss, yhat, logits], feed_dict={x_tf:X})\r\n        if (step % 10 == 0):\r\n            accuracy = np.mean(tf.equal(tf.argmax(prediction, 1), y).eval())\r\n            print(\" - %i\\tLoss: %.4f\\tAccuracy: %.4f\" %(step, current_loss, accuracy))\r\n\r\n    save_path = saver.save(sess, \"feedforward.ckpt\")\r\n\r\n\r\n# loop \r\ndt = []\r\nfor i in range(100):\r\n    t0 = time.time()\r\n    result = rebuild_model(\"feedforward.ckpt\")\r\n    t1 = time.time()\r\n    print(t1 - t0)\r\n    dt.append(t1 - t0)\r\n\r\n\r\nplt.figure()\r\nplt.plot(dt)\r\nplt.ylabel(\"execution time [s]\")\r\nplt.xlabel(\"iteration\")\r\nplt.title(\"saver.restore()\")\r\n\r\nplt.show()\r\ninput()\r\n\r\n```", "I see the problem. You need to call `tf.reset_default_graph()` before importing the meta graph again. This behavior is non-obvious and surprising, I agree. It's one of those things where we wish we could go back in time and change.\r\n\r\nI'm closing this issue out because I think the problem is resolved. However I would like to discuss a little more about what we could have done better. Maybe we could update the doc for tf.train.import_meta_graph? Maybe there's some way we could display a warning? It might help if you could share more information about what you're trying to accomplish with this technique.", "Thanks! I stumbled upon this when writing unit tests, where each test rebuilds the (a) graph\u00a0from disk.", "That's great that you're testing the boundaries of TensorFlow for us. If you discover anything else in the future, please bring it to our attention. We're always looking for opportunities to make TensorFlow better.", "I do have another problem with unittests and tensorflow. Every unit test by itself runs successfully, but when using pytest to run them all I get errors. It is quite a lot of code though, when I condensed it down to a somewhat manageable amount I ll open a separate issue. Thanks for the very quick response!", "That might be a better question for [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow). I usually run tests using Bazel. For example `bazel test //tensorflow/python/...`"]}, {"number": 14154, "title": "Clarify documentation of stacked cuDNN RNNs", "body": "Currently the documentation for cuDNN RNNs is unclear as to whether the RNNs (LSTMs or GRUs), when they are bidirectional and configured with multiple layers, integrate the outputs from both directions at a given layer n before sending it to the next layer n+1, or whether each direction works independently of the other. I.e. forward layers send information just to the forward layers above, and similarly for the backward direction. The difference in behavior is quite important and it would be great if the documentation is updated to state which behavior is being carried out.", "comments": ["@alquraishi This is good doc feedback; thank you for bringing it to our attention. I'm assuming you mean the [cudnn_rnn.py](https://github.com/tensorflow/tensorflow/blob/a6a6188/tensorflow/contrib/cudnn_rnn/python/layers/cudnn_rnn.py) file that @protoget has been working on?\r\n\r\nI'm happy to assign him this issue, but is there any chance you'd consider sending us a pull request updating the doc? Simple fixes like this are a good way to add the \"I contributed to TensorFlow\" notch to your belt and it sounds like you've already got this figured out.", "@jart I actually don't know the answer to the question though :-)  I'm not sure if cuDNN (yes in cudnn_rnn) is fusing the outputs from both directions before sending to the next layer or not. I was looking for it in the docs and couldn't find it. I think it depends on what cuDNN itself is doing.", "@protoget could the docs in cudnn_rnn.py be improved in the way our friend is asking?", "@alquraishi \r\nFor TF Cudnn layers, think TF as a wrapper on top of Nvidia Cudnn library. There is a very limited set of config TF can do. So you probably can refer to Nvidia's CuDNN manual if the doc is missing something.\r\n\r\nTo answer ur question:\r\n> when they are bidirectional and configured with multiple layers, integrate the outputs from both directions at a given layer n before sending it to the next layer n+1, or whether each direction works independently of the other?\r\n\r\nAt the end of each layer, it concates output from both directions (forward goes first), which becomes the input for next layer, for both directions.\r\n\r\nThanks for your input. It would be appreciated if you would like to improve the doc as well. Else we can make the change. :)\r\n", "@protoget thank you for the clarification. That is what I was wondering about. I couldn't really find the answer in the cuDNN docs and I figured since it's exposed via TF that TF docs should document the behavior, even if it's not necessarily modifiable.\r\n\r\nI haven't submitted any PR requests for TF yet so I would have to sign the CLA, etc, which may take some time. I'm happy to do it but feel free to change the docs yourself as well.\r\n\r\nThanks again!", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Closing due to inactivity. But anyone is still welcome to send us a PR improving these docs."]}, {"number": 14153, "title": "Comment out dynamic slice tests that rely on wrapping", "body": "The wrapping behaviour has not yet been specified so it can't be tested. See https://groups.google.com/forum/#!topic/xla-dev/XQ7LbZOg9Nc\r\n", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Can one of the admins verify this patch?", "> The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.\r\n\r\nI am me. That is to say, the git commit contains my work email but I'm submitting using my non-work github account (since I don't have a work github account). Both emails are in our work CLA group.", "Once we have the approval, I can push the change through as you the email has CLA and you have expressed your consent for merging the PR.", "If you add your work email to your GitHub account that would also resolve the CLA issue. ", "Ah cool I have done that now, thanks.", "@hawkinsp could you take a look?", "@Timmmm can you resolve the conflicts? I don't know what would be correct here.", "@Timmmm could you pull rebase and push again? Otherwise it looks like it should be good to go, assuming the tests pass.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing for now. Please reopen if you have an update."]}, {"number": 14152, "title": "Fix typos", "body": "This PR fixes some typos: tf.constant, tf.placeholder.", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "Can one of the admins verify this patch?", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 14151, "title": "The new version of the code about Wide and Deep model has not yet defined 'centered_bias'", "body": "Hi,\r\nI'm reading the new version tensorflow code about Wide and deep model. the model is in tf.estimator.DNNLinearCombinedClassifier. In the tf.contrib.learn.DNNLinearCombinedClassifier,we can find that the variable 'centered_bias' is defined in head.py,It;s the bias of model's output.However, in the new version code. eg. the implemention based on 'tf.feature_column' and 'tf.estimator', this variale has not been find.It's only add dnn_logits and linear_logits. Is it a bug? or I do not find it.\r\nthanks.\r\nXiangfu Shi", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 14150, "title": "Unable to use tf.multinomial in Android. No OpKernel was registered to support Op 'Multinomial'", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS Sierra 10.12.6, Android APK 23\r\n- **TensorFlow installed from (source or binary)**: pip\r\n- **TensorFlow version (use command below)**: 1.3\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:N/A\r\n- **CUDA/cuDNN version**:N/A\r\n- **GPU model and memory**:N/A\r\n\r\n### Describe the problem\r\nI cannot use tf.multinomial in Android while it works in mac. The error message is in the next section. I'm using tensorflow from [TensorFlowSharp](https://github.com/migueldeicaza/TensorFlowSharp) and [ml-agents](https://github.com/Unity-Technologies/ml-agents) in Unity. Unity builds my game uisng tensorflow for Android.\r\n\r\n### Source code / logs\r\nTFException: No OpKernel was registered to support Op 'Multinomial' with these attrs.  Registered devices: [CPU], Registered kernels:\\<no registered kernels\\>\r\n[[Node: multinomial/Multinomial = Multinomial[T=DT_FLOAT, seed=0, seed2=0](dense_2/MatMul, multinomial/Multinomial/num_samples)]]\r\n  at TensorFlow.TFStatus.CheckMaybeRaise (TensorFlow.TFStatus incomingStatus, System.Boolean last) [0x0004a] in <252020d87a4e4581ad2cfe3f9cc7a0ac>:0 ", "comments": ["Please check out [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) and Google for similar issues. It's a common issue with Android development to make sure the right kernels get included in a build. Please note they use the registration pattern."]}, {"number": 14149, "title": "Update comments for variables in get_started", "body": "This PR updates the comments in the Getting Started guide to make it more clear to which variables they refer to.", "comments": ["Can one of the admins verify this patch?"]}, {"number": 14148, "title": "Nameerror: could not find operator mpisize in dynamic library mpi_collectives.so", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:ubuntu 16.04 with 4.4 kernel\r\n- **TensorFlow installed from (source or binary)**: source code\r\n- **TensorFlow version (use command below)**: latest(git clone from github in 2017/10/30)\r\n- **Python version**: 2.7 \r\n- **Bazel version (if compiling from source) : 0.70(latest)\r\n- **CUDA/cuDNN version**: 8.0/6.0\r\n- **GPU model and memory**: tesla k40c\r\n- **Exact command to reproduce**: python -c \"import tensforflow.contrib.mpi_collectives\"", "comments": ["Yep, it seem that, the dynamic lib didn't contain a MPISize. so i think it is caused by the dynamic lib.\r\nbut how to fix it ?", "MPI is a very new contrib package that we're not able to officially support yet. Please reach out to @jthestness. See also https://github.com/tensorflow/tensorflow/pull/12299."]}, {"number": 14147, "title": "tf.cast zeroes out input tensor when GPU does not have any free memory", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: tf-nightly-gpu binary from 10/31/2017\r\n- **TensorFlow version (use command below)**: ('v1.3.0-rc1-4007-gc44f67a', '1.5.0-dev20171031')\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: CUDA v8, cuDNN v6\r\n- **GPU model and memory**: 1080 Ti (11 GB)\r\n- **Exact command to reproduce**:\r\n\r\nx = tf.cast(tensor, tf.float32)\r\nx = tf.to_float(tensor)\r\n\r\nAbove commands return a tensor with all values of `tensor` set to zero WHEN:\r\nThe gpu is in full use by another tensorflow process. I'm trying to cast the tensor from dtype=tf.uint16 to tf.float32, using above commands, but whenever this runs while the gpu is in full use (i.e. I get a CUDA out-of-memory error), the program completes execution normally but the casting commands set the tensor to zeros.\r\nI upgraded from an older nightly-gpu binary to the current one, but didn't help. User should not try to use an already in-use gpu, but this error should at least be communicated to the user.\r\n\r\nIf I set the gpu to a different gpu with available memory, or if I hide all gpus to force it to use CPU, I do not observe this issue.", "comments": ["Thanks for the report,\r\n\r\n@yzhwang and @zheng-xq have been investigating some issues with multiple processes and the GPU memory allocator and might have some more detail to share.\r\n\r\nIn the mean time, if possible, could you package up detailed instructions to reproduce the problem - ideally a `.py`  file that we could run (perhaps twice, one for each process) that reproduces the issue?\r\n\r\nThanks.", "Hi @mmuneebs Thanks for reporting the issue. See my comment for another issue here: https://github.com/tensorflow/tensorflow/issues/14169#issuecomment-342256834", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Close this. Please refer to https://github.com/tensorflow/tensorflow/issues/14169#issuecomment-355498651"]}, {"number": 14146, "title": "[feature request] Adding c++ serving session api to c api ", "body": "Or is it recommended to just use TF_NewSession C API for session serving?  Is TF_Session.Run thread safe?  Thanks in advance!", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 14145, "title": "generator_input_fn for tensorflow estimator?", "body": "I find there is `numpy_input_fn` and `pandas_input_fn` to construct the input_fn for tensorflow estimator. \r\n\r\nHowever, sometimes I need a more flexible constructor that can create the `input_fn` from a custom generator/iterator. It seems that Tensorflow doesn't have this feature currently. \r\n\r\nThere is a [generator_input_fn](https://github.com/tensorflow/tensorflow/pull/7045/files#diff-58e32b22d643f3a61d9bbeee2bec89b6R27) for `tensorflow.contrib.learn` but I think that is not compatible with Tensorflow Estimator. \r\n\r\nIs there any plan to add `generator_input_fn` for tensorflow estimator?", "comments": ["@ispirmustafa would probably know.", "You can use `Dataset.from_generator` in your input_fn.", "@ispirmustafa Thanks, but while `numpy_input_fn` supports \"dict of numpy array object\" for data source, it seems that `Dataset.from_generator` doesn't support dictionary structure.\r\n\r\n", "@mrry could you please comment about Dataset.from_generator?", "I believe `Dataset.from_generator()` *does* support yielding dictionaries (of NumPy arrays), as long as you specify a dictionary-based structure in the `output_types` argument. If this is not the case, please file a bug with a way to reproduce the problem!", "@mrry Hi Derek, does Dataset.from_generator() support multithreading like QueueRunner?\r\n\r\nThanks"]}]