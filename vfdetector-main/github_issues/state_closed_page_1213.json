[{"number": 16777, "title": "Tensorflow object recognition SSD is really slow with tf 1.5 ", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@danishshres Please edit the description of this issue (the above post) to provide the requested details. In its current form, your issue is likely to be closed with the reason \"not enough information\" in the next few days.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you."]}, {"number": 16776, "title": "Unnamed Op not showing in list_tensors command during debug session", "body": "I have these two unnamed op tensors `logits` and `outputs` under a variable scope, but the `lt` command isn't listing these two tensors under the op 'MatMul' and 'Softmax' during the `tfdbg` session after a test run on a checkpoint. Here is a snapshot of the code:\r\n```\r\n    with tf.variable_scope(scope):\r\n        d_inputs = dropout(inputs, keep_prob=keep_prob, is_train=is_train)\r\n\t\td_memory = dropout(memory, keep_prob=keep_prob, is_train=is_train)\r\n\t\tJX = tf.shape(inputs)[1]\r\n\r\n\t\twith tf.variable_scope(\"attention\"):\r\n\t\t\tinputs_ = tf.nn.relu(dense(d_inputs, hidden, use_bias=False, scope=\"inputs\"))\r\n\t\t\tmemory_ = tf.nn.relu(dense(d_memory, hidden, use_bias=False, scope=\"memory\"))\r\n\t\t\toutputs = tf.matmul(inputs_, tf.transpose(memory_, [0, 2, 1])) / (hidden ** 0.5)\r\n\t\t\tmask = tf.tile(tf.expand_dims(mask, axis=1), [1, JX, 1])\r\n            # The tensor down below \r\n\t\t\tlogits = tf.nn.softmax(softmax_mask(outputs, mask))\r\n            # And the tensor down below here as well\r\n\t\t\toutputs = tf.matmul(logits, memory)\r\n\t\t\tres = tf.concat([inputs, outputs], axis=2)\r\n```\r\nWhat can I do to retrieve these variables for testing purposes on `tfdbg`?  \r\nFor alternative purposes, can I retrieve them using the normal tensorflow session by using `tf.add_to_collection(op_name, tensor)` as mentioned in [this answer][1]?\r\n\r\n\r\n  [1]: https://stackoverflow.com/questions/44639260/retrieving-an-unnamed-variable-in-tensorflow", "comments": ["Nagging Assignee @caisq: It has been 229 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 16775, "title": "tf.layers.Dense work error with unit is a Tensor", "body": "OS Platform and Distribution\r\nTensorFlow installed from\r\ndocker hub\r\n\r\nTensorFlow version\r\n1.5.0\r\n\r\nCUDA/cuDNN version\r\nCUDA9\r\n\r\nGPU model and memory\r\nTITAN XP\r\n\r\nExact command to reproduce\r\nN/A\r\n\r\n### Describe the problem\r\n`tf.layers.Dense._rnn_output_size()` not work when `self.units` is a Tensor.\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@tensorflowbutler fix", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I suspect that units must always be an attribute constant rather than a tensor. @ebrevdo, can you confirm?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "`units` is an `int`, as mentioned explicitly in the docs. It's not something that you can dynamically change as part of a computation graph, it's a hardcoded architecture parameter (since it determines the dimensions of the layer's weights).\r\n\r\nDocs:\r\n\r\n```\r\nunits: Positive integer, dimensionality of the output space.\r\n```"]}, {"number": 16774, "title": "Missing functionality in Keras TensorBoard callback ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Binary.\r\n- **TensorFlow version (use command below)**: 1.5.0 (Keras 2.1.2-tf)\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: 9.0 / 7.0\r\n- **GPU model and memory**: GTX 1070 8GB\r\n- **Exact command to reproduce**: NA\r\n\r\n### Describe the problem\r\n\r\nThe TensorBoard-callback in Keras supports the writing of embeddings to the log, but this is not supported in the TensorFlow version of Keras, even though the doc-string of the TensorFlow version actually lists these parameters.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/579125e87af201ae6b6fa872b6dc3f3ecb400de9/tensorflow/python/keras/_impl/keras/callbacks.py#L643-L651\r\n\r\nBut these are missing from the `__init__`:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/579125e87af201ae6b6fa872b6dc3f3ecb400de9/tensorflow/python/keras/_impl/keras/callbacks.py#L656-L662\r\n\r\nThis is the original Keras implementation which has e.g. `embeddings_freq`:\r\n\r\nhttps://github.com/keras-team/keras/blob/ad00676b80556a6354180a1bfa3009d4db316d3e/keras/callbacks.py#L642-L650\r\n\r\nThere seems to be a difference between the original Keras implementation and the version in TensorFlow. I am wondering if these are somehow developed in parallel so it is actually not the original Keras that is included with TensorFlow?\r\n\r\nThanks!", "comments": ["This functionality currently relies on feature from `contrib`, while `tf.keras` is in core TF. Thus it cannot be supported by `tf.keras` yet.", "Nagging Assignee @fchollet: It has been 198 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @fchollet: It has been 213 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 16773, "title": "TensorBoard: Fit domain to data in 1.5 under Windows cuts off max values", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Using TensorBoard in custom U-Net implementation\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 7\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.5\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: CUDA 9.0, CuDNN 7\r\n- **GPU model and memory**: GeForce GTX 1050 Ti (4 GB), 32 GB RAM\r\n- **Exact command to reproduce**: tensorboard with tf.summary.scalar\r\n\r\n### Describe the problem\r\nAfter updating from 1.4 to 1.5 I have the problems that the y-scale for the scalar graphs in TensorBoard seems to be misscalculated. The maximum values are not included in the shown graph, they are cut off.\r\n\r\n![image](https://user-images.githubusercontent.com/31246640/35806246-1efb92ee-0a7f-11e8-8441-17ebe97473b7.png)\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nGPU model and memory", "I updated the missing fields.", "/cc @jart ", "TensorBoard has a new GitHub repo: https://github.com/tensorflow/tensorboard Would you be able to refile this issue over there?"]}, {"number": 16771, "title": "core dump is occured using reduce_sum", "body": "Core dump is occured when I tried thus code `tf.reduce_sum(tf.constant([1, 2, 3], dtype=tf.uint8))`.\r\nI consider reduction function is not supported tf.uint8, so please `raise TypeError`.\r\nThe code I linked looks relevant the problem. (I couldn't send Pull Request and propose good code.)\r\nI snipped a part of the error message.\r\nPlease fix.\r\n\r\n\u65e5\u672c\u8a9e\uff1a\r\n `tf.reduce_sum(tf.constant([1, 2, 3], dtype=tf.uint8))`\u3092\u5b9f\u884c\u3057\u305f\u969b\u306b\u30b3\u30a2\u30c0\u30f3\u30d7\u304c\u767a\u751f\u3057\u307e\u3057\u305f\r\n\u6050\u3089\u304freduce\u7cfb\u306e\u95a2\u6570\u3067tf.uint8\u3092\u30b5\u30dd\u30fc\u30c8\u3057\u3066\u3044\u306a\u3044\u304b\u3089\u3060\u3068\u601d\u3044\u307e\u3059\u3000\u305d\u306e\u305f\u3081`raise TypeError`\u306a\u3069\u3057\u3066\u307b\u3057\u3044\u3067\u3059\r\n\u3046\u307e\u3044\u30b3\u30fc\u30c9\u3082\u601d\u3044\u3064\u304b\u305a\u30d7\u30eb\u30ea\u30af\u30a8\u30b9\u30c8\u3082\u51fa\u305b\u306a\u304b\u3063\u305f\u306e\u3067\uff0c\u3053\u306e\u554f\u984c\u306b\u95a2\u9023\u3059\u308b\u3068\u601d\u308f\u308c\u308b\u30b3\u30fc\u30c9\u3092\u30ea\u30f3\u30af\u3057\u307e\u3057\u305f\uff0e\r\n\u30a8\u30e9\u30fc\u30e1\u30c3\u30bb\u30fc\u30b8\u306e\u4e00\u90e8\u3092\u8cbc\u308a\u4ed8\u3051\u307e\u3057\u305f\u3000\u4fee\u6b63\u304a\u9858\u3044\u3057\u307e\u3059\r\n\r\n## Environment\r\ntf.__version__ -> '1.4.1'\r\ncv2.__version__ -> '3.3.1'\r\nnp.__version__ -> '1.14.0'\r\n\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/bd0d204700df1a1a245b0593a11efd8ede139311/tensorflow/python/ops/math_ops.py#L1318\r\n\r\npython test.py\r\n2018-02-05 19:23:40.942499: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n*** Error in `/home/dl-box/miniconda2/envs/onyx/bin/python': double free or corruption (!prev): 0x00005636654f79f0 ***\r\n======= Backtrace: =========\r\n/lib/x86_64-linux-gnu/libc.so.6(+0x777e5)[0x7f7371da17e5]\r\n/lib/x86_64-linux-gnu/libc.so.6(+0x8037a)[0x7f7371daa37a]\r\n/lib/x86_64-linux-gnu/libc.so.6(cfree+0x4c)[0x7f7371dae53c]\r\n/usr/lib/x86_64-linux-gnu/libcuda.so.1(+0x30c906)[0x7f731ee39906]\r\n/usr/lib/x86_64-linux-gnu/libcuda.so.1(+0x30c974)[0x7f731ee39974]\r\n/usr/lib/x86_64-linux-gnu/libcuda.so.1(+0x1ef281)[0x7f731ed1c281]\r\n/usr/lib/x86_64-linux-gnu/libcuda.so.1(+0x1c79fb)[0x7f731ecf49fb]\r\n/usr/lib/x86_64-linux-gnu/libcuda.so.1(cuInit+0x4d)[0x7f731ed47abd]\r\n/home/dl-box/miniconda2/envs/onyx/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(+0x89b796)[0x7f73268a4796]\r\n/home/dl-box/miniconda2/envs/onyx/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN9perftools8gputools4cuda10CUDADriver4InitEv+0x5d)[0x7f73268a495d]\r\n/home/dl-box/miniconda2/envs/onyx/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZNK9perftools8gputools4cuda12CudaPlatform18VisibleDeviceCountEv+0x12)[0x7f73268b48a8]\r\n/home/dl-box/miniconda2/envs/onyx/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN10tensorflow20BaseGPUDeviceFactory17GetValidDeviceIdsERKSsPSt6vectorIiSaIiEE+0xf0)[0x7f73267d86b0]\r\n/home/dl-box/miniconda2/envs/onyx/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN10tensorflow20BaseGPUDeviceFactory13CreateDevicesERKNS_14SessionOptionsERKSsPSt6vectorIPNS_6DeviceESaIS8_EE+0x1b1)[0x7f73267db3b3]\r\n/home/dl-box/miniconda2/envs/onyx/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN10tensorflow13DeviceFactory10AddDevicesERKNS_14SessionOptionsERKSsPSt6vectorIPNS_6DeviceESaIS8_EE+0x159)[0x7f73267f8ddd]\r\n/home/dl-box/miniconda2/envs/onyx/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN10tensorflow20DirectSessionFactory10NewSessionERKNS_14SessionOptionsE+0x98)[0x7f732a1be5ba]\r\n/home/dl-box/miniconda2/envs/onyx/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN10tensorflow10NewSessionERKNS_14SessionOptionsEPPNS_7SessionE+0xfb)[0x7f732683666c]\r\n/home/dl-box/miniconda2/envs/onyx/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so(TF_NewDeprecatedSession+0x21)[0x7f732807e03f]\r\n/home/dl-box/miniconda2/envs/onyx/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so(+0xfa3267)[0x7f7327d5e267]\r\n/home/dl-box/miniconda2/envs/onyx/bin/python(_PyCFunction_FastCallDict+0x91)[0x56366192b7d1]\r\n/home/dl-box/miniconda2/envs/onyx/bin/python(+0x19e17c)[0x5636619bb17c]\r\n/home/dl-box/miniconda2/envs/onyx/bin/python(_PyEval_EvalFrameDefault+0x30a)[0x5636619ddbba]\r\n/home/dl-box/miniconda2/envs/onyx/bin/python(+0x197354)[0x5636619b4354]\r\n/home/dl-box/miniconda2/envs/onyx/bin/python(+0x19824f)[0x5636619b524f]\r\n/home/dl-box/miniconda2/envs/onyx/bin/python(+0x19e255)[0x5636619bb255]\r\n/home/dl-box/miniconda2/envs/onyx/bin/python(_PyEval_EvalFrameDefault+0x10b8)[0x5636619de968]\r\n/home/dl-box/miniconda2/envs/onyx/bin/python(+0x1974f6)[0x5636619b44f6]\r\n/home/dl-box/miniconda2/envs/onyx/bin/python(_PyFunction_FastCallDict+0x1bc)[0x5636619b56fc]\r\n/home/dl-box/miniconda2/envs/onyx/bin/python(_PyObject_FastCallDict+0x26f)[0x56366192bc5f]\r\n/home/dl-box/miniconda2/envs/onyx/bin/python(_PyObject_Call_Prepend+0x63)[0x5636619308c3]\r\n/home/dl-box/miniconda2/envs/onyx/bin/python(PyObject_Call+0x3e)[0x56366192b69e]\r\n/home/dl-box/miniconda2/envs/onyx/bin/python(+0x16a80b)[0x56366198780b]\r\n/home/dl-box/miniconda2/envs/onyx/bin/python(+0x19e4b7)[0x5636619bb4b7]\r\n/home/dl-box/miniconda2/envs/onyx/bin/python(_PyObject_FastCallDict+0x8b)[0x56366192ba7b]\r\n/home/dl-box/miniconda2/envs/onyx/bin/python(+0x19e2ce)[0x5636619bb2ce]\r\n/home/dl-box/miniconda2/envs/onyx/bin/python(_PyEval_EvalFrameDefault+0x30a)[0x5636619ddbba]\r\n/home/dl-box/miniconda2/envs/onyx/bin/python(PyEval_EvalCodeEx+0x329)[0x5636619b5d39]\r\n/home/dl-box/miniconda2/envs/onyx/bin/python(PyEval_EvalCode+0x1c)[0x5636619b6adc]\r\n/home/dl-box/miniconda2/envs/onyx/bin/python(+0x214be4)[0x563661a31be4]\r\n/home/dl-box/miniconda2/envs/onyx/bin/python(PyRun_FileExFlags+0xa1)[0x563661a31fe1]\r\n/home/dl-box/miniconda2/envs/onyx/bin/python(PyRun_SimpleFileExFlags+0x1c4)[0x563661a321e4]\r\n/home/dl-box/miniconda2/envs/onyx/bin/python(Py_Main+0x5ff)[0x563661a35cbf]\r\n/home/dl-box/miniconda2/envs/onyx/bin/python(main+0xee)[0x5636618fcdbe]\r\n/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf0)[0x7f7371d4a830]\r\n/home/dl-box/miniconda2/envs/onyx/bin/python(+0x1c75eb)[0x5636619e45eb]\r\n======= Memory map: ========\r\n56366181d000-563661adb000 r-xp 00000000 08:01 4994230                    /home/dl-box/miniconda2/envs/onyx/bin/python3.6\r\n563661cdb000-563661cde000 r--p 002be000 08:01 4994230                    /home/dl-box/miniconda2/envs/onyx/bin/python3.6\r\n563661cde000-563661d41000 rw-p 002c1000 08:01 4994230                    /home/dl-box/miniconda2/envs/onyx/bin/python3.6\r\n563661d41000-563661d72000 rw-p 00000000 00:00 0 \r\n56366386c000-5636659ec000 rw-p 00000000 00:00 0                          [heap]\r\n7f72b4000000-7f72b4021000 rw-p 00000000 00:00 0 \r\n7f72b4021000-7f72b8000000 ---p 00000000 00:00 0 \r\n.\r\n\r\n7f7370df8000-7f7370dfb000 r-xp 00000000 08:01 4988702                    /home/dl-box/miniconda2/envs/onyx/lib/python3.6/lib-dynload/_heapq.cpython-36m-x86_64-linux-gnu.so\r\n7f7370dfb000-7f7370ffa000 ---p 00003000 08:01 4988702                    /home/dl-box/miniconda2/envs/onyx/lib/python3.6/lib-dynload/_heapq.cpython-36m-x86_64-linux-gnu.so\r\n7f7370ffa000-7f7370ffb000 r--p 00002000 08:01 4988702                    /home/dl-box/miniconda2/envs/onyx/lib/python3.6/lib-dynload/_heapq.cpython-36m-x86_64-linux-gnu.so\r\n7f7370ffb000-7f7370ffd000 rw-p 00003000 08:01 4988702                    /home/dl-box/miniconda2/envs/onyx/lib/python3.6/lib-dynload/_heapq.cpython-36m-x86_64-linux-gnu.so\r\n7f7370ffd000-7f737103d000 rw-p 00000000 00:00 0 \r\n7f737103d000-7f7371412000 r--p 00000000 08:01 1837470                    /usr/lib/locale/locale-archive\r\n7f7371412000-7f737151a000 r-xp 00000000 08:01 131145                     /lib/x86_64-linux-gnu/libm-2.23.so\r\n7f737151a000-7f7371719000 ---p 00108000 08:01 131145                     /lib/x86_64-linux-gnu/libm-2.23.so\r\n7f7371719000-7f737171a000 r--p 00107000 08:01 131145                     /lib/x86_64-linux-gnu/libm-2.23.so\r\n7f737171a000-7f737171b000 rw-p 00108000 08:01 131145                     /lib/x86_64-linux-gnu/libm-2.23.so\r\n7f737171b000-7f7371722000 r-xp 00000000 08:01 131862                     /lib/x86_64-linux-gnu/librt-2.23.so\r\n7f7371722000-7f7371921000 ---p 00007000 08:01 131862                     /lib/x86_64-linux-gnu/librt-2.23.so\r\n7f7371921000-7f7371922000 r--p 00006000 08:01 131862                     /lib/x86_64-linux-gnu/librt-2.23.so\r\n7f7371922000-7f7371923000 rw-p 00007000 08:01 131862                     /lib/x86_64-linux-gnu/librt-2.23.so\r\n7f7371923000-7f7371925000 r-xp 00000000 08:01 131483                     /lib/x86_64-linux-gnu/libutil-2.23.so\r\n7f7371925000-7f7371b24000 ---p 00002000 08:01 131483                     /lib/x86_64-linux-gnu/libutil-2.23.so\r\n7f7371b24000-7f7371b25000 r--p 00001000 08:01 131483                     /lib/x86_64-linux-gnu/libutil-2.23.so\r\n7f7371b25000-7f7371b26000 rw-p 00002000 08:01 131483                     /lib/x86_64-linux-gnu/libutil-2.23.so\r\n7f7371b26000-7f7371b29000 r-xp 00000000 08:01 131431                     /lib/x86_64-linux-gnu/libdl-2.23.so\r\n7f7371b29000-7f7371d28000 ---p 00003000 08:01 131431                     /lib/x86_64-linux-gnu/libdl-2.23.so\r\n7f7371d28000-7f7371d29000 r--p 00002000 08:01 131431                     /lib/x86_64-linux-gnu/libdl-2.23.so\r\n7f7371d29000-7f7371d2a000 rw-p 00003000 08:01 131431                     /lib/x86_64-linux-gnu/libdl-2.23.so\r\n7f7371d2a000-7f7371eea000 r-xp 00000000 08:01 131427                     /lib/x86_64-linux-gnu/libc-2.23.so\r\n7f7371eea000-7f73720ea000 ---p 001c0000 08:01 131427                     /lib/x86_64-linux-gnu/libc-2.23.so\r\n7f73720ea000-7f73720ee000 r--p 001c0000 08:01 131427                     /lib/x86_64-linux-gnu/libc-2.23.so\r\n7f73720ee000-7f73720f0000 rw-p 001c4000 08:01 131427                     /lib/x86_64-linux-gnu/libc-2.23.so\r\n7f73720f0000-7f73720f4000 rw-p 00000000 00:00 0 \r\n7f73720f4000-7f737210c000 r-xp 00000000 08:01 131426                     /lib/x86_64-linux-gnu/libpthread-2.23.so\r\n7f737210c000-7f737230b000 ---p 00018000 08:01 131426                     /lib/x86_64-linux-gnu/libpthread-2.23.so\r\n7f737230b000-7f737230c000 r--p 00017000 08:01 131426                     /lib/x86_64-linux-gnu/libpthread-2.23.so\r\n7f737230c000-7f737230d000 rw-p 00018000 08:01 131426                     /lib/x86_64-linux-gnu/libpthread-2.23.so\r\n7f737230d000-7f7372311000 rw-p 00000000 00:00 0 \r\n7f7372311000-7f7372337000 r-xp 00000000 08:01 131335                     /lib/x86_64-linux-gnu/ld-2.23.so\r\n7f7372344000-7f7372509000 rw-p 00000000 00:00 0 \r\n7f737252d000-7f737252e000 rw-p 00000000 00:00 0 \r\n7f737252e000-7f737252f000 rwxp 00000000 00:00 0 \r\n7f737252f000-7f7372536000 r--s 00000000 08:01 2230848                    /usr/lib/x86_64-linux-gnu/gconv/gconv-modules.cache\r\n7f7372536000-7f7372537000 r--p 00025000 08:01 131335                     /lib/x86_64-linux-gnu/ld-2.23.so\r\n7f7372537000-7f7372538000 rw-p 00026000 08:01 131335                     /lib/x86_64-linux-gnu/ld-2.23.so\r\n7f7372538000-7f7372539000 rw-p 00000000 00:00 0 \r\n7ffdfa56f000-7ffdfa58f000 rwxp 00000000 00:00 0                          [stack]\r\n7ffdfa58f000-7ffdfa590000 rw-p 00000000 00:00 0 \r\n7ffdfa5c0000-7ffdfa5c3000 r--p 00000000 00:00 0                          [vvar]\r\n7ffdfa5c3000-7ffdfa5c5000 r-xp 00000000 00:00 0                          [vdso]\r\nffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0                  [vsyscall]\r\nAborted (core dumped)", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Excuse me, I missed CONTRIBUTE.md.\r\nMy run environment is following.\r\n\r\nOS Platform and Distribution: Ubuntu 16.04\r\nTensorFlow installed from: pip\r\nTensorFlow version: 1.4.1\r\nBazel version: 0.9.0\r\nCUDA/cuDNN version: Cuda compilation tools, release 9.1, V9.1.85\r\nGPU model and memory:  GeForce GTX 1080 Ti * 3, 11172MiB * 3\r\nExact command to reproduce:\r\n\r\n```python\r\nimport tensorflow as tf\r\nwith tf.Session() as sess:\r\n  sess.run(tf.reduce_sum(tf.constant([1, 2, 3], dtype=tf.uint8)))\r\n```", "@rmlarsen can you take a look or redirect? Thanks!", "\r\nWas able to to resolve the issue with TF v2.5 ,please find the [gist ](https://colab.research.google.com/gist/mohantym/20603960837aff638a7817000c375f66/16771.ipynb)here ..Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/16771\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/16771\">No</a>\n"]}, {"number": 16770, "title": "TensorBoard Projector has been blocked by CORS policy: No 'Access-Control-Allow-Origin'", "body": "**At http://projector.tensorflow.org/ website error.**\r\n### System information\r\n- **Have I written custom code**:\r\nno\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nmacOS 10.13.3, \r\nSafari: Version 11.0.3 (13604.5.6)\r\nChrome: Version 63.0.3239.132 (Official Build) (64-bit)  \r\n- **TensorFlow installed from (source or binary)**:\r\nno\r\n\r\nI am trying to publish my data over official projector.tensorflow.org website.\r\n1) Prepared: config, data, metadata.\r\n2) Hosted them on ([Gist](https://gist.githubusercontent.com/VolodymyrPavliukevych/fae3d662ca170666d1e82a85bf530043/raw/efa07424c08b2e8bd6d96f59501685ae578a6018/rada_full_packed_projector_config.json)/[google storage](https://storage.googleapis.com/api.octadero.com/rada/rada_full_packed_projector_config.json)/[my own website](http://octadero.com/API/rada_full_packed_projector_config.json)) \r\nWith and without https.\r\n3) Trying to view on projector: [Published config](http://projector.tensorflow.org/?config=https://gist.githubusercontent.com/VolodymyrPavliukevych/fae3d662ca170666d1e82a85bf530043/raw/efa07424c08b2e8bd6d96f59501685ae578a6018/rada_full_packed_projector_config.json)\r\n\r\nReceive error: Failed to load [cut](https://gist.github.com/VolodymyrPavliukevych/8e28b38560086b17148aa6d30aa3e264/raw/a63528b9d22bfa9d43c9c906642099f9774713fe/rada_full_packed): Redirect from '[cut](https://gist.github.com/VolodymyrPavliukevych/8e28b38560086b17148aa6d30aa3e264/raw/a63528b9d22bfa9d43c9c906642099f9774713fe/rada_full_packed)' to '[cut](https://gist.githubusercontent.com/VolodymyrPavliukevych/8e28b38560086b17148aa6d30aa3e264/raw/a63528b9d22bfa9d43c9c906642099f9774713fe/rada_full_packed)' has been blocked by CORS policy: No 'Access-Control-Allow-Origin' header is present on the requested resource. Origin 'http://projector.tensorflow.org' is therefore not allowed access.\r\n\r\n![error-screen](https://user-images.githubusercontent.com/1786378/35802959-c419a2c0-0a7a-11e8-9de7-4e71bb0f30e9.png)\r\n\r\nThere is way to launch chrome with `--disable-web-security` key, but that ruins idea to share data in public. \r\n", "comments": ["Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hi,\r\n\r\nSorry for the delay here. Seems like the error is due to the fact that `gist.github.com/VolodymyrPavliukevych/...` redirects to `gist.githubusercontent.com/VolodymyrPavliukevych`, and the latter one has `Access-Control-Allow-Origin: *` in the header.\r\n\r\nTry updating the `tensorPath` and `metadataPath` in your [config.json](https://gist.githubusercontent.com/VolodymyrPavliukevych/fae3d662ca170666d1e82a85bf530043/raw/efa07424c08b2e8bd6d96f59501685ae578a6018/rada_full_packed_projector_config.json) to point to `gist.githubusercontent.com` instead of `gist.github.com`.\r\n\r\nHope this helps!", "Hi, thanks for explanation. [It works](http://projector.tensorflow.org/?config=https://gist.githubusercontent.com/VolodymyrPavliukevych/fe9d724a2d47085798dce306b22a377f/raw/749c9e066c2fe5f7b1decb062c00803f3b72f6da/rada_full_packed_projector_config.json).\r\nI have one more question, why I can't share on my own web site or [Google storage](https://storage.googleapis.com/api.octadero.com/rada/projector/rada_full_packed_projector_config.json)? \r\nThe same error on [projector](http://projector.tensorflow.org/?config=https://storage.googleapis.com/api.octadero.com/rada/projector/rada_full_packed_projector_config.json).\r\nIs that because I have to add \"access-control-allow-origin:*\" in headers at my hosting? ", "That's correct, the server that hosts your website should serve with \"access-control-allow-origin:*\", which is unusual for out-of-the-box hosting, since it's not an API service.", "Just wanted to add that you may have to restart your browser or otherwise reset things, I spent a good time messing with configurations before I noticed I was getting 304 (no change) return codes, and Firefox was apparently caching the access control policy as well as the contents.  Works now :-)"]}, {"number": 16769, "title": "remove keep_dims warning in maxout layer", "body": "Trivially replaced keep_dims with keepdims", "comments": []}, {"number": 16768, "title": "OOM when allocating tensor with shape", "body": "I'm using tensorflow 1.3, tested on a linux machine with 2 NVIDIA Tesla K80 cards,\r\nhowever, I keep getting OOM error on GPU, but it does not happen when using cpu for training:\r\n\r\n-------------------------------------\r\n---log below\r\n-------------------------------------\r\n\r\nException happened during training, message: OOM when allocating tensor with shape[1792,4096]\r\n\t [[Node: projectx/trainig_gpu_0/gradients/projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/MatMul_36_grad/MatMul_1 = MatMul[T=DT_FLOAT, transpose_a=true, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/concat_36, projectx/trainig_gpu_0/gradients/projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/BiasAdd_36_grad/tuple/control_dependency)]]\r\n\t [[Node: projectx/trainig_gpu_0/gradients/concat/_2929 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_646323_projectx/trainig_gpu_0/gradients/concat\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\nCaused by op 'projectx/trainig_gpu_0/gradients/projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/MatMul_36_grad/MatMul_1', defined at:\r\n  File \"sync_train.py\", line 383, in <module>\r\n    main()\r\n  File \"sync_train.py\", line 380, in main\r\n    train(config)\r\n  File \"sync_train.py\", line 64, in train\r\n    train_model = projectx.projectx(n_gpu, config, is_training=True, reuse=False)\r\n  File \"/kaldi/exp/tacotron/exp_2/projectx.py\", line 291, in __init__\r\n    grads_and_vars = self.optimizer.compute_gradients(loss)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py\", line 414, in compute_gradients\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 581, in gradients\r\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 353, in _MaybeCompile\r\n    return grad_fn()  # Exit early\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 581, in <lambda>\r\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_grad.py\", line 922, in _MatMulGrad\r\n    grad_b = math_ops.matmul(a, grad, transpose_a=True)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py\", line 1891, in matmul\r\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 2437, in _mat_mul\r\n    name=name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\n...which was originally created as op 'projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/MatMul_36', defined at:\r\n  File \"sync_train.py\", line 383, in <module>\r\n    main()\r\n[elided 1 identical lines from previous traceback]\r\n  File \"sync_train.py\", line 64, in train\r\n    train_model = projectx.projectx(n_gpu, config, is_training=True, reuse=False)\r\n  File \"/kaldi/exp/tacotron/exp_2/projectx.py\", line 189, in __init__\r\n    feed_previous=feed_previous)\r\n  File \"/kaldi/exp/tacotron/exp_2/projectx.py\", line 427, in seq2seq\r\n    pre_alignments)\r\n  File \"/kaldi/exp/tacotron/exp_2/decoder.py\", line 99, in __call__\r\n    attention_rnn_outputs, new_attention_rnn_state, context, alignments = self._attention_rnn_cell(prenet_output, state, pre_alignments)\r\n  File \"/kaldi/exp/tacotron/exp_2/attention.py\", line 427, in __call__\r\n    lstm_output, next_lstm_state = cell(lstm_inputs, states[i + 1])\r\n  File \"/kaldi/exp/tacotron/exp_2/zoneout_lstm.py\", line 48, in __call__\r\n    output, new_state = self._cell(inputs, state, scope)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 183, in __call__\r\n    return super(RNNCell, self).__call__(inputs, state)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/base.py\", line 575, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 611, in call\r\n    lstm_matrix = self._linear1([inputs, m_prev])\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 1189, in __call__\r\n    res = math_ops.matmul(array_ops.concat(args, 1), self._weights)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py\", line 1891, in matmul\r\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 2437, in _mat_mul\r\n    name=name)\r\n\r\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1792,4096]\r\n\t [[Node: projectx/trainig_gpu_0/gradients/projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/MatMul_36_grad/MatMul_1 = MatMul[T=DT_FLOAT, transpose_a=true, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/concat_36, projectx/trainig_gpu_0/gradients/projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/BiasAdd_36_grad/tuple/control_dependency)]]\r\n\t [[Node: projectx/trainig_gpu_0/gradients/concat/_2929 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_646323_projectx/trainig_gpu_0/gradients/concat\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]", "comments": ["Well, I think it's because you're running out of memory. Unfortunately there isn't any easy fix. If you provide more detailed information we could take a quick look to see if there's something obvious.", "@drpngx  thx for attention;\r\n\r\nI tried batch size 64 or 32, both failed, I also tried:\r\n        custom_config=tf.ConfigProto(log_device_placement=True, allow_soft_placement=True)\r\n        custom_config.gpu_options.allocator_type = 'BFC'\r\n        custom_config.gpu_options.per_process_gpu_memory_fraction = 0.90\r\n\r\nfailed too;\r\n\r\nthe error happened when computing the gradient, \r\n\r\nhere're my GPUs:\r\n\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla K80           Off  | 259C:00:00.0     Off |                    0 |\r\n| N/A   30C    P8    26W / 149W |      0MiB / 11439MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla K80           Off  | 3A8D:00:00.0     Off |                    0 |\r\n| N/A   26C    P8    32W / 149W |      0MiB / 11439MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n", "Normally I think we would allocate the memory on all detected devices. Could you check the initial message to see if they are detected and used? You can use `with tf.device('/gpu:1)` to manually place.", "yes, I manually placed the training set, here on my machine n_gpu is 2, tower_encoder_inputs and tower_decoder_inputs is splitted according to n_gpu\r\n\r\n        for i in range(n_gpu):\r\n            with tf.variable_scope(variable_scope, reuse=True if i > 0 else reuse):\r\n                with tf.device(\"/gpu:{}\".format(i)):\r\n                    with tf.name_scope(\"trainig_gpu_{}\".format(i)):\r\n                        print(\"building train graph on GPU {}\".format(i))\r\n\r\n                        #outputs, attentions, stop_tokens, n_predicted = self.seq2seq(\r\n                        outputs, attentions= seq2seq(\r\n                            self.tower_encoder_inputs[i],\r\n                            self.tower_decoder_inputs[i])\r\n\r\n\r\n\r\nthx\r\n", "it  turns out my lstm dimension is too big, I used 2 layer lstm in my network with dim 1024, when I change it to 256, OOM error gone;", "Hi, I also encountered this problem. I ran my code using the GPU version and got this error. However, when I used my other laptop using the CPU version, there was no error. Without changing anything. Have you found the reason for this?", "@chaine09 It depends on what your exact hardware setting is. It'll take ages to train your network on the CPU. Just trying lowering the batch size until you can get it to run on the GPU.", "I am doing performance testing inception model flask api with gunicorn (creating multiple process) Error: OOM when allocating tensor with shape[800,1280,3] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [[Node: Cast = CastDstT=DT_FLOAT, SrcT=DT_UINT8, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]] Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\nCaused by op 'Cast', defined at: File \"/usr/local/bin/gunicorn\", line 11, in sys.exit(run()) File \"/usr/local/lib/python3.5/dist-packages/gunicorn/app/wsgiapp.py\", line 61, in run WSGIApplication(\"%(prog)s [OPTIONS] [APP_MODULE]\").run() File \"/usr/local/lib/python3.5/dist-packages/gunicorn/app/base.py\", line 223, in run super(Application, self).run() File \"/usr/local/lib/python3.5/dist-packages/gunicorn/app/base.py\", line 72, in run Arbiter(self).run() File \"/usr/local/lib/python3.5/dist-packages/gunicorn/arbiter.py\", line 212, in run self.manage_workers() File \"/usr/local/lib/python3.5/dist-packages/gunicorn/arbiter.py\", line 545, in manage_workers self.spawn_workers() File \"/usr/local/lib/python3.5/dist-packages/gunicorn/arbiter.py\", line 616, in spawn_workers self.spawn_worker() File \"/usr/local/lib/python3.5/dist-packages/gunicorn/arbiter.py\", line 583, in spawn_worker worker.init_process() File \"/usr/local/lib/python3.5/dist-packages/gunicorn/workers/base.py\", line 134, in init_process self.run() File \"/usr/local/lib/python3.5/dist-packages/gunicorn/workers/sync.py\", line 124, in run self.run_for_one(timeout) File \"/usr/local/lib/python3.5/dist-packages/gunicorn/workers/sync.py\", line 68, in run_for_one self.accept(listener) File \"/usr/local/lib/python3.5/dist-packages/gunicorn/workers/sync.py\", line 30, in accept self.handle(listener, client, addr) File \"/usr/local/lib/python3.5/dist-packages/gunicorn/workers/sync.py\", line 135, in handle self.handle_request(listener, req, client, addr) File \"/usr/local/lib/python3.5/dist-packages/gunicorn/workers/sync.py\", line 176, in handle_request respiter = self.wsgi(environ, resp.start_response) File \"/usr/local/lib/python3.5/dist-packages/flask/app.py\", line 1997, in call return self.wsgi_app(environ, start_response) File \"/usr/local/lib/python3.5/dist-packages/flask/app.py\", line 1982, in wsgi_app response = self.full_dispatch_request() File \"/usr/local/lib/python3.5/dist-packages/flask/app.py\", line 1612, in full_dispatch_request rv = self.dispatch_request() File \"/usr/local/lib/python3.5/dist-packages/flask/app.py\", line 1598, in dispatch_request return self.view_functionsrule.endpoint File \"/home/ubuntu/cv_workspace/computer_vision_services.py\", line 1480, in classify_bulk input_operation, output_operation, tf_session=sess) File \"/home/ubuntu/cv_workspace/src/apis/ImgClassification.py\", line 228, in classifyImageInSess t = read_tensor_from_image_file(file_name) File \"/home/ubuntu/cv_workspace/src/apis/ImgClassification.py\", line 51, in read_tensor_from_image_file float_caster = tf.cast(image_reader, tf.float32) File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py\", line 758, in cast return gen_math_ops.cast(x, base_type, name=name) File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 919, in cast \"Cast\", x=x, DstT=DstT, name=name) File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper op_def=op_def) File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3160, in create_op op_def=op_def) File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1625, in init self._traceback = self._graph._extract_stack() # pylint: disable=protected-access\r\n\r\n\r\n\r\n################ GPU \r\n![image](https://user-images.githubusercontent.com/2271152/41229156-a212955c-6d98-11e8-8a7e-8d41001f3036.png)\r\n\r\n\r\n\r\n", "i got the same problem,then used smaller size of lstm hidden dim, i solved it.", "It would be run If you set lowering batch size as well Input size ", "I found that it helps to build your own generator function that yields X and y in the shape needed. You'll have to use `model.fit_generator`, and make sure you can change around the size of the 'sub-batches' within the generator. For example:\r\n\r\n```python\r\ndef generator(self, maxbatch=None):\r\n\r\n    # Let `alldata` just be all your data, of course.\r\n    for batch in alldata:\r\n        # ... do stuff to your batch if you want\r\n        x, y = batch\r\n        if not maxbatch:\r\n            return x, y\r\n\r\n        for i in range(0, size, maxbatch):\r\n            if i + maxbatch > size:\r\n                yield x[i: size], y[i: size]\r\n            else:\r\n                yield x[i: i+maxbatch], y[i: i+maxbatch]\r\n\r\n# Then you can fit your model with this function\r\nmodel.fit_generator(self.generator, ...)\r\n```\r\n\r\nI also found that (if appropriate) using dropout can help.", "`OOM when allocating tensor with shape[320000,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc`\r\nI got this message while training CNN\r\nSpecifying batch_size 16, 32, 64, 128 did not help\r\n\r\n", "> it turns out my lstm dimension is too big, I used 2 layer lstm in my network with dim 1024, when I change it to 256, OOM error gone;\r\n\r\n\r\n\r\n> I am facing the similiar issue. How to change this configuration.\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[64,672,9,9] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n\t [[node SecondStageFeatureExtractor/cell_12/comb_iter_0/right/separable_3x3_2/separable_conv2d/depthwise (defined at/models/research/slim/nets/nasnet/nasnet_utils.py:202) ]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n", "maybe, kill -9 PID and run again works?", "ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[24,1280,10,10] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n         [[{{node gradients/BoxPredictor_1/ClassPredictor/Conv2D_grad/Conv2DBackpropInput}} = Conv2DBackpropInput[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](gradients/BoxPredictor_1/ClassPredictor/Conv2D_grad/Conv2DBackpropInput-0-LayoutOptimizer, BoxPredictor_1/ClassPredictor/weights/read/_3117, gradients/BoxPredictor_1/ClassPredictor/Conv2D_grad/Conv2DBackpropFilter-2-TransposeNHWCToNCHW-LayoutOptimizer, ^gradients/BoxPredictor_1/ClassPredictor/BiasAdd_grad/BiasAddGrad)]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n         [[{{node gradients/FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/depthwise_grad/tuple/control_dependency_1/_6251}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_14788...pendency_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\nGuys, this is the error that I encounter during training. What do you think is the solution on thois one?", "I am getting the same error", "For the people stuck with this in models other than mnist.\r\nthe reason for this is the high amount of parameters (please check your `model.summary()`).\r\n\r\nA good method to drastically lower these parameters is to add:\r\n`subsample=(2, 2)` (careful it lowers the resolution of images/data) in all the Convolutional layers above that Flatten layer,\r\nif subsample doesn't work then it is `stride=(2, 2)`.", "can someone please explain why i get this error with an nvidea rtx 2070super and not with nvidia gtx 1050ti  with the same code and same parameters? \r\n\r\n (0) Resource exhausted: OOM when allocating tensor with shape[64,58,28,28] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n\t [[{{node batch_normalization_47/FusedBatchNorm}}]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n\t [[loss/mul/_5287]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n  (1) Resource exhausted: OOM when allocating tensor with shape[64,58,28,28] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n\t [[{{node batch_normalization_47/FusedBatchNorm}}]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n0 successful operations.\r\n0 derived errors ignored.", "Reduce the size of image\n\nOn Thu, 19 Sep, 2019, 10:48 PM Misterio0, <notifications@github.com> wrote:\n\n> can someone please explain why i get this error with an nvidea rtx\n> 2070super and not with nvidia gtx 1050ti with the same code and same\n> parameters?\n>\n> (0) Resource exhausted: OOM when allocating tensor with shape[64,58,28,28]\n> and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator\n> GPU_0_bfc\n> [[{{node batch_normalization_47/FusedBatchNorm}}]]\n> Hint: If you want to see a list of allocated tensors when OOM happens, add\n> report_tensor_allocations_upon_oom to RunOptions for current allocation\n> info.\n>\n>  [[loss/mul/_5287]]\n>\n> Hint: If you want to see a list of allocated tensors when OOM happens, add\n> report_tensor_allocations_upon_oom to RunOptions for current allocation\n> info.\n>\n> (1) Resource exhausted: OOM when allocating tensor with shape[64,58,28,28]\n> and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator\n> GPU_0_bfc\n> [[{{node batch_normalization_47/FusedBatchNorm}}]]\n> Hint: If you want to see a list of allocated tensors when OOM happens, add\n> report_tensor_allocations_upon_oom to RunOptions for current allocation\n> info.\n>\n> 0 successful operations.\n> 0 derived errors ignored.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/16768?email_source=notifications&email_token=AAK7M5ONFNZVFEVAB7HT4E3QKPXSFA5CNFSM4EPEAQVKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD7E5A6I#issuecomment-533319801>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAK7M5OAMZ435DDXOCI2T6DQKPXSFANCNFSM4EPEAQVA>\n> .\n>\n", "> it turns out my lstm dimension is too big, I used 2 layer lstm in my network with dim 1024, when I change it to 256, OOM error gone;\r\n\r\nCould you please explain how did you change the dim to 256?", "In the configuration file. Change the dimensions\n\nOn Sun, 22 Sep, 2019, 12:00 AM modeepfake, <notifications@github.com> wrote:\n\n> it turns out my lstm dimension is too big, I used 2 layer lstm in my\n> network with dim 1024, when I change it to 256, OOM error gone;\n>\n> Could you please explain how did you change the dim to 256?\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/16768?email_source=notifications&email_token=AAK7M5NFO77DXCY3GB35GWTQK2RSHA5CNFSM4EPEAQVKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD7I27RI#issuecomment-533835717>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAK7M5IUTOG6IH32XAROOTDQK2RSHANCNFSM4EPEAQVA>\n> .\n>\n", "> It would be run If you set lowering batch size as well Input size\r\n\r\nit worked", "I had this same issue on Google Colab. Wiredly enough, when I took a few layers out of my network the number of parameters went up (since a few max poolings were gone). This solved the problem however... So the comment regarding parameter number is incorrect. If I'm missing something, let me know!\r\n\r\nregards", "I had the same problem. I solved this problem by reducing the number of layer neurons.", "Even I got the same problem while working on NVIDIA Tegra X1 (nvgpu)/integrated and it has 62GB of memory \r\nI have also tried reducing the batch_size and image dimensions as well \r\n\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[1,1,2048,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Add] name: conv5_block3_1_conv/kernel/Initializer/random_uniform/\r\n\r\ntried with batch_size=32,16\r\n\r\nIf there is any other alternative to solve the problem\r\n\r\nDo share\r\n\r\nThanks in advance\r\n ", "> Even I got the same problem while working on NVIDIA Tegra X1 (nvgpu)/integrated and it has 62GB of memory\r\n> I have also tried reducing the batch_size and image dimensions as well\r\n> \r\n> tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[1,1,2048,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Add] name: conv5_block3_1_conv/kernel/Initializer/random_uniform/\r\n> \r\n> tried with batch_size=32,16\r\n> \r\n> If there is any other alternative to solve the problem\r\n> \r\n> Do share\r\n> \r\n> Thanks in advance\r\n\r\nAlso if we have any formula to calculate the batch_size or epochs from the Number of input images \r\nPlease share", "I am facing the same problem. The model is trained well on CPU and the problem only appears on GPU. Isn't there a solution? It is not reasonable to change the Python code to solve it", "Your GPU memory is less, decrease the batch size and try.\n\nOn Mon, Jun 1, 2020, 12:50 Orwa Kassab <notifications@github.com> wrote:\n\n> I am facing the same problem. The model is trained well on CPU and the\n> problem only appears on GPU. Isn't there a solution? It is not reasonable\n> to change the Python code to solve it\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/16768#issuecomment-636659972>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ADECJFHQKQUO33RF6SPWUOTRUNJDBANCNFSM4EPEAQVA>\n> .\n>\n", "> Your GPU memory is less, decrease the batch size and try.\r\n> [\u2026](#)\r\n> On Mon, Jun 1, 2020, 12:50 Orwa Kassab ***@***.***> wrote: I am facing the same problem. The model is trained well on CPU and the problem only appears on GPU. Isn't there a solution? It is not reasonable to change the Python code to solve it \u2014 You are receiving this because you commented. Reply to this email directly, view it on GitHub <[#16768 (comment)](https://github.com/tensorflow/tensorflow/issues/16768#issuecomment-636659972)>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/ADECJFHQKQUO33RF6SPWUOTRUNJDBANCNFSM4EPEAQVA> .\r\n\r\nIt did not work. I tried multiple values for the batch size and still getting the same exception.\r\nI monitored the usage of my GPU and it only reached about 14% as max at the first moments of training the model!!!!!!!!!\r\nMy GPU info: NVIDIA GeForce GTX 1050 Ti with a 4GB memory", "1050 has 4 gb memory, which is pretty low. Only reducing batch size won't\nwork. Resize all your images to 30*30. Run a loop through the images to\nresize them. Then try to train with batch size of 16.\n\n\nOn Mon, Jun 1, 2020, 13:52 Orwa Kassab <notifications@github.com> wrote:\n\n> Your GPU memory is less, decrease the batch size and try.\n> \u2026 <#m_7891378449595475481_>\n> On Mon, Jun 1, 2020, 12:50 Orwa Kassab *@*.***> wrote: I am facing the\n> same problem. The model is trained well on CPU and the problem only appears\n> on GPU. Isn't there a solution? It is not reasonable to change the Python\n> code to solve it \u2014 You are receiving this because you commented. Reply to\n> this email directly, view it on GitHub <#16768 (comment)\n> <https://github.com/tensorflow/tensorflow/issues/16768#issuecomment-636659972>>,\n> or unsubscribe\n> https://github.com/notifications/unsubscribe-auth/ADECJFHQKQUO33RF6SPWUOTRUNJDBANCNFSM4EPEAQVA\n> .\n>\n> It did not work. I tried multiple values for the batch size and still\n> getting the same exception.\n> I monitored the usage of my GPU and it only reached about 14% as max at\n> the first moments of training the model!!!!!!!!!\n> My GPU info: NVIDIA GeForce GTX 1050\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/16768#issuecomment-636691820>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ADECJFDKBIWXZAWYDEIJIC3RUNQOBANCNFSM4EPEAQVA>\n> .\n>\n", "OOM means out out of memory, GPU usage and graphics memory usage are two\ndifferent things\n\nOn Mon, Jun 1, 2020, 13:56 Ritaprava Dutta <ritaprava95@gmail.com> wrote:\n\n> 1050 has 4 gb memory, which is pretty low. Only reducing batch size won't\n> work. Resize all your images to 30*30. Run a loop through the images to\n> resize them. Then try to train with batch size of 16.\n>\n>\n> On Mon, Jun 1, 2020, 13:52 Orwa Kassab <notifications@github.com> wrote:\n>\n>> Your GPU memory is less, decrease the batch size and try.\n>> \u2026 <#m_-4117985182330873088_m_7891378449595475481_>\n>> On Mon, Jun 1, 2020, 12:50 Orwa Kassab *@*.***> wrote: I am facing the\n>> same problem. The model is trained well on CPU and the problem only appears\n>> on GPU. Isn't there a solution? It is not reasonable to change the Python\n>> code to solve it \u2014 You are receiving this because you commented. Reply to\n>> this email directly, view it on GitHub <#16768 (comment)\n>> <https://github.com/tensorflow/tensorflow/issues/16768#issuecomment-636659972>>,\n>> or unsubscribe\n>> https://github.com/notifications/unsubscribe-auth/ADECJFHQKQUO33RF6SPWUOTRUNJDBANCNFSM4EPEAQVA\n>> .\n>>\n>> It did not work. I tried multiple values for the batch size and still\n>> getting the same exception.\n>> I monitored the usage of my GPU and it only reached about 14% as max at\n>> the first moments of training the model!!!!!!!!!\n>> My GPU info: NVIDIA GeForce GTX 1050\n>>\n>> \u2014\n>> You are receiving this because you commented.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/16768#issuecomment-636691820>,\n>> or unsubscribe\n>> <https://github.com/notifications/unsubscribe-auth/ADECJFDKBIWXZAWYDEIJIC3RUNQOBANCNFSM4EPEAQVA>\n>> .\n>>\n>\n", "> OOM means out out of memory, GPU usage and graphics memory usage are two different things\r\n> [\u2026](#)\r\n> On Mon, Jun 1, 2020, 13:56 Ritaprava Dutta ***@***.***> wrote: 1050 has 4 gb memory, which is pretty low. Only reducing batch size won't work. Resize all your images to 30*30. Run a loop through the images to resize them. Then try to train with batch size of 16. On Mon, Jun 1, 2020, 13:52 Orwa Kassab ***@***.***> wrote: > Your GPU memory is less, decrease the batch size and try. > \u2026 <#m_-4117985182330873088_m_7891378449595475481_> > On Mon, Jun 1, 2020, 12:50 Orwa Kassab *@*.***> wrote: I am facing the > same problem. The model is trained well on CPU and the problem only appears > on GPU. Isn't there a solution? It is not reasonable to change the Python > code to solve it \u2014 You are receiving this because you commented. Reply to > this email directly, view it on GitHub <#16768 (comment) > <[#16768 (comment)](https://github.com/tensorflow/tensorflow/issues/16768#issuecomment-636659972)>>, > or unsubscribe > https://github.com/notifications/unsubscribe-auth/ADECJFHQKQUO33RF6SPWUOTRUNJDBANCNFSM4EPEAQVA > . > > It did not work. I tried multiple values for the batch size and still > getting the same exception. > I monitored the usage of my GPU and it only reached about 14% as max at > the first moments of training the model!!!!!!!!! > My GPU info: NVIDIA GeForce GTX 1050 > > \u2014 > You are receiving this because you commented. > Reply to this email directly, view it on GitHub > <[#16768 (comment)](https://github.com/tensorflow/tensorflow/issues/16768#issuecomment-636691820)>, > or unsubscribe > <https://github.com/notifications/unsubscribe-auth/ADECJFDKBIWXZAWYDEIJIC3RUNQOBANCNFSM4EPEAQVA> > . >\r\n\r\nIt worked with a batch size of 8. Thanks very much", "I am using Google Colab. and i used TPU and it worked. ", "Well done.\n\nOn Sun, Jun 7, 2020, 9:10 AM Arvina Kori <notifications@github.com> wrote:\n\n> I am using Google Colab. and i used TPU and it worked.\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/16768#issuecomment-640168396>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AIT525RNW3FTKDF3UUPNZHLRVM4N3ANCNFSM4EPEAQVA>\n> .\n>\n", "> It would be run If you set lowering batch size as well Input size\r\n\r\ndear bro.. can you tell me, how to change batch size... I mean, which file has batch size... \r\n", "Me too, I got a same ERROR!", "This wasn't an issue with the earlier versions of keras and tensorflow. I had tf 2.0 installed originally and it handled much larger than (1000,10554) matrices without batch training.\r\nSome users are reporting this as a potential issue with python v > 3.75. ", "Sometimes killing jobs helps \r\n`kill %N`\r\nMemory can be used by background processes. Free that.", "Lowering batch size only hides your underlying problem, you need to serialize your data(instead of loading everything in your GPU,RAM) by converting your data to TRFRecord files:\r\nhttps://www.tensorflow.org/guide/data\r\nhttps://www.tensorflow.org/tutorials/load_data/tfrecord\r\n\r\nUse tdfs.load to load these types of files:\r\nhttps://colab.research.google.com/github/tensorflow/datasets/blob/master/docs/overview.ipynb", "Reducing the batch_size helped me!", "Reduce the batch_size doesn't help me. I run using terminal (with tmux) and I got that error, but when I run it in my notebook, it worked smoothly. Anybody can help me? I have to run it in terminal (with tmux)", "for me, reducing batch-size solve problem", "I success after changing smaller batch size!!!So inspired!!!!", "downgrading tensorflow and tensorflow-gpu to 2.5 if you are already on version 2.6 or later might help, at least it did with me.", "downgrading tensorflow and tensorflow-gpu to 2.5 helped. thx!!", "I have a similar problem. \r\nI tried batch size 64 or 32,16,8 all failed. \r\nI also downgrade to tensorflow and tensorflow-gpu to 2.5 and still getting the same error. \r\n\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: failed to allocate memory [Op:Mul]\r\n\r\nHere is the link to the error: https://pastebin.com/raw/TMSCH9Va\r\n\r\nI ran the same code yesterday and it was fine with batch size 64. But I needed to stop to change something and after that I keep getting this error. \r\n\r\n<img width=\"731\" alt=\"image\" src=\"https://user-images.githubusercontent.com/74671619/137164238-6f39cb77-740d-40ff-9040-31eaed1307de.png\">\r\n\r\nThere is no running process and still \r\n\r\nSum Total of in-use chunks: 7.17GiB\r\n2021-10-13 11:14:03.092854: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] total_region_allocated_bytes_: 7773224960 memory_limit_: 7773224960 available bytes: 0 curr_region_allocation_bytes_: 15546449920\r\n2021-10-13 11:14:03.092863: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Stats:\r\nLimit:                      7773224960\r\nInUse:                      7702514176\r\nMaxInUse:                   7702514688\r\nNumAllocs:                        2285\r\nMaxAllocSize:               1761195520\r\nReserved:                            0\r\nPeakReserved:                        0\r\nLargestFreeBlock:                    0\r\n\r\n2021-10-13 11:14:03.092920: W tensorflow/core/common_runtime/bfc_allocator.cc:467] ***************************************************************************************************x\r\n\r\n$pip freeze\r\nabsl-py==0.14.1\r\nalembic==1.4.3\r\nargon2-cffi==20.1.0\r\nasn1crypto==0.24.0\r\nastunparse==1.6.3\r\nasync-generator==1.10\r\nattrs==20.3.0\r\nbackcall==0.2.0\r\nbackports.entry-points-selectable==1.1.0\r\nbleach==3.2.1\r\ncached-property==1.5.2\r\ncachetools==4.2.4\r\ncertifi==2020.12.5\r\ncertipy==0.1.3\r\ncffi==1.12.3\r\nchardet==3.0.4\r\nclang==5.0\r\nconda==4.8.1\r\nconda-package-handling==1.3.11\r\ncryptography==2.7\r\ncycler==0.10.0\r\ndecorator==4.4.2\r\ndefusedxml==0.6.0\r\ndistlib==0.3.3\r\nentrypoints==0.3\r\nfilelock==3.3.0\r\nflatbuffers==1.12\r\nfuture==0.18.2\r\ngast==0.4.0\r\ngoogle-auth==1.35.0\r\ngoogle-auth-oauthlib==0.4.6\r\ngoogle-pasta==0.2.0\r\ngrpcio==1.34.1\r\nh5py==3.1.0\r\nidna==2.8\r\nimportlib-metadata==3.4.0\r\nipykernel==5.4.3\r\nipython==7.19.0\r\nipython-genutils==0.2.0\r\nipywidgets==7.5.1\r\njedi==0.18.0\r\nJinja2==2.11.2\r\njoblib==1.1.0\r\njson5==0.9.5\r\njsonschema==3.2.0\r\njupyter-client==6.1.11\r\njupyter-core==4.7.0\r\njupyter-telemetry==0.1.0\r\njupyterhub==1.2.2\r\njupyterlab==2.2.9\r\njupyterlab-pygments==0.1.2\r\njupyterlab-server==1.2.0\r\nkeras==2.6.0\r\nkeras-nightly==2.5.0.dev2021032900\r\nKeras-Preprocessing==1.1.2\r\nkiwisolver==1.3.2\r\nlibarchive-c==2.8\r\nlibclang==11.1.0\r\nllvmlite==0.37.0\r\nMako==1.1.4\r\nMarkdown==3.3.4\r\nMarkupSafe==1.1.1\r\nmatplotlib==3.4.3\r\nmistune==0.8.4\r\nmtcnn==0.1.1\r\nnbclient==0.5.1\r\nnbconvert==6.0.7\r\nnbformat==5.1.1\r\nnbgitpuller==0.9.0\r\nnbresuse==0.3.6\r\nnest-asyncio==1.4.3\r\nnotebook==6.1.5\r\nnteract-on-jupyter==2.1.3\r\nnumba==0.54.1\r\nnumpy==1.19.5\r\noauthlib==3.1.0\r\nopencv-python==4.5.3.56\r\nopt-einsum==3.3.0\r\npackaging==20.8\r\npamela==1.0.0\r\npandas==1.3.3\r\npandocfilters==1.4.3\r\nparso==0.8.1\r\npexpect==4.8.0\r\npickleshare==0.7.5\r\nPillow==8.3.2\r\nplatformdirs==2.4.0\r\nprometheus-client==0.9.0\r\nprompt-toolkit==3.0.10\r\nprotobuf==3.18.1\r\npsutil==5.8.0\r\nptyprocess==0.7.0\r\npyasn1==0.4.8\r\npyasn1-modules==0.2.8\r\npycosat==0.6.3\r\npycparser==2.19\r\nPygments==2.7.4\r\npyOpenSSL==19.0.0\r\npyparsing==2.4.7\r\npyrsistent==0.17.3\r\nPySocks==1.7.0\r\npython-dateutil==2.8.1\r\npython-editor==1.0.4\r\npython-json-logger==2.0.1\r\npytz==2021.3\r\npyzmq==21.0.0\r\nrequests==2.22.0\r\nrequests-oauthlib==1.3.0\r\nrsa==4.7.2\r\nruamel.yaml.clib==0.2.2\r\nruamel_yaml==0.15.46\r\nscikit-learn==1.0\r\nSend2Trash==1.5.0\r\nsix==1.15.0\r\nSQLAlchemy==1.3.22\r\ntb-nightly==2.7.0a20211010\r\ntensorboard==2.6.0\r\ntensorboard-data-server==0.6.1\r\ntensorboard-plugin-wit==1.8.0\r\ntensorflow==2.5.0\r\ntensorflow-estimator==2.5.0\r\ntensorflow-gpu==2.5.0\r\ntensorflow-io-gcs-filesystem==0.21.0\r\ntermcolor==1.1.0\r\nterminado==0.9.2\r\ntestpath==0.4.4\r\nthreadpoolctl==3.0.0\r\ntorch==1.6.0\r\ntornado==5.1.1\r\ntqdm==4.32.1\r\ntraitlets==5.0.5\r\ntyping-extensions==3.7.4.3\r\nurllib3==1.24.2\r\nvirtualenv==20.8.1\r\nvirtualenv-clone==0.5.7\r\nwcwidth==0.2.5\r\nwebencodings==0.5.1\r\nWerkzeug==2.0.2\r\nwidgetsnbextension==3.5.1\r\nwrapt==1.12.1\r\nzipp==3.4.0", "opening this issue as @unwritten shows the same error. I tried the batch size and the different solutions online but none of them worked for me.\r\nI trained the model yesterday on a single GPU which was found but when I increase my batch size by converting the model on both GPU then I am facing the same error.\r\nAny kind help will be appriciated\r\nThank you", "is this tensor size [2,512,512,512]  too big to be trained on GPU on colab? I am getting the following error\r\nOOM when allocating tensor with shape[2,512,512,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [[{{node Abs-0-0-TransposeNCHWToNHWC-LayoutOptimizer}}]]\r\n\r\nI have image of size 512*512 and batch size of 2"]}, {"number": 16767, "title": "Is there any way to reduce the model fie size of adam optimizer?", "body": "I find that the model file size is three times bigger after I switch optimizer from `tf.train.GradientDescentOptimizer` to `tf.train.AdamOptimizer`. And here is the reason I found on stackoverflow:\r\n\r\n>ADAM adds two running means (for gradient and square of gradient) as additional non-trainable parameters for each trainable parameter\r\n\r\nThus, is there any way to reduce the model fie size of adam optimizer?\r\n\r\nTHANKS!!", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 16766, "title": "with tf.control_dependencies fails on return", "body": "Hello,\r\nhere is an undesired behavior:\r\n\r\nfails\r\n>  with tf.control_dependencies([check_num_pnt]):\r\n>         out_feat = seg_sum \r\n> return out_feat\r\n\r\nworks\r\n>  with tf.control_dependencies([check_num_pnt]):\r\n>         out_feat = seg_sum * 1.0\r\n> return out_feat\r\n\r\nps: I should have said fails on copied tensor", "comments": ["What do you mean, fails? It jut hangs? Do you have cycles in your dependency graph?", "It runs through but does not do the check. I think that the reason is copying somehow is not a node on the graph. ", "Right, I think you aren't adding any nodes to the graph by simply assigning a new Python variable. I guess that out_feat = tf.identity(seg_sum) would work?", "It would, of course. But I think it's non-obvious for the user that tf.control_dependencies would not work on copying since it's not creating a node of the graph. ", "I'm not sure what you are suggesting. Do you think there's some part of the documentation that could be improved here? I'm not sure it's feasible to try to make Python do what you want here.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 28 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 16765, "title": "Resolve Programmatic mistake.", "body": "SPECIES should have been either imported here or it should have referenced from iris_data. The corresponding code is correct but the conflict in the documentation.", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it.\r\n", "The CLA hasn't yet resolved. Mind taking another look?", "Nevermind, looks like CLA bot is having issues. I verified that you've signed the CLA.", "I've verified the CLA manually. Merging."]}, {"number": 16764, "title": "Does it makes sense to use AdamOptimizer with Dropout?", "body": "I was experimenting with Dropout and I tried to check the number of weights updated in every iteration.\r\nMy network has an input layer of size 100 and output layer of size 1, and I use dropout with keep_prob of 0.8.  With this configuration, I am expecting to update every time around 80 neurons. I tried to check this, and I got weird results. I asked in stackexchange and someone got the right answer: the optimizer was updating all the weights.\r\n\r\nI was using Adam, and when I changed to GradientDescent, Adagrad and Adadelta it worked well. I haven't tried more optimizers thought.\r\n\r\nHere is the code\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n# As input, 100 random numbers.\r\ninput_size = 100\r\noutput_size = 1\r\n\r\nx = tf.placeholder(tf.float32,[None, input_size],name=\"input\")\r\ny = tf.placeholder(tf.float32,[None, output_size],name=\"labels\")\r\n\r\nwith tf.variable_scope(\"dense1\") as scope:\r\n    W = tf.get_variable(\"W\",shape=[input_size,output_size],initializer=tf.keras.initializers.he_uniform())\r\n    b = tf.get_variable(\"b\",initializer=tf.zeros([output_size]))\r\n    dropped = tf.nn.dropout(x,0.8)\r\n    dense = tf.matmul(dropped,W)+b\r\n\r\neval_pred = tf.nn.sigmoid(dense,name=\"prediction\")\r\n\r\ncost = tf.reduce_mean(tf.losses.absolute_difference(eval_pred,y))\r\n#train_step = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)\r\ntrain_step = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\r\ntrain_step = tf.train.AdadeltaOptimizer(learning_rate=0.01).minimize(cost)\r\n\r\n\r\n# 20 epochs, batch size of 1\r\nepochs = 20\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n\r\n    allWeights = []\r\n    for i in range(epochs):\r\n\r\n        x_raw = np.random.random((1,input_size))\r\n        y_raw = np.random.random((1,output_size))\r\n        [_,c,d,w]=sess.run([train_step,cost,dropped,W], feed_dict={x: x_raw, y: y_raw})\r\n        #print(\"Epoch {0}/{1}. Loss: {2}\".format(i+1,epochs,c))\r\n\r\n        # Numbers will be around 20% of input_size (17-22)\r\n        print(np.sum(d==0))\r\n        allWeights.append(w)\r\n\r\nprint(\"Calculate the difference between W_i and W_{i-1}\")\r\nfor wi in range(1,len(allWeights)):\r\n    difference = allWeights[wi]-allWeights[wi-1]\r\n    # I expect that there will be around 20 weights that won't be updated\r\n    # so the difference between the current weight and the previous one\r\n    # should be zero.\r\n    print(np.sum(difference==0))\r\n```\r\n\r\nJust in case is not clear enough in the code, I'm printing two sets of numbers:\r\nThe first set is the number of zeros in the masked dropout layer, and since I'm using 0.8 keep_prob, I should have around 20% of zeros (so, I should get a number around 20). This part works well.\r\nThe second set counts how many weights were NOT updated (difference between the previous weight and the current weight). Therefore, I am expecting these two sets to display the same numbers.\r\n\r\nAgain, with Adam doesn't work because it updates more weights whereas GradientDescend, Adadelta and Adagrad it works well.\r\n\r\nQuestion:\r\nIs this a bug or is it supposed to be like this?\r\nIn the latter case, does it make sense to use Adam with Dropout?\r\n", "comments": ["I don't think it's a bug. Optimizers can work differently. Closing this. You should maybe ask a targeted question on stackoverflow.", "i also need to know it.\r\nAdamOptimizer with Dropout is good or bad?"]}, {"number": 16763, "title": "CMake (Windows): Added support for ninja build and some fixes/changes", "body": "Most changes have comments stuck with them, but here's the summary of it.  \r\n\r\nChanges made:\r\n- Added EXACT flag on find_package CUDA to clarify it is building for exactly that version, the output \"minimum required\" can be misleading\r\n- Tests if the compiler is compatible with CUDA before it starts building\r\n- BYPRODUCTS/BUILD_BYPRODUCTS are added for Ninja to search for dependency, otherwise it will output the error shown below  \r\n`ninja: error: 'zlib/install/lib/zlibstatic.lib', needed by 'proto_text.exe', missing and no known rule to make it`\r\n- Visual Studio is the only generator with that will output under $(Configuration) directory, thus removed if it is not Visual Studio\r\n- Fixed CONFIGURE_COMMAND causing some CMake versions to break (it resets to default generator, the [CMake](https://cmake.org/files/v3.6/cmake-3.6.3-win64-x64.zip) I tried uses NMake Makefiles as default, which causes the output below  \r\n  ```\r\n  ...\r\n  Submodule path 'third_party/benchmark': checked out '360e66c1c4777c99402cf8cd535aa510fee16573'\r\n  Performing update step for 'protobuf'\r\n  No patch step for 'protobuf'\r\n  Performing configure step for 'protobuf'\r\n  CMake Error at CMakeLists.txt:12 (project):\r\n    Generator\r\n\r\n      NMake Makefiles\r\n\r\n    does not support platform specification, but platform\r\n\r\n      x64\r\n\r\n    was specified.\r\n\r\n  -- Building for: NMake Makefiles\r\n  -- Configuring incomplete, errors occurred!\r\n  ...\r\n  ```\r\n\r\nAdditional Info: The current version of CMake has a problem with its ninja generator, as it may produce an error below  \r\n`ninja: fatal: CreateProcess: The filename or extension is too long.`  \r\nThe CMake with its ninja generator improved can be found [here](https://gitlab.kitware.com/cmake/cmake/merge_requests/1604)\r\n", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@jhseu Before merging this, we should probably run a manual Windows CMake GPU build in addition to the regular presubmits."]}, {"number": 16762, "title": "build android demo error", "body": "ERROR: missing input file '@local_jdk//:jre/lib/resources.jar'\r\nERROR: /home/wangmeng/.cache/bazel/_bazel_wangmeng/b0a6578298b02ab8f9d039326e51a46f/external/bazel_tools/tools/android/BUILD:104:1: @bazel_tools//tools/android:gen_java_lang_extras_jar: missing input file '@local_jdk//:jre/lib/resources.jar'\r\nTarget //tensorflow/examples/android:tensorflow_demo failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /home/wangmeng/.cache/bazel/_bazel_wangmeng/b0a6578298b02ab8f9d039326e51a46f/external/bazel_tools/tools/android/BUILD:104:1 1 input file(s) do not exist\r\nINFO: Elapsed time: 22.408s, Critical Path: 0.03s\r\nFAILED: Build did NOT complete successfully\r\n", "comments": ["@wm901115nwpu : Could you please provide all information asked for in the issue template?\r\nFrom the little snippet provided, it seems the issue is that bazel is unable to find some files it expects from the JDK, so perhaps your JDK installation is broken?\r\n\r\nFull instructions to reproduce will be helpful.", "I run bazel build -c opt //tensorflow/examples/android:tensorflow_demo\r\njava version:java 9.0.4\r\nJava(TM) SE Runtime Environment (build 9.0.4+11)\r\nJava HotSpot(TM) 64-Bit Server VM (build 9.0.4+11, mixed mode)\r\nbazel version: bazel release 0.10.0\r\n", "Closing this as this appears to be a bazel issue.\r\n\r\nSee https://github.com/tensorflow/tensorflow/issues/16723#issuecomment-363985153\r\nand follow progress on https://github.com/bazelbuild/bazel/issues/3988\r\n\r\nThanks!"]}, {"number": 16761, "title": "Core dumped after checking failed", "body": "### System information\r\n== cat /etc/issue ===============================================\r\nLinux 84a2861a8739 3.10.0-229.el7.x86_64 #1 SMP Fri Mar 6 11:36:42 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"7 (Core)\"\r\nVERSION_ID=\"7\"\r\nCENTOS_MANTISBT_PROJECT_VERSION=\"7\"\r\nREDHAT_SUPPORT_PRODUCT_VERSION=\"7\"\r\n\r\n== are we in docker =============================================\r\nYes\r\n\r\n== compiler =====================================================\r\nc++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-4)\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux 84a2861a8739 3.10.0-229.el7.x86_64 #1 SMP Fri Mar 6 11:36:42 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.3)\r\nprotobuf (3.5.0.post1)\r\ntensorflow-tensorboard (0.4.0rc3)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.5.0\r\ntf.GIT_VERSION = v1.5.0-0-g37aa430d84\r\ntf.COMPILER_VERSION = v1.5.0-0-g37aa430d84\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\ntf_env_collect.sh: line 105: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n\r\n\r\n### Describe the problem\r\nI used keras using TensorFlow backend to train a batch of 300\\*300\\*3 RGB image.\r\nThe program occurred core dumped after printing check failed log as below.\r\n\r\n### Source code / logs\r\nF tensorflow/core/kernels/maxpooling_op.cc:177] Check failed: input_backprop_index >= in_start && input_backprop_index < in_end Invalid input backprop index: -1491167680, 2803712000, 2806515712\r\n\r\nI traced tensorflow source code, it should be check operation. And backprop index shouldn't be negative. I don't know tensorflow well, why can appear such problem? I think this error is root cause of code dumped. Could you help me solve this problem?\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Thanks for your update. Here are fields you need.\r\nHave I written custom code. N/A\r\nOS Platform and Distribution. Centos7 and Linux 84a2861a8739 3.10.0-229.el7.x86_64\r\nTensorFlow installed from. Using PIP\r\nTensorFlow version. 1.4/1.5\r\nBazel version. N/A\r\nCUDA/cuDNN version. N/A\r\nGPU model and memory. N/A\r\nExact command to reproduce.  Using Keras (Tensorflow as backend)", "Can you post a script we can run to reproduce this problem?\r\n\r\ncc/ @vrv , I'm not sure who else should handle this", "@skye I think the problem is that the input_backprop_index is an int, not an int64.", "@skye @vrv , Thanks for your comments.\r\nI already attach script in [here](https://stackoverflow.com/questions/48574868/tensorflow-core-dumped-after-check-failed), while a batch of training image is very large, it's not possible to upload them.\r\nI just change input_backprop_index from int to int64 in 172 line located in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/maxpooling_op.cc\r\nRight?", "Has this issue been addressed?", "I'm having a similar issue. It seems like there is an integer overflow in `maxpooling_op.cc` when training on large images. I'm currently mitigating this by reducing the resolution of the input images.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 71 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!", "> @skye @vrv , Thanks for your comments.\r\n> I already attach script in [here](https://stackoverflow.com/questions/48574868/tensorflow-core-dumped-after-check-failed), while a batch of training image is very large, it's not possible to upload them.\r\n> I just change input_backprop_index from int to int64 in 172 line located in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/maxpooling_op.cc\r\n> Right?\r\n\r\nHave you solved this issue?\r\n\r\nI have a similar error. I changed 'int' to 'int64' from line 169 to line 173 in tensorflow/blob/master/tensorflow/core/kernels/maxpooling_op.cc, then compiled tensorflow using bazel and install the whl file. But it still does not work."]}, {"number": 16760, "title": "Feature request: Adding data_format argument to lstm2d.separable_lstm", "body": "Since ndlstm is used for 2D data such as images, I think it would be nice to include a `data_format` argument in lstm2d.separable_lstm in case one decides to use the channels first format (NCHW) for their images (i.e. Using CNN having NCHW data format followed by NDLSTM).", "comments": ["Is this still open?? But the ndlstm is removed from tensorflow:master.", "Oh yeah, I forgot to close this one. Closing it."]}, {"number": 16759, "title": "Fix typo", "body": "", "comments": []}, {"number": 16758, "title": "import(ing) tensorflow fails to load at runtime.", "body": "I have written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nusing the Keras wrapper.  That works, but system failure and re-installation (new machine) provided the following:\r\n--------------------------------------------------------------------------------------------------------------------------------\r\nPython 2.7.12 (default, Dec  4 2017, 14:50:18) \r\n[GCC 5.4.0 20160609] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import keras\r\n/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\nUsing TensorFlow backend.\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python2.7/dist-packages/keras/__init__.py\", line 3, in <module>\r\n    from . import utils\r\n  File \"/usr/local/lib/python2.7/dist-packages/keras/utils/__init__.py\", line 6, in <module>\r\n    from . import conv_utils\r\n  File \"/usr/local/lib/python2.7/dist-packages/keras/utils/conv_utils.py\", line 9, in <module>\r\n    from .. import backend as K\r\n  File \"/usr/local/lib/python2.7/dist-packages/keras/backend/__init__.py\", line 83, in <module>\r\n    from .tensorflow_backend import *\r\n  File \"/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py\", line 5, in <module>\r\n    import tensorflow as tf\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n---------------------------------------------------------------------------------------------------------------------------------\r\n", "comments": ["We require cuda9 for the latest version. Did you install that? You may have to adjust your `LD_LIBRARY_PATH`.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 16757, "title": "tf.contrib.layers.optimize_loss() to support mixed precision training", "body": "ISSUE: Referring to [source code](https://github.com/tensorflow/tensorflow/blob/r1.5/tensorflow/contrib/layers/python/layers/optimizers.py),  it is evident that mixed precision gradients is not supported in `tf.contrib.layers.optimize_loss`. \r\nHere is the `snip` of assertion\u2026\r\n```\r\nopt = tf.contrib.layers.optimize_loss(\r\n    base_loss, global_step=global_step,\r\n    clip_gradients=clip_grad, increment_global_step=True, **train_params)\r\n\r\nTypeError: Tensors in list passed to 'values' of 'Pack' Op have types [float16, float32, float32, float16, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float16] that don't all match.\r\n```\r\nDescription:\r\nThis was observed during training resnet50 (this involves mixed precision batch_norm). Just curious to know whether there is a roadmap to have mixed precision gradients support in `tf.contrib.layers.optimize_loss`.\r\n\r\nSystem information\r\n\t\u2022\t**OS Platform and Distribution *: Linux Centos 7.2\r\n\t\u2022\tTensorFlow installed from (source or binary): 1.5.0\r\n\t\u2022\tTensorFlow version (use command below):  v1.5.0-0-g37aa430d84 1.5.0\r\n\t\u2022\tPython version: 3.4.5\r\n\t\u2022\tBazel version (if compiling from source): No\r\n\t\u2022\tCUDA/CUDAnn version: CUDA 9.1 and CUDAnn 7.0 with latest Nvidia driver\r\n\t\u2022\tGPU model and memory: Volta 100, 16GiB\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nCUDA/cuDNN version\nExact command to reproduce", "This works intermittently.. \r\n```\r\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n with tf.control_dependencies(update_ops):\r\n     opt = tf.contrib.layers.optimize_loss(total_loss, global_step=global_step,\r\n              clip_gradients=FLAGS.clip_grad, increment_global_step=True,**params)\r\n```", "Hi @tensorflowbutler ,  Here is the info,\r\nHave I written custom code :  NO\r\nCUDA/CUDAnn version: CUDA 9.1 and CUDAnn 7.0 with latest Nvidia driver", "cc/ @martinwicke ", "There is no plan to update contrib.layers.optimize_loss. If you require mixed precision updates, it is best to write the loss update yourself (you can use the implementation of optimize_loss or training.create_train_op as an example)."]}, {"number": 16756, "title": "Fix the mac builds on r1.6", "body": "Without this setting, it runs 2to3 on the python files. In master, it's set to PY2 which is also wrong.\r\n\r\nTested that the builds pass with this change:\r\nhttp://ci.tensorflow.org/view/Experimental/job/jhseu-mac-test/", "comments": []}, {"number": 16755, "title": "Multilayer CNN Softmax Script Error", "body": "Hi, I transferred your script for the Softmax regression and Multilayer CNN from your guide:\r\nhttps://www.tensorflow.org/versions/r0.12/tutorials/mnist/pros/.\r\n\r\nI get the following error whether I am running just the simple Softmax regression model or the full Multilayer CNN. Seems like an issue with the arguments used for the cross_entropy, but I don't know what the issue is...could you please help?\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-2-997a50686e0d> in <module>()\r\n     39     # between the target and the softmax activation function applied to the model's prediction.\r\n     40 \r\n---> 41 cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, y_))\r\n     42 \r\n     43     # Note that tf.nn.softmax_cross_entropy_with_logits internally applies the softmax\r\n\r\n~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py in softmax_cross_entropy_with_logits(_sentinel, labels, logits, dim, name)\r\n   1742   \"\"\"\r\n   1743   _ensure_xent_args(\"softmax_cross_entropy_with_logits\", _sentinel,\r\n-> 1744                     labels, logits)\r\n   1745 \r\n   1746   # TODO(pcmurray) Raise an error when the labels do not sum to 1. Note: This\r\n\r\n~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py in _ensure_xent_args(name, sentinel, labels, logits)\r\n   1696   if sentinel is not None:\r\n   1697     raise ValueError(\"Only call `%s` with \"\r\n-> 1698                      \"named arguments (labels=..., logits=..., ...)\" % name)\r\n   1699   if labels is None or logits is None:\r\n   1700     raise ValueError(\"Both labels and logits must be provided.\")\r\n\r\nValueError: Only call `softmax_cross_entropy_with_logits` with named arguments (labels=..., logits=..., ...)\r\n\r\nHere is my script (essentially copied from the guide):\r\n\r\n# Multilayer CNN using Tensorflow\r\n# Load Data\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nmnist = input_data.read_data_sets('MNIST_data', one_hot=True)\r\n\r\n# Start Tensorflow InteractiveSession\r\nimport tensorflow as tf\r\nsess = tf.InteractiveSession()\r\n\r\n# Build Softmax regression model. Define our model and training loss function.\r\n      \r\n    # creating nodes for the input images and target output classes\r\n\r\nx = tf.placeholder(tf.float32, shape=[None, 784])\r\ny_ = tf.placeholder(tf.float32, shape=[None, 10])\r\nsess.run(tf.global_variables_initializer())\r\n\r\n\r\n# Weight initialization\r\ndef weight_variable(shape):\r\n  initial = tf.truncated_normal(shape, stddev=0.1)\r\n  return tf.Variable(initial)\r\n\r\ndef bias_variable(shape):\r\n  initial = tf.constant(0.1, shape=shape)\r\n  return tf.Variable(initial)\r\n\r\n# Convolutional and pooling operations\r\n# Our convolutions uses a stride of one and are zero padded so that\r\n# the output is the same size as the input. Our pooling is plain \r\n# old max pooling over 2x2 blocks. To keep our code cleaner, let's \r\n# also abstract those operations into functions.\r\n\r\ndef conv2d(x, W):\r\n  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\r\n\r\ndef max_pool_2x2(x):\r\n  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\r\n                        strides=[1, 2, 2, 1], padding='SAME')\r\n\r\n# Implement first convolutional layer. t will consist of convolution, \r\n# followed by max pooling. The convolution will compute 32 features \r\n# for each 5x5 patch. Its weight tensor will have a shape of [5, 5, 1, 32].\r\n# The first two dimensions are the patch size, the next is the number of \r\n# input channels, aTo apply the layer, we first reshape x to a 4d tensor, with the second and third dimensions corresponding to image width and height, and the final dimension corresponding to the number of color channels.nd the last is the number of output channels. We will \r\n# also have a bias vector with a component for each output channel.\r\n\r\nW_conv1 = weight_variable([5, 5, 1, 32])\r\nb_conv1 = bias_variable([32])\r\n\r\n# To apply the layer, we first reshape x to a 4d tensor, with the second \r\n# and third dimensions corresponding to image width and height, and the \r\n# final dimension corresponding to the number of color channels.\r\n\r\nx_image = tf.reshape(x, [-1,28,28,1])\r\n\r\n# Then convolve x_image with the weight tensor, add the bias, apply the\r\n# ReLU function, and finally max pool. The max_pool_2x2 method will reduce \r\n# the image size to 14x14.\r\n\r\nh_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\r\nh_pool1 = max_pool_2x2(h_conv1)\r\n\r\n# Second convolutional layer.\r\n# In order to build a deep network, we stack several layers of this type. \r\n# The second layer will have 64 features for each 5x5 patch.\r\n\r\nW_conv2 = weight_variable([5, 5, 32, 64])\r\nb_conv2 = bias_variable([64])\r\n\r\nh_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\r\nh_pool2 = max_pool_2x2(h_conv2)\r\n\r\n# Densely connected layer. \r\n# Now that the image size has been reduced to 7x7, we add a fully-connected\r\n# layer with 1024 neurons to allow processing on the entire image. We \r\n# reshape the tensor from the pooling layer into a batch of vectors, \r\n# multiply by a weight matrix, add a bias, and apply a ReLU.\r\n\r\nW_fc1 = weight_variable([7 * 7 * 64, 1024])\r\nb_fc1 = bias_variable([1024])\r\n\r\nh_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\r\nh_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\r\n\r\n# Dropout\r\n# To reduce overfitting, we will apply dropout before the readout layer. \r\n# We create a placeholder for the probability that a neuron's output is \r\n# kept during dropout. This allows us to turn dropout on during training, \r\n# and turn it off during testing. TensorFlow's tf.nn.dropout op \r\n# automatically handles scaling neuron outputs in addition to masking them, \r\n# so dropout just works without any additional scaling.\r\n\r\nkeep_prob = tf.placeholder(tf.float32)\r\nh_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\r\n\r\n# Readout layer.  one layer softmax regression.\r\n\r\nW_fc2 = weight_variable([1024, 10])\r\nb_fc2 = bias_variable([10])\r\n\r\ny_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\r\n\r\n# Train and Evaluate model.\r\n\r\ncross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_conv, y_))\r\n\r\ntrain_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\r\n\r\ncorrect_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\r\n\r\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\r\n\r\nsess.run(tf.global_variables_initializer())\r\n\r\nfor i in range(20000):\r\n  batch = mnist.train.next_batch(50)\r\n  if i%100 == 0:\r\n    train_accuracy = accuracy.eval(feed_dict={\r\n        x:batch[0], y_: batch[1], keep_prob: 1.0})\r\n    print(\"step %d, training accuracy %g\"%(i, train_accuracy))\r\n  train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\r\nprint(\"test accuracy %g\"%accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))\r\n\r\n                                              ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Yes. I never got it to work.\n\n________________________________\nFrom: Alfred Sorten Wolf <notifications@github.com>\nSent: Tuesday, February 20, 2018 11:45:50 AM\nTo: tensorflow/tensorflow\nCc: peggyn7; Author\nSubject: Re: [tensorflow/tensorflow] Multilayer CNN Softmax Script Error (#16755)\n\n\nNagging Awaiting Response: It has been 14 days with no activityand the awaiting response label was assigned. Is this still an issue?\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/16755#issuecomment-367093590>, or mute the thread<https://github.com/notifications/unsubscribe-auth/Aidkgn2iQXcNQsBkCbZ87J6IOPqvUXQaks5tWx_OgaJpZM4R4q0i>.\n", "Please follow tutorials from https://www.tensorflow.org/tutorials/layers instead. Thanks.\r\n", "Thank you. Will try that.\n\n________________________________\nFrom: Yanping Huang <notifications@github.com>\nSent: Thursday, February 22, 2018 3:51:58 PM\nTo: tensorflow/tensorflow\nCc: peggyn7; Author\nSubject: Re: [tensorflow/tensorflow] Multilayer CNN Softmax Script Error (#16755)\n\n\nPlease follow tutorials from https://www.tensorflow.org/tutorials/layers instead. Thanks.\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/16755#issuecomment-367862303>, or mute the thread<https://github.com/notifications/unsubscribe-auth/Aidkgq_PBd5mZ2kaHj4DChGxoC9Q2xbMks5tXf2egaJpZM4R4q0i>.\n", "Closing due to lack of activity but please reopen if you believe there is a bug."]}, {"number": 16754, "title": "Fix issue for JNI library loading", "body": "This issue is founded when running a Java web app from docker. Current NativeLibrary load JNI from a relative path, the program is expecting to load the library from an absolute path. ", "comments": []}, {"number": 16753, "title": "Virtual GPU config crashes TensorFlow after physical GPU loaded", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian 9.3\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: ('v1.5.0-2132-gbdea071e68', '1.5.0')\r\n- **Python version**: 2.7.13\r\n- **Bazel version (if compiling from source)**:0.9.0\r\n- **GCC/Compiler version (if compiling from source)**: (Debian 4.9.2-10) 4.9.2\r\n- **CUDA/cuDNN version**: 9.1/7.0\r\n- **GPU model and memory**: K40m, 11439 MB\r\n- **Exact command to reproduce**: see the following script\r\n\r\n### Describe the problem\r\n\r\nCheck failed when creating virtual GPU device after loading physical GPU information with `tensorflow.python.client.device_lib.list_local_devices`.\r\n\r\n### Source code / logs\r\n\r\nSource code:\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.core.protobuf import config_pb2\r\nfrom tensorflow.python.client import device_lib\r\n\r\ndevice_lib.list_local_devices()\r\n\r\nvirtual_device_gpu_options = config_pb2.GPUOptions(\r\n    visible_device_list='0',\r\n    experimental=config_pb2.GPUOptions.Experimental(virtual_devices=[\r\n        config_pb2.GPUOptions.Experimental.VirtualDevices(\r\n            memory_limit_mb=[200, 300])]))\r\nconfig = config_pb2.ConfigProto(gpu_options=virtual_device_gpu_options)\r\n\r\nwith tf.Session(config=config) as sess:\r\n    with tf.device('/gpu:1'):\r\n        result = sess.run(tf.constant(42))\r\n```\r\n\r\nLogs:\r\n\r\n```\r\n2018-02-04 20:36:14.145943: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2018-02-04 20:36:23.613248: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1208] Found device 0 with properties:\r\nname: Tesla K40m major: 3 minor: 5 memoryClockRate(GHz): 0.745\r\npciBusID: 0000:02:00.0\r\ntotalMemory: 11.17GiB freeMemory: 11.09GiB\r\n2018-02-04 20:36:23.806370: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1208] Found device 1 with properties:\r\nname: Tesla K40m major: 3 minor: 5 memoryClockRate(GHz): 0.745\r\npciBusID: 0000:03:00.0\r\ntotalMemory: 11.17GiB freeMemory: 11.09GiB\r\n2018-02-04 20:36:24.019343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1208] Found device 2 with properties:\r\nname: Tesla K40m major: 3 minor: 5 memoryClockRate(GHz): 0.745\r\npciBusID: 0000:82:00.0\r\ntotalMemory: 11.17GiB freeMemory: 11.09GiB\r\n2018-02-04 20:36:24.341878: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1208] Found device 3 with properties:\r\nname: Tesla K40m major: 3 minor: 5 memoryClockRate(GHz): 0.745\r\npciBusID: 0000:83:00.0\r\ntotalMemory: 11.17GiB freeMemory: 11.09GiB\r\n2018-02-04 20:36:24.342631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1223] Device peer to peer matrix\r\n2018-02-04 20:36:24.342769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1229] DMA: 0 1 2 3\r\n2018-02-04 20:36:24.342789: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1239] 0:   Y Y N N\r\n2018-02-04 20:36:24.342803: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1239] 1:   Y Y N N\r\n2018-02-04 20:36:24.342815: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1239] 2:   N N Y Y\r\n2018-02-04 20:36:24.342827: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1239] 3:   N N Y Y\r\n2018-02-04 20:36:24.342846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1308] Adding visible gpu devices: 0, 1, 2, 3\r\n2018-02-04 20:36:25.856113: I tensorflow/core/common_runtime/gpu/gpu_device.cc:989] Creating TensorFlow device (/device:GPU:0 with 10755 MB memory) -> physical GPU (device: 0, name: Tesla K40m, pci bus id: 0000:02:00.0, compute capability: 3.5)\r\n2018-02-04 20:36:26.092252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:989] Creating TensorFlow device (/device:GPU:1 with 10753 MB memory) -> physical GPU (device: 1, name: Tesla K40m, pci bus id: 0000:03:00.0, compute capability: 3.5)\r\n2018-02-04 20:36:26.329914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:989] Creating TensorFlow device (/device:GPU:2 with 10755 MB memory) -> physical GPU (device: 2, name: Tesla K40m, pci bus id: 0000:82:00.0, compute capability: 3.5)\r\n2018-02-04 20:36:26.567499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:989] Creating TensorFlow device (/device:GPU:3 with 10753 MB memory) -> physical GPU (device: 3, name: Tesla K40m, pci bus id: 0000:83:00.0, compute capability: 3.5)\r\n2018-02-04 20:36:26.852624: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1308] Adding visible gpu devices: 0\r\n2018-02-04 20:36:26.852709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:989] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 200 MB memory) -> physical GPU (device: 0, name: Tesla K40m, pci bus id: 0000:02:00.0, compute capability: 3.5)\r\n2018-02-04 20:36:26.852868: F tensorflow/core/common_runtime/gpu/gpu_id_utils.cc:42] Check failed: cuda_gpu_id.value() == result.first->second (0 vs. 1)Mapping the same TfGpuId to a different CUDA GPU id. TfGpuId: 1 Existing mapped CUDA GPU id: 1 CUDA GPU id being tried to map to: 0\r\n```", "comments": ["This is a known issue, it has nothing to do with the virtual gpu feature. The problem is that some options (like the ones for gpu) in session config cannot be changed once they're set. `device_lib.list_local_devices()` invokes the [DeviceFactory::AddDevices()](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/client/device_lib.i#L32) which takes an empty SessionOptions, and later when `tf.Session(config=config)` is called the gpu options in `config` will be ignored.\r\n\r\nI believe if you remove the `list_local_devices()` call it should work. I'm closing this one, and please refer to #8021 #8136 #9374 for more information.", "No idea if commenting on a closed ticket will do any good (my case is it won't) but this feature worked just fine in Tensorflow 1.3 even with first calling list_local_devices(). It does not work in Tensorflow 1.8. I also tried 1.3 with cuda 9 and that does not have any issues.  \r\nBTW, recommending removal of list_local_devices() without any suggestion as to how to accomplish that functionality through other means is not useful to those of us who need that information. I've looked through the tickets that you referenced and none of them explain how to achieve this.", "This is what I met when running mpirun with hvd.", "Hi @rundembear and @venuswu,\r\n\r\nSorry for the late reply, I missed the updates here. Would you please try `device_lib.list_local_devices(session_config=config)` where `config` is the `ConfigProto` that is used later to create your session? This can make sure the same config is used within the process.\r\n\r\nLet me know if there are any other questions. Thanks.", "@aaroey, it's a pretty bad user experience that TF crashes with an assertion error due to this. Any fundamental reason for this restriction?", "@alsrgv, we already fix the crash issue in master branch and it should be in r1.9. If you build from master there should not be a crash, but an error will be shown instead.", "@aaroey, thanks - that's better. But is there a fundamental reason for this restriction?", "@alsrgv this is because device allocators are shared objects in the process, and GPUOptions controls how allocators are created and bind to which device, so if device mapping is changed, the allocator-device bindings will be messed up, and an error will need to be returned before any damage to the runtime."]}, {"number": 16752, "title": "Unable to run custom model (tensorflow) on android using Android studio ", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 7\r\n- **TensorFlow installed from (source or binary)**:\r\ndownloaded from Githhub\r\n- **TensorFlow version (use command below)**:\r\n1.5.0\r\n- **Python version**: \r\n3.6\r\n- **Bazel version (if compiling from source)**:\r\nNA, not compiling from source\r\n- **GCC/Compiler version (if compiling from source)**: \r\nNA\r\n- **CUDA/cuDNN version**:\r\nNA\r\n- **GPU model and memory**:\r\nNA\r\n- **Exact command to reproduce**:\r\nNA\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nI have followed the instruction as per \"https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2/index.html\" as were able to generate and successfully run APK for default model. I got problems when i replaced the default graph.pb file from rounded_graph.pb (retained name as graph only) file which is optimized version for android as per instructions in above mentioned link\r\nUpdated graphs are working using command line \r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n02-04 15:09:09.642: E/art(23153): No implementation found for long org.tensorflow.contrib.android.RunStats.allocate() (tried Java_org_tensorflow_contrib_android_RunStats_allocate and Java_org_tensorflow_contrib_android_RunStats_allocate__)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): FATAL EXCEPTION: main\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): Process: org.tensorflow.demo, PID: 23153\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): java.lang.RuntimeException: Failed to load model from 'file:///android_asset/graph.pb'\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat org.tensorflow.contrib.android.TensorFlowInferenceInterface.<init>(TensorFlowInferenceInterface.java:100)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat org.tensorflow.demo.TensorFlowImageClassifier.create(TensorFlowImageClassifier.java:103)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat org.tensorflow.demo.ClassifierActivity.onPreviewSizeChosen(ClassifierActivity.java:132)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat org.tensorflow.demo.CameraActivity$1.onPreviewSizeChosen(CameraActivity.java:159)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat org.tensorflow.demo.CameraConnectionFragment.setUpCameraOutputs(CameraConnectionFragment.java:421)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat org.tensorflow.demo.CameraConnectionFragment.openCamera(CameraConnectionFragment.java:428)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat org.tensorflow.demo.CameraConnectionFragment.access$000(CameraConnectionFragment.java:64)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat org.tensorflow.demo.CameraConnectionFragment$1.onSurfaceTextureAvailable(CameraConnectionFragment.java:95)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat android.view.TextureView.getHardwareLayer(TextureView.java:368)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat android.view.View.updateDisplayListIfDirty(View.java:15173)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat android.view.View.draw(View.java:15969)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat android.view.ViewGroup.drawChild(ViewGroup.java:3612)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat android.view.ViewGroup.dispatchDraw(ViewGroup.java:3402)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat android.view.View.updateDisplayListIfDirty(View.java:15191)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat android.view.View.draw(View.java:15969)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat android.view.ViewGroup.drawChild(ViewGroup.java:3612)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat android.view.ViewGroup.dispatchDraw(ViewGroup.java:3402)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat android.view.View.draw(View.java:16202)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat android.view.View.updateDisplayListIfDirty(View.java:15196)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat android.view.View.draw(View.java:15969)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat android.view.ViewGroup.drawChild(ViewGroup.java:3612)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat android.view.ViewGroup.dispatchDraw(ViewGroup.java:3402)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat android.view.View.updateDisplayListIfDirty(View.java:15191)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat android.view.View.draw(View.java:15969)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat android.view.ViewGroup.drawChild(ViewGroup.java:3612)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat android.view.ViewGroup.dispatchDraw(ViewGroup.java:3402)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat android.view.View.updateDisplayListIfDirty(View.java:15191)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat android.view.View.draw(View.java:15969)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat android.view.ViewGroup.drawChild(ViewGroup.java:3612)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat android.view.ViewGroup.dispatchDraw(ViewGroup.java:3402)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat android.view.View.draw(View.java:16202)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat com.android.internal.policy.PhoneWindow$DecorView.draw(PhoneWindow.java:2690)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat android.view.View.updateDisplayListIfDirty(View.java:15196)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat android.view.ThreadedRenderer.updateViewTreeDisplayList(ThreadedRenderer.java:281)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat android.view.ThreadedRenderer.updateRootDisplayList(ThreadedRenderer.java:287)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat android.view.ThreadedRenderer.draw(ThreadedRenderer.java:322)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat android.view.ViewRootImpl.draw(ViewRootImpl.java:2627)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat android.view.ViewRootImpl.performDraw(ViewRootImpl.java:2446)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat android.view.ViewRootImpl.performTraversals(ViewRootImpl.java:2079)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat android.view.ViewRootImpl.doTraversal(ViewRootImpl.java:1119)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat android.view.ViewRootImpl$TraversalRunnable.run(ViewRootImpl.java:6060)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat android.view.Choreographer$CallbackRecord.run(Choreographer.java:858)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat android.view.Choreographer.doCallbacks(Choreographer.java:670)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat android.view.Choreographer.doFrame(Choreographer.java:606)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat android.view.Choreographer$FrameDisplayEventReceiver.run(Choreographer.java:844)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat android.os.Handler.handleCallback(Handler.java:746)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat android.os.Handler.dispatchMessage(Handler.java:95)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat android.os.Looper.loop(Looper.java:148)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat android.app.ActivityThread.main(ActivityThread.java:5443)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat java.lang.reflect.Method.invoke(Native Method)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:728)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): \tat com.android.internal.os.ZygoteInit.main(ZygoteInit.java:618)\r\n02-04 15:09:10.105: E/AndroidRuntime(23153): Caused by: java.io.IOException: Not a valid TensorFlow Graph serialization: NodeDef mentions attr 'dilations' not in Op<name=Conv2D; signature=input:T, filter:T -> output:T; attr=T:type,allowed=[DT_HALF, DT_FLOAT]; attr=strides:list(int); attr=use_cudnn_on_gpu:bool,default=true; attr=padding:string,allowed=[\"SAME\", \"VALID\"]; attr=data_format:string,default=\"NHWC\",allowed=[\"NHWC\", \"NCHW\"]>; NodeDef: Mobi\r\n\r\n", "comments": ["It looks to me like you're trying to load a model generated by a newer version of TensorFlow on an older version which does not understand all its features (in particular, the dilations attr of the Conv2D op).", "@martinwicke facing same problem how to solve it. I posted it on stack overflow no answer yet.", "I am trying to solve the same kind of issue for almost a month but still it is not solved. So I am really hoping someone can help us. I am also a windows os user.", "I am trying to solve the same kind of issue for almost a month but still it is not solved. So I am really hoping someone can help us. I am also a windows os user.\r\n\r\nMe too.", "You need to change \"libtensorflow_demo.so\" file, the file attached in github code is not correct.\r\nAdd it under which ever arch you are using \"arm64-v8a\" or  'armeabi-v7a\" or \"x86\" or \"x86_64\"\r\nAlso you need to add Jar file \"libandroid_tensorflow_inference_java.jar\" under Library.\r\n\r\n\r\n[libandroid_tensorflow_inference_java.zip](https://github.com/tensorflow/tensorflow/files/1757273/libandroid_tensorflow_inference_java.zip)\r\n\r\n[libtensorflow_demo.zip](https://github.com/tensorflow/tensorflow/files/1757275/libtensorflow_demo.zip)\r\nThis is for x86_64 arch\r\n", "i changed the \"libtensorflow_demo.so\" file. Even then am facing the same issue\r\n\r\nE/AndroidRuntime: FATAL EXCEPTION: IntentService[Prediction Service]\r\n    Process: com.cognizant.oralclassifier, PID: 25426\r\n    java.lang.RuntimeException: Native TF methods not found; check that the correct native libraries are present in the APK.\r\n        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.prepareNativeRuntime(TensorFlowInferenceInterface.java:544)\r\n        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.<init>(TensorFlowInferenceInterface.java:60)\r\n        at com.cognizant.oralclassifier.analyze.PredictionService.onHandleIntent(PredictionService.java:66)\r\n        at android.app.IntentService$ServiceHandler.handleMessage(IntentService.java:76)\r\n        at android.os.Handler.dispatchMessage(Handler.java:106)\r\n        at android.os.Looper.loop(Looper.java:193)\r\n        at android.os.HandlerThread.run(HandlerThread.java:65)", "> You need to change \"libtensorflow_demo.so\" file, the file attached in github code is not correct.\r\n> Add it under which ever arch you are using \"arm64-v8a\" or 'armeabi-v7a\" or \"x86\" or \"x86_64\"\r\n> Also you need to add Jar file \"libandroid_tensorflow_inference_java.jar\" under Library.\r\n> \r\n> [libandroid_tensorflow_inference_java.zip](https://github.com/tensorflow/tensorflow/files/1757273/libandroid_tensorflow_inference_java.zip)\r\n> \r\n> [libtensorflow_demo.zip](https://github.com/tensorflow/tensorflow/files/1757275/libtensorflow_demo.zip)\r\n> This is for x86_64 arch\r\n\r\nStill doesn't work", "Hi\r\n\r\nI had the same problem. I think it likely to be \"martinwicke commented on Feb 6, 2018\" comment. I tried different .pb file downloaded from COCO than the one I got from the tutorial. I no longer see that problem, i still got errors but moved further."]}, {"number": 16751, "title": "CUDA Fail in Tensorflow Inference on Jetson TX2", "body": "Hi,\r\n\r\nI am getting CUDA fail error for model inference on Jetson TX2 aarch64. I have built the TF source (Version 1.3) for python 3.5 from this github repo:\r\nhttps://github.com/jetsonhacks/installTensorFlowTX2\r\n\r\n- Ubuntu 16.04\r\n- Bazel 0.5.2\r\n- CUDA 8 \r\n- cuDNN 6. \r\n\r\nThe relevant discussion on NVIDIA dev forum directed me to post this here:\r\nhttps://devtalk.nvidia.com/default/topic/1029256/jetson-tx2/cuda-fail-when-running-tensorflow-inference/post/5236860/\r\n\r\nTF does work for smaller sized models, but for larger sized models the inference fails. I appreciate if you can please take look at this.\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Thanks for your reply. Here are the specs of system and libraries I'm trying to run inference for.\r\n\r\n-Linux 16.04\r\n-TF complied from source with bazel\r\n-TF 1.3\r\n-Bazel 0.5.2\r\n-CUDA 8\r\n-cuDNN 6\r\n-GPU Nvidia Jetson Tegra TX2\r\n-Running the demo inference on this github repository: https://github.com/matterport/Mask_RCNN\r\n\r\n", "Hi, are you able to create a smaller repro script? We likely won't be able to debug this otherwise. Also, could you post the error message you receive and any other relevant information?", "Actually, this may be related to #15075. Regardless, we don't have the ability to support or debug Jetson TX2 issues, so I'm marking contributions welcome.", "hi pvaezi , I just met the same problem when I trying to use Inception v3 to classify image, which is a tutorial in TensorFlow repository, i flashed my TX2 with jetpack 3.1, and cuda 8.0, cudnn 6, I can train cifar10 so I think it is not the environment problem, so have you solve this problem? waiting for your response. ", "Hi @pvaezi! Is this issue still valid ?  Have you checked above [comment](https://github.com/tensorflow/tensorflow/issues/16751#issuecomment-382026371) yet?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 16750, "title": "ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory (when trying to run object detection)", "body": "Hello, I am trying to install the object-detection module of tensorflow but when running the following command: \r\npython3 object_detection/builders/model_builder_test.py\r\n\r\nI get the following error. I have install CUDA 8.0,9.0,9.1, and cuDNN 6 and 7 but still have the following error. I appreciate your advice, thank you! \r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"object_detection/builders/model_builder_test.py\", line 18, in <module>\r\n    import tensorflow as tf\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.", "comments": ["Had same issue\r\n\r\nsudo apt-get install cuda-9-0 \r\n\r\nworked for me even after I had Cuda-9-0 installed.\r\n", "@adekunleba When I run that command, I get the following:\r\n\r\n>Reading package lists... Done\r\n>Building dependency tree       \r\n>Reading state information... Done\r\n>E: Unable to locate package cuda-9-0\r\n\r\nCould you please advise what to do? Thank you in advance!", "@JSANJ do you have this in your rc file ?\r\n `export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/extras/CUPTI/lib64`", "@JSANJ you should add `cuda/lib64` to your `LD_LIBRARY_PATH`, in addition to @calper-ql suggestion, I also did ```export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64```", "if you have multiple versions of CUDA installed, you probably need this instead:\r\n```\r\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-9.0/lib64\r\n```\r\nThis is because `/usr/local/cuda` is usually just a symlink to one of the versions. if you have multiple versions, it may be a symlink to another version.", "@calper-ql @adekunleba @gunan Thank you all, your suggestions worked for installation on Windows using Ubuntu Bash\r\n\r\nAs a side note:\r\nI also tried installing it on a Linux version (Ubuntu 16.04) and found that I wasn't actually installing CUDA due to the Ubuntu Software Center malfunctioning, but using commands instead worked as in: https://askubuntu.com/questions/760034/waiting-to-install-for-ever-ubuntu-software-16-04.", "@JSANJ `sudo apt-get install cuda-9.0` instead of `sudo apt-get install cuda-9-0` worked for me", "Hi , I tried all of the above but still getting the following error:\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/na/anaconda3/envs/tensorflow/lib/python2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/home/na/anaconda3/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/na/anaconda3/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/na/anaconda3/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/na/anaconda3/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/na/anaconda3/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n*************************************************************************\r\nNot only that if i run import tensorflow as tf using spider in anaconda2 I get the same error as above but with libculas.so.8.0 : ImportError: libcublas.so.8.0: cannot open shared object file: No such file or directory.\r\nI checked and i do have this file in lib in anaconda2, so I am puzzled .  I tried every thing in the tensorflow link but no success.  I am working with Ubuntu latest version 16.04.3.\r\nHelp please!\r\n", "That is a CUDA library, anaconda2 should not, and would not have that library.\r\nAlso, one of your error messages complain about  cuda 9.0, and the 2nd complain about cuda 8.0\r\nyou will need to pick which version of TF you want to work with, then install the corresponding CUDA version.", "Gunan, thank you .  I decided to work with anaconda3 , do I need to install Cuda 9.2 as an update ?  in anaconda3 I did the following commands: \r\nsudo -H apt-get install git\r\n\tsudo -H apt-get install python3\r\n\tsudo -H apt-get install python3-matplotlib\r\n\tsudo -H apt-get install python3-pip\r\n\tsudo -H pip3 install --upgrade tensorflow\r\nand it seems everything was updated fine , then I tried to import tensorflow as tf and got the error again\r\n", "Anaconda is just a python distribution.\r\nCUDA and cuDNN aare independent of Anaconda. You need to install anaconda separately, then you need to install CUDA separately.\r\nLooks like you installed anaconda.\r\nBut now you need to follow NVIDIA's instructions to install CUDA 9.0, if you like to work with the latest version of tensorflow that uses GPU.\r\nAlternatively, you can use tensorflow without GPU.\r\n\r\nPlease make sure to read and follow the instructions here:\r\nhttps://www.tensorflow.org/install/", "Gunan, thank you .  I decided to work with anaconda3 , do I need to install Cuda 9.2 as an update ?  in anaconda3 I did the following commands: \r\nsudo -H apt-get install git\r\n\tsudo -H apt-get install python3\r\n\tsudo -H apt-get install python3-matplotlib\r\n\tsudo -H apt-get install python3-pip\r\n\tsudo -H pip3 install --upgrade tensorflow\r\nand it seems everything was updated fine , then I tried to import tensorflow as tf and got the error again\r\n", "well I did a full installation of cuda9.2 , with the latest version of tensorflow v 1.8 which is compatible . I used the following link :http://www.python36.com/how-to-install-tensorflow-gpu-with-cuda-9-2-for-python-on-ubuntu/ .  \\After 3+ hours all went successfully and when it came to import tensorflow as tf , frustratingly enough i got the same error as before :    Any Ideas ?  Thank you \r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/na/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/home/na/anaconda3/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/na/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/na/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/na/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/na/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/na/anaconda3/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/na/anaconda3/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n", "https://www.tensorflow.org/install/install_linux#nvidia_requirements_to_run_tensorflow_with_gpu_support\r\nAs stated in the web page above, TF 1.8 is compatible with CUDA 9.0\r\nWe have no prebuilt binaries that are compatible with cuda 9.2. for cuda 9.2, you need to build from sources.", "Thank you .  I seem to have resolved the issue as on the command line i tested the tensoflow with Hello world and worked so happy until i realsied that tensoflow 1.8 was running on cpu and no gpu .  then i did the following :from tensorflow.python.client import device_lib followed by print(device_lib.list_local_devices()): \"/device:CPU:0\"\r\ndevice_type: \"CPU\"\r\nmemory_limit: 268435456\r\nlocality {\r\n}\r\nincarnation: 14605564845436371737\r\nHow do I get GPU working and how come it is not ?  Am I missing some installation ?", "After banging my head around a couple of days, installing cuda 9.2 but failing tf builds from scratch  uninstalling then downgrading to 9.0 to use pre compiled binaries : \r\n\r\nDirty hack till tf updates cuda support \r\n\r\n**GCC :** (Ubuntu 7.3.0-16ubuntu3) 7.3.0\r\n**OS :** \r\nlinux -4.15.0-20-generic\r\nx86_64\r\nDISTRIB_ID=Ubuntu\r\nDISTRIB_RELEASE=18.04\r\n**GPU :**  \r\n00:07.0 3D controller: NVIDIA Corporation GV100 [Tesla V100 SXM2] (rev a1)\r\n\r\n**CUDA :**  \r\nDriver version : 396.26 \r\nToolkit : 9.2 and 9.0 both installed by sudo sh cuda_9.*****-run  (download from nvidia ) but for installing 9.0 say N when asks to install display driver.\r\n\r\n\r\nthen do as said above : #issuecomment-365089833 \r\n\r\n LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-9.0/lib64 >> ~/.bashrc\r\n", "thank you for the suggestion as i am stuck not be able to get tf gpu working on tf 1.8 on cuda 9.2.  could you guide from where did you install cuda 9.0 as the nvidia download points immedaitely to 9.2 version.", "I did re-install cuda 9.0 after I uninstalled 9.2 and then reinstalled tf v.1.8 .  the validation on terminal of tf was successful but again it was running on cpu and not gpu!! .  I am absolutely puzzled why i cannot get tf gpu working tried every thing under the sun -very frustrating "]}, {"number": 16749, "title": "Cross Compiled For Rpi Successfully on Gentoo. No Scope and Session support.", "body": "Hello everyone.\r\nI cross compiled from tensorflow master using armv7a-hardfloat-linux-gnueabi-gcc built using crossdev on Gentoo AMD64\r\n\r\n_**make -j9 -f tensorflow/contrib/makefile/Makefile HOST_OS=LINUX TARGET=PI OPTFLAGS=\"-Os -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize\" CXX=armv7a-hardfloat-linux-gnueabi-g++**_\r\n\r\n I am able to execute on Rpi, the sample label_image.cc program by compiling it manually using\r\n\r\n**_armv7a-hardfloat-linux-gnueabi-g++ -Wl,--whole-archive lib/libtensorflow-core.a -Wl,--whole-archive lib/libnsync.a -Wl,--no-whole-archive lib/libprotobuf.a -ldl -lm -lpthread -lz -I include/ -I include/public/ label_image.cc -std=c++11 `armv7a-hardfloat-linux-gnueabi-pkg-config --cflags --libs libjpeg` -o test_**\r\n\r\nHowever, when trying to compile a program that uses tensorflow::Scope and tensorflow::ClientSession, i get undefined references to them\r\n\r\n\r\n _$ armv7a-hardfloat-linux-gnueabi-g++ -Wl,--whole-archive lib/libtensorflow-core.a -Wl,--whole-archive lib/libnsync.a -Wl,--no-whole-archive lib/libprotobuf.a -ldl -lm -lpthread -lz -I include/ -I include/public/ test.cpp -std=c++11 -o test\r\n/tmp/ccZyvm2X.o: In function `main':\r\ntest.cpp:(.text+0x9c): undefined reference to `tensorflow::Scope::NewRootScope()'\r\ntest.cpp:(.text+0x114): undefined reference to `tensorflow::Input::Initializer::Initializer(std::initializer_list<tensorflow::Input::Initializer> const&)'\r\ntest.cpp:(.text+0x128): undefined reference to `tensorflow::ops::Const(tensorflow::Scope const&, tensorflow::Input::Initializer const&)'\r\ntest.cpp:(.text+0x1a4): undefined reference to `tensorflow::Input::Initializer::Initializer(std::initializer_list<tensorflow::Input::Initializer> const&)'\r\ntest.cpp:(.text+0x1b8): undefined reference to `tensorflow::ops::Const(tensorflow::Scope const&, tensorflow::Input::Initializer const&)'\r\ntest.cpp:(.text+0x220): undefined reference to `tensorflow::Scope::WithOpName(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const'\r\ntest.cpp:(.text+0x280): undefined reference to `tensorflow::ops::MatMul::MatMul(tensorflow::Scope const&, tensorflow::Input, tensorflow::Input, tensorflow::ops::MatMul::Attrs const&)'\r\ntest.cpp:(.text+0x2a4): undefined reference to `tensorflow::Scope::~Scope()'\r\ntest.cpp:(.text+0x2dc): undefined reference to `tensorflow::ClientSession::ClientSession(tensorflow::Scope const&)'\r\ntest.cpp:(.text+0x334): undefined reference to `tensorflow::ClientSession::Run(std::vector<tensorflow::Output, std::allocator<tensorflow::Output> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) const'\r\ntest.cpp:(.text+0x448): undefined reference to `tensorflow::ClientSession::~ClientSession()'\r\ntest.cpp:(.text+0x484): undefined reference to `tensorflow::Scope::~Scope()'\r\ntest.cpp:(.text+0x53c): undefined reference to `tensorflow::Scope::~Scope()'\r\ntest.cpp:(.text+0x5e4): undefined reference to `tensorflow::ClientSession::~ClientSession()'\r\ntest.cpp:(.text+0x62c): undefined reference to `tensorflow::Scope::~Scope()'\r\ncollect2: error: ld returned 1 exit status_\r\n\r\nHas support for **Scope** and **ClientSession** intentionally been left out of the Makefile?\r\nIs there a way to add support for it?\r\n\r\nRegards\r\nMandar Joshi", "comments": ["Found a solution. Removed ANDROID_TYPES from CXXFLAGS \r\nHowever, for some reason I get Bus Error when compiling with neon-vfpv4 Raspberry Pi 3 on Gentoo with GCC 5.4.0-r4\r\nGuess, that's a cross compiler issue.\r\nClosing."]}, {"number": 16748, "title": "Fix the Windows GPU build #2", "body": "", "comments": []}, {"number": 16747, "title": "No documentation on the order of eigenvalues returned by tf.self_adjoint_eig ", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:('v1.4.0-rc1-11-g130a514', '1.4.0')\r\n- **Python version**: 2.7.14\r\n\r\n\r\n### Describe the problem\r\nFrom the documentation of `tf.self_adjoint_eig` I cannot see what the order of eigenvalues is. I tried with several examples and found they were sorted in ascending order. Does this always hold?\r\n\r\n", "comments": ["TensorFlow doc doesn't tell much about that. But TensorFlow use `Eigen` to calculate the eigenvalues:\r\nhttps://github.com/tensorflow/tensorflow/blob/ba032db13627945e7cc772dbd3d85d257aef3ab9/tensorflow/core/kernels/self_adjoint_eig_op.cc#L58-L65\r\n\r\nAnd document of `Eigen` says:\r\n\r\n> The eigenvalues are repeated according to their algebraic multiplicity, so there are as many eigenvalues as rows in the matrix. The eigenvalues are sorted in increasing order.\r\nhttps://eigen.tuxfamily.org/dox/classEigen_1_1SelfAdjointEigenSolver.html#a3df8721abcc71132f7f02bf9dfe78e41\r\n\r\nSo the order is guranteed by `Eigen`.\r\n\r\nFor clarification, I think adding this to TensorFlow documents may be better.\r\n\r\nUPDATE:\r\nIt also holds for GPU implementation. See CUDA doc:\r\n> a real array of dimension n. The eigenvalue values of A, in ascending order ie, sorted so that W(i) <= W(i+1).\r\n> http://docs.nvidia.com/cuda/cusolver/#cuds-lt-t-gt-syevd", "@rmlarsen could you update the docs? I think @annarev can help direct you to where they should be updated.", "These seem to be relevant files for SelfAdjointEig documentation:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/base_api/api_def_SelfAdjointEig.pbtxt\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/base_api/api_def_SelfAdjointEigV2.pbtxt", "I think @rmlarsen has the flu, and @qmick seems to have discovered the answer, so I'm gonna mark contributions welcome for now. Please feel free to submit a PR!"]}]