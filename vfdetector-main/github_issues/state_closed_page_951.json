[{"number": 24903, "title": "Update kinesis dataset to match v2 API", "body": "Since DatasetSource now requires varient_tensor to be passed into constructor,\r\nthe kinesis data will need to be updated to fix the incompatibility issue.\r\n\r\nSee related fix in PR #24899.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["`GPU CC` and `GPU Python 3` tests failed to build because `nvcc` couldn't be found, which is unrelated to this PR. The rest of the failing checks seem to be existing failures. I see a recently merged commit having the same set of failing checks. I'll just set this to ready-to-pull."]}, {"number": 24902, "title": "Cherrypick of \"Fix tf_driver_test\"", "body": "Use the proper path for the tested model.", "comments": []}, {"number": 24901, "title": "Set default value of TF_BUILD_VERSION arg to r1.13", "body": "", "comments": ["Let's wait on this until we have at least the 1.13 rc0 out.\r\n@aselle @angerson is this updated in the release branch? Once we merge the release branch back, this would be updated if the branch has the correct value."]}, {"number": 24900, "title": "Cherry pick of \"Fix tensor_utils_test\"", "body": "Fix tensor_utils_test\r\n\r\nUpdate another test target relying on lite/kernels:test_util, which\r\npulls in TF and requires the tf_cc_test build rule.\r\n\r\nA follow-up CL will remove this requirement for this and other kernel\r\ntests.", "comments": []}, {"number": 24899, "title": "Fix Ignite Dataset (V2 API)", "body": "Hi, @mrry, @yongtang.\r\n\r\nDuring the update on Dataset V2 API `Ignite Dataset` has been broken (I guess nobody runs manual tests). The errors looks like that:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"ignite_dataset_test.py\", line 47, in test_ignite_dataset_with_plain_client\r\n    ds = IgniteDataset(cache_name=\"SQL_PUBLIC_TEST_CACHE\", port=42300)\r\n  File \"/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 318, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/ignite/python/ops/ignite_dataset_ops.py\", line 738, in __init__\r\n    super(IgniteDataset, self).__init__()\r\nTypeError: __init__() missing 1 required positional argument: 'variant_tensor'\r\n```\r\n\r\nThe problem is that `DatasetSource` now requires `varient_tensor` to be passed into constructor. I guess fixed this problem.\r\n\r\nPS: @yongtang, I found that Kafka currently has the same issue, so I fixed it too. Please have a look.", "comments": ["The fix looks good to me. Thanks! I also created a PR #24903 for kinesis fix. In addition, we will need to carry the fix to tensorflow/io  (tensorflow/io#54) once we move from tensorflow 1.12 to tensorflow 1.13 (to be released soon). ", "@yongtang, as far as I understand 1.13 branch is about to be cut off. Let me mention that it's important to include this fix into the release.", "@dmitrievanthony It's cut off a month ago. @ewilderj Who should we speak to if we'd like to cherry-pick this to r1.13?", "#24903 also needs to be cherry-picked into R1.13. Since branch R1.13 is already in place, I think this PR and PR #24903 could be cherry-picked once they are merged into the master.", "Folks, it's not clear when this PR will be merged? I see that #24903 has already been merged, but not this one. Do we have some issues with it?", "Ideally this one may need to be in r1.13 before the 1.13 release.", "cc @aselle ", "Guys, what about this PR? I guess it has to be merged and than cherry-picked into 1.13, but it looks like the process is stuck.", "@ymodak what do you need me to do? the changes in this PR seems to be already approved", "This PR is failing internal presubmits with the following error:\r\n\r\n```\r\nHOURGLASS IMPORT DETECTED\r\n\r\n'import tensorflow' statements are no longer allowed in the internal\r\nTensorFlow codebase, with the exception of /examples/ and /g3doc/\r\ndirectories. This is because they prevent Blaze and TAP from caching\r\nanything.\r\n\r\nPlease import modules directly. Also please make sure your Python\r\nBUILD rules depend on all the labels whose sources they import.\r\n```", "Hi, @jsimsa. It's a bit strange restriction for tests, but okay, I've removed import of `tensorflow`. Please have a look.", "Thanks, @jsimsa, @aselle please don't forget to cherry-pick these changes into 1.13.", "Folks, @jsimsa, @aselle let me remind that it's important to push this fix into `1.13`. Do you have plans to do that soon?", "I also sent an internal ping for this cherry pick.", "This has been merged into r1.13 #25290", "Hi, @aselle, @ewilderj. I'm a bit embarrassed, but it looks like I have to ask to revert cherry pick. \r\n\r\nThe reason for this fix is commit https://github.com/tensorflow/tensorflow/commit/9a63f5b8432f0057d5099e2aea0c8f57467c65db, but I realized that it wasn't included into 1.13. It's not included and so that the correspondent fix should not be included too. Unfortunately, for now we have broken 1.13.0rc1.\r\n\r\nIs it possible to revert the cherry pick?", "I've tagged this internally too, will await @aselle confirmation.", "Hi, @aselle. Any update?", "This will not make RC2 but I will try to get it into final.\n-A\n\nOn Fri, Feb 15, 2019 at 8:25 AM Anton Dmitriev <notifications@github.com>\nwrote:\n\n> Hi, @aselle <https://github.com/aselle>. Any update?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/24899#issuecomment-464110778>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAT52mwCgahKLzJ6EY5zT5DafUcW65-Iks5vNt93gaJpZM4Z-a7b>\n> .\n>\n", "Him @aselle. It looks like RC2 still has this issue, have you tried to revert this cherry-pick in it? Anyway, I hope this will be node in final release."]}, {"number": 24898, "title": "TensorFlow tfrecord OutOfRange error", "body": "In [src/datasets/h36m_edit.py](https://github.com/MatthewD1993/hmr_extend/blob/5a496695c0ebdc8207a847f4d83c49426a25e934/src/datasets/h36m_edit.py#L196-L229):\r\n```\r\nwith tf.Session() as sess:\r\n    reader = tf.TFRecordReader()\r\n    coder = ImageCoder()\r\n\r\n    fqueue = tf.train.string_input_producer(files, num_epochs=1, shuffle=False, name=\"input\")\r\n    _, example_serialized = reader.read(fqueue)\r\n\r\n    sess.run(tf.local_variables_initializer())\r\n    sess.run(tf.global_variables_initializer())\r\n\r\n    coord = tf.train.Coordinator()\r\n    threads = tf.train.start_queue_runners(coord=coord)\r\n\r\n    fidx = 0\r\n    total_imgs = 0\r\n    image, image_size, label, center, fname, pose, shape, gt3d, has_smpl3d = parse_example_proto(example_serialized)\r\n\r\n    while not coord.should_stop():\r\n        fidx += 1\r\n        tf_filename = out_path% fidx\r\n\r\n        print('Starting tfrecord file %s \\n' % tf_filename)\r\n        with tf.python_io.TFRecordWriter(tf_filename) as writer:\r\n            for i in tqdm(range(train_shards)):  # min(train_shards, image_bs.shape[0])\r\n                image_v, image_size_v, label_v, center_v, fname_v, pose_v, shape_v, gt3d_v, has_smpl3d_v = sess.run(\r\n                    [image, image_size, label, center, fname, pose, shape, gt3d, has_smpl3d])\r\n                image_s = coder.encode_jpeg(image_v)\r\n                example = convert_to_example_wmosh(image_s, fname_v, image_size_v[0], image_size_v[1],\r\n                                                   label_v, center_v, gt3d_v, pose_v, shape_v)\r\n                writer.write(example.SerializeToString())\r\n                total_imgs += 1\r\n\r\n    coord.request_stop()\r\n    coord.join(threads)\r\n```\r\n\r\nSometimes the inner loop stops before it reaches the maximum iter limit (train_shards) 500. \r\n```\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 500/500 [00:02<00:00, 225.07it/s]\r\nStarting tfrecord file /home/cdeng/tf_datasets/tf_records_human36m_wjoints/train_modified/train_0011.tfrecord \r\n\r\n 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 478/500 [00:02<00:00, 225.58it/s]Starting tfrecord file /home/cdeng/tf_datasets/tf_records_human36m_wjoints/train_modified/train_0012.tfrecord \r\n\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 500/500 [00:02<00:00, 230.37it/s]\r\n```\r\nAnd when it writes to the number 625 tfrecord file, there is OutOfRange error (it supposes to finish with more than 3000  tfrecord files, cause human36m train has 1559985 images and each tfrecord contains 500 images). I guess it's because the *image queue* is not handled correctly, maybe the producer is too slow?\r\n\r\n```\r\n/home/cdeng/tf_datasets/tf_records_human36m_wjoints/train_modified/train_0625.tfrecord \r\n 36%|\u2588\u2588\u2588\u258c      | 180/500 [00:00<00:01, 221.50it/s]2019-01-13 22:47:40.946736: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: FIFOQueue '_0_input' is closed and has insufficient elements (requested 1, current size 0)\r\n\t [[Node: ReaderReadV2 = ReaderReadV2[_device=\"/job:localhost/replica:0/task:0/cpu:0\"](TFRecordReaderV2, input)]]\r\n2019-01-13 22:47:40.946816: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: FIFOQueue '_0_input' is closed and has insufficient elements (requested 1, current size 0)\r\n\t [[Node: ReaderReadV2 = ReaderReadV2[_device=\"/job:localhost/replica:0/task:0/cpu:0\"](TFRecordReaderV2, input)]]\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/cdeng/star_repos/hmr/src/datasets/h36m_edit.py\", line 233, in <module>\r\n    [image, image_size, label, center, fname, pose, shape, gt3d, has_smpl3d])\r\n  File \"/home/cdeng/.virtualenvs/hmr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 895, in run\r\n    run_metadata_ptr)\r\n  File \"/home/cdeng/.virtualenvs/hmr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1124, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/cdeng/.virtualenvs/hmr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1321, in _do_run\r\n    options, run_metadata)\r\n  File \"/home/cdeng/.virtualenvs/hmr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1340, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.OutOfRangeError: FIFOQueue '_0_input' is closed and has insufficient elements (requested 1, current size 0)\r\n\t [[Node: ReaderReadV2 = ReaderReadV2[_device=\"/job:localhost/replica:0/task:0/cpu:0\"](TFRecordReaderV2, input)]]\r\n\t [[Node: ParseSingleExample/ParseExample/ParseExample/_21 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_52_ParseSingleExample/ParseExample/ParseExample\", tensor_type=DT_INT64, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\r\n\r\nCaused by op u'ReaderReadV2', defined at:\r\n  File \"/home/cdeng/star_repos/hmr/src/datasets/h36m_edit.py\", line 204, in <module>\r\n    _, example_serialized = reader.read(fqueue)\r\n  File \"/home/cdeng/.virtualenvs/hmr/local/lib/python2.7/site-packages/tensorflow/python/ops/io_ops.py\", line 194, in read\r\n    return gen_io_ops._reader_read_v2(self._reader_ref, queue_ref, name=name)\r\n  File \"/home/cdeng/.virtualenvs/hmr/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 423, in _reader_read_v2\r\n    queue_handle=queue_handle, name=name)\r\n  File \"/home/cdeng/.virtualenvs/hmr/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"/home/cdeng/.virtualenvs/hmr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/home/cdeng/.virtualenvs/hmr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nOutOfRangeError (see above for traceback): FIFOQueue '_0_input' is closed and has insufficient elements (requested 1, current size 0)\r\n\t [[Node: ReaderReadV2 = ReaderReadV2[_device=\"/job:localhost/replica:0/task:0/cpu:0\"](TFRecordReaderV2, input)]]\r\n\t [[Node: ParseSingleExample/ParseExample/ParseExample/_21 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_52_ParseSingleExample/ParseExample/ParseExample\", tensor_type=DT_INT64, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\r\n\r\n\r\nProcess finished with exit code 1\r\n```\r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=00-bug-performance-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Moreover, please provide as many details as possible to find root cause of the issue. Thanks.", "I think the correct way is to handle it with exception handling as suggested in the docs."]}, {"number": 24897, "title": "Fusing context and cell output in AttentionWrapper ", "body": "**System information**\r\n- TensorFlow version (you are using): 1.12\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\n`AttentionWrapper` fuses the `cell_output` and the `context` vector using an `array_ops.concat` operation when specifying an attention layer: [link](https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py#L1067)\r\n\r\nIn my case, this behaviour is not flexible enough, I and would like to be able to define my own function to replace the `concat` operation.\r\n\r\n**Will this change the current api? How?**\r\nSimilarly to `cell_input_fn`, I would like to have a `attention_layer_fn` in `AttentionWrapper`, which by default will perform a `concat`, preserving the old behaviour. This function would be passed to `_compute_attention`, which could be modified like this:\r\n\r\n```\r\nif attention_layer is not None:\r\n  attention_layer_input = attention_layer_fn(cell_output, context)\r\n  attention = attention_layer(attention_layer_input)\r\nelse:\r\n  attention = context\r\n```\r\nwhere\r\n```\r\nif attention_input_fn is None:\r\n  attention_input_fn = (\r\n      lambda cell_output, context: array_ops.concat([cell_output, context], 1))\r\n```\r\n\r\n**Who will benefit with this feature?**\r\nTensorFlow users designing new attention architectures may find this feature useful.\r\nThe users have to implement their own function taking `cell_output` and `context` as inputs.\r\n\r\n**Any Other info.**\r\nI am not a super experienced developer, so I would also like to ask for advice.\r\n\r\nI implemented the feature locally and successfully trained a network, but ran into a new problem.\r\n\r\nThe main goal would be to debug/visualise some internal ops defined by the user inside the new function, just like with `alignment_history`. And here is where things get complicated.\r\n\r\nSince these operations are created inside the `while_loop` of `dynamic_rnn`, I'd have to use a `TensorArray` and write the variables at every step (otherwise it says that the operation has been marked as not fetchable), but this also requires `state.time` when calling `write()`. Then the object returned by `write()` should be added to the `AttentionWrapperState`, to show up in the final_state when running `dynamic_rnn`. Is there a simpler way to visualise all the necessary ops inside the envisaged `attention_input_fn` ?\r\n\r\n\r\n", "comments": ["@qlzh727 fyi.", "Hello,\r\nTo state the feature a bit more clear, imagine you wanted to debug the ops created in a custom `cell_input_fn`, which is already supported in `AttentionWrapper`. How would you design this ?\r\n\r\nWould it be fine to design `cell_input_fn` as a callable with several methods, e.g. `__call__`, `build`, `update`, where you implement all the necessary stuff  from the outside, while `AttentionWrapper` forwards your custom output to another `AttentionWrapperState` member called `payload`, so that you can find everything there in the final state returned by `dynamic_rnn` ?\r\n\r\nI am just trying to make it more simple to design new attention architectures, as everybody has their own ideas of what they should look like. Subclassing `AttentionWrapper` would result in a lot of duplicated code. Am I missing here something, such as a more simple way to visualise my variables ?"]}, {"number": 24896, "title": "TF Keras io_utils_test missing test cases add", "body": "1-test case added for ask_to_proceed_with_overwrite", "comments": ["@Dayananda-V can you please check the test failures ", "@rthadur \r\n\r\nstill build error exist from this change?", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F24896) for more info**.\n\n<!-- need_author_consent -->", "@Dayananda-V please sign CLA and re base your branch", "Corrupt PR is replaced with #27368. "]}, {"number": 24895, "title": "Issue with training of object detection api (ssd_mobilenet_v2_coco.config)", "body": "Command Used:\r\npython train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v2_coco.config\r\n\r\nPlease guide me if i am doing something wrong.\r\n\r\nError Trace:\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 163, in <module>\r\n    tf.app.run()\r\n  File \"/home/tayyab/Desktop/tensor_android/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"train.py\", line 91, in main\r\n    FLAGS.pipeline_config_path)\r\n  File \"/home/tayyab/Desktop/models/object_detection/utils/config_util.py\", line 43, in get_configs_from_pipeline_file\r\n    text_format.Merge(proto_str, pipeline_config)\r\n  File \"/home/tayyab/Desktop/tensor_android/local/lib/python2.7/site-packages/google/protobuf/text_format.py\", line 536, in Merge\r\n    descriptor_pool=descriptor_pool)\r\n  File \"/home/tayyab/Desktop/tensor_android/local/lib/python2.7/site-packages/google/protobuf/text_format.py\", line 590, in MergeLines\r\n    return parser.MergeLines(lines, message)\r\n  File \"/home/tayyab/Desktop/tensor_android/local/lib/python2.7/site-packages/google/protobuf/text_format.py\", line 623, in MergeLines\r\n    self._ParseOrMerge(lines, message)\r\n  File \"/home/tayyab/Desktop/tensor_android/local/lib/python2.7/site-packages/google/protobuf/text_format.py\", line 638, in _ParseOrMerge\r\n    self._MergeField(tokenizer, message)\r\n  File \"/home/tayyab/Desktop/tensor_android/local/lib/python2.7/site-packages/google/protobuf/text_format.py\", line 763, in _MergeField\r\n    merger(tokenizer, message, field)\r\n  File \"/home/tayyab/Desktop/tensor_android/local/lib/python2.7/site-packages/google/protobuf/text_format.py\", line 837, in _MergeMessageField\r\n    self._MergeField(tokenizer, sub_message)\r\n  File \"/home/tayyab/Desktop/tensor_android/local/lib/python2.7/site-packages/google/protobuf/text_format.py\", line 763, in _MergeField\r\n    merger(tokenizer, message, field)\r\n  File \"/home/tayyab/Desktop/tensor_android/local/lib/python2.7/site-packages/google/protobuf/text_format.py\", line 837, in _MergeMessageField\r\n    self._MergeField(tokenizer, sub_message)\r\n  File \"/home/tayyab/Desktop/tensor_android/local/lib/python2.7/site-packages/google/protobuf/text_format.py\", line 763, in _MergeField\r\n    merger(tokenizer, message, field)\r\n  File \"/home/tayyab/Desktop/tensor_android/local/lib/python2.7/site-packages/google/protobuf/text_format.py\", line 837, in _MergeMessageField\r\n    self._MergeField(tokenizer, sub_message)\r\n  File \"/home/tayyab/Desktop/tensor_android/local/lib/python2.7/site-packages/google/protobuf/text_format.py\", line 730, in _MergeField\r\n    (message_descriptor.full_name, name))\r\ngoogle.protobuf.text_format.ParseError: 86:7 : Message type \"object_detection.protos.SsdFeatureExtractor\" has no field named \"use_depthwise\".", "comments": ["Thank you for your post. We noticed you have not filled out the fields in the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Along with the template, please provide as many details as possible to find the root cause of the issue. It would be great if you provide a small code to reproduce the error. Thanks.", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 24894, "title": "Lite: Gather operator test case improvement", "body": "Improving test case for Gather operator by covering missed scenarios.\r\n\r\n1:> 1D Input with 1D Indices \r\n2:> Intermediate Axis Slicing\r\n3:> Intermediate Axis Scalar Pick\r\n4:> Last Axis Scalar Pick\r\n", "comments": []}, {"number": 24893, "title": "the results of tf.image.decode_image() and opencv.imread() are different ", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No\r\n- **TensorFlow installed from (source or binary)**: \r\n- **TensorFlow version (use command below)**: 1.10\r\n- **Python version**: python2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: cuda9.0-cudnn7.0\r\n- **GPU model and memory**: V100,P40\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n```\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nthe results of tf.image.decode_image() and opencv.imread() are different. i read a image file by \r\ntf.gfile.GFile(filename,'r').read() + tf.image.decode_image() and opencv2.imread() respectively. and i get the different results. \r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nimage_data = tf.gfile.GFile(filename,'r').read()\r\ntf.reset_default_graph()  \r\ngraph = tf.Graph()        \r\nwith graph.as_default() as g:   \r\n  with tf.Session(graph=g) as session:\r\n      image = session.run(tf.image.decode_image(image_data, channels = 3))\r\n      print(image)\r\n\r\n[[[  0   0   5]\r\n  [ 79  80  85]\r\n  [ 66  63  70]\r\n  ...\r\n  [233 236 215]\r\n  [233 236 215]\r\n  [233 236 215]]\r\n\r\n [[  0   0   5]\r\n  [ 78  79  84]\r\n  [ 64  61  68]\r\n  ...\r\n  [233 236 215]\r\n  [233 236 215]\r\n  [233 236 215]]\r\n\r\n [[  0   1   6]\r\n  [ 77  78  83]\r\n  [ 61  58  65]\r\n  ...\r\n  [233 236 215]\r\n  [233 236 215]\r\n  [233 236 215]]\r\n\r\n ...\r\n\r\n [[  0   2   0]\r\n  [134 140 138]\r\n  [133 137 136]\r\n  ...\r\n  [ 69  58  75]\r\n  [ 69  60  79]\r\n  [ 72  63  82]]\r\n\r\n [[  0   2   0]\r\n  [134 140 138]\r\n  [133 137 136]\r\n  ...\r\n  [ 41  28  45]\r\n  [ 49  38  55]\r\n  [ 61  49  69]]\r\n\r\n [[  0   3   1]\r\n  [129 135 133]\r\n  [139 143 142]\r\n  ...\r\n  [ 29  15  32]\r\n  [ 25  12  29]\r\n  [ 22   9  27]]]\r\n\r\nimage = cv2.imread(FLAGS.image_dir + '18455308.jpg')\r\nimage = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\r\nprint(image)\r\n\r\n[[[  0   0   7]\r\n  [ 80  80  88]\r\n  [ 66  63  72]\r\n  ...\r\n  [235 237 216]\r\n  [235 237 216]\r\n  [235 237 216]]\r\n\r\n [[  0   0   7]\r\n  [ 79  79  87]\r\n  [ 64  61  70]\r\n  ...\r\n  [235 237 216]\r\n  [235 237 216]\r\n  [235 237 216]]\r\n\r\n [[  0   2   9]\r\n  [ 77  77  85]\r\n  [ 61  58  67]\r\n  ...\r\n  [235 237 216]\r\n  [235 237 216]\r\n  [235 237 216]]\r\n\r\n ...\r\n\r\n [[  0   1   0]\r\n  [135 139 138]\r\n  [134 138 137]\r\n  ...\r\n  [ 69  57  77]\r\n  [ 72  60  82]\r\n  [ 75  63  85]]\r\n\r\n [[  0   1   0]\r\n  [135 139 138]\r\n  [134 138 137]\r\n  ...\r\n  [ 41  28  46]\r\n  [ 51  38  58]\r\n  [ 62  49  69]]\r\n\r\n [[  0   2   1]\r\n  [131 135 134]\r\n  [140 144 143]\r\n  ...\r\n  [ 29  15  32]\r\n  [ 25  12  30]\r\n  [ 22   9  27]]]\r\n\r\ni am wondering is that normal? and i found this may cause different predict results with the same model. ", "comments": ["Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support. Github is mainly for addressing bugs in installation and performance. Thanks!", "The default JPEG decoder used by TensorFlow in `tf.image.decode_image()` sacrifices image accuracy in favor of decoding speed (see #5072). This results in slightly different pixel values when compared to other decoders.\r\n\r\nIf you want to maintain image accuracy (and match the results of `cv2.imread()`), then you can use `tf.image.decode_jpeg(image_data, channels = 3, dct_method='INTEGER_ACCURATE')` to decode your JPEG file in TensorFlow.", "I think it was resolved. Closing due to lack of recent activity. Please open new ticket if you see similar issue. Thanks!", "I found sometimes the two function results are also diferent in image shape. For example, tf's (3456,4608,3) while opencv's (4608,3456,3)"]}, {"number": 24892, "title": "\"Transient arrays with strings are not supported yet\" when converting GraphDef to tflite file", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.12\r\n- Python version: 3.5\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 9.0/7.4\r\n- GPU model and memory: GTX1080Ti 11GB\r\n\r\n\r\n**Describe the current behavior**\r\nNo file was created.\r\n\r\n**Describe the expected behavior**\r\nA .tflite file is converted.\r\n\r\n**Code to reproduce the issue**\r\nThe model cann't be provided.\r\n\r\n**Other info / logs**\r\n```\r\n CUDA_VISIBLE_DEVICES=\"2\" tflite_convert \\\r\n  --output_file=frozen.tflite \\\r\n  --graph_def_file=model.pb \\\r\n  --input_arrays=input_image \\\r\n  --output_arrays=train_net/recon_image\r\n2019-01-14 17:09:08.298351: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-01-14 17:09:08.549870: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6325\r\npciBusID: 0000:82:00.0\r\ntotalMemory: 10.92GiB freeMemory: 10.76GiB\r\n2019-01-14 17:09:08.549910: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\r\n2019-01-14 17:09:08.874792: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-01-14 17:09:08.874835: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 \r\n2019-01-14 17:09:08.874846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N \r\n2019-01-14 17:09:08.875110: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10409 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:82:00.0, compute capability: 6.1)\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/tflite_convert\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 412, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 408, in run_main\r\n    _convert_model(tflite_flags)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 162, in _convert_model\r\n    output_data = converter.convert()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/lite.py\", line 453, in convert\r\n    **converter_kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/convert.py\", line 342, in toco_convert_impl\r\n    input_data.SerializeToString())\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/convert.py\", line 135, in toco_convert_protos\r\n    (stdout, stderr))\r\nRuntimeError: TOCO failed see console for info.\r\nb'2019-01-14 17:09:13.458649: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\\n2019-01-14 17:09:13.697666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6325\\npciBusID: 0000:82:00.0\\ntotalMemory: 10.92GiB freeMemory: 10.56GiB\\n2019-01-14 17:09:13.697710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\\n2019-01-14 17:09:14.017280: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\\n2019-01-14 17:09:14.017321: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 \\n2019-01-14 17:09:14.017329: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N \\n2019-01-14 17:09:14.018988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10209 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:82:00.0, compute capability: 6.1)\\n2019-01-14 17:09:14.053557: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Round\\n2019-01-14 17:09:14.061350: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ImageSummary\\n2019-01-14 17:09:14.061401: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1127] Op node missing output type attribute: train_net/recon_image\\n2019-01-14 17:09:14.067392: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 168 operators, 287 arrays (0 quantized)\\n2019-01-14 17:09:14.070050: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 168 operators, 287 arrays (0 quantized)\\n2019-01-14 17:09:14.105968: W tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_random_uniform.cc:85] RandomUniform op outputting \"build_towers/tower_0/train_net_inference_one_pass/train_net/random_uniform_1/RandomUniform\" is truly random (using /dev/random system entropy). Therefore, cannot resolve as constant. Set \"seed\" or \"seed2\" attr non-zero to fix this\\n2019-01-14 17:09:14.135825: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 47 operators, 89 arrays (0 quantized)\\n2019-01-14 17:09:14.136127: W tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_random_uniform.cc:85] RandomUniform op outputting \"build_towers/tower_0/train_net_inference_one_pass/train_net/random_uniform_1/RandomUniform\" is truly random (using /dev/random system entropy). Therefore, cannot resolve as constant. Set \"seed\" or \"seed2\" attr non-zero to fix this\\n2019-01-14 17:09:14.136547: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 47 operators, 89 arrays (0 quantized)\\n2019-01-14 17:09:14.137116: F tensorflow/contrib/lite/toco/tooling_util.cc:1674] Transient arrays with strings are not supported yet\\nAborted (core dumped)\\n'\r\nNone\r\n```\r\n\r\nWhat does \"Transient arrays with strings are not supported yet\" means? What should I modify in my model?", "comments": ["I have the same issue too. Any updates?", "I am testing on a more simplified graph and the error disappears.\r\n\r\nBut I still want to know what caused the error and what does the error message means.", "@Iamanorange what did you change in your graph to fix it?", "Remove most of OPs. Almost leaving only convolution and basic math/logic OPs.\r\n\r\nThat's why I don't know where the error was.", "Strings aren't fully supported by TensorFlow Lite. I recommend testing your original model with the `tf-nightly` since it's some thing we are actively working on. If your original model doesn't work with the latest nightly, can you provide your model (or a minimal model that reproduces the error) along with the command you used to try and convert that model.\r\n\r\nReassigning to @haozha111 who is working on adding support for HashTables (which is related to general string support).", "I'm seeing that you are still using tensorflow/contrib/lite/toco, probably you can update your tf repo to the latest and test if that helps. Also, a minimum model to reproduce the issue might be useful to debug. Thanks.", "Thank you for helping.\r\n\r\nI gave up using tflite due to lack of  OP supports.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=24892\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=24892\">No</a>\n"]}, {"number": 24891, "title": "fixed typo", "body": "", "comments": []}, {"number": 24890, "title": "XLA Compiler Error \"(...)C++ compilation of rule '//tensorflow/compiler/xla:window_util' failed (Exit 2): python.exe failed: error executing command\"", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Version 1809 Betriebssystembuild: 17763.253\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): source (install impossible)\r\n- TensorFlow version: 1.12\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: virtualenv: yes; pip: yes anaconda: no\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source):-\r\n- CUDA/cuDNN version: 10.0/7\r\n- GPU model and memory: RTX 2070 8GB\r\n- Microsoft Visual Studio Version: VS 2017 Enterprise (I am student)\r\n\r\n\r\n\r\n**Describe the problem**\r\nIf i build the pip package, than  following error occured:\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nERROR: F:/*/tensorflow/tensorflow/compiler/xla/BUILD:711:1: C++ compilation of rule '//tensorflow/compiler/xla:window_util' failed (Exit 2): python.exe failed: error executing command\r\n  cd C:/users/bjoern_std/_bazel_bjoern_std/h2irjqdy/execroot/org_tensorflow\r\n\r\n**Any other info / logs**\r\nLog one step before the error and one step after:\r\n\"(...)Hinweis: Einlesen der Datei:    external/com_google_absl\\absl/numeric/int128_no_intrinsic.inc\r\nERROR: F:/*/tensorflow/tensorflow/compiler/xla/BUILD:711:1: C++ compilation of rule '//tensorflow/compiler/xla:window_util' failed (Exit 2): python.exe failed: error executing command\r\n  cd C:/users/*/_bazel_bjoern_std/h2irjqdy/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Tools\\MSVC\\14.16.27023\\ATLMFC\\include;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Tools\\MSVC\\14.16.27023\\include;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\include\\um;F:\\Windows Kits\\10\\include\\10.0.17763.0\\ucrt;F:\\Windows Kits\\10\\include\\10.0.17763.0\\shared;F:\\Windows Kits\\10\\include\\10.0.17763.0\\um;F:\\Windows Kits\\10\\include\\10.0.17763.0\\winrt;F:\\Windows Kits\\10\\include\\10.0.17763.0\\cppwinrt\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Tools\\MSVC\\14.16.27023\\ATLMFC\\lib\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Tools\\MSVC\\14.16.27023\\lib\\x64;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\lib\\um\\x64;F:\\Windows Kits\\10\\lib\\10.0.17763.0\\ucrt\\x64;F:\\Windows Kits\\10\\lib\\10.0.17763.0\\um\\x64;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Tools\\MSVC\\14.16.27023\\bin\\HostX64\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\MSBuild\\15.0\\bin\\Roslyn;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\Team Tools\\Performance Tools\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\Team Tools\\Performance Tools;F:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\\\x64;F:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\Common7\\IDE\\CommonExtensions\\Microsoft\\FSharp\\;F:\\Windows Kits\\10\\bin\\10.0.17763.0\\x64;F:\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\\\MSBuild\\15.0\\bin;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\Common7\\IDE\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\Common7\\Tools\\;;C:\\Windows\\system32;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Program Files/Python36/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Program Files/Python36/lib/site-packages\r\n    SET TEMP=C:\\Users\\*~1\\AppData\\Local\\Temp\r\n    SET TF_CUDA_CLANG=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=7.0,7.5\r\n    SET TF_CUDA_VERSION=10.0\r\n    SET TF_CUDNN_VERSION=7\r\n    SET TF_NEED_CUDA=1\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n    SET TMP=C:\\Users\\*~1\\AppData\\Local\\Temp\r\n  C:/Program Files/Python36/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/genfiles /Ibazel-out/x64_windows-opt/bin /Iexternal/nsync /Ibazel-out/x64_windows-opt/genfiles/external/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/genfiles/external/local_config_sycl /Ibazel-out/x64_windows-opt/bin/external/local_config_sycl /Iexternal/protobuf_archive /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/genfiles/external/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/gif_archive /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive /Ibazel-out/x64_windows-opt/bin/external/gif_archive /Iexternal/jpeg /Ibazel-out/x64_windows-opt/genfiles/external/jpeg /Ibazel-out/x64_windows-opt/bin/external/jpeg /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/genfiles/external/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/genfiles/external/fft2d /Ibazel-out/x64_windows-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-opt/genfiles/external/highwayhash /Ibazel-out/x64_windows-opt/bin/external/highwayhash /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/genfiles/external/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/genfiles/external/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/protobuf_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/src /Iexternal/gif_archive/lib /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/lib /Ibazel-out/x64_windows-opt/bin/external/gif_archive/lib /Iexternal/gif_archive/windows /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/windows /Ibazel-out/x64_windows-opt/bin/external/gif_archive/windows /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive/src /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive/src /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DEIGEN_HAS_TYPE_TRAITS=0 /D__CLANG_SUPPORT_DYN_ANNOTATION__ /DTF_USE_SNAPPY /showIncludes /MD /O2 /DNDEBUG -w /arch:AVX /Fobazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/_objs/window_util/window_util.o /c tensorflow/compiler/xla/window_util.cc\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nHinweis: Einlesen der Datei: .\\tensorflow/compiler/xla/window_util.h(...)\"\r\n\"(...) Hinweis: Einlesen der Datei:   external/com_google_absl\\absl/strings/string_view.h\r\nHinweis: Einlesen der Datei:    C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Tools\\MSVC\\14.16.27023\\include\\cassert\r\nHinweis: Einlesen der Datei:     F:\\Windows Kits\\10\\include\\10.0.17763.0\\ucrt\\assert.h\r\nHinweis: Einlesen der Datei: .\\tensorflow/core/platform/logging.h\r\nHinweis: Einlesen der Datei:  .\\tensorflow/core/platform/default/logging.h\r\nHinweis: Einlesen der Datei:   .\\tensorflow/core/platform/macros.h\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 536,864s, Critical Path: 314,01s\r\nINFO: 2538 processes: 2538 local.\r\nFAILED: Build did NOT complete successfully(...)\"\r\n\r\nNOTE from threadauthor: I have short the log, because they have only read acknowledges!\r\nA star (*) in the path means, that i anonymised my account name!\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@gittyhub2018  Please check [tested build configurations](https://www.tensorflow.org/install/source_windows). Please check Bazel version. For the system configuration you mentioned, MSVC 2015, Bazel 0.15.0, cuDNN 7, CUDA 9.0  are supported. Could you try installing CUDA 9.0 and check whether the issue persists? Thanks!", "Error says **cd** failed. Is possible that you ran out of disk space or have a permission issue? I don't think this is related with cuda version.", "> \r\n> \r\n> Error says **cd** failed. Is possible that you ran out of disk space or have a permission issue? I don't think this is related with cuda version.\r\n\r\nI have try to compile it one times with admin rights. ( I know it isn't well, but i can proof if the error is a permission error). And now i get follow new error:\r\n\r\n\"bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\n(...)\r\nHinweis: Einlesen der Datei:  C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Tools\\MSVC\\14.16.27023\\include\\cassert\r\nHinweis: Einlesen der Datei:   F:\\Windows Kits\\10\\include\\10.0.17763.0\\ucrt\\assert.h\r\nHinweis: Einlesen der Datei: external/llvm/include\\llvm/TableGen/StringMatcher.h\r\nHinweis: Einlesen der Datei: external/llvm/include\\llvm/TableGen/StringToOffsetTable.h\r\nHinweis: Einlesen der Datei:  C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Tools\\MSVC\\14.16.27023\\include\\cctype\r\nHinweis: Einlesen der Datei: external/llvm/include\\llvm/TableGen/TableGenBackend.h\r\nHinweis: Einlesen der Datei: C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Tools\\MSVC\\14.16.27023\\include\\cassert\r\nHinweis: Einlesen der Datei:  F:\\Windows Kits\\10\\include\\10.0.17763.0\\ucrt\\assert.h\r\n(...)\r\nHinweis: Einlesen der Datei: C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Tools\\MSVC\\14.16.27023\\include\\forward_list\r\nc:\\users\\*\\_bazel_*\\h2irjqdy\\execroot\\org_tensorflow\\external\\llvm\\utils\\tablegen\\asmmatcheremitter.cpp(1466) : fatal error C1001: Interner Compilerfehler.\r\n(Compilerdatei \"d:\\agent\\_work\\1\\s\\src\\vctools\\compiler\\utc\\src\\p2\\main.c\", Zeile 187)\r\n Vereinfachen oder \u00e4ndern Sie das Programm im Umfeld der oben aufgef\u00fchrten Positionen. W\u00e4hlen\r\nSie im Men\u00fc \"Hilfe\" von Visual C++ den Befehl \"Technischer Support\",\r\noder \u00f6ffnen Sie die Hilfedatei des technischen Supports, um weitere Informationen zu erhalten.\r\n  cl!InvokeCompilerPassW()+0x752ab\r\n  cl!InvokeCompilerPassW()+0x74715\r\n  cl!InvokeCompilerPassW()+0x7504c\r\n  cl!InvokeCompilerPassW()+0x5bfc4\r\n  cl!InvokeCompilerPassW()+0x54d5c\r\n  cl!InvokeCompilerPassW()+0x51036\r\n  cl!InvokeCompilerPassW()+0x509fa\r\n  cl!CloseTypeServerPDB()+0x494af\r\n  cl!o_exp()+0x5a\r\n  cl!BaseThreadInitThunk()+0x14\r\n  cl!RtlUserThreadStart()+0x21\r\n\r\nc:\\users\\*\\_bazel_*\\h2irjqdy\\execroot\\org_tensorflow\\external\\llvm\\utils\\tablegen\\asmmatcheremitter.cpp(1466) : fatal error C1001: Interner Compilerfehler.\r\n(Compilerdatei \"d:\\agent\\_work\\1\\s\\src\\vctools\\compiler\\utc\\src\\common\\error.c\", Zeile 835)\r\n Vereinfachen oder \u00e4ndern Sie das Programm im Umfeld der oben aufgef\u00fchrten Positionen. W\u00e4hlen\r\nSie im Men\u00fc \"Hilfe\" von Visual C++ den Befehl \"Technischer Support\",\r\noder \u00f6ffnen Sie die Hilfedatei des technischen Supports, um weitere Informationen zu erhalten.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 380,364s, Critical Path: 121,13s\r\nINFO: 1241 processes: 1241 local.\r\nFAILED: Build did NOT complete successfully\"\r\n\r\nI don't know what this mean. In VS C++ under \"Technischer Support\" doesn't exist a theme.", "> \r\n> \r\n> @gittyhub2018 Please check [tested build configurations](https://www.tensorflow.org/install/source_windows). Please check Bazel version. For the system configuration you mentioned, MSVC 2015, Bazel 0.15.0, cuDNN 7, CUDA 9.0 are supported. Could you try installing CUDA 9.0 and check whether the issue persists? Thanks!\r\n\r\nI will test it in two weeks, 'cause i'm at the moment in the exam time. Sry for my cause inconvience.", "@samikama When you done with your exams try this [link](https://github.com/rnreich/ubuntu-tensorflow-gpu-all-versions) and install tensorflow-gpu and let us know the status. Thanks!", "@jvishnuvardhan Did you mean to tag @gittyhub2018 ? I am not aware of any upcoming exams ;)", "@samikama You are right. I meant to tag @gittyhub2018 . Sorry for the mistake. Thanks!", "@gittyhub2018 Please let me know if the issue was resolved or not. If there was no response, I will assume it was resolved. Thanks!", "I am closing this issue. Please open a new ticket when new information is available. Thanks!", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)", "> \r\n> \r\n> @gittyhub2018 Please check [tested build configurations](https://www.tensorflow.org/install/source_windows). Please check Bazel version. For the system configuration you mentioned, MSVC 2015, Bazel 0.15.0, cuDNN 7, CUDA 9.0 are supported. Could you try installing CUDA 9.0 and check whether the issue persists? Thanks!\r\n\r\nI'm not able to install CUDA 9, because my hardware (RTX 2070) is newer than the supported Version.\r\nI've upload a screenshot from the error of the CUDA 9 installer.\r\nI don't have checked anymore after this error, because it makes no sense. Any other idea or i must wait for a newer version?\r\n![error_cuda_9](https://user-images.githubusercontent.com/33618201/52917464-2b33da80-32ec-11e9-9cc4-7fcb221641c9.jpg)\r\n", "@gittyhub2018 You could try to install new version of TF. Please also check the solution provided [here](https://github.com/tensorflow/tensorflow/issues/25692) to disable the character limit on Windows10. Thanks!", "@jvishnuvardhan \r\nI have install TF 2.0 nightly directly via pip and it works (See the code under these Article). I don't need to compiled it from source. At the beginning i thought with building from source i can use CUDA 10 with my graphic card. But this was my fault. Sry.\r\nOnly one information appear: AVX2 support ist not compiled with the nightly. Could this fixed in the final build?\r\nI have a AMD Ryzen 7 2700 CPU. And thank you for your information about the 260 character issue.\r\n\r\n\r\n` python -c \"import tensorflow as tf; tf.enable_eager_execution(); print(tf.reduce_sum(tf.random_normal([1000, 1000])))\"\r\n2019-02-20 08:06:44.972272: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2019-02-20 08:06:45.013126: I tensorflow/stream_executor/platform/default/dso_loader.cc:161] successfully opened CUDA library nvcuda.dll locally\r\n2019-02-20 08:06:45.389586: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1464] Found device 0 with properties:\r\nname: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.62\r\npciBusID: 0000:0a:00.0\r\ntotalMemory: 8.00GiB freeMemory: 6.59GiB\r\n2019-02-20 08:06:45.395495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1543] Adding visible gpu devices: 0\r\n2019-02-20 08:06:46.023658: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1015] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-02-20 08:06:46.027051: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1021]      0\r\n2019-02-20 08:06:46.029131: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] 0:   N\r\n2019-02-20 08:06:46.031437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1146] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6319 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:0a:00.0, compute capability: 7.5)\r\ntf.Tensor(1558.7668, shape=(), dtype=float32)`", "@gunan, \r\n*AVX2 support is not compiled with the nightly. Could this fixed in the final build (TF2.0)?*", "Our pip builds aim to be able to work an the widest platforms possible.\r\nIf we build AVX2 into them, the packages wont work on a wide range of older CPUs.\r\nSo our packages will still not be built with avx2 in 2.0 final."]}, {"number": 24889, "title": "Removal of warning from backend.py file", "body": "Removed the compiler warning where rate to be used in the dropout op", "comments": ["@fchollet & @hgadig , can you pls review the PR", "@fchollet  Could you please approve this PR.", "@fchollet & @hgadig , i have updated the code as per the comments,kindly check and if all ok  then kindly approve the PR.", "@amitsrivastava78  Did you get a chance to look on reviewer comments? Please let us know on the update. Also request you have a look on conflicts. Thanks!", "@gbaned , sorry for the delay in response,  I have checked the implementation and this seems to have changed, so this PR is no more valid, i am closing this PR.\r\n\r\nThanks alot for your support, will keep interacting with you on other PRs.\r\n\r\nRegards\r\nAmit"]}, {"number": 24888, "title": "Tflite Op missing:Stack", "body": "**System information**\r\n- OS Platform and Distribution (Linux Ubuntu 16.04):\r\n- TensorFlow installed from ( pip ):\r\n- TensorFlow version ( 1.10 ):\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.contrib.lite.toco_convert(). Here is a list of operators for which  you will need custom implementations: Stack.\r\n\r\n```\r\n", "comments": ["As far as I know, the Op 'Stack' is closely related to Op 'Strided_slice', and both of them seems easy to be implemented. \r\nSo I wonder why these Ops weren not supported by the standard TensorFlow Lite runtime. ", "@Archernarkiu Could you provide an example code to reproduce the error? That will help us to find the root-cause of the issue? Thanks!", "@jvishnuvardhan Thanks for your help, and my code shows below:\r\n\r\n>  resize_image(image, size,method=tf.image.ResizeMethod.BILINEAR,align_corners=False):\r\n    \"\"\"Resize an image and bounding boxes. \"\"\"\r\n    # Resize image.\r\n    with tf.name_scope('resize_image'):\r\n        height, width, channels = _ImageDimensions(image)\r\n        image = tf.expand_dims(image, 0)\r\n        image = tf.image.resize_images(image, size,method, align_corners)\r\n        image = tf.reshape(**image, tf.stack([size[0], size[1], channels])**)\r\n        return image\r\n\r\nThe bold code is an example of calling 'stack' Op explicitly. However, in most cases, when I have defined my model, some Op like 'strided_slice' and 'stack' will be added into GraphDef automatically.", "@Archernarkiu : I have tested your piece of code modified as below\r\n\r\n```\r\ninput = tf.placeholder(dtype = \"float32\", shape=(224, 224, 3))\r\nindices = tf.placeholder(dtype = \"int32\", shape=(3))\r\nexpand = tf.expand_dims(input, 0)\r\nout = tf.reshape(expand, tf.stack([indices[0], indices[1], indices[2]])) \r\n```\r\n\r\n\r\nThe GraphDef never contains any operator name as \"Stack\".\r\ntf.stack() internally maps to \"Pack\" operator which was deprecated and removed in earlier Tensorflow version.\r\n\r\nAnd TOCO can successfully translate \"Pack\" operator as TFLite runtime has it.\r\n\r\nNote: I have tested above code in both Tensorflow 1.10 and 1.12. Both are good to go.\r\n\r\n\r\nSo i suggest you crosscheck your logs properly may be misleading info. If you are still sure this is the problem.\r\n\r\nThen try a small test program which can reproduce the issue, and share it here, so that i can debug further.\r\n\r\nHope it helps.\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 24887, "title": "Distributed training fails when using CollectiveAllReduceStrategy", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n16.04 \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\nSource\r\n- TensorFlow version (use command below):\r\ntensorflow (1.12.0rc0)\r\n- Python version:\r\nPython3.4\r\n- Bazel version (if compiling from source):\r\nbazel 0.16\r\n- GCC/Compiler version (if compiling from source):\r\ngcc4.8.5\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nI am trying to employing CollectiveAllReduceStrategy upon tensorflow official model resnet following the instructions from https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute#multi-worker-training.\r\nCode: https://github.com/threeleafzerg/models\r\nSteps:\r\n1) Prepare dataset: python3 cifar10_download_and_extract.py --${PWD}/cifar_data\r\n2) Start Worker1: https://github.com/threeleafzerg/models/blob/master/official/resnet/worker1.sh\r\n3) Start Worker2: https://github.com/threeleafzerg/models/blob/master/official/resnet/worker2.sh\r\nI expect that the distributed training could start successfully. \r\n\r\nBut unfortunately, I got python exceptions. \r\n  File \"/home/zhouhaiy/.local/lib/python3.4/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 403, in _init_from_arg s\r\n    initial_value() if init_from_fn else initial_value,\r\n  File \"/home/zhouhaiy/.local/lib/python3.4/site-packages/tensorflow/contrib/distribute/python/collective_all_reduce_strategy.py\", lin e 180, in _overridden_initial_value_fn\r\n    group_size, group_key, collective_instance_key)\r\n  File \"/home/zhouhaiy/.local/lib/python3.4/site-packages/tensorflow/python/ops/collective_ops.py\", line 94, in broadcast_send\r\n    'Parameter group_size to broadcast_send must be at least 2.')\r\nValueError: Parameter group_size to broadcast_send must be at least 2.\r\n\r\n**Describe the expected behavior**\r\nDistributed training can start successfully. \r\n\r\n**Code to reproduce the issue**\r\nI have uploaded my experiment code in my private branch: https://github.com/threeleafzerg/models\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Could you try with `tf-nightly`? I believe we fixed a couple of bugs related to collective ops after we cut r1.12.", "byronyi,\r\nThanks for your quick response.\r\nFollowing your instructions, I install tf-nightly. I found two issues with tf-nightly build.\r\n1) There's bug in this line: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/distribute/estimator_training.py#L302. It should be \"if 'evaluator' in cluster_spec.jobs:\".\r\n2) After fix 1) bug, it complains that we should use standalone client mode when using estimator.train\r\n  File \"/home/zhouhaiy/.local/lib/python3.4/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1180, in _train_model_distributed\r\n    hooks)\r\n  File \"/home/zhouhaiy/.local/lib/python3.4/site-packages/tensorflow/python/distribute/estimator_training.py\", line 310, in estimator_train\r\n    raise ValueError('Only `STANDALONE_CLIENT` mode is supported when you call '\r\nValueError: Only `STANDALONE_CLIENT` mode is supported when you call `estimator.train`\r\n\r\nSo I followed the following post to launch standalone client. But I still met the issue. Can you help check whether there's any issue in my procedure? \r\nPost: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute#standalone-client-mode \r\nStep:\r\n1.  Launch cluster.  \r\n`#!/bin/bash\r\n\r\nexport TF_CONFIG='{\r\n    \"cluster\": {\r\n          \"worker\": [\"192.168.20.50:1111\", \"192.168.20.52:1112\"]\r\n            },\r\n    \"task\": {\"type\": \"worker\", \"index\": 0}\r\n}'\r\npython3 tf_std_server.py`\r\nand \r\n`#!/bin/bash\r\n\r\nexport TF_CONFIG='{\r\n    \"cluster\": {\r\n          \"worker\": [\"192.168.20.50:1111\", \"192.168.20.52:1112\"]\r\n            },\r\n          \"task\": {\"type\": \"worker\", \"index\": 1}\r\n}'\r\npython3 tf_std_server.py`\r\ntf_std_server.py is from ecosystem. https://github.com/tensorflow/ecosystem/blob/master/distribution_strategy/tf_std_server.py\r\n2. Change Distribute Config\r\n\r\n`  distribution_strategy = collective_all_reduce_strategy.CollectiveAllReduceStrategy(\r\n                            num_gpus_per_worker=0)\r\n   run_config = tf.estimator.RunConfig(\r\n      experimental_distribute=tf.contrib.distribute.DistributeConfig(\r\n        train_distribute=distribution_strategy,\r\n        remote_cluster={\"worker\": [\"192.168.20.50:1111\", \"192.168.20.52:1112\"]}),\r\n      session_config=session_config,\r\n      save_checkpoints_secs=60*60*24,\r\n      )`\r\nAnd run the real resnet model:\r\npython3 cifar10_main.py --data_dir=${PWD}/cifar_data\r\n\r\nBut I encounter this assertion failure:\r\n  File \"/home/zhouhaiy/.local/lib/python3.4/site-packages/tensorflow/python/distribute/mirrored_strategy.py\", line 704, in _batch_reduce_to\r\n    reduce_op, value_destination_pairs)\r\n  File \"/home/zhouhaiy/.local/lib/python3.4/site-packages/tensorflow/python/distribute/cross_device_ops.py\", line 277, in batch_reduce\r\n    return self._batch_reduce(reduce_op, value_destination_pairs)\r\n  File \"/home/zhouhaiy/.local/lib/python3.4/site-packages/tensorflow/python/distribute/cross_device_ops.py\", line 872, in _batch_reduce\r\n    [v[0] for v in value_destination_pairs])\r\n  File \"/home/zhouhaiy/.local/lib/python3.4/site-packages/tensorflow/python/distribute/cross_device_ops.py\", line 912, in _batch_all_reduce\r\n    \"Id\")\r\n  File \"/home/zhouhaiy/.local/lib/python3.4/site-packages/tensorflow/python/distribute/cross_device_utils.py\", line 353, in build_collective_reduce\r\n    raise ValueError('num_workers * len(input_tensors) must be 2 or greater')\r\nValueError: num_workers * len(input_tensors) must be 2 or greater\r\n\r\n\r\n\r\n\r\n\r\n", "@yuefengz Could you take a look here?", "The error message points out that you need to have two replicas to make `CollectiveAllReduceStrategy` to work. Either at least 2 GPUs on a single machine or multiple machines.\r\n\r\nFor distributed training, please use `tf.estimator.train_and_evaluate` API. \r\n\r\nClosing this issue, feel free to re-open it.\r\n\r\n\r\n", "@yuefengz  \r\nMy scnario is that we use CPU only to do the distributed training. So I pass num_gpus_per_worker=0 to CollectiveAllReduceStrategy. Do you mean currently CollectiveAllReduceStrategy doesn't support CPU only distributed training? ", "If you use cpu only, you have to have at least two machines.", "Try not touch num_gpus_per_worker, i.e. leave it as it\u2019s default, and try again.\r\n\r\nI tried myself a couple of months ago and it did work with CPU only TF. Let me know if your case does not.", "@yuefengz Yes, I did do the distributed training on two machine with CPU only. (192.168.20.50, 192.168.20.52)\r\n@byronyi I tried to leave num_gpus_per_worker as default. (=2) But the error is still the same.\r\nFollowing is what I found after I debug the code:\r\nIn standalone client mode, CollectiveAllReduceExtended will use default init function _initialize_local_worker in which it will set num_workers to 1.\r\n`\r\n  def _initialize_local_worker(self, num_gpus_per_worker):\r\n    \"\"\"Initializes the object for local training.\"\"\"\r\n    self._is_chief = True\r\n    **self._num_workers = 1**\r\n\r\n    if num_gpus_per_worker:\r\n      local_devices = tuple(\r\n          \"/device:GPU:%d\" % i for i in range(num_gpus_per_worker)\r\n      )\r\n    else:\r\n      local_devices = (\"/device:CPU:0\",)\r\n`\r\nThus its CollectiveAllReduce.num_workers is set to 1 too. That's why I got the error. \"ValueError: num_workers * len(input_tensors) must be 2 or greater\".\r\nSo does that mean standalone client mode is not suitable for multi machine distributed training? \r\nI will modify code to use tf.estimator.train_and_evaluate so that I can use the independent worker mode. \r\n\r\nBackground: I am a developer from Intel tensorflow team focusing upon multi-node. Currently, we are using horovod as allreduce solution. But it's said that distributed strategy will be a trend in tensorflow for allreduce solution. So I am evaluating the impact of distributed strategy. Any comments and helps are highly appreciated!\r\n\r\n", "Sorry, could you give a minimal reproducing example? I have tried my self with latest nightly and failed to find any problems.\r\n\r\nhttps://gist.github.com/ca1b55e5a5423d5b3abb9efc6fd34b80", "By the way, I could not find a way to get rid of the following warning:\r\n\r\n```\r\nWARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\r\n```\r\n\r\n@yuefengz Any idea what is wrong?", "@byronyi might be something wrong with the model. I didn't see it with ResNet50. I'll re-run examples to reproduce this warning later.", "@byronyi Thank you for your enlightening script. I tried it both on our company's cluster and my private machine.  On our cluster the worker script got hang and in my private machine (a cleaner environment), the worker script got error. I have pasted the log in https://gist.github.com/ca1b55e5a5423d5b3abb9efc6fd34b80. Can you help to check? ", "@yuefengz Can you provide the full script of resnet50 example? Thanks!", "I have no idea why you met 'https' scheme not supported in proxy URI problem. I never saw that before.\r\n\r\nCould you replace localhost with 127.0.0.1? Also running in an TF docker might help.", "@byronyi  I set it as 127.0.0.1, but it's the same.\r\nI found that it's caused by invalid setting of https_proxy. After I set https_proxy as the correct one, my private desktop got hang too as my clusters. Anyway, I think it's related with grpc's https_proxy, do you know how to disable https_proxy run-time for grpc?  Also, I am confused that even within single machine (localhost), the tf server needs https_proxy.  ", "@byronyi  I also asked one of my colleague to run the same set of scripts and got the same result. \r\n\r\nThe hang python call stack is as follows:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\n^CTraceback (most recent call last):\r\n  File \"worker.py\", line 48, in <module>\r\n    eval_spec=eval_spec)\r\n  File \"/home/sunbear/miniconda2/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/training.py\", line 462, in train_and_evaluate\r\n    estimator, train_spec, eval_spec, _TrainingExecutor)\r\n  File \"/home/sunbear/miniconda2/lib/python2.7/site-packages/tensorflow/python/distribute/estimator_training.py\", line 289, in train_and_evaluate\r\n    session_config=run_config.session_config)\r\n  File \"/home/sunbear/miniconda2/lib/python2.7/site-packages/tensorflow/python/distribute/distribute_coordinator.py\", line 778, in run_distribute_coordinator\r\n    session_config, rpc_layer)\r\n  File \"/home/sunbear/miniconda2/lib/python2.7/site-packages/tensorflow/python/distribute/distribute_coordinator.py\", line 466, in _run_between_graph_client\r\n    coord.join(threads_to_join)\r\n  File \"/home/sunbear/miniconda2/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 363, in join\r\n    while any(t.is_alive() for t in threads) and not self.wait_for_stop(1.0):\r\n  File \"/home/sunbear/miniconda2/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 311, in wait_for_stop\r\n    return self._stop_event.wait(timeout)\r\n  File \"/home/sunbear/miniconda2/lib/python2.7/threading.py\", line 614, in wait\r\n    self.__cond.wait(timeout)\r\n  File \"/home/sunbear/miniconda2/lib/python2.7/threading.py\", line 359, in wait\r\n    _sleep(delay)\r\nKeyboardInterrupt", "@byronyi  I can run the distributed training successfully(CollectiveAllReduceStrategy) on my local machine. But it's still failed in our company cluster. (same set of software) Our clusters's is managed by slurm. Does it conflicts with gRPC? Is there any pre-cautions for using gRPC from network perspective? \r\n\r\nFailed log for independent worker mode:\r\nINFO:tensorflow:Graph was finalized.\r\n2019-01-24 15:55:33.368302: I tensorflow/core/distributed_runtime/master_session.cc:1192] Start master session 725fac0e10d1fc7b with   config: device_filters: \"/job:worker/task:1\" device_filters: \"/job:worker/task:1\" allow_soft_placement: true graph_options { rewrite_  options { meta_optimizer_iterations: ONE scoped_allocator_optimization: ON scoped_allocator_opts { enable_op: \"CollectiveReduce\" enab  le_op: \"CollectiveReduce\" enable_op: \"CollectiveReduce\" } } } experimental { collective_group_leader: \"/job:worker/replica:0/task:0\"   }\r\n2019-01-24 15:55:33.519166: E tensorflow/core/distributed_runtime/rpc_collective_executor_mgr.cc:90] Bad response [Unavailable: Socke  t closed\r\nAdditional GRPC error information:\r\n{\"created\":\"@1548316533.519114543\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file  _line\":1036,\"grpc_message\":\"Socket closed\",\"grpc_status\":14}] from GetStepSequenceAsync call to /job:worker/replica:0/task:0\r\n2019-01-24 15:55:33.519293: E tensorflow/core/distributed_runtime/master_session.cc:1493] Bad status from collective_executor_mgr->Re  freshStepIdSequence: Unavailable: Socket closed\r\n\r\nFailed Log from standalone client mode:\r\nINFO:tensorflow:Graph was finalized.\r\nINFO:tensorflow:An error was raised while a session was being created. This may be due to a preemption of a connected worker or parameter server. A new session will be created. This error may also occur due to a gRPC failure caused by high memory or network bandwidth usage in the parameter servers. If this error occurs repeatedly, try increasing the number of parameter servers assigned to the job. Error: Socket closed\r\n", "@byronyi root caused this issue. It's caused by http_proxy settings. Thank you all for your support. \r\n", "@threeleafzerg Nevermind, and thanks for reporting your issue.", "@byronyi BTW, do you have any public design doc about distribute strategy or public future plan about distribute strategy which can be shared with us? Thanks! ", "I\u2019ll suggest you to take a look at https://github.com/tensorflow/community/blob/master/rfcs/20181016-replicator.md and https://github.com/tensorflow/community/pull/55. ", "@byronyi  Thanks!", "@byronyi Sorry for bothering you again. Do you know any open material about HorovodDistributionStrategy? \r\n", "Sorry I have no knowledge of that.", "How to work around this error , I am using TPU Distribute strategy?\r\n \r\nFailed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:worker/replica:0/task:0/device:CPU:0 in order to run ExperimentalAutoShardDataset: Unable to parse tensor proto\r\nAdditional GRPC error information:\r\n{\"created\":\"@1564365854.431149350\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Unable to parse tensor proto\",\"grpc_status\":3} [Op:ExperimentalAutoShardDataset] "]}, {"number": 24886, "title": "tf-nightly-2.0-preview failed to install on windows1809", "body": "**System information**\r\n- OS Platform and Distribution:Windows 10 1809 x64\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: tf-nightly-2.0-preview\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: use pip\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: cuda 9.0 \r\n- GPU model and memory: gtx 1070 \r\n\r\n\r\n\r\n**Describe the problem**\r\nWhen I try to install tf-nightly-2.0-preview on windows 10(1809) using follow command.\r\n`pip install tf-nightly-2.0-preview`\r\nI meet problem about follow error information:\r\n`Could not install packages due to an EnvironmentError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\anqin\\\\AppData\\\\Local\\\\Temp\\\\pip-install-hhl6pefq\\\\tf-nightly-2.0-preview\\\\tf_nightly_2.0_preview-1.13.0.dev20190113.data/purelib/tensorflow/include/tensorflow/include/external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorSyclConvertToDeviceExpression.h'`\r\n\r\nDoes anyone know how to solve it?\r\n", "comments": ["This is likely due to path length limits on Windows. There should be a way to disable this limit on Windows 10.\r\n\r\nyou can try following instructions here:\r\nhttps://mspoweruser.com/ntfs-260-character-windows-10/\r\n\r\nif you are home user of windows, you can try following instructions here:\r\nhttps://www.itprotoday.com/windows-10/enable-long-file-name-support-windows-10", "Thank you!  This (enabling long path names) worked for me."]}, {"number": 24885, "title": "Complete support for building C++ API on windows using Bazel", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.11.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nFeature: Correctly use Bazel to build Tensorflow C++ API on Windows.\r\n\r\nCurrent state: Now we can build Tensorflow C++ API on Windows using Bazel by `bazel build //tensorflow:libtensorflow_cc.so` (I even wrote a [script](https://github.com/guikarist/tensorflow-windows-build-script) to do this). However, there are some problems:\r\n\r\n1. [necessary symbols are not exported](https://github.com/tensorflow/tensorflow/blob/ac4d3682939dac303082097d25ebda805409acde/tensorflow/BUILD#L521) and [#23542](https://github.com/tensorflow/tensorflow/issues/23542)\r\n1. The built libraries have `.so` extensions which are not for Windows platform, although after being renamed to `.dll` the dynamic libraries can be used.\r\n1. There is no information about how to use built third-party `.a` files.\r\n\r\n**Will this change the current api? How?**\r\n\r\nYes. The API may become `//tensorflow:libtensorflow_cc.dll`.\r\n\r\n**Who will benefit with this feature?**\r\n\r\n1. Everyone who wants to build Tensorflow C++ API on Windows.\r\n1. Everyone who wants to use the latest Tensorflow C++ API on Windows (the last version of C++ API which can be built officially on Windows is 1.10.0).\r\n\r\n**Any Other info.**\r\nNo.", "comments": ["FYI @gunan @allenlavoie @asimshankar ", "@meteorcloudy for windows build guidelines.\r\nI think in another issue there was a guide we could use.\r\n@lamberta for docs updates.", "I'm working on this, will send a solution in the next two days.", "@guikarist Can you tell me how do people usually use the `tensorflow.dll` ? In Bazel build or in CMake or some other way?", "@meteorcloudy \r\n\r\n> @guikarist Can you tell me how do people usually use the `tensorflow.dll` ? In Bazel build or in CMake or some other way?\r\n\r\nTypically we use `tensorflow.dll` in our own project. For example, I add the directory of Tensorflow source files (`*.h` and `*.cc`) into **Additional Include Directories**  and then add `tensorflow.dll` into linker setting in **Visual Studio**.\r\n\r\nWe just want to use Tensorflow as part of our own project. Although there is a way to do this by building with Bazel, it is not compatible with Visual Studio (even with Windows). So dynamic/static libraries are better.\r\n\r\n**Update:**\r\nI use `tensorflow_cc.dll` rather than `tensorflow.dll` more exactly.", "@guikarist I hope https://github.com/tensorflow/tensorflow/pull/24963 could fix the dynamic linking issue.\r\nFor a static TensorFlow C library, because Bazel never has the ability to build a static library that contains all its dependencies, we cannot do it right now. But it would be fixed in future when C++ rules can be written in Starlark. (https://github.com/bazelbuild/bazel/issues/4570)", "@meteorcloudy We use a similar workflow to what @guikarist described. If there were pre-build Windows binaries that would be great. Otherwise we have no opposition to building from source, as long as there is a canonical process to do so.", "@Cory-Kramer  With https://github.com/tensorflow/tensorflow/pull/24963, you can do it just by \r\n`bazel build --config=opt //tensorflow:tensorflow.dll`\r\nWe'll also release prebuilt libraries soon.", "@meteorcloudy \r\nBrilliant for your work!\r\nSo, what about the third-party libraries? Will they still be built to `*.a` files? I have no idea how deal with these files to be honest.", "What do you mean by \"third-party libraries\"? Do you mean a static library for TensorFlow that you can link to?", "@meteorcloudy \r\nI mean just their literal meaning which is corresponding to libraries in `third_party` directory. Some of them is necessary for using Tensorflow on Windows.", "Ah, I see. `tensorflow.dll` itself doesn't depend on any third party libraries at runtime. If you want to use them in your project, I think you can just build them with Bazel and add the static library (extension is `.lib` on Windows) to your project.", "@meteorcloudy \r\nYou mean building them one by one outside Tensorflow source?", "Well, if you build the TensorFlow pip package (or tensorflow.dll) with Bazel, I'm sure they will also be built, then you can find them under `bazel-bin/third_party`", "My understanding is that `tensorflow.dll` will (statically) link in all the dependencies needed for TensorFlow, so you do not need to build individual dependencies and package them one-by-one.", "@asimshankar \r\n\r\nThis is what command `tree -L 2` outputs (using WSL) in `bazel-bin/` which is built by the former `bazel build //tensorflow:libtensorflow_cc.so`:\r\n\r\n```\r\n.\r\n\u251c\u2500\u2500 external\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 boringssl\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 com_google_absl\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 com_googlesource_code_re2\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 double_conversion\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 farmhash_archive\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 fft2d\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 gif_archive\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 grpc\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 highwayhash\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 jpeg\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 jsoncpp_git\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 lmdb\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 nsync\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 org_sqlite\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 png_archive\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 protobuf_archive\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 snappy\r\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 zlib_archive\r\n\u251c\u2500\u2500 result-0.txt\r\n\u251c\u2500\u2500 result.txt\r\n\u2514\u2500\u2500 tensorflow\r\n    \u251c\u2500\u2500 c\r\n    \u251c\u2500\u2500 cc\r\n    \u251c\u2500\u2500 contrib\r\n    \u251c\u2500\u2500 core\r\n    \u251c\u2500\u2500 liblibtensorflow_cc.so.exp\r\n    \u251c\u2500\u2500 liblibtensorflow_cc.so.ifso\r\n    \u251c\u2500\u2500 libtensorflow_cc.so\r\n    \u251c\u2500\u2500 libtensorflow_cc.so-2.params\r\n    \u251c\u2500\u2500 libtensorflow_cc.so.runfiles\r\n    \u251c\u2500\u2500 libtensorflow_cc.so.runfiles_manifest\r\n    \u251c\u2500\u2500 stream_executor\r\n    \u2514\u2500\u2500 tools\r\n\r\n27 directories, 7 files\r\n\r\n```\r\n\r\nSo do you mean it is correct to directly link `libtensorflow_cc.so` (which is indeed `libtensorflow_cc.dll`)? Then I can use third-party dependencies besides Tensorflow?\r\n\r\n[What we used to do with Tensorflow](https://github.com/tensorflow/tensorflow/issues/24885#issuecomment-454801655) is:\r\n\r\n> Typically we use tensorflow.dll in our own project. For example, I add the directory of Tensorflow source files (*.h and *.cc) into Additional Include Directories and then add tensorflow.dll into linker setting in Visual Studio.\r\n>\r\n> We just want to use Tensorflow as part of our own project. Although there is a way to do this by building with Bazel, it is not compatible with Visual Studio (even with Windows). So dynamic/static libraries are better.\r\n>\r\n> Update:\r\n> I use tensorflow_cc.dll rather than tensorflow.dll more exactly.\r\n\r\nIn terms of third-party dependencies, we build them outside the Tensorflow project. And use them just like what we do with Tensorflow. Because we have no idea how to deal with `*.a` files. For example, it is `bazel-bin/external/jpeg`:\r\n\r\n```\r\njpeg\r\n\u251c\u2500\u2500 _objs\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 jpeg\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jaricom.o\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jcapimin.o\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jcapistd.o\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jcarith.o\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jccoefct.o\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jccolor.o\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jcdctmgr.o\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jchuff.o\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jcinit.o\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jcmainct.o\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jcmarker.o\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jcmaster.o\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jcomapi.o\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jcparam.o\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jcphuff.o\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jcprepct.o\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jcsample.o\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jctrans.o\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jdapimin.o\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jdapistd.o\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jdarith.o\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jdatadst.o\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jdatasrc.o\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jdcoefct.o\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jdcolor.o\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jddctmgr.o\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jdhuff.o\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jdinput.o\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jdmainct.o\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jdmarker.o\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jdmaster.o\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jdmerge.o\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jdphuff.o\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jdpostct.o\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jdsample.o\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jdtrans.o\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jerror.o\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jfdctflt.o\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jfdctfst.o\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jfdctint.o\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jidctflt.o\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jidctfst.o\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jidctint.o\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jidctred.o\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jmemmgr.o\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jmemnobs.o\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jquant1.o\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jquant2.o\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 jutils.o\r\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 simd_none\r\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 jsimd_none.o\r\n\u251c\u2500\u2500 libjpeg.a\r\n\u251c\u2500\u2500 libjpeg.a-2.params\r\n\u251c\u2500\u2500 libsimd_none.a\r\n\u2514\u2500\u2500 libsimd_none.a-2.params\r\n\r\n3 directories, 54 files\r\n\r\n```", "> Well, if you build the TensorFlow pip package (or tensorflow.dll) with Bazel, I'm sure they will also be built, then you can find them under `bazel-bin/third_party`\r\n\r\n@meteorcloudy When I build `libtensorflow_cc.so`, the third-party libraries are built, too (in `bazel-bin/external` which is same as the output directory of `//tensorflow/tools/pip_package:build_pip_package`).\r\n\r\nWhat confuses me is the extensions of these libraries. I will try [what @asimshankar said](https://github.com/tensorflow/tensorflow/issues/24885#issuecomment-454829156).", "@guikarist Oh, right, it should be `bazel-bin/external` instead of `bazel-bin/third_party`. All those dependencies are statically linked in the shared library (DLL), so you don't need them at runtime.", "@meteorcloudy Then, what about compiling? I think source codes of these dependencies are needed.", "@guikarist I think what you need are the header files of those dependencies?\r\nMaybe this could help:\r\nhttps://stackoverflow.com/questions/42898577/list-of-headers-to-use-tensorflow-c-api-using-libtensorflow-cc-so", "@meteorcloudy, will your solution provide an interface BUILD file?\r\n1. The following targets are enough for now to be built, [1](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/BUILD#L515), [2](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/BUILD#L550):\r\n  - //tensorflow:libtensorflow_cc.so\r\n  - //tensorflow:install_headers;\r\n2. But there's no standalone wrapping interface library neither for Bazel, nor for CMake. An example for CMake [3](https://github.com/Microsoft/vcpkg/pull/5222/files#diff-b1465f91cff230939224b1de329368a0R1), some simple example from bazel documentation [4](https://docs.bazel.build/versions/master/cpp-use-cases.html#adding-dependencies-on-precompiled-libraries);\r\n3. vcpkg port proposal for tensorflow-cc [5](https://github.com/Microsoft/vcpkg/pull/5222/files#diff-eab36df6075256c6494aa137da1a9c1cR34);\r\n4. Do you need some assistance in testing?", "@nartes That sounds a good idea. But I'm not very familiar which headers should be included for the shared library. \r\n@gunan Do you know anyone could help on this?", "rename the .so to .dll, it does not work for me", "@MIKOCHAO Try [my script](https://github.com/guikarist/tensorflow-windows-build-script), bro.", "@guikarist I successfully ran the following commands:\r\nbazel build //tensorflow:libtensorflow_cc.so\r\nbazel build --config=opt //tensorflow:tensorflow.dll\r\n\r\nNow how do It in my own Visual Studio project. Which directories should I add into Additional Include Directories? There are a lot of directories with .h files (like some directories in C:\\tensorflow\\bazel-tensorflow\\external\\ and C:\\tensorflow\\bazel-tensorflow\\tensorflow\\)\r\n\r\nAnd will I need to copy the tensorflow.dll or tensorflow:libtensorflow_cc.so (renamed to tensorflow:libtensorflow_cc.dll) file to the visual studio project directory?\r\n\r\nThank you\r\n", "@rounakskm First of all, `//tensorflow:tensorflow.dll` target is for C API. And this is a [wiki about how to use the built results](https://github.com/guikarist/tensorflow-windows-build-script/wiki/Using-the-built-results), where I have published a script (a gist). Then you should add the sources to `Additional Include Directories` and `tensorflow_cc.lib` to `linker` (I don't remember the exact name). Don't forget to add sources of the third party in `$sourceDir\\bazel-source\\external\\`.", "How do I get the required symbols to avoid linker errors. \r\nI am getting an unresolved external symbol error:\r\n\r\nerror LNK2001: unresolved external symbol \"class tensorflow::Status __cdecl tensorflow::LoadSavedModel(struct tensorflow::SessionOptions const &,class tensorflow::RunOptions const &,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class std::unordered_set<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,struct std::hash<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,struct std::equal_to<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,class std::allocator<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > > > const &,struct tensorflow::SavedModelBundle * const)\" (?LoadSavedModel@tensorflow@@YA?AVStatus@1@AEBUSessionOptions@1@AEBVRunOptions@1@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@AEBV?$unordered_set@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@U?$hash@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@U?$equal_to@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@V?$allocator@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@@6@QEAUSavedModelBundle@1@@Z)", "Hello,\r\n\r\nI think it has to do with the fact that I am using TF 2.0 and the modifications in the BUILD file are different.\r\nI added the missing symbols to the tf_exported_symbols_mscv.lds and ran the build again but that does not solve my linking errors. I get the same symbols as missing.\r\n\r\nSome background on how I ran the build->\r\nFor TF 2.0 BUILD file I cannot use the git -diff file with the changes mentioned in (#22047 (comment)).\r\nSo I made the required changes manually (line 538 and 661 in this case), added the tf_exported_symbols.lds with my missing symbols and ran the bazel build //tensorflow:tensorflow_cc.\r\n\r\nLine 558 of the tensorflow/BUILD file in r2.0 branch has the following:\r\nadd win_def_file for tensorflow_cc\r\nwin_def_file = select({\r\nWe need this DEF file to properly export symbols on Windows\r\n\"//tensorflow:windows\": \":tensorflow_filtered_def_file\",\r\n\"//conditions:default\": None,\r\n}),\r\n\r\nBut I cant figure out the syntax of adding the tf_exported_symbols_mscv.lds\r\nPlease help", "@rounakskm No any other similar code blocks? If so, this trick may not work on TF 2.0.", "No the other part of tf_cc_shared object is the same.\r\nWith minor changes. Like name =libtensorflow_cc has been changed to name = tensorflow_cc. \r\n\r\nBut the # add win_def_file for tensorflow_cc comment and section below is new. ", "I'm a bit confused: I'm trying to build tensorflow for C++ too but should I follow @guikarist 's or @meteorcloudy 's way?", "You can use  @guikarist 's  script. It will be easier. and Depending on the version check the tensorflow/BUILD file in the tensorflow repo directory and patch it", "@rounakskm thanks!!!", "I tried executing //tensorflow:libtensorflow_cc.so via @guikarist 's script and I guess it worked\r\nbut the PS-script that extracts the tensorflow library is looking for liblibtensorflow_cc.so.ifso which I dont have\r\nIts also looking for header-files in /core but I dont have any\r\nI have following files and directories:\r\n/c\r\n/cc\r\n/contrib\r\n/core\r\n/libtensorflow_cc.so.runfiles\r\n/stream_executor\r\n/tools\r\n-libtensorflow_cc.so\r\n-libtensorflow_cc.so.if.exp\r\n-libtensorflow_cc.so.if.lib\r\n-libtensorflow_cc.so.runfiles_manifest\r\n-libtensorflow_cc.so-2.params\r\n\r\nwhy am I missing the files?\r\ndid I miss a step?", "Which version did you used?\r\n@MPROXX", "@guikarist I chose Tensorflow v1.13.1\r\nsry for the late reply, it was a busy week", "@rounakskm what do you mean by \"patching\" the tensorflow/BUILD file?\r\nEdit: found out, what you meant x) but that doesnt solve my current problem", "> @guikarist I chose Tensorflow v1.13.1\r\n> sry for the late reply, it was a busy week\r\n\r\nI am sorry that I didn't test the built results of `v1.13.1` of which I just tested building. Maybe renaming `libtensorflow_cc.so.if.lib` to `xxx.lib` works.", "> \r\n> \r\n> > @guikarist I chose Tensorflow v1.13.1\r\n> > sry for the late reply, it was a busy week\r\n> \r\n> I am sorry that I didn't test the built results of `v1.13.1` of which I just tested building. Maybe renaming `libtensorflow_cc.so.if.lib` to `xxx.lib` works.\r\n\r\nIt's ok it's ok, thanks for your effort and your support!! =D\r\nI'll try that and come back here to report whether it was successful!", "@guikarist \r\nI renamed libtensorflow_cc.so.if.lib to tensorflow_cc.lib\r\nI've made a project in Visual Studio\r\nI added **...\\source\\bazel-source\\external** and **...\\bin\\tensorflow\\include** to Additional Include Directories\r\nI added **...\\bin\\tensorflow\\lib** to Additional Library Directories\r\n\r\nI made a quick main.cpp:\r\n`#include \"main.h\"\r\n#include <tensorflow/cc/ops/standard_ops.h>\r\n//#include <tensorflow/core/framework/graph.pb.h> missing, I dont have protobuf\r\n#include <tensorflow/core/graph/graph.h>\r\n#include <tensorflow/core/framework/tensor.h>\r\n#include <tensorflow/core/lib/core/status.h>\r\n#include <tensorflow/core/platform/env.h>\r\n//#include <tensorflow/core/protobuf/config.pb.h> missing, I have no protobuf\r\n#include <tensorflow/core/public/session.h>\r\n#include <tensorflow/core/public/session_options.h>\r\n#include <tensorflow/core/util/command_line_flags.h>\r\n\r\nmain::main()\r\n{}\r\n\r\nmain::~main()\r\n{}\r\n`\r\n\r\nI build it and it doesnt find the headers:\r\n`Error (active)\tE1696\tcannot open source file \"tensorflow/cc/ops/array_ops.h\"\t\r\nError (active)\tE1696\tcannot open source file \"tensorflow/core/framework/tensor.pb.h\"\t\r\nError (active)\tE1696\tcannot open source file \"absl/strings/str_cat.h\"\t\r\nError (active)\tE1696\tcannot open source file \"tensorflow/cc/ops/candidate_sampling_ops.h\"\t\r\nError (active)\tE1696\tcannot open source file \"tensorflow/cc/ops/control_flow_ops.h\"\t\r\nError (active)\tE1696\tcannot open source file \"tensorflow/cc/ops/data_flow_ops.h\"\t\r\nError (active)\tE1696\tcannot open source file \"tensorflow/cc/ops/image_ops.h\"\t\r\nError (active)\tE1696\tcannot open source file \"tensorflow/cc/ops/io_ops.h\"\t\r\nError (active)\tE1696\tcannot open source file \"tensorflow/cc/ops/linalg_ops.h\"\t\r\nError (active)\tE1696\tcannot open source file \"tensorflow/cc/ops/logging_ops.h\"\t\r\nError (active)\tE1696\tcannot open source file \"tensorflow/cc/ops/lookup_ops.h\"\t\r\nError (active)\tE1696\tcannot open source file \"tensorflow/cc/ops/math_ops.h\"\t\r\nError (active)\tE1696\tcannot open source file \"tensorflow/cc/ops/nn_ops.h\"\t\r\nError (active)\tE1696\tcannot open source file \"tensorflow/cc/ops/no_op.h\"\t\r\nError (active)\tE1696\tcannot open source file \"tensorflow/cc/ops/parsing_ops.h\"\t\r\nError (active)\tE1696\tcannot open source file \"tensorflow/cc/ops/random_ops.h\"\t\r\nError (active)\tE1696\tcannot open source file \"tensorflow/cc/ops/sparse_ops.h\"\t\r\nError (active)\tE1696\tcannot open source file \"tensorflow/cc/ops/state_ops.h\"\t\r\nError (active)\tE1696\tcannot open source file \"tensorflow/cc/ops/string_ops.h\"\t\r\nError (active)\tE1696\tcannot open source file \"tensorflow/cc/ops/training_ops.h\"\t\r\nError (active)\tE1696\tcannot open source file \"tensorflow/cc/ops/user_ops.h\r\n....\"\t`\r\nthe list goes further\r\n\r\nmy current thoughts:\r\nI'm not sure where and how to install protobuf during the build process of tensorflow yet\r\nAm I missing the header-files because I renamed the wrong tensorflow_cc.lib?\r\nAre my project settings for including the library wrong?", "Its interesting, regardless of which TF version I try to build (I tried 1.11.0 this time), I always get the same files as mentioned above after building", "I made a README explaining how I built the tensorflow .lib and .dll files for the Tensorflow C++ APIp on Windows. The tutorial is step by step and starts at the very beginning, so you may have to scroll down past steps you have already done, like checking your hardware, installing Bazel etc.\r\nHere is the url: https://github.com/sitting-duck/stuff/tree/master/ai/tensorflow/build_tensorflow_1.14_source_for_Windows\r\n\r\nProbably you will want to scroll all the way down to this part:\r\nhttps://github.com/sitting-duck/stuff/tree/master/ai/tensorflow/build_tensorflow_1.14_source_for_Windows#step-7-build-the-dll\r\n\r\nIt shows how to pass command to create .lib and .dll.\r\n\r\nThen to test your .lib you should link into your c++ project,\r\n\r\nThen it will show you how to identify and fix the missing symbols using the TF_EXPORT macro\r\n\r\nI am actively working on making this tutorial better so feel free to respond to my comment here or email: ashley.tharp@gmail.com with feedback or if you are confused on a particular part. :)", "@guikarist , could you send please a sample program from you build of tensorflow? I have used it to build tensorflow, have used the correct files and linkers in microsoft visual studio 2017. However, It does not seem to work since the \"tensorflow/core/framework/graph.pb.h\" file is missing on most tutorial that I find. For testing, I have been using this tutorial: https://medium.com/@mohamedtamer92/tensorflow-how-to-export-freeze-models-with-python-api-and-deploy-object-detection-models-with-a6bbb74afe1c", "> \r\n> \r\n> @Cory-Kramer With #24963, you can do it just by\r\n> `bazel build --config=opt //tensorflow:tensorflow.dll`\r\n> We'll also release prebuilt libraries soon.\r\n\r\nHas anything replaced this? When I try the bazel command above on r2.3 source, I get \"ERROR: config value opt is not defined in any .rc file\". I also do not see any evidence of prebuilt libraries :-( Has there been a 2.0+ release with prebuilt libraries for Windows? Anything released in the 1.x series?", "Further to above, in building r2.3 out of the box on Windows, it seems like one may need to change --config=opt to -c opt:\r\n    bazel build -c opt //tensorflow:tensorflow.dll\r\n\r\nOr maybe not [according to this](https://stackoverflow.com/questions/46319386/whats-the-difference-between-c-opt-and-config-opt-when-building-tensorflow-f). The first command immediately goes nowhere; the variation at least makes some progress. Which is correct as of r2.3?\r\n\r\nUPDATE: My bad. The configuration process (python ./configure.py) creates a .tf_configure.bazelrc file with the results of configuration. I looked at it via Windows Explorer... and must have then accidentally deleted it prior to trying the bazel command back on the command line :-( Since I executed this command in a cmd window, and straight after a directory listing, I could see both the .tf_configure.bazelrc and .bazelrc files there (one of which was now a ghost). So I duly went barking up the wrong tree! Re-running the configuration process confirms that the original --config=opt version is correct - compilation of .r2.3 source is under way :-)"]}, {"number": 24884, "title": "Fix warnings in feature_column.py", "body": "This fix fixes warnings in feature_column.py where `math_ops.to_int64`\r\nhas been deprecated and replaced with `math_ops.cast`.", "comments": []}, {"number": 24883, "title": "Issue/24374: tf.einsum function to compute the trace.", "body": "Proposition for solving issue connected with tf.einsum function that is not computing the trace of a tensor properly.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->"]}, {"number": 24882, "title": "Tensorflow import issue", "body": "**System information**\r\n-Windows 10\r\n-Python 3.6\r\n-TensorFlow installed using pip3\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\nI am getting Importerror as  **DLL load failed: A dynamic link library (DLL) initialization routine failed.**\r\n```\r\n# Copy and paste here\r\nimport tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Selvi\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Selvi\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Selvi\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Selvi\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Selvi\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<pyshell#0>\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\Selvi\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Selvi\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Selvi\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Selvi\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Selvi\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Selvi\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Selvi\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Selvi\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>> \r\n\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Did you Download and install the Microsoft Visual C++ 2015 Redistributable Update 3?\r\n", "No. ", "You have to install it. Please take a look at these [installation steps](https://www.tensorflow.org/install/pip) and select windows for steps 1 and 2.", "Hi thank you for your suggestions. I have installed according to\nyour instructions and followed the steps. While executing Step 2 I am\ngetting the following error message.\nC:\\Users\\Selvi>>virtualenv --system-site-packages -p python3 ./venv\n\"The executable python3 (from --python=python3) does not exist\"\n\nand/or\nC:\\Users\\Selvi>>virtualenv --system-site-packages -p python ./venv\n\"The executable python (from --python=python) does not exist\"\n\nplease help to solve this problem.\n\n\nOn Tue, Jan 15, 2019 at 11:45 PM ymodak <notifications@github.com> wrote:\n\n> You have to install it. Please take a look at these installation steps\n> <https://www.tensorflow.org/install/pip> and select windows for steps 1\n> and 2.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/24882#issuecomment-454493283>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AsfrYkp0ebuJ1ZCsFZHgMTD6RmSi1w2rks5vDhqpgaJpZM4Z9XXq>\n> .\n>\n\n\n-- \n\n*With Regards,*\n\n*Dr.M.Selvi, M.E., M.B.A., PhD(Engg),*\n*e-Mail:* drselvimunuswamy@gmail.com\n", "Step 2 is for using TF in Virtual Env, Can you try this,\r\n>virtualenv -p python venv-tf\r\n>venv-tf\\Scripts\\activate\r\n>pip3 install tensorflow\r\n\r\nAlso you can proceed to Step 3 and do a system wide TF installation, by doing this you don't have to create a virtual env every time you want to use TF.", "Hi I have tried what you said again getting same error message\n[image: image.png]\n\n*Again getting the same error*\n\n>>> import tensorflow as tf\nTraceback (most recent call last):\n  File\n\"C:\\Users\\Selvi\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\",\nline 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File\n\"C:\\Users\\Selvi\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\",\nline 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File\n\"C:\\Users\\Selvi\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\",\nline 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname,\ndescription)\n  File \"C:\\Users\\Selvi\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\",\nline 243, in load_module\n    return load_dynamic(name, filename, file)\n  File \"C:\\Users\\Selvi\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\",\nline 343, in load_dynamic\n    return _load(spec)\nImportError: DLL load failed: A dynamic link library (DLL) initialization\nroutine failed.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"<pyshell#0>\", line 1, in <module>\n    import tensorflow as tf\n  File\n\"C:\\Users\\Selvi\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\",\nline 24, in <module>\n    from tensorflow.python import pywrap_tensorflow  # pylint:\ndisable=unused-import\n  File\n\"C:\\Users\\Selvi\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\",\nline 49, in <module>\n    from tensorflow.python import pywrap_tensorflow\n  File\n\"C:\\Users\\Selvi\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\",\nline 74, in <module>\n    raise ImportError(msg)\nImportError: Traceback (most recent call last):\n  File\n\"C:\\Users\\Selvi\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\",\nline 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File\n\"C:\\Users\\Selvi\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\",\nline 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File\n\"C:\\Users\\Selvi\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\",\nline 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname,\ndescription)\n  File \"C:\\Users\\Selvi\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\",\nline 243, in load_module\n    return load_dynamic(name, filename, file)\n  File \"C:\\Users\\Selvi\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\",\nline 343, in load_dynamic\n    return _load(spec)\nImportError: DLL load failed: A dynamic link library (DLL) initialization\nroutine failed.\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.\n\nOn Thu, Jan 17, 2019 at 12:04 AM ymodak <notifications@github.com> wrote:\n\n> Step 2 is for using TF in Virtual Env, Can you try this,\n>\n> virtualenv -p python venv-tf\n> venv-tf\\Scripts\\activate\n> pip3 install tensorflow\n> Also you can proceed to Step 3 and do a system wide TF installation, by\n> doing this you don't have to create a virtual env every time you want to\n> use TF.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/24882#issuecomment-454888652>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AsfrYv1Z4VFsej3hXbuulOYsPcGUF4DQks5vD3DJgaJpZM4Z9XXq>\n> .\n>\n\n\n-- \n\n*With Regards,*\n\n*Dr.M.Selvi, M.E., M.B.A., PhD(Engg),*\n*e-Mail:* drselvimunuswamy@gmail.com\n", "Please refer this [link](https://kb.froglogic.com/display/KB/Article+-+Errors+with+%28third+party%29+Python+modules) which helps you to resolve the issue. Other than that, I suspect your CPU doesnot support AVX instructions set. Please refer following issues to know more,\r\n#17386 , #19584", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 24881, "title": "Support per channel quantized ops", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):1.12\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThe current quantization Ops -  [tf.quantization](https://www.tensorflow.org/api_docs/python/tf/quantization), [tf.nn.quantized_conv2d](https://www.tensorflow.org/api_docs/python/tf/quantization) support only per layer quantization. For per channel quantization the `min` and `max` values should be of dimension `[d]`, one for each channel instead of being a single `float` value now.\r\n\r\n**Will this change the current api? How?**\r\nYes. The ops will take `[d]` dimensional `min` and `max` values, one per channel. And the `output_min` and `output_max` would also be  `[d]` dimensional .\r\n**Who will benefit with this feature?**\r\nAnyone who wants to use per channel quantization.\r\n**Any Other info.**\r\n", "comments": ["Do we have per channel quantization support in the tflite converter ?", "@shoubhik,\r\nPlease refer the documentation of [Quantization Spec](https://www.tensorflow.org/lite/performance/quantization_spec) and [Post Training Quantization](https://www.tensorflow.org/lite/performance/post_training_quantization#representation_for_quantized_tensors) which has a mention about Support for **`per Channel Quantized Ops`**. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 24880, "title": "Cherrypick disk space debugging changes.", "body": "", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n<!-- need_author_consent -->"]}, {"number": 24879, "title": "how to control the dimension of operation tf.spectral.fft2() ", "body": "Hi TF, I am new here and wondering that\r\n\r\nfor a tf.tensor (placeholder) normally defined as shape=[batch_size, image_height, image_width, channel_number], along which dimensions would fft2 operate on?\r\n\r\nwhat if I just want to do 2d fft in image domain, i.e. the middle two dimensions of the above fourth-order tensor?\r\n\r\nhuge thanks", "comments": ["Numerically it seems that fft2 acquiescently operates on the first two dimensions of a predefined tensor", "@YunyanYao Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support. Github is mainly for addressing bugs in installation and performance. Responses to your questions will help others also if you post them on Stackoverflow. Thanks!", "Please post support related questions in StackOverflow. We encourage users to submit an issue/feature request here. Closing the issue. Thanks!"]}, {"number": 24878, "title": "Replace deprecated Cuda driver API", "body": "Replaced deprecated cuda driver API calls with recommended ones. Note these are deprecated since CUDA 5.0 (2013) so should be safe to remove.\r\n\r\n- [x] [` cuDeviceComputeCapability`](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__DEVICE__DEPRECATED.html#group__CUDA__DEVICE__DEPRECATED_1gdc50ce6a6e0a593158d4ccb3567e0545)\r\n- [x] [`cuDeviceGetProperties `](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__DEVICE__DEPRECATED.html#group__CUDA__DEVICE__DEPRECATED_1ged20a6d946d0217b3b1e0a40df6a43a6)", "comments": []}, {"number": 24877, "title": "TFLite Upsample Nearest Neighbors", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macos\r\n- TensorFlow installed from (source or binary): 1.12 /  tf-nightly-1.13.0.dev20190112\r\n- TensorFlow version (or github SHA if from source):\r\n\r\ncommand: tflite_convert --keras_model_file=model.h5 --output_file=model.tflite --input_array=input_1 --inference_type=FLOAT --input_shape=1,512,512,3\r\n\r\nI'm trying to convert a keras model that has an upsample nearest neighbors op. In tf 1.12 this op is not supported, and I saw it was added later on but not yet in the official release. When I used tf-nightly I get an error:\r\n\r\nerror:\r\n2019-01-13 09:12:33.214920: I tensorflow/core/grappler/devices.cc:53] Number of eligible GPUs (core count >= 8): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2019-01-13 09:12:33.215726: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session\r\n2019-01-13 09:12:33.275871: E tensorflow/core/grappler/grappler_item_builder.cc:636] Init node initial_conv/kernel/Assign doesn't exist in graph\r\n\r\nIs there some commit I can use to convert my model?\r\n", "comments": ["I am running into the same issue. The tflite file is created but instantiating the interpreter with the loaded model throws an error.", "any updates?", "The problem doesn't reproduce with TensorFlow 1.13.0-rc0. As far as I care, this issue can be closed.", "@Kolefn Could you also try with TF1.13 and let us know whether it resolved the issue or not? Thanks!"]}, {"number": 24876, "title": "This adds TFLite build script for generic aarch64 boards", "body": "The script is modeled after the existing Raspberry Pi script, the documentation is modeled likewise. This was tested on Odroid C2 by running the compiled `benchmark_model` tool on the official MobileNet v1 1.0 model from [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models.md).", "comments": ["@petewarden  Could you PTAL and approve"]}, {"number": 24875, "title": "How to define a variable with different value for each tower under MirroredStrategy", "body": "I tried to train model on multi-gpus with tf estimator and MirroredStrategy. And I found that variables of the model are wrapped as MirroredVariables which are always in sync among towers.\r\n\r\nHowever, there is a variable that needs to have different value for each tower on every step of training, How to define such variable ? that is, a variable with locality T, refers to (https://www.tensorflow.org/api_docs/python/tf/contrib/distribute/DistributionStrategy)", "comments": ["@CtfGo Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support. Github is mainly for addressing bugs in installation and performance. Responses to your questions will help others also if you post them on Stackoverflow. Thanks!", "I have the same question or feature request. The question posted at [Stackoverflow](https://stackoverflow.com/questions/54166123/how-to-define-a-variable-with-different-value-for-each-tower-under-mirroredstrat) doesn't have any response. @jvishnuvardhan ", "@CtfGo Please check if there is any solution [here](https://github.com/tensorflow/tensorflow/issues?utf8=%E2%9C%93&q=is%3Aissue+is%3Aclosed+MirroredStrategy). If you don't find any solution, then please fill the details in the [template](https://github.com/tensorflow/tensorflow/issues/new?template=00-bug-performance-issue.md) to find root cause of the issue. Thanks!", "@CtfGo @x10000year  can you describe the use case, why you need variables which are not synced across towers? Also would you want these variables to be synced at all, or never synced? ", "@guptapriya \r\n\r\nOne of my use cases is to implement truncated back propagation for a unrolled rnn. In this case, for every training step and every tower we read only n steps from a sequence, and compute n steps of the unrolled rnn. The computation starts with the last rnn states from the same sequence, and at the end stores the new rnn states into local variables.  The per tower local/temporary variables are never synced. For each tower we have a custom data input op that guarantees every data sequence is processed by the same tower.", "You could try to create variables with `VariableSynchronization.ON_READ`.  If that doesn't work, you could look into creating a new distribution strategy which implements your logic of syncing them (or in your case not syncing them at all?).  Please re-open if those methods don't help.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=24875\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=24875\">No</a>\n"]}, {"number": 24874, "title": "Eager mode / AutoGraph control dependencies", "body": "In the current main examples of `autograph` which uses eager mode when calling `opt.minimize` there is this comment:\r\n```\r\n  # Autograph automatically adds the necessary `tf.control_dependencies` here.\r\n  # (Without them nothing depends on `opt.minimize`, so it doesn't run.)\r\n  # This makes it much more like eager-code.\r\n```\r\nSo my question is could you clarify how this magic is happening behind the scenes (e.g. what are the conditions for which autograph creates these dependencies and if one wants to somehow force in eager mode some form of dependencies, how to achieve it. For instance consider the following \"python\" code:\r\n```\r\ndef some_fn(x, m, t):\r\n   if t % 100 == 0:\r\n       m = m * 0.9 + x ** 2 * 0.1\r\n   return x / tf.sqrt(m)\r\n```\r\nHere we want the final value of` to always be either the input or the updated value dependable on `t`. How can we force this control dependency? ", "comments": ["Note that TF 2.0 includes an automatic control dependencies mechanism that is better than autograph's explicit `tf.control_dependencies`. So if at all possible we recommend using `tf.function` (`tf.function` also uses autograph) which ensures the stateful ops in the graph are sequenced in the order in which they were created (that is, program order). We'll update autograph to use this new system soon as well.\r\n\r\nI'll describe the old mechanism below, for completeness. Then at the end I'll describe what happens in the case of the `if` statement, which doesn't need control dependencies at all.\r\n\r\n**The old mechanism for control dependencies:**\r\n\r\nIn the case of `opt.minimize`, we used `tf.control_dependencies`. There are a few heuristics, the most important being that a function whose value is not used will be sequenced before all the statement that follow it. For example:\r\n\r\n```\r\nopt.minimize(...)\r\n... other computations ...\r\n```\r\n\r\nis transformed into:\r\n\r\n```\r\nwith tf.control_dependencies([opt.minimize(...)]):\r\n  ... other computations ...\r\n```\r\n\r\n**Data dependencies due to if statements**\r\n\r\nIn the case of the example you listed:\r\n```\r\n   if t % 100 == 0:\r\n       m = m * 0.9 + x ** 2 * 0.1\r\n   return x / tf.sqrt(m)\r\n```\r\n\r\nthe code is transformed into:\r\n\r\n```\r\n  m = tf.cond(\r\n    t % 100 == 0,\r\n    lambda: m * 0.9 + x ** 2 * 0.1,\r\n    lambda: m)\r\n  return x / tf.sqrt(m)\r\n```\r\n\r\nThe functional form of TensorFlow's conditionals hopefully makes it more obvious that the modified variable has an explicit data dependency to the conditional that precedes it. So `m` will always have the correct value.\r\n\r\nLet me know if anything else is unclear.", "Perhaps the document is also useful for understanding:\r\nhttps://github.com/tensorflow/community/pull/20/files#diff-d5334516fd698a57cc5641aed4068e7fR166"]}]