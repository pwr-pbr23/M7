[{"number": 45699, "title": "Refactor reference implementation from lite/kernels/internal/reference/reference_ops.h into own headers", "body": "PR2 from #45693", "comments": ["@stephanboner Can you please resolve conflicts? Thanks!", "@gbaned done!", "This PR is being review internally due to a test failure. I am waiting for an approval before merging it.", "This one is merged in https://github.com/tensorflow/tensorflow/commit/3654281b029c3186bb2168ca20a4e1558f0620eb"]}, {"number": 45698, "title": "Failed to run Benchmark ", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS7.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): Latest Master 203f9256a596170151e4ddc93aeda6ed2905e8e7\r\n- Python version:\r\n- Bazel version (if compiling from source): 3.1\r\n- GCC/Compiler version (if compiling from source): 10.2\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nThere is no output of benchmark\r\n```\r\nbazel --output_user_root=$build_dir run --copt=-O3 //tensorflow/core/kernels/image:non_max_suppression_op_benchmark_test -- --benchmarks=../\r\n```\r\n\r\n**Describe the expected behavior**\r\nIt should output the performance data of unit test.\r\n\r\nIsolate to this Commit: https://github.com/tensorflow/tensorflow/commit/29bb0deb26db7179eefc87e750fd8755b2c9def3 which enables the new benchmark feature.\r\n\r\n", "comments": ["Is there any document about this new benchmark? Looking into the source code, it confuse me. Since the https://github.com/tensorflow/tensorflow/blob/c70e89e48a87fb76698beefcd9d7588b6f53c97f/tensorflow/core/platform/default/test_benchmark.cc#L55-L62 will set no args. But the benchmark do needs args to trigger the testing https://github.com/tensorflow/tensorflow/blob/c70e89e48a87fb76698beefcd9d7588b6f53c97f/tensorflow/core/platform/default/test_benchmark.cc#L159-L228.\r\nI think that's why there is no output of the benchmark results.", "https://github.com/tensorflow/tensorflow/commit/bba12d0401800fbf873ea35f34517a8c47a54272 should have fixed the issue ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45698\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45698\">No</a>\n"]}, {"number": 45697, "title": "[ROCm] Updating Stream Executor Blas interface (for ROCm) to call rocblas APIs\u2026", "body": "\u2026 that have been added in recent ROCm versions.\r\n\r\nThe ROCm implementation for the SE BLAS interface had a lot of the routines stubbed out because the underlying rocblas APIs were not yet implemented (at the time). Recent rocblas versions have been adding support for those BLAS APIs and this commit hooks them up.\r\n\r\n\r\n------------------------------------------------\r\n\r\n\r\n/cc @cheshire @chsigg @nvining-work @hawkinsp @inailuig ", "comments": []}, {"number": 45696, "title": "Refactor flatbuffer_conversions.cc/h for BATCH_TO_SPACE_ND and SPACE_TO_BATCH_ND", "body": "PR1 in #45693", "comments": ["@thaink is the reason for the failing oneDNN test as well the missing ruy include? Thanks!", "@stephanboner Can you please resolve conflicts? Thanks!", "@gbaned done!"]}, {"number": 45695, "title": "micro: Port TRANSPOSE from lite to micro", "body": "@tensorflow/micro\r\n\r\nThis issue tracks my work porting operator TRANSPOSE from lite to micro. @advaitjain \r\n\r\nIt will be delivered in a series of PRs.\r\n\r\nPR 1 (merged): Refactor flatbuffer_conversions #45439 \r\nPR 2 (merged): Refactor transpose reference op #45438 \r\nPR 3 (merged): Copy of the reference kernel from lite to micro without changes #45843 \r\nPR 4: Modify the micro kernel, port the tests and add the kernel to the micro build (as three separate commits) #47446\r\n\r\n", "comments": ["@driedler Sorry for a delayed response. I missed your comment. Actually this issue relates to porting the TRANSPOSE op, the files you liked are for the TRANSPOSE_CONV op. It was ported here https://github.com/tensorflow/tensorflow/commit/d9841dfd9689f9c4e0bc4e1229dbc354f01ebc1b\r\n\r\nIs it the transpose or transpose_conv you are interested in? :)", "@patriklaurell Yes. My apologies, TRANSPOSE_CONV has the issue. Please disregard.", "Hi @patriklaurell,\r\n\r\nWhich is the status of your PR5? I would need to consume TRANSPOSE from TFLM as well and I would prefer not to duplicate the effort, if you are already working on it :)\r\n\r\nThank you.", "I created a PR [48192](https://github.com/tensorflow/tensorflow/pull/48192) that should solve this issue.", "@dmpiergiacomo sorry for a late response. I have been on vacation over the easter week. I don't know if it is still relevant but I have the code for PR5 ready locally. I have not uploaded it since it depends on the changes in PR4 #47446. ", "With the merge of #47446 this issue is fixed.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45695\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45695\">No</a>\n"]}, {"number": 45694, "title": "Fix for TensorFlow Lite to parse tensor shape signature safely on big endian machines", "body": "Fix issue #45210.\r\nPassing an int vector whose value has been corrected for endianness instead of passing the raw data pointer from the `shape_signature` field of a flatbuffer model. Also, check for emptiness just in case the vector is empty but the `data()` pointer points to some random places in the memory. This fix guarantees that the `dims_signature` field of tensors will be set correctly on big endian machines when a TF Lite model is interpreted.", "comments": ["Could you please take a look at the PR @terryheo? Thank you."]}, {"number": 45693, "title": "micro: port ops BATCH_TO_SPACE_ND and SPACE_TO_BATCH_ND from lite ", "body": "@tensorflow/micro\r\n\r\nDisclaimer: This is my first contribution to a bigger open source project, so please let me know if I'm doing something wrong or if I forget something - I highly appreciate your feedback.\r\n\r\nIn my project I use causal convolution, implemented with the Keras Layer Conv1D. After the conversion to the TFLite model, the convolution will be performed by the Conv2D op. This requires the ops BATCH_TO_SPACE_ND and SPACE_TO_BATCH_ND and since causal convolution has a huge field of use, I think that it makes sense to add these two ops to tflite micro as well. Since these ops are inverses of each other and (at least sometimes) used together, I thought it is appropriate to create only one issue to add both ops.\r\n\r\nI already got a model running on my mcu using these ops and now I would like to share this with the other tensorflow users. I am not sure yet about all the required steps and which Pull Requests I need to make but I am sure that I will figure this out and I will link the pull requests to this issue and document it properly.\r\n\r\nSteps\r\n* PR 1: refactor flatbuffer_conversions parsing function #45696 \r\n* PR 2: refactor reference implementation from lite/kernels/internal/reference/reference_ops.h into its own header without making any changes. #45699\r\n* PR 3: copy the reference kernel from lite to micro and adjust it to micro by removing optimized ops. Add the File to the build. #45704\r\n* PR 4 (by @njeffrie): Port batch_to_space from TFLite to micro for int8 and float #46681\r\n* PR 5 (by @njeffrie): Port space_to_batch from TFLite to micro for int8 and float #46714\r\n* PR 6: Bugfix in batch_to_space_nd.cc #47304\r\n", "comments": ["Hi Stephan,\r\n\r\nWe are getting some quite high-priority requests from internal teams to port basic versions of these operators. In order to speed things along, I have created a TFLM float and int8 implementation for batch_to_space along with relevant tests in [this PR](https://github.com/tensorflow/tensorflow/pull/46681/files). I hope this can be a good starting point for fully featured batch_to_space and for space_to_batch, and help serve as an example for porting TFLM tests.", "> Hi Stephan,\r\n> \r\n> We are getting some quite high-priority requests from internal teams to port basic versions of these operators. In order to speed things along, I have created a TFLM float and int8 implementation for batch_to_space along with relevant tests in [this PR](https://github.com/tensorflow/tensorflow/pull/46681/files). I hope this can be a good starting point for fully featured batch_to_space and for space_to_batch, and help serve as an example for porting TFLM tests.\r\n\r\nHi Nat,\r\n\r\nThanks for your PR! Looks really good to me. I think I could continue working on this next week, this week I won't have any time. If it's so time critical, feel free to do it, if not, I will do it next week :)", "Sounds good. I think with the time pressure, I'll upload a similar version for space_to_batch_nd so that our internal teams can go ahead with their work. Hopefully these versions can serve as a starting point, and if you have additional features or especially additional tests we can work together to land those.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45693\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45693\">No</a>\n"]}, {"number": 45692, "title": "Unable to compile the TFLite for Microcontrollers C++ Library on Raspberry Pi", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Raspbian GNU/Linux 10 (buster) Linux 5.4.72-v7+  armv7l\r\n- TensorFlow installed from (source or binary): source\r\n- Tensorflow version (commit SHA if source): 2.4.0 (582c8d236cb079023657287c318ff26adb239002)\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):  Raspberry Pi 3 Model B Rev 1.2\r\n\r\n**Describe the problem**\r\nUnable to compile the TFLite for Microcontrollers C++ Library\r\nI have tested from 2.4.0 rc0 to current 2.4.0 release and got the same error\r\n\r\n`tensorflow/lite/micro/tools/make/Makefile:418: warning: overriding recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_int8'\r\ntensorflow/lite/micro/tools/make/Makefile:418: warning: ignoring old recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_int8'\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"https://github.com/google/gemmlowp/archive/719139ce755a0f31cbf1c37f7f98adcc7fc9f425.zip\" \"7e8191b24853d75de2af87622ad293ba\" tensorflow/lite/micro/tools/make/downloads/gemmlowp\r\ndownloading https://github.com/google/gemmlowp/archive/719139ce755a0f31cbf1c37f7f98adcc7fc9f425.zip\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"http://mirror.tensorflow.org/github.com/google/flatbuffers/archive/dca12522a9f9e37f126ab925fd385c807ab4f84e.tar.gz\" \"dfa0ac3073b78ddacdcacf8ca189be91\" tensorflow/lite/micro/tools/make/downloads/flatbuffers\r\ndownloading http://mirror.tensorflow.org/github.com/google/flatbuffers/archive/dca12522a9f9e37f126ab925fd385c807ab4f84e.tar.gz\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"https://github.com/google/ruy/archive/5bb02fbf90824c2eb6cd7418f766c593106a332b.zip\" \"c720b1743360259ac45809a321f8f26c\" tensorflow/lite/micro/tools/make/downloads/ruy\r\ndownloading https://github.com/google/ruy/archive/5bb02fbf90824c2eb6cd7418f766c593106a332b.zip\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_micro_person_data_grayscale_2020_05_27.zip\" \"55b85f76e2995153e660391d4a209ef1\" tensorflow/lite/micro/tools/make/downloads/person_model_grayscale\r\ndownloading https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_micro_person_data_grayscale_2020_05_27.zip\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_micro_person_data_int8_grayscale_2020_06_23.zip\" \"9b5b6d4677dd0a91b1bb992d1c4c0417\" tensorflow/lite/micro/tools/make/downloads/person_model_int8\r\ndownloading https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_micro_person_data_int8_grayscale_2020_06_23.zip\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"http://mirror.tensorflow.org/developer.arm.com/-/media/Files/downloads/gnu-rm/7-2018q2/gcc-arm-none-eabi-7-2018-q2-update-linux.tar.bz2\" \"299ebd3f1c2c90930d28ab82e5d8d6c0\" tensorflow/lite/micro/tools/make/downloads/gcc_embedded\r\ndownloading http://mirror.tensorflow.org/developer.arm.com/-/media/Files/downloads/gnu-rm/7-2018q2/gcc-arm-none-eabi-7-2018-q2-update-linux.tar.bz2\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"https://storage.googleapis.com/download.tensorflow.org/models/tflite/cifar_image_recognition_model_2020_05_27.zip\" \"1f4607b05ac45b8a6146fb883dbc2d7b\" tensorflow/lite/micro/tools/make/downloads/image_recognition_model\r\ndownloading https://storage.googleapis.com/download.tensorflow.org/models/tflite/cifar_image_recognition_model_2020_05_27.zip\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"http://mirror.tensorflow.org/www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\" \"c32a1d4ab5d03f1284b67883e8d87530\" tensorflow/lite/micro/tools/make/downloads/cifar10 patch_cifar10_dataset\r\ndownloading http://mirror.tensorflow.org/www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"http://mirror.tensorflow.org/github.com/mborgerding/kissfft/archive/v130.zip\" \"438ba1fef5783cc5f5f201395cc477ca\" tensorflow/lite/micro/tools/make/downloads/kissfft patch_kissfft\r\ndownloading http://mirror.tensorflow.org/github.com/mborgerding/kissfft/archive/v130.zip\r\nFinished patching kissfft\r\narm-none-eabi-g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -DTF_LITE_DISABLE_X86_NEON -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -mcpu=cortex-m7 -DTF_LITE_MCU_DEBUG_LOG -mthumb -mfloat-abi=soft -funsigned-char -mlittle-endian -Wno-type-limits -Wno-unused-private-field -fomit-frame-pointer -MD -DCPU_M7=1  -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/micro_error_reporter.cc -o tensorflow/lite/micro/tools/make/gen/cortex_m_generic_cortex-m7/obj/tensorflow/lite/micro/micro_error_reporter.o\r\ntensorflow/lite/micro/tools/make/downloads/gcc_embedded/bin//arm-none-eabi-g++: 1: tensorflow/lite/micro/tools/make/downloads/gcc_embedded/bin//arm-none-eabi-g++:ELF: not found\r\ntensorflow/lite/micro/tools/make/downloads/gcc_embedded/bin//arm-none-eabi-g++: 1: tensorflow/lite/micro/tools/make/downloads/gcc_embedded/bin//arm-none-eabi-g++: Syntax error: Unterminated quoted string\r\nmake: *** [tensorflow/lite/micro/tools/make/Makefile:430: tensorflow/lite/micro/tools/make/gen/cortex_m_generic_cortex-m7/obj/tensorflow/lite/micro/micro_error_reporter.o] Error 2\r\n`\r\n\r\nI was able to compile to C++ library on 2.2.0 and 2.3.1 and run a small sample with 2.3.1 on the Raspberry Pi 3 Model B\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=cortex_m_generic TARGET_ARCH=cortex-m7 microlite\r\n", "comments": ["------------------------\r\n\r\n### System information\r\n\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Raspbian GNU/Linux 10 (buster)\r\n-   **Code ran from** : Raspberry Pi 3 B\r\n\r\n### Describe the problem\r\nI haven't installed tensorrflow. I was just runing a [Tutorial of sparkfun edge](https://codelabs.developers.google.com/codelabs/sparkfun-tensorflow). So maybe it is irrelevant but my case also produces same error message.\r\nI copied the tensorflow repository and ran the commands in that tutorial. And on the **Build the binary** section (in page 4), the command got an error with the same message.\r\n\r\nThe command with error:\r\n`make -f tensorflow/lite/micro/tools/make/Makefile TARGET=sparkfun_edge micro_speech_bin`\r\n\r\nThe error message:\r\n\r\n> tensorflow/lite/micro/tools/make/downloads/flatbuffers already exists, skipping the download.\r\ntensorflow/lite/micro/tools/make/downloads/pigweed already exists, skipping the download.\r\ntensorflow/lite/micro/tools/make/downloads/gcc_embedded/bin/arm-none-eabi-g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -DTF_LITE_DISABLE_X86_NEON -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DCMSIS_NN -DPART_apollo3 -DAM_PACKAGE_BGA -DAM_PART_APOLLO3 -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DNDEBUG -DTF_LITE_MCU_DEBUG_LOG -D __FPU_PRESENT=1 -DARM_MATH_CM4 -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m4 -mthumb -mfpu=fpv4-sp-d16 -mfloat-abi=hard -std=gnu++11 -Wvla -Wall -Wextra -Wno-missing-field-initializers -Wno-strict-aliasing -Wno-type-limits -Wno-unused-function -Wno-unused-parameter -fno-delete-null-pointer-checks -fno-threadsafe-statics -fomit-frame-pointer -fno-use-cxa-atexit -nostdlib -ggdb -O3 -I. -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.2.0/boards_sfe/common/third_party/hm01b0 -isystemtensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -isystemtensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/DSP/Include/ -Itensorflow/lite/micro/tools/make/downloads/gcc_embedded//arm-none-eabi/ -Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.2.0/mcu/apollo3/ -Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.2.0/mcu/apollo3/regs -Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.2.0/mcu/apollo3/hal -Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.2.0/CMSIS/AmbiqMicro/Include/ -Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.2.0/boards_sfe/edge/bsp -Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.2.0/devices/ -Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.2.0/utils/  -Itensorflow/lite/micro/tools/make/downloads/cmsis -Itensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/Core/Include -Itensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/DSP/Include -Itensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include -Itensorflow/lite/micro/tools/make/downloads/kissfft -Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.2.0/boards_sfe/common/third_party/lis2dh12/ -c tensorflow/lite/micro/examples/micro_speech/main.cc -o tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/obj/tensorflow/lite/micro/examples/micro_speech/main.o\r\ntensorflow/lite/micro/tools/make/downloads/gcc_embedded/bin/arm-none-eabi-g++: 1: tensorflow/lite/micro/tools/make/downloads/gcc_embedded/bin/arm-none-eabi-g++: ELF: not found\r\ntensorflow/lite/micro/tools/make/downloads/gcc_embedded/bin/arm-none-eabi-g++: 1: tensorflow/lite/micro/tools/make/downloads/gcc_embedded/bin/arm-none-eabi-g++: Syntax error: Unterminated quoted string\r\nmake: *** [tensorflow/lite/micro/tools/make/Makefile:618: tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/obj/tensorflow/lite/micro/examples/micro_speech/main.o] Error 2\r\n\r\n(My coment may contain an error because I'm not familiar with reporting issues. If it does, please notice me.)", "Hi @ParadoxShmaradox ! We are checking to see whether you still need help in this issue .Supported Micro controllers for Tensorflow can be found [here](https://www.tensorflow.org/lite/microcontrollers). You can refer these threads for installing TFLite in Rasberry pi. [link1](https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/blob/master/Raspberry_Pi_Guide.md),[link2](https://blog.paperspace.com/tensorflow-lite-raspberry-pi/).Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "@mohantym I no longer need help due to lack of support and communication (this issues is almost a year old) I decided not to use TFLite for my project and I'm pretty sure the links you provide were not using TFLite 2.4.0. As the issues stated 2.2 and 2.3 worked.", "Ok  @ParadoxShmaradox ! Can you check out these threads [link1 ](https://github.com/PINTO0309/Tensorflow-bin#usage) ,[link2 ](https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi)then ?  Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45692\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45692\">No</a>\n"]}, {"number": 45691, "title": "Add Layer Dense2d", "body": "Dense2d is an extended version of the Dense layer that allows building multiple dense layers in parallel or, in other words, multiple mini machines. \r\n\r\nMakes sense in multiple applications. E.g., if you want to make predictions on multiple asset prices, the exact behavior is different for each asset. Then, there might be a model outputting (batch_size, n_assets, features), and one is looking for a layer that outputs one price for each asset for this given pre-trained model. Dense2d could be used for this. \r\n\r\n\r\n```\r\n>>> assets = tf.ones((1000,136,10))\r\n>>> my_dense = Dense2d(1)\r\n>>> prices = my_dense(input_tensor)\r\n```\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45691) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "Thanks for the PR! This is something that is too niche to be added to the core API (see API design guidelines here: https://github.com/keras-team/governance/blob/master/keras_api_design_guidelines.md#carefully-weigh-whether-a-new-feature-should-be-included ). We recommend that you discuss this layer with the maintainers of the TF Addons repo instead: https://github.com/tensorflow/addons", "Thanks for your reply. Would there be an interest instead in a generalized dense layer (say e.g. DenseKernel), where you can pass the full kernel shape (not just one dimension of it)?  "]}, {"number": 45690, "title": "MeanIoU: Support y_pred passed as a logits tensor or probability tensor", "body": "Just like categorical_accuracy and sparse_categorical_accuracy, this change allow MeanIoU metric support y_pred to be passed as a logits tensor.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45690) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "Fixes https://github.com/tensorflow/tensorflow/issues/45598", "> You need to add tests for the logit case\r\n\r\nThanks for reminding, just added :)", "I've created a PR adding support for passing probabilities to `MeanIoU`, #47410. Could you take a look and let me know if you have any recommendations for the interface? I ask because I see this PR adds a `from_logits=False`, and I'm not sure if the best solution is to have both `from_logits` and `from_probability` as toggles, or if there is a better option.\r\n\r\nWhile this PR also adds support for probabilities, #47410 is different because it does not use argmax, allowing for multilabel classification, as discussed in https://github.com/tensorflow/tensorflow/issues/39173#issuecomment-655022504", "> I've created a PR adding support for passing probabilities to `MeanIoU`, #47410. Could you take a look and let me know if you have any recommendations for the interface? I ask because I see this PR adds a `from_logits=False`, and I'm not sure if the best solution is to have both `from_logits` and `from_probability` as toggles, or if there is a better option.\r\n> \r\n> While this PR also adds support for probabilities, #47410 is different because it does not use argmax, allowing for multilabel classification, as discussed in [#39173 (comment)](https://github.com/tensorflow/tensorflow/issues/39173#issuecomment-655022504)\r\n\r\nThis PR doesn't change original interface at all. It compares the shapes of `y_true` and `y_pred` to decide whether using argmax on `y_pred`.", "@motionlife Can you please address Ubuntu Sanity errors? Thanks!", "> @motionlife Can you please address Ubuntu Sanity errors? Thanks!\r\n\r\nfixed, just found that K is no longer an alias of backend, replace K with backend, now All checks have passed", "@motionlife Can you please address Ubuntu Sanity errors? Thanks!", "> ess Ubuntu Sanity errors? Thanks!\r\n\r\nThe Ubuntu Sanity error log shows one line is too long, I break it into multiple lines, hope it will pass", "@nikitamaia We could connect this to https://github.com/tensorflow/tensorflow/issues/45598 and put that issue in progress,"]}, {"number": 45689, "title": "Remove requirement that first dimensions of inputs are equal when using train_on_batch and test_on_batch", "body": "This PR reverts part of commit 56a0ce87911236765633d2a873e706ebc6401ef9 that introduced the requirement that all input tensors have equal first dimensions when using `model.train_on_batch` and `model.test_on_batch`. \r\n\r\nChecking that all inputs have the same first dimensions introduces a pointless constraint, which makes it impossible to deal with dynamic data such as graphs in Graph Neural Networks (see [here](https://github.com/danielegrattarola/spektral/blob/master/examples/graph_prediction/general_gnn.py) for an example of code that relied on `train_on_batch` that stopped working in TF 2.4).\r\n\r\nNote that this constraint is also implemented in `model.fit`. However, `train_on_batch` and `test_on_batch` are to be intended as somewhat lower-level than the full training loop, and so it makes sense to allow more flexibility in their usage. \r\n\r\nBy introducing this additional constraint, users can only use the Keras API if their data conforms to the classic `[batch, ...]` format. This feels like a very limiting perspective going forward, since it imposes a strong constraint on data (a constraint which is not even required by the rest of the Keras API, since layers allow for any tensor manipulation).", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45689) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "Thanks for the PR! In this case, the error message serves as a useful safeguard for common use cases. All these APIs are meant to be used with inputs that have the same batch size (a general constraint throughout training APIs in Keras).\r\n\r\nIf you need to use inputs with differing batch dimensions, e.g. for GNNs, I recommend writing your own low-level training loop, which will end up being much cleaner. See this guide: https://keras.io/guides/writing_a_training_loop_from_scratch/", "Thanks for the reply, I realize I was not looking at the bigger picture here. \r\n\r\nCheers"]}, {"number": 45688, "title": "Problem with installing Tensorflow on Anaconda", "body": "**System information**\r\n- Mac OS 11.0.1 (Big Sur)\r\n- TensorFlow installed from Anaconda\r\n- TensorFlow version: 2.0\r\n- Python version: 3.8.5 and 3.7.9\r\n- Installed using Conda\r\n- GPU model and memory: Intel 3 GHz 6-Core i5 Processor \r\n\r\n\r\nHello I have a problem with anaconda and tensorflow. I tried to install tensorflow on Python 3.8.5 but then I get always an error. So I downgraded Python to 3.7.9 and then I was able to install tensorflow. But when I run a script by using tensorflow I get the warning, that my processor is not optimized for tensorflow (I'm using a intel i5 3GHz 6-core processor on my mac). I tried it with the steps in the following link, but it still don't work.\r\nhttps://towardsdatascience.com/optimize-your-cpu-for-deep-learning-424a199d7a87\r\nHow can I run Tensorflow on my computer?\r\n\r\nThank you for your help.", "comments": ["@Lysapala,\r\nInstallation issues within the Anaconda environment are tracked in the Anaconda repo.\r\n\r\nCould you please submit a new issue using [this link](https://github.com/ContinuumIO/anaconda-issues/issues) and fill in the template, so that the issue can be tracked there. Thanks!", "Okay, thanks. I thought it is a tensor flow-problem because of the processor-issue.", "Marking the issue as closed since the installation involves Anaconda. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45688\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45688\">No</a>\n"]}, {"number": 45686, "title": "ERROR: flatbuffers::flatbuffer_version_string Multiply defined Global Symbol", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution: Windows 10\r\n- TensorFlow installed from: source\r\n- Tensorflow version (commit SHA if source): latest\r\n- Target platform: STM32F407IG using Keil uVision with Compiler v6\r\n\r\n\r\n\r\n**Problem**\r\n\r\nI have been trying to compile the hello_world example generated with TFLite using Keil uVision, and compiler v6 from them. After compiling for a little while, it stops with an error and I get the following output:\r\n\r\n\r\nBuild started: Project: keil_project\r\n*** Using Compiler 'V6.12', folder: 'C:\\Keil_v5\\ARM\\ARMCLANG\\Bin'\r\nBuild target 'hello_world'\r\ncompiling common.c...\r\ncompiling error_reporter.cc...\r\ncompiling tensor_utils.cc...\r\ncompiling quantization_util.cc...\r\ncompiling op_resolver.cc...\r\ncompiling flatbuffer_conversions.cc...\r\ncompiling keyword_scrambled_model_data.cc...\r\ncompiling debug_log.cc...\r\ncompiling kernel_util.cc...\r\ncompiling constants.cc...\r\ncompiling main.cc...\r\ncompiling all_ops_resolver.cc...\r\ncompiling model.cc...\r\ncompiling output_handler.cc...\r\ncompiling activations.cc...\r\ncompiling main_functions.cc...\r\ncompiling arg_min_max.cc...\r\ncompiling add.cc...\r\ncompiling ceil.cc...\r\ncompiling circular_buffer.cc...\r\ncompiling comparisons.cc...\r\ncompiling conv.cc...\r\ncompiling concatenation.cc...\r\ncompiling depthwise_conv.cc...\r\ncompiling dequantize.cc...\r\ncompiling ethosu.cc...\r\ncompiling elementwise.cc...\r\ncompiling flexbuffers_generated_data.cc...\r\ncompiling detection_postprocess.cc...\r\ncompiling floor.cc...\r\ncompiling fully_connected.cc...\r\ncompiling kernel_runner.cc...\r\ncompiling hard_swish.cc...\r\ncompiling kernel_util.cc...\r\ncompiling logical.cc...\r\ncompiling l2norm.cc...\r\ncompiling logistic.cc...\r\ncompiling maximum_minimum.cc...\r\ncompiling neg.cc...\r\ncompiling pack.cc...\r\ncompiling mul.cc...\r\ncompiling pad.cc...\r\ncompiling pooling.cc...\r\ncompiling prelu.cc...\r\ncompiling quantize.cc...\r\ncompiling reduce.cc...\r\ncompiling resize_nearest_neighbor.cc...\r\ncompiling round.cc...\r\ncompiling reshape.cc...\r\ncompiling split.cc...\r\ncompiling softmax.cc...\r\ncompiling shape.cc...\r\ncompiling split_v.cc...\r\ncompiling strided_slice.cc...\r\ncompiling sub.cc...\r\ncompiling svdf.cc...\r\ncompiling svdf_common.cc...\r\ncompiling unpack.cc...\r\ncompiling tanh.cc...\r\ncompiling greedy_memory_planner.cc...\r\ncompiling linear_memory_planner.cc...\r\ncompiling micro_error_reporter.cc...\r\ncompiling memory_helpers.cc...\r\ncompiling micro_profiler.cc...\r\ncompiling micro_string.cc...\r\ncompiling micro_allocator.cc...\r\ncompiling micro_time.cc...\r\ncompiling micro_interpreter.cc...\r\ncompiling micro_utils.cc...\r\ncompiling recording_simple_memory_allocator.cc...\r\ncompiling recording_micro_allocator.cc...\r\ncompiling simple_memory_allocator.cc...\r\ncompiling test_conv_model.cc...\r\ncompiling retarget_io.c...\r\ncompiling test_helpers.cc...\r\ncompiling schema_utils.cc...\r\nlinking...\r\n.\\Objects\\hello_world.axf: Warning: L6439W: Multiply defined Global Symbol flatbuffers::flatbuffer_version_string defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(op_resolver.o) rejected in favor of Symbol defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(flatbuffer_conversions.o).\r\n.\\Objects\\hello_world.axf: Warning: L6439W: Multiply defined Global Symbol flatbuffers::flatbuffer_version_string defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(all_ops_resolver.o) rejected in favor of Symbol defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(flatbuffer_conversions.o).\r\n.\\Objects\\hello_world.axf: Warning: L6439W: Multiply defined Global Symbol flatbuffers::flatbuffer_version_string defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(main_functions.o) rejected in favor of Symbol defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(flatbuffer_conversions.o).\r\n.\\Objects\\hello_world.axf: Warning: L6439W: Multiply defined Global Symbol flatbuffers::flatbuffer_version_string defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(add.o) rejected in favor of Symbol defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(flatbuffer_conversions.o).\r\n.\\Objects\\hello_world.axf: Warning: L6439W: Multiply defined Global Symbol flatbuffers::flatbuffer_version_string defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(mul.o) rejected in favor of Symbol defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(flatbuffer_conversions.o).\r\n.\\Objects\\hello_world.axf: Warning: L6439W: Multiply defined Global Symbol flatbuffers::flatbuffer_version_string defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(reshape.o) rejected in favor of Symbol defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(flatbuffer_conversions.o).\r\n.\\Objects\\hello_world.axf: Warning: L6439W: Multiply defined Global Symbol flatbuffers::flatbuffer_version_string defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(shape.o) rejected in favor of Symbol defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(flatbuffer_conversions.o).\r\n.\\Objects\\hello_world.axf: Warning: L6439W: Multiply defined Global Symbol flatbuffers::flatbuffer_version_string defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(memory_helpers.o) rejected in favor of Symbol defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(flatbuffer_conversions.o).\r\n.\\Objects\\hello_world.axf: Warning: L6439W: Multiply defined Global Symbol flatbuffers::flatbuffer_version_string defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(micro_allocator.o) rejected in favor of Symbol defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(flatbuffer_conversions.o).\r\n.\\Objects\\hello_world.axf: Warning: L6439W: Multiply defined Global Symbol flatbuffers::flatbuffer_version_string defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(micro_interpreter.o) rejected in favor of Symbol defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(flatbuffer_conversions.o).\r\n.\\Objects\\hello_world.axf: Warning: L6439W: Multiply defined Global Symbol flatbuffers::flatbuffer_version_string defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(recording_micro_allocator.o) rejected in favor of Symbol defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(flatbuffer_conversions.o).\r\n.\\Objects\\hello_world.axf: Warning: L6439W: Multiply defined Global Symbol flatbuffers::flatbuffer_version_string defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(simple_memory_allocator.o) rejected in favor of Symbol defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(flatbuffer_conversions.o).\r\n.\\Objects\\hello_world.axf: Warning: L6439W: Multiply defined Global Symbol flatbuffers::flatbuffer_version_string defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(test_helpers.o) rejected in favor of Symbol defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(flatbuffer_conversions.o).\r\n.\\Objects\\hello_world.axf: Warning: L6439W: Multiply defined Global Symbol flatbuffers::flatbuffer_version_string defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(schema_utils.o) rejected in favor of Symbol defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(flatbuffer_conversions.o).\r\n.\\Objects\\hello_world.axf: Error: L6320W: Ignoring --entry command. Cannot find argument 'Reset_Handler'.\r\n.\\Objects\\hello_world.axf: Warning: L6320W: Ignoring --first command. Cannot find argument '__Vectors'.\r\nNot enough information to list image symbols.\r\nNot enough information to list load addresses in the image map.\r\nFinished: 2 information, 15 warning and 1 error messages.\r\n\".\\Objects\\hello_world.axf\" - 1 Error(s), 15 Warning(s).\r\nTarget not created.\r\nBuild Time Elapsed:  00:02:12\r\n\r\n\r\n\r\nWhat I could find out, is that flatbuffer_version_string is defined under flatbuffers.h, and initialised there too, so that every file including flatbuffers.h will define flatbuffer_version_string all over again. But I still cannot find a solution.\r\n\r\n\r\n\r\n\r\n**Steps**\r\n\r\n- Generate hello_world exmaple with MAKE, using the command \"make -f tensorflow/lite/micro/tools/make/Makefile generate_projects hello_world\"\r\n- Open keil_project.uvprojx under hello_world/keil.\r\n- Build .\r\n- Wait for build to stop with error.\r\n", "comments": ["@petewarden Pete, could you help reassign this bug to someone who's more familiar with Keil generated project?\r\n\r\nThanks,\r\nTiezhen", "Fixed commenting the following lines under flatbuffers.h:\r\n\r\n2741 to 2748\r\n\r\n//extern volatile __attribute__((weak)) const char *flatbuffer_version_string;\r\n//volatile __attribute__((weak)) const char *flatbuffer_version_string =\r\n//  \"FlatBuffers \"\r\n//  FLATBUFFERS_STRING(FLATBUFFERS_VERSION_MAJOR) \".\"\r\n//  FLATBUFFERS_STRING(FLATBUFFERS_VERSION_MINOR) \".\"\r\n//  FLATBUFFERS_STRING(FLATBUFFERS_VERSION_REVISION);\r\n\r\n//#endif  // !defined(_WIN32) && !defined(__CYGWIN__)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45686\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45686\">No</a>\n"]}, {"number": 45685, "title": "ERROR on running frozen.pb at tf2.x (google.protobuf.message.DecodeError: Error parsing message )", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution : Ubuntu 18.04 LTS\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): tensorflow2.3.1\r\n- Python version: python3.6\r\n- CUDA/cuDNN version: CUDA 10.1, cuDNN7.6\r\n- GPU model and memory: 8 GiB\r\n\r\n\r\nI have my pytorch model and converted it to onnx model.\r\nNextly, it is converted to tf2.3.1 saved_model.pb (including variables folder with data files) type model using onnx-tf and I checked it ran perfectly. :)\r\n\r\nNow, I converted it to frozen.pb type model following here: https://stackoverflow.com/questions/59657166/convert-frozen-model-pb-to-savedmodel\r\nand finally got the frozen_model.pb file.\r\nthe converting code is below:\r\n<pre><code>\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2_as_graph\r\nfrom tensorflow.lite.python.util import run_graph_optimizations, get_grappler_config\r\nimport numpy as np\r\n\r\nOD_SAVED_DIR = \"./od_tf_model\"\r\n\r\nOD_FROZEN_DIR = \"./frozen_models/od\"\r\n\r\ndef frozen_keras_graph(func_model):\r\n    frozen_func, graph_def = convert_variables_to_constants_v2_as_graph(func_model)\r\n\r\n    input_tensors = [\r\n        tensor for tensor in frozen_func.inputs\r\n        if tensor.dtype != tf.resource\r\n    ]\r\n    output_tensors = frozen_func.outputs\r\n    graph_def = run_graph_optimizations(\r\n        graph_def,\r\n        input_tensors,\r\n        output_tensors,\r\n        config=get_grappler_config([\"constfold\", \"function\"]),\r\n        graph=frozen_func.graph)\r\n\r\n    return graph_def\r\n    \r\ndef convert_saved_model_to_pb(path_to_saved_model, path_to_frozen_model):\r\n    model_dir = path_to_saved_model\r\n    model = tf.saved_model.load(model_dir)\r\n    func_model = model.signatures[\"serving_default\"]\r\n    graph_def = frozen_keras_graph(func_model)\r\n    tf.io.write_graph(graph_def, path_to_frozen_model, 'frozen_graph.pb')\r\n    \r\nconvert_saved_model_to_pb(OD_SAVED_DIR, OD_FROZEN_DIR)\r\n\r\n</code></pre>\r\n\r\nNow, I am trying to run frozen_model.pb on my tensorflow 2.3.1 but got an error.\r\n\r\n<pre><code>\r\n...\r\ngraph_def = tf.compat.v1.GraphDef()\r\nloaded = graph_def.ParseFromString(open('path/to/frozen/pb/file/frozen_model.pb','rb').read())\r\n...\r\n</code></pre>\r\n\r\nand got following error\r\n\r\n<pre><code>\r\nTraceback (most recent call last):\r\n  File \"run_frozen.py\", line 392, in <module>\r\n    loaded = graph_def.ParseFromString(open(PB_MODEL_PATH,'rb').read())\r\ngoogle.protobuf.message.DecodeError: Error parsing message\r\n</code></pre>\r\n\r\nis anyone solved this problem?\r\n\r\n\r\ni.e) running saved_model.pb file on tensorflow 2.3.1 code is below:\r\n<pre><code>\r\nmodel = tf.saved_model.load('path/to/saved/model/including/folder')\r\n    infer = model.signatures[\"serving_default\"]\r\n    outputs = infer(images)\r\n</code></pre>", "comments": ["@seongkyun,\r\nOn running the given code snippet, I am facing an error stating `OSError: SavedModel file does not exist at: ./od_tf_model/{saved_model.pbtxt|saved_model.pb}`.\r\n\r\nIn order to reproduce the issue reported here, could you please provide the complete code along with all the files required to run the code. Thanks!", "I think the tf2.x saved model to tf1.x frozen model process is done wrongly.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45685\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45685\">No</a>\n"]}, {"number": 45684, "title": "tensorflow_text.BertTokenizer not giving correct offset values in strings that have special characters", "body": "Is there a bug in tensorflow_text.BertTokenizer()?\r\n------------------------\r\n\r\nPlease let me know if I am mistaken. I will close the issue.\r\n\r\n\r\nSystem information\r\n\r\n-  Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow): Yes\r\n\r\n-   OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab (using a TPU worker through a hosted runtime)\r\n\r\n-   Exact command to reproduce: Please see the below code.\r\n\r\n\r\nDescribe the problem:\r\n\r\nI am using the SQUAD dataset for a BERT project. I needed offsets for each wordpiece token and thus decided to use the BertTokenizer class from tensorflow_text. A lot of the instances in the dataset have special characters like Greek letters, etc. In each sentence that contains such special characters, I noticed that the tokenizer is messing up the offsets of tokens that come after the special character.\r\n\r\n\r\nSource code:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_hub as hub\r\nimport tensorflow_text as text\r\n\r\n# Get the vocab file\r\ntfhub_handle_encoder = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\"\r\ntest_encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='encoder')\r\nvocab_file = test_encoder.resolved_object.vocab_file.asset_path.numpy()\r\n\r\nvocab_file = vocab_file.decode(\"utf-8\")\r\nlower_case = test_encoder.resolved_object.do_lower_case.numpy()\r\n\r\n# String has a Greek alphabet\r\nmystr = u\"The difference in the above factors for the case of \u03b8=0 is the reason that most broadcasting uses vertical polarization.\".encode('UTF-8')\r\nmystr = tf.constant(mystr)  # mystr is a unicode string tensor\r\n\r\nprint(mystr.numpy().decode(\"utf-8\"))           # \u03b8 gets printed correct\r\nprint(mystr.numpy())                                     # Prints the utf-8 encoding of \u03b8\r\nprint(tf.strings.substr(mystr, pos=80, len=12, unit='UTF8_CHAR').numpy())    # The token 'broadcasting' starts at offset 80\r\nprint(tf.strings.substr(mystr, pos=44, len=4, unit='UTF8_CHAR').numpy())      # The token 'case' starts at offset 44\r\n\r\nbert_tokenizer = text.BertTokenizer(vocab_lookup_table=vocab_file, lower_case=lower_case)\r\n(tokens, offset_starts, offset_limits) = bert_tokenizer.tokenize_with_offsets([mystr])\r\ncontext_offsets = tf.stack([offset_starts.flat_values, offset_limits.flat_values], axis=-1)\r\ncontext_offsets = context_offsets.numpy()\r\n\r\nprint(context_offsets)\r\n```\r\n\r\nOutput:\r\n\r\nThe BertTokenizer correctly detects the starting offset for 'case' to be 44 but messes up the starting offset for 'broadcasting' (it thinks the starting offset is 81). Please see the below output:\r\n\r\n```\r\nThe difference in the above factors for the case of \u03b8=0 is the reason that most broadcasting uses vertical polarization.\r\nb'The difference in the above factors for the case of \\xce\\xb8=0 is the reason that most broadcasting uses vertical polarization.'\r\nb'broadcasting'\r\nb'case'\r\n[[  0   3]\r\n [  4  14]\r\n [ 15  17]\r\n [ 18  21]\r\n [ 22  27]\r\n [ 28  35]\r\n [ 36  39]\r\n [ 40  43]\r\n **[ 44  48]**    # 'case' starts at offset 44\r\n [ 49  51]\r\n [ 52  54]          # The tokenizer reads \u03b8 as two tokens. How do we change this behavior?\r\n [ 54  55]\r\n [ 55  56]\r\n [ 57  59]\r\n [ 60  63]\r\n [ 64  70]\r\n [ 71  75]\r\n [ 76  80]\r\n **[ 81  93]**   # 'broadcasting' starts at offset 80, not 81\r\n [ 94  98]\r\n [ 99 107]\r\n [108 113]\r\n [113 120]\r\n [120 121]]\r\n```", "comments": ["@Doyel \r\n\r\nPlease, help me with colab link or reproducible code with supporting files.It helps me in localizing the issue faster.I am not able to reproduce the issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/0efaf387d7d2d4e0ecded3048dbdfd52/untitled576.ipynb).Thanks!", "@ravikyram \r\n\r\nThank you for taking up this bug. I have created an updated github gist [here](https://colab.research.google.com/gist/Doyel/002bca423a3083db48a8e447e24f32f5/untitled576.ipynb)\r\n\r\nPlease let me know if you need any additional information. Thank you again!", "@Doyel Looks like this is more related to `tensorflow-text`. So, can you please post in their repo [here](https://github.com/tensorflow/text). Thanks! \r\n\r\nOpen an issue in their repo and close it here. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45684\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45684\">No</a>\n"]}, {"number": 45683, "title": "Copy TFLite kernel exp.cc and exp_test.cc into the Micro kernel directory; PR3 to port TFL op EXP to TFLM", "body": "This is PR3 for issue #45415 (https://github.com/tensorflow/tensorflow/issues/45415): Micro: port op EXP from Lite", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "Somehow this PR is showing dependencies on my previous PRs. It should be independent of them. I am closing this PR."]}, {"number": 45682, "title": "fix `modular_filesystem` call itself", "body": "@mihaimaruseac \r\n\rClose https://github.com/tensorflow/io/issues/1222\r\n\r\ncc @yongtang ", "comments": ["@mihaimaruseac Could you take a look at this PR ? Thank you!", "Let's also cherrypick this on the r2.4 branch (if not already)"]}, {"number": 45681, "title": "AttributeError: module 'tensorflow._api.v2.sets' has no attribute 'set_intersection'", "body": "Even though I tried, I couldn't solve this mistake.\r\n\r\n**Error Log**\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-4-138183a2a830> in <module>\r\n      1 # Create model object in inference mode.\r\n----> 2 model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR, config=config)\r\n      3 \r\n      4 # Load weights trained on MS-COCO\r\n      5 model.load_weights(COCO_MODEL_PATH, by_name=True)\r\n\r\n~\\**\\Mask_RCNN\\mrcnn\\model.py in __init__(self, mode, config, model_dir)\r\n   1838         self.model_dir = model_dir\r\n   1839         self.set_log_dir()\r\n-> 1840         self.keras_model = self.build(mode=mode, config=config)\r\n   1841 \r\n   1842     def build(self, mode, config):\r\n\r\n~\\**\\Mask_RCNN\\mrcnn\\model.py in build(self, mode, config)\r\n   2044             # output is [batch, num_detections, (y1, x1, y2, x2, class_id, score)] in\r\n   2045             # normalized coordinates\r\n-> 2046             detections = DetectionLayer(config, name=\"mrcnn_detection\")(\r\n   2047                 [rpn_rois, mrcnn_class, mrcnn_bbox, input_image_meta])\r\n   2048 \r\n\r\n**\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py in __call__(self, *args, **kwargs)\r\n    949     # >> model = tf.keras.Model(inputs, outputs)\r\n    950     if _in_functional_construction_mode(self, inputs, args, kwargs, input_list):\r\n--> 951       return self._functional_construction_call(inputs, args, kwargs,\r\n    952                                                 input_list)\r\n    953 \r\n\r\n**\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py in _functional_construction_call(self, inputs, args, kwargs, input_list)\r\n   1088           layer=self, inputs=inputs, build_graph=True, training=training_value):\r\n   1089         # Check input assumptions set after layer building, e.g. input shape.\r\n-> 1090         outputs = self._keras_tensor_symbolic_call(\r\n   1091             inputs, input_masks, args, kwargs)\r\n   1092 \r\n\r\n**\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py in _keras_tensor_symbolic_call(self, inputs, input_masks, args, kwargs)\r\n    820       return nest.map_structure(keras_tensor.KerasTensor, output_signature)\r\n    821     else:\r\n--> 822       return self._infer_output_signature(inputs, args, kwargs, input_masks)\r\n    823 \r\n    824   def _infer_output_signature(self, inputs, args, kwargs, input_masks):\r\n\r\n**\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py in _infer_output_signature(self, inputs, args, kwargs, input_masks)\r\n    861           # TODO(kaftan): do we maybe_build here, or have we already done it?\r\n    862           self._maybe_build(inputs)\r\n--> 863           outputs = call_fn(inputs, *args, **kwargs)\r\n    864 \r\n    865         self._handle_activity_regularization(inputs, outputs)\r\n\r\n**\\anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py in wrapper(*args, **kwargs)\r\n    668       except Exception as e:  # pylint:disable=broad-except\r\n    669         if hasattr(e, 'ag_error_metadata'):\r\n--> 670           raise e.ag_error_metadata.to_exception(e)\r\n    671         else:\r\n    672           raise\r\n\r\nAttributeError: in user code:\r\n\r\n    **\\Mask_RCNN\\mrcnn\\model.py:810 call  *\r\n        detections_batch = utils.batch_slice(\r\n    **\\Mask_RCNN\\mrcnn\\utils.py:820 batch_slice  *\r\n        output_slice = graph_fn(*inputs_slice)\r\n    **\\Mask_RCNN\\mrcnn\\model.py:720 refine_detections_graph  *\r\n        keep = tf.sets.set_intersection(tf.expand_dims(keep, 0),\r\n\r\n    AttributeError: module 'tensorflow._api.v2.sets' has no attribute 'set_intersection'\r\n\r\n**model.py** arranged according to here. \r\n\r\nhttps://github.com/matterport/Mask_RCNN/issues/1070#issuecomment-740430758", "comments": ["@cahitberkay,\r\nIn order to reproduce the issue reported here, could you please provide the TensorFlow version, the complete code and the dataset you are using. Thanks!", "Also, please take a look at [this comment](https://github.com/tensorflow/tensorflow/issues/43982#issuecomment-713403271) from similar issue #43982 and let us know if it helps. Thanks!", "> \r\n> \r\n> @cahitberkay,\r\n> In order to reproduce the issue reported here, could you please provide the TensorFlow version, the complete code and the dataset you are using. Thanks!\r\n\r\ntensorflow==2.4.0\r\n\r\nDataset = Mask RCNN (mask_rcnn_coco.h5)\r\n\r\nFile = https://github.com/matterport/Mask_RCNN/blob/master/samples/demo.ipynb", "> \r\n> \r\n> Also, please take a look at [this comment](https://github.com/tensorflow/tensorflow/issues/43982#issuecomment-713403271) from similar issue #43982 and let us know if it helps. Thanks!\r\n\r\nI've looked at the question that was opened earlier and read the comments there, but I haven't made any progress.\r\n\r\nFor this reason, I am thinking of trying to do it through CoLab, but my first request is to run it on my own device..", "> File = https://github.com/matterport/Mask_RCNN/blob/master/samples/demo.ipynb\r\n\r\n@cahitberkay,\r\nOn running the code, I am facing an error stating `AttributeError: module 'coco' has no attribute 'CocoConfig'`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/48ae78c06cfd846f18e2b3685e6bd37f/45681.ipynb).\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide a minimal code snippet so that we can reproduce the issue on our end. Thanks!", "I solved the problem by trying. It was partly due to version errors.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45681\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45681\">No</a>\n"]}, {"number": 45680, "title": "ERROR: C:/pathtensor/tensorflow-avx2/tensorflow/python/BUILD:5771:1: C++ compilation of rule '//tensorflow/python:_tf_stack.so' failed (Exit 2): python.exe failed: error executing command", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\nBonjour monsieur et madame,\r\n\r\nI am struggling to compile tensorflow2.3 from source, and it met the problem which I can not solve, please help.\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 professional\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None\r\n- TensorFlow installed from (source or binary): tensorflow r-2.3\r\n- TensorFlow version: r-2.3\r\n- Python version: Anaconda python 3.8\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 3.1\r\n- GCC/Compiler version (if compiling from source): MSVC2019\r\n- CUDA/cuDNN version: CUDA10.1, cudnn 7.6\r\n- GPU model and memory: two Geforce GTX1080\r\n\r\n\r\n\r\n**Describe the problem** \r\n```\r\nERROR: C:/pathtensor/tensorflow-avx2/tensorflow/python/BUILD:5771:1: C++ compilation of rule '//tensorflow/python:_tf_stack.so' failed (Exit 2): python.exe failed: error executing command\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\nset PATH=%PATH%; //////\"include msvc path, bazel vc path, cuda path\"\r\nbazel clean\r\nbazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n```\r\nINFO: Reading rc options for 'build' from c:\\pathtensor\\tensorflow-avx2\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=C:/Users/rapid/anaconda3/python.exe --action_env PYTHON_LIB_PATH=C:/Users/rapid/anaconda3/lib/site-packages --python_path=C:/Users/rapid/anaconda3/python.exe --config=xla --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1 --action_env TF_CUDA_COMPUTE_CAPABILITIES=6.1 --config=cuda --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:short_logs in file c:\\pathtensor\\tensorflow-avx2\\.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file c:\\pathtensor\\tensorflow-avx2\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file c:\\pathtensor\\tensorflow-avx2\\.bazelrc: --define=with_xla_support=true\r\nINFO: Found applicable config definition build:cuda in file c:\\pathtensor\\tensorflow-avx2\\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file c:\\pathtensor\\tensorflow-avx2\\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1\r\nINFO: Found applicable config definition build:opt in file c:\\pathtensor\\tensorflow-avx2\\.tf_configure.bazelrc: --copt=/arch:AVX2 --define with_default_optimizations=true\r\nINFO: Found applicable config definition build:cuda in file c:\\pathtensor\\tensorflow-avx2\\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file c:\\pathtensor\\tensorflow-avx2\\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1\r\nINFO: Found applicable config definition build:windows in file c:\\pathtensor\\tensorflow-avx2\\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --verbose_failures --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:monolithic in file c:\\pathtensor\\tensorflow-avx2\\.bazelrc: --define framework_shared_object=false\r\nINFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (412 packages loaded, 26776 targets configured).\r\nINFO: Found 1 target...\r\nERROR: C:/pathtensor/tensorflow-avx2/tensorflow/python/BUILD:5771:1: C++ compilation of rule '//tensorflow/python:_tf_stack.so' failed (Exit 2): python.exe failed: error executing command\r\n  cd C:/users/rapid/_bazel_rapid/kk2upf35/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29333\\ATLMFC\\include;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29333\\include;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.8\\include\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\winrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\cppwinrt\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29333\\ATLMFC\\lib\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29333\\lib\\x64;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.8\\lib\\um\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.18362.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.18362.0\\um\\x64\r\n``` \r\n", "comments": ["Please post the full error, there has to be something after `error executing command`. In fact, even the command you posted is truncated. Bazel prints exactly all the `cd`,`SET` and other commands needed to build the failing target.", "Closing due to lack of actionable information and unresponsiveness.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45680\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45680\">No</a>\n"]}, {"number": 45679, "title": "Fix bazel build with -DTF_LITE_STATIC_MEMORY.", "body": "Tested that the following command passes:\r\n```\r\nbazel test tensorflow/lite/micro/...  --test_tag_filters=-no_oss --build_tag_filters=-no_oss --copt=-DTF_LITE_STATIC_MEMORY\r\n```\r\n\r\nThis PR was created to address http://b/175642155 and https://github.com/tensorflow/tensorflow/pull/45535#issuecomment-744901727\r\n\r\nIn reality, while there is a mismatch between the internal and external CI (which should indeed be addressed), that wasn't the source of the problem. See https://github.com/tensorflow/tensorflow/pull/45535#issuecomment-745057559 for more information.\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 45678, "title": "clangformat all the files under the micro directory.", "body": "```\ntensorflow/lite/micro/tools/make/downloads/pigweed/pw_presubmit/py/pw_presubmit/format_code.py tensorflow/lite/micro/ -e \"\\.inc\" -e \"\\.py\" --fix\n```\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "Unclear why this PR was not merged, but it looks like the corresponding commit is in."]}, {"number": 45677, "title": "update estimator version after estimator final release", "body": "", "comments": []}, {"number": 45675, "title": "Prevent crash of tensorflow if shape is too large for tf.sparse.reorder", "body": "This PR tries to address the issue raised in #45392 where tensorflow crashes if shape of sparse tensor is too large for tf.sparse.reorder\r\n\r\nThis PR adds additional checks and exit gracefully if the shape is too large.\r\n\r\nThis PR fixes #45392.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["In my opinion, sparse tensors should work if the number of **possible** elements overflow 64 bit.", "@eiennohito The PR has been updated and now it should be possible to see reorder work with large sparse tensors.", "Thanks @mihaimaruseac for the review. The PR has been updated. The Sanity test failure seems to be unrelated to this PR (also fails with other PRs/commits). Please take a look and let me know if there are any other issues.", "@yongtang Yes probably it was caused by https://github.com/tensorflow/tensorflow/pull/43040 merge", "@yongtang  Can you please address Ubuntu Sanity errors? Thanks!", "@mihaimaruseac @gbaned The Sanity tests have been fixed against the latest master HEAD. Please take a look."]}, {"number": 45674, "title": "Add check to ensure that files in tensorflow/lite/micro have proper license and formatting", "body": "Fixes http://b/175315163 and http://b/169948621\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 45673, "title": "Manually fix licenses for the files to ensure consistent formatting.", "body": "Once this change is merged, we can turn on a presubmit check that tests\nfor the licenses.\n\nAddresses http://b/175315163\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 45672, "title": "Download pigweed as part of the TensorflowLite Micro makefile.", "body": "We will be using scripts from pigweed to add checks for licenses and\nclang-formatting (follow-on changes).\n\nDownloading and patching pigweed takes ~6 seconds on my local machine.\n\nAddresses http://b/175315163\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 45671, "title": "Default Installation is causing errors", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Nope\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 2.4 released 12/14/2020\r\n- Python version: 3.8.6 installed directly from the official site\r\n- Installed using virtualenv? pip? conda?: Nope\r\n- Bazel version (if compiling from source): Nope\r\n- GCC/Compiler version (if compiling from source): Nope\r\n- CUDA/cuDNN version: both 11.1/8.0.5 and 11.0/8.0.2\r\n- GPU model and memory: GEForce GTX1650 with 2 GB RAM\r\n\r\n\r\n\r\n**Describe the problem**\r\nUnable to do a default installation from source\r\nIf I use CUDA 11.0 with cuDNN 8.0.2.39 for 11.0, the error is\r\n_E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED_\r\n\r\nAlternatively, if I use CUDA 11.1 with cuDNN 8.0.5.39 for 11.1, the error is\r\n_Could not load dynamic library 'cusolver64_10.dll'; dlerror: cusolver64_10.dll not found\r\nCannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices..._\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\npip install tensorflow\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@quasar66,\r\n> Alternatively, if I use CUDA 11.1 with cuDNN 8.0.5.39 for 11.1, the error is\r\n> _Could not load dynamic library 'cusolver64_10.dll'; dlerror: cusolver64_10.dll not found_\r\n\r\nTensorFlow v2.4 and TF-nightly are built and tested against CUDA 11. Hence, could you please install CUDA 11 instead of CUDA 11.1 to resolve this issue. \r\n\r\nAlso, go through these similar issues [#44567](https://github.com/tensorflow/tensorflow/issues/44567#issuecomment-729504167), [#45258](https://github.com/tensorflow/tensorflow/issues/45258#issuecomment-739871133) for more information. \r\n", "> If I use CUDA 11.0 with cuDNN 8.0.2.39 for 11.0, the error is\r\n> _E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED_\r\n\r\nRegarding the `CUBLAS_STATUS_ALLOC_FAILED_` error, could you please provide the exact sequence of commands and code that you executed before running into the error, so that we can look into this. Thanks!", "HI..\r\nThe following is a toy code that will reflect the CUBLAS_STATUS_ALLOC_FAILED problem:\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom sklearn.datasets import load_sample_image\r\n\r\nchina = load_sample_image(\"china.jpg\") / 255\r\nflower = load_sample_image(\"flower.jpg\") / 255\r\nimages = np.array([china, flower])\r\nbatch_size, height, width, channels = images.shape\r\n\r\nfilters = np.zeros(shape=(7, 7, channels, 2), dtype=np.float32)\r\nfilters[:, 3, :, 0] = 1  # vertical line\r\nfilters[3, :, :, 1] = 1  # horizontal line\r\n\r\noutputs = tf.nn.conv2d(images, filters, strides=1, padding=\"SAME\")\r\n\r\n\r\nThe corresponding warning and error messages are as below:\r\n\r\n_2020-12-15 09:28:45.403935: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-12-15 09:28:46.956383: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-12-15 09:28:46.957868: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2020-12-15 09:28:48.041053: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1650 computeCapability: 7.5\r\ncoreClock: 1.56GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 119.24GiB/s\r\n2020-12-15 09:28:48.041363: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-12-15 09:28:48.048683: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-12-15 09:28:48.048843: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2020-12-15 09:28:48.052785: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2020-12-15 09:28:48.054430: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2020-12-15 09:28:48.062725: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2020-12-15 09:28:48.065722: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2020-12-15 09:28:48.066828: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2020-12-15 09:28:48.067032: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2020-12-15 09:28:48.611438: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-12-15 09:28:49.115463: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1650 computeCapability: 7.5\r\ncoreClock: 1.56GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 119.24GiB/s\r\n2020-12-15 09:28:49.115759: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-12-15 09:28:49.115905: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-12-15 09:28:49.116066: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2020-12-15 09:28:49.116219: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2020-12-15 09:28:49.116363: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2020-12-15 09:28:49.116504: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2020-12-15 09:28:49.116651: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2020-12-15 09:28:49.116834: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2020-12-15 09:28:49.117020: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2020-12-15 09:28:51.110467: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-12-15 09:28:51.110634: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n2020-12-15 09:28:51.110740: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n2020-12-15 09:28:51.112568: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2903 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2020-12-15 09:28:51.117135: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-12-15 09:28:51.254734: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2020-12-15 09:28:53.830147: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 0\r\n\r\n2020-12-15 09:28:53.880606: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 0\r\n\r\n2020-12-15 09:28:54.168327: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-12-15 09:28:55.246945: **E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED**\r\n\r\nProcess finished with exit code 0_\r\n", "@quasar66,\r\nPlease try limiting the GPU memory growth using any one of the methods listed in [this guide](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) and let us know if it helps.\r\n\r\nAlso, please take a look at [this comment](https://github.com/tensorflow/tensorflow/issues/7072#issuecomment-687649994) from similar issue #7072 and check if it works. Thanks!", "Thanks !!\r\nThe comment link you provided worked...\r\nAm closing the issue.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45671\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45671\">No</a>\n"]}, {"number": 45670, "title": "Unable to read", "body": "I'm using Ubuntu", "comments": ["@za13 It could be nice if you could really minimize your code example surface. \r\nMany lines depend on file input that slow down our reproducibility.  \r\nIt would be nice to have a minimal example that we could just copy, paste and run or a Colab.", "For TensorRT c++ code there is the Nvidia support forum https://forums.developer.nvidia.com/t/tensorrt-error-input-dynamic-input-is-missing-dimensions-in-profile-0/139505", "It was just a pointer to a support request like this one.\r\nWhat I meant Is that I don't know if currently we give support to TensorRT c++ but probably the Nvidia forum Is the right place.", "@za13,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here along with all the files required to run it. Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45670\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45670\">No</a>\n"]}, {"number": 45669, "title": "tensorflow 2.4 depends on tensorflow-estimator 2.4 which was not released in tandem", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.15\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.4\r\n- Python version: 3.7.7\r\n- Installed using virtualenv? pip? conda?: pipenv 2020.11.15\r\n\r\n**Describe the problem**\r\ntensorflow 2.4 depends on `tensorflow-estimator>=2.4,<2.5` but the latest release of tensorflow-estimator on PyPI is 2.3, so resolving dependencies fails.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\n$ cat Pipfile\r\n[[source]]\r\nurl = \"https://pypi.org/simple\"\r\nverify_ssl = true\r\nname = \"pypi\"\r\n\r\n[packages]\r\ntensorflow = \"==2.4\"\r\n\r\n[dev-packages]\r\n\r\n[requires]\r\npython_version = \"3.7\"\r\n\r\n$ pip install pipenv==2020.11.15\r\n$ pipenv install\r\n$ pipenv install\r\nPipfile.lock not found, creating...\r\nLocking [dev-packages] dependencies...\r\nLocking [packages] dependencies...\r\nBuilding requirements...\r\nResolving dependencies...\r\n\u2718 Locking Failed! \r\n[ResolutionFailure]:   File \"/Users/micahsmith/.pyenv/versions/3.7.7/lib/python3.7/site-packages/pipenv/resolver.py\", line 741, in _main\r\n[ResolutionFailure]:       resolve_packages(pre, clear, verbose, system, write, requirements_dir, packages, dev)\r\n[ResolutionFailure]:   File \"/Users/micahsmith/.pyenv/versions/3.7.7/lib/python3.7/site-packages/pipenv/resolver.py\", line 709, in resolve_packages\r\n[ResolutionFailure]:       requirements_dir=requirements_dir,\r\n[ResolutionFailure]:   File \"/Users/micahsmith/.pyenv/versions/3.7.7/lib/python3.7/site-packages/pipenv/resolver.py\", line 692, in resolve\r\n[ResolutionFailure]:       req_dir=requirements_dir\r\n[ResolutionFailure]:   File \"/Users/micahsmith/.pyenv/versions/3.7.7/lib/python3.7/site-packages/pipenv/utils.py\", line 1403, in resolve_deps\r\n[ResolutionFailure]:       req_dir=req_dir,\r\n[ResolutionFailure]:   File \"/Users/micahsmith/.pyenv/versions/3.7.7/lib/python3.7/site-packages/pipenv/utils.py\", line 1108, in actually_resolve_deps\r\n[ResolutionFailure]:       resolver.resolve()\r\n[ResolutionFailure]:   File \"/Users/micahsmith/.pyenv/versions/3.7.7/lib/python3.7/site-packages/pipenv/utils.py\", line 833, in resolve\r\n[ResolutionFailure]:       raise ResolutionFailure(message=str(e))\r\n[pipenv.exceptions.ResolutionFailure]: Warning: Your dependencies could not be resolved. You likely have a mismatch in your sub-dependencies.\r\n  First try clearing your dependency cache with $ pipenv lock --clear, then try the original command again.\r\n Alternatively, you can use $ pipenv install --skip-lock to bypass this mechanism, then run $ pipenv graph to inspect the situation.\r\n  Hint: try $ pipenv lock --pre if it is a pre-release dependency.\r\nERROR: Could not find a version that matches tensorflow-estimator<2.5.0,>=2.4.0rc0 (from tensorflow==2.4->-r /var/folders/mp/7s96qjnn7tl6nyjk729y16jw0000gn/T/pipenvz7_8pbn3requirements/pipenv-5b66o2rx-constraints.txt (line 2))\r\nTried: 1.10.6, 1.10.7, 1.10.8, 1.10.9, 1.10.10, 1.10.11, 1.10.12, 1.13.0, 1.14.0, 1.15.0, 1.15.1, 1.15.2, 2.0.0, 2.0.1, 2.1.0, 2.2.0, 2.3.0\r\nSkipped pre-versions: 1.13.0rc0, 1.14.0rc0, 1.14.0rc1, 2.1.0rc0, 2.2.0rc0, 2.3.0rc0, 2.4.0rc0\r\nThere are incompatible versions in the resolved dependencies:\r\n```\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nAs of 20:45 UTC (2020-12-14) tensorflow-estimator 2.4 has not been released on PyPI.\r\n<img width=\"1236\" alt=\"image\" src=\"https://user-images.githubusercontent.com/5473606/102133703-815c0100-3e23-11eb-8765-9e6736b098fa.png\">\r\n\r\n", "comments": ["nevermind - I misread the log, as it looks like tensorflow 2.4 allows tensorflow-estimator 2.4.0rc0. This is actually just pipenv problem. Solve this by allowing pre-releases (`pipenv install --pre tensorflow==2.4`) or specifying the exact pre-release in the pipfile.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45669\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45669\">No</a>\n"]}, {"number": 45667, "title": "Build tensorflow from source, issues about CUDA lib and header paths", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.15.3\r\n- Python version: 3.6.9\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 0.25.0\r\n- GCC/Compiler version (if compiling from source): 7.5\r\n- CUDA/cuDNN version: 11.0/8.0.4\r\n- GPU model and memory: GTX 1060 6GB\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI followed [this](https://www.tensorflow.org/install/source) tutorial to install tensorflow from source, I would like to link my tensorflow to tensorRT and CUDA/CUDNN, when I ran the configure.py it seems that it couldn't locate the CUDA lib and header files, so I listed all the possible paths for CUDA libs and header files with comma-separated as required in configure, as the following:\r\n\r\n/home/myName/Downloads/TensorRT-7.2.1.6.Ubuntu-18.04.x86_64-gnu.cuda-11.0.cudnn8.0/TensorRT-7.2.1.6,\r\n/usr/local/cuda-11.0/targets/x86_64-linux/include,\r\n/usr/local/cuda-11.0/targets/x86_64-linux/lib,\r\n/usr/local/cuda-11.0/bin,\r\n/usr/local/cuda-11.0/nvvm/libdevice,\r\n/usr/local/cuda/nvvm/libdevice\r\n\r\nThen it complained that it couldn't find libdevice*.10.bc, but this really doesn't make sense since /usr/local/cuda-11.0/nvvm/libdevice &  /usr/local/cuda/nvvm/libdevice are the paths containing libdevice.10.bc\r\n\r\nI have already verified TensorRT's installation by running its sample projects, did I do anything wrong? Thank you in advance!\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@yummychop,\r\nEvery TensorFlow release is compatible with certain a CUDA and cuDNN version. You can check the compatibility from the [tested build configurations](https://www.tensorflow.org/install/source#gpu). \r\n\r\nSince TensorFlow 1.x is not actively supported. Could you please try building TensorFlow v2.4 with CUDA 11 and cuDNN 8 and check if it works. Thanks!", "thank you @amahendrakar , i installed tensorflow 2.4.0 and the problem got resolved, I followed [this](https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html) tutorial provided by NVIDIA, it requires tensorflow 1.15.3 which is misleading... ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45667\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45667\">No</a>\n"]}]