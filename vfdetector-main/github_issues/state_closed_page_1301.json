[{"number": 14083, "title": "AssetFileDef is not updated when using Saved model builder.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n- **TensorFlow version (use command below)**:\r\nPulled from current master, but code is same at 1.4\r\n- **Python version**: \r\n2.7\r\n- **Bazel version (if compiling from source)**:\r\n0.5.4\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n-get latest tensorflow-serving\r\n-add to mnist_saved_model.py\r\n```\r\nasset_path = tf.constant(\"/tmp/asset.txt\", dtype=tf.string, name=\"PreProcessingSettings\")\r\n  tf.add_to_collection(tf.GraphKeys.ASSET_FILEPATHS, asset_path)\r\n....\r\nassets_collection = tf.get_collection(tf.GraphKeys.ASSET_FILEPATHS),\r\n...\r\nbuilder.save(as_text=True)\r\n```\r\nbuild mnist_saved_model.py\r\nexecute\r\nobserve that exported model meta_graph_defs do not have AssetFileDef populated\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nIt seems that from the definition of [meta graph def](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/meta_graph.proto)\r\nthat AssetFileDef should be populated, however instead the assets are just added to a collection... should it not be both?\r\nIt seems it should but it is not implemented. Would not be too hard to implement.\r\n\r\n### Source code / logs\r\nN/A", "comments": ["@davidsoergel @sukritiramesh  - mind taking a look?", "I've faced this issue as well. Waiting for response. Thanks!", "Please check with the latest version of TensorFlow. Feel free to close this issue if it still persists. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=14083\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=14083\">No</a>\n"]}, {"number": 14082, "title": "Handle nil return from TF_TensorData", "body": "With some memory allocators, attempting to allocate 0 bytes will return\r\na null pointer. This specifically happens when building tensorflow with\r\nmkl support. If TF_TensorData returns null, the go code to create a\r\nslice from the data leads to a null pointer exception. This fixes the\r\nissue by checking for the nil return and returning a slice zero value to\r\n(nil) to the caller. Fixes #13764.", "comments": ["Can one of the admins verify this patch?", "To clarify, the null pointer is created by the memory allocator in TF_AllocateTensor, but it is returned to the go side via TF_TensorData", "Jenkins, test this please"]}, {"number": 14080, "title": "@function.Defun lose input tensor shapes ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NA\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.12.5 (16F73)\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:1.3.0\r\n- **Python version**: 2.7.13\r\n- **Bazel version (if compiling from source)**:NA\r\n- **CUDA/cuDNN version**:NA\r\n- **GPU model and memory**:NA\r\n- **Exact command to reproduce**:\r\n```\r\nimport  tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.python.framework import function\r\n# Now let us see some property of @function of tf. It will make all its inputs loose \"shape\"\r\n\r\n@function.Defun(tf.float32, tf.float32)\r\ndef plus(A, B):\r\n\r\n    result = A + B\r\n    result.set_shape((5, 5))\r\n    print result.get_shape()\r\n    return result\r\n\r\ndef linear(A, B, C):\r\n    D = plus(A, B) + C\r\n    print D.get_shape()\r\n    return D\r\n\r\n# @function.Defun(\r\n#     tf.float32, shape_func=lambda op: [op.inputs[0].get_shape()])\r\n# def Foo(x):\r\n#     print x.get_shape, 'see'\r\n#     return x + 1.0\r\n\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\nA = tf.constant(np.arange(25).reshape((5, 5)), dtype=tf.float32)\r\nB = tf.ones((5, 5))\r\nC = tf.ones((5, 5))\r\nE = plus(A, B)\r\n# d = Foo(tf.constant(2.0))\r\nD = linear(A, B, C)\r\nprint sess.run(E)\r\nprint sess.run(D)\r\n# print sess.run(d)\r\n```\r\n\r\n### Describe the problem\r\nAll tensors going through the decorated function lose their shapes, as well as the output tensor. Without this decoration, there is no problem.  btw, shape_fun seems doesn't work\r\n", "comments": ["FYI `@Defun` is not a publicly supported API, which is why it does not appear in the API docs at https://www.tensorflow.org/api_docs/python/\r\n\r\nThat said,  @iganichev  has been looking into this.", "As @asimshankar mentioned, `@Defun` is experimental and has a number of features missing including shape propagation. This feature is on our radar but it is currently not high enough priority for us to know a rough timeline for its completion. ", "Yes, it's implemented through the C API but it's not enabled by default.", "Shape propagation should be working with @tf.function now."]}, {"number": 14079, "title": "How to get difference or square between two tensor\uff1f", "body": "I define two subnets that output the tensor of the same dimension use functional model. I want get difference between them. for example\uff1atensor1 [1,2,3,4,5], tensor2 [1,1,1,1,1], output=tensor1 -tensor2 =\r\nCan you tell me how to get difference between two tensor\uff0cor square of one tensor\uff1f\r\nThank you very much.", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 14078, "title": "Fix a minor typo in readme", "body": "", "comments": ["Can one of the admins verify this patch?"]}, {"number": 14077, "title": "Add gradient tests for `tf.maximum` and `tf.minimum`", "body": "Was looking into adding gradient tests for `tf.clip_by_value` (https://github.com/tensorflow/tensorflow/pull/13998#discussion_r147542917)\r\nand then noticed that there is no gradient tests in `math_grad_test.py`\r\nfor `tf.maximum` and `tf.minimum`.\r\n\r\nThink it makes sense to add a gradient test to cover `tf.maximum` and `tf.minimum`\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 14076, "title": "experimental", "body": "", "comments": ["Can one of the admins verify this patch?"]}, {"number": 14074, "title": "A problem about AttentionWrapper", "body": "Hi,\r\nIt is different to use Bahdanau Attention and Luong Attention in seq2seq model. When I use Bahdanau Attention I found a problem. \r\nThe hint said that when use Bahdanau Attention, need to set the output_attention=False when create AttentionWrapper. But if I set this parameter = False, I cant get the attention information in every timestamp and can not calculate the logits from both attention and cell_output.\r\nIs there any good way to get the attention of every timestamp?\r\nAnd if simply use cell_output to get the logits is quite similar with the way which both use attention and cell_output?\r\nThx a lot.", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 14073, "title": "Explicitly pass label_count to confusion_matrix.", "body": "If batch size gets small enough, there can be not enough classes\r\nin the validation batch, and auto-inference on each step may\r\nreturn different sizes of the confustion matrices, and their\r\naddition will fail", "comments": ["Can one of the admins verify this patch?", "resolved in https://github.com/tensorflow/tensorflow/pull/14600"]}, {"number": 14072, "title": "Feature request: support convert python object to tensor automatical, and can back to object in py_func or other mechod.", "body": "I want to use python object as tensor in Tensorflow and can convert it back to object when useing tf.py_func method,  to  support using other python package.", "comments": ["Have you considered [pickling](https://docs.python.org/2/library/pickle.html)? It should be possible to do this today. I am of course curious to learn more about how tf.py_func has been beneficial to your use case. I will also note that it is a feature that ought to be used judiciously. I'm going to close this one out, but if my suggestion doesn't offer solutions, let me know and I'll re-open.", "@jart Now I use cPickle to do this, I want to embed FST in to tensorflow for beam search decoding."]}, {"number": 14071, "title": "tf.train.MonitoredTrainingSession does not have request_stop() method", "body": "When i use tf.train.MonitoredTrainingSession class like below, I cannot use sess.request_stop() to stop the sess, because MonitoredTrainingSession does not have this method.\r\n\r\ntensorflow version is 1.3\r\n\r\n```\r\nwith tf.train.MonitoredTrainingSession(checkpoint_dir=\"/tmp/train_logs\",\r\n                                       config=config,\r\n                                       hooks=hooks) as sess:\r\n  while not mon_sess.should_stop():\r\n      sess.run(train_op)\r\n      sess.request_stop()\r\n```", "comments": ["I'm sorry, I don't quite follow the issue here. Yes, `request_stop` is not a method on the `Session` or `MonitoredSession` classes. Perhaps you could add more detail on what you're trying to do.", "I want to early stop the iteration over epoch, which maybe very time consume. So I have to use sess.request_stop() to stop the while loop like other Session, otherwise i have use `break` to stop the while loop.", "I'm not quite following what you're referring to. Neither `Session` nor `MonitoredSession` have a `request_stop()` method, so I'm not sure what you mean by \"like other Session\".", "@asimshankar Sorry, I mean coord has a request_stop() method , and coord is in MonitoredSession, so i think this method may exists. Now I think tf.train.StopAtStepHook  this maybe what I want.", "@asimshankar I think the question is how do you manually stop a MonitoredTrainingSession? There should be a way to do this.", "Is there a way to manually stop a MonitoredTrainingSession? I have encountered an issue: my chief worker stops at an exception while non-chief workers keeps running"]}, {"number": 14070, "title": "Feature request : add weight normalization", "body": "can you implement [weight norm](https://arxiv.org/pdf/1602.07868.pdf) ?\r\n\r\nI want to use it as follows.\r\n```python\r\n    x = tf.layers.conv2d(x, filter_size=32, kernel_size=[3,3], strides=2)\r\n    x = weight_norm(x)\r\n```\r\nIs it possible?\r\n", "comments": ["@martinwicke do you have thoughts on this [weight normalization](https://arxiv.org/pdf/1602.07868.pdf) paper?", "It is definitely possible, and contributions implementing it are welcome. ", "@taki0112 Not sure whether I understand weight normalization correctly?\r\n\r\nIt is indeed `Dense` layer which always normalizes its kernel when called (and add a new parameter `g`), right?\r\n\r\n<img width=\"298\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1112263/32213990-61f1d676-be58-11e7-8469-dab62722a46e.png\">\r\n", "@facaiy \r\nYes, i think may be right !\r\n\r\nIt does not matter whether it is a dense layer or a conv layer.\r\nI think that the weights used in each layer are normalized and multiplied by the trainable parameter g.", "@martinwicke \r\nI want to take this! Can somebody help me to start? This will be my first contribution!", "@divyanshj16 \r\noh.. really thank you\r\nIt would be great if we could see it as soon as possible.\r\nThank you for your implementation.", "@taki0112 , I found a relative code [here,](https://github.com/PFCM/weightnorm/blob/master/weightnorm/) \r\n@martinwicke Plz take a look at it, can you verify it?\r\n@divyanshj16 this may help you take off", "PS, for details on how to derive the formulas from scratch, some discussion is here -- https://discuss.openai.com/t/proof-of-weight-normalizations-mechanism/3026/3", "This has been implemented in tensorflow/addons. Issue can be closed.\r\n\r\n[Code](https://github.com/tensorflow/addons/blob/master/tensorflow_addons/layers/wrappers.py)\r\n[Example](https://github.com/tensorflow/addons/blob/master/tensorflow_addons/examples/layers_weightnormalization.ipynb)", "Thanks, Sean."]}, {"number": 14069, "title": "bug - when try train object detection in gcloud [ImportError: No module named 'tensorflow.python.eager']", "body": "\r\n### System information\r\n- **I tried to train the model using tensorflow objdet api**:\r\n- **OS Platform and Distribution Linux Ubuntu 16.04 (gcloud VM)**:\r\n- **TensorFlow installed from python pip**:\r\n- **TensorFlow version 1.3.0**:\r\n- **Python version 2.7**: \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA 8.0 /cuDNN 6.0**:\r\n- **The typical installation steps followed**:\r\n\r\n### Describe the problem\r\nwhen i trying to train the Mobilenet model the following error came up. I already trained using the same steps in my local PC without any errors. I couldn't find any solution related to this error.    \r\n\r\n### Source code / logs\r\nragulh28@ubuntu1gpu:~/project/models/research/object_detection$ python train.py --logtostderr --train_dir=training/\r\n --pipeline_config_path=ssd_mobilenet_v1_lap.config\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 49, in <module>\r\n    from object_detection import trainer\r\n  File \"/home/ragulh28/project/models/research/object_detection/trainer.py\", line 33, in <module>\r\n    from deployment import model_deploy\r\n  File \"/home/ragulh28/project/models/research/slim/deployment/model_deploy.py\", line 106, in <module>\r\n    from tensorflow.python.eager import context\r\nImportError: No module named eager\r\n", "comments": ["@ragulh26 Can you update your tensorflow to the latest nightly version?\r\n```\r\npip install --upgrade --force-reinstall tf-nightly\r\n```\r\nor\r\n```\r\npip install --upgrade --force-reinstall tf-nightly-gpu\r\n```", "I had the same problem", "cc @derekjchow @martinwicke @gunan  for possible insight.\r\n\r\nLooks like gcloud is using an old version of tensorflow install, but is instructing users to run the model from a new version of the source code?", "The error is in slim/model_deploy, so that must be new code. Is eager properly exposed in nightly?", "@martinwicke Yes, eager is in nightly. How is tf installed on gcloud? I'm not familiar with that part.", "If 1.3 is installed there it will simply not work.\n", "It seems that this breakage was introduced in https://github.com/tensorflow/models/commit/cbb624791068590026f50f01d370d9328fe8ebf2 which made the `slim` code depend on the master branch of TensorFlow instead of the release version\r\n\r\n@sguada : Is `models/research/slim` meant to depend on building TensorFlow from the HEAD of master branch? Or should the change mentioned above be rolled back to keep it compatible with the least official release (1.3.0)", "After installing the tf-nightly version the problem got resolved. Thanks for the help. ", "any status to this? I also have the same problem but installing tf-nightly did not fix the error.", "@chensteven please see the latest comment by @ragulh26 ", "> @ragulh26 Can you update your tensorflow to the latest nightly version?\r\n> \r\n> ```\r\n> pip install --upgrade --force-reinstall tf-nightly\r\n> ```\r\n> \r\n> or\r\n> \r\n> ```\r\n> pip install --upgrade --force-reinstall tf-nightly-gpu\r\n> ```\r\n\r\nIt tells me that \r\n`ERROR: Could not find a version that satisfies the requirement tf-nightly (from versions: none)\r\nERROR: No matching distribution found for tf-nightly\r\n`"]}, {"number": 14068, "title": "why don't work this code???", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nMy source code is this\r\n\r\nimport tensorflow as tf\r\nimport random\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\n\r\ntf.set_random_seed(777)\r\n\r\n\r\ndef MinMaxScaler(data):\r\n    numerator = data - np.min(data, 0)\r\n    denominator = np.max(data, 0) - np.min(data, 0)\r\n    # noise term prevents the zero division\r\n    return numerator / (denominator + 1e-7)\r\n\r\ntraining_epochs = 19\r\nbatch_size = 50\r\n\r\nxy = np.loadtxt('train.csv', delimiter=',', dtype=np.float32) #read training data set\r\nxy2 = np.loadtxt('test.csv', delimiter=',', dtype=np.float32) #read test data set\r\n\r\ntrain_x_batch, train_y_batch = \\\r\n    tf.train.batch([xy[1:], xy[0:1]], batch_size=50)\r\n\r\ntrain_x_batch2, train_y_batch2 = \\\r\n    tf.train.batch([xy2[1:], xy2[0:1]], batch_size=50)\r\n\r\n#print(x_data.shape, y_data.shape) check data shape\r\n\r\nnb_classes= 10 #0-9 labels\r\n\r\nX = tf.placeholder(tf.float32, [None, 784])\r\nY = tf.placeholder(tf.float32, [None, nb_classes])\r\n\r\nW = tf.Variable(tf.random_normal([784, nb_classes]), name='weight')\r\nb = tf.Variable(tf.random_normal([nb_classes]), name='bias')\r\n\r\nhypothesis = tf.nn.softmax(tf.matmul(X, W) + b)# made hypothesis using softmax\r\n\r\ncost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\r\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\r\n\r\n# Test model\r\nis_correct = tf.equal(tf.arg_max(hypothesis, 1), tf.arg_max(Y, 1))\r\n# Calculate accuracy\r\naccuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\r\n\r\nwith tf.Session() as sess:\r\n    #initialize tensorflow variables\r\n    sess.run(tf.global_variables_initializer())\r\n    #Trianing\r\n    for epoch in range(training_epochs):\r\n        avg_cost = 0\r\n        total_batch = int(950 / 50)\r\n        \r\n        for i in range(total_batch):\r\n            batch_xs, batch_ys = sess.run([train_x_batch, train_y_batch])\r\n            c, _ = sess.run([cost, optimizer], feed_dict={\r\n                            X: batch_xs, Y: batch_ys})\r\n            avg_cost += c / total_batch\r\n            \r\n        print('Epoch:', '%04d' % (epoch + 1),\r\n              'cost =', '{:.9f}'.format(avg_cost))\r\n\r\n    print(\"Learning finished\")\r\n    \r\n    batch_xs2, batch_ys2 = sess.run([train_x_batch2, train_y_batch2])\r\n    acc = accuracy.eval(session=sess, feed_dict={X:batch_xs2 , Y:batch_ys2})\r\n    print(\"%f\", acc)\r\n    \r\n\r\nInstructions for updating:\r\nUse `argmax` instead\r\nWARNING:tensorflow:From x-wingide-python-shell://151066184/2:43: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `argmax` instead\r\n2017-10-29 17:41:38.014455: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-10-29 17:41:38.014701: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n\r\nAnd don't working..... where is problem??", "comments": []}, {"number": 14067, "title": "R1.2", "body": "17.2.7", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->"]}, {"number": 14066, "title": "Fix position of arguments of nce_loss calculation", "body": "The nce_loss positional arguments in word2vec_simple.py are incorrect, leading to the following traceback:\r\n\r\nTraceback (most recent call last):\r\nFile \"word2vec_simple.py\", line 168, in \r\nnum_sampled, vocabulary_size))\r\nFile \"/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/nn_impl.py\", line 1151, in nce_loss\r\nname=name)\r\nFile \"/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/nn_impl.py\",line 981, in _compute_sampled_logits\r\nsampled_logits = math_ops.matmul(inputs, sampled_w, transpose_b=True)\r\nFile \"/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py\", line 1844, in matmul\r\na, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\r\nFile \"/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 1289, in _mat_mul\r\ntranspose_b=transpose_b, name=name)\r\nFile \"/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 526, in apply_op\r\ninferred_from[input_arg.type_attr]))\r\nTypeError: Input 'b' of 'MatMul' Op has type float32 that does not match type int32 of argument 'a'.\r\n\r\nThis patch fixes the positional arguments of nce_loss, and adds keyword arguments to make it more explicit.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 14065, "title": "[feature] \"Periodic\" mode for tf.pad", "body": "\r\n### System information\r\nN/A\r\n\r\n\r\n### Describe the problem\r\nIf I have a matrix\r\n```\r\n[[1, 2, 3],\r\n [4, 5, 6],\r\n [7, 8, 9]]\r\n```\r\n\r\nIt would be nice if I could use something like tf.pad( ... 'PERIODIC') to pad around the dimensions to get a matrix like\r\n```\r\n[ [9, 7, 8, 9, 7],\r\n  [3, 1, 2, 3, 1],\r\n  [6, 4, 5, 6, 4],\r\n  [9, 7, 8, 9, 7],\r\n  [3, 1, 2, 3, 1] ]\r\n```\r\n\r\nWhile there seems to be a work around by manually slicing and joining, this would seem nifty to have in tf.pad already to make expressions that assume periodic boundary conditions a bit easier.\r\n### Source code / logs\r\nN/A\r\n", "comments": ["Hi I would like to work on this. Can I take up this issue?", "Hi, I don't know whether the issue is already resolved, but if not I would like to give it a try. I was thinking of the following solution: \r\n\r\n![image](https://user-images.githubusercontent.com/45478473/109420646-e7dcce00-79d3-11eb-92f0-eeb50b3609af.png)\r\n\r\n\r\nOf course, adoptions and tests need to be made before the code can be implemented in TensorFlow. I would like to take up and finish the issue if still needed? This is my first time contributing so if you have some tips for me I would love to hear them. ", "This is a stale issue with no recent activity. I am closing this issue. Check the recent TF versions and open a new issue/feature. Thanks"]}, {"number": 14064, "title": "Fix position of arguments of nce_loss calculation on tensorflow_basic tutorial", "body": "The nce_loss positional arguments in word2vec_simple.py are incorrect, leading to the following traceback:\r\n\r\nTraceback (most recent call last):\r\n  File \"word2vec_simple.py\", line 168, in <module>\r\n    num_sampled, vocabulary_size))\r\n  File \"/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/nn_impl.py\", line 1151, in nce_loss\r\n    name=name)\r\n  File \"/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/nn_impl.py\",line 981, in _compute_sampled_logits\r\n    sampled_logits = math_ops.matmul(inputs, sampled_w, transpose_b=True)\r\n  File \"/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py\", line 1844, in matmul\r\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\r\n  File \"/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 1289, in _mat_mul\r\n    transpose_b=transpose_b, name=name)\r\n  File \"/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 526, in apply_op\r\n    inferred_from[input_arg.type_attr]))\r\nTypeError: Input 'b' of 'MatMul' Op has type float32 that does not match type int32 of argument 'a'.\r\n\r\nThis patch fixes the positional arguments of nce_loss, and adds keyword arguments to make it more explicit.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->", "I signed it again"]}, {"number": 14063, "title": "Branch 173772851", "body": "", "comments": []}, {"number": 14062, "title": "Possible Memory Leak with Pet-variant Detection model on Android", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS Sierra+Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.3.0\r\n- **Python version**: Python 3.6.1 :: Anaconda custom (64-bit)\r\n- **Bazel version (if compiling from source)**: Build label: 0.5.4-homebrew\r\n- **CUDA/cuDNN version**: Not used\r\n- **GPU model and memory**: Not used\r\n- **Exact command to reproduce**: X\r\n\r\n### Describe the problem\r\n\r\nWhen loading a trained-from-scratch sdd_mobilenet_v1 (frozen_inference_graph.pb) in place of \"file:///android_asset/ssd_mobilenet_v1_android_export.pb\" for the TF Detect app, the screen goes blank white and crashes, without an error logged. I have been following the pet example and have gotten passed the GraphDef Invalid error(s). In my case, the problem occurs after:\r\n\r\n`I/TensorFlowInferenceInterface: TensorFlow native methods not found, attempting to load via tensorflow_inference`\r\n`I/TensorFlowInferenceInterface: Successfully loaded TensorFlow native methods (RunStats error may be ignored)`\r\n\r\nWhen loading my custom ssd mobilenet model (5621 labels), I assume the model fails to load because it hangs on the white screen before crashing and I dont see:\r\n\r\n`I/TensorFlowInferenceInterface: Model load took 502ms, TensorFlow version: 1.4.0-rc1`\r\n`I/TensorFlowInferenceInterface: Successfully loaded model from 'file:///android_asset/ssd_mobilenet_v1_android_export.pb'`\r\n\r\nOne notable difference is my model file is 432M while the example is 28M\r\n`432M Oct 26 22:34 frozen_inference_graph.pb`\r\n`28M Oct 20 23:04 ssd_mobilenet_v1_android_export.pb`\r\n\r\nWhen loading my model (I've enabled large heap), the Android profiler shows the memory used increases to around 2GB until the crash. I have tried using the transform_graph util, though any produced pb file gives GraphDef invalid or doesn't fix the issue.\r\n\r\n<img width=\"1264\" alt=\"screen shot 2017-10-28 at 4 48 20 pm\" src=\"https://user-images.githubusercontent.com/2712171/32139524-06a3137a-bc00-11e7-9635-d1762bbdb32c.png\">\r\n\r\nHere is a summary of the pb file:\r\n\r\n`bazel-bin/tensorflow/tools/graph_transforms/summarize_graph --in_graph=frozen_inference_graph.pb`\r\n`Found 1 possible inputs: (name=image_tensor, type=uint8(4), shape=[?,?,?,3])\r\nNo variables spotted.\r\nFound 4 possible outputs: (name=detection_boxes, op=Identity) (name=detection_scores, op=Identity) (name=detection_classes, op=Identity) (name=num_detections, op=Identity)\r\nFound 87826925 (87.83M) const parameters, 0 (0) variable parameters, and 90093 control_edges\r\nOp types used: 85323 Const, 33735 Gather, 28107 Minimum, 22484 Maximum, 16964 Reshape, 11281 Cast, 11265 Sub, 11248 Greater, 11242 Split, 11242 Where, 5696 Slice, 5683 ConcatV2, 5682 Mul, 5675 StridedSlice, 5659 Pack, 5658 Shape, 5654 Add, 5628 Squeeze, 5624 Unpack, 5621 ZerosLike, 5621 NonMaxSuppression, 229 Identity, 48 Fill, 45 ExpandDims, 37 Tile, 35 Relu6, 35 FusedBatchNorm, 34 Conv2D, 28 RealDiv, 28 Range, 28 Switch, 23 Enter, 13 Merge, 13 DepthwiseConv2dNative, 12 BiasAdd, 9 TensorArrayV3, 7 NextIteration, 6 Sqrt, 5 TensorArrayWriteV3, 5 TensorArrayGatherV3, 5 Exit, 5 TensorArraySizeV3, 5 Assert, 4 TensorArrayScatterV3, 4 Equal, 4 TensorArrayReadV3, 3 Rank, 3 Transpose, 2 All, 2 Exp,2 GreaterEqual, 2 LoopCond, 2 Less, 1 LogicalAnd, 1 TopKV2, 1 Size, 1 ResizeBilinear, 1 Placeholder, 1 Sigmoid`\r\n`To use with tensorflow/tools/benchmark:benchmark_model try these arguments:\r\nbazel run tensorflow/tools/benchmark:benchmark_model -- --graph=frozen_inference_graph.pb --show_flops --input_layer=image_tensor --input_layer_type=uint8 --input_layer_shape=-1,-1,-1,3 --output_layer=detection_boxes,detection_scores,detection_classes,num_detections`\r\n\r\nIs this a memory leak? I don't think the app should be in the GB's\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n`tensorflow/examples/android/src/org/tensorflow/demo/TensorFlowObjectDetectionAPIModel.java`\r\n`tensorflow/examples/android/src/org/tensorflow/demo/DetectorActivity.java`\r\n\r\n(from models)\r\n`models/research/object_detection/create_pet_tf_record.py`\r\n`models/research/object_detection/export_inference_graph.py`\r\n`ssd_mobilenet_v1.config`\r\n\r\n\r\n", "comments": ["It sounds like you replaced the example with a different model. So your best chance of getting support is on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is a large community that reads questions there.", "Though it isn't the example, I should still be able to replace the same model with another of the same architecture! I have asked on stackoverflow, but I think the people who made the app might have the best insight (the apps are supposed to serve as foundations, right?). \r\n\r\nAnyway, do you have an example you can point me to to train ssd_mobilenet_v1 compatible with TF Detect? "]}, {"number": 14061, "title": "Add int64 input tensor support for `tf.invert_permutation`", "body": "This fix tries to add int64 input tensor support for `tf.invert_permutation`.\r\n\r\nIn the docs of the TensorFlow (https://www.tensorflow.org/api_docs/python/tf/invert_permutation), it was specified that the input tensor `x` could be either int32 or int64:\r\n```\r\nx: A Tensor. Must be one of the following types: int32, int64. 1-D.\r\n```\r\n\r\nHowever, int64 was actually not supported.\r\n\r\nThis fix adds the int64 support by adding template to the class `InvertPermutationOp` so that both int64 and int32 could be processed.\r\n\r\nThis fix also adds additional test cases so that changes could be covered.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n", "comments": ["Can one of the admins verify this patch?", "Jenkins. test this please.", "Oh great, github flake.\r\n", "Jenkins, test this please.", "replicate_model_fn_test failure is almost certainly unrelated."]}, {"number": 14060, "title": "Feature request: add tensor.<xyz> methods that redirect to tf.<xyz>(tensor,...)", "body": "Any TensorFlow API function func that takes `tensor` as a first argument could be implemented as `tensor.func(...)`. This is implemented in PyTorch/numpy and can make formulas more concise. Transpose is a frequent special case that deserves a shortcut like `T`.\r\n\r\ncc @shoyer for numpy API wisdom\r\n\r\nCompare:\r\n\r\nNumpy:\r\n```\r\nb = a @ a.T\r\nb.trace()\r\n```\r\n\r\nPyTorch:\r\n```\r\nb =  a @ a.t()\r\nb.trace()\r\n```\r\n\r\nTensorFlow:\r\n```\r\nb = a @ tf.transpose(a)\r\ntf.trace(b)\r\n```", "comments": ["This certainly convenient, but I'm not sure it was actually a good design choice for NumPy or would be for TensorFlow.\r\n\r\n`numpy.ndarray` has 71 non-private attributes*:\r\nT, all, any, argmax, argmin, argpartition, argsort, astype, base, byteswap, choose, clip, compress, conj, conjugate, copy, ctypes, cumprod, cumsum, data, diagonal, dot, dtype, dump, dumps, fill, flags, flat, flatten, getfield, imag, item, itemset, itemsize, max, mean, min, nbytes, ndim, newbyteorder, nonzero, partition, prod, ptp, put, ravel, real, repeat, reshape, resize, round, searchsorted, setfield, setflags, shape, size, sort, squeeze, std, strides, sum, swapaxes, take, tobytes, tofile, tolist, tostring, trace, transpose, var, view\r\n\r\nThere's no clear rule for what is a method vs. a function (most methods are also functions, but the converse is not always true). Only a fraction of these are actually widely used. Many of them are attractive nuisances with better alternatives elsewhere, but since we care about backwards compatibility the old methods are never removed. We no longer add new methods, for fear of making this list longer.\r\n\r\nPurely from an organization point of view, having all these methods makes the internal dependency graph a complete mess. Basically everything ends up depending on everything else in a completely circular way.\r\n\r\nIn practice, I think more numeric type promotion (https://github.com/tensorflow/tensorflow/issues/8224) would make a bigger difference for readability of TensorFlow code. I see lots of codes littered with calls to `tf.constant` and `tf.cast` that should be totally unnecessary.\r\n\r\n* Generated with `', '.join(entry for entry in dir(np.array(0)) if not entry.startswith('__'))`.", "Fair enough, my main readability issue was with `tf.transpose` being too long, but one could also `def t(x): return tf.transpose(x)`"]}, {"number": 14059, "title": "Adding a feed for boolean tensors to TensorFlowInferenceInterface", "body": "For #13601, adding the requested implementation to `TensorFlowInferenceInterface`.\r\n", "comments": ["Can one of the admins verify this patch?", "I could find no existing location for unit tests under the Android contrib project, and so added no unit test for this change.  If such a thing exists, please point me to it and i'll add one there.", "Jenkins, test this please."]}, {"number": 14058, "title": "Fixes build breakage", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->"]}, {"number": 14057, "title": "CNN gives core dumpped but for the same data LSTM is running successfully", "body": "\r\n![screenshot from 2017-10-28 19-55-09](https://user-images.githubusercontent.com/18217467/32135309-517eaf52-bc1a-11e7-8f0a-cab3b3a03619.png)\r\n![screenshot from 2017-10-28 19-54-17](https://user-images.githubusercontent.com/18217467/32135312-553fcfc2-bc1a-11e7-9e4f-4d4dedf992b9.png)\r\n\r\ncuda 7.5 and cudann 5.0\r\ntensorflow 0.10.0 and keras 1.2.0\r\n\r\nLSTM program runs successfully. For CNN program, it is showing the below error. hHow to correct this?\r\n\r\n\r\n", "comments": ["I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you.\r\n\r\nIn particular, detailed instructions on reproducing the problem will be helpful."]}, {"number": 14056, "title": "Half Normal Distribution (and inverse error function)", "body": "## Inverse Error Function\r\n\r\nAdded `erfinv` as a simple function of `ndtri` (the api mirrors the approach taken in `scipy.special`. Mainly to prevent distributions with methods in terms of the inverse error function reimplementing their own wrappers around `ndtri`.\r\n\r\n## Half Normal Distribution\r\n```python\r\nX ~ Normal(0.0, scale)\r\nY = |X|\r\n# then Y ~ HalfNormal(scale)\r\n```\r\n\r\nAdded the Half Normal distribution to `contrib.distributions`. Main things to look at is how I'm dealing with the pdf discontinuity.\r\n\r\nA discontinuity in the pdf that sends values to 0.0 is easy to represent in `prob`, (by multiplying my a tensor with 0.0s at the relevant positions) than `log_prob` (which requires masking -inf wherever x < 0). I'd be interested if anyone has any suggestions for correct ways to implement the latter, as I think this is a case where `log_prob` is more numerically stable (along the support of the distribution that is).", "comments": ["Can one of the admins verify this patch?", "Sanity checks failure seems to be unrelated to the content of the PR:\r\n```\r\nThe command '/bin/sh -c /install/install_deb_packages.sh' returned a non-zero code: 100\r\nERROR: docker build failed. Dockerfile is at /var/lib/jenkins/workspace/tensorflow-pull-requests-sanity/tensorflow/tools/ci_build/Dockerfile.cpu\r\n```\r\n", "Jenkins, test this please.", "The details mostly look good.  Is there a reason you didn't use the [absolutevalue Bijector](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distributions/python/ops/bijectors/absolute_value_impl.py#L34) to implement this class?  It would take care of sampling and log_prob for you.", "Thank for bringing that to my attention. If that's available, I'll use it to implement `HalfNormal` (and might refactor that as a special case of a `FoldedNormal` in a different PR).\r\n\r\nAs an aside, the docs state that bijectors implement bijective differentiable transforms, but the absolute value function is not a bijection. Am I right in guessing bijectors is moving towards being a more general set of transforms for distributions?", "Yes; it's an extension now that bijector inverses (and inverse log det\njacobian functions) can return tuples - under the assumption that the\npreimage is always len(tuple)-valued.  I believe that the special case of\nt=0 should continue to work for HalfNormal due to the way\nTransformedDistribution treats these values, but if not then you may have\nto special case that value.\n\nOn Fri, Nov 10, 2017 at 2:55 PM, Charles Shenton <notifications@github.com>\nwrote:\n\n> Thank for bringing that to my attention. If that's available, I'll\n> implement a the FoldedNormal with HalfNormal as a special case.\n>\n> As an aside, the docs state that bijectors implement bijective\n> differentiable transforms, but the absolute value function is not a\n> bijection. Am I right in guessing bijectors is moving towards being a more\n> general set of transforms for distributions?\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/14056#issuecomment-343608567>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimwNO0YaFuj41jX5LymhyF9bj_kljks5s1NRngaJpZM4QJ65_>\n> .\n>\n", "Fixed things up. Tests were previously failing since the scipy `halfnorm`'s first arg was not `scale` as I assumed, but `loc`, where loc is both the mean of the base distribution and the fold point.\r\n\r\nAlso I didn't end up using the bijector and subclassing `TransformedDistribution`, because the bijector wasn't passing through static shapes in `prob` and `log_prob`, meaning the static shape of the result was `<unknown>`.", "Could someone manually trigger the test run? Container build failed on the sanity tests for some reason.\r\n```\r\nERROR: docker build failed. Dockerfile is at /var/lib/jenkins/workspace/tensorflow-pull-requests-sanity/tensorflow/tools/ci_build/Dockerfile.cpu\r\nBuild was aborted\r\nAborted by unknown\r\nUnable to get pull request builder trigger!!\r\n```", "@tensorflow-jenkins test this please.", "Looks like one of the macOS tests timed out, so tests will need to be rerun. Apologies for the trouble. \r\n```\r\n//tensorflow/python/kernel_tests:slice_op_test                          TIMEOUT in 301.0s\r\n```", "Wiki is fine\n\nOn Mon, Nov 20, 2017, 2:44 PM Charles Shenton <notifications@github.com>\nwrote:\n\n> *@cshenton* commented on this pull request.\n> ------------------------------\n>\n> In\n> tensorflow/contrib/distributions/python/kernel_tests/half_normal_test.py\n> <https://github.com/tensorflow/tensorflow/pull/14056#discussion_r152132770>\n> :\n>\n> > +            dist.log_prob, dist.prob, dist.log_survival_function,\n> +        ]:\n> +          print(func.__name__)\n> +          value = func(x)\n> +          grads = gradients_impl.gradients(value, [scale])\n> +          with self.test_session(graph=g):\n> +            variables.global_variables_initializer().run()\n> +            self.assertAllFinite(value)\n> +            self.assertAllFinite(grads[0])\n> +\n> +  def testHalfNormalEntropy(self):\n> +    with self.test_session():\n> +      scale = np.array([[1.0, 2.0, 3.0]])\n> +      halfnorm = hn_lib.HalfNormal(scale=scale)\n> +\n> +      expected_entropy = 0.5 * np.log(np.pi * scale ** 2.0 / 2.0) + 0.5\n>\n> Will do. Is a link to wikipedia fine? There's the 1961 paper on the folded\n> normal distributions but that's paywalled so less easy to check.\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/14056#discussion_r152132770>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim3aTD9GoDVjrpOuaSHSVHMe3T-5rks5s4gDTgaJpZM4QJ65_>\n> .\n>\n", "Do the remaining checks need to be manually triggered, or is the Ubuntu CC just taking a really long time?"]}, {"number": 14055, "title": "Fix failing build on Fedora 26 (stropts.h)", "body": "Tensorflow branches 1.4 and master do not build on Fedora 26. When bazel builds curl as a third-party package, the file `stropts.h` is always included, while it is no longer present on many Linux distributions (including Fedora). Curl can be easily built without it by removing the `HAVE_STROPTS` flag.\r\n\r\nThis problem was already mentioned (tensorflow/serving#320). The solution proposed there is to create an empty `/usr/include/stropts.h` as root. Because that is not always possible, just removing the HAVE_STROPTS flag seems to be a better solution.", "comments": ["Can one of the admins verify this patch?", "I can confirm that this problem occurs on redhat machines too.", "Jenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please."]}, {"number": 14054, "title": "[Keras] TimeDistributed wrapper error:  'NoneType' object has no attribute 'as_list'", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: built from master pulled on 10/26\r\n- **Python version**: 3.6\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: See below\r\n\r\n### Describe the problem\r\nIssue arises when using TimeDistributed wrapper in release *1.4*. See simple code example below which works fine in version *1.3*.\r\n\r\n### Source code / logs\r\n```\r\ndef td():\r\n    model = tf.keras.models.Sequential()\r\n    model.add(tf.keras.layers.Dense(8, input_shape=(16,)))\r\n    model.add(tf.keras.layers.Dense(4))\r\n    model.summary()\r\n    \r\n    frame_input = tf.keras.layers.Input(shape=(10, 16))\r\n    x = tf.keras.layers.TimeDistributed(model)(frame_input)\r\n    x = tf.keras.layers.Flatten()(x)\r\n    \r\n    full_model = tf.keras.models.Model(inputs=frame_input, outputs=x)\r\n    full_model.summary()\r\n```\r\nproduces this trace:\r\n\r\n```\r\n     x = tf.keras.layers.TimeDistributed(model)(frame_input)\r\n  File \"/Users/guest/dev/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/topology.py\", line 252, in __call__\r\n    output = super(Layer, self).__call__(inputs, **kwargs)\r\n  File \"/Users/guest/dev/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 575, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/Users/guest/dev/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/layers/wrappers.py\", line 238, in call\r\n    output_shape = self._compute_output_shape(input_shape).as_list()\r\n  File \"/Users/guest/dev/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/layers/wrappers.py\", line 193, in _compute_output_shape\r\n    child_input_shape).as_list()\r\nAttributeError: 'NoneType' object has no attribute 'as_list'\r\n```", "comments": ["Run the code `td()` on tf 1.5-nightly, it seems good?\r\n\r\n```bash\r\n~/Downloads \u276f\u276f\u276f python a.py\r\n1.5.0-dev20171021\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #\r\n=================================================================\r\ndense_1 (Dense)              (None, 8)                 136\r\n_________________________________________________________________\r\ndense_2 (Dense)              (None, 4)                 36\r\n=================================================================\r\nTotal params: 172\r\nTrainable params: 172\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #\r\n=================================================================\r\ninput_1 (InputLayer)         (None, 10, 16)            0\r\n_________________________________________________________________\r\ntime_distributed_1 (TimeDist (None, 10, 4)             172\r\n_________________________________________________________________\r\nflatten_1 (Flatten)          (None, 40)                0\r\n=================================================================\r\nTotal params: 172\r\nTrainable params: 172\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n```", "I built tensorflow from master so I assumed I had the latest. Where do you get the nightly builds from?\r\n\r\nEDIT:\r\nI found the nightly build [here](https://pypi.python.org/pypi/tf-nightly) and installed the python 3.6 version. Even though I installed this 3.6 version, I get this warning when I import tensorflow:\r\n```\r\ncompiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\r\n```\r\nI'm still getting the error so now I'm thinking it's a 3.6 issue.", "I verified that I'm getting the same issue in a python 3.5 environment. Must be some other issue with my setup? \ud83d\ude15 \r\n\r\n```\r\n    x = tf.keras.layers.TimeDistributed(model)(frame_input)\r\n  File \"/home/guest/anaconda3/envs/tf35/lib/python3.5/site-packages/tensorflow/python/keras/_impl/keras/engine/topology.py\", line 253, in __call__\r\n    output = super(Layer, self).__call__(inputs, **kwargs)\r\n  File \"/home/guest/anaconda3/envs/tf35/lib/python3.5/site-packages/tensorflow/python/layers/base.py\", line 591, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/home/guest/anaconda3/envs/tf35/lib/python3.5/site-packages/tensorflow/python/keras/_impl/keras/layers/wrappers.py\", line 238, in call\r\n    output_shape = self._compute_output_shape(input_shape).as_list()\r\n  File \"/home/guest/anaconda3/envs/tf35/lib/python3.5/site-packages/tensorflow/python/keras/_impl/keras/layers/wrappers.py\", line 193, in _compute_output_shape\r\n    child_input_shape).as_list()\r\nAttributeError: 'NoneType' object has no attribute 'as_list'\r\n```", "@fchollet If you have a moment our friend is reporting an API regression in `tf.keras.layers.TimeDistributed` from 1.3 -> 1.4.", "So...while trying to debug this I discovered something odd. It works fine when I put a breakpoint in the `_compute_output_shape` method in TimeDistributed. When I remove the breakpoint and just run it, I get the error. This seems to be some kind of race condition. Can someone who can get this to work describe their environment? (I'm using the anaconda python 3.6 dist). Anyone have insight as to why this might be happening?", "I found out what the issue is. It's a bug in tensorflow. I have a PR that fixes it. I just am awaiting permission to push it up.", "@jart What is the procedure for getting permission to push up a PR? I filled out the form and was expecting an email or something but haven't received anything yet.", "I wasn't aware there was a form. Is that for the Google CLA? If that's signed, you should be able to just write the code and send us the PR. Then someone on the team should hopefully review it in a day or so.\r\n\r\nPlease note you can't push to tensorflow/tensorflow. Typically people click the fork button, push to their personal repo, and then use that to create the PR.", "Yes it was the Google CLA.\r\n\r\nThanks for the tip, the PR is up.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Amazing, [tooth's solution](https://github.com/tensorflow/tensorflow/commit/d34c1d2978ec43f6600fc2f59d24055a972e09c6) has really fixed the problem happening on tensorflow 1.4.  Move the block left for two spaces (two character positions) is all I did to %USERPROFILE%\\AppData\\Local\\Programs\\Python\\Python36\\Lib\\site-packages\\tensorflow\\python\\layers\\base.py \r\n\r\nI suffered the problem when trying to play the @fchollet's demo: \r\n\"Integrating Keras & TensorFlow - The Keras workflow, expanded (TensorFlow Dev Summit 2017)\"\r\nYouTube ID: UeheTiBJ0Io\r\n\r\nError message :  \r\n\r\n    %USERPROFILE%\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\layers\\wrappers.py in _compute_output_shape(self, input_shape)\r\n        191                                                  input_shape[2:])\r\n        192     child_output_shape = self.layer._compute_output_shape(  # pylint: disable=protected-access\r\n    --> 193         child_input_shape).as_list()\r\n        194     timesteps = input_shape[1]\r\n        195     return tensor_shape.TensorShape([child_output_shape[0], timesteps] +\r\n\r\n    AttributeError: 'NoneType' object has no attribute 'as_list'\r\n\r\nIf you are looking for solution for the same problem this is it.\r\nYou need to fix it manually so far on tf 1.4\r\n  "]}, {"number": 14053, "title": " Bug when using estimator with tf.data.Dataset.from_tensor_slices", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Windows 10\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:1.5.0-dev20171026\r\n- **Python version**: 3.6.3\r\n\r\n### Describe the problem\r\nWhen I use tf.data and from_tensor_slices with estimator to build dataset from ndarray, tf.estimator will use much memory(peak 6900MB, final 4600MB with [simple mnist cnn](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/convolutional_network.ipynb)) and create very huge event files\uff1a\r\nevents.out.tfevents 1.28GB\r\ngraph.pbtxt: 1.20GB\r\nmodel.ckpt-1.meta: 370MB\r\nThen I use cifar10 input pipeline from [resnet](https://github.com/tensorflow/models/blob/master/official/resnet/cifar10_main.py) as input_fn, everything back to normal.\r\n\r\n### Source code / logs\r\nreplace code in [notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/convolutional_network.ipynb) cell 6 with code below.\r\n```Python\r\ndef ndarray_iters(datas,\r\n                batch_size):\r\n    dataset = tf.data.Dataset.from_tensor_slices(tuple(datas))\r\n    batched_dataset = dataset.batch(batch_size)\r\n    iterator = batched_dataset.make_one_shot_iterator()\r\n    next_iters = list(iterator.get_next())\r\n    return next_iters\r\ndef train_input_fn():\r\n    imgs, labels = ndarray_iters([mnist.train.images, mnist.train.labels], 128)\r\n    return {'images': imgs}, labels\r\nmodel.train(train_input_fn, steps=1000)\r\n```", "comments": ["@jsimsa : Mind taking a look? Thanks.", "@traveller59 the problem is rooted in the fact that `from_tensor_slices` will create a constant-value tensor that has the raw data, which will be recorded in GraphDef.\r\n\r\nAn alternative approach (that avoids recording the raw data in GraphDef) to achieve the same behavior is to use `tf.data.from_generator`:\r\n\r\n```\r\n...\r\ndef make_generator(images, labels):\r\n\r\n  def _generator():\r\n    for image, label in zip(images, labels):\r\n      yield image, label\r\n\r\n  return _generator\r\n\r\n\r\ndef train_input_fn():\r\n  mnist = input_data.read_data_sets('/tmp/data/', one_hot=False)\r\n  dataset = tf.data.Dataset.from_generator(\r\n      make_generator(mnist.train.images, mnist.train.labels),\r\n      (tf.float32, tf.float32))\r\n  batched_dataset = dataset.batch(batch_size)\r\n  iterator = batched_dataset.make_one_shot_iterator()\r\n  imgs, labels = list(iterator.get_next())\r\n  return {'images': imgs}, labels\r\n...\r\n```", "Thanks, it seems that from_tensor_slices should be deprecated."]}, {"number": 14052, "title": "Fix minor typo", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Jenkins, test this please."]}]