[{"number": 38638, "title": "Dilated convolution pass not working on standard TCN model", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (or github SHA if from source): 17dfa4e121c080a547e9cf6443b8fe2ae9ed45ed\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\n# Copy and paste here the exact command (in Python)\r\nmodel = load_model(model_path, custom_objects={'TCN': TCN})\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.experimental_new_converter = True\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\r\n```\r\n```\r\n# Copy and paste here the exact command (tf_tfl_translate command-line utility)\r\ntf_tfl_translate  --tf-input-arrays=input_1 --tf-input-shapes=1,784,1 --tf-output-arrays=dense/BiasAdd --print-function-result-mapping -o=$HOME/Desktop/converted.tflite --emit-builtin-tflite-ops ~/smnist.pb\r\n```\r\n**The output from the converter invocation**\r\n\r\n```\r\n# Copy and paste the output here. (tf_tfl_translate output)\r\n'main' inputs:\r\n\tname: 'input_1' buffer: 0\r\n'main' outputs:\r\n\tname: 'dense/BiasAdd' buffer: 271 loc(\"dense/BiasAdd\")\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n[smnist.zip](https://github.com/tensorflow/tensorflow/files/4492741/smnist.zip) contains the model in both .h5 and .pb format, and the converted Flatbuffers.\r\n\r\n\r\n**Failure details**\r\nI'm converting the Keras-TCN model from https://github.com/philipperemy/keras-tcn to TF Lite.\r\n\r\nMethod 1:\r\nI convert the Keras model (.h5) to a TF Lite Flatbuffer via the Python API as above. The conversion is successful, inference with the model has the correct accuracy, but I want to get rid of the SpaceToBatchNd and BatchToSpaceNd nodes resulting from the Conv1D ops present in the model. Commit f54bb6f5578b931d79884302768996ba1073f685 claims to do so solving issue #29509. However this does not happen to be so in my converted model, which I attach.\r\n\r\nMethod 2:\r\nTo investigate further, I built the tf_tfl_translate tool from source and invoked it with the command above on a GraphDef (.pb) of the model. Conversions happen to be successful but again the STB and BTS ops are still present in the TF Lite Flatbuffer. The sequence of ops in the GraphDef seems to comply to the sequences specified in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/lite/transforms/dilated_conv.h. \r\nHowever, this time the ExpandDims and Squeeze ops are correctly converted to Reshape operations, as specified here https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/lite/transforms/optimize_patterns.td, whereas this would not happen with the method above, and although I would like to avoid this being done on my final Flatbuffer.\r\n\r\n", "comments": ["Hi,\r\n\r\nThe current dilated conv rewriting pass only supports Conv2D:\r\nhttps://github.com/tensorflow/tensorflow/blob/b422faa5e39f0ea2e9502ed5d974f23c36768894/tensorflow/compiler/mlir/lite/transforms/prepare_tf.cc#L629\r\n\r\nIf your model contains Conv1D, unfortunately that's not supported now.", "Thank you for your interest,\r\n\r\nBut actually the model presents exactly a Conv2D op, as the Conv1D is converted to an ExpandDims -> Conv2D -> Squeeze sequence. So in the frozen graph there is exactly a SpaceToBatchND -> Expand -> Conv2D -> Squeeze -> BatchToSpaceND -> BiasAdd sequence which this comment states is supported\r\nhttps://github.com/tensorflow/tensorflow/blob/adbacb4206cf440f3628e7240a53d8e0f12122b6/tensorflow/compiler/mlir/lite/transforms/dilated_conv.h#L50\r\nThe same comment cites Wavenet as a use case for this sequence, which is very similar to my model.", "I took a look at your graph. And I think the pattern doesn't get matched because this line:\r\nhttps://github.com/tensorflow/tensorflow/blob/adbacb4206cf440f3628e7240a53d8e0f12122b6/tensorflow/compiler/mlir/lite/transforms/dilated_conv.h#L284\r\n\r\nFor example, tensor name: tcn/residual_block_1_1/conv1D_1/SpaceToBatchND/block_shape only has 1 element '2', so the pattern got rejected since it's less than 2 elements.\r\n\r\nCould you change your model code where you specify the `block_shape` argument for the SpaceToBatchND op? Assuming you want to apply dilation rate 2 on both the height and width, you can just pass in [2, 2] in this case.", "Thank you.\r\n\r\nThe problem is that I didn't create the ExpandDims -> Conv2D -> Squeeze sequence explicitly, so I don't have direct access to them. I have dilated Conv1D layers in my model which are handled by TensorFlow this way. The complexity that TF introduces to handle Conv1D is actually quite surprising. So I think that the conversion should take care of supporting dilated Conv1D as it does dilated Conv2D. I could extend it myself in a pull request if that could help. ", "I see. I think the TF high level python API just rewrite the dilated conv into SpaceToBatch -> Conv -> BatchToSpace automatically. If you could help extend this pass by supporting Conv1D that will be great. Thanks!", "Hi @haozha111,\r\n\r\nUnfortunately I cannot make it to work on this, is there a chance someone could solve this?\r\nI kind of depend on it, but if this weren't possible I'd go with an ugly custom pass in my hardware delegates for turnaround time reasons.\r\n\r\nI would be available to support with more information on how to reproduce the issue, etc.", "Hi,\r\n\r\nI wouldn't have much time working on support for dilated conv1d. Do you mind either file a github issue or write the custom pass on your own? I believe the work should be  very similar with the current 2d case with little modifications.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38638\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38638\">No</a>\n"]}, {"number": 38637, "title": "Why there are serveral topkv2 ops", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):ubuntu18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):tensorflow2.1\r\n- Python version:3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:10.1\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n![Screenshot from 2020-04-17 18-27-35](https://user-images.githubusercontent.com/17592563/79559932-27ab0780-80d9-11ea-9472-f89f98a04ea8.png)\r\n**Describe the expected behavior**\r\nOnly one topkV2\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nhttps://github.com/google/automl/blob/028789605f1f140b00c045f77be2c4e13638d17c/efficientdet/det_model_fn.py#L313\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n[trace.json](https://drive.google.com/open?id=1lPiHuSDjdcBL9IoQg3kRHE3Ff2toyVwB)", "comments": ["Can you please provide a reproducible code to validate the issue reported?\r\nThanks!", "> Can you please provide a reproducible code to validate the issue reported?\r\n> Thanks!\r\n\r\nhttps://colab.research.google.com/drive/1duD_fPzP0cEVK-EmJEEDtagNnX3XjgJP\r\nThe trace file generated at automl/efficientdet/trace.json", "Why do you expect to see only one operation in the timeline? Estimator will run the graph in a loop and each execution will show up on the timeline.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38637\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38637\">No</a>\n", "I use the session directly without Estimator for inference.", "Execute time seems 6 times slower than execute top_k directly with the same shape input test case."]}, {"number": 38636, "title": "ImportError: DLL load failed", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): pip install tensorflow\r\n- TensorFlow version: 2.1\r\n- Python version: 3.7.7 64-bit\r\n- Installed using virtualenv? pip? conda?: no\r\n- CUDA/cuDNN version: 10.1/  v7.6.5 (November 5th, 2019), for CUDA 10.1\r\n- GPU model and memory: GTX 1070 Nividia 8GB\r\n- GPU driver 445.75\r\n- CPU Intel Celeron G3900 2,8 GHz\r\n\r\n**Describe the problem**\r\nwhile trying import tensorflow i got such error below. i used Visual Studio Code, i have tried different versions of tensorflow 2.0, 2.1 ang GPU version - still this problem :(\r\n\r\nI have installed newest VC_redist.x64, i have also in my system env variables path to:\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\bin\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\libnvvp\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\extras\\CUPTI\\lib64\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\include\r\nC:\\tools\\cuda\\bin\r\nC:\\tools\\cuda\\include\r\nC:\\tools\\cuda\\lib\\x64\r\n\r\nI have tried every solution which a found on Github but nothing works form me, and please help me with it.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nimport tensorflow \r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Dominik\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Dominik\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Dominik\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Dominik\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Dominik\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: Procedura inicjowania biblioteki do\\u0142\\u0105czanej dynamicznie (DLL) nie powiod\\u0142a si\\u0119.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"c:\\Users\\Dominik\\Documents\\Python Scripts\\NLP\\06-Deep-Learning\\import tensorflow.py\", line 1, in <module>\r\n    import tensorflow\r\n  File \"C:\\Users\\Dominik\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Users\\Dominik\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\Dominik\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\Dominik\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\Dominik\\AppData\\Local\\Programs\\Python\\Python37\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\Dominik\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Dominik\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Dominik\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Dominik\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Dominik\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Dominik\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Dominik\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: Procedura inicjowania biblioteki do\\u0142\\u0105czanej dynamicznie (DLL) nie powiod\\u0142a si\\u0119.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n\n* For TF-GPU - See point 1\n* For TF-CPU - See point 2\n\n-----------------------------------------------------------------------------------------------\n\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\n*TF Version >= 1.13 requires CUDA 10.0 and TF Version < 1.13 (till TF 1.5) requires CUDA 9.0.*\n\n* If you have above configuration and using _**Windows**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n  * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n* If you have above configuration and using _**Ubuntu/Linux**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n  * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n* If error still persists then, apparently your CPU model does not support AVX instruction sets.\n  * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\n Therefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n* Try Google Colab to use TensorFlow.\n  * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true).You get pre-installed latest stable TF version. Also you can use```pip install``` to install any other preferred TF version.\n  * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n  * All you need is a good internet connection and you are all set.\n* Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*", "@DominikManiak, one solution is downloading and installing visual studio 2015-2019 x86 and x64 from here:[https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads)\r\nOther solution is downgrading tensorflow to 2.0:\r\n`pip install tensorflow==2.0`", "> @DominikManiak, one solution is downloading and installing visual studio 2015-2019 x86 and x64 from here:https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads\r\n> Other solution is downgrading tensorflow to 2.0:\r\n> `pip install tensorflow==2.0`\r\n\r\nThank you khimraj!\r\n\r\nBut... as i described i did both solutions and it didn't help me! :( Any other ideas?", " Referer issue [#38636](https://github.com/tensorflow/tensorflow/issues/38636).", "@ refer issue [#38636](https://github.com/tensorflow/tensorflow/issues/38636).", "@DominikManiak \r\nIs your python 64 bits? Your CPU does not support AVX2 instructions?\r\n\r\nplease refer to below issues.\r\n#36167 (comment)\r\n\r\n#36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204", "Closing as duplicate.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156\r\n\r\n#36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38636\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38636\">No</a>\n"]}, {"number": 38635, "title": "How to Save Model with TF2.x Keras Multiworker Distributed Training?", "body": "**Description:**\r\n\r\nI've created a  `Keras` model, and trained it with `multiworker distributed strategy`.\r\n\r\nEvery worker uses the same python scripts for training. I uses `model.save` function for model saving with the same `hdfs path`.\r\n\r\nAfter training, every worker would try to save model to the `path`, and it will cause race condition, because all of them want to handle the same `variables files`.\r\n\r\n**Code snippets**\r\n\r\n```python\r\ntf_config = {\r\n    \"task\": {\r\n        \"index\": 0,\r\n        \"type\": \"worker\"\r\n    },\r\n    \"cluster\": {\r\n        \"worker\": [\"localhost:21834\", \"localhost:27271\"],\r\n    }\r\n}\r\nos.environ[\"TF_CONFIG\"] = json.dumps(tf_config)\r\nprint(json.loads(os.environ[\"TF_CONFIG\"]))\r\n\r\n\r\ndef main(argv):\r\n    del argv  # Unused\r\n    BATCH_SIZE = 100\r\n    SAMPLE_SIZE = 50000\r\n    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n    with strategy.scope():\r\n        model = tf.keras.Sequential([\r\n            layers.Dense(64, activation='relu'),\r\n            layers.Dense(32, activation='relu'),\r\n            layers.Dense(1, activation='sigmoid')\r\n        ])\r\n        model.compile(optimizer=tf.keras.optimizers.Adam(),\r\n                      loss=tf.keras.losses.BinaryCrossentropy(),\r\n                      metrics=[tf.keras.metrics.AUC()])\r\n    log_dir = FLAGS.logs\r\n    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,\r\n                                                          histogram_freq=1,\r\n                                                          update_freq='epoch')\r\n    train_dataset = tf.data.Dataset.from_tensor_slices(\r\n        (np.random.randint(1000, size=(SAMPLE_SIZE, 31)),\r\n         np.random.randint(2, size=(SAMPLE_SIZE, 1))))\r\n    train_dataset = train_dataset.batch(BATCH_SIZE)\r\n    validation_dataset = tf.data.Dataset.from_tensor_slices(\r\n        (np.random.randint(1000, size=(SAMPLE_SIZE, 31)),\r\n         np.random.randint(2, size=(SAMPLE_SIZE, 1))))\r\n    validation_dataset = validation_dataset.batch(BATCH_SIZE)\r\n    options = tf.data.Options()\r\n    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\r\n    train_dataset = train_dataset.with_options(options)\r\n    validation_dataset = validation_dataset.with_options(options)\r\n    model.fit(train_dataset,\r\n              epochs=5,\r\n              steps_per_epoch=10,\r\n              validation_data=validation_dataset,\r\n              validation_steps=5)\r\n    model_dir = FLAGS.logs + '/models'\r\n    model.save(model_dir)\r\n\r\nif __name__ == '__main__':\r\n    app.run(main)\r\n```\r\n\r\n**Question:**\r\n\r\nIn my opinion, the save should only be done by the `chief worker` not all of them. So\r\n\r\n1. Is there some problem with my `save` code?\r\n2. Should I change the path for different workers? If so, which model is the final model for serving?\r\n3. Is there a way that chief worker saves model only?\r\n\r\nThanks ~\r\n", "comments": ["Did you get a chance to see [Multi-worker training with Keras](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#multi-worker_configuration) example? That should help answer some of your questions. Thanks!", "Ideally the save should only be done by the chief worker, but it is a bit unfortunate that currently all workers in `MultiworkerMirroredStrategy` will try to save the model, and because they run the same piece of code, they will save to the same location. AFAIK the main reason behind this is `model.save` is not and in principle should not be made to be aware of which worker is running it.\r\n\r\nI heard there are plans to make it better. For now you can use different saving paths for different workers , and at the end delete the paths if the code is running on non-chief workers. The model saved by chief worker can be used for serving (in fact models saved by different workers are identical).", "@ymodak Yes, I've read that before. But I don't  think it will help with my doubts about  **Model  Save**.", "@ckkuang Thanks very much~.  Really helpful to me."]}, {"number": 38634, "title": "Support quantized int8 and uint8 in TFLu mean operator", "body": "", "comments": ["Meghna,\r\ncould you review this pull request?", "@advaitjain could you take a look at this? ", "@patriklaurell Can you please check @advaitjain, @hajuho comments and keep us posted. Thanks!", "Rebased on tip of master to get latest changes to the AllocatePersistentBuffer API, to get the unit test passing.", "Still fails test\r\nmake -f tensorflow/lite/micro/tools/make/Makefile test_kernel_reduce_test\r\n\r\n@advaitjain  could you review this again?", "@hajuho running \r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile clean\r\nmake -f tensorflow/lite/micro/tools/make/Makefile test_kernel_reduce_test\r\n```\r\nI got the test to pass with the following output.\r\n```\r\ntensorflow/lite/micro/testing/test_linux_binary.sh tensorflow/lite/micro/tools/make/gen/linux_x86_64/bin/kernel_reduce_test '~~~ALL TESTS PASSED~~~'\r\ntensorflow/lite/micro/tools/make/gen/linux_x86_64/bin/kernel_reduce_test: PASS\r\n```\r\n", "@patriklaurell Can you please resolve conflicts? Thanks!", "@patriklaurell \r\nThe test is successful with the baseline code of this PR, but fails when I rebased to the HEAD.\r\nCould you take a look?", "> @patriklaurell\r\n> The test is successful with the baseline code of this PR, but fails when I rebased to the HEAD.\r\n> Could you take a look?\r\n\r\n@hajuho I rebased on master to get latest changes to CreateQuantizedTensor and addressed your comments. All tests pass for me now standing on commit 48dff25", "@hajuho can you please help merge this PR ?", "For now, this PR fails with next command which is a part of google internal tests.\r\ntensorflow/tools/ci_build/ci_build.sh MICRO \"tensorflow/lite/micro/tools/ci_build/test_all.sh\"\r\n\r\n/workspace/tensorflow/lite/micro/tools/make/downloads/gcc_embedded/bin/../lib/gcc/arm-none-eabi/7.3.1/thumb/v7e-m/libgcc.a(unwind-arm.o): In function `get_eit_entry':\r\nunwind-arm.c:(.text+0x138): undefined reference to `__exidx_end'\r\nunwind-arm.c:(.text+0x13c): undefined reference to `__exidx_start'\r\ncollect2: error: ld returned 1 exit status\r\ntensorflow/lite/micro/tools/make/Makefile:358: recipe for target 'tensorflow/lite/micro/tools/make/gen/stm32f4_cortex-m4/bin/kernel_reduce_test' failed\r\nmake: *** [tensorflow/lite/micro/tools/make/gen/stm32f4_cortex-m4/bin/kernel_reduce_test] Error 1\r\nmake: *** Waiting for unfinished jobs....\r\n\r\n@patriklaurell Can you also take a look into this?", "@patriklaurell Can you please check @petewarden's comments and keep us posted ? Thanks!", "@patriklaurell, Any update on this PR? Please. Thanks!", "@patriklaurell, Any update on this PR? and can you please resolve conflicts. Thanks!", "Sorry for the delay. I've been on my summer vacation the last weeks. I now resolved the conflicts with master and addressed @petewarden's comment.", "Apologies for the delay, and for leaving this PR in the review queue for many months, and for the rest of my comment feeling like rewinding this PR back to the start.\r\n\r\nI had a similar comment on your other PR, but repeating here as well:\r\n * I wanted to use this opportunity to point you to our [updated contributing guidelines](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md).\r\n\r\n * In particular, could you create a github issue with more context on why you need int8 and uint8 support per our [porting reference kernels guidelines](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md#reference-kernel-implementations)?\r\n\r\nThanks for working with us to roll out these new processes -- we're working on improving the process for everyone involved.", "@patriklaurell  Can you please check @advaitjain's comments and keep us posted ? Thanks!", "I pushed a commit to @patriklaurell's branch to fix the error due to an internal build variant that is not currently exposed externally and was causing head scratching [here](https://github.com/tensorflow/tensorflow/pull/38634#discussion_r474535671)\r\n\r\nHere are my commands, adapted from [this gist](https://gist.github.com/elfrank/c08256de9c15e41e1781):\r\n```\r\ngit remote add patriklaurell git@github.com:patriklaurell/tensorflow.git\r\ngit fetch patriklaurell\r\ngit checkout -b patriklaurell-tflu-8bit-mean patriklaurell/tflu-8bit-mean\r\n\r\n# make the fixes and commit to the local branch\r\n\r\ngit push patriklaurell tflu-8bit-mean\r\n```"]}, {"number": 38633, "title": "How does the \"tensorflow/compiler/mlir/tensorflow/ir/tf_generated_ops.td\" file generated?", "body": "How does the \"tensorflow/compiler/mlir/tensorflow/ir/tf_generated_ops.td\" file generated?", "comments": ["@jpienaar Hi, is there any script in TF to generate the contents in \"tensorflow/compiler/mlir/tensorflow/ir/tf_generated_ops.td\" ?", "This question is better asked on StackOverflow since it is not a bug or feature request. There is also a larger community that reads questions there and provide better and faster support for such issues. Thanks!", "OK. Thanks anyway."]}, {"number": 38632, "title": "Some bugs of CategoricalCrossentropy and SparseCategoricalCrossentropy in tf.keras.losses ", "body": "It seems that there are some bugs of CategoricalCrossentropy and SparseCategoricalCrossentropy. The codes below will go wrong in tensorflow with version 2.2.0rc2.\r\n1. Error of no gradients provided with categorical crossentropy .\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\nimport tensorflow.keras.backend as K\r\nfrom tensorflow.keras.layers import *\r\n\r\ni = Input([], dtype='int32')\r\no = Dense(2, activation='softmax')(i[:, None])\r\nl = tf.keras.losses.categorical_crossentropy(K.one_hot(i, 2), o)\r\nm = keras.Model(inputs=i, outputs=o)\r\nm.add_loss(l)\r\nm.compile(optimizer='adam')\r\nm.fit(np.ones([1]))\r\n```\r\nand the same with the code below\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\nimport tensorflow.keras.backend as K\r\nfrom tensorflow.keras.layers import *\r\n\r\ni = Input([], dtype='int32')\r\no = Dense(2, activation='softmax')(i[:, None])\r\nl = tf.keras.losses.CategoricalCrossentropy()(K.one_hot(i, 2), o)\r\nm = keras.Model(inputs=i, outputs=o)\r\nm.add_loss(l)\r\nm.compile(optimizer='adam')\r\nm.fit(np.ones([1]))\r\n```\r\n2. Showing error that 'Sparse ops are not supported with functional models with built-in layer wrapping...' with sparse categorical crossentropy.\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\nimport tensorflow.keras.backend as K\r\nfrom tensorflow.keras.layers import *\r\n\r\ni = Input([], dtype='int32')\r\no = Dense(2, activation='softmax')(i[:, None])\r\nl = tf.keras.losses.sparse_categorical_crossentropy(i, o)\r\nm = keras.Model(inputs=i, outputs=o)\r\nm.add_loss(l)\r\nm.compile(optimizer='adam')\r\nm.fit(np.ones([1]))\r\n```\r\nas well as the one below\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\nimport tensorflow.keras.backend as K\r\nfrom tensorflow.keras.layers import *\r\n\r\ni = Input([], dtype='int32')\r\no = Dense(2, activation='softmax')(i[:, None])\r\nl = tf.keras.losses.SparseCategoricalCrossentropy()(i, o)\r\nm = keras.Model(inputs=i, outputs=o)\r\nm.add_loss(l)\r\nm.compile(optimizer='adam')\r\nm.fit(np.ones([1]))\r\n```\r\n", "comments": ["The CategoricalCrossentropy sample code works with TF v2.1 but fails with TF v2.2.0-rc3 and TF-nightly. Whereas code for SparseCategoricalCrossentropy fails with all three versions. Please find the attached gist below\r\n- TF v2.1\r\nhttps://colab.research.google.com/gist/amahendrakar/9593cd3693f10da12f4c8eea9cecc735/38632-2-1.ipynb\r\n\r\n- TF v2.2.0-rc3\r\nhttps://colab.research.google.com/gist/amahendrakar/70de2b98f68054f90be8c1244f165946/38632-2-2.ipynb\r\n\r\n- TF-nightly\r\nhttps://colab.research.google.com/gist/amahendrakar/1399f73ff9092916d4c9ab408093e067/38632-tf-nightly.ipynb\r\n\r\nThanks!", "Thank you @gdhy9064. Could you provide more information on the use case you are looking to solve?\r\n\r\nFor the above use cases you can provide the loss in the compile API. If you want to create a loss that is dependent on layer inputs we recommend that you use `add_loss` API in layer `call`. Here is an example [colab](https://colab.sandbox.google.com/drive/1zzLcJ2A2qofIvv94YJ3axRknlA6cBSIw).", "@pavithrasv Yes, I'd like to create a loss which is depend on some layer inputs of a model. Sometimes I prefer to customize these losses with `add_loss` API outside of a model for convenience, though it maybe not be of a neat structure. In my case, I can simply use `tf.gather` and `log` instead of `sparse_categorical_crossentropy` to meet my needs, but it would be better if I can use `sparse_categorical_crossentropy` or `categorical_crossentropy` API directly. And many thanks for your guidances on colab.", "Thank you @gdhy9064, one simple workaround you can do for both the cases above is : remove activation parameter from the Dense layer and add a separate Activation layer like \r\n\r\n```\r\n    o = layers.Dense(2)(i[:, None])\r\n    o = layers.Activation('softmax')(o)\r\n```\r\n\r\nWe will fix the underlying issue soon.", "Thank you @pavithrasv , it works well with `categorical_crossentropy`. It is a little pity that I can't still use with `sparse_categorical_crossentropy`.", "I made the change in this nightly [colab](https://colab.sandbox.google.com/gist/amahendrakar/1399f73ff9092916d4c9ab408093e067/38632-tf-nightly.ipynb#scrollTo=6vSINBWRA9GB). The workaround works for `sparse_categorical_crossentropy` as well. PTAL and let me know.", "Yes, it works for `sparse_categorical_crossentropy` in nightly version.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38632\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38632\">No</a>\n"]}, {"number": 38631, "title": "ValueError: No gradients provided for any variable", "body": "I just don't see the problem here, so I created two simple examples. One simple Feed Forwad network to solve XOR and one simple LSTM for a word-completion task. The XOR works as it should but the word-completion keeps throwing the `ValueError` and it's not clear _why_.\r\n\r\n\r\n**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Ubuntu 18.04\r\n- TensorFlow installed from: pip\r\n- TensorFlow version: tf-nightly==2.2.0.dev20200410\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\n\r\nTrying to `fit()` a model raises `ValueError: No gradients provided for any variable` for no apparent reason.\r\n\r\n**Describe the expected behavior**\r\n\r\nIt should work as shown in the XOR example.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n## Not working: Word Completion\r\n\r\nI created an executable [gist here](https://gist.github.com/stefan-falk/42ef89c6636fd9f91fc471584659512f).\r\n\r\nThe exception I am getting:\r\n\r\n```none\r\nValueError: No gradients provided for any variable: ['embedding/embeddings:0', 'rnn/gru_cell/kernel:0', 'rnn/gru_cell/recurrent_kernel:0', 'rnn/gru_cell/bias:0', 'rnn_1/gru_cell_1/kernel:0', 'rnn_1/gru_cell_1/recurrent_kernel:0', 'rnn_1/gru_cell_1/bias:0', 'time_distributed/kernel:0', 'time_distributed/bias:0', 'outputs/kernel:0', 'outputs/bias:0'].\r\n````\r\n\r\n\r\n<details>\r\n  <summary>Click to show code</summary>\r\n\r\n```python\r\nimport random\r\nfrom functools import partial\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow_datasets.core.features.text import SubwordTextEncoder\r\n\r\nEOS = '<eos>'\r\nPAD = '<pad>'\r\n\r\nRESERVED_TOKENS = [EOS, PAD]\r\nEOS_ID = RESERVED_TOKENS.index(EOS)\r\nPAD_ID = RESERVED_TOKENS.index(PAD)\r\n\r\ndictionary = [\r\n    'verstehen',\r\n    'verstanden',\r\n    'vergessen',\r\n    'verlegen',\r\n    'verlernen',\r\n    'vertun',\r\n    'vertan',\r\n    'verloren',\r\n    'verlieren',\r\n    'verlassen',\r\n    'verhandeln',\r\n]\r\n\r\ndictionary = [word.lower() for word in dictionary]\r\n\r\n\r\ndef get_model(params) -> keras.models.Model:\r\n\r\n    inputs = layers.Input((None,), dtype=tf.int64, name='inputs')\r\n\r\n    x = inputs\r\n\r\n    vocab_size = params['vocab_size']\r\n    hidden_size = params['hidden_size']\r\n    max_input_length = params['max_input_length']\r\n    max_target_length = params['max_target_length']\r\n\r\n    x = layers.Embedding(vocab_size, hidden_size, input_length=max_input_length)(x)\r\n\r\n    # Encoder\r\n    x = layers.RNN(layers.GRUCell(hidden_size))(x)\r\n    x = layers.RepeatVector(max_target_length)(x)\r\n\r\n    # Deoder\r\n    x = layers.RNN(layers.GRUCell(hidden_size), return_sequences=True)(x)\r\n    x = layers.TimeDistributed(layers.Dense(hidden_size, activation='relu'))(x)\r\n\r\n    # Outputs\r\n    output_dense_layer = layers.Dense(vocab_size, activation='softmax')\r\n    outputs = layers.TimeDistributed(output_dense_layer, name='outputs')(x)\r\n\r\n    return keras.models.Model(inputs=[inputs], outputs=[outputs])\r\n\r\n\r\ndef sample_generator(text_encoder: SubwordTextEncoder, max_sample: int = None):\r\n    count = 0\r\n\r\n    while True:\r\n        random.shuffle(dictionary)\r\n\r\n        for word in dictionary:\r\n\r\n            for i in range(1, len(word)):\r\n\r\n                inputs = word[:i]\r\n                targets = word\r\n\r\n                example = dict(\r\n                    inputs=text_encoder.encode(inputs) + [EOS_ID],\r\n                    targets=text_encoder.encode(targets) + [EOS_ID],\r\n                )\r\n                count += 1\r\n\r\n                yield example\r\n\r\n                if max_sample is not None and count >= max_sample:\r\n                    print('Reached max_samples (%d)' % max_sample)\r\n                    return\r\n\r\n\r\ndef make_dataset(generator_fn, params, training):\r\n\r\n    dataset = tf.data.Dataset.from_generator(\r\n        generator_fn,\r\n        output_types={\r\n            'inputs': tf.int64,\r\n            'targets': tf.int64,\r\n        }\r\n    )\r\n\r\n    if training:\r\n        dataset = dataset.shuffle(100)\r\n\r\n    dataset = dataset.padded_batch(\r\n        params['batch_size'],\r\n        padded_shapes={\r\n            'inputs': (None,),\r\n            'targets': (None,)\r\n        },\r\n    )\r\n\r\n    if training:\r\n        dataset = dataset.map(lambda example: to_train_example(example, params=params)).repeat()\r\n\r\n    return dataset\r\n\r\n\r\ndef to_train_example(example: dict, params: dict):\r\n    # Make sure targets are one-hot encoded\r\n    example['targets'] = tf.one_hot(example['targets'], depth=params['vocab_size'])\r\n    return example\r\n\r\n\r\ndef main():\r\n\r\n    text_encoder = SubwordTextEncoder.build_from_corpus(\r\n        iter(dictionary),\r\n        target_vocab_size=1000,\r\n        max_subword_length=6,\r\n        reserved_tokens=RESERVED_TOKENS\r\n    )\r\n\r\n    generator_fn = partial(sample_generator, text_encoder=text_encoder, max_sample=10)\r\n\r\n    params = dict(\r\n        batch_size=20,\r\n        vocab_size=text_encoder.vocab_size,\r\n        hidden_size=32,\r\n        max_input_length=30,\r\n        max_target_length=30,\r\n        enable_metrics_in_training=True\r\n    )\r\n\r\n    model = get_model(params)\r\n\r\n    model.compile(\r\n        optimizer=keras.optimizers.Adam(0.001),\r\n        loss='categorical_crossentropy',\r\n    )\r\n\r\n    assert len(model.trainable_variables), 'There are no trainable_variables'\r\n    model.summary()\r\n\r\n    train_dataset = make_dataset(generator_fn, params, training=True)\r\n\r\n    model.fit(\r\n        train_dataset,\r\n        epochs=5,\r\n        steps_per_epoch=100,\r\n    )\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n</details>\r\n\r\n## Working: XOR\r\n\r\nThis example works as it should. Here I am subclassing `keras.models.Model`. The reason why I am using the functional API above is s.t. I can debug more easily.\r\n\r\n<details>\r\n  <summary>Click to show code</summary>\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\n\r\ndef xor_data():\r\n    inputs = [[0, 0], [0, 1], [1, 0], [1, 1]]\r\n    targets = [[0], [1], [1], [0]]\r\n    while True:\r\n        for x, y in zip(inputs, targets):\r\n            yield x, y\r\n\r\n\r\nclass FeedForwardNetwork(keras.models.Model):\r\n\r\n    def __init__(self, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n        self._layers = [\r\n            keras.layers.Dense(4, activation='sigmoid'),\r\n            keras.layers.Dense(4, activation='sigmoid'),\r\n            keras.layers.Dense(1, activation='sigmoid')\r\n        ]\r\n\r\n    def call(self, x, **kwargs):\r\n        for layer in self._layers:\r\n            x = layer(x)\r\n        return x\r\n\r\n\r\nclass XorCallback(keras.callbacks.Callback):\r\n\r\n    def on_epoch_end(self, epoch, logs=None):\r\n        all_data = [[0, 0], [0, 1], [1, 0], [1, 1]]\r\n        y = self.model.predict(all_data)\r\n\r\n        print('\\nPredictions: ')\r\n        print((y > 0.5) * 1)\r\n\r\n\r\ndef main():\r\n\r\n    dataset = tf.data.Dataset.from_generator(xor_data, output_types=(tf.int64, tf.int64)).batch(10).shuffle(100)\r\n\r\n    train_dataset = dataset.repeat()\r\n    dev_dataset = dataset\r\n\r\n    for batch in dataset:\r\n        print(batch)\r\n        break\r\n\r\n    model_internal = FeedForwardNetwork()\r\n\r\n    inputs = keras.layers.Input(shape=(2,))\r\n    logits = model_internal(inputs)\r\n\r\n    model = keras.models.Model(inputs=[inputs], outputs=[logits])\r\n\r\n    model.compile(\r\n        optimizer=keras.optimizers.Adam(0.0001),\r\n        loss='mse',\r\n        metrics=['mse']\r\n    )\r\n\r\n    model.summary()\r\n\r\n    model_fp = '/tmp/xor/model'\r\n\r\n    callbacks = [\r\n        keras.callbacks.ModelCheckpoint(\r\n          model_fp,\r\n          save_best_only=True,\r\n          save_weights_only=False\r\n        ),\r\n        XorCallback()\r\n    ]\r\n\r\n    model.fit(\r\n        train_dataset,\r\n        epochs=5,\r\n        steps_per_epoch=5000,\r\n        validation_data=dev_dataset,\r\n        validation_steps=100,\r\n        callbacks=callbacks\r\n    )\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n</details>\r\n\r\n[stackoverflow](https://stackoverflow.com/questions/61249708/valueerror-no-gradients-provided-for-any-variable-tensorflow-2-0-keras)\r\n\r\n### Potentially related\r\n\r\n- https://github.com/tensorflow/tensorflow/issues/1511\r\n- https://github.com/tensorflow/tensorflow/issues/27949\r\n", "comments": ["i am able to replicate this issue, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/62f46d7c5e446ca443a763d9c84bec53/38631.ipynb) same issue is replicatable on [2.1 as well](https://colab.sandbox.google.com/gist/Saduf2019/cf26b50082f74a3eb15806b7cc08f086/38631.ipynb)", "@Saduf2019 I have to say that I am not 100% sure if I am doing everything right w.r.t. dimensions in the word completion example. So maybe there's an issue in the cross-entropy loss function between the targets and the predictions. But it's hard to tell because the exception does not give any clues unfortunately.\r\n", "Thanks, @stefan-falk . The code above will take some work to debug, and StackOverflow is a better place to do that, as there is a community of users that can help. If there is a specific bug you can point to, please refile with a minimal example and a description of the bug.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38631\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38631\">No</a>\n"]}, {"number": 38630, "title": "[Intel MKL] Enable BF16 Softmax/SoftmaxGrad", "body": "This PR enables BF16 Softmax/SoftmaxGrad in MKLDNN backend and fixes accuracy with accumulate type in reduce operation.\r\n\r\n* Enable BF16 Softmax with MKLDNN implementation, it implement with accumulate type inside.\r\n* Enable BF16 SoftmaxGrad with Eigen implementation. Main modification is to implement SumReducer with accumulate type. It helps to avoid accuracy loss for BF16 reduction. The new UT can show the difference W/O accumulate type.\r\n\r\nBackup:\r\nSoftmax with bfloat16 numeric would generally be used through auto-mixed-precision grappler pass. Default behavior of auto-mixed-precision is **NOT** to convert Softmax from float32 to bfloat16 (blacklisted for conversion). However, user could override default behavior to use bfloat16 precision of Softmax and SoftmaxGrad.\r\n\r\nSigned-off-by: Lu Teng teng.lu@intel.com", "comments": ["I separate BF16 Softmax from this PR because it has dependency with MKLDNN. We'll have another PR to submit forward part, thanks!", "@penpornk @Zantares We will update this PR soon and would like to put this on hold for a short time.\r\nThanks.", "@Zantares, @mdfaijul Any update on this PR? Please. Thanks!", "> @Zantares, @mdfaijul Any update on this PR? Please. Thanks!\r\n\r\nWe are trying to add new util function to have a better control of BF16 unit test. It will be updated soon in 1~2 days, thanks.", "Hi @gbaned @penpornk , we have fully updated this PR, now it's ready to be reviewed, thanks.\r\nReally sorry for the delay because we need to take care of accuracy for BF16 operations.", "@penpornk @Zantares  \r\nI am also from Intel. Wondering why this PR is reverted:https://github.com/tensorflow/tensorflow/commit/08b5c94d57ed1aed1120ffc0ec0a2450be61a144 after a merge : https://github.com/tensorflow/tensorflow/commit/cc088ebc3c0925a059c406efcfdaa46debbac821\r\nWas the revert an error? or Can you hint, how we could fix and resubmit this PR as gives good performance for some of the models.", "@nammbash @Zantares It was automatically rolled back because it broke our internal-only tests. I have been meaning to look at it but haven't had time yet. Will update soon. Sorry for the delay!", "> @nammbash @Zantares It was automatically rolled back because it broke our internal-only tests. I have been meaning to look at it but haven't had time yet. Will update soon. Sorry for the delay!\r\n\r\nWhat's kind of this error? Is it a crash or an accuracy test failure? Because I used accumulate type, so the accuracy will be improved for BF16 `SumReducer` and it may show different result if the internal case uses an old accuracy for comparison. Please tell us if any help we can provide, thanks!", "> What's kind of this error? Is it a crash or an accuracy test failure? Because I used accumulate type, so the accuracy will be improved for BF16 SumReducer and it may show different result if the internal case uses an old accuracy for comparison. Please tell us if any help we can provide, thanks!\r\n\r\nIt's an end-to-end model test. The end results differ significantly. I might not have time to dig into it before the 2.3 branch cut, so I'm thinking we can just wrap the new reducer with `INTEL_MKL && ENABLE_INTEL_MKL_BFLOAT16` for now. I'm confirming with my teammate if they are okay with this. If so, I'll bring back the changes.", "@penpornk Not sure I understand. If there is a problem, then wrapping it in MKL will also have the problem and test failures or shortcomings as opposed to fixing it. Correct?\r\nFYI @agramesh1 @Zantares \r\n\r\nReason is, clearly there is a failure/shortcoming with internal testing, that we are not aware of. Letting it be merged and run only for MKL would be accept shortcomings only for MKL module is better suited if we know what the exact shortcomings/failure are.\r\n\r\nSorry, just being careful with the software product quality from our side.", "@nammbash \r\n\r\n> If there is a problem, then wrapping it in MKL will also have the problem and test failures or shortcomings as opposed to fixing it. Correct?\r\n\r\nCorrect. Wrapping the reducer in `INTEL_MKL` only helps shielding the potential bug from vanilla TensorFlow, which will help this PR pass the currently failing test and be able to stay in the repo. It won't fix the potential bug in the MKL build. \r\n\r\n'potential' bug: There is a chance that the program works correctly but got vastly different results because of a severe error propagation in the original test result which uses just `bf16`. \r\n\r\nI asked @rmlarsen to help double check and he didn't see anything obviously wrong with the reducer either. We will bring back the PR with the `INTEL_MKL` guard around the reducer and will try to confirm if this is a bug or just the test's fault.\r\n\r\n\r\n\r\n", "@penpornk @nammbash my opinion: \r\n\r\nWe added BF16 MklSoftmax, SoftmaxGrad and Eigen SumReducer with accumulate type. Gradients will be different if the test is using BF16 Softmax without `INTEL_MKL` because accumulate type is only for MklSoftmax rather than Eigen Softmax.\r\nOtherwise it should be someting wrong if the failure is only appearing in SumReducer, . Please keep us posted if find anything wrong, thanks!", "@Zantares @nammbash Sorry for the late update! It was the test's fault and I fixed it now. I'm just waiting on a wider coverage test results to see if there's anything else failing. If things go well, I should be able to bring this PR back tomorrow."]}, {"number": 38629, "title": "Does not have sufficient slices for partitioned tensor", "body": "https://stackoverflow.com/questions/56930685/pywrap-tensorflow-checkpoint-reader-fails-for-ftrl-states-of-partitioned-variabl\r\ni meet the same problem, can anyone solve it? thanks", "comments": ["@yanghzcc \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Also include your TensorFlow version. \r\n\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n\r\n", "@yanghzcc,\r\nplease update as per above comment", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 38628, "title": "Autograph unrolling for loop", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows, Linux on cluster\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de410 2.1.0\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: reproducible on CPU or GPU\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\n\r\nIn a function decorated by `@tf.function`, I use a `for` loop to run an RNN step-by-step for beam search. There is a reduced example in the gist below.\r\n\r\nExecuting the function for the first time takes a long time (10 seconds in this minimal example, 300 seconds for my true model) and creates a very large graph (traced with `tf.summary.trace_on`; 3.5MB in this minimal example, 30-50MB for my true model, and more or less crashing Tensorboard when I try to open it.) \r\n\r\nIf I write the same loop as a `tf.while_loop`, first execution is nearly instant, and the resulting graph is healthy (180KB minimal example, 500KB for my true model.) `back_prop = True` or `False` makes almost no difference.\r\n\r\nLater iterations perform more or less identically for both.\r\n\r\nIt looks like instead of converting the `for` loop to `tf.while_loop`, AutoGraph unrolls it completely.\r\n\r\nNote that using `unroll=True` in the RNN (since it's only one step anyway) slashes AutoGraph tracing and graph size in half, perhaps not so surprisingly. \r\n\r\n**Describe the expected behavior**\r\nI would expect AutoGraph to interpret the `for` loop as a `tf.while_loop`.\r\n\r\nI understand there is always first execution overhead, but it seems very unnecessary here.\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1aALb-2wQoHIomJ7S5sLwJYmcciX5kpoM\r\nhttps://gist.github.com/meowcat/8b3b4b9c66264e685e339ff4e43af882\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@meowcat \r\n\r\nI tried to reproduce the issue in colab with TF 2.1.0.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/686c5a13c62ef638a6f7004d2781ac8b/untitled784.ipynb). Is this the expected behavior?.Thanks!", "Hi,\r\n\r\nyes, your gist reproduces the bug correctly. Specifically, the first iteration of the `decode_beam_forloop` takes 14 seconds, and it produces an `events.out.tfevents.....v2` with 3.5MB file size.\r\n\r\n", "Hi, please use tf.range() instead of range() https://www.tensorflow.org/api_docs/python/tf/range", "Hi,\r\nthanks, that works, but isn't the point of Autograph / tf.function that I shouldn't have to do this?", "We wanted to give users an option to create a graph with an unroll loop, and this is how users can control it.  Though yes, I agree that it's not too apparent and can be confusing.  We are discussing improving this but no decision at this point."]}, {"number": 38627, "title": "feature_column._is_v2_column always return true", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version (use command below): 2.1\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): bazel 2.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nUpgrade me code from TF1.14 to TF2.0, I got the unexpected error:\r\n```\r\n  File \"/use/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/canned/linear.py\", line 432, in linear_logit_fn\r\n    variables.remove(bias)\r\nValueError: list.remove(x): x not in list\r\n```\r\n\r\n**Describe the expected behavior**\r\nIt should works and train successfully.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport collections\r\nimport tensorflow.compat.v1 as tf\r\n\r\ntf.disable_v2_behavior()\r\ntf.disable_eager_execution()\r\n\r\nINPUT = [\"name\", \"id\", \"study\"]\r\nLABEL = [\"label\"]\r\nALL = LABEL + INPUT\r\n\r\ndef input_fn():\r\n    def parse_data(value):\r\n        input_defaults = [[\" \"] for i in range(1, 4)]\r\n        label_defaults = [['0']]\r\n        all_columns = collections.OrderedDict(zip(ALL, tf.io.decode_csv(value, record_defaults= label_defaults + input_defaults)))\r\n        labels = all_columns.pop(LABEL[0])\r\n        features = all_columns\r\n        return features, labels\r\n    # Extract lines from input files using the Dataset API.\r\n    dataset = tf.data.Dataset.from_tensor_slices([['a', 'b', 'c', '0'], ['b', 'b', 'c', '1']])\r\n    dataset = dataset.batch(1)\r\n    dataset = dataset.map(parse_data)\r\n    return dataset\r\n\r\ndef train():\r\n    input_column = []\r\n    for name in INPUT:\r\n        input_column.append(tf.feature_column.categorical_column_with_hash_bucket(\r\n            name, hash_bucket_size=3))\r\n    model = tf.estimator.LinearClassifier(feature_columns=input_column)\r\n    model.train(input_fn=lambda: input_fn())\r\n\r\nif __name__ == \"__main__\":\r\n    train()\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Check the code in https://github.com/tensorflow/estimator/blob/7ec4e5d9160c8a924d50c062b38c5153c4c86f29/tensorflow_estimator/python/estimator/canned/linear.py#L418-L445\r\nfeature_column_lib.is_feature_column_v2(feature_columns) seems always return true which is defined in \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/feature_column/feature_column_v2.py", "The issue is fixed in TF2.2.0rc3 with tf-estimator-2.2", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38627\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38627\">No</a>\n"]}, {"number": 38626, "title": "Add Post-training integer quantization converter for depth_to_space", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): 1.15.2 / 2.1\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\n    model_name = args.model_name + \"_integer_quant\" +\"_%d\" %(args.input_width) + \".tflite\"\r\n    converter.representative_dataset = representative_data_gen\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.inference_input_type = tf.uint8\r\n    converter.inference_output_type = tf.uint8\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n    \r\n    tflite_model_quant = converter.convert()\r\n    tflite_model_quant_file = tflite_models_dir/model_name\r\n    tflite_model_quant_file.write_bytes(tflite_model_quant)\r\n    print('Convert using full integer quantization DONE !!!')\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\nNote the output min/max is different from the input min/max for op RESIZE_BILINEAR at index 29 in subgraph 0. This is legal but should happens rarely.\r\nNote the output min/max is different from the input min/max for op RESIZE_BILINEAR at index 68 in subgraph 0. This is legal but should happens rarely.\r\nTraceback (most recent call last):\r\n  File \"gen_quan_tflite.py\", line 122, in <module>\r\n    main()    \r\n  File \"gen_quan_tflite.py\", line 116, in main\r\n    tflite_model_quant = converter.convert()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/lite.py\", line 993, in convert\r\n    inference_output_type)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/lite.py\", line 239, in _calibrate_quantize_model\r\n    inference_output_type, allow_float)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/optimize/calibrator.py\", line 78, in calibrate_and_quantize\r\n    np.dtype(output_type.as_numpy_dtype()).num, allow_float)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.py\", line 115, in QuantizeModel\r\n    return _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_QuantizeModel(self, input_py_type, output_py_type, allow_float)\r\nRuntimeError: Quantization not yet supported for op: DEPTH_TO_SPACE\r\n```\r\n_RuntimeError: Quantization not yet supported for op: DEPTH_TO_SPACE_\r\n\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\n# Put link here or attach to the issue.\r\n```\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- Producing wrong results and/or decrease in accuracy\r\n- Producing correct results, but the model is slower than expected (model generated from old converter)\r\n\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@WenguoLi  In order to expedite the trouble-shooting process, could you please provide the complete code along with the dataset to reproduce the issue reported here. Thanks!", "Hi saikumarchalla,\r\n\r\nI think the problem is  already obvious that  the Quantization of depth_to_space is not supported in TFLiteConverter.  Just add support, do it. OK?  Thanks!\r\n", "Hi saikumarchalla,\r\n\r\nDo you have any plans to support integer quantization of depth_to_space  in the  TFLiteConverter?\r\n\r\nThanks\r\nwenguo\r\n", "It seems that the kernel of depth_to_space has supported int8 quanzation,  here:\r\nhttps://github.com/tensorflow/tensorflow/blob/4ce6a9b7a46bbd574baa9a566d3ae5ae82f14b32/tensorflow/lite/kernels/depth_to_space.cc#L108\r\nOnly  not  supported on the TFLiteConverter. \r\n\r\nany updates ?\r\n\r\nThanks", "I got same issue, I am using TensorFlow 2.2.0\r\nWhen I use python API to convert  tflite model, show below log\uff0cas I know, \"toco\" is the old tflite converter, and not used for now. I tried to set \"converter.experimental_new_converter = True\", still show below log, seems the new converter is not enbaled, is there any way to solve this ?\r\n\r\n2020-06-11 16:35:50.293990: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 59 operators, 89 arrays (0 quantized)\r\n2020-06-11 16:35:50.294485: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 59 operators, 89 arrays (0 quantized)\r\n2020-06-11 16:35:50.295003: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 19 operators, 48 arrays (0 quantized)\r\n2020-06-11 16:35:50.295232: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 19 operators, 48 arrays (0 quantized)\r\n2020-06-11 16:35:50.295468: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:329] Total transient array allocated size: 58982400 bytes, theoretical optimal value: 39321600 bytes.\r\n2020-06-11 16:35:50.295735: F tensorflow/contrib/lite/toco/tflite/export.cc:315] Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops. Here is a list of operators for which you will need custom implementations:  DEPTH_TO_SPACE.", "@WenguoLi \r\nIs this still an issue, could you please try on the latest tf version and update.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38626\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38626\">No</a>\n"]}, {"number": 38625, "title": "ImageDataGenerator complains about lack of stratification but I'm doing regression", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): `yes` but it's basic\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `macOS 10.15.4`\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): `binary`\r\n- TensorFlow version (use command below): `v2.1.0-rc2-17-ge5bf8de410 2.1.0`\r\n- Python version: `3.7.7`\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nWhen using `ImageDataGenerator.flow()` with a `validation_split` argument, a `ValueError` exception is raised that states:\r\n\r\n> Training and validation subsets have different number of classes after the split. If your numpy arrays are sorted by the label, you might want to shuffle them.\r\n\r\nHowever, my labels data is continuous (floats) and I'm doing regression.\r\n\r\n**Describe the expected behavior**\r\n\r\nThis code should not throw. At the very least, it should be a warning and I should be able to silence it.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```python\r\ny = 500 * numpy.random.rand(200)  # labels are float values\r\ndatagen = ImageDataGenerator(validation_split=0.2)\r\ngen_train = datagen.flow(\r\n    x, y, shuffle=True, subset='training'\r\n)\r\n```\r\n\r\n", "comments": ["Looks like a problem in `keras-preprocessing` which is already reported here https://github.com/keras-team/keras-preprocessing/issues/214", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38625\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38625\">No</a>\n"]}, {"number": 38624, "title": "tf.keras h5 file to pb xx is not in graph", "body": "my tensorflow version is 2.1, i want from tf.keras .h5 model transform to pb file. but i get follow  issues. \r\n\r\n\r\n```\r\n\r\ndef freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\r\n\r\n    graph = session.graph\r\n    with graph.as_default():\r\n        freeze_var_names = list(set(v.op.name for v in tf.compat.v1.global_variables()).difference(keep_var_names or []))\r\n        output_names = output_names or []\r\n        output_names += [v.op.name for v in tf.compat.v1.global_variables()]\r\n        # Graph -> GraphDef ProtoBuf\r\n        input_graph_def = graph.as_graph_def()\r\n        if clear_devices:\r\n            for node in input_graph_def.node:\r\n                node.device = \"\"\r\n        frozen_graph = tf.compat.v1.graph_util.convert_variables_to_constants(session, input_graph_def,\r\n                                                      output_names, freeze_var_names)\r\n        return frozen_graph\r\n```\r\n\r\n```\r\nfrozen_graph = freeze_session(tf.compat.v1.keras.backend.get_session(),\r\n                              output_names=[out.op.name for out in model.outputs])\r\n\r\n```\r\nAssertionError                            Traceback (most recent call last)\r\n<ipython-input-10-ec0ab52adbd4> in <module>\r\n      3 \r\n      4 frozen_graph = freeze_session(tf.compat.v1.keras.backend.get_session(),\r\n----> 5                               output_names=[out.op.name for out in model.outputs])\r\n\r\n<ipython-input-8-b5cce8f2cb8f> in freeze_session(session, keep_var_names, output_names, clear_devices)\r\n     12                 node.device = \"\"\r\n     13         frozen_graph = tf.compat.v1.graph_util.convert_variables_to_constants(session, input_graph_def,\r\n---> 14                                                       output_names, freeze_var_names)\r\n     15         return frozen_graph\r\n\r\n~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/util/deprecation.py in new_func(*args, **kwargs)\r\n    322               'in a future version' if date is None else ('after %s' % date),\r\n    323               instructions)\r\n--> 324       return func(*args, **kwargs)\r\n    325     return tf_decorator.make_decorator(\r\n    326         func, new_func, 'deprecated',\r\n\r\n~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py in convert_variables_to_constants(sess, input_graph_def, output_node_names, variable_names_whitelist, variable_names_blacklist)\r\n    275   # This graph only includes the nodes needed to evaluate the output nodes, and\r\n    276   # removes unneeded nodes like those involved in saving and assignment.\r\n--> 277   inference_graph = extract_sub_graph(input_graph_def, output_node_names)\r\n    278 \r\n    279   # Identify the ops in the graph.\r\n\r\n~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/util/deprecation.py in new_func(*args, **kwargs)\r\n    322               'in a future version' if date is None else ('after %s' % date),\r\n    323               instructions)\r\n--> 324       return func(*args, **kwargs)\r\n    325     return tf_decorator.make_decorator(\r\n    326         func, new_func, 'deprecated',\r\n\r\n~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py in extract_sub_graph(graph_def, dest_nodes)\r\n    195   name_to_input_name, name_to_node, name_to_seq_num = _extract_graph_summary(\r\n    196       graph_def)\r\n--> 197   _assert_nodes_are_present(name_to_node, dest_nodes)\r\n    198 \r\n    199   nodes_to_keep = _bfs_for_reachable_nodes(dest_nodes, name_to_input_name)\r\n\r\n~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py in _assert_nodes_are_present(name_to_node, nodes)\r\n    150   \"\"\"Assert that nodes are present in the graph.\"\"\"\r\n    151   for d in nodes:\r\n--> 152     assert d in name_to_node, \"%s is not in graph\" % d\r\n    153 \r\n    154 \r\n\r\nAssertionError: dense_5/Identity is not in graph\r\n", "comments": ["@jetou \r\n\r\nLooks like code is incomplete. Request you to share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "[colab](https://drive.google.com/open?id=1F5qT5y_rG8sIyYOJNDpz-QsgRNACgPtf)", "I have tried on colab with TF version 2.1.0, 2.2.0-rc3 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/884adce46883d11342c8c4e14b887cbb/untitled785.ipynb). Thanks!", "@jetou The freeze graph APIs - freeze_graph.py and converter_variables_to_constants - will not be supported in TensorFlow 2.0.\r\nFor more information you can refer to the following [question](https://stackoverflow.com/questions/55562078/tensorflow-2-0-frozen-graph-support)", "I solve this problem in the tensorflow2.1 just use\r\n```\r\nfrom tensorflow.keras.models import Model, load_model\r\nmodel = load_model(MODEL_FULLPATH)\r\nmodel.save(MODEL_FULLPATH_MINUS_TERMINATION)\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38624\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38624\">No</a>\n"]}, {"number": 38623, "title": "Tensor eats up all my memory in gpu", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@Piyush008,\r\nCould you please check [this TensorFlow guide](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) to limit the GPU memory growth and let us know if it helps? Thanks!", "Any updates regarding this issue? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38623\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38623\">No</a>\n"]}, {"number": 38622, "title": "tf.custom_gradient expects an additional output when declaring temp Variable()", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Fedora 29\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de\r\n- Python version: Python 3.7.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nThanks in advance for your time.\r\n\r\nI'm attempting to implement a custom loss function. It requires storage of a temporary variable, and custom gradient. For many reasons, numerical stability and flexibility, I'd like to just implement the gradient by hand. The model here is just a `foo/bar` model.\r\n\r\nIt seems it\u2019s related to this issue: https://github.com/tensorflow/tensorflow/issues/31945\r\n\r\nI'm also happy to open a PR.\r\n\r\nHowever, I haven't been able to get the patch suggested to work, because I can't build from source via some `Bazel` issue.\r\n\r\nAny references are appreciated. \r\n\r\nI've also tried to implement the loss function as a subclass of type `Loss`, but was unsuccessful.\r\n\r\nThanks for all of the hard work that goes into this project.\r\n\r\n**Describe the expected behavior**\r\n\r\nRun model via EagerExecution. When using `GradientTape()` to evaluate gradients, I'm getting error: `ValueError: not enough values to unpack (expected 2, got 1)`. The loss function is only a function of one input, and thus only has one partial derivative, wrt to that input.\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Layer\r\nimport numpy as np\r\n\r\nclass Linear(Layer):\r\n  \"\"\"y = w.x + b\"\"\"\r\n\r\n  def __init__(self, units=32):\r\n      super(Linear, self).__init__()\r\n      self.units = units\r\n\r\n  def build(self, input_shape):\r\n      self.w = self.add_weight(shape=(input_shape[-1], self.units),\r\n                               initializer='random_normal',\r\n                               trainable=True)\r\n      self.b = self.add_weight(shape=(self.units,),\r\n                               initializer='random_normal',\r\n                               trainable=True)\r\n\r\n  def call(self, inputs):\r\n      return tf.matmul(inputs, self.w) + self.b\r\n\r\n\r\n@tf.custom_gradient\r\ndef loss_fn(x):\r\n    r = tf.Variable(tf.zeros([100]), dtype = tf.float32)\r\n    ## create r\r\n    def grad(df, variables = None):\r\n        return [df * 2 * tf.reduce_sum(r)]\r\n    \r\n    return tf.pow(tf.norm(r), 2), grad\r\n\r\nM = 100\r\nm = np.arange(0, M)\r\nx = [[m / M]]\r\n\r\nlinear_layer = Linear(10)\r\n\r\noptimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices(x)\r\n\r\nfor step, x in enumerate(dataset):\r\n    \r\n    with tf.GradientTape() as tape:\r\n        logits = linear_layer(x)\r\n        loss = loss_fn(x)\r\n\r\n    gradients = tape.gradient(loss, linear_layer.trainable_weights)\r\n    optimizer.apply_gradients(zip(gradients, linear_layer.trainable_weights))\r\n```\r\n", "comments": ["@drezap I tried to reproduce the issue but  i am facing different error. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/9ea522c31dd28140ae98549c42e8b3cf/untitled.ipynb).\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here. Thanks!\r\n", "@saikumarchalla Whoops! Here:\r\n\r\nThanks for your time!\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Layer\r\nimport numpy as np\r\n\r\nclass Linear(Layer):\r\n  \"\"\"y = w.x + b\"\"\"\r\n\r\n  def __init__(self, units=32):\r\n      super(Linear, self).__init__()\r\n      self.units = units\r\n\r\n  def build(self, input_shape):\r\n      self.w = self.add_weight(shape=(input_shape[-1], self.units),\r\n                               initializer='random_normal',\r\n                               trainable=True)\r\n      self.b = self.add_weight(shape=(self.units,),\r\n                               initializer='random_normal',\r\n                               trainable=True)\r\n\r\n  def call(self, inputs):\r\n      return tf.matmul(inputs, self.w) + self.b\r\n\r\n\r\n@tf.custom_gradient\r\ndef loss_fn(x):\r\n    r = tf.Variable(tf.zeros([100]), dtype = tf.float32)\r\n    ## create r\r\n    def grad(df, variables = None):\r\n        return [df * 2 * tf.reduce_sum(r)]\r\n    \r\n    return tf.pow(tf.norm(r), 2), grad\r\n\r\nM = 100\r\nm = np.arange(0, M)\r\nx = [[m / M]]\r\n\r\nlinear_layer = Linear(10)\r\n\r\noptimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices(x)\r\n\r\nfor step, x in enumerate(dataset):\r\n    \r\n    with tf.GradientTape() as tape:\r\n        logits = linear_layer(x)\r\n        loss = loss_fn(x)\r\n\r\n    gradients = tape.gradient(loss, linear_layer.trainable_weights)\r\n    optimizer.apply_gradients(zip(gradients, linear_layer.trainable_weights))\r\n```\r\n", "I could be able to replicate the issue on colab.Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/207a8bd425cf07e0f746cb32f012759c/untitled26.ipynb).Thanks!.", "That\u2019s exactly the error I\u2019m getting.\n\n\u201cnot enough values to unpack...\u201d\n\nOn Friday, April 24, 2020, saikumarchalla <notifications@github.com> wrote:\n\n> I could be replicate the issue on colab.Please find the gist here\n> <https://colab.research.google.com/gist/saikumarchalla/207a8bd425cf07e0f746cb32f012759c/untitled26.ipynb>\n> .Thanks!.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/38622#issuecomment-618794430>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACY543BC62OCG73OUT5VOWDROEIIBANCNFSM4MKQHG4A>\n> .\n>\n\n\n-- \nBest,\n\nAndre Zapico\n", "@saikumarchalla \r\n\r\nmore generally, the question is \"how to implement custom loss function that requires storage of a temp `tf.Variable`, iteration, and a custom gradient\".\r\n\r\nI could get custom loss functions that were trivial to \"compile\" with some models (this dummy one, and helpful friend on the TF forum's model), for example using `tf.mean()` and other simple functions, which i guess is using TF's autodiff. \r\n\r\nWhat I'd like to do, is add some exception that \"ignores\" TF's autodiff for the function body, and only takes into account the custom gradient I've implemented. The body will be complex and could blow out memory, and be unstable. And is probably no good with autodiff, via intuition.\r\n\r\nHowever, when I've tried to add custom gradients, won't \"compile\". Temp variable, won't \"compile\".\r\n\r\nI haven't investigated debugging the problem.\r\n", "any progress on this?", "@drezap Can you please explain little more about this loss functions (`loss_fn`)? There is a `grad` function inside loss function with two arguments and the return of `loss_fn` has `grad` without any arguments. Input `x` was also not used at all. \r\n\r\n```\r\n@tf.custom_gradient\r\ndef loss_fn(x):\r\n    r = tf.Variable(tf.zeros([100]), dtype = tf.float32)\r\n    ## create r\r\n    def grad(df, variables = None):\r\n        return [df * 2 * tf.reduce_sum(r)]\r\n    \r\n    return tf.pow(tf.norm(r), 2), grad\r\n```\r\n", "@jvishnuvardhan \r\nYeah, happy to, thank you.\r\n\r\nThe loss function implementation has 3 caveats:\r\n1. storage of a temporary variable,\r\n2. requires that I have nested for loops\r\n3. accumulation.\r\n\r\nNo work arounds.\r\n\r\nI haven't been able to get a model to run, using TF's datatypes, with an implementation of this loss function. If I have a long input sequence, I'm anticipating any expression graph generated would blow out memory, which is an additional reason I'd like to implement it myself.\r\n\r\nIf this still isn't clear, can I please email you a python implementation of the loss function? I'd just ask that you not publicize it.\r\n\r\nTLDR:\r\n>There is a grad function inside loss function\r\n 1. How do I include my own gradients for a loss function, and include this in a model?\r\n\r\nIf this isn't possible with the current modeling language, I'm willing to go as far as implementing `C` code within a compiled model, or something like that.\r\n", "Mind if I just email you the loss function?\n\nIt should make it clear\n\nOn Wednesday, April 29, 2020, Vishnuvardhan Janapati <\nnotifications@github.com> wrote:\n\n> @drezap <https://github.com/drezap> Can you please explain little more\n> about this loss functions (loss_fn)? There is a grad function inside loss\n> function with two arguments and the return of loss_fn has grad without\n> any arguments. Input x was also not used at all.\n>\n> @tf.custom_gradient\n> def loss_fn(x):\n>     r = tf.Variable(tf.zeros([100]), dtype = tf.float32)\n>     ## create r\n>     def grad(df, variables = None):\n>         return [df * 2 * tf.reduce_sum(r)]\n>\n>     return tf.pow(tf.norm(r), 2), grad\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/38622#issuecomment-620990964>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACY543AIS2BRXZSJ4HPMNGLRO6Y5PANCNFSM4MKQHG4A>\n> .\n>\n\n\n-- \nBest,\n\nAndre Zapico\n", "May be this issue is similar to [this @custom_gradient](https://github.com/tensorflow/tensorflow/issues/36168) issue. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38622\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38622\">No</a>\n"]}, {"number": 38621, "title": "TopKV2 comp", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nhttps://github.com/tensorflow/tensorflow/blob/ddabed4285d27785213322d05dcbe0ebc392849d/tensorflow/core/kernels/topk_op.cc#L183\r\nI found TopK use stable comp when I set sorted=False.\r\nIs it will faster if use unstable comp?\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": []}, {"number": 38620, "title": "Mask missing in restored Keras SavedModel", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **macOS 10.15.4** and **Linux Ubuntu 18.04**\r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version (use command below): **v2.1.0-rc2-17-ge5bf8de410 2.1.0** and **v2.2.0-rc3**\r\n- Python version: **3.6.0**\r\n- CUDA/cuDNN version: **N/A**\r\n- GPU model and memory: **N/A**\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nCalling the model directly returns correct results:\r\n```\r\narray([[1, 2], [3, 0]], dtype=int32)\r\n```\r\nBut if we save the model to a SavedModel and restore it, the new model fails to pass the mask from the first layer to the second layer, resulting in:\r\n```\r\nValueError: Could not find matching function to call loaded from the SavedModel.\r\n  Positional arguments (2 total):\r\n    * Tensor(\"inputs:0\", shape=(None, 2), dtype=int32)\r\n    * None\r\n  Keyword arguments: {}\r\n\r\nExpected these arguments to match one of the following 1 option(s):\r\n\r\nOption 1:\r\n  Positional arguments (2 total):\r\n    * TensorSpec(shape=(None, 2), dtype=tf.int32, name='inputs')\r\n    * TensorSpec(shape=(None, 2), dtype=tf.bool, name='mask')\r\n  Keyword arguments: {}\r\n```\r\n\r\nI've updated TensorFlow to v2.2.0-rc3 and nightly, the issue is still reproducible.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe model restored from the SavedModel should behave exactly the same as the original model: the first layer passing mask to the second layer.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nColab notebook:\r\n[https://colab.research.google.com/drive/1VlL9on7myJIX9xP2efIwvgESmWGafdfx](https://colab.research.google.com/drive/1VlL9on7myJIX9xP2efIwvgESmWGafdfx)\r\n\r\nPlain text:\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\nclass MyMasking(tf.keras.layers.Layer):\r\n\r\n    def call(self, inputs):\r\n        return inputs\r\n\r\n    def compute_mask(self, inputs, mask=None):\r\n        mask = tf.not_equal(inputs, 0)\r\n        return mask\r\n\r\n\r\nclass MyLayer(tf.keras.layers.Layer):\r\n\r\n    def call(self, inputs, mask=None):\r\n        return inputs\r\n\r\n\r\nsamples = tf.constant([[1, 2], [3, 0]], dtype=tf.int32)\r\nmodel = tf.keras.Sequential([MyMasking(), MyLayer()])\r\ntf.print(model.predict(samples))\r\n\r\nmodel.save('./temp_model', save_format='tf')\r\n\r\nnew_model = tf.keras.models.load_model('./temp_model')\r\ntf.print(new_model.predict(samples))\r\n```", "comments": ["I have tried on colab with TF version 2.1.0 , 2.2.0-rc3 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/2bfd3ff096ee1f5a05413a4544152ca2/untitled781.ipynb). Thanks!", "@peakji Agree with you. I think this is more related to saving and loading of mask in subclass model. I tried your approach (predict, *.save, *.load_model, predict) on a subclassed model without a mask, everything works as expected. [Here](https://colab.research.google.com/gist/jvishnuvardhan/e8e2105ec638271848e4bdf6e76fd3d3/38620.ipynb) is a gist for our reference. Thanks!", "Any update on this one? Just ran into the same issue when adapting to tf 2.x", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38620\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38620\">No</a>\n", "This is not an error. As we are using custom layers in the model, we need to use `custom_objects` while loading the model. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/b4355c380f72d14c4f78d026acee3223/untitled201.ipynb). Thanks!\r\n\r\n```\r\nnew_model = tf.keras.models.load_model('./temp_model',custom_objects={'MyMasking':MyMasking(),'MyLayer':MyLayer()})\r\ntf.print(new_model.predict(samples))\r\n```", "@jvishnuvardhan Thanks for circling back!\r\n\r\nThis is weird since we are saving the model in the SavedModel format. According to the [docs](https://www.tensorflow.org/guide/keras/save_and_serialize#savedmodel_format) and our experience, `custom_objects` are only required while using the old H5 format, which has a known limitation of not including computation graph of custom objects.\r\n\r\nI'm not sure why this could work for SavedModels, but still it is really inconvenient, if not impossible, to pass in `custom_objects` while using TensorFlow Serving.\r\n\r\nActually we've worked around this issue by replacing all functional calls into subclassed models and manually passing the masks all the way down. The new SavedModel works correctly in both Python and TF Serving, without using `custom_objects`."]}, {"number": 38619, "title": "InvalidArgumentError: 2 root error(s) found.   (0) Invalid argument:  indices[39,107] = 85525 is not in [0, 85525) \t [[node model_5/embedding_8/embedding_lookup (defined at <ipython-input-203-ee38e6490bb7>:2) ]] \t [[model_5/embedding_8/embedding_lookup/_14]]   (1) Invalid argument:  indices[39,107] = 85525 is not in [0, 85525)  in  Colab . ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nInvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument:  indices[39,107] = 85525 is not in [0, 85525)\r\n\t [[node model_5/embedding_8/embedding_lookup (defined at <ipython-input-203-ee38e6490bb7>:2) ]]\r\n\t [[model_5/embedding_8/embedding_lookup/_14]]\r\n  (1) Invalid argument:  indices[39,107] = 85525 is not in [0, 85525)\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n**This is the Error message I got,** \r\n`nvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-203-ee38e6490bb7> in <module>()\r\n      1 model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics= ['accuracy'])\r\n----> 2 a = model.fit( train_padded, y_train ,epochs=15, validation_data=(  test_padded  , y_test  ) , callbacks = learningrate_callback,   batch_size=128   )\r\n\r\n8 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     58     ctx.ensure_initialized()\r\n     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n---> 60                                         inputs, attrs, num_outputs)\r\n     61   except core._NotOkStatusException as e:\r\n     62     if name is not None:\r\n\r\nInvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument:  indices[39,107] = 85525 is not in [0, 85525)\r\n\t [[node model_5/embedding_8/embedding_lookup (defined at <ipython-input-203-ee38e6490bb7>:2) ]]\r\n\t [[model_5/embedding_8/embedding_lookup/_14]]\r\n  (1) Invalid argument:  indices[39,107] = 85525 is not in [0, 85525)\r\n\t [[node model_5/embedding_8/embedding_lookup (defined at <ipython-input-203-ee38e6490bb7>:2) ]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_train_function_10924]\r\n\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node model_5/embedding_8/embedding_lookup:\r\n model_5/embedding_8/embedding_lookup/10486 (defined at /usr/lib/python3.6/contextlib.py:81)\r\n\r\nInput Source operations connected to node model_5/embedding_8/embedding_lookup:\r\n model_5/embedding_8/embedding_lookup/10486 (defined at /usr/lib/python3.6/contextlib.py:81)\r\n\r\nFunction call stack:\r\ntrain_function -> train_function`\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\r\n**Here's a part of the source code ,** \r\n`\r\ninput_tensor = tf.keras.Input(  shape = ( maxlength , )  )\r\n\r\nembedd = tf.keras.layers.Embedding( input_dim = vocab     , output_dim = 300 , embedding_initalizer=  tf.keras.initializers.Constant(embedding_matrix) ,  input_length = maxlength  ) (input_tensor)\r\n\r\nconv1d_1 = tf.keras.layers.Conv1D (  filters = 16 , kernel_size = 3 , strides = 1 , padding = 'valid'  , activation = 'relu', kernel_initializer = 'he_uniform' ) (embedd)\r\nconv1d_2 =tf.keras.layers.Conv1D (  filters = 16 , kernel_size = 3 , strides = 1 , padding = 'valid'  , activation = 'relu', kernel_initializer = 'he_uniform' ) (embedd)\r\nconv1d_3 = tf.keras.layers.Conv1D (  filters = 16 , kernel_size = 3 , strides = 1 , padding = 'valid'  , activation = 'relu', kernel_initializer = 'he_uniform' ) (embedd)\r\nconcat_1 = tf.keras.layers.concatenate(   [conv1d_1, conv1d_2 , conv1d_3 ] ) \r\n`\r\n\r\nhere embedding_matrix has a shape of (85525, 300) (unknown token included while tokenizing)\r\nI'm trying to replicate the  Example at https://keras.io/examples/pretrained_word_embeddings/\r\nwith a different structure.\r\n", "comments": ["try this   tf.keras.layers.Embedding(input_dim = vocab+1,...)\r\n\r\ninput_dim: int > 0. Size of the vocabulary,\r\n      i.e. maximum integer index + 1.\r\n\r\n\r\n", "@zoomself  Thanks for replying !!! I tried it , but still getting the error, Strangely the error disappears if I run it in tensorflow 2.1,\r\nAnd I also went through similar issues, \r\n1) [https://github.com/tensorflow/tensorflow/issues/23698](url)\r\n2) [https://stackoverflow.com/questions/59867929/invalidargumenterror-2-root-errors-found-incompatible-shapes-in-tensorflow-t](url)\r\n\r\n", "@SangamSwadiK,\r\nI was able to run the example given on Keras without any issues. However, while running the sample code you have provided I'm facing an error stating `NameError: name 'vocab' is not defined`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/a23b2ad04bb953e42c9534c68d8b22eb/38619.ipynb#scrollTo=n_cot3240tQl&line=4&uniqifier=1). \r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the TensorFlow version you are using. Thanks!", "Closing this as I resolved the issue, The solution was to define a function \r\nmy_func (shape , dtype = None):\r\n   Line 1 ------\r\n   Line 2 ------\r\n\r\n   return embedding_matrix\r\nas mentioned at the end of this  https://keras.io/initializers/ \r\nEmbedding layer changes to \r\n\r\nembedd = tf.keras.layers.Embedding( input_dim = vocab , output_dim = 300 , embedding_initalizers = my_func , input_length = maxlength ) (input_tensor)\r\n\r\nHowever, **tf.keras.initializers.Constant(embedding_matrix)** should have worked, \r\nbut Since I cannot provide the full code as this is a part of an assignment. I'm closing this .\r\n\r\nThanks @amahendrakar , @zoomself  for replying.\r\n\r\n", "@SangamSwadiK Hello, could you please elaborate exactly how you resolved it and what exactly you wrote inside that function, am facing similar error and am not getting any clue to resolve it. ", "It can be resolved by changing the input_dim to vocabulary+1", "Yes as @nilay121 suggested changing the input_dim works for the error...", "I was facing this error while working on my CNN model on Google Colab, which was working fine before. Tuned the batch size and vocab size +1, but didn't help. Turns out it was related to the Colab GPU usage for my account, by changing to a new account I was able to resolve the error. Not sure if it might help others facing the same issue.\r\n\r\n`InvalidArgumentError:  indices[2,450] = 88585 is not in [0, 88584)\r\n\t [[node sequential_13/embedding_14/embedding_lookup (defined at <ipython-input-54-f7471122976f>:1) ]] [Op:__inference_train_function_22625]`"]}, {"number": 38618, "title": "Keras ProgbarLogger: OverflowError: cannot convert float infinity to integer", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n\r\n**Describe the current behavior**\r\n\r\nIt seems that if, at some time accuracy is equal 0 this code will error and stop training.\r\n\r\nWith TF 2.2-rc3 the following piece of code returns an error:\r\n\r\n```\r\nimport tensorflow as tf\r\ndata = tf.random.uniform(shape=(1000, 784),  maxval=15)\r\nlabels = tf.random.uniform(shape=(1000,10), maxval = 1, dtype=tf.int32)\r\nmodel = tf.keras.Sequential([\r\n  tf.keras.layers.Dense(32, activation = \"relu\"),\r\n  tf.keras.layers.Dense(10, activation = \"softmax\")\r\n])\r\nmodel.compile(loss = \"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\r\nmodel.fit(data, labels, callbacks = [tf.keras.callbacks.ProgbarLogger()])\r\n```\r\n\r\n> OverflowError: cannot convert float infinity to integer\r\n\r\n**Describe the expected behavior**\r\n\r\nWith current 2.1, no error is returned:\r\n\r\n```\r\nimport tensorflow as tf\r\ndata = tf.random.uniform(shape=(1000, 784),  maxval=15)\r\nlabels = tf.random.uniform(shape=(1000,10), maxval = 1, dtype=tf.int32)\r\nmodel = tf.keras.Sequential([\r\n  tf.keras.layers.Dense(32, activation = \"relu\"),\r\n  tf.keras.layers.Dense(10, activation = \"softmax\")\r\n])\r\nmodel.compile(loss = \"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\r\nmodel.fit(data, labels, callbacks = [tf.keras.callbacks.ProgbarLogger()])\r\n```\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nSee colab here for 2.2-rc3 version: https://colab.research.google.com/drive/1Cyu0pBYCYtoxzUs2JPc-oSRr_WG9PNQt\r\n\r\nAnd here with 2.1 working version:\r\n\r\nhttps://colab.research.google.com/drive/1Aq7vhnt91C8MbNP35RUrhszR9xomn-hM\r\n", "comments": ["Was able to reproduce the issue with Tf2.2.rc3.\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/616b04d15c11f18a139348b142c17414/untitled513.ipynb). Thanks! ", "@dfalbel hello, \r\nIf you want to use Keras ProgbarLogger in TensorFlow version 1, Then I would suggest two solution:-\r\n1. Either use **_tf.compat.v1.keras.callbacks.ProgbarLogger( )_**  instead of tf.keras.callbacks.ProgbarLogger( )\r\nor,\r\n2. Provide a parameter   **count_mode='steps'** i.e ProgbarLogger( count_mode='steps') because you will have to look for batches seen", "@gulshanrana10 I don't understand your answer.\r\n\r\n1.  I am not using TF 1, I am using 2.2-rc3. \r\n2. How can I use the `count_mode=\"samples\"`? I don't want to use the `steps` mode. Why does it work in 2.1 and not in 2.2-rc3?\r\n\r\n\r\n ", "@dfalbel Thanks for the issue!\r\n\r\nYou shouldn't be passing the `ProgbarLogger` directly in `Model.fit`, it is created automatically or not based on the `verbose` argument. This works for me:\r\n\r\n```python\r\nimport tensorflow as tf\r\ndata = tf.random.uniform(shape=(1000, 784),  maxval=15)\r\nlabels = tf.random.uniform(shape=(1000,10), maxval = 1, dtype=tf.int32)\r\nmodel = tf.keras.Sequential([\r\n  tf.keras.layers.Dense(32, activation = \"relu\"),\r\n  tf.keras.layers.Dense(10, activation = \"softmax\")\r\n])\r\nmodel.compile(loss = \"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\r\nmodel.fit(data, labels, verbose=1)\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38618\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38618\">No</a>\n", "Fair enough. Although, if it's unusable IMHO it should not be exported."]}, {"number": 38617, "title": "[2.2rc3] Distibuted training with Keras and ThreadPoolDataset runs out of memory", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2.0rc3 and tf-nightly\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: 10.1 / 7.6.5.32\r\n- GPU model and memory: 4 x NVIDIA V100 on GCP\r\n\r\n**Describe the current behavior**\r\n\r\nWhen running the code below with cached training and validations datasets in a multi-GPU environment (I am using a GCP VM with 312GB of memory and 4 NVIDIA V100s) memory increases during each validation run until the VM runs out of memory. This behaviour can be observed on 2.2.0-rc3 and on the latest nightly.\r\n\r\nIt looks like the validation dataset is not properly cached since I can still see network access during validation and the memory usage drops below the theoretical cached memory requirements after validation has finished and then increases linearly during the next validation round to a point larger than the memory usage in the previous epoch.\r\nIn the example below I intentionally use an very large validation set to make this memory increase very obvious and make training crash within the first 5 epochs. This behaviour can also be observed with other datasets, but the memory increase will be less noticible on smaller datsets.\r\n\r\n**In which cases is the memory usage still stable?**\r\nTo narrow down the possible causes for this I found two cases where this issue doesn't exist:\r\n\r\n1. When running on a single GPU memory usage is stable.\r\n\r\n2. Tensorflow Datasets uses a [`_PrivateThreadPoolDataset`](https://github.com/tensorflow/tensorflow/blob/8e0eecc8e396f8c1859b1b3954a89a41da8b5b45/tensorflow/python/data/ops/dataset_ops.py#L360-L362) by setting the [`experimental_threading.private_threadpool_size=16`](https://github.com/tensorflow/datasets/blob/8277548d5bdbc264a50d84ef702adc15bee8d4ae/tensorflow_datasets/core/tfrecords_reader.py#L62) as a default option. When disabling this option the memory usage is stable again. Unfortunately this is not a valid workaround in userland since the dataset option cannot be overwritten with `experimental_threading.private_threadpool_size=None` as it expects an integer.\r\n\r\n@yhliang2018 @tomerk @byronyi This seems to be a complicated interaction between `tf.data`, `tf.keras` and `tf.distribute`, do you have an idea what could cause this behaviour? Please let me know what additional information I could provide.\r\nI've ran into similar issues with `experimental_threading.private_threadpool_size` on TF 2.0.0 in the past though never investigated the root cause in detail, so this might not be an entirely new regression.\r\n\r\n**Describe the expected behavior**\r\n\r\nMemory usage should be stable after the first epoch.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\n\r\n\r\nbatch_size = 1024\r\n\r\ndataset = tfds.load(\r\n    \"imagenet2012:5.0.0\",\r\n    decoders={\"image\": tfds.decode.SkipDecoding()},\r\n    split=\"train\",\r\n    data_dir=\"gs://my-cloud-bucket\",\r\n)\r\n\r\nval_dataset = tfds.load(\r\n    \"imagenet2012:5.0.0\",\r\n    decoders={\"image\": tfds.decode.SkipDecoding()},\r\n    split=\"validation\",\r\n    data_dir=\"gs://my-cloud-bucket\",\r\n)\r\n\r\n\r\ndef _decode_and_center_crop(image_bytes):\r\n    \"\"\"Crops to center of image with padding then scales image_size.\"\"\"\r\n    shape = tf.image.extract_jpeg_shape(image_bytes)\r\n    image_height = shape[0]\r\n    image_width = shape[1]\r\n    image_size = 224\r\n\r\n    padded_center_crop_size = tf.cast(\r\n        (\r\n            (image_size / (image_size + 32))\r\n            * tf.cast(tf.minimum(image_height, image_width), tf.float32)\r\n        ),\r\n        tf.int32,\r\n    )\r\n\r\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\r\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\r\n    crop_window = tf.stack(\r\n        [offset_height, offset_width, padded_center_crop_size, padded_center_crop_size]\r\n    )\r\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\r\n    return tf.image.resize(image, [image_size, image_size], method=\"bicubic\")\r\n\r\n\r\ndef preprocessing(data):\r\n    return tf.cast(_decode_and_center_crop(data[\"image\"]), tf.float32), data[\"label\"]\r\n\r\ndataset = (\r\n    dataset.cache()\r\n    .map(preprocessing, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n    .batch(batch_size)\r\n    .prefetch(1)\r\n)\r\n\r\nval_dataset = (\r\n    val_dataset.cache()\r\n    .map(preprocessing, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n    .batch(batch_size)\r\n    .prefetch(1)\r\n)\r\n\r\nwith tf.distribute.MirroredStrategy().scope():\r\n    model = tf.keras.models.Sequential(\r\n        [\r\n            tf.keras.layers.GlobalMaxPool2D(input_shape=(224, 224, 3)),\r\n            tf.keras.layers.Dense(1000, activation=\"softmax\",),\r\n        ]\r\n    )\r\n\r\n    model.compile(\r\n        optimizer=\"adam\",\r\n        loss=\"sparse_categorical_crossentropy\",\r\n        metrics=[\"accuracy\", \"sparse_top_k_categorical_accuracy\"],\r\n    )\r\n\r\nmodel.fit(\r\n    val_dataset, epochs=5, validation_data=dataset,\r\n)\r\n```\r\n\r\n**Other info / logs**\r\n\r\nTo monitor memory usage over time tools like [`ytop`](https://github.com/cjbassi/ytop/) can be used.\r\n", "comments": ["Yes, I think those arguments are obsolete and should be removed. Thank you for your PR", "@lgeiger \r\nas there is a pr monitoring the issue, please let us know if we can move this to closed status", ">  as there is a pr monitoring the issue, please let us know if we can move this to closed status\r\n\r\n@Saduf2019 @Conchylicultor is refering to the PR on the [TensorFlow Datasets repo](https://github.com/tensorflow/datasets/pull/1893). This doesn't solve the underlying issue in TensorFlow though, so this issue is not resolved.", "@lgeiger you mentioned with 1 GPU memory was stable. Was that without using any distribution strategy? or with MirroredStrategy + 1 GPU? Also, what batch size was used for the 1 GPU case? \r\n\r\nAlso, does the issue go away if there is no validation dataset at all? \r\n", "> you mentioned with 1 GPU memory was stable. Was that without using any distribution strategy? or with MirroredStrategy + 1 GPU?\r\n\r\nThis was without using any distribution strategy.\r\n\r\n> Also, what batch size was used for the 1 GPU case?\r\n\r\nBoth `MirroredStrategy` and single GPU used batch size 1024.\r\n\r\n> Also, does the issue go away if there is no validation dataset at all?\r\n\r\nIf I recall correctly this only happens with a validation set, but I will explicitely test this later today again just to make sure.", "> Also, does the issue go away if there is no validation dataset at all?\r\n\r\n@guptapriya I just double checked and memory usage is completely flat when there is no validation dataset.", "@goldiegadde @guptapriya @ymodak After writing down this issue yesterday I discovered another related broken behaviour in the latest RC. I wrote down my findings in #38655", "Thanks a lot for filing this issue @lgeiger. Your analysis has helped us narrow down on a possible root cause which we will investigate further and fix. For now this does look like a regression in 2.2rc3. \r\nWe still don't understand why removing experimental_threading.private_threadpool_size helps though.\r\n\r\n", "@lgeiger to help us confirm that this is a regression, would you mind running your check against TF 2.1? both for the OOM issue, and the network usage. ", "> @lgeiger to help us confirm that this is a regression, would you mind running your check against TF 2.1? both for the OOM issue, and the network usage.\r\n\r\n@guptapriya This is a regression introduced between 2.1 and 2.2. I re-ran the checks with TF 2.1 and both issues are not present there (i.e. memory a network usage are completely stable).\r\n\r\nNote that with TF 2.1 I am running into #36126 instead so I am seeing a lot of unnecessary error messages each epoch, but I don't think this is related.", "Thanks @lgeiger for confirming, appreciate it! We know the root cause of the memory issue, and have a fix in the works. I am surprised the network usage is also stable though, because we thought the root cause for the network issue is something else.. maybe there is some interaction still between the 2 issues that we do not understand.\r\n\r\n", "@guptapriya Thanks for investigating! In my journey of upgrading from 2.0 to 2.2 I discovered my hopefully last GPU memory related problem. I wrote down the findings in #38675.", "Thanks @lgeiger, I ll ask some folks in XLA to take a look at that one.", "I just retested this with the latest nightly and it seems that this has been fixed by 7ebbab819e736319ec35b48e31f4d62fbad6626b as well. I'm still not sure what the interaction with the thread pool dataset is though.", "Hm, are you saying that after https://github.com/tensorflow/tensorflow/commit/7ebbab819e736319ec35b48e31f4d62fbad6626b, there is no increase in memory usage? \r\nThat seems a bit surprising because my understanding was that the fix for the memory leak and caching issues were actually different.. (and we still haven't submitted the fix for memory leak, though it is close to be submitted). \r\nIs memory usage completely stable after this change, or does it still increase a little bit? \r\n\r\n ", ">  Is memory usage completely stable after this change, or does it still increase a little bit?\r\n\r\nI can retest this more carefully tomorrow, but at least the memory increase seemed not to be as catastrophic. But I will double check, since I only briefly tested this.", "@lgeiger I got memory error on second epoch when evaluate val set. Then, I see on the memory usage, at the begining it only takes 35%, but it's going up time by time until I got OOM. Is it same problem with you?\r\n\r\nI use Multi Workers Mirrored Strategy btw", "@alimhanif are you using the latest nightly or 2.2.0rc3?", "yes, i'm using that version\r\n", "> yes, i'm using that version\r\n\r\nIf you are using `tensorflow==2.2.0rc3`, could you give the latest version of `tf-nightly` a try and see if the issue is still there?", "The latest version of TF Nightly is not able to use Tensorboard Callback. Will fix it then start the training :)\r\n\r\nTF Nightly version: `2.2.0.dev20200428`", "@lgeiger We've checked in the [fix](dac6d6ae7c381fb93438d8553b2596bfd4828c49) for the memory leak regression. I know you mentioned that the issue was not there post the cache fix. Let us know if you are still running into issues. Thanks!", "@lgeiger after training for 5 hrs, TF Nightly version: `2.2.0.dev20200428` shows the flat memory utilization.\r\n\r\nBut for sure, I have another error on this version (cant use Tensorboard callback). Will make another issue", "@anj-s @guptapriya Thank you very much for the fix. Reliably reproducing and tracking down this issue was rather painful, so I'm very happy that it is fixed now. I double checked with todays `tf-nightly` and the memory usage is perfectly stable for many epochs \ud83d\ude80 \r\n\r\nOn `tf-nightly` I now get the following warning, but I am not sure if this is related to the fix:\r\n```\r\ntensorflow/core/kernels/data/captured_function.cc:458]\r\nDisabling multi-device execution for a function that uses the experimental_ints_on_device attribute.\r\n```", "can we do a cherry picking this `2.2.0.dev20200428` version to the `2.2.0rc3`?\r\n\r\nI need to train my model with Tensorboard, btw\r\nTF 2.2 RC 3 is almost perfect, it's only have memory leak problem in my experiences\r\n\r\n\r\nI think latest TF Nighly that I use is not able to read on gcs bucket (especially for Tensorboard CallBack)\r\n", "> can we do a cherry picking this 2.2.0.dev20200428 version to the 2.2.0rc3?\r\n\r\n@alimhanif This is in progress and will probably be included in the stable release of 2.2.0. See #38996", "They close it and move to existing PR\r\nhttps://github.com/tensorflow/tensorflow/pull/39002\r\n", "@lgeiger and @alimhanif thanks for verifying that the issue is fixed with tf-nightly", "Closing this bug since the issue has been fixed. Thank you for reporting this!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38617\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38617\">No</a>\n", "Thanks, I can confirm that this doesn't show up in rc4 anymore"]}, {"number": 38616, "title": "[Intel MKL] Enabling Conv2D bfloat16 fusions", "body": "", "comments": []}, {"number": 38615, "title": "Not able to import tensorflow", "body": "**System information**\r\n- OS Platform: Windows10\r\n- TensorFlow installed from (source or binary): Installed tensorflow with Python's pip package manager\r\n- TensorFlow version: tensorflow 2\r\n- Python version: Python 3.7.4\r\n- Installed using virtualenv? pip? conda?: pip 20.0.2\r\n- Bazel version (if compiling from source): No\r\n- GCC/Compiler version (if compiling from source): No\r\n- CUDA/cuDNN version: No\r\n- GPU model and memory:\r\n\r\n**Describe the problem**\r\nI am not able to import tensorflow after installing it using pip.\r\nI get: ImportError: DLL load failed: The specified module could not be found.\r\nMicrosoft Visual C++ Redistributable for Visual Studio 2015 is installed.\r\nDesperate for help.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\A492267\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\A492267\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\A492267\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\A492267\\AppData\\Local\\Continuum\\anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\A492267\\AppData\\Local\\Continuum\\anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\A492267\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Users\\A492267\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\A492267\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\A492267\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\A492267\\AppData\\Local\\Continuum\\anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\A492267\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\A492267\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\A492267\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\A492267\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\A492267\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\A492267\\AppData\\Local\\Continuum\\anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\A492267\\AppData\\Local\\Continuum\\anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n```\r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n\n* For TF-GPU - See point 1\n* For TF-CPU - See point 2\n\n-----------------------------------------------------------------------------------------------\n\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\n*TF Version >= 1.13 requires CUDA 10.0 and TF Version < 1.13 (till TF 1.5) requires CUDA 9.0.*\n\n* If you have above configuration and using _**Windows**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n  * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n* If you have above configuration and using _**Ubuntu/Linux**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n  * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n* If error still persists then, apparently your CPU model does not support AVX instruction sets.\n  * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\n Therefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n* Try Google Colab to use TensorFlow.\n  * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true).You get pre-installed latest stable TF version. Also you can use```pip install``` to install any other preferred TF version.\n  * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n  * All you need is a good internet connection and you are all set.\n* Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*", "@MaximusFred, one solution is downloading and installing visual studio 2015-2019 x86 and x64 from here:[https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads)\r\nOther solution is downgrading tensorflow to 2.0:\r\n`pip install tensorflow==2.0`", "@MaximusFred \r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download [the latest microsoft visual c++ redistributable from here.](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads)\r\n.Also, please follow the instructions from to install from [Tensorflow website](https://www.tensorflow.org/install/source_windows).\r\n\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #36167 and see if it helps you.\r\nAlso, refer similar issues.\r\n#36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204\r\nThanks!", "Hi,\n\nThanks for your help.\nI did have Visual C++ x64 installed but not x86.\nAfter installation of Visual C++ x86, I could import tensorflow.\n\nBig thank you!\nFredrik\n\nDen fre 17 apr. 2020 kl 05:39 skrev Khimraj <notifications@github.com>:\n\n> @MaximusFred <https://github.com/MaximusFred>, one solution is\n> downloading and installing visual studio 2015-2019 x86 and x64 from here:\n> https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads\n> Other solution is downgrading tensorflow to 2.0:\n> pip install tensorflow==2.0\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/38615#issuecomment-615021245>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/APFXLDNMNOSDS3WL5SHYZWDRM7FPFANCNFSM4MKGXBBQ>\n> .\n>\n", "> @MaximusFred\r\n> \r\n> What is make/model of your cpu?\r\n> I suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\n> Make sure to download [the latest microsoft visual c++ redistributable from here.](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads)\r\n> .Also, please follow the instructions from to install from [Tensorflow website](https://www.tensorflow.org/install/source_windows).\r\n> \r\n> Please, check Your CPU/Python is on 32 bits?Please, refer #36167 and see if it helps you.\r\n> Also, refer similar issues.\r\n> #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204\r\n> Thanks!\r\n\r\nThank's for your help.\r\nIt was solved by installing both x64 and x86 of Visual C++. Earlier I had only x64.\r\n//Fredrik", "You need MSVC 2019 redistributable, not the 2015 one.", "Thanks Mihai!\n\nYes, I also realized when installing MSVC 2019 also recommended and linked\nearlier in this thread then it worked to import tensorflow.\n\nHave a nice weekend!\nFredrik\n\nDen fre 17 apr. 2020 kl 18:38 skrev Mihai Maruseac <notifications@github.com\n>:\n\n> You need MSVC 2019 redistributable, not the 2015 one.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/38615#issuecomment-615346835>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/APFXLDNDOMQLEC2XA5WBZXDRNCAY7ANCNFSM4MKGXBBQ>\n> .\n>\n", "Awesome. Closing the issue as it has been resolved.\r\n\r\nAlso, thanks for the additional data point: we need to tell people to also make sure they install both x64 and x86 versions of Visual C++", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38615\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38615\">No</a>\n", "Hi, Yes this solved the problem with importing tensorflow.\n\nThanks\nFredrik\n\nDen fre 17 apr. 2020 kl 19:51 skrev Mihai Maruseac <notifications@github.com\n>:\n\n> Awesome. Closing the issue as it has been resolved.\n>\n> Also, thanks for the additional data point: we need to tell people to also\n> make sure they install both x64 and x86 versions of Visual C++\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/38615#issuecomment-615374336>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/APFXLDJXRO4SRN4IWJLMKVDRNCHR7ANCNFSM4MKGXBBQ>\n> .\n>\n"]}, {"number": 38614, "title": "[Intel MKL] Fixing mkl_eager_op_rewrite compile error", "body": "This PR fixes build error in --config=mkl introduced by an API change in EagerOperation.", "comments": []}, {"number": 38613, "title": "in train_validation_split raise ValueError( ValueError: `validation_split` is only supported for Tensors or NumPy arrays, found: (array([[[[0.24705882]", "body": "**PyCharm\r\nDebian 10.3\r\nPython 3.8.2\r\nCuda 10.2\r\ncuDNN 7.6\r\nQuadro K5200d drivers 440.82\r\nTensorflow 2.2.0-rc2**\r\n\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\r\nimport pickle\r\n\r\npickle_in = open('X.pickle', 'rb')\r\nX = pickle.load(pickle_in)\r\npickle_in = open('y.pickle', 'rb')\r\ny = pickle.load(pickle_in)\r\nX = X/255.0\r\n\r\nmodel = Sequential()\r\nmodel.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\r\nmodel.add(Activation('relu'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\nmodel.add(Conv2D(256, (3, 3)))\r\nmodel.add(Activation('relu'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\nmodel.add(Flatten())\r\nmodel.add(Dense(64))\r\nmodel.add(Dense(1))\r\nmodel.add(Activation('sigmoid'))\r\nmodel.compile(loss='binary_crossentropy',\r\n              optimizer='adam',\r\n              metrics=['accuracy'])\r\nmodel.fit(X, y, batch_size=64, epochs=3, validation_split=0.3)\r\n\r\n\r\n2020-04-16 23:15:02.420846: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-04-16 23:15:02.438037: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:03:00.0 name: Quadro K5200 computeCapability: 3.5\r\ncoreClock: 0.771GHz coreCount: 12 deviceMemorySize: 7.94GiB deviceMemoryBandwidth: 179.05GiB/s\r\n2020-04-16 23:15:02.438206: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\r\n2020-04-16 23:15:02.439862: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-04-16 23:15:02.441466: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-04-16 23:15:02.441717: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-04-16 23:15:02.443456: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-04-16 23:15:02.444460: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-04-16 23:15:02.448122: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-04-16 23:15:02.448136: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1598] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-04-16 23:15:02.448470: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-04-16 23:15:02.480562: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2394460000 Hz\r\n2020-04-16 23:15:02.484972: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fd3dc000b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-04-16 23:15:02.484998: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-04-16 23:15:02.487117: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-04-16 23:15:02.487134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      \r\nTraceback (most recent call last):\r\n  File \"/home/dominik/PycharmProjects/TensorFlow/dog_cat/model.py\", line 25, in <module>\r\n    model.fit(X, y, batch_size=64, epochs=3, validation_split=0.3)\r\n  File \"/home/dominik/PycharmProjects/TensorFlow/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 66, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n  File \"/home/dominik/PycharmProjects/TensorFlow/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 732, in fit\r\n    data_adapter.train_validation_split((x, y, sample_weight),\r\n  File \"/home/dominik/PycharmProjects/TensorFlow/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\", **line 1314, in train_validation_split\r\n    raise ValueError(\r\nValueError: `validation_split` is only supported for Tensors or NumPy arrays, found:** (array([[[[0.24705882],\r\n         [0.3372549 ],\r\n         [0.36078431],", "comments": ["@domindominik,\r\nIn order to reproduce the error, could you please share both the pickle files you are using in the code? Thanks!", "@amahendrakar below\r\n\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport os\r\nimport cv2\r\nimport random\r\nimport pickle\r\nfrom tqdm import tqdm\r\n\r\nDATADIR = '/home/dominik/PetImages'\r\nCATEGORIES = ['Dog', 'Cat']\r\n\r\nfor category in CATEGORIES:\r\n    path = os.path.join(DATADIR, category)\r\n    for img in os.listdir(path):\r\n        img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\r\n        plt.imshow(img_array, cmap='gray')\r\n        plt.show()\r\n        break\r\n    break\r\nprint(img_array.shape)\r\n\r\nIMG_SIZE = 60\r\nnew_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\r\nplt.imshow(new_array, cmap='gray')\r\nplt.show()\r\n\r\ntraining_data = []\r\n\r\ndef create_training_data():\r\n    for category in CATEGORIES:\r\n\r\n        path = os.path.join(DATADIR, category)\r\n        class_num = CATEGORIES.index(category)\r\n\r\n        for img in tqdm(os.listdir(path)):\r\n            try:\r\n                img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\r\n                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\r\n                training_data.append([new_array, class_num])\r\n            except Exception as e:\r\n                pass\r\ncreate_training_data()\r\n\r\nrandom.shuffle(training_data)\r\n\r\nfor sample in training_data:\r\n    print(sample[1])\r\nX = []\r\ny = []\r\n\r\nfor features, label in training_data:\r\n    X.append(features)\r\n    y.append(label)\r\n\r\nX = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\r\n\r\npickle_out = open('X.pickle', 'wb')\r\npickle.dump(X, pickle_out)\r\npickle_out.close()\r\npickle_out = open('y.pickle', 'wb')\r\npickle.dump(y, pickle_out)\r\npickle_out.close()", "I got the same error and resolved it by changing y into a numpy array", "@domindominik,\r\nI was able to run the code without any issues with TF v2.2.0-rc3. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/0bde199280812170ec898e5f680127cc/38613.ipynb). Thanks!", "> I got the same error and resolved it by changing y into a numpy array\r\n\r\nI have no experience in coding, so can you show me how the code looks if you chance y into a numpy array?\r\n", "sure y=np.array(y) this is the only change you have to make.I have attatched the code below.Since I am getting some issue while uploading the folder .I have added a screenshot below\r\n![Screenshot (8)](https://user-images.githubusercontent.com/41339621/80211779-ede77d00-8653-11ea-9e86-c682f70dae22.png)\r\n\r\n\r\n", "To add to that, after making \"y\" an array, I had to append the labels in a different way to avoid a different error. Here's what I had:\r\n![Conv Fix](https://user-images.githubusercontent.com/42564419/80300835-c9bf9500-8754-11ea-9369-c9ed67cf9c87.png)\r\nI commented out the previous code that caused the post-numpy array-ization error.\r\nThis seemed to work on my custom dataset.", "Thank you very much!! Very helpful.", "> I got the same error and resolved it by changing y into a numpy array\r\n\r\nperfect", "> I was able to run the code without any issues with TF v2.2.0-rc3. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/0bde199280812170ec898e5f680127cc/38613.ipynb). Thanks!\r\n\r\n@domindominik,\r\nIs this still an issue? Please feel free to close the issue if resolved. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "I tried everything proposed up here but nothing worked. \r\n\r\nUntil i changed 'categorical_crossentropy' to 'sparse_categorical_crossentropy', in the compiling function.\r\n\r\nHope this will help someone. ", "I am still having issues.\r\nplease help..\r\n\r\nerror is:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:/Users/priyanshi burad/Desktop/color_train.py\", line 29, in <module>\r\n    model.fit(X,y,batch_size=32,validation_split=0.1)\r\n  File \"C:\\Users\\priyanshi burad\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 66, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n  File \"C:\\Users\\priyanshi burad\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 797, in fit\r\n    shuffle=False))\r\n  File \"C:\\Users\\priyanshi burad\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\", line 1338, in train_validation_split\r\n    functools.partial(_split, indices=train_indices), arrays)\r\n  File \"C:\\Users\\priyanshi burad\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\", line 617, in map_structure\r\n    structure[0], [func(*x) for x in entries],\r\n  File \"C:\\Users\\priyanshi burad\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\", line 617, in <listcomp>\r\n    structure[0], [func(*x) for x in entries],\r\n  File \"C:\\Users\\priyanshi burad\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\", line 1335, in _split\r\n    return array_ops.gather_v2(t, indices)\r\n  File \"C:\\Users\\priyanshi burad\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 180, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"C:\\Users\\priyanshi burad\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 4541, in gather_v2\r\n    batch_dims=batch_dims)\r\n  File \"C:\\Users\\priyanshi burad\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 180, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"C:\\Users\\priyanshi burad\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 4524, in gather\r\n    return gen_array_ops.gather_v2(params, indices, axis, name=name)\r\n  File \"C:\\Users\\priyanshi burad\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 3755, in gather_v2\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"C:\\Users\\priyanshi burad\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 6653, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: indices[0] = 0 is not in [0, 0) [Op:GatherV2]\r\n\r\n\r\n\r\nI don't know how to resolve it.\r\n\r\n\r\nmy code is:\r\n\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport os\r\nimport cv2\r\nD=\"C:\\\\Users\\\\priyanshi burad\\\\Desktop\\\\colour\"\r\nCATEGORIES=[\"red\",\"blue\",\"green\"]\r\n\r\nimg_size=50\r\ntraining_data = []\r\ndef create_training_data():\r\n    for category in CATEGORIES:\r\n        path = os.path.join(D,category)\r\n        class_num=CATEGORIES.index(category)\r\n        for img in os.listdir(path):\r\n            try:\r\n                img_array=cv2.imread(os.path.join(path,img))\r\n                img_size=50\r\n                new_array=cv2.resize(img_array,(img_size,img_size))\r\n                training_data.append([new_array,class_num])\r\n            except Exception as e:\r\n                pass\r\ncreate_training_data()\r\nprint(len(training_data))\r\n\r\n\r\n\r\nimport random\r\nrandom.shuffle(training_data)\r\nfor sample in training_data:\r\n    print(sample[1])\r\n\r\n\r\n\r\nX = []\r\ny = []\r\ny=np.array(y)\r\nfor features,label in training_data:\r\n    X.append(features)\r\n    np.array((y,label))\r\nX = np.array(X).reshape(-1,img_size, img_size,1)\r\n\r\n\r\n\r\nimport pickle\r\n\r\npickle_out= open(\"X.pickle\",\"wb\")\r\npickle.dump(X, pickle_out)\r\npickle_out.close()\r\n\r\npickle_out= open(\"y.pickle\",\"wb\")\r\npickle.dump(y,pickle_out)\r\npickle_out.close()\r\npickle_in=open(\"X.pickle\",\"rb\")\r\nX= pickle.load(pickle_in)\r\nprint(X[1])\r\n\r\n\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten,Conv2D, MaxPooling2D\r\nimport pickle\r\n\r\nX=pickle.load(open(\"X.pickle\",\"rb\"))\r\ny=pickle.load(open(\"y.pickle\",\"rb\"))\r\n\r\nX=X/255.0\r\n\r\nmodel= Sequential()\r\nmodel.add(Conv2D(64, (3,3), input_shape = X.shape[1:]))\r\nmodel.add(Activation(\"relu\"))\r\nmodel.add(MaxPooling2D(pool_size=(2,2)))\r\n\r\nmodel.add(Conv2D(64, (3,3)))\r\nmodel.add(Activation(\"relu\"))\r\nmodel.add(MaxPooling2D(pool_size=(2,2)))\r\n\r\nmodel.add(Flatten())\r\nmodel.add(Dense(64))\r\n\r\nmodel.add(Dense(1))\r\nmodel.add(Activation('sigmoid'))\r\n\r\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\r\n\r\nmodel.fit(X,y,batch_size=32,validation_split=0.1)\r\n\r\n\r\n\r\n\r\n \r\n\r\n", "> I am still having issues.\r\n> please help..\r\n\r\n@priyanshib22,\r\nPlease submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the issue template, so that we can track it there. Thanks!", "> sure y=np.array(y) this is the only change you have to make.I have attatched the code below.Since I am getting some issue while uploading the folder .I have added a screenshot below\r\n> ![Screenshot (8)](https://user-images.githubusercontent.com/41339621/80211779-ede77d00-8653-11ea-9e86-c682f70dae22.png)\r\n\r\nAfter changing to array you will face range issues while doing the split.", "I get a \"IndexError: list index out of range\" message on the model.fit(X, y, batch_size=5, epochs=3, validation_split=0.1). Did anybody else get this issue, and how do I fix this?", "@sruthisentil,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!", "> \r\n> \r\n> I got the same error and resolved it by changing y into a numpy array\r\n\r\nThis one worked ^^", "import numpy as np\r\nX = np.array(X)\r\ny= np.array(y)\r\nthat will change your X,y  to numpy arrays and solve the problem\r\nthis would be also done just before feeding your neural network to make sure that the NN received numpy arrays ", "My answer is not related to your issue since you used Sequential. But this could be helpful to others.\r\n\r\nI got the same error when I used the Keras functional API. _I **named both my NN outputs and my training set `x`** by mistake._\r\n\r\nSo the trainset **`x`** was overwritten by the NN output **`x`** which made it of type:\r\n`tensorflow.python.keras.engine.keras_tensor.KerasTensor` \r\n\r\nand resulted in this error:\r\n`# ValueError:  ``validation_split`` is only supported for Tensors or NumPy arrays, found following types in the input: [<class 'tensorflow.python.keras.engine.keras_tensor.KerasTensor'>]`\r\n\r\n\r\n", "> My answer is not related to your issue since you used Sequential. But this could be helpful to others.\r\n> \r\n> I got the same error when I used the Keras functional API. _I **named both my NN outputs and my training set `x`** by mistake._\r\n> \r\n> So the trainset **`x`** was overwritten by the NN output **`x`** which made it of type:\r\n> `tensorflow.python.keras.engine.keras_tensor.KerasTensor`\r\n> \r\n> and resulted in this error:\r\n> `` # ValueError: ``validation_split`` is only supported for Tensors or NumPy arrays, found following types in the input: [<class 'tensorflow.python.keras.engine.keras_tensor.KerasTensor'>] ``\r\n\r\n\r\n**You can solve this by converting the y to array ,then drop into the y to y.pickle file**\r\n\r\nX = []\r\ny = []\r\n\r\nfor features,label in training_data:\r\n    X.append(features)\r\n    y.append(label)\r\n\r\nprint(X[0].reshape(-1, IMG_SIZE, IMG_SIZE, 1))\r\n\r\nX = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\r\ny = np.array(y)", "**80% + percent validation accuracy achieved by increasing feature map to 128 and using more epoch (=5)**\r\n\r\n_use below model_\r\n\r\nmodel = Sequential()\r\n\r\nmodel.add( Conv2D(64, (3, 3), input_shape=X.shape[1:]) )\r\nmodel.add(Activation('relu'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\n\r\nmodel.add(Conv2D(64, (3, 3)))\r\nmodel.add(Activation('relu'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\n\r\nmodel.add(Conv2D(128, (3, 3)))\r\nmodel.add(Activation('relu'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\n\r\nmodel.add(Conv2D(128, (3, 3)))\r\nmodel.add(Activation('relu'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\n\r\nmodel.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\r\n\r\nmodel.add(Dense(64))\r\n\r\nmodel.add(Dense(1))\r\nmodel.add(Activation('sigmoid'))\r\n\r\nmodel.compile(loss='binary_crossentropy',\r\n              optimizer='adam',\r\n              metrics=['accuracy'])\r\n\r\nm=model.fit(X, y, batch_size=32, epochs=5, validation_split=0.1)", "hey guys \r\nI am too working the same project so it would be help full for me if any of you please share the whole code please!!!!\r\nthank you"]}, {"number": 38612, "title": "No Upsampling2D layer in Tensorflow C++", "body": "I am Rahul, an M.Sc. in Digital Engineering student at University of Magdeburg, Germany. I am currently pursuing my master thesis and it involves developing a U-Net (a neural network designed for biomedical image segmentation, you can read more about it here : https://arxiv.org/pdf/1505.04597.pdf). And deploying/loading it in a C++ Application. The U-Net has a couple of layers namely Concatenate, UpSampling2D which are not common in Image processing. \r\n\r\nTo deploy it I am guessing I need all the layers used U-Net available in Tensorflow C++ here : https://www.tensorflow.org/api_docs/cc/group/nn-ops\r\n\r\nI can't find Upsampling2D there. \r\n\r\nAm I looking at the wrong location? \r\n\r\nI would like to know if  loading the protobuf and predicting using it is independent of the structure and layers used in the model I developed or it only supports standard layers such as flatten, convolution, maxpool etc\u2026.", "comments": ["@mortalrahu There is a workaround I used to use which combines the API of \r\n```\r\ntf.transpose \r\ntf.image.resize\r\n```\r\nyou can take a try", "@mortalrahu\r\nplease update on the above comment", "@Leslie-Fang \r\n\r\nThanks for the response.\r\n\r\nAre there any open source projects that are completely built from scratch in Tensorflow C++ , meaning trained and working completely in C++.  (Especially if it has the Upsampling alternative usage you suggest) \r\n\r\nor projects where m\u00f3del is trained in python, frozen, saved and then loaded in C++ to do predictions", "@mortalrahu Sorry, haven't used the C++ API before,", "Upsampling2d is just tf.image.resize; it's implemented using a bunch of different ops depending on what type of resizing you want to do (see the code in https://github.com/tensorflow/tensorflow/blob/fadfcdf27ba3973525b7ef24f5d2cb10a13231bd/tensorflow/python/ops/image_ops_impl.py#L1316 ); those individual ops are all available through the TF C++ API.\r\n\r\nWe do recommend though that you use python to build your model and C++ to execute it, as taking gradients and doing optimization in the TF C++ API directly doesn't work very well right now."]}, {"number": 38611, "title": "transformers.modeling_tf_utils  - loading weights file from cache at None (!)", "body": "Hi, desperately need your help! \r\nWe are trying to run TF and transformers (bert-multilingual-base-uncased) in our docker environment. Starting the training, we are getting the following error:\r\n\r\n`INFO     transformers.modeling_tf_utils  - loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-tf_model.h5 from cache at None`\r\n\r\nThis happens, however, only with weights, config file and vocab are linked successfully.\r\nFor instance:\r\n`transformers.configuration_utils  - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at bert_cache/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.c7892120c5a9b21e515abc904e398dbabddf9510b122f659063cbf361fe16868`\r\n\r\nAny idea why this can happen?\r\n\r\nMany thanks in advance for any feedback and help!\r\n\r\n\r\n\r\n", "comments": ["@almois \r\nissue reported does not seem related tensorflow, please share steps executed/code before this error was faced.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38611\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38611\">No</a>\n"]}, {"number": 38610, "title": "How to right pad data in Tensorflow Transform to feed into a LSTM ?", "body": "I am building a data pipeline using Tensorflow Transform using Apache Beam. Inside the preprocessing function I am generating vocabulary which is converting my input sentences to list of integers.\r\nMy data has 3 features:\r\nColumn 1 = Context (dtype = String) (Sentences of varying Length)\r\nColumn 2 = Utterance (dtype = String) (Sentences of varying Length)\r\nColumn 3 = Label (0/1)\r\nI want to right pad my list of integers as soon as they are generated in the preprocessing function below to a max length of 160 words.\r\nExample:\r\n\"I love Pizza\" --> [34, 67, 78] --> Max length I want = 10\r\nthen I want [34, 67, 78,0,0,0,0,0,0,0] and if the length of my sentence is already greater than 10, then I want to trim the extra portion to make it length  =10\r\n\r\nNow to use tf.keras.preprocessing.sequence.pad_sequences you need as input a list of sequences but as shown below, my mapped_context and mapped_utterance are tf.int64 tensors. So I am not able to use the padding functionality of Keras.\r\nCan someone please help me achieve this ?\r\n\r\nThe reference code I am following is Tensorflow Sentiment Analysis Example:\r\nhttps://github.com/tensorflow/transform/blob/599691c8b94bbd6ee7f67c11542e7fef1792a566/examples/sentiment_example.py\r\n------------------------------------------------------------------------------------------------------------\r\n\r\nMy code's preprocessing function is below:\r\n\r\n![image](https://user-images.githubusercontent.com/27782859/79493187-1a195300-7fef-11ea-9d5c-560787e137b8.png)\r\n\r\n\r\nMy Dataset is below:\r\n\r\n![image](https://user-images.githubusercontent.com/27782859/79533358-a5bdce80-8045-11ea-8e40-d820cf935f42.png)\r\n\r\n", "comments": ["@anantvir, you can right pad a sequence using following code.\r\n```\r\nfrom keras.preprocessing.sequence import pad_sequences\r\npadded = pad_sequences(sequences, padding='post', maxlen=10, truncating='post')\r\n```\r\n", "> \r\n> \r\n> @anantvir, you can right pad a sequence using following code.\r\n> \r\n> ```\r\n> from keras.preprocessing.sequence import pad_sequences\r\n> padded = pad_sequences(sequences, padding='post', maxlen=10, truncating='post')\r\n> ```\r\n\r\nThat was my question. To use pad_sequences, the sequences argument should be a list of sequences\r\nexample\r\n[\r\n  [83, 91, 1, 645, 1253, 927],\r\n  [73, 8, 3215, 55, 927],\r\n  [71, 1331, 4231]\r\n]\r\nIn my case I do not have this list. What I have is a tensor. Because it is inside the preprocessing_fn in Apache Beam pipeline. Please see the code screenshot attached.", "can someone please update me on this issue ?  @ravikyram @khimraj ?", "@anantvir, You need to first use Tokenizer to convert text into sequences. For more detail refer [this](https://towardsdatascience.com/multi-class-text-classification-with-lstm-using-tensorflow-2-0-d88627c10a35) tutorial.", "> \r\n> \r\n> @anantvir, You need to first use Tokenizer to convert text into sequences. For more detail refer [this](https://towardsdatascience.com/multi-class-text-classification-with-lstm-using-tensorflow-2-0-d88627c10a35) tutorial.\r\n\r\n@khimraj  Are you familiar with Tensorflow Transform ? I am asking this because as I explained above, I want to make all sentences of equal length inside the preprocessing_fn in the apache beam pipeline. We cannot use Tokenizer as it also requires a list of sequences. That is WHAT I DONT HAVE. I want to pad tensors not list of sequences.\r\n\r\nPlease refer to: \r\nhttps://github.com/tensorflow/transform/blob/599691c8b94bbd6ee7f67c11542e7fef1792a566/examples/sentiment_example.py\r\n\r\nThis is the code I am using as a reference and I want to make all sentences of equal length inside the preprocessing_fn starting at line 181.\r\n\r\nI hope you are following what I want to convey.\r\n\r\nPlease let me know in case you need any further information.", "@anantvir, Sorry for misunderstanding your query. I will try to solve problem after reading your references.", "Any updates ?", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\nThanks!\r\n"]}, {"number": 38609, "title": "Docker ERROR: Could not find a version that satisfies the requirement tensorflow-cpu", "body": "I am trying to build a docker image and when the docker build reaches the tensorflow-cpu requirement, I get the following error: \r\n\r\n```\r\nERROR: Could not find a version that satisfies the requirement tensorflow-cpu (from socialworks-nn==0.0.7->-r requirements.txt (line 16)) (from versions: none)\r\nERROR: No matching distribution found for tensorflow-cpu (from socialworks-nn==0.0.7->-r requirements.txt (line 16))\r\n```\r\nHere is my Dockerfile:\r\n\r\n```\r\nFROM python:3.6-alpine3.7\r\n\r\nRUN apk add --no-cache python3-dev \\\r\n    && pip3 install --upgrade pip\r\n\r\nRUN apk --no-cache add git\r\nRUN apk add mariadb-dev\r\n\r\nWORKDIR /socialworks-api\r\n\r\nCOPY . /socialworks-api\r\n\r\nRUN pip3 --no-cache-dir install -r requirements.txt\r\n```\r\n\r\nMay I ask what should I run to install tensorflow? My application must run on Python 3.6. I am new to Docker, this is my first build. Also, I have tried commenting out tensorflow, but I am receiving the same error with numpy.\r\n\r\nI have also tried running this command in my Dockerfile:\r\n\r\n`RUN python3 -m pip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl`\r\n\r\nBut after that, I would receive this error: \r\n\r\n`ERROR: tensorflow_gpu-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl is not a supported wheel on this platform.`\r\n\r\nAny help will be appreciated", "comments": ["@DeivydasLipskis,\r\nCould you please check [this comment](https://github.com/tensorflow/tensorflow/issues/37316#issuecomment-595389330) from a similar issue and let us know if it helps? Thanks!", "I have added the line to my dockerfile:\r\n\r\n`RUN python3.6 -m pip install --upgrade pip`\r\n\r\nBut the error is still the same:\r\n```\r\nERROR: Could not find a version that satisfies the requirement tensorflow-cpu (from socialworks-nn==0.0.7->-r requirements.txt (line 16)) (from versions: none)\r\nERROR: No matching distribution found for tensorflow-cpu (from socialworks-nn==0.0.7->-r requirements.txt (line 16))\r\n```\r\nUnfortunately, it did not help.", "So the issue was that in order to install tensorflow-cpu, I need to use \r\n`FROM tensorflow/tensorflow:2.1.0-py3`\r\nor `FROM nvidia/cuda:10.1-cudnn7-runtime-ubuntu18.04`\r\nBut using one of these, the size of the image gets really big. May I ask what FROM I need to use to make the size as small as possible? I only need to use tensorflow-cpu.\r\n", "For help installing TensorFlow, you'll get better support by asking a question on [Stack Overflow](https://stackoverflow.com/questions/tagged/tensorflow). If by doing so you find an installation bug, please file another issue here. Good luck!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38609\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38609\">No</a>\n"]}]