[{"number": 20631, "title": "Broken TFLite Model Benchmark Tool Example", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.9\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.10.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: n/a\r\n- **Exact command to reproduce**: adb shell taskset f0 /data/local/tmp/benchmark_model --graph=/data/local/tmp/mobilenet.tflite --input_layer=\"input\" --input_layer_shape=\"1,224,224,3\" --num_threads=-1\r\n\r\n### Describe the problem\r\nI'm attempting to replicate an Android TFLite model benchmark from https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/tools/benchmark.\r\n\r\nThe benchmark works on my own model but the example is broken. The tflife graph is the mobilenet graph from https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_v1_224_android_quant_2017_11_08.zip\r\n\r\nThe benchmark otherwise works with my own tflite models.\r\n\r\n### Source code / logs\r\nI'm running the following command:\r\n\r\n```\r\nadb shell taskset f0 /data/local/tmp/benchmark_model --graph=/data/local/tmp/mobilenet.tflite --input_layer=\"input\" --input_layer_shape=\"1,224,224,3\" --num_threads=-1\r\n```\r\n\r\n```\r\nWARNING: linker: \"/data/local/tmp/benchmark_model\" unused DT entry: type 0xf arg 0x6ca\r\nnative : benchmark_model.cc:469 Graph: [/data/local/tmp/mobilenet.tflite]\r\nnative : benchmark_model.cc:470 Init ops:\r\nnative : benchmark_model.cc:471 Input layers: [input]\r\nnative : benchmark_model.cc:472 Input shapes: [1,224,224,3]\r\nnative : benchmark_model.cc:473 Input types: [float]\r\nnative : benchmark_model.cc:474 Output layers: [output:0]\r\nnative : benchmark_model.cc:475 Target layers: []\r\nnative : benchmark_model.cc:476 Num runs: [1000]\r\nnative : benchmark_model.cc:477 Inter-inference delay (seconds): [-1.0]\r\nnative : benchmark_model.cc:478 Inter-benchmark delay (seconds): [-1.0]\r\nnative : benchmark_model.cc:480 Num threads: [-1]\r\nnative : benchmark_model.cc:481 Benchmark name: []\r\nnative : benchmark_model.cc:482 Output prefix: []\r\nnative : benchmark_model.cc:483 Show sizes: [0]\r\nnative : benchmark_model.cc:484 Warmup runs: [1]\r\nnative : benchmark_model.cc:251 Loading TensorFlow.\r\nnative : benchmark_model.cc:258 Got config, 0 devices\r\ncan't determine number of CPU cores: assuming 4\r\ncan't determine number of CPU cores: assuming 4\r\nnative : benchmark_model.cc:496 Initialized session in 0.00239s\r\nnative : benchmark_model.cc:327 Running benchmark for max 1 iterations, max -1 seconds without detailed stat logging, with -1s sleep between inferences\r\nnative : benchmark_model.cc:306 Error during inference: Invalid argument: Session was not created with a graph before Run()!\r\nnative : benchmark_model.cc:348 Failed on run 0\r\nnative : benchmark_model.cc:566 Timing failed with Invalid argument: Session was not created with a graph before Run()!\r\n```\r\n", "comments": ["Solved by re-building\r\n```\r\n bazel build --config=monolithic --config=android_arm64 --cxxopt='--std=c++11' --copt=-DTFLITE_PROFILING_ENABLED tensorflow/contrib/lite/tools/benchmark:benchmark_model\r\n```"]}, {"number": 20630, "title": "Eager execution guide: using GradientTape with keras.model and tf.keras.layer", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Fedora 28\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**:  1.9\r\n- **Python version**:  3.6\r\n- **Bazel version**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:N/A\r\n\r\n### Describe the problem\r\n\r\nThe eager execution doc \r\n\r\nhttps://www.tensorflow.org/programmers_guide/eager \r\n\r\ndoes not provide a simple complete example that shows how to use gradient tape optimization with keras layers (instead of tfe.Variables). \r\nIt would be great if that could be added, as I seem to be getting gradients that are None when I try to replace tfe.Variables in the following, running, example (copied from the doc but even more simplified for ease of experimentation):\r\n\r\n```\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\ntfe = tf.contrib.eager\r\nn = 10\r\nx = tf.random_normal([n, 2])\r\nnoise = tf.random_normal([n, 2])\r\ny = x * 3 + 2 + noise\r\n\r\nclass Model(tf.keras.Model):\r\n  def __init__(self):\r\n    super(Model, self).__init__()\r\n    self.W = tfe.Variable(5., name='weight')\r\n    self.B = tfe.Variable(10., name='bias')\r\n  def predict(self, inputs):\r\n    return inputs * self.W + self.B\r\n\r\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\r\n\r\nmodel = Model()\r\nwith tf.GradientTape() as tape:\r\n  error = model.predict(x) - y\r\n  loss_value = tf.reduce_mean(tf.square(error))\r\ngradients = tape.gradient(loss_value, model.variables)\r\nprint(gradients)\r\noptimizer.apply_gradients(zip(gradients, model.variables),\r\n                            global_step=tf.train.get_or_create_global_step())\r\n\r\n```\r\n\r\nby a Keras layer instead of weight and a bias (based on, but again simplified, the MNISTModel from the doc):\r\n\r\n```\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\n\r\nn = 10\r\nx = tf.random_normal([n, 2])\r\nnoise = tf.random_normal([n, 2])\r\n\r\ny = x * 3 + 2 + noise\r\n\r\nclass Model(tf.keras.Model):\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        self.dense1 = tf.keras.layers.Dense(units = 1, activation='relu')\r\n    def call(self, inputs):\r\n        return self.dense1(x)\r\n\r\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\r\n\r\nmodel = Model()\r\nwith tf.GradientTape() as tape:\r\n  error = model.predict(x) - y\r\n  loss_value = tf.reduce_mean(tf.square(error))\r\ngradients = tape.gradient(loss_value, model.variables)\r\n# now gradients are None\r\nprint(gradients)\r\noptimizer.apply_gradients(zip(gradients, model.variables),\r\n                            global_step=tf.train.get_or_create_global_step())\r\n                            \r\n```\r\n\r\nIn this version, the gradients are None. Same if I use another construct from the doc (again, simplified):\r\n\r\n```\r\nmodel = tf.keras.Sequential([\r\n  tf.keras.layers.Dense(1, input_shape=(2,))  # must declare input shape\r\n])\r\n```\r\n\r\nIt would be great if the doc could be extended to show a complete tf.keras.layer example with GradientTape. Thank you!\r\n\r\n\r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Sorry, filled in now.", "@skeydan : Thanks for bringing this up.  The documentation you're pointing to is a bit misleading, we shouldn't override `predict` there, we should override `call` since `tf.keras.Model.predict` isn't meant to be overridden.\r\n\r\nI'll update the getting started guide. And as for other examples, there are a bunch in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/eager/python/examples, and we'll be adding more to www.tensorflow.org soon.\r\n\r\nAs for an explanation, `tf.keras.Model.predict` returns a `numpy.ndarray`, not a `tf.Tensor`. And the TensorFlow libraries cannot differentiate through numpy conversions or operations. So, if you change your second example to use `model(x)` instead of `model.predict(x)`, it should work out fine:\r\n\r\n```python\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\n\r\nn = 10\r\nx = tf.random_normal([n, 2])\r\nnoise = tf.random_normal([n, 2])\r\n\r\ny = x * 3 + 2 + noise\r\n\r\nclass Model(tf.keras.Model):\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        self.dense1 = tf.keras.layers.Dense(units = 1, activation='relu')\r\n    def call(self, inputs):\r\n        return self.dense1(x)\r\n\r\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\r\n\r\nmodel = Model()\r\nwith tf.GradientTape() as tape:\r\n  error = model(x) - y  # NOT model.predict(x) - y\r\n  loss_value = tf.reduce_mean(tf.square(error))\r\ngradients = tape.gradient(loss_value, model.variables)\r\nprint(gradients)\r\noptimizer.apply_gradients(zip(gradients, model.variables),\r\n                            global_step=tf.train.get_or_create_global_step())\r\n```\r\n\r\nAnd similarly, if we change the first example to override `call` instead of `predict`, it would work out fine too (which is the documentation fix to be made):\r\n\r\n```python\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\ntfe = tf.contrib.eager\r\nn = 10\r\nx = tf.random_normal([n, 2])\r\nnoise = tf.random_normal([n, 2])\r\ny = x * 3 + 2 + noise\r\n\r\nclass Model(tf.keras.Model):\r\n  def __init__(self):\r\n    super(Model, self).__init__()\r\n    self.W = tfe.Variable(5., name='weight')\r\n    self.B = tfe.Variable(10., name='bias')\r\n  # Overriding call not predict\r\n  def call(self, inputs):\r\n    return inputs * self.W + self.B\r\n\r\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\r\n\r\nmodel = Model()\r\nwith tf.GradientTape() as tape:\r\n  error = model(x) - y\r\n  loss_value = tf.reduce_mean(tf.square(error))\r\ngradients = tape.gradient(loss_value, model.variables)\r\nprint(gradients)\r\noptimizer.apply_gradients(zip(gradients, model.variables),\r\n                            global_step=tf.train.get_or_create_global_step())\r\n```\r\n\r\nFYI @random-forests @fchollet @pavithrasv @yashk2810 - regarding documentation or other improvements that we might be able to make to reduce confusion between `tf.keras.Model.predict(x)` and `tf.keras.Model(x)`.\r\n\r\nHope that helps.", "Thank you for the detailed explanation!", "+ FYI @raymond-yuan (who is also working on creating some examples for our documentation.)"]}, {"number": 20629, "title": "Improve the documentation for tf.tables_initializer", "body": "### System information\r\n\r\nNo relevant for this issue.\r\n\r\n### Describe the problem\r\n\r\nThe current documentation for [`tf.tables_initializer`](https://www.tensorflow.org/api_docs/python/tf/tables_initializer) states\r\n\r\n> Returns an Op that initializes all tables of the default graph.\r\n\r\nAs a beginner, it is not clear to me why we would need tables in TF. So, I think the documentation should add a few examples of use-cases where tables are created (initialized) and used. In other words, the documentation should briefly answer the questions \r\n\r\n1. Why do we need tables in TF? \r\n2. What does it even mean to initialize a table? \r\n   1. In which cases would tables need (or not) to be initialized? \r\n3. What kind of tables are these? Hash-tables?\r\n4. In general, what are examples of use-cases where tables are used?\r\n\r\nIn the documentation, there's also \"See the guide: Variables > Sparse Variable Updates\", but [Variables > Sparse Variable Updates](https://www.tensorflow.org/api_guides/python/state_ops#Sparse_Variable_Updates), if I understood correctly, doesn't answer the questions above.\r\n\r\nGiven that I don't think I am very qualified to answer the questions above, I decided to open this issue, instead of submitting a PR.\r\n\r\n### Source code / logs\r\n \r\nNo relevant for this issue.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@ysuematsu can you provide some more detailed examples? \r\n\r\nIf you have any more extensive examples you could put them in [docs_src/api_guides](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/docs_src/api_guides/python) \r\n\r\napi_guides are automatically linked to by any api-pages for python objects they mention.\r\n\r\nBut we would welcome examples from anyone.", "I found something useful to understand table in https://www.tensorflow.org/guide/low_level_intro\r\n> Feature columns can have internal state, like layers, so they often need to be initialized. Categorical columns use tf.contrib.lookup internally and these require a separate initialization op, tf.tables_initializer.", "Good point @jayhenry. \r\n\r\nAnyone want to send a PR adding a link from [the source file](https://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/python/ops/lookup_ops.py#L63) to  that [low_level_intro](https://www.tensorflow.org/guide/low_level_intro)", "It has been 26 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 20628, "title": "prefetch_to_device doesn't overlap copy(HtoD) with computation and also may fail", "body": "\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04.4 LTS\r\n- **TensorFlow installed from (source or binary)**: pip3\r\n- **TensorFlow version (use command below)**:1.8.0\r\n- **Python version**: 3\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:9.0\r\n- **GPU model and memory**:GTX 1070\r\n- **Exact command to reproduce**:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nx = tf.placeholder(shape=[None, 128], dtype=np.float32)\r\ny = tf.placeholder(shape=[None], dtype=np.float32)\r\n\r\nds = tf.data.Dataset.from_tensor_slices((x, y))\r\nds = ds.repeat()\r\nds = ds.batch(1024)\r\n############################################\r\nprefetch = 1024 # if it's bigger than 512 it results in an error\r\n############################################\r\nds = ds.apply(tf.contrib.data.prefetch_to_device(\"/gpu:0\", prefetch))\r\nit = ds.make_initializable_iterator()\r\n\r\nw = tf.get_variable(\r\n                    name='w',\r\n                    initializer=tf.contrib.layers.xavier_initializer(),\r\n                    shape=[128, 1],\r\n                    dtype=tf.float32)\r\nb = tf.get_variable(\r\n                    name='b',\r\n                    initializer=tf.contrib.layers.xavier_initializer(),\r\n                    shape=[1],\r\n                    dtype=tf.float32)\r\n\r\noptimizer = tf.train.AdamOptimizer(1e-3)\r\n\r\nnext_x, next_y = it.get_next()\r\nprediction = tf.matmul(next_x, w) + b\r\ntrain = optimizer.minimize((prediction - next_y)**2)\r\n\r\nwith tf.Session() as session:\r\n    x_val = np.random.normal(size=(1024*1024, 128)).astype(np.float32)\r\n    y_val = np.random.normal(size=(1024 * 1024)).astype(np.float32)\r\n    session.run(tf.global_variables_initializer())\r\n    session.run(it.initializer, feed_dict={x: x_val, y: y_val})\r\n    for _ in range(1024):\r\n        session.run(train)\r\n```\r\n\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\nv1.8.0-0-g93bc2e2072 1.8.0\r\n\r\n### Describe the problem\r\n\r\nIf I specify `prefetch` size to 128 I don't see any parallel data copy(HtoD) overlapping gpu computation(check screenshot and nvidia profiler report).\r\n<img width=\"1072\" alt=\"screen shot 2018-07-08 at 17 36 25\" src=\"https://user-images.githubusercontent.com/2821871/42421527-412a3124-82d7-11e8-94e7-1f9114d88cec.png\">\r\n\r\n\r\nAnd If I increase it it fails\r\n```\r\n$ python3 test.py \r\n2018-07-08 17:32:18.472300: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-07-08 17:32:18.550265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-07-08 17:32:18.550841: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: \r\nname: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.683\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 7.93GiB freeMemory: 7.83GiB\r\n2018-07-08 17:32:18.550852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0\r\n2018-07-08 17:32:18.711860: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-07-08 17:32:18.711888: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 \r\n2018-07-08 17:32:18.711907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N \r\n2018-07-08 17:32:18.712052: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7568 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2018-07-08 17:32:25.310286: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:650] failed to record completion event; therefore, failed to create inter-stream dependency\r\n2018-07-08 17:32:25.310311: I tensorflow/stream_executor/stream.cc:4737] stream 0x1448efe0 did not memcpy host-to-device; source: 0x7f8feae97000\r\n2018-07-08 17:32:25.310316: E tensorflow/stream_executor/stream.cc:309] Error recording event in stream: error recording CUDA event on stream 0x1448f080: CUDA_ERROR_DEINITIALIZED; not marking stream as bad, as the Event object may be at fault. Monitor for further errors.\r\n2018-07-08 17:32:25.310323: E tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_DEINITIALIZED\r\n2018-07-08 17:32:25.310328: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:208] Unexpected Event status: 1\r\nAborted (core dumped)\r\n```\r\n\r\nIt'd be nice to have some canonical examples how to use it.\r\n\r\n\r\n[prefetch_on_device.nvvp.zip](https://github.com/tensorflow/tensorflow/files/2173870/prefetch_on_device.nvvp.zip)\r\n", "comments": ["/CC @rohan100jain", "@rohan100jain any progress? ", "So usually I'd recommend a prefetch buffer size of something in the range of 1-4. 1 should mostly work because you want to pipeline the next computation while the current one is going on. With this large number, you're just creating a lot of memory pressure on the GPU without much benefit. \r\n\r\nAlso, can you increase the size of the data that is being transferred over? Make it some really large tensor. Then we might be able to see the overlap. ITs possible that overlap is possible even in this scenario but the copied tensor is small and is competing with other CPU work (not shown on this GPU profile).", "Ok, I've changed the script a bit. BTW now it's TF 1.10\r\nBriefly I set `prefetch_to_device` buffer to 1(it could be described in the documentation that it's multiplier for batch size) and input size from 128 to 1024 to make computation more time consuming. \r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nx = tf.placeholder(shape=[None, 1024], dtype=np.float32)\r\ny = tf.placeholder(shape=[None], dtype=np.float32)\r\n\r\nds = tf.data.Dataset.from_tensor_slices((x, y))\r\nds = ds.repeat()\r\nds = ds.batch(1024)\r\nds = ds.apply(tf.contrib.data.prefetch_to_device(\"/gpu:0\", 1))\r\nit = ds.make_initializable_iterator()\r\n\r\nw = tf.get_variable(\r\n                    name='w',\r\n                    initializer=tf.contrib.layers.xavier_initializer(),\r\n                    shape=[1024, 1],\r\n                    dtype=tf.float32)\r\nb = tf.get_variable(\r\n                    name='b',\r\n                    initializer=tf.contrib.layers.xavier_initializer(),\r\n                    shape=[1],\r\n                    dtype=tf.float32)\r\n\r\noptimizer = tf.train.AdamOptimizer(1e-3)\r\n\r\nnext_x, next_y = it.get_next()\r\nprediction = tf.matmul(next_x, w) + b\r\ntrain = optimizer.minimize((prediction - next_y)**2)\r\n\r\ngpu_options = tf.GPUOptions(force_gpu_compatible=True)\r\nconfig = tf.ConfigProto(gpu_options=gpu_options, allow_soft_placement=True)\r\nwith tf.Session() as session:\r\n    x_val = np.random.normal(size=(1024*1024, 1024)).astype(np.float32)\r\n    y_val = np.random.normal(size=(1024 * 1024)).astype(np.float32)\r\n    session.run(tf.global_variables_initializer())\r\n    session.run(it.initializer, feed_dict={x: x_val, y: y_val})\r\n    for _ in range(1024):\r\n        session.run(train)\r\n\r\n\r\n```\r\n\r\nI don't see any improvements at all. \r\nAlso there're a big gap after each iteration(it's longer than H2D copy plus computation itself). Is it a TF or/and python overhead?\r\n\r\nI've implemented before overlapping copying into a device with computation in c++. And result was predictable and much faster. Does make sense to use TF C++ API? \r\n\r\nA new screenshot and profiling report are attached. \r\n![screen shot 2018-08-17 at 12 29 14](https://user-images.githubusercontent.com/2821871/44262247-a66fe300-a21a-11e8-9f3d-18ed16ed957f.png)\r\n[prefetch_on_device.nvvp 2.zip](https://github.com/tensorflow/tensorflow/files/2296907/prefetch_on_device.nvvp.2.zip)\r\n\r\n\r\n \r\n\r\n", "Same issue here, a ctx synchronize always blocks the H2D memory copy before the complete of GPU computing. @sh1ng I saw a lot of space between two H2D in your profiling results, maybe you should reduce the cpu processing time (e.g. data preprocssing) first, which worked for me", "@jiazhe0909 I've posted simple example and there's no processing at all, besides \r\n```\r\nds = tf.data.Dataset.from_tensor_slices((x, y))\r\nds = ds.repeat()\r\nds = ds.batch(1024)\r\n```\r\nIf it causes intensive CPU processing, there's an issue in `tf.data.Dataset`, but I'd like to get some feedback from the team. \r\n\r\n\r\n", "Same issue with tf 2.1.0, using model.fit and tf.keras, no overlap even when using copy_to_device.", "Same issue with tf 2.4.0, no overlap when using copy_to_device or prefetch_to_device", "Please see https://github.com/tensorflow/tensorflow/issues/43905#issuecomment-823675760", "@sh1ng \r\nCould you please try on latest stable version of tf and let us know if this is still an issue.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/20628\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/20628\">No</a>\n"]}, {"number": 20627, "title": "Predictions from provided tflite files of same Mobilenet model do not match", "body": "Hi Guys. I downloaded the Mobilenet_v1_1.0_224 and Mobilenet_v1_1.0_224_quant from the links provided in the documentation. I then use the tflite models provided in both of these for prediction using the tflite interpeter [Link](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/python_api.md#using-the-interpreter-from-a-model-file-) I am able to make the predictions. However, the tflite predictions from the 'quant' version are very random. The float tflite model makes the correct prediction while the quantised tfite model makes very weird predictions. I even comapred the answers to normal frozen models provided and the quantized tflite model does not match the predictions at all while the float tflite makes all the predictions same as the frozen .pb model. \r\n\r\nI then tried the same thing with different mobilenet versions but the same observations.\r\nKindly help.\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.10 dev\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**:\r\n\r\n\r\n\r\n### Describe the problem\r\nI then use the tflite models provided in both of these for prediction using the tflite interpeter [Link](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/python_api.md#using-the-interpreter-from-a-model-file-) I am able to make the predictions. However, the tflite predictions from the 'quant' version are very random. The float tflite model makes the correct prediction while the quantised tfite model makes very weird predictions. I even comapred the answers to normal frozen models provided and the quantized tflite model does not match the predictions at all while the float tflite makes all the predictions same as the frozen .pb model. \r\n\r\nI then tried the same thing with different mobilenet versions but the same observations.\r\n\r\nI am also making sure that I use the exact same images for both so that is not an issue just in case.\r\nKindly help.\r\n\r\n### Source code / logs\r\nimg = np.array(PIL.Image.open(image_path).resize((224, 224))).astype(np.uint8) / 128 - 1\r\n\r\n\timg = img.reshape(1,224,224,3)\r\n\t#print (img.shape)\r\n\t\r\n\tstart = time.time()\r\n\r\n\tinterpreter.set_tensor(input_details[0]['index'], img)\r\n\r\n\tinterpreter.invoke()\r\n\tend = time.time()\r\n\r\n\toutput_data = interpreter.get_tensor(output_details[0]['index'])\r\n\t#output_data_1 = interpreter.get_tensor(output_details[1]['index'])\r\n\r\n\t#print(output_data.argmax(),output_data.max())\r\n\tlabel_map = imagenet.create_readable_names_for_imagenet_labels()  \r\n\tprint(\"Top 1 Prediction: \", output_data.argmax(),label_map[output_data.argmax()], output_data.max(), k+1)\r\n", "comments": ["@suharshs, could you please handle this issue.", "```\r\nimg = np.array(PIL.Image.open(image_path).resize((224, 224))).astype(np.uint8) / 128 - 1\r\n```\r\n\r\nthis looks wrong as it will make the range -1 to 1 . you want to feed 0,255 which is probably what it was\r\nso try feeding\r\n```\r\nimg = np.array(PIL.Image.open(image_path).resize((224, 224))).astype(np.uint8)\r\n```\r\nbut check by checking mins and maxes of the img array once done ...\r\n", "I am closing this issue due to inactivity, if aselle's instructions do not work, please reopen. Thanks!", "Hi @aselle it worked thanks !! Sorry for the inactivity @suharshs ", "There is one more issue I am facing. The prediction time for INT TFLITE is more than that of FLOAT TFLITE. I am also having this for the ssd-mobilenet INT TFLITE and FLOAT TFLITE. Any suggestions why this is happening ?\r\n", "Sure, could you create an new issue describing what you are seeing and how to reproduce the latency numbers? ", "Sure. Thanks a lot\r\n", "Hi, \r\nI downloaded the tar file of Mobilenet v1.0-224 from the list of hosted quantized TF .pb and .lite model. I am unable to get the same numbers when I run inference for .pb model and .tflite model. \r\n\r\nI am feeding image net data resized to (224,224,3). When I run inference and check the argmax of the output vector, the indices of .pb model and .tflite model are different. \r\n\r\nI did use image.astype(np.uint8). When I print the interpreter's input and output details, I do see a tuple of numbers in the quantization key of the tensor dictionary. \r\nFor example: \r\n[{'name': 'input', 'shape': array([  1, 299, 299,   3], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.0078125, 128), 'index': 315}]\r\n [{'name': 'output', 'shape': array([   1, 1001], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.07597498595714569, 53), 'index': 316}]\r\n\r\nHow do I use this information to match the results of the .pb and .tflite model? Any advice would be really appreciated! \r\n\r\n\r\n", "Hi.\r\nmy predictions of model lite are very different from true label(category_id)in val coco annotation 2017.\r\nwhy?\r\nmy model is wrong?category ids are not true labels?\r\nplease help me"]}, {"number": 20626, "title": "Dataset.concatenate() agrees to concat dictionaries with different keys", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS 10.13.5\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: NA\r\n\r\n### Describe the problem\r\n\r\nIt seems like Dataset.concatenate will concatenate datasets of dictionaries with different keys (values from the second key will be concatenated to the first one). \r\n(a small demo is attached)\r\n\r\nI've looked at `python/data/util/nest.py` and in `_recursive_assert_same_structure` it seems like `_yield_value` only returns values for dictionaries. \r\nIs that intended? I would expect it to either fail or put None in the missing fields. \r\n\r\n### Source code / logs\r\n\r\nThe following code: \r\n```\r\nds1 = tf.data.Dataset.from_tensor_slices({'f1': list(range(20)), 'f2': [1]*20})\r\nds2 = tf.data.Dataset.from_tensor_slices({'f2': list(range(100,120)), 'f3': [2]*20})\r\ndataset = ds1.concatenate(ds2).batch(5)\r\ncurr_batch = dataset.make_one_shot_iterator().get_next()\r\n\r\nwith tf.Session() as sess:\r\n  for _ in range(8):\r\n    data = sess.run(curr_batch)\r\n    print(data)\r\n```\r\nResults in:\r\n```\r\n{'f1': array([0, 1, 2, 3, 4], dtype=int32), 'f2': array([1, 1, 1, 1, 1], dtype=int32)}\r\n{'f1': array([5, 6, 7, 8, 9], dtype=int32), 'f2': array([1, 1, 1, 1, 1], dtype=int32)}\r\n{'f1': array([10, 11, 12, 13, 14], dtype=int32), 'f2': array([1, 1, 1, 1, 1], dtype=int32)}\r\n{'f1': array([15, 16, 17, 18, 19], dtype=int32), 'f2': array([1, 1, 1, 1, 1], dtype=int32)}\r\n{'f1': array([100, 101, 102, 103, 104], dtype=int32), 'f2': array([2, 2, 2, 2, 2], dtype=int32)}\r\n{'f1': array([105, 106, 107, 108, 109], dtype=int32), 'f2': array([2, 2, 2, 2, 2], dtype=int32)}\r\n{'f1': array([110, 111, 112, 113, 114], dtype=int32), 'f2': array([2, 2, 2, 2, 2], dtype=int32)}\r\n{'f1': array([115, 116, 117, 118, 119], dtype=int32), 'f2': array([2, 2, 2, 2, 2], dtype=int32)}\r\n```\r\n", "comments": ["ping @mrry ", "Nagging Assignee @shivaniag: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @shivaniag: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Good catch, thanks for finding this bug. Fixed this :)", "This would now fail, closing this issue. ", "I still face this issue , tensorflow 2.2. \r\n\r\n![image](https://user-images.githubusercontent.com/7234284/88757030-f602fd00-d119-11ea-8d83-0f80fd49f293.png)\r\n", "From the snippet, this failed. This is expected behavior, the issue was that earlier it agreed to concat dictionaries with different keys.\r\nPlease let us know if this behavior is not what you expect/you have a use-case for the behavior different than failing trying to concatenate dataset with different keys. "]}, {"number": 20625, "title": "How do I represent a TShape in the TF_Output object ?", "body": "In tesnorflow/core array_op.cc file,the Reshape defined as follows [Source Code](https://github.com/tensorflow/tensorflow/blob/35287be3bb7daa0448af064f5d005a25201d6853/tensorflow/core/ops/array_ops.cc#L1274)\r\n```c\r\nREGISTER_OP(\"Reshape\")\r\n    .Input(\"tensor: T\")\r\n    .Input(\"shape: Tshape\")\r\n    .Output(\"output: T\")\r\n    .Attr(\"T: type\")\r\n    .Attr(\"Tshape: {int32, int64} = DT_INT32\")\r\n    .SetShapeFn([](InferenceContext* c) {\r\n      return SetOutputShapeForReshape(c);\r\n    });\r\n```\r\nin tensorflow/c_api.cc file,the TF_AddInput and TF_Output define as follows\r\n```c\r\nvoid TF_AddInput(TF_OperationDescription* desc, TF_Output input) {\r\n  desc->node_builder.Input(&input.oper->node, input.index);\r\n}\r\n```\r\n```c\r\ntypedef struct TF_Output {\r\n  TF_Operation* oper;\r\n  int index;  // The index of the output within oper.\r\n} TF_Output;\r\n```\r\nWhen I bind \"Reshape\" method in c#,the first parameter \"input tensor:T\" can be replaced with the previous return value .The second paramater type is \"long[]\". How do i convert object type  'long[]' to 'TF_Output'?\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 20623, "title": "fix the bug that InMemoryEvaluatorHook prints in every iter", "body": "For issue #20622 ", "comments": ["Sure, I will try to write a unit test. \r\n\r\nCurrently, I can confirm that the logger can print evaluation result with a correct freq with my code after changing the code. The problem is that the original code cannot reach`self._timer.update_last_triggered_step(self._iter_count)` as it is after `return`", "@ispirmustafa I didn't see this before I made the PR - but here is a fix w/ test. https://github.com/tensorflow/tensorflow/pull/20822", "Can this be closed given #20822 is merged?"]}, {"number": 20622, "title": "InMemoryEvaluatorHook failed to control printing freq with every_n_iter ", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**:  v1.9.0-rc2-202-g6d5b8b7cae 1.10.0-dev20180707\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**:  https://gist.github.com/matthew-z/aec927462416969b9b4e9c39a15eabca\r\n \r\n### Describe the problem\r\n\r\nInMemoryEvaluatorHook prints the evaluation result after every training iteration even every_n_iter is set to 10.\r\n \r\n### Source code / logs\r\n\r\nSource code: [https://gist.github.com/matthew-z/aec927462416969b9b4e9c39a15eabca](https://gist.github.com/matthew-z/aec927462416969b9b4e9c39a15eabca)\r\n\r\n\r\nLog: https://gist.github.com/matthew-z/921990af6957a1d9292759c933a1c16a", "comments": ["Here is a PR w/ test to fix this https://github.com/tensorflow/tensorflow/pull/20822", "@matthew-z does that fix your problem?", "Yes. I will close it"]}, {"number": 20621, "title": "error LNK 2001 private:void _cdecl tensorflow::GraphDef::InternalSwap(class tensorflow::GraphDef *)", "body": "I have built tensorflow.lib and tensorflow.dll successfully in visual studio 2015,but I'm trying to use those two files I got this error LNK 2001 private:void _cdecl tensorflow::GraphDef::InternalSwap(class tensorflow::GraphDef *)\r\nCan anyone help me to fix this error?\r\n\r\nOS:Win 10 visual studio 2015 \r\nCMake 3.12.0\r\nswigwin-3.0.12\r\nGPU:Nivdia 1080 ti\r\nCuda 9.0 \r\nCudnn 7\r\nI just following this website\r\nhttps://hk.saowen.com/a/e1c9146c6f02026ab6d25f79c5a21a847be18c734ff0dfe58bf74b72331b604b\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "OS:Win 10 visual studio 2015\r\nCMake 3.12.0\r\nswigwin-3.0.12\r\nGPU:Nivdia 1080 ti\r\nCuda 9.0\r\nCudnn 7\r\nAfter compiler I added  include Path \r\nC:\\tensorflow-master\\tensorflow\\contrib\\cmake\\build\\Release;\r\nC:\\tensorflow-master\\tensorflow\\contrib\\cmake\\build\\external\\nsync\\public\r\nC:\\tensorflow-master\\tensorflow\\contrib\\cmake\\build\\protobuf\\src\\protobuf\\src;\r\nC:\\tensorflow-master\\tensorflow\\contrib\\cmake\\build\\external\\eigen_archive;\r\nC:\\tensorflow-master\\tensorflow\\contrib\\cmake\\build;C:\\tensorflow-master;\r\nC:\\tensorflow-master\\third_party\\eigen3\r\n\r\nlib Path:\r\nC:\\tensorflow-master\\tensorflow\\contrib\\cmake\\build\\Release\r\n\r\nAnd \r\ntensorflow.lib", "It shouldn't happen if you built from scratch. Can you confirm that you did that? You used the cmake build, right? Did you try bazel?", "I just meet the same problem,error LNK2001", "It has been 44 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "I meet the same problem in tensorflow 1.9 and 1.10 version, tensorflow 1.8 doesn't have this problem. May be this is a bug in tensorflwo windows version, since google say they won't support build tensorflow on windows with cmake.", "Closing due to staleness. Please use the latest version for TensorFlow and build again. Feel free to open a new issue if it still persists. Thanks!"]}, {"number": 20620, "title": "Why dense layer cannot be speed up in tf.contrib.trt ?", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Ubuntu14.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**:  2.7.5\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9.0 / 7.0.5\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n\r\nWhen using tf.contrib.trt.create_inference_graph, I meet the following error, it seems that dense layer is not support because the input tensor is not rank 4 ? Why has this request on dense layer ?\r\n\r\n> subgraph conversion error for subgraph_index:9 due to: \"Unimplemented: Require 4 dimensional input. Got 2 dense/MatMul\" SKIPPING\r\n\r\n### Source code / logs\r\n\r\nNeed not to code, clear above ...\r\n", "comments": ["@aaroey @samikama may be able to answer this.", "@jiarenyf, TRT 3 doesn't have support for matmul support. Could you please try with TRT4.0 after PR #20755  and #20350 merged?", "@samikama Thank you. But I first reshape the input of dense layer to rank 4, and reshape back to rank 2 after this layer, it seams that the dense with input rank 4 can be speed up by the tensorrt.", "Hi @jiarenyf, have you tried with the updated code? Please let me know if there are still problems.", "However, it came out with new problems as follow:\r\n\r\n`Can't determine the device, constructing an allocator at device 0`\r\n\r\nWhy tensorflow so diffcult to use ...", "@jiarenyf Apologies about the problem. I replied your comment in another issue, please take a look and help to try again. For the matmul support, I think there is one more PR #21075 that we may need to patch before it can work. Please let me know.\r\n\r\nThanks.", "Hi @jiarenyf, both PRs are merged, would you mind trying it again?", "Hi @aaroey What should I do to try it again ? Should I `pull the master` and `build from source` or just `pip install` ?", "@jiarenyf please `pull the master` and `build from source`. The fixes are not in the release yet.", "@aaroey Thank you. But I cannot build from source for the time being, I will try after the fixes being released."]}, {"number": 20619, "title": "ReduceLROnPlateau with native optimizer: 'TFOptimizer' object has no attribute 'lr'", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nmacOS 10.12.6\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary (pip)\r\n\r\n- **TensorFlow version (use command below)**:\r\n1.9.0rc2\r\n\r\n- **Python version**: \r\nPython 3.6.4 :: Anaconda custom (x86_64)\r\n\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n\r\n- **GCC/Compiler version (if compiling from source)**:\r\nN/A\r\n\r\n- **CUDA/cuDNN version**:\r\nN/A\r\n\r\n- **GPU model and memory**:\r\nN/A\r\n\r\n- **Exact command to reproduce**:\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import Sequential\r\nfrom tensorflow.layers import Dense\r\nfrom tensorflow.python.training.adam import AdamOptimizer\r\nimport numpy as np\r\n\r\nmodel = Sequential()\r\nmodel.add(Dense(8, input_shape=(2, )))\r\nmodel.add(Dense(1, activation='softmax'))\r\nmodel.compile(optimizer=AdamOptimizer(), loss='mse')\r\n\r\nlr_schedule = tf.keras.callbacks.ReduceLROnPlateau()\r\n\r\nx = np.random.uniform(0, 1, (100, 2))\r\ny = np.random.uniform(0, 1, (100, 1))\r\nmodel.fit(x=x, y=y, callbacks=[lr_schedule], validation_split=0.2)\r\n```\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nUsing a native optimizer (AdamOptimizer) I can't get ReduceLROnPlateau to work, but it does work using an optimizer from tf.keras.optimizers. Only TF native optimizers are supported in Eager mode, so right now I just don't use ReduceLROnPlateau while in eager mode, but I thought this should be reported. Thank you.\r\n\r\n### Source code / logs\r\n```\r\n  File \"/Users/ken/Documents/Projects/keras-try/src/lr.py\", line 16, in <module>\r\n    model.fit(x=x, y=y, callbacks=[lr_schedule], validation_split=0.2)\r\n  File \"/Users/ken/anaconda/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 1348, in fit\r\n    validation_steps=validation_steps)\r\n  File \"/Users/ken/anaconda/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\", line 277, in fit_loop\r\n    callbacks.on_epoch_end(epoch, epoch_logs)\r\n  File \"/Users/ken/anaconda/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\", line 95, in on_epoch_end\r\n    callback.on_epoch_end(epoch, logs)\r\n  File \"/Users/ken/anaconda/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\", line 921, in on_epoch_end\r\n    logs['lr'] = K.get_value(self.model.optimizer.lr)\r\nAttributeError: 'TFOptimizer' object has no attribute 'lr'\r\n```\r\n", "comments": ["I suspect the problem is coming from here:\r\nhttps://github.com/keras-team/keras/blob/master/keras/callbacks.py#L1037\r\nwhere it asked the optimizer for lr. On the other hand native AdamOptimizer uses _lr:\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/python/training/adam.py#L94\r\n\r\nI wonder if we could make a check in the callback, if it's keras optimizer, adjust self.lr, if it's tf optimizer, adjust self._lr. else raise error.", "as I was checking, tf native optimizers mostly use 1) self._lr, 2) self._learning_rate. Maybe we could consider unifying them.", "Review is completed and still working on implementation. It appears to be more challenging in distributed versions.", "Any progress?", "Yes. We're near to finish and stay tuned.", "I faced a very similar problem, but while using SGD optimizer. \r\nThe problem for me was in the file **[condaEnv]\\Lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py**, specifically, in function **on_epoch_begin**. I changed the code (I know it is bad) to this: \r\n\r\n```\r\ndef on_epoch_begin(self, epoch, logs=None):\r\n    # if not hasattr(self.model.optimizer, 'lr'):   <=== Original  self.model.optimizer.optimizer._learning_rate\r\n    if not hasattr(self.model.optimizer.optimizer, '_learning_rate'):\r\n      raise ValueError('Optimizer must have a \"lr\" attribute.')\r\n    try:  # new API\r\n      # lr = float(K.get_value(self.model.optimizer.lr))\r\n      lr = float(self.model.optimizer.optimizer._learning_rate)\r\n      lr = self.schedule(epoch, lr)\r\n    except TypeError:  # Support for old API for backward compatibility\r\n      lr = self.schedule(epoch)\r\n    if not isinstance(lr, (float, np.float32, np.float64)):\r\n      raise ValueError('The output of the \"schedule\" function '\r\n                       'should be float.')\r\n    # K.set_value(self.model.optimizer.lr, lr)\r\n    self.model.optimizer.optimizer._learning_rate = lr\r\n    if self.verbose > 0:\r\n      print('\\nEpoch %05d: LearningRateScheduler reducing learning '\r\n            'rate to %s.' % (epoch + 1, lr))\r\n```\r\n\r\nIt works for me. ", "I still have this problem when using \r\n\r\n`from tensorflow.contrib.opt import AdamWOptimizer`\r\n\r\nand \r\n\r\n`from tensorflow.keras.callbacks import ReduceLROnPlateau`\r\n\r\nwith tensorflow-gpu==1.12.0", "Similar issue with `LearningRateScheduler`.", "I think this may solve the problem https://github.com/uber/horovod/issues/42.\r\nThis is my code:\r\n```\r\nlearning_rate = K.variable(0.001)\r\nadamW = tf.contrib.opt.AdamWOptimizer(weight_decay=1e-4,\r\n                                      learning_rate=learning_rate, \r\n                                      beta1=0.9, beta2=0.999, \r\n                                      epsilon=1e-08, name='AdamW')\r\nopt= TFOptimizer(adamW)\r\nopt.lr = learning_rate\r\nmodel.compile(optimizer=opt, loss=ssd_loss.compute_loss)\r\n```\r\n\r\n", "@chuong98 \r\nI get this, any ideas?\r\n('Could not interpret optimizer identifier:', <keras.optimizers.TFOptimizer object at 0x7fdebff42828>)", "@hadaev8  did you import keras from tf.keras ?", "Nope, it raise error then i try to import\r\nNo module named 'tensorflow.keras.optimizers.TFOptimizer'", "This is the code I got it work:\r\n```\r\n    from keras.optimizers import TFOptimizer\r\n    learning_rate = K.variable(0.001)\r\n    adamW = tf.contrib.opt.AdamWOptimizer(weight_decay=1e-4,\r\n                                      learning_rate=learning_rate,\r\n                                      beta1=0.9, beta2=0.999,\r\n                                      epsilon=1e-08, name='AdamW')\r\n    opt= TFOptimizer(adamW)\r\n    opt.lr = learning_rate\r\n    model.compile(optimizer=opt, loss=ssd_loss.compute_loss)\r\n```\r\nCould you compile it sucessfully?", "Oh, i get it, all model's layers should be from keras, but not tf.keras\r\nAnd for tpu keras model i need tf.keras model, unlucky.", "had the same issue when i  used tf.train.AdamOptimizer.\r\ni solved the ReduceLR issue by passing 'adam' to model.compile\r\n`model.compile(optimizer='adam',loss='mse')\r\n`\r\n\r\n\r\n", "@Avimor88 , @hadaev8 , @chuong98 , @WillBrennan , @Tzeny , @kenfehling , @malikaltakrori , can you try to sync up with master version (or tf nightly) to see if it works? We published a new set of optimizers to deal with this issue, you can simply use it by:\r\nopt = tf.keras.Adam(learning_rate=0.01)\r\nmodel.compile(opt, loss=?, metrics=?)\r\ncallbacks = [LearningRateScheduler(your_own_schedule)]\r\nmodel.fit(x,y, num_epochs=?, callbacks=callbacks)", "@tanzhenyu  , it worked using opt = tf.keras.optimizers.Adam(lr=0.001) with tensorflow version 1.12 !\r\nThank you", "i had a same issue when using EarlyStopping, ReduceLROnPlateau. (tensorflow version 1.12)\r\n\r\ncallbacks = [EarlyStopping, ReduceLROnPlateau]\r\n-> it doesn't work:(\r\nwhen i use optimizer, opt = tf.keras.optimizers.Adam(lr=0.001)\r\nError message is \r\n'must be an instance of tf.train.Optimizer, not a <class 'tensorflow.python.keras.optimizers.Adam'\r\n\r\nso i used optimizer=tf.train.AdamOptimizer(),\r\nbut i get this `AttributeError: 'TFOptimizer' object has no attribute 'lr'\r\n\r\nany ideas?", "> i had a same issue when using EarlyStopping, ReduceLROnPlateau. (tensorflow version 1.12)\r\n> \r\n> callbacks = [EarlyStopping, ReduceLROnPlateau]\r\n> -> it doesn't work:(\r\n> when i use optimizer, opt = tf.keras.optimizers.Adam(lr=0.001)\r\n> Error message is\r\n> 'must be an instance of tf.train.Optimizer, not a <class 'tensorflow.python.keras.optimizers.Adam'\r\n> \r\n> so i used optimizer=tf.train.AdamOptimizer(),\r\n> but i get this `AttributeError: 'TFOptimizer' object has no attribute 'lr'\r\n> \r\n> any ideas?\r\n\r\nSame here. This breaks all models compiled with keras_to_tpu and means you can't get any sort of learning rate decay working; pretty big bug imo.", "@SooDevv @indrasweb is there a code snippet that you can share?", "> @SooDevv @indrasweb is there a code snippet that you can share?\r\n\r\nI have put together a minimal example here on Colab; https://colab.research.google.com/drive/1wuOzZIYU7Tw1bHd6tE1SO0JWslZ7Q11f\r\n\r\nNote this happens for both sequential and functional models.", "@tanzhenyu may you advise, how to upgrade tensorflow on google colab tpu?\r\nI tried \r\n!pip uninstall tensorflow -y\r\n!pip install tf-nightly\r\nBut get error then try to train", "@tanzhenyu I have two issues when using `tf.enable_eager_execution` or not\r\n\r\n## 1. Not using tf.enable_eager_execution\r\n```\r\n# Build callbacks\r\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau\r\nreduce_lr = ReduceLROnPlateau(monitor='val_acc', patience=2, verbose=1, factor=0.5, min_lr=0.00001)\r\nearlystop = EarlyStopping(patience=5)\r\ncallbacks = [reduce_lr, earlystop]\r\n```\r\n```\r\n# model\r\nfrom tensorflow.keras import models\r\nfrom tensorflow.keras import layers \r\nfrom tensorflow.keras import optimizers\r\n\r\nmodel = models.Sequential()\r\nmodel.add(Dense(1024, activation='relu', input_dim=tr_x.shape[1]))\r\nmodel.add(Dense(512, activation='relu'))\r\nmodel.add(Dense(256, activation='relu'))\r\nmodel.add(Dropout(.5))\r\nmodel.add(Dense(num_classes, activation='sigmoid'))\r\n\r\nmodel.compile(loss=tf.keras.losses.binary_crossentropy,\r\n             optimizer=tf.train.AdamOptimizer(),\r\n             metrics=['accuracy'])\r\n\r\nhistory = model.fit(x=tr_x, y=tr_y, \r\n                    batch_size=batch_size, \r\n                    epochs=30, \r\n                    callbacks=callbacks,\r\n                    validation_data=(val_x, val_y))\r\n```\r\n### Error logs\r\n```\r\nAttributeError                Traceback (most recent call last)\r\n<ipython-input-19-ebfb0c90ead7> in <module>\r\n      3                     epochs=30,\r\n      4                     callbacks=callbacks,\r\n----> 5                     validation_data=(val_x, val_y))\r\n\r\n/usr/local/var/pyenv/versions/3.6.7/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n   1637           initial_epoch=initial_epoch,\r\n   1638           steps_per_epoch=steps_per_epoch,\r\n-> 1639           validation_steps=validation_steps)\r\n   1640 \r\n   1641   def evaluate(self,\r\n\r\n/usr/local/var/pyenv/versions/3.6.7/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py in fit_loop(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\r\n    237             for l, o in zip(model.metrics_names, val_outs):\r\n    238               epoch_logs['val_' + l] = o\r\n--> 239     callbacks.on_epoch_end(epoch, epoch_logs)\r\n    240     if callbacks.model.stop_training:\r\n    241       break\r\n\r\n/usr/local/var/pyenv/versions/3.6.7/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py in on_epoch_end(self, epoch, logs)\r\n    212     logs = logs or {}\r\n    213     for callback in self.callbacks:\r\n--> 214       callback.on_epoch_end(epoch, logs)\r\n    215 \r\n    216   def on_batch_begin(self, batch, logs=None):\r\n\r\n/usr/local/var/pyenv/versions/3.6.7/envs/tensorflow/lib/python3.6/site-packages/keras/callbacks.py in on_epoch_end(self, epoch, logs)\r\n   1101     def on_epoch_end(self, epoch, logs=None):\r\n   1102         logs = logs or {}\r\n-> 1103         logs['lr'] = K.get_value(self.model.optimizer.lr)\r\n   1104         current = logs.get(self.monitor)\r\n   1105         if current is None:\r\n\r\nAttributeError: 'TFOptimizer' object has no attribute 'lr'\r\n```\r\n\r\n## 2. Using tf.enable_eager_execution()\r\n```\r\nmodel = models.Sequential()\r\nmodel.add(Dense(1024, activation='relu', input_dim=tr_x.shape[1]))\r\nmodel.add(Dense(512, activation='relu'))\r\nmodel.add(Dense(256, activation='relu'))\r\nmodel.add(Dropout(.5))\r\nmodel.add(Dense(num_classes, activation='sigmoid'))\r\n\r\nopt = tf.keras.optimizers.Adam(lr=0.001)\r\nmodel.compile(loss=tf.keras.losses.binary_crossentropy,\r\n             optimizer=opt,\r\n             metrics=['accuracy'])\r\n```\r\n### Error logs\r\n```\r\nValueError                    Traceback (most recent call last)\r\n<ipython-input-15-96630dc39e85> in <module>\r\n      8 model.compile(loss=tf.keras.losses.binary_crossentropy,\r\n      9              optimizer=opt,\r\n---> 10              metrics=['accuracy'])\r\n\r\n/usr/local/var/pyenv/versions/3.6.7/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/checkpointable/base.py in _method_wrapper(self, *args, **kwargs)\r\n    472     self._setattr_tracking = False  # pylint: disable=protected-access\r\n    473     try:\r\n--> 474       method(self, *args, **kwargs)\r\n    475     finally:\r\n    476       self._setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n/usr/local/var/pyenv/versions/3.6.7/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\r\n    408       raise ValueError(\r\n    409           'optimizer must be an instance of tf.train.Optimizer, not '\r\n--> 410           'a %s' % type(optimizer))\r\n    411 \r\n    412     self.optimizer = optimizers.get(optimizer)\r\n\r\nValueError: optimizer must be an instance of tf.train.Optimizer, not a <class 'tensorflow.python.keras.optimizers.Adam'>\r\n```\r\nand then i use tf.train.Optimizer , same issues occurs. \r\n`AttributeError: 'TFOptimizer' object has no attribute 'lr'`", "@SooDevv there are two changes in order to make this work:\r\n1. use tf.keras instead of keras -- the new optimizers is only available through tf, not keras yet, we have further plans to develop that in keras repo.\r\n2. replace tf.train.AdamOptimizer with tf.keras.optimizers.Adam", "@SooDevv \r\nExample:\r\n\r\n`import tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.python.keras.callbacks import EarlyStopping, ReduceLROnPlateau\r\nreduce_lr = ReduceLROnPlateau(monitor='val_acc', patience=2, verbose=1, factor=0.5, min_lr=0.00001)\r\nearlystop = EarlyStopping(patience=5)\r\ncallbacks = [reduce_lr, earlystop]\r\nfrom tensorflow.keras import models\r\nfrom tensorflow.keras import layers \r\nfrom tensorflow.keras import optimizers\r\n\r\nmodel = models.Sequential()\r\nmodel.add(layers.Dense(10, activation='relu', input_dim=tr_x.shape[1]))\r\nmodel.add(layers.Dropout(.5))\r\nmodel.add(layers.Dense(2, activation='sigmoid'))\r\n\r\nmodel.compile(loss=tf.keras.losses.binary_crossentropy,\r\n             optimizer=tf.keras.optimizers.Adam(),\r\n             metrics=['accuracy'])\r\n\r\nhistory = model.fit(x=tr_x, y=tr_y, \r\n                    batch_size=5, \r\n                    epochs=30, \r\n                    callbacks=callbacks)`", "@hadaev8 I cannot edit your colab, can you try changing tf.train.AdamOptimizer to tf.keras.optimizers.Adam?", "@tanzhenyu\r\nIts not mine colab, its work, couz it copy keras adam parameters to tensorflow adam every step.\r\nIt would be better to do it without copying every step.", "@tanzhenyu \r\n\r\nyes, it works when 'not'  tensorflow eager execution mode! \r\nbut it's not working when tensorflow eager execution mode. \r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.python.keras.callbacks import EarlyStopping, ReduceLROnPlateau\r\n\r\ntf.enable_eager_execution() <-- this **\r\n\r\nreduce_lr = ReduceLROnPlateau(monitor='val_acc', patience=2, verbose=1, factor=0.5, min_lr=0.00001)\r\nearlystop = EarlyStopping(patience=5)\r\ncallbacks = [reduce_lr, earlystop]\r\nfrom tensorflow.keras import models\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras import optimizers\r\n\r\nmodel = models.Sequential()\r\nmodel.add(layers.Dense(10, activation='relu', input_dim=tr_x.shape[1]))\r\nmodel.add(layers.Dropout(.5))\r\nmodel.add(layers.Dense(8, activation='sigmoid'))\r\n\r\nmodel.compile(loss=tf.keras.losses.binary_crossentropy,\r\noptimizer=tf.keras.optimizers.Adam(),\r\nmetrics=['accuracy'])\r\n\r\nhistory = model.fit(x=tr_x, y=tr_y,\r\nbatch_size=5,\r\nepochs=30,\r\ncallbacks=callbacks)\r\n```\r\n\r\nValueError: optimizer must be an instance of tf.train.Optimizer, not a <class 'tensorflow.python.keras.optimizers.Adam'>", "Another problem with keras model to tpu, keras adam and ReduceLROnPlateau: it doesnt save current lr to checkpoint.", "@SooDevv Oh I see. Thanks for pointing it out. This has been fixed on 12/11/18, but it seems that it's not in the 1.12 release (which is on Nov). There are two options 1) use 1.13 rc which was released a week ago, 2) use tf nightly.", "@hadaev8 I suspect if it's also because of 1.12 release -- can you try 1.13 or tf-nightly? In colab is should be just one-line -- !pip install tf-nightly, that should replace 1.12", "@tanzhenyu \r\n1.13 need cuda 10, right? I have no idea how to update cuda on colab.", "Problem still here on tf 1.13", "@hadaev8 It works on my 13.1 version code. Which version are you running?", "Same\r\nHere is code\r\nhttps://colab.research.google.com/drive/1Px3i0bXFaJdFdMD2F1-WVO3Cp1xM90Iu?authuser=2#scrollTo=BX2SOcy6lRws", "I am having a similar problem. Not able to use ReduceLROnPlateau along with TPU because of the optimizer constraint. \r\n\r\nI already tried changing from tf.train.AdamOptimizer to tf.keras.optimizers.Adam . But in this case get TPU exception \"TPU ....... not fetchable\". Unless I use the tf.train.AdamOptimizer without the ReduceLROnPlateau , TPU gives an error. Kindly suggest the ideal way to use adaptive LR along with TPU. ", "So I revisited this just now, looks like it is specifically a problem when training using a generator. model.fit() does not have this bug, where model.fit_generator() does.\r\n\r\nAs a temporary workaround you can just modify the source of the ReduceLROnPlateau:\r\n\r\n```\r\nfrom tensorflow.python.util.tf_export import tf_export\r\nfrom tensorflow.python.keras.callbacks import Callback\r\n\r\n@tf_export('keras.callbacks.ReduceLROnPlateau')\r\nclass ReduceLROnPlateauMODIFIED(Callback):\r\n  \"\"\"Reduce learning rate when a metric has stopped improving.\r\n\r\n  Models often benefit from reducing the learning rate by a factor\r\n  of 2-10 once learning stagnates. This callback monitors a\r\n  quantity and if no improvement is seen for a 'patience' number\r\n  of epochs, the learning rate is reduced.\r\n\r\n  Example:\r\n\r\n  Arguments:\r\n      monitor: quantity to be monitored.\r\n      factor: factor by which the learning rate will\r\n          be reduced. new_lr = lr * factor\r\n      patience: number of epochs with no improvement\r\n          after which learning rate will be reduced.\r\n      verbose: int. 0: quiet, 1: update messages.\r\n      mode: one of {auto, min, max}. In `min` mode,\r\n          lr will be reduced when the quantity\r\n          monitored has stopped decreasing; in `max`\r\n          mode it will be reduced when the quantity\r\n          monitored has stopped increasing; in `auto`\r\n          mode, the direction is automatically inferred\r\n          from the name of the monitored quantity.\r\n      min_delta: threshold for measuring the new optimum,\r\n          to only focus on significant changes.\r\n      cooldown: number of epochs to wait before resuming\r\n          normal operation after lr has been reduced.\r\n      min_lr: lower bound on the learning rate.\r\n  \"\"\"\r\n\r\n  def __init__(self,\r\n               monitor='val_loss',\r\n               factor=0.1,\r\n               patience=10,\r\n               verbose=0,\r\n               mode='auto',\r\n               min_delta=1e-4,\r\n               cooldown=0,\r\n               min_lr=0,\r\n               **kwargs):\r\n    super(ReduceLROnPlateauMODIFIED, self).__init__()\r\n\r\n    self.monitor = monitor\r\n    if factor >= 1.0:\r\n      raise ValueError('ReduceLROnPlateau ' 'does not support a factor >= 1.0.')\r\n    if 'epsilon' in kwargs:\r\n      min_delta = kwargs.pop('epsilon')\r\n      logging.warning('`epsilon` argument is deprecated and '\r\n                      'will be removed, use `min_delta` instead.')\r\n    self.factor = factor\r\n    self.min_lr = min_lr\r\n    self.min_delta = min_delta\r\n    self.patience = patience\r\n    self.verbose = verbose\r\n    self.cooldown = cooldown\r\n    self.cooldown_counter = 0  # Cooldown counter.\r\n    self.wait = 0\r\n    self.best = 0\r\n    self.mode = mode\r\n    self.monitor_op = None\r\n    self._reset()\r\n\r\n  def _reset(self):\r\n    \"\"\"Resets wait counter and cooldown counter.\r\n    \"\"\"\r\n    if self.mode not in ['auto', 'min', 'max']:\r\n      logging.warning('Learning Rate Plateau Reducing mode %s is unknown, '\r\n                      'fallback to auto mode.', self.mode)\r\n      self.mode = 'auto'\r\n    if (self.mode == 'min' or\r\n        (self.mode == 'auto' and 'acc' not in self.monitor)):\r\n      self.monitor_op = lambda a, b: np.less(a, b - self.min_delta)\r\n      self.best = np.Inf\r\n    else:\r\n      self.monitor_op = lambda a, b: np.greater(a, b + self.min_delta)\r\n      self.best = -np.Inf\r\n    self.cooldown_counter = 0\r\n    self.wait = 0\r\n\r\n  def on_train_begin(self, logs=None):\r\n    self._reset()\r\n\r\n  def on_epoch_end(self, epoch, logs=None):\r\n    logs = logs or {}\r\n#     print(\"DEBUG\")\r\n#     print(self.model.optimizer.optimizer._opt._lr)\r\n#     print(dir(self.model.optimizer.optimizer._opt._lr))\r\n#     print(K.get_value(self.model.optimizer.optimizer._opt._lr))\r\n    logs['lr'] = self.model.optimizer.optimizer._opt._lr\r\n    current = logs.get(self.monitor)\r\n    if current is None:\r\n      logging.warning('Reduce LR on plateau conditioned on metric `%s` '\r\n                      'which is not available. Available metrics are: %s',\r\n                      self.monitor, ','.join(list(logs.keys())))\r\n\r\n    else:\r\n      if self.in_cooldown():\r\n        self.cooldown_counter -= 1\r\n        self.wait = 0\r\n\r\n      if self.monitor_op(current, self.best):\r\n        self.best = current\r\n        self.wait = 0\r\n      elif not self.in_cooldown():\r\n        self.wait += 1\r\n        if self.wait >= self.patience:\r\n          old_lr = float(self.model.optimizer.optimizer._opt._lr)\r\n          if old_lr > self.min_lr:\r\n            new_lr = old_lr * self.factor\r\n            new_lr = max(new_lr, self.min_lr)\r\n            self.model.optimizer.optimizer._opt._lr = new_lr\r\n            if self.verbose > 0:\r\n              print('\\nEpoch %05d: ReduceLROnPlateau reducing learning '\r\n                    'rate to %s.' % (epoch + 1, new_lr))\r\n            self.cooldown_counter = self.cooldown\r\n            self.wait = 0\r\n\r\n  def in_cooldown(self):\r\n    return self.cooldown_counter > 0\r\n```\r\n\r\n```def get_callbacks():\r\n\r\n  callbacks = list()\r\n\r\n  callbacks.append(ReduceLROnPlateauMODIFIED(\r\n    monitor='loss',\r\n    factor=0.5,   # lr = lr*factor\r\n    patience=1,  # how many epochs no change\r\n    verbose=1\r\n  ))\r\n    \r\n  return callbacks\r\n```\r\n\r\nHere is a colab notebook for further clarification: https://colab.research.google.com/drive/13L41k6sOvsm-UIm67cpC_kvCDsObqdDZ", "@pradeepelavarasan If you convert to tf.keras.optimizers.Adam, what is not fetchable? Can you give the full error message, and code snippet as well?", "> had the same issue when i used tf.train.AdamOptimizer.\r\n> i solved the ReduceLR issue by passing 'adam' to model.compile\r\n> `model.compile(optimizer='adam',loss='mse') `\r\n\r\nThis worked. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=20619\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=20619\">No</a>\n", "> > had the same issue when i used tf.train.AdamOptimizer.\r\n> > i solved the ReduceLR issue by passing 'adam' to model.compile\r\n> > `model.compile(optimizer='adam',loss='mse') `\r\n> \r\n> This worked. Thanks!\r\n\r\nI don't think solves the original problem, which was about using tf.train.AdamOptimizer. Last I checked, tf.train optimizers were necessary to get TPU training to really work.\r\n\r\nUnless the problem of using ReduceLR w/ tf.train optimizers has been fixed or the issue w/ TPU training using Keras optimizers has been fixed, I would still consider this an open issue.", "> I don't think solves the original problem, which was about using tf.train.AdamOptimizer. Last I checked, tf.train optimizers were necessary to get TPU training to really work.\r\n> \r\n> Unless the problem of using ReduceLR w/ tf.train optimizers has been fixed or the issue w/ TPU training using Keras optimizers has been fixed, I would still consider this an open issue.\r\n\r\nWhat do you mean Keras optimizer not working with TPU?", "I spent quite a bit of time trying to get TPU optimization working with the Keras optimizers about a month ago and it didn't seem to work correctly. I could use tf.train.Adam and the loss would go down (but I couldn't decay the learning rate).  With the Keras optimizer the loss would not go down (I tried various multiples of the learning rate, in case there was a batch sizing issue, but it didn't seem to help). \r\n\r\nI have a Colab Notebook I was working with.  It would need a little work to be ready to share, but here are the relevant bits of the final state of my code:\r\n```\r\nfrom tensorflow.keras.applications.xception import Xception as base_cnn\r\n\r\nBASE_LR = 1e-3\r\n\r\nUSE_KERAS_OPTIMIZER = True\r\n\r\nbase_model = base_cnn(include_top=False, input_shape=(256,256,3), pooling='avg')\r\npredictions = Dense(N_CLASSES, activation='softmax')(base_model.output)\r\nmodel = Model(inputs=base_model.inputs,outputs=predictions)\r\nmodel = add_regularization(model)\r\n\r\nprint('WARNING: keras optimizer has an 8x scaled learning rate as a hack to see if that is the issue with slow training')\r\n\r\nif USE_KERAS_OPTIMIZER:\r\n  opt = tf.keras.optimizers.Adam(lr=1e-3)\r\n  \r\n  def step_decay(epoch):\r\n    initial_lrate = BASE_LR\r\n    drop = 0.1\r\n    epochs_drop = epochs / 3.0\r\n    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\r\n    return 8.0*lrate\r\n  \r\n  lrate = LearningRateScheduler(step_decay, verbose=True)\r\n  callbacks = [lrate]\r\n\r\nelse:\r\n  opt = tf.train.AdamOptimizer(learning_rate=BASE_LR, ep)\r\n  callbacks = []\r\n                                      \r\nmodel.compile(optimizer=opt,\r\n              loss='categorical_crossentropy', \r\n              metrics=['categorical_accuracy'])\r\n\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\n\r\ntpu = tf.contrib.cluster_resolver.TPUClusterResolver()\r\ntpu_model = tf.contrib.tpu.keras_to_tpu_model(\r\n    model,\r\n    strategy=tf.contrib.tpu.TPUDistributionStrategy(tpu))\r\n\r\ndatestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\r\nhist = tpu_model.fit(\r\n    lambda: make_dataset(training=True),\r\n    steps_per_epoch=steps_per_epoch,\r\n    epochs=epochs,\r\n    validation_data=lambda: make_dataset(training=False),\r\n    validation_steps=N_validation_batches, callbacks=callbacks)\r\n```\r\n\r\nI came to the conclusion that I was asking too much from `tf.contrib.tpu.keras_to_tpu_model` and gave up.  I would be happy to share a working version of this if someone would be interested in helping me debug.", "> > > had the same issue when i used tf.train.AdamOptimizer.\r\n> > > i solved the ReduceLR issue by passing 'adam' to model.compile\r\n> > > `model.compile(optimizer='adam',loss='mse') `\r\n> > \r\n> > \r\n> > This worked. Thanks!\r\n> \r\n> I don't think solves the original problem, which was about using tf.train.AdamOptimizer. Last I checked, tf.train optimizers were necessary to get TPU training to really work.\r\n> \r\n> Unless the problem of using ReduceLR w/ tf.train optimizers has been fixed or the issue w/ TPU training using Keras optimizers has been fixed, I would still consider this an open issue.\r\n\r\nIt is true what you are saying. The solution worked for my particular use case but I would still consider that as a hack rather than a solution. Tensorflow provides so many versions of the same optimizer with slightly different parameter names that choosing one over the other causes certain problems in particular use cases. The issue should remain open. Please do correct me if I'm wrong, though.", "Using Keras with tensorflow optimizers wasn't the best experience. Instead we merged tensorflow optimizers and keras optimizers, and they're all under keras optimizers now. So by doing model.compile(optimizer='adam') it is by default using the new keras optimizers. This is the right way to do it, not a hack. If you want to specify your own hyperparameters, you can create one by doing tf.keras.optimizers.Adam starting in 1.14.", "@tanzhenyu \nThank you for the clarification. This helped a lot. So will the other legacy versions of Tensorflow optimizers be deprecated in future and keras optimizers be used or does Tensorflow 1.x API still use it? ", "> I think this may solve the problem [horovod/horovod#42](https://github.com/horovod/horovod/issues/42).\r\n> This is my code:\r\n> \r\n> ```\r\n> learning_rate = K.variable(0.001)\r\n> adamW = tf.contrib.opt.AdamWOptimizer(weight_decay=1e-4,\r\n>                                       learning_rate=learning_rate, \r\n>                                       beta1=0.9, beta2=0.999, \r\n>                                       epsilon=1e-08, name='AdamW')\r\n> opt= TFOptimizer(adamW)\r\n> opt.lr = learning_rate\r\n> model.compile(optimizer=opt, loss=ssd_loss.compute_loss)\r\n> ```\r\n\r\nWhen we need do learning rate decay or ReduceLROnPlateau, we shouldn't use like this. Use the optimizers of tf.keras, not tf.train or tf.contrib.opt.\r\n\r\nReduceLROnPlateau indeed changes the value `opt.lr`, but it's not the true learning rate of the optimizer, when we see the source code, we can find the true learning rate is `opt._lr`, so the ReduceLROnPlateau actually doesn't work.\r\n\r\n", "For all -- please use 1.14 and migrate to tf.keras.optimizers so that this wouldn't become an issue for you.", "> For all -- please use 1.14 and migrate to tf.keras.optimizers so that this wouldn't become an issue for you.\r\n\r\nBut some optimizers like AdamW are not implemented in tf.keras.optimizers", "> > For all -- please use 1.14 and migrate to tf.keras.optimizers so that this wouldn't become an issue for you.\r\n> \r\n> But some optimizers like AdamW are not implemented in tf.keras.optimizers\r\n\r\nIf this is strongly desired, file a request and either we can make this into tf core or tf addon.", "> had the same issue when i used tf.train.AdamOptimizer.\r\n> i solved the ReduceLR issue by passing 'adam' to model.compile\r\n> `model.compile(optimizer='adam',loss='mse') `\r\n\r\nyes this work thnaks", "yes this works\r\n", "you should use keras.optimizer not tensorflow.keras.optimizer"]}, {"number": 20618, "title": "RAM", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 20617, "title": "Fix unused status error in convert_graph.cc", "body": "", "comments": ["LGTM", "There seems to be some problem with the approval, @yifeif."]}, {"number": 20616, "title": "Correct CUDA v9.0 download link", "body": "Changed link to download CUDA to the right version.\r\n\r\nWould have done it for cuDNN as well, but linking to the specific versions doesn't seem to be possible, thanks NVIDIA.\r\n\r\nNo reason this should be linking to the newest version when TS needs v9.0. Seriously, my master's thesis needs to be done by August, I dont have time to install CUDA twice, shiiieeet.", "comments": []}, {"number": 20615, "title": "undefined reference to 'fegetround' in android", "body": "Build command failed.\r\nError while executing process /Users/luojie/Library/Android/sdk/cmake/3.6.4111459/bin/cmake with arguments {--build /Users/luojie/WorkSpaceOfSaiDeSheng/WorkSpaceOfOpenCV/android/OpencvDemo/opencvlib/.externalNativeBuild/cmake/debug/armeabi-v7a --target native-lib}\r\n[1/1] Linking CXX shared library ../../../../build/intermediates/cmake/debug/obj/armeabi-v7a/libnative-lib.so\r\nFAILED: : && /Users/luojie/Library/Android/sdk/ndk-bundle/toolchains/llvm/prebuilt/darwin-x86_64/bin/clang++  --target=armv7-none-linux-androideabi --gcc-toolchain=/Users/luojie/Library/Android/sdk/ndk-bundle/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64 --sysroot=/Users/luojie/Library/Android/sdk/ndk-bundle/sysroot -fPIC -isystem /Users/luojie/Library/Android/sdk/ndk-bundle/sysroot/usr/include/arm-linux-androideabi -D__ANDROID_API__=16 -g -DANDROID -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -march=armv7-a -mfloat-abi=softfp -mfpu=vfpv3-d16 -mthumb -Wa,--noexecstack -Wformat -Werror=format-security  -frtti -fexceptions -std=c++11 -std=gnu++11 -fexceptions -frtti -O0 -fno-limit-debug-info  -Wl,--exclude-libs,libgcc.a -Wl,--exclude-libs,libatomic.a -nostdlib++ --sysroot /Users/luojie/Library/Android/sdk/ndk-bundle/platforms/android-16/arch-arm -Wl,--build-id -Wl,--warn-shared-textrel -Wl,--fatal-warnings -Wl,--fix-cortex-a8 -Wl,--no-undefined -Wl,-z,noexecstack -Qunused-arguments -Wl,-z,relro -Wl,-z,now -shared -Wl,-soname,libnative-lib.so -o ../../../../build/intermediates/cmake/debug/obj/armeabi-v7a/libnative-lib.so CMakeFiles/native-lib.dir/src/main/cpp/AIEdgeDectectionPublic/zf_log.c.o CMakeFiles/native-lib.dir/src/main/cpp/AIEdgeDectectionPublic/fm_ocr_scanner.cpp.o CMakeFiles/native-lib.dir/src/main/cpp/AIEdgeDectectionPublic/EdgeDetection.cpp.o CMakeFiles/native-lib.dir/src/main/cpp/AIEdgeDectectionPublic/AIEdgeDetection.cpp.o CMakeFiles/native-lib.dir/src/main/cpp/AIEdgeDectectionPublic/imgproc/scannerLite.cpp.o CMakeFiles/native-lib.dir/src/main/cpp/AIEdgeDectectionPublic/imgproc/ImgEffect.cpp.o CMakeFiles/native-lib.dir/src/main/cpp/native-lib.cpp.o  ../../../../src/main/jniLibs/armeabi-v7a/libtensorflow-core.a ../../../../src/main/jniLibs/armeabi-v7a/libprotobuf.a ../../../../src/main/jniLibs/armeabi-v7a/libnsync.a ../../../../src/main/jniLibs/armeabi-v7a/libopencv_imgcodecs.a ../../../../src/main/jniLibs/armeabi-v7a/liblibjasper.a ../../../../src/main/jniLibs/armeabi-v7a/liblibjpeg.a ../../../../src/main/jniLibs/armeabi-v7a/libIlmImf.a ../../../../src/main/jniLibs/armeabi-v7a/liblibtiff.a ../../../../src/main/jniLibs/armeabi-v7a/liblibpng.a ../../../../src/main/jniLibs/armeabi-v7a/libopencv_java3.so -llog -lz -latomic -lm \"/Users/luojie/Library/Android/sdk/ndk-bundle/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/libgnustl_static.a\" && :\r\n../../../../src/main/jniLibs/armeabi-v7a/libtensorflow-core.a(setround.o):setround.cc:function tensorflow::port::ScopedSetRound::ScopedSetRound(int): error: undefined reference to 'fegetround'\r\n../../../../src/main/jniLibs/armeabi-v7a/libtensorflow-core.a(setround.o):setround.cc:function tensorflow::port::ScopedSetRound::ScopedSetRound(int): error: undefined reference to 'fesetround'\r\n../../../../src/main/jniLibs/armeabi-v7a/libtensorflow-core.a(setround.o):setround.cc:function tensorflow::port::ScopedSetRound::~ScopedSetRound(): error: undefined reference to 'fesetround'\r\n../../../../src/main/jniLibs/armeabi-v7a/libprotobuf.a(strutil.o):strutil.cc:function tf_protobuf3_private::protobuf::safe_strtof(char const*, float*): error: undefined reference to 'strtof'\r\nclang++: error: linker command failed with exit code 1 (use -v to see invocation)\r\nninja: build stopped: subcommand failed.\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 16 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 20614, "title": "Cuda Configuration Error: Cannot find libdevice.10.bc under /usr/local/cuda-8.0", "body": "I want to install TensorFlow through compiling from source. After I finished the configure, I run the command `bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`. But some errors occurs. Here are the info:\r\n```\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n\tFile \"/home/cxt/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1166\r\n\t\t_create_local_cuda_repository(repository_ctx)\r\n\tFile \"/home/cxt/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1001, in _create_local_cuda_repository\r\n\t\t_find_nvvm_libdevice_dir(repository_ctx, cuda_config)\r\n\tFile \"/home/cxt/tensorflow/third_party/gpus/cuda_configure.bzl\", line 724, in _find_nvvm_libdevice_dir\r\n\t\tauto_configure_fail((\"Cannot find libdevice.10.bc un...))\r\n\tFile \"/home/cxt/tensorflow/third_party/gpus/cuda_configure.bzl\", line 210, in auto_configure_fail\r\n\t\tfail((\"\\n%sCuda Configuration Error:%...)))\r\n\r\nCuda Configuration Error: Cannot find libdevice.10.bc under /usr/local/cuda-8.0\r\nWARNING: Target pattern parsing failed.\r\nERROR: error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n\tFile \"/home/cxt/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1166\r\n\t\t_create_local_cuda_repository(repository_ctx)\r\n\tFile \"/home/cxt/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1001, in _create_local_cuda_repository\r\n\t\t_find_nvvm_libdevice_dir(repository_ctx, cuda_config)\r\n\tFile \"/home/cxt/tensorflow/third_party/gpus/cuda_configure.bzl\", line 724, in _find_nvvm_libdevice_dir\r\n\t\tauto_configure_fail((\"Cannot find libdevice.10.bc un...))\r\n\tFile \"/home/cxt/tensorflow/third_party/gpus/cuda_configure.bzl\", line 210, in auto_configure_fail\r\n\t\tfail((\"\\n%sCuda Configuration Error:%...)))\r\n\r\nCuda Configuration Error: Cannot find libdevice.10.bc under /usr/local/cuda-8.0\r\n```\r\nI tried the solution on this [link](https://github.com/tensorflow/tensorflow/issues/17801). But it didn't work. \r\nThe following are the info about my PC.\r\n### System information\r\n- **Have I written custom code**: No\r\n- **OS Platform and Distribution**:Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:source\r\n- **TensorFlow version (use command below)**:bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n- **Python version**:3.6 \r\n- **Bazel version**:0.15.0\r\n- **GCC/Compiler version**:5.5.0\r\n- **CUDA/cuDNN version**:8.0/5.1.10\r\n- **GPU model and memory**:N/A\r\n- **Exact command to reproduce**:N/A\r\n\r\n### Describe the problem\r\nI want to install TensorFlow on my system. But I tried plenty of ways, none of them works. It took me a lot of time. Can anybody help me out or provide me a useful way?\r\nThanks a lot!\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow version\nGPU model and memory\nExact command to reproduce", "@tensorflowbutler Thank you for your response. I feel so sorry for my fault. I have update this issue as you command. Wish your response. Thank you.", "This is a duplicate of #17801. Unfortunately, it seems TensorFlow does not currently work with Cuda 8.0, so I recommend upgrading to at least Cuda 9.0."]}, {"number": 20613, "title": "[bug] May be a bug of gradients calculation.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:1.9\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nI write a test case to test operation ops::Square and it's gradient function SquareGrad, here is the code:\r\n\r\n    TEST(MathTest, SingleComplex64Function) {\r\n     //\r\n     //data prepare\r\n     Scope root = Scope::NewRootScope();\r\n     DataType x_type = DataTypeToEnum<complex64>::v();\r\n     TensorShape shape({1});\r\n\r\n     Tensor x_data(x_type, shape);\r\n     auto x_data_flat = x_data.flat<complex64>();\r\n     x_data_flat(0) = {1,2};\r\n     \r\n     //\r\n     //graph construct: y = x*x, x and y are complex64\r\n     auto x = ops::Placeholder(root, x_type, Placeholder::Shape(shape));\r\n     auto y = ops::Square(root, x);\r\n\r\n     ClientSession session(root);\r\n     std::vector<Tensor> outputs;\r\n\r\n     //\r\n     //calculate y\r\n     Status s = session.Run({{x, x_data}} , {y}, &outputs);\r\n     ASSERT_TRUE(s.ok());\r\n     ASSERT_TRUE(outputs.size() == 1);\r\n     ASSERT_TRUE(outputs[0].dtype() == DT_COMPLEX64);\r\n     ASSERT_TRUE(outputs[0].flat<complex64>()(0) == complex64(-3,4));\r\n\r\n     std::vector<Output> grads;\r\n     s = AddSymbolicGradients(root, {y}, {x}, &grads);\r\n     ASSERT_TRUE(s.ok());\r\n     \r\n     //\r\n     //calculate gradients of y w.r.t x\r\n     std::vector<Tensor> grad_outputs;\r\n     s = session.Run({{x, x_data}}, grads, &grad_outputs);\r\n     ASSERT_TRUE(s.ok());\r\n     ASSERT_TRUE(grad_outputs.size() == 1);\r\n     ASSERT_TRUE(grad_outputs[0].dtype() == DT_COMPLEX64);\r\n     ASSERT_TRUE(grad_outputs[0].flat<complex64>()(0) == complex64(2,4));\r\n    }\r\n\r\nwhen i run it, i get this error:\r\n\r\n    [ RUN      ] MathTest.SingleComplex64Function\r\n    tensorflow/cc/gradients/math_grad_test.cc:84: Failure\r\n    Value of: grad_outputs[0].flat<complex64>()(0) == complex64(2,4)\r\n      Actual: false\r\n      Expected: true\r\n    [  FAILED  ] MathTest.SingleComplex64Function (39 ms)\r\n\r\nI find the calculated value of grad_outputs[0] is (2, -4) , is this correct?\r\n\r\n\r\nAccording to my knowledge, i have defined a function R -> R : y = x^2 (x and y are complex64),  the first derived function of it should be : dy/dx = 2x, and according to the definition of gradient: grad y(x) = [dy/dx], so if z=1+2i, y should be (1+2i)(1+2i)=1+4i+4i*i =1+4i-4=-3+4i,  and dy/dx = 2(1+2i)=2+4i, so theoretically grad y should be [(2,4)].\r\n", "comments": ["I have found a issue talking about the similar problem: https://github.com/tensorflow/tensorflow/issues/3850\r\nbut still i don't understand girving's explanation.\r\n\r\nSuppose x = x_real + i * x_imag, y = y_real + i * y_imag,  i have also found out what AddSymbolicGradients really try to calculate is something like z = dy_real/dx_real + i * dy_real/dx_imag. \r\n\r\nIs this the definition of gradient for complex-valued function in tensorflow ?\r\n", "This is not a bug, and yes , what is really output is dy_real/dx_real + i * dy_real/dx_imag for complex-valued functions y=f(x).\r\n\r\nI think the key to your question is the definition of gradient. let me explain it.\r\n\r\n* for real-valued R -> R funciton f:  y = f(x) , the gradient of f w.r.t x is grad f(x) = df/dx,  \r\n\r\nSuppose what we aim to minimize a graph's output L, and L = z, z = z(y), y = y(x), we know that dz/dx = dz/dy * dy/dx, \r\nso we can calculate gradient w.r.t any variant in this graph like this :\r\n\r\n```\r\ngrad L(z) = dL/dz = 1\r\ngrad L(y) = dL/dy = dL/dz * dz/dy = grad L(z) * dz/dy\r\ngrad L(x) = dL/dx = dL/dy * dy/dx = grad L(y) * dy/dx\r\n```\r\n\r\nthis is called the reversed calculation of gradients.\r\n\r\n\r\nBut for complexed-valued function C -> C: z = f(w),  if we define gradient f(w) = dz/dw, the chain rule can not apply directly.\r\n\r\nso, we change the definition:\r\n\r\n* for complex-valued C -> C function f: z = f(w), we define two gradient: the real gradient of f w.r.t w is grad_real f(w) = dz_real/dw_real + i * dz_real/dw_imag, \r\nthe imaginary gradient of f w.r.t w is grad_imag f(w) = dz_imag/dw_real + i * dz_imag/dw_imag.\r\n\r\n\r\nNow, let's see how to calcualte the real gradient and the imag gradient.\r\n\r\nSimilarly suppose we get a graph: L = z, z = z(y), y = y(x),  L,z,y,x are all complex variants. By the definition, we do it like this:\r\n\r\n```\r\ngrad_real L(z) = (dz_real/dz_real + i * dz_real/dz_imag) = (1, 0)\r\ngrad_imag L(z) = (dz_imag/dz_real + i * dz_imag/dz_real) = (0, i)\r\n\r\ngrad_real L(y) = (dz_real/dy_real + i * dz_real/dy_imag) \r\n= (1, 0) * (dz_real/dy_real + i * dz_real/dy_imag)\r\n\r\nAnd we know that for a analytic fucntion z = z(y) , wo get Cauchy\u2013Riemann equations:\r\ndz_real/dy_real = dz_imag/dy_imag\r\ndz_real/dy_imag = - dz_imag/dz_real\r\n\r\nand we know that :\r\ndz/dy = dz_real/dy_real + i * dz_imag/dz_real\r\n      = dz_real/dy_real - i * dz_real/dy_imag\r\n      = dz_imag/dy_imag + i * dz_imag/dy_real\r\n      = dz_imag/dy_imag - i * dz_real/dy_imag\r\n\r\nso, \r\ngrad_real L(y) = (1, 0) * (dz_real/dy_real + i * dz_real/dy_imag) \r\n               = grad_real(z) * conjugate(dz_real/dy_real - i * dz_real/dy_imag)\r\n               = grad_real(z) * conjugate(dz/dy)\r\n\r\n\r\ngrad_imag L(y) = (dz_imag/dy_real + i * dz_imag/dy_imag) \r\n               = (0, i) * (dz_imag/dy_imag - i * dz_imag/dy_real)\r\n               = grad_imag(z) * conjugate(dz_imag/dy_imag + i * dz_imag/dy_real)\r\n               = grad_imag(z) * conjugate(dz/dy)\r\n\r\n\r\n\r\ngrad_real L(x) = dz_real/dx_real + i * dz_real/dx_imag\r\n= (dz_real/dy_real * dy_real/dx_real + dz_real/dy_imag * dy_imag/dx_real) + i * (dz_real/dy_real*dy_real/dx_imag + dz_real/dy_imag * dy_imag/dx_imag)\r\n= dz_real/dy_real * (dy_real/dx_real + i * dy_real/dx_imag) +  dz_real/dy_imag * (dy_imag/dx_real + i * dy_imag/dx_imag)\r\n= dz_real/dy_real * (dy_real/dx_real + i * dy_real/dx_imag) +  i * dz_real/dy_imag * (dy_imag/dx_imag - i * dy_imag/dx_real)\r\n= dz_real/dy_real * conjugate(dy_real/dx_real - i * dy_real/dx_imag) + i * dz_real/dy_imag * conjugate(dy_imag/dx_imag + i * dy_imag/dx_real)\r\n= dz_real/dy_real * conjugate(dy/dx) + i * dz_real/dy_imag * conjugate(dy/dx)\r\n= (dz_real/dy_real + i * dz_real/dy_imag) *  conjugate(dy/dx)\r\n= grad_real(y) * conjugate(dy/dx)\r\n\r\n\r\n\r\ngrad_real L(x) = dz_imag/dx_real + i * dz_imag/dx_imag\r\n= (dz_imag/dy_real * dy_real/dx_real + dz_imag/dy_imag * dy_imag/dx_real) + i * (dz_imag/dy_real * dy_real/dx_imag + dz_imag/dy_imag * dy_imag/dx_imag)\r\n= dz_imag/dy_real * (dy_real/dx_real + i * dy_real/dx_imag) + dz_imag/dy_imag * (dy_imag/dx_real + i * dy_imag/dx_imag)\r\n= dz_imag/dy_real * (dy_real/dx_real + i * dy_real/dx_imag) + i * dz_imag/dy_imag * (dy_imag/dx_imag - dy_imag/dx_real)\r\n= dz_imag/dy_real * conjugate(dy_real/dx_real - i * dy_real/dx_imag) + i * dz_imag/dy_imag * conjugate(dy_imag/dx_imag + dy_imag/dx_real)\r\n= dz_imag/dy_real * conjugate(dy/dx) + i * dz_imag/dy_imag * conjugate(dy/dx)\r\n= (dz_imag/dy_real + i * dz_imag/dy_imag) * conjugate(dy/dx)\r\n= grad_imag(y) * conjugate(dy/dx)\r\n```\r\n\r\nSumming up, for the graph: L = z, z = z(y), y = y(x), for any real variant t, we define L gradient w.r.t t is Grad L(t) = grad L(t), for any \r\ncomplex variant t we define L gradient w.r.t t Grad L(t) = grad_real L(t); because for any real variant x, x = conjugate(x), now we can calculate Grad L w.r.t any variant by the same chain rules :\r\n\r\n```\r\nGrad L(z) = 1,\r\nGrad L(y) = Grad L(z) * conjugate(dz/dy)\r\nGrad L(x) = Grad L(y) * conjugate(dy/dx)\r\n```\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "Can we close this issue?\r\n", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 44 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 20612, "title": "Fix: Typo in the release note", "body": "There is a small typo in the release note (RELEASE.md).\r\nThis patch fixes it.", "comments": []}, {"number": 20611, "title": "Adding matrix square root op", "body": "This PR adds the matrix square root op support requested in #9202. \r\n\r\nThe implementation calls the Eigen sqrt() method in a manner nearly identical to the matrix exp() and log() calls. The gradient computation is included, with an explanation given in the inline comments of linalg_grad.py. \r\n\r\nAny feedback would be much appreciated!", "comments": ["@jonathanwyatt16 Thanks a lot for the contribution. I will review this this week.", "@rmlarsen Thank you very much for the feedback! I\u2019ve just pushed a commit that I believe includes the requested changes.", "@rmlarsen do the changes look good to you?\r\n\r\n@jonathanwyatt16 could you resolve conflicts?", "@martinwicke Thanks for the comment, I\u2019ve just rebased this PR to resolve the merge conflicts.", "I've just rebased this PR again to resolve another merge conflict.", "Any update please ?", "I just triggered the tests. @rmlarsen can you check that your comments have been addressed?", "I've just rebased this PR again to resolve another merge conflict.", "Nagging Reviewer @rmlarsen: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "I've just rebased this PR again to resolve another merge conflict.", "I think you addressed @rmlarsen's comments. @rmlarsen, please speak up if I'm wrong.", "@jonathanwyatt16, could you look into the test failures?", "@martinwicke I\u2019ve just pushed a commit that I believe addresses six of the test failures--still troubleshooting the clang-format check failure. Would it be possible to trigger the tests again?", "Hi, \r\nI need this matrix square root badly. I was wondering, can I have it by building Tensorflow from source? \r\nAny help would be really appreciated since I am a newbie.  ", "@martinwicke Thanks for triggering the tests. I've just pushed a commit that I believe addresses the clang-format test failure. Would it be possible to trigger the tests again?\r\n\r\n@MarziEd I'm hopeful that we will be ready to merge this PR soon. Until then, feel free to clone the matrix_square_root branch from my fork and build it by following [this documentation](https://www.tensorflow.org/install/source)."]}, {"number": 20610, "title": "c++ inference time is different with python", "body": "I met a very puzzled question\uff0cImodel inference time is 16ms by python\uff0cand then I use c++ to get inference time is 50ms.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "It has been 34 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 20609, "title": "Add scope name to TF_AddGradients", "body": "Actually, adding gradients ops to a graph using `TF_AddGradients` always put them under the \"gradients/*\" scope. This PR is to allow a client to create the gradients under a different scope. \r\n\r\nIt also adds support of that feature into the Java client, so it is possible to do:\r\n```java\r\nops.gradients(scope.withSubScope(\"grads\"), output, inputs);  // -> \"grads/[GradientOpName]\" \r\n```\r\nNote that adding an op name to the scope don't make sense in this case and will simply be ignored\r\n```java\r\nops.gradients(scope.withSubScope(\"grads\").withName(\"op\"), output, inputs);  // -> same as above\r\n```", "comments": ["Hi Asim, \r\n\r\nJust to make sure that I understood correctly, you are suggesting that we should enforce prefix uniqueness in a graph? But aren't the suffixes '_#' only applied on the op names? \r\n\r\nI not too aware of the internal details for handling names clashing, I thought that prefixes worked a bit like folders. For example, calling twice `TF_AddGradients` could create a node named `gradients/OnesLike` in the first call and `gradients/OnesLike_1` in the second. Or are you saying that it will end up to be more something like `gradients/OnesLike` and `gradients_1/OnesLike`?\r\n\r\nI agree the second case would be confusing, or even problematic. Specially if you call `ops.gradients(...)` twice, you expect all gradients to be created under the same scope, like any other operator:\r\n```java\r\nOps sub = ops.withSubScope(\"sub\");\r\nsub.matMul(...); // => sub/MatMul\r\nsub.matMul(...); // => sub/MatMul_1\r\nsub.gradients(...); // => sub/OnesLike, etc. ??\r\nsub.gradients(...); // => sub/OnesLike_1 or sub_1/OnesLike, etc. ??\r\n```\r\n...yeah, maybe we need to have a talk to clarify all this :)\r\n", "Ok, sleeping on it, I understood that it would make sense to do `gradients/, gradients_1/, gradients_2/, etc.` since it is the best way to distinguish ops created by separate calls of `TF_AddGradients`.\r\n\r\nI have some suggestion in mind to make all that clear, I'll ping you soon to talk about it, thanks. ", "@asimshankar : friendly reminder to please take a look at the latest version of this PR, thanks!", "@asimshankar : Updated!"]}, {"number": 20608, "title": "Allow restoring subgraph from checkpoint for weight sharing between different models", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS Sierra\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.7.0-3-g024aecf414 1.7.0\r\n- **Python version**: 3\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: Read the summary below\r\n\r\n### Describe the problem\r\nThere are many scenarios where we want to do parameter sharing between two different models (let's call `A` and `B`). If `A` is trained and checkpointed, we can either load all of the variables of `A` or selectively load variables from A's checkpoint by passing a list `L` containing all shared params. But the second way is not helpful if we don't know `L` beforehand. Can we have a way by which we can share variables just based on the common subset between two graphs.\r\nPS - I am assuming that parameters of `A` and `B` follow same naming convention in scopes.\r\n\r\n### Source code / logs\r\nHere is what I was trying to do using metagraph but this also doesn't seem to work. Please note that I deliberately do `tf.reset_default_graph()` as objective is to share parameter with a checkpointed model. Sorry lots of redundant lines in the code.\r\n```\r\nimport tensorflow as tf\r\nmnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\r\n\r\ndef model_1(features, labels):\r\n    with tf.variable_scope(\"input\", reuse=tf.AUTO_REUSE):\r\n        out = tf.reshape(features, [-1, 28, 28, 1])\r\n    with tf.variable_scope(\"conv_1_5x5\", reuse=tf.AUTO_REUSE):\r\n        out = tf.layers.conv2d(\r\n           inputs=out,\r\n           filters=32,\r\n           kernel_size=[5, 5],\r\n           padding=\"same\",\r\n           activation=tf.nn.relu)\r\n    with tf.variable_scope(\"conv_2_3x3\", reuse=tf.AUTO_REUSE):\r\n        out = tf.layers.conv2d(\r\n           inputs=out,\r\n           filters=32,\r\n           kernel_size=[3, 3],\r\n           padding=\"same\",\r\n           activation=tf.nn.relu)\r\n    with tf.variable_scope(\"pool_3_2x2\", reuse=tf.AUTO_REUSE):\r\n        out = tf.layers.max_pooling2d(inputs=out, pool_size=[2, 2], strides=2)\r\n    with tf.variable_scope(\"conv_4_3x3\", reuse=tf.AUTO_REUSE):\r\n        out = tf.layers.conv2d(\r\n           inputs=out,\r\n           filters=64,\r\n           kernel_size=[3, 3],\r\n           padding=\"same\",\r\n           activation=tf.nn.relu)\r\n    with tf.variable_scope(\"dense\", reuse=tf.AUTO_REUSE):\r\n        flattened = tf.reshape(out, [-1, 14 * 14 * 64])\r\n        logits = tf.layers.dense(flattened, units=10, activation=tf.nn.relu)\r\n    softmax = tf.nn.softmax(logits, name=\"softmax\")\r\n\r\n    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\r\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\r\n    train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_or_create_global_step())\r\n    return loss, train_op\r\n\r\ndef model_2(features, labels, old_graph):\r\n    with old_graph.as_default():\r\n        with tf.variable_scope(\"input\", reuse=tf.AUTO_REUSE):\r\n            out = tf.reshape(features, [-1, 28, 28, 1])\r\n        with tf.variable_scope(\"conv_1_5x5\", reuse=tf.AUTO_REUSE):\r\n            out = tf.layers.conv2d(\r\n               inputs=out,\r\n               filters=32,\r\n               kernel_size=[5, 5],\r\n               padding=\"same\",\r\n               activation=tf.nn.relu)\r\n        with tf.variable_scope(\"conv_2_3x3\", reuse=tf.AUTO_REUSE):\r\n            out = tf.layers.conv2d(\r\n               inputs=out,\r\n               filters=32,\r\n               kernel_size=[3, 3],\r\n               padding=\"same\",\r\n               activation=tf.nn.relu)\r\n        with tf.variable_scope(\"pool_3_2x2\", reuse=tf.AUTO_REUSE):\r\n            out = tf.layers.max_pooling2d(inputs=out, pool_size=[2, 2], strides=2)\r\n        # This is the place where model 1 and 2 are actually different\r\n        with tf.variable_scope(\"conv_4_5x5\", reuse=tf.AUTO_REUSE):\r\n            out = tf.layers.conv2d(\r\n               inputs=out,\r\n               filters=64,\r\n               kernel_size=[5, 5],\r\n               padding=\"same\",\r\n               activation=tf.nn.relu)\r\n        with tf.variable_scope(\"dense\", reuse=tf.AUTO_REUSE):\r\n            flattened = tf.reshape(out, [-1, 14 * 14 * 64])\r\n            logits = tf.layers.dense(flattened, units=10, activation=tf.nn.relu)\r\n        softmax = tf.nn.softmax(logits, name=\"softmax\")\r\n\r\n    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\r\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\r\n    train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_or_create_global_step())\r\n    return loss, train_op\r\n\r\nfeatures = tf.placeholder(shape=[None, 784], dtype=tf.float32, name=\"features\")\r\nlabels = tf.placeholder(shape=[None], dtype=tf.int32, name=\"labels\")\r\n\r\nsteps = 0\r\nloss_op, train_op = model_1(features, labels)\r\n\r\nwith tf.train.MonitoredTrainingSession(checkpoint_dir=\"./model_1\") as sess:\r\n    while steps <= 20:\r\n        features_, labels_ = mnist.train.next_batch(100)\r\n        loss, _ = sess.run([loss_op, train_op], feed_dict={\r\n            \"features:0\": features_, \"labels:0\": labels_})\r\n        if steps % 5 == 0:\r\n            print(\"loss at step {} = {}\".format(steps, loss))\r\n        steps += 1\r\n\r\ntf.reset_default_graph()\r\ntf.train.import_meta_graph(\"./model_1/model.ckpt-\"+str(steps)+\".meta\")\r\nold_graph = tf.get_default_graph()\r\n\r\nsteps = 0\r\nloss_op, train_op = model_2(features, labels, old_graph)\r\n\r\nwith tf.train.MonitoredTrainingSession(checkpoint_dir=\"./model_2\") as sess:\r\n    while steps <= 20:\r\n        features_, labels_ = mnist.train.next_batch(100)\r\n        loss, _ = sess.run([loss_op, train_op], feed_dict={\r\n            \"features:0\": features_, \"labels:0\": labels_})\r\n        if steps % 5 == 0:\r\n            print(\"loss at step {} = {}\".format(steps, loss))\r\n        steps += 1\r\n\r\n", "comments": ["If you can organize the \"subgraphs\" into `tf.keras.Model` objects, then object-based checkpointing ([`tf.train.Checkpoint` in TF 1.9](https://www.tensorflow.org/versions/r1.9/api_docs/python/tf/train/Checkpoint)) is hopefully the most intuitive.\r\n\r\nHowever, this is something that's in the early stages of being exposed and we haven't finished integration with `MonitoredSession` etc. just yet. @allenlavoie will have updates on that end, so assigning to him.\r\n\r\nAs an example of how things might look:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nL = tf.keras.layers # Shorthand\r\n\r\ndef make_shared_model():\r\n    return tf.keras.Sequential([\r\n        L.Conv2D(filters=32, kernel_size=(5, 5), padding='same', activation=tf.nn.relu, input_shape=[28, 28, 1]),\r\n        L.Conv2D(filters=32, kernel_size=(3,3), padding='same', activation=tf.nn.relu),\r\n        L.MaxPool2D(pool_size=(2,2), strides=2)])\r\n\r\ndef make_model1():\r\n    return tf.keras.Sequential([\r\n        make_shared_model(),\r\n        L.Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation=tf.nn.relu),\r\n        L.Flatten(),\r\n        L.Dense(10)])\r\n\r\nmodel1 = make_model1()\r\n# Create a checkpoint with just the shared portion, which is model1.layers[0]\r\nckpt1 = tf.train.Checkpoint(m = model1.layers[0])\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    for step in range(20):\r\n        # Get your data and train model1\r\n        # Save a checkpoint of the shared components\r\n        pass\r\n    ckpt1.save(\"/tmp/checkpoint\")\r\n\r\n\r\n# Could have done all this in a single graph, but demonstrating two graphs\r\n# just to be parallel to the code you had above\r\ntf.reset_default_graph()\r\n\r\ndef make_model2():\r\n    return tf.keras.Sequential([\r\n        make_shared_model(),\r\n        L.Conv2D(filters=64, kernel_size=(5, 5), padding='same', activation=tf.nn.relu),\r\n        L.Flatten(),\r\n        L.Dense(10)])\r\n\r\nmodel2 = make_model2()\r\n# Since model1.layers[0] and model2.layers[0] are isomorphic objects, they can\r\n# save and load checkpoints compatible with each other\r\nckpt2 = tf.train.Checkpoint(m = model2.layers[0])\r\nstatus = ckpt2.restore(\"/tmp/checkpoint\")\r\n\r\nwith tf.Session() as sess:\r\n  status.initialize_or_restore()\r\n  # At this point, the weights for the common portion have been restored\r\n```\r\n\r\nHope this is helpful while we make object based checkpointing more pervasive.", "@tremblerz,\r\nThis can be easily achieved in **`Tensorflow 2.x`** using [tf.keras.model.save_weights](https://www.tensorflow.org/api_docs/python/tf/keras/Model#save_weights) and then reuse those **`Weights`** by **`Loading the Weights`** using [tf.keras.model.load_weights](https://www.tensorflow.org/api_docs/python/tf/keras/Model#load_weights). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 20607, "title": "Performance drop for CPU graph calculation after upgrading from 1.3 to newer versions", "body": "- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS 7\r\n- **TensorFlow installed from (source or binary)**: binary (pip)\r\n- **TensorFlow version (use command below)**: 1.4 and newer for CPU\r\n- **Python version**: 2.7.5 3.5.4\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: python test1.py\r\n\r\n### Describe the problem\r\n\r\nInitially I found training using CPU in TensorFlow 1.8 is slow and it appeared only one thread was used. I can confirm multi-threading for individual operations works well (like multiplying two large matrices). But it seems only one operation can be done at the same time. Specifying inter_op_parallelism_threads doesn't help. For version 1.8 and 1.9rc2 and the following test code I wrote, the CPU usage was almost consistently 100% (1 core) according to \"top\". For version 1.3 and 1.0, it was obvious that the following code used multiple cores and ran significantly faster. I reproduced most results with both Python versions. Here are my test results:\r\n- Version time printed\r\n- 1.0, 1.3: 20s\r\n- 1.4, 1.5: 50s\r\n- 1.6: 60s\r\n- 1.7, 1.8: 80s\r\n- 1.9 rc2: 130s\r\n\r\nThe above results were based on a 64GB, 20-core (2x E5 without hyperthreading) machine. I also tried on an AWS t2.2xlarge instance (32GB, 8-core) with Ubuntu 16.04, Python 3.5.2, TensorFlow 1.3 and 1.9 and got similar results.\r\n\r\n### Source code / logs\r\n```\r\nimport tensorflow as tf\r\nimport time\r\nA=[None]*100000\r\nB=[None]*100000\r\nfor i in range(0,2):\r\n\tA[i]=tf.Variable(tf.random_normal([100,100]))\r\n\tB[i]=tf.Variable(tf.random_normal([100,100]))\r\nfor i in range(2,100000):\r\n\tA[i]=tf.matmul(A[i-1],A[i-2])\r\n\tB[i]=tf.matmul(B[i-1],B[i-2])\r\nc=tf.matmul(A[-1],B[-1])\r\nprint('graph created')\r\n#the config doesn't help\r\nconfig=tf.ConfigProto()\r\nconfig.intra_op_parallelism_threads = 4\r\nconfig.inter_op_parallelism_threads = 10\r\nt=time.time()\r\ns=tf.Session(config=config)\r\ns.run(tf.global_variables_initializer())\r\ns.run(c)\r\nprint(tf.__version__)\r\nprint(time.time()-t)\r\n```\r\n\r\nAppended:\r\nI just tried to collect the timeline in 1.3, 1.8 and 1.9rc. They all look similar and reported 5-10 seconds with good parallelism. Maybe the issue is caused by overhead from other parts like scheduling?\r\nTimeline:\r\n<img width=\"943\" alt=\"timeline\" src=\"https://user-images.githubusercontent.com/10559772/42405897-56f3c342-81d0-11e8-9a8b-bf4a08a6a6c7.png\">\r\nTimeline enlarged:\r\n<img width=\"886\" alt=\"timeline_enlarged\" src=\"https://user-images.githubusercontent.com/10559772/42405908-6ec434c0-81d0-11e8-871b-a25a559dc745.PNG\">\r\n", "comments": ["This seems like a reasonable plot. Have you tried the `tf.while_loop` construct? I'm not sure what the realistic scenario you're trying to handle.", "It has been 44 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 20606, "title": "[bug] events.out.tfevents files do not get closed.", "body": "Running a python script that trains and tests MANY models (tf.Estimator) failes with\r\n```\r\ntf.estimator Error: ResourceExhausted: too many open files (TF keeps events.out.tfevents files open)\r\n```\r\n\r\nStackoverflow question & proposed solution (by another person): https://stackoverflow.com/questions/50956551/tf-estimator-error-resourceexhausted-too-many-open-files-tf-keeps-events-out\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 1604\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: \r\n== cat /etc/issue ===============================================\r\nLinux gpubox1 4.4.0-128-generic #154-Ubuntu SMP Fri May 25 14:15:18 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.4 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux gpubox1 4.4.0-128-generic #154-Ubuntu SMP Fri May 25 14:15:18 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy                              1.14.3   \r\nnumpydoc                           0.8.0    \r\nprotobuf                           3.6.0    \r\ntensorflow                         1.9.0rc0 \r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.9.0-rc0\r\ntf.GIT_VERSION = b'v1.8.0-3463-g39ea5a7'\r\ntf.COMPILER_VERSION = b'v1.8.0-3463-g39ea5a7'\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nFri Jul  6 21:20:55 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 396.26                 Driver Version: 396.26                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 108...  Off  | 00000000:01:00.0  On |                  N/A |\r\n| 42%   61C    P2    63W / 250W |  10664MiB / 11175MiB |     24%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce GTX 108...  Off  | 00000000:02:00.0 Off |                  N/A |\r\n| 29%   44C    P2    56W / 250W |  10631MiB / 11178MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1092      G   /usr/lib/xorg/Xorg                            89MiB |\r\n|    0     19389      C   python                                     10563MiB |\r\n|    1     19389      C   python                                     10619MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-9.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0.176\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart_static.a\r\n- **Bazel ver:** NA\r\n- **CUDA/cuDNN**: 9.0 / 7.1\r\n- **GPU**: NVIDIA 1080 Ti\r\n\r\n- **Exact command to reproduce**: N/A; Running  `tf.Estimator.train ` for some 1000 different estimators reproduces the problem.\r\n\r\n### Describe the problem\r\nEach tensorflow run does not close events.out.tfevent. Instead, they remain open, and eventually stops the process because there are too many open file handles.\r\n\r\n### Source code / logs\r\nI'll do that on request\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory", "@nfelt @jart : Mind taking a look?\r\n\r\n@nfelt : Will the move to the resource-based summaries affect this?", "@renxida I assume you are using different model directories for every Estimator?  The open FileWriters are cached in FileWriterCache, you could manually clear it which I think should close the open file handles: https://www.tensorflow.org/api_docs/python/tf/summary/FileWriterCache\r\n\r\n@asimshankar I'm working on Estimator changes that will cause it to stop using FileWriterCache which should probably reduce the likelihood of this issue, though I haven't looked at that aspect of the change in particular.\r\n", "@nfelt neat! I will try that. We'll see how it goes.", "@renxida,\r\n Can you please let us know if [this comment](https://github.com/tensorflow/tensorflow/issues/20606#issuecomment-403994951) has resolved your issue? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/20606\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/20606\">No</a>\n"]}, {"number": 20605, "title": "Quantize: lacking min/max data, when i transform the quantize graph to tflite", "body": "tensorflow version: 1.8\r\nmodel: my model structure is very simliar to mobilenet-v1\r\ndesc: I trained my model use the tf.contrib.quantize.create_training_graph function, and call the tf.contrib.quantize.create_eval_graph() when i generate the eval graph.\r\nAnd then I use \r\nbazel-bin/tensorflow/contrib/lite/toco/toco \\\r\n--input_file=/home/admin_pc/model_test/output_quant.pb \\\r\n--output_file=/home/admin_pc/model_test/mobilenet_qg.tflite \\\r\n--input_fromat=TENSORFLOW_GRAPHDEF \\\r\n--output_format=TFLITE \\\r\n--inference_type=QUANTIZED_UINT8 \\\r\n--input_array=image \\\r\n--output_array=Openpose/MConv_Stage3_L2_5_pointwise/BatchNorm/FusedBatchNorm \\\r\n--input_shape=1,224,224,3 \\\r\n--change_concat_input_ranges=false \\\r\n--std_value=128 --mean_value=128\r\nto generate the tflite\r\n\r\nbut some errores occurs:\r\n\r\n2018-07-07 12:34:56.116401: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 58 operators, 169 arrays (1 quantized)\r\n2018-07-07 12:34:56.117540: F tensorflow/contrib/lite/toco/tooling_util.cc:1581] Array MobilenetV1/Conv2d_1_depthwise/depthwise, which is an input to the Conv operator producing the output array MobilenetV1/Conv2d_1_pointwise/Relu6, is lacking min/max data, which is necessary for quantization. Either target a non-quantized output format, or change the input graph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you do not care about the accuracy of results.\r\nAborted (core dumped)\r\n", "comments": ["Could you be more specific on what operations you are talking about? Is this for tensorflow lite's quantization or the tensorflow operations?\r\n\r\nIf this is for tensorflow, the relu6 operations have the quantization range fixed at 0 and 6 which helps quantize to good accuracy. Relu can also be quantized but needs the quantization range to be known. This can be accomplished with TensorFlow Lite's quantized models https://www.tensorflow.org/performance/quantization \r\n\r\nTFLite supports quantization of ReLU and ReLU6 so you should consider using that.", "I am very sorry to bother you. i will modify my question soon.", "@suharshs looking forward to your reply", "it's all of my faults", "Out of curiosity, what was the issue?", "well,I don't know it's a bug or my fault when i  roughed read the paper (Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference) today . I will describe the problem detailed.\r\n\r\nstandard mobilenet-v1 structure:  depthwise conv - bn - relu - pointwise conv - bn - relu\r\n\r\nmy model structure: depthwise conv - pointwise conv - bn - relu \r\n\r\nSo, i changed my model structure to the standard mobilenet-v1 structure.  Problem solved.\r\n\r\nIt looks like that i need the standard mobilenet-v1 structure which i want to quantize the model.\r\n\r\nI read the paper 'Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference' today. The paper said that we can quantize the  conv layer without a bn. \r\n\r\nSo it's a bug or that we must add a bn/relu layer after the conv layer which we want to quantize.\r\n\r\n@suharshs looking forward to your reply.\r\n", "@suharshs can you answer my question ? T.T"]}, {"number": 20604, "title": "First push", "body": "", "comments": ["thanks @martinwicke. Yea, will go through them and clean up the stale files later."]}, {"number": 20603, "title": "Fix the order of adding const nodes, and avoid adding them multiple times.", "body": "", "comments": []}, {"number": 20602, "title": "Different accuracy for quantized model when inferring from one image vs batch", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: pip\r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**: gtx 1080ti\r\n- **Exact command to reproduce**:\r\n\r\n```\r\n!python quantize_graph.py \\\r\n--input=logs_vgg/frozen_model.pb \\\r\n--output_node_names=\"vgg_16/fc8/squeezed\" --print_nodes --output=logs_vgg/quantized_graph.pb \\\r\n--mode=eightbit --logtostderr\r\n```\r\n```\r\ntf.reset_default_graph()\r\ntf.set_random_seed(0)\r\ng2 = load_graph('logs_vgg/quantized_model.pb')\r\nctr = 0\r\nwith g2.as_default():\r\n    with tf.Session(graph=g2) as sess:\r\n        restored_x = g2.get_tensor_by_name('import/Inputs:0')\r\n        restored_logits = g2.get_tensor_by_name('import/vgg_16/fc8/squeezed:0')\r\n        times = []\r\n        for i in range(num_images):\r\n            t0 = time.time()\r\n            a = sess.run(restored_logits, feed_dict={restored_x:np.expand_dims(images[i, :, :, :], axis=0)})\r\n            t1 = time.time()\r\n            total_time = t1 - t0\r\n            times.append(total_time)\r\n            pred = np.argsort(a[0])\r\n            pred = pred[-5:]\r\n            if y[i] in set(pred):\r\n                ctr += 1   \r\nprint(\"TF Prediction acc:\" + str(ctr/float(num_images)))\r\nprint(\"Inference time(ms): \" + str((sum(times[1:])/float(num_images - 1)) * 1000))\r\n```\r\nTF Prediction acc:0.79\r\nInference time(ms): 56.0006372856\r\n\r\n```\r\ntf.reset_default_graph()\r\ntf.set_random_seed(0)\r\ng = load_graph('logs_vgg/quantized_model.pb')\r\nwith g.as_default():\r\n    with tf.Session(graph=g) as sess:\r\n        restored_x = g.get_tensor_by_name('import/Inputs:0')\r\n        restored_logits = g.get_tensor_by_name('import/vgg_16/fc8/squeezed:0')\r\n        t0 = time.time()\r\n        a = sess.run(restored_logits, feed_dict={restored_x:images})\r\n        t1 = time.time()\r\n        total_time = t1 - t0\r\nctr = 0\r\nfor i in range(num_images):\r\n    pred = np.argsort(a[i])\r\n    pred = pred[-5:]\r\n    if y[i] in set(pred):\r\n        ctr += 1   \r\nprint(\"TF Prediction acc:\" + str(ctr/float(num_images)))\r\nprint(\"Inference time(ms): \" + str(total_time/float(num_images) * 1000))\r\n```\r\n\r\n\r\n TF Prediction acc:0.76\r\nInference time(ms): 406.530380249\r\n\r\n\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nDifferent accuracy for quantized model when inferring from one image vs batch\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Are you sure you are comparing the exact same images? If so, you should be able to find a single image that gets a different prediction in single image and batch.", "Yes, the images are the same.", "This only happens with a quantized model, for a regular model, this is not reproducible.", "This is because the quantize_graph does dynamic quantization per input tensor. So if the batch is larger then the min max will be worse for accuracy. This is working as intending as quantization efforts have been focused to TFLite. https://www.tensorflow.org/performance/quantization\r\n\r\nWe will also be offering some easier tools for quantization soon that should help with this.", "Is there any efforts for using tf.lite on GPU ?"]}, {"number": 20601, "title": "Branch 203518000", "body": "", "comments": []}]