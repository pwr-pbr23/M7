[{"number": 15558, "title": "ONNX Model Support", "body": "Please add support for the [ONNX](https://github.com/onnx/onnx) open model standard or a conversion tool would be great.", "comments": ["Closing as duplicate of #12888."]}, {"number": 15556, "title": "Little error in example how to read data", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.3.0-rc2-20-g0787eee 1.3.0\r\n- **Python version**: 3.5.4\r\n- **Exact command to reproduce**: \r\n\r\n```\r\npython tensorflow/tensorflow/examples/how_tos/reading_data/convert_to_record.py\r\npython tensorflow/tensorflow/examples/how_tos/reading_data/fully_connected_reader.py\r\n```\r\n\r\n### Describe the problem\r\n\r\nError when doing the above second command: `AttributeError: module 'tensorflow' has no attribute 'data'` The data attribute is actually in the contrib module then you should change the line 108 of the file `tensorflow/tensorflow/examples/how_tos/reading_data/fully_connected_reader.py`:\r\n\r\n`dataset = tf.data.TFRecordDataset(filename)`\r\nto\r\n`dataset = tf.contrib.data.TFRecordDataset(filename)`\r\n\r\nSorry, I don't know if I should have submitted a pull request for such a little change.\r\n\r\n", "comments": ["In fact, the docs are correct: these APIs moved from `tf.contrib.data` to `tf.data` in TensorFlow 1.4.", "Sorry but it is not the case on github: https://github.com/tensorflow/tensorflow/tree/r1.4/tensorflow . Or maybe I didn't understand what you said.", "It's imported here: https://github.com/tensorflow/tensorflow/blob/438604fc885208ee05f9eef2d0f2c630e1360a83/tensorflow/python/__init__.py#L81\r\n\r\n(From your **System Information**, it looks like you're using TensorFlow 1.3, which is an older version.)", "Ok, sorry. I got it thank you."]}, {"number": 15555, "title": "RGB<->YIQ colorspace conversion", "body": "Implementation of functions for colorspace conversion RGB <-> YIQ, according to [https://en.wikipedia.org/wiki/YIQ#Formulas](Wikipedia).\r\nThis PR adds CPP functions and python wrappers in tf.image namespace", "comments": ["Can one of the admins verify this patch?", "Thanks @mystic123! @martinwicke would you be able to review this PR?", "Yeah, I will fix this issues and implement YUV in the way you suggested ;)", "@mystic123 did the last comment resolve all comments?", "@drpngx Yes, it resolves all issues", "Thanks! OK, then it should be ready for review @martinwicke ", "Sorry, I should have thought of this earlier, but it appears to me that ColorTransform is a special case of `tf.tensordot` -- could you simply use that in your python conversion functions? Then we save ourselves an extra kernel to build, test, and maintain.", "@martinwicke sure, I thought about it as well but you suggested implementing kernel so I did it that way. No problem, I will refactor python wrappers to user tensorflow ops", "@drpngx @martinwicke last commit adds implementation which uses tensordot op", "(Okay for API review)", "Hooray! Thank you!", "`image_ops_test` fails:\r\n\r\n```\r\nERROR: testBatch (__main__.RGBToYUVTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/tmpfs/tmp/bazel/execroot/org_tensorflow/bazel-out/k8-py3-opt/bin/tensorflow/python/image_ops_test.runfiles/org_tensorflow/tensorflow/python/ops/image_ops_test.py\", line 126, in testBatch\r\n    inp = np.random.randint(0, 255, *shape).astype(nptype) / 255\r\n  File \"mtrand.pyx\", line 905, in mtrand.RandomState.randint\r\nTypeError: randint() takes at most 4 positional arguments (6 given)\r\n\r\n```", "@drpngx I fixed that bug, now all tests should pass", "Thanks @mystic123 !\r\n\r\nJenkins, test this please", "```\r\n======================================================================\r\nERROR: testBatch (__main__.RGBToYUVTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/k8-py3-opt/bin/bazel_pip/tensorflow/python/image_ops_test.runfiles/org_tensorflow/bazel_pip/tensorflow/python/ops/image_ops_test.py\", line 126, in testBatch\r\n    inp = np.random.rand(shape).astype(nptype)\r\n  File \"mtrand.pyx\", line 1363, in mtrand.RandomState.rand\r\n  File \"mtrand.pyx\", line 861, in mtrand.RandomState.random_sample\r\n  File \"mtrand.pyx\", line 167, in mtrand.cont0_array\r\nTypeError: 'tuple' object cannot be interpreted as an integer\r\n\r\n----------------------------------------------------------------------\r\nRan 30 tests in 1.281s\r\n\r\nFAILED (errors=1)\r\narea_ratio_hist  [247 325 299 258 237 221 169 127  83  34]\r\n================================================================================\r\n```", "@drpngx now it should work", "Jenkins, test this please.", "@mystic123 please fix the goldens:\r\n\r\n```\r\n    $ bazel build tensorflow/tools/api/tests:api_compatibility_test\r\n    $ bazel-bin/tensorflow/tools/api/tests/api_compatibility_test \\\r\n          --update_goldens True\r\n```", "@drpngx \r\nI executed that command, and everything is ok:\r\n\r\n```\r\n$ bazel-bin/tensorflow/tools/api/tests/api_compatibility_test --update_goldens True\r\ns.s.\r\n----------------------------------------------------------------------\r\nRan 4 tests in 0.001s\r\n\r\nOK (skipped=2)\r\n```\r\n\r\n\r\n```\r\n$ git status\r\n\r\nOn branch feature/rgb_to_yiq\r\nYour branch is up-to-date with 'origin/feature/rgb_to_yiq'.\r\nnothing to commit, working directory clean\r\n```\r\n", "Can you run this on Mac or Linux, with Python2? The compatibility tests don't execute on Py3 (since what Py3 would return is different).", "@martinwicke it looks like api_compatibilty failures. It looks like the goldens were not updated. @mystic123 can you try updating them one more time?", "@drpngx @martinwicke @rmlarsen thank you for your help! :) \r\nI ran api_compatibility_test on Python2 in docker with Ubuntu.", "@mystic123 Thanks!"]}, {"number": 15554, "title": "Tensorflow Lite exhibits longer inference time when build with Android NN API on Google Pixel 1", "body": "\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n- **TensorFlow version (use command below)**:\r\n'v1.3.0-rc1-6055-gfdf34a8', '1.4.0'\r\n- **Python version**: \r\n2.7.12\r\n- **Bazel version (if compiling from source)**:\r\n0.8.0\r\n- **GCC/Compiler version (if compiling from source)**:\r\nGCC 5.4.0-6ubuntu1~16.04.5\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\nWhen enabling the NN API usage (edit interpreter.cc) one would expect that due to the HW acceleration, inferece times would be shorter. However, exactly the opposite happens. I tried this using the included demo app, using mobilenet_quant_v1_224.tflite and a custom (not quantized) model that I converted to tflite. In the mobilenet case, inference takes around 80ms without NNAPI, and ~100ms with NNAPI . My custom model, which sadly I cannot share , takes 40ms without , and ~90ms with NNAPI.\r\n\r\nPlease note that I have also tested this with FAST_SINGLE_ANSWER  and SUSTAINED_SPEED  NNAPI preference settings. There was no significant change in inference times.\r\n\r\nMy Pixel Build number is: OPM1.171019.011\r\n\r\n### Source code / logs\r\n--- a/tensorflow/contrib/lite/interpreter.cc\r\n+++ b/tensorflow/contrib/lite/interpreter.cc\r\n@@ -51,7 +51,7 @@ Interpreter::Interpreter(ErrorReporter* error_reporter)\r\n   tensors_.reserve(kSlotsToReserve);\r\n   nodes_and_registration_.reserve(kSlotsToReserve);\r\n   next_allocate_node_id_ = 0;\r\n-  UseNNAPI(false);\r\n+  UseNNAPI(true);\r\n", "comments": ["Thanks for reporting this.\r\n\r\nPixel 2016 currently does not have a hardware driver for NN API. So your model is actually running on CPU, plus some overhead for model initialization.\r\n\r\nAnother source of overhead is for input and output tensors. If the buffer is not already created as shared memory, an copy might occur during invocation. We will keep improving the integration to make it more efficient.", "Thanks. Which devices do have a HW acceleration support?\n\nOn Dec 22, 2017 02:30, \"Miao Wang\" <notifications@github.com> wrote:\n\n> Thanks for reporting this.\n>\n> Pixel 2016 currently does not have a hardware driver for NN API. So your\n> model is actually running on CPU, plus some overhead for model\n> initialization.\n>\n> Another source of overhead is for input and output tensors. If the buffer\n> is not already created as shared memory, an copy might occur during\n> invocation. We will keep improving the integration to make it more\n> efficient.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15554#issuecomment-353491740>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AMnuX4BqkDv8UpYWsNUMQshDEm9Dmp9qks5tCvgxgaJpZM4RJyeJ>\n> .\n>\n", "@miaowang14 Can you please share, is there a device available today that does have a working NN API hw acceleration ? I have a pretty big decision to make based on this information. Thanks!", "@miaowang14 @aselle is there a list of hardware/phones where TF actually uses GPU or DSP? I think TF light is a great idea but without hardware support I m not sure how much adoption it will see.\r\n\r\nIf you know of at least one supported phone we can get started developing with that! I would very much like to work with TF as opposed to a custom solution but we will need HW support :-)", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@miaowang14  just a gentle reminder. Any update you could share? Or at least an indication  if this issue is of priority?", "mkassner, there are many devices coming out with hardware acceleration.  I'll see if we can get permission to share a list of devices.\r\n\r\nThanks for looking into TFLite and the NNAPI.", "@jeanlucbr  a list of supported devices would be great! Will some of the devices be released this year?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@mkassner Unfortunately, I could not get permission from the vendors to reveal the devices, as many are unannounced.  Keep your eyes open, I expect multiple announcements soon.  It's worth the wait; the acceleration is significant.", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "So does the Pixel 2 XL also does not feature a hardware driver for nnapi? ", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "We are recently doing phone ai benchmarking and checking the support of NNAPI on various android devices. The situation is now quite sad: in brief, only Huawei phones with Kirin's 970 NPU have drivers for it, while other devices are still lacking them. In general, the situation with TFLite/NNAPI is even more curious:\r\n\r\n1) TF Lite is ~1.5-2 times slower and consumes ~1.5-2 times more RAM than TF Mobile.\r\n2) If drivers for NNAPI are not available, this slows TFLite down by another 1.5-2 times.\r\n3) NNAPI supports only a subset of TF Lite operations, therefore the majority of models will crush after running with nnapi acceleration - even Inception-v1 and MobileNets are currently not supported, not to mention more complicated models.\r\n\r\nWe are trying to keep track of the situation and are publishing the current performance of tflite/tfmobile on various devices on our webpage: http://ai-benchmark.com/ranking. The phones supporting nnapi are denoted by green (currently it's only huawei), and as soon as new devices with acceleration support are released they will be added to this rating (btw, you can also find the running time and memory requirements for several classification and image-to-image translation networks on this page). If you need to check whether your phone supports nnapi, you can also run the app on it and at the end you will either get a message that it is supported, or a warning that it lacks the drivers.\r\n\r\nHope this helps.", "@aiff22 \r\nThank you for the report.\r\n\r\nWhich model are you benchmarking on these devices.\r\n\r\nAre you using the Java API?\r\n - TF Lite being 1.5- 2 times slower seems unlikely, I want to make sure if you are setting the number of threads correctly. We recently added this to our Java API, please give it a try.\r\n- Java API is slower if you are not using ByteBuffer, there is a warning in the Javadoc about this:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/java/src/main/java/org/tensorflow/lite/Interpreter.java#L134\r\nMake sure you are using the right format for inputs.\r\n\r\nIf you are using C++, can you use our newly added benchmarking binary and see if the numbers are same:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/tools/benchmark\r\nand report detailed numbers on which ops seem slower.\r\n\r\nTensorflow mobile has a similar binary in:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/benchmark\r\n", "@shashishekhar \r\n\r\nThank you for a fast reply. Yes, we are using Java API (with ByteBuffer), setting #threads wasn't available just recently, so it was set to some default number. We will try to play around it now and will let you know whether this will lead to better performance. However, most likely this won't solve the issue with RAM, and we guess that those two might be connected to each other.\r\n\r\nWe benchmarked several models, including Inception-v1/v3, Inception-ResNet-v1, SRCNN, ResNet, VGG-19, ICNet and some other architectures (including both image classification and image-to-image translation nets). The difference between inference times for TFMobile/TFLite/TFLite+NNAPI was quite stable among all networks. In particular, this also applies for SRCNN that consists of two ops: conv and relu, and thus can be considered as a simplest baseline.", "We tried changing the number of threads, but this didn't solve the issue. Moreover, it seems that setNumThreads method has absolutely no affect on TF Lite's performance - for values of 1, 2, 4, 8, 100 and 1000 the results were identical (up to several ms).\r\n\r\nHere is more information regarding the performance of TF Mobile, TF Lite and TF Lite + NNAPI. We've just tested SRCNN (image deblurring, 200x200px) and VGG-19 (image super-res, 192x192px) networks on two devices - Nexus 5X with Android 8.1 and Razer Phone with Android 7.1, below are the results:\r\n\r\nNexus 5X, TF Mobile: SRCNN - 0.8s, VGG-19 - 4.5s\r\nNexus 5X, TF Lite: SRCNN - 1.8s, VGG-19 - 8s\r\nNexus 5X, TF Lite + NNAPI: SRCNN - 2.7s, VGG-19 - 15.7s\r\n\r\nRazer Phone, TF Mobile: SRCNN - 0.36s, VGG-19 - 1.8s\r\nRazer Phone, TF Lite: SRCNN - 0.71s, VGG-19 - 3.3s\r\n\r\nAs mentioned before, changing the number of threads didn't lead to any improvements.", "The lack of hardware acceleration on Android is a huge problem. The Pixel 2 even has a special processor called the Pixel Visual Core, but as far as I can tell it's not used by the Android NN API. And this is Google's flagship Android Phone! Meanwhile the iPhone has great hardware acceleration with Metal...", "Note, regarding the performance difference between TF Mobile and TF Lite was probably due to issue #16851 which caused a regression in TF Lite convolution performance. TFLite performance should be same/better than TF mobile."]}, {"number": 15553, "title": "[XLA] Replace GCC vector extension with portable Intel SIMD", "body": "MSVC does not support GCC vector extension, so replace it with Intel SIMD library. Eigen also uses Intel SIMD library to implement vector functions.\r\n\r\nMSVC does not have `__SSE4_1__` macro, although it does define `__AVX__` when building with `/arch:AVX`. Since Eigen enables SSE4.1 when `__AVX__` is defined ([`Eigen/Core`](https://bitbucket.org/eigen/eigen/src/034b6c3e101792a3cc3ccabd9bfaddcabe85bb58/Eigen/Core?at=default&fileviewer=file-view-default#Core-156)), we just follow Eigen.\r\n\r\n#15213", "comments": ["Can one of the admins verify this patch?", "@sanjoy might taking a look? Thanks!", "@rongjiecomputer did you address @sanjoy comments?", "Jenkins, test this please.", "@drpngx \r\n\r\n> @rongjiecomputer did you address @sanjoy comments?\r\n\r\nOnly partially. @sanjoy left these two comments.\r\n\r\n1. Avoid `#define TF_XLA_HAS_SSE4_1 1` and just have `#define TF_XLA_HAS_SSE4_1`: I have done this part.\r\n\r\n2. Putting all of these definitions (`TF_XLA_HAS_*`) in a single header: I gave the explanation why I did not follow this comment.\r\n\r\n> I have eventually decided to not splitting the definitions out into a new header file. That requires adding a new cc_library and make three cc_library(name = \"cpu_runtime_*\") to depend on it due to Bazel's strict header dependency. Seems over-complicated.", "SG, thanks! Let's what what @sanjoy thinks."]}, {"number": 15552, "title": "Tensorflow 1.4 C++ API considerably slower than Python", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source with all optimizations\r\n- **TensorFlow version (use command below)**: 1.4\r\n- **Python version**:  3.5.2\r\n- **Bazel version (if compiling from source)**: 0.8.1\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: 8.0 / 6.0\r\n- **GPU model and memory**: GTX960M\r\n\r\n### Describe the problem\r\n\r\nI was trying to run several models and evaluate the performance with different batch sizes in python and c++ and noticed that the c++ API version is considerably slower than the python one. Both were compiled with the same optimizations and with cuda support. \r\n\r\nWhen I try to predict the output of a single 256x256 image in python it takes me 0.5 seconds, and when i do it in tensorflow c++ api it takes me 1.7 seconds. Notice that in python I was using a non deployed model (without freezing and transforming graph), whereas in C++ I did those transformations. \r\n\r\nDoes anyone knows why this is happening? Is it because of the frozen and transformed graph?\r\n\r\nI always thought the C++ API would be at least as fast as the Python version. \r\n", "comments": ["I will leave here some of the times for the predictions with different batch sizes:\r\n\r\nBatch Size  Python (s)  C++ (s)\r\n       1         |       0.5   |       1.7\r\n       32       |       0.6   |     1.8\r\n       128     |      0.9    |     2.2", "Can you try to repro using the same model from both Python and C++, to narrow down the sources of differences?", "The difference in time from optimizing or not the models are in the order of 100 ms in c++- The model is the same of the one used in python. So the problem still persists. I will try a different model and post the results today ", "I tried the Inception example and noticed that the C++ version had a better performance then the Python one. But with a simple model with a few Convolutional layers, batch normalization and dropout layers. \r\nThe frozen model is optimized via via graph transformed tool have an execution time considerably slower. \r\n\r\nThe code is very similar to the one used in https://www.tensorflow.org/tutorials/image_recognition. \r\n\r\nAm I doing something wrong? ", "After running some warm up runs in the session, I was able to increase massively the performance in c++. \r\n\r\nProblem solved.", "Could you please let us know what kind of warm up runs you did to speed it up? Thank you so much.", "Hi @csytracy. I did some warm-up runs similar to the test_benchmark code:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/benchmark/benchmark_model.cc", "@Goldesel23  what is the technique you used to increase the speed of c++ can you pls share those tips \r\nThanks in advance ", "If you follow the code in the samples you will get a good speed. You just need to understand that the first batches will be slower. In my application I just run some warm-up runs on the start after the session initialization. ", "@Goldesel23 Could you supply me more detailed warm-up you used ?"]}, {"number": 15551, "title": "MutilGPU Model can't restore model", "body": "Hi,I use multiple GPU training with tensorflow1.0, and save the model sucessfully, when I restore the model, the error appear here  #line:saver=tf.train.Saver(tf.all_variables()),ValueError:No Variable to save.\r\nI try restore model in training,it works.So,i am confused what is wrong?can anybody help me.\r\nThank you", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 15550, "title": "fix random_distributions_test", "body": "[iota](http://en.cppreference.com/w/cpp/algorithm/iota) is declared in \\<numeric\\>\r\n", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 15549, "title": "Segmetation fault(coredump)", "body": "Hi guys, am getting Segmentation fault(coredump) while running keras with tensorflow background in CPU . Can anyone help me \r\n(imag) research2@research-Precision-T1700:~/imag/ftune$ python3 finetune.py\r\nUsing TensorFlow backend.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nFound 55 images of 2 classes\r\nFound 54 images of 2 classes\r\nSegmentaion falut(Coredump)\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Note we can't support custom code."]}, {"number": 15548, "title": "Bug fix for example_parsing_ops_test", "body": "Delay tensor allocation.\r\n\r\nStatic variables' initialization order is not determined in C++.\r\nIn one static variable's constructor, you can't access other static variables unless they are constexpr, which is not true for tensor's allocators.\r\n\r\nReplacing it with std::call_once.\r\n\r\nhttps://isocpp.org/wiki/faq/ctors#static-init-order", "comments": ["Can one of the admins verify this patch?", "Conflict resolved. \r\n\r\n@mrry , can you review this?", "Jenkins, test this please."]}, {"number": 15547, "title": "how to build tensorflow-lite.so  for linux  ubuntu, thanks?", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "Hi, so is it solved?", "> Hi, so is it solved?\r\n\r\nasked on StackOverflow is better"]}, {"number": 15546, "title": "DIGITS tensorflow distributed problem \uff1aGraph is finalized and cannot be modified", "body": "\r\nI am a DIGITS novice, and I want to change the TensorFlow single machine from DIGITS to distributed\u3002The following is the code that I modified\u3002\r\n\r\nsv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\r\n            logdir='/root/digits/digits/jobs/hello',\r\n                        saver=saver,\r\n                        init_op=init_op,\r\n                        )\r\n\r\nwith sv.prepare_or_wait_for_session(server.target) as sess:\r\n    inf_model.start_queue_runners(sess)\r\n    val_model.start_queue_runners(sess)\r\n    train_model.start_queue_runners(sess)\r\n    try\uff1a\r\n         while not train_model.queue_coord.should_stop():\r\n            _, summary_str, step = sess.run                                 ([train_model.train,\r\n                                                                                        train_model.summary,\r\n                                                                                        train_model.global_step],\r\n                                                                                        feed_dict=feed_dict,\r\n                                                                                        options=run_options,\r\n                                                                                        run_metadata=run_metadata)\r\n\r\nand run it,but error:\r\n\r\n2017-12-19 06:56:23.696588: I tensorflow/core/distributed_runtime/master_session.cc:999] S\r\ntart master session ad6a96f02b5d215f with config: \r\nINFO:tensorflow:Starting standard services.\r\n2017-12-19 06:56:23 [INFO] Starting standard services.\r\nINFO:tensorflow:Starting queue runners.\r\nINFO:tensorflow:Saving checkpoint to path /root/digits/digits/jobs/xwk/model.ckpt\r\n2017-12-19 06:56:23 [INFO] Starting queue runners.\r\n2017-12-19 06:56:23 [INFO] Saving checkpoint to path /root/digits/digits/jobs/xwk/model.ck\r\nptINFO:tensorflow:Errorreported to Coordinator: , exceptions.KeyError: 'pyfunc_1'     [[Node: val/data/data_reader = PyFuncTin=[DT_STRING], Tout=[DT_STRING, DT_INT32,DT_INT64,DT_INT32],token=\"pyfunc_1\", _device=\"/job:ps/replica:0/task:0/cpu:0\"]]2017-12-19 06:56:23 [INFO] Error reported to Coordinator: , exceptions.KeyError: 'pyfunc_1'  [[Node: val/data/data_reader = PyFuncTin=[DT_STRING], Tout=[DT_STRING, DT_INT32,DT_INT64, DT_INT32],token=\"pyfunc_1\",_device=\"/job:ps/replica:0/task:0/cpu:0\"]]INFO:tensorflow:/root/digits/digits/jobs/xwk/model.ckpt is not in all_model_checkpoint_pat\r\nhs. Manually adding it.2017-12-19 06:56:24 [INFO] /root/digits/digits/jobs/xwk/model.ckpt is not in all_model_che\r\nckpoint_paths. Manually adding it.2017-12-19 06:56:24 [INFO] Starting queue runners (val)\r\nTraceback (most recent call last):\r\n  File \"/root/digits/digits/tools/tensorflow/main.py\", line 779, in \r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 48\r\n, in run    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"/root/digits/digits/tools/tensorflow/main.py\", line 629, in main\r\n    val_model.start_queue_runners(sess)\r\n  File \"/root/digits/digits/tools/tensorflow/model.py\", line 208, in start_queue_runners\r\n    tf.add_to_collection(digits.GraphKeys.QUEUE_RUNNERS, qr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 4\r\n248, in add_to_collection    get_default_graph().add_to_collection(name, value)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2\r\n792, in add_to_collection    self._check_not_finalized()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2\r\n181, in _check_not_finalized    raise RuntimeError(\"Graph is finalized and cannot be modified.\")\r\nRuntimeError: Graph is finalized and cannot be modified.\r\n\r\nDo you know what is the cause of the final problem?\r\nIf you know, please tell me.\r\nthx.", "comments": ["start queue runners before create the session", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "@michaelisard OK", "@snnn It is solved, in superviser() \uff0call the build Graph must be finished the summary_op."]}, {"number": 15545, "title": "Update Imageops Docs", "body": "Created using update_api_def.sh and tested using api_test, all tests passed.\r\n\r\nINFO: Build completed successfully, 2 total actions\r\n//tensorflow/core:api_test                                               PASSED in 0.1s\r\n\r\nExecuted 1 out of 1 test: 1 test passes.\r\nThere were tests whose specified size is too big. Use the --test_verbose_timeout_warnings command line option to see which ones these are.\r\n", "comments": ["Can one of the admins verify this patch?"]}, {"number": 15544, "title": "support unknown shape for `sparse_multiclass_hing_loss`", "body": "Fix #15480.\r\n\r\n### How to test\r\n\r\n+ [x] add test case.\r\n+ [ ] pass all tests.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 15543, "title": "Maven Nightlies", "body": "@asimshankar Would it be possible to start releasing Maven nightlies for the Java API packages? I only depend on the org.tensorflow Proto package, but I guess it would be more consistent to do that for all the Java API packages.", "comments": ["@eaplatanios : We just released [maven packages for 1.5.0-rc0](https://search.maven.org/#artifactdetails%7Corg.tensorflow%7Cproto%7C1.5.0-rc0%7Cjar), so I'm hoping your immediate need is met.\r\n\r\nI'm tempted to defer setting up releases of Maven nightlies till there is a stronger, more consistent demand for them. Sound reasonable?", "Closing this out for now. Please re-open if you disagree. Thanks."]}, {"number": 15541, "title": "TensorFlow grpc+gdr runtime error", "body": "I have built tensorflow with gdr support following the [instruction](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/gdr)\uff0cbut when I run tf_cnn_benchmarks I got a runtime error.\r\n![image](https://user-images.githubusercontent.com/3050438/34240351-43330544-e648-11e7-9543-aaea5abadc48.png)\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "thank you for the kindly reply, actually I was running tf-cnn-benchmark downloaded [there](https://github.com/tensorflow/benchmarks). nv_peer_memory has been installed.\r\nMy OS Platform and Distribution is Centos 7.1\r\nMy tensorflow is installed from master banch which version is 1.4.0\r\nBazel version is 0.7.0\r\nCUDA/cuDNN are 8.0 and 6.0\r\nGPU mode is K40c GPU and memory is 11439MiB\r\n\r\nI also try grpc+verbs mode and it runs fine.", "Hi @ooyanglinoo, it seems to be a gRPC error. Do you meet the same error running in pure `grpc` runtime?", "Please attach the detailed logs with environment variables:\r\n```\r\nexport TF_CPP_MIN_LOG_LEVEL=0\r\nexport TF_CPP_MIN_VLOG_LEVEL=2\r\nexport GRPC_POLL_STRATEGY=poll  # Try this and see if you could reproduce\r\n```", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@skye Shall we close this issue for now? It appears that I cannot reproducce the issue. \r\n\r\n@ooyanglinoo please let me know if you\u2019ve met further problems. Thanks!", "Sure, go ahead and close if this can't be repro'd anymore. Thanks.", "@skye it seems only the owner or a member in TF team could close the issue :(", "Didn't realize that, thanks!", "I reproduced this.\r\nCan anyone reopen this issue? I need help.\r\n\r\nOS Platform and Distribution: CentOS 7.5\r\nTensorFlow installed from: Download the source from this repo.\r\nTensorFlow version: 1.10.0\r\nBazel version: 0.16.1\r\nCUDA/cuDNN version: 9.2/7.2\r\nGPU model and memory: NVIDIA P2000 with 5 GB.\r\nExact command to reproduce: run tf-cnn-benchmark. grpc mode and grpc+verbs mode seem to be fine.\r\n\r\nI attach the log with the environmental variables (https://github.com/tensorflow/tensorflow/issues/15541?_pjax=%23js-repo-pjax-container#issuecomment-353841661).\r\n\r\n[tf-log.txt](https://github.com/tensorflow/tensorflow/files/2321619/tf-log.txt)", "I will take a look into this. ", "Probably, I found the root cause of the error.\r\nIn my cluster, each node is connected with IB and ethernet.\r\nWhen the program is passed ethernet IP addr as host addresses, the connection is failed as this issue.\r\n(Despite the IB connection is linked up.)\r\nWhen the program is passed IPoIB addr, the error is disappeared!\r\nHowever, I wonder why the error is caused when not using IPoIB.\r\n\r\nIf my hypothesis is true, I suggest grpc+gdr mode (and grpc+verbs mode?) to display some warning when the initial connection does not use IPoIB.", "@aru132 Thanks for your comments. Indeed, the GDR plugin is developed under RDMA over Converged Ethernet (RoCE) environment, and it assumes the IP address user passes to the GRPC server could also be used for RDMA.\r\n\r\nStandalone IB without IPoIB fails to satisfy this condition, so error arises from the GRPC side.\r\n\r\nBtw, it would be much appreciated if you could contribute to the docs of GDR, as I currently have no access to any IB cluster."]}, {"number": 15540, "title": "No PyPi package for Tensorflow 1.4 on Windows?", "body": "Running Windows 10 with Python 3.6.\r\nJust wondering when that's gonna come out.", "comments": ["If you can, could you point me to an older version or an alternate installation?", "Thanks pointing that out. While TensorFlowers check this issue you could download the binary [here](https://github.com/tensorflow/tensorflow#installation) to get v1.4 running.", "@gunan can you comment?", "we have the 1.4.1 packages missing, because that patch release only affected features that are not enabled on windows. However, we do have 1.4.0 packages on pypi:\r\nhttps://pypi.python.org/pypi/tensorflow/1.4.0\r\nhttps://pypi.python.org/pypi/tensorflow-gpu/1.4.0"]}, {"number": 15539, "title": "Remove quantized_matmul_op_for_hexagon_test in Windows build scripts", "body": "", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 15538, "title": "[solved]session->Create(graph_def) No OpKernel was registered to support Op 'RandomUniform'", "body": "issue: crashes when loading pb file in C++ program:\r\n\r\n>     Session * session;\r\n>     GraphDef graph_def;\r\n>     SessionOptions opts;\r\n>     TF_CHECK_OK(ReadBinaryProto(Env::Default(), heartPrintPbPathFile, &graph_def));\r\n>     TF_CHECK_OK(NewSession(opts, &session));\r\n>     TF_CHECK_OK(session->Create(graph_def));\r\n> \r\n\r\nos: linux\r\ntrain: python\r\ninference: C++ interface\r\n\r\nruntime error on inference part:\r\n\r\n> Non-OK-status: session->Create(graph_def) status: Invalid argument: No OpKernel was registered to support Op 'RandomUniform' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n>   <no registered kernels>\r\n> \r\n> \t [[Node: rnn_net/rnn/dropout_63/random_uniform/RandomUniform = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=0, seed2=0](rnn_net/rnn/dropout_63/Shape)]]\r\n\r\npython train code:\r\n\r\n>         with tf.variable_scope(\"rnn_net\") as scope:\r\n>             cell = []\r\n>             for i in range(num_layers):\r\n>                 cell.append(tf.nn.rnn_cell.LSTMCell(hidden_size, state_is_tuple=True))\r\n> \r\n>             cell = tf.nn.rnn_cell.MultiRNNCell(cell)\r\n> \r\n>             cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob = self.keep_prob)\r\n> \r\n>             initial_state = cell.zero_state(batch_size, tf.float32)\r\n> \r\n>             input_list = tf.unstack(conv_output, axis=1)\r\n> \r\n>             rnn_output, _ = tf.nn.static_rnn(cell, input_list, dtype=tf.float32)\r\n>             self.rnn_output = rnn_output[-1]\r\n>             print \"rnn output shape: \"\r\n>             print self.rnn_output.get_shape()\r\n\r\nhint one : i found that if i delete this line:\r\ncell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob = self.keep_prob)\r\nthen the trained pb file can be loaded normally.\r\n\r\nhint two: beside rnn part, i also have cnn part in the network, and the dropout in cnn works just fine:\r\n\r\n>         with tf.variable_scope(\"conv_net\") as scope:\r\n>             filters = [15, 11, 7, 5]\r\n>             kernel_size = [64, 64, 32, 32]\r\n>             input_layer = tf.reshape(self.x, [-1, seq_len, 1])\r\n>             conv1_1 = tf.layers.conv1d(inputs=input_layer, filters=filters[0], kernel_size=kernel_size[0], padding=\"same\", activation=tf.nn.tanh, name='conv1_1')\r\n>             conv1_2 = tf.layers.conv1d(inputs=conv1_1, filters=filters[1], kernel_size=kernel_size[1], padding=\"same\", activation=tf.nn.tanh)\r\n>             pool1 = tf.layers.max_pooling1d(inputs=conv1_2, pool_size=4, strides=4)\r\n> \r\n>             bn1 = batch_norm(pool1, self.is_train, scope='bn1')\r\n>             dropout1 = tf.layers.dropout(inputs=bn1, rate=(1 - self.keep_prob))\r\n> \r\n>             conv2_1 = tf.layers.conv1d(inputs=dropout1, filters=filters[2], kernel_size=kernel_size[2], padding=\"same\", activation=tf.nn.tanh, name='conv2_1')\r\n>             conv2_2 = tf.layers.conv1d(inputs=conv2_1, filters=filters[3], kernel_size=kernel_size[3], padding=\"same\", activation=tf.nn.tanh)\r\n>             pool2 = tf.layers.max_pooling1d(inputs=conv2_2, pool_size=4, strides=4)\r\n> \r\n>             bn2 = batch_norm(pool2, self.is_train, scope='bn2')\r\n>             dropout2 = tf.layers.dropout(inputs=bn2, rate=(1 - self.keep_prob))\r\n> \r\n>             conv3_1 = tf.layers.conv1d(inputs=dropout2, filters=filters[2], kernel_size=kernel_size[2], padding=\"same\", activation=tf.nn.tanh)\r\n>             conv3_2 = tf.layers.conv1d(inputs=conv3_1, filters=filters[3], kernel_size=kernel_size[3], padding=\"same\", activation=tf.nn.tanh)\r\n>             pool3 = tf.layers.max_pooling1d(inputs=conv2_2, pool_size=2, strides=2)\r\n> \r\n>             conv_output = pool3\r\n\r\n", "comments": ["btw: i've been always using this way to build tensor lib :\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile", "i solved this issue by \r\nhttps://dato.ml/drop-dropout-from-frozen-model/\r\nmy code to delete RandomUniform of pb file:\r\n\r\n> # coding=utf-8\r\n> import tensorflow as tf\r\n> from tensorflow.core.framework import graph_pb2\r\n> \r\n> def display_nodes(nodes):\r\n>     for i, node in enumerate(nodes):\r\n>         print('%d %s %s' % (i, node.name, node.op))\r\n>         for j, n in enumerate(node.input):\r\n>             print(u'\u2514\u2500\u2500\u2500 %d \u2500 %s' % (j, n))\r\n> \r\n> \r\n> graph = tf.GraphDef()\r\n> with tf.gfile.Open('/data/run/daniel/project/06_deepID/deconv/RNN/top_pb/cnn_rnn2s.acc0.640000.pb', 'r') as f:\r\n>     data = f.read()\r\n>     graph.ParseFromString(data)\r\n> \r\n> # display_nodes(graph.node)\r\n> \r\n> graph.node[2354].input[0] = 'rnn_net/rnn/rnn/multi_rnn_cell/cell_1/cell_1/lstm_cell/lstm_cell_63/mul_2'\r\n> \r\n> nodes = graph.node[:2339] + graph.node[2350:]\r\n> \r\n> \r\n> output_graph = graph_pb2.GraphDef()\r\n> output_graph.node.extend(nodes)\r\n> with tf.gfile.GFile('./frozen_model_without_dropout.pb', 'w') as f:\r\n>     f.write(output_graph.SerializeToString())\r\n> \r\n> \r\n> graph = tf.GraphDef()\r\n> with tf.gfile.Open('./frozen_model_without_dropout.pb', 'r') as f:\r\n>     data = f.read()\r\n>     graph.ParseFromString(data)\r\n> \r\n> display_nodes(graph.node)", "pb file before change:\r\n\r\n> 2338 rnn_net/rnn/rnn/multi_rnn_cell/cell_1/cell_1/lstm_cell/lstm_cell_63/mul_2 Mul\r\n> \u2514\u2500\u2500\u2500 0 \u2500 rnn_net/rnn/rnn/multi_rnn_cell/cell_1/cell_1/lstm_cell/lstm_cell_63/Sigmoid_2\r\n> \u2514\u2500\u2500\u2500 1 \u2500 rnn_net/rnn/rnn/multi_rnn_cell/cell_1/cell_1/lstm_cell/lstm_cell_63/Tanh_1\r\n> 2339 rnn_net/rnn/dropout_63/Shape Shape\r\n> \u2514\u2500\u2500\u2500 0 \u2500 rnn_net/rnn/rnn/multi_rnn_cell/cell_1/cell_1/lstm_cell/lstm_cell_63/mul_2\r\n> 2340 rnn_net/rnn/dropout_63/random_uniform/min Const\r\n> 2341 rnn_net/rnn/dropout_63/random_uniform/max Const\r\n> 2342 rnn_net/rnn/dropout_63/random_uniform/RandomUniform RandomUniform\r\n> \u2514\u2500\u2500\u2500 0 \u2500 rnn_net/rnn/dropout_63/Shape\r\n> 2343 rnn_net/rnn/dropout_63/random_uniform/sub Sub\r\n> \u2514\u2500\u2500\u2500 0 \u2500 rnn_net/rnn/dropout_63/random_uniform/max\r\n> \u2514\u2500\u2500\u2500 1 \u2500 rnn_net/rnn/dropout_63/random_uniform/min\r\n> 2344 rnn_net/rnn/dropout_63/random_uniform/mul Mul\r\n> \u2514\u2500\u2500\u2500 0 \u2500 rnn_net/rnn/dropout_63/random_uniform/RandomUniform\r\n> \u2514\u2500\u2500\u2500 1 \u2500 rnn_net/rnn/dropout_63/random_uniform/sub\r\n> 2345 rnn_net/rnn/dropout_63/random_uniform Add\r\n> \u2514\u2500\u2500\u2500 0 \u2500 rnn_net/rnn/dropout_63/random_uniform/mul\r\n> \u2514\u2500\u2500\u2500 1 \u2500 rnn_net/rnn/dropout_63/random_uniform/min\r\n> 2346 rnn_net/rnn/dropout_63/add Add\r\n> \u2514\u2500\u2500\u2500 0 \u2500 dropout\r\n> \u2514\u2500\u2500\u2500 1 \u2500 rnn_net/rnn/dropout_63/random_uniform\r\n> 2347 rnn_net/rnn/dropout_63/Floor Floor\r\n> \u2514\u2500\u2500\u2500 0 \u2500 rnn_net/rnn/dropout_63/add\r\n> 2348 rnn_net/rnn/dropout_63/div RealDiv\r\n> \u2514\u2500\u2500\u2500 0 \u2500 rnn_net/rnn/rnn/multi_rnn_cell/cell_1/cell_1/lstm_cell/lstm_cell_63/mul_2\r\n> \u2514\u2500\u2500\u2500 1 \u2500 dropout\r\n> 2349 rnn_net/rnn/dropout_63/mul Mul\r\n> \u2514\u2500\u2500\u2500 0 \u2500 rnn_net/rnn/dropout_63/div\r\n> \u2514\u2500\u2500\u2500 1 \u2500 rnn_net/rnn/dropout_63/Floor\r\n> 2350 w1 Const\r\n> 2351 w1/read Identity\r\n> \u2514\u2500\u2500\u2500 0 \u2500 w1\r\n> 2352 b1 Const\r\n> 2353 b1/read Identity\r\n> \u2514\u2500\u2500\u2500 0 \u2500 b1\r\n> 2354 softmax/xw_plus_b/MatMul MatMul\r\n> \u2514\u2500\u2500\u2500 0 \u2500 rnn_net/rnn/dropout_63/mul\r\n> \u2514\u2500\u2500\u2500 1 \u2500 w1/read", "pb file after change:\r\n\r\n> 2338 rnn_net/rnn/rnn/multi_rnn_cell/cell_1/cell_1/lstm_cell/lstm_cell_63/mul_2 Mul\r\n> \u2514\u2500\u2500\u2500 0 \u2500 rnn_net/rnn/rnn/multi_rnn_cell/cell_1/cell_1/lstm_cell/lstm_cell_63/Sigmoid_2\r\n> \u2514\u2500\u2500\u2500 1 \u2500 rnn_net/rnn/rnn/multi_rnn_cell/cell_1/cell_1/lstm_cell/lstm_cell_63/Tanh_1\r\n> 2339 w1 Const\r\n> 2340 w1/read Identity\r\n> \u2514\u2500\u2500\u2500 0 \u2500 w1\r\n> 2341 b1 Const\r\n> 2342 b1/read Identity\r\n> \u2514\u2500\u2500\u2500 0 \u2500 b1\r\n> 2343 softmax/xw_plus_b/MatMul MatMul\r\n> \u2514\u2500\u2500\u2500 0 \u2500 rnn_net/rnn/rnn/multi_rnn_cell/cell_1/cell_1/lstm_cell/lstm_cell_63/mul_2\r\n> \u2514\u2500\u2500\u2500 1 \u2500 w1/read", "Closing since this seems resolved: please reopen if I misunderstood!", "yes, it's resolved. @michaelisard "]}, {"number": 15537, "title": "tensorflow lite converter: why empty lite file", "body": "I have a tensorflow pb file and try to use tflite converter to convert it to lite file,but the result file is empty and no error. I don't know why\r\n`bazel run --config=opt --copt=-msse4.1 --copt=-msse4.2  //tensorflow/contrib/lite/toco:toco --   --input_file=/Users/Lavector/code/keras_to_tensorflow/mobilenet_regression.pb   --output_file=/tmp/mobilenet_regression.lite   --input_format=TENSORFLOW_GRAPHDEF   --output_format=TFLITE   --inference_type=FLOAT   --input_shape=1,224,224,3   --input_array=input   --output_array=output_node0`\r\n\r\n`WARNING: Config values are not defined in any .rc file: opt\r\nINFO: Analysed target //tensorflow/contrib/lite/toco:toco (0 packages loaded).\r\nINFO: Found 1 target...\r\nTarget //tensorflow/contrib/lite/toco:toco up-to-date:\r\n  bazel-bin/tensorflow/contrib/lite/toco/toco\r\nINFO: Elapsed time: 0.223s, Critical Path: 0.01s\r\nINFO: Build completed successfully, 1 total action\r\n\r\nINFO: Running command line: bazel-bin/tensorflow/contrib/lite/toco/toco '--input_file=/Users/Lavector/code/keras_to_tensorflow/mobilenet_regression.pb' '--output_file=/tmp/mobilenet_regression.lite' '--input_format=TENSORFLOW_GRAPHDEF' '--output_format=TFLITE' '--inference_type=FLOAT' '--input_shape=1,224,224,3' '--input_array=input' '--output_array=output_node0'\r\n2017-12-21 11:20:11.351544: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 472 operators, 695 arrays (0 quantized)\r\n2017-12-21 11:20:11.384604: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 85 operators, 200 arrays (0 quantized)\r\n2017-12-21 11:20:11.387067: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 85 operators, 200 arrays (0 quantized)`\r\n\r\nI use mobilenet to do regression, and it is successful when test.\r\nI test the example code of [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/cmdline_examples.md](url), and it is correct.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@tensorflowbutler \r\nIt works well when use the tensorflow/android example, but not correct when tensorflow lite.\r\n\r\n1. Have I written custom code: yes, I used keras mobile-net (it uses custom layer). Then I converted it to freezed graph. Finally, tried to use it by tf-lite.\r\n2. OS Platform and Distribution: OS 10.11.6\r\n3. TensorFlow installed from: git clone latest version\r\n4. TensorFlow version: 1.4.0\r\n5. Bazel version: 0.9.0-homebrew\r\nI tested in cpu-only machine.\r\n6. Exact command to reproduce: I just build \"toco\" and not build \"freeze_graph\", I also test the tf official mobilenet freezed graph by bazel-bin/tensorflow/contrib/lite/toco/toco, it was also not correct. so I finally used \"bazel run\" and the tf official mobilenet freezed graph generated correct lite file and work well in lite example, but my model generated empty file with no error.\r\n\r\nI have also noticed some instruction of custom code, I don't know whether it is related.\r\nIf needed, the \"pb\" model can be supported for test.\r\nWish it is helpful. Thank you~", "I have same problem when use tensorflow.contrib.slim.python.slim.nets.resnet_v2 model \r\n```\r\nbazel run --config=opt \\\r\ntensorflow/contrib/lite/toco:toco -- \\\r\n--input_file=/Users/ir/WORK/models/models/models/frozen_graph.pb \\\r\n--input_format=TENSORFLOW_GRAPHDEF \\\r\n--output_format=TFLITE \\\r\n--output_file=/Users/ir/WORK/models/models/models/nsfw.lite \\\r\n--inference_type=FLOAT \\\r\n--input_type=FLOAT \\\r\n--input_arrays=input \\\r\n--output_arrays=resnet_v2_50/predictions/Reshape_1 \\\r\n--input_shapes=1,224,224,3\r\n```\r\n2017-12-25 10:21:40.195960: W tensorflow/contrib/lite/toco/toco_cmdline_flags.cc:178] --input_type is deprecated. It was an ambiguous flag that set both --input_data_types and --inference_input_type. If you are trying to complement the input file with information about the type of input arrays, use --input_data_type. If you are trying to control the quantization/dequantization of real-numbers input arrays in the output file, use --inference_input_type.\r\n2017-12-25 10:21:40.821969: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 433 operators, 634 arrays (0 quantized)\r\n\r\n\r\n**OS Platform and Distribution:OS 10.13.1\r\nTensorFlow installed from: git clone latest version\r\nTensorFlow version: 1.4.1\r\nBazel version:0.9.0 homebrew\r\nPython 2.7.14 :: Anaconda, Inc.**", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "@guoxiaolu Would you be able to provide your .pb file? ", "https://pan.baidu.com/s/1jJDgfJK\r\nThis is my pb file. @andrehentz ", "This looks like a bug that is triggered by specifying a nonexistent tensor as input. Perhaps you meant to specify \"input_1\" as your input array, instead of simply \"input\"?", "It works, Thank you very much~", "Hi, \r\nI faced a similar error while building a tflite file from .pb file for speech model. I noticed that when I give --allow_custom_ops, it creates a tflite file of 6MB. But, without that, the created tflite files is zero bytes. \r\nWhen I upload this tflite filr with --allow_custom_ops, it gives a run time error asking for the custom ops to be defined. Could you please help me out with this? \r\n\r\nThanks! ", "@Madhusakth , I think it's better to ask in a new issue and your problem is different from what I faced~ Good luck", "@guoxiaolu \u4f60\u662f\u600e\u4e48\u89e3\u51b3\u7684\uff1f\u6211\u4e5f\u540c\u6837\u7684\u95ee\u9898\uff01"]}, {"number": 15536, "title": "Use a trailing period to define floating point constant.", "body": "In the example code for basic usage of tf.estimator, the array y_eval uses the constant \"-7\", when the rest of surrounding constants use trailing periods.  The corresponding code for a custom estimator has the trailing period.  (compare line 378 to line 450)", "comments": ["Can one of the admins verify this patch?"]}, {"number": 15535, "title": "[iOS/tflite] Add ability to build a specific arch", "body": "Add a flag to build only the arch you want. By default build all.\r\nAlso check number of CPUs when invoking the make commands -j\r\n\r\nTEST=tensorflow/contrib/lite/build_ios_universal_lib.sh -a arm64", "comments": ["Can one of the admins verify this patch?", "@petewarden can you please review. Thx", "Hi @powderluv, it seems that there is a merge conflict for one file on this PR. Can u resolve it? Thanks.", "@powderluv, please rebase the PR to resolve the merge conflict, so that I can run the test for this PR again.", "@powderluv  Please address the test failures. Thanks !", "@powderluv  Can you please run clang-format on your code.", "@powderluv  Please look into the merging conflicts.", "The code base has changed much since my original patch for tflite. I will rebase in the next few weeks. ", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@shahzadlone @qlzh727 @hgadig You stuff spend one and a half year to approve a trivial PR, meanwhile, the author has to receive a spam email from your robot every 14 days. So great. \r\n\r\nSorry for posting an unrelated message to this PR. I am the author of PR https://github.com/tensorflow/tensorflow/pull/15201. No one responses to my PR, but I have to suffer from the every-14-day spam message from tensorflow's robot. I believe it's my right to request an explanation and apologies from you tensorflow stuff. ", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!", "Good to close. I had to wait a year for the PR approval by which time the code base evolved. I will re-open a new PR in the future when I have time to revisit the issue. "]}, {"number": 15534, "title": "mpi_collectives: Refactor to fix build issues", "body": "After TF commit 5c7f9e3, the mpi_collectives package would no longer build\r\nops and kernels. This build issue caused mpi_collectives import to fail in\r\nPython with the following error: \"NameError: Could not find operator MPISize\r\nin dynamic library mpi_collectives.so\" (ref: issue #13875).\r\n\r\nTo fix this issue, add build targets to ensure both ops and kernels are built.\r\nNote, also refactored the build targets and directory structure to more\r\nclosely match other contrib packages.", "comments": ["Can one of the admins verify this patch?", "@gunan for some reason the windows cmake builds don't appear to complete?", "Jenkins, test this please.\r\nUnfortunately, we are still working to move cmake build to the new stack. So that one is still on jenkins."]}, {"number": 15533, "title": "added note about weights gradient in compute_weighted_loss", "body": "Added a note concerning the gradient computation w.r.t. weights in `losses.compute_weighted_loss`, \r\nsee #15046.\r\n\r\nI have only added it to this function, and not the other losses (like mean_squared_error) because it is a rare cornercase that should be documented somewhere, but is of no relevance to most users. ", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please."]}, {"number": 15532, "title": "Update API Docs", "body": "Updated api dos for SampleDistortedBoundingBox v1 and v2 to update tf.image_summary to tf.summary.image", "comments": ["Can one of the admins verify this patch?", "Thanks @wagonhelm. These files are auto-generated from the code. Could you updated in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/image_ops_impl.py as well?", "[Done](https://github.com/tensorflow/tensorflow/pull/15532/files), thanks!\r\n\r\n", "Thanks @wagonhelm, could you run tensorflow/core/api_def/update_api_def.sh and check what are the pbtxt files that should be updated, as well as check run bazel test tensorflow/core:api_test passes? ", " [Done](https://github.com/tensorflow/tensorflow/pull/15545) on a new branch, tests passed, will delete this pull request once new one is pushed.\r\n\r\nThanks for walking me through the process @yifeif ", "I'm closing this one in favor of #15545. Thanks again!"]}, {"number": 15531, "title": "Fix sample_distorted_bounding_box where min_object_covered could be None", "body": "This fix tried to address the issue raised in #15529 where not providing min_object_covered a value will result in a ValueError. In the docstring, however, min_object_covered has been described as default to 0.1.\r\n\r\nThe reason for the issue is that when sample_distorted_bounding_box switched to V2, min_object_covered has been changed form an attr to an input. As input could not have a default value, min_object_covered=None will result in an error.\r\n\r\nThis fix adds the check so that a default value 0.1 will be provided if min_object_covered=None.\r\n\r\nThis fix fixes #15529.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n", "comments": ["Thanks @rmlarsen for the review. The PR has been updated.", "(Good for API change, thanks!)", "@yongtang please resolve conflict."]}, {"number": 15530, "title": "Bug?: reading from Google Cloud Storage appears to be accessing cached version", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.4.0-rc1-11-g130a514 1.4.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: observed on CPU & GPU.\r\n- **GPU model and memory**: observed on CPU & GPU.\r\n- **Exact command to reproduce**: See below.\r\n\r\n### Describe the problem\r\n\r\nHow this arose:\r\n\r\nWe are trying to set up a basic distributed TF version, where we have separate pods (on Kubernetes) doing validation and training (a simple version, with 1 of each).  GCS is used as the backend to store model output (checkpoints, etc.).\r\n\r\nThe validation pod periodically (via Experiment\u2019s continuous_eval https://github.com/tensorflow/tensorflow/blob/f5f2f789ea395e585ddcbc43e088fa63d6b41d0e/tensorflow/contrib/learn/python/learn/experiment.py#L564) periodically polls for new checkpoints to evaluate (https://github.com/tensorflow/tensorflow/blob/f5f2f789ea395e585ddcbc43e088fa63d6b41d0e/tensorflow/contrib/learn/python/learn/experiment.py#L517).  \r\n\r\nIf it doesn\u2019t find a new checkpoint, it (per the underlying code) echoes out \u201cNo new checkpoint ready for evaluation\u201d and continues to wait for a new one.\r\n\r\nIn practice, we found that, even as the training pod produces new checkpoints, the validation pod *never* picks up a new checkpoint, beyond the first one.  I.e., it collects an initial checkpoint, does evaluation, and then, in all future cycles, echoes out \"No new checkpoint ready for evaluation\".\r\n\r\nIn debugging, we found that the checkpoint file the saver tries to load up is always found to be some earlier iteration of the file (https://github.com/tensorflow/tensorflow/blob/f5f2f789ea395e585ddcbc43e088fa63d6b41d0e/tensorflow/python/training/saver.py#L1005) -- i.e., it seems like the GCS file loader reads the file once, and, from then on out, is continuous accessing a cached version of the data.  \r\n\r\nDigging into the code further, this appears to be an issue with how the file reader (file_io.read_file_to_string(...) and downstream methods) loads GCS files.  We were able to replicate this behavior separately, below.\r\n\r\n**Help appreciated!**\r\n\r\n* Is this intended behavior?  Is there, e.g., some sort of GCS setting we have incorrectly set?  \r\n* Assuming we're seeing something real, is there any suggested remediation here, with regards to our validation behavior?  Our next step is going to be to try monkey-patching some of the tf functions to just pull the GCS file local to disk and read it from there...although this is of course not preferred.\r\n\r\nAs a side note, Experiment does have a number of references to special handling around using GCS as the backend, although I don't have enough context to say if this is relevant to what we are seeing or not (https://github.com/tensorflow/tensorflow/blob/f5f2f789ea395e585ddcbc43e088fa63d6b41d0e/tensorflow/contrib/learn/python/learn/experiment.py#L94, https://github.com/tensorflow/tensorflow/blob/f5f2f789ea395e585ddcbc43e088fa63d6b41d0e/tensorflow/contrib/learn/python/learn/experiment.py#L269).\r\n\r\n### Source code / logs\r\n\r\nBelow is a pair of scripts that will replicate this issue.  Run the \u201cBasic Reader\u201d (using the same loading interface from get_checkpoint_state) and the \u201cBasic Writer\u201d simultaneously, and the reader will initially catch whatever is in the file, and then never update, even as the writer continues to write.\r\n\r\nOther things we tried:\r\n\r\n* Changing the read/write path to local disk, instead of GCS.  This, unsurprisingly, worked.\r\n* A version of a reader with a context manager (below), to try to reset the reader each loop, and an explicit file.close() (not shown).  Both had the same behavior, i.e., new reads didn't provide the updated file.\r\n* Writing from the same file (process) that we read from: probably unsurprisingly, this *does* work; i.e., the writing activity either updates the local cache or otherwise convinces the reader to grab a fresh copy from GCS (we didn\u2019t actually test which this might be).  \r\n\r\nBasic Reader:\r\n\r\n```python\r\n#!/usr/bin/env python\r\n\r\nfrom tensorflow.python.lib.io import file_io\r\nimport time\r\nimport os\r\n\r\nfile='gs://[MYPATH]'\r\n\r\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = '/usr/src/app/gcloud/keys/google-auth.json'\r\n\r\ndef read():\r\n    counter=0\r\n    while counter<15:\r\n        print(\"reading...\")\r\n        print(\"Contents:\")\r\n        print(file_io.read_file_to_string(file))\r\n        print(\"\")\r\n        print(\"Sleeping for a second...\")\r\n        time.sleep(3)\r\n        print(\"\")\r\n        print(\"\")\r\n        print(\"\")\r\n        counter +=1\r\n\r\nread()\r\n```\r\n\r\nBasic writer\r\n\r\n```python\r\n#!/usr/bin/env python\r\n\r\nfrom tensorflow.python.lib.io import file_io\r\nimport time\r\nimport os\r\n\r\nfile='gs://[MYPATH]'\r\n\r\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = '/usr/src/app/gcloud/keys/google-auth.json'\r\n\r\ndef write():\r\n    counter=0\r\n    while counter<15:\r\n        with file_io.FileIO(file, mode='w') as f:\r\n            f.write(str(counter))\r\n        print(\"Wrote {}\".format(counter))\r\n        print(\"\")\r\n        print(\"Sleeping for a second...\")\r\n        time.sleep(3)\r\n        print(\"\")\r\n        print(\"\")\r\n        print(\"\")\r\n        counter +=1\r\n\r\nwrite()\r\n```\r\n\r\nReader with context manager\r\n\r\n```python\r\n#!/usr/bin/env python\r\n\r\nfrom tensorflow.python.lib.io import file_io\r\n\r\nimport time\r\nimport os\r\n\r\nfile='gs://[MYPATH]'\r\n\r\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = '/usr/src/app/gcloud/keys/google-auth.json'\r\n\r\ndef read():\r\n    counter=0\r\n    while counter<15:\r\n        print(\"reading...\")\r\n        print(\"Contents:\")\r\n        with file_io.FileIO(file, mode='r') as f:\r\n           print(f.read())\r\n        print(\"\")\r\n        print(\"Sleeping for a second...\")\r\n        time.sleep(3)\r\n        print(\"\")\r\n        print(\"\")\r\n        print(\"\")\r\n        counter +=1\r\n\r\nread()\r\n```", "comments": ["You may set env var GCS_READ_CACHE_MAX_SIZE_MB to zero, and try it again. ", "We tested this and this seems to work.  Is this the intended behavior of the GCS tf reader?  What are the (broader) consequences of setting GCS_READ_CACHE_MAX_SIZE_MB=0?  It isn't clear to me what the consequences are of embracing this approach (maybe nothing negative!).", "Yes, it's by design.  Distributed filesystems like GCS are designed for write once, read multiple times.  You shouldn't rely on the content of the \"checkpoint\" file, because you need to read/write it multiple times.", "> Yes, it's by design. Distributed filesystems like GCS are designed for write once, read multiple times. You shouldn't rely on the content of the \"checkpoint\" file, because you need to read/write it multiple times.\r\n\r\nWith all due respect, this doesn't make much sense.  Tensorflow is designed to use GCS as a backend for model saving, and TF is built to use GCS as a back-end for model saving.  tf.Experiment is built with a continuous_eval mode which tries to load the checkpoint repeatedly, to discover state and use it appropriately.  Even if we try to chalk this up to tf.Experiment being in contrib (albeit maintained and built by Googlers), we still have the problem that GCS is advertised as a viable and supported backend storage for Tensorflow.  \r\n\r\nMeaning, if Tensorflow is going to advertise that it supports GCS as a backend, functionality should not randomly not work.  This is a pretty basic scenario for it to break.  It isn't unreasonable for this scenario to work, given that I can work with write-multiple-times, read-multiple times with the existing GCS APIs, with no problems...outside of Tensorflow.\r\n\r\n\r\n\r\n\r\n", "This (TF's) behavior is also inconsistent with the behavior of the Google Python implementation (https://googlecloudplatform.github.io/google-cloud-python/latest/storage/blobs.html) for interacting with GCS.\r\n\r\nE.g., performing the same read-write loop while using the provided \"download_as_string()\" has the expected result--i.e., it correctly pulls the updated file, each time this is invoked.", "@tensorflowbutler I would like to see this better addressed.  TF GCS read/write acting entirely differently than GCS read/write works with the baseline Google-built GCS library doesn't seem healthy.  Further, the failure mode here is very subtle, as what is \"failing\" (at least against what I would argue is reasonable expectation, given how the Google product works everywhere else) is very subtle to discover and diagnose (i.e., the underlying failure is hidden under many layers of API calls).", "@hormati WDYT?", "Potentially related: I ran into the same issue when running standard estimator-based training on GCloud. In my case the trainer was accessing a GCS bucket in a different region (europe-west1 vs europe-west3, respectively). Setting GCS_READ_CACHE_MAX_SIZE_MB to 0 did not help. Placing both the worker and the bucket in a single region fixed the issue for me.", "I'm not clear whether this bug persists into the current TF version (there\nare some vague release notes as to improvements on GC storage), but it is\nvery disappointing not to get a more proactive response here from the TF\nteam--it shouldn't be extra hard to use Google products...with Google\nproducts.\n\nOn Sat, Mar 10, 2018 at 1:46 PM, jonasz <notifications@github.com> wrote:\n\n> Potentially related: I ran into the same issue when running standard\n> estimator-based training on GCloud. In my case the trainer was accessing a\n> GCS bucket in a different region (europe-west1 vs europe-west3,\n> respectively). Setting GCS_READ_CACHE_MAX_SIZE_MB to 0 did not help.\n> Placing both the worker and the bucket in a single region fixed the issue\n> for me.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15530#issuecomment-372069548>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AEc6EoC52pqzU9bg53ukrwMZuVA_l_Djks5tdEmxgaJpZM4RI6gT>\n> .\n>\n", "I had an issue of the local RAM filling up and crashing VM when loading many soundfiles from Google Cloud Bucket on Colaboratory, using `tf.data` on Tensorflow 1.8.\r\nSetting `GCS_READ_CACHE_MAX_SIZE_MB=0` solved the issue for me.\r\nMaybe this feature could be part of the already-existing `tf.data.Dataset.cache()` and `tf.data.Dataset.prefetch()` functionality?\r\n", "Ran into a similar issue reading data from a GCS bucket with `tf.read_file`. Without setting `GCS_READ_CACHE_MAX_SIZE_MB=0` all RAM was consumed by the cache. Not only was this useless (we were caching with tf Dataset anyways), it was also difficult to debug and not documented behaviour.", "Hi @cbockman! \r\nIt seems you are using older versions(1.x versions) of Tensorflow. We recommend that you upgrade  your code base to 2.x  versions as many features and bug fixes has been done in newer versions and let us know if the issue still persists in newer versions. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/15530\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/15530\">No</a>\n"]}, {"number": 15529, "title": "Sample Distorted Bounding Box Bug", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nStock Example\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n16.04.3 LTS\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n- **TensorFlow version (use command below)**:\r\n1.4.0\r\n- **Python version**: \r\nPython 3.5.2\r\n- **Bazel version (if compiling from source)**:\r\n0.7.0\r\n- **GCC/Compiler version (if compiling from source)**:\r\n5.4.0\r\n- **CUDA/cuDNN version**:\r\ncuda_9.0.176_384.81 / cudnn 7\r\n- **GPU model and memory**:\r\nGTX 755M 2gb Memory x2\r\n- **Exact command to reproduce**:\r\nbegin, size, bbox_for_draw = tf.image.sample_distorted_bounding_box(\r\n        tf.shape(image),\r\n        bounding_boxes=bounding_boxes)\r\n\r\n\r\n### Describe the problem\r\nWhen using the tf.image.sample_distorted_bounding_box() function the parameter min_object_covered seems to default to value None which causes an error (ValueError: None values not supported.)  If you give an argument for min_object_covered it seems to work fine.\r\n\r\nThere seems to be two versions of this function in the source [v2 which takes min_object_covered](https://github.com/tensorflow/tensorflow/blob/fc49f43817e363e50df3ff2fd7a4870ace13ea13/tensorflow/core/ops/image_ops.cc#L930) as a argument and a [v1](https://github.com/tensorflow/tensorflow/blob/fc49f43817e363e50df3ff2fd7a4870ace13ea13/tensorflow/core/ops/image_ops.cc#L844) which has the default value of 0.1 as an attribute.  It appears v2 is the one [being used](https://github.com/tensorflow/tensorflow/blob/73658420db2498ad7f07363bfa72cba6e2d9fdd2/tensorflow/python/ops/image_ops_impl.py#L1536).  Not sure what approach is best to take for fixing this bug but believe the root of the issue is coming from tensorflow/core/ops/image_ops.cc\r\n\r\n### Source code / logs\r\nAttached\r\n[boundingbox.txt](https://github.com/tensorflow/tensorflow/files/1576836/boundingbox.txt)\r\n\r\nExamples of code being implemented [here](https://github.com/wagonhelm/image_augment/blob/master/DataAug.ipynb).\r\n\r\n", "comments": ["I think the issue is that, when sample_distorted_bounding_box switched to V2, min_object_covered has been changed form an attr to an input. As input could not have a default value like attr, min_object_covered=None will result in an error. Created a PR #15531 for the fix.", "Nagging Awaiting TensorFlower: It has been 14 days with no activityand the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly."]}, {"number": 15528, "title": "Update debugger.md", "body": "When working thru the examples, I'm seeing just `Softmax` rather than `softmax:Softmax`.  If this is indeed correct the output image also needs to be updated.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "Thanks @kagemusha. Could you fix your cla?", "I signed the CLA", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 15527, "title": "[Bug] Tensorflow serving loads incorrect model weights when using saved model main_op", "body": "\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Debian 3.16.36 x86_64\r\n- **TensorFlow installed from (source or binary)**: Binary (pip)\r\n- **TensorFlow version (use command below)**: Git version 1.4.0-19-ga52c8d9 Release version 1.4.1\r\n- **Python version**: 2.7.9\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**: See attached gist\r\n\r\nHey there,\r\n\r\nI've documented a lot of this bug over on this [issue](https://github.com/tensorflow/serving/issues/656) on the tensorflow serving repository, which was closed with the direction to open an issue here.\r\n\r\nEssentially, the bug is as follows: I have an embedding layer in Keras that uses some pre-initialized weights. When I export the model for use by TF serving, I note the following behavior:\r\n\r\n- The keras model itself has no issue outputting the correct results\r\n- The exported model itself (upon inspection) has the correct weights\r\n- The result values from TF serving are incorrect.\r\n\r\nI've narrowed it down to an issue with `tensorflow.python.ops.variables.global_variables_initializer`, as follows:\r\n\r\nWhen specifying the `main_op` argument in `tensorflow.saved_model.builder.SavedModelBuilder.add_meta_graph_and_variables`, if I use `tensorflow.saved_model.main_op.main_op()` I encounter this issue. \r\n\r\nIf instead, I use a control flow group that excludes the global variables initializer as follows:\r\n\r\n```\r\nmain_op_new = control_flow_ops.group(                 \r\n            lookup_ops.tables_initializer(),          \r\n            variables.local_variables_initializer(),  \r\n)                                                     \r\n```\r\n\r\nI do not encounter this issue. I've attached the full code for reproducing this issue (with the caveat of needing `tensorflow_model_server` running) [here](https://gist.github.com/zmjjmz/ce9c7a896933a02953cae0069a2ca21e)\r\n\r\nHere's the example output with the global variables initializer: https://gist.github.com/zmjjmz/d739cdfa52148eb814450e48cbf8ddb6\r\n\r\nIf you comment out line 83 of the repro code and uncomment line 84, you should get the correct output as shown here: https://gist.github.com/zmjjmz/9edee5b4eeff94f383122545d80ee55f\r\n\r\nThanks for helpin out\r\n\r\n\r\n\r\n\r\n", "comments": ["Someone posted a day ago https://github.com/tensorflow/serving/issues/656#issuecomment-352907846:\r\n\r\n> for me, the difference here disappeared in a recent TF / keras update ... as far as I could understand it it was a difference in TF prediction and TF Serving prediction that existed briefly and was fixed in a recent release\r\n\r\nDoes that resolve the issue for you?", "I was just able to reproduce it with the versions I listed in the issue, which are the latest versions afaict from the Github releases page. \r\n\r\nIf those are the updated versions then, no -- but maybe I'm missing something newer?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "This is still an issue for the versions I listed (not sure if I need to reply to @tensorflowbutler), although I may take some time later to see if it's an issue w/TF 1.5rc.", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "Hey there,\r\n\r\nI think this isn't really an issue -- using the `weights` argument is deprecated and as per the solution outlined to [a similar issue I had](https://github.com/tensorflow/tensorflow/issues/16386), if I use a `keras.initializers.Constant(value=embedding_mat)` in its stead I cannot reproduce this issue.\r\n\r\nThat said, I'm still not sure why the inspected checkpoint would show all zeros in the embedding matrix in this case and not in the other case, but I'll leave that unsolved.", "> ### System information\r\n> * **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n> * **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Debian 3.16.36 x86_64\r\n> * **TensorFlow installed from (source or binary)**: Binary (pip)\r\n> * **TensorFlow version (use command below)**: Git version 1.4.0-19-ga52c8d9 Release version 1.4.1\r\n> * **Python version**: 2.7.9\r\n> * **Bazel version (if compiling from source)**:\r\n> * **GCC/Compiler version (if compiling from source)**:\r\n> * **CUDA/cuDNN version**:\r\n> * **GPU model and memory**:\r\n> * **Exact command to reproduce**: See attached gist\r\n> \r\n> Hey there,\r\n> \r\n> I've documented a lot of this bug over on this [issue](https://github.com/tensorflow/serving/issues/656) on the tensorflow serving repository, which was closed with the direction to open an issue here.\r\n> \r\n> Essentially, the bug is as follows: I have an embedding layer in Keras that uses some pre-initialized weights. When I export the model for use by TF serving, I note the following behavior:\r\n> \r\n> * The keras model itself has no issue outputting the correct results\r\n> * The exported model itself (upon inspection) has the correct weights\r\n> * The result values from TF serving are incorrect.\r\n> \r\n> I've narrowed it down to an issue with `tensorflow.python.ops.variables.global_variables_initializer`, as follows:\r\n> \r\n> When specifying the `main_op` argument in `tensorflow.saved_model.builder.SavedModelBuilder.add_meta_graph_and_variables`, if I use `tensorflow.saved_model.main_op.main_op()` I encounter this issue.\r\n> \r\n> If instead, I use a control flow group that excludes the global variables initializer as follows:\r\n> \r\n> ```\r\n> main_op_new = control_flow_ops.group(                 \r\n>             lookup_ops.tables_initializer(),          \r\n>             variables.local_variables_initializer(),  \r\n> )                                                     \r\n> ```\r\n> I do not encounter this issue. I've attached the full code for reproducing this issue (with the caveat of needing `tensorflow_model_server` running) [here](https://gist.github.com/zmjjmz/ce9c7a896933a02953cae0069a2ca21e)\r\n> \r\n> Here's the example output with the global variables initializer: https://gist.github.com/zmjjmz/d739cdfa52148eb814450e48cbf8ddb6\r\n> \r\n> If you comment out line 83 of the repro code and uncomment line 84, you should get the correct output as shown here: https://gist.github.com/zmjjmz/9edee5b4eeff94f383122545d80ee55f\r\n> \r\n> Thanks for helpin out\r\n\r\nThank you for point it out and it helps a lot ^_^"]}]