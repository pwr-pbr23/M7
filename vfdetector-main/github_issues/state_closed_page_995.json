[{"number": 23538, "title": "Implement TFLite Micro's preprocessor with CMSIS libs; create Apollo3 tests", "body": "The purpose of this pull request is to implement a portion of the TFLite Micro code using the ARM CMSIS-DSP libs. More specifically, we implement a fixed-point version of the preprocessing function that takes raw audio samples and turns them into a spectrogram.\r\n\r\nThe added files allow users to create a *.bin that can test this function on the Apollo3 device.\r\n\r\nThis code is still in an experimental stage.\r\n\r\nThe primary maintainer of the micro code is @petewarden ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "Nagging Assignee @aaroey: It has been 21 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 23537, "title": "Failed to obtain tensors of correct batch size", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Red hat 7.3\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 1.10\r\n- Python version: 2.7.5\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 9.0\r\n- GPU model and memory: P5000, 16G\r\n\r\n\r\nThe following code is just retrieving specific digits from mnist of specified batch size. But the returned tensor is just of one sample:\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport sonnet as snt\r\n\r\nclass Input(snt.AbstractModule):\r\n    def __init__(self, batch_size, name = \"input\"):\r\n        super(Input, self).__init__(name = name)\r\n\r\n        mnist = tf.keras.datasets.mnist\r\n\r\n        (X_train, Y_train), (X_test, Y_test) = mnist.load_data()\r\n\r\n        train_filter = np.where((Y_train == 0 ) | (Y_train == 1))\r\n        test_filter = np.where((Y_test == 0) | (Y_test == 1))\r\n\r\n        X_train, Y_train = X_train[train_filter], Y_train[train_filter]\r\n        X_test, Y_test = X_test[test_filter], Y_test[test_filter]\r\n\r\n        print(X_train.shape)\r\n        print(Y_train.shape)\r\n\r\n        with self._enter_variable_scope():\r\n            self._db_train = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\r\n            self._db_test = tf.data.Dataset.from_tensor_slices((X_test, Y_test))\r\n\r\n            self._db_train.repeat(-1)\r\n            self._db_test.repeat(-1)\r\n\r\n            self._db_train.batch(batch_size)\r\n            self._db_test.batch(batch_size)\r\n\r\n            # self._db_train.apply(tf.contrib.data.batch_and_drop_remainder(batch_size))\r\n            # self._db_test.apply(tf.contrib.data.batch_and_drop_remainder(batch_size))\r\n\r\n            self._it_train = self._db_train.make_one_shot_iterator()\r\n            self._it_test = self._db_test.make_one_shot_iterator()\r\n\r\n\r\n    def _build(self, is_training = True):\r\n\r\n        if is_training:\r\n            inputs, labels = self._it_train.get_next()\r\n        else:\r\n            inputs, labels = self._it_test.get_next()\r\n\r\n        return inputs, labels\r\n\r\ndef test():\r\n    input_ = Input(32)\r\n\r\n    inputs, labels = input_()\r\n\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n\r\n        inputs_val, labels_val = sess.run([inputs, labels])\r\n\r\n\r\n        print(inputs_val.shape)\r\n        print(labels_val.shape)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    test()\r\n\r\n```\r\n\r\nThe output of the above code snippet is as follows:\r\n\r\n```\r\n(12665, 28, 28)\r\n(12665,)\r\n(28, 28)\r\n()\r\n\r\n```\r\nNote I deleted something irrelevant.", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "Actually I have posted it on StackOverflow but no one gives some suggestions. https://stackoverflow.com/questions/53153933/failed-to-obtain-tensors-of-correct-batch-size\r\n\r\nIt is okay if it is ranked as not a defeat since I have found some workaround. \r\n\r\nThanks\r\n "]}, {"number": 23536, "title": "Build CMAKE C++/Cuda project with tensorflow in debug mode", "body": "**System information**\r\n- OS Platform and Distribution: Ubuntu 18.04\r\n- TensorFlow installed from binary\r\n- TensorFlow version: 1.11.0\r\n- Python version: 3.6.5\r\n- Installed using pip\r\n- CUDA/cuDNN version: 9.0\r\n- GPU model and memory: Tesla K and V series\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI am trying to integrate tensorflow in my CMAKE C++/Cuda project. It _works_ when I link tensorflow when I do simply `cmake .`, which is for `RelWithDebInfo` build. However, it fails on `Release` or `Debug` mode. I speculate that the `libtensorflow_framework.so` that I have on my `dist-packages/tensorflow/` is not meant to be for that. Below, I share 2 different ways I try to link tensorflow in debug mode. Both give me the same error as found further below.\r\n\r\n**_Can you hint me what goes wrong?_**\r\n\r\n## Add Tensorflow N.1\r\n```\r\nSET(TF_ROOT \"/usr/local/lib/python3.6/dist-packages/tensorflow/\")\r\ninclude_directories(SYSTEM \"${TF_ROOT}/include/\")\r\nlink_directories(\"${TF_ROOT}\")\r\ntarget_link_libraries(\r\n\tCppLib.Tests\r\n  \tdebug tensorflow_framework\r\n)\r\n```\r\n\r\n## Add Tensorflow N.2\r\n```\r\nSET(TF_ROOT \"/usr/local/lib/python3.6/dist-packages/tensorflow/\")\r\ninclude_directories(SYSTEM \"${TF_ROOT}/include/\")\r\n#link_directories(\"${TF_ROOT}\")\r\n\r\nadd_executable(CppLib.Tests ${SRC} ${GMOCK_SRC} ${GTEST_SRC} ${GTEST_MAIN_SRC})\r\n\r\nadd_library(tensorflow_framework_ SHARED IMPORTED)\r\nset_target_properties(tensorflow_framework_ PROPERTIES\r\n                IMPORTED_LOCATION_DEBUG \"${TF_ROOT}/libtensorflow_framework.so\")\r\nset_target_properties(tensorflow_framework_ PROPERTIES\r\n                IMPORTED_LOCATION_RELEASE \"${TF_ROOT}/libtensorflow_framework.so\")\r\n\r\ntarget_link_libraries(\r\n\tCppLib.Tests\r\n        tensorflow_framework_\r\n)\r\n```\r\n\r\n\r\n\r\n## Error\r\n```\r\n#ERROR\r\nCMakeFiles/CppLib.Tests.dir/TestTensorflow.cpp.o: In function `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >* tensorflow::internal::MakeCheckOpString<long, int>(long const&, int const&, char const*)':\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/platform/default/logging.h:187: undefined reference to `tensorflow::internal::CheckOpMessageBuilder::NewString[abi:cxx11]()'\r\ncollect2: error: ld returned 1 exit status\r\ntests/CppLib/CMakeFiles/CppLib.Tests.dir/build.make:799: recipe for target 'bin/Debug/CppLib.Tests' failed\r\nmake[2]: *** [bin/Debug/CppLib.Tests] Error 1\r\nCMakeFiles/Makefile2:1169: recipe for target 'tests/CppLib/CMakeFiles/CppLib.Tests.dir/all' failed\r\nmake[1]: *** [tests/CppLib/CMakeFiles/CppLib.Tests.dir/all] Error 2\r\nMakefile:140: recipe for target 'all' failed\r\nmake: *** [all] Error 2\r\n```\r\n", "comments": ["Any progress on that? ", "cmake is now community support, please try building with bazel.", "@skye unfortunately that is not an option, as I integrate Tensorflow on an existing hybrid python/cpp/Cuda library. The migration to Bazel would be difficult and risky at the moment. If you expect that the existing binary installed in the dist-packages folder will not work in debug, what is the best way to proceed with cmake?", "I'm not a build expert, but I think you have to rebuild the entire binary from scratch. Or maybe you can build the needed TensorFlow binaries with bazel, and then link them via your existing cmake project.", "Hi @pnik073 ,\r\nWe are checking to see if you still need help on this issue. We recommend that you upgrade to 2.6 which is latest stable version of TF and let us know if the issue still persists in newer versions. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23536\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23536\">No</a>\n"]}, {"number": 23535, "title": " tf.manip.roll implementation details", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: 1.8\r\n- Doc Link: https://www.tensorflow.org/api_docs/python/tf/manip/roll\r\n\r\n\r\n**Describe the documentation issue**\r\nHow is this operation executed? If i ask the tensor of size n to shift by 2, is there a new tensor allocated with size n+2, and the rotation happens by two memcopies of the first/last two values(based on direction of rotation) ?\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n", "comments": []}, {"number": 23534, "title": "Tensorflow Lite Android - No Operation named [input] in the Graph", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Nexus 5X, Samsung S5\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:  1.12.0-rc2\r\n- Python version: 3\r\n- Installed using virtualenv? pip? conda?:  Google Colab\r\n- Bazel version (if compiling from source): Google Colab\r\n- GCC/Compiler version (if compiling from source): Google Colab\r\n- CUDA/cuDNN version: Google Colab\r\n- GPU model and memory: Google Colab\r\n\r\n\r\nI am building a simple random model with keras, export it as tensorflow lite model from Google Colab, load it on Android, but i am unnable to run classification\r\n\r\nMy Colab notebook file is available here:\r\nhttps://colab.research.google.com/drive/1vf9wVXlNQ77GUF_JyL3bKv0M72aK0xvG\r\n\r\nThe resulting model can be found here\r\n[model.zip](https://github.com/tensorflow/tensorflow/files/2550726/model.zip)\r\n\r\nWhen i open the model with Netron it looks like following:\r\n![model](https://user-images.githubusercontent.com/2804470/48028365-9695da00-e14b-11e8-8ae1-e262510afead.png)\r\n\r\nThe Android code:\r\n` \r\nimplementation 'org.tensorflow:tensorflow-android:1.12.0-rc1'\r\n\r\nprivate static final String MODEL_FILE = \"file:///android_asset/model.tflite\";\r\nprivate static final String INPUT_NODE = \"input\";\r\nprivate static final long[] INPUT_SIZE = {3};\r\nprivate static final String[] OUTPUT_NODES = {\"output/Softmax\"};\r\nprivate static final String OUTPUT_NODE = \"output/Softmax\";\r\nprivate static final int OUTPUT_SIZE = 2;\r\n\r\ninferenceInterface = new TensorFlowInferenceInterface(inputStream);\r\nfloat[] data = new float[]{100,200,300};\r\ninferenceInterface.feed(INPUT_NODE, data, INPUT_SIZE);`\r\n\r\nBut I when i call \"feed\", I always always an exception:\r\n` java.lang.IllegalArgumentException: No Operation named [input] in the Graph`\r\n\r\nI tried different input names, and also specify the input from the converter, but still no success and i am really stucked here. \r\nDoes anyone have an idea what i am actually doing wrong?\r\n\r\n", "comments": ["Have you tried \"input_input\" ? In the Netron, the id says it's \"input_input\".", "Yeah, i tried already a lot. Always same result:\r\nNo Operation named [input_input] in the Graph\r\n\r\nI may think that the model could be the problem, cause its very small (2kb).. but i don't know..\r\n", "@myfknoll : You are using Tensorflow Java API with Tensorflow lite. Instead you should use Tensorflow Lite [Java API](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java). For an example check this [example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/java/demo/app/src/main/java/com/example/android/tflitecamerademo/ImageClassifier.java#L99)", "@shashishekhar you're right\r\n@myfknoll check official [tutorial](https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2-tflite/#6)", "Yeah it finally solved my problem. Thank you very much!\r\n", "> Yeah it finally solved my problem. Thank you very much!\r\n\r\n@myfknoll how you solve it?hope your relply", "how you solve it?", "I am using now a different dependency\r\n`implementation 'org.tensorflow:tensorflow-lite:1.10.0'`\r\n\r\nYou can find this also in the tutorial linked by @nickaein-a above", "can anyone help me to use the latest version. I think i am lost", "Hi @myfknoll !\r\nI'm having the same problem, but running in react-native.\r\nWhere did you put this line \"implementation 'org.tensorflow:tensorflow-lite:1.10.0'\"\r\n\r\nThank you", "@GustavoMSevero You can try in your app's `build.gradle` file under `dependencies { }`."]}, {"number": 23533, "title": "Failed to load the native TensorFlow runtime error", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows 10 Home\r\n- TensorFlow installed from (source or binary): from cmd using command \"pip install tensorflow\"\r\n- TensorFlow version:  tensorflow-1.11.0\r\n- Python version: 3.6.7amd 64\r\n- Installed using virtualenv? pip? conda?: pip install tensorflow\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Problem**\r\n I thought that was a problem due to my python version Python 3.7.1, so I deleted it and installed a new python 3.6.7.\r\nThen I used cmd to get to script and run \"pip install tensorflow\"\r\nThen I created a simple py file \"import tensorflow as tf\" \r\n\r\nI got so many mistakes, such as \r\n\"ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\nFailed to load the native TensorFlow runtime..\"\r\n\r\nso I removed \"pip uninstall tensorflow\". Then, I tried \"pip install --upgrade tensorflow\" still same errors.\r\nThen \"pip3 install --upgrade tensorflow\" Still cant use tensorflow\r\nPlease help me\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\elmia\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\elmia\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\elmia\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\elmia\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\elmia\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\elmia\\Desktop\\python_test\\deep_learning_tensorflow_sucks\\1.py\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\elmia\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\elmia\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\elmia\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\elmia\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\elmia\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\elmia\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\elmia\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\elmia\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["Looks the same as this one. \r\n\r\nTensor flow is breaking with Failed to load the native TensorFlow runtime #23526\r\n\r\nThe other user had to revert back to 1.5\r\n\r\nMy guess is the DLLs aren't found properly and that's why it's not showing the correct missing dll on dynamic_load()\r\n\r\nJust curious but did you see any eager execution errors when trouble shooting?", "> Looks the same as this one.\r\n> \r\n> Tensor flow is breaking with Failed to load the native TensorFlow runtime #23526\r\n> \r\n> The other user had to revert back to 1.5\r\n> \r\n> My guess is the DLLs aren't found properly and that's why it's not showing the correct missing dll on dynamic_load()\r\n> \r\n> Just curious but did you see any eager execution errors when trouble shooting?\r\n\r\n:(( Thanks, \r\nThats too bad. I spent so much time downloading new version, but then I accidently downloaded 3.6.7 win-32, so I again reinstalled to 3.6.7 win-64, sinc eI read somewhere that only win-64 supports tesorflow. Still erorrs. So I thought that it might be be my old version kind of brings problem to the new one. So I removed completely my old Python and had to reinstall all the libraries like pandas, numpy, etc.\r\nDid you say the other user had to switch back to 3.5 x or 3.6.5?\r\nLike I mentioned I get \"ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\"", "Check if #19584 helps", "> Check if #19584 helps\r\n\r\n@eachabys  Any update ?", "In \"awaiting response\" status for more than 3 days. Hence closing this. Please post the updates here if any, we will reopen the issue. Thanks !"]}, {"number": 23532, "title": "Correct error messages in graph transforms.", "body": "Correct copy-pasted mistakes in error messages in transforms: remove_attribute, rename_attribute, rename_op.", "comments": ["Nagging Reviewer @petewarden: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 44 days with no activity and the `awaiting review` label has been applied."]}, {"number": 23531, "title": "Porting tf-coriander to Tensorflow r1.11. Have one general question and one building issue.", "body": "Hello\r\nI am working on to improve tf-coriander and to port it on Tensorflow r1.11 since I could not find someone else's doing it on the web. If there is someone else working on this, please let me know if you want to work together and share the result with others. \r\nAfter spending a few weeks working on this, I got the stage to compile properly but fail in the linker. The error message is like:\r\n\"..._pywrap_tensorflow_internal.so: undefined symbol: _ZNK10tensorflow7functor4TileIN5Eigen9GpuDeviceEiiEclERKS3_PNS_6TensorERKS7_N4absl4SpanIKiEE\". \r\n\r\nI only have tile_functor commented out 'GOOGLE_CUDA\" as Hugh Perkins suggested. It seems to me that Tensorflow r1.11 separates  the functor API from their implementations, and pack those into different share objects. I am not sure which BUILD file need be changed to make them linked statically. (I have tried 'framework_shared_object' label.)\r\n\r\nCould someone share some lights on how to address the issue? or is it the right path to port tf-coriander?\r\n\r\n\r\nthank you very much,\r\n\r\nSteven Wang \r\n\r\n", "comments": ["@alextp any ideas?", "The issue is closed. "]}, {"number": 23530, "title": "Limit Tensorflow running in 1 thread", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.10\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): 0.16.1\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nI was asked to write a C++ program running inference on Tensorflow without using more than 1 thread. Here is my code: \r\n\r\n```\r\n    tensorflow::SessionOptions options;\r\n    tensorflow::ConfigProto & config = options.config;\r\n    config.set_inter_op_parallelism_threads(1);\r\n    config.set_intra_op_parallelism_threads(1);\r\n    config.set_use_per_session_threads(false);\r\n    \r\n   // also tried set_use_per_session_threads to true and 1 thread pool\r\n    //auto* p = config.add_session_inter_op_thread_pool();\r\n    //p->set_global_name(\"large pool\");\r\n    //p->set_num_threads(1);\r\n\r\n```\r\n\r\nThe program indeed uses only 1 core (cpu by htop only 99-100%), however, it always spawns 10 thread. Am I missing anything? \r\n\r\nThank you in advance. \r\n", "comments": ["I was able to log the name of the thread pools:\r\n\r\n> name: Eigen num_threads: 1\r\n> name: Compute0 num_threads: 1\r\n> name: graph_runner num_threads: 1\r\n> name: graph_runner num_threads: 1\r\n> name: graph_runner num_threads: 1\r\n> name: graph_runner num_threads: 1\r\n> name: graph_runner num_threads: 1\r\n> name: graph_runner num_threads: 1\r\n> name: graph_runner num_threads: 1\r\n> name: graph_runner num_threads: 1\r\n> name: graph_runner num_threads: 1\r\n> name: graph_runner num_threads: 1\r\n\r\nI guess the `Eigen` and `Compute0` thread pools are controlled by the inter and intra configs. \r\nthe \"graph_runner\" are created by this constructor `GraphRunner::GraphRunner(Env* env)`. However, I'm not sure where these methods is called.  Does anyone have an idea? Thanks.", "I could disable the `graph_runner`s but those threads are still there!!! The number of threads is roughly the number of CPU cores. ", "@hoavt-54 is your problem solved?", "@quietsmile Hi, no, I didn't. Had to use another framework eventually. "]}, {"number": 23529, "title": "Oscillating validation loss", "body": "I am training a deep CNN for image dehazing, and during training I plot the training and the validations losses. I plot the validation loss every 1 epoch. I do all the process on Tensorflow framework in Ubuntu 16 . The problem is my training loss decrease really well but my validation loss oscillates and it does not have a decreasing shape. Is it normal?\r\n\r\nHere are the images of training and validation loss plots. The orange one is the training loss and the blue one is the validation loss.\r\n\r\nThe x-axis on the train plot is the iteration number and the x-axis on the validation plot is the epoch number.\r\n\r\n![train](https://user-images.githubusercontent.com/32719272/48010891-7daa0c00-e0ec-11e8-95b3-863e66b0f044.png)\r\n\r\n![validation](https://user-images.githubusercontent.com/32719272/48010905-84d11a00-e0ec-11e8-9502-bc0eb65fd177.png)\r\n\r\n\r\nI really appreciate if you help me with this problem to figure out what part of my approach is wrong", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow-tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n", "> This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow-tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nI asked this question in stackoverflow and they told me to aski it here and it is not related to them!!!!\r\n"]}, {"number": 23528, "title": "Convert to .tflite from saved checkpoint", "body": "**System information**\r\n- TensorFlow version: 1.10.0\r\n- Are you willing to contribute it  Yes:\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\ntflite_convert --output_file=test.tflite --saved_model_dir=test_checkpoint_saved_data\r\n\r\noutput:\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/tflite_convert\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 370, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 366, in run_main\r\n    _convert_model(tflite_flags)\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 94, in _convert_model\r\n    converter = _get_toco_converter(flags)\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 81, in _get_toco_converter\r\n    return converter_fn(**converter_kwargs)\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/contrib/lite/python/lite.py\", line 268, in from_saved_model\r\n    output_arrays, tag_set, signature_key)\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/contrib/lite/python/convert_saved_model.py\", line 239, in freeze_saved_model\r\n    meta_graph = _get_meta_graph_def(saved_model_dir, tag_set)\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/contrib/lite/python/convert_saved_model.py\", line 61, in _get_meta_graph_def\r\n    return loader.load(sess, tag_set, saved_model_dir)\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/saved_model/loader_impl.py\", line 211, in load\r\n    loader = SavedModelLoader(export_dir)\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/saved_model/loader_impl.py\", line 230, in __init__\r\n    self._saved_model = _parse_saved_model(export_dir)\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/saved_model/loader_impl.py\", line 80, in _parse_saved_model\r\n    constants.SAVED_MODEL_FILENAME_PB))\r\nIOError: SavedModel file does not exist at: test_checkpoint_saved_data/{saved_model.pbtxt|saved_model.pb}\r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\n\r\n**Any Other info.**\r\n", "comments": ["Please provide following information:\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:", "Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No, just want to convert trained ckpt\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): MAC OS 10.13.6\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Pixel XL\r\nTensorFlow installed from (source or binary): binary\r\nTensorFlow version (use command below): 1.11.0\r\nPython version: 2.7\r\nBazel version (if compiling from source): not using Bazel\r\nGCC/Compiler version (if compiling from source): not using GCC\r\nCUDA/cuDNN version: don't know\r\nGPU model and memory: NVIDIA GeForce GT 750M 2048 MB\r\n                                            Intel Iris Pro 1536 MB", "Nagging Assignee @ymodak: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I think the converter only supports SavedModel format, not the checkpoint, metadata format.", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)", "Can we convert checkpoint to .tflite? "]}, {"number": 23527, "title": "Acc and loss evaluate to 0.00 and trains for only 1 global steps.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:No\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):1.12\r\n- Python version:3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:1xTesla K80  12GB GDDR5 VRAM\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nMy model's accuracy and loss are evaluating to 0.\r\n\r\n**Describe the expected behavior**\r\nThe global steps should be 1625 but it's 1.\r\nThe acc and loss shouldn't be equal to 0 as both of them are contradicting each other.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nmy input function,keras estimator,train_and_evaluate are as follows:\r\n```\r\ndef make_input_fn(addrs,labels,batch_size,mode):\r\n    \r\n    filename_dataset = tf.data.Dataset.from_tensor_slices((addrs,labels))     \r\n    \r\n    dataset = filename_dataset.apply(tf.contrib.data.map_and_batch(lambda addrs, labels: tuple(tf.py_func(\r\n        process, [addrs, labels], [tf.uint8, labels.dtype])),\r\n                                                                   batch_size,\r\n                                                                   num_parallel_batches=2,\r\n                                                                   drop_remainder=False))\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n      num_epochs = None # indefinitely\r\n      dataset = dataset.apply(tf.contrib.data.shuffle_and_repeat(buffer_size = 10000))\r\n    else:\r\n      num_epochs = 1\r\n      dataset = dataset.repeat(num_epochs)\r\n   \r\n    dataset = dataset.prefetch(buffer_size=batch_size)\r\n    images,labels = dataset.make_one_shot_iterator().get_next()\r\n    images.set_shape([None,512,512,3])\r\n    labels.set_shape([None,1])\r\n    return images,labels\r\n`\r\n\r\ndef keras_estimator(model_dir,config):\r\n  base_model = Xception(weights='imagenet', include_top=False,input_shape = (512,512,3),classes = 5)\r\n\r\n\r\n  x = base_model.output\r\n  x = GlobalAveragePooling2D()(x)\r\n  \r\n  x = Dense(1024, activation='relu')(x)\r\n  x = Dropout(0.2)(x)\r\n  x = Dense(256, activation='relu')(x)\r\n  x = Dropout(0.2)(x)\r\n  \r\n  predictions = Dense(5, activation='softmax')(x)\r\n\r\n  \r\n  model = Model(inputs=base_model.input, outputs=predictions)\r\n\r\n  \r\n  for layer in base_model.layers:\r\n      layer.trainable = False\r\n  model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\r\n  estimator = tf.keras.estimator.model_to_estimator(keras_model=model,model_dir=model_dir,\r\n    config=config)\r\n  return estimator\r\n\r\n\r\ndef train_and_evaluate(model_dir):\r\n  t_batch_size = 512\r\n  e_batch_size = 64\r\n  num_epochs = 25\r\n  import pandas as pd\r\n  df = pd.read_csv('/content/trainLabels.csv')\r\n  from random import shuffle\r\n  addrs = ['/content/train/train/' + str(df.iloc[i]['image']) + '.jpeg' for i in range(len(df))]\r\n  labels = df['level'].values.tolist()\r\n  c = list(zip(addrs, labels))\r\n  shuffle(c)\r\n  addrs1, labels1 = zip(*c)\r\n  train_addrs = addrs1[0 : int(0.9 * len(addrs))]\r\n  train_labels = labels1[0 : int(0.9 * len(labels))]\r\n  val_addrs = addrs1[ int(0.9 * len(addrs)) : ]\r\n  val_labels = labels1[ int(0.9 * len(addrs)) : ]\r\n  train_addrs = list(train_addrs)\r\n  train_labels = list(train_labels)\r\n  val_addrs = list(val_addrs)\r\n  val_labels = list(val_labels)\r\n  \r\n  run_config = tf.estimator.RunConfig(save_checkpoints_secs=300)\r\n  \r\n  estimator = keras_estimator(model_dir,run_config)\r\n  \r\n  t_max_steps = (len(train_addrs) // t_batch_size) * num_epochs\r\n  \r\n  train_spec = tf.estimator.TrainSpec(input_fn = lambda : make_input_fn(train_addrs,train_labels,t_batch_size,mode=tf.estimator.ModeKeys.TRAIN),max_steps = t_max_steps)\r\n  \r\n  eval_spec = tf.estimator.EvalSpec(input_fn = lambda : make_input_fn(val_addrs,val_labels,e_batch_size,mode=tf.estimator.ModeKeys.EVAL),steps = None,start_delay_secs=10,\r\n        throttle_secs=300)\r\n  \r\n  \r\n  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n```\r\n    \r\n\r\n\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n`INFO:tensorflow:Running training and evaluation locally (non-distributed).\r\nINFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 300.\r\nWARNING:tensorflow:From <ipython-input-7-80b5bdaf35df>:9: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.experimental.map_and_batch(...)`.\r\nWARNING:tensorflow:From <ipython-input-7-80b5bdaf35df>:12: shuffle_and_repeat (from tensorflow.contrib.data.python.ops.shuffle_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.experimental.shuffle_and_repeat(...)`.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='/content/training/keras/keras_model.ckpt', vars_to_warm_start='.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})\r\nINFO:tensorflow:Warm-starting from: ('/content/training/keras/keras_model.ckpt',)\r\nINFO:tensorflow:Warm-starting variable: dense/kernel; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: dense/bias; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: dense_1/kernel; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: dense_1/bias; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: dense_2/kernel; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: dense_2/bias; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: Adam/iterations; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: Adam/lr; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: Adam/beta_1; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: Adam/beta_2; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: Adam/decay; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: training/Adam/Variable; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: training/Adam/Variable_1; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: training/Adam/Variable_2; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: training/Adam/Variable_3; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: training/Adam/Variable_4; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: training/Adam/Variable_5; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: training/Adam/Variable_6; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: training/Adam/Variable_7; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: training/Adam/Variable_8; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: training/Adam/Variable_9; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: training/Adam/Variable_10; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: training/Adam/Variable_11; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: training/Adam/Variable_12; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: training/Adam/Variable_13; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: training/Adam/Variable_14; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: training/Adam/Variable_15; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: training/Adam/Variable_16; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: training/Adam/Variable_17; prev_var_name: Unchanged\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Graph was finalized.\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Saving checkpoints for 0 into /content/training/model.ckpt.\r\nINFO:tensorflow:Saving checkpoints for 1 into /content/training/model.ckpt.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Starting evaluation at 2018-11-05-13:21:17\r\nINFO:tensorflow:Graph was finalized.\r\nINFO:tensorflow:Restoring parameters from /content/training/model.ckpt-1\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Finished evaluation at 2018-11-05-13:22:08\r\nINFO:tensorflow:Saving dict for global step 1: acc = 0.0, global_step = 1, loss = 0.0\r\nINFO:tensorflow:Saving 'checkpoint_path' summary for global step 1: /content/training/model.ckpt-1\r\nINFO:tensorflow:Loss for final step: None.`\r\n\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow-tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n"]}, {"number": 23526, "title": "Tensor flow is breaking with Failed to load the native TensorFlow runtime", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWindows 8.1\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNo Mobile device used\r\n- TensorFlow installed from (source or binary):\r\n   Binary used Pip command\r\n- TensorFlow version:\r\nName: tensorflow\r\nVersion: 1.11.0\r\nSummary: TensorFlow is an open source machine learning framework for everyone.\r\nHome-page: https://www.tensorflow.org/\r\nAuthor: Google Inc.\r\nAuthor-email: opensource@google.com\r\nLicense: Apache 2.0\r\nLocation: c:\\users\\avinash.t\\anaconda3\\lib\\site-packages\r\nRequires: tensorboard, numpy, six, keras-applications, setuptools, grpcio, gast,\r\n protobuf, termcolor, wheel, astor, absl-py, keras-preprocessing\r\n\r\n\r\n- Python version:\r\nC:\\Users\\avinash.t>python --version\r\nPython 3.6.7 :: Anaconda, Inc.\r\n- Installed using virtualenv? pip? conda?:\r\npip\r\n- Bazel version (if compiling from source):\r\n Not complied from Source\r\n- GCC/Compiler version (if compiling from source):\r\nNo use of GCC\r\n- CUDA/cuDNN version:\r\nNo use of CUDA\r\n- GPU model and memory:\r\nNo use o GPU\r\n\r\n\r\n**Describe the problem**\r\nSimple import is not working\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nC:\\Users\\avinash.t>python\r\nPython 3.6.7 |Anaconda, Inc.| (default, Oct 28 2018, 19:44:12) [MSC v.1915 64 bi\r\nt (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_\r\ntensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_\r\ntensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_\r\ntensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript\r\nion)\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routin\r\ne failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\",\r\nline 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-im\r\nport\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init_\r\n_.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_\r\ntensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_\r\ntensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_\r\ntensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_\r\ntensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript\r\nion)\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routin\r\ne failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_probl\r\nems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>>\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nC:\\Users\\avinash.t>python\r\nPython 3.6.7 |Anaconda, Inc.| (default, Oct 28 2018, 19:44:12) [MSC v.1915 64 bi\r\nt (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_\r\ntensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_\r\ntensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_\r\ntensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript\r\nion)\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routin\r\ne failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\",\r\nline 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-im\r\nport\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init_\r\n_.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_\r\ntensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_\r\ntensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_\r\ntensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_\r\ntensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript\r\nion)\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routin\r\ne failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_probl\r\nems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>>\r\n\r\n\r\n", "comments": ["@skye  PTAL", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since I suspect this is an environment issue. Thanks.\r\n"]}, {"number": 23525, "title": "tf.contrib.estimator.InMemoryEvaluatorHook does not work with MirroredStrategy", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux 3.10.0-862.9.1.el7.x86_64, CentOS 7.5\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): ('v1.9.0-rc2-2836-g05f8ea8', '1.10.0')\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): 0.16.1\r\n- GCC/Compiler version (if compiling from source): c++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-28)\r\n- CUDA/cuDNN version: 9.2.148\r\n- GPU model and memory: 8x GTX 1080 Ti, 11178MiB\r\n\r\n**Describe the current behavior**\r\nWhen using tf.contrib.estimator.InMemoryEvaluatorHook together with tf.contrib.distribute.MirroredStrategy on more than 1 GPU I get an error as soon as evaluation starts:\r\nRuntimeError: Graph is finalized and cannot be modified.\r\n\r\nThis does not occur when using OneDeviceStrategy. The documentation of InMemoryEvaluatorHook states: \"It doesn't support multi-node distributed mode.\" However, I am on a single node and multi-GPU\r\n\r\n**Describe the expected behavior**\r\nThe error should not occur. If multi-GPU support is not supported yet, will it be in the future? The documentation should also be updated then.\r\n\r\n**Code to reproduce the issue**\r\ndistribution = tf.contrib.distribute.MirroredStrategy(num_gpus=2)\r\nrun_config = tf.estimator.RunConfig(train_distribute=distribution)\r\nestimator = tf.estimator.Estimator(model.model_fn, config=run_config)\r\nevaluator = tf.contrib.estimator.InMemoryEvaluatorHook(estimator, eval_input_fn, steps=20, every_n_iter=20)\r\nestimator.train(train_input_fn, steps=100, hooks=[evaluator])\r\n\r\n**Other info / logs**\r\nTraceback (most recent call last):\r\n  File \"/home/xxx/.vscode/extensions/ms-python.python-2018.9.2/pythonFiles/experimental/ptvsd_launcher.py\", line 115, in <module>\r\n    vspd.run(filename, port_num, run_as, *sys.argv[1:])\r\n  File \"/home/xxx/.vscode/extensions/ms-python.python-2018.9.2/pythonFiles/experimental/ptvsd/ptvsd/debugger.py\", line 43, in run\r\n    run_main(address, filename, run_as, *args, **kwargs)\r\n  File \"/home/xxx/.vscode/extensions/ms-python.python-2018.9.2/pythonFiles/experimental/ptvsd/ptvsd/_local.py\", line 52, in run_main\r\n    runner(addr, name, kind == 'module', *extra, **kwargs)\r\n  File \"/home/xxx/.vscode/extensions/ms-python.python-2018.9.2/pythonFiles/experimental/ptvsd/ptvsd/runner.py\", line 32, in run\r\n    set_trace=False)\r\n  File \"/home/xxx/.vscode/extensions/ms-python.python-2018.9.2/pythonFiles/experimental/ptvsd/ptvsd/_vendored/pydevd/pydevd.py\", line 1107, in run\r\n    return self._exec(is_module, entry_point_fn, module_name, file, globals, locals)\r\n  File \"/home/xxx/.vscode/extensions/ms-python.python-2018.9.2/pythonFiles/experimental/ptvsd/ptvsd/_vendored/pydevd/pydevd.py\", line 1114, in _exec\r\n    pydev_imports.execfile(file, globals, locals)  # execute the script\r\n  File \"/home/xxx/test/mv_estimator/train.py\", line 91, in <module>\r\n    tf.app.run()\r\n  File \"/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/home/xxx/test/mv_estimator/train.py\", line 80, in main\r\n    estimator.train(train_input_fn, steps=100, hooks=[evaluator])\r\n  File \"/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 354, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1177, in _train_model\r\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n  File \"/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1320, in _train_model_distributed\r\n    saving_listeners)\r\n  File \"/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1400, in _train_with_estimator_spec\r\n    log_step_count_steps=self._config.log_step_count_steps) as mon_sess:\r\n  File \"/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 504, in MonitoredTrainingSession\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 920, in __init__\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 643, in __init__\r\n    self._sess = _RecoverableSession(self._coordinated_creator)\r\n  File \"/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1106, in __init__\r\n    _WrappedSession.__init__(self, self._create_session())\r\n  File \"/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1111, in _create_session\r\n    return self._sess_creator.create_session()\r\n  File \"/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 806, in create_session\r\n    hook.after_create_session(self.tf_sess, self.coord)\r\n  File \"/home/xxx/.local/lib/python2.7/site-packages/tensorflow/contrib/estimator/python/estimator/hooks.py\", line 178, in after_create_session\r\n    self._evaluate(session)\r\n  File \"/home/xxx/.local/lib/python2.7/site-packages/tensorflow/contrib/estimator/python/estimator/hooks.py\", line 181, in _evaluate\r\n    var_name_to_value = train_session.run(self._var_name_to_train_var)\r\n  File \"/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 887, in run\r\n    run_metadata_ptr)\r\n  File \"/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1095, in _run\r\n    self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\r\n  File \"/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 429, in __init__\r\n    self._fetch_mapper = _FetchMapper.for_fetch(fetches)\r\n  File \"/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 249, in for_fetch\r\n    return _DictFetchMapper(fetch)\r\n  File \"/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 387, in __init__\r\n    _FetchMapper.for_fetch(fetch) for fetch in fetches.values()\r\n  File \"/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 255, in for_fetch\r\n    return _ElementFetchMapper(fetches, contraction_fn)\r\n  File \"/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 284, in __init__\r\n    fetch, allow_tensor=True, allow_operation=True))\r\n  File \"/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3473, in as_graph_element\r\n    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)\r\n  File \"/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3496, in _as_graph_element_locked\r\n    temp_obj = _as_graph_element(obj)\r\n  File \"/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 148, in _as_graph_element\r\n    return conv_fn()\r\n  File \"/home/xxx/.local/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/values.py\", line 513, in _as_graph_element\r\n    return self._get_cross_tower()\r\n  File \"/home/xxx/.local/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/values.py\", line 505, in _get_cross_tower\r\n    total = math_ops.add_n(all_components)\r\n  File \"/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py\", line 2145, in add_n\r\n    inputs = ops.convert_n_to_tensor_or_indexed_slices(inputs)\r\n  File \"/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1355, in convert_n_to_tensor_or_indexed_slices\r\n    values=values, dtype=dtype, name=name, as_ref=False)\r\n  File \"/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1326, in internal_convert_n_to_tensor_or_indexed_slices\r\n    value, dtype=dtype, name=n, as_ref=as_ref))\r\n  File \"/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1285, in internal_convert_to_tensor_or_indexed_slices\r\n    value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1124, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1258, in _dense_var_to_tensor\r\n    return var._dense_var_to_tensor(dtype=dtype, name=name, as_ref=as_ref)  # pylint: disable=protected-access\r\n  File \"/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1213, in _dense_var_to_tensor\r\n    return self.value()\r\n  File \"/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 647, in value\r\n    return self._read_variable_op()\r\n  File \"/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 730, in _read_variable_op\r\n    self._dtype)\r\n  File \"/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py\", line 508, in read_variable_op\r\n    \"ReadVariableOp\", resource=resource, dtype=dtype, name=name)\r\n  File \"/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3232, in create_op\r\n    self._check_not_finalized()\r\n  File \"/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2905, in _check_not_finalized\r\n    raise RuntimeError(\"Graph is finalized and cannot be modified.\")\r\n", "comments": ["@josh11b, since this bug is either in distribution strategies or estimators, can you please triage this?", "Hi @KDMueller! \r\nIt seems you are using older versions(1.x versions) of Tensorflow which is not supported any more. Please visit these links to upgrade your code-base to latest versions.Yoy may refer to [link](https://www.tensorflow.org/addons),[link2](https://www.tensorflow.org/guide/migrate). Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 23524, "title": "Python - Import Error of Tensorflow GPU in Ubuntu ", "body": "I am using python 3 with ubuntu 18.04 and have installed the following packages:\r\ncuda 9.0\r\ncudnn-9.0-linux-x64-v7.3.1.20 \r\ntensorflow gpu 1.5\r\n\r\nBut I am still getting an error while importing tensorflow. \r\n`Traceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/rafey/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/home/rafey/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 81, in <module>\r\n    from tensorflow.python import keras\r\n  File \"/home/rafey/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/__init__.py\", line 24, in <module>\r\n    from tensorflow.python.keras import activations\r\n  File \"/home/rafey/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/activations/__init__.py\", line 22, in <module>\r\n    from tensorflow.python.keras._impl.keras.activations import elu\r\n  File \"/home/rafey/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/__init__.py\", line 21, in <module>\r\n    from tensorflow.python.keras._impl.keras import activations\r\n  File \"/home/rafey/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/activations.py\", line 23, in <module>\r\n    from tensorflow.python.keras._impl.keras import backend as K\r\n  File \"/home/rafey/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/backend.py\", line 36, in <module>\r\n    from tensorflow.python.layers import base as tf_base_layers\r\n  File \"/home/rafey/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 25, in <module>\r\n    from tensorflow.python.keras.engine import base_layer\r\n  File \"/home/rafey/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/engine/__init__.py\", line 23, in <module>\r\n    from tensorflow.python.keras.engine.base_layer import InputSpec\r\n  File \"/home/rafey/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 33, in <module>\r\n    from tensorflow.python.keras import backend\r\n  File \"/home/rafey/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/backend/__init__.py\", line 22, in <module>\r\n    from tensorflow.python.keras._impl.keras.backend import abs\r\nImportError: cannot import name 'abs'`\r\n", "comments": ["Can you please try using the latest version of TensorFlow and build again.?\r\nYou can try TF 1.11.", "ok got it thanks", "Were you able to solve your issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 23523, "title": "Using tf.data.dataset for inference with high efficiency", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: master\r\n- Doc Link:  https://www.tensorflow.org/guide/datasets\r\n\r\ntf.data.dataset provide a high level api for data importing and preprocessing. It's convenient to be used in training process, however, it is unclear how to use them in the inference scenario.  We want to reuse the data preprocessing code  for inference which using the interface of tf.data.dataset. By google search and self experiment I found the following ways to use tf.data.dataset for inference:\r\n\r\n```python\r\n#using a placeholder\r\nx = tf.placeholder(tf.float32, shape=[None,2])\r\ndataset = tf.data.Dataset.from_tensor_slices(x)\r\ndata = np.random.sample((100,2))\r\niter = dataset.make_initializable_iterator() # create the iterator\r\nel = iter.get_next()\r\nwith tf.Session() as sess:\r\n    # feed the placeholder with data\r\n    sess.run(iter.initializer, feed_dict={ x: data }) \r\n    print(sess.run(el)) # output [ 0.52374458  0.71968478]\r\n```\r\n\r\nBut the way above will bring some cost, Specifically ,there are two main operations:\r\n1.   we must run the function sess.run  one more time\r\n2.   dataset object need be to initialize each time\r\n\r\n is it efficient to do in this way,  Or we need another way to use tf.data.dataset so that we can keep efficiency?\r\n\r\n\r\n-------------------------------\r\nUpdate:\r\nusing tf.contrib.data.get_single_element  or tf.data.experimental.get_single_element", "comments": ["Regarding your update, does using `tf.data.experimental.get_single_element()` work well for your case?", "> Regarding your update, does using `tf.data.experimental.get_single_element()` work well for your case?\r\n\r\n@mrry  Yes", "Great! In that case, I'll close this issue."]}, {"number": 23522, "title": "Conversion from pb to tflite fails", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: - \r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.11.0-0-gc19e29306c\r\n- Python version: python 3.5\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: only CPU\r\n- GPU model and memory: - \r\n\r\n**Describe the current behavior**\r\nI built a Neural network with the following code:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\nclass AlexNet(object):\r\n    \"\"\"Implementation of the AlexNet.\"\"\"\r\n\r\n    def __init__(self, x, keep_prob, num_classes, skip_layer,\r\n                 weights_path='DEFAULT'):\r\n        \"\"\"Create the graph of the AlexNet model.\r\n        Args:\r\n            x: Placeholder for the input tensor.\r\n            keep_prob: Dropout probability.\r\n            num_classes: Number of classes in the dataset.\r\n            skip_layer: List of names of the layer, that get trained from\r\n                scratch\r\n            weights_path: Complete path to the pretrained weight file, if it\r\n                isn't in the same folder as this code\r\n        \"\"\"\r\n        # Parse input arguments into class variables\r\n        self.X = x\r\n        self.NUM_CLASSES = num_classes\r\n        self.KEEP_PROB = keep_prob\r\n        self.SKIP_LAYER = skip_layer\r\n\r\n        if weights_path == 'DEFAULT':\r\n            self.WEIGHTS_PATH = 'bvlc_alexnet.npy'\r\n        else:\r\n            self.WEIGHTS_PATH = weights_path\r\n\r\n        # Call the create function to build the computational graph of AlexNet\r\n        self.create()\r\n\r\n    def create(self):\r\n        \"\"\"Create the network graph.\"\"\"\r\n        # 1st Layer: Conv (w ReLu) -> Lrn -> Pool\r\n        conv1 = conv(self.X, 11, 11, 96, 4, 4, padding='VALID', name='conv1')\r\n        norm1 = lrn(conv1, 2, 2e-05, 0.75, name='norm1')\r\n        pool1 = max_pool(norm1, 3, 3, 2, 2, padding='VALID', name='pool1')\r\n\r\n        # 2nd Layer: Conv (w ReLu)  -> Lrn -> Pool with 2 groups\r\n        conv2 = conv(pool1, 5, 5, 256, 1, 1, groups=2, name='conv2')\r\n        norm2 = lrn(conv2, 2, 2e-05, 0.75, name='norm2')\r\n        pool2 = max_pool(norm2, 3, 3, 2, 2, padding='VALID', name='pool2')\r\n\r\n        # 3rd Layer: Conv (w ReLu)\r\n        self.conv3 = conv(pool2, 3, 3, 384, 1, 1, name='conv3')\r\n\r\n        # 4th Layer: Conv (w ReLu) splitted into two groups\r\n        conv4 = conv(self.conv3, 3, 3, 384, 1, 1, groups=2, name='conv4')\r\n\r\n        # 5th Layer: Conv (w ReLu) -> Pool splitted into two groups\r\n        conv5 = conv(conv4, 3, 3, 256, 1, 1, groups=2, name='conv5')\r\n        self.pool5 = max_pool(conv5, 3, 3, 2, 2, padding='VALID', name='pool5')\r\n\r\n        # 6th Layer: Flatten -> FC (w ReLu) -> Dropout\r\n        flattened = tf.reshape(self.pool5, [-1, 6*6*256])\r\n        fc6 = fc(flattened, 6*6*256, 4096, name='fc6')\r\n        dropout6 = dropout(fc6, self.KEEP_PROB)\r\n\r\n        # 7th Layer: FC (w ReLu) -> Dropout\r\n        fc7 = fc(dropout6, 4096, 4096, name='fc7')\r\n        dropout7 = dropout(fc7, self.KEEP_PROB)\r\n\r\n        # 8th Layer: FC and return unscaled activations\r\n        self.fc8 = fc(dropout7, 4096, self.NUM_CLASSES, relu=False, name='fc8')\r\n\r\n    def load_initial_weights(self, session):\r\n        \"\"\"Load weights from file into network.\r\n        As the weights from http://www.cs.toronto.edu/~guerzhoy/tf_alexnet/\r\n        come as a dict of lists (e.g. weights['conv1'] is a list) and not as\r\n        dict of dicts (e.g. weights['conv1'] is a dict with keys 'weights' &\r\n        'biases') we need a special load function\r\n        \"\"\"\r\n        # Load the weights into memory\r\n        weights_dict = np.load(self.WEIGHTS_PATH, encoding='bytes').item()\r\n\r\n        # Loop over all layer names stored in the weights dict\r\n        for op_name in weights_dict:\r\n\r\n            # Check if layer should be trained from scratch\r\n            if op_name not in self.SKIP_LAYER:\r\n\r\n                with tf.variable_scope(op_name, reuse=True):\r\n\r\n                    # Assign weights/biases to their corresponding tf variable\r\n                    for data in weights_dict[op_name]:\r\n\r\n                        # Biases\r\n                        if len(data.shape) == 1:\r\n                            var = tf.get_variable('biases', trainable=False)\r\n                            session.run(var.assign(data))\r\n\r\n                        # Weights\r\n                        else:\r\n                            var = tf.get_variable('weights', trainable=False)\r\n                            session.run(var.assign(data))\r\n\r\n\r\ndef conv(x, filter_height, filter_width, num_filters, stride_y, stride_x, name,\r\n         padding='SAME', groups=1):\r\n    \"\"\"Create a convolution layer.\r\n    Adapted from: https://github.com/ethereon/caffe-tensorflow\r\n    \"\"\"\r\n    # Get number of input channels\r\n    input_channels = int(x.get_shape()[-1])\r\n\r\n    # Create lambda function for the convolution\r\n    convolve = lambda i, k: tf.nn.conv2d(i, k,\r\n                                         strides=[1, stride_y, stride_x, 1],\r\n                                         padding=padding)\r\n\r\n    with tf.variable_scope(name) as scope:\r\n        # Create tf variables for the weights and biases of the conv layer\r\n        weights = tf.get_variable('weights', shape=[filter_height,\r\n                                                    filter_width,\r\n                                                    input_channels/groups,\r\n                                                    num_filters])\r\n        biases = tf.get_variable('biases', shape=[num_filters])\r\n\r\n    if groups == 1:\r\n        conv = convolve(x, weights)\r\n\r\n    # In the cases of multiple groups, split inputs & weights and\r\n    else:\r\n        # Split input and weights and convolve them separately\r\n        input_groups = tf.split(axis=3, num_or_size_splits=groups, value=x)\r\n        weight_groups = tf.split(axis=3, num_or_size_splits=groups,\r\n                                 value=weights)\r\n        output_groups = [convolve(i, k) for i, k in zip(input_groups, weight_groups)]\r\n\r\n        # Concat the convolved output together again\r\n        conv = tf.concat(axis=3, values=output_groups)\r\n\r\n    # Add biases\r\n    bias = tf.reshape(tf.nn.bias_add(conv, biases), tf.shape(conv))\r\n\r\n    # Apply relu function\r\n    relu = tf.nn.relu(bias, name=scope.name)\r\n\r\n    return relu\r\n\r\n\r\ndef fc(x, num_in, num_out, name, relu=True):\r\n    \"\"\"Create a fully connected layer.\"\"\"\r\n    with tf.variable_scope(name) as scope:\r\n\r\n        # Create tf variables for the weights and biases\r\n        weights = tf.get_variable('weights', shape=[num_in, num_out],\r\n                                  trainable=True)\r\n        biases = tf.get_variable('biases', [num_out], trainable=True)\r\n\r\n        # Matrix multiply weights and inputs and add bias\r\n        act = tf.nn.xw_plus_b(x, weights, biases, name=scope.name)\r\n\r\n    if relu:\r\n        # Apply ReLu non linearity\r\n        relu = tf.nn.relu(act)\r\n        return relu\r\n    else:\r\n        return act\r\n\r\n\r\ndef max_pool(x, filter_height, filter_width, stride_y, stride_x, name,\r\n             padding='SAME'):\r\n    \"\"\"Create a max pooling layer.\"\"\"\r\n    return tf.nn.max_pool(x, ksize=[1, filter_height, filter_width, 1],\r\n                          strides=[1, stride_y, stride_x, 1],\r\n                          padding=padding, name=name)\r\n\r\n\r\ndef lrn(x, radius, alpha, beta, name, bias=1.0):\r\n    \"\"\"Create a local response normalization layer.\"\"\"\r\n    return tf.nn.local_response_normalization(x, depth_radius=radius,\r\n                                              alpha=alpha, beta=beta,\r\n                                              bias=bias, name=name)\r\n\r\n\r\ndef dropout(x, keep_prob):\r\n    \"\"\"Create a dropout layer.\"\"\"\r\n    return tf.nn.dropout(x, keep_prob)\r\n```\r\nI loaded the weights from a .npy file and saved the resulting pb file. I saved the resulting pb file with this code:\r\nfrom tensorflow.python.tools import freeze_graph\r\n\r\n```\r\nSAVED_MODEL_PATH = \"save_path/\"\r\nMODEL_NAME = \"cut_net_pool5\"\r\n\r\ninput_graph = SAVED_MODEL_PATH + MODEL_NAME + '.pb'\r\n# any other saver to use other than default\r\ninput_saver = \"\"\r\n# earlier definition file format text or binary\r\ninput_binary = True\r\n# checkpoint file to merge with graph definition\r\ninput_checkpoint = SAVED_MODEL_PATH + MODEL_NAME + '.ckpt'\r\n# output nodes inn our model\r\noutput_node_names = 'output'\r\nrestore_op_name = 'save/restore_all'\r\nfilename_tensor_name = 'save/Const:0'\r\n# output path\r\noutput_graph = SAVED_MODEL_PATH + '2frozen_' + MODEL_NAME + '.pb'\r\n# default True\r\nclear_devices = True\r\ninitializer_nodes = \"\"\r\nvariable_names_blacklist = \"\"\r\n\r\nfreeze_graph.freeze_graph(\r\n    input_graph,\r\n    input_saver,\r\n    input_binary,\r\n    input_checkpoint,\r\n    output_node_names,\r\n    restore_op_name,\r\n    filename_tensor_name,\r\n    output_graph,\r\n    clear_devices,\r\n    initializer_nodes,\r\n    variable_names_blacklist\r\n)\r\n```\r\n\r\n\r\n**Describe the expected behavior**\r\nI tried to export the resulting .pb file to tflite via this command in the terminal:\r\n\r\n`tflite_convert --output_file=test.tflite --graph_def_file=frozen.pb --input_arrays=Placeholder --output_arrays=output `\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nThe command fails with the following message:\r\n\r\n`tensorflow/contrib/lite/toco/tooling_util.cc:981] Check failed: name.substr(colon_pos +1).find_first_not_of(\"0123456789\") == string::npos (1 vs. 18446744073709551615)Array name must only have digits after colon\\nAborted (core dumped)\\n`\r\n", "comments": ["This works now with regard to issue #22897 "]}, {"number": 23521, "title": "How to save tensorflow model and the model size is small?", "body": "I met a weird thing. I try to reproduce a project, the author provides network architecture without training codes. So I write the training code to train using Tensorflow. The author also provides the model he trained.\r\n![image](https://user-images.githubusercontent.com/30546375/47996051-35154180-e132-11e8-9129-153c32bed604.png)\r\nHere is my model I saved: \r\n![image](https://user-images.githubusercontent.com/30546375/47996065-3fcfd680-e132-11e8-8fc9-8c6ae6439315.png)\r\nI don't understand, if we use the same architecture, don't we get nearly the same model size? Why I save the model is so different with author provided..... Maybe I also save other parameters except weights.... Can anyone help me?", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 23520, "title": "Install from pip, run test and results in \"illegal instruction\"", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 9 AMD 64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Intel Laptop\r\n- TensorFlow installed from (source or binary): pip per [https://www.tensorflow.org/install/pip](https://www.tensorflow.org/install/pip)\r\n- TensorFlow version: 1.11.0\r\n- Python version: 3.5.3\r\n- Installed using virtualenv? pip? conda?: venv per [https://www.tensorflow.org/install/pip](https://www.tensorflow.org/install/pip) \r\n- Bazel version (if compiling from source):  N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n- set up venv and activate\r\n- pip install tensorflow per [https://www.tensorflow.org/install/pip](https://www.tensorflow.org/install/pip)\r\n- verify install per [https://www.tensorflow.org/install/pip](https://www.tensorflow.org/install/pip)\r\n- import of tensorflow causes \"illegal instruction\" raised.", "comments": ["I suspect that your CPU doesn't support [AVX instructions](https://en.wikipedia.org/wiki/Advanced_Vector_Extensions#CPUs_with_AVX) which is a requirement for installing TensorFlow 1.6 and above.", "Hmm.  I did say AMD64 but that was a furphy.\n\nThe laptop is acutally an x64 being an intel i5 which (apparently) has AVX\naccording to good old wikipedia.\n\nBUT if I look I have a i5 460M chip, which intel tells me has Intel\u00ae\nSSE4.1, Intel\u00ae SSE4.2 extensions.  I am still trying to get my head around\nit but yes, if AVX is separated to SSE4.1 and 4.2 then my playing with\ntensorflow is kaput.\n\nIt the lack of AVX would certainly account for both the docker and pip\ninstall attempts failing.\n\nThanks ymodak, I will delete my install of tensorflow and look at other\nframeworks.\n\n\nOn Tue, Nov 6, 2018 at 8:09 AM ymodak <notifications@github.com> wrote:\n\n> I suspect that your CPU doesn't support AVX instructions\n> <https://en.wikipedia.org/wiki/Advanced_Vector_Extensions#CPUs_with_AVX>\n> which is a requirement for installing TensorFlow 1.6 and above.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/23520#issuecomment-436043912>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABsMK2vMYNEd-jfpaFy1RAvOhQRwRpSLks5usLAMgaJpZM4YOP82>\n> .\n>\n", "In fact, if I run cat /proc/cpuinfo and look at flags there is no avx for\ncertain.\n\nOn Tue, Nov 6, 2018 at 6:01 PM Asterion Daedalus <\norganicmonkeymotion@gmail.com> wrote:\n\n> Hmm.  I did say AMD64 but that was a furphy.\n>\n> The laptop is acutally an x64 being an intel i5 which (apparently) has AVX\n> according to good old wikipedia.\n>\n> BUT if I look I have a i5 460M chip, which intel tells me has Intel\u00ae\n> SSE4.1, Intel\u00ae SSE4.2 extensions.  I am still trying to get my head around\n> it but yes, if AVX is separated to SSE4.1 and 4.2 then my playing with\n> tensorflow is kaput.\n>\n> It the lack of AVX would certainly account for both the docker and pip\n> install attempts failing.\n>\n> Thanks ymodak, I will delete my install of tensorflow and look at other\n> frameworks.\n>\n>\n> On Tue, Nov 6, 2018 at 8:09 AM ymodak <notifications@github.com> wrote:\n>\n>> I suspect that your CPU doesn't support AVX instructions\n>> <https://en.wikipedia.org/wiki/Advanced_Vector_Extensions#CPUs_with_AVX>\n>> which is a requirement for installing TensorFlow 1.6 and above.\n>>\n>> \u2014\n>> You are receiving this because you authored the thread.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/23520#issuecomment-436043912>,\n>> or mute the thread\n>> <https://github.com/notifications/unsubscribe-auth/ABsMK2vMYNEd-jfpaFy1RAvOhQRwRpSLks5usLAMgaJpZM4YOP82>\n>> .\n>>\n>\n", "Tensorflow 1.6 and above __supports__ CPUs without avx. It is the official release binary (the one you download from pip) that does not support CPUs without avx. To use Tensorflow on old CPUs you'll need to build a binary on your own.", "I was running into this as well. I'm using an old Phenom II CPU. Bit annoying there isn't a way to get a version through pip without the AVX requirement. I personally have no direct use for this project, but a program I was trying to test out depends on it. I'll probably just give up on it, as this is more hassle than it's worth, already wasted way too much time this past week leading up to the discovery that this was the problem all along.", "Yep. Was only interested in getting a tensor setup going to start learning\nabout it. Other options out there.\n\nOn Fri, 9 Nov. 2018, 8:09 pm Uradamus <notifications@github.com wrote:\n\n> I was running into this as well. I'm using an old Phenom II CPU. Bit\n> annoying there isn't a way to get a version through pip without the AVX\n> requirement. I personally have no direct use for this project, but a\n> program I was trying to test out depends on it. I'll probably just give up\n> on it, as this is more hassle than it's worth, already wasted way too much\n> time this past week leading up to the discovery that this was the problem\n> all along.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/23520#issuecomment-437302577>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABsMKz3hN9HV5FsVk3VNqpAvttBjCWcsks5utU1egaJpZM4YOP82>\n> .\n>\n"]}, {"number": 23519, "title": "[SOLVED]Kernel restarting when running hello world example within docker image", "body": "**System information**\r\n- OS Platform and Distribution is debian 9 amd64:\r\n- Intel based laptop:\r\n- TensorFlow installed from (source or binary): docker image per [https://www.tensorflow.org/install/](https://www.tensorflow.org/install/ )\r\n- TensorFlow version: latest per docker based instructions\r\n- Python version: whatever docker image pulls down\r\n- Installed using virtualenv? pip? conda?: docker image\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n\r\n**Describe the problem**\r\nFirst computation on 1_hello_tensorflow.ipynb causes kernel restart when it is run.  In fact, it appears any code cell that includes imports tensorflow causes kernel restart.  Straight python code cells, including those using other libraries such as numpy (but have not called tensorflow) run.\r\n", "comments": ["#9829\r\n\r\nPlease refer [this](https://stackoverflow.com/questions/50011676/python-kernel-dies-when-importing-tensorflow-1-7)", "Ignore @harshini-gadige who has not applied any critical thinking enquiry at all.  Why on earth we would want Microsoft style user help on any forum is beyond me ;)\r\n\r\nWith a little work, I discovered the version of tensorflow required a cpu instruction my poor old laptop does not have.  I was not running a gpu as I had not installed cuda toolkit (it isn't really tuned for Debian).  So the article @harshini-gadige pointed at was as unfortunately as relevant to the problem as the fact that peanuts are used in making dynamite LOL\r\n\r\nI think then I get twice the kudos points, I get the ones @harshini-gadige would have got for erroneously closing the issue AND points for actually closing the issue.\r\n\r\nThe time limit on responses is otherwise a shoddy IT department's trick for claiming performance where there is none.  ", "> Ignore @Harshini-Gadige who has not applied any critical thinking enquiry at all. Why on earth we would want Microsoft style user help on any forum is beyond me ;)\r\n> \r\n> With a little work, I discovered the version of tensorflow required a cpu instruction my poor old laptop does not have. I was not running a gpu as I had not installed cuda toolkit (it isn't really tuned for Debian). So the article @Harshini-Gadige pointed at was as unfortunately as relevant to the problem as the fact that peanuts are used in making dynamite LOL\r\n> \r\n> I think then I get twice the kudos points, I get the ones @Harshini-Gadige would have got for erroneously closing the issue AND points for actually closing the issue.\r\n> \r\n> The time limit on responses is otherwise a shoddy IT department's trick for claiming performance where there is none.\r\n\r\nCan you please be more precise, how did you solve the issue? "]}, {"number": 23518, "title": "Change `int` to `uint` or `size_t` where applicable", "body": "Change `int` to `uint` or `size_t` where applicable. \r\nBased on compiler warnings & common sense. \r\n\r\nNote: there is **a lot** more to be done on this subject. ", "comments": ["@skye, any news?", "Hi sorry missed this somehow, thanks for the ping. I'm running tests to make sure this doesn't cause any errors with our prechecks.", "@olicht please take a look at the build errors.", "@olicht  Please rebase to resolve the branch conflicts.", "closing this PR due to lack of activity."]}, {"number": 23517, "title": "Update year to 2018", "body": "Update year to 2018 on some files", "comments": []}, {"number": 23516, "title": "tensorflow conv2d NCHW with mkl slower than NHWC without mkl  on cpu platform", "body": "I tested tensorflow conv2d with NCHW with mkl slower than NHWC wihout mkl on cpu platform.\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Y\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N\r\n- TensorFlow installed from (source or binary):binary\r\ntf-mkl:\r\npip install https://storage.googleapis.com/intel-optimized-tensorflow/tensorflow-1.11.0-cp27-cp27mu-linux_x86_64.whl \r\ntf-no-mkl:\r\nwget https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.11.0-cp27-none-linux_x86_64.whl\r\npip install tensorflow-1.11.0-cp27-none-linux_x86_64.whl\r\n\r\n- TensorFlow version (use command below):1.111.0\r\n- Python version:2.7.5\r\n- Bazel version (if compiling from source):N\r\n- GCC/Compiler version (if compiling from source):N\r\n- CUDA/cuDNN version:N\r\n- GPU model and memory:N\r\n- CPU: Intel(R) Xeon(R) CPU E5-2682 v4 @ 2.50GHz\r\nThread(s) per core:    2\r\nCore(s) per socket:    16\r\nSocket(s):             2\r\n\r\n**Describe the current behavior**\r\ntensorflow conv2d with NCHW with mkl slower than NHWC wihout mkl on cpu platform.\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\nimport os\r\nimport sys\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport time\r\n\r\n\"\"\"\r\ndefault NWC\r\ninput=[batch, in_width, in_channels]  filter=[ filter_width, in_channels, out_channels]\r\n=>tf.nn.conv2d  [batch,1,in_width, in_channels]  [1, filter_width, in_channels, out_channels]\r\n\r\nmkl NCW\r\ninput=[batch, in_channels, in_width] filter=[filter_width, in_channels, out_channels]\r\n=>tf.nn.conv2d []\r\n\"\"\"\r\n\r\nbatch = 32\r\nin_width = 102400\r\nin_height = 1\r\nin_channels = 128\r\nfilter_width = 3\r\nfilter_height = 1\r\nout_channels = 128\r\ndata_format_2d = sys.argv[2]\r\nn = 100\r\nstart = 99\r\ninput_2d = {}\r\n\r\nif len(sys.argv) > 1 and sys.argv[1] == \"ow\":\r\n    print(\"ow=1\")\r\n    in_width,in_height = in_height, in_width\r\n    filter_width,filter_height = filter_height, filter_width\r\nelse:\r\n    print(\"oh=1\")\r\nif data_format_2d == \"NCHW\":\r\n    input_ = np.array(np.arange(1, 1 + batch*in_width*in_height*in_channels).reshape([batch, in_channels, in_height,in_width]), dtype=np.float32)\r\nelse:\r\n    input_ = np.array(np.arange(1, 1 + batch*in_width*in_height*in_channels).reshape([batch, in_height,in_width,in_channels]), dtype=np.float32)\r\nfor i in range(start,n):\r\n    input_2d[i] = input_ + i*n*i*1.0\r\nkernel_2d = np.array(np.arange(1, 1 + filter_width*filter_height*in_channels*out_channels), dtype=np.float32).reshape([filter_height, filter_width, in_channels, out_channels])\r\na = tf.placeholder(dtype=tf.float32)\r\nconv2d = tf.nn.conv2d(a, kernel_2d, [1,1,1,1], 'VALID', data_format=data_format_2d)\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.intra_op_parallelism_threads = 32\r\nconfig.inter_op_parallelism_threads = 2\r\nsess = tf.Session(config=config)\r\n\r\nprint(\"start\")\r\nsess.run(tf.global_variables_initializer())\r\nstart_ts = time.time()\r\nfor i in range(start,n):\r\n    sess.run(conv2d, feed_dict={a:input_2d[i]})\r\nprint(\"data_format_2d=%s %d epoch cost %f\" % (data_format_2d,n-start, (time.time()) - start_ts))\r\nprint(\"shape:\")\r\nprint(np.shape(input_2d[start]))\r\n```\r\ncmd to exec\r\n```\r\nMKLDNN_VERBOSE=0 KMP_BLOCKTIME=0 CUDA_VISIBLE_DEVICES=-1  OMP_NUM_THREADS=32 python conv2d.py  oh NHWC\r\n```\r\nor\r\n```\r\nMKLDNN_VERBOSE=0 KMP_BLOCKTIME=0 CUDA_VISIBLE_DEVICES=-1  OMP_NUM_THREADS=32 python conv2d.py  oh NCHW\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\ntf-1.11.0 with mkl\r\n```\r\nMKLDNN_VERBOSE=1 KMP_BLOCKTIME=0 CUDA_VISIBLE_DEVICES=-1  OMP_NUM_THREADS=32 python conv2d.py  ow NCHW\r\n```\r\noutput:\r\n```\r\now=1\r\n2018-11-05 17:17:06.151312: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nstart\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_nchw out:f32_nChw8c,num:1,32x128x102400x1,83.739\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_hwio out:f32_OIhw8i8o,num:1,128x128x3x1,0.157959\r\nmkldnn_verbose,exec,convolution,jit:avx2,forward_training,fsrc:nChw8c fwei:OIhw8i8o fbia:undef fdst:nChw8c,alg:convolution_direct,mb32_g1ic128oc128_ih102400oh102398kh3sh1dh0ph0_iw1ow1kw1sw1dw0pw0,1010.23\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_nChw8c out:f32_nchw,num:1,32x128x102398x1,181.471\r\ndata_format_2d=NCHW 1 epoch cost 2.135184\r\nshape:\r\n(32, 128, 102400, 1)\r\n```\r\ntf-1.11.0 without mkl\r\n```\r\nMKLDNN_VERBOSE=0 KMP_BLOCKTIME=0 CUDA_VISIBLE_DEVICES=-1  OMP_NUM_THREADS=32 python conv2d.py  oh NHWC\r\n```\r\noutput\r\n```\r\noh=1\r\n2018-11-05 17:16:30.370120: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nstart\r\ndata_format_2d=NHWC 1 epoch cost 1.402521\r\nshape:\r\n(32, 1, 102400, 128)\r\n```\r\n", "comments": ["Hi,\r\nProbably the value in your input_ is too large.\r\nI modified your code to fill in input_ with random numbers in range (0, 1) with holding the same shape, and mkl version shows a little better performance than non-mkl one. Please check the following code snippet.\r\n```import os\r\nimport sys\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport time\r\n\r\n\"\"\"\r\ndefault NWC\r\ninput=[batch, in_width, in_channels]  filter=[ filter_width, in_channels, out_channels]\r\n=>tf.nn.conv2d  [batch,1,in_width, in_channels]  [1, filter_width, in_channels, out_channels]\r\n\r\nmkl NCW\r\ninput=[batch, in_channels, in_width] filter=[filter_width, in_channels, out_channels]\r\n=>tf.nn.conv2d []\r\n\"\"\"\r\n\r\nbatch = 32\r\nin_width = 102400\r\nin_height = 1\r\nin_channels = 128\r\nfilter_width = 3\r\nfilter_height = 1\r\nout_channels = 128\r\ndata_format_2d = sys.argv[2]\r\nn = 10\r\n\r\nif len(sys.argv) > 1 and sys.argv[1] == \"ow\":\r\n    print(\"ow=1\")\r\n    in_width,in_height = in_height, in_width\r\n    filter_width,filter_height = filter_height, filter_width\r\nelse:\r\n    print(\"oh=1\")\r\nif data_format_2d == \"NCHW\":\r\n    input_ = np.random.rand(batch, in_channels, in_height,in_width)\r\nelse:\r\n    input_ = np.random.rand(batch, in_height,in_width,in_channels)\r\n# print(input_)\r\nkernel_2d = np.random.rand(filter_height, filter_width, in_channels, out_channels)\r\na = tf.placeholder(dtype=tf.float32)\r\nconv2d = tf.nn.conv2d(a, kernel_2d, [1,1,1,1], 'VALID', data_format=data_format_2d)\r\n\r\nconfig = tf.ConfigProto()\r\n# config.intra_op_parallelism_threads = 32\r\n# config.inter_op_parallelism_threads = 2\r\nsess = tf.Session(config=config)\r\n\r\nprint(\"start\")\r\nsess.run(tf.global_variables_initializer())\r\nfor i in range(0, 10):\r\n    # print(\"Warm-up: %d\" % (i))\r\n    sess.run(conv2d, feed_dict={a:input_})\r\nstart_ts = time.time()\r\nfor i in range(0, n):\r\n    # print(\"Test: %d\" % (i))\r\n    sess.run(conv2d, feed_dict={a:input_})\r\nprint(\"data_format_2d=%s %d epoch cost %f\" % (data_format_2d,n, (time.time()) - start_ts))\r\nprint(\"shape:\")\r\nprint(np.shape(input_))\r\n```\r\n\r\nFollowing is the command and performance of running with non-MKL:\r\n```\r\n# MKLDNN_VERBOSE=0 KMP_BLOCKTIME=0 CUDA_VISIBLE_DEVICES=-1  OMP_NUM_THREADS=32 python conv2d_3.py  oh NHWC\r\n\r\n2018-11-26 22:25:41.560019: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\noh=1\r\nstart\r\ndata_format_2d=NHWC 10 epoch cost 37.556874\r\nshape:\r\n(32, 1, 102400, 128)\r\n```\r\n\r\nFollowing is the command and performance of running with MKL:\r\n```\r\n# MKLDNN_VERBOSE=0 KMP_BLOCKTIME=0 CUDA_VISIBLE_DEVICES=-1  OMP_NUM_THREADS=32 python conv2d_3.py  ow NCHW\r\n\r\n2018-11-26 22:27:55.706737: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-11-26 22:27:55.718858: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\now=1\r\nstart\r\ndata_format_2d=NCHW 10 epoch cost 32.204718\r\nshape:\r\n(32, 128, 102400, 1)\r\n```\r\n\r\n- CPU:\r\nIntel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz\r\nThread(s) per core: 2\r\nCore(s) per socket: 16\r\nSocket(s): 2", "closing due to inactivity. "]}, {"number": 23515, "title": "[Eager] Error when training LinearRegressor estimator (Tensor.op)", "body": "**System information**\r\n- Have I written custom code: **Yes**.\r\n- OS Platform and Distribution: **Ubuntu 18.04.1 LTS (Bionic Beaver)**.\r\n- TensorFlow installed from: **binary**.\r\n- TensorFlow version: **1.11.0**.\r\n- Python version: **3.6.6**.\r\n- CUDA/cuDNN version: **N/A. CPU only**.\r\n- GPU model and memory: **N/A. CPU only**.\r\n- Other: **Jupyter notebook, 4.4.0**\r\n\r\n**Describe the current behavior**\r\n\r\nIn Eager mode, training a LinearRegressor Estimator calling `model.train(..)` fails with the following error:\r\n\r\n```text\r\nTensor.op is meaningless when eager execution is enabled.\r\n``` \r\n\r\n**Describe the expected behavior**\r\n\r\n`model.train(..)` returns without exceptions.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\n# Uncomment the following line after running the script in Graph mode\r\n# tf.enable_eager_execution()\r\n\r\nfeature_names = ['a', 'b', 'c']\r\nfeature_columns = [tf.feature_column.numeric_column(key=name) for name in feature_names]\r\n\r\ntrue_w = [[-2.0], [4.0], [1.0]]\r\ntrue_b = [0.5]\r\nnoise_level = 0.1\r\n\r\nbatch_size = 32\r\nnum_batches = 1000\r\nsteps = 100\r\n\r\ndef synthetic_dataset(w, b, noise_level, batch_size, num_batches):\r\n    return synthetic_dataset_helper(w, b,\r\n                                    tf.shape(w)[0], noise_level, batch_size,\r\n                                    num_batches)\r\n\r\n\r\ndef synthetic_dataset_helper(w, b, num_features, noise_level, batch_size,\r\n                             num_batches):\r\n    def batch(_):\r\n        x = tf.random_normal([batch_size, num_features])\r\n        y = tf.matmul(x, w) + b + noise_level * tf.random_normal([])\r\n        x_dict = {}\r\n        for idx, name in enumerate(feature_names):\r\n            x_dict[name] = x[:, idx]\r\n        return x_dict, y\r\n\r\n    return tf.data.Dataset.range(num_batches).map(batch)\r\n\r\ndataset = synthetic_dataset(true_w, true_b, noise_level, batch_size, num_batches)\r\n\r\ndef input_train():\r\n    return (dataset.make_one_shot_iterator().get_next())\r\n\r\nmodel = tf.estimator.LinearRegressor(feature_columns=feature_columns)\r\nmodel.train(input_fn=input_train, steps=steps)\r\n```\r\n\r\nThe code is similar to the following official examples: [linear_regression.py](https://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/examples/get_started/regression/linear_regression.py) and [linear_regression.py (eager)](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/linear_regression/linear_regression.py).\r\n", "comments": ["Is this still an issue for you? I was not able to execute your script by disabling eager execution. Can you please confirm this?", "Yes @ymodak. Same error. But you were right in that the previous script did not work after disabling eager execution. I edited the post with a modified version of the script that works in graph mode but not in eager mode.", "The issue here is that, in Estimator, when creating a Dataset in the input_fn, you need to create the entire Dataset in the input_fn, not just the iterator. So if you replace\r\n\r\n```\r\ndef input_train():\r\n    return (dataset.make_one_shot_iterator().get_next())\r\n```\r\n\r\nwith\r\n\r\n```\r\ndef input_train():\r\n    dataset = synthetic_dataset(true_w, true_b, noise_level, batch_size, num_batches)\r\n    return (dataset.make_one_shot_iterator().get_next())\r\n```\r\nyour code just works."]}, {"number": 23514, "title": "poor performance on conv2d with ow=1", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:no\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):'v1.11.0-0-gc19e293', '1.11.0'\r\n- Python version:2.7.5\r\n- Bazel version (if compiling from source):no\r\n- GCC/Compiler version (if compiling from source):no\r\n- CUDA/cuDNN version:no\r\n- GPU model and memory:no\r\n\r\n\r\nI try to optimize the voice synthesize model, with a lot of conv1d invoke fllowing the steps of https://www.tensorflow.org/guide/performance/overview#optimizing_for_cpu, I found the results on tensorflow v1.11.0 with mkl version is slower 50% than the same tensorflow without mkl.\r\n\r\nFirst of all, based on https://github.com/intel/mkl-dnn/issues/285,  I test conv2d NCHW with respectively oh=1 or ow=1. I found ow=1 slower slower than oh=1.\r\n\r\nSo I doubt how to optimize cpu performance on conv1d ???\r\n\r\n\r\n-----------------------------------------------------------------------------\r\n\r\n### Environment\r\nIntel MKL-DNN includes hardware-specific optimizations and may behave\r\ndifferently on depending on the compiler and build environment. Include\r\nthe following information to help reproduce the issue:\r\n* Intel(R) Xeon(R) CPU E5-2682 v4 @ 2.50GHz\r\n* company internal version based on centos\r\n* 4.9.2\r\n* empty value\r\n* no cmake\r\n* tensorflow 1.11\r\n\r\n### Steps to reproduce\r\n\r\n```\r\nisntall fllowing https://software.intel.com/en-us/articles/intel-optimization-for-tensorflow-installation-guide\r\n# Python 2.7\r\npip install https://storage.googleapis.com/intel-optimized-tensorflow/tensorflow-1.11.0-cp27-cp27mu-linux_x86_64.whl \r\n\r\n```\r\now =1  cmd\r\n\r\n```\r\nMKLDNN_VERBOSE=1 KMP_BLOCKTIME=0 CUDA_VISIBLE_DEVICES=-1  OMP_NUM_THREADS=32 python conv2d.py  ow\r\n\r\now=1\r\n2018-11-05 15:42:35.614687: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nstart\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_nchw out:f32_nChw8c,num:1,32x128x102400x1,85.6531\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_hwio out:f32_OIhw8i8o,num:1,128x128x3x1,0.134766\r\nmkldnn_verbose,exec,convolution,jit:avx2,forward_training,fsrc:nChw8c fwei:OIhw8i8o fbia:undef fdst:nChw8c,alg:convolution_direct,mb32_g1ic128oc128_ih102400oh102398kh3sh1dh0ph0_iw1ow1kw1sw1dw0pw0,991.858\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_nChw8c out:f32_nchw,num:1,32x128x102398x1,174.348\r\ndata_format_2d=NCHW 100 epoch cost 2.131751\r\n```\r\noh =1  cmd\r\n\r\n```\r\nMKLDNN_VERBOSE=1 KMP_BLOCKTIME=0 CUDA_VISIBLE_DEVICES=-1  OMP_NUM_THREADS=32 python conv2d.py  oh\r\noh=1\r\n2018-11-05 15:42:05.224622: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nstart\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_nchw out:f32_nChw8c,num:1,32x128x1x102400,91.1848\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_hwio out:f32_OIhw8i8o,num:1,128x128x1x3,0.152832\r\nmkldnn_verbose,exec,convolution,jit:avx2,forward_training,fsrc:nChw8c fwei:OIhw8i8o fbia:undef fdst:nChw8c,alg:convolution_direct,mb32_g1ic128oc128_ih1oh1kh1sh1dh0ph0_iw102400ow102398kw3sw1dw0pw0,835.947\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_nChw8c out:f32_nchw,num:1,32x128x1x102398,199.113\r\ndata_format_2d=NCHW 100 epoch cost 1.990710\r\n\r\n```\r\n\r\n### the code shows\r\n\r\n```\r\nimport os\r\nimport sys\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport time\r\n\r\n\"\"\"\r\ndefault NWC\r\ninput=[batch, in_width, in_channels]  filter=[ filter_width, in_channels, out_channels]\r\n=>tf.nn.conv2d  [batch,1,in_width, in_channels]  [1, filter_width, in_channels, out_channels]\r\n\r\nmkl NCW\r\ninput=[batch, in_channels, in_width] filter=[filter_width, in_channels, out_channels]\r\n=>tf.nn.conv2d []\r\n\"\"\"\r\n\r\nbatch = 32\r\nin_width = 102400\r\nin_height = 1\r\nin_channels = 128\r\nfilter_width = 3\r\nfilter_height = 1\r\nout_channels = 128\r\ndata_format_2d = 'NCHW'\r\nn = 100\r\nstart = 99\r\ninput_2d = {}\r\n\r\nif len(sys.argv) == 2 and sys.argv[1] == \"ow\":\r\n    print(\"ow=1\")\r\n    in_width,in_height = in_height, in_width\r\n    filter_width,filter_height = filter_height, filter_width\r\nelse:\r\n    print(\"oh=1\")\r\nif data_format_2d == \"NCHW\":\r\n    input_ = np.array(np.arange(1, 1 + batch*in_width*in_height*in_channels).reshape([batch, in_channels, in_height,in_width]), dtype=np.float32)\r\nelse:\r\n    input_ = np.array(np.arange(1, 1 + batch*in_width*in_height*in_channels).reshape([batch, in_height,in_width,in_channels]), dtype=np.float32)\r\nfor i in range(start,n):\r\n    input_2d[i] = input_ + i*n*i*1.0\r\nkernel_2d = np.array(np.arange(1, 1 + filter_width*filter_height*in_channels*out_channels), dtype=np.float32).reshape([filter_height, filter_width, in_channels, out_channels])\r\na = tf.placeholder(dtype=tf.float32)\r\nconv2d = tf.nn.conv2d(a, kernel_2d, [1,1,1,1], 'VALID', data_format=data_format_2d)\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.intra_op_parallelism_threads = 32\r\nconfig.inter_op_parallelism_threads = 2\r\nsess = tf.Session(config=config)\r\n\r\nprint(\"start\")\r\nsess.run(tf.global_variables_initializer())\r\nstart_ts = time.time()\r\nfor i in range(start,n):\r\n    sess.run(conv2d, feed_dict={a:input_2d[i]})\r\nprint(\"data_format_2d=%s %d epoch cost %f\" % (data_format_2d,n, (time.time()) - start_ts))\r\n```\r\n\r\n", "comments": ["Hi,\r\nRegarding to the performance gap of Intel TensorFlow, please check my reply in https://github.com/intel/mkl-dnn/issues/285.\r\n\r\nSince the calculation is 1D convolution, why not trying conv1d like the following snippet?\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport time\r\n\r\n\"\"\"\r\ndefault NWC\r\ninput=[batch, in_width, in_channels]  filter=[ filter_width, in_channels, out_channels]\r\n=>tf.nn.conv2d  [batch,1,in_width, in_channels]  [1, filter_width, in_channels, out_channels]\r\n\r\nmkl NCW\r\ninput=[batch, in_channels, in_width] filter=[filter_width, in_channels, out_channels]\r\n=>tf.nn.conv2d []\r\n\"\"\"\r\n\r\nbatch = 32\r\nin_width = 102400\r\nin_channels = 128\r\nfilter_width = 3\r\nout_channels = 128\r\ndata_format= \"NCW\"\r\nn = 10\r\n\r\nif data_format == \"NCW\":\r\n    input_ = np.random.rand(batch, in_channels, in_width)\r\n# print(input_)\r\nkernel_1d = np.random.rand(filter_width, in_channels, out_channels).astype(np.float32)\r\na = tf.placeholder(dtype=tf.float32)\r\nconv1d = tf.nn.conv1d(a, kernel_1d, 1, 'VALID', data_format=data_format)\r\n\r\nconfig = tf.ConfigProto()\r\n# config.intra_op_parallelism_threads = 32\r\n# config.inter_op_parallelism_threads = 2\r\nsess = tf.Session(config=config)\r\n\r\nprint(\"start\")\r\nsess.run(tf.global_variables_initializer())\r\nfor i in range(0, 10):\r\n    # print(\"Warm-up: %d\" % (i))\r\n    sess.run(conv1d, feed_dict={a:input_})\r\nstart_ts = time.time()\r\nfor i in range(0, n):\r\n    # print(\"Test: %d\" % (i))\r\n    sess.run(conv1d, feed_dict={a:input_})\r\nprint(\"data_format_1d=%s %d epoch cost %f\" % (data_format, n, (time.time()) - start_ts))\r\nprint(\"shape:\")\r\nprint(np.shape(input_))\r\n```", "closing due to inactivity. "]}, {"number": 23513, "title": "[TF1.7][CUDA9.0] TF compilation from source with GPU failed, something goes wrongly with protobuf_archive", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NO\r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version: r1.7\r\n- Python version: Python 2.7\r\n- Installed using virtualenv? pip? conda?: Source \r\n- Bazel version (if compiling from source): 0.10.1\r\n- GCC/Compiler version (if compiling from source): gcc-5.3\r\n- CUDA/cuDNN version: 9.0 / 7.0.5\r\n- GPU model and memory: TitanXP 12GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nThe CPU back-end TF compilation is successfully done, then I tried to compile the Tensorflow from source with GPU back-end support.\r\nAfter following the tutorial [](https://www.tensorflow.org/install/source), the compilation is proceeded to about: [23xx/6xxx], then an error popped up which is as follow:\r\n```\r\n.\r\n.\r\n.\r\ntensorflow/contrib/lite/arena_planner.cc: In member function 'TfLiteStatus tflite::ArenaPlanner::CalculateAllocationOfInternalTensors(int)':\r\ntensorflow/contrib/lite/arena_planner.cc:232:18: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   if (node_index < graph_info_->num_nodes()) {\r\n                  ^\r\ntensorflow/contrib/lite/arena_planner.cc: In member function 'TfLiteStatus tflite::ArenaPlanner::CalculateDeallocationOfInternalTensors(int)':\r\ntensorflow/contrib/lite/arena_planner.cc:245:18: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   if (node_index < graph_info_->num_nodes()) {\r\n                  ^\r\nINFO: From Compiling external/snappy/snappy-stubs-internal.cc:\r\ncc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++\r\ncc1plus: warning: unrecognized command line option '-Wno-shift-negative-value'\r\nINFO: From Compiling external/snappy/snappy-stubs-internal.cc [for host]:\r\ncc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++\r\ncc1plus: warning: unrecognized command line option '-Wno-shift-negative-value'\r\nINFO: From Compiling external/snappy/snappy.cc:\r\ncc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++\r\ncc1plus: warning: unrecognized command line option '-Wno-shift-negative-value'\r\nINFO: From Compiling external/snappy/snappy.cc [for host]:\r\ncc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++\r\ncc1plus: warning: unrecognized command line option '-Wno-shift-negative-value'\r\nERROR: /home/web_server/.cache/bazel/_bazel_web_server/9c777c8c1d70f40d5fbbacb4888a5481/external/protobuf_archive/BUILD:259:1: Linking of rule '@protobuf_archive//:js_embed' failed (Exit 1)\r\n/usr/bin/ld: bazel-out/host/bin/external/protobuf_archive/_objs/js_embed/external/protobuf_archive/src/google/protobuf/compiler/js/embed.o: unrecognized relocation (0x2a) in section `.text._ZL7AddFilePKcPSo.constprop.25'\r\n/usr/bin/ld: final link failed: Bad value\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 72.355s, Critical Path: 26.30s\r\nFAILED: Build did NOT complete successfully\r\n```\r\nI guess this shall be an known issue and something relevant to a configuration problem, however, I didn't find anything helpful.\r\n\r\n-------\r\n```\r\nBelow is my ./configuration:\r\n$ ./configure \r\nWARNING: Running Bazel server needs to be killed, because the startup options are different.\r\nYou have bazel 0.10.1 installed.\r\nDo you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: n\r\nNo jemalloc as malloc support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: n\r\nNo Google Cloud Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Hadoop File System support? [Y/n]: n\r\nNo Hadoop File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: n\r\nNo Amazon S3 File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Apache Kafka Platform support? [y/N]: n\r\nNo Apache Kafka Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: n\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with GDR support? [y/N]: n\r\nNo GDR support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with VERBS support? [y/N]: n\r\nNo VERBS support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 9.0]: \r\n\r\n\r\nPlease specify the location where CUDA 9.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /home/web_server/xiaolun/cuda-9.0\r\n\r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7.0]: \r\n\r\n\r\nPlease specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /home/web_server/xiaolun/cuda-9.0]:\r\n\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: n\r\nNo TensorRT support will be enabled for TensorFlow.\r\n\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size. [Default is: 6.1]\r\n\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]: n\r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /bin/gcc]: /home/web_server/gcc-5.3\r\n\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: n\r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See tools/bazel.rc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\nConfiguration finished\r\n\r\n```\r\n\r\n------------\r\n\r\nI reset the environment variables and recompile it. This time the error is as below:\r\n```\r\nINFO: From Compiling tensorflow/contrib/lite/arena_planner.cc:\r\ntensorflow/contrib/lite/arena_planner.cc: In member function 'virtual TfLiteStatus tflite::ArenaPlanner::PlanAllocations()':\r\ntensorflow/contrib/lite/arena_planner.cc:82:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = 0; i < graph_info_->num_nodes(); ++i) {\r\n                     ^\r\ntensorflow/contrib/lite/arena_planner.cc:101:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = 0; i < graph_info_->num_nodes(); ++i) {\r\n                     ^\r\ntensorflow/contrib/lite/arena_planner.cc: In member function 'virtual TfLiteStatus tflite::ArenaPlanner::ExecuteAllocations(int, int)':\r\ntensorflow/contrib/lite/arena_planner.cc:139:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = 0; i < graph_info_->num_tensors(); ++i) {\r\n                     ^\r\ntensorflow/contrib/lite/arena_planner.cc: In member function 'TfLiteStatus tflite::ArenaPlanner::CalculateAllocationOfInternalTensors(int)':\r\ntensorflow/contrib/lite/arena_planner.cc:232:18: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   if (node_index < graph_info_->num_nodes()) {\r\n                  ^\r\ntensorflow/contrib/lite/arena_planner.cc: In member function 'TfLiteStatus tflite::ArenaPlanner::CalculateDeallocationOfInternalTensors(int)':\r\ntensorflow/contrib/lite/arena_planner.cc:245:18: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   if (node_index < graph_info_->num_nodes()) {\r\n                  ^\r\nERROR: /home/web_server/.cache/bazel/_bazel_web_server/924cb0e3b475d8bc0bc364784129b908/external/protobuf_archive/BUILD:259:1: Linking of rule '@protobuf_archive//:js_embed' failed (Exit 1)\r\n/usr/bin/ld: bazel-out/host/bin/external/protobuf_archive/_objs/js_embed/external/protobuf_archive/src/google/protobuf/compiler/js/embed.o: unrecognized relocation (0x2a) in section `.text._ZL7AddFilePKcPSo.constprop.25'\r\n/usr/bin/ld: final link failed: Bad value\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 90.569s, Critical Path: 47.78s\r\nFAILED: Build did NOT complete successfully\r\n\r\n```\r\n\r\nAny idea will be welcome.\r\n", "comments": ["Can you use the latest version of TensorFlow and build again if possible?", "Hi,\r\nI solved this problem by changing gcc version from 5.3 to 4.9.3.\r\nNow the compilation is successfully done.\r\n\r\nThanks."]}, {"number": 23512, "title": "there is no gen_image_ops.py file", "body": "from tensorflow.python.ops import gen_image_ops\r\nI want to see the code about non_max_suppression,but it in gen_image_ops,but I don't find it ,so where is the file?", "comments": ["You can find the non_max_suppression code [here : tensorflow/python/ops/image_ops_impl.py](https://github.com/tensorflow/tensorflow/blob/f43d458a318d4d97298710654f1692f6e8364f82/tensorflow/python/ops/image_ops_impl.py#L2067)", "So, the `gen_*` files used in python are essentially the bridge between the C++ definition and the tf code invoked and interpreted from python land. \r\n\r\nI am fairly sure that you would end up generating those files if you did a full build of tensorflow. They are I believe, generated at build time and auto-generated. \r\n\r\nHere are some pointers that might be helpful:\r\nC++ op registration: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/image_ops.cc#L714\r\n\r\nC++ op definition:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/non_max_suppression_op.cc#L282\r\n\r\nC++ op compute base function (probably what you want):\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/non_max_suppression_op.cc#L127\r\n", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 23511, "title": "tf.GradientTape() not support to MaxPool3D", "body": "**System information**\r\n- Have I written custom code: **yes**\r\n- OS Platform and Distribution: **Linux Ubuntu 16.04**\r\n- TensorFlow installed from (source or binary): **binary, pip3 install**\r\n- TensorFlow version (use command below): **1.10**\r\n- Python version: **3.5**\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: **CUDA Version 9.1.85**\r\n- GPU model and memory: **TITAN V, 12066MB**\r\n\r\n**Describe the current behavior**\r\nI'm trying to create a simple cnn3d network, which contains a convolutional layer in 3d, a max pooling in 3d and a fully connected layer, all this using eager mode.\r\nWhen trying to calculate the gradients with an optimizer I get this error:\r\n`TypeError: 'NoneType' object has no attribute '__getitem__'`\r\nThen, when trying to remove the MaxPool3D layer or replace with AveragePooling3D, the gradient calculation works without problems.\r\n\r\n**Describe the expected behavior**\r\nCalculate the gradients using the eager mode with tf.GradientTape() in a network with a Conv3D layer and a MaxPool3D layer.\r\n\r\n**Code to reproduce the issue**\r\n```python3\r\nfrom __future__ import absolute_import, division, print_function\r\n\r\nimport tensorflow as tf\r\n\r\n# enable eager mode\r\ntf.enable_eager_execution()\r\ntf.set_random_seed(0)\r\nnp.random.seed(0)\r\n\r\n\r\nx = tf.random_uniform((10,5,10,10,3))\r\ny = tf.random_uniform((10, 5))\r\n\r\n\r\nclass MyModel(tf.keras.Model):\r\n  def __init__(self):\r\n    super(MyModel, self).__init__()\r\n    self.conv3d_1 = tf.keras.layers.Conv3D(filters=6,\r\n                                           kernel_size=(3,3,3))\r\n    self.max_pool_1 = tf.keras.layers.MaxPool3D(pool_size=(2, 2, 2))\r\n    self.flatten = tf.keras.layers.Flatten()\r\n    self.dense_1 = tf.layers.Dense(5)\r\n\r\n  def call(self, input, training=False):\r\n    \"\"\"Run the model.\"\"\"\r\n    model = self.conv3d_1(input)\r\n    model = self.max_pool_1(model)\r\n    model = self.flatten(model)\r\n    model = self.dense_1(model)\r\n    \r\n    return model\r\n\r\nmodel = MyModel()\r\n\r\ndef loss(model, x, y):\r\n  logits = model(x)\r\n  return tf.losses.softmax_cross_entropy(onehot_labels=y, logits=logits), logits\r\n\r\ndef grad(model, inputs, targets):\r\n  with tf.GradientTape() as tape:\r\n    loss_value, logits = loss(model, inputs, targets)\r\n  return loss_value, logits, tape.gradient(loss_value, model.trainable_variables)\r\n\r\n# Optimize the model\r\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\r\nglobal_step = tf.train.get_or_create_global_step()\r\nloss_value, logits, grads = grad(model, x, y)\r\noptimizer.apply_gradients(zip(grads, model.variables), global_step)\r\n```\r\n\r\n**Other info / logs**\r\nThe traceback error:\r\n\r\n```python3\r\nTypeErrorTraceback (most recent call last)\r\n<ipython-input-4-56cdb9bbe4e5> in <module>()\r\n     44 # Optimize the model\r\n     45 optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\r\n---> 46 loss_value, logits, grads = grad(model, x, y)\r\n     47 optimizer.apply_gradients(zip(grads, model.variables), global_step)\r\n\r\n<ipython-input-4-56cdb9bbe4e5> in grad(model, inputs, targets)\r\n     40   with tf.GradientTape() as tape:\r\n     41     loss_value, logits = loss(model, inputs, targets)\r\n---> 42   return loss_value, logits, tape.gradient(loss_value, model.trainable_variables)\r\n     43 \r\n     44 # Optimize the model\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/eager/backprop.pyc in gradient(self, target, sources, output_gradients)\r\n    899         nest.flatten(target),\r\n    900         flat_sources,\r\n--> 901         output_gradients=output_gradients)\r\n    902 \r\n    903     if not self._persistent:\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/eager/imperative_grad.pyc in imperative_grad(tape, target, sources, output_gradients)\r\n     62       target,\r\n     63       sources,\r\n---> 64       output_gradients)\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/eager/backprop.pyc in _gradient_function(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads)\r\n    115     return [None] * num_inputs\r\n    116 \r\n--> 117   return grad_fn(mock_op, *out_grads)\r\n    118 \r\n    119 \r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_grad.pyc in _MaxPool3DGrad(op, grad)\r\n    180   return gen_nn_ops.max_pool3d_grad(\r\n    181       op.inputs[0],\r\n--> 182       op.outputs[0],\r\n    183       grad,\r\n    184       ksize=op.get_attr(\"ksize\"),\r\n\r\nTypeError: 'NoneType' object has no attribute '__getitem__'\r\n```\r\n\r\n", "comments": []}, {"number": 23510, "title": "'Model' object has no attribute '_is_graph_network'", "body": "\r\nI create the model use Model(intput, output) function, but the model cannot use tf.keras.estimator.model_to_estimator(), why? The error is \"'Model' object has no attribute '_is_graph_network'\", I can't understand.", "comments": ["It looks like you haven't used a template to create this issue. Please resubmit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation.\r\n"]}, {"number": 23509, "title": "Does keras saved Model can output tensor?", "body": "I found H5 file saved by save API, the output is ONLY layer info supported which is not so convinced when deployed.\r\nIs there exist method that can saved tensor info of Model ??\r\nThanks in andvance!", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}]