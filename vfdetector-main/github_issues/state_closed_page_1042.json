[{"number": 22047, "title": "Tensorflow failed to build on x64 when build with MSVC", "body": "System information:\r\n*Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNA\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows server 2016\r\n*TensorFlow installed from (source or binary):\r\nSource\r\n*TensorFlow version (use command below):\r\nhttps://github.com/tensorflow/tensorflow/commit/24787842adfefe35f5a520313d775b14c29f143a#diff-47cae0cad6802731af5c3d2c42011880\r\n*Python version:\r\nAnaconda 4.1.1 (Python 3.5 64-bit)\r\n*Bazel version (if compiling from source):\r\nN/A\r\n*GCC/Compiler version (if compiling from source):\r\nVS2017 15.7.2\r\n*CUDA/cuDNN version:\r\nNVidia CUDA Toolkit 8.0\r\nNVidia CUDNN 5.1\r\n*GPU model and memory:\r\nN/A\r\n*Exact command to reproduce:\r\nN/A\r\n\r\nDescribe the problem:\r\nTensorflow failed to build on x64. This issue can be reproduced from the master revision commit/24787842adfefe35f5a520313d775b14c29f143a#diff-47cae0cad6802731af5c3d2c42011880. This should be tensorflow source issue, could you please help take a look at this? Thanks!\r\n\r\nThe failures like:\r\nThe whole log file please see attachment.\r\n[log_x64_build.log](https://github.com/tensorflow/tensorflow/files/2347551/log_x64_build.log)\r\n\r\nRepro steps:\r\n\r\n1.git clone https://github.com/tensorflow/tensorflow D:\\Tensorflow\\src\r\n2.pushd D:\\Tensorflow\r\n3.set PreferredToolArchitecture=x64\r\n4.set rel=Release\r\n5.set CUDNN_HOME=\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\cuda\"\r\n6.set PY=C:\\ProgramData\\Anaconda3\r\n7.set CL=/FS /permissive-\r\n8.cmake D:\\Tensorflow\\src\\tensorflow\\contrib\\cmake -A x64 -DCMAKE_BUILD_TYPE=Release -DPYTHON_EXECUTABLE=C:\\ProgramData\\Anaconda3\\python.exe -DPYTHON_LIBRARIES=C:\\ProgramData\\Anaconda3\\libs\\python36.lib -DSWIG_EXECUTABLE=D:\\Tensorflow\\swigwin-3.0.12\\swig.exe -Dtensorflow_BUILD_PYTHON_TESTS=ON -Dtensorflow_BUILD_SHARED_LIB=ON\r\n9.MSBuild /m /p:Configuration=Release;Platform=x64 /p:WindowsTargetPlatformVersion=10.0.17134.0 tensorflow.sln /t:Rebuild\r\n\r\nBuild Message:\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Tools\\MSVC\\14.14.26428\\bin\\HostX64\\x64\\CL.exe /c /ID:\\Tensorflow\\src /ID:\\Tensorflow\\build_x64 /ID:\\Tensorflow\\build_x64\\external\\zlib_archive /I\"D:\\Tensorflow\\build_x64\\external\\gif_archive\\giflib-5.1.4\" /ID:\\Tensorflow\\build_x64\\external\\png_archive /ID:\\Tensorflow\\build_x64\\external\\jpeg_archive /ID:\\Tensorflow\\build_x64\\external\\lmdb /ID:\\Tensorflow\\build_x64\\external\\eigen_archive /ID:\\Tensorflow\\src\\third_party\\eigen3 /ID:\\Tensorflow\\build_x64\\gemmlowp\\src\\gemmlowp /ID:\\Tensorflow\\build_x64\\jsoncpp\\src\\jsoncpp /ID:\\Tensorflow\\build_x64\\external\\farmhash_archive /ID:\\Tensorflow\\build_x64\\external\\farmhash_archive\\util /ID:\\Tensorflow\\build_x64\\external\\highwayhash /ID:\\Tensorflow\\build_x64\\cub\\src\\cub /ID:\\Tensorflow\\build_x64\\external\\nsync\\public /ID:\\Tensorflow\\build_x64\\protobuf\\src\\protobuf\\src /ID:\\Tensorflow\\build_x64\\re2\\install\\include /ID:\\Tensorflow\\build_x64\\external\\sqlite /ID:\\Tensorflow\\build_x64\\double_conversion\\src\\double_conversion /ID:\\Tensorflow\\build_x64\\grpc\\src\\grpc\\include /ID:\\Tensorflow\\build_x64\\snappy\\src\\snappy /nologo /W3 /WX- /diagnostics:caret /MP /O2 /Ob2 /D WIN32 /D _WINDOWS /D NDEBUG /D _ITERATOR_DEBUG_LEVEL=0 /D SQLITE_OMIT_LOAD_EXTENSION /D EIGEN_AVOID_STL_ARRAY /D WIN64 /D NOMINMAX /D _WIN32_WINNT=0x0A00 /D WIN32_LEAN_AND_MEAN /D NOGDI /D PLATFORM_WINDOWS /D TENSORFLOW_USE_EIGEN_THREADPOOL /D EIGEN_HAS_C99_MATH /D TF_COMPILE_LIBRARY /D _HAS_EXCEPTIONS=0 /D GRPC_ARES=0 /D TF_USE_SNAPPY /D \"CMAKE_INTDIR=\\\"Release\\\"\" /D _MBCS /GF /Gm- /MD /GS /fp:precise /Zc:wchar_t /Zc:forScope /Zc:inline /GR /openmp /std:c++14 /Fo\"tf_core_lib.dir\\Release\\\\\" /Fd\"tf_core_lib.dir\\Release\\tf_core_lib.pdb\" /Gd /TP /wd4267 /wd4244 /wd4800 /wd4503 /wd4554 /wd4996 /wd4348 /wd4018 /wd4099 /wd4146 /wd4305 /wd4307 /wd4715 /wd4722 /wd4723 /wd4838 /wd4309 /wd4334 /wd4003 /wd4506 /wd4577 /FC /errorReport:queue  /EHs-c- /bigobj D:\\Tensorflow\\src\\tensorflow\\core\\lib\\bfloat16\\bfloat16.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\core\\arena.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\core\\bitmap.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\core\\coding.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\core\\status.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\core\\threadpool.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\db\\snapfn.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\db\\sqlite.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\gif\\gif_io.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\hash\\crc32c.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\hash\\crc32c_accelerate.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\hash\\hash.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\histogram\\histogram.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\io\\block.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\io\\block_builder.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\io\\buffered_inputstream.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\io\\compression.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\io\\format.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\io\\inputbuffer.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\io\\inputstream_interface.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\io\\iterator.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\io\\path.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\io\\random_inputstream.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\io\\record_reader.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\io\\record_writer.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\io\\snappy\\snappy_inputbuffer.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\io\\snappy\\snappy_outputbuffer.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\io\\table.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\io\\table_builder.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\io\\two_level_iterator.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\io\\zlib_compression_options.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\io\\zlib_inputstream.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\io\\zlib_outputbuffer.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\jpeg\\jpeg_handle.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\jpeg\\jpeg_mem.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\monitoring\\collection_registry.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\monitoring\\sampler.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\png\\png_io.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\random\\distribution_sampler.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\random\\random.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\random\\random_distributions.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\random\\simple_philox.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\random\\weighted_picker.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\strings\\base64.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\strings\\numbers.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\strings\\ordered_code.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\strings\\proto_serialization.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\strings\\proto_text_util.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\strings\\scanner.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\strings\\str_util.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\strings\\strcat.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\strings\\stringprintf.cc D:\\Tensorflow\\src\\tensorflow\\core\\lib\\wav\\wav_io.cc D:\\Tensorflow\\src\\tensorflow\\core\\platform\\abi.cc D:\\Tensorflow\\src\\tensorflow\\core\\platform\\cpu_feature_guard.cc D:\\Tensorflow\\src\\tensorflow\\core\\platform\\cpu_info.cc D:\\Tensorflow\\src\\tensorflow\\core\\platform\\denormal.cc D:\\Tensorflow\\src\\tensorflow\\core\\platform\\file_system.cc D:\\Tensorflow\\src\\tensorflow\\core\\platform\\file_system_helper.cc D:\\Tensorflow\\src\\tensorflow\\core\\platform\\protobuf_util.cc D:\\Tensorflow\\src\\tensorflow\\core\\platform\\setround.cc D:\\Tensorflow\\src\\tensorflow\\core\\platform\\stacktrace_handler.cc D:\\Tensorflow\\src\\tensorflow\\core\\platform\\tensor_coding.cc D:\\Tensorflow\\src\\tensorflow\\core\\platform\\default\\device_tracer.cc D:\\Tensorflow\\src\\tensorflow\\core\\platform\\default\\human_readable_json.cc D:\\Tensorflow\\src\\tensorflow\\core\\platform\\default\\logging.cc D:\\Tensorflow\\src\\tensorflow\\core\\platform\\default\\mutex.cc D:\\Tensorflow\\src\\tensorflow\\core\\platform\\default\\protobuf.cc D:\\Tensorflow\\src\\tensorflow\\core\\platform\\default\\string_coding.cc D:\\Tensorflow\\src\\tensorflow\\core\\framework\\resource_handle.cc D:\\Tensorflow\\src\\tensorflow\\core\\platform\\windows\\net.cc D:\\Tensorflow\\src\\tensorflow\\core\\platform\\windows\\port.cc D:\\Tensorflow\\src\\tensorflow\\core\\platform\\windows\\windows_file_system.cc\r\nbfloat16.cc\r\n         arena.cc\r\n         bitmap.cc\r\n         coding.cc\r\n         status.cc\r\n         threadpool.cc\r\n         snapfn.cc\r\n         sqlite.cc\r\n         gif_io.cc\r\n         crc32c.cc\r\n         crc32c_accelerate.cc\r\n         hash.cc\r\n         histogram.cc\r\n         block.cc\r\n         block_builder.cc\r\n         buffered_inputstream.cc\r\n         compression.cc\r\n         format.cc\r\n         inputbuffer.cc\r\n         inputstream_interface.cc\r\n         iterator.cc\r\n         path.cc\r\n         random_inputstream.cc\r\n         record_reader.cc\r\n         record_writer.cc\r\n         snappy_inputbuffer.cc\r\n   297>D:\\Tensorflow\\src\\tensorflow\\core\\lib\\core\\stringpiece.h(34,10): error C1083:  Cannot open include file: 'absl/strings/string_view.h': No such file or directory (d:\\agent\\_work\\3\\s\\src\\vctools\\compiler\\cxxfe\\sl\\p1\\c\\p0prepro.c:1711) [D:\\Tensorflow\\build_x64\\tf_core_lib.vcxproj]", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nMobile device", "we just build the tensorflow when build with MSVC on windows.\r\n\r\nMobile device:\r\nN/A\r\n", "I believe the preferred way to build TF on windows is to use bazel now. @gunan can comment further.", "Yes. New install guide is here:\r\nhttps://github.com/tensorflow/docs/blob/master/site/en/install/install_sources_windows.md\r\nLooks like it did not make it to the website yet. @MarkDaoust can we push this page to the 1.10 docs, or do we need to wait until 1.11?", "Fixed:\r\n\r\nhttps://www.tensorflow.org/install/install_sources_windows", "not working for me at the moment. See [here](https://github.com/tensorflow/tensorflow/issues/21880#issuecomment-420782866)\r\n\r\nIt is a bit confusing to build the target **libtensorflow_cc.so** on a windows machine. (shared object on windows?!?! CMake is much more accurate and clearer in this case, but unfortunatly you dropped the CMake support in 1.10)", "You should not be building libtensorflow_cc.so on windows.\r\nMaybe you are looking at the wrong documentation?\r\n\r\n**EDIT**: I think I may be wrong about this, please ignore this comment.", "But what is the main target for the c++ library?", "@PinkySan I was able to successfully build the TensorFlow C++ library by building `//tensorflow:libtensorflow_cc.so` from the `v1.11.0-rc0` tag. Without any changes, this built a version of `libtensorflow_cc.so` that only had the `TF_*` and `TFE_*` symbols in it. After closer examination, those are the only explicitly exported symbols, so it was expected. In order to get the necessary C++ symbols, I modified `tensorflow/BUILD` as follows:\r\n```\r\n--- a/tensorflow/BUILD\r\n+++ b/tensorflow/BUILD\r\n@@ -562,7 +562,10 @@ tf_cc_shared_object(\r\n             \"-Wl,-exported_symbols_list\",  # This line must be directly followed by the exported_symbols.lds file\r\n             \"$(location //tensorflow:tf_exported_symbols.lds)\",\r\n         ],\r\n-        \"//tensorflow:windows\": [],\r\n+        \"//tensorflow:windows\": [\r\n+           \"-def:\" +  # This line must be directly followed by the exported_symbols_msvc.lds file\r\n+            \"$(location //tensorflow:tf_exported_symbols_msvc.lds)\",\r\n+       ],\r\n         \"//conditions:default\": [\r\n             \"-z defs\",\r\n             \"-Wl,--version-script\",  #  This line must be directly followed by the version_script.lds file\r\n@@ -571,6 +574,7 @@ tf_cc_shared_object(\r\n     }),\r\n     deps = [\r\n         \"//tensorflow:tf_exported_symbols.lds\",\r\n+        \"//tensorflow:tf_exported_symbols_msvc.lds\",\r\n         \"//tensorflow:tf_version_script.lds\",\r\n         \"//tensorflow/c:c_api\",\r\n         \"//tensorflow/c/eager:c_api\",\r\n@@ -586,6 +590,7 @@ exports_files(\r\n     [\r\n         \"tf_version_script.lds\",\r\n         \"tf_exported_symbols.lds\",\r\n+        \"tf_exported_symbols_msvc.lds\",\r\n     ],\r\n )\r\n\r\n```\r\nI then had to provide a DEF file that contained the symbols. I called it `tf_exported_symbols_msvc.lds`, but that was really just an arbitrary decision. Luckily there is already a script for extracting the symbols that may be found in the `tensorflow/contrib/cmake/tools` directory called `create_def_file.py`. The trick was finding the .lib files to include as inputs to the script. Due to the limitations of number of symbols (65535) that may be exported in a DLL, I had to pare down the symbols. It took some trial and error to find a suitable list that fell under the limit. It is nice that `create_def_file.py` shows the number of symbols as the second output value, making iteration of the list easier.\r\n\r\nAs for parameters to that script, you need a semicolon separated list of the files for the inputs, the `--output` parameter should be `tensorflow/tf_exported_symbols_msvc.lds`, the `--bitness` should be `64` or `32` (based one whether you are building for x64 or x86), and the `--target` should be either `tensorflow` or `tensorflow_cc` (use the former to match the name from the cmake builds). After running the script, the appropriate `tf_exported_symbols_msvc.lds` file should be created. You can now build the C++ target. If all is successful, you will need to rename `libtensorflow_cc.so` to `tensorflow.dll` (or `tensorflow_cc.dll` if you specified that when creating the DEF file). Likewise, rename `libtensorflow_cc.so.if.lib` to `tensorflow.lib` (or `tensorflow_cc.lib`). You may not know for sure if you have all of the symbols you need until you link against your own executable.\r\n\r\nI could provide the list of libraries that I ended up using, but you may have other required symbols an my list is probably not the best. The libraries come from the `c`, `cc`, and `core` directories. You could also probably get the list of .lib file names by running through the cmake process, then seeing which .lib files it uses. Ideally someone with more knowledge of TensorFlow could automate this process.", "FWIW, I've included a fix for CMake and Windows in this patch here:\r\nhttps://github.com/bytedeco/javacpp-presets/blob/beb0a4a/tensorflow/tensorflow-windows.patch", "I've also found it wasn't too hard to assemble manually all the .obj files from Bazel into a command file that we can then feed to MSVC instead of a static library `cl @/path/to/objs.dos ...`:\r\nhttps://github.com/bytedeco/javacpp-presets/blob/4200f910a8c511ea67416de0bd1af67251b61a12/tensorflow/cppbuild.sh#L197-L213\r\n", "> @PinkySan I was able to successfully build the TensorFlow C++ library by building `//tensorflow:libtensorflow_cc.so` from the `v1.11.0-rc0` tag. Without any changes, this built a version of `libtensorflow_cc.so` that only had the `TF_*` and `TFE_*` symbols in it. After closer examination, those are the only explicitly exported symbols, so it was expected. In order to get the necessary C++ symbols, I modified `tensorflow/BUILD` as follows:\r\n> \r\n> ```\r\n> --- a/tensorflow/BUILD\r\n> +++ b/tensorflow/BUILD\r\n> @@ -562,7 +562,10 @@ tf_cc_shared_object(\r\n>              \"-Wl,-exported_symbols_list\",  # This line must be directly followed by the exported_symbols.lds file\r\n>              \"$(location //tensorflow:tf_exported_symbols.lds)\",\r\n>          ],\r\n> -        \"//tensorflow:windows\": [],\r\n> +        \"//tensorflow:windows\": [\r\n> +           \"-def:\" +  # This line must be directly followed by the exported_symbols_msvc.lds file\r\n> +            \"$(location //tensorflow:tf_exported_symbols_msvc.lds)\",\r\n> +       ],\r\n>          \"//conditions:default\": [\r\n>              \"-z defs\",\r\n>              \"-Wl,--version-script\",  #  This line must be directly followed by the version_script.lds file\r\n> @@ -571,6 +574,7 @@ tf_cc_shared_object(\r\n>      }),\r\n>      deps = [\r\n>          \"//tensorflow:tf_exported_symbols.lds\",\r\n> +        \"//tensorflow:tf_exported_symbols_msvc.lds\",\r\n>          \"//tensorflow:tf_version_script.lds\",\r\n>          \"//tensorflow/c:c_api\",\r\n>          \"//tensorflow/c/eager:c_api\",\r\n> @@ -586,6 +590,7 @@ exports_files(\r\n>      [\r\n>          \"tf_version_script.lds\",\r\n>          \"tf_exported_symbols.lds\",\r\n> +        \"tf_exported_symbols_msvc.lds\",\r\n>      ],\r\n>  )\r\n> ```\r\n> I then had to provide a DEF file that contained the symbols. I called it `tf_exported_symbols_msvc.lds`, but that was really just an arbitrary decision. Luckily there is already a script for extracting the symbols that may be found in the `tensorflow/contrib/cmake/tools` directory called `create_def_file.py`. The trick was finding the .lib files to include as inputs to the script. Due to the limitations of number of symbols (65535) that may be exported in a DLL, I had to pare down the symbols. It took some trial and error to find a suitable list that fell under the limit. It is nice that `create_def_file.py` shows the number of symbols as the second output value, making iteration of the list easier.\r\n> \r\n> As for parameters to that script, you need a semicolon separated list of the files for the inputs, the `--output` parameter should be `tensorflow/tf_exported_symbols_msvc.lds`, the `--bitness` should be `64` or `32` (based one whether you are building for x64 or x86), and the `--target` should be either `tensorflow` or `tensorflow_cc` (use the former to match the name from the cmake builds). After running the script, the appropriate `tf_exported_symbols_msvc.lds` file should be created. You can now build the C++ target. If all is successful, you will need to rename `libtensorflow_cc.so` to `tensorflow.dll` (or `tensorflow_cc.dll` if you specified that when creating the DEF file). Likewise, rename `libtensorflow_cc.so.if.lib` to `tensorflow.lib` (or `tensorflow_cc.lib`). You may not know for sure if you have all of the symbols you need until you link against your own executable.\r\n> \r\n> I could provide the list of libraries that I ended up using, but you may have other required symbols an my list is probably not the best. The libraries come from the `c`, `cc`, and `core` directories. You could also probably get the list of .lib file names by running through the cmake process, then seeing which .lib files it uses. Ideally someone with more knowledge of TensorFlow could automate this process.\r\n\r\nDo you have a list of the libraries available for me? I think I try to add too many of them. Thanks beforehand.", "Unfortunatly I do not have a list of the libraries", "@gittyupagain @Steroes : \r\nI have tried to implement the solution as mentioned by you.\r\nBut still getting an error message.\r\n**For Ex: Consider one of the error messages**\r\nmain.obj:-: error: LNK2019: unresolved external symbol \"public: __cdecl tensorflow::ops::MatMul::MatMul(class tensorflow::Scope const &,class tensorflow::Input,class tensorflow::Input,struct tensorflow::ops::MatMul::Attrs const &)\" (??0MatMul@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@1AEBUAttrs@012@@Z) referenced in function main\r\n\r\nBut using approach mentioned i have linked the symbols to tensorflow_cc.so file:\r\nPlease have a look on the exported symbols of tensorflow_cc.so file:\r\n\r\n 346  159 001E4270 ?TransposeA@Attrs@MatMul@ops@tensorflow@@QEAA?AU1234@_N@Z\r\n 347  15A 001EECC0 ?TransposeA@MatMul@ops@tensorflow@@SA?AUAttrs@123@_N@Z\r\n 348  15B 001E4280 ?TransposeB@Attrs@MatMul@ops@tensorflow@@QEAA?AU1234@_N@Z\r\n\r\nI think the following link is present in the file still its giving an error message\r\n", "Hi, @HackersSpirit. Do you know how to link the built `.so` files to VS? And which ones should be renamed to `.dll`. I have searched quite a lot but found nothing on it. Then I cannot even work on your problems.", "@guikarist : Currently i am working with qt creator. I will have to look into building the libray using visual studio", "@HackersSpirit It is what I have to do, too.\r\nI have asked a [question](https://stackoverflow.com/questions/53809398/how-to-program-with-c-api-library-on-windows-using-bazel) on stackoverflow, and I hope people with similar problems can work together to solve it.", "Well, to summarize my TensorFlow journey: Here is the build script I use to compile it with bazel. \r\nDirectory structure:\r\ntensorflow/\r\n   bin\r\n   build (build.ps1)\r\n   source (tensorflow v.1.11.0)\r\n\r\n- Place build.ps1 in build dir.\r\n- Patch for symbols based on earlier post.\r\n- Use FindTensorFlow.CMake file to find the TensorFlow package.\r\nSee [tf_changes_msvc.zip](https://github.com/tensorflow/tensorflow/files/2687763/tf_changes_msvc.zip)\r\n for contents. This is how I got it to work for the inference example from https://github.com/PatWie/tensorflow-cmake/tree/master/inference/cc.\r\n\r\n", "> Well, to summarize my TensorFlow journey: Here is the build script I use to compile it with bazel.\r\n> Directory structure:\r\n> tensorflow/\r\n> bin\r\n> build (build.ps1)\r\n> source (tensorflow v.1.11.0)\r\n> \r\n> * Place build.ps1 in build dir.\r\n> * Patch for symbols based on earlier post.\r\n> * Use FindTensorFlow.CMake file to find the TensorFlow package.\r\n>   See [tf_changes_msvc.zip](https://github.com/tensorflow/tensorflow/files/2687763/tf_changes_msvc.zip)\r\n>   for contents. This is how I got it to work for the inference example from https://github.com/PatWie/tensorflow-cmake/tree/master/inference/cc.\r\n\r\nIt is extremely wonderful! But I haven't tested the script. Where does the zip file come from?", "@guikarist I created the zip/scripts myself. Gathered from all kinds of sources like github and StackOverflow. It took me 5 weeks fulltime to get it working on windows, Ubuntu 14 and 16 and an old CentOs linux machine. Also have the bash Linux build script if someone is in need.", "@Steroes What a big job! I have read the whole script, now I have some questions:\r\n\r\n1. Visual Studio is one of the requirements of your script, right?\r\n1. I found that your script does not add support for CUDA. So I just need to add some other environment variables if I need CUDA? If so, where do you find the exact name of these variable? I tried but found no official ones.\r\n\r\nThank you!", "@guikarist \r\n1. Indeed, I use VS Community 2015 Update 3\r\n2. I don't need CUDA indeed, so I disabled it. If you look into the configure.py script located in the tensorflow source code, then you can find the TF_** variables, which can be set as environment variables. There are also TF_ variables about CUDA. You can set them in the script to the needed value.\r\n\r\nEDIT:\r\nI don't know if the build with CUDA works. Did not try that.", "> You should not be building libtensorflow_cc.so on windows.\r\n\r\n@gunan Could you please explain that a little further? Why should we not build that and:\r\n\r\n> But what is the main target for the c++ library?\r\n\r\nThanks.", "@mnboos I think I may be wrong about not building libtensorflow_cc.so on windows.\r\n\r\n@meteorcloudy could you provide a summary of how to build libtensorflow_cc.dll on windows?\r\nMaybe a short script, or a list of commands and finally where the dll gets generated?", "> @mnboos I think I may be wrong about not building libtensorflow_cc.so on windows.\r\n> \r\n> @meteorcloudy could you provide a summary of how to build libtensorflow_cc.dll on windows?\r\n> Maybe a short script, or a list of commands and finally where the dll gets generated?\r\n\r\nSee: https://github.com/guikarist/tensorflow-windows-build-script", "@Steroes 's solution looks good to me", "@gittyupagain it is nice to find your export C++ symbol method here. However as tf evolves to 2.0, the old cc API fadeout and I am trying to keep that up-to-date with this PR https://github.com/tensorflow/tensorflow/pull/26152\r\n\r\nIn the past necessary exported symbols can be found with the cmake contrib python file, at current status all tf library are built via bazel, and C++ symbols are exported with this [file](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tf_exported_symbols.lds)\r\n\r\nfrom your description an extra MSVC symbol is needed, could you provide the previous one that you are using as a reference?\r\n\r\nAt present stage most of the core symbols are not exported, my current solution is to copy all compiled libs and linking them all to the executable but still get quite a lot of problem for C++ API to be fully functional. Any help will be appreciated.", "@Steroes I'm using your script to build tensorflow_cc.dll.\r\n\r\nI get link errors when testing in my app. I added the symbols to tf_exported_symbols_msvc and place that file in source->tensorflow along with tf_exported_symbols and other files. After recompiling I get the same link errors. Is this the correct file location? Doesn't look like my changes to this file are sticking or something.", "@ttdd11 You didn't apply the patch my script applied automatically. (e.g. [for Tensorflow 1.12.0](https://github.com/guikarist/tensorflow-windows-build-script/blob/master/patches/cpp_symbol.1.12.0.patch))", "@guikarist very correct thanks!", "@gittyupagain hi ,  it seems the BUILD file is version-specific \uff0c could u explain how to find that build file  and how to modify?  i found the build file under tensorflow is quite different compared with yours. thx a lot!"]}, {"number": 22046, "title": "Fix documentation issue with `tf.nn.conv1d`", "body": "The `tf.nn.conv1d` supports float16, float32, and float64\r\nthough in `tf.nn.conv1d.__doc__` only float16 and float32\r\nare mentioned. This fix updates the doc string to add\r\nfloat64 as the supported data type.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 22045, "title": "Add float16 support for NonMaxSuppressionV{2,3,4}", "body": "This fix tries to address the issue raised in #20199 where there was no float16 support for NonMaxSuppressionV2. As NonMaxSuppressionV2 is the earlier versions of API\r\nand there are newer versions of NonMaxSuppression: NonMaxSuppressionV2, NonMaxSuppressionV3, NonMaxSuppressionV4, this fix exposes the float16 support to all of the above.\r\n(Note in the master the default version used is NonMaxSuppressionV3)\r\n\r\nThis fix fixes #20199.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks @azaks2 for the review. The PR has been updated and the previous test failure:\r\n```\r\n//tensorflow/core/ops/compat:backwards_compatibility_test\r\n```\r\nhas also been fixed. Please take a look and let me know if there are any issues."]}, {"number": 22044, "title": "Fix syntax error in single_image_random_dot_stereograms caused by locale", "body": "This fix tries to address the issue raised in #21164 where the single_image_random_dot_stereograms in different locale (like de_DE) caused syntax error in python like:\r\n```\r\nFile \"<string>\", line 28\r\n    def single_image_random_dot_stereograms(depth_values, hidden_surface_removal=True, convergence_dots_size=8, dots_per_inch=72, eye_separation=2,5, mu=0,333299994, normalize=True, normalize_max=-100, normalize_min=100, border_level=0, number_colors=256, output_image_shape=[1024, 768, 1], output_data_window=[1022, 757], name=None):\r\n                                                                                                                                      ^\r\nSyntaxError: invalid syntax\r\n```\r\n\r\nThe issue was that the float to string conversion in `python_op_gen_internal.cc` triggered snprintf (in `FloatToBuffer`) which is local dependent and generates something like `eye_separatiion=2,5` in DE locale.\r\n\r\nThis fix replaced the float to string conversion with locale-independent\r\n```\r\n      std::ostringstream s;\r\n      s.imbue(std::locale::classic());\r\n```\r\n\r\nThis fix fixes #21164.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Nice.\r\n", "Can you fix the import order (see linter log: https://source.cloud.google.com/results/invocations/ff5695b9-ca62-4de2-8c9a-b3b8d2f7a69c/targets/%2F%2Ftensorflow%2Ftools%2Fci_build:gen_ci_clang_format_out/log)", "Wait, your last commit already ran clang-format? Is that stale?", "Is this a difference between clang-format versions? I'm afraid of those...", "Thanks @martinwicke for the review. The PR has been updated to address the issue in `Experimental clang-format Check` failure.\r\n\r\nThe clang-format might be different over versions.  I am wondering if there is some information about platform, clang-format version, and the command line in `Experimental clang-format Check`? That probably helps so that developer could run the `Experimental clang-format Check` before submit the PR.", "Tried several different versions of clang-format, it looks like clang-format installed through `Ubuntu 16.04` almost matches the behavior of `Experimental clang-format Check`. except that the following line\r\n```\r\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\r\n```\r\nwill be moved and sorted in `clang-format -i --style=Google`, while `Experimental clang-format Check` will keep the above line at the beginning of the included headers.\r\n\r\nI remember placing `unsupported/Eigen/CXX11/Tensor` was intentional and had a discussion before. Did a search but couldn't find the discussion and related PR though.", "@yongtang I'll pull this in and hope for the best.\r\n\r\n@yifeif This is the dreaded lint problem. Which version of clang-format are we using exactly?", "We are using 3.9 with -style=google in `Experimental clang-format Check`. We can add this information to the failure log in addition to the diff. @martinwicke pulling should be okay."]}, {"number": 22043, "title": "GPU: tf.Session() needs 1-3 minutes at first start", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux powerai 4.4.0-109-generic #132-Ubuntu SMP Tue Jan 9 20:00:40 UTC 2018 ppc64le ppc64le ppc64le GNU/Linux\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nsource (R1.10)\r\n- **TensorFlow version (use command below)**:\r\nR1.10\r\n- **Python version**:\r\n3.5.2\r\n- **Bazel version (if compiling from source)**:\r\n1.15.2\r\n- **GCC/Compiler version (if compiling from source)**:\r\n5.4.0 20160609 \r\n- **CUDA/cuDNN version**:\r\nCUDA 9.0, cuDNN 7.1\r\n- **GPU model and memory**:\r\n4 x  Tesla P100-SXM2-16GB\r\n- **Exact command to reproduce**:\r\nimport tensorflow as tf\r\nsess = tf.Session()\r\n\r\n### Describe the problem\r\n\r\nEvery first time I start a tensorflow session, it needs 1-3 minutes to finish. \r\nIt freezes at the point \"2018-09-04 08:32:04.920181: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0, 1, 2, 3\"\r\n\r\nAfter waiting and restarting the python script it works in 1-5 seconds.\r\n\r\n### Source code / logs\r\nThere is no additional source code or loginformation.", "comments": ["Its likely jit compiling the binary PTX up to your device's compute capacity. See for e.g. https://devtalk.nvidia.com/default/topic/810047/ptx-compute-2-0-not-forward-compatible-with-gtx-970-/", "I don't know how this link should solve the issue ?", "Tensorflow binaries are compiled against specific compute capabilities (3.5 and 5.2 IIRC). If you your device is a different compute capability, the PTX is jit-compiled to your device's compute capability. Since there is a great deal of CUDA code, this initially takes some time, but I believe its a once off thing.", "okay, which compute capabilitie I need to choose for my graphiccards above ?", "It's 6.0", "Seeing the 970 cause the JIT rebuild in cycles, as if the delay is something that get's GC'd. Kind of adds some lame delays waiting for the GPUs to be accessible. "]}, {"number": 22042, "title": "Could not use adagrad or adadelta optimizer in eager mode(gpu)", "body": "https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/generative_examples/text_generation.ipynb\r\nChange to use tf.train.AdagradOptimizer then will get below error \r\nNotFoundError: No registered 'ResourceSparseApplyAdagrad' OpKernel for GPU devices compatible with node ResourceSparseApplyAdagrad = ResourceSparseApplyAdagrad[T=DT_FLOAT, Tindices=DT_INT32, update_slots=true, use_locking=false](dummy_input, dummy_input, dummy_input, dummy_input, dummy_input)\r\n\t.  Registered:  device='CPU'; T in [DT_DOUBLE]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_DOUBLE]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_BFLOAT16]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_BFLOAT16]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_HALF]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_HALF]; Tindices in [DT_INT32]\r\n [Op:ResourceSparseApplyAdagrad]", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Came to post the same thing, only for me it's using the RMS Optimizer with an RNN and Eager. I note that using the same RMS Optimizer with a vanilla ANN works fine. Here is my error for RMS + RNN:\r\n\r\n```\r\nNotFoundError: No registered 'ResourceSparseApplyRMSProp' OpKernel for GPU devices compatible with node ResourceSparseApplyRMSProp = ResourceSparseApplyRMSProp[T=DT_FLOAT, Tindices=DT_INT32, use_locking=false](dummy_input, dummy_input, dummy_input, dummy_input, dummy_input, dummy_input, dummy_input, dummy_input, dummy_input)\r\n\t.  Registered:  device='CPU'; T in [DT_HALF]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_HALF]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_DOUBLE]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_DOUBLE]; Tindices in [DT_INT64]\r\n [Op:ResourceSparseApplyRMSProp]\r\n```\r\n\r\nI also note that this example was done using tf.keras and Eager. If I use regular Keras and static graphs, I am able to use RMSProp with any RNN I build.\r\n\r\nHave I written custom code: No\r\nOS Platform and Distribution: Windows 10 x64 1603 LTSB\r\nTensorFlow installed from: Self compiled\r\nTensorFlow version: 1.10\r\nBazel version: 0.16.1\r\nCUDA/cuDNN version: 9.2 / 7.1.4\r\nGPU model and memory: GTX 1060m\r\nExact command to reproduce:\r\n\r\n```\r\n    model = tf.keras.models.Sequential()\r\n    model.add(layers.Embedding(NUM_WORDS, 128, input_length=MAXLEN))\r\n    model.add(layers.SeparableConv1D(64,\r\n                     5,\r\n                     padding='valid',\r\n                     activation='relu',\r\n                     strides=1))\r\n    model.add(layers.MaxPooling1D(pool_size=4))\r\n        \r\n    model.add(layers.LSTM(256, dropout=0.2, recurrent_dropout=0.5, kernel_initializer=tf.keras.initializers.VarianceScaling(scale=1.0, mode='fan_avg', seed=None), return_sequences=False))\r\n\r\n    model.add(layers.Dense(128, activation='elu', kernel_initializer=tf.keras.initializers.VarianceScaling(scale=1.0, mode='fan_avg', seed=None)))\r\n    model.add(layers.BatchNormalization(momentum=0.9))\r\n    model.add(layers.Dropout(0.5))\r\n    model.add(layers.Dense(128, activation='elu', kernel_initializer=tf.keras.initializers.VarianceScaling(scale=1.0, mode='fan_avg', seed=None)))\r\n    model.add(layers.BatchNormalization(momentum=0.9))\r\n    model.add(layers.Dropout(0.5))\r\n    model.add(layers.Dense(y_num_classes, activation='sigmoid'))\r\n\r\n    # Eager doesn't like RMSProp here for some reason, causes errors\r\n    model.compile(optimizer=tf.train.RMSPropOptimizer(learning_rate=0.01), loss='binary_crossentropy', metrics=['acc'])\r\n    \r\n    model.fit(...)\r\n```\r\n\r\nAfter I get this error, if I switch to Adam as the optimizer, the code .fit() code runs, but the gradient descent never works, that is, training loss never changes and the weights never change (so training is basically not happening). The only way to fix that is to reset the python kernel entirely, but even then, if you use Adam from the start, you can only call model.fit() once, and then any other calls never train, each epoch runs, but nothing happens.\r\n\r\nAll of these Eager bugs seem to be happening only with RNNs though, I do not get any of these issues when just using Dense layers and no Conv or RNN layers.\r\n\r\nSo it seems that something may be very seriously wrong with Eager + tf.keras + Conv/RNN layers on GPU.", "Thanks for the report.\r\nI don't see a `with tf.device(\"/gpu:0\")` in the colab, so it should be able to use the CPU kernel.\r\n\r\n@akshaym @allenlavoie : Mind taking a look?\r\n\r\nCC @alextp ", "I get this same error even when using a `with tf.device(\"/gpu:0\")` block in\nmy code.\n\n\n\n\nOn Mon, 17 Sep 2018, 18:30 Asim Shankar, <notifications@github.com> wrote:\n\n> Thanks for the report.\n> I don't see a with tf.device(\"/gpu:0\") in the colab, so it should be able\n> to use the CPU kernel.\n>\n> @akshaym <https://github.com/akshaym> @allenlavoie\n> <https://github.com/allenlavoie> : Mind taking a look?\n>\n> CC @alextp <https://github.com/alextp>\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/22042#issuecomment-422213114>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AYqzNErsS7Rz9in2vGivGL7IoDxlZfxLks5ucD6SgaJpZM4WYS6L>\n> .\n>\n", "@trias702 : Yeah, a `with tf.device(\"/gpu:0\")` mean \"run everything under this on the GPU\", at which point I expect it to fail with the error above (complaining that there is no GPU implementation of `ResourceSparseApplyRMSProp`. However, without that directive, it should be able to use the CPU kernel (as so your original report does sound like a bug)", "@asimshankar This error is happening because once the variable has been placed on the GPU we need to run any op which updates it (including `ResourceSparseApplyRMSProp` here) on the same device the variable is, which means there's nothing the eager placer can do but fail.\r\n\r\nThe correct long-term fix here is to register the adagrad optimizers for CPU and GPU.\r\n\r\nThe correct short-term fix is to force-place the variables on the CPU by building the model inside a `with tf.device('cpu:0'):` block.", "Looking at the code it seems like we never implement the ResourceSparseApply* ops for any optimizer on the GPU. So we should fix this by probably auto-placing the variables in the keras Embedding layer on the CPU if in eager and there are GPUs available (so as to not interfere with TPUs).", "Thanks very much for the fix!\r\n\r\nHowever, I think there might be a small bug in line 129 of your change to tensorflow/python/keras/layers/embeddings.py:\r\n\r\nshould it not be: \r\n\r\n`with ops.device('/cpu:0'):`\r\n\r\ninstead of:\r\n\r\n`with ops.device('cpu:0'):`", "I don't think this makes a difference; if you look at the test, it passes\nnow, so it's doing the right thing.\n\nOn Wed, Sep 19, 2018 at 11:36 AM trias702 <notifications@github.com> wrote:\n\n> Thanks very much for the fix!\n>\n> However, I think there might be a small bug in line 129 of your change to\n> tensorflow/python/keras/layers/embeddings.py:\n>\n> should it not be:\n>\n> with ops.device('/cpu:0'):\n>\n> instead of:\n>\n> with ops.device('cpu:0'):\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/22042#issuecomment-422910431>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxYLODsb3ZOVY5vFbSC1epUOZ_AKoks5uco7GgaJpZM4WYS6L>\n> .\n>\n\n\n-- \n - Alex\n", "Ok, just wanted to check since I've only ever seen it done the other way.\r\n\r\nJust a quick thing though, I think there is another strange bug here happening somewhere. I've slipstreamed the changes from your commit into my local Tensorflow 1.10 installation on Windows. I was then able to successfully run, my code as written above, so from that perspective, job done, your fix seems to work.\r\n\r\nHowever, when I immediately re-ran the model.fit() again, I hit this error:\r\n\r\n```\r\nFile \"C:\\Unix\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 1903, in <lambda>\r\n    shape, dtype=dtype, partition_info=partition_info)\r\n\r\n  File \"C:\\Unix\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py\", line 549, in __call__\r\n    q, r = gen_linalg_ops.qr(a, full_matrices=False)\r\n\r\n  File \"C:\\Unix\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_linalg_ops.py\", line 1518, in qr\r\n    _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n\r\n  File \"<string>\", line 3, in raise_from\r\n\r\nInternalError: E:\\Github\\tensorflow\\tensorflow\\core\\kernels\\cuda_solvers.cc:468: cuSolverDN call failed with status =7 [Op:Qr]\r\n```\r\n\r\nSo I tried again, and got:\r\n\r\n```\r\n...\r\nInternalError: E:\\Github\\tensorflow\\tensorflow\\core\\kernels\\cuda_solvers.cc:468: cuSolverDN call failed with status =6 [Op:Qr]\r\n```\r\n\r\nI looked into both files, and saw that this might be happening from failed memory allocation calls, so I did a quick Google search which brought me to: https://github.com/tensorflow/tensorflow/issues/19671\r\n\r\nI tried the workaround from that, by calling \r\n\r\ntf.set_random_seed(1)\r\n\r\nAnd this fixed the cuSolverDN issue, however, trying to run model.fit() now gives the following error every time:\r\n\r\n`ResourceExhaustedError: OOM when allocating tensor with shape[10000,128] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cuda_host_bfc [Op:AssignVariableOp]`\r\n\r\nThere's definitely something odd going on with TF Eager and memory allocation. Doing the set_random_seed(1) trick fixes the strange QR issue, but the model is for some reason then trying to fit the entire training set into memory, even though I have batch_size = 16 set, so it should not be doing that.\r\n\r\nFurthermore, if I just restart the entire kernel, everything works, but only for the first call to model.fit(). Any subsequent calls to model.fit() will hit all of the issues I noted above, first the cuSolverDN issue, then the OOM issue.", "Just forgot, in case it might help, the actual line in my own code which triggers the cuSolverDN error is:\r\n\r\n`model.add(layers.LSTM(256, dropout=0.2, recurrent_dropout=0.5, kernel_initializer=tf.keras.initializers.VarianceScaling(scale=1.0, mode='fan_avg', seed=None), return_sequences=False))`\r\n\r\nWhen I fix that using the tf.set_random_seed(1) trick, the line of code which gives me the OOM error is:\r\n\r\n`model.add(layers.Embedding(NUM_WORDS, 128, input_length=MAXLEN))`\r\n\r\nThis explains why I don't see this issue with any of my vanilla neural nets which just use Dense layers. Those models I can call .fit() over and over again it works fine each time. So something with Eager is broken inside Embedding and LSTM.", "On Wed, Sep 19, 2018 at 12:33 PM trias702 <notifications@github.com> wrote:\n\n> There's definitely something odd going on with TF Eager and memory\n> allocation. Doing the set_random_seed(1) trick fixes the strange QR issue,\n> but the model is for some reason then trying to fit the entire training set\n> into memory, even though I have batch_size = 16 set, so it should not be\n> doing that.\n>\nAre you using tf.data to do your batching? It is possible if you're loading\nnumpy data without tf.data that you'll accidentally copy the entire dataset\nto the GPU only to perform slicing of a minibatch on the GPU. If you open\nanother bug for this issue with some example code I can help you debug.\n\n\n> Furthermore, if I just restart the entire kernel, everything works, but\n> only for the first call to model.fit(). Any subsequent calls to model.fit()\n> will hit all of the issues I noted above, first the cuSolverDN issue, then\n> the OOM issue.\n>\nCan you file an issue with instructions to reproduce for the cuSolverDN\nissue? Hopefully after testing that it still appears on nightly?\n", "Same issue, is this issue solved? Changing `AdaGrad` to `Adam` works (in GPU/CPU mixed mode). It seems to be the problem of `AdaGrad` (which works only if completely in CPU mode)."]}, {"number": 22041, "title": "Can't Import Keras from Tensorflow library", "body": "I've been trying to import keras from tensorflow using the following statement (using Python in PyCharm):\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n```\r\n\r\nIt should work as far as I know but I still get the following message for some reason:\r\n```\r\nfrom tensorflow import keras\r\nImportError: cannot import name 'keras' from 'tensorflow'(/Users/gabork/PycharmProjects/Tester/venv/lib/python3.7/site-packages/tensorflow/__init__.py)\r\n```\r\n\r\n**EDIT:** As I'm using PyCharm with Tensorflow version 0.12.0 and Python 3.7, I'm suspecting version issues. I tried updating to Tensorflow 1.10.1 through PyCharm (venv) but got the following message:\r\n```\r\nCollecting tensorflow==1.10.1\r\n\r\n Could not find a version that satisfies the requirement tensorflow==1.10.1 (from versions: )\r\nNo matching distribution found for tensorflow==1.10.1\r\n```\r\n\r\n**EDIT2:** Downgrading to Python 2.7 seems to fix the issue. But I would still prefer someone more knowledgable to maybe give me a reason why it worked like this.\r\n\r\n\r\n\r\nHave I written custom code: \r\n> No, only code written is shown above\r\n\r\nOS Platform and Distribution: \r\n> OSX 10.13.6\r\n\r\nTensorFlow version\r\n> 0.12.0\r\n\r\nGPU model and memory\r\n> Intel HD Graphics 6000 1536 MB graphics\r\n> 8 GB 1600 MHz DDR3 Memory\r\n\r\nExact command to reproduce\r\n> Shown Above\r\n\r\nMobile device\r\n> N/A", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Tensorflow still doesn't support python 3.7 . See #20517 ", "@gergoh You'll have to update your version of Tensorflow, tf.keras didn't exist in 0.12. You can use Python 2.7 or Python 3.6"]}, {"number": 22040, "title": "AttributeError: 'TPUInfeedOutfeedSessionHook' object has no attribute '_name'", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\nProbably irrelevant for this bug\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux debian\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: not sure, it's baked into an image\r\n- **TensorFlow version (use command below)**: b'v1.9.0-0-g25c197e' 1.9.0\r\n- **Python version**: 3.6.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n\r\n### Describe the problem\r\n\r\nRunning a TPUEstimator and passing initial_infeed_sleep_secs to the TPUConfig results in:\r\n\r\nException in thread InfeedController:\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\r\n    self.run()\r\n  File \"/opt/conda/lib/python3.6/threading.py\", line 864, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 421, in _run_infeed\r\n    logging.info('%s thread sleeping for %d seconds.', self._name,\r\nAttributeError: 'TPUInfeedOutfeedSessionHook' object has no attribute '_name'\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nUsage of self._name here is invalid\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py#L421\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "done", "Nagging Assignee @rjpower: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Thanks for the report and apologies for the delay.\r\n\r\nThis has been fixed internally.  It should show up in Github shortly and be available in the TF 1.12 release."]}, {"number": 22039, "title": "Session.run () takes a long time", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:N/A\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**:1.9.0\r\n- **Python version**:3.6\r\n- **Bazel version (if compiling from source)**:N/A\r\n- **GCC/Compiler version (if compiling from source)**:6.4.0\r\n- **CUDA/cuDNN version**:9.0 / 7.0\r\n- **GPU model and memory**:GeForce GTX960\r\n- **Exact command to reproduce**:N/A\r\n\r\n### Describe the problem\r\n\r\nEach time tensorflow performs session.run() for target detection, the detection time of the first image is very long (including some initialization operations, of course), while the detection time of other images is normal. Suppose I detect the image under a certain path (for example, there are ten images), the time of detecting the first image is relatively long, and the remaining nine images are relatively short (basically consistent). However, my operation in practical application is as follows: the session.run() is called every once in a while to detect a picture. I hope that the detection time after the first one is normal except for the long time. However, through my test (guess), after exiting the loop logic of detection, tensorflow redid a series of initialization operations the next time the detection was done, which puzzled me.\r\n\r\nWith other frameworks, such as mxnet, initial detection takes longer. And then the detection time is normal, as if it's not doing some initialization anymore. I thought, could tensorflow do the same thing?\r\n\r\n### Source code / logs\r\n\r\n2018-09-04 14:48:59.111510: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.57GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\ntime\uff1a5627.940655ms\r\n0.9999423027038574\r\ntime\uff1a657.650471ms\r\n0.9996843338012695\r\ntime\uff1a676.565170ms\r\n0.9722122550010681\r\n0.6320008635520935\r\ntime\uff1a667.881966ms\r\n0.996504545211792\r\n", "comments": ["I met a similar problem. It seems to be caused by some lazy operations. That means some operations will not be executed until the graph actually runs.", "@bignamehyp Is there a way to solve this problem?", "@gulingfengze is this still an issue? Could you load TF 1.12 (stable) or latest version and check whether the issue persists? It is will be good if you can share a simplified code to reproduce the issue. If it was solved by newer version, please close the issue. Thanks!", "@jvishnuvardhan The latest version still has this problem, and my guess is that the program did some initialization like gpu at the beginning of execution.", "@jvishnuvardhan I work on XLA, so this is outside of my area of ownership.", "> my guess is that the program did some initialization like gpu at the beginning of execution.\r\n\r\nQuite possibly.  To address this, when you run configure.py, you need to ensure that you tell TF to build for sm_XY corresponding to your machine's GPU.", "@gulingfengze  \r\nPlease confirm if this is still an issue, or if possible use later stable versions of tf and let us know.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/22039\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/22039\">No</a>\n"]}, {"number": 22038, "title": "Keras Save / Load with Eager", "body": "https://github.com/tensorflow/tensorflow/blob/ec357afc00c8fb0e9c99754afd78d47fc0ab2116/tensorflow/python/keras/engine/network.py#L1357-L1361\r\n\r\nSince TensorFlow uses more and more of Eager execution. \r\nWould it make sense to add a bit more verbose mention on how to save a non-graph model?\r\n\r\n\r\nJust mentioning more explicitly the reason and linking `tfe.Checkpoint` docs would be already of great help.\r\n\r\nAny objections?", "comments": ["agree, actually, why we limit to graph network at the beginning?", "cc @fchollet and @alextp ", "@allenlavoie , assigning to you", "I am not sure, if we need to adapt save method for Eager, but I would at least add an explicit reference to `tfe.Checkpoint` and docs, so once a person sees it she or he can figure out what to do next.\r\n\r\nPlease let me know if I can help", "There is an effort to make `Model.save` work with TensorFlow formats (SavedModel). Right now it requires `get_config()` and `from_config()` to be implemented for subclassed Models, but that could change (@k-w-w has been iterating on `tf.contrib.saved_model.save_keras_model`). \r\n\r\nOn the doc issue, maybe we should reference `save_weights`/`load_weights` from `save` for now? Those work fine with subclassed `Model`s (+eager). It won't save the `Model`, but neither does `tf.train.Checkpoint`. Happy to review a PR for that @lc0 , or I can get to it (but not super soon).", "@allenlavoie just added a small reference. \r\n\r\nAny better way to do it?", "Since the fix is merged, I am closing the issue. Looking forward to the second stage. Thanks @allenlavoie for support!"]}, {"number": 22037, "title": "compile issue for bazel build tensorflow/python/tools:optimize_for_inference", "body": "optimize_for_inference.py\r\n\r\n### System information:\r\n\r\n## OS platform and distribution\r\nLinux debian 4.9.0-6-amd64 #1 SMP Debian 4.9.82-1+deb9u3 (2018-03-02) x86_64 GNU/Linux\r\n\r\n## Tensorflow version\r\nTensorflow version r1.9 (compiled from source for CPU)\r\n\r\n## CUDA/cuDNN \r\nn/a\r\n\r\n## gpu model and memory\r\nn/a\r\n\r\n## Python version\r\nPython version 3.5\r\n\r\n## Bazel version\r\nBazel version 0.16.0 \r\n\r\n## Gcc version\r\ngcc version 6.3.0\r\n\r\n## No custom code used, just standard script:\r\noptimize_for_inference.py\r\n\r\n### Command to reproduce:\r\n\r\nbazel build tensorflow/python/tools:optimize_for_inference\r\n\r\n### Mobile device \r\n\r\nn/a\r\n\r\n### Problem:\r\n\r\nThe result is a failure to build.  \r\n\r\n### Source code / logs\r\nThe output is rather lengthy, but here is the relevant \"ERROR\" part:\r\n\r\nERROR: /home/david/tensorflow/tensorflow/contrib/coder/BUILD:144:1: C++ compilation of rule '//tensorflow/contrib/coder:python/ops/_coder_ops.so' failed (Exit 1)\r\nIn file included from /usr/include/c++/6/map:60:0,\r\n                 from external/protobuf_archive/src/google/protobuf/stubs/common.h:40,\r\n                 from external/protobuf_archive/src/google/protobuf/stubs/atomicops.h:59,\r\n                 from external/protobuf_archive/src/google/protobuf/stubs/atomic_sequence_num.h:33,\r\n                 from external/protobuf_archive/src/google/protobuf/arena_impl.h:38,\r\n                 from external/protobuf_archive/src/google/protobuf/arena.h:54,\r\n                 from ./tensorflow/core/platform/default/protobuf.h:22,\r\n                 from ./tensorflow/core/platform/protobuf.h:31,\r\n                 from ./tensorflow/core/platform/default/string_coding.h:23,\r\n                 from ./tensorflow/core/platform/tensor_coding.h:29,\r\n                 from ./tensorflow/core/framework/resource_handle.h:19,\r\n                 from ./tensorflow/core/framework/allocator.h:24,\r\n                 from ./tensorflow/core/framework/op_kernel.h:23,\r\n                 from tensorflow/contrib/coder/kernels/pmf_to_cdf_op.cc:24:\r\n/usr/include/c++/6/bits/stl_tree.h: In instantiation of 'std::pair<std::_Rb_tree_iterator<_Val>, std::_Rb_tree_iterator<_Val> > std::_Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::equal_range(const _Key&) [with _Key = std::__cxx11::basic_string<char>*; _Val = std::__cxx11::basic_string<char>*; _KeyOfValue = std::_Identity<std::__cxx11::basic_string<char>*>; _Compare = google::protobuf::Map<std::__cxx11::basic_string<char>, tensorflow::AttrValue>::InnerMap::KeyCompare; _Alloc = google::protobuf::Map<std::__cxx11::basic_string<char>, tensorflow::AttrValue>::MapAllocator<std::__cxx11::basic_string<char>*>]':\r\n/usr/include/c++/6/bits/stl_tree.h:2298:49:   required from 'std::_Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::size_type std::_Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::erase(const _Key&) [with _Key = std::__cxx11::basic_string<char>*; _Val = std::__cxx11::basic_string<char>*; _KeyOfValue = std::_Identity<std::__cxx11::basic_string<char>*>; _Compare = google::protobuf::Map<std::__cxx11::basic_string<char>, tensorflow::AttrValue>::InnerMap::KeyCompare; _Alloc = google::protobuf::Map<std::__cxx11::basic_string<char>, tensorflow::AttrValue>::MapAllocator<std::__cxx11::basic_string<char>*>; std::_Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::size_type = long unsigned int]'\r\n/usr/include/c++/6/bits/stl_set.h:602:30:   required from 'std::set<_Key, _Compare, _Alloc>::size_type std::set<_Key, _Compare, _Alloc>::erase(const key_type&) [with _Key = std::__cxx11::basic_string<char>*; _Compare = google::protobuf::Map<std::__cxx11::basic_string<char>, tensorflow::AttrValue>::InnerMap::KeyCompare; _Alloc = google::protobuf::Map<std::__cxx11::basic_string<char>, tensorflow::AttrValue>::MapAllocator<std::__cxx11::basic_string<char>*>; std::set<_Key, _Compare, _Alloc>::size_type = long unsigned int; std::set<_Key, _Compare, _Alloc>::key_type = std::__cxx11::basic_string<char>*]'\r\nexternal/protobuf_archive/src/google/protobuf/map.h:619:9:   required from 'void google::protobuf::Map<Key, T>::InnerMap::erase(google::protobuf::Map<Key, T>::InnerMap::iterator) [with Key = std::__cxx11::basic_string<char>; T = tensorflow::AttrValue; google::protobuf::Map<Key, T>::InnerMap::iterator = google::protobuf::Map<std::__cxx11::basic_string<char>, tensorflow::AttrValue>::InnerMap::iterator_base<google::protobuf::Map<std::__cxx11::basic_string<char>, tensorflow::AttrValue>::KeyValuePair>]'\r\nexternal/protobuf_archive/src/google/protobuf/map.h:1139:5:   required from 'google::protobuf::Map<Key, T>::iterator google::protobuf::Map<Key, T>::erase(google::protobuf::Map<Key, T>::iterator) [with Key = std::__cxx11::basic_string<char>; T = tensorflow::AttrValue]'\r\nexternal/protobuf_archive/src/google/protobuf/map.h:1144:20:   required from 'void google::protobuf::Map<Key, T>::erase(google::protobuf::Map<Key, T>::iterator, google::protobuf::Map<Key, T>::iterator) [with Key = std::__cxx11::basic_string<char>; T = tensorflow::AttrValue]'\r\nexternal/protobuf_archive/src/google/protobuf/map.h:1147:23:   required from 'void google::protobuf::Map<Key, T>::clear() [with Key = std::__cxx11::basic_string<char>; T = tensorflow::AttrValue]'\r\nexternal/protobuf_archive/src/google/protobuf/map_field_inl.h:180:3:   required from 'void google::protobuf::internal::MapField<Derived, Key, T, key_wire_type, value_wire_type, default_enum_value>::Clear() [with Derived = tensorflow::NameAttrList_AttrEntry_DoNotUse; Key = std::__cxx11::basic_string<char>; T = tensorflow::AttrValue; google::protobuf::internal::WireFormatLite::FieldType kKeyFieldType = (google::protobuf::internal::WireFormatLite::FieldType)9u; google::protobuf::internal::WireFormatLite::FieldType kValueFieldType = (google::protobuf::internal::WireFormatLite::FieldType)11u; int default_enum_value = 0]'\r\nbazel-out/k8-opt/genfiles/tensorflow/core/framework/attr_value.pb.h:1764:15:   required from here\r\n/usr/include/c++/6/bits/stl_tree.h:1739:5: internal compiler error: Segmentation fault\r\n     }\r\n     ^\r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Now, I'm getting this error when I try to build the pip package.  If I recall correctly, I  believe I encountered a similar problem before, when trying to build from source, but  was able to build on a subsequent attempt.  However, I don't remember doing anything differently.\r\n\r\nERROR: /home/david/tensorflow/tensorflow/core/grappler/BUILD:61:1: C++ compilation of rule '//tensorflow/core/grappler:graph_view' failed (Exit 1)\r\ntensorflow/core/grappler/graph_view.cc: In constructor 'tensorflow::grappler::GraphView::GraphView(tensorflow::GraphDef*)':\r\ntensorflow/core/grappler/graph_view.cc:22:1: internal compiler error: Segmentation fault\r\n GraphView::GraphView(GraphDef* graph) : graph_(graph) {\r\n ^~~~~~~~~\r\n", "Ok, I'm able to compile it now. However, I didn't do anything different.  I merely ran the command a second time.  Perhaps its  a bug in Bazel. I'm not sure, but it seems as though running the build command a second time after a clean ./configure did the trick.  Bazel does seem to be a bit flaky, as multiple runs would of often land on a  different error.   If anyone is looking into this, I'd be happy to provide more information, if you need it.  Thanks.  ", "```\r\n/usr/include/c++/6/bits/stl_tree.h:1739:5: internal compiler error: Segmentation fault\r\n```\r\nThis line suggests to me that this is actually a segmentation fault in gcc 6", "Yeah this looks like a compiler bug. So I'm marking as contributions welcome.", "Thanks. It is indeed an intermittent error.  I\"ve since been able to compile it, but not sure what I did differently.  On successive runs, the error message is different each time.   ", "I've noticed strange non deterministic errors as well, when some steps of the compilation works occasionally ", "I cannot reproduce with gcc 7. Since this is nondeterministic, I could have just gotten lucky. But since this issue is old and hard to reproduce, I'll close for now. Feel free to file a new issue if this is still a problem."]}, {"number": 22036, "title": "eager mode using tf.train.Checkpoint not support setting max models to keep", "body": "tensorflow 1.10.1\r\nIt looks tf.train.Checkpoint not support setting max models to keep.\r\nIn tensorflow/python/training/checkpointable/util.py\r\nline 1123   def save(self, file_prefix, checkpoint_number=None, session=None):\r\nline 1178 self._last_save_saver = saver_lib.Saver(var_list=named_variables)\r\nlooks like we need saver_lib.Saver(var_list=named_variables, max_to_keep=...) \r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Yes, there was some back and forth on APIs for this. We settled on a separate checkpoint manager object, [tf.contrib.checkpoint.CheckpointManager](https://github.com/tensorflow/tensorflow/blob/102e0de242eccb2ac4664761183a7771b0a7c7af/tensorflow/python/training/checkpoint_management.py#L461), and no checkpoint deletion by default. Could you take a look and let me know if CheckpointManager works for you?", "@allenlavoie  Thanks, this works, I need to update tf code, closing this issue."]}, {"number": 22035, "title": "LSTMCell base article at rnn_cell_impl.py", "body": "The implemented LSTMCell makes use of the \"forget gate\". The base article pointed out in both comments (lines 650 ~ 661) and tensorflow's documentation does not cover the concept of \"forget gate\".\r\nThis concept was introduced by Gers et al. only in 1999 with \"Learning to forget: Continual prediction with LSTM\".", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Hi Eugene, any insights where the implementation came from?", "LGTM", "Hi Vin\u00edcius, thanks for the pull request.\r\n\r\nCan you submit this to the *master* branch? This release branch has already been cut and the build team is not accepting docs cherry-picks.\r\n\r\nThanks!"]}, {"number": 22034, "title": "ubuntu image with tensorflow preinstalled ", "body": "Hi every one\r\nI tried install tensorflow-gpu on my pc I failed i tried all possible ways I failed to build it from source I failed to run it from docker image\r\ncan on you share ubuntu image with tensorflow-gpu installed on it i iam despaired really need to run tensorlow-gpu\r\nand I will be very thankful\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Did you try using out docker containers?\r\nhttps://www.tensorflow.org/install/install_linux#InstallingDocker\r\n\r\nUnfortunately, outside docker we cannot provide full ubuntu system images with tensorflow, due to licensing restrictions."]}, {"number": 22033, "title": "Accuracy oscillates between ~0% and ~70% when creating multiple models", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No\r\n- **TensorFlow installed from (source or binary)**: PyPi (pip)\r\n- **TensorFlow version (use command below)**: b'v1.10.0-rc1-19-g656e7a2b34' 1.10.0\r\n- **Python version**: 3.6.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: CUDA 9.0\r\n- **GPU model and memory**: GTX 1060 6 GB\r\n- **Exact command to reproduce**:\r\n  - Download bug.py and training.csv from this Gist: https://gist.github.com/ChrisSwinchatt/97304761e9f875dfd34e3339891a5475\r\n  - Run python bug.py\r\n\r\n### Describe the problem\r\n\r\n**I've found that the problem doesn't occur when TensorFlow is forced to use the CPU**\r\n\r\nFull disclosure: I also opened an issue with Keras (https://github.com/keras-team/keras/issues/11070) because I'm not sure whether the bug is with TensorFlow or Keras. I am using tf.Keras, though, rather than just Keras with TensorFlow as a backend.\r\n\r\nI'm using a sequence-to-sequence model based on the Keras blogpost which I've wrapped into a fairly complicated object (although the issue also occurs with a simplified version linked below). When I create a new model (which I have to do for gridsearch and for clearing the TF session when the graph gets too big and slows down training) it starts with accuracy of either 0% or 70%.\r\n\r\nHere are a pair of screenshots that show what I mean:\r\n\r\nGood: https://i.imgur.com/7mT5Siv.png\r\nBad: https://i.imgur.com/MZ3NdCB.png\r\n\r\nYou can see in the first screenshot that the accuracy is low but trending upwards. In the second, the accuracy of two models starts at 70% and doesn't increase (another model starts at 3% and also doesn't increase).\r\n\r\nThis happens whether I create new, blank models or load pretrained weights into new models with model.load_weights().\r\n\r\n### Source code / logs\r\n\r\nSource code and screenshots are above.", "comments": ["@ChrisSwinchatt could you please share a simple code snippet that reproduces this issue?", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "@omalleyt12 \r\n\r\nI did, please take a look at my Gist: https://gist.github.com/ChrisSwinchatt/97304761e9f875dfd34e3339891a5475\r\n\r\nThe bug is not triggered if you add:\r\n```python\r\n    with tf.Session(config=tf.ConfigProto(device_count={'GPU':0})) as session:\r\n        K.set_session(session)\r\n        # ...\r\n```\r\n\r\nWhich is what leads me to believe this is a TensorFlow-GPU (or one of its client libs) problem.\r\n\r\n@bignamehyp \r\nI think I did include all the (relevant) information in the issue template. I'm not in need of support because I have found that training on CPU alleviates the problem. I just think it's something you should be aware of in case there is a bug in TensorFlow. Of course, it's possible it's actually a bug in CUDA, CUDNN, the drivers, Windows, etc. or even my hardware, but just in case.\r\n\r\n**The bug is that when training on GPU and creating a second (or third, or fourth, or *N*th) model, its accuracy on the first batch/epoch will be either ~0% or ~70% and never really improve.** The first model works as expected but this obviously isn't good for model selection/cross validation where we're obliged to make several models.\r\n\r\nPlease let me know if any other information will help you.", "Nagging Assignee @bignamehyp: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@ChrisSwinchatt This problem is likely caused by your GPU setup. Try to do a sanity check first and use a small sample first.", "Closing this as it is not a bug nor feature request, feel free to open a new issue if problem persists."]}, {"number": 22032, "title": "[1.10.1] sparse_tensor_dense_matmul_op.cc doesn't compile", "body": "command\r\n```\r\ng++ -w -std=c++11 -O2 -fPIC -gsplit-dwarf -pthread -I. -I/usr/include/gemmlowp -I/usr/include/jsoncpp -I/usr/include/llvm-c-7 -I/usr/include/llvm-7 -Ithird_party/toolchains/gpus/cuda/ -c tensorflow/core/kernels/sparse_tensor_dense_matmul_op.cc -o tensorflow/core/kernels/sparse_tensor_dense_matmul_op.o -Idebian/embedded/eigen\r\n```\r\nwhere debian/embedded/eigen is the eigen source code directory whose version is the same as specified in bazel workspace.\r\n\r\nerror output\r\n```\r\ntensorflow/core/kernels/sparse_tensor_dense_matmul_op.cc: In instantiation of \u2018static tensorflow::Status tensorflow::functor::SparseTensorDenseMatMulFunctor<Eigen::ThreadPoolDevice, T, Tindices, ADJ_A, ADJ_B>::Compute(const CPUDevice&, typename tensorflow::TTypes<T>::Matrix, typename tensorflow::TTypes<T>::ConstMatrix, typename tensorflow::TTypes<T>::ConstVec, typename tensorflow::TTypes<T>::ConstMatrix) [with T = std::complex<double>; Tindices = int; bool ADJ_A = true; bool ADJ_B = false; tensorflow::CPUDevice = Eigen::ThreadPoolDevice; typename tensorflow::TTypes<T>::Matrix = Eigen::TensorMap<Eigen::Tensor<std::complex<double>, 2, 1, long int>, 16, Eigen::MakePointer>; typename tensorflow::TTypes<T>::ConstMatrix = Eigen::TensorMap<Eigen::Tensor<const int, 2, 1, long int>, 16, Eigen::MakePointer>; typename tensorflow::TTypes<T>::ConstVec = Eigen::TensorMap<Eigen::Tensor<const std::complex<double>, 1, 1, long int>, 16, Eigen::MakePointer>; typename tensorflow::TTypes<T>::ConstMatrix = Eigen::TensorMap<Eigen::Tensor<const std::complex<double>, 2, 1, long int>, 16, Eigen::MakePointer>]\u2019:\r\ntensorflow/core/kernels/sparse_tensor_dense_matmul_op.cc:148:5:   required from \u2018void tensorflow::SparseTensorDenseMatMulOp<Device, T, Tindices>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = std::complex<double>; Tindices = int]\u2019\r\ntensorflow/core/kernels/sparse_tensor_dense_matmul_op.cc:42:8:   required from here\r\ntensorflow/core/kernels/sparse_tensor_dense_matmul_op.cc:305:30: error: no matching function for call to \u2018std::array<int, 2>::array(int, int)\u2019\r\n         Eigen::array<int, 2> shuffle(1, 0);  // preserve dimension order\r\n                              ^~~~~~~\r\nIn file included from ./tensorflow/core/kernels/sparse_tensor_dense_matmul_op.h:19,\r\n                 from tensorflow/core/kernels/sparse_tensor_dense_matmul_op.cc:20:\r\n/usr/include/c++/8/array:94:12: note: candidate: \u2018std::array<int, 2>::array()\u2019\r\n     struct array\r\n            ^~~~~\r\n/usr/include/c++/8/array:94:12: note:   candidate expects 0 arguments, 2 provided\r\n/usr/include/c++/8/array:94:12: note: candidate: \u2018constexpr std::array<int, 2>::array(const std::array<int, 2>&)\u2019\r\n/usr/include/c++/8/array:94:12: note:   candidate expects 1 argument, 2 provided\r\n/usr/include/c++/8/array:94:12: note: candidate: \u2018constexpr std::array<int, 2>::array(std::array<int, 2>&&)\u2019\r\n/usr/include/c++/8/array:94:12: note:   candidate expects 1 argument, 2 provided\r\ntensorflow/core/kernels/sparse_tensor_dense_matmul_op.cc: In instantiation of \u2018static tensorflow::Status tensorflow::functor::SparseTensorDenseMatMulFunctor<Eigen::ThreadPoolDevice, T, Tindices, ADJ_A, ADJ_B>::Compute(const CPUDevice&, typename tensorflow::TTypes<T>::Matrix, typename tensorflow::TTypes<T>::ConstMatrix, typename tensorflow::TTypes<T>::ConstVec, typename tensorflow::TTypes<T>::ConstMatrix) [with T = std::complex<double>; Tindices = int; bool ADJ_A = true; bool ADJ_B = true; tensorflow::CPUDevice = Eigen::ThreadPoolDevice; typename tensorflow::TTypes<T>::Matrix = Eigen::TensorMap<Eigen::Tensor<std::complex<double>, 2, 1, long int>, 16, Eigen::MakePointer>; typename tensorflow::TTypes<T>::ConstMatrix = Eigen::TensorMap<Eigen::Tensor<const int, 2, 1, long int>, 16, Eigen::MakePointer>; typename tensorflow::TTypes<T>::ConstVec = Eigen::TensorMap<Eigen::Tensor<const std::complex<double>, 1, 1, long int>, 16, Eigen::MakePointer>; typename tensorflow::TTypes<T>::ConstMatrix = Eigen::TensorMap<Eigen::Tensor<const std::complex<double>, 2, 1, long int>, 16, Eigen::MakePointer>]\u2019:\r\ntensorflow/core/kernels/sparse_tensor_dense_matmul_op.cc:149:5:   required from \u2018void tensorflow::SparseTensorDenseMatMulOp<Device, T, Tindices>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = std::complex<double>; Tindices = int]\u2019\r\ntensorflow/core/kernels/sparse_tensor_dense_matmul_op.cc:42:8:   required from here\r\ntensorflow/core/kernels/sparse_tensor_dense_matmul_op.cc:305:30: error: no matching function for call to \u2018std::array<int, 2>::array(int, int)\u2019\r\n         Eigen::array<int, 2> shuffle(1, 0);  // preserve dimension order\r\n                              ^~~~~~~\r\nIn file included from ./tensorflow/core/kernels/sparse_tensor_dense_matmul_op.h:19,\r\n                 from tensorflow/core/kernels/sparse_tensor_dense_matmul_op.cc:20:\r\n/usr/include/c++/8/array:94:12: note: candidate: \u2018std::array<int, 2>::array()\u2019\r\n     struct array\r\n            ^~~~~\r\n/usr/include/c++/8/array:94:12: note:   candidate expects 0 arguments, 2 provided\r\n/usr/include/c++/8/array:94:12: note: candidate: \u2018constexpr std::array<int, 2>::array(const std::array<int, 2>&)\u2019\r\n/usr/include/c++/8/array:94:12: note:   candidate expects 1 argument, 2 provided\r\n/usr/include/c++/8/array:94:12: note: candidate: \u2018constexpr std::array<int, 2>::array(std::array<int, 2>&&)\u2019\r\n/usr/include/c++/8/array:94:12: note:   candidate expects 1 argument, 2 provided\r\n```\r\n\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: debian experimental\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.10.1\r\n- **Python version**: not related\r\n- **Bazel version (if compiling from source)**: not related\r\n- **GCC/Compiler version (if compiling from source)**: g++ (Debian 8.2.0-4) 8.2.0\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: above\r\n", "comments": ["`-DEIGEN_AVOID_STL_ARRAY` solved this issue."]}, {"number": 22031, "title": "Fix issues in maxout layer", "body": "Originally pointed out by @ilblackdragon in [this pull request](https://github.com/tensorflow/tensorflow/pull/5528#discussion_r92219222):\r\nshape[axis] should = num_units instead of = -1.\r\n\r\nThe original implementation causes some trouble. For example, if I am to maxout a tensor x = (?, 32, 32, 256) and reduce it into a tensor of shape (?, 32, 32, 128), by using `x = tf.contrib.layers.maxout(x, 128)`, I get a tensor of shape (?, 32, 32, ?) instead of (?, 32, 32, 128).\r\n\r\nThe shape of the tensor after the maxout can be inferred with num_units, so there is no need to use -1 here. This also causes inconvenience and raises errors when using a dense layer afterwards (or any layer that needs the value of the last dimension).\r\n\r\nIn addition, the original documentation is problematic in saying that \"num_units should be a multiple of 'axis'\", and this PR fixes the description.", "comments": []}, {"number": 22030, "title": "Inconsistent behaviour of CSVDataset with eager disabled/enabled", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.10.0-12-g4dcfddc5d1 1.10.1\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: see below\r\n\r\n### Describe the problem\r\n\r\nThe following code snippet works without any errors in eager mode but fails when eager is disabled\r\n\r\n```python\r\nimport tensorflow as tf\r\ndata = tf.contrib.data.CsvDataset(\"foo.csv\", [tf.constant(0, dtype=tf.int64)])\r\ndata.make_one_shot_iterator().get_next()\r\n```\r\n\r\nwheer `foo.csv` is generated by `echo 42 > foo.csv`. The issue is fixed by making 42 a single-element list following the example in the `CsvDataset` [documentation](https://www.tensorflow.org/api_docs/python/tf/contrib/data/CsvDataset).\r\n\r\n### Source code / logs\r\n\r\n#### Eager execution disabled\r\n\r\n```python\r\n>>> data.make_one_shot_iterator().get_next()\r\nTraceback (most recent call last):\r\n  File \"[...]/tensorflow/python/framework/ops.py\", line 1576, in _create_c_op\r\n    c_op = c_api.TF_FinishOperation(op_desc)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Shape must be rank 1 but is rank 0 for 'CSVDataset' (op: 'CSVDataset') with input shapes: [], [], [], [], [], [], [0], [].\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"[...]/tensorflow/python/data/ops/dataset_ops.py\", line 170, in make_one_shot_iterator\r\n    six.reraise(ValueError, err)\r\n  File \"[...]/six.py\", line 692, in reraise\r\n    raise value.with_traceback(tb)\r\nValueError: Shape must be rank 1 but is rank 0 for 'CSVDataset' (op: 'CSVDataset') with input shapes: [], [], [], [], [], [], [0], [].\r\n```\r\n\r\n#### Eager execution enabled\r\n\r\n```\r\n>>> data.make_one_shot_iterator().get_next()\r\n(<tf.Tensor: id=53, shape=(), dtype=int64, numpy=42>,)\r\n```", "comments": ["Thanks @rachellim! "]}, {"number": 22029, "title": "tf-1.10.1 freeze_graph 'list index out of range'", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.10.1\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9.0 / 7.1\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nI use the following command to freeze a `ckpt` file to `pb` file:\r\n\r\n```\r\n    freeze_graph(\r\n        input_saver=None,\r\n        input_graph=pbTxt,\r\n        input_binary=False,\r\n        clear_devices=True,\r\n        output_graph=pbPath,\r\n        restore_op_name=None,\r\n        initializer_nodes=\"\",\r\n        input_meta_graph=None,\r\n        filename_tensor_name=None,\r\n        input_saved_model_dir=None,\r\n        output_node_names=outputOp,\r\n        variable_names_blacklist='',\r\n        input_checkpoint=iCheckpoint,\r\n    )\r\n```\r\n\r\nIn `tf-1.8.0` everything seems well, however in tf-1.10.1, I meet the following error:\r\n\r\n```\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/tools/freeze_graph.py\", line 254, in freeze_graph\r\n    checkpoint_version=checkpoint_version)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/tools/freeze_graph.py\", line 128, in freeze_graph_with_def_protos\r\n    var_list=var_list, write_version=checkpoint_version)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1281, in __init__\r\n    self.build()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1293, in build\r\n    self._build(self._filename, build_save=True, build_restore=True)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1330, in _build\r\n    build_save=build_save, build_restore=build_restore)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 756, in _build_internal\r\n    saveables = self._ValidateAndSliceInputs(names_to_saveables)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 663, in _ValidateAndSliceInputs\r\n    for converted_saveable_object in self.SaveableObjectsForOp(op, name):\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 637, in SaveableObjectsForOp\r\n    variable, \"\", name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 121, in __init__\r\n    self.handle_op = var.op.inputs[0]\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2001, in __getitem__\r\n    return self._inputs[i]\r\nIndexError: list index out of range\r\n```\r\n\r\nI wonder why the `tf-1.10` is not tested before release ???\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nGPU model and memory\nExact command to reproduce", "@tensorflowbutler Updated.", "same issues #22019 \r\n\r\nIf I use tensorflow 1.10 version freeze graph.py, it comes with\r\n**ImportError: cannot import name 'checkpoint_management' tf1.10**\r\n\r\nIf I use working version such as tensorflow 1.8, message comes below\r\n\r\nTraceback (most recent call last):\r\n  File \"freeze_graph.py\", line 382, in <module>\r\n    run_main()\r\n  File \"freeze_graph.py\", line 379, in run_main\r\n    app.run(main=my_main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/companyai8way/tf110/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"freeze_graph.py\", line 378, in <lambda>\r\n    my_main = lambda unused_args: main(unused_args, flags)\r\n  File \"freeze_graph.py\", line 272, in main\r\n    flags.saved_model_tags, checkpoint_version)\r\n  File \"freeze_graph.py\", line 254, in freeze_graph\r\n    checkpoint_version=checkpoint_version)\r\n  File \"freeze_graph.py\", line 128, in freeze_graph_with_def_protos\r\n    var_list=var_list, write_version=checkpoint_version)\r\n  File \"/home/companyai8way/tf110/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1281, in __init__\r\n    self.build()\r\n  File \"/home/companyai8way/tf110/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1293, in build\r\n    self._build(self._filename, build_save=True, build_restore=True)\r\n  File \"/home/companyai8way/tf110/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1330, in _build\r\n    build_save=build_save, build_restore=build_restore)\r\n  File \"/home/companyai8way/tf110/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 756, in _build_internal\r\n    saveables = self._ValidateAndSliceInputs(names_to_saveables)\r\n  File \"/home/companyai8way/tf110/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 663, in _ValidateAndSliceInputs\r\n    for converted_saveable_object in self.SaveableObjectsForOp(op, name):\r\n  File \"/home/companyai8way/tf110/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 637, in SaveableObjectsForOp\r\n    variable, \"\", name)\r\n  File \"/home/companyai8way/tf110/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 121, in __init__\r\n    self.handle_op = var.op.inputs[0]\r\n  File \"/home/companyai8way/tf110/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2001, in __getitem__\r\n    return self._inputs[i]\r\nIndexError: list index out of range\r\n\r\n\r\n", "/CC @petewarden, can you take a look?", "@petewarden Help ...", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Tensorflow is hard to use with so many bugs, I wonder why not fixing bugs before developing new functionality.\r\n\r\nAnd the issue block on github is useless, since no useful answers are provided. I wonder maybe turn to using pytorch would be better."]}, {"number": 22026, "title": "fix a minor issue for tf.split document", "body": "", "comments": []}, {"number": 22025, "title": "[Bug] max_pool_with_argmax has different behaviour on CPU and GPU", "body": "The function ``max_pool_with_argmax`` has different behaviour concerning the returned indices depending on the GPU or CPU backend.\r\n\r\n- On GPU, returned indices do not take into account batch dimension. \r\nIt returns indices such that `[b, y, x, c] -> (y * width + x) * channels + c `\r\n- On CPU, returned indices take into account batch dimension.\r\nIt returns indices such that `[b, y, x, c] -> ((b * height + y) * width + x) * channels + c `\r\n\r\nIsn't it a problem if one wants to run models with the same graph including this op both on CPU and GPU ?\r\n\r\nHere is an example to show the problem:\r\n``` python\r\nimport numpy as np\r\nimport tensorflow as tf \r\n\r\ni = tf.placeholder(tf.float32, [None, 4, 4, 1])\r\np, ind = tf.nn.max_pool_with_argmax(i, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\r\n\r\nwith tf.Session() as sess:\r\n    a = np.random.randint(10, size=(2, 4, 4, 1))\r\n    print a[:,:,:,0]\r\n    print sess.run(p, {i: a})[:,:,:,0]\r\n    print sess.run(ind, {i: a})[:,:,:,0]\r\n```\r\nWhich results in something like this on CPU:\r\n```\r\n[[[0 3 0 0]\r\n  [4 0 4 8]\r\n  [7 8 4 5]\r\n  [2 3 4 5]]\r\n\r\n [[2 5 2 7]\r\n  [4 3 4 6]\r\n  [8 3 2 0]\r\n  [7 5 9 3]]]\r\n[[[4. 8.]\r\n  [8. 5.]]\r\n\r\n [[5. 7.]\r\n  [8. 9.]]]\r\n[[[ 4  7]\r\n  [ 9 11]]\r\n\r\n [[17 19]\r\n  [24 30]]]\r\n```\r\nAnd something like this on GPU:\r\n```\r\n[[[8 3 5 5]\r\n  [0 0 0 3]\r\n  [6 9 4 1]\r\n  [2 0 8 8]]\r\n\r\n [[4 6 6 5]\r\n  [5 7 9 3]\r\n  [7 6 1 3]\r\n  [1 6 1 0]]]\r\n[[[8. 5.]\r\n  [9. 8.]]\r\n\r\n [[7. 9.]\r\n  [7. 3.]]]\r\n[[[ 0  2]\r\n  [ 9 14]]\r\n [[ 5  6]\r\n  [ 8 11]]]\r\n```\r\nThis was obtained with 1.9 CPU/GPU tensorflow versions.\r\nI think it would be more convenient to have the same behaviour on CPU and GPU, isn't it ?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Have I written custom code: yes \r\nOS Platform: Ubuntu 16.04 LTS for CPU install and centOS 7 for GPU install\r\n\r\nBut I don't think these informations are necessary ? \r\nThe problem is really that the behaviour of the function is different for tensorflow 1.9 CPU and tensorflow 1.9 GPU. It should be reproducible no matter the CPU/GPU installations I guess.", "@qlzh727 Could you take a look at this?  Is it expected behavior?", "Not sure why I am cc'ed here, if I was showing up from the change history, I guess that was because I was on sync rotation for PRs. Probably should find someone who is the real owner of maxpooling_op.cc", "OK, looks like this code is kind of old, with no real maintainer.  @rmlarsen do you know what's expected here?  I don't see any notice in the documentation that CPU/GPU behavior should differ.", "Any news about this ?\r\n\r\nI do not know if this can help but in fact the documentation of the function is correct for the CPU behaviour. Plus, the function has been available first on GPU and on a later TF version for CPU.\r\nI guess that there was a wrong doc when the function was available only for GPU, and that when it has been later implemented for CPU, it has been inspired from the doc, leading to two different behaviours. ", "Nagging Assignee @poxvoculi: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I'm looking into the issue and I'll try to fix it later. Thank @mpaillassa for your detailed feedback :-)", "@mpaillassa Hi, would you mind taking a test on nightly version? `pip install tf-nightly-gpu`", "@facaiy Hi, I tried the code of my first post on CPU with `pip install tf-nightly` and on GPU with `pip install tf-nightly-gpu`. Both on Ubuntu 16.04 and with CUDA 9.0 and cuDNN 7.3.1 for GPU.\r\nBut I still get the same behaviours: CPU takes into account the batch dimension in the returned indices while GPU does not.  \r\nDid I do something wrong ?", "Thank @mpaillassa for your report, I can reproduce the bug in my experimental PR #23370. \r\n\r\nUnfortunately, there is something wrong with my working station environment, and I cannot find an alternative recently. So I'd like to mark the issue as \"Contributions Welcome\", and feel free to take over it and go head if anyone is interested :-) .  cc @rmlarsen ", "I create the PR #23993 to fix the issue, please take a look, thanks :-)", "Hello, I am new to the tensorflow community. Is this a good issue to get familiar with the tensorflow open-source?", "Welcome, @bono1567! As you see, PR #25241 and #23993 have been in the process to fix the issue. I'll update here when they are merged. "]}, {"number": 22022, "title": "tflite_convert ", "body": "huangwei@ubuntu:~$ tflite_convert --output_file=foo.tflite  --graph_def_file=opt_mnist_graph.pb --input_arrays=the_input_2 --output_arrays=out_2/truediv\r\n2018-09-03 01:34:59.887931: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nTraceback (most recent call last):\r\n  File \"/home/huangwei/.local/lib/python3.5/site-packages/tensorflow/python/framework/importer.py\", line 418, in import_graph_def\r\n    graph._c_graph, serialized, options)  # pylint: disable=protected-access\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: NodeDef expected inputs '' do not match 1 inputs specified; Op<name=Const; signature= -> output:dtype; attr=value:tensor; attr=dtype:type>; NodeDef: batch_normalization_55/cond/batchnorm/add/y = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [] values: 1.1e-05>](batch_normalization_55/cond/Switch)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/huangwei/.local/bin/tflite_convert\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/home/huangwei/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 370, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/home/huangwei/.local/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/home/huangwei/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 366, in run_main\r\n    _convert_model(tflite_flags)\r\n  File \"/home/huangwei/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 94, in _convert_model\r\n    converter = _get_toco_converter(flags)\r\n  File \"/home/huangwei/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 81, in _get_toco_converter\r\n    return converter_fn(**converter_kwargs)\r\n  File \"/home/huangwei/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/lite.py\", line 220, in from_frozen_graph\r\n    _import_graph_def(graph_def, name=\"\")\r\n  File \"/home/huangwei/.local/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/huangwei/.local/lib/python3.5/site-packages/tensorflow/python/framework/importer.py\", line 422, in import_graph_def\r\n    raise ValueError(str(e))\r\nValueError: NodeDef expected inputs '' do not match 1 inputs specified; Op<name=Const; signature= -> output:dtype; attr=value:tensor; attr=dtype:type>; NodeDef: batch_normalization_55/cond/batchnorm/add/y = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [] values: 1.1e-05>](batch_normalization_55/cond/Switch)\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "my model is generated by tf 1.4.0\uff0ci use tf 1.10.0 to convert my model to the tflite, i can use \"freeze_graph.py\" to convert my model, also, can use \"optimize_for_inference.py\" to convert my frozen_graph to pb, but the last step to generate tflite has this bug  ", "i use keras tosave my model use the format of h5, the backend is  tf tensorflow-gpu (1.4.0), the model architecture is densnet+ctc", "Any chance you can attach or share the model? What format did you use when saving the model? Were you using tf 1.10.0 to freeze the graph?", "ValueError: Didn't find custom op for name 'Merge' with version 1\r\nDidn't find custom op for name 'Switch' with version 1\r\nDidn't find custom op for name 'SquaredDifference' with version 1\r\nRegistration failed.\r\n", "Generalized support for Merge and Switch are a ways off, as they're control flow mechanisms, but I've added SquaredDifference to our todo list. It would really help if you could share the model, or how you generated the model, so we can see if there's a way to remove the Merge/Switch ops.", "i have perfectly solved my problems by using orignal tensorflow, i found use keras will generate the ops that is not supported by tflite", "@jdduke ", "Thanks for the feedback @dlml. We're always looking to improve native Keras support, but I'll go ahead and close this for now.", "> i have perfectly solved my problems by using orignal tensorflow, i found use keras will generate the ops that is not supported by tflite\r\n\r\nHow didi you solve the problem @dlml ? I have the same problem. Any help is welcomed", "@Elites2017 you can use tf.keras to train model "]}, {"number": 22021, "title": "fix compile warning: 'message_count' may be used uninitialized in this function", "body": "Fix comile warning like,\r\ntensorflow/core/kernels/encode_proto_op.cc: In member function 'virtual void tensorflow::{anonymous}::EncodeProtoOp::Compute(tensorflow::OpKernelContext*)':\r\ntensorflow/core/kernels/encode_proto_op.cc:586:5: warning: 'message_count' may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     for (int message_index = 0; message_index < message_count;\r\n     ^\r\ntensorflow/core/kernels/encode_proto_op.cc:519:9: note: 'message_count' was declared here\r\n     int message_count;", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "Looks like there is no CLA for you.\r\nPlease follow the comments by googlebot to sign the cla for your github account, then reopen the PR."]}, {"number": 22020, "title": "What is the difference b.t. tf.layers.dense and tf.layers.Dense", "body": "Hey guys,\r\n\r\nJust as the title. I tried to search the Stack Overflow but I can't figure it out. I mean, how to choose the layer b.t. dense and Dense when I construct a neural network?   \r\n\r\n\r\nPlease go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["https://github.com/tensorflow/tensorflow/blob/9fa3d27c4bdd534eaff88ea2c4a7119e3174dbbf/tensorflow/python/layers/core.py#L32-L33\r\n\r\n`tf.layers.Dense` returns an instance of `class Dense`.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/9fa3d27c4bdd534eaff88ea2c4a7119e3174dbbf/tensorflow/python/layers/core.py#L116-L117\r\nhttps://github.com/tensorflow/tensorflow/blob/9fa3d27c4bdd534eaff88ea2c4a7119e3174dbbf/tensorflow/python/layers/core.py#L174-L188\r\n\r\n`tf.layers.dense` first creates an instance of `tf.layers.Dense`, then calls `return layer.apply(inputs)`, which returns a tensor.\r\n\r\n----\r\n\r\nThe same goes for:\r\n- `tf.layers.Dropout` and `tf.layers.dropout`\r\n- `tf.layers.Flatten` and `tf.layers.flatten`\r\n- `tf.layers.AveragePooling1D` and `tf.layers.average_pooling1d`\r\n- and many more ... (refer to the code)"]}, {"number": 22019, "title": "[Bug] ImportError: cannot import name 'checkpoint_management' tf1.10", "body": "In tf1.10, when I run freeze_graph, it comes error with msg above.\r\n\r\nI cloned source file from github and tried it different way and still have same issues.\r\n\r\nThanks for your help\r\n\r\n File \"freeze_graph.py\", line 20, in <module>\r\n    from tensorflow.python.training import checkpoint_management\r\nImportError: cannot import name 'checkpoint_management'\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MAC & Ubuntu\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: tf1.10\r\n- **Python version**: 3.6\r\n- **Exact command to reproduce**:\r\npython ~/tensorflow/tensorflow/python/tools/freeze_graph.py\r\n\r\n", "comments": ["same problem", "Same problem. I get the following error in both tensor flow 1.10.1 and 1.8.0\r\n\r\n  File \"freeze_graph.py\", line 58, in <module>\r\n    from tensorflow.python.training import checkpoint_management\r\nImportError: cannot import name 'checkpoint_management'\r\n", "Same problem. ", "Same problem", "commant \r\nfrom tensorflow.python.training import checkpoint_management\r\nadd\r\nimport tensorflow as tf\r\n\r\nchange line 119\r\nnot checkpoint_management.checkpoint_exists(input_checkpoint)):\r\nto\r\nnot tf.train.checkpoint_exists(input_checkpoint)):", "same problem", "same problem\r\n\r\nEDIT: @JunhyukHyun suggestion fixed it for me - thanks!", "@JunhyukHyun Thank you for finding the fix. Would you mind submitting a PR with this code change?", "@JunhyukHyun I edited the freeze_graph.py, however, it introduced a new error: \r\n\r\n      File \"/Users/cvsanbuenaventura/tensorflow/tensorflow/python/tools/freeze_graph.py\", line 365, in freeze_graph\r\n    checkpoint_version=checkpoint_version)\r\n      File \"/Users/cvsanbuenaventura/tensorflow/tensorflow/python/tools/freeze_graph.py\", line 141, in freeze_graph_with_def_protos\r\n    for node in input_meta_graph_def.graph_def.node:\r\n    AttributeError: 'int' object has no attribute 'graph_def'", "@aselle Can you help here?", "Any update here, I am hitting the same issue trying to follow: https://github.com/movidius/ncappzoo/tree/master/tensorflow/mobilenets\r\n", "Fixed this issue with the following.\r\n\r\n`tensorflow/python/training/checkpoint_management` function `checkpoint_exists` is exported as `train.checkpoint_exists`, so I made the following changes to `tensorflow/python/tools/freeze_graph.py`:\r\n\r\n```diff\r\n# Change import\r\n-   from tensorflow.python.training import checkpoint_management\r\n+   from tensorflow.train import checkpoint_exists\r\n...\r\n# Change call\r\n-   not checkpoint_management.checkpoint_exists(input_checkpoint)):\r\n+   not checkpoint_exists(input_checkpoint)):\r\n```\r\n\r\nI ran `tensorflow/tools/ci_build/ci_build.sh CPU bazel test //tensorflow/python/tools:freeze_graph_test` and it checked out.\r\n\r\nIf it works for those of you having issues, I'll put it into a PR.", "If None of the above works for you, copy this code into freeze_graph.py for a Short Term Fix\r\n# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n# ==============================================================================\r\n\"\"\"Converts checkpoint variables into Const ops in a standalone GraphDef file.\r\n\r\nThis script is designed to take a GraphDef proto, a SaverDef proto, and a set of\r\nvariable values stored in a checkpoint file, and output a GraphDef with all of\r\nthe variable ops converted into const ops containing the values of the\r\nvariables.\r\n\r\nIt's useful to do this when we need to load a single file in C++, especially in\r\nenvironments like mobile or embedded where we may not have access to the\r\nRestoreTensor ops and file loading calls that they rely on.\r\n\r\nAn example of command-line usage is:\r\nbazel build tensorflow/python/tools:freeze_graph && \\\r\nbazel-bin/tensorflow/python/tools/freeze_graph \\\r\n--input_graph=some_graph_def.pb \\\r\n--input_checkpoint=model.ckpt-8361242 \\\r\n--output_graph=/tmp/frozen_graph.pb --output_node_names=softmax\r\n\r\nYou can also look at freeze_graph_test.py for an example of how to use it.\r\n\r\n\"\"\"\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport argparse\r\nimport sys\r\n\r\nfrom google.protobuf import text_format\r\n\r\nfrom tensorflow.core.framework import graph_pb2\r\nfrom tensorflow.core.protobuf import saver_pb2\r\nfrom tensorflow.python import pywrap_tensorflow\r\nfrom tensorflow.python.client import session\r\nfrom tensorflow.python.framework import graph_util\r\nfrom tensorflow.python.framework import importer\r\nfrom tensorflow.python.platform import app\r\nfrom tensorflow.python.platform import gfile\r\nfrom tensorflow.python.training import saver as saver_lib\r\n\r\nFLAGS = None\r\n\r\n\r\ndef freeze_graph(input_graph,\r\n                 input_saver,\r\n                 input_binary,\r\n                 input_checkpoint,\r\n                 output_node_names,\r\n                 restore_op_name,\r\n                 filename_tensor_name,\r\n                 output_graph,\r\n                 clear_devices,\r\n                 initializer_nodes,\r\n                 variable_names_blacklist=\"\"):\r\n  \"\"\"Converts all variables in a graph and checkpoint into constants.\"\"\"\r\n\r\n  del restore_op_name, filename_tensor_name  # Unused by updated loading code.\r\n\r\n  if not gfile.Exists(input_graph):\r\n    print(\"Input graph file '\" + input_graph + \"' does not exist!\")\r\n    return -1\r\n\r\n  if input_saver and not gfile.Exists(input_saver):\r\n    print(\"Input saver file '\" + input_saver + \"' does not exist!\")\r\n    return -1\r\n\r\n  # 'input_checkpoint' may be a prefix if we're using Saver V2 format\r\n  if not saver_lib.checkpoint_exists(input_checkpoint):\r\n    print(\"Input checkpoint '\" + input_checkpoint + \"' doesn't exist!\")\r\n    return -1\r\n\r\n  if not output_node_names:\r\n    print(\"You need to supply the name of a node to --output_node_names.\")\r\n    return -1\r\n\r\n  input_graph_def = graph_pb2.GraphDef()\r\n  mode = \"rb\" if input_binary else \"r\"\r\n  with gfile.FastGFile(input_graph, mode) as f:\r\n    if input_binary:\r\n      input_graph_def.ParseFromString(f.read())\r\n    else:\r\n      text_format.Merge(f.read(), input_graph_def)\r\n  # Remove all the explicit device specifications for this node. This helps to\r\n  # make the graph more portable.\r\n  if clear_devices:\r\n    for node in input_graph_def.node:\r\n      node.device = \"\"\r\n\r\n  _ = importer.import_graph_def(input_graph_def, name=\"\")\r\n\r\n  with session.Session() as sess:\r\n    if input_saver:\r\n      with gfile.FastGFile(input_saver, mode) as f:\r\n        saver_def = saver_pb2.SaverDef()\r\n        if input_binary:\r\n          saver_def.ParseFromString(f.read())\r\n        else:\r\n          text_format.Merge(f.read(), saver_def)\r\n        saver = saver_lib.Saver(saver_def=saver_def)\r\n        saver.restore(sess, input_checkpoint)\r\n    else:\r\n      var_list = {}\r\n      reader = pywrap_tensorflow.NewCheckpointReader(input_checkpoint)\r\n      var_to_shape_map = reader.get_variable_to_shape_map()\r\n      for key in var_to_shape_map:\r\n        try:\r\n          tensor = sess.graph.get_tensor_by_name(key + \":0\")\r\n        except KeyError:\r\n          # This tensor doesn't exist in the graph (for example it's\r\n          # 'global_step' or a similar housekeeping element) so skip it.\r\n          continue\r\n        var_list[key] = tensor\r\n      saver = saver_lib.Saver(var_list=var_list)\r\n      saver.restore(sess, input_checkpoint)\r\n      if initializer_nodes:\r\n        sess.run(initializer_nodes)\r\n\r\n    variable_names_blacklist = (variable_names_blacklist.split(\",\") if\r\n                                variable_names_blacklist else None)\r\n    output_graph_def = graph_util.convert_variables_to_constants(\r\n        sess,\r\n        input_graph_def,\r\n        output_node_names.split(\",\"),\r\n        variable_names_blacklist=variable_names_blacklist)\r\n\r\n  with gfile.GFile(output_graph, \"wb\") as f:\r\n    f.write(output_graph_def.SerializeToString())\r\n  print(\"%d ops in the final graph.\" % len(output_graph_def.node))\r\n\r\n\r\ndef main(unused_args):\r\n  freeze_graph(FLAGS.input_graph, FLAGS.input_saver, FLAGS.input_binary,\r\n               FLAGS.input_checkpoint, FLAGS.output_node_names,\r\n               FLAGS.restore_op_name, FLAGS.filename_tensor_name,\r\n               FLAGS.output_graph, FLAGS.clear_devices, FLAGS.initializer_nodes,\r\n               FLAGS.variable_names_blacklist)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n  parser = argparse.ArgumentParser()\r\n  parser.register(\"type\", \"bool\", lambda v: v.lower() == \"true\")\r\n  parser.add_argument(\r\n      \"--input_graph\",\r\n      type=str,\r\n      default=\"\",\r\n      help=\"TensorFlow \\'GraphDef\\' file to load.\")\r\n  parser.add_argument(\r\n      \"--input_saver\",\r\n      type=str,\r\n      default=\"\",\r\n      help=\"TensorFlow saver file to load.\")\r\n  parser.add_argument(\r\n      \"--input_checkpoint\",\r\n      type=str,\r\n      default=\"\",\r\n      help=\"TensorFlow variables file to load.\")\r\n  parser.add_argument(\r\n      \"--output_graph\",\r\n      type=str,\r\n      default=\"\",\r\n      help=\"Output \\'GraphDef\\' file name.\")\r\n  parser.add_argument(\r\n      \"--input_binary\",\r\n      nargs=\"?\",\r\n      const=True,\r\n      type=\"bool\",\r\n      default=False,\r\n      help=\"Whether the input files are in binary format.\")\r\n  parser.add_argument(\r\n      \"--output_node_names\",\r\n      type=str,\r\n      default=\"\",\r\n      help=\"The name of the output nodes, comma separated.\")\r\n  parser.add_argument(\r\n      \"--restore_op_name\",\r\n      type=str,\r\n      default=\"save/restore_all\",\r\n      help=\"The name of the master restore operator.\")\r\n  parser.add_argument(\r\n      \"--filename_tensor_name\",\r\n      type=str,\r\n      default=\"save/Const:0\",\r\n      help=\"The name of the tensor holding the save path.\")\r\n  parser.add_argument(\r\n      \"--clear_devices\",\r\n      nargs=\"?\",\r\n      const=True,\r\n      type=\"bool\",\r\n      default=True,\r\n      help=\"Whether to remove device specifications.\")\r\n  parser.add_argument(\r\n      \"--initializer_nodes\",\r\n      type=str,\r\n      default=\"\",\r\n      help=\"comma separated list of initializer nodes to run before freezing.\")\r\n  parser.add_argument(\r\n      \"--variable_names_blacklist\",\r\n      type=str,\r\n      default=\"\",\r\n      help=\"\"\"\\\r\n      comma separated list of variables to skip converting to constants\\\r\n      \"\"\")\r\n  FLAGS, unparsed = parser.parse_known_args()\r\n  app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n\r\n", "Tensorflow, Are you guys seriously ignore this issues and nobody cares about it? ", "@aiscientist  Sorry it fell through the crack.\r\n@gargn Could you please take a look at this bug?\r\n\r\n", "@aiscientist, did @IMBurbank's suggestion work for you?", "Let me know if the proposed solution is useful to those of you struggling with this issue. I would be happy to put it into a PR.", "> @aiscientist, did @IMBurbank's suggestion work for you?\r\n\r\nYes, it works for me. But please update for this on the default version.", "> Let me know if the proposed solution is useful to those of you struggling with this issue. I would be happy to put it into a PR.\r\n\r\nIt works, thanks for your help, glad if this work on the default version so the others won't suffer from this issue.", "This is a bit of a stream-of-consciousness response. I slowly typed it as I dug back into this issue.\r\n\r\nFirst, I decided to go back and confirm your error in 1.10 - and if it still existed in the current 1.11. \r\n\r\nI cloned the tensorflow repo, entered it and built a current dockerfile\r\n```bash\r\ndocker build -f tensorflow/tools/dockerfiles/dockerfiles/nvidia.Dockerfile -t tf tensorflow/tools/dockerfiles/\r\n\r\ndocker run --rm --runtime=nvidia -u $(id -u):$(id -g) -v $(pwd):/my-devel -it tf\r\n```\r\n\r\nI entered the directory and ran `freeze_graph.py`\r\n```bash\r\ncd my-devel/\r\npython3 tensorflow/python/tools/freeze_graph.py\r\n```\r\n\r\nAnd received the expected message\r\n```\r\nInput checkpoint '' doesn't exist!\r\n```\r\n\r\nI added a `TF_PACKAGE` `build-arg` the `nvidia.Dockerfile` to install tensorflow 1.10.1 and rebuilt.\r\n```bash\r\ndocker build -f tensorflow/tools/dockerfiles/dockerfiles/nvidia.Dockerfile -t tf --build-arg TF_PACKAGE=tensorflow-gpu==1.10.1 tensorflow/tools/dockerfiles/\r\n\r\ndocker run --rm --runtime=nvidia -u $(id -u):$(id -g) -v $(pwd):/my-devel -it tf\r\n```\r\nThis time running gave the same error originally reported.\r\n```bash\r\nTraceback (most recent call last):\r\n  File \"tensorflow/python/tools/freeze_graph.py\", line 58, in <module>\r\n    from tensorflow.python.training import checkpoint_management\r\nImportError: cannot import name 'checkpoint_management'\r\n```\r\n\r\nThe changes I proposed above fixed the issue.\r\n\r\nNext, I started digging through the diff between 1.10 and 1.11 branches... \r\n\r\nI was hoping to find the specific fix that was applied between 1.10 and 1.11 in case it could be used to patch 1.10. It was bugging me that I hadn't tracked down how it was fixed in 1.11.\r\n\r\nWhile looking through the `freeze_graph.py`commit history, it slowly dawned on me that the `checkpoint_management` system changed significantly between 1.10 and 1.11. So much so that `checkpoint_managment.py` didn't exist when 1.10 was released. \r\n\r\nI decided that the problem might be an issue of calling `freeze_graph.py` in a local uncompiled branch that was newer than the 1.10 binary in the python lib. \r\n\r\nI switched the the git r1.10 branch. Then started back up the container with tf-1.10.1 installed.\r\n```bash\r\ngit checkout r1.10\r\ndocker run --rm --runtime=nvidia -u $(id -u):$(id -g) -v $(pwd):/my-devel -it tf\r\n```\r\nAnd ran `free_graph.py` using tensorflow 1.10.1 on the r1.10 branch - it worked!\r\n\r\nThe fix I originally proposed is only papering over this compatibility issue by pointing to the `tf_export`'ed `checkpoint_exist` module made available in the python API.\r\n\r\n----\r\n\r\nTo be honest - I was excited about putting in another PR, but I don't think it will help here.\r\n\r\nI should have noticed this last time I looked at the issue. If you want to run files in the uncompiled tensorflow project with a legacy tensorflow version installed locally, make sure to checkout the branch that matches the local version.\r\n\r\nThe issue can also be avoided by updating to the current release locally, or doing development using the tensorflow dockerfiles to ensure a supported environment.\r\n\r\nIf I missed something, please let me know.", "adopt JunhyukHyun methon, it is ok\r\nadopt Isaac Burbank  proposed solution, it raises question?\r\n\r\npython tensorflow/python/tools/freeze_graph.py \r\nTraceback (most recent call last):\r\n  File \"tensorflow/python/tools/freeze_graph.py\", line 60, in <module>\r\n    from tensorflow.train import checkpoint_management\r\nImportError: No module named 'tensorflow.train'", "@zkl99999 What version of Tensorflow do you have installed, and which branch are you using to run `freeze_graph.py`?", "Reference the solution provided by @IMBurbank (https://github.com/tensorflow/tensorflow/issues/22019#issuecomment-425378974) in order to freeze a graph in TensorFlow 1.10. Closing this bug since there is a workaround provided.", "> @zkl99999 What version of Tensorflow do you have installed, and which branch are you using to run `freeze_graph.py`?\r\n\r\n\r\n\r\n> Fixed this issue with the following.\r\n> \r\n> `tensorflow/python/training/checkpoint_management` function `checkpoint_exists` is exported as `train.checkpoint_exists`, so I made the following changes to `tensorflow/python/tools/freeze_graph.py`:\r\n> \r\n> ```diff\r\n> # Change import\r\n> -   from tensorflow.python.training import checkpoint_management\r\n> +   from tensorflow.train import checkpoint_exists\r\n> ...\r\n> # Change call\r\n> -   not checkpoint_management.checkpoint_exists(input_checkpoint)):\r\n> +   not checkpoint_exists(input_checkpoint)):\r\n> ```\r\n> \r\n> I ran `tensorflow/tools/ci_build/ci_build.sh CPU bazel test //tensorflow/python/tools:freeze_graph_test` and it checked out.\r\n> \r\n> If it works for those of you having issues, I'll put it into a PR.\r\n\r\nHi, it still not work using this solution, and it just report: Traceback (most recent call last):\r\n  File \"/home/stella/tensorflow/tensorflow/python/tools/freeze_graph.py\", line 58, in <module>\r\n    from tensorflow.train import checkpoint_exists\r\nImportError: No module named 'tensorflow.train'\r\nI use tf 1.4.0. Can you help me, thx!", "I encountered the same problem. @Stellaxx24", "\u8fd9\u4e2a\u548ctensorflow\u7684\u7248\u672c\u6709\u5173\u7cfb \u4f60\u5b89\u88c51.11.0\u8bd5\u8bd5\r\n\r\n\u83b7\u53d6 Outlook for iOS<https://aka.ms/o0ukef>\r\n________________________________\r\n\u53d1\u4ef6\u4eba: ZHANG ZHAOXIANG <notifications@github.com>\r\n\u53d1\u9001\u65f6\u95f4: Sunday, May 5, 2019 7:45:01 PM\r\n\u6536\u4ef6\u4eba: tensorflow/tensorflow\r\n\u6284\u9001: Stellaxx24; Mention\r\n\u4e3b\u9898: Re: [tensorflow/tensorflow] [Bug] ImportError: cannot import name 'checkpoint_management' tf1.10 (#22019)\r\n\r\n\r\nI encountered the same problem. @Stellaxx24<https://github.com/Stellaxx24>\r\n\r\n\u2015\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/22019#issuecomment-489409184>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AKQJ7Y7XZXWEFR3CZPOHSETPT2UBZANCNFSM4FS4F36Q>.\r\n"]}, {"number": 22018, "title": "TensorflowLite compile error on Android", "body": "I can't compile apk file when trying to use Firebase MLKIT using Tensorflowlite. I see this error when compiling:\r\n\r\nThe last line says: ```<br>...while parsing org/tensorflow/lite/DataType$1.class<br>1 error; aborting```\r\n\r\n```\r\ndx tool failed:<br>UNEXPECTED TOP-LEVEL EXCEPTION:<br>com.android.dx.cf.iface.ParseException: bad class file magic (cafebabe) or version (0034.0000)<br>\t\r\nat com.android.dx.cf.direct.DirectClassFile.parse0(DirectClassFile.java:472)<br>\t\r\nat com.android.dx.cf.direct.DirectClassFile.parse(DirectClassFile.java:406)<br>\t\r\nat com.android.dx.cf.direct.DirectClassFile.parseToInterfacesIfNecessary(DirectClassFile.java:388)<br>\t\r\nat com.android.dx.cf.direct.DirectClassFile.getMagic(DirectClassFile.java:251)<br>\t\r\nat com.android.dx.command.dexer.Main.processClass(Main.java:709)<br>\t\r\nat com.android.dx.command.dexer.Main.processFileBytes(Main.java:678)<br>\t\r\nat com.android.dx.command.dexer.Main.access$300(Main.java:83)<br>\t\r\nat com.android.dx.command.dexer.Main$1.processFileBytes(Main.java:607)<br>\t\r\nat com.android.dx.cf.direct.ClassPathOpener.processArchive(ClassPathOpener.java:284)<br>\t\r\nat com.android.dx.cf.direct.ClassPathOpener.processOne(ClassPathOpener.java:166)<br>\t\r\nat com.android.dx.cf.direct.ClassPathOpener.process(ClassPathOpener.java:144)<br>\t\r\nat com.android.dx.command.dexer.Main.processOne(Main.java:637)<br>\t\r\nat com.android.dx.command.dexer.Main.processAllFiles(Main.java:506)<br>\t\r\nat com.android.dx.command.dexer.Main.runMultiDex(Main.java:335)<br>\t\r\nat com.android.dx.command.dexer.Main.run(Main.java:245)<br>\t\r\nat com.android.dx.command.dexer.Main.main(Main.java:215)<br>\t\r\nat com.android.dx.command.Main.main(Main.java:106)<br>\r\n...while parsing org/tensorflow/lite/DataType$1.class<br>1 error; aborting\r\n```\r\n\r\nWe're on a Mac, JDK version is 1.8.0_152 and compiling with IntelliJ IDE. Any thought?", "comments": ["SOLVED. This was a problem with AdobeAIR SDK. Not related to tensorflow. cheers.\r\nhttps://tracker.adobe.com/#/view/AIR-4198726  "]}, {"number": 22017, "title": "Additional arguments (vmin, vmax, and clip) for summary.image", "body": "This PR adds the additional arguments (`vmin`, `vmax`, and `clip`) to `summary.image` for the request by [the issue-12883](https://github.com/tensorflow/tensorflow/issues/12883).", "comments": ["@alextp Could you please help review this PR when you have time?", "@nfelt can you review this?", "@nfelt Could you help review this PR when you have time?", "@feihugis Thank you for sending the PR.  Unfortunately, there was a miscommunication on our part - our plans for the `tf.summary.image()` op are actually to keep it narrowly focused on the minimum necessary for summary export (i.e. PNG conversion), and thus not add further normalization controls.\r\n\r\nIn fact, for TF 2.0, we are planning on reducing the normalization provided by the op to just do [`tf.image.convert_image_dtype(tensor, tf.uint8, saturate=True)`](https://www.tensorflow.org/api_docs/python/tf/image/convert_image_dtype).  With that change, float tensors will always be scaled according to an expected range of `[0.0, 1.0)`, so it will no longer adjust the scaling range based on the input data max and min values.  This brings it into consistency with all the various `tf.image.*` ops which only support `tf.image.convert_image_dtype()` as an input conversion.\r\n\r\nI understand that it adds some convenience to be able to use the op's built-in normalization for a wider range of numerical inputs, but ultimately it doesn't scale well to parameterize the op so that the normalization can be fine-tuned.  So for TF 2.0 we're doing only the most minimal conversion and recommending that users who need more than that do their own explicit normalization.", "@nfelt Thanks for your reply! Good to know these plans. It makes sense to reduce the normalization provided by the op. ", "@feihugis thanks for understanding!  I will close this and comment with an explanation on the original issue."]}, {"number": 22016, "title": "tf.image.resize_images cannot be applied to NCHW format directly?", "body": "Referencing TensorFlow's Performance Guide, It seems NCHW data format is better than NHMC data format in many situations.\r\nSome existing models has already been implemented in NCHW data format, especially using keras, but tf.image.resize_images cannot be applied to NCHW format directly, I think.\r\nAre there any way excepting that we implement by ourselves?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@S-aiueo32  You can use ` tf.transpose`", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "I have also encounter this problem. `tf.transpose` works but it make the model much more slower. Using trace of MTCNN network, look like 'tf.transpose' take 40% of time. So for me this is still probemalic to use resize operation with NHWC format"]}, {"number": 22015, "title": "Fix minor typos", "body": "Fix minor typos", "comments": []}, {"number": 22014, "title": "tf.assign - inputs not compatible with expected types - misleading error message", "body": "-----------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nyes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\n\r\nTF checkpoint I have built\r\n```\r\n/tmp/tensorflow# git log   \r\ncommit 09792df012c22622324f085f46edde33006c7355\r\nAuthor: A. Unique TensorFlower <gardener@tensorflow.org>\r\nDate:   Sun Aug 26 02:07:11 2018 -0700\r\n\r\n    compat: Update forward compatibility horizon to 2018-08-26\r\n    \r\n    PiperOrigin-RevId: 210266798\r\n```\r\n\r\n```\r\n== cat /etc/issue ===============================================\r\nLinux 3bed2f328777 4.13.0-38-generic #43~16.04.1-Ubuntu SMP Wed Mar 14 17:48:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.5 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nYes\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux 3bed2f328777 4.13.0-38-generic #43~16.04.1-Ubuntu SMP Wed Mar 14 17:48:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.14.5)\r\nprotobuf (3.6.1)\r\ntensorflow (1.10.0)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.10.0\r\ntf.GIT_VERSION = b'unknown'\r\ntf.COMPILER_VERSION = b'unknown'\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /usr/local/cuda/lib64/stubs:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64/stubs:/usr/local/cuda/lib64/stubs:/usr/local/cuda/extras/CUPTI/lib64:/lib/amd64/server/:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server:/opt/boost/lib:/opt/conda/lib/:/usr/local/cuda/lib64/:/opt/conda/lib/R/lib/:/usr/local/nvidia/lib64/:/usr/local/nvidia/lib:/lib/x86_64-linux-gnu:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu:/opt/opencv/lib\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nWed Aug 29 19:57:14 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 396.26                 Driver Version: 396.26                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 108...  Off  | 00000000:01:00.0  On |                  N/A |\r\n|  0%   41C    P0    76W / 250W |    708MiB / 11177MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart.so.9.2.148\r\n/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart_static.a\r\n\r\n```\r\n**Bazel** version\r\n```\r\n$ bazel version\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nBuild label: 0.16.0\r\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Tue Jul 31 17:01:24 2018 (1533056484)\r\nBuild timestamp: 1533056484\r\nBuild timestamp as int: 1533056484\r\n```\r\n\r\n**CUDNN** version:\r\n```\r\n$ nvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2018 NVIDIA Corporation\r\nBuilt on Tue_Jun_12_23:07:04_CDT_2018\r\nCuda compilation tools, release 9.2, V9.2.148\r\n```\r\n\r\n**GPU**: GEFORCE GTX 1080Ti, 11GB, GIGABYTE AORUS\r\n\r\n- **Exact command to reproduce**:\r\n\r\n```\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport tensorflow as tf\r\n\r\ndef scope_1():\r\n    print(\"DS1 SCOPE =============\")\r\n    with tf.variable_scope(\"scope_1\", reuse=tf.AUTO_REUSE):\r\n        x = tf.get_variable(\"x\", initializer=lambda: tf.zeros(shape=[], dtype=tf.float32), dtype=tf.float32\r\n                               , trainable=False, use_resource=True)             \r\n        print(\"graph: {}\".format(x.graph))\r\n        print(\"scope: {}\".format(tf.get_variable_scope().name))\r\n        print(\" name: {}\".format(x.name))\r\n        print(\"  var: {}\".format(str(x)))\r\n        current_scope = tf.get_variable_scope()       \r\n        assign_one = tf.assign(x, 1.0, name=\"x_is_one\")\r\n    \r\n    def scope_2(inputs, label):        \r\n        print(\"initial scope: {}\".format(tf.get_variable_scope().name))\r\n        print(\"DS1 SCOPE =============\")\r\n        #with tf.variable_scope(\"scope_1\", reuse=tf.AUTO_REUSE):\r\n        with tf.variable_scope(current_scope, reuse=tf.AUTO_REUSE):\r\n            y = tf.get_variable(\"x\", initializer=lambda: tf.zeros(shape=[], dtype=tf.float32), dtype=tf.float32\r\n                                   , trainable=False)#, use_resource=True)         \r\n            print(\"graph: {}\".format(y.graph))\r\n            print(\"scope: {}\".format(tf.get_variable_scope().name))\r\n            print(\" name: {}\".format(y.name))\r\n            print(\"  var: {}\".format(str(y)))\r\n            print(\"=============\")\r\n            print(y)\r\n            print(inputs)\r\n            #assign_two = tf.assign(y, tf.add(tf.cast(inputs, dtype=tf.float32),1.0), name=\"inputs_plus_1\")\r\n            assign_two = tf.identity(tf.assign(y, tf.add(tf.cast(inputs, dtype=tf.float32), 1.0, name=\"first_assign\")))\r\n            with tf.control_dependencies([assign_two]):\r\n                #with tf.control_dependencies([tf.scatter_nd_update(y, [[0]], [0.22])]):\r\n                return y.read_value(), label\r\n            #return x,label\r\n    \r\n    # test that original x is mutable\r\n    with tf.control_dependencies([assign_one]):\r\n        dataset = (tf.data.Dataset.from_tensor_slices(([1,2,3,4,5], [-1,-2,-3,-4,-5]))\r\n                    .map(scope_2)\r\n                    .batch(1)\r\n                    .repeat(1)        \r\n                    )\r\n    return dataset\r\n    \r\n                \r\nwith tf.variable_scope(\"scope_0\"):\r\n        dataset_fn = scope_1()\r\n\r\nwith tf.variable_scope(\"iterator\"):\r\n    # Define iterator from_string_handle. In general it is useful to have\r\n    # this kind of iterator if one wants to switch between train and validation\r\n    # within the training loop.        \r\n    iterator_t = dataset_fn.make_initializable_iterator()\r\n    iterator_handle = tf.placeholder(tf.string, shape=[], name=\"iterator_handle\")\r\n    iterator = tf.data.Iterator.from_string_handle(iterator_handle, \r\n                                                iterator_t.output_types,\r\n                                                iterator_t.output_shapes)\r\n    \r\n    def get_next_item():\r\n        next_elem = iterator.get_next(name=\"next_element\")\r\n        x, y = tf.cast(next_elem[0], tf.float32), next_elem[1]# tf.cast(next_elem[1], tf.int32)\r\n        return x, y    \r\nwith tf.Session(config=tf.ConfigProto(device_count={'GPU': 0})) as sess:\r\n\r\n    sess.run([tf.global_variables_initializer(),tf.local_variables_initializer()])\r\n    handle_t = sess.run(iterator_t.string_handle())\r\n    # Run data iterator initialisation\r\n    sess.run(iterator_t.initializer)\r\n    print(sess.graph.get_operations()) \r\n    while True:\r\n        try:\r\n            print(sess.run(get_next_item(), feed_dict={iterator_handle:handle_t}))\r\n        except tf.errors.OutOfRangeError:\r\n                        print(\"End of training dataset.\")\r\n                        break        \r\n    print()\r\n    print(\"global vars: {}\".format(tf.global_variables()))\r\n    print(\"local vars: {}\".format(tf.local_variables()))\r\n    print(tf.get_default_graph().get_name_scope())\r\n```\r\n### Describe the problem\r\nWhen using `tf.get_variable` inside function called in `dataset.map()` if it is without `use_resource=True`, which the original variable has, the assign statement gives an error\r\n```\r\nTypeError: In op 'scope_1/Assign', input types ([tf.float32, tf.float32]) are not compatible with expected types ([tf.float32_ref, tf.float32])\r\n```\r\nalthough the variables used in the assign statement have expected types\r\n```\r\n<tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32>\r\n<tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32_ref>\r\n```\r\nWhen passing `use_resource=True` to `tf.get_variable` within `def scope_2` they become \r\n```\r\n<tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32>\r\n<tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32>\r\n```\r\nand the whole code works as expected. \r\n\r\nI think the error message in the first case is misleading, because variables have expected types and the whole thing fails. Maybe the input types and expected types in the error msg should be swapped?\r\n\r\nAlso, as far as I understand `trainable` and `use_resource` flags should be inferred from the original variable when using `tf.get_variable`, so different behavior dependent on the use_resource flag makes me even more confused.\r\n\r\n### Source code / logs\r\nRUN 1 - without `use_resource=True`\r\n```\r\nDS1 SCOPE =============\r\ngraph: <tensorflow.python.framework.ops.Graph object at 0x7ffb011d4390>\r\nscope: scope_0/scope_1\r\n name: scope_0/scope_1/x:0\r\n  var: <tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32>\r\ninitial scope: \r\nDS1 SCOPE =============\r\ngraph: <tensorflow.python.framework.ops.Graph object at 0x7ffb011d4390>\r\nscope: scope_0/scope_1\r\n name: scope_0/scope_1/x_1:0\r\n  var: <tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32_ref>\r\n=============\r\n<tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32_ref>\r\nTensor(\"arg0:0\", shape=(), dtype=int32)\r\nTraceback (most recent call last):\r\n  File \"bug.py\", line 51, in <module>\r\n    dataset_fn = scope_1()\r\n  File \"bug.py\", line 43, in scope_1\r\n    .map(scope_2)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1005, in map\r\n    return MapDataset(self, map_func)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 2216, in __init__\r\n    map_func, \"Dataset.map()\", input_dataset)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1473, in __init__\r\n    self._function.add_to_graph(ops.get_default_graph())\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 479, in add_to_graph\r\n    self._create_definition_if_needed()\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 335, in _create_definition_if_needed\r\n    self._create_definition_if_needed_impl()\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 344, in _create_definition_if_needed_impl\r\n    self._capture_by_value, self._caller_device)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 865, in func_graph_from_py_func\r\n    outputs = func(*func_graph.inputs)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1411, in tf_data_structured_function_wrapper\r\n    ret = func(*nested_args)\r\n  File \"bug.py\", line 34, in scope_2\r\n    assign_two = tf.identity(tf.assign(y, tf.add(tf.cast(inputs, dtype=tf.float32), 1.0, name=\"first_assign\")))\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/state_ops.py\", line 221, in assign\r\n    validate_shape=validate_shape)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 61, in assign\r\n    use_locking=use_locking, name=name)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 738, in create_op\r\n    **kwargs)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3254, in create_op\r\n    op_def=op_def)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1728, in __init__\r\n    input_types))\r\nTypeError: In op 'scope_1/Assign', input types ([tf.float32, tf.float32]) are not compatible with expected types ([tf.float32_ref, tf.float32])\r\n```\r\n\r\nRUN 2 - with `use_resource=True`\r\n```\r\nDS1 SCOPE =============\r\ngraph: <tensorflow.python.framework.ops.Graph object at 0x7f61af7de390>\r\nscope: scope_0/scope_1\r\n name: scope_0/scope_1/x:0\r\n  var: <tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32>\r\ninitial scope: \r\nDS1 SCOPE =============\r\ngraph: <tensorflow.python.framework.ops.Graph object at 0x7f61af7de390>\r\nscope: scope_0/scope_1\r\n name: scope_0/scope_1/x_1:0\r\n  var: <tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32>\r\n=============\r\n<tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32>\r\nTensor(\"arg0:0\", shape=(), dtype=int32)\r\n2018-09-02 21:07:18.255844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-09-02 21:07:18.257387: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.721\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 10.92GiB freeMemory: 10.23GiB\r\n2018-09-02 21:07:18.258168: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0\r\n2018-09-02 21:07:18.258274: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-09-02 21:07:18.258631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 \r\n2018-09-02 21:07:18.258993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N \r\n2018-09-02 21:07:18.272483: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n[<tf.Operation 'scope_0/scope_1/x/Initializer/zeros' type=Const>, <tf.Operation 'scope_0/scope_1/x' type=VarHandleOp>, <tf.Operation 'scope_0/scope_1/x/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp>, <tf.Operation 'scope_0/scope_1/x/Assign' type=AssignVariableOp>, <tf.Operation 'scope_0/scope_1/x/Read/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'scope_0/scope_1/Const' type=Const>, <tf.Operation 'scope_0/scope_1/x_is_one' type=AssignVariableOp>, <tf.Operation 'scope_0/scope_1/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'scope_0/tensors/component_0' type=Const>, <tf.Operation 'scope_0/tensors/component_1' type=Const>, <tf.Operation 'scope_0/scope_1/x_1/Initializer/zeros' type=Const>, <tf.Operation 'scope_0/scope_1/x_1' type=VarHandleOp>, <tf.Operation 'scope_0/scope_1/x_1/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp>, <tf.Operation 'scope_0/scope_1/x_1/Assign' type=AssignVariableOp>, <tf.Operation 'scope_0/scope_1/x_1/Read/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'scope_0/batch_size' type=Const>, <tf.Operation 'scope_0/drop_remainder' type=Const>, <tf.Operation 'scope_0/count' type=Const>, <tf.Operation 'iterator/IteratorV2' type=IteratorV2>, <tf.Operation 'iterator/TensorSliceDataset' type=TensorSliceDataset>, <tf.Operation 'iterator/MapDataset' type=MapDataset>, <tf.Operation 'iterator/BatchDatasetV2' type=BatchDatasetV2>, <tf.Operation 'iterator/RepeatDataset' type=RepeatDataset>, <tf.Operation 'iterator/MakeIterator' type=MakeIterator>, <tf.Operation 'iterator/IteratorToStringHandle' type=IteratorToStringHandle>, <tf.Operation 'iterator/iterator_handle' type=Placeholder>, <tf.Operation 'iterator/IteratorFromStringHandleV2' type=IteratorFromStringHandleV2>, <tf.Operation 'iterator/IteratorToStringHandle_1' type=IteratorToStringHandle>, <tf.Operation 'init' type=NoOp>, <tf.Operation 'init_1' type=NoOp>]\r\n(array([2.], dtype=float32), array([-1], dtype=int32))\r\n(array([3.], dtype=float32), array([-2], dtype=int32))\r\n(array([4.], dtype=float32), array([-3], dtype=int32))\r\n(array([5.], dtype=float32), array([-4], dtype=int32))\r\n(array([6.], dtype=float32), array([-5], dtype=int32))\r\nEnd of training dataset.\r\n\r\nglobal vars: [<tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32>, <tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32>]\r\nlocal vars: []\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nCUDA/cuDNN version\nGPU model and memory", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@mpekalski Thanks for reporting this, Making error message more self-explanatory is an ongoing effort. "]}]