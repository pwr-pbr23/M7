[{"number": 28628, "title": "Unexpected tf.version.GIT_VERSION for nightly builds", "body": "In colab, use the following commands to reproduce:\r\n\r\n```\r\n!pip uninstall tensorflow -yq && pip install tf-nightly-gpu==1.14.1.dev20190510 -q\r\nimport tensorflow as tf\r\ntf.version.GIT_VERSION\r\n```\r\n\r\nWhich shows ``v1.12.1-1705-g978532afa9`` (last hex is not any git commit btw)\r\n\r\nShouldn't it be the git version used to build that package instead?\r\n\r\nGentle ping @gunan and @yifeif ", "comments": ["@byronyi Able to reproduce the issue. Please find attached log\r\n\r\n'v1.12.1-1705-g978532afa9'", "The instructions in the new bug template (https://github.com/tensorflow/tensorflow/issues/new?template=00-bug-performance-issue.md) say for TF 1.X to use\r\n\r\n`python -c \"import tensorflow as tf; print(tf.GIT_VERSION)\"`", "> The instructions in the new bug template (https://github.com/tensorflow/tensorflow/issues/new?template=00-bug-performance-issue.md) say for TF 1.X to use\r\n> \r\n> `python -c \"import tensorflow as tf; print(tf.GIT_VERSION)\"`\r\n\r\nI tried with\r\n\r\n```\r\n!pip uninstall tensorflow -yq && pip install tf-nightly-gpu==1.14.1.dev20190512 -q\r\nimport tensorflow as tf\r\nprint(tf.GIT_VERSION)\r\n```\r\n\r\nand I got:\r\n\r\n```\r\nb'v1.13.1-2-g09e3b09e69'\r\n```\r\n\r\nI am not aware of how I should track a particular commit or snapshot for which the package is built.", "@av8ramit Looking at this, I also cannot find g09e3b09e69.\r\nIs it possible we create a bad commit when we run update_version during the nightly package build?", "I don't think so. Let me take a look and investigate.", "So git_version is behaving unexpectedly for tf_nightly packages. Unfortunately the hash logic is complicated and pulls from GitHub. Unfortunately, we may have to rely on the date for a window for the hash. We are not offering much support for nightly packages. ", "@av8ramit does this mean I better not rely on GIT_VERSION strong in the nightly package?", "I did find a way to map from TensorFlow's GIT_VERSION to a commit hash (I just was unable to find this issue to update it)\r\n\r\nHere is an example:\r\n```\r\n$ pip3 install tf-nightly==1.15.0.dev20190627\r\n\r\n$ python3 -c \"import tensorflow as tf; print(tf.version.GIT_VERSION)\"\r\nv1.12.1-5074-g51b4b90060\r\n\r\n$ git clone https://github.com/tensorflow/tensorflow\r\n$ cd tensorflow\r\n$ git rev-list -n 1 v1.12.1-5074-g51b4b90060\r\n51b4b90060bf2fb46b100648e739899054a84bea\r\n```", "@byronyi yes unfortunately I would advise against it."]}, {"number": 28627, "title": "Fixes broken GPU build", "body": "```\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n\tFile \"/tensorflow_src/third_party/gpus/cuda_configure.bzl\", line 1244\r\n\t\t_create_local_cuda_repository(repository_ctx)\r\n\tFile \"/tensorflow_src/third_party/gpus/cuda_configure.bzl\", line 960, in _create_local_cuda_repository\r\n\t\t_get_cuda_config(repository_ctx)\r\n\tFile \"/tensorflow_src/third_party/gpus/cuda_configure.bzl\", line 686, in _get_cuda_config\r\n\t\tfind_cuda_config(repository_ctx, [\"cuda\", \"cudnn\"])\r\n\tFile \"/tensorflow_src/third_party/gpus/cuda_configure.bzl\", line 666, in find_cuda_config\r\n\t\tauto_configure_fail((\"Failed to run find_cuda_config...))\r\n\tFile \"/tensorflow_src/third_party/gpus/cuda_configure.bzl\", line 274, in auto_configure_fail\r\n\t\tfail((\"\\n%sCuda Configuration Error:%...)))\r\n\r\nCuda Configuration Error: Failed to run find_cuda_config.py: Traceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/external/org_tensorflow/third_party/gpus/find_cuda_config.py\", line 493, in <module>\r\n    main()\r\n  File \"/root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/external/org_tensorflow/third_party/gpus/find_cuda_config.py\", line 485, in main\r\n    for key, value in sorted(find_cuda_config().items()):\r\n  File \"/root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/external/org_tensorflow/third_party/gpus/find_cuda_config.py\", line 454, in find_cuda_config\r\n    if tuple(cuda_version.split(\".\")) < (10, 1):\r\nTypeError: unorderable types: str() < int()\r\n\r\nWARNING: Target pattern parsing failed.\r\nERROR: error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n\tFile \"/tensorflow_src/third_party/gpus/cuda_configure.bzl\", line 1244\r\n\t\t_create_local_cuda_repository(repository_ctx)\r\n\tFile \"/tensorflow_src/third_party/gpus/cuda_configure.bzl\", line 960, in _create_local_cuda_repository\r\n\t\t_get_cuda_config(repository_ctx)\r\n\tFile \"/tensorflow_src/third_party/gpus/cuda_configure.bzl\", line 686, in _get_cuda_config\r\n\t\tfind_cuda_config(repository_ctx, [\"cuda\", \"cudnn\"])\r\n\tFile \"/tensorflow_src/third_party/gpus/cuda_configure.bzl\", line 666, in find_cuda_config\r\n\t\tauto_configure_fail((\"Failed to run find_cuda_config...))\r\n\tFile \"/tensorflow_src/third_party/gpus/cuda_configure.bzl\", line 274, in auto_configure_fail\r\n\t\tfail((\"\\n%sCuda Configuration Error:%...)))\r\n\r\nCuda Configuration Error: Failed to run find_cuda_config.py: Traceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/external/org_tensorflow/third_party/gpus/find_cuda_config.py\", line 493, in <module>\r\n    main()\r\n  File \"/root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/external/org_tensorflow/third_party/gpus/find_cuda_config.py\", line 485, in main\r\n    for key, value in sorted(find_cuda_config().items()):\r\n  File \"/root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/external/org_tensorflow/third_party/gpus/find_cuda_config.py\", line 454, in find_cuda_config\r\n    if tuple(cuda_version.split(\".\")) < (10, 1):\r\nTypeError: unorderable types: str() < int()\r\n\r\nINFO: Elapsed time: 17.630s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n```", "comments": ["Thanks Bairen for providing a fix."]}, {"number": 28626, "title": "TF 2.0: Invalid argument: Unsupported type: 21", "body": "Hello guys,\r\n\r\nDid any of you encounter this weird warning when training any network:\r\n\r\n`E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] constant folding failed: Invalid argument: Unsupported type: 21`\r\n\r\n<img width=\"1175\" alt=\"warning_error\" src=\"https://user-images.githubusercontent.com/3697692/57572249-f4b3ba80-7417-11e9-90c0-8c6bdc223da1.png\">\r\n\r\nI am using Adam optimizer, however, I tried other optimizers but nothing changes!\r\n\r\nAnd, this warning appeared just after I set-up the Tensorboard using callbacks:\r\n`tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)`\r\n\r\nTo set-up the Tensorboard I used: `pip install -q tf-nightly-2.0-preview` \r\n\r\n\r\nThanks\r\n\r\n", "comments": ["I'm experiencing this as well with the latest nightly build. In my case, the error appears when using `tf.function` and calling `tape.gradient` but only when the model calls an instance of `tf.keras.layers.LSTM`. Disabling the layer call, `tf.function` or `tape.gradient` does not result in the same error.\r\n\r\nEDIT: I've verified the dtypes are all float32 where appropriate so unless it's internal to the layer I don't think that's the issue.\r\n\r\nEDIT: Rolling back to `tf-nightly-2.0-preview==2.0.0.dev20190503` provides an additional warning which looks related:\r\n\r\n```\r\n2019-05-11 11:51:32.197165: W tensorflow/core/common_runtime/executor.cc:2237] [/device:CPU:0] Executor start aborting: Invalid argument: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\r\n\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\r\n```", "Thanks for the feedback @jimfleming.\r\n\r\nI am not sure if it has an impact on anything, in my case during the optimization process! ", "Please provide details about what platform you are using (operating system, architecture).Also help us to get minimal reproducible code to expedite the trouble-shooting process. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "@achandraa thanks for the reply.\r\n\r\nI resolved the issue by uninstalling `tf-nightly-2.0-preview` and installing fresh TF 2.0 GPU Alpha using `pip install tensorflow-gpu==2.0.0-alpha0`. \r\n\r\nThe problem was that the TF was not using GPUs anymore (after nightly installed), therefore my machine CPUs with `tf-nightly-2.0-preview` caused those warning! \r\n\r\nFor the sake of reporting, I can share some of the specs of the machine I'm using, even though, I was able to fix the issue myself.\r\n\r\n- Code: a FC network using tf.keras.Sequential and tf.keras.layers. Using tf.keras.optimizers.Adam (however the issue happens with other optimizers as well, included from the same module tf.keras.optimizers)\r\n- OS: Arch Linux\r\n- TF: GPU 2.0 Alpha\r\n- Tf-nightly: pip install -q tf-nightly-2.0-preview (this install caused the issue)\r\n- Python: 3.7.2\r\n- GPUs: GeForce GTX 1080 (with CUDA)\r\n- CPUs (XLA_CPU): 10 x Intel(R) Core(TM) i7-6950X CPU @ 3.00GHz\r\n\r\nI'm going to close this. Thanks.\r\n", "Have the same issue.\r\n\r\nI can't use the alpha version.", "Have the same issue +1. Hi, @ljakupi, how did you solve this? Thanks!", "Uninstall all the TF versions you have in your system, then fresh install TF2.0 using: `pip install tensorflow-gpu==2.0.0-alpha0`", "I'm experiencing this with Tensorflow=2.0.0-beta0.", "Same here with 2.0.0-beta0. I have uninstalled all previous versions of tensorflow and did a fresh install. I'm using the CPU version and the error occurred during fit(). ", "this error happend in 2.0.0-beta0 when use dropout, someone know how to do?", "Same error for me with 2.0.0-beta0.\r\nEven worse. It seems connected to a memory leak, too. But only happens after validation runs apparently.", "problem+1, some solution?", "Just tried GPU 2.0.0-beta0 and report the same as tbhunderbird. Ran through 492 epoches before my early stop kicked in. I received the error: tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] constant folding failed: Invalid argument: Unsupported type: 21, almost immediately 4 times. but it ran. I was watching memory somewhat and it did not appear to be growing. However, the 2nd GPU appears do be doing almost no work but I see a lot of \"copy\" going on. FYI, when I run the same on Keras, backed TF 1.13, I see both GPUs doing almost the same amount of CUDA activity. On the bright side, I think we can rule out a memory leak.", "I also have this issue - I found that with beta0, the training failed (not using the GPU build), but with beta1, the training works - but still outputs these errors.", "> I also have this issue - I found that with beta0, the training failed (not using the GPU build), but with beta1, the training works - but still outputs these errors.\r\n\r\nI have the same issue with gpu-2.0.0-beta1. Did you fix it? I am just dong a simple masking and LSTM (only 2.0.0 beta1 support cudnnlstm with masking).", "![Screenshot 2019-06-20 at 11 56 50 PM](https://user-images.githubusercontent.com/23133817/59872329-245ec680-93b7-11e9-8eeb-75cbfde3d0ee.png)\r\n\r\n\r\nSame issue here with beta1 version.", "getting this", "Thank you all. Looks like many users are having the issue on TF GPU 2.0-beta1. Can we open a new issue and try to triage from that. It will be good for us to track for better and faster resolution. ", "what is the \"21\" type? this happens to us even on 1.14...", "> what is the \"21\" type? this happens to us even on 1.14...\r\n\r\nI believe is the `tf.variant` dtype: https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/core/framework/types.proto#L39", "would it be possible to print a better warning/error to explain what is 21 and how to resolve the issue?\r\n", "The same happens with tensorflow-1.14.0", "the same happens with tf-gpu-2.0.0beta1\r\n", "Just to help future strugglers:\r\n\r\nI'm in `tf==2.3.1` and got this error when using `tf.py_function(_, _ (tf.variant, tf.float64))`.\r\n\r\nChanged `tf.variant` to `tf.int8` and it worked. It is not semantically correct in my case, but it works.\r\n"]}, {"number": 28625, "title": "tensorflow, does this do LSTM", "body": "Hello,\r\n\r\nwhat deos this do ?\r\n\r\nLSTM shows the same photo:\r\n0.38, 0.57, 0.31, -0.2\r\n\r\ni don't know what this do !\r\n\r\ncan some one help me with this?\r\n\r\n```\r\nEpoch 20/20\r\n24946/24946 [==============================] - 2s 73us/step - loss: 0.7507 - acc: 0.5095\r\n1/1 [==============================] - 0s 343ms/step\r\n[0.5433492]\r\n{'loss': [5.713657855987549, 3.161858320236206, 1.936337947845459, 1.414483904838562, 1.1403812170028687, 1.0629992485046387, 1.071506142616272, 1.062917947769165, 1.036919116973877, 1.0067180395126343, 0.9582799673080444, 0.9101015329360962, 0.88093501329422, 0.8427869081497192, 0.8161187767982483, 0.7948179244995117, 0.7787773013114929, 0.7691016793251038, 0.7594917416572571, 0.7506719827651978], 'acc': [0.4476068317890167, 0.4788743555545807, 0.49587106704711914, 0.4971538484096527, 0.49302494525909424, 0.4872123897075653, 0.4887356758117676, 0.49174216389656067, 0.4925839900970459, 0.49286457896232605, 0.4937865734100342, 0.4937865734100342, 0.495269775390625, 0.4943878650665283, 0.4947887361049652, 0.4935460686683655, 0.4955103099346161, 0.4991982579231262, 0.49703359603881836, 0.50946044921875]}\r\n```", "comments": ["@007fred50 It looks like you haven't used a template to create this issue. Please resubmit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation.\r\n"]}, {"number": 28624, "title": "TypeError: '<' not supported between instances of 'list' and 'tuple'.", "body": "\r\nI **git pull http directly** and **not git fork any more**...\r\nSo, I do **NOT** provide **pull request** for now.\r\n\r\nA bug:\r\n```\r\nTypeError: '<' not supported between instances of 'list' and 'tuple'.\r\n```\r\n\r\nFile **third_party/gpus/find_cuda_config.py**\r\n\r\nLine 447:\r\n```\r\n    if cuda_version.split(\".\") < (10, 1):\r\n```\r\nto\r\n```\r\n    ver = cuda_version.split(\".\")\r\n    ver1 = int(ver[0])\r\n    ver2 = int(ver[1])\r\n    if (ver1, ver2) < (10, 1):\r\n```\r\n\r\nBTW, I'm using **CUDA 10.1** \r\n\r\n\r\nCheers\r\nPei", "comments": ["@jiapei100 Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "@jiapei100 \r\nFile **third_party/gpus/find_cuda_config.py**\r\n### if cuda_version.split(\".\") < (10, 1):\r\nmodified to\r\n###  if tuple(int(v) for v in cuda_version.split(\".\")) < (10, 1): \r\nDoes this resolve your issue? \r\n", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 28623, "title": "Add warm up steps for CosineDecay", "body": "Sometimes,we may use freeze training or warm up training,so I add warm up steps to ensure we can get correct learning rate.And CosineDecay init don't need global step,I fixed CosineDecay doc by the way.", "comments": []}, {"number": 28622, "title": "expected conv2d_input to have 4 dimensions with shape(1, 1)", "body": "after i fit my machinelerning i trying to predict a new one. as image named demo1.jpg\r\n\r\nwhat i expected get new feature. into my library:\r\n\r\nMy detailts:\r\nRTX 2080\r\nTensorflow  1.13.1\r\nCuda 10.0\r\n\r\nCan some one help me? Thanks !!\r\n\r\nI'm using tf.keras and I'm getting following error:\r\n\r\n> ValueError: Error when checking input: expected conv2d_input to have 4 dimensions, but got array with shape (1, 1)\r\n\r\nI got new error then i mode.predict a new (feature):\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport pickle\r\nimport cv2\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\r\n\r\nIMG_SIZE = 50\r\n\r\ndef prepare(file):\r\n    img_array = cv2.imread(file, cv2.IMREAD_GRAYSCALE)\r\n    new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\r\n\r\n    predictdata = tf.reshape(new_array, (1, 50, 50))\r\n    predictdata = np.expand_dims(predictdata, -1)\r\n    return predictdata\r\n\r\n\r\npickle_ind = open(\"x.pickle\", \"rb\")\r\nx = pickle.load(pickle_ind)\r\nx = np.array(x, dtype=float)\r\nx = np.expand_dims(x, -1)\r\n\r\npickle_ind = open(\"y.pickle\", \"rb\")\r\ny = pickle.load(pickle_ind)\r\n\r\nn_batch = len(x)\r\n\r\nmodel = Sequential()\r\nmodel.add(Conv2D(32, (3, 3), activation='relu', input_shape=(50, 50, 1)))\r\nmodel.add(MaxPooling2D((2, 2)))\r\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\r\nmodel.add(MaxPooling2D((2, 2)))\r\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\r\nmodel.add(Flatten())\r\nmodel.add(Dense(1, activation='softmax'))\r\n\r\nmodel.summary()\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='binary_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(x, y, epochs=1, batch_size=n_batch)\r\nprediction = model.predict([prepare('demo1.jpg')], batch_size=n_batch, steps=1, verbose=1)\r\n\r\nprint(prediction)\r\n```", "comments": ["@007fred50 In the earlier opened issue [link](https://github.com/tensorflow/tensorflow/issues/28442) , seems like you have got the stackoverflow link.Can you please refer this. As this is the custom code after I ran the code it says 'Errno 2] No such file or directory: 'x.pickle'. ", "hello\r\n\r\nyou can download it here:\r\nhttp://people8people.com/x.pickle\r\nhttp://people8people.com/y.pickle", "can you test it ?", "@007fred50 Ran the code, Able to get the ouptut , please find the below details.\r\nLayer (type) Output Shape Param # \r\n conv2d (Conv2D) (None, 48, 48, 32) 320 \r\nmax_pooling2d (MaxPooling2D) (None, 24, 24, 32) 0 \r\n conv2d_1 (Conv2D) (None, 22, 22, 64) 18496 \r\n max_pooling2d_1 (MaxPooling2 (None, 11, 11, 64) 0 \r\nconv2d_2 (Conv2D) (None, 9, 9, 64) 36928 \r\nflatten (Flatten) (None, 5184) 0 \r\n dense (Dense) (None, 1) 5185 \r\nTotal params: 60,929 Trainable params: 60,929 Non-trainable params: 0 \r\n\r\n", "Hello,\r\n\r\ncan you send me a code? i can't read that messages you gave me", "@007fred50 Ran the same code provided here, in jupyter notebook. ", "Here is my jupyter notebook. with photo\r\n\r\n<img width=\"976\" alt=\"Screenshot 2019-05-17 at 14 18 06\" src=\"https://user-images.githubusercontent.com/25218874/57927575-b8b4a580-78ae-11e9-997a-158db8934200.png\">", "@007fred50 Seems like you have not received any error after executing in jupyter notebook.Please confirm if I can close this issue from my side", "disable GPU:\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\r\n\r\nI got this error then i don't use GPU:\r\n\r\n```\r\n24946/24946 [==============================] - 16s 631us/sample - loss: 7.6647 - acc: 0.5001\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 67, in <module>\r\n    prediction = model.predict([prepare('demo1.jpg')], batch_size=n_batch, steps=1, verbose=1)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py\", line 1080, in predict\r\n    x, check_steps=True, steps_name='steps', steps=steps)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py\", line 2690, in _standardize_user_data\r\n    exception_prefix='input')\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_utils.py\", line 377, in standardize_input_data\r\n    'with shape ' + str(data_shape))\r\nValueError: Error when checking input: expected conv2d_input to have 4 dimensions, but got array with shape (1, 1)\r\n```", "@007fred50 I ran the code with GPU and CPU but no error message was displayed. This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.If you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!\r\n", "@007fred50 Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 28621, "title": "About transformer", "body": "transformer:https://www.tensorflow.org/alpha/tutorials/text/transformer#top_of_page\r\n\r\nHas anyone run this experiment, and the results of my run have not reached the official results. I posted my code and helped me find the reason.\r\n\r\n```python\r\nimport tensorflow_datasets as tfds\r\nimport tensorflow as tf\r\n\r\nimport time\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nfrom tqdm.auto import tqdm\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\n\r\nprint(tf.__version__)\r\n\r\nif tf.test.is_gpu_available():\r\n    device = \"/gpu:0\"\r\nelse:\r\n    device = \"/cpu:0\"\r\n\r\n\r\nprint(\"(1):Reading dataset and token......\")\r\nexamples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,as_supervised=True)\r\ntrain_examples, val_examples = examples['train'], examples['validation']\r\n\r\ntokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\r\n    (en.numpy() for pt, en in train_examples), target_vocab_size=2 ** 13)\r\n\r\ntokenizer_pt = tfds.features.text.SubwordTextEncoder.build_from_corpus(\r\n    (pt.numpy() for pt, en in train_examples), target_vocab_size=2 ** 13)\r\n\r\n\r\nBUFFER_SIZE = 20000\r\nBATCH_SIZE = 64\r\n\r\n\"\"\"Add a start and end token to the input and target.\"\"\"\r\n\r\n\r\ndef encode(lang1, lang2):\r\n    lang1 = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(\r\n        lang1.numpy()) + [tokenizer_pt.vocab_size + 1]\r\n\r\n    lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(\r\n        lang2.numpy()) + [tokenizer_en.vocab_size + 1]\r\n\r\n    return lang1, lang2\r\n\r\n\r\n\"\"\"Note: To keep this example small and relatively fast, drop examples with a length of over 40 tokens.\"\"\"\r\n\r\nMAX_LENGTH = 40\r\n\r\n\r\ndef filter_max_length(x, y, max_length=MAX_LENGTH):\r\n    return tf.logical_and(tf.size(x) <= max_length,\r\n                          tf.size(y) <= max_length)\r\n\r\n\r\n\"\"\"Operations inside `.map()` run in graph mode and receive a graph tensor that do not have a numpy attribute. The `tokenizer` expects a string or Unicode symbol to encode it into integers. Hence, you need to run the encoding inside a `tf.py_function`, which receives an eager tensor having a numpy attribute that contains the string value.\"\"\"\r\n\r\n\r\ndef tf_encode(pt, en):\r\n    return tf.py_function(encode, [pt, en], [tf.int64, tf.int64])\r\n\r\nprint(\"(2):Encode and padded batch......\")\r\ntrain_dataset = train_examples.map(tf_encode)\r\ntrain_dataset = train_dataset.filter(filter_max_length)\r\n# cache the dataset to memory to get a speedup while reading from it.\r\ntrain_dataset = train_dataset.cache()\r\ntrain_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(\r\n    BATCH_SIZE, padded_shapes=([-1], [-1]))\r\ntrain_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\r\n\r\nval_dataset = val_examples.map(tf_encode)\r\nval_dataset = val_dataset.filter(filter_max_length).padded_batch(\r\n    BATCH_SIZE, padded_shapes=([-1], [-1]))\r\n\r\n\r\n\r\ndef get_angles(pos, i, d_model):\r\n    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\r\n    return pos * angle_rates\r\n\r\n\r\ndef positional_encoding(position, d_model):\r\n    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\r\n                            np.arange(d_model)[np.newaxis, :],\r\n                            d_model)\r\n\r\n    # apply sin to even indices in the array; 2i\r\n    sines = np.sin(angle_rads[:, 0::2])\r\n\r\n    # apply cos to odd indices in the array; 2i+1\r\n    cosines = np.cos(angle_rads[:, 1::2])\r\n\r\n    pos_encoding = np.concatenate([sines, cosines], axis=-1)\r\n\r\n    pos_encoding = pos_encoding[np.newaxis, ...]\r\n\r\n    return tf.cast(pos_encoding, dtype=tf.float32)\r\n\r\n\r\ndef create_padding_mask(seq):\r\n    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\r\n\r\n    # add extra dimensions so that we can add the padding\r\n    # to the attention logits.\r\n    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\r\n\r\n\r\n\r\ndef create_look_ahead_mask(size):\r\n    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\r\n    return mask  # (seq_len, seq_len)\r\n\r\n\r\n\r\ndef scaled_dot_product_attention(q, k, v, mask):\r\n    \"\"\"Calculate the attention weights.\r\n    q, k, v must have matching leading dimensions.\r\n    The mask has different shapes depending on its type(padding or look ahead)\r\n    but it must be broadcastable for addition.\r\n\r\n    Args:\r\n      q: query shape == (..., seq_len_q, depth)\r\n      k: key shape == (..., seq_len_k, depth)\r\n      v: value shape == (..., seq_len_v, depth)\r\n      mask: Float tensor with shape broadcastable\r\n            to (..., seq_len_q, seq_len_k). Defaults to None.\r\n\r\n    Returns:\r\n      output, attention_weights\r\n    \"\"\"\r\n\r\n    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\r\n\r\n    # scale matmul_qk\r\n    dk = tf.cast(tf.shape(k)[-1], tf.float32)\r\n    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\r\n\r\n    # add the mask to the scaled tensor.\r\n    if mask is not None:\r\n        scaled_attention_logits += (mask * -1e9)\r\n\r\n    # softmax is normalized on the last axis (seq_len_k) so that the scores\r\n    # add up to 1.\r\n    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\r\n\r\n    output = tf.matmul(attention_weights, v)  # (..., seq_len_v, depth)\r\n\r\n    return output, attention_weights\r\n\r\nclass MultiHeadAttention(tf.keras.layers.Layer):\r\n    def __init__(self, d_model, num_heads):\r\n        super(MultiHeadAttention, self).__init__()\r\n        self.num_heads = num_heads\r\n        self.d_model = d_model\r\n\r\n        assert d_model % self.num_heads == 0\r\n\r\n        self.depth = d_model // self.num_heads\r\n\r\n        self.wq = tf.keras.layers.Dense(d_model)\r\n        self.wk = tf.keras.layers.Dense(d_model)\r\n        self.wv = tf.keras.layers.Dense(d_model)\r\n\r\n        self.dense = tf.keras.layers.Dense(d_model)\r\n\r\n    def split_heads(self, x, batch_size):\r\n        \"\"\"Split the last dimension into (num_heads, depth).\r\n        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\r\n        \"\"\"\r\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\r\n        return tf.transpose(x, perm=[0, 2, 1, 3])\r\n\r\n    def call(self, v, k, q, mask):\r\n        batch_size = tf.shape(q)[0]\r\n\r\n        q = self.wq(q)  # (batch_size, seq_len, d_model)\r\n        k = self.wk(k)  # (batch_size, seq_len, d_model)\r\n        v = self.wv(v)  # (batch_size, seq_len, d_model)\r\n\r\n        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\r\n        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\r\n        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\r\n\r\n        # scaled_attention.shape == (batch_size, num_heads, seq_len_v, depth)\r\n        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\r\n        scaled_attention, attention_weights = scaled_dot_product_attention(\r\n            q, k, v, mask)\r\n\r\n        scaled_attention = tf.transpose(scaled_attention,\r\n                                        perm=[0, 2, 1, 3])  # (batch_size, seq_len_v, num_heads, depth)\r\n\r\n        concat_attention = tf.reshape(scaled_attention,\r\n                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_v, d_model)\r\n\r\n        output = self.dense(concat_attention)  # (batch_size, seq_len_v, d_model)\r\n\r\n        return output, attention_weights\r\n\r\n\r\ndef point_wise_feed_forward_network(d_model, dff):\r\n    return tf.keras.Sequential([\r\n        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\r\n        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\r\n    ])\r\n\r\nclass EncoderLayer(tf.keras.layers.Layer):\r\n    def __init__(self, d_model, num_heads, dff, rate=0.1):\r\n        super(EncoderLayer, self).__init__()\r\n\r\n        self.mha = MultiHeadAttention(d_model, num_heads)\r\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\r\n\r\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\r\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\r\n\r\n        self.dropout1 = tf.keras.layers.Dropout(rate)\r\n        self.dropout2 = tf.keras.layers.Dropout(rate)\r\n\r\n    def call(self, x, training, mask):\r\n        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\r\n        attn_output = self.dropout1(attn_output, training=training)\r\n        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\r\n\r\n        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\r\n        ffn_output = self.dropout2(ffn_output, training=training)\r\n        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\r\n\r\n        return out2\r\n\r\n\r\nclass DecoderLayer(tf.keras.layers.Layer):\r\n    def __init__(self, d_model, num_heads, dff, rate=0.1):\r\n        super(DecoderLayer, self).__init__()\r\n\r\n        self.mha1 = MultiHeadAttention(d_model, num_heads)\r\n        self.mha2 = MultiHeadAttention(d_model, num_heads)\r\n\r\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\r\n\r\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\r\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\r\n        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\r\n\r\n        self.dropout1 = tf.keras.layers.Dropout(rate)\r\n        self.dropout2 = tf.keras.layers.Dropout(rate)\r\n        self.dropout3 = tf.keras.layers.Dropout(rate)\r\n\r\n    def call(self, x, enc_output, training,\r\n             look_ahead_mask, padding_mask):\r\n        # enc_output.shape == (batch_size, input_seq_len, d_model)\r\n\r\n        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\r\n        attn1 = self.dropout1(attn1, training=training)\r\n        out1 = self.layernorm1(attn1 + x)\r\n\r\n        attn2, attn_weights_block2 = self.mha2(\r\n            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\r\n        attn2 = self.dropout2(attn2, training=training)\r\n        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\r\n\r\n        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\r\n        ffn_output = self.dropout3(ffn_output, training=training)\r\n        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\r\n\r\n        return out3, attn_weights_block1, attn_weights_block2\r\n\r\n\r\nclass Encoder(tf.keras.layers.Layer):\r\n    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\r\n                 rate=0.1):\r\n        super(Encoder, self).__init__()\r\n\r\n        self.d_model = d_model\r\n        self.num_layers = num_layers\r\n\r\n        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\r\n        self.pos_encoding = positional_encoding(input_vocab_size, self.d_model)\r\n\r\n        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)\r\n                           for _ in range(num_layers)]\r\n\r\n        self.dropout = tf.keras.layers.Dropout(rate)\r\n\r\n    def call(self, x, training, mask):\r\n        seq_len = tf.shape(x)[1]\r\n\r\n        # adding embedding and position encoding.\r\n        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\r\n        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\r\n        x += self.pos_encoding[:, :seq_len, :]\r\n\r\n        x = self.dropout(x, training=training)\r\n\r\n        for i in range(self.num_layers):\r\n            x = self.enc_layers[i](x, training, mask)\r\n\r\n        return x  # (batch_size, input_seq_len, d_model)\r\n\r\n\r\nclass Decoder(tf.keras.layers.Layer):\r\n    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\r\n                 rate=0.1):\r\n        super(Decoder, self).__init__()\r\n\r\n        self.d_model = d_model\r\n        self.num_layers = num_layers\r\n\r\n        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\r\n        self.pos_encoding = positional_encoding(target_vocab_size, self.d_model)\r\n\r\n        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\r\n                           for _ in range(num_layers)]\r\n        self.dropout = tf.keras.layers.Dropout(rate)\r\n\r\n    def call(self, x, enc_output, training,\r\n             look_ahead_mask, padding_mask):\r\n        seq_len = tf.shape(x)[1]\r\n        attention_weights = {}\r\n\r\n        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\r\n        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\r\n        x += self.pos_encoding[:, :seq_len, :]\r\n\r\n        x = self.dropout(x, training=training)\r\n\r\n        for i in range(self.num_layers):\r\n            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\r\n                                                   look_ahead_mask, padding_mask)\r\n\r\n            attention_weights['decoder_layer{}_block1'.format(i + 1)] = block1\r\n            attention_weights['decoder_layer{}_block2'.format(i + 1)] = block2\r\n\r\n        # x.shape == (batch_size, target_seq_len, d_model)\r\n        return x, attention_weights\r\n\r\n\r\n\"\"\"## Create the Transformer\r\n\r\nTransformer consists of the encoder, decoder and a final linear layer. The output of the decoder is the input to the linear layer and its output is returned.\r\n\"\"\"\r\n\r\n\r\nclass Transformer(tf.keras.Model):\r\n    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\r\n                 target_vocab_size, rate=0.1):\r\n        super(Transformer, self).__init__()\r\n\r\n        self.encoder = Encoder(num_layers, d_model, num_heads, dff,\r\n                               input_vocab_size, rate)\r\n\r\n        self.decoder = Decoder(num_layers, d_model, num_heads, dff,\r\n                               target_vocab_size, rate)\r\n\r\n        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\r\n\r\n    def call(self, inp, tar, training, enc_padding_mask,\r\n             look_ahead_mask, dec_padding_mask):\r\n        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\r\n\r\n        # dec_output.shape == (batch_size, tar_seq_len, d_model)\r\n        dec_output, attention_weights = self.decoder(\r\n            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\r\n\r\n        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\r\n\r\n        return final_output, attention_weights\r\n\r\n\r\nnum_layers = 4\r\nd_model = 128\r\ndff = 512\r\nnum_heads = 8\r\n\r\ninput_vocab_size = tokenizer_pt.vocab_size + 2\r\ntarget_vocab_size = tokenizer_en.vocab_size + 2\r\ndropout_rate = 0.1\r\n\r\n\r\nclass CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\r\n    def __init__(self, d_model, warmup_steps=4000):\r\n        super(CustomSchedule, self).__init__()\r\n\r\n        self.d_model = d_model\r\n        self.d_model = tf.cast(self.d_model, tf.float32)\r\n\r\n        self.warmup_steps = warmup_steps\r\n\r\n    def __call__(self, step):\r\n        arg1 = tf.math.rsqrt(step)\r\n        arg2 = step * (self.warmup_steps ** -1.5)\r\n\r\n        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\r\n\r\n\r\nlearning_rate = CustomSchedule(d_model)\r\n\r\noptimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\r\n                                     epsilon=1e-9)\r\n\r\ntemp_learning_rate_schedule = CustomSchedule(d_model)\r\n\r\nplt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\r\nplt.ylabel(\"Learning Rate\")\r\nplt.xlabel(\"Train Step\")\r\n\r\n\"\"\"## Loss and metrics\r\n\r\nSince the target sequences are padded, it is important to apply a padding mask when calculating the loss.\r\n\"\"\"\r\n\r\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(\r\n    from_logits=True, reduction='none')\r\n\r\n\r\ndef loss_function(real, pred):\r\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\r\n    loss_ = loss_object(real, pred)\r\n\r\n    mask = tf.cast(mask, dtype=loss_.dtype)\r\n    loss_ *= mask\r\n\r\n    return tf.reduce_mean(loss_)\r\n\r\n\r\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\r\ntrain_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\r\nval_loss = tf.keras.metrics.Mean(name='val_loss')\r\nval_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='val_accuracy')\r\n\r\n\"\"\"## Training and checkpointing\"\"\"\r\n\r\ntransformer = Transformer(num_layers, d_model, num_heads, dff,\r\n                          input_vocab_size, target_vocab_size, dropout_rate)\r\n\r\n\r\ndef create_masks(inp, tar):\r\n    # Encoder padding mask\r\n    enc_padding_mask = create_padding_mask(inp)\r\n\r\n    # Used in the 2nd attention block in the decoder.\r\n    # This padding mask is used to mask the encoder outputs.\r\n    dec_padding_mask = create_padding_mask(inp)\r\n\r\n    # Used in the 1st attention block in the decoder.\r\n    # It is used to pad and mask future tokens in the input received by\r\n    # the decoder.\r\n    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\r\n    dec_target_padding_mask = create_padding_mask(tar)\r\n    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\r\n\r\n    return enc_padding_mask, combined_mask, dec_padding_mask\r\n\r\n\r\n\"\"\"Create the checkpoint path and the checkpoint manager. This will be used to save checkpoints every `n` epochs.\"\"\"\r\n\r\ncheckpoint_path = \"./checkpoints/train\"\r\n\r\nckpt = tf.train.Checkpoint(transformer=transformer,\r\n                           optimizer=optimizer)\r\n\r\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\r\n\r\n# if a checkpoint exists, restore the latest checkpoint.\r\nif ckpt_manager.latest_checkpoint:\r\n    ckpt.restore(ckpt_manager.latest_checkpoint)\r\n    print('Latest checkpoint restored!!')\r\n\r\n\r\nEPOCHS = 200\r\n\r\n\r\ntrain_num = len([1 for _, _ in train_dataset])\r\n\r\n@tf.function\r\ndef train_step(inp, tar):\r\n    tar_inp = tar[:, :-1]\r\n    tar_real = tar[:, 1:]\r\n\r\n    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\r\n\r\n    with tf.GradientTape() as tape:\r\n        predictions, _ = transformer(inp, tar_inp,\r\n                                     True,\r\n                                     enc_padding_mask,\r\n                                     combined_mask,\r\n                                     dec_padding_mask)\r\n        loss = loss_function(tar_real, predictions)\r\n\r\n    gradients = tape.gradient(loss, transformer.trainable_variables)\r\n    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\r\n\r\n    train_loss(loss)\r\n    train_accuracy(tar_real, predictions)\r\n\r\n@tf.function\r\ndef val_step(inp, tar):\r\n    tar_inp = tar[:, :-1]\r\n    tar_real = tar[:, 1:]\r\n    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\r\n    predictions, _ = transformer(inp,tar_inp,\r\n                                enc_padding_mask=enc_padding_mask,\r\n                                look_ahead_mask=combined_mask,\r\n                                dec_padding_mask=dec_padding_mask,\r\n                                training=False)\r\n    loss = loss_function(tar_real, predictions)\r\n    ppl = tf.exp(loss)\r\n    val_loss(ppl)\r\n    val_accuracy(tar_real, predictions)\r\n\r\nprint(\"(3):Traning model......\")\r\n\"\"\"Portuguese is used as the input language and English is the target language.\"\"\"\r\n\r\nfor epoch in range(EPOCHS):\r\n    train_loss.reset_states()\r\n    train_accuracy.reset_states()\r\n    val_loss.reset_states()\r\n    val_accuracy.reset_states()\r\n\r\n    print('Epoch {}'.format(epoch + 1))\r\n    start = time.time()\r\n    # inp -> portuguese, tar -> english\r\n    with tqdm(total=train_num * BATCH_SIZE) as pbar:\r\n        for inp, tar in train_dataset:\r\n            train_step(inp, tar)\r\n            pbar.update(BATCH_SIZE)\r\n\r\n    for inp, tar in val_dataset:\r\n        val_step(inp, tar)\r\n\r\n    end = time.time()\r\n    print('train_loss {:.4f}\\ttrain_acc {:.2f}\\t'\r\n          'val_loss {:.4f}\\tval_acc {:.2f}\\t'\r\n          'time {:.2f}s'.format(train_loss.result(),\r\n                                train_accuracy.result() * 100,\r\n                                val_loss.result(),\r\n                                val_accuracy.result() * 100,\r\n                                end - start,\r\n                                ))\r\n\r\n\r\ndef evaluate(inp_sentence):\r\n    start_token = [tokenizer_pt.vocab_size]\r\n    end_token = [tokenizer_pt.vocab_size + 1]\r\n\r\n    # inp sentence is portuguese, hence adding the start and end token\r\n    inp_sentence = start_token + tokenizer_pt.encode(inp_sentence) + end_token\r\n    encoder_input = tf.expand_dims(inp_sentence, 0)\r\n\r\n    # as the target is english, the first word to the transformer should be the\r\n    # english start token.\r\n    decoder_input = [tokenizer_en.vocab_size]\r\n    output = tf.expand_dims(decoder_input, 0)\r\n\r\n    for i in range(MAX_LENGTH):\r\n        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\r\n            encoder_input, output)\r\n\r\n        # predictions.shape == (batch_size, seq_len, vocab_size)\r\n        predictions, attention_weights = transformer(encoder_input,\r\n                                                     output,\r\n                                                     False,\r\n                                                     enc_padding_mask,\r\n                                                     combined_mask,\r\n                                                     dec_padding_mask)\r\n\r\n        # select the last word from the seq_len dimension\r\n        predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\r\n\r\n        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\r\n\r\n        # return the result if the predicted_id is equal to the end token\r\n        if tf.equal(predicted_id, tokenizer_en.vocab_size + 1):\r\n            return tf.squeeze(output, axis=0), attention_weights\r\n\r\n        # concatentate the predicted_id to the output which is given to the decoder\r\n        # as its input.\r\n        output = tf.concat([output, predicted_id], axis=-1)\r\n\r\n    return tf.squeeze(output, axis=0), attention_weights\r\n\r\n\r\ndef plot_attention_weights(attention, sentence, result, layer):\r\n    fig = plt.figure(figsize=(16, 8))\r\n\r\n    sentence = tokenizer_pt.encode(sentence)\r\n\r\n    attention = tf.squeeze(attention[layer], axis=0)\r\n\r\n    for head in range(attention.shape[0]):\r\n        ax = fig.add_subplot(2, 4, head + 1)\r\n\r\n        # plot the attention weights\r\n        ax.matshow(attention[head][:-1, :], cmap='viridis')\r\n\r\n        fontdict = {'fontsize': 10}\r\n\r\n        ax.set_xticks(range(len(sentence) + 2))\r\n        ax.set_yticks(range(len(result)))\r\n\r\n        ax.set_ylim(len(result) - 1.5, -0.5)\r\n\r\n        ax.set_xticklabels(\r\n            ['<start>'] + [tokenizer_pt.decode([i]) for i in sentence] + ['<end>'],\r\n            fontdict=fontdict, rotation=90)\r\n\r\n        ax.set_yticklabels([tokenizer_en.decode([i]) for i in result\r\n                            if i < tokenizer_en.vocab_size],\r\n                           fontdict=fontdict)\r\n\r\n        ax.set_xlabel('Head {}'.format(head + 1))\r\n\r\n    plt.tight_layout()\r\n    plt.show()\r\n\r\n\r\ndef translate(sentence, plot=''):\r\n    result, attention_weights = evaluate(sentence)\r\n\r\n    predicted_sentence = tokenizer_en.decode([i for i in result\r\n                                              if i < tokenizer_en.vocab_size])\r\n\r\n    print('Input: {}'.format(sentence))\r\n    print('Predicted translation: {}'.format(predicted_sentence))\r\n\r\n    if plot:\r\n        plot_attention_weights(attention_weights, sentence, result, plot)\r\n\r\nprint(\"(4):Evaluate model......\")\r\ntranslate(\"este \u00e9 um problema que temos que resolver.\")\r\nprint(\"Real translation: this is a problem we have to solve .\")\r\n\r\ntranslate(\"os meus vizinhos ouviram sobre esta ideia.\")\r\nprint(\"Real translation: and my neighboring homes heard about this idea .\")\r\n\r\ntranslate(\"vou ent\u00e3o muito rapidamente partilhar convosco algumas hist\u00f3rias de algumas coisas m\u00e1gicas que aconteceram.\")\r\nprint(\r\n    \"Real translation: so i 'll just share with you some stories very quickly of some magical things that have happened .\")\r\n\r\n\"\"\"You can pass different layers and attention blocks of the decoder to the `plot` parameter.\"\"\"\r\n\r\ntranslate(\"este \u00e9 o primeiro livro que eu fiz.\", plot='decoder_layer4_block2')\r\nprint(\"Real translation: this is the first book i've ever done.\")\r\n\r\n```", "comments": ["@SunYanCN Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "@gadagashwini I run https://www.tensorflow.org/alpha/tutorials/text/transformer in Calab with GPU.", "@SunYanCN Could you provide more details about the issue and context? Thanks!.", "Thank you for your reply, I will give more detailed results.\r\nFirst run very slowly, this may be the reason for my machine - TeslaP4 8G GPU. The second official did not add a validation set, my code above added a validation set to see if the model is over-fitting. The result of the third run was that the fit began after about 20 epoch, and the correct rate was only about 28%. I cannot guarantee the complete reproducibility of the results due to some random factors. Below are the results of my run on colab and the results of the local run, the parameters are consistent with the official except for epoch. The version of TF is: tf-nightly-gpu-2.0-preview.\r\nofficial:\r\n```python\r\nEpoch 1 Batch 0 Loss 4.4091 Accuracy 0.0000\r\nEpoch 1 Batch 500 Loss 2.8389 Accuracy 0.4932\r\nEpoch 1 Loss 2.4228 Accuracy 0.5203\r\nTime taken for 1 epoch: 319.307568073 secs\r\n\r\nEpoch 2 Batch 0 Loss 1.3543 Accuracy 0.6036\r\nEpoch 2 Batch 500 Loss 1.1689 Accuracy 0.6405\r\nEpoch 2 Loss 1.1440 Accuracy 0.6460\r\nTime taken for 1 epoch: 251.259027004 secs\r\n\r\nEpoch 3 Batch 0 Loss 1.1281 Accuracy 0.6554\r\nEpoch 3 Batch 500 Loss 1.0244 Accuracy 0.6723\r\nEpoch 3 Loss 1.0115 Accuracy 0.6754\r\nTime taken for 1 epoch: 70.5513730049 secs\r\n\r\nEpoch 4 Batch 0 Loss 1.0113 Accuracy 0.6764\r\nEpoch 4 Batch 500 Loss 0.9192 Accuracy 0.6975\r\nEpoch 4 Loss 0.9037 Accuracy 0.7019\r\nTime taken for 1 epoch: 70.917550087 secs\r\n\r\nEpoch 5 Batch 0 Loss 0.9030 Accuracy 0.7027\r\nEpoch 5 Batch 500 Loss 0.8099 Accuracy 0.7260\r\nSaving checkpoint for epoch 5 at ./checkpoints/train/ckpt-1\r\nEpoch 5 Loss 0.7974 Accuracy 0.7293\r\nTime taken for 1 epoch: 73.0342350006 secs\r\n\r\nEpoch 6 Batch 0 Loss 0.8077 Accuracy 0.7360\r\nEpoch 6 Batch 500 Loss 0.7201 Accuracy 0.7475\r\nEpoch 6 Loss 0.7084 Accuracy 0.7503\r\nTime taken for 1 epoch: 70.6219291687 secs\r\n\r\nEpoch 7 Batch 0 Loss 0.7275 Accuracy 0.7451\r\nEpoch 7 Batch 500 Loss 0.6304 Accuracy 0.7688\r\nEpoch 7 Loss 0.6182 Accuracy 0.7719\r\nTime taken for 1 epoch: 72.2072319984 secs\r\n\r\nEpoch 8 Batch 0 Loss 0.6404 Accuracy 0.7730\r\nEpoch 8 Batch 500 Loss 0.5517 Accuracy 0.7887\r\nEpoch 8 Loss 0.5430 Accuracy 0.7911\r\nTime taken for 1 epoch: 70.9613239765 secs\r\n\r\nEpoch 9 Batch 0 Loss 0.5784 Accuracy 0.7829\r\nEpoch 9 Batch 500 Loss 0.4962 Accuracy 0.8035\r\nEpoch 9 Loss 0.4900 Accuracy 0.8052\r\nTime taken for 1 epoch: 68.5947010517 secs\r\n\r\nEpoch 10 Batch 0 Loss 0.5201 Accuracy 0.7956\r\nEpoch 10 Batch 500 Loss 0.4545 Accuracy 0.8145\r\nSaving checkpoint for epoch 10 at ./checkpoints/train/ckpt-2\r\nEpoch 10 Loss 0.4493 Accuracy 0.8161\r\nTime taken for 1 epoch: 68.6737399101 secs\r\n\r\nEpoch 11 Batch 0 Loss 0.4883 Accuracy 0.8055\r\nEpoch 11 Batch 500 Loss 0.4206 Accuracy 0.8240\r\nEpoch 11 Loss 0.4164 Accuracy 0.8251\r\nTime taken for 1 epoch: 69.4070420265 secs\r\n\r\nEpoch 12 Batch 0 Loss 0.4413 Accuracy 0.8195\r\nEpoch 12 Batch 500 Loss 0.3937 Accuracy 0.8317\r\nEpoch 12 Loss 0.3902 Accuracy 0.8328\r\nTime taken for 1 epoch: 70.7441010475 secs\r\n\r\nEpoch 13 Batch 0 Loss 0.4223 Accuracy 0.8236\r\nEpoch 13 Batch 500 Loss 0.3716 Accuracy 0.8380\r\nEpoch 13 Loss 0.3685 Accuracy 0.8389\r\nTime taken for 1 epoch: 71.3240449429 secs\r\n\r\nEpoch 14 Batch 0 Loss 0.4037 Accuracy 0.8265\r\nEpoch 14 Batch 500 Loss 0.3511 Accuracy 0.8442\r\nEpoch 14 Loss 0.3483 Accuracy 0.8450\r\nTime taken for 1 epoch: 75.1278469563 secs\r\n\r\nEpoch 15 Batch 0 Loss 0.3782 Accuracy 0.8331\r\nEpoch 15 Batch 500 Loss 0.3339 Accuracy 0.8493\r\nSaving checkpoint for epoch 15 at ./checkpoints/train/ckpt-3\r\nEpoch 15 Loss 0.3318 Accuracy 0.8499\r\nTime taken for 1 epoch: 71.4256718159 secs\r\n\r\nEpoch 16 Batch 0 Loss 0.3577 Accuracy 0.8409\r\nEpoch 16 Batch 500 Loss 0.3195 Accuracy 0.8537\r\nEpoch 16 Loss 0.3172 Accuracy 0.8544\r\nTime taken for 1 epoch: 70.8179049492 secs\r\n\r\nEpoch 17 Batch 0 Loss 0.3447 Accuracy 0.8466\r\nEpoch 17 Batch 500 Loss 0.3055 Accuracy 0.8579\r\nEpoch 17 Loss 0.3033 Accuracy 0.8587\r\nTime taken for 1 epoch: 68.7967669964 secs\r\n\r\nEpoch 18 Batch 0 Loss 0.3385 Accuracy 0.8487\r\nEpoch 18 Batch 500 Loss 0.2931 Accuracy 0.8620\r\nEpoch 18 Loss 0.2910 Accuracy 0.8626\r\nTime taken for 1 epoch: 67.865557909 secs\r\n\r\nEpoch 19 Batch 0 Loss 0.3198 Accuracy 0.8503\r\nEpoch 19 Batch 500 Loss 0.2818 Accuracy 0.8657\r\nEpoch 19 Loss 0.2797 Accuracy 0.8665\r\nTime taken for 1 epoch: 67.9785480499 secs\r\n\r\nEpoch 20 Batch 0 Loss 0.3110 Accuracy 0.8557\r\nEpoch 20 Batch 500 Loss 0.2726 Accuracy 0.8684\r\nSaving checkpoint for epoch 20 at ./checkpoints/train/ckpt-4\r\nEpoch 20 Loss 0.2706 Accuracy 0.8692\r\nTime taken for 1 epoch: 71.4560930729 secs\r\n```\r\nColab(No validation set):\r\n```python\r\nEpoch 1 Batch 0 Loss 4.4481 Accuracy 0.0000\r\nEpoch 1 Batch 500 Loss 3.6211 Accuracy 0.0405\r\nEpoch 1 Loss 3.3870 Accuracy 0.0524\r\nTime taken for 1 epoch: 630.2422263622284 secs\r\n\r\nEpoch 2 Batch 0 Loss 2.7912 Accuracy 0.1001\r\nEpoch 2 Batch 500 Loss 2.4382 Accuracy 0.1176\r\nEpoch 2 Loss 2.3831 Accuracy 0.1230\r\nTime taken for 1 epoch: 495.0642912387848 secs\r\n\r\nEpoch 3 Batch 0 Loss 2.3392 Accuracy 0.1482\r\nEpoch 3 Batch 500 Loss 2.1384 Accuracy 0.1470\r\nEpoch 3 Loss 2.1120 Accuracy 0.1501\r\nTime taken for 1 epoch: 148.81153535842896 secs\r\n\r\nEpoch 4 Batch 0 Loss 2.1304 Accuracy 0.1727\r\nEpoch 4 Batch 500 Loss 1.9199 Accuracy 0.1714\r\nEpoch 4 Loss 1.8868 Accuracy 0.1758\r\nTime taken for 1 epoch: 125.57066106796265 secs\r\n\r\nEpoch 5 Batch 0 Loss 1.8778 Accuracy 0.2010\r\nEpoch 5 Batch 500 Loss 1.6968 Accuracy 0.2003\r\nSaving checkpoint for epoch 5 at ./checkpoints/train/ckpt-1\r\nEpoch 5 Loss 1.6701 Accuracy 0.2036\r\nTime taken for 1 epoch: 116.66682600975037 secs\r\n\r\nEpoch 6 Batch 0 Loss 1.6858 Accuracy 0.2192\r\nEpoch 6 Batch 500 Loss 1.5010 Accuracy 0.2218\r\nEpoch 6 Loss 1.4778 Accuracy 0.2238\r\nTime taken for 1 epoch: 86.72514414787292 secs\r\n\r\nEpoch 7 Batch 0 Loss 1.4574 Accuracy 0.2340\r\nEpoch 7 Batch 500 Loss 1.3128 Accuracy 0.2439\r\nEpoch 7 Loss 1.2905 Accuracy 0.2460\r\nTime taken for 1 epoch: 87.45780897140503 secs\r\n\r\nEpoch 8 Batch 0 Loss 1.2689 Accuracy 0.2669\r\nEpoch 8 Batch 500 Loss 1.1475 Accuracy 0.2630\r\nEpoch 8 Loss 1.1349 Accuracy 0.2645\r\nTime taken for 1 epoch: 101.89115715026855 secs\r\n\r\nEpoch 9 Batch 0 Loss 1.1215 Accuracy 0.2834\r\nEpoch 9 Batch 500 Loss 1.0331 Accuracy 0.2781\r\nEpoch 9 Loss 1.0247 Accuracy 0.2788\r\nTime taken for 1 epoch: 86.37149453163147 secs\r\n\r\nEpoch 10 Batch 0 Loss 1.0124 Accuracy 0.2965\r\nEpoch 10 Batch 500 Loss 0.9445 Accuracy 0.2893\r\nSaving checkpoint for epoch 10 at ./checkpoints/train/ckpt-2\r\nEpoch 10 Loss 0.9383 Accuracy 0.2893\r\nTime taken for 1 epoch: 79.97530317306519 secs\r\n\r\nEpoch 11 Batch 0 Loss 0.9529 Accuracy 0.3049\r\nEpoch 11 Batch 500 Loss 0.8746 Accuracy 0.2988\r\nEpoch 11 Loss 0.8715 Accuracy 0.2988\r\nTime taken for 1 epoch: 81.03851366043091 secs\r\n\r\nEpoch 12 Batch 0 Loss 0.8880 Accuracy 0.3129\r\nEpoch 12 Batch 500 Loss 0.8198 Accuracy 0.3067\r\nEpoch 12 Loss 0.8160 Accuracy 0.3060\r\nTime taken for 1 epoch: 101.49828553199768 secs\r\n\r\nEpoch 13 Batch 0 Loss 0.8088 Accuracy 0.3197\r\nEpoch 13 Batch 500 Loss 0.7700 Accuracy 0.3118\r\nEpoch 13 Loss 0.7684 Accuracy 0.3118\r\nTime taken for 1 epoch: 86.86038994789124 secs\r\n\r\nEpoch 14 Batch 0 Loss 0.7651 Accuracy 0.3243\r\nEpoch 14 Batch 500 Loss 0.7282 Accuracy 0.3185\r\nEpoch 14 Loss 0.7279 Accuracy 0.3180\r\nTime taken for 1 epoch: 81.0619957447052 secs\r\n\r\nEpoch 15 Batch 0 Loss 0.7379 Accuracy 0.3349\r\nEpoch 15 Batch 500 Loss 0.6923 Accuracy 0.3238\r\nSaving checkpoint for epoch 15 at ./checkpoints/train/ckpt-3\r\nEpoch 15 Loss 0.6937 Accuracy 0.3234\r\nTime taken for 1 epoch: 80.88019394874573 secs\r\n\r\nEpoch 16 Batch 0 Loss 0.6983 Accuracy 0.3412\r\nEpoch 16 Batch 500 Loss 0.6617 Accuracy 0.3287\r\nEpoch 16 Loss 0.6611 Accuracy 0.3279\r\nTime taken for 1 epoch: 79.34229493141174 secs\r\n\r\nEpoch 17 Batch 0 Loss 0.6588 Accuracy 0.3476\r\nEpoch 17 Batch 500 Loss 0.6292 Accuracy 0.3320\r\nEpoch 17 Loss 0.6326 Accuracy 0.3317\r\nTime taken for 1 epoch: 79.55837631225586 secs\r\n\r\nEpoch 18 Batch 0 Loss 0.6609 Accuracy 0.3509\r\nEpoch 18 Batch 500 Loss 0.6054 Accuracy 0.3357\r\nEpoch 18 Loss 0.6078 Accuracy 0.3356\r\nTime taken for 1 epoch: 88.51137733459473 secs\r\n\r\nEpoch 19 Batch 0 Loss 0.6138 Accuracy 0.3450\r\nEpoch 19 Batch 500 Loss 0.5845 Accuracy 0.3404\r\nEpoch 19 Loss 0.5851 Accuracy 0.3398\r\nTime taken for 1 epoch: 79.66434621810913 secs\r\n\r\nEpoch 20 Batch 0 Loss 0.5793 Accuracy 0.3594\r\nSaving checkpoint for epoch 20 at ./checkpoints/train/ckpt-4\r\nEpoch 20 Loss 0.5634 Accuracy 0.3428\r\nTime taken for 1 epoch: 79.7306261062622 secs\r\n```\r\nLocalization (validation set):\r\n```python\r\nEpoch 1\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 44992/44992 [11:27<00:00, 12.72it/s]\r\ntrain_ppl 34.2444\ttrain_acc 4.97\tval_ppl 14.3158\tval_acc 9.88\ttime 705.11s\r\nEpoch 2\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 44992/44992 [10:28<00:00, 127.29it/s]\r\ntrain_ppl 10.8922\ttrain_acc 12.51\tval_ppl 9.3847\tval_acc 14.07\ttime 630.37s\r\nEpoch 3\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 44992/44992 [03:52<00:00, 232.76it/s]\r\ntrain_ppl 8.2826\ttrain_acc 15.14\tval_ppl 7.7277\tval_acc 16.09\ttime 234.72s\r\nEpoch 4\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 44992/44992 [03:46<00:00, 199.87it/s]\r\ntrain_ppl 6.5978\ttrain_acc 17.71\tval_ppl 6.1918\tval_acc 18.77\ttime 228.85s\r\nEpoch 5\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 44992/44992 [03:49<00:00, 200.55it/s]\r\ntrain_ppl 5.2749\ttrain_acc 20.39\tval_ppl 5.1585\tval_acc 21.00\ttime 231.67s\r\nSaving checkpoint for epoch 5 at ./checkpoints/o_train/ckpt-1\r\nEpoch 6\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 44992/44992 [03:43<00:00, 213.87it/s]\r\ntrain_ppl 4.3767\ttrain_acc 22.41\tval_ppl 4.5751\tval_acc 22.49\ttime 225.89s\r\nEpoch 7\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 44992/44992 [03:42<00:00, 218.24it/s]\r\ntrain_ppl 3.6294\ttrain_acc 24.53\tval_ppl 3.9920\tval_acc 24.09\ttime 224.32s\r\nEpoch 8\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 44992/44992 [03:46<00:00, 232.85it/s]\r\ntrain_ppl 3.0980\ttrain_acc 26.47\tval_ppl 3.7746\tval_acc 24.79\ttime 228.97s\r\nEpoch 9\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 44992/44992 [03:44<00:00, 239.29it/s]\r\ntrain_ppl 2.7678\ttrain_acc 27.87\tval_ppl 3.5705\tval_acc 25.64\ttime 226.23s\r\nEpoch 10\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 44992/44992 [03:43<00:00, 230.09it/s]\r\ntrain_ppl 2.5436\ttrain_acc 28.94\tval_ppl 3.4663\tval_acc 26.20\ttime 225.23s\r\nSaving checkpoint for epoch 10 at ./checkpoints/o_train/ckpt-2\r\nEpoch 11\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 44992/44992 [03:40<00:00, 245.01it/s]\r\ntrain_ppl 2.3776\ttrain_acc 29.83\tval_ppl 3.4482\tval_acc 26.31\ttime 222.26s\r\nEpoch 12\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 44992/44992 [03:41<00:00, 257.37it/s]\r\ntrain_ppl 2.2500\ttrain_acc 30.59\tval_ppl 3.4032\tval_acc 26.55\ttime 223.20s\r\nEpoch 13\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 44992/44992 [03:39<00:00, 255.28it/s]\r\ntrain_ppl 2.1497\ttrain_acc 31.18\tval_ppl 3.3787\tval_acc 26.71\ttime 221.46s\r\nEpoch 14\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 44992/44992 [03:38<00:00, 233.30it/s]\r\ntrain_ppl 2.0664\ttrain_acc 31.75\tval_ppl 3.3953\tval_acc 26.88\ttime 220.44s\r\nEpoch 15\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 44992/44992 [03:36<00:00, 232.35it/s]\r\ntrain_ppl 1.9903\ttrain_acc 32.33\tval_ppl 3.4401\tval_acc 26.74\ttime 218.78s\r\nSaving checkpoint for epoch 15 at ./checkpoints/o_train/ckpt-3\r\nEpoch 16\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 44992/44992 [03:38<00:00, 251.13it/s]\r\ntrain_ppl 1.9304\ttrain_acc 32.77\tval_ppl 3.4418\tval_acc 26.79\ttime 220.73s\r\nEpoch 17\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 44992/44992 [03:39<00:00, 258.43it/s]\r\ntrain_ppl 1.8757\ttrain_acc 33.18\tval_ppl 3.4877\tval_acc 26.71\ttime 221.12s\r\nEpoch 18\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 44992/44992 [03:37<00:00, 254.54it/s]\r\ntrain_ppl 1.8315\ttrain_acc 33.56\tval_ppl 3.5823\tval_acc 26.55\ttime 219.96s\r\nEpoch 19\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 44992/44992 [03:38<00:00, 257.17it/s]\r\ntrain_ppl 1.7889\ttrain_acc 33.94\tval_ppl 3.6283\tval_acc 26.21\ttime 220.20s\r\nEpoch 20\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 44992/44992 [03:37<00:00, 227.05it/s]\r\ntrain_ppl 1.7540\ttrain_acc 34.22\tval_ppl 3.6021\tval_acc 26.83\ttime 219.68s\r\nSaving checkpoint for epoch 20 at ./checkpoints/o_train/ckpt-4\r\n\r\nProcess finished with exit code 0\r\n```\r\nI want to confirm if there is a problem with my code or if there is a problem with the official results.", "@gadagashwini @tensorflowbutler @ry @jmhodges ", "more details:https://github.com/SDBurt/Transformer-TF/issues/1", "@SunYanCN Thanks for the detailed information. ", "Closing since a duplicate #28405", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28621\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28621\">No</a>\n", "pt.numpy()\r\nen.numpy()\r\nthey don't have attribute numpy()\r\nsomebody can help is the same transformer example of SunYan"]}, {"number": 28620, "title": "Split a few \"assert(a && b);\" statements into separate assertions for more precise diagnostics", "body": "Split a few assertions of the form \"assert(a && b);\" into \"assert(a); assert(b);\" for more precise diagnostics.\r\n\r\nThe following lengthy explanation is only in the pull request.  The commit message is more concise.\r\n\r\nIn the case of tensorflow, I only found a few instances of this in the whole source tree, although I only looked rather superficially.\r\n\r\nHere is a brief pep talk about why I think each assert(a && b) statement should be separated into separate assertions.\r\n\r\n1. Assertion failures are often sporadic, and users who report them may\r\n   not be in a position to efficiently narrow them down further, so it\r\n   is important to get as much precision from each assertion failure as\r\n   possible.\r\n\r\n2. It is a more efficient use of developers time when a bug report\r\n   arrives if the problem has been narrowed down that much more.  A\r\n   new bug report may initially attract more interest, so, if the\r\n   problem has been narrowed down that much more, it may increase the\r\n   chance that developers may invest the time to try to resolve the\r\n   problem, and also reduce unnecessary traffic on the developer mailing\r\n   list about possible causes of the bug that separating the assertion\r\n   was able to rule out.\r\n   \r\n3. It's often more readable, sometimes eliminating parentheses or\r\n   changing multi-line conditions to separate single line conditions.\r\n\r\n4. When using a debugger to step over an assertion failure in the\r\n   first part of the statement, the second part is still tested.\r\n\r\n5. Providing separate likelihood hints to the compiler in the form\r\n   of separate assert statements does not require the compiler to\r\n   be quite as smart to recognize that it should optimize both branches,\r\n   although I do not know if that makes a difference for any compiler\r\n   commonly used to compile X (that is, I suspect that they are all\r\n   smart enough to realize is that \"a && b\" is likely true, then \"a\"\r\n   is likely true and \"b\" is likely true).\r\n\r\nI am having trouble building under bazel right now on an Ubuntu 19.04 x86_64 system, with and without these changes and I am not currently a tensorflow user.  So, I apologize for the lack of testing and test case.  I hope that the changes are simple enough so that if your automated testing is OK with it, you will accept it.  Thank you for considering these changes.", "comments": ["Thanks, James.\n\nAs someone who is not a Tensorflow user or developer, I am not quite sure\nhow best to proceed, but my guess is that I should not cancel my merge\nrequest and, instead, either you (preferably) or I should submit your\nimprovement afterward as a separate pull request, because your improvement\ndoes not point to a problem in my change and is equally applicable before\nand after my change, and, as a matter of principle, I would rather have the\nseparate changes each go through the automated testing that apparently\nhappens with these pull requests.\n\nHowever, I imagine you probably are more involved in Tensorflow development\nand would, therefore, know better what should be done, so please just let\nme know how you think we should proceed, and I'll almost certainly defer to\nyour judgement on this.\n\nThanks for taking the time to consider this patch and generating an\nimprovement to it, and thanks in advance for any guidance on how to proceed.\n\nAdam\n\nP.S. I realize that the cc's in this email are to noreply.github.com.  I\ninclude them just on the off chance that there is some automation that\nmight use them, again because I am basically unfamiliar with Tensorflow\ndevelopment practices.\n\nOn Sat, May 11, 2019 at 7:44 AM James Ring <notifications@github.com> wrote:\n\n> *@sjamesr* approved this pull request.\n> ------------------------------\n>\n> In\n> tensorflow/lite/java/ovic/src/test/java/org/tensorflow/ovic/OvicClassifierTest.java\n> <https://github.com/tensorflow/tensorflow/pull/28620#discussion_r283099191>\n> :\n>\n> > @@ -123,7 +123,8 @@ public void ovicClassifier_latencyNotNull() throws Exception {\n>    public void ovicClassifier_mismatchedInputResolutionFails() throws Exception {\n>      classifier = new OvicClassifier(labelsInputStream, lowResModel);\n>      int[] inputDims = classifier.getInputDims();\n> -    assertThat((inputDims[1] == 128) && (inputDims[2] == 128)).isTrue();\n> +    assertThat(inputDims[1] == 128).isTrue();\n>\n> this can be assertThat(inputDims[1]).isEqualTo(128), which will improve\n> the error message\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/28620#pullrequestreview-236379783>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AFKY5SHLTJI6BDZE743YARLPU3LUVANCNFSM4HMHV7BQ>\n> .\n>\n", "@adamjrichter can you please resolve conflicts.", "Hey @adamjrichter, sorry I missed your response. To proceed, you should just sync your master branch to tensorflow HEAD, rebase your change on top of that, then push your change branch back to adamjrichter:master. This should take care of the conflicts and allow the automated tests to run.", "@sjamesr , I am sorry if I did the wrong there here, but, just after I sent my previous message to you, I noticed \"resolve conflicts\" button, which I tried, which allowed me to edit a diff conflict and apparently resubmit it.  I think that that may be why @rthadur requested a review from you just now.  So, I am not sure if further action by me right now would just mess things up.\r\n\r\nIf you think I should still do \"git fetch\", \"git merge\", resolve conflicts, and \"git push\", please let me knjow.  I don't think I need to do a true \"git rebase\", but please let me know if you think I really need to do that too.\r\n\r\nThank you for your attention to this, and I apologize for my confusion.", "Thank you, @sjamesr and @rthadur for getting these changes merged!"]}, {"number": 28619, "title": "pass sample weight into py_func", "body": "I tried to use the sklearn roc_auc_score function in a customized metric and would like to pass the sample weight. Here is my code:\r\n\r\n```\r\nfrom sklearn.metrics import roc_auc_score\r\nfrom keras.layers import Input, Dense\r\nfrom keras.models import Model\r\nfrom keras import backend as K\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom functools import partial\r\n\r\ndef auc(weight):\r\n    def metric(y_true, y_pred):\r\n        score = tf.py_func(partial(roc_auc_score, sample_weight=weight), (y_true, y_pred), tf.float32)\r\n        K.get_session().run(tf.local_variables_initializer())\r\n        return score\r\n    return metric\r\n\r\nx=Input(shape=(10, ))\r\nweights = Input(shape=(1,))\r\nhidden = Dense(10, activation='relu')(x)\r\nresult = Dense(1, activation='sigmoid')(hidden)\r\nmodel = Model(inputs=[x, weights], outputs=result)\r\nmodel.compile('adam', 'binary_crossentropy', metrics=[auc(weights)])\r\n\r\nX = np.random.rand(10000, 10)\r\ny = np.random.randint(2, size=(10000, 1))\r\nw = np.random.rand(10000, 1)\r\nX_val = np.random.rand(100, 10)\r\ny_val = np.random.randint(2, size=(100, 1))\r\nw_val = np.random.rand(100, 1)\r\nmodel.fit([X, w], y, epochs=20, sample_weight=w.flatten(), validation_data=([X_val, w_val], y_val), verbose=2)\r\n\r\n```\r\nBut I got the error message:\r\nTypeError: object of type 'Tensor' has no len()\r\n\t [[{{node metrics_7/metric/PyFunc}} = PyFunc[Tin=[DT_FLOAT, DT_FLOAT], Tout=[DT_FLOAT], token=\"pyfunc_2\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_dense_4_target_5_0_2, dense_4/Sigmoid)]]\r\n\r\n", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "> Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n> \r\n> Make sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n> \r\n> We ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n\r\nSure. I'm using tensorflow 1.13.1 on Python 3.6 and Win 10. I pip installed tensorflow.", "I was able to reproduce the mentioned output on Colab with TensorFlow version 1.13.1.", "> I was able to reproduce the mentioned output on Colab with TensorFlow version 1.13.1.\r\n\r\nI also ran on Colab, but got the following error:\r\n\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\nWARNING:tensorflow:From <ipython-input-2-ac708db9cd1d>:3: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\ntf.py_func is deprecated in TF V2. Instead, use\r\n    tf.py_function, which takes a python function which manipulates tf eager\r\n    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\r\n    an ndarray (just call tensor.numpy()) but having access to eager tensors\r\n    means `tf.py_function`s can use accelerators such as GPUs as well as\r\n    being differentiable using a gradient tape.\r\n    \r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.cast instead.\r\nTrain on 10000 samples, validate on 100 samples\r\nEpoch 1/20\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-2-ac708db9cd1d> in <module>()\r\n     19 y_val = np.random.randint(2, size=(100, 1))\r\n     20 w_val = np.random.rand(100, 1)\r\n---> 21 model.fit([X, w], y, epochs=20, sample_weight=w.flatten(), validation_data=([X_val, w_val], y_val), verbose=2)\r\n\r\n5 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)\r\n    526             None, None,\r\n    527             compat.as_text(c_api.TF_Message(self.status.status)),\r\n--> 528             c_api.TF_GetCode(self.status.status))\r\n    529     # Delete the underlying status object from memory otherwise it stays alive\r\n    530     # as there is a reference to status from this from the traceback due to\r\n\r\nInvalidArgumentError: TypeError: object of type 'Tensor' has no len()\r\nTraceback (most recent call last):\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py\", line 207, in __call__\r\n    ret = func(*args)\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/sklearn/metrics/ranking.py\", line 356, in roc_auc_score\r\n    sample_weight=sample_weight)\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/sklearn/metrics/base.py\", line 77, in _average_binary_score\r\n    return binary_metric(y_true, y_score, sample_weight=sample_weight)\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/sklearn/metrics/ranking.py\", line 328, in _binary_roc_auc_score\r\n    sample_weight=sample_weight)\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/sklearn/metrics/ranking.py\", line 618, in roc_curve\r\n    y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/sklearn/metrics/ranking.py\", line 399, in _binary_clf_curve\r\n    check_consistent_length(y_true, y_score, sample_weight)\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\", line 231, in check_consistent_length\r\n    lengths = [_num_samples(X) for X in arrays if X is not None]\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\", line 231, in <listcomp>\r\n    lengths = [_num_samples(X) for X in arrays if X is not None]\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\", line 148, in _num_samples\r\n    return len(x)\r\n\r\nTypeError: object of type 'Tensor' has no len()\r\n\r\n\r\n\t [[{{node metrics/metric/PyFunc}}]]", "@acmilannesta The issue is with sklearn not with Tensorflow. Please post this issue in sklearn as `validation.py` in sklearn need to be updated with exception handling for this data type. Length estimation is throwing error. Thanks!\r\n\r\nI am closing here. You can open it in sklearn repo. thanks!", "To follow up on this for anyone who comes across it.\r\n\r\nThe issue is that by passing sample weights through partial rather than as arguments in py_func (p.s. you can change to [numpy_function](https://www.tensorflow.org/api_docs/python/tf/numpy_function) to get rid of the deprecation warning), you're actually passing a symbolic Tensor to `roc_auc_score`, which doesn't know how to handle it. scikit-learn is going to add clearer error messaging (https://github.com/scikit-learn/scikit-learn/pull/14369), and I'm going to make make Tensors more explicit about when they can't be converted to NumPy arrays (https://github.com/tensorflow/tensorflow/pull/30694)."]}, {"number": 28618, "title": "[tflite] make GPU delegate elementwise ops work", "body": "fix the problem in https://github.com/tensorflow/tensorflow/issues/28606", "comments": []}, {"number": 28617, "title": "Add docstring for v2 version of tf.squeeze()", "body": "The TensorFlow 2.0 API docs have an empty entry for `tf.squeeze`, because the underlying function in `array_ops` doesn't have a docstring. This PR adds a docstring to that function. The docstring is mostly copied from that of `tf.compat.v1.squeeze`, with a few minor tweaks to reflect things that have changed in the v2 API version of the op.", "comments": []}, {"number": 28616, "title": "Add broadcasting support for `tf.where`", "body": "This is a rework on #15982\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks @alextp. The existing gradient tests also include where_v2 in `tensorflow/python/kernel_tests/cwise_ops_test.py`, the exiting non-gradient tests include where_v2 in   `tensorflow/python/kernel_tests/where_op_test.py`.\r\n\r\nI will add additional test cases for broadcast that are special to where_v2 only. Will update again shortly.", "@rthadur @alextp The failed tests are due to the api update. Should I update the api golden to make the tests pass?", "@alexp @rthadur I updated the PR for api goldens. Believe all tests should pass now. Please let me know if there are any issues.", "It appears there is still something wrong:\r\n\r\n```\r\nimport tensorflow.compat.v2 as tf\r\ntf.enable_v2_behavior()\r\n\r\nimport numpy as np\r\n\r\nprint(np.where(tf.constant([[True]]), tf.ones([1, 10]), tf.zeros([1, 10])))\r\nprint(tf.where(tf.constant([[True]]), tf.ones([1, 10]), tf.zeros([1, 10])))\r\n\r\nInvalidArgumentError: Inputs to operation Select of type Select must have the same size and shape.  Input 0: [1,1] != input 1: [1,10] [Op:Select]\r\n```", "@martinwicke Sorry about that. I will take a look and will fix it as soon as possible.", "@martinwicke I tried with tf-nightly it looks fine:\r\n```\r\n# python\r\nPython 2.7.15rc1 (default, Nov 12 2018, 14:31:15) \r\n[GCC 7.3.0] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import numpy as np   \r\n>>> \r\n>>> import tensorflow.compat.v2 as tf\r\n>>> tf.enable_v2_behavior()\r\n>>> \r\n>>> import numpy as np\r\n>>> \r\n>>> print(np.where(tf.constant([[True]]), tf.ones([1, 10]), tf.zeros([1, 10])))\r\n2019-05-17 16:20:24.989573: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n2019-05-17 16:20:25.050076: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2500000000 Hz\r\n2019-05-17 16:20:25.050408: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563b3ef74c80 executing computations on platform Host. Devices:\r\n2019-05-17 16:20:25.050437: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\r\n>>> print(tf.where(tf.constant([[True]]), tf.ones([1, 10]), tf.zeros([1, 10])))\r\ntf.Tensor([[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]], shape=(1, 10), dtype=float32)\r\n>>> \r\n>>> print(tf.version.VERSION)\r\n1.14.1-dev20190517\r\n>>> print(tf.version.GIT_VERSION)\r\nv1.12.1-2154-g3df6d99f3f\r\n>>> \r\n```\r\n\r\nWondering if there is a version difference?", "Yeah, you are right. For some reason my nightly didn't pick this up yet, and I'm confused about that.", "Thanks @martinwicke. Appreciate the help \ud83d\udc4d ", "Changes are merged in to Master, closing this PR."]}, {"number": 28615, "title": "Update array_ops", "body": "Updated tf.identity with an example.", "comments": ["@yifeif Can you please merge this to r2.0. Thanks!", "Closing this PR as this not against `master`, please open a new PR against `master` \r\nCC @mihaimaruseac"]}, {"number": 28614, "title": "Keras RNN example from docs does not support statefulness when multilayer", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.6.7 (Anaconda)\r\n- CUDA/cuDNN version: 9.2/7.3.1\r\n- GPU model and memory: GTX 1070 Ti\r\n\r\n**Describe the current behavior**\r\nModifying the example code given [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN) to have `stateful=True` leads to the following error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"tmp.py\", line 6, in <module>\r\n    y = layer(x)\r\n  File \"/home/davis/software/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 701, in __call__                                                                                                                                \r\n    return super(RNN, self).__call__(inputs, **kwargs)\r\n  File \"/home/davis/software/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 538, in __call__                                                                                                                               \r\n    self._maybe_build(inputs)\r\n  File \"/home/davis/software/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1603, in _maybe_build                                                                                                                          \r\n    self.build(input_shapes)\r\n  File \"/home/davis/software/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 636, in build                                                                                                                                   \r\n    self.reset_states()\r\n  File \"/home/davis/software/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 906, in reset_states                                                                                                                            \r\n    tensor_shape.as_shape(dim).as_list()))\r\n  File \"/home/davis/software/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\", line 2833, in set_value\r\n    value = np.asarray(value, dtype=dtype(x))\r\n  File \"/home/davis/software/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\", line 1015, in dtype\r\n    return x.dtype.base_dtype.name\r\nAttributeError: 'list' object has no attribute 'dtype'\r\n```\r\n\r\n**Describe the expected behavior**\r\nCode should run with no error\r\n\r\n**Code to reproduce the issue**\r\n```\r\ncells = [tf.keras.layers.LSTMCell(32), tf.keras.layers.LSTMCell(64)]\r\nx = tf.keras.Input(batch_shape=(42, None, 5)) \r\nlayer = tf.keras.layers.RNN(cells, stateful=True)\r\ny = layer(x)\r\n```", "comments": ["Thank you for reporting the issue, will fix it soon.", "This should be now fixed in https://github.com/tensorflow/tensorflow/commit/12250556493fe7757bd97f397e3483e7c0e022b1."]}, {"number": 28613, "title": "Issue with transformer guide", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/alpha/tutorials/text/transformer\r\n## Description of issue (what needs changing):\r\nformat and normalization layer.\r\n\r\n### Clear description\r\n1. when describe the multi-head attention, there is this text in one line:  \"Multi-head attention consists of four parts: * Linear layers and split into heads. * Scaled dot-product attention. * Concatenation of heads. * Final linear layer. \"\r\n  I guess the * mean bullet items. It is not formatted correctly.\r\n2. The EncoderLayer and DecoderLayer use LayerNormalization. There is no LayerNormalization in keras.layers. There is only BatchNormalization.\r\n3. Evaluate step creates mask. No mask is needed since a) there is no padding for a single sentence. b) there is no look_ahead since we are trying to predict next words. \r\n", "comments": ["@jasonzhang2022 Thanks for finding this. First one  was already updated in `master` branch [here](https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/text/transformer.ipynb) and it will be correctly rendered when the update are pushed to website. We will take a look at the rest of the two issues. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@jasonzhang2022 Regarding item(2), There is `LayerNormalization` [here](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/LayerNormalization). There is a difference between `LayerNormalization` and `BatchNormalization`.\r\n\r\nRegarding item(3), there is a look_ahead and the functionality was described in the tutorial \r\n```\r\nThe look-ahead mask is used to mask the future tokens in a sequence. In other words, the mask indicates which entries should not be used.\r\n\r\nThis means that to predict the third word, only the first and second word will be used. Similarly to predict the fourth word, only the first, second and the third word will be used and so on.\r\n\r\n```\r\nI think this is not an issue. I am closing this, but please let me know if I'm mistaken. Thanks!"]}, {"number": 28612, "title": "[ROCm] Adding ROCm support for variable ops", "body": "This PR adds ROCm support for variable ops\r\n\r\nThis is a trivial change...please review and merge.\r\n\r\nThanks\r\n\r\ndeven\r\n\r\n--------------------------------\r\n\r\n@tatianashp , @whchung just FYI", "comments": []}, {"number": 28611, "title": "[ROCm] Adding ROCm support for the training ops", "body": "This PR adds ROCm support for the training ops.\r\n\r\nThis is a trivial change...please review and merge. \r\n\r\nThanks\r\n\r\ndeven\r\n\r\n--------------------------------------\r\n\r\n@tatianashp , @whchung just FYI\r\n\r\n", "comments": ["@deven-amd can you please check build errors", "@rthadur, how do I access the log with the build errors...I do not see any links on my end"]}, {"number": 28610, "title": "tf.cast() throws an error using DEVICE_PLACEMENT_EXPLICIT that contradicts the argument's .device", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubunut 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.13.1-2-g09e3b09e69 1.13.1\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: Cuda 10\r\n- GPU model and memory: GeForce GTX 1080\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nAfter getting tf.shape() on a tensor that's on the GPU, the resulting tensor says it's on the GPU. Calling tf.cast() has an error saying it's on the CPU. Explicitly calling .gpu() makes tf.cast() work, even though .device tells me it's on the gpu.\r\n\r\n**Describe the expected behavior**\r\nEither tf.cast() gives me a tensor that tells me it's on the CPU, or else tf.cast() should work correctly. Either the tensor's .device is wrong, or tf.cast() is wrong.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nfrom __future__ import print_function\r\nimport tensorflow as tf\r\ntf.enable_eager_execution(device_policy=tf.contrib.eager.DEVICE_PLACEMENT_EXPLICIT)\r\nimport numpy as np\r\n\r\n# Create an arbitrary tensor on the GPU.\r\na = tf.convert_to_tensor(np.zeros((5, 5)), dtype=tf.float32).gpu()\r\nprint(a.device)  # says it's on the GPU, correctly\r\n\r\nb = tf.shape(a)\r\nprint(b.device)  # says it's on the GPU\r\n\r\nc = tf.cast(b, tf.float32) # error, says first argument is on the CPU\r\nc = tf.cast(b.gpu(), tf.float32)  # no error\r\n\r\nprint(b.device == b.gpu().device)  # True\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["\r\n@kenryd Able to reproduce the issue with the provided code.\r\n/job:localhost/replica:0/task:0/device:GPU:0\r\n/job:localhost/replica:0/task:0/device:GPU:0\r\n\r\n---> 14 c = tf.cast(b, tf.float32) # error, says first argument is on the CPU\r\nInvalidArgumentError: Tensors on conflicting devices: cannot compute Cast as input #0 was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:CPU:0) Tensors can be copied explicitly using .gpu() or .cpu() methods, or transparently copied by using tf.enable_eager_execution(device_policy=tfe.DEVICE_PLACEMENT_SILENT). Copying tensors between devices may slow down your model [Op:Cast] name: Cast/", "You're being bit by the fact that tensorflow doesn't actually store most int32 tensors on the GPU, they're on the GPU device (technically, for graph execution purposes) but stored in host memory.\r\n\r\nDrawbacks like this make it quite hard to effectively use explicit device placement.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28610\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28610\">No</a>\n", "> they're on the GPU device... but stored in host memory.\r\n\r\nWhat does it mean to be on the GPU device if it's not in GPU memory @alextp ? In any case, this is still a bug and this does not resolve the issue.", "I've closed the bug because it's not fixable. TF has a weird definition of\nwhat being on the gpu device is, because of the graph executor. Something\nis on the GPU device if the op kernels run in the GPU executor, not if it's\nin the GPU memory.\n\nA way conceptualize why this is necessary is the kernel for zeros. It has\nto run on the GPU device because it has a GPU output, but its input is a\nshape tensor that has to be in the CPU memory because we need it to\ndetermine how much memory to allocate for the output and the allocator runs\non the CPU. So the input to zeros is a host memory tensor in the GPU device\n(since it's an input to a GPU kernel). This is incidentally why most int32\noperations have inputs and outputs in host memory, as if you need to\nslightly preprocess a shape before passing it to zeros you don't want it to\nbounce back and forth between the GPU and the host CPU.\n\nOn Mon, May 20, 2019 at 3:15 PM kenryd <notifications@github.com> wrote:\n\n> GPU device (technically, for graph execution purposes) but stored in host\n> memory.\n>\n> What does it mean to be on the GPU device if it's not in GPU memory\n> @alextp <https://github.com/alextp> ? In any case, this is still a bug\n> and this does not resolve the issue.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/28610?email_source=notifications&email_token=AAABHRPMQ6NOVSQGYMRPYC3PWMPHJA5CNFSM4HMGAKOKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV2GSSA#issuecomment-494168392>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHROWNZWP2VUW2RGMLADPWMPHJANCNFSM4HMGAKOA>\n> .\n>\n\n\n-- \n - Alex\n", "> A way conceptualize why this is necessary is the kernel for zeros. It has to run on the GPU device because it has a GPU output, but its input is a shape tensor that has to be in the CPU memory because we need it to determine how much memory to allocate for the output and the allocator runs on the CPU. So the input to zeros is a host memory tensor in the GPU device (since it's an input to a GPU kernel). This is incidentally why most int32 operations have inputs and outputs in host memory, as if you need to slightly preprocess a shape before passing it to zeros you don't want it to bounce back and forth between the GPU and the host CPU.\r\n\r\nIn this case, is the input to `zeros` from host memory, then be automatically transferred to GPU memory?"]}, {"number": 28609, "title": "Add scope support for automatic mixed precision", "body": "Allows users to manually prevent certain parts of the graph from being processed by the mixed precision grappler pass", "comments": ["@reedwm ", "Thanks for the review, just pushed some changes addressing the comments", "@reedwm Just removed the V2 exposure and addressed merge conflicts.", "/CC @alextp, can you give API approval? (Better to give it now than internally, since than @MattConley can respond to feedback).\r\n\r\nAlso note, I can fix minor lint errors when importing which for some reason aren't being reported.", "@alextp any update on this review ?", "Because the enable_mixed_precision_graph_rewrite API will not be updated for TF 2, and because the tf.keras mixed precision API is recommended for mixed precision training, we do not want scope support to be added for AMP. So I'm closing this PR. I appreciate the work put into this PR, and apologize for not being able to accept it."]}, {"number": 28608, "title": "Tensorflow gives incorrect results for simple example", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.13.1-2-g09e3b09e69 1.13.1\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 10.0\r\n- GPU model and memory: GeForce GTX 1080\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nThe actual output is [0.7853982 1.1071488]\r\n\r\n**Describe the expected behavior**\r\nThe expected output is [0.7853982 0.7853982]\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\n\r\nfrom __future__ import print_function\r\nimport tensorflow as tf\r\n\r\nwith tf.device('cpu:0'):\r\n    val0 = tf.ones((1,), dtype=tf.float32)\r\n    val0 = tf.Print(val0, [])\r\n    a = tf.atan2(val0, val0)\r\n    b = tf.atan2(val0 + 1, val0 + 1)\r\n    c = tf.concat([a, b], axis=0)\r\n    c = tf.identity(c)\r\n\r\nwith tf.Session() as sess:\r\n    print(sess.run(c))\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\r\n\r\n", "comments": ["@kenryd I could reproduce this with TF1.13.1. However, with TF2.0, the results are what you expected. [GitHub Gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/40edc993f3c1ce726a47dddd5bff7c6d/untitled151.ipynb) is here for TF2.0.0-alpha0. Thanks!", "@jvishnuvardhan In your link tf.Print is removed (I believe it was deprecated and removed in 2.0?). However, removing the tf.Print op in 1.13 also \"fixes\" the behavior, so this does not address the problem.\r\n\r\nMy concern is that this bug is some memory problem, and \"just use 2.0\" isn't a solution. It's also not possible for everyone to just upgrade to 2.0 since tensorflow is often integrated into large projects where upgrading takes a significant amount of time.\r\n\r\nThe answer I get for the second value is 1.1071488 which is equal to atan2(2.0, 1.0), so tensorflow may be reading memory erroneously. If so, this is very concerning and undercuts any results I get in tensorflow: how do I know they're correct if there are bugs like this? Is there a way to avoid this specific bug in tensorflow 1.13?", "This looks like a grappler bug which was fixed a few months ago. Can you reproduce it on nightly?", "@alextp After installing tf-nightly (1.14.1) it does indeed seem to be fixed. Thanks! Do you by chance know which commit fixed this issue?", "No, not off the top of my head.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28608\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28608\">No</a>\n"]}, {"number": 28607, "title": "Unable to install tensorflow-gpu (python 3.6 windows)", "body": "Where did I go wrong?\r\n\r\n**System information**\r\n- OS Platform and Distribution: Windows 10 Pro 10.0.17134 Build 17134\r\n- CUDA Version: 10.1\r\n- cuDNN version: 7.5.1\r\n- TensorFlow version: tensorflow-gpu 1.13.1\r\n- Python version: python-3.6.8-amd64 (64-bit version)\r\n- Installed using pip in virtualenv\r\n- GPU model and memory: NVIDIA GeForce GTX 980 4GB VRAM\r\n(checked versions here: https://www.tensorflow.org/install/gpu)\r\n(and step by step followed this: https://www.tensorflow.org/install/pip)\r\n\r\n**Describe the problem**\r\nTensorFlow GPU does not import correctly in Python 3.6 on Windows.\r\nCannot find any \r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nInstalled cuda_10.1.105_418.96_win10.exe\r\nDownloaded and copied the 3 folders from cudnn-10.1-windows10-x64-v7.5.1.10 into C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1 (replaced files)\r\nDownload and install the Microsoft Visual C++ 2015 Redistributable Update 3 from provided link\r\nClean install of Python 3.6.8 (added to path) (C:\\Python36 & C:\\Python36\\Scripts)\r\n\r\n```\r\nC:\\> pip --version\r\npip 19.1.1 from c:\\python36\\lib\\site-packages\\pip (python 3.6)\r\nC:\\> pip install -U pip virtualenv\r\nC:\\> virtualenv --system-site-package ./tfgpu\r\nC:\\>.\\tfgpu\\Scripts\\activate\r\n\r\n(tfgpu) C:\\>pip list\r\nPackage    Version\r\n---------- -------\r\npip        19.1.1\r\nsetuptools 41.0.1\r\nvirtualenv 16.5.0\r\nwheel      0.33.3\r\n\r\n(tfgpu) C:\\> pip install --upgrade tensorflow-gpu\r\n(tfgpu) C:\\> pip list\r\nPackage              Version\r\n-------------------- -------\r\nabsl-py              0.7.1\r\nastor                0.7.1\r\ngast                 0.2.2\r\ngrpcio               1.20.1\r\nh5py                 2.9.0\r\nKeras-Applications   1.0.7\r\nKeras-Preprocessing  1.0.9\r\nMarkdown             3.1\r\nmock                 3.0.5\r\nnumpy                1.16.3\r\npip                  19.1.1\r\nprotobuf             3.7.1\r\nsetuptools           41.0.1\r\nsix                  1.12.0\r\ntensorboard          1.13.1\r\ntensorflow-estimator 1.13.0\r\ntensorflow-gpu       1.13.1\r\ntermcolor            1.1.0\r\nvirtualenv           16.5.0\r\nWerkzeug             0.15.2\r\nwheel                0.33.3\r\n\r\n(tfgpu) C:\\>python -c \"import tensorflow as tf\"\r\nTraceback (most recent call last):\r\n  File \"C:\\tfgpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\tfgpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\tfgpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\tfgpu\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\tfgpu\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"C:\\tfgpu\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\tfgpu\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\tfgpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\tfgpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\tfgpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\tfgpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\tfgpu\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\tfgpu\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n\r\n\r\n\r\n**Any other info / logs**\r\nAlso created a new project in PyCharm (project name: tfgpu) with the existing interpreter which successfully finds tensorflow after typing \"import t\" and lists it under Settings->Project: tfgpu -> Project Interpreter.\r\n\r\nRun gives the exact same error as above.", "comments": ["You are using CUDA10.1, the current version of Tensorflow GPU requires CUDA10.0(see https://www.tensorflow.org/install/gpu).Please reinstall the correct version of CUDA and cuDNN.", "Thanks, got it working :)"]}, {"number": 28606, "title": "tflite GPU Delegate sub operator not supported", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes (But the model should be running)\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Oneplus3 (Android 8.0)\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 1.13\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): 5.4.0\r\n- CUDA/cuDNN version: NIL\r\n- GPU model and memory: NIL\r\n\r\n**Describe the current behavior**\r\nThe TFLite GPU Delegate benchmark tool provides support for sub operator  to run on the GPU of the mobile. \r\n**sub operator which was included to the model, is not running on the GPU as part of GPU delegate, but falls back to CPU.** \r\n\r\n\r\n**Describe the expected behavior**\r\nSub operator should be running on the GPU as per the documentation provided. \r\n\r\n**Code to reproduce the issue**\r\nAttached with this, is the models and error logs of the models. The model is a modified version of the Deeplab GPU delegate model provided by google. (Input size is 197)\r\n\r\n**Graph Appending Code**\r\n\r\n**trial.tflite(sub model)**\r\n```\r\noutput1 = tf.reshape(tf.strided_slice(tf.get_default_graph().get_tensor_by_name(\"ResizeBilinear_2:0\"), begin=[0,0,0,0], end=[1,197,197,1], strides=[1,1,1,1]), shape=[1,-1])\r\noutput2 = tf.reshape(tf.strided_slice(tf.get_default_graph().get_tensor_by_name(\"ResizeBilinear_2:0\"), begin=[0,0,0,1], end=[1,197,197,2], strides=[1,1,1,1]), shape=[1,-1])\r\noutput3 = tf.subtract(output2, output1)\r\n\r\n```\r\n\r\n**Benchmark Tool Log**\r\n\r\n**trial.tflite(sub model)**\r\n\r\n\r\n`adb shell /data/local/tmp/benchmark_model_gpu --graph=/data/local/tmp/trial.tflite --use_gpu=true`\r\n\r\nLoaded model /data/local/tmp/trial.tflite\r\nresolved reporter\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite delegate for GPU.\r\nERROR: Next operations are not supported by GPU delegate:\r\nSUB: Incorrect operation type passed\r\nFirst 74 operations will run on the GPU, and the remaining 1 on the CPU.\r\nApplied GPU delegate.\r\nInitialized session in 744.972ms\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds\r\ncount=11 first=91729 curr=37009 min=36876 max=91729 avg=46306.5 std=16106\r\n\r\nRunning benchmark for at least 50 iterations and at least 1 seconds\r\ncount=50 first=37205 curr=37165 min=36706 max=37530 avg=37075.8 std=158\r\n\r\n============================== Summary by node type ==============================\r\n\t             [Node type]\t  [count]\t  [avg ms]\t    [avg %]\t    [cdf %]\t  [mem KB]\t[times called]\r\n\t                DELEGATE\t        1\t    37.034\t    99.906%\t    99.906%\t     0.000\t        1\r\n\t                     SUB\t        1\t     0.035\t     0.094%\t   100.000%\t     0.000\t        1\r\n\r\nTimings (microseconds): count=50 first=37200 curr=37161 min=36700 max=37520 avg=37070.4 std=158\r\nMemory (bytes): count=0\r\n2 nodes observed\r\n\r\n\r\nAverage inference timings in us: Warmup: 46306.5, Init: 744972, no stats: 37075.8\r\n\r\n**TFLITE File**\r\nThe tflite file is attached below:\r\n[trial.tflite](https://github.com/tensorflow/tensorflow/files/3167392/trial.tflite.zip)\r\n\r\n**Screenshot of modified part**\r\n![image](https://user-images.githubusercontent.com/41156980/57543244-7a305f80-7371-11e9-960c-efd3262ca719.png)\r\n\r\n", "comments": ["it's a tiny problem. sent a one-line patch PR to fix it. @impjdi", "Closing this issue since the associated PR has been merged. Feel free to reopen if the problem still persists. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28606\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28606\">No</a>\n", "The problem still seems to exist with the recent 'tflite-gpu-experimental-0.0.1' and' tflite-gpu-nightly-0.0.0' versions for android.The tflite models runs without errors in CPU mode; but in GPU we encountered SIGSEV errors.The same tflite models also seems to work in PC(linux TF13). We used the latest tensorflow nightly to  convert the tensorflow models(pb) to tensorflow lite.\r\n\r\nAlso, we were able to benchmark the models with android benchmark tool in CPU mode but it failed in GPU mode.The problem seems to be with the elementwise-operators (except mul) in GPU when either one of the input is a constant(tf.constant ) or an input from previous node.\r\n\r\n**Benchmark Tool error:-**\r\n_Segmentation Fault(using use_gpu=true)_\r\n\r\n**Android error:-**\r\n```\r\n05-21 12:35:34.290 18734-18789/android.example.com.tflitecamerademo A/libc: Fatal signal 11 (SIGSEGV), code 1, fault addr 0x0 in tid 18789 (CameraBackgroun)\r\n05-21 12:35:34.376 18794-18794/? A/DEBUG: *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***\r\n    Build fingerprint: 'OnePlus/OnePlus3/OnePlus3:8.0.0/OPR1.170623.032/1812060016:user/release-keys'\r\n    Revision: '0'\r\n    ABI: 'arm'\r\n    pid: 18734, tid: 18789, name: CameraBackgroun  >>> android.example.com.tflitecamerademo <<<\r\n    signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x0\r\n    Cause: null pointer dereference\r\n        r0 c4103e00  r1 00000000  r2 00025e24  r3 00000000\r\n        r4 bf69160c  r5 bf691680  r6 c6df16f0  r7 e7b2e1c8\r\n        r8 bf691670  r9 bf691658  sl 00000036  fp 00000000\r\n        ip e7b275e4  sp bf691524  lr c23c0385  pc e7ab028a  cpsr 20000030\r\n05-21 12:35:34.404 18794-18794/? A/DEBUG: backtrace:\r\n        #00 pc 0001928a  /system/lib/libc.so (__memcpy_base_aligned+61)\r\n        #01 pc 0000f381  /data/app/android.example.com.tflitecamerademo-T97nhEX610tKBNaxHGXKqg==/lib/arm/libtensorflowlite_gpu_jni.so\r\n        #02 pc 0000f19d  /data/app/android.example.com.tflitecamerademo-T97nhEX610tKBNaxHGXKqg==/lib/arm/libtensorflowlite_gpu_jni.so\r\n        #03 pc 0000eced  /data/app/android.example.com.tflitecamerademo-T97nhEX610tKBNaxHGXKqg==/lib/arm/libtensorflowlite_gpu_jni.so\r\n05-21 12:35:34.405 18794-18794/? A/DEBUG:     #04 pc 0000e347  /data/app/android.example.com.tflitecamerademo-T97nhEX610tKBNaxHGXKqg==/lib/arm/libtensorflowlite_gpu_jni.so\r\n        #05 pc 0000acaf  /data/app/android.example.com.tflitecamerademo-T97nhEX610tKBNaxHGXKqg==/lib/arm/libtensorflowlite_gpu_jni.so\r\n        #06 pc 000cbe8d  /data/app/android.example.com.tflitecamerademo-T97nhEX610tKBNaxHGXKqg==/lib/arm/libtensorflowlite_jni.so\r\n        #07 pc 000cbc43  /data/app/android.example.com.tflitecamerademo-T97nhEX610tKBNaxHGXKqg==/lib/arm/libtensorflowlite_jni.so\r\n        #08 pc 000cba7d  /data/app/android.example.com.tflitecamerademo-T97nhEX610tKBNaxHGXKqg==/lib/arm/libtensorflowlite_jni.so\r\n        #09 pc 0000abb3  /data/app/android.example.com.tflitecamerademo-T97nhEX610tKBNaxHGXKqg==/lib/arm/libtensorflowlite_gpu_jni.so\r\n        #10 pc 000ccf85  /data/app/android.example.com.tflitecamerademo-T97nhEX610tKBNaxHGXKqg==/lib/arm/libtensorflowlite_jni.so\r\n        #11 pc 000ce741  /data/app/android.example.com.tflitecamerademo-T97nhEX610tKBNaxHGXKqg==/lib/arm/libtensorflowlite_jni.so\r\n        #12 pc 00005d37  /data/app/android.example.com.tflitecamerademo-T97nhEX610tKBNaxHGXKqg==/lib/arm/libtensorflowlite_jni.so (Java_org_tensorflow_lite_NativeInterpreterWrapper_applyDelegate+34)\r\n        #13 pc 003e02a9  /system/lib/libart.so (art_quick_generic_jni_trampoline+40)\r\n        #14 pc 003dc561  /system/lib/libart.so (art_quick_invoke_stub_internal+64)\r\n        #15 pc 003e0bdb  /system/lib/libart.so (art_quick_invoke_static_stub+226)\r\n        #16 pc 000ac47b  /system/lib/libart.so (_ZN3art9ArtMethod6InvokeEPNS_6ThreadEPjjPNS_6JValueEPKc+182)\r\n        #17 pc 001f1807  /system/lib/libart.so (_ZN3art11interpreter34ArtInterpreterToCompiledCodeBridgeEPNS_6ThreadEPNS_9ArtMethodEPKNS_7DexFile8CodeItemEPNS_11ShadowFrameEPNS_6JValueE+238)\r\n        #18 pc 001ed99b  /system/lib/libart.so (_ZN3art11interpreter6DoCallILb1ELb0EEEbPNS_9ArtMethodEPNS_6ThreadERNS_11ShadowFrameEPKNS_11InstructionEtPNS_6JValueE+442)\r\n        #19 pc 0020d815  /system/lib/libart.so (_ZN3art11interpreterL8DoInvokeILNS_10InvokeTypeE0ELb1ELb0EEEbPNS_6ThreadERNS_11ShadowFrameEPKNS_11InstructionEtPNS_6JValueE+264)\r\n        #20 pc 00209419  /system/lib/libart.so (_ZN3art11interpreter17ExecuteSwitchImplILb0ELb0EEENS_6JValueEPNS_6ThreadEPKNS_7DexFile8CodeItemERNS_11ShadowFrameES2_b+32096)\r\n        #21 pc 001d4389  /system/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadEPKNS_7DexFile8CodeItemERNS_11ShadowFrameENS_6JValueEb+420)\r\n        #22 pc 001d96bb  /system/lib/libart.so (_ZN3art11interpreter33ArtInterpreterToInterpreterBridgeEPNS_6ThreadEPKNS_7DexFile8CodeItemEPNS_11ShadowFrameEPNS_6JValueE+142)\r\n        #23 pc 001ed985  /system/lib/libart.so (_ZN3art11interpreter6DoCallILb1ELb0EEEbPNS_9ArtMethodEPNS_6ThreadERNS_11ShadowFrameEPKNS_11InstructionEtPNS_6JValueE+420)\r\n        #24 pc 0020c907  /system/lib/libart.so (_ZN3art11interpreterL8DoInvokeILNS_10InvokeTypeE1ELb1ELb0EEEbPNS_6ThreadERNS_11ShadowFrameEPKNS_11InstructionEtPNS_6JValueE+294)\r\n        #25 pc 002093e7  /system/lib/libart.so (_ZN3art11interpreter17ExecuteSwitchImplILb0ELb0EEENS_6JValueEPNS_6ThreadEPKNS_7DexFile8CodeItemERNS_11ShadowFrameES2_b+32046)\r\n        #26 pc 001d4389  /system/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadEPKNS_7DexFile8CodeItemERNS_11ShadowFrameENS_6JValueEb+420)\r\n        #27 pc 001d96bb  /system/lib/libart.so (_ZN3art11interpreter33ArtInterpreterToInterpreterBridgeEPNS_6ThreadEPKNS_7DexFile8CodeItemEPNS_11ShadowFrameEPNS_6JValueE+142)\r\n        #28 pc 001ecda1  /system/lib/libart.so (_ZN3art11interpreter6DoCallILb0ELb0EEEbPNS_9ArtMethodEPNS_6ThreadERNS_11ShadowFrameEPKNS_11InstructionEtPNS_6JValueE+552)\r\n        #29 pc 0020c719  /system/lib/libart.so (_ZN3art11interpreterL8DoInvokeILNS_10InvokeTypeE1ELb0ELb0EEEbPNS_6ThreadERNS_11ShadowFrameEPKNS_11InstructionEtPNS_6JValueE+300)\r\n        #30 pc 00209351  /system/lib/libart.so (_ZN3art11interpreter17ExecuteSwitchImplILb0ELb0EEENS_6JValueEPNS_6ThreadEPKNS_7DexFile8CodeItemERNS_11ShadowFrameES2_b+31896)\r\n        #31 pc 001d4389  /system/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadEPKNS_7DexFile8CodeItemERNS_11ShadowFrameENS_6JValueEb+420)\r\n        #32 pc 001d96bb  /system/lib/libart.so (_ZN3art11interpreter33ArtInterpreterToInterpreterBridgeEPNS_6ThreadEPKNS_7DexFile8CodeItemEPNS_11ShadowFrameEPNS_6JValueE+142)\r\n        #33 pc 001ecda1  /system/lib/libart.so (_ZN3art11interpreter6DoCallILb0ELb0EEEbPNS_9ArtMethodEPNS_6ThreadERNS_11ShadowFrameEPKNS_11InstructionEtPNS_6JValueE+552)\r\n        #34 pc 0020c719  /system/lib/libart.so (_ZN3art11interpreterL8DoInvokeILNS_10InvokeTypeE1ELb0ELb0EEEbPNS_6ThreadERNS_11ShadowFrameEPKNS_11InstructionEtPNS_6JValueE+300)\r\n        #35 pc 00209351  /system/lib/libart.so (_ZN3art11interpreter17ExecuteSwitchImplILb0ELb0EEENS_6JValueEPNS_6ThreadEPKNS_7DexFile8CodeItemERNS_11ShadowFrameES2_b+31896)\r\n        #36 pc 001d4389  /system/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadEPKNS_7DexFile8CodeItemERNS_11ShadowFrameENS_6JValueEb+420)\r\n        #37 pc 001d96bb  /system/lib/libart.so (_ZN3art11interpreter33ArtInterpreterToInterpreterBridgeEPNS_6ThreadEPKNS_7DexFile8CodeItemEPNS_11ShadowFrameEPNS_6JValueE+142)\r\n        #38 pc 001ecda1  /system/lib/libart.so (_ZN3art11interpreter6DoCallILb0ELb0EEEbPNS_9ArtMethodEPNS_6ThreadERNS_11ShadowFrameEPKNS_11InstructionEtPNS_6JValueE+552)\r\n05-21 12:35:34.406 18794-18794/? A/DEBUG:     #39 pc 0020c719  /system/lib/libart.so (_ZN3art11interpreterL8DoInvokeILNS_10InvokeTypeE1ELb0ELb0EEEbPNS_6ThreadERNS_11ShadowFrameEPKNS_11InstructionEtPNS_6JValueE+300)\r\n        #40 pc 00209351  /system/lib/libart.so (_ZN3art11interpreter17ExecuteSwitchImplILb0ELb0EEENS_6JValueEPNS_6ThreadEPKNS_7DexFile8CodeItemERNS_11ShadowFrameES2_b+31896)\r\n        #41 pc 001d4389  /system/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadEPKNS_7DexFile8CodeItemERNS_11ShadowFrameENS_6JValueEb+420)\r\n        #42 pc 001d96bb  /system/lib/libart.so (_ZN3art11interpreter33ArtInterpreterToInterpreterBridgeEPNS_6ThreadEPKNS_7DexFile8CodeItemEPNS_11ShadowFrameEPNS_6JValueE+142)\r\n        #43 pc 001ecda1  /system/lib/libart.so (_ZN3art11interpreter6DoCallILb0ELb0EEEbPNS_9ArtMethodEPNS_6ThreadERNS_11ShadowFrameEPKNS_11InstructionEtPNS_6JValueE+552)\r\n        #44 pc 0020b4e5  /system/lib/libart.so (_ZN3art11interpreterL8DoInvokeILNS_10InvokeTypeE2ELb0ELb0EEEbPNS_6ThreadERNS_11ShadowFrameEPKNS_11InstructionEtPNS_6JValueE+416)\r\n        #45 pc 002092ed  /system/lib/libart.so (_ZN3art11interpreter17ExecuteSwitchImplILb0ELb0EEENS_6JValueEPNS_6ThreadEPKNS_7DexFile8CodeItemERNS_11ShadowFrameES2_b+31796)\r\n        #46 pc 001d4389  /system/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadEPKNS_7DexFile8CodeItemERNS_11ShadowFrameENS_6JValueEb+420)\r\n        #47 pc 001d96bb  /system/lib/libart.so (_ZN3art11interpreter33ArtInterpreterToInterpreterBridgeEPNS_6ThreadEPKNS_7DexFile8CodeItemEPNS_11ShadowFrameEPNS_6JValueE+142)\r\n        #48 pc 001ecda1  /system/lib/libart.so (_ZN3art11interpreter6DoCallILb0ELb0EEEbPNS_9ArtMethodEPNS_6ThreadERNS_11ShadowFrameEPKNS_11InstructionEtPNS_6JValueE+552)\r\n        #49 pc 0020b4e5  /system/lib/libart.so (_ZN3art11interpreterL8DoInvokeILNS_10InvokeTypeE2ELb0ELb0EEEbPNS_6ThreadERNS_11ShadowFrameEPKNS_11InstructionEtPNS_6JValueE+416)\r\n        #50 pc 002092ed  /system/lib/libart.so (_ZN3art11interpreter17ExecuteSwitchImplILb0ELb0EEENS_6JValueEPNS_6ThreadEPKNS_7DexFile8CodeItemERNS_11ShadowFrameES2_b+31796)\r\n        #51 pc 001d4389  /system/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadEPKNS_7DexFile8CodeItemERNS_11ShadowFrameENS_6JValueEb+420)\r\n        #52 pc 001d9609  /system/lib/libart.so (_ZN3art11interpreter30EnterInterpreterFromEntryPointEPNS_6ThreadEPKNS_7DexFile8CodeItemEPNS_11ShadowFrameE+92)\r\n        #53 pc 003bcbad  /system/lib/libart.so (artQuickToInterpreterBridge+960)\r\n        #54 pc 003e0321  /system/lib/libart.so (art_quick_to_interpreter_bridge+32)\r\n        #55 pc 000052cb  /dev/ashmem/dalvik-jit-code-cache (deleted)\r\n05-21 12:35:35.498 1027-1027/? E//system/bin/tombstoned: Tombstone written to: /data/tombstones//tombstone_08\r\n05-21 12:35:35.531 574-574/? E/lowmemorykiller: Error writing /proc/18734/oom_score_adj; errno=22\r\n05-21 12:35:35.642 1218-1672/? E/InputDispatcher: channel 'eb028d android.example.com.tflitecamerademo/com.example.android.tflitecamerademo.CameraActivity (server)' ~ Channel is unrecoverably broken and will be disposed!\r\n05-21 12:37:00.783 2909-14166/? E/AppsUpload: Privacy bit setting explicitly disabled\r\n```\r\nModels:-\r\n\r\nError seems to be with elementwise operators like add or sub in GPU delegate .... \r\n1. Add with Constant (Fatal signal 11 (SIGSEGV) - Error)\r\n![1](https://user-images.githubusercontent.com/1130185/58099675-c3966f80-7bf9-11e9-881d-1ab4855466fc.png)\r\n\r\n2. Add and Sub with Constant (Fatal signal 11 (SIGSEGV) - Error)\r\n![2](https://user-images.githubusercontent.com/1130185/58099681-c729f680-7bf9-11e9-9c54-59e211e18fe6.png)\r\n\r\n3. Sub with two runtime input (Fatal signal 11 (SIGSEGV) - Error)\r\n![3](https://user-images.githubusercontent.com/1130185/58099690-ca24e700-7bf9-11e9-8362-8568b485c098.png)\r\n\r\nHow can we resolve this issue ??", "@anilsathyan7 FYR, after the one-line change (merged into master already), I was able to run this model with GPU Delegated enabled command line benchmark_model and label_image.", "@freedomtan How can i ensure that latest library is included in the **android application**? Is it updated in both '0.0.1-experimental' and gpu-nightly branches?\r\nI tried the demo app at https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo.\r\nIt uses 'org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly' aar from [jcenter](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo#building-in-android-studio-with-tensorflow-lite-aar-from-jcenter). \r\n\r\nDoes it include latest updates? Or should i build from source ?\r\nCan you please mention the steps that you followed to verify the same ?\r\n\r\n", "I don't know how aar binaries are updated. I built [benchmark_model](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark) and [label_image](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/label_image/label_image.md) from source code. The benchmark_model has `--use_gpu` option already. I have patches for label_image to use GPU delegate, see my PR https://github.com/tensorflow/tensorflow/pull/27464"]}, {"number": 28605, "title": "FailedPreconditionError (see above for traceback): Attempting to use uninitialized value ", "body": "FailedPreconditionError (see above for traceback): Attempting to use uninitialized value con1/bias\r\n         [[node con1/bias/read (defined at c:/users/tran thi diem/documents/diem/research/reinforcementlearning/code/codeforlearningreinforcement/cnnandrl/object-recognition-cifar-10-master/code/codechuan/10_5_2019/2_5_2019_cifar_modify_bangchay_testingwithrl.py:60) ]]\r\n\r\n**System information**\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Window10\r\n\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.6\r\n**Describe the current behavior**\r\nI have this bug but I don't know how to fix this\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nimport tensorflow as tf\r\nfrom keras.datasets import cifar10\r\nfrom keras.utils import np_utils\r\nfrom keras.preprocessing.image import ImageDataGenerator\r\n#from tesnsorlow import keras\r\n\r\n\r\n#import random\r\nimport numpy as np\r\nfrom scipy.stats import spearmanr\r\n\r\nfrom sklearn.utils import shuffle\r\n\r\ndef lr_schedule(epoch):\r\n    lrate = 0.001\r\n    if epoch > 75:\r\n        lrate = 0.0005\r\n    elif epoch > 100:\r\n        lrate = 0.0003        \r\n    return lrate\r\nclass DQNetwork:\r\n    \r\n    def __init__(self):\r\n#        self.state_size = state_size\r\n #       self.action_size = action_size\r\n        self.action = tf.placeholder(tf.float32)\r\n        if (self.action == 0):\r\n            test = 5\r\n        else:\r\n            test = 7\r\n            \r\n        sess = tf.Session()\r\n        sess.run(tf.global_variables_initializer())\r\n        self.learning_rate = tf.placeholder(tf.float32)\r\n\r\n        self.inputs_ = tf.placeholder(tf.float32, [None, *state_size])       \r\n\r\n        self.label = tf.placeholder(tf.float32, [None,10])\r\n        \r\n\r\n        self.filter_size_layer1 = tf.placeholder(tf.float32) \r\n        self.filter_size_layer2 = tf.placeholder(tf.float32) \r\n        self.filter_size_layer3 = tf.placeholder(tf.float32) \r\n        self.filter_size_layer4 = tf.placeholder(tf.float32) \r\n        self.filter_size_layer5 = tf.placeholder(tf.float32) \r\n        self.filter_size_layer6 = tf.placeholder(tf.float32)\r\n        ###############\r\n#layer 1\r\n        # Input is 84x84x4 \r\n        self.conv1 = tf.layers.conv2d(inputs=self.inputs_,\r\n                                      filters=1,\r\n                                      kernel_size=[test,test],\r\n                                     \r\n                                      #kernel_size=[3, 3],\r\n                                      #kernel_size=[self.filter_size_layer1,self.filter_size_layer1],\r\n                                      strides=[1,1],\r\n                                      padding=\"same\",  \r\n                                      kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-4),\r\n                                      name = \"con1\",\r\n                                      reuse=tf.AUTO_REUSE\r\n                                      #kernel_regularizer=regularizers.l2(1e-4)\r\n                                      )\r\n       \r\n       \r\n        self.weight_conv1 = tf.all_variables()\r\n        \r\n        \r\n        self.conv1_activation = tf.nn.elu(self.conv1)\r\n\r\n        self.conv1_batchnorm = tf.layers.batch_normalization(self.conv1_activation,\r\n                                                             training=True,\r\n                                                             epsilon=1e-5\r\n                                                             )\r\n        \r\n       \r\n      \r\n#layer2\r\n\r\n        self.conv2 = tf.layers.conv2d(inputs=self.conv1_batchnorm,\r\n                                      filters=32,\r\n                                      kernel_size=[3, 3],\r\n                                      strides=[1,1],\r\n                                      padding=\"same\",\r\n                                      kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-4),\r\n                                      name = \"con2\",\r\n                                      reuse=tf.AUTO_REUSE\r\n                                      #kernel_regularizer=regularizers.l2(1e-4)\r\n                                      )\r\n        self.conv2_activation = tf.nn.elu(self.conv2)\r\n        self.conv2_batchnorm = tf.layers.batch_normalization(self.conv2_activation,\r\n                                                             training=True,\r\n                                                             epsilon=1e-5\r\n                                                             )\r\n        self.conv2_Maxpool = tf.layers.max_pooling2d(self.conv2_batchnorm, 1, 2)\r\n        #self.conv2_Maxpool = tf.layers.MaxPooling2D(self.conv2_batchnorm, pool_size =(2,2)) \r\n        self.conv2_Dropout = tf.layers.dropout (self.conv2_Maxpool,rate = 0.2)\r\n        \r\n\r\n#layer3\r\n        self.conv3 = tf.layers.conv2d(inputs=self.conv2_Dropout,\r\n                                      filters=64,\r\n                                      kernel_size=[3,3],\r\n                                      strides=[1,1],\r\n                                      padding=\"same\",\r\n                                      kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-4),\r\n                                      name = \"con3\",\r\n                                      reuse=tf.AUTO_REUSE\r\n                                      \r\n                                      #kernel_regularizer=regularizers.l2(1e-4)\r\n                                      )\r\n        self.conv3_activation = tf.nn.elu(self.conv3)\r\n\r\n        self.conv3_batchnorm = tf.layers.batch_normalization(self.conv3_activation,\r\n                                                             training=True,\r\n                                                             epsilon=1e-5\r\n                                                             )\r\n#layer 4\r\n       \r\n\r\n\r\n        self.conv4 = tf.layers.conv2d(inputs=self.conv3_batchnorm,\r\n                                      filters=64,\r\n                                      kernel_size=[3, 3],\r\n                                      strides=[1,1],\r\n                                      padding=\"same\",\r\n                                      kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-4),\r\n                                      reuse=tf.AUTO_REUSE,\r\n                                      name = \"con4\"\r\n                                      \r\n                                      #kernel_regularizer=regularizers.l2(1e-4)\r\n                                      )\r\n        \r\n        self.conv4_activation = tf.nn.elu(self.conv4)\r\n        self.conv4_batchnorm = tf.layers.batch_normalization(self.conv4_activation,\r\n                                                             training=True,\r\n                                                             epsilon=1e-5\r\n                                                             )\r\n        self.conv4_Maxpool = tf.layers.max_pooling2d(self.conv4_batchnorm, 1, 2)\r\n\r\n        #self.conv4_Maxpool = tf.layers.MaxPooling2D(self.conv4_batchnorm, pool_size =(2,2)) \r\n        self.conv4_Dropout = tf.layers.dropout (self.conv4_Maxpool,rate = 0.3)\r\n        \r\n        \r\n#layer5\r\n        self.conv5 = tf.layers.conv2d(inputs=self.conv4_Dropout,\r\n                                      filters=128,\r\n                                      kernel_size=[3,3],\r\n                                      strides=[1,1],\r\n                                      padding=\"same\",\r\n                                      kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-4),\r\n                                      reuse=tf.AUTO_REUSE,\r\n                                      name = \"con5\"\r\n                                     # kernel_regularizer=regularizers.l2(1e-4)\r\n                                      )\r\n        self.conv5_activation = tf.nn.elu(self.conv5)\r\n\r\n        self.conv5_batchnorm = tf.layers.batch_normalization(self.conv5_activation,\r\n                                                             training=True,\r\n                                                             epsilon=1e-5\r\n                                                             )\r\n#layer 6\r\n       \r\n\r\n\r\n        self.conv6 = tf.layers.conv2d(inputs=self.conv5_batchnorm,\r\n                                      filters=128,\r\n                                      kernel_size=[3, 3],\r\n                                      strides=[1,1],\r\n                                      padding=\"same\",\r\n                                      kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-4),\r\n                                      reuse=tf.AUTO_REUSE,\r\n                                      name = \"con6\"\r\n                                     # kernel_regularizer=regularizers.l2(1e-4)\r\n                                      )\r\n        self.conv6_activation = tf.nn.elu(self.conv6)\r\n        self.conv6_batchnorm = tf.layers.batch_normalization(self.conv6_activation,\r\n                                                             training=True,\r\n                                                             epsilon=1e-5\r\n                                                             )\r\n        self.conv6_Maxpool = tf.layers.max_pooling2d(self.conv6_batchnorm, 1, 2)\r\n\r\n       # self.conv6_Maxpool = tf.layers.MaxPooling2D(self.conv6_batchnorm, pool_size =(2,2)) \r\n        self.conv6_Dropout = tf.layers.dropout (self.conv6_Maxpool,rate = 0.4)   \r\n      \r\n          \r\n#fully connected        \r\n        \r\n           \r\n\r\n        self.flatten = tf.layers.flatten(self.conv6_Dropout)\r\n        ## --> [43,264]\r\n\r\n        self.output = tf.layers.dense(inputs=self.flatten,\r\n                                      kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-4),\r\n                                      #kernel_regularizer=regularizers.l2(1e-4),\r\n                                      units=10,\r\n                                      activation='softmax')\r\n        \r\n        self.targets1 = tf.squeeze(tf.cast(self.label, tf.int32))   # Get predicted values by finding which logit is the greatest\r\n        self.targets = tf.cast(tf.argmax(self.targets1, 1), tf.int32)        \r\n        self.batch_predictions = tf.cast(tf.argmax(self.output, 1), tf.int32)\r\n        self.predicted_correctly = tf.equal(self.batch_predictions, self.targets)\r\n    # Average the 1's and 0's (True's and False's) across the batch size\r\n        self.accuracy = tf.reduce_mean(tf.cast(self.predicted_correctly, tf.float32))\r\n        \r\n        \r\n        self.cross_entropy =  tf.nn.softmax_cross_entropy_with_logits_v2(logits= self.output, labels=self.label)\r\n        self.gradient = tf.gradients (self.cross_entropy,self.weight_conv1)  \r\n        self.loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits_v2(logits= self.output, labels=self.label) )\r\n          \r\n        #cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits_v2(logits = self.output,label = self.label)\r\n       # self.loss = tf.reduce_mean(cross_entropy, name='cross_entropy')\r\n        self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate,decay=1e-6).minimize(self.loss)\r\n        \r\n        sess = tf.Session()\r\n        sess.run(tf.global_variables_initializer())\r\n        \r\n        \r\n       \r\n        #self.optimizer = tf.train.AdamOptimizer(self.learning_rate,decay=1e-6).minimize(self.loss)\r\n\r\n\r\n(X_train, y_train), (X_test, y_test) = cifar10.load_data()\r\n#X_train = X_train.astype('float32')\r\n#X_test = X_test.astype('float32')\r\n#X_train = normalize(X_train)\r\n#X_test = normalize(X_test)\r\n#y_train = np_utils.to_categorical(y_train)\r\n#y_test = np_utils.to_categorical(y_test)\r\n#num_classes = y_test.shape[1]\r\nX_train = X_train.astype('float32')\r\nX_test = X_test.astype('float32')\r\n\r\n#z-score\r\nmean = np.mean(X_train,axis=(0,1,2,3))\r\nstd = np.std(X_train,axis=(0,1,2,3))\r\nX_train = (X_train-mean)/(std+1e-7)\r\nX_test = (X_test-mean)/(std+1e-7)\r\n\r\nnum_classes = 10\r\ny_train = np_utils.to_categorical(y_train,num_classes) # convert the numbers to onehot vector\r\ny_test = np_utils.to_categorical(y_test,num_classes)\r\n#DQNetwork\r\n\r\naction_size = 3\r\n#learning_rate = 0.00001\r\n\r\nstate_size = [32,32,3]\r\n\r\n# Exploration parameters for epsilon greedy strategy\r\nexplore_start = 1.0            # exploration probability at start\r\nexplore_stop = 0.01            # minimum exploration probability \r\n           # exponential decay rate for exploration prob\r\n\r\n# Q learning hyperparameters\r\ngamma = 0.95  \r\nalpha = 0.5\r\n\r\n \r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\n\r\nQtable_before =np.zeros((12500,3))\r\nnp.random.seed(100)\r\nexp_exp_tradeoff = 0.25\r\n\r\n\r\n\r\n\r\ngenerations = 20\r\nbatch = 64\r\n\r\n\r\ndef train_process (X_train,y_train,batch,before_loss_train,learnig_rate):\r\n  #  gradient_descent_test = []\r\n    number_image = 0   \r\n    \r\n    while (number_image <len(X_train[0:64])):\r\n        before_state = X_train[number_image: number_image +batch]\r\n        before_Y_label = y_train [number_image: number_image +batch]\r\n   #while i<20:\r\n               \r\n        _,current_loss,current_accuracy = sess.run([dqn.optimizer,dqn.loss,dqn.accuracy],\r\n                                                   feed_dict={dqn.inputs_:before_state,dqn.label:before_Y_label,dqn.learning_rate:learning_rate})\r\n     \r\n \r\n        number_image = number_image + batch   \r\n            \r\n               \r\n        \r\n       \r\n    return current_loss,current_accuracy\r\n        \r\n        #print(\"step {}: eps {} : loss {} \".format (i,eps, current_loss))\r\ntrain_loss = []\r\ntrain_accuracy = []\r\ntest_loss = []\r\ntest_accuracy = []\r\nbefore_loss_train = 100\r\neps =0\r\n\r\n\r\ndatagen = ImageDataGenerator(\r\n    rotation_range=15,\r\n    width_shift_range=0.1,\r\n    height_shift_range=0.1,\r\n    horizontal_flip=True,\r\n    )\r\ndatagen.fit(X_train)\r\n\r\n\r\nnumber_image = 0   \r\nlearning_rate = 0.1\r\n\r\ngradient_total = []\r\nshape = []\r\n\r\ni =0\r\naction = 1\r\ndqn = DQNetwork()\r\n    \r\nwhile (number_image <len(X_train[0:640])):\r\n    before_state = X_train[number_image: number_image +batch]\r\n    before_Y_label = y_train [number_image: number_image +batch]\r\n   # test = sess.run(dqn.test,feed_dict = {dqn.action:action})\r\n    conv1 = sess.run([dqn.conv1],feed_dict = {dqn.inputs_: before_state,dqn.action:action})\r\n   #while i<20:\r\n    weight = sess.run(dqn.weight_conv1,feed_dict = {dqn.inputs_: before_state,dqn.action:action})       \r\n    gradient = sess.run (dqn.gradient,feed_dict={dqn.inputs_:before_state,dqn.label:before_Y_label,dqn.learning_rate:learning_rate,dqn.action:action})\r\n    _,current_loss,current_accuracy = sess.run([dqn.optimizer,dqn.loss,dqn.accuracy],\r\n                                               feed_dict={dqn.inputs_:before_state,dqn.label:before_Y_label,dqn.learning_rate:learning_rate,dqn.action:action})\r\n    number_image = number_image + batch\r\n    gradient = np.mean (gradient [0])\r\n    gradient_total.append(gradient)\r\n    shape.append (np.array (weight[0]).shape)\r\n    i = i+1\r\n#    print (\"\\n weigth\",weight [0])\r\n#    print (\"\\n bias\",weight [1])\r\n#    print (\"\\n gradient weight\", np.mean (gradient[0]))\r\n#    print (\"\\n gradient bias\", gradient[1])\r\n    print(\"step {}: loss {} : accuracy {} \".format (i, current_loss,current_accuracy))\r\n    \r\n    tf.reset_default_graph()\r\nprint (gradient_total)   \r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Will it be possible to provide a minimal code snippet that can replicate the issue since it will be easier to debug and decide on the same. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 28604, "title": "Build tf with trt 6", "body": "Enable building TF with TRT 6.0 (onwards). \r\nChanges:\r\n1. tensorrt_configure.bzl -   Include new header files when tensorrt version >= 6.\r\n2. find_cuda_config.py - Retrieve version information from NvInferVerison.h for tensorrt version >= 6.\r\n3. convert_nodes.cc - Add new virtual methods override to derived classes of ITensor.\r\n4. convert_nodes_test.cc -  Add new virtual methods override to derived classes of ITensor.", "comments": []}, {"number": 28603, "title": "Remove cudaGraph API from cuda_runtime_10_0", "body": "cudaGraph APIs are not currently used in TensorFlow. This is the simplest fix to\r\nmake TensorFlow's cuda runtime wrappers compatible with both CUDA 10.0 and 10.1.\r\n\r\nAttention @chsigg ", "comments": []}, {"number": 28602, "title": "Custom layer build should fail if it includes tensor computations", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.0\r\n- Python version: Any\r\n\r\n**Describe the current behavior**\r\nSince https://github.com/tensorflow/tensorflow/commit/9df99aafc3480638cd8d2bf6a0caeab111b68302 it is required that there not be tensor computations in a layer's `build` method. We noticed this [because our layer serialization failed](https://github.com/tensorflow/addons/issues/203 ) after this commit. The fix was to move tensor computation into `call` method, though it was a silent fail and  would have been very difficult to troubleshoot if it didn't fail on a nightly. \r\n\r\n**Describe the expected behavior**\r\nThe build method should raise some kind of error saying that tensor computation is not allowed in build. It should also probably be mentioned in the documentation of TF or Keras:\r\nhttps://www.tensorflow.org/tutorials/eager/custom_layers\r\nhttps://keras.io/layers/writing-your-own-keras-layers/\r\n\r\nIt's sort of suggested but not well enough imo, especially with the hard to find fail it produces\r\n\r\n**Code to reproduce the issue**\r\nhttps://github.com/tensorflow/addons/pull/208\r\n\r\n", "comments": ["HI @seanpmorgan, thanks for the issue! Everything in `build` should happen only once regardless, so for most cases Tensor computations in `build` are safe. Could you provide a minimal repro of why this is not true in your case?", "I think there's more to this than simply running computations in build... but:\r\n\r\nHere is it failing on tf2-nightly. If you uncomment `self._keras_style = False` it will pass:\r\nhttps://colab.research.google.com/drive/1gyYBwE5r4scMc74JTSXaEd75v49-3IT2\r\n\r\nHere it is passing on tf2-alpha:\r\nhttps://colab.research.google.com/drive/1SNn9DG9C0eDCrATG-bNW_9d53DtEw5cl", "I see, thanks! This pattern will not work in general in 2.0 (for example, as a standalone layer in eager or a tf.function training loop). That's because in eager, once you call `tf.nn.l2_normalize` on a Variable, this is just a constant value that is no longer tied to the Variable\r\n", "Ah thank you. Sorry this was overlooked when converting a layer from tf 1.x", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28602\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28602\">No</a>\n"]}, {"number": 28601, "title": "Request for public APIs or alternatives: TF-Addons", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n**System information**\r\n- TensorFlow version (you are using): TF 2.0\r\n\r\nAs part of our effort to make [tensorflow/addons](https://github.com/tensorflow/addons) independent of internal TensorFlow core APIs we've identified several things that we cannot find public API alternatives for; or would like to discuss other options. One such option is to copy the code statically into addons, but for anything that is likely to change in the future this isn't great.\r\n\r\ncc @karmel \r\n\r\n## Testing\r\nWe heavily depend on testing decorators that are not exposed in the public API.\r\n\r\n* [tensorflow.python.framework.test_util.run_all_in_graph_and_eager_modes](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/test_util.py#L893)\r\n* [tensorflow.python.framework.test_util.run_in_graph_and_eager_modes](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/test_util.py#L957)\r\n* [tensorflow.python.keras.keras_parameterized.run_all_keras_modes](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/keras_parameterized.py#L176)\r\n* [tensorflow.python.keras.testing_utils.layer_test](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/testing_utils.py#L71)\r\n    * `layer_test` has been very helpful for catching serialization issues and other problems with our custom layers.\r\n* [tensorflow.python.framework.test_util.run_deprecated_v1](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/test_util.py#L1128)\r\n     * `run_deprecated_v1` can probably be statically copied over? We should be looking to move away from this anyhow.\r\n\r\n## Keras\r\n* [tensorflow.python.keras.losses.LossFunctionWrapper](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/losses.py#L177)\r\n     * `LossFunctionWrapper` is used to convert our loss functions to a Keras Loss. It seems extensively used in core repository, but there is no API. Should we be refactoring our functions to inherit from base Loss? And if so why not do that internally as well?\r\n* [tensorflow.python.keras.engine.base_layer_utils.mark_checked](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/base_layer_utils.py#L414)\r\n     * This looks like we could easily re-create it using tf.nest. Just want to confirm that this isn't expected to used outside of TF often.\r\n \r\n\r\n## RNN Ops\r\n* [tensorflow.python.ops. rnn._transpose_batch_time](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn.py#L43)\r\n* [tensorflow.python.ops. rnn_cell_impl.assert_like_rnncell](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell_impl.py#L75)\r\n* [tensorflow.python.ops. rnn_cell_impl._zero_state_tensors](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell_impl.py#L171)\r\n\r\n## Control Flow\r\n* [tensorflow.python.ops.control_flow_util.GetContainingXLAContext](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/control_flow_util.py#L200)\r\n* [tensorflow.python.ops.control_flow_util.GetContainingWhileContext](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/control_flow_util.py#L178)\r\n", "comments": ["Thanks @seanpmorgan . Will review with the API owners.\r\n\r\ncc @fchollet for the Keras methods, and cc @qlzh727 for the RNN methods, in case you have any opinions about whether these should/can be made public.", "I will take look for the RNN one, I might have missed them during rewrite for the seq2seq. The control flow one might also be in similar case.", "For the ones below\r\n\r\n```\r\ntensorflow.python.framework.test_util.run_all_in_graph_and_eager_modes\r\ntensorflow.python.framework.test_util.run_in_graph_and_eager_modes\r\ntensorflow.python.keras.keras_parameterized.run_all_keras_modes\r\ntensorflow.python.framework.test_util.run_deprecated_v1\r\n```\r\n\r\n@isaprykin has been working on a more reliable testing infrastructure for tf2 that will allow finer grained control over the combinations. I'd say wait for that.\r\n\r\nFor `tensorflow.python.keras.testing_utils.layer_test` I think you should fork  it and expose it. It mostly only depends on public tf symbols (and we need to make the cudnn deterministic one public) but it has no reason to be subject to TF's stability guarantees and release cadence. I think it'd be a seriously useful tool which third-party layer writer will get by importing addons.\r\n\r\nRe LossFunctionWrapper I encourage you to inherit from Loss instead.\r\n\r\ntranspose_batch_time you should fork.\r\n\r\nRe the XLA context one and the while context one I'd really really like to make it so you don't have any need for those things to exist. Can we discuss separately why do you need them?", "I think the XLA context was in the seq2seq code https://github.com/tensorflow/tensorflow/blob/e4371880b1a50f7b23b2375c9c7c7fcd2dcf1a5b/tensorflow/contrib/seq2seq/python/ops/decoder.py#L189. I didn't remove them during the rewrite since I don't want to break XLA when we port the addon code back to third_party/tensorflow_addons. ", "Can we expose [`tf.keras.metrics.MeanMetricWrapper`](https://github.com/tensorflow/tensorflow/blob/18e619cc61952cb89863dc2a8d5338bd321453ef/tensorflow/python/keras/metrics.py#L526) to the public API? We're implementing new metrics in TensorFlow Addons, and some of them are stateless functions that can be easily wrapped in `MeanMetricWrapper`.", "@brainnoise -- thoughts on MeanMetricWrapper?", "> @brainnoise -- thoughts on MeanMetricWrapper?\r\n\r\nJust confirming this is the correct username? Looks like it may have been deleted?", "@pavithrasv ", "@karmel @qlzh727 @pavithrasv Any update on this?", "`MeanMetricWrapper` is an internal helper class to be able to create `Metric` class wrapper around stateless metric functions. This is not required for use with any Keras API. eg., you can pass a stateless fn `def my_metric(y_true, y_pred)` to the Keras compile API as is. \r\n\r\nBecause of this i am leaning towards having a version of the `MeanMetricWrapper` in the Add ons repo may be?", "The API owners are moving to a rotation based system instead of the meetings. Adding a couple of owners to follow along.", "/cc @yarri-oss", "Just to notify that the related RFC was just claimed closed/implemented:\r\n\r\nhttps://github.com/tensorflow/community/pull/278#issuecomment-1026236966", "@bhack, do u still have a list of the function/private API that are used by addon? I can revisit them and see if we will rewrite or expose test related __intenral__ APIs.", "From a quick check on the TFA checkout `grep -R  \"from tensorflow.python.keras\" --include=*.py`:\r\n```\r\ntensorflow_addons/utils/types.py:    from tensorflow.python.keras.engine import keras_tensor\r\ntensorflow_addons/utils/test_utils.py:    from tensorflow.python.keras.testing_utils import layer_test  # noqa: F401\r\ntensorflow_addons/seq2seq/attention_wrapper.py:from tensorflow.python.keras.engine import base_layer_utils\r\n\r\n```\r\n", "Also a little bit unrelated but we had to replicate code for:\r\nhttps://github.com/tensorflow/tensorflow/pull/45742\r\n\r\nBut I don't know if it will be still a problem in the next months if you want to migrate this ops in keras cv.\r\n\r\n", "Trying to address one of the import in https://github.com/tensorflow/addons/pull/2655.\r\n\r\nFor KerasTensor type check, I don't think we are going to expose is it as any public API soon(although it is a return value from a public API), I will need to check with API owner @fchollet for this. I don't think it will be too bad if we remove that in the tensorlike list.\r\n\r\nSimilar for layer_test, since it is a testing util (and keras hasn't historically expose as test util), I will need API owner @fchollet  to approve this change as well (decide an API namespace). ", "The layer_test is available now as `tf.keras.__internal__.utils.layer_test` in tf-nightly. See https://github.com/keras-team/keras/commit/08872b30c71c038954c9e72739687e78a60130e4", "@seanpmorgan Could you please refer to the [comment](https://github.com/tensorflow/tensorflow/issues/28601#issuecomment-1035599709) above and let us know if it helps?Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "@seanpmorgan I think that as we cannot have `keras_tensor` exposed this could be closed.", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28601\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28601\">No</a>\n"]}, {"number": 28600, "title": "Tensorflow Lite GPU Delegate - Argmax Op Request", "body": "<em>Tensorflow Lite GPU Delegate - Argmax Op Request</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): Tensorflow 1.13\r\nTensorflow Lite GPU Delegate 0.0.0-nightly\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThe current GPU delegate does not support Argmax op to run on the mobile GPU. It is one of the major stuff needed to be done for effective deeplab processing on the GPU, which could also be helpful in running the entire model on the GPU in the later stage. So, it would be useful if you could provide support for Argmax functionality on TFLite-GPU Delegate. In the current context, deeplab execution falls back from GPU to CPU on introduction of Argmax op. If that could not be done, can there be any other alternative to Argmax that can be used to effect a 2 class segmentation, or can a combination of ops can be added unto the base Model to effect a complete TFLite GPU Delegate model. If so, what has to be done?\r\n\r\n**Will this change the current api? How?**\r\nNo. It is one more tflite op request. So, it must not affect other ops and functionality\r\n\r\n**Who will benefit with this feature?**\r\nEdge learning and Deeplab developers\r\n", "comments": ["Are there any chances that Argmax GPU Delegate op will be introduced in the near future??", "@SanthoshRajendiran \r\n\r\nGiven how resource constraint we are, we probably can't accommodate these individual requests =/\r\n\r\nWe have max pooling with indices.  Could you use that, or customize with that as a basis?  After all, we opensourced the GPU backend so you can customize it for your needs ;)", "@impjdi  Thank you.. Will look into customizing the GPU Backend code."]}, {"number": 28599, "title": "Restore from SavedModel not compatible with tf.distribute", "body": "**System information**\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Archlinux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):  v1.12.0-9492-g2c319fb415 2.0.0-alpha0\r\n- Python version: 3.6.0\r\n\r\n**Describe the current behavior**\r\n\r\nI'm using Tensorflow Hub to restore a model from a SavedModel, I do expect the restore to work even if it executed inside a distribution strategy scope, but instead it raises an exception.\r\n\r\n**Describe the expected behavior**\r\n\r\nIt should load the model correctly even it the restore from the savedmodel is performed inside a distribution strategy context.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow_hub as hub\r\n\r\nstrategy = tf.distribute.MirroredStrategy()\r\n\r\nwith strategy.scope():\r\n\r\n    model = tf.keras.Sequential(\r\n        [\r\n            hub.KerasLayer(\r\n                \"https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/2\",\r\n                output_shape=[2048],\r\n                trainable=True,\r\n            )\r\n        ]\r\n    )\r\n```\r\n\r\n**Other info / logs**\r\n\r\nThe exception:\r\n\r\n```\r\n  File \"/data/pgaleone/env/lib/python3.6/site-packages/tensorflow_hub/keras_layer.py\", line 98, in __init__\r\n    self._func = module_v2.load(handle)\r\n  File \"/data/pgaleone/env/lib/python3.6/site-packages/tensorflow_hub/module_v2.py\", line 80, in load\r\n    return tf_v1.saved_model.load_v2(module_handle)\r\n  File \"/data/pgaleone/env/lib/python3.6/site-packages/tensorflow/python/saved_model/load.py\", line 324, in load\r\n    export_dir)\r\n  File \"/data/pgaleone/env/lib/python3.6/site-packages/tensorflow/python/saved_model/load.py\", line 63, in __init__\r\n    self._setup_functions_captures()\r\n  File \"/data/pgaleone/env/lib/python3.6/site-packages/tensorflow/python/saved_model/load.py\", line 100, in _setup_functions_captures\r\n    for node_id in proto.bound_inputs]\r\n  File \"/data/pgaleone/env/lib/python3.6/site-packages/tensorflow/python/saved_model/load.py\", line 100, in <listcomp>\r\n    for node_id in proto.bound_inputs]\r\n  File \"/data/pgaleone/env/lib/python3.6/site-packages/tensorflow/python/saved_model/load.py\", line 117, in _get_tensor_from_node\r\n    return obj.handle\r\n  File \"/data/pgaleone/env/lib/python3.6/site-packages/tensorflow/python/distribute/values.py\", line 317, in __getattr__\r\n    return getattr(self.get(), name)\r\nAttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'handle'\r\n```", "comments": ["Thanks for the report. @seemuch is looking into SavedModels + distribution strategies.", "This works now with the latest 2.0 nightly. Please check out https://www.tensorflow.org/beta/tutorials/distribute/save_and_load for more details.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28599\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28599\">No</a>\n"]}]