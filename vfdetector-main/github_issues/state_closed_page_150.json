[{"number": 50348, "title": "Bug in model.fit", "body": "When calling model.fit in tensorflow version 2.5.0 the validation_data parameter doesn't work if the model takes 2 or more inputs. Same code in tensorflow 2.3.0 works perfectly fine. \r\n\r\n\r\n`history = model.fit(\r\n\t[X_train_pairs[:, 0], X_train_pairs[:, 1]], np.array(y_train),\r\n  validation_data = ([X_val_pairs[:, 0], X_val_pairs[:, 1]], np.array(y_val)),\r\n\tbatch_size=256,\r\n\tepochs=25)`\r\n\r\nThis is my code. ", "comments": ["@JawadAr ,\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code and dataset to reproduce the issue reported here.\r\nThanks!", "```\r\ndef build_siamese_model(inputShape, embeddingDim=48):\r\n\t# specify the inputs for the feature extractor network\r\n\tinputs = Input(inputShape)\r\n\r\n\t# define the first set of CONV => RELU => POOL => DROPOUT layers\r\n\tx = Conv2D(64, (2, 2), padding=\"same\", activation=\"relu\")(inputs)\r\n\tx = MaxPooling2D(pool_size=(2, 2))(x)\r\n\tx = Dropout(0.3)(x)\r\n\r\n\t# second set of CONV => RELU => POOL => DROPOUT layers\r\n\tx = Conv2D(64, (2, 2), padding=\"same\", activation=\"relu\")(x)\r\n\tx = MaxPooling2D(pool_size=2)(x)\r\n\tx = Dropout(0.3)(x)\r\n\r\n        # prepare the final outputs\r\n\tpooledOutput = GlobalAveragePooling2D()(x)\r\n\toutputs = Dense(embeddingDim)(pooledOutput)\r\n\r\n\t# build the model\r\n\tmodel = Model(inputs, outputs)\r\n\r\n\t# return the model to the calling function\r\n\treturn model\r\n\r\ndef euclidean_distance(vectors):\r\n\t# unpack the vectors into separate lists\r\n\t(featsA, featsB) = vectors\r\n\r\n\t# compute the sum of squared distances between the vectors\r\n\tsumSquared = K.sum(K.square(featsA - featsB), axis=1,\r\n\t\tkeepdims=True)\r\n\r\n\t# return the euclidean distance between the vectors\r\n\treturn K.sqrt(K.maximum(sumSquared, K.epsilon()))\r\n\r\n\r\nprint(\"[INFO] building siamese network...\")\r\nimgA = Input(shape=config.IMG_SHAPE)\r\nimgB = Input(shape=config.IMG_SHAPE)\r\nfeatureExtractor = build_siamese_model(config.IMG_SHAPE)\r\nfeatsA = featureExtractor(imgA)\r\nfeatsB = featureExtractor(imgB)\r\n\r\ndistance = Lambda(utils.euclidean_distance)([featsA, featsB])\r\noutputs = Dense(1, activation=\"sigmoid\")(distance)\r\nmodel = Model(inputs=[imgA, imgB], outputs=outputs)\r\n```\r\n\r\n**_This is the code. The data is mnist and can be prepared using make_pairs function below:_**\r\n\r\n```\r\ndef make_pairs(images, labels):\r\n\t# initialize two empty lists to hold the (image, image) pairs and\r\n\t# labels to indicate if a pair is positive or negative\r\n\tpairImages = []\r\n\tpairLabels = []\r\n\t# calculate the total number of classes present in the dataset\r\n\t# and then build a list of indexes for each class label that\r\n\t# provides the indexes for all examples with a given label\r\n\tnumClasses = len(np.unique(labels))\r\n\tidx = [np.where(labels == i)[0] for i in range(0, numClasses)]\r\n\t# loop over all images\r\n\tfor idxA in range(len(images)):\r\n\t\t# grab the current image and label belonging to the current\r\n\t\t# iteration\r\n\t\tcurrentImage = images[idxA]\r\n\t\tlabel = labels[idxA]\r\n\t\t# randomly pick an image that belongs to the *same* class\r\n\t\t# label\r\n\t\tidxB = np.random.choice(idx[label])\r\n\t\tposImage = images[idxB]\r\n\t\t# prepare a positive pair and update the images and labels\r\n\t\t# lists, respectively\r\n\t\tpairImages.append([currentImage, posImage])\r\n\t\tpairLabels.append([1])\r\n\t\t# grab the indices for each of the class labels *not* equal to\r\n\t\t# the current label and randomly pick an image corresponding\r\n\t\t# to a label *not* equal to the current label\r\n\t\tnegIdx = np.where(labels != label)[0]\r\n\t\tnegImage = images[np.random.choice(negIdx)]\r\n\t\t# prepare a negative pair of images and update our lists\r\n\t\tpairImages.append([currentImage, negImage])\r\n\t\tpairLabels.append([0])\r\n\t# return a 2-tuple of our image pairs and labels\r\n\treturn (np.array(pairImages), np.array(pairLabels))\r\n\r\n# load MNIST dataset and scale the pixel values to the range of [0, 1]\r\nprint(\"[INFO] loading MNIST dataset...\")\r\n(trainX, trainY), (testX, testY) = mnist.load_data()\r\ntrainX = trainX / 255.0\r\ntestX = testX / 255.0\r\n# add a channel dimension to the images\r\ntrainX = np.expand_dims(trainX, axis=-1)\r\ntestX = np.expand_dims(testX, axis=-1)\r\n# prepare the positive and negative pairs\r\nprint(\"[INFO] preparing positive and negative pairs...\")\r\n(pairTrain, labelTrain) = utils.make_pairs(trainX, trainY)\r\n(pairTest, labelTest) = utils.make_pairs(testX, testY)\r\n```\r\n\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Please don't close it, work on it.", "@JawadAr ,\r\n\r\nLooks like the code is incompleted.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/53eabc52f366dddd315ef7740fcd5cf1/untitled50348.ipynb).Can you please provide the complete code and dataset to trouble-shoot the issue.Thanks!", "oh, I have to send the whole working code? The other part of the code is confidential by the client. I have sent all of the parts causing issue, if these are not enough I think the issue can be closed.", "@JawadAr ,\r\n\r\nPlease refer this link for more information on model.fit.It helps.[Link](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit).Also please feel free to move this issue to closed status if issue has resolved.Thanks ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50348\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50348\">No</a>\n"]}, {"number": 50347, "title": "Build TFLite Micro for riscv32_mcu", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux  Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): N/A\r\n- TensorFlow version: Tensorflow 2.4.2\r\n- Python version: 3.8.5\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): gcc 9.3.0, riscv64-unknown-elf-gcc (GCC) 10.2.0\r\n- CUDA/cuDNN version: N/A \r\n- GPU model and memory: N/A\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): RISC-V\r\n\r\n\r\n**Describe the problem**\r\nCannot build TFLite Micro for RISC-V target (TARGET=riscv32_mcu). The error message is shown at the bottom.\r\nThe command below doesn't work since the file _riscv32_mcu_makefile.inc_ doesn't exist.\r\n`make -f tensorflow/lite/micro/tools/make/Makefile TARGET=riscv32_mcu hello_world`\r\n\r\nDoes anyone successfully build the hello_world example for RISCV (riscv32_mcu)? \r\nWhat are the exact commands and steps?\r\n\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n1. `git clone --depth 1 https://github.com/tensorflow/tensorflow.git`\r\n2. `cd tensorflow`\r\n3. `vi tensorflow/lite/micro/tools/make/targets/mcu_riscv_makefile.inc`\r\n4. change `ifeq ($(TARGET), riscv32_mcu)` to `ifeq ($(TARGET), mcu_riscv)`  \r\n5. Move two FLAGS (`-fno-threadsafe-statics` and `-fno-use-cxa-atexit`) from PLATFORM_FLAGS to CXXFLAGS\r\n6. `make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mcu_riscv TARGET_ARCH=riscv32_mcu hello_world`\r\n\r\n\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\r\n\r\n$ make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mcu_riscv TARGET_ARCH=riscv32_mcu hello_world\r\ntensorflow/lite/micro/tools/make/downloads/flatbuffers already exists, skipping the download.\r\ntensorflow/lite/micro/tools/make/downloads/pigweed already exists, skipping the download.\r\ntensorflow/lite/micro/tools/make/downloads/person_model_int8 already exists, skipping the download.\r\nriscv64-unknown-elf-g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -Werror -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -DTF_LITE_DISABLE_X86_NEON -O3 -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter -DMCU_RISCV -march=rv32imac -mabi=ilp32 -mcmodel=medany -mexplicit-relocs -fno-builtin-printf -fno-exceptions -DTF_LITE_MCU_DEBUG_LOG -DTF_LITE_USE_GLOBAL_CMATH_FUNCTIONS -fno-unwind-tables -ffunction-sections -fdata-sections -funsigned-char -Wvla -Wall -Wextra -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wno-unused-parameter -Wno-write-strings -Wunused-function -fno-delete-null-pointer-checks -fomit-frame-pointer -Os -fpermissive -fno-rtti -fno-threadsafe-statics -fno-use-cxa-atexit --std=gnu++11 -I. -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/include -Itensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/drivers/ -Itensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env -Itensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/freedom-e300-hifive1 -Itensorflow/lite/micro/tools/make/downloads/kissfft -o tensorflow/lite/micro/tools/make/gen/mcu_riscv_riscv32_mcu_micro/bin/hello_world tensorflow/lite/micro/tools/make/gen/mcu_riscv_riscv32_mcu_micro/obj/tensorflow/lite/micro/examples/hello_world/main.o tensorflow/lite/micro/tools/make/gen/mcu_riscv_riscv32_mcu_micro/obj/tensorflow/lite/micro/examples/hello_world/main_functions.o tensorflow/lite/micro/tools/make/gen/mcu_riscv_riscv32_mcu_micro/obj/tensorflow/lite/micro/examples/hello_world/model.o tensorflow/lite/micro/tools/make/gen/mcu_riscv_riscv32_mcu_micro/obj/tensorflow/lite/micro/examples/hello_world/output_handler.o tensorflow/lite/micro/tools/make/gen/mcu_riscv_riscv32_mcu_micro/obj/tensorflow/lite/micro/examples/hello_world/constants.o tensorflow/lite/micro/tools/make/gen/mcu_riscv_riscv32_mcu_micro/lib/libtensorflow-microlite.a -Wl,--fatal-warnings -Wl,--gc-sections -Ttensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/freedom-e300-hifive1/flash.lds -nostartfiles -Ltensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env --specs=nano.specs -lm\r\n/opt/riscv/lib/gcc/riscv64-unknown-elf/10.2.0/../../../../riscv64-unknown-elf/bin/ld: warning: cannot find entry symbol _start; not setting start address\r\ncollect2: error: ld returned 1 exit status\r\nmake: *** [tensorflow/lite/micro/examples/hello_world/Makefile.inc:44: tensorflow/lite/micro/tools/make/gen/mcu_riscv_riscv32_mcu_micro/bin/hello_world] Error 1\r\n\r\n\r\n", "comments": ["I see tf-lite-micro has its own standalone repository now. \r\nI see makefile inside respective example folders.\r\nhttps://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/examples/hello_world/riscv32_mcu/Makefile.inc", "Thanks for sharing the link. However, these examples still can't be successfully built for riscv32_mcu in tf-lite-micro's own repository.\r\n\r\nIf we want to build the example in this repository, are there any steps/commands that work well?", "Closing this issue in the light of https://github.com/tensorflow/tflite-micro/issues/203\r\nThanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50347\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50347\">No</a>\n"]}, {"number": 50346, "title": "What direction is tensorflow moving to ", "body": "I decided to formulate this as an issue as did not find any more appropriate place to write it.\r\n\r\nCurrently, there is a huge work done in between  tensorflow1 to tensorflow2. Tf 1 was heavily dependent on graphs, while tf2 has moved to functions. Meanwhile, there is a  `tf.compat` which tries to establish compatibility.  All this stuff rises some complexity. e.g,  someone may accounter to :  I want to eliminate variables so that my model run faster ( not with tf serving) ... ah... ok, I need to use tf.compat.v1 then transform it to graph... then you encounter with different methods of loading a model, with some functions in tf , some functions in tf.compat.v1 etc. All this stuff become not intuitive . I understand that all this stuff is transitional and will settle down at some point.  What would be nice to know in advance is where is it moving, so what is the destination. Is it a process of getting rid of graphs and becoming something similar to pytorch API ?Or,  will it settle down and become some nice combination of graphs+ functions so that the api is clear and comprehensible? What a tensorflow user should expect , what should they try to understand and what will be gone ? What will be the model of tensorflow in the future ?\r\n\r\nIt would be nice if you could explain your vision related to tensorflow  either in this issue or some different material. ", "comments": ["`@tf.function` decorator takes a Python imperative TF eager code and builds a graph behind the scenes.\r\n\r\nSince this is not discussing bugs/features, but more a question about philosophy, maybe it's better to ask it on the [Community Forum](https://discuss.tensorflow.org/) instead?", "Closing the issue in favor of Mihai's comment. Thanks!"]}, {"number": 50344, "title": "Request for support of ExtractImagePatches in Tensorflow Lite", "body": "**System information**\r\n-  Ubuntu 20.04:\r\n- TensorFlow version 2.5:\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, ELU, L2_NORMALIZATION, LOGISTIC, MAX_POOL_2D, MUL, RESHAPE, RESIZE_BILINEAR, SOFTMAX, SUB, TRANSPOSE. Here is a list of operators for which you will need custom implementations: ExtractImagePatches.\r\n```\r\n", "comments": ["How can one give custom define tf.image.extract_patches op for converting model to a tflite model? @saikumarchalla @jvishnuvardhan ", "Hi @namanbiyani \r\nYou can leverage TF SELECT option during conversion and run this Op as TF op.\r\nSee the [guide](https://www.tensorflow.org/lite/guide/ops_select#convert_a_model)", "Yes, I could convert using that after changing version. Also, how do i set inference input output type as int8, its showing the error for this line :- converter.inference_input_type = tf.int8, that it can be assigned only float32. @karimnosseir ", "@MeghnaNatraj Can you provide instructions for setting input/output\r\n\r\nThanks", "@namanbiyani You can try [Integer with float fallback (using default float input/output)](https://www.tensorflow.org/lite/performance/post_training_quantization#integer_with_float_fallback_using_default_float_inputoutput) but use the `inference_input_type` and `inference_output_type` flags\r\n\r\n```\r\nimport tensorflow as tf\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_dataset\r\nconverter.inference_input_type = tf.int8  # or tf.uint8\r\nconverter.inference_output_type = tf.int8  # or tf.uint8\r\ntflite_quant_model = converter.convert()\r\n```"]}, {"number": 50343, "title": "NotImplementedError: Cannot convert a symbolic Tensor to a numpy array", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): stock\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 11\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source and binary\r\n- TensorFlow version (use command below): 2.5 and nightly\r\n- Python version: 3.9\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): gcc11 \r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**Describe the current behavior**\r\n`tf.reduce_mean` fails on a ragged tensor with `NotImplemeted` exception (see full error below). It happens in graph (non-eager) mode only. Furthermore, it *probably* does not happen with order versions of Numpy (1.19.*) but it definitely happens on Numpy 1.20.3\r\n\r\n**Describe the expected behavior**\r\nIt must calculate means.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): yes\r\n- Briefly describe your candidate solution(if contributing):\r\n * The current code does not catch this type of exception which is probably new in Numpy 1.20\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nFollowing is the full error message.\r\n\r\n```\r\nNotImplementedError: in user code:\r\n\r\n    /home/eli/virtenvs/tf_mkl/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:869 train_function  *\r\n        return step_function(self, iterator)\r\n    /home/eli/virtenvs/tf_mkl/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:859 step_function  **\r\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n    /home/eli/virtenvs/tf_mkl/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:1286 run\r\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    /home/eli/virtenvs/tf_mkl/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:2849 call_for_each_replica\r\n        return self._call_for_each_replica(fn, args, kwargs)\r\n    /home/eli/virtenvs/tf_mkl/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:3632 _call_for_each_replica\r\n        return fn(*args, **kwargs)\r\n    /home/eli/virtenvs/tf_mkl/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:852 run_step  **\r\n        outputs = model.train_step(data)\r\n    /home/eli/virtenvs/tf_mkl/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:813 train_step\r\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\r\n    /home/eli/virtenvs/tf_mkl/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:539 minimize\r\n        grads_and_vars = self._compute_gradients(\r\n    /home/eli/virtenvs/tf_mkl/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:591 _compute_gradients\r\n        grads_and_vars = self._get_gradients(tape, loss, var_list, grad_loss)\r\n    /home/eli/virtenvs/tf_mkl/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:473 _get_gradients\r\n        grads = tape.gradient(loss, var_list, grad_loss)\r\n    /home/eli/virtenvs/tf_mkl/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py:1084 gradient\r\n        flat_grad = imperative_grad.imperative_grad(\r\n    /home/eli/virtenvs/tf_mkl/lib/python3.9/site-packages/tensorflow/python/eager/imperative_grad.py:71 imperative_grad\r\n        return pywrap_tfe.TFE_Py_TapeGradient(\r\n    /home/eli/virtenvs/tf_mkl/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py:159 _gradient_function\r\n        return grad_fn(mock_op, *out_grads)\r\n    /home/eli/virtenvs/tf_mkl/lib/python3.9/site-packages/tensorflow/python/ops/math_grad.py:522 _UnsortedSegmentSumGrad\r\n        return _GatherDropNegatives(grad, op.inputs[1])[0], None, None\r\n    /home/eli/virtenvs/tf_mkl/lib/python3.9/site-packages/tensorflow/python/ops/math_grad.py:488 _GatherDropNegatives\r\n        array_ops.ones([array_ops.rank(gathered)\r\n    /home/eli/virtenvs/tf_mkl/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206 wrapper\r\n        return target(*args, **kwargs)\r\n    /home/eli/virtenvs/tf_mkl/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py:3216 ones\r\n        output = _constant_if_small(one, shape, dtype, name)\r\n    /home/eli/virtenvs/tf_mkl/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py:2900 _constant_if_small\r\n        if np.prod(shape) < 1000:\r\n    <__array_function__ internals>:5 prod\r\n        \r\n    /home/eli/virtenvs/tf_mkl/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3030 prod\r\n        return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,\r\n    /home/eli/virtenvs/tf_mkl/lib/python3.9/site-packages/numpy/core/fromnumeric.py:87 _wrapreduction\r\n        return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\r\n    /home/eli/virtenvs/tf_mkl/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:867 __array__\r\n        raise NotImplementedError(\r\n\r\n    NotImplementedError: Cannot convert a symbolic Tensor (gradient_tape/tree_model/inner_node/RaggedReduceMean/RaggedReduceSum/sub:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported\r\n```", "comments": ["Closing this issue because very latest code has the fix already (merged just recently):\r\nhttps://github.com/tensorflow/tensorflow/blob/b731fef171004dd30ce8fddf9ba24dad9af7dc31/tensorflow/python/ops/array_ops.py#L2902", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50343\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50343\">No</a>\n"]}, {"number": 50342, "title": "[MLIR] Add conversion of unary operation from lmhlo to affine", "body": "Unary op converter has been added from lmhlo to affine dialect.\r\nThe changes have been made as a part of `lhlo-legalize-to-affine` pass.", "comments": []}, {"number": 50341, "title": "fatal error C1007", "body": "fatal error C1007: \u65e0\u6cd5\u8bc6\u522b\u7684\u6807\u5fd7\u201c-ReducedOptimizeHugeFunctions\u201d(\u5728\u201cp2\u201d\u4e2d)\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 16.981s, Critical Path: 1.50s\r\nINFO: 8 processes: 8 internal.\r\nFAILED: Build did NOT complete successfully\r\n", "comments": ["@XDSwang ,\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the following information\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary):\r\nTensorFlow version:\r\nPython version:\r\nInstalled using virtualenv? pip? conda?:\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\n \r\nand the exact sequence of commands / steps that you executed before running into the problem\r\n\r\nThanks!", "win64\r\nTensorFlow-master\r\npython3.6/3.88/3.95\r\nvirtualenv/conda\r\nvs2019/c++2019..\r\ncuda10.2/cudnn8.2\r\nbazel3.7.2/5.\r\n...\r\n@tilakrayal \r\n Used above ", "@XDSwang ,\r\n\r\nInstallation issues within the Anaconda environment are tracked in the Anaconda repo.\r\n \r\nCould you please submit a new issue using [this link](https://github.com/ContinuumIO/anaconda-issues/issues) and fill in the template, so that the issue can be tracked there. Thanks!", "You need a very recent MSVC compiler", "\r\n\r\n> You need a very recent MSVC compiler\r\nThe latest is not 2019?\r\nAm I wrong about my circumstances\r\n\r\n\r\n", "See https://www.tensorflow.org/install/source_windows#install_visual_c_build_tools_2019\r\n\r\nMake sure you have both the redistributable and the tools installed, not only the IDE", "> See https://www.tensorflow.org/install/source_windows#install_visual_c_build_tools_2019\r\n> \r\n> Make sure you have both the redistributable and the tools installed, not only the IDE\r\n\r\n```\r\ntensorflow/python/lib/core/bfloat16.cc(633): error C2664: \u201cbool tensorflow::`anonymous-namespa\r\nce'::Initialize::<lambda_d8b2248041b81857d7d292521348d314>::operator ()(const char *,PyUFuncGen\r\nericFunction,const std::array<int,3> &) const\u201d: \u65e0\u6cd5\u5c06\u53c2\u6570 2 \u4ece\u201cvoid (__cdecl *)(char **,npy_\r\nintp *,npy_intp *,void *)\u201d\u8f6c\u6362\u4e3a\u201cPyUFuncGenericFunction\u201d\r\ntensorflow/python/lib/core/bfloat16.cc(633): note: \u5728\u5339\u914d\u76ee\u6807\u7c7b\u578b\u7684\u8303\u56f4\u5185\u6ca1\u6709\u5177\u6709\u8be5\u540d\u79f0\u7684\u51fd\u6570\r\ntensorflow/python/lib/core/bfloat16.cc(627): note: \u53c2\u89c1\u201ctensorflow::`anonymous-namespace'::Ini\r\ntialize::<lambda_d8b2248041b81857d7d292521348d314>::operator ()\u201d\u7684\u58f0\u660e\r\ntensorflow/python/lib/core/bfloat16.cc(637): error C2664: \u201cbool tensorflow::`anonymous-namespa\r\nce'::Initialize::<lambda_d8b2248041b81857d7d292521348d314>::operator ()(const char *,PyUFuncGen\r\nericFunction,const std::array<int,3> &) const\u201d: \u65e0\u6cd5\u5c06\u53c2\u6570 2 \u4ece\u201cvoid (__cdecl *)(char **,npy_\r\nintp *,npy_intp *,void *)\u201d\u8f6c\u6362\u4e3a\u201cPyUFuncGenericFunction\u201d\r\ntensorflow/python/lib/core/bfloat16.cc(637): note: \u5728\u5339\u914d\u76ee\u6807\u7c7b\u578b\u7684\u8303\u56f4\u5185\u6ca1\u6709\u5177\u6709\u8be5\u540d\u79f0\u7684\u51fd\u6570\r\ntensorflow/python/lib/core/bfloat16.cc(627): note: \u53c2\u89c1\u201ctensorflow::`anonymous-namespace'::Ini\r\ntialize::<lambda_d8b2248041b81857d7d292521348d314>::operator ()\u201d\u7684\u58f0\u660e\r\ntensorflow/python/lib/core/bfloat16.cc(641): error C2664: \u201cbool tensorflow::`anonymous-namespa\r\nce'::Initialize::<lambda_d8b2248041b81857d7d292521348d314>::operator ()(const char *,PyUFuncGen\r\nericFunction,const std::array<int,3> &) const\u201d: \u65e0\u6cd5\u5c06\u53c2\u6570 2 \u4ece\u201cvoid (__cdecl *)(char **,npy_\r\nintp *,npy_intp *,void *)\u201d\u8f6c\u6362\u4e3a\u201cPyUFuncGenericFunction\u201d\r\ntensorflow/python/lib/core/bfloat16.cc(641): note: \u5728\u5339\u914d\u76ee\u6807\u7c7b\u578b\u7684\u8303\u56f4\u5185\u6ca1\u6709\u5177\u6709\u8be5\u540d\u79f0\u7684\u51fd\u6570\r\ntensorflow/python/lib/core/bfloat16.cc(627): note: \u53c2\u89c1\u201ctensorflow::`anonymous-namespace'::Ini\r\ntialize::<lambda_d8b2248041b81857d7d292521348d314>::operator ()\u201d\u7684\u58f0\u660e\r\ntensorflow/python/lib/core/bfloat16.cc(644): error C2664: \u201cbool tensorflow::`anonymous-namespa\r\nce'::Initialize::<lambda_d8b2248041b81857d7d292521348d314>::operator ()(const char *,PyUFuncGen\r\nericFunction,const std::array<int,3> &) const\u201d: \u65e0\u6cd5\u5c06\u53c2\u6570 2 \u4ece\u201cvoid (__cdecl *)(char **,npy_\r\nintp *,npy_intp *,void *)\u201d\u8f6c\u6362\u4e3a\u201cPyUFuncGenericFunction\u201d\r\ntensorflow/python/lib/core/bfloat16.cc(644): note: \u5728\u5339\u914d\u76ee\u6807\u7c7b\u578b\u7684\u8303\u56f4\u5185\u6ca1\u6709\u5177\u6709\u8be5\u540d\u79f0\u7684\u51fd\u6570\r\ntensorflow/python/lib/core/bfloat16.cc(627): note: \u53c2\u89c1\u201ctensorflow::`anonymous-namespace'::Ini\r\ntialize::<lambda_d8b2248041b81857d7d292521348d314>::operator ()\u201d\u7684\u58f0\u660e\r\ntensorflow/python/lib/core/bfloat16.cc(648): error C2664: \u201cbool tensorflow::`anonymous-namespa\r\nce'::Initialize::<lambda_d8b2248041b81857d7d292521348d314>::operator ()(const char *,PyUFuncGen\r\nericFunction,const std::array<int,3> &) const\u201d: \u65e0\u6cd5\u5c06\u53c2\u6570 2 \u4ece\u201cvoid (__cdecl *)(char **,npy_\r\nintp *,npy_intp *,void *)\u201d\u8f6c\u6362\u4e3a\u201cPyUFuncGenericFunction\u201d\r\ntensorflow/python/lib/core/bfloat16.cc(648): note: \u5728\u5339\u914d\u76ee\u6807\u7c7b\u578b\u7684\u8303\u56f4\u5185\u6ca1\u6709\u5177\u6709\u8be5\u540d\u79f0\u7684\u51fd\u6570\r\ntensorflow/python/lib/core/bfloat16.cc(627): note: \u53c2\u89c1\u201ctensorflow::`anonymous-namespace'::Ini\r\ntialize::<lambda_d8b2248041b81857d7d292521348d314>::operator ()\u201d\u7684\u58f0\u660e\r\ntensorflow/python/lib/core/bfloat16.cc(652): error C2664: \u201cbool tensorflow::`anonymous-namespa\r\nce'::Initialize::<lambda_d8b2248041b81857d7d292521348d314>::operator ()(const char *,PyUFuncGen\r\nericFunction,const std::array<int,3> &) const\u201d: \u65e0\u6cd5\u5c06\u53c2\u6570 2 \u4ece\u201cvoid (__cdecl *)(char **,npy_\r\nintp *,npy_intp *,void *)\u201d\u8f6c\u6362\u4e3a\u201cPyUFuncGenericFunction\u201d\r\ntensorflow/python/lib/core/bfloat16.cc(652): note: \u5728\u5339\u914d\u76ee\u6807\u7c7b\u578b\u7684\u8303\u56f4\u5185\u6ca1\u6709\u5177\u6709\u8be5\u540d\u79f0\u7684\u51fd\u6570\r\ntensorflow/python/lib/core/bfloat16.cc(627): note: \u53c2\u89c1\u201ctensorflow::`anonymous-namespace'::Ini\r\ntialize::<lambda_d8b2248041b81857d7d292521348d314>::operator ()\u201d\u7684\u58f0\u660e\r\n[13,438 / 13,648] 75 actions, 3 running\r\n    Compiling tensorflow/c/c_api_experimental.cc; 13s local\r\n[13,441 / 13,648] 72 actions, 0 running\r\n    [Scann] Compiling tensorflow/core/grappler/optimizers/data/map_vectorization.cc; 549s\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n[13,441 / 13,648] 72 actions, 0 running\r\n    [Scann] Compiling tensorflow/core/grappler/optimizers/data/map_vectorization.cc; 549s\r\nINFO: Elapsed time: 15910.390s, Critical Path: 393.21s\r\n[13,441 / 13,648] 72 actions, 0 running\r\n    [Scann] Compiling tensorflow/core/grappler/optimizers/data/map_vectorization.cc; 549s\r\nINFO: 10536 processes: 10536 local.\r\n[13,441 / 13,648] 72 actions, 0 running\r\n    [Scann] Compiling tensorflow/core/grappler/optimizers/data/map_vectorization.cc; 549s\r\nFAILED: Build did NOT complete successfully\r\n```\r\n???\r\n\r\n", "What version of `numpy` are you using?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50341\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50341\">No</a>\n"]}, {"number": 50340, "title": "Failed to build TensorFlow package builder", "body": "Can anyone help me fix this error:\r\n```\r\n$ bazel build --config=opt --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow/tools/pip_package:build_pip_package\r\n...\r\nERROR: /mnt/Archive/Downloads/tensorflow/tensorflow/tools/pip_package/BUILD:179:10: error loading package '@com_github_grpc_grpc//': at /home/notooth/.cache/bazel/_bazel_notooth/d5b3b610a5827abc5897d0ec1ef6cd79/external/com_github_grpc_grpc/bazel/grpc_build_system.bzl:28:6: at /home/notooth/.cache/bazel/_bazel_notooth/d5b3b610a5827abc5897d0ec1ef6cd79/external/build_bazel_rules_apple/apple/ios.bzl:37:5: initialization of module 'apple/internal/ios_rules.bzl' failed and referenced by '//tensorflow/tools/pip_package:licenses'\r\n...\r\n```", "comments": ["Can you fill in issue template? What is the commit you are building at?\r\n\r\nWhat is the full error log?", "> Can you fill in issue template? What is the commit you are building at?\r\n\r\nMy system information:\r\n- Debian 11\r\n- HP DL380 G7\r\n- TensorFlow r2.5 failed to install from source\r\n- Python 3.9.2\r\n- Installed using virtualenv 20.4.7, pip 21.1.2\r\n- Bazel 5.0.0-pre.20210604.6\r\n- GCC 10.2.1\r\n- No CUDA\r\n\r\nI am building Tensorflow to use Google T5.\r\n\r\nHere is the full error log:\r\n```\r\n$ bazel build --config=opt --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow/tools/pip_package:build_pip_package\r\nStarting local Bazel server and connecting to it...\r\nWARNING: Option 'java_toolchain' is deprecated\r\nWARNING: Option 'host_java_toolchain' is deprecated\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=121\r\nINFO: Reading rc options for 'build' from /mnt/Archive/Downloads/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /mnt/Archive/Downloads/tensorflow/.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2\r\nINFO: Reading rc options for 'build' from /mnt/Archive/Downloads/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/home/notooth/.venv-py3/bin/python3 --action_env PYTHON_LIB_PATH=/home/notooth/.venv-py3/lib/python3.9/site-packages --python_path=/home/notooth/.venv-py3/bin/python3\r\nINFO: Found applicable config definition build:short_logs in file /mnt/Archive/Downloads/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /mnt/Archive/Downloads/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:opt in file /mnt/Archive/Downloads/tensorflow/.tf_configure.bazelrc: --copt=-march=native --host_copt=-march=native --copt=-mssse3 --host_copt=-mssse3 --copt=-mcx16 --host_copt=-mcx16 --copt=-msse4.1 --host_copt=-msse4.1 --copt=-msse4.2 --host_copt=-msse4.2 --copt=-mpopcnt --host_copt=-mpopcnt\r\nINFO: Found applicable config definition build:linux in file /mnt/Archive/Downloads/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /mnt/Archive/Downloads/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nERROR: Traceback (most recent call last):\r\n        File \"/home/notooth/.cache/bazel/_bazel_notooth/d5b3b610a5827abc5897d0ec1ef6cd79/external/build_bazel_rules_apple/apple/internal/testing/ios_rules.bzl\", line 63, column 71, in <toplevel>\r\n                _ios_internal_ui_test_bundle = rule_factory.create_apple_bundling_rule(\r\n        File \"/home/notooth/.cache/bazel/_bazel_notooth/d5b3b610a5827abc5897d0ec1ef6cd79/external/build_bazel_rules_apple/apple/internal/rule_factory.bzl\", line 955, column 55, in _create_apple_bundling_rule\r\n                rule_attrs.append(_common_binary_linking_attrs(rule_descriptor))\r\n        File \"/home/notooth/.cache/bazel/_bazel_notooth/d5b3b610a5827abc5897d0ec1ef6cd79/external/build_bazel_rules_apple/apple/internal/rule_factory.bzl\", line 233, column 21, in _common_binary_linking_attrs\r\n                apple_common.objc_proto_aspect,\r\nError: 'apple_common' value has no field or method 'objc_proto_aspect'\r\nINFO: Repository functools32_archive instantiated at:\r\n  /mnt/Archive/Downloads/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  /mnt/Archive/Downloads/tensorflow/tensorflow/workspace2.bzl:1118:21: in workspace\r\n  /mnt/Archive/Downloads/tensorflow/tensorflow/workspace2.bzl:418:20: in _tf_repositories\r\n  /mnt/Archive/Downloads/tensorflow/third_party/repo.bzl:112:21: in tf_http_archive\r\nRepository rule _tf_http_archive defined at:\r\n  /mnt/Archive/Downloads/tensorflow/third_party/repo.bzl:65:35: in <toplevel>\r\nINFO: Repository go_sdk instantiated at:\r\n  /mnt/Archive/Downloads/tensorflow/WORKSPACE:23:14: in <toplevel>\r\n  /mnt/Archive/Downloads/tensorflow/tensorflow/workspace0.bzl:117:20: in workspace\r\n  /home/notooth/.cache/bazel/_bazel_notooth/d5b3b610a5827abc5897d0ec1ef6cd79/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps\r\n  /home/notooth/.cache/bazel/_bazel_notooth/d5b3b610a5827abc5897d0ec1ef6cd79/external/io_bazel_rules_go/go/toolchain/toolchains.bzl:379:28: in go_register_toolchains\r\n  /home/notooth/.cache/bazel/_bazel_notooth/d5b3b610a5827abc5897d0ec1ef6cd79/external/io_bazel_rules_go/go/private/sdk.bzl:65:21: in go_download_sdk\r\nRepository rule _go_download_sdk defined at:\r\n  /home/notooth/.cache/bazel/_bazel_notooth/d5b3b610a5827abc5897d0ec1ef6cd79/external/io_bazel_rules_go/go/private/sdk.bzl:53:35: in <toplevel>\r\nINFO: Repository highwayhash instantiated at:\r\n  /mnt/Archive/Downloads/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  /mnt/Archive/Downloads/tensorflow/tensorflow/workspace2.bzl:1111:28: in workspace\r\n  /mnt/Archive/Downloads/tensorflow/tensorflow/workspace2.bzl:65:16: in _initialize_third_party\r\n  /mnt/Archive/Downloads/tensorflow/third_party/highwayhash/workspace.bzl:6:20: in repo\r\n  /mnt/Archive/Downloads/tensorflow/third_party/repo.bzl:112:21: in tf_http_archive\r\nRepository rule _tf_http_archive defined at:\r\n  /mnt/Archive/Downloads/tensorflow/third_party/repo.bzl:65:35: in <toplevel>\r\nINFO: Repository aws instantiated at:\r\n  /mnt/Archive/Downloads/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  /mnt/Archive/Downloads/tensorflow/tensorflow/workspace2.bzl:1111:28: in workspace\r\n  /mnt/Archive/Downloads/tensorflow/tensorflow/workspace2.bzl:56:8: in _initialize_third_party\r\n  /mnt/Archive/Downloads/tensorflow/third_party/aws/workspace.bzl:9:20: in repo\r\n  /mnt/Archive/Downloads/tensorflow/third_party/repo.bzl:112:21: in tf_http_archive\r\nRepository rule _tf_http_archive defined at:\r\n  /mnt/Archive/Downloads/tensorflow/third_party/repo.bzl:65:35: in <toplevel>\r\nINFO: Repository rules_java instantiated at:\r\n  /mnt/Archive/Downloads/tensorflow/WORKSPACE:23:14: in <toplevel>\r\n  /mnt/Archive/Downloads/tensorflow/tensorflow/workspace0.bzl:117:20: in workspace\r\n  /home/notooth/.cache/bazel/_bazel_notooth/d5b3b610a5827abc5897d0ec1ef6cd79/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:29:18: in grpc_extra_deps\r\n  /home/notooth/.cache/bazel/_bazel_notooth/d5b3b610a5827abc5897d0ec1ef6cd79/external/com_google_protobuf/protobuf_deps.bzl:34:21: in protobuf_deps\r\nRepository rule http_archive defined at:\r\n  /home/notooth/.cache/bazel/_bazel_notooth/d5b3b610a5827abc5897d0ec1ef6cd79/external/bazel_tools/tools/build_defs/repo/http.bzl:339:31: in <toplevel>\r\nINFO: Repository llvm-project instantiated at:\r\n  /mnt/Archive/Downloads/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  /mnt/Archive/Downloads/tensorflow/tensorflow/workspace2.bzl:1118:21: in workspace\r\n  /mnt/Archive/Downloads/tensorflow/tensorflow/workspace2.bzl:667:9: in _tf_repositories\r\n  /mnt/Archive/Downloads/tensorflow/third_party/llvm/workspace.bzl:10:20: in repo\r\n  /mnt/Archive/Downloads/tensorflow/third_party/repo.bzl:112:21: in tf_http_archive\r\nRepository rule _tf_http_archive defined at:\r\n  /mnt/Archive/Downloads/tensorflow/third_party/repo.bzl:65:35: in <toplevel>\r\nINFO: Repository enum34_archive instantiated at:\r\n  /mnt/Archive/Downloads/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  /mnt/Archive/Downloads/tensorflow/tensorflow/workspace2.bzl:1118:21: in workspace\r\n  /mnt/Archive/Downloads/tensorflow/tensorflow/workspace2.bzl:506:20: in _tf_repositories\r\n  /mnt/Archive/Downloads/tensorflow/third_party/repo.bzl:112:21: in tf_http_archive\r\nRepository rule _tf_http_archive defined at:\r\n  /mnt/Archive/Downloads/tensorflow/third_party/repo.bzl:65:35: in <toplevel>\r\nINFO: Repository rules_python instantiated at:\r\n  /mnt/Archive/Downloads/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  /mnt/Archive/Downloads/tensorflow/tensorflow/workspace2.bzl:1118:21: in workspace\r\n  /mnt/Archive/Downloads/tensorflow/tensorflow/workspace2.bzl:973:20: in _tf_repositories\r\n  /mnt/Archive/Downloads/tensorflow/third_party/repo.bzl:112:21: in tf_http_archive\r\nRepository rule _tf_http_archive defined at:\r\n  /mnt/Archive/Downloads/tensorflow/third_party/repo.bzl:65:35: in <toplevel>\r\nINFO: Repository clog instantiated at:\r\n  /mnt/Archive/Downloads/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  /mnt/Archive/Downloads/tensorflow/tensorflow/workspace2.bzl:1111:28: in workspace\r\n  /mnt/Archive/Downloads/tensorflow/tensorflow/workspace2.bzl:57:9: in _initialize_third_party\r\n  /mnt/Archive/Downloads/tensorflow/third_party/clog/workspace.bzl:6:20: in repo\r\n  /mnt/Archive/Downloads/tensorflow/third_party/repo.bzl:112:21: in tf_http_archive\r\nRepository rule _tf_http_archive defined at:\r\n  /mnt/Archive/Downloads/tensorflow/third_party/repo.bzl:65:35: in <toplevel>\r\nINFO: Repository sobol_data instantiated at:\r\n  /mnt/Archive/Downloads/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  /mnt/Archive/Downloads/tensorflow/tensorflow/workspace2.bzl:1111:28: in workspace\r\n  /mnt/Archive/Downloads/tensorflow/tensorflow/workspace2.bzl:75:15: in _initialize_third_party\r\n  /mnt/Archive/Downloads/tensorflow/third_party/sobol_data/workspace.bzl:6:20: in repo\r\n  /mnt/Archive/Downloads/tensorflow/third_party/repo.bzl:112:21: in tf_http_archive\r\nRepository rule _tf_http_archive defined at:\r\n  /mnt/Archive/Downloads/tensorflow/third_party/repo.bzl:65:35: in <toplevel>\r\nINFO: Repository ruy instantiated at:\r\n  /mnt/Archive/Downloads/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  /mnt/Archive/Downloads/tensorflow/tensorflow/workspace2.bzl:1111:28: in workspace\r\n  /mnt/Archive/Downloads/tensorflow/tensorflow/workspace2.bzl:74:8: in _initialize_third_party\r\n  /mnt/Archive/Downloads/tensorflow/third_party/ruy/workspace.bzl:6:20: in repo\r\n  /mnt/Archive/Downloads/tensorflow/third_party/repo.bzl:112:21: in tf_http_archive\r\nRepository rule _tf_http_archive defined at:\r\n  /mnt/Archive/Downloads/tensorflow/third_party/repo.bzl:65:35: in <toplevel>\r\nINFO: Repository absl_py instantiated at:\r\n  /mnt/Archive/Downloads/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  /mnt/Archive/Downloads/tensorflow/tensorflow/workspace2.bzl:1118:21: in workspace\r\n  /mnt/Archive/Downloads/tensorflow/tensorflow/workspace2.bzl:489:20: in _tf_repositories\r\n  /mnt/Archive/Downloads/tensorflow/third_party/repo.bzl:112:21: in tf_http_archive\r\nRepository rule _tf_http_archive defined at:\r\n  /mnt/Archive/Downloads/tensorflow/third_party/repo.bzl:65:35: in <toplevel>\r\nINFO: Repository gif instantiated at:\r\n  /mnt/Archive/Downloads/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  /mnt/Archive/Downloads/tensorflow/tensorflow/workspace2.bzl:1118:21: in workspace\r\n  /mnt/Archive/Downloads/tensorflow/tensorflow/workspace2.bzl:358:20: in _tf_repositories\r\n  /mnt/Archive/Downloads/tensorflow/third_party/repo.bzl:112:21: in tf_http_archive\r\nRepository rule _tf_http_archive defined at:\r\n  /mnt/Archive/Downloads/tensorflow/third_party/repo.bzl:65:35: in <toplevel>\r\nINFO: Repository rules_proto instantiated at:\r\n  /mnt/Archive/Downloads/tensorflow/WORKSPACE:23:14: in <toplevel>\r\n  /mnt/Archive/Downloads/tensorflow/tensorflow/workspace0.bzl:117:20: in workspace\r\n  /home/notooth/.cache/bazel/_bazel_notooth/d5b3b610a5827abc5897d0ec1ef6cd79/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:29:18: in grpc_extra_deps\r\n  /home/notooth/.cache/bazel/_bazel_notooth/d5b3b610a5827abc5897d0ec1ef6cd79/external/com_google_protobuf/protobuf_deps.bzl:42:21: in protobuf_deps\r\nRepository rule http_archive defined at:\r\n  /home/notooth/.cache/bazel/_bazel_notooth/d5b3b610a5827abc5897d0ec1ef6cd79/external/bazel_tools/tools/build_defs/repo/http.bzl:339:31: in <toplevel>\r\nERROR: /mnt/Archive/Downloads/tensorflow/tensorflow/tools/pip_package/BUILD:179:10: error loading package '@com_github_grpc_grpc//': at /home/notooth/.cache/bazel/_bazel_notooth/d5b3b610a5827abc5897d0ec1ef6cd79/external/com_github_grpc_grpc/bazel/grpc_build_system.bzl:28:6: at /home/notooth/.cache/bazel/_bazel_notooth/d5b3b610a5827abc5897d0ec1ef6cd79/external/build_bazel_rules_apple/apple/ios.bzl:22:5: initialization of module 'apple/internal/testing/ios_rules.bzl' failed and referenced by '//tensorflow/tools/pip_package:licenses'\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis failed\r\nINFO: Elapsed time: 8.916s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (193 packages loaded, 3622 targets configured)\r\n    currently loading: @com_google_protobuf// ... (3 packages)\r\n    Fetching ...docker; Cloning 251f6a68b439744094faff800cd029798edf9faa of https://github.com/bazelbuild/rules_docker.\\\r\ngit\r\n```\r\n", "```\r\nERROR: Traceback (most recent call last):\r\n        File \"/home/notooth/.cache/bazel/_bazel_notooth/d5b3b610a5827abc5897d0ec1ef6cd79/external/build_bazel_rules_apple/apple/internal/testing/ios_rules.bzl\", line 63, column 71, in <toplevel>\r\n                _ios_internal_ui_test_bundle = rule_factory.create_apple_bundling_rule(\r\n        File \"/home/notooth/.cache/bazel/_bazel_notooth/d5b3b610a5827abc5897d0ec1ef6cd79/external/build_bazel_rules_apple/apple/internal/rule_factory.bzl\", line 955, column 55, in _create_apple_bundling_rule\r\n                rule_attrs.append(_common_binary_linking_attrs(rule_descriptor))\r\n        File \"/home/notooth/.cache/bazel/_bazel_notooth/d5b3b610a5827abc5897d0ec1ef6cd79/external/build_bazel_rules_apple/apple/internal/rule_factory.bzl\", line 233, column 21, in _common_binary_linking_attrs\r\n                apple_common.objc_proto_aspect,\r\nError: 'apple_common' value has no field or method 'objc_proto_aspect'\r\n```\r\n\r\nSeems like a Bazel issue.\r\n\r\nAre you building from `r2.5` branch?", "> Are you building from `r2.5` branch?\r\n\r\nYes, I had already chosen Tensorflow r2.5:\r\n```\r\n$ git checkout  r2.5           \r\nBranch 'r2.5' set up to track remote branch 'r2.5' from 'origin'.\r\nSwitched to a new branch 'r2.5'\r\n```\r\n", "Can you try\r\n\r\n```\r\nbazel clean --expunge && bazel build  --config=opt --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\nIf it has the same error, then it is a Bazel issue, not a TF one", "> Can you try\r\n> \r\n> ```\r\n> bazel clean --expunge && bazel build  --config=opt --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow/tools/pip_package:build_pip_package\r\n> ```\r\n> \r\n> If it has the same error, then it is a Bazel issue, not a TF one\r\n\r\nI tried, and it had the same error. I think it is a Bazel issue.", "Hi @notooth1 ! Did you check  with [--define=no_tensorflow_py_deps=true](https://www.tensorflow.org/install/source_windows#bazel_build_options)  command in 2.8 too?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50340\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50340\">No</a>\n"]}, {"number": 50339, "title": "[MLIR][DISC] add a pass to lower some ops to library calls", "body": "This PR implements the logic to lower some specific ops to external library calls.\r\n\r\nHere the external function is model by a `disc_ral.dispatch` op. We use\r\n`disc_ral.dispatch` to serve as a unified entrance of disc external\r\ncalls due to following reasons.\r\n\r\n- `disc_ral.dispatch` ensures that the first argument is always the\r\n  `disc_ral.context`\r\n- `disc_ral.dispatch` simplifies the logic to handle different instantiations of\r\n  one op for different devices and different element types. For example, we may\r\n  have GEMM ops with different element types.", "comments": []}, {"number": 50338, "title": "No registered 'swish_f32' OpKernel for GPU devices compatible with node {{node swish_3/swish_f32}}", "body": "When the model trained from tensorflow 1.4 infer images with tensorflow 2.3, the following message reported :\r\n\r\n\"2021-06-18 16:00:41.022400: W tensorflow/core/grappler/utils/graph_view.cc:836] No registered 'swish_f32' OpKernel for GPU devices compatible with node {{node swish_3/swish_f32}}.  Registered:  <no registered kernels>\".   \r\n\r\nMy codes  implemented with tf 1.x  has been upgraded to tf2.3.  So i need to check  the model trained previously  can run normally with tf2.3.  Session  is replaced as tf.compat.v1.Session(). And the model trained with tf2.3 without this message during inference. There is also no error report when training.  Whether the message can be dropped?\r\n\r\n**System information**\r\n\r\n- OS Platform: Windows 7\r\n- TensorFlow version : 2.3\r\n- Python version: 3.7.1\r\n- cudatoolkit  version: 10.1\r\n- GPU : RTX 2060\r\n\r\n\r\n", "comments": ["@lyw615 \r\nCould you please share the simple standalone code to reproduce the issue reported here.\r\nAlso refer similar issues [link1](https://github.com/google/automl/issues/24), [link2](https://www.programmersought.com/article/58604670525/) .Thanks", "I have tried the two links. But there is no   \" relu_fn = tf.nn.swish\" in my code. And  my code is mixed with tf and  tf.keras.", "The message only reported **once** during the operation  \"self.sess.run(y, feed_dict={x: predict_tile[i * batch_size:(i + 1) * batch_size, :]})\" .  And it company with the following message :\r\n\r\n2021-06-21 10:35:45.565000: W tensorflow/core/grappler/utils/graph_view.cc:836] No registered 'swish_f32' OpKernel for GPU devices compatible with node {{node swish_69/swish_f32}}\r\n\t.  Registered:  <no registered kernels>\r\n2021-06-21 10:35:45.988000: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2021-06-21 10:35:46.387000: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2021-06-21 10:35:47.976000: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: Invoking GPU asm compilation is supported on Cuda non-Windows platforms only\r\nRelying on driver to perform ptx compilation. \r\nModify $PATH to customize ptxas location.\r\nThis message will be only logged once.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@lyw615 \r\n\r\nCould you please provide the standalone code/colab gist.Thanks", "Sorry\uff0c but the project is commercial code. Maybe i will ignore the problem by adding \"os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\"", "@lyw615 \r\n\r\nCould you please refer this [blog](https://dobromyslova.medium.com/making-work-tensorflow-with-nvidia-rtx-3090-on-windows-10-7a38e8e582bf) and also see the  [comment](https://stackoverflow.com/questions/65676011/invoking-gpu-asm-compilation-is-supported-on-cuda-non-windows-platforms-only-re),  hope it helps.Thanks", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 50337, "title": "[PluggableDevice] Enable TF_TensorFromProto", "body": "Add a new C API `TF_TensorFromProto`, which is designed for converting TensorProto(TF_Buffer) to Tensor(TF_Tensor). Typical use case is to get tensor value from TensorProto in graph pass.", "comments": ["This is failing some tests internally. Suggested some changes that should make this work. PTAL.", "Thanks @saxenasaurabh. Already addressed all your comments.", "> Thanks @saxenasaurabh. Already addressed all your comments.\r\n\r\nThanks!"]}, {"number": 50336, "title": "tf.where function is less friendly in TensorFlow2.x than it in TenosrFlow1.x", "body": "In TensorFlow1.x\uff08including TensorFlow 1.15\uff09, the function of tf.where function can broadcast automaticly just like this:\r\n\r\n`x=tf.constant([[0.1, 0.1, 0.1], [0.1, 0.1, 0.1]], dtype=tf.float32)\r\ny=tf.constant([[0.2, 0.2, 0.2], [0.2, 0.2, 0.2]], dtype=tf.float32)\r\nm=tf.constant([0.4, 0.6],dtype=tf.float32)\r\nz=tf.where(m>0.5, x, y)\r\n`\r\nIt can get the right result z=[[0.2, 0.2, 0.2], [0.1, 0.1, 0.1]]\r\n\r\nHowever, when it run in TensorFlow2.x\uff0c it will give the error like this:\r\n**InvalidArgumentError: condition [2], then [2,3], and else [2,3] must be broadcastable [Op:SelectV2]**\r\n\r\nSo, why not continue the automatic broadcast feature in TensorFlow1.x\uff1f ", "comments": ["@yjiangling I update your code to change the size of condition as shown below. With that change, everything is working as expected. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/2673eb7b71bd63f4d0d3fb8e1d5f662d/untitled999.ipynb). Thanks!\r\n\r\n```\r\nx=tf.constant([[0.1, 0.1, 0.1], [0.1, 0.1, 0.1]], dtype=tf.float32) # shape [2 3]\r\ny=tf.constant([[0.2, 0.2, 0.2], [0.2, 0.2, 0.2]], dtype=tf.float32) # shape [2 3]\r\n# m=tf.constant([0.4, 0.6],dtype=tf.float32) # this fails as shape is [2]\r\nm=tf.constant([[0.4], [0.6]],dtype=tf.float32) # this works as expected (shape[2 1])\r\n\r\nz=tf.where(m>0.5, x, y)\r\nprint(z) #Output is `tf.Tensor([[0.2 0.2 0.2] [0.1 0.1 0.1]], shape=(2, 3), dtype=float32)`\r\n```", "> @yjiangling I update your code to change the size of condition as shown below. With that change, everything is working as expected. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/2673eb7b71bd63f4d0d3fb8e1d5f662d/untitled999.ipynb). Thanks!\r\n> \r\n> ```\r\n> x=tf.constant([[0.1, 0.1, 0.1], [0.1, 0.1, 0.1]], dtype=tf.float32) # shape [2 3]\r\n> y=tf.constant([[0.2, 0.2, 0.2], [0.2, 0.2, 0.2]], dtype=tf.float32) # shape [2 3]\r\n> # m=tf.constant([0.4, 0.6],dtype=tf.float32) # this fails as shape is [2]\r\n> m=tf.constant([[0.4], [0.6]],dtype=tf.float32) # this works as expected (shape[2 1])\r\n> \r\n> z=tf.where(m>0.5, x, y)\r\n> print(z) #Output is `tf.Tensor([[0.2 0.2 0.2] [0.1 0.1 0.1]], shape=(2, 3), dtype=float32)`\r\n> ```\r\n\r\nOh, I see, thanks a lot for the help."]}, {"number": 50335, "title": "Error when loading tflite model created from saved_model", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 19041.1052\r\n- TensorFlow installation (pip package or built from source): pip package\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): 2.5.0\r\n\r\n### 2. Code\r\n\r\nProvide code to help us reproduce your issues using one of the following options:\r\n\r\n- I converted a saved model using `tf.lite.TFLiteConverter.from_saved_model()`\r\n\r\n### 3. Failure after conversion\r\nIf the conversion is successful, but the generated model is wrong, then state what is wrong:\r\n\r\n- Model cannot be loaded by `tflite_runtime.interpreter.Interpreter`\r\n- I get the following error: _ValueError: Op builtin_code out of range: 131. Are you using old TFLite binary with newer model?Registration failed._\r\n- I have confirmed that both `tensorflow` and `tflite_runtime` and on version 2.5.0\r\n\r\n### 4. (optional) RNN conversion support\r\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n\r\n### 5. (optional) Any other info / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@kylegerber94 Can you please share a simple standalone code to reproduce the error? Thanks!", "Could you double check the TensorFlow version you are using for running the model?\r\nThe builtin op number 131 definitely exists in v2.5.0 https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/lite/builtin_ops.h#L161. ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50335\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50335\">No</a>\n"]}, {"number": 50333, "title": "[INTEL MKL] Enable mkl group conv", "body": "Group conv is already enabled in CPU. This PR enables mkl group conv.", "comments": ["@penpornk  Can you please review this PR ? Thanks!", "It looks like the [Linux GPU](https://source.cloud.google.com/results/invocations/bc5f88db-c0bb-429f-b5e2-7ac1b70fe5bd/targets) failure is related to this PR. Could you please help take a look at `//tensorflow/python/kernel_tests:conv_ops_test_gpu`? Thank you!", "> It looks like the [Linux GPU](https://source.cloud.google.com/results/invocations/bc5f88db-c0bb-429f-b5e2-7ac1b70fe5bd/targets) failure is related to this PR. Could you please help take a look at `//tensorflow/python/kernel_tests:conv_ops_test_gpu`? Thank you!\r\n\r\nLooks like Eigen conv backward doesn't support group conv, so I removed the changes for gradient test case."]}, {"number": 50332, "title": "Cannot import module for version mismatch?", "body": "Always we are facing importing module problems. As for example, I am using TF 1.14 and keras 2.2.5. but it can not import this:\r\nImportError: cannot import name 'imagenet_utils' from 'tensorflow.keras.applications' (unknown location)\r\nI don't know where it is, due to unknown location. Also I don't know the imagenet_utils is available in TF 1.14 or not!\r\n\r\nIs there any documentation about it, especially for all version. Are there any differences or added method differences between TF 1.14 and TF 2.5? \r\nWould you provide any documentation about these issues? ", "comments": ["@ML-BOSS ,\r\n\r\nCan you please, fill [issue template](https://github.com/tensorflow/models/issues/new/choose).Also please take a look at this links for more information on Imagenet_utils.It helps.\r\n[Link1](https://www.tensorflow.org/api_docs/python/tf/keras/applications/imagenet_utils),[Link2](https://stackoverflow.com/questions/63527148/cannot-import-name-imagenet-utils-from-tensorflow-keras-applications),[Link3](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/applications/imagenet_utils.py).\r\n\r\nThanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 50331, "title": "Adding no_rocm tags to current tests that are broken on ROCm.", "body": "Tests to be re-enabled after diagnosing, using no_rocm tags so CSB tests pass. ", "comments": ["@stevenireeves  Can you please fix build failures ? Thanks!", "@gbaned It's all ready. ", "@stevenireeves  Can you please resolve conflicts? Thanks!", "@gbaned can you run the other tests? ", "@gbaned can you remove the awaiting response tag? ", "@gbaned any updates on this PR?", "> @gbaned any updates on this PR?\r\n\r\n@stevenireeves  Sorry for the delay. It is under review phase. Thank you.", "@gbaned any updates? It's been in review for over a week. "]}, {"number": 50330, "title": "TF Container does not contain TensorRT. Impossible to use TF-TRT", "body": "CC; @bixia1 @sanjoy @pkanwar23 @whitefangbuck\r\n\r\nWe just noticed today that the tensorflow container seems to be incorrectly built:\r\n\r\n```bash\r\ndocker run -it --rm \\\r\n>    --gpus=\"all\" \\\r\n>    --shm-size=2g --ulimit memlock=-1 --ulimit stack=67108864 \\\r\n>    tensorflow/tensorflow:latest-gpu\r\n\r\nroot@9bc628c19dc5:/workspace# python\r\n>>> from tensorflow.python.compiler.tensorrt import trt_convert as trt\r\n>>> conversion_params = trt.TrtConversionParams()\r\n>>> converter = trt.TrtGraphConverterV2(\r\n...     conversion_params=conversion_params)\r\n\r\n2021-06-17 20:18:13.799235: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvrtc.so.11.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\n2021-06-17 20:18:13.799292: F tensorflow/compiler/tf2tensorrt/stub/nvinfer_stub.cc:49] getInferLibVersion symbol not found.\r\nAborted (core dumped)\r\n```\r\n\r\nAs you can see TensorRT can not load and seems missing (maybe linked to TRT7 upgrade ?)\r\n\r\nThis is an issue because TF-TRT uses extentisely this container in our talks, tutorials and blog posts (NV and Google side)\r\n\r\nThanks for your help to address it", "comments": [" adding /usr/local/cuda-11.1/lib64 to the LD_LIBRARY_PATH resolves the issue for me.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50330\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50330\">No</a>\n"]}, {"number": 50329, "title": "[INTEL MKL] Update onednn to v2.3_rc2", "body": "Update onednn to v2.3_rc2.\r\nFixes few critical bugs and provides additional architecture support.", "comments": []}, {"number": 50328, "title": "Restore original framebuffer in ForceSyncTurning", "body": "It broken OpenGL framebuffer in some render engine that using OpenGL state cache.\r\nSo we need to backup and restore current framebuffer.", "comments": ["@impjdi Can you please review this PR ? Thanks!", "> I don't think we can approve this CL because this would break ForceSyncTurning which is used for current samples / benchmarks.\r\n> \r\n> However, we agree that there is inefficiency on our end because we create a new framebuffer and don't notify the user. Maybe we shouldn't call ForceSyncTurning from the caller if framebuffer already exists?\r\n\r\nIt called it only when initializing, and it want to fix Mali GPU bug (it said in the header).\r\nSo the framebuffer is a dummy and unused framebuffer after initializing.\r\n", "@impjdi  Can you please assist on above comments from @metarutaiga. Thanks!", "@impjdi Any update on this PR? Please. Thanks!", "@gbaned We cannot approve this CL as it would break current samples and benchmarks.  The author claims that dummy framebuffer is only used at initialization and unused after initialization, but OpenGL is a state machine, and the glBindFramebuffer is permanent for the duration until you unbind it.  The proper fix is probably not to call ForceSyncTurning if framebuffer already exists.\r\n", "@metarutaiga Can you please check @impjdi's comments and keep us posted ? Thanks!"]}, {"number": 50327, "title": "Prevent glGetError infinity", "body": "if EGL context is not available, the glGetError always return GL_INVALID_OPERATION.\r\n\r\nI use my simple EGL to WGL wrapper, and found an infinite loop when destroying interpreter.\r\nhttps://github.com/metarutaiga/TensorFlowLiteReduce/blob/master/third_party/EGL/egl_windows.c", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F50327) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "@impjdi Can you please review this PR ? Thanks!", "FYI this PR broke internal Google apps and will be rolled back."]}, {"number": 50326, "title": "Getting nan-loss or CUDA_ERROR_ILLEGAL_ADDRESS after upgrading to 2.5.0 ", "body": "**System information**\r\n- OS Platform and Distribution: Ubuntu 16.04\r\n- TensorFlow version: v2.5.0-rc3-213-ga4dfb8d1a71 \r\n- Python version: 3.8\r\n- CUDA/cuDNN version: 11.1\r\n- GPU model and memory: 4x GeFoce 1080 GTX\r\n- NVIDIA driver version: 465.19.01\r\n\r\n**Describe the current behavior**\r\n\r\nAfter upgrading to 2.5.0 none of my experiments work anymore. \r\n\r\nTo be precise, ~any training where I am using LSTM-layers~ (turns out the LSTM-layer itself cannot be the problem) one of my models always ends in either a `nan`-loss or a `CUDA_ERROR_ILLEGAL_ADDRESS`. This issue does not come right at the beginning. I am usually able to get a few hundred batches through before the problem occurs.\r\n\r\nI don't know if these are two related or separate issues though. All I can say is that this problem did not exist before.\r\n\r\n**Describe the expected behavior**\r\n\r\nWork as previously in 2.4.1\r\n\r\n**Other info / logs**\r\n\r\n```\r\n2021-06-18 11:33:42.734501: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered\r\n2021-06-18 11:33:42.734547: F tensorflow/core/common_runtime/device/device_event_mgr.cc:221] Unexpected Event status: 1\r\nFatal Python error: Aborted\r\n\r\nThread 0x00007f14d7afd700 (most recent call first):\r\n  File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.9/multiprocessing/pool.py\", line 576 in _handle_results\r\n  File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.9/threading.py\", line 892 in run\r\n  File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.9/threading.py\", line 954 in _bootstrap_inner\r\n  File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.9/threading.py\", line 912 in _bootstrap\r\n\r\nThread 0x00007f1780ff9700 (most recent call first):\r\n  File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.9/multiprocessing/pool.py\", line 528 in _handle_tasks\r\n  File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.9/threading.py\", line 892 in run\r\n  File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.9/threading.py\", line 954 in _bootstrap_inner\r\n  File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.9/threading.py\", line 912 in _bootstrap\r\n\r\nThread 0x00007f17817fa700 (most recent call first):\r\n  File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.9/selectors.py\", line 416 in select\r\n  File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.9/multiprocessing/connection.py\", line 936 in wait\r\n  File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.9/multiprocessing/pool.py\", line 499 in _wait_for_updates\r\n  File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.9/multiprocessing/pool.py\", line 519 in _handle_workers\r\n  File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.9/threading.py\", line 892 in run\r\n  File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.9/threading.py\", line 954 in _bootstrap_inner\r\n  File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.9/threading.py\", line 912 in _bootstrap\r\n\r\nThread 0x00007f1781ffb700 (most recent call first):\r\n  File \"/home/sfa2021-06-18 11:33:42.736286: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered\r\nlk/miniconda2021-06-18 11:33:42.736311: E tensorflow/stream_executor/cuda/cuda_driver.cc:1085] could not wait stream on event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered\r\n3/envs/asr2/lib/p2021-06-18 11:33:42.736331: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered\r\nython3.9/2021-06-18 11:33:42.736358: F tensorflow/core/common_runtime/device/device_event_mgr.cc:221] Unexpected Event status: 1\r\nmultiprocessing/pool.py\", line 114 in workerAborted (core dumped)\r\n```", "comments": ["You may want to upgrade your cuda version to 11.2 if using pre built TF 2.5 binary.\r\nSee tested builds https://www.tensorflow.org/install/source#gpu", "Oh, I didn't even check. I don't know why but something made me thing I'd need 11.1. I really, really hope that this is the problem because I am not at all in the mood for hunting one additional issue at this point \ud83d\ude06 \r\n\r\nI'll install 11.2 and post my update/close the issue if it turns out to be working.", "@ymodak Unfortunately the problem still exists. I have first removed/purged the NVIDIA components and then installed CUDA 11.2\r\n\r\n```\r\nsudo apt purge nvidia-* -y && sudo apt autoremove -y \r\nsudo apt-get install cuda-11-2 -y\r\n```\r\nI've also installed cuDNN 8.1.0\r\n\r\n```\r\ntar -xvf cudnn-11.2-linux-x64-v8.1.0.77.tgz && \\\r\nsudo cp -a cuda/include/* /usr/local/cuda/include/ && \\\r\nsudo cp -a cuda/lib64/* /usr/local/cuda/lib64/ && \\\r\nsudo ldconfig\r\n```\r\n\r\nbut after 600 or 700 batches the loss keeps becoming `nan` \ud83d\ude1e  \r\n\r\n### Update\r\n\r\nOr I am getting (as before):\r\n\r\n````\r\nE tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered\r\n2021-06-18 09:30:43.886827: F tensorflow/core/common_runtime/device/device_event_mgr.cc:221] Unexpected Event status: 1\r\nFatal Python error: Aborted\r\n\r\nThread 0x00007fcc657fa700 (most recent call first):\r\n  File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.8/multiprocessing/pool.py\", line 576 in _handle_results\r\n  File \"/home/sfalk/minicond2021-06-18 09:30:43.886995: E tensorflow/stream_executor/dnn.cc:729] CUDNN_STATUS_EXECUTION_FAILED\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1882): 'cudnnRNNForward( cudnn.handle(), rnn_desc.handle(), rnn_fwd_mode, reinterpret_cast<const int*>(seq_lengths_data.opaque()), input_desc.data_handle(), input_data.opaque(), output_desc.data_handle(), output_data->opaque(), input_h_desc.handle(), input_h_data.opaque(), output_h_data->opaque(), input_c_desc.handle(), input_c_data.opaque(), output_c_data->opaque(), rnn_desc.ParamsSizeInBytes(), params.opaque(), workspace.size(), workspace.opaque(), reserve_space.size(), reserve_space.opaque())'\r\na3/envs/asr2/lib/python3.8/th2021-06-18 09:30:43.887065: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at cudnn_rnn_ops.cc:1560 : Internal: Failed to call ThenRnnForward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 512, 1152, 1, 405, 19, 1152] \r\nreading.py\", line 870 in run\r\n  File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.8/threading.py\", line 932 in _bootstrap_inner\r\n  File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.8/threading.py\", line 890 in _bootstrap\r\n\r\nThread 0x00007fcc65ffb700 (most recent call first):\r\n  File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.8/multiprocessing/pool.py\", line 528 in _handle_tasks\r\n  File \"/home/sfal2021-06-18 09:30:43.887445: I tensorflow/stream_executor/stream.cc:1404] [stream=0x55e1da721d40,impl=0x55e1cd83bef0] did not wait for [stream=0x55e1da720e90,impl=0x55e1d3e74dd0]\r\nk/minicond2021-06-18 09:30:43.887465: E tensorflow/stream_executor/cuda/cuda_driver.cc:1202] failed to enqueue async memcpy from host to device: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered; GPU dst: 0x7fc78865c200; host src: 0x7fc703c74a00; size: 4536=0x11b8\r\na3/envs2021-06-18 09:30:43.887477: E tensorflow/stream_executor/stream.cc:334] Error recording event in stream: Error recording CUDA event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered; not marking stream as bad, as the Event object may be at fault. Monitor for further errors.\r\n/asr2/2021-06-18 09:30:43.887488: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered\r\nlib/p2021-06-18 09:30:43.887497: F tensorflow/core/common_runtime/device/device_event_mgr.cc:221] Unexpected Event status: 1\r\nython3.8/Aborted (core dumped)\r\n```\r\n", "I wonder if this fails if you force run on cpu? Also have you tried with TF 2.5 final version?", "I can try running a training on the CPU but I'd have to use a much smaller model of course.\r\n\r\nWhat do you mean by final version? My pip version is `2.5.0`.", "In your issue template, it says 2.5.0-rc3 version. By final I meant 2.5.0 .\r\nBasically 2.5.0 is later than 2.5.0-rc3\r\nhttps://github.com/tensorflow/tensorflow/tags", "Hm, I didn't even notice. That's how the `GIT_VERSION` is named apparently. I definitely installed `tensorflow==2.5.0`\r\n\r\n```\r\n$ pip install tensorflow -U \r\nRequirement already satisfied: tensorflow in ./miniconda3/envs/asr2/lib/python3.9/site-packages (2.5.0)\r\nRequirement already satisfied: numpy~=1.19.2 in ./miniconda3/envs/asr2/lib/python3.9/site-packages (from tensorflow) (1.19.5)\r\n...\r\n```\r\n\r\nWhich gives me:\r\n\r\n```\r\n$ python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n2021-06-18 20:52:18.648285: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\r\nv2.5.0-rc3-213-ga4dfb8d1a71 2.5.0", "I let the training run on CPU over the weekend. It's still running as it should. No nan-loss or exception was thrown.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@stefan-falk \r\n\r\nCould you please confirm if the issue still persist.Thanks", "So far I was not able to fix this problem. I'm still getting this error but I am unsure whether it's me or Tensorflow. All I know is that I didn't have this issue in 2.4.1.", "Can you please run the model with the env var `CUDA_LAUNCH_BLOCKING` set to `1` and attach the full logs?", "@sanjoy If it's alright then I'll try this in the course of the next few days (maybe over the weekend). I'm a bit busy atm and for this I'll have to run a few experiments because the exception does not get thrown every time.", "Hi, Have you tried the above mentioned step, also please try in the latest Tensorflow version and let us know if the issue still persists. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50326\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50326\">No</a>\n"]}, {"number": 50324, "title": "`tf.keras.applications.EfficientNetB0` raises `CustomMaskWarning` when trained or saved", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.8\r\n- CUDA version: \r\n```\r\nnvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2021 NVIDIA Corporation\r\nBuilt on Sun_Feb_14_22:08:44_Pacific_Standard_Time_2021\r\nCuda compilation tools, release 11.2, V11.2.152\r\nBuild cuda_11.2.r11.2/compiler.29618528_0\r\n```\r\n- cuDNN version: `cudnn-11.2-windows-x64-v8.1.1.33`\r\n- GPU model and memory: RTX 2070 Super\r\n\r\n\r\n\r\n**Describe the current behavior**\r\nWhen saving an `EfficientNetB0` model to file (\".pb\"-file) it shows a warning:\r\n```\r\nE:\\projects\\DaKanjiRecognizerML\\.venv\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\r\n  warnings.warn('Custom mask layers require a config and must override '\r\n```\r\nThis also happens when converting to tf lite.\r\n\r\n**Describe the expected behavior**\r\nSaving should work without a problem.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\neff_net = tf.keras.applications.EfficientNetB0(\r\n    include_top=True,\r\n    weights=None,\r\n    input_shape=(64, 64, 1),\r\n    classes=6543,\r\n    pooling=\"max\",\r\n    classifier_activation=\"softmax\"\r\n)\r\n\r\neff_net.save(PATH_TO_SAVE_TO)\r\n```\r\n", "comments": ["Was able to reproduce the issuein TF 2.5. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/94de5063ad4d015ef7d21223d93fdb58/untitled.ipynb). Thanks!", "The warning was added with TF 2.5 in commit [076b5be](https://github.com/tensorflow/tensorflow/commit/076b5be77d3933deb726de1fc9d954d284985b1e#diff-08518b3b0b22c5c9dc137c911fa716fe820878cb9bbf95806ffca0e0296ec7fa)\r\nThe model saves without warnings in TF 2.4", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Still curious about this.", "same for MobileNetV3", "So why does the warning come up? Is it a bug or intentional?\r\n\r\nto add to the issue, i get this warning when exporting both to .pb and .h5 formats, so it does not seem to be save model format-dependant", "@CaptainDario Agree with you that it was an issue with old versions.\r\n\r\nThis was resolved in recent `tf-nightly`. [Here](https://colab.research.google.com/gist/jvishnuvardhan/32df0d4b8d83ac13e30e575ac8757835/untitled1128.ipynb) is a gist for reference. If you want to use a stable version, then it will be available in the upcoming TF2.8 in near future. Thanks!\r\n\r\nPlease verify once and close the issue if the issue got resolved for you. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50324\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50324\">No</a>\n"]}, {"number": 50323, "title": "Prevent glGetError infinity", "body": "if EGL context is not available, the glGetError always return GL_INVALID_OPERATION.\r\n\r\nI use my simple EGL to WGL wrapper, and found an infinite loop when destroying interpreter.\r\nhttps://github.com/metarutaiga/TensorFlowLiteReduce/blob/master/third_party/EGL/egl_windows.cc\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F50323) for more info**.\n\n<!-- need_sender_cla -->", "@metarutaiga  Can you please sign CLA. Thanks!", "@googlebot I signed it!", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F50323) for more info**.\n\n<!-- need_sender_cla -->", "Sorry, the committed mail is empty."]}, {"number": 50322, "title": "Keras models with shared layers are loaded incorrectly from h5 file", "body": "**System information**\r\nDefault colab instance.\r\n\r\n**Describe the current behavior**\r\nResults of the different usages of a shared layer are permuted after the save/load procedure (h5 saved model format). This changes the network topology in an unpredictable manner.\r\n\r\n**Describe the expected behavior**\r\nResults are not permuted and the topology is intact.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):  yes\r\n- Briefly describe your candidate solution(if contributing):\r\nThe problem seems to lie in this part of model loading code (it has moved to a different file in tf 2.5, but is essentially the same)\r\nhttps://github.com/tensorflow/tensorflow/blob/0931ea3d985bb9c8fdd054a5e29c4129623c849b/tensorflow/python/keras/engine/network.py#L1834-L1836\r\nIt handles the nodes we cannot compute right now. Such nodes are put back to the queue.\r\nThe problem arises when we process a shared layer (so there are several nodes associated with it) and some of the first nodes are not ready for computation, while some later nodes are. This leads to the order of nodes' computation being broken. This broken order is then used to find inputs for the following nodes, which leads to results permutation.\r\n\r\nI propose two possible solutions:\r\n1. Stop node processing process for entire layer upon encountering a node which is not ready for computation. This guaranties the correct order of computation, but would slow down the loading process to a certain degree.\r\n2. Note the original node order while iterating over them and fill the node_index_map keys accordingly. This way the original order will be broken, but we would have information about it while searching for the following nodes' inputs.\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1F8Rii8SmlURIlBK7GmMIkJwt5YkT6E7F?usp=sharing\r\n", "comments": ["@Saduf2019 \r\nI was able to replicate the issue reported here.Please find the [gist](https://colab.research.google.com/gist/UsharaniPagadala/1244d3ced00934198e3f28e624f1c0cb/keras_bug_example.ipynb#scrollTo=MnhrUXsMuFap). Thanks", "Please post this issue on [keras-team/keras](https://github.com/keras-team/keras/issues) repo.\r\nTo know more see;\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999", "> Please post this issue on [keras-team/keras](https://github.com/keras-team/keras/issues) repo.\r\n> To know more see;\r\n> https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999\r\n\r\nI've [duplicated](https://github.com/keras-team/keras/issues/14838) the issue. Hope it helps.\r\n\r\nCould you maybe provide an estimation on when someone is going to fix the problem? It seems to be a critical one, as h5 saving format is essentially broken. ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "This issue should not be stale. I have no idea why \"awaiting response\" label is still on.", "Isn't tracked now at https://github.com/keras-team/keras/issues/14838?", "@hidanom closing this issue as this was resolved. Thanks for confirming here https://github.com/keras-team/keras/issues/14838", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50322\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50322\">No</a>\n"]}, {"number": 50321, "title": "Convertion LSTM with Tensorflow Lite", "body": "I trained a simple model with Keras :\r\n\r\n```\r\nmodel = tf.keras.models.Sequential([\r\n                             tf.keras.layers.LSTM(20, time_major=False, unroll=False, input_shape=(28,28)), \r\n                             tf.keras.layers.Dense(10, activation=tf.nn.softmax, name='output')\r\n])\r\n```\r\nThen, I converted my model with TfLite:\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(\"mnist_lstm_model\")\r\nconverter.experimental_new_converter = True\r\ntflite_model = converter.convert()\r\n```\r\n\r\nI obtain a UNIDIRECTIONNAL_SEQUENCE_LSTM layer instead of LSTM. But I really need a LSTM layer for inference.\r\n\r\nThank you !\r\n", "comments": ["@renjie-liu could you take a look at this?", "Hi, unidirectional_sequence_lstm is a superset of lstm, can you elaborate why it does not work for you? thanks!", "@kassianl26, Sorry for late response.\r\n\r\nDid you refer above comment? Can you elaborate why it does not work for you? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50321\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50321\">No</a>\n"]}, {"number": 50319, "title": "Interpreter fails when trying to load CenterNet(Java API)", "body": "I have created an android example app and I am trying to run CenterNet(320x320, 512x512, 640x640).\r\n\r\nWhen I run this model(trained): \r\n[centernet_320x320.zip](https://github.com/tensorflow/tensorflow/files/6668027/centernet_320x320.zip) in CPU works fine, but when I try to run with GPU Delegate I get the following error:\r\n`java.lang.RuntimeException: java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.`\r\n\r\nWhen I run this two models(pre trained):\r\n[centernet_model_test_640x640.zip](https://github.com/tensorflow/tensorflow/files/6668051/centernet_model_test_640x640.zip)\r\n[centernet_model_test_512x512.zip](https://github.com/tensorflow/tensorflow/files/6668049/centernet_model_test_512x512.zip)\r\nThey work in CPU and GPU.\r\n\r\nNow, when I try to run the same models in a Qualcomm device for better performance, for the first model I get the same behavior and for the two pre trained models when I run them in CPU they work(the output is good), but when I run the same model in QC **GPU the output of the models is not correct**. (its just weird when the same model in CPU gives good predictions and in GPU it gives really bad predictions)\r\n\r\n\r\n```\r\n    implementation 'org.tensorflow:tensorflow-lite:2.3.0'\r\n    implementation 'org.tensorflow:tensorflow-lite-gpu:2.3.0'\r\n```\r\nI have tried with 2.3.0 , 2.4.0 and 2.5.0 and I get the same behavior.\r\n", "comments": ["in the centernet 320x320, you have bunch of subgraphs.  try to remove those.\r\n\r\ni'm not really sure the centernet models run on the GPU as I see bunch of incompatible ops.  do you have any logs?", "Hi @impjdi,\r\nThe two pre trained models (512x512, and 640x640) in the first device (RK chip) worked on GPU, I am assuming this because the interpreter time dropped almost by 50% (from 1.2 seconds/frame to 0.6 seconds/frame) and the output was correct.\r\nBut when I change the device(Qualcomm) interpreter time drops but the output is not correct. \r\nThe only change here is the GPU of the device :/ \r\nI can't find any useful logs because there isn't any warning or crashes of interpreter.", "Hi @ArtanBerisha1! \r\nWe are checking to see if you still need help in this issue .Attaching similar issues  for answers [link1](https://github.com/tensorflow/tensorflow/issues/46388#issuecomment-871074251),[link2](https://github.com/tensorflow/tensorflow/issues/49224),[link3](https://github.com/tensorflow/tensorflow/issues/46388#issuecomment-871074251),[link4](https://stackoverflow.com/questions/60826779/valueerror-none-is-only-supported-in-the-1st-dimension-tensor-flatbuffer-data/60869999#60869999) ? Please create a new issue if the issue is replicating in latest version TF 2.6 . Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50319\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50319\">No</a>\n"]}, {"number": 50318, "title": "set --distinct_host_configuration=false by default for linux builds", "body": "default linux build of TF compiles both host and target.\r\nHost is not needed for most scenarios, and skipping is speeds up the build by 40% on a DGX", "comments": ["@sanjoy , this is the change we discussed during the Google/NVidia meeting today."]}, {"number": 50317, "title": "Fixed tfl to tosa lowering for convolutions with mixed quantization", "body": "Mixed quantization operation should dequantize all inputs, perform the operation in floating point, then Quantize back to the output type. This change adds this behavior to the convolution operation lowerings (2D, transpose, and depthwise).\r\n\r\nIt includes some refactoring including removing no longer needed utilities and lowering tflite constants to tosa consts as a separate pattern rewrite. This is performed to guarantee quantization information remains for all other tflite to tosa lowerings without depending on op lowerings stripping quantization information off of i32 tensors (not supported by tosa).", "comments": ["Generally looks good. We've never seen cases with mixed precision so didn't catch this before.", "@rsuderman  Can you please check @sjarus's comments and resolve conflicts?. Thanks!"]}, {"number": 50316, "title": "[INTEL MKL] Fix failing auto_mixed_precision test", "body": "The test, TEST_F(AutoMixedPrecisionTest, FusedBatchNorm), compares FP16 (emulated in Eigen when run on CPU) with float32 results. It is well known that float32 addition is not associative, so difference in results between different implementations is possible. Even using different number of threads leads to different values of variance computation (a part of FusedBatchNorm computation) as the reduction happens in different sequence. \r\nMoreover, if we change the input shape to {1, 28, 28, 4} or {8, 28, 28, 16} and many others, even the Stock TF fails (i.e. with TF_ENABLE_ONEDNN_OPTS=0). Given these, we think it is appropriate to increase the rtol", "comments": ["Is this test failing at head right now that it needs a fix?", "@rohan100jain : when oneDNN is enabled with the environment flag TF_ENABLE_ONEDNN_OPTS=1, then this test fails. \r\n--config=cuda enables this test. Now, with config=cuda and oneDNN enabled, the test fails on CPU. And I put our analysis in the PR description above."]}, {"number": 50315, "title": "Unable to change task parameter in tensorflow decision forest (from default classification)", "body": "**System information**\r\n- Have I written custom code: No\r\n- OS: Linux Ubuntu 18.04\r\n- TensorFlow installed from: pip\r\n- TensorFlow version: 2.5.0\r\n- Python version: 3.8.10\r\n- Tensorflow decision forests version: 0.1.6\r\n\r\n**Current Behavior**\r\nAttempting to use the tensorflow decision forests module for regression.\r\nFollowing this example:\r\nhttps://www.tensorflow.org/decision_forests/migration#training_configuration\r\n\r\nThe problematic behavior is that the only way to change the task from the default (classification) to anything else is by specifying `task=tf.keras.Task.REGRESSION` (or `RANKING`) . But `module 'tensorflow.keras' has no attribute 'Task'` and so the model can only be used for classification as this is the default setting.\r\n\r\nThis behavior occurs both locally as well as on Google Colab notebooks.\r\n\r\n**Expected Behavior**\r\nThe module `tf.keras` would contain the attribute `Task` and the `task` parameter would be adjustable.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_decision_forests as tfdf\r\nmodel = tfdf.keras.GradientBoostedTreesModel(task=tf.keras.Task.REGRESSION)\r\n```\r\n", "comments": ["Closing this issue. There's a typo in the documentation- it should be `task = tfdf.keras.Task.REGRESSION`; the `Task` attribute is part of the tfdf.keras module, not tf.keras", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50315\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50315\">No</a>\n"]}]