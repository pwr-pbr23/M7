[{"number": 40084, "title": "tflite accuracy tool what parameter \"--model_output_labels\" is?", "body": "i have a tflite accuracy test with tflite accuracy tool in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/accuracy/ilsvrc, but i do not konw parameter \"--model_output_labels\" is, anyone would tell me, i will appreciate.", "comments": []}, {"number": 40083, "title": "RNN unrolled, cannot be converted using from_keras_model", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): tf-nightly\r\n- TensorFlow version (or github SHA if from source):\r\n-  version is 2.2.0.dev20200508, git version is v1.12.1-31489-g6047d50555\r\n\r\n**Describe the current behavior**\r\nConversion fails with the following:\r\n```\r\nValueError: Cannot unroll a RNN if the time dimension is undefined.\r\n- If using a Sequential model, specify the time dimension by passing an `input_shape` or `batch_input_shape` argument to your first layer. If your first layer is an Embedding, you can also use the `input_length` argument.\r\n- If using the functional API, specify the time dimension by passing a `shape` or `batch_shape` argument to your Input layer.\r\n```\r\nThis seems to be due to the following addition to the from_keras_model conversion: \r\n```\r\nself._keras_model.save(temp_dir, save_format=\"tf\")\r\n```\r\n\r\n**Describe the expected behavior**\r\nConversion should succeed as it did before in previous versions.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.keras.models import Model\r\n\r\nclass SimpleModel(Model):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.model_name = \"mnist\"\r\n        self.train_data = None\r\n        self.test_data = None\r\n        self.calib_data = None\r\n        self.num_calib = 1000\r\n        # (data preprocessing) Normalize the input image so that\r\n        # each pixel value is between 0 to 1.\r\n        self.pre_process = lambda x: x / 255.0\r\n\r\n        self._load_data()\r\n\r\n    def _load_data(self):\r\n        # Load MNIST dataset\r\n        mnist = tf.keras.datasets.mnist\r\n\r\n        # _data: (images, labels)\r\n        self.train_data, self.test_data = mnist.load_data()\r\n        self.calib_data = self.pre_process(\r\n            self.train_data[0][0 : self.num_calib].astype(np.float32)\r\n        )\r\n\r\n    def train(self):\r\n        cell = tf.keras.layers.GRUCell(3)\r\n\r\n        model = tf.keras.models.Sequential([\r\n            tf.keras.layers.Input(shape=(28, 28), name='input'),\r\n            #tf.keras.layers.LSTM(32),\r\n            tf.keras.layers.RNN(cell, unroll=True),\r\n            tf.keras.layers.Flatten(),\r\n            tf.keras.layers.Dense(10, activation=tf.nn.softmax, name='output')\r\n        ])\r\n        model.summary()\r\n\r\n        train_images = self.pre_process(self.train_data[0])\r\n        train_labels = self.train_data[1]\r\n        test_images = self.pre_process(self.test_data[0])\r\n        test_labels = self.test_data[1]\r\n        # Train the digit classification model\r\n        model.compile(\r\n            optimizer=\"adam\",\r\n            loss=\"sparse_categorical_crossentropy\",\r\n            metrics=[\"accuracy\"],\r\n        )\r\n        model.fit(\r\n            train_images,\r\n            train_labels,\r\n            epochs=1,\r\n            validation_data=(test_images, test_labels),\r\n        )\r\n        # dump SavedModel - ANOTHER BUG HERE!\r\n        #model.save(str(self.savedModel_dir))\r\n\r\n        return model\r\n\r\n    def eval(self, tflite_model_path: str):\r\n        interpreter = tf.lite.Interpreter(model_path=str(tflite_model_path))\r\n        interpreter.allocate_tensors()\r\n\r\n        input_index = interpreter.get_input_details()[0][\"index\"]\r\n        output_index = interpreter.get_output_details()[0][\"index\"]\r\n\r\n        # (data preprocessing) Normalize the input image so that\r\n        # each pixel value is between 0 to 1.\r\n        test_images = self.pre_process(self.test_data[0])\r\n        test_labels = self.test_data[1]\r\n        # Run predictions on every image in the \"test\" dataset.\r\n        prediction_digits = []\r\n        for test_image in test_images:\r\n            # Pre-processing: add batch dimension and convert to float32 to match with\r\n            # the model's input data format.\r\n            test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\r\n            interpreter.set_tensor(input_index, test_image)\r\n\r\n            # Run inference.\r\n            interpreter.invoke()\r\n\r\n            # Post-processing: remove batch dimension and find the digit with highest\r\n            # probability.\r\n            output = interpreter.tensor(output_index)\r\n            digit = np.argmax(output()[0])\r\n            prediction_digits.append(digit)\r\n\r\n        # Compare prediction results with ground truth labels to calculate accuracy.\r\n        accurate_count = 0\r\n        for index, _ in enumerate(prediction_digits):\r\n            if prediction_digits[index] == test_labels[index]:\r\n                accurate_count += 1\r\n        accuracy = accurate_count * 1.0 / len(prediction_digits)\r\n\r\n        return accuracy\r\n\r\n    def _get_calib_data_func(self):\r\n        def representative_data_gen():\r\n            for input_value in self.calib_data:\r\n                input_value = np.expand_dims(input_value, axis=0).astype(np.float32)\r\n                yield [input_value]\r\n\r\n        return representative_data_gen\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    temp = SimpleModel()\r\n    model = temp.train()\r\n\r\n    converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.representative_dataset = temp._get_calib_data_func()\r\n\r\n    # save INT8 tflite\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8,\r\n        tf.lite.OpsSet.SELECT_TF_OPS]\r\n    converter.experimental_new_converter = True\r\n    tflite_model_INT8 = converter.convert()\r\n    open(\"lstm_unrolled_int8.tflite\", \"wb\").write(tflite_model_INT8)\r\n\r\n```\r\n\r\n\r\n", "comments": ["@SaoirseARM \r\nI ran the code shared and face invalid syntax, please share complete indented code such that we can replicate the issue.\r\nPlease fin the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/430d59df445734d21c37863f0247d7bd/untitled216.ipynb)", "Apologies @Saduf2019 , I have updated the comment with syntax error free code.", "I am able to replicate this issue, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/2ecfe7da64972d28285859574bffb33e/untitled216.ipynb) Thanks!", "There may be a misunderstanding here, this issue can be seen building from source or from tf-nightly and not in the released tensorflow v2.2.\r\nI have updated the description above. Apologies for the confusion.", "@renjie-liu can you take a look?", "Seems the issue is indeed for keras_model reloading, can you try convert from saved_model instead? see https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/experimental_new_converter/Keras_LSTM_fusion_Codelab.ipynb\r\n\r\nmeanwhile, Hi Jaesung, can you help take a look about the keras_model import? I supscious it's because keras model serialize/deserialze caused the problem.\r\n\r\nthanks!", "Thank you, I am actually already using the workaround you suggested above and it works absolutely fine. Thanks for looking into this! ", "Great!", "Hi @renjie-liu, with the example you pointed. I tried some quantization experiments, seems:\r\n\r\n   for dynamic range quantization, weights within RNN still FP32;\r\n   for fully integer quantization, currently failed with a runtime error:\r\n```RuntimeError: tensorflow/lite/kernels/unidirectional_sequence_lstm.cc:349 input_to_output_weights->dims->data[1] != n_input (28 != 0)Node number 0 (UNIDIRECTIONAL_SEQUENCE_LSTM) failed to prepare.```\r\n\r\n[Colab example](https://colab.research.google.com/drive/1pF0bjfj19r2bN0H49J-8l4VFT4Vx60_i?usp=sharing)", "fully integer quantization is not done yet. \r\n\r\nbut the dynamic range quantization should work, can you share any code snippet?\r\n\r\nthanks!", "This is no longer an issue in tensorflow-2.4.1. Thanks! Closing issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40083\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40083\">No</a>\n", "> Hi @renjie-liu, with the example you pointed. I tried some quantization experiments, seems:\r\n> \r\n> for dynamic range quantization, weights within RNN still FP32;\r\n> for fully integer quantization, currently failed with a runtime error:\r\n> `RuntimeError: tensorflow/lite/kernels/unidirectional_sequence_lstm.cc:349 input_to_output_weights->dims->data[1] != n_input (28 != 0)Node number 0 (UNIDIRECTIONAL_SEQUENCE_LSTM) failed to prepare.`\r\n> \r\n> [Colab example](https://colab.research.google.com/drive/1pF0bjfj19r2bN0H49J-8l4VFT4Vx60_i?usp=sharing)\r\n\r\nI have the same error with Tensorflow-2.5.0 (MNIST_LSTM -> fully integer quantization)\r\nRuntimeError: tensorflow/lite/kernels/unidirectional_sequence_lstm.cc:895 input_to_output_weights->dims->data[1] != n_input (28 != 528)Node number 0 (UNIDIRECTIONAL_SEQUENCE_LSTM) failed to prepare\r\n", "@kassianl26 please upload a new issue with the information regarding how to reproduce your issue instead of commenting on the closed issue."]}, {"number": 40082, "title": "Conversion dot op from LHLO to Affine", "body": "Add conversions from MLIR Lhlo dot op to affine loops. These conversions are run as part of -lhlo-legalize-to-affine pass.", "comments": ["Please address @joker-eph  comments from https://github.com/tensorflow/tensorflow/pull/40069 (the use of auto 42-45 isn't addressed)."]}, {"number": 40081, "title": "Cannot save RNN-based model as a saved model format", "body": "**System information**\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.3 LTS\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b (GIT_VERSION), 2.2.0 (VERSION)\r\n- Python version: 3.7.5\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: Titan RTX, ~25GB\r\n\r\n**Describe the current behavior**\r\n\r\nI wrote a simple code that builds RNN using GRUCells and save with signature function.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ngru_encoder = tf.keras.layers.RNN([tf.keras.layers.GRUCell(200) for _ in range(4)], return_sequences=True)\r\n\r\n\r\ngru_encoder(tf.keras.Input((32, 200)))\r\n\r\n\r\n@tf.function(\r\n    input_signature=[tf.TensorSpec(shape=[None, None, None], dtype=tf.float32)]  # batch, sequence length, hidden size\r\n)\r\ndef _signature_fn(input_embedding):\r\n    return gru_encoder(input_embedding)\r\n\r\n\r\ntf.saved_model.save(gru_encoder, \"./test-model/1\", signatures=_signature_fn)\r\n```\r\n\r\nAnd the result of above script is\r\n\r\n```sh\r\n$ python test.py\r\n2020-06-02 18:42:20.388075: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-06-02 18:42:20.525918: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:19:00.0 name: TITAN RTX computeCapability: 7.5\r\ncoreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.65GiB deviceMemoryBandwidth: 625.94GiB/s\r\n2020-06-02 18:42:20.527134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties: \r\npciBusID: 0000:1a:00.0 name: TITAN RTX computeCapability: 7.5\r\ncoreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.65GiB deviceMemoryBandwidth: 625.94GiB/s\r\n2020-06-02 18:42:20.528153: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 2 with properties: \r\npciBusID: 0000:67:00.0 name: TITAN RTX computeCapability: 7.5\r\ncoreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.65GiB deviceMemoryBandwidth: 625.94GiB/s\r\n2020-06-02 18:42:20.529164: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 3 with properties: \r\npciBusID: 0000:68:00.0 name: TITAN RTX computeCapability: 7.5\r\ncoreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.65GiB deviceMemoryBandwidth: 625.94GiB/s\r\n2020-06-02 18:42:20.536014: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-06-02 18:42:20.537691: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-06-02 18:42:20.539509: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-06-02 18:42:20.539851: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-06-02 18:42:20.541872: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-06-02 18:42:20.542990: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-06-02 18:42:20.547193: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-06-02 18:42:20.556527: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0, 1, 2, 3\r\n2020-06-02 18:42:20.556829: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n2020-06-02 18:42:20.590996: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3500000000 Hz\r\n2020-06-02 18:42:20.593061: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f254c000b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-06-02 18:42:20.593123: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-06-02 18:42:21.337111: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55f0e31589f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-06-02 18:42:21.337154: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): TITAN RTX, Compute Capability 7.5\r\n2020-06-02 18:42:21.337163: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): TITAN RTX, Compute Capability 7.5\r\n2020-06-02 18:42:21.337171: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): TITAN RTX, Compute Capability 7.5\r\n2020-06-02 18:42:21.337179: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (3): TITAN RTX, Compute Capability 7.5\r\n2020-06-02 18:42:21.339651: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:19:00.0 name: TITAN RTX computeCapability: 7.5\r\ncoreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.65GiB deviceMemoryBandwidth: 625.94GiB/s\r\n2020-06-02 18:42:21.341408: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties: \r\npciBusID: 0000:1a:00.0 name: TITAN RTX computeCapability: 7.5\r\ncoreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.65GiB deviceMemoryBandwidth: 625.94GiB/s\r\n2020-06-02 18:42:21.343151: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 2 with properties: \r\npciBusID: 0000:67:00.0 name: TITAN RTX computeCapability: 7.5\r\ncoreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.65GiB deviceMemoryBandwidth: 625.94GiB/s\r\n2020-06-02 18:42:21.344891: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 3 with properties: \r\npciBusID: 0000:68:00.0 name: TITAN RTX computeCapability: 7.5\r\ncoreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.65GiB deviceMemoryBandwidth: 625.94GiB/s\r\n2020-06-02 18:42:21.344938: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-06-02 18:42:21.344955: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-06-02 18:42:21.344969: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-06-02 18:42:21.344984: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-06-02 18:42:21.345001: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-06-02 18:42:21.345016: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-06-02 18:42:21.345031: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-06-02 18:42:21.356549: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0, 1, 2, 3\r\n2020-06-02 18:42:21.356628: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-06-02 18:42:21.367494: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-06-02 18:42:21.367544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 1 2 3 \r\n2020-06-02 18:42:21.367553: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N N N N \r\n2020-06-02 18:42:21.367559: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 1:   N N N N \r\n2020-06-02 18:42:21.367565: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 2:   N N N N \r\n2020-06-02 18:42:21.367570: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 3:   N N N N \r\n2020-06-02 18:42:21.374361: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22604 MB memory) -> physical GPU (device: 0, name: TITAN RTX, pci bus id: 0000:19:00.0, compute capability: 7.5)\r\n2020-06-02 18:42:21.376397: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 22604 MB memory) -> physical GPU (device: 1, name: TITAN RTX, pci bus id: 0000:1a:00.0, compute capability: 7.5)\r\n2020-06-02 18:42:21.377956: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 22604 MB memory) -> physical GPU (device: 2, name: TITAN RTX, pci bus id: 0000:67:00.0, compute capability: 7.5)\r\n2020-06-02 18:42:21.379349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 22581 MB memory) -> physical GPU (device: 3, name: TITAN RTX, pci bus id: 0000:68:00.0, compute capability: 7.5)\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 16, in <module>\r\n    tf.saved_model.save(gru_encoder, \"./test-model/1\", signatures=_signature_fn)\r\n  File \"{PROJECT_DIRECTORY}/env/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py\", line 951, in save\r\n    obj, export_dir, signatures, options, meta_graph_def)\r\n  File \"{PROJECT_DIRECTORY}/env/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py\", line 1012, in _build_meta_graph\r\n    signature_serialization.validate_saveable_view(checkpoint_graph_view)\r\n  File \"{PROJECT_DIRECTORY}/env/lib/python3.7/site-packages/tensorflow/python/saved_model/signature_serialization.py\", line 268, in validate_saveable_view\r\n    saveable_view.root):\r\n  File \"{PROJECT_DIRECTORY}/env/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py\", line 108, in list_dependencies\r\n    extra_dependencies = self.list_extra_dependencies(obj)\r\n  File \"{PROJECT_DIRECTORY}/env/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py\", line 137, in list_extra_dependencies\r\n    self._serialization_cache)\r\n  File \"{PROJECT_DIRECTORY}/env/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 2746, in _list_extra_dependencies_for_serialization\r\n    .list_extra_dependencies_for_serialization(serialization_cache))\r\n  File \"{PROJECT_DIRECTORY}/env/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/base_serialization.py\", line 74, in list_extra_dependencies_for_serialization\r\n    return self.objects_to_serialize(serialization_cache)\r\n  File \"{PROJECT_DIRECTORY}/env/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py\", line 73, in objects_to_serialize\r\n    serialization_cache).objects_to_serialize)\r\n  File \"{PROJECT_DIRECTORY}/env/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py\", line 94, in _get_serialized_attributes\r\n    serialized_attr.set_and_validate_objects(object_dict)\r\n  File \"{PROJECT_DIRECTORY}/env/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/serialized_attributes.py\", line 212, in set_and_validate_objects\r\n    ' {})'.format(object_dict[key], key))\r\nValueError: Object dictionary contained a non-trackable object: (None, None, None, None) (for key states)\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nI think this script should run without any exception and actually it works fine in `tensorflow==2.1.0`.\r\n", "comments": ["I have tried in colab with TF 2.2, nightly versions and was able to reproduce the issue.However i do not see any issue with TF version 2.1.0.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/e3832a3e8ea68a4e31f4396635e1cd73/untitled46.ipynb).Thanks!", "I'm facing same issue only when using TF 2.2.", "I face a similar issue when trying to save a model (in saved model format) containing Bidirectional ConvLSTM2D layers. I get:\r\n`ValueError: Object dictionary contained a non-trackable object: (None, None) (for key states)`\r\n\r\nI tried using sequential model, and the functional api. I have no issues when saving in h5 format. Unfortunately, that is of no use for me as I need the .pb file.\r\n\r\nBelow is my model:\r\n\r\n```\r\ninptensor = Input(shape=(None, 96, 2, 2), dtype='float32')\r\ntdens = TimeDistributed(Dense(4))(inptensor)\r\nbn1 = BatchNormalization()(tdens)\r\nac0 = Activation('tanh')(bn1)\r\nbd1 = Bidirectional(ConvLSTM2D(2, (3, 3),\r\n                               return_sequences=True,\r\n                               padding='same',\r\n                               go_backwards=True,\r\n                               activation='tanh',\r\n                               recurrent_activation='sigmoid'))(ac0)\r\nbd2 = Bidirectional(ConvLSTM2D(2, (3, 3),\r\n                               return_sequences=True,\r\n                               padding='same',\r\n                               go_backwards=True,\r\n                               activation='tanh',\r\n                               recurrent_activation='sigmoid'))(bd1)\r\nbd3 = Bidirectional(ConvLSTM2D(2, (3, 3),\r\n                               return_sequences=False,\r\n                               padding='same',\r\n                               go_backwards=True,\r\n                               activation='tanh',\r\n                               recurrent_activation='sigmoid'))(bd2)\r\ndens2 = Dense(2)(bd3)\r\nbn4 = BatchNormalization()(dens2)\r\nac3 = Activation('relu')(bn4)\r\ndens3 = Dense(1)(ac3)\r\nbn5 = BatchNormalization()(dens3)\r\nac4 = Activation('relu')(bn5)\r\nmodel = tf.keras.Model(inptensor, ac4)\r\n```\r\n\r\nEDIT:\r\nI tried both variable and fixed sequence length and got the same error.\r\nI'm running arch linux and use Pzthon 3.8, and tf-gpu on a Nvidia RTX 2080 Ti.\r\n\r\nUPDATE:\r\n**Error only occurs in 2.2.0! 2.1.0 works fine!**\r\nI installed 2.1.0 into a virtual-env but didn't properly activate that virtual-env in my IDE, thus remained with 2.2.0 the entire time. Omg, what a blunder. Please apologize any circumstances my error may have caused. I for one wasted days on this idiotic mistake.", "Doublepost to notify dev team about my mistake (see Update in previous comment) so they may not waste any time trying to recreate my error in tf 2.1.0. This comment can be deleted.", "Sorry for the late reply. This issue has the same root cause as https://github.com/tensorflow/tensorflow/issues/40328, and we are sending a fix for it. Should be fixed in 2.3 release.", "Should be fixed by https://github.com/tensorflow/tensorflow/commit/47582983cb1064b5bb81233db4f0adeeaa10b74d.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40081\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40081\">No</a>\n"]}, {"number": 40080, "title": "Use proper mmap_allocator for TFLite", "body": "When BUILD_WITH_MMAP is ture, mmap_allocator.cc should be used\r\n", "comments": []}, {"number": 40079, "title": "Use proper mmap_allocator for TFLite", "body": "When BUILD_WITH_MMAP is true,\r\nmmap_allocator_disabled.cc should be excluded\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40079) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!\r\n\r\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40079) for more info**.\n\n<!-- need_author_cla -->", "@googlebot  I fixed it."]}, {"number": 40078, "title": "tensorflow.python.framework.errors_impl.InvalidArgumentError: input depth must be evenly divisible by filter depth: 544 vs 96 [Op:Conv2D]", "body": "Does anyone ever experience like this? Please give enlightenment, thank you\r\n\"tensorflow.python.framework.errors_impl.InvalidArgumentError: input depth must be evenly divisible by filter depth: 544 vs 96 [Op:Conv2D]\"\r\n![1](https://user-images.githubusercontent.com/58981061/83502393-b211c400-a4eb-11ea-8b0c-891c664ac43d.PNG)\r\n", "comments": ["@benedives47,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and the TensorFlow version you are using. \r\n\r\nAlso, please take a look at [#28777](https://github.com/tensorflow/tensorflow/issues/28777) and [this](https://stackoverflow.com/a/60212961) similar StackOverflow query. Thanks!\r\n", "TF : 2.0\r\nsource code:\r\n    conv2_block2_0_bn = model.get_layer('conv2_block2_0_bn')(conv2_block1_concat)\r\n    conv2_block2_0_bn = tf.Variable(conv2_block2_0_bn)\r\n    conv2_block2_0_relu = model.get_layer('conv2_block2_0_relu')(conv2_block2_0_bn)\r\n    conv2_block2_0_relu = tf.Variable(conv2_block2_0_relu)\r\n    conv2_block2_1_conv = model.get_layer('conv2_block2_1_conv')(conv2_block2_0_relu)\r\n    conv2_block2_1_conv = tf.Variable(conv2_block2_1_conv)\r\n    conv2_block2_1_bn = model.get_layer('conv2_block2_1_bn')(conv2_block2_1_conv)\r\n    conv2_block2_1_bn = tf.Variable(conv2_block2_1_bn)\r\n    conv2_block2_1_relu = model.get_layer('conv2_block2_1_relu')(conv2_block2_1_bn)\r\n    conv2_block2_1_relu = tf.Variable(conv2_block2_1_relu)\r\n    conv2_block2_2_conv = model.get_layer('conv2_block2_2_conv')(conv2_block2_1_relu)\r\n    conv2_block2_2_conv = tf.Variable(conv2_block2_2_conv)\r\n    X=concatenate([conv2_block2_0_bn, conv2_block2_0_relu, conv2_block2_1_conv, conv2_block2_1_bn, conv2_block2_1_relu, conv2_block2_2_conv])\r\n    conv2_block2_concat= tf.Variable(X)", "@benedives47,\r\nOn running the code I'm facing an error stating `NameError: name 'model' is not defined`. \r\n\r\nCould you please provide the complete code to reproduce the issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "input this code : \r\nfrom tensorflow.keras.models import load_model\r\nmodel=load_model('models_that_have_been_trained.h5')", "@benedives47,\r\nI am facing an error stating `ValueError: No such layer: conv2_block2_0_bn` on running the code with a sample `h5` file. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/771b9eae34aaed3496ace560dd09b629/40078.ipynb#scrollTo=_PXLisD1am_u).\r\n\r\nCould you please share the complete code or the `.py` file you are running along with all the supporting files. Thanks!", "you have \"model.summary()\". because to see the layer name in your model. What model are you wearing?", "@benedives47,\r\nPlease check [this gist](https://colab.research.google.com/gist/amahendrakar/c594d2ca090be93c62a73e6a4c3f1e60/40078.ipynb) for the sample `h5` model I am using.\r\n\r\nIn order to reproduce the error, please share the complete code and all the supporting files you are using in the code. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 40077, "title": "Use proper mmap_allocator for TFLite", "body": "When BUILD_WITH_MMAP is true,\r\nmmap_allocator_disabled.cc should be excluded\r\n\r\nChange-Id: I2014e79de12512f77c8d3f7a91258dfeef33fb7a\r\nSigned-off-by: Semun Lee <semun.lee@samsung.com>", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40077) for more info**.\n\n<!-- need_sender_cla -->", "> @googlebot I signed it!\r\n\r\n"]}, {"number": 40076, "title": "Concatenate ReLU and Batch Norm error", "body": "**System information**\r\n- Have I written custom code that uses Tensorflow's cReLU function:\r\n\r\n```\r\n...\r\nx = tf.keras.layers.Conv2D(32, (3, 3), padding=\"same\")(inputs)\r\nx = tf.keras.layers.Activation(\"relu\")(x)\r\nx = tf.keras.layers.BatchNormalization(axis=chanDim)(x)\r\nx = tf.keras.layers.Conv2D(32, (3, 3), padding=\"same\")(x)\r\nx = tf.keras.layers.Lambda(lambda t: tf.nn.crelu(x))(x)\r\nx = tf.keras.layers.BatchNormalization(axis=chanDim)(x)\r\nx = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\r\nx = tf.keras.layers.Dropout(0.25)(x)\r\n...\r\n```\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 10.1 / 7.6.5\r\n- GPU model and memory: RTX 2070 Super (8 GB)\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nI get the following error:\r\n\r\n```\r\n...\r\nValueError: Input 0 of layer batch_normalization_1 is incompatible with the layer: expected ndim=4, found ndim=2. Full shape received: [None, 20]\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nThere was no error in Tensorflow 1.X.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n`zip` containing the source code: https://www.dropbox.com/s/65q937qqqqa8x8d/keras-vs-tensorflow.zip?dl=0\r\n\r\n**Question**\r\n\r\nIt seems there is difference in either cReLU or Batch Norm implementation in Tensorflow 2.X compared to Tensorflow 1.X. Can someone point out what the change is so I can modify my code?\r\n", "comments": ["@abhishekthanki \r\nPlease share simple standalone code such that we can replicate the issue faced or if possible share a colab gist with the error faced.", "@Saduf2019 I have provided a link to the `zip` that contains the code I'm using and you can use it to replicate it on your machine.", "@gowthamkpr Any update on this issue? ", "@abhishekthanki I am not able to reproduce this. Please share a colab gist so that we can reproduce this issue. Thanks!", "@gowthamkpr I'm sharing two colab notebooks, one with TF 1.15 and another with TF 2.2. The code is exactly the same but I'm getting a error in TF 2.2 while there is no such error in TF 1.15. Here are the links to the colab notebooks:\r\n\r\n1. TF 1.15: https://colab.research.google.com/drive/10Rc3z2lgW4Xk1BbP0W2toStZ3HjPjGlJ?usp=sharing\r\n2. TF 2.2: https://colab.research.google.com/drive/1XXYYwXG7iWtwpVawqPL0SLrkA7dwTdYV?usp=sharing", "@omalleyt12 Just following up on this issue. Did you get a chance to take a look at it?", "Any updates here? It's been quite a while. ", "Was able to reproduce the issue in TF 2.5 as well. Thanks!", "Hi There,\n\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \n\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40076\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40076\">No</a>\n"]}, {"number": 40074, "title": "[Feature Request] Multiple primals and tangents for tf.autodiff.ForwardAccumulator", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.2\r\n- Are you willing to contribute it (Yes/No): Not Sure\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n`tf.autodiiff.ForwardAccumulator` only supports Tensors or Variables as primals and tangents. To take JVP of a Neural Network in forward mode, each and every variable is needed to be treated seperately. So, that would mean, number of of iterations needed for the JVP would be equal to the total number of variables. That's completely infeasible.\r\n\r\n**Will this change the current api? How?**\r\nYes, instead of one primal and one tangent in `tf.autodiff.ForwardAccumulator` it would have the capability to take multiple primals and tangents and when .jvp() is called it should return a list of all the JVPs.\r\n**Who will benefit with this feature?**\r\n\r\nAnyone who is need forward mode JVPs / HVPs of multiple variables in their code.\r\n\r\n**Any Other info.**\r\nHere's a sample of how that should work:\r\n```python3\r\nx = tf.eye(2)\r\nlayer = tf.keras.layers.Dense(1)\r\nlayer.build([None, 2])\r\ntangents = [tf.eye(*variable.shape) for variable in layer.trainable_variables]\r\nwith tf.autodiff.ForwardAccumulator(layer.trainable_variables, tangents) as acc:\r\n  output = layer(x)\r\n  jvps = acc.jvp(output)\r\n```\r\nwhere `jvps` should be a list of the jvp of the variables and tangents.\r\n\r\nCC: @allenlavoie", "comments": ["Forward-mode autodiff always computes slices of the output (loss.shape) rather than slices of the input (variables). It's the same shape as finite differences (`(f(x) - f(x + h)) / h`).\r\n\r\nTo compute a Hessian-vector product, see the ForwardAccumulator docstring: https://github.com/tensorflow/tensorflow/blob/2b96b9c02d81f54dc4959bde2f8b4d92af72679c/tensorflow/python/eager/forwardprop.py#L274-L285\r\n\r\nFor computing Hessians efficiently you do need to add looping (e.g. from `tf.vectorized_map`): https://github.com/tensorflow/tensorflow/blob/071a3892f6cc2653f38923cd967f62385c3b2f3e/tensorflow/python/eager/forwardprop_test.py#L172-L191\r\n\r\nThere is a GSoC student this summer working on supporting batches of tangents for each variable, basically pushing `vectorized_map` inside `ForwardAccumulator`. This would allow the forward pass of the model to run eagerly, which you don't get with `vectorized_map` + `ForwardAccumulator` right now (since vectorized_map traces the function). So you'd figure out a batch of tangents that you wanted to compute forwardprop for in advance, and `acc.jvp` would return a corresponding batch of forwardprop runs. Is that something you'd find useful?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Hey @allenlavoie , this looks great! Thanks for the help. :)\r\nClosing the issue for now.", "Hi! I would like to contribute to this issue taking over from last year's GSOC work :)\r\nHow can I start?", "@ymodak Can we reopen this? Who could mentor @IAL32?"]}, {"number": 40073, "title": "bugfix: add a method to check penalty number availability", "body": "add a method to check penalty number availability\r\n\r\nfixs #37196 \r\nrelates to #37634", "comments": []}, {"number": 40072, "title": "tf.nn.relu on nan inputs returns zeros on GPU", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, see below.\r\n- OS Platform and Distribution: Ubuntu 16.04, Google Colab\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 2.2\r\n- Python version: 3.6.9\r\n- CUDA/cuDNN version: 10.1, 7.6.5\r\n- GPU model and memory: GeForce GTX 1080 (8GB) (and Google Colab)\r\n\r\n**Describe the current behavior**\r\n\r\nThe behavior of `tf.nn.relu` when fed with `nan`-valued inputs is inconsistent between GPU and CPU:\r\n\r\n- The CPU returns `nan` (as expected).\r\n- The GPU returns `0`, obfuscating `nan` in the computation.\r\n\r\nSimilar (but not quite identical) behavior has been described for `TF1.14` in #32730 and subsequently fixed in `TF1.15.3`, but remains in `TF2.2`\r\n\r\n**Describe the expected behavior**\r\n\r\nAll devices should show consistent behavior like the CPU, returning `nan`.\r\n\r\n**Standalone code to reproduce the issue**\r\nThe following snippet can also be found in [this Colab notebook](https://colab.research.google.com/drive/1LtUP4TySBPd7Ng80qdQtB2UIl3mtF3cX?usp=sharing).\r\n\r\nThe first three assertions do not fail. The last three assertions all fail. This is a bit different from #32730, where the `x1` assertion worked.\r\n```python\r\nx1 = tf.nn.relu(np.nan)\r\nx2 = tf.nn.relu(np.nan * tf.random.normal(shape=[]))\r\nx3 = tf.nn.relu(tf.Variable(np.nan))\r\nwith tf.device(\"/cpu:0\"):\r\n    x1_cpu = tf.nn.relu(np.nan)\r\n    x2_cpu = tf.nn.relu(np.nan * tf.random.normal(shape=[]))\r\n    x3_cpu = tf.nn.relu(tf.Variable(np.nan))\r\n\r\nassert np.all(np.isnan(x1_cpu.numpy()))\r\nassert np.all(np.isnan(x2_cpu.numpy()))\r\nassert np.all(np.isnan(x3_cpu.numpy()))\r\n\r\nassert np.all(np.isnan(x1.numpy()))\r\nassert np.all(np.isnan(x2.numpy()))\r\nassert np.all(np.isnan(x3.numpy()))\r\n```", "comments": ["I am able to replicate the issue, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/8c1d1b6eef1745a6d5e6d992f27e65db/untitled208.ipynb) Thanks!", "Please be aware of https://github.com/tensorflow/tensorflow/issues/42885#issuecomment-687151509 if you are going to retest this on colab again. \r\nThat duplicated issue was closed as solved but it isn't solved.", "@msoelch Looks like this was resolved in `2.4rc3` that was just released. Can you please check and let us know whether it was resolved or not for you. [Here](https://colab.research.google.com/gist/jvishnuvardhan/e9db6d904cd5c816f78ec7c53bd4962c/untitled208.ipynb) is the gist for our reference. Thanks!\r\n\r\nPlease close the issue if this was resolved in recent TF version. Thanks!", "Looks good, thank you!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40072\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40072\">No</a>\n"]}, {"number": 40071, "title": "[MLIR] Add conversions dot op from LHLO to Affine", "body": "Add conversion from MLIR lhlo dot op to affine loops. These conversions are\r\nrun as part of -lhlo-legalize-to-affine passes.\r\n\r\nSigned-off-by: Prashant Kumar <prashantk@polymagelabs.com>", "comments": ["Posted comment on #40069 (please avoid closing/creating identical PR, you can always force-push to you branch to update the state and it should update the PR appropriately)."]}, {"number": 40070, "title": "Class Weights InvalidArgumentError", "body": "Hi, I am using class weights for my unbalanced dataset, and i am getting this error \r\n\r\n```\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-38-797b60fa6b57> in <module>\r\n      6     steps_per_epoch=STEPS_PER_EPOCH,\r\n      7     validation_data=get_validation_dataset(),\r\n----> 8     class_weight=class_weigths\r\n      9 )\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    817         max_queue_size=max_queue_size,\r\n    818         workers=workers,\r\n--> 819         use_multiprocessing=use_multiprocessing)\r\n    820 \r\n    821   def evaluate(self,\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    340                 mode=ModeKeys.TRAIN,\r\n    341                 training_context=training_context,\r\n--> 342                 total_epochs=epochs)\r\n    343             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)\r\n    344 \r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\r\n    126         step=step, mode=mode, size=current_batch_size) as batch_logs:\r\n    127       try:\r\n--> 128         batch_outs = execution_function(iterator)\r\n    129       except (StopIteration, errors.OutOfRangeError):\r\n    130         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in execution_function(input_fn)\r\n     96     # `numpy` translates Tensors to values in Eager mode.\r\n     97     return nest.map_structure(_non_none_constant_value,\r\n---> 98                               distributed_function(input_fn))\r\n     99 \r\n    100   return execution_function\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/util/nest.py in map_structure(func, *structure, **kwargs)\r\n    566 \r\n    567   return pack_sequence_as(\r\n--> 568       structure[0], [func(*x) for x in entries],\r\n    569       expand_composites=expand_composites)\r\n    570 \r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/util/nest.py in <listcomp>(.0)\r\n    566 \r\n    567   return pack_sequence_as(\r\n--> 568       structure[0], [func(*x) for x in entries],\r\n    569       expand_composites=expand_composites)\r\n    570 \r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in _non_none_constant_value(v)\r\n    128 \r\n    129 def _non_none_constant_value(v):\r\n--> 130   constant_value = tensor_util.constant_value(v)\r\n    131   return constant_value if constant_value is not None else v\r\n    132 \r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py in constant_value(tensor, partial)\r\n    820   \"\"\"\r\n    821   if isinstance(tensor, ops.EagerTensor):\r\n--> 822     return tensor.numpy()\r\n    823   if not is_tensor(tensor):\r\n    824     return tensor\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in numpy(self)\r\n    940     \"\"\"\r\n    941     # TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\r\n--> 942     maybe_arr = self._numpy()  # pylint: disable=protected-access\r\n    943     return maybe_arr.copy() if isinstance(maybe_arr, np.ndarray) else maybe_arr\r\n    944 \r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in _numpy(self)\r\n    908       return self._numpy_internal()\r\n    909     except core._NotOkStatusException as e:\r\n--> 910       six.raise_from(core._status_to_exception(e.code, e.message), None)\r\n    911 \r\n    912   @property\r\n\r\n/opt/conda/lib/python3.7/site-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: {{function_node __inference_distributed_function_431665}} Compilation failure: Detected unsupported operations when trying to compile graph has_valid_nonscalar_shape_true_402714_const_0[] on XLA_TPU_JIT: DenseToDenseSetOperation (No registered 'DenseToDenseSetOperation' OpKernel for XLA_TPU_JIT devices compatible with node {{node has_invalid_dims/DenseToDenseSetOperation}}\r\n\t.  Registered:  device='CPU'; T in [DT_INT8]\r\n  device='CPU'; T in [DT_INT16]\r\n  device='CPU'; T in [DT_INT32]\r\n  device='CPU'; T in [DT_INT64]\r\n  device='CPU'; T in [DT_UINT8]\r\n  device='CPU'; T in [DT_UINT16]\r\n  device='CPU'; T in [DT_STRING]\r\n){{node has_invalid_dims/DenseToDenseSetOperation}}\r\n\t [[has_valid_nonscalar_shape]]\r\n\t [[loss/dense_4_loss/weighted_loss/broadcast_weights/assert_broadcastable/is_valid_shape]]\r\n\tTPU compilation failed\r\n\t [[tpu_compile_succeeded_assert/_2606253191027783421/_10]]\r\n```\r\n\r\nI am using Kaggle Notebook with TPU. And Tensorflow version is `2.1.0` and Keras version is `2.2.4-tf`. Here's the [kaggle notebook](https://www.kaggle.com/shubhamai/melanoma-classification?scriptVersionId=35284218) to reproduce the error. Thanks.", "comments": ["Refer to this [link](https://github.com/tensorflow/tensorflow/issues/37575) it might be helpful. Update your class_weigths to a list instead of a dictionary \r\n`class_weigths = [0.176754, 9.823246]` the model is being trained with this change", "Hi, @PradeepNalluri Thanks, brother. It worked! But why we have to change `class_weight` in GPU and TPU ? ", "TBH I am not sure why there is a problem if we are using a dict but as mentioned in the referred link class_weight being a dict is working with `tf-nightly` even for TPU. And here is the [gist link](https://colab.research.google.com/gist/jvishnuvardhan/93e24a5043b797e0f47b641b5ead2723/keras-class-weight-tpu.ipynb) ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40070\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40070\">No</a>\n", "When I change my class_weight from a Dict: `class_weight = {0:1.0, 1:1.0}` to a List `class_weight = [1.0, 1.0]`, I get the following error:\r\n```\r\n  File \"/Users/.../anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 66, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n  File \"/Users/.../anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 815, in fit\r\n    model=self)\r\n  File \"/Users/.../anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\", line 1117, in __init__\r\n    dataset = dataset.map(_make_class_weight_map_fn(class_weight))\r\n  File \"/Users/.../anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\", line 1229, in _make_class_weight_map_fn\r\n    class_ids = list(sorted(class_weight.keys()))\r\nAttributeError: 'list' object has no attribute 'keys'\r\n```\r\nOn the other hand, if I use a Dict, I get the following error:\r\n```\r\n  File \"/Users/.../anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 66, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n  File \"/Users/.../anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 848, in fit\r\n    tmp_logs = train_function(iterator)\r\n  File \"/Users/.../anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 580, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/Users/.../anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 644, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/Users/.../anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2420, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"/Users/.../anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1665, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"/Users/.../anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1746, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/Users/.../anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 598, in call\r\n    ctx=ctx)\r\n  File \"/Users/.../anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError:  indices[4] = 2 is not in [0, 2)\r\n\t [[{{node GatherV2}}]]\r\n\t [[IteratorGetNext]] [Op:__inference_train_function_10811]\r\nFunction call stack:\r\ntrain_function\r\n```\r\nI'm using tensorflow 2.2.0.", "I also have the same problem (with same results as @kevinashaw ) for TF 2.3. Is it possible to re-open this ticket?", "Has anybody solved this issue? Or found a workaround? ", "@YokoHono the only way I have been able to get it to work is by including the weights in a TF dataset with the input and labels.", "@markmace Can you share how you did that?\r\n", "Also struggling with this issue with TF 2.3", "Same issue as @kevinashaw ", "Same issue with TF 2.3\r\n\r\n", "Same issue with TF 2.4.1 :/", "Same issue as @kevinashaw with tf 2.4.1 - does anyone have a solution?", "Same issue as @kevinashaw ", "same problem here tf 2.7.0", "Same here, what is the solution for this?", "any solution for this issue?", "I am getting the same errors with version 2.8.0.\r\n\r\nWhen using a list for class_weight, I got:\r\n```\r\n  File \"/opt/local/stow/Python-3.7.1/lib/python3.7/site-packages/keras/engine/data_adapter.py\", line 1416, in _make_class_weight_map_fn\r\n    class_ids = list(sorted(class_weight.keys()))\r\nAttributeError: 'list' object has no attribute 'keys'\r\n```\r\nWhen using a dictionary for class_weight, I got:\r\n```\r\n  File \"/opt/local/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\", line 55, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:\r\n\r\nindices[24] = 18 is not in [0, 10)\r\n         [[{{node GatherV2}}]]\r\n         [[IteratorGetNext]] [Op:__inference_train_function_43205]\r\n```\r\nThese errors occur even when I give all classes a weight 1.\r\n\r\nHowever, when I do not use class_weight in model.fit(), the training for my model just works fine.\r\n\r\nSo it looks like I just cannot use class_weight in training. But my classes are highly imbalanced; not using class weights would train a useless model.\r\n\r\nI would really appreciate any solution or workaround for this issue.\r\n\r\nThank you very much!\r\n", "I am getting the same errors with version 2.8.0 and python 3.9.\r\nI need to use class_weight cause of my data is heavily imbalanced.\r\nIf I run without the class_weight, no errors.\r\nIs there solutions or workarounds for this issue?\r\nThanks", "How I got it going in the end:\r\n\r\n```\r\nlabels = np.array([])\r\nfor data, label in training:\r\n  labels = np.concatenate([labels, np.argmax(label, axis=-1)])  \r\n\r\nvalue, count = np.unique(labels, return_counts=True)\r\nweights = 1/(count/np.mean(count))\r\nclass_weights = {key:val for key, val in enumerate(weights)}\r\n\r\nepochs = 2\r\nhistory = model.fit(training, epochs=epochs, validation_data=validation, class_weight=class_weights)\r\n```\r\n\r\nThe real bug here is that the documentation on this very important feature is terrible.", "> I am getting the same errors with version 2.8.0 and python 3.9. I need to use class_weight cause of my data is heavily imbalanced. If I run without the class_weight, no errors. Is there solutions or workarounds for this issue? Thanks\r\n\r\nI used focal loss to handle imbalance as I was not able to use class_weight..\r\n\r\nthis link helped me:\r\nhttps://www.dlology.com/blog/multi-class-classification-with-focal-loss-for-imbalanced-datasets/", "Thanks @bill-connelly for sharing your code!\r\n\r\nUsing your code, I tested it out by giving all classes an equal weight 1.0 (I have 10 classes), as the following:\r\n```\r\nweights = np.ones(10)\r\nclass_weights_dict = {key:val for key, val in enumerate(weights)}\r\n```\r\nThe created `class_weights_dict`, which is assigned to `class_weight` in `model.fit()`, is the following: \r\n```\r\n{0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 1.0, 7: 1.0, 8: 1.0, 9: 1.0}\r\n```\r\nAnd I also verified that my true labels array `train_true_labels` contains only integers 0-9, as the following:\r\n```\r\nvalues = np.unique(train_true_labels)\r\nprint(values)\r\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\r\n```\r\nHowever, I am still getting the same error and tensorflow still failed:\r\n```\r\n  File \"/opt/local/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\", line 55, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:\r\n\r\nindices[9] = 16 is not in [0, 10)\r\n         [[{{node GatherV2}}]]\r\n         [[IteratorGetNext]] [Op:__inference_train_function_43205]\r\n```\r\n\r\nAny ideas?\r\n\r\nThank you so much!\r\n", "@lauraht Sorry, I'm honestly still kinda confused as to how my one worked [and it really did, I've got the confusion matrix to prove it]. I don't understand how this dictionary of \"labels\" got mapped to the actually training dataset, as it's a one-hot encoded vector. i.e. how does the model know that the key [1] in the class_weights_dict is related training dataset of [0 1 0 0 0 0 0]? ", "Thank you @bill-connelly!\r\nI guess probably there may be a bug that only occurs under certain circumstances......\r\n"]}, {"number": 40069, "title": "[MLIR] Add conversions dot op from LHLO to Affine", "body": "Dot op was not present from LHLO to Affine conversion.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40069) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!\r\n", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40069) for more info**.\n\n<!-- need_author_consent -->", "@googlebot I consent."]}, {"number": 40068, "title": "Model fit incredibly slow ", "body": "Hi, I've been trying to fit the following model for the last 3 hours and the the only output displayed by the model is 'Epoch 1/5'. I noticed when others fitted their models, the output would also display 'Train on X samples, Validate on X samples' and thought maybe the lack of seeing that display and the model hanging are related \r\n```\r\n#Creating the first layer \r\nmodel = Sequential()\r\nmodel.add(Conv1D(2,0, activation = 'relu', input_shape = X_train[0].shape  ))\r\nmodel.add(BatchNormalization())\r\nmodel.add(MaxPool1D())\r\nmodel.add(Dropout(0.4))\r\n\r\n#Adding second layer\r\nmodel.add(Conv1D(4, 0, activation = 'relu'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(MaxPool1D())\r\nmodel.add(Dropout(0.4))\r\n\r\n#Adding third layer\r\nmodel.add(Conv1D(8, 0, activation = 'relu'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(MaxPool1D())\r\nmodel.add(Dropout(0.4))\r\n\r\n#Adding fourth layer\r\nmodel.add(Conv1D(16, 0, activation = 'relu'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(MaxPool1D())\r\nmodel.add(Dropout(0.5))\r\n\r\nmodel.add(Flatten())\r\n\r\nmodel.add(Dense(16, activation = 'relu'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(Dropout(0.5)) \r\nmodel.add(Dense(16, activation = 'relu'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(Dropout(0.5))\r\n\r\nmodel.add(Dense(4, activation = 'softmax'))\r\n\r\nmodel.summary() \r\n\r\nmodel.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\r\nhistory = model.fit(X_train, y_train, epochs = 5, batch_size = 5, validation_data = (X_test, y_test), verbose = 1)\r\n```\r\n\r\nBelow is the result of model.summary():\r\n```\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nconv1d (Conv1D)              (None, 2, 2)              2         \r\n_________________________________________________________________\r\nbatch_normalization (BatchNo (None, 2, 2)              8         \r\n_________________________________________________________________\r\nmax_pooling1d (MaxPooling1D) (None, 1, 2)              0         \r\n_________________________________________________________________\r\ndropout (Dropout)            (None, 1, 2)              0         \r\n_________________________________________________________________\r\nconv1d_1 (Conv1D)            (None, 2, 4)              4         \r\n_________________________________________________________________\r\nbatch_normalization_1 (Batch (None, 2, 4)              16        \r\n_________________________________________________________________\r\nmax_pooling1d_1 (MaxPooling1 (None, 1, 4)              0         \r\n_________________________________________________________________\r\ndropout_1 (Dropout)          (None, 1, 4)              0         \r\n_________________________________________________________________\r\nconv1d_2 (Conv1D)            (None, 2, 8)              8         \r\n_________________________________________________________________\r\nbatch_normalization_2 (Batch (None, 2, 8)              32        \r\n_________________________________________________________________\r\nmax_pooling1d_2 (MaxPooling1 (None, 1, 8)              0         \r\n_________________________________________________________________\r\ndropout_2 (Dropout)          (None, 1, 8)              0         \r\n_________________________________________________________________\r\nconv1d_3 (Conv1D)            (None, 2, 16)             16        \r\n_________________________________________________________________\r\nbatch_normalization_3 (Batch (None, 2, 16)             64        \r\n_________________________________________________________________\r\nmax_pooling1d_3 (MaxPooling1 (None, 1, 16)             0         \r\n_________________________________________________________________\r\ndropout_3 (Dropout)          (None, 1, 16)             0         \r\n_________________________________________________________________\r\nflatten (Flatten)            (None, 16)                0         \r\n_________________________________________________________________\r\ndense (Dense)                (None, 16)                272       \r\n_________________________________________________________________\r\nbatch_normalization_4 (Batch (None, 16)                64        \r\n_________________________________________________________________\r\ndropout_4 (Dropout)          (None, 16)                0         \r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 16)                272       \r\n_________________________________________________________________\r\nbatch_normalization_5 (Batch (None, 16)                64        \r\n_________________________________________________________________\r\ndropout_5 (Dropout)          (None, 16)                0         \r\n_________________________________________________________________\r\ndense_2 (Dense)              (None, 4)                 68        \r\n=================================================================\r\nTotal params: 890\r\nTrainable params: 766\r\nNon-trainable params: 124\r\n_________________________________________________________________\r\n```\r\n\r\nGiven the size of the model, I find it hard to believe that it would take over 3 hours for anything to happen. Any insight is appreciated and happy to provide any additional information. ", "comments": ["X_train is a numpy array of size [100,1,1] and is type float32. y_train is of size [100,2] and is of type int32", "@njoshi94,\r\nOn running the code, I am facing an error stating \r\n`ValueError: Dimensions must be equal, but are 2 and 4 for '{{node binary_crossentropy/mul}} = Mul[T=DT_FLOAT](IteratorGetNext:1, binary_crossentropy/Log)' with input shapes: [5,2], [5,4].` \r\nPlease find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/e77bdef0b806baab0f8e67b2bbbabf46/40068.ipynb#scrollTo=0-BxKBKqdTIJ). \r\n\r\nCould you please provide the complete code and the dataset you are using, so that I can reproduce the issue on my end. Thanks!", "Also, please provide the TensorFlow version you are using. Thanks!", "Version - 2.2.0\r\n\r\nFull code: \r\n```\r\nimport pandas as pd \r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import Sequential \r\nfrom tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization, Conv2D, MaxPool2D \r\nfrom tensorflow.keras.layers import GlobalMaxPooling2D \r\nfrom tensorflow.keras.layers import Conv1D, MaxPool1D, GlobalMaxPooling1D\r\nfrom tensorflow.keras.layers import LSTM \r\nfrom tensorflow.keras.optimizers import Adam \r\nfrom sklearn.model_selection import train_test_split\r\nimport matplotlib.pyplot as plt \r\n\r\n#Importing Data \r\nfilename = 'RecipesData.xlsx'\r\ndf = pd.read_excel(filename)\r\n\r\n#Splitting the different restrictions into a list \r\ndf['Restrictions'] = df['Restrictions'].str.split(', ')\r\n\r\n#Splitting the data into test and training \r\nX = df.drop(['Food', 'Url', 'Recipe', 'Extra Words', 'Total Words',\r\n             'Restrictions', 'None', 'Gluten Free', 'Vegan', 'Refined Sugar Free'], axis = 1)\r\ny = df.drop(['Food', 'Url', 'Recipe', 'Total Words', 'Extra Words', 'Percent', \r\n             'Restrictions'], axis = 1)\r\n\r\nX = X.to_numpy()\r\nX = X[...,np.newaxis]\r\ny = y.to_numpy()\r\n\r\n#test_size split to have 100 test and 20 train values \r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.8333, random_state = 0)\r\n\r\nX_train = X_train.astype('float32')\r\nX_test = X_test.astype('float32')\r\ny_train = y_train.astype('int32')\r\ny_test = y_test.astype('int32')\r\n\r\n#Bulding the Model\r\n#Creating the first layer \r\nmodel = Sequential()\r\nmodel.add(Conv1D(2,0, activation = 'relu', input_shape = X_train[0].shape  ))\r\nmodel.add(BatchNormalization())\r\nmodel.add(MaxPool1D())\r\nmodel.add(Dropout(0.4))\r\n\r\n#Adding second layer\r\nmodel.add(Conv1D(4, 0, activation = 'relu'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(MaxPool1D())\r\nmodel.add(Dropout(0.4))\r\n\r\n#Adding third layer\r\nmodel.add(Conv1D(8, 0, activation = 'relu'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(MaxPool1D())\r\nmodel.add(Dropout(0.4))\r\n\r\n#Adding fourth layer\r\nmodel.add(Conv1D(16, 0, activation = 'relu'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(MaxPool1D())\r\nmodel.add(Dropout(0.5))\r\n\r\nmodel.add(Flatten())\r\n\r\nmodel.add(Dense(16, activation = 'relu'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(Dropout(0.5))\r\n\r\nmodel.add(Dense(16, activation = 'relu'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(Dropout(0.5))\r\n\r\nmodel.add(Dense(4, activation = 'softmax'))\r\n\r\nmodel.summary() \r\n\r\nmodel.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\r\n\r\nhistory = model.fit(X_train, y_train, epochs = 5, batch_size = 5, validation_data = (X_test, y_test), verbose = 1)\r\n\r\n```\r\n\r\nDataset:\r\n[RecipesData.xlsx](https://github.com/tensorflow/tensorflow/files/4718278/RecipesData.xlsx)\r\n", "Let me know if you need anything else! ", "After experimenting for a while, the issue arose from including `model.add(MaxPool1D())` to all the layers beyond the first. "]}, {"number": 42804, "title": "bn - Add support for Bengali Language", "body": "Requesting a new feature. Bengali language translation.\r\nI am willing to contribute.", "comments": ["Hi @tazihad  , Sorry for the delay.\r\nUnfortunately, each language (community-driven or officially supported) adds additional maintenance costs so we're prioritizing new languages based on a combination of site metrics and community support. No plans to add Bengali translations but will keep an eye on site. Thanks"]}, {"number": 40067, "title": "Doc recommends stateful training but the batches are shuffled?", "body": "I'm reading this tutorial page from the documentation: https://www.tensorflow.org/tutorials/text/text_generation\r\n\r\nThe GRU layer is stateful so it remembers its state between batches. But the batches are shuffled. Therefore I think the stateful parameter should be `False`. ", "comments": ["Can I try to work on this?", "> Therefore I think the stateful parameter should be False.\r\n\r\n+1\r\n\r\n> @paulis-reece :  Can I try to work on this?\r\n\r\nYes. Send me a PR on tensorflow_docs. Be sure to link it to this bug so we don't forget to close it.\r\n", "Thanks! However, since I am new to this. I am not sure where the parameter is located.", "I am currently searching for which parameter this is referring to.", "Look for the \"stateful=True\" line.\r\n", "Okay thanks I will get on it!", "I changed the GRU stateful to false, should I also change the Test For GRU V2 Layer stateful parameter as well?", "No. \r\n\r\nThe only file that needs to be changed is:\r\n\r\nhttps://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/text_generation.ipynb\r\n", "@bjourne \r\nIs this still an issue.", "This was resolved by: https://github.com/tensorflow/docs/commit/7f4d0f1780089308106b8ee3336dca24b730d84f"]}, {"number": 40066, "title": "Tensorflow S3 read performance improvement", "body": "Hello TF community,\r\n\r\nI have a well-tuned input pipeline build using tf.data API. The data is loaded from S3 in TFRecord format. Running one pipeline gives me 7GB/min IO throughput on a p2.8xlarge EC2 instance. If I created two input pipeline instances in two separate processes, the IO throughput is almost doubled. I am thinking if it is possible to create two input pipelines in two processes and feed into one model training process. This will increase the IO throughput significantly.\r\n\r\nIs it possible to share TF dataset between processes?\r\n\r\nThanks, Weiqi", "comments": ["@weiqi1028 \r\n\r\nThis question is better asked on StackOverflow since it is not a bug or feature request. There is also a larger community that reads questions there and provide better and faster support for such issues. Thanks!"]}, {"number": 40064, "title": "[MLIR] Add ConvOp conversion from xla_hlo -> xla_lhlo -> linalg", "body": "Add conversion for the convolution op from the xla_hlo dialect\r\nto xla_lhlo, and from xla_lhlo to linalg. The two conversion passes\r\nin context here are -hlo-legalize-to-lhlo and -lhlo-legalize-to-linalg.  A part\r\nof the code for the LHLO to Linalg conversion is borrowed and adapted from\r\nIREE's xla_hlo to linalg conversion (https://github.com/google/iree/).\r\n\r\nSigned-off-by: Uday Bondhugula <uday@polymagelabs.com>", "comments": ["For visibility: @MaheshRavishankar @pifon2a @stellaraccident @sherhut ", "Sorry, looks like I force pushed instead of adding a address review commit; but changes should be easy to spot here. ", "Gentle ping @joker-eph anything else here?", "@bondhugula Can you please address Ubuntu Sanity errors? Thanks!", "> @bondhugula Can you please address Ubuntu Sanity errors? Thanks!\r\n\r\nDone, thanks."]}, {"number": 40063, "title": "tf.estimator.BoostedTreesClassifier does support multi-classes: AttributeError: 'NoneType' object has no attribute 'is_compatible_with'", "body": "System information\r\nI am using colab to reproduce the issue and the ipynb is attached below.\r\n\r\nYou can collect some of this information using our environment capture\r\ntf.version.GIT_VERSION: v2.2.0-0-g2b96f3662b \r\ntf.version.VERSION:  2.2.0\r\n\r\nDescribe the current behavior\r\ncannot train tf.estimator.BoostedTreesClassifier on multi-classes data\r\n\r\nDescribe the expected behavior\r\nChange the last 100 samples' label to a third class in following tutorial:\r\nhttps://www.tensorflow.org/tutorials/estimator/boosted_trees#train_and_evaluate_the_model\r\n\r\nStandalone code to reproduce the issue\r\nhttps://colab.research.google.com/drive/13vl2mV7C_62HxKCw7-uuMp5WMm_OxYGL?usp=sharing\r\n\r\nOther info / logs Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nerror is show in the last cell of the colab notebook.\r\n\r\n```\r\nINFO:tensorflow:Using default config.\r\nWARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpf3g1hc6c\r\nINFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpf3g1hc6c', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\r\ngraph_options {\r\n  rewrite_options {\r\n    meta_optimizer_iterations: ONE\r\n  }\r\n}\r\n, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/canned/boosted_trees.py:398: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\r\nINFO:tensorflow:Calling model_fn.\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-35-828fa5064808> in <module>()\r\n      8 # The model will stop training once the specified number of trees is built, not\r\n      9 # based on the number of steps.\r\n---> 10 est.train(train_input_fn, max_steps=100)\r\n     11 \r\n     12 # Eval.\r\n\r\n13 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/parallel_for/gradients.py in batch_jacobian(output, inp, use_pfor, parallel_iterations)\r\n    111   \"\"\"\r\n    112   output_shape = output.shape\r\n--> 113   if not output_shape[0].is_compatible_with(inp.shape[0]):\r\n    114     raise ValueError(\"Need first dimension of output shape (%s) and inp shape \"\r\n    115                      \"(%s) to match.\" % (output.shape, inp.shape))\r\n\r\nAttributeError: 'NoneType' object has no attribute 'is_compatible_with'\r\n```", "comments": ["I am able to replicate this issue, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/c27357d1f805ca47a449ebd6d126596e/untitled208.ipynb)", "@neilteng Do we need to change the label for the last 100 samples as mentioned in your description? Does your code is a standalone code that is ready with the above change or we need to manually change the dataset?\r\n\r\n> Change the last 100 samples' label to a third class in following tutorial:\r\n\r\n", "@jvishnuvardhan, the standalaone code linked above already changes the label. ", "> @jvishnuvardhan, the standalaone code linked above already changes the label.\r\n\r\nYes, the code linked has been changed. Thank you rsk7.", "Hey, I had the same problem with BoostedTreesClassifier model, multi class in TF 2.2.0 (which should be available in this version). Does this bug has a fix already? When I changed manually the condition to:\r\nif inp.shape[0] != output_shape[0]:\r\nrais ValueError....\r\nEverything seem to work OK. But surely the fix should be done within TF code.", "Yes. Facing the same issue.\r\n\r\nOn tensorflow 2.2.0", "I've got the same issue on TF 2.2 and 2.3\r\n@RonAd-1 could you share how you have fixed this?", "I wrote exactly what I did, \r\ngo to the relevant line in the file and change it to what I did. \r\nBut surely it's not a real fix, just did it to check \r\nif my guess was correct. ", "This was fixed in \r\n\r\nhttps://github.com/tensorflow/estimator/commit/2a3cf0794da5016bdb92239783e53859ca66c018", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40063\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40063\">No</a>\n", "I think tf.disable_v2_behavior() fixes the problem too."]}, {"number": 40062, "title": "[TF-TRT] Fix TopK Combined NMS", "body": "CC: @bixia1 \r\n\r\nin tftrt, the tf.image.combined_non_max_suppression op is replaced by a trt engine using batchedNMSplugin, with the minimal reproducible code piece, there's something not aligned.\r\n\r\nTRT nms plugin has argument \"topK\", and we will only do IOU operation on the topK box in each class, so we set topK to boxes count in each class, to fix mismatch between TF and TRT.\r\n\r\n\r\n", "comments": ["@bixia1 as requested here is a small script to reproduce the issue:\r\n\r\n```python\r\nimport os\r\nimport time\r\n\r\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"]=\"3\"\r\n\r\nimport numpy as np\r\nimport tensorflow.compat.v1 as tf\r\n\r\ntf.disable_v2_behavior()\r\n\r\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\r\n\r\n\r\nN = 2048\r\nnum_classes = 90\r\n\r\npre_nms_boxes = tf.placeholder(tf.float32, name='boxes', shape=[1, N, 1, 4])\r\npre_nms_scores = tf.placeholder(tf.float32, name='scores', shape=[1, N, num_classes])\r\nmax_detetion_points = N\r\nmax_boxes_to_draw = 30\r\niou_threshold = 0.1\r\nmin_score_thresh = 0.001\r\n\r\npost_nms_boxes, post_nms_scores, post_nms_classes, post_nms_num_valid_boxes = \\\r\n    tf.image.combined_non_max_suppression(\r\n        pre_nms_boxes,\r\n        pre_nms_scores,\r\n        max_output_size_per_class=max_detetion_points,\r\n        max_total_size=max_boxes_to_draw,\r\n        iou_threshold=iou_threshold,\r\n        score_threshold=min_score_thresh,\r\n        pad_per_class=False\r\n    )\r\n\r\nnp.random.seed(666)\r\nnp_boxes = np.random.rand(1, 2048, 1, 4)\r\nnp_scores = np.random.rand(1, 2048, 90)\r\n\r\nwith tf.Session() as sess:\r\n    tf_output = sess.run([\r\n            post_nms_boxes,\r\n            post_nms_scores,\r\n            post_nms_classes,\r\n            post_nms_num_valid_boxes\r\n        ],\r\n        feed_dict={pre_nms_boxes: np_boxes, pre_nms_scores: np_scores}\r\n    )\r\n\r\nwith tf.Session() as sess:\r\n    output = [\r\n        tf.identity(i)\r\n        for i in [\r\n            post_nms_boxes,\r\n            post_nms_scores,\r\n            post_nms_classes,\r\n            post_nms_num_valid_boxes\r\n        ]\r\n    ]\r\n\r\n    output_node_names = [node.name.split(':')[0] for node in output]\r\n\r\n    graphdef = tf.graph_util.convert_variables_to_constants(\r\n        sess, sess.graph_def, output_node_names\r\n    )\r\n\r\n    fetches = [pre_nms_boxes.name, pre_nms_scores.name] + [i.name for i in output]\r\n\r\n    from tensorflow.python.compiler.tensorrt import trt_convert as trt\r\n\r\n    converter = trt.TrtGraphConverter(\r\n        nodes_blacklist=[t.split(':')[0] for t in fetches],\r\n        input_graph_def=graphdef,\r\n        max_workspace_size_bytes=1 << 32,    # 4Gb\r\n        minimum_segment_size=1,\r\n        precision_mode='FP32')\r\n    infer_graph = converter.convert()\r\n\r\n    goutput = tf.import_graph_def(infer_graph, return_elements=fetches)\r\n\r\n    inputs, output = goutput[:2], goutput[2:]\r\n\r\n    trt_output = sess.run(\r\n        output,\r\n        feed_dict={inputs[0]: np_boxes, inputs[1]: np_scores}\r\n    )\r\n\r\n\r\nprint(\"tf_output  - post_nms_classes:\", tf_output[2])\r\nprint(\"trt_output - post_nms_classes:\", trt_output[2])\r\n```\r\n\r\nIf you run with current upstream tensorflow:\r\n```bash\r\n$ python main.py \r\n\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/compat/v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nnon-resource variables are not supported in the long term\r\ntf_output - post_nms_classes: [[89.  6. 85. 51. 68.  1.  1. 78. 51. 27. 61. 63. 30. 49. 78. 45.  8. 46.\r\n  76. 10. 35. 37. 24. 77. 13. 16. 35. 27. 28. 25.]]\r\ntrt_output - post_nms_classes: [[89.  6. 85. 51. 68.  1.  1.  1. 78. 51. 27. 85. 61. 63. 30. 49. 78. 45.\r\n   8. 46. 76. 10. 35. 37. 24. 77. 13. 16. 35. 27.]]\r\n```\r\n\r\nAs you can see the two results are different.\r\n\r\n```bash\r\n$ python main.py \r\n\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nnon-resource variables are not supported in the long term\r\ntf_output  - post_nms_classes: [[89.  6. 85. 51. 68.  1.  1. 78. 51. 27. 61. 63. 30. 49. 78. 45.  8. 46.\r\n  76. 10. 35. 37. 24. 77. 13. 16. 35. 27. 28. 25.]]\r\ntrt_output - post_nms_classes: [[89.  6. 85. 51. 68.  1.  1. 78. 51. 27. 61. 63. 30. 49. 78. 45.  8. 46.\r\n  76. 10. 35. 37. 24. 77. 13. 16. 35. 27. 28. 25.]]\r\n```\r\n\r\nAnd now with this change we are good ;)", "Thanks for the test case. Can we find a way to turn in into a test in the tree?", "@DEKHTIARJonathan  Can you please check @bixia1's comments and keep us posted. Thanks!", "@DEKHTIARJonathan Can you please address Ubuntu Sanity errors? Thanks!", "@gbaned styling fixed ;)", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40062) for more info**.\n\n<!-- need_author_consent -->", "@googlebot I consent.", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40062) for more info**.\n\n<!-- ok -->", "I have improved the converter and updated the C++ unit test of CombinedNMS converter. The changes are in branch https://github.com/tfeher/tensorflow/tree/tftrt/topk_combined_nms_fix\r\n\r\n~~@DEKHTIARJonathan  I did not want to force push to your branch, but it would make sense to continue the discussion here. Please consider pulling the changes from my branch here. (Note @bixia1 that my branch contains a changeset of #39990, that should be reviewed before this PR).~~\r\n\r\nHere are the changes:\r\n- We have to set top_k so that keep_top_k <= top_k, otherwise the plugin does not work with TRT 7.0\r\n- Enabled explicit batch mode for the converter, added test for it. Dynamic shapes are not supported by the plugin.\r\n- Added test to check the clib_boxes attribute usage\r\n- Added test with nonzero score threshold\r\n- [Added test](https://github.com/tfeher/tensorflow/blob/be6466539460f7158248f805ff85c47b903d8347/tensorflow/compiler/tf2tensorrt/convert/convert_nodes_test.cc#L2983-L3007) where the boxes are defined as y_max, x_max, y_min, x_min. This test will fail with TRT 7.1.2 or earlier and highlights that the CombinedNMS plugin is not compatible with the TF definition of the operator in these versions of TRT. @DEKHTIARJonathan  please check if this test works with 7.1.3.\r\n  Because of this error, it is not safe to convert CombinedNMS before 7.1.3. We should probably add an\r\n  ```c++\r\n  #if !IS_TRT_VERSION_GE(7, 1, 3, 0)\r\n    errors::Unimplemented(\"TensorRT BatchedNMS Plugin reqires TensorRT 7.1.3 or later\");\r\n  #endif\r\n  ```\r\nto the converter.  What is your opinion @bixia1?\r\n\r\n", "@tfeher please feel free to force push to my branch ;) I will test with TRT 7.1.3\r\n", "Updated this PR with the changes mentioned [above](https://github.com/tensorflow/tensorflow/pull/40062#issuecomment-657263292).", "@DEKHTIARJonathan Can you please resolve conflicts? Thanks!", "@DEKHTIARJonathan  Any update on this PR? Please. Thanks!", "Yes, I think @tfeher is working on it.", "I have updated the PR, from my side it is ready @bixia1.\r\n- Checked that it works with TRT 7.1.3.\r\n- Disabled conversion with any earlier TRT version. TRT plugin is inconsitent with TF in earlier versions and  [this](https://github.com/DEKHTIARJonathan/tensorflow/blob/269bdf6756d4cd5ca175c4cf85583fd85e40f485/tensorflow/compiler/tf2tensorrt/convert/convert_nodes_test.cc#L3131-L3134) test would fail. FYI @DEKHTIARJonathan ", "@DEKHTIARJonathan Can you please check @bixia1's comments and keep us posted ? Thanks!", "@DEKHTIARJonathan Any update on this PR? Please. Thanks!", "@DEKHTIARJonathan Any update on this PR? Please. Thanks!", "Do not close, WIP", "Steps to confirm:\r\n- [x] Checked that it works with TRT 7.1.3.\r\n- [x] Disabled conversion with any earlier TRT version. TRT plugin is inconsitent with TF in earlier versions and this test would fail", "@bixia1 @tfeher we are good to merge ;)\r\n\r\n[combined_nms_test.log](https://github.com/tensorflow/tensorflow/files/5460526/combined_nms_test.log)\r\n\r\nAttached is the log of the unittest. All passed and good ;)", "One more test failure, tensorflow/python/compiler/tensorrt:combined_nms_test with TRT 7.1.3:\r\n\r\nTraceback (most recent call last):\r\n  File \"/build/work/12b5ec9ab1fdb4841ff23eda896242132066/google3/runfiles/google3/third_party/tensorflow/python/framework/test_util.py\", line 1397, in decorated\r\n    return f(self, *args, **kwargs)\r\n  File \"/build/work/12b5ec9ab1fdb4841ff23eda896242132066/google3/runfiles/google3/third_party/tensorflow/python/compiler/tensorrt/test/tf_trt_integration_test_base.py\", line 1107, in _Test\r\n    self.RunTest(run_params)\r\n  File \"/build/work/12b5ec9ab1fdb4841ff23eda896242132066/google3/runfiles/google3/third_party/tensorflow/python/compiler/tensorrt/test/tf_trt_integration_test_base.py\", line 1030, in RunTest\r\n    GraphState.INFERENCE)\r\n  File \"/build/work/12b5ec9ab1fdb4841ff23eda896242132066/google3/runfiles/google3/third_party/tensorflow/python/compiler/tensorrt/test/tf_trt_integration_test_base.py\", line 910, in _VerifyGraphDef\r\n    graph_state)\r\n  File \"/build/work/12b5ec9ab1fdb4841ff23eda896242132066/google3/runfiles/google3/third_party/tensorflow/python/compiler/tensorrt/test/tf_trt_integration_test_base.py\", line 848, in _VerifyGraphDefV1\r\n    self._VerifyConnections(expected_engines, original_gdef, gdef_to_verify)\r\n  File \"/build/work/12b5ec9ab1fdb4841ff23eda896242132066/google3/runfiles/google3/third_party/tensorflow/python/compiler/tensorrt/test/tf_trt_integration_test_base.py\", line 670, in _VerifyConnections\r\n    (sorted(expected_input_map.items()), sorted(actual_input_map.items())))\r\nAssertionError: {'TRTEngineOp_0': {'pre_nms_scores', 'pre_nms_boxes'}, 'input_0': set(), 'input_1': set(), 'output_0': {'TRTEngineOp_0'}, 'output_1': {'TRTEngineOp_0'}, 'output_2': {'TRTEngineOp_0'}, 'output_3': {'TRTEngineOp_0'}, 'pre_nms_boxes': {'input_0'}, 'pre_nms_scores': {'input_1'}} != {'TRTEngineOp_0': {'input_1', 'input_0'}, 'input_0': set(), 'input_1': set(), 'output_0': {'TRTEngineOp_0'}, 'output_1': {'TRTEngineOp_0'}, 'output_2': {'TRTEngineOp_0'}, 'output_3': {'TRTEngineOp_0'}} (\r\nexpected:\r\n[('TRTEngineOp_0', {'pre_nms_scores', 'pre_nms_boxes'}), ('input_0', set()), ('input_1', set()), ('output_0', {'TRTEngineOp_0'}), ('output_1', {'TRTEngineOp_0'}), ('output_2', {'TRTEngineOp_0'}), ('output_3', {'TRTEngineOp_0'}), ('pre_nms_boxes', {'input_0'}), ('pre_nms_scores', {'input_1'})]\r\nvs actual:\r\n[('TRTEngineOp_0', {'input_1', 'input_0'}), ('input_0', set()), ('input_1', set()), ('output_0', {'TRTEngineOp_0'}), ('output_1', {'TRTEngineOp_0'}), ('output_2', {'TRTEngineOp_0'}), ('output_3', {'TRTEngineOp_0'})])\r\nrepr() of differing entries:\r\n'TRTEngineOp_0': {'pre_nms_scores', 'pre_nms_boxes'} != {'input_1', 'input_0'}\r\n\r\nMissing entries:\r\n'pre_nms_boxes': {'input_0'}\r\n'pre_nms_scores': {'input_1'}", "@DEKHTIARJonathan  Can you please check @bixia1's comments and keep us posted ? Thanks!", "@bixia1 the C++ unittest wasn't properly deactivated with TRT < 7.1.3.\r\n\r\nFixed has been applied and commits squashed: https://github.com/tensorflow/tensorflow/pull/40062/files#diff-\r\nb9ae60f058d9ae72ddc0625fa93ff1b8441e57db5695e8444d18becdb3abea2dL2976-R3002\r\n\r\nPlease run the CI again ;) And we can merge if good", "It seems to me that this failure is still reproducible\r\nhttps://github.com/tensorflow/tensorflow/pull/40062#issuecomment-722816879\r\n", "@DEKHTIARJonathan Can you please check @bixia1's comments and keep us posted ? Thanks!", "Reassigning this issue to @tfeher ", "@tfeher could we finalized the last few bits of that one ?", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I have updated the PR. I could not reproduce the failing test yet. According to the log it is caused by the V1 graph def verification (`_VerifyGraphDefV1`) which is known to be finicky. The nodes in the converted graphs has to match the lists given by `ExpectedEnginesToBuild`. I have removed the extra identity nodes added by the new test, in principle this should fix the error."]}, {"number": 40061, "title": "from_dlpack leaking memory", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\n2.2.0\r\n- Python version:\r\n3.6\r\n- CUDA/cuDNN version:\r\n10.1\r\n- GPU model and memory:\r\nV100 32GB\r\n\r\nTensorFlow `from_dlpack` causes permanent decreases in available GPU memory, leading to OOM issues when called iteratively (e.g. for feeding Keras models, see [this example](https://github.com/NVIDIA/NVTabular/blob/master/nvtabular/tf_dataloader.py)). Since the tensor returned by `from_dlpack` points to the original capsule, I would expect the memory to be freed as soon as the capsule (and any of its pointers) are destroyed.\r\n\r\nI've reproduced the issue, as well as provided comparisons with PyTorch behavior, in [this repo](https://github.com/alecgunny/tf-dlpack-repro). A basic example, run in the environment defined by the Dockerfile in the linked repo, would be\r\n```\r\nimport tensorflow as tf\r\ntf.config.set_logical_device_configuration(\r\n  tf.config.list_physical_devices('GPU')[0],\r\n  [tf.config.LogicalDeviceConfiguration(memory_limit=8192)]\r\n)\r\nfrom tensorflow.experimental.dlpack import from_dlpack\r\n\r\nimport numpy as np\r\nimport numba\r\nimport cudf\r\n\r\n\r\ndef get_free_mem():\r\n  return numba.cuda.current_context().get_memory_info().free\r\n\r\n\r\ndef make_data(to_tf=False):\r\n  df = cudf.DataFrame({'a': np.random.randn(1000000), 'b': np.random.randn(1000000)})\r\n  if to_tf:\r\n    x = {col: from_dlpack(df[col].to_dlpack()) for col in df.columns}\r\n\r\n# initialize tf gpu\r\nx = tf.random.normal((1,))\r\n\r\nmem_before = get_free_mem()\r\nmake_data()\r\nprint('CuDF memory delta: {} B'.format(mem_before - get_free_mem()))\r\n\r\nmem_before = get_free_mem()\r\nmake_data(to_tf=True)\r\nprint('CuDF to TensorFlow memory delta: {} B'.format(mem_before - get_free_mem()))\r\n```\r\nThe output of which will look something like\r\n```\r\nCuDF memory delta: 0 B\r\nCuDF to TensorFlow memory delta: 16777216 B\r\n```", "comments": ["@alecgunny \r\n\r\nI am seeing the error message (`ModuleNotFoundError: No module named 'cudf`).Request you to help me with the reproducible code in our environment.It helps us in localizing the issue faster.Thanks!", "Hey @ravikyram , I use cudf because I need a GPU library external to TensorFlow from which to generate data and pass into TF via dlpack, though in theory I can use any GPU library which has dlpack compatibility (e.g. PyTorch). An environment with the appropriate packages can be found in the Dockerfile in the linked repo above.\r\n\r\nThat said, when you say \"our environment,\" can you elaborate what you mean by that? I can try to get the appropriate packages installed there as well. Would the 2.2.0-gpu dockerhub container work as an appropriate stand-in?", "@alecgunny \r\n\r\nPlease, help me in reproducing the issue .Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/5a4b18dd41c2635781fe5b0adb4fae2e/untitled945.ipynb).Thanks!", "@ravikyram [Here](https://colab.research.google.com/drive/1HmXq9JW9-zea2diQLGolr63m7T2LwfQV#scrollTo=dPizKdTSCzgY)'s a working repro notebook (switched to using PyTorch to simplify the environment, but the idea is the same)", "@alecgunny \r\n\r\nPlease grant me the access for the gist provided. Thanks!", "@ravikyram sorry about that! Permissions should be public now"]}, {"number": 40060, "title": "[ROCm] Switch r2.1 to use ROCm 3.7", "body": "This PR updates Dockerfile.rocm and the ROCm CI scripts to use ROCm 3.7. \r\n\r\nIn addition to that there are also other commits to enable/disable some unit-tests (or subtests within unit tests) on the ROCm platform. See individual commit messages for details.\r\n\r\nThese changes are required for the ROCm CI scripts to successfully run on the ROCm platform with ROCm 3.7.\r\n\r\nThanks\r\n\r\n------------------------------\r\n\r\n/cc @mihaimaruseac @sunway513 ", "comments": ["> Is this intended to be a cherry-pick to 2.1?\r\n\r\nYes, the intent is to cherry-pick these changes into 2.1. ", "It will have to wait until we do another patch release for 2.1 (until we find the need to fix another security issue)", "> It will have to wait until we do another patch release for 2.1 (until we find the need to fix another security issue)\r\n\r\n@mihaimaruseac , that is okay. thank you.", "/cc @nvining-work ", "Should we be making these changes on `master` instead? And then backporting them once fully done?\r\n\r\nThis becomes quite large and risky to incorporate in a patch release.", "@mihaimaruseac we can take that approach too.\r\n\r\nSome of the changes are already on master...I will be filing a PR for the rest soon (before end of week). The goal is to ensure that the `r2.1` and `master` branches do not regress when we switch to ROCm 3.5 (or higher). I do not know how long it will be to the next patch release, and hence am using this PR to collect  all the changes we need to make it into r2.1.  \r\n\r\nIf you think that the next patch release is imminent, then let me know and I can trim the ROCm 3.5 specific commits from this PR and file a new PR for them once the patch release goes out. \r\nElse I would like continue to use this PR for the ROCm 3.5 specific commits as well.\r\n\r\nAlso note that most if not all of the changes are ROCm specific, so it should not have any impact on non-ROCm TF builds.\r\n\r\nthanks", "There is no plan yet for a new patch release. We can keep using this PR then, but we need to ensure the changes are mirrored in master too."]}, {"number": 40059, "title": "TFLiteConverter: Exception: Merge of two inputs that differ on more than one predicate", "body": "**System information**\r\nMac OSX\r\nTF 2.1.0rc1\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\n    model = tf.saved_model.load(curr_dir + \"saved_model\")\r\n    concrete_func = model.signatures[\r\n    tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\r\n    concrete_func.inputs[0].set_shape([1, 256, 256, 3])\r\n\r\n    converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n\r\n    tflite_model = converter.convert()\r\n\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n2020-06-01 14:47:13.833020: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-06-01 14:47:13.844343: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f9c3ef017f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-06-01 14:47:13.844362: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-06-01 14:47:17.869419: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2020-06-01 14:47:17.869495: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-06-01 14:47:18.085471: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: graph_to_optimize\r\n2020-06-01 14:47:18.085496: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0.002ms.\r\n2020-06-01 14:47:18.085502: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-06-01 14:47:24.250143: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2020-06-01 14:47:24.250232: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-06-01 14:47:25.306823: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: graph_to_optimize\r\n2020-06-01 14:47:25.306844: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 6392 nodes (-2001), 10177 edges (-2235), time = 673.165ms.\r\n2020-06-01 14:47:25.306853: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 6392 nodes (0), 10177 edges (0), time = 172.856ms.\r\nTraceback (most recent call last):\r\n  File \"/Users/me/object_detection/ssdlite_mobilenetv2/ssdlite_mobilenet_v2_coco_2018_05_09/convert.py\", line 33, in <module>\r\n    main()\r\n  File \"/Users/me/object_detection/ssdlite_mobilenetv2/ssdlite_mobilenet_v2_coco_2018_05_09/convert.py\", line 26, in main\r\n    tflite_model = converter.convert()\r\n  File \"/Users/me/miniconda3/envs/tf_lite/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 518, in convert\r\n    **converter_kwargs)\r\n  File \"/Users/me/miniconda3/envs/tf_lite/lib/python3.7/site-packages/tensorflow/lite/python/convert.py\", line 496, in toco_convert_impl\r\n    enable_mlir_converter=enable_mlir_converter)\r\n  File \"/Users/me/miniconda3/envs/tf_lite/lib/python3.7/site-packages/tensorflow/lite/python/convert.py\", line 227, in toco_convert_protos\r\n    raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: See console for info.\r\n2020-06-01 14:47:29.716790: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:144] Ignored output_format.\r\n2020-06-01 14:47:29.716811: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:147] Ignored drop_control_dependency.\r\nTraceback (most recent call last):\r\n  File \"/Users/me/miniconda3/envs/tf_lite/bin/toco_from_protos\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/Users/me/miniconda3/envs/tf_lite/lib/python3.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 93, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/Users/me/miniconda3/envs/tf_lite/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/Users/me/miniconda3/envs/tf_lite/lib/python3.7/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/Users/me/miniconda3/envs/tf_lite/lib/python3.7/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/Users/me/miniconda3/envs/tf_lite/lib/python3.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 56, in execute\r\n    enable_mlir_converter)\r\n**Exception: Merge of two inputs that differ on more than one predicate** {s(Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater:0,else), s(Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/cond/cond/pred_id:0,then)} and {s(Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater:0,else), s(Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/cond/cond/pred_id:0,else), s(Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/cond/Greater:0,else)}\r\n        for node {{node Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/cond/cond/Merge}}\r\n\r\n```\r\n\r\nModel obtained from Model Zoo\r\nhttp://download.tensorflow.org/models/object_detection/ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz\r\n\r\n\r\n**Any other info / logs**\r\n\r\nI'm trying to convert the `ssdlite_mobilenet_v2_coco` pre-trained model from the TF Model Zoo to TFLite. I understand that those models were made on TF1.2, so I was using the saved_model to generate a concrete function (because TFLite doesn't like dynamic input shapes), and from there convert to TFLite. \r\n\r\nRunning into conversion errors on a Merge (please see output log above)\r\n\r\nI came across this page https://www.tensorflow.org/lite/guide/ops_select outlining that control ops (such as merge) are unsupported, to specify to use TF ops whenever required. But still no luck.\r\n\r\nIs there anything strikingly wrong with my process?", "comments": ["@aselva-eb,\r\nI was able to reproduce the error with [TF v2.2](https://colab.research.google.com/gist/amahendrakar/ba23d7511c74ab701344158d9c6b80da/40059.ipynb). However, the issue seems to be fixed in the latest [TF-nightly](https://colab.research.google.com/gist/amahendrakar/6982b57d12597e9bbd82af2fa5d8bf12/40059-tf-nightly.ipynb). \r\nI was able to run the code without any issues. Please find the attached gist. Thanks!", "@amahendrakar \r\nThanks for reproducing and pointing me to a fixed version. Appreciate it! tf-nightly did also solve the issue for me.\r\n\r\nHowever, I noticed that the relative file size of the quantized model is still quite large and comparable to the original pb. Inspecting the weights on operations supported by tflite (according to https://www.tensorflow.org/lite/guide/ops_compatibility), they all seem to be float32 type.\r\n\r\nSo I modified my conversion code above to avoid use of the experimental flags:\r\n`converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]` \r\n\r\nreplacing it with a stable-version documented configuration:  \r\n`converter.optimizations = [tf.lite.Optimize.DEFAULT]`\r\n\r\nIn this case the conversion fails again at:\r\n\r\n```\r\nerror: 'tf.TensorArrayWriteV3' op is neither a custom op nor a flex op\r\nerror: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):\r\n        tf.NonMaxSuppressionV2 {T = f32, T_threshold = f32, device = \"\"}\r\n        tf.Size {device = \"\"}\r\n        tf.TensorArrayGatherV3 {device = \"\", element_shape = #tf.shape<100>}\r\n        tf.TensorArrayGatherV3 {device = \"\", element_shape = #tf.shape<100x4>}\r\n        tf.TensorArrayGatherV3 {device = \"\", element_shape = #tf.shape<300x300x3>}\r\n        tf.TensorArrayGatherV3 {device = \"\", element_shape = #tf.shape<3>}\r\n        tf.TensorArrayGatherV3 {device = \"\", element_shape = #tf.shape<>}\r\n        tf.TensorArrayReadV3 {device = \"\"}\r\n        tf.TensorArrayScatterV3 {device = \"\"}\r\n        tf.TensorArraySizeV3 {device = \"\"}\r\n        tf.TensorArrayV3 {clear_after_read = true, device = \"\", dtype = f32, dynamic_size = false, element_shape = #tf.shape<*>, identical_element_shapes = false, tensor_array_name = \"\"}\r\n        tf.TensorArrayV3 {clear_after_read = true, device = \"\", dtype = i32, dynamic_size = false, element_shape = #tf.shape<*>, identical_element_shapes = false, tensor_array_name = \"\"}\r\n        tf.TensorArrayWriteV3 {device = \"\"}\r\n```\r\n\r\n\r\n", "closing this particular issue as the first issue has been resolved. Thanks @amahendrakar. \r\n\r\nI created a new one for the followup inquiry:\r\nhttps://github.com/tensorflow/tensorflow/issues/40129"]}, {"number": 40058, "title": "TF can't access GPU", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 10\r\n- GPU model and memory: RTX 2060\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n![image](https://user-images.githubusercontent.com/31595943/83438657-69acc480-a45f-11ea-80a1-5cb3efb584d3.png)\r\n\r\n**Describe the expected behavior**\r\nTensorflow should be able to use GPU, but in the current case it is not able to while pytorch is able in the same scenario \r\n\r\n\r\n\r\n", "comments": ["TF 2.1 pre built binary supportas cuda 10.1\r\nPlease switch to cuda 10.1 version", "> TF 2.1 pre built binary supportas cuda 10.1\r\n> Please switch to cuda 10.1 version\r\n\r\nI am using cuda 10.1, sorry for not being specific earlier", "Did you add cuda, cudnn, cupti paths to your environment?\r\nSee https://www.tensorflow.org/install/gpu#windows_setup", "I have updated to TF2.2 and the now I am able to see GPU listed\r\n![image](https://user-images.githubusercontent.com/31595943/85055787-c3c9bb80-b1bb-11ea-8a53-1386949ee281.png)\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40058\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40058\">No</a>\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40058\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40058\">No</a>\n", "Can anyone explain this irregular behavior with TensorFlow 2.2.0 (installed via pip) with GPU  with cuda 10.1 \r\n![image](https://user-images.githubusercontent.com/31595943/85893857-48968400-b811-11ea-9134-ca260fb29191.png)\r\n![image](https://user-images.githubusercontent.com/31595943/85894030-93180080-b811-11ea-8ef3-ef1542eb088c.png)\r\n\r\nWhy is the TensorFlow acknowledging GPU after I test for GPU in the torch\r\nI still can't run my model in GPU, please help !!!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40058\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40058\">No</a>\n"]}, {"number": 40057, "title": "Fix doc rendering error in tf.ragged.constant", "body": "This PR tries to address the issue raised in #40052 where\r\nthe doc rendering of `ragged_rank` in `tf.ragged.constant`\r\ncaused invalid alignment.\r\n\r\nThe issue was because `max(0, K - 1 - len(inner_shape))` crossed\r\ntwo lines and messed up the doc rendering.\r\n\r\nThis PR fixes #40052.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 40056, "title": "org.tensorflow:tensorflow-lite-gpu nightly build error vs 2.2.0", "body": "Im not even sure how reportable this bug is, since it is occurring on a nightly build (6/1/2020), Code was working on friday. \r\n\r\n\r\nHowever when I change from nightly build to 2.2.0, works just fine:\r\n`fliteOptions.addDelegate(GpuDelegate()) \r\n`\r\nbut on nightly:\r\nE/TestRunner: java.lang.UnsatisfiedLinkError: dlopen failed: cannot locate symbol \"_ZN6tflite3gpu2cl9Arguments11kArgsPrefixE\" referenced by \"/data/app/~~KImLgowMRYSuuZbqgdRMgw==/com.qualcomm.style-LOBlofHPnlpjAMuiEJPKFQ==/lib/arm64/libtensorflowlite_gpu_jni.so\"...\r\n        at java.lang.Runtime.loadLibrary0(Runtime.java:1087)\r\n\r\ntrips it when it is loading the library \r\n\r\nThe models used are the same models found in the style transfer android sample project, and I will link them in this post. \r\n\r\nAlso, if there is a special procedure or contact for nightly build bugs, let me know. \r\n\r\nThe models are quantized, which means it should fallback to the cpu, I think this is just a general linking build issue, rather than a gpu delegate problem. I will include the models for reproduce ability \r\n\r\nmodels:\r\n[quantized_models.zip](https://github.com/tensorflow/tensorflow/files/4712477/quantized_models.zip)\r\n\r\n\r\n", "comments": ["I have meet the same problem.\r\n`\r\n java.lang.UnsatisfiedLinkError: dlopen failed: cannot locate symbol \"_ZN6tflite3gpu2cl9Arguments11kArgsPrefixE\" referenced by \"/data/app/com.xxx-8FBD0MkjWXbICJ6ppEx-Mw==/lib/arm/libtensorflowlite_gpu_jni.so\"...\r\n\r\n2020-06-03 21:27:31.084 15529-16058/com.xxx W/System.err:     at java.lang.Runtime.loadLibrary0(Runtime.java:1016)\r\n\r\n2020-06-03 21:27:31.084 15529-16058/com.xxx W/System.err:     at java.lang.System.loadLibrary(System.java:1657)\r\n\r\n2020-06-03 21:27:31.084 15529-16058/com.xxx W/System.err:     at org.tensorflow.lite.gpu.GpuDelegate.<clinit>(GpuDelegate.java:126)\r\n`", "I have encountered the same problem with tflite gpu nightly \r\njava.lang.UnsatisfiedLinkError: dlopen failed: cannot locate symbol \"_ZN6tflite3gpu2cl9Arguments11kArgsPrefixE\" referenced by \"/data/app/com.at.nikhil.speciesai-yi_FcI_5AKGpV5FjG1HVTg==/lib/arm64/libtensorflowlite_gpu_jni.so\"...\r\n\r\nCode was working totally fine just few days back. However, when built with tflite 2.2.0, it worked like charm. \r\nThe model is a standard keras CNN model.", "This is fixed now. You need to clear grade cache to download recent nightly aar file.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40056\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40056\">No</a>\n", "~~Invalidated cache (\"File\" -> \"Invalidate Caches/Restart\") still seeing this issue when using GPU delegate.~~\r\n\r\nEDIT: \r\n\r\nActually used `rm -rf $HOME/.gradle/caches/` and now it works fine."]}, {"number": 40054, "title": "in resolution of [Wsign-compare] warning id 7", "body": "I reexpress the array limit of `handled_signals` and the  index `i` in terms of size_t to resolve build warning: \r\n\r\n`tensorflow/core/platform/default/stacktrace_handler.cc:\r\ntensorflow/core/platform/default/stacktrace_handler.cc: In function 'void tensorflow::testing::InstallStacktraceHandler()':\r\ntensorflow/core/platform/default/stacktrace_handler.cc:103:21: warning: comparison of integer expressions of different signedness: 'int' and 'long unsigned int' [-Wsign-compare]\r\n  103 |   for (int i = 0; i < sizeof(handled_signals) / sizeof(int); i++) {\r\n      |                   ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~`\r\n\r\n@mihaimaruseac ", "comments": []}, {"number": 40053, "title": "AttributeError: module 'tensorflow.python.framework.ops' has no attribute '_set_call_cpp_shape_fn'", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Installed using anaconda and though conda install -c anaconda keras\r\n- TensorFlow version: 1.14.0\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?: condo AND tried using pip with a fresh install\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): Not sure\r\n- CUDA/cuDNN version: Not using gpu\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nI am trying to run a .py program from the anaconda prompt. The program is using tensorflow and keras. I have set up a new condo environment and tried to install tensorflow and keras using both pip and conda. Once in this environment I try to run the code and it produces the error message:\r\nAttributeError: module 'tensorflow.python.framework.ops' has no attribute '_set_call_cpp_shape_fn'\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI activate my new environment, change to the directory of my code, the call python vad.py.\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n(PythonCPU) C:\\Users\\User\\Documents\\SpeechRecognition\\task4>python vad.py\r\nC:\\Users\\User\\anaconda3\\envs\\PythonCPU\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\nC:\\Users\\User\\anaconda3\\envs\\PythonCPU\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\nC:\\Users\\User\\anaconda3\\envs\\PythonCPU\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\nC:\\Users\\User\\anaconda3\\envs\\PythonCPU\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\nC:\\Users\\User\\anaconda3\\envs\\PythonCPU\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\nC:\\Users\\User\\anaconda3\\envs\\PythonCPU\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\nUsing TensorFlow backend.\r\nTraceback (most recent call last):\r\n  File \"vad.py\", line 3, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\User\\anaconda3\\envs\\PythonCPU\\lib\\site-packages\\tensorflow\\__init__.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\User\\anaconda3\\envs\\PythonCPU\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 63, in <module>\r\n    from tensorflow.python.framework.framework_lib import *  # pylint: disable=redefined-builtin\r\n  File \"C:\\Users\\User\\anaconda3\\envs\\PythonCPU\\lib\\site-packages\\tensorflow\\python\\framework\\framework_lib.py\", line 25, in <module>\r\n    from tensorflow.python.framework.ops import Graph\r\n  File \"C:\\Users\\User\\anaconda3\\envs\\PythonCPU\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 54, in <module>\r\n    from tensorflow.python.platform import app\r\n  File \"C:\\Users\\User\\anaconda3\\envs\\PythonCPU\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 23, in <module>\r\n    from absl.app import run as _run\r\n  File \"C:\\Users\\User\\anaconda3\\envs\\PythonCPU\\lib\\site-packages\\absl\\app.py\", line 35, in <module>\r\n    import pdb\r\n  File \"C:\\Users\\User\\anaconda3\\envs\\PythonCPU\\lib\\pdb.py\", line 76, in <module>\r\n    import code\r\n  File \"C:\\Users\\User\\Documents\\SpeechRecognition\\task4\\code.py\", line 4, in <module>\r\n    import keras\r\n  File \"C:\\Users\\User\\anaconda3\\envs\\PythonCPU\\lib\\site-packages\\keras\\__init__.py\", line 3, in <module>\r\n    from . import utils\r\n  File \"C:\\Users\\User\\anaconda3\\envs\\PythonCPU\\lib\\site-packages\\keras\\utils\\__init__.py\", line 6, in <module>\r\n    from . import conv_utils\r\n  File \"C:\\Users\\User\\anaconda3\\envs\\PythonCPU\\lib\\site-packages\\keras\\utils\\conv_utils.py\", line 9, in <module>\r\n    from .. import backend as K\r\n  File \"C:\\Users\\User\\anaconda3\\envs\\PythonCPU\\lib\\site-packages\\keras\\backend\\__init__.py\", line 1, in <module>\r\n    from .load_backend import epsilon\r\n  File \"C:\\Users\\User\\anaconda3\\envs\\PythonCPU\\lib\\site-packages\\keras\\backend\\load_backend.py\", line 90, in <module>\r\n    from .tensorflow_backend import *\r\n  File \"C:\\Users\\User\\anaconda3\\envs\\PythonCPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 9, in <module>\r\n    from tensorflow.python.ops import image_ops as tf_image_ops\r\n  File \"C:\\Users\\User\\anaconda3\\envs\\PythonCPU\\lib\\site-packages\\tensorflow\\python\\ops\\image_ops.py\", line 28, in <module>\r\n    from tensorflow.python.ops.gen_image_ops import *\r\n  File \"C:\\Users\\User\\anaconda3\\envs\\PythonCPU\\lib\\site-packages\\tensorflow\\python\\ops\\gen_image_ops.py\", line 19, in <module>\r\n    from tensorflow.python.framework import common_shapes as _common_shapes\r\n  File \"C:\\Users\\User\\anaconda3\\envs\\PythonCPU\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\", line 729, in <module>\r\n    ops._set_call_cpp_shape_fn(call_cpp_shape_fn)\r\nAttributeError: module 'tensorflow.python.framework.ops' has no attribute '_set_call_cpp_shape_fn'", "comments": ["python version is 3.6 sorry forgot to add", "@ollieyeahlong \r\nPlease refer to these [links](https://stackoverflow.com/questions/40046619/keras-tensorflow-gives-the-error-no-attribute-control-flow-ops) , [link1](https://github.com/tensorflow/models/issues/6177#issuecomment-639569604) let us know if it helps.\r\nAlso is there any particular reason for using an old version of tensorflow, would you like to upgrade to later versions and let us know if the issue exist.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40053\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40053\">No</a>\n"]}]