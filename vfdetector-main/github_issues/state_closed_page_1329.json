[{"number": 13229, "title": "where is tensorflow slim pretrained models??", "body": "It seems that the url of the pretrained model below has been deleted.\r\n[https://github.com/tensorflow/models/tree/master/slim#Pretrained](https://github.com/tensorflow/models/tree/master/slim#Pretrained)\r\n\r\nSo, where are the tensorflow slim pretrained models now?", "comments": ["I've found it here:\r\n[https://github.com/tensorflow/models/tree/master/research/slim](https://github.com/tensorflow/models/tree/master/research/slim)"]}, {"number": 13228, "title": "add image gradient op", "body": "A [paper][1] I am reimplementing recently uses [image gradient loss][2]. Numpy offers [np.gradient][3] to achieve this task i.e. `np.gradient(image, axis=0), np.gradient(image, axis=1)` however tensorflow lacks this feature or at least documentation about how to use `tf.gradients` to get this done.\r\n\r\nTherefore I propose to either send a PR where I add a gradient image op which uses fixed 2d convolution, i.e.:\r\n\r\n```python\r\nimport numpy as np\r\nimport scipy as sp\r\nimport tensorflow as tf\r\n\r\nfrom skimage import io\r\nfrom skimage import color\r\n\r\nfrom matplotlib import pyplot as plt\r\n\r\nimage = color.rgb2gray(io.imread('image.jpg'))\r\nimage_rs = image.reshape([1] + list(image.shape) + [1])\r\n\r\nxgrad = np.gradient(image, axis=0)\r\nygrad = np.gradient(image, axis=1)\r\n\r\nimage_ph = tf.placeholder(tf.float32, image_rs.shape)\r\n\r\nx_weight = tf.reshape(tf.constant([-1, 0, +1], tf.float32), [3, 1, 1, 1])\r\ny_weight = tf.reshape(x_weight, [1, 3, 1, 1])\r\n\r\nxgrad_ts = tf.nn.conv2d(image_ph, x_weight, [1, 1, 1, 1], 'SAME')\r\nygrad_ts = tf.nn.conv2d(image_ph, y_weight, [1, 1, 1, 1], 'SAME')\r\n\r\nwith tf.Session() as sess:\r\n    xgrad2, ygrad2 = sess.run([xgrad_ts, ygrad_ts], feed_dict={image_ph: image_rs})\r\n\r\n    print(xgrad2.shape)\r\n    fig, axes = plt.subplots(2, 3)\r\n    axes[0, 0].imshow(image)\r\n    axes[0, 1].imshow(xgrad)\r\n    axes[0, 2].imshow(ygrad)\r\n    axes[1, 0].imshow(image)\r\n    axes[1, 1].imshow(xgrad2[0,:,:,0])\r\n    axes[1, 2].imshow(ygrad2[0,:,:,0])\r\n    plt.show()\r\n\r\n```\r\n\r\nor have someone update documentation of `tf.gradients`.\r\n\r\nFor the first I could provide a PR if this is considered interesting for tensorflow and not \"too implementation specific\".\r\n\r\n[1]: https://arxiv.org/abs/1612.05362\r\n[2]: https://en.wikipedia.org/wiki/Image_gradient\r\n[3]: https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.gradient.html\r\n", "comments": ["You are mixing up spatial (image gradients) which are respect to a change in position and gradients with respect to parameters of the ML model. Using a fixed 2d convolution as you show here is the correct solution. tf.gradients is used only for computing gradients with respect to variables in your model. In this case, creating a convolution that uses a constant to implement a gradient is what you want to do. Creating a helper function to do this may or may not be interesting. @shlens, do you have any thoughts?", "Agreed with @aselle. Note the spatial gradient is computed in [total_variation](https://www.tensorflow.org/api_docs/python/tf/image/total_variation) and you might be able to employ this function as a prototype for your particular needs.", "Yes, [total_variation][1] was what I was looking for. Its [implementation][2] should also be more efficient then the one I came up. \r\n\r\n[1]: https://www.tensorflow.org/api_docs/python/tf/image/total_variation\r\n[2]: https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/python/ops/image_ops_impl.py#L1455-L1466", "Glad it resolved your issue!", "@aselle , I think the total_variation(according to the given code) is for difference but not for gradient. The two are different."]}, {"number": 13227, "title": "commit 5c7f9e3 breaks windows bazel build", "body": "commit 5c7f9e316d8c7735308a217310350d416d7498cc in #13224 breaks windows bazel build\r\n\r\nFirst, _rpath_linkopts() in tensorflow/tensorflow.bzl should not add rpath args to link.exe\r\nSecond,  the link command generated from bazel is incorrect, \r\n\r\nlink.exe /nologo /OUT:bazel-out/host/bin/tensorflow/python/gen_set_ops_py_wrappers_cc.exe -Lbazel-out/host/bin/_solib_x64_windows/_U_S_Stensorflow_Spython_Cgen_Uset_Uops_Upy_Uwrappers_Ucc___Utensorflow **tensorflow_framework** /SUBSYSTEM:CONSOLE -pthread /MACHINE:X64 @bazel-out/host/bin/tensorflow/python/gen_set_ops_py_wrappers_cc.exe-2.params /DEFAULTLIB:msvcrt.lib\r\n\r\nThe addDynamicInputLinkOptions function in src/main/java/com/google/devtools/build/lib/rules/cpp/CppLinkActionBuilder.java doesn't support Windows.", "comments": ["Will investigate, and possibly file a Bazel bug. In the meantime you should be able to use --config=monolithic to work around the issue.", "Actually it sounds like @meteorcloudy is aware of the issue and working on Windows dynamic linking. Sorry you ran into this!", "I believe this was fixed. I think the build still fails, but due to a different issue now.\r\nClosing."]}, {"number": 13226, "title": "protobuf download Checksum was changed", "body": "The following error caused by protobuf.\r\nChecksum has changed.\r\n\r\n*****\r\nERROR: ~/tensorflow/tensorflow/tools/pip_package/BUILD:100 :1: no such package '@protobuf//': java.io.IOException: Error downloading \r\n...\r\nChecksum was e5fdeee6b28cf6c38d61243adff06628baa434a22b5ebb7432d2a7fbabbdb13d but wanted 6d43b9d223ce09e5d4ce8b0060cb8a7513577a35a64c7e3dad10f0703bf3ad93 and referenced by '//tensorflow/tools/pip_package:licenses'\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted\r\n*****", "comments": ["Can one of the admins verify this patch?", "@jayhpark530, thanks for your PR! By analyzing the history of the files in this pull request, we identified @jart, @tensorflower-gardener and @kirilg to be potential reviewers.", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "Pre-empted by: https://github.com/tensorflow/tensorflow/pull/13219"]}, {"number": 13225, "title": "Is there any tool to see the tensorflow net's topology?", "body": "As you known, we could use \"http://ethereon.github.io/netscope/quickstart.html\" to catch sight of the caffe network's  topology structure. Is there any one for tensorflow?", "comments": ["[TensorBoard](https://github.com/tensorflow/tensorboard) includes a graph visualizer, documented [here](https://www.tensorflow.org/get_started/graph_viz), with more details including a paper about it [here](http://idl.cs.washington.edu/papers/tfgraph/)."]}, {"number": 13224, "title": "Branch 169629054", "body": "", "comments": ["@caisq, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @tensorflower-gardener and @ysuematsu to be potential reviewers.", "I believe this re-breaks https://github.com/tensorflow/tensorflow/issues/1924.\r\n\r\nAll of the jpeg symbols are now exported in `libtensorflow_framework.so`.", "@msarett can you find the commit that caused the issue?", "Sorry I meant to indicate this one:\r\n5c7f9e316d8c7735308a217310350d416d7498cc", "Thanks! Continuing discussion on the original bug."]}, {"number": 13223, "title": "Fixes for Raspberry Pi cross-compilation issues", "body": "The nightly builds have been failing recently, and it's due to a BoringSSL change that brings in system headers on our cross-compilation Docker image. I've worked around this my moving the system headers for OpenSSL, and made a couple of other changes to improve reliability by ensuring the workspace is clean before starting.", "comments": ["@petewarden, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener to be a potential reviewer."]}, {"number": 13222, "title": "SVD in TensorFlow is slower than in numpy", "body": "I am observing that on my machine SVD in tensorflow is running significantly slower than in numpy. I have GTX 1080 GPU, and expecting SVD to be at least as fast as when running the code using CPU (numpy).\r\n\r\n**Environment Info**\r\n\r\nOperating System\r\n\r\n    lsb_release -a\r\n    No LSB modules are available.\r\n    Distributor ID:\tUbuntu\r\n    Description:\tUbuntu 16.10\r\n    Release:\t16.10\r\n    Codename:\tyakkety\r\n\r\nInstalled version of CUDA and cuDNN:\r\n\r\n    ls -l /usr/local/cuda-8.0/lib64/libcud*\r\n    -rw-r--r-- 1 root      root    556000 Feb 22  2017 /usr/local/cuda-8.0/lib64/libcudadevrt.a\r\n    lrwxrwxrwx 1 root      root        16 Feb 22  2017 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0\r\n    lrwxrwxrwx 1 root      root        19 Feb 22  2017 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.61\r\n    -rwxr-xr-x 1 root      root    415432 Feb 22  2017 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.61\r\n    -rw-r--r-- 1 root      root    775162 Feb 22  2017 /usr/local/cuda-8.0/lib64/libcudart_static.a\r\n    lrwxrwxrwx 1 voldemaro users       13 Nov  6  2016 /usr/local/cuda-8.0/lib64/libcudnn.so -> libcudnn.so.5\r\n    lrwxrwxrwx 1 voldemaro users       18 Nov  6  2016 /usr/local/cuda-8.0/lib64/libcudnn.so.5 -> libcudnn.so.5.1.10\r\n    -rwxr-xr-x 1 voldemaro users 84163560 Nov  6  2016 /usr/local/cuda-8.0/lib64/libcudnn.so.5.1.10\r\n    -rw-r--r-- 1 voldemaro users 70364814 Nov  6  2016 /usr/local/cuda-8.0/lib64/libcudnn_static.a\r\n\r\nTensorFlow Setup\r\n\r\n    python -c \"import tensorflow; print(tensorflow.__version__)\"\r\n    I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\n    I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\n    I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\n    I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\n    I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\n    1.0.0\r\n\r\n**Code:**\r\n\r\n    '''\r\n    Created on Sep 21, 2017\r\n    \r\n    @author: voldemaro\r\n    '''\r\n    import numpy as np\r\n    import tensorflow as tf\r\n    import time;\r\n    import numpy.linalg as NLA;\r\n    \r\n    \r\n    \r\n    \r\n    N=1534;\r\n    \r\n    svd_array = np.random.random_sample((N,N));\r\n    svd_array = svd_array.astype(complex);\r\n    \r\n    specVar = tf.Variable(svd_array, dtype=tf.complex64);\r\n    \r\n    [D2, E1,  E2] = tf.svd(specVar);\r\n    \r\n    init_OP = tf.global_variables_initializer();\r\n    \r\n    with tf.Session() as sess:\r\n        # Initialize all tensorflow variables\r\n        start = time.time();\r\n        sess.run(init_OP);\r\n        print 'initializing variables: {} s'.format(time.time()-start);\r\n        \r\n        start_time = time.time();\r\n        [d, e1, e2]  = sess.run([D2, E1,  E2]);\r\n        print(\"Tensorflow SVD ---: {} s\" . format(time.time() - start_time));\r\n    \r\n    \r\n    # Equivalent numpy \r\n    start = time.time();\r\n    \r\n    u, s, v = NLA.svd(svd_array);   \r\n    print 'numpy SVD  ---: {} s'.format(time.time() - start);\r\n\r\n\r\nCode Trace:\r\n\r\n    W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n    W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n    W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n    W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n    I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n    I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \r\n    name: GeForce GTX 1080\r\n    major: 6 minor: 1 memoryClockRate (GHz) 1.7335\r\n    pciBusID 0000:01:00.0\r\n    Total memory: 7.92GiB\r\n    Free memory: 7.11GiB\r\n    I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \r\n    I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \r\n    I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)\r\n    initializing variables: 0.230546951294 s\r\n    Tensorflow SVD ---: 6.56117296219 s\r\n    numpy SVD  ---: 4.41714000702 s\r\n\r\n", "comments": ["Note that you are comparing GPU implementation with CPU implementation. Not sure what expected speed of GPU SVD is, but some algorithms run faster on CPU.\r\n\r\nYou can compare CPU vs CPU implementation by setting `CUDA_VISIBLE_DEVICES=` before running your test. I believe numpy is still faster here because CPU implementation of SVD in TF is single core.", "When I profile the code, I see that numpy is spreading the load across all 8 CPU cores ( Intel i7 ), so I was somewhat expecting to see the benefit of having so many (2560) CUDA cores.\r\nlooks like earlier there were some efforts to take advantage of GPU showing 5x improvement over Intel MKL - https://s3.amazonaws.com/academia.edu.documents/30806706/Sheetal09Singular.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1506052362&Signature=gCpal%2Fk2dCnhAUXgYE4sgjqPNOo%3D&response-content-disposition=inline%3B%20filename%3DSingular_value_decomposition_on_GPU_usin.pdf", "cc @shamanDevel who added SVD-on-GPU op  (ps @vslobody that paper link is dead, what's the paper title?)", "there is another copy on NVIDIA site\r\nhttps://devtalk.nvidia.com/cmd/default/download-comment-attachment/43356/", "Hi, yes, it was me who added the GPU implementation for the SVD.\r\nIt calls cuSOLVER internally (http://docs.nvidia.com/cuda/cusolver/index.html#cuds-lt-t-gt-gesvd). I'm not aware of any direct comparisons between numpy and cuSOLVER.\r\n\r\nI'm always a bit suspicious about these timing comparisons of individual operations. GPU operations are inherently asynchronous. So all comparisons with a CPU implementation involve some overhead.\r\nIn your case, this is a synchronization hidded when running the SVD: Tensorflow waits for the operation to be finished. And the variables are probably also in CPU memory?\r\n\r\nFurther, there is another synchronization hidden in the SVD operation: It has to check if the solver has thrown an error. This also involves a memcpy from GPU to CPU. In larger scenarios, the check is delayed until the data is on the CPU (no overhead), but here, sess.run() has to wait for that as well.\r\n\r\nRunning a single SVD on the GPU is probably not that optimal when compared to a CPU implementation when all the data is in CPU memory.\r\nIt first becomes useful if all your data is only in GPU memory and the memcpy would be the largest bottleneck. That's what I experienced in my usecases.", "Actually I think the difference here is the algorithm, data copies are O(n^2), while SVD is O(n^3), so transfers become negligible for large enough computation. You can get rid of transfers by changing key line to this:\r\n\r\n    [d, e1, e2]  = sess.run([D2.op, E1.op,  E2.op]);\r\n\r\nHere's an updated benchmark that avoids data transfers https://github.com/yaroslavvb/stuff/blob/master/gpu_svd_bench.py\r\n\r\nOn head Tensorflow vs. MKL-enabled numpy on I7/GTX 1080 it gives following:\r\n```\r\nnumpy default        min:   243.52, median:   246.42, mean:   259.49\r\nnumpy gesvd          min:  1294.13, median:  1296.60, mean:  1298.93\r\nnumpy gesdd          min:   242.36, median:   242.80, mean:   245.32\r\nTF CPU               min:   950.77, median:  1080.64, mean:  1050.79\r\nTF GPU               min:  5483.26, median:  5520.19, mean:  5571.15\r\n```\r\n\r\nNumpy/scipy uses divide-and-conquer gesdd algorithm by default, but it seems even gesvd is faster than GPU version.\r\n\r\ncc @mattjj since KFAC needs fast SVDs", "cc @rmlarsen (tldr; for float32 N=1500 matrices, MKL SVD seems 4x faster than anything in TF, CPU or GPU)", "Ok, MKL is a completely different league than all the others. That is protected code by Intel highly optimized for their Intel CPU's. TF on the CPU (hence Eigen) can't compete with that because TF must run on any machine, e.g. also on AMD CPUs.\r\nThat the GPU-version is that much slower is still surprising.\r\n\r\nSide note: It is possible that Eigen also uses MKL in the backend.\r\nhttps://eigen.tuxfamily.org/dox/TopicUsingIntelMKL.html\r\nCan this be enabled for TF as well?", "@rmlarsen, do you know of anybody who has time to look into this in detail?\r\n", "+1 for this problem. Using `with tf.device(\"/cpu:0\")` is much faster.", "On CPU performance: TF's CPU implementation of the SVD (from eigen) does not utilize multi-threading, whereas MKL's implementation does.\r\n\r\nAs @shamanDevel points out: It would be great if, when built with MKL, TF would also turn on MKL support in eigen, since then eigen would use the MKL SVD internally.\r\n\r\nAside: Using MKL does not limit CPU support. MKL will also run (pretty fast) on AMD CPUs.", "Was able to resolve  the issue with TF v2.5 , TF took 4.5 seconds and numpy 6.86 seconds now unlike previous scenario  .please find the[ gist](https://colab.research.google.com/gist/mohantym/d70dd4c4429597303e3ade64001f90a0/13222.ipynb#scrollTo=ElUkZhQKAKZI) here ..Thanks! ", "The main reason that Eigen and hence TF lags MKL in SVD performance is the lack of multi-threading in Eigen core. The only multi-threading there is done through OpenMP parallelization of the matrix multiplication operation. TF does not want to add a dependence on OpenMP, so for now this op will remain single-threaded. There are plans to move the ThreadPool in Eigen/unsupported (the same threadpool that drives TF) to Eigen/Core. This would allow TF to use multi-threading in Eigen/Core ops. @cantonios is working on this, I believe.\r\n\r\nAs far as using the MKL version of SVD in TF, it is not that simple. The \"MKL version\" of TF is not linked with the entire \"legacy\" MKL library, but rather separate libraries like MKL oneDNN etc. developed for deep learning. The part of MKL containing LAPACK routines is not released in open source form, and might never be due to dependence on legacy Fortran code (just my guess, I don't work for Intel).", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/13222\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/13222\">No</a>\n"]}, {"number": 13221, "title": "Memory leak in zeros_like/Tile", "body": "So I'm trying to figure out why my resnets are running out of memory, and it seems that there's a memory leak in Tile and zeros_like operations.\r\n\r\nThose ops have memory allocated during each session run but there's no `__LOG_MEMORY__` deallocation messages corresponding to them. The sum of missing deallocations matches the amount of memory leaked as reported by allocator as `max_bytes_in_use` (accessed through `tf.contrib.memory_stats.MaxBytesInUse` op)\r\n\r\nHere's a simplified repro, at each sess.run, the memory grows by 1.15 GB until it crashes with OOM\r\nhttps://github.com/yaroslavvb/stuff/blob/master/resnet_leak_report2.py\r\n\r\nWhen I run it, I see\r\n```\r\nRun 0, GBs in use 2.30\r\nRun 1, GBs in use 3.60\r\nRun 2, GBs in use 4.75\r\nRun 3, GBs in use 5.90\r\n2017-09-21 14:56:31.994302: W tensorflow/core/common_runtime/bfc_allocator.cc:273] Allocator (GPU_0_bfc) ran out of memory trying to allocate 137.33MiB.  Current allocation summary follows....\r\n```\r\n\r\nOffending ops:\r\n```\r\ngradients/leaky_relu_grad/zeros_like 576MB\r\ngradients/Sum_grad/Tile  576MB\r\n```\r\nVersion:\r\nUbuntu 16:04\r\nofficial TensorFlow Linux GPU Python 3.5 nightly wheel from [today](https://ci.tensorflow.org/view/tf-nightly/job/tf-nightly-linux/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3.5,label=gpu-linux/lastSuccessfulBuild/artifact/pip_test/whl/tf_nightly_gpu-1.head-cp35-cp35m-linux_x86_64.whl)\r\n\r\n```\r\nversion: 1.4.0-dev20170921\r\n__git_version__: v1.3.0-rc1-2408-ge9d5ee1\r\nCommit https://github.com/tensorflow/tensorflow/commit/e9d5ee1\r\n\r\n```", "comments": ["The last merge had some memory leak fixes, but doesn't seem to address this one, same behavior against\r\nhttps://github.com/tensorflow/tensorflow/commit/ea94bbe9fa9f9b3d01fb057c02ef7873d76bf09c\r\n\r\ncc @reedwm in case he knows who is in charge of leaks\r\n\r\nSome additional notes:\r\n1. Leak only happens when adding new tensors to the graph. \r\n2. Closing/reopening the session reclaims memory.", "I would normally triage to @zheng-xq but he's on vacation. So I'll take a look.", "In your example, you create new nodes in every iteration of the loop, which will make the graph larger every iteration. Moving all graph constructions outside the loop causes the memory issue to go away:\r\n\r\n```python\r\n# test whether memory gets cleared on creating new sessions\r\nimport sys, os, math, random\r\n\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nif __name__=='__main__':\r\n  try:\r\n    sess = tf.Session()\r\n    size = 12000\r\n    num_runs = 10\r\n\r\n    images = tf.random_uniform([size, size])\r\n    var = tf.Variable(tf.ones_like(images))\r\n    sess.run(var.initializer)\r\n    def relu(x):\r\n      return tf.where(tf.less(x, 0.0), x, x, name='leaky_relu')\r\n    cost = tf.reduce_sum(relu(images+var))\r\n\r\n    grads = tf.gradients(cost, var)\r\n    memop = tf.contrib.memory_stats.MaxBytesInUse()\r\n\r\n    for i in range(10):\r\n      _, memuse = sess.run([grads, memop])\r\n      print(\"Run %d, GBs in use %.2f\"%(i, memuse/10**9))\r\n  except:\r\n    pass\r\n  finally:\r\n    [memuse] = sess.run([tf.contrib.memory_stats.MaxBytesInUse()])\r\n    print(\"Memory GBs in use %.2f\"%(memuse/10**9,))\r\n```\r\n\r\nModifying the graph every iteration has a performance impact since it prevents some internal caching, but I am not sure why it causes the memory leak. Perhaps some tensors are being cached in GPU memory each time the session is run with a modified graph. @cwhipkey, any ideas?", "Indeed, modifying the graph triggers TF_ExtendGraph, but this overhead is something like 1ms, and insignificant for interactive use (ie, modify the graph, test it, modify some more, etc), meanwhile leaking 1GB of GPU memory is significant", "One guess was that, for a given feed/fetch combination,   images = tf.random_uniform([size, size])   is a constant, and so constant folding could make a constant out of it.\r\n\r\nConstant folding in common_runtime only makes a constant if it's <10MB, while that tensor is 144MB.\r\n\r\nHowever, constant folding in grappler may not apply this limit - at least, I couldn't find it looking over just now (core/grappler/optimizers/constant_folding.cc).", "How do I turn off grappler? I still get leak if I create session as follows\r\n\r\n```\r\nconfig = tf.ConfigProto(graph_options=tf.GraphOptions(optimizer_options=tf.OptimizerOptions(opt_level=tf.OptimizerOptions.L0)))\r\nsess = tf.Session(config=config)\r\n```", "also pass a rewrite_options value to tf.GraphOptions(), and in that one turn off constant folding.", "Updated https://github.com/yaroslavvb/stuff/blob/master/resnet_leak_report2.py with new options, still leaks\r\n\r\n```\r\n    rewrite_options = rewriter_config_pb2.RewriterConfig(\r\n      disable_model_pruning=True,\r\n      constant_folding=rewriter_config_pb2.RewriterConfig.OFF,\r\n      memory_optimization=rewriter_config_pb2.RewriterConfig.MANUAL)\r\n    optimizer_options = tf.OptimizerOptions(opt_level=tf.OptimizerOptions.L0)\r\n    graph_options=tf.GraphOptions(optimizer_options=optimizer_options,\r\n                                  rewrite_options=rewrite_options)\r\n    config = tf.ConfigProto(graph_options=graph_options)\r\n    sess = tf.Session(config=config)\r\n\r\n```", "A const op is gradient in the gradient of tf.where. tf.where calls the \"Select\" op, whose gradient is [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_grad.py#L907). The gradient calls tf.zeros_like, [which calls](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_ops.py#L1492) tf.zeros, [which calls](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_ops.py#L1439) tf.constant.\r\n\r\nEverytime a GPU constant op is created, it allocates memory for the constant on the GPU. Every time a session is run with new feeds and fetches, it creates a new Executor which creates new OpKernels, including the new Const op which allocates memory again. Stateful nodes are cached so new OpKernels for stateful nodes are not recreated every time. See [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/direct_session.cc#L1186). But Const is not stateful, so it is recreated every iteration since the sesssion is run with new fetches.\r\n\r\n@cwhipkey what do you think is the solution here? Why are only stateful kernel cached? If all nodes were cached, this problem wouldn't occur.\r\n", "Nice investigation!\r\n\r\nSo Const allocates memory during kernel construction rather than during `Compute` which makes it special. It might be better if `tf.zeros` called an op similar to `zeros_like` which [allocates](https://github.com/tensorflow/tensorflow/blob/40eef4473bda90442bb55fcc67842f097c024580/tensorflow/core/kernels/constant_op.cc#L294) memory during `Compute`.\r\n\r\nI was also surprised to see large memory allocations on CPU followed by Stream::ThenMemcpy, even though the entire network is inside `with (\"gpu..)` block. I'm guessing this is because const allocates the memory using MakeTensorFromProto which first creates zero array on CPU\r\n\r\n\r\ncc @vrv because of [comments](https://github.com/tensorflow/tensorflow/blob/a41b510f0358d1212d2c548d65c4b1c5e4aa01c0/tensorflow/core/framework/device_base.h#L209) on refactoring of MakeTensorFromProto\r\n\r\n", "tf.zeros has two implementations, one which uses constants and one which uses fill. I do not understand why we prefer the one which uses constant as it leads to huge graphs (and we should be able to constant-fold fill anyway). For eager we already never use the constant one as it leads to huge CPU-to-GPU copies of the constants every time you run zeros which is face-palmingly bad  behavior.\r\n\r\nNow that we have real performance benchmarks does anyone oppose me investigating whether we can make tf.zeros always use fill?", "/CC @zffchen78 you removed caching of stateless ops in 3d91f5585a46a8ac4de16b9f90fc0b1f703812e5. Would it be possible to add caching for const op? The other solution is to avoid constants in graphs.\r\n\r\n@alextp, what performance benchmarks are you referring to? I think we should make it always use fill.", "@reedwm: @tfboyd has benchmarks we should not regress on.\r\n\r\nCaching const op will not help here since caching the constants in the GPU is itself the problem.\r\n\r\n@reedwm , are you willing to work with @tfboyd to make sure always using fill will not be a regression?", "Caching the const op here would fix the issue, since it would only be cached once, instead of once per set of fetches. \r\n\r\nI think @tfboyd's benchmarks are [tf_cnn_benchmarks](https://github.com/tensorflow/benchmarks/tree/master/scripts/tf_cnn_benchmarks), which does not use tf.zeros (if it did use a GPU const ops, we would have the same issue and use way too much memory). If we cannot cache the const op, perhaps we should write a microbenchmark testing tf.zeros itself. I would be happy to help.", "tf.zeros are often created by gradients_impl internally. I find it unlikely\nthat this benchmark doesn't use it.\n\nOn Thu, Oct 5, 2017 at 4:12 PM, Reed <notifications@github.com> wrote:\n\n> Caching the const op here would fix the issue, since it would only be\n> cached once, instead of once per set of fetches.\n>\n> I think @tfboyd <https://github.com/tfboyd>'s benchmarks are\n> tf_cnn_benchmarks\n> <https://github.com/tensorflow/benchmarks/tree/master/scripts/tf_cnn_benchmarks>,\n> which does not use tf.zeros (if it did use a GPU const ops, we would have\n> the same issue and use way too much memory). If we cannot cache the const\n> op, perhaps we should write a microbenchmark testing tf.zeros itself. I\n> would be happy to help.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13221#issuecomment-334616745>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxWLBlaDcUW53SWvIqvE-2GU-kyLFks5spWJcgaJpZM4Pf_kL>\n> .\n>\n\n\n\n-- \n - Alex\n", "You're right, I printed all the Const ops and there were 712 Const ops, some which were created under gradient code. The ops total 431797 bytes on resnet50, so the memory leak issue will likely have a negligible impact, but we can measure its performance when switching to using fill.\r\n", "btw, using tf.ones to make 1GB tensor takes forever (probably huge constant like with tf.zeros). Using `Fill` implementation works better, but crashes above 2GB, I guess fill has some size limit", "I suspect the main leak is not from ones/ones_like but from tf.where.\r\n\r\nI modified the example code, so that it doesn't create new nodes in the computation graph while iterating. The script checks memory usage depending on which implementation of relu is in use. \r\n\r\n```\r\nimport sys, os, math, random\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nif __name__=='__main__':\r\n  def run_iters(relu):\r\n    from tensorflow.core.protobuf import rewriter_config_pb2\r\n    rewrite_options = rewriter_config_pb2.RewriterConfig(\r\n      disable_model_pruning=True,\r\n      constant_folding=rewriter_config_pb2.RewriterConfig.OFF,\r\n      memory_optimization=rewriter_config_pb2.RewriterConfig.MANUAL)\r\n    optimizer_options = tf.OptimizerOptions(opt_level=tf.OptimizerOptions.L0)\r\n    graph_options=tf.GraphOptions(optimizer_options=optimizer_options,\r\n                                  rewrite_options=rewrite_options)\r\n    config = tf.ConfigProto(graph_options=graph_options)\r\n\r\n    sess = tf.Session(config=config)\r\n    \r\n    size = 12000\r\n    num_runs = 20\r\n\r\n    images = tf.random_uniform([size, size])\r\n\r\n    var = tf.Variable(tf.ones_like(images))\r\n    sess.run(var.initializer)\r\n\r\n    cost = tf.reduce_sum(relu(images+var))\r\n    grads = tf.gradients(cost, var)\r\n\r\n    memuse, memuse2 = sess.run([tf.contrib.memory_stats.MaxBytesInUse(), tf.contrib.memory_stats.BytesInUse()])\r\n    print(\"Init: GBs in use %.2f, %.2f\"%(memuse/10**9,memuse2/10**9))\r\n    for i in range(10):\r\n      _, memuse, memuse2 = sess.run([grads, tf.contrib.memory_stats.MaxBytesInUse(), tf.contrib.memory_stats.BytesInUse()])\r\n      print(\"Run %d, GBs in use %.2f, %.2f\"%(i, memuse/10**9,memuse2/10**9))\r\n    [memuse] = sess.run([tf.contrib.memory_stats.MaxBytesInUse()])\r\n    print(\"Memory GBs in use %.2f\"%(memuse/10**9,))\r\n    \r\n    sess.close()\r\n\r\n  alpha = 0.1\r\n\r\n  def relu_nowhere(x):\r\n    retval = alpha*x*tf.cast(tf.less(x, 0.), dtype=tf.float32) + x*tf.cast(tf.less(-x, 0.), dtype=tf.float32)\r\n    return retval\r\n  run_iters(relu_nowhere)\r\n\r\n  tf.reset_default_graph()\r\n\r\n  def relu_where(x):\r\n    retval = tf.where(tf.less(x, 0.0), alpha*x, x, name='leaky_relu')\r\n    return retval\r\n  run_iters(relu_where)\r\n\r\n```\r\n\r\nThis is the output on my machine with tensorflow 1.4:\r\n```\r\n(python3) daniyar@sleepy-prism:~/tmp/tile_leak$ python test.py\r\n2017-11-27 14:27:12.809165: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2017-11-27 14:27:13.038106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:\r\nname: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531\r\npciBusID: 0000:02:00.0\r\ntotalMemory: 11.90GiB freeMemory: 11.75GiB\r\n2017-11-27 14:27:13.038243: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:02:00.0, compute capability: 6.1)\r\nInit: GBs in use 1.15, 0.58\r\nRun 0, GBs in use 1.15, 0.58\r\nRun 1, GBs in use 2.45, 0.58\r\nRun 2, GBs in use 2.45, 0.58\r\nRun 3, GBs in use 2.45, 0.58\r\nRun 4, GBs in use 2.45, 0.58\r\nRun 5, GBs in use 2.45, 0.58\r\nRun 6, GBs in use 2.45, 0.58\r\nRun 7, GBs in use 2.45, 0.58\r\nRun 8, GBs in use 2.45, 0.58\r\nRun 9, GBs in use 2.45, 0.58\r\nMemory GBs in use 2.45\r\n2017-11-27 14:27:15.457164: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:02:00.0, compute capability: 6.1)\r\nInit: GBs in use 2.45, 0.58\r\nRun 0, GBs in use 2.45, 1.15\r\nRun 1, GBs in use 2.45, 1.73\r\nRun 2, GBs in use 3.02, 2.30\r\nRun 3, GBs in use 3.60, 2.88\r\nRun 4, GBs in use 4.18, 3.46\r\nRun 5, GBs in use 4.75, 4.03\r\nRun 6, GBs in use 5.33, 4.61\r\nRun 7, GBs in use 5.90, 5.18\r\nRun 8, GBs in use 6.48, 5.76\r\nRun 9, GBs in use 7.06, 6.34\r\nMemory GBs in use 7.63\r\n```\r\n\r\nAs you can see, the memory usage doubles after the first iteration, but then stays the same, when using leaky relu without tf.where. On the other hand, leaky relu with tf.where keeps allocating more memory.", "@ebrevdo is the temporary memory allocated to cub not being freed at the end of the kernel?", "Actually, there may be no memory leak...\r\n\r\nI've put the memory_stat calls outside the graph, and everything started to work faster and without memory growth:\r\n\r\n```\r\nimport sys, os, math, random\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nif __name__=='__main__':\r\n  def run_iters(relu):\r\n    from tensorflow.core.protobuf import rewriter_config_pb2\r\n    rewrite_options = rewriter_config_pb2.RewriterConfig(\r\n      disable_model_pruning=True,\r\n      constant_folding=rewriter_config_pb2.RewriterConfig.OFF,\r\n      memory_optimization=rewriter_config_pb2.RewriterConfig.MANUAL)\r\n    optimizer_options = tf.OptimizerOptions(opt_level=tf.OptimizerOptions.L0)\r\n    graph_options=tf.GraphOptions(optimizer_options=optimizer_options,\r\n                                  rewrite_options=rewrite_options)\r\n    config = tf.ConfigProto(graph_options=graph_options)\r\n\r\n    sess = tf.Session(config=config)\r\n    \r\n    size = 12000\r\n    num_runs = 20\r\n\r\n    images = tf.random_uniform([size, size])\r\n\r\n    var = tf.Variable(tf.ones_like(images))\r\n    sess.run(var.initializer)\r\n\r\n    cost = tf.reduce_sum(relu(images+var))\r\n    grads = tf.gradients(cost, var)\r\n\r\n    get_mem_max = tf.contrib.memory_stats.MaxBytesInUse()\r\n    get_mem_use = tf.contrib.memory_stats.BytesInUse()\r\n\r\n    memuse, memuse2 = sess.run([get_mem_max, get_mem_use])\r\n    print(\"Init: GBs in use %.2f, %.2f\"%(memuse/10**9,memuse2/10**9))\r\n    for i in range(10):\r\n      _, memuse, memuse2 = sess.run([grads, get_mem_max, get_mem_use])\r\n      print(\"Run %d, GBs in use %.2f, %.2f\"%(i, memuse/10**9,memuse2/10**9))\r\n    memuse = sess.run(get_mem_max)\r\n    print(\"Memory GBs in use %.2f\"%(memuse/10**9,))\r\n\r\n    sess.close()\r\n\r\n  alpha = 0.1\r\n\r\n  def relu_nowhere(x):\r\n    retval = alpha*x*tf.cast(tf.less(x, 0.), dtype=tf.float32) + x*tf.cast(tf.less(-x, 0.), dtype=tf.float32)\r\n    return retval\r\n  run_iters(relu_nowhere)\r\n\r\n  tf.reset_default_graph()\r\n\r\n  def relu_where(x):\r\n    retval = tf.where(tf.less(x, 0.0), alpha*x, x, name='leaky_relu')\r\n    return retval\r\n  run_iters(relu_where)\r\n\r\n```\r\n\r\nResult:\r\n\r\n```\r\n(python3) daniyar@sleepy-prism:~/tmp/tile_leak$ python test.py\r\n2017-11-27 17:21:07.930448: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2017-11-27 17:21:08.190181: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:\r\nname: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531\r\npciBusID: 0000:02:00.0\r\ntotalMemory: 11.90GiB freeMemory: 11.75GiB\r\n2017-11-27 17:21:08.190208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:02:00.0, compute capability: 6.1)\r\nInit: GBs in use 1.15, 0.58\r\nRun 0, GBs in use 1.15, 0.58\r\nRun 1, GBs in use 2.45, 0.58\r\nRun 2, GBs in use 2.45, 0.58\r\nRun 3, GBs in use 2.45, 0.58\r\nRun 4, GBs in use 2.45, 0.58\r\nRun 5, GBs in use 2.45, 0.58\r\nRun 6, GBs in use 2.45, 0.58\r\nRun 7, GBs in use 2.45, 0.58\r\nRun 8, GBs in use 2.45, 0.58\r\nRun 9, GBs in use 2.45, 0.58\r\nMemory GBs in use 2.45\r\n2017-11-27 17:21:10.560484: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:02:00.0, compute capability: 6.1)\r\nInit: GBs in use 2.45, 0.58\r\nRun 0, GBs in use 2.45, 1.15\r\nRun 1, GBs in use 2.45, 1.15\r\nRun 2, GBs in use 2.45, 1.15\r\nRun 3, GBs in use 2.45, 1.15\r\nRun 4, GBs in use 2.45, 1.15\r\nRun 5, GBs in use 2.45, 1.15\r\nRun 6, GBs in use 2.45, 1.15\r\nRun 7, GBs in use 2.45, 1.15\r\nRun 8, GBs in use 2.45, 1.15\r\nRun 9, GBs in use 2.45, 1.15\r\nMemory GBs in use 2.45\r\n```\r\n\r\nIt seems memory_stat has imperfect memory management. :)\r\n", "@dantkz what you are seeing is related. Calling tf.contrib.memory_stats.MaxBytesInUse() in loop body appends a new op to your graph. Ideally this should only leak CPU memory and not GPU memory. However, the way const's are implemented, any modification of the graph will cause TensorFlow to create new copies of all consts (see reedwm comment on Oct 2). The bottom line is that const nodes will leak GPU memory, and by extension \"tf.ones/tf.zeros\" since they use const.", "@yaroslavvb Ah, yes, as you pointed out this behavior happens when the computation graph is being updated. I got carried away trying to find if a memory leak in my code is related to this memory leak, so I just re-did what you guys had above. My code doesn't grow the graph, but it uses `tf.scatter`/`tf.gather`/`tf.unique`/`tf.where` to modify `tf.ones` and `tf.constant` tensors depending on the input elements, I was trying to find a \"common denominator\" example.\r\n\r\nAnyway, there is another interesting behaviour of `tf.ones` and `tf.constant`:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport time\r\n\r\nif __name__=='__main__':\r\n    size_max = 2**8-1\r\n    iter_num = 20\r\n    size = tf.placeholder(shape=[], dtype=tf.int32)\r\n\r\n    with tf.device(\"/gpu:0\"):\r\n        ones_list = []\r\n        ones_list.append(tf.constant(np.ones([size_max, size_max, size_max], dtype=np.float32), dtype=tf.float32, name='tf.constant'))\r\n        ones_list.append(tf.ones([size_max, size_max, size_max], dtype=tf.float32, name='tf.ones'))\r\n        ones_list.append(tf.convert_to_tensor(np.ones([size_max, size_max, size_max], dtype=np.float32), dtype=tf.float32, name='tf.convert_to_tensor'))\r\n        ones_list.append(tf.get_variable('tf.get_variable', dtype=tf.float32, initializer=ones_list[0], trainable=False))\r\n\r\n        versions = []\r\n        for ones in ones_list:\r\n            ones_crop = ones[0:size, 0:size, 0:size]\r\n            noise = tf.random_uniform([size, size, size], dtype=tf.float32)\r\n            cost = tf.reduce_mean(ones_crop + noise)\r\n            grads = tf.gradients(cost, ones)\r\n            versions.append((cost, grads, ones))\r\n\r\n\r\n    get_mem_max = tf.contrib.memory_stats.MaxBytesInUse()\r\n\r\n    with tf.Session(config=tf.ConfigProto(allow_soft_placement=False)) as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        #sess.graph.finalize()\r\n\r\n        # hot start\r\n        for cost, grads, ones in versions:\r\n            cur_size = np.random.randint(size_max//2+1, size_max, dtype=np.int32)\r\n            cost_val, _, memuse = sess.run([cost, grads, get_mem_max], feed_dict={size: cur_size})\r\n\r\n        for cost, grads, ones in versions:\r\n            print(\"########################\")\r\n            print(\"Ones: \", ones)\r\n            bef_memuse = sess.run(get_mem_max)\r\n            bef_time = time.time()\r\n            #print(\"Memory GBs in use %.3f before iters\"%(bef_memuse/10**9))\r\n            for i in range(iter_num):\r\n                get_mem_use = tf.contrib.memory_stats.BytesInUse()\r\n                cur_size = np.random.randint(size_max//2+1, size_max, dtype=np.int32)\r\n                cost_val, _, memuse, memuse2 = sess.run([cost, grads, get_mem_max, get_mem_use], feed_dict={size: cur_size})\r\n                #print(\"Run %d, %f, GBs in use %.3f, %.3f.\"%(i, cost_val, memuse/10**9,memuse2/10**9))\r\n            aft_memuse = sess.run(get_mem_max)\r\n            aft_time = time.time()\r\n            print('Time for %d iters: %.3fs.' % (iter_num, aft_time-bef_time))\r\n            #print(\"Memory GBs in use %.3f after iters\"%(aft_memuse/10**9))\r\n            print(\"Diff GBs: %.3f\"%((aft_memuse-bef_memuse)/10**9))\r\n\r\n```\r\n\r\nThe result:\r\n```\r\n2017-11-27 19:03:49.210727: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2017-11-27 19:03:49.389706: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1062] Found device 0 with properties:\r\nname: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 11.90GiB freeMemory: 11.75GiB\r\n2017-11-27 19:03:49.389731: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1152] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n########################\r\nOnes:  Tensor(\"tf.constant:0\", shape=(255, 255, 255), dtype=float32, device=/device:GPU:0)\r\nTime for 20 iters: 2.445s.\r\nDiff GBs: 1.310\r\n########################\r\nOnes:  Tensor(\"tf.ones:0\", shape=(255, 255, 255), dtype=float32, device=/device:GPU:0)\r\nTime for 20 iters: 0.770s.\r\nDiff GBs: 1.302\r\n########################\r\nOnes:  Tensor(\"tf.convert_to_tensor:0\", shape=(255, 255, 255), dtype=float32, device=/device:GPU:0)\r\nTime for 20 iters: 2.467s.\r\nDiff GBs: 1.354\r\n########################\r\nOnes:  <tf.Variable 'tf.get_variable:0' shape=(255, 255, 255) dtype=float32_ref>\r\nTime for 20 iters: 0.411s.\r\nDiff GBs: 0.023\r\n```\r\n\r\nI assumed that `tf.constant` and `tf.ones` would be equivalent, but they have significantly different runtime. \r\n\r\nAlso, a non-trainable `tf.Variable` is much more memory efficient. I guess it doesn't participate in the computation graph, so it doesn't get copied when the computation graph is updated. \r\n", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "The summarize this bug -- there's memory leak because tf.ones/tf.zeros use tf.constant. tf.constant doesn't deallocate it's memory by design. Recommended fix is to move tf.ones/tf.zeros to always tf.fill instead.", "tf.ones/tf.zeros is more efficient if you only create the graph once and\nrun it through the session many times.  if you're adding new tf.zeros each\ntime you call session.run, you're already slowing down your execution by\nmodifying the graph between executions - and that's a bigger problem.\n\nOn Wed, Dec 20, 2017 at 10:16 AM, Yaroslav Bulatov <notifications@github.com\n> wrote:\n\n> The summarize this bug -- there's memory leak because tf.ones/tf.zeros use\n> tf.constant. tf.constant doesn't deallocate it's memory by design.\n> Recommended fix is to move tf.ones/tf.zeros to always tf.fill instead.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13221#issuecomment-353141471>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimwgw7uwpbdODbnLlh9IuvyBURQ8Pks5tCU8HgaJpZM4Pf_kL>\n> .\n>\n", "It's a common pattern during interactive prototyping, and the slowdown is not significant. IE, 1ms additional slowdown between cell evaluations is not noticeable. Leaking 1GB of GPU memory between cell evaluations is noticeable.", "I have discussed with @reedwm, @mrry, @asimshankar, and @alextp. \r\n\r\n@alextp has a pending change to have ops like zeros_like not to use constant. Even though that might cause a small performance drop, it should cause less overall trouble.\r\n\r\nIn the long term, we want to switch to a constant manager within resource manager. When a ConstantOp is initialized, it will use its TensorProto as a key to query whether the same tensor had already been created. If so, the same Tensor will be used. If we provide a hash function for TensorProto, the constant manager can be implemented as a simple unordered_map. So if the same Python op have multiple C++ op, they will all share the same underlying Tensor. \r\n\r\nThis will solve the leaking Tensor on device, but it still doesn't solve the underlying duplicated TensorProto for multiple ConstantOp, which should be addressed separately.\r\n\r\nI am marking this as contribution welcome, unless someone has cycles to work on it. \r\n", "@zheng-xq What about other operations like tf.where, tf.tile, maybe even tf.range and tf.cast? Do they get fixed by @alextp's change?", "@dantkz I think so, `tf.zeros` is a very old op and somewhat unique in that it redirects to `tf.constant`. Newer ops should not rely on `tf.constant`", "Just wanted to post an update on #9091 (cpu growth unbounded with feed dicts).\r\n- the error is not due to graph not being fixed\r\n- it is not caused by using`tf.zeros` vs. `tf.fill`\r\n- I'm using `cudnnLstm`, not `dynamic_rnn`, so while_loops are also not the cause\r\n\r\nAfter updating to Tensorflow 1.6, the CPU growth dropped from the original 640mb/hr down to 25mb/hr (at most, not sure what the curve looks like). [e.g process starts out with 13.2gb of RAM, and 8 hours later is at 13.4gb of RAM]\r\n\r\nI now consider this issue fixed to the best of my knowledge!", "Thanks for the update!\n\nOn Fri, Mar 2, 2018 at 8:45 AM, Jonathan Raiman <notifications@github.com>\nwrote:\n\n> Just wanted to post an update on #9091\n> <https://github.com/tensorflow/tensorflow/issues/9091> (cpu growth\n> unbounded with feed dicts).\n>\n>    - the error is not due to graph not being fixed\n>    - it is not caused by usingtf.zeros vs. tf.fill\n>    - I'm using cudnnLstm, not dynamic_rnn, so while_loops are also not\n>    the cause\n>\n> After updating to Tensorflow 1.6, the CPU growth dropped from the original\n> 640mb/hr down to 25mb/hr (at most, not sure what the curve looks like).\n> [e.g process starts out with 13.2gb of RAM, and 8 hours later is at 13.4gb\n> of RAM]\n>\n> I now consider this issue fixed to the best of my knowledge!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13221#issuecomment-369978188>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim0o0cEhRyTiil28W2YCVCNuU93z2ks5taXcPgaJpZM4Pf_kL>\n> .\n>\n", "@vrv it may have been a bug in cub.  we've since updated both cuda and cub (i think); so whatever it was, it's been resolved."]}, {"number": 13220, "title": "CPU Build Fails on OSX Sierra ", "body": "I'm getting a cpu-only Bazel build failure on OSX.  TensorFlow has already been .configure'd with default options.\r\n\r\nMessage below. \r\n\r\n```\r\n$ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\nWARNING: /Users/kevin/projects/tensorflow/tensorflow/core/BUILD:1653:1: in includes attribute of cc_library rule //tensorflow/core:framework_headers_lib: '../../external/nsync/public' resolves to 'external/nsync/public' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /Users/kevin/projects/tensorflow/tensorflow/tensorflow.bzl:913:30.\r\nWARNING: /Users/kevin/projects/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.\r\nWARNING: /Users/kevin/projects/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.\r\nINFO: Found 1 target...\r\nERROR: /private/var/tmp/_bazel_kevin/1cea9193b5038a270f5e322e515767d8/external/nsync/BUILD:397:1: C++ compilation of rule '@nsync//:nsync_cpp' failed (Exit 1): cc_wrapper.sh failed: error executing command\r\n  (cd /private/var/tmp/_bazel_kevin/1cea9193b5038a270f5e322e515767d8/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/anaconda/bin:/Users/kevin/bin:/usr/local/Cellar/coreutils/8.28/libexec/gnubin:/usr/local/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/anaconda/bin/python \\\r\n    PYTHON_LIB_PATH=/anaconda/lib/python3.5/site-packages \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL=0 \\\r\n    TMPDIR=/var/folders/g9/fdv74qn92qs7yw31tlj4q0lw0000gn/T/ \\\r\n  external/local_config_cc/cc_wrapper.sh -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-march=native' -MD -MF bazel-out/local-py3-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/time_internal.pic.d -fPIC -iquote external/nsync -iquote bazel-out/local-py3-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/local-py3-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/local-py3-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsync//internal -I./external/nsync//platform/posix '-D_POSIX_C_SOURCE=200809L' -pthread -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c external/nsync/internal/time_internal.c -o bazel-out/local-py3-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/time_internal.pic.o).\r\nIn file included from external/nsync/internal/time_internal.c:16:\r\nIn file included from ./external/nsync//platform/c++11/platform.h:29:\r\nIn file included from /Library/Developer/CommandLineTools/usr/include/c++/v1/mutex:189:\r\nIn file included from /Library/Developer/CommandLineTools/usr/include/c++/v1/__mutex_base:17:\r\n/Library/Developer/CommandLineTools/usr/include/c++/v1/__threading_support:156:1: error: unknown type name 'mach_port_t'\r\nmach_port_t __libcpp_thread_get_port();\r\n^\r\n/Library/Developer/CommandLineTools/usr/include/c++/v1/__threading_support:300:1: error: unknown type name 'mach_port_t'\r\nmach_port_t __libcpp_thread_get_port() {\r\n^\r\n/Library/Developer/CommandLineTools/usr/include/c++/v1/__threading_support:301:12: error: use of undeclared identifier 'pthread_mach_thread_np'\r\n    return pthread_mach_thread_np(pthread_self());\r\n           ^\r\n3 errors generated.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 0.830s, Critical Path: 0.26s\r\n```\r\n\r\n**TensorFlow commit:** e9d5ee1ebffba25cef65f1f354b9e4ca9bcea10c\r\n\r\n**Mac OSX Sierra 10.12.6 (16G29)**\r\n**Command Line Tools:**\r\n```\r\n$ pkgutil --pkg-info=com.apple.pkg.CLTools_Executables\r\npackage-id: com.apple.pkg.CLTools_Executables\r\nversion: 9.0.0.0.1.1504363082\r\nvolume: /\r\nlocation: /\r\ninstall-time: 1505966986\r\ngroups: com.apple.FindSystemFiles.pkg-group\r\n```\r\n\r\n**Environment Capture Script:**\r\n```\r\n$ cat tf_env.txt\r\n\r\n== cat /etc/issue ===============================================\r\nDarwin MacBook-Pro.localdomain 16.7.0 Darwin Kernel Version 16.7.0: Thu Jun 15 17:36:27 PDT 2017; root:xnu-3789.70.16~2/RELEASE_X86_64 x86_64\r\nMac OS X 10.12.6\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nApple LLVM version 9.0.0 (clang-900.0.37)\r\nTarget: x86_64-apple-darwin16.7.0\r\nThread model: posix\r\nInstalledDir: /Library/Developer/CommandLineTools/usr/bin\r\n\r\n== uname -a =====================================================\r\nDarwin MacBook-Pro.localdomain 16.7.0 Darwin Kernel Version 16.7.0: Thu Jun 15 17:36:27 PDT 2017; root:xnu-3789.70.16~2/RELEASE_X86_64 x86_64\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.1)\r\nnumpydoc (0.7.0)\r\nprotobuf (3.4.0)\r\ntensorflow (1.3.0)\r\ntensorflow-tensorboard (0.1.6)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.3.0\r\ntf.GIT_VERSION = v1.3.0-rc2-20-g0787eee\r\ntf.COMPILER_VERSION = v1.3.0-rc2-20-g0787eee\r\nSanity check: array([1], dtype=int32)\r\nTraceback (most recent call last):\r\n  File \"/Users/kevin/projects/tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 48, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\nImportError: No module named 'tensorflow.python.pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/Users/kevin/projects/tensorflow/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/Users/kevin/projects/tensorflow/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/Users/kevin/projects/tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 59, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/Users/kevin/projects/tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 48, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\nImportError: No module named 'tensorflow.python.pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\ntools/tf_env_collect.sh: line 105: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n```\r\n\r\n**Bazel Information:**\r\n```\r\n$ bazel info\r\nbazel-bin: /private/var/tmp/_bazel_kevin/1cea9193b5038a270f5e322e515767d8/execroot/org_tensorflow/bazel-out/local-py3-opt/bin\r\nbazel-genfiles: /private/var/tmp/_bazel_kevin/1cea9193b5038a270f5e322e515767d8/execroot/org_tensorflow/bazel-out/local-py3-opt/genfiles\r\nbazel-testlogs: /private/var/tmp/_bazel_kevin/1cea9193b5038a270f5e322e515767d8/execroot/org_tensorflow/bazel-out/local-py3-opt/testlogs\r\ncharacter-encoding: file.encoding = ISO-8859-1, defaultCharset = ISO-8859-1\r\ncommand_log: /private/var/tmp/_bazel_kevin/1cea9193b5038a270f5e322e515767d8/command.log\r\ncommitted-heap-size: 1180MB\r\nexecution_root: /private/var/tmp/_bazel_kevin/1cea9193b5038a270f5e322e515767d8/execroot/org_tensorflow\r\ngc-count: 15\r\ngc-time: 1146ms\r\ninstall_base: /var/tmp/_bazel_kevin/install/ed43083c802b447b0d9313e2450af83f\r\njava-home: /Library/Java/JavaVirtualMachines/jdk1.8.0_144.jdk/Contents/Home/jre\r\njava-runtime: Java(TM) SE Runtime Environment (build 1.8.0_144-b01) by Oracle Corporation\r\njava-vm: Java HotSpot(TM) 64-Bit Server VM (build 25.144-b01, mixed mode) by Oracle Corporation\r\nmax-heap-size: 3817MB\r\nmessage_log: /private/var/tmp/_bazel_kevin/1cea9193b5038a270f5e322e515767d8/message.log\r\noutput_base: /private/var/tmp/_bazel_kevin/1cea9193b5038a270f5e322e515767d8\r\noutput_path: /private/var/tmp/_bazel_kevin/1cea9193b5038a270f5e322e515767d8/execroot/tensorflow/bazel-out\r\npackage_path: %workspace%\r\nrelease: release 0.5.4-homebrew\r\nserver_pid: 37949\r\nused-heap-size: 394MB\r\nworkspace: /Users/kevin/projects/tensorflow\r\n```", "comments": ["same error here", "Got the same error building TF for Java from source on Sierra 10.12.6.\r\n`ERROR: /private/var/tmp/_bazel_florin/5b9ecfceebaed4e3d76cf68084343c78/external/nsync/BUILD:397:1: C++ compilation of rule '@nsync//:nsync_cpp' failed (Exit 1).\r\nIn file included from external/nsync/internal/wait.c:16:\r\nIn file included from ./external/nsync//platform/c++11/platform.h:29:\r\nIn file included from /Library/Developer/CommandLineTools/usr/include/c++/v1/mutex:189:\r\nIn file included from /Library/Developer/CommandLineTools/usr/include/c++/v1/__mutex_base:17:\r\n/Library/Developer/CommandLineTools/usr/include/c++/v1/__threading_support:156:1: error: unknown type name 'mach_port_t'\r\nmach_port_t __libcpp_thread_get_port();\r\n^\r\n/Library/Developer/CommandLineTools/usr/include/c++/v1/__threading_support:300:1: error: unknown type name 'mach_port_t'\r\nmach_port_t __libcpp_thread_get_port() {\r\n^\r\n/Library/Developer/CommandLineTools/usr/include/c++/v1/__threading_support:301:12: error: use of undeclared identifier 'pthread_mach_thread_np'\r\n    return pthread_mach_thread_np(pthread_self());\r\n           ^\r\n`", "Similar issue here while building in android studio\r\nAndroid Studio: v3.0\r\nOS: Ubuntu v16.04\r\nbazel version:\r\n```\r\n$ bazel version\r\nBuild label: 0.5.4\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Fri Aug 25 10:00:00 2017 (1503655200)\r\nBuild timestamp: 1503655200\r\nBuild timestamp as int: 1503655200\r\n```\r\n\r\n\r\nError log:\r\n```\r\nInformation:Gradle tasks [:assembleDebug]\r\nWarning:/home/abhishek/tensorflow/tensorflow/core/BUILD:995:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:avgpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWarning:/home/abhishek/tensorflow/tensorflow/core/BUILD:995:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:bounds_check.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWarning:/home/abhishek/tensorflow/tensorflow/core/BUILD:995:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWarning:/home/abhishek/tensorflow/tensorflow/core/BUILD:995:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops_common.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWarning:/home/abhishek/tensorflow/tensorflow/core/BUILD:995:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops_gradients.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWarning:/home/abhishek/tensorflow/tensorflow/core/BUILD:995:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_activations.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWarning:/home/abhishek/tensorflow/tensorflow/core/BUILD:995:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_attention.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWarning:/home/abhishek/tensorflow/tensorflow/core/BUILD:995:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_backward_cuboid_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWarning:/home/abhishek/tensorflow/tensorflow/core/BUILD:995:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_backward_spatial_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWarning:/home/abhishek/tensorflow/tensorflow/core/BUILD:995:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_cuboid_convolution.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWarning:/home/abhishek/tensorflow/tensorflow/core/BUILD:995:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_pooling.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWarning:/home/abhishek/tensorflow/tensorflow/core/BUILD:995:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_softmax.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWarning:/home/abhishek/tensorflow/tensorflow/core/BUILD:995:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_spatial_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWarning:/home/abhishek/tensorflow/tensorflow/core/BUILD:995:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_volume_patch.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWarning:/home/abhishek/tensorflow/tensorflow/core/BUILD:995:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:fifo_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWarning:/home/abhishek/tensorflow/tensorflow/core/BUILD:995:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:maxpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWarning:/home/abhishek/tensorflow/tensorflow/core/BUILD:995:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:ops_util.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWarning:/home/abhishek/tensorflow/tensorflow/core/BUILD:995:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:ops_util.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWarning:/home/abhishek/tensorflow/tensorflow/core/BUILD:995:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:padding_fifo_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWarning:/home/abhishek/tensorflow/tensorflow/core/BUILD:995:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:pooling_ops_common.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWarning:/home/abhishek/tensorflow/tensorflow/core/BUILD:995:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:pooling_ops_common.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWarning:/home/abhishek/tensorflow/tensorflow/core/BUILD:995:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:queue_base.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWarning:/home/abhishek/tensorflow/tensorflow/core/BUILD:995:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:queue_op.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWarning:/home/abhishek/tensorflow/tensorflow/core/BUILD:995:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:typed_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWarning:/home/abhishek/tensorflow/tensorflow/core/BUILD:995:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_entry.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWarning:/home/abhishek/tensorflow/tensorflow/core/BUILD:995:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_scorer.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWarning:/home/abhishek/tensorflow/tensorflow/core/BUILD:995:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_search.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWarning:/home/abhishek/tensorflow/tensorflow/core/BUILD:995:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_decoder.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWarning:/home/abhishek/tensorflow/tensorflow/core/BUILD:995:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_loss_util.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWarning:/home/abhishek/tensorflow/tensorflow/core/BUILD:995:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:naming.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWarning:/home/abhishek/tensorflow/tensorflow/core/BUILD:995:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:naming.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWarning:/home/abhishek/tensorflow/tensorflow/core/BUILD:995:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWarning:/home/abhishek/tensorflow/tensorflow/core/BUILD:995:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nError:/home/abhishek/.cache/bazel/_bazel_abhishek/769564cbdc3893ccf2b392872ca61e6b/external/nsync/BUILD:397:1: C++ compilation of rule '@nsync//:nsync_cpp' failed (Exit 1)\r\nError:Execution failed for task ':buildNativeBazel'.\r\n> Process 'command '/usr/bin/bazel'' finished with non-zero exit value 1\r\nInformation:BUILD FAILED in 1m 28s\r\nInformation:2 errors\r\nInformation:33 warnings\r\n```", "I started getting this error yesterday after upgrading XCODE for a different project I'm working on.\r\n\r\ntemporary hack fix in `__threading_support`\r\n\r\n```c++\r\n#ifndef _MACH_PORT_T\r\n#define _MACH_PORT_T\r\n#include <sys/_types.h> /* __darwin_mach_port_t */\r\ntypedef __darwin_mach_port_t mach_port_t;\r\nmach_port_t pthread_mach_thread_np(pthread_t);\r\n#endif /* _MACH_PORT_T */\r\n```\r\n\r\nObviously not a long term solution, however it unblocks me today.", "Same problem here, fresh install of Sierra and all instructions followed to the letter from TF.\r\n\r\nI did update the command line tools though! CPU only version\r\n\r\n```\r\nWARNING: /Users/rajesh/Documents/tensorflow/tensorflow/core/BUILD:1773:1: in includes attribute of cc_library rule //tensorflow/core:framework_headers_lib: '../../external/nsync/public' resolves to 'external/nsync/public' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /Users/rajesh/Documents/tensorflow/tensorflow/tensorflow.bzl:1029:30.\r\nWARNING: /Users/rajesh/Documents/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.\r\nWARNING: /Users/rajesh/Documents/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.\r\nINFO: Found 1 target...\r\nERROR: /private/var/tmp/_bazel_rajesh/4eba0b84eb73970e1bf50eaa1737d728/external/nsync/BUILD:397:1: C++ compilation of rule '@nsync//:nsync_cpp' failed (Exit 1).\r\nIn file included from external/nsync/internal/time_internal.c:16:\r\nIn file included from ./external/nsync//platform/c++11/platform.h:29:\r\nIn file included from /Library/Developer/CommandLineTools/usr/include/c++/v1/mutex:189:\r\nIn file included from /Library/Developer/CommandLineTools/usr/include/c++/v1/__mutex_base:17:\r\n/Library/Developer/CommandLineTools/usr/include/c++/v1/__threading_support:156:1: error: unknown type name 'mach_port_t'\r\nmach_port_t __libcpp_thread_get_port();\r\n^\r\n/Library/Developer/CommandLineTools/usr/include/c++/v1/__threading_support:300:1: error: unknown type name 'mach_port_t'\r\nmach_port_t __libcpp_thread_get_port() {\r\n^\r\n/Library/Developer/CommandLineTools/usr/include/c++/v1/__threading_support:301:12: error: use of undeclared identifier 'pthread_mach_thread_np'\r\n    return pthread_mach_thread_np(pthread_self());\r\n           ^\r\n3 errors generated.\r\n```", "the same error\r\n\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\nWARNING: /Users/mac/Documents/GitHub/tensorflow/tensorflow/core/BUILD:1773:1: in includes attribute of cc_library rule //tensorflow/core:framework_headers_lib: '../../external/nsync/public' resolves to 'external/nsync/public' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /Users/mac/Documents/GitHub/tensorflow/tensorflow/tensorflow.bzl:1032:30.\r\nWARNING: /Users/mac/Documents/GitHub/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.\r\nWARNING: /Users/mac/Documents/GitHub/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.\r\nINFO: Found 1 target...\r\nERROR: /private/var/tmp/_bazel_mac/235f7843b01edb5217d4c0b999a2ec13/external/nsync/BUILD:397:1: C++ compilation of rule '@nsync//:nsync_cpp' failed (Exit 1): cc_wrapper.sh failed: error executing command \r\n  (cd /private/var/tmp/_bazel_mac/235f7843b01edb5217d4c0b999a2ec13/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/local/bin/python2 \\\r\n    PYTHON_LIB_PATH=/usr/local/Cellar/python/2.7.13_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL=0 \\\r\n    TMPDIR=/var/folders/c_/73pp1t892b33t_7_pj20l6dm0000gn/T/ \\\r\n  external/local_config_cc/cc_wrapper.sh -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-march=native' -MD -MF bazel-out/local-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/counter.pic.d -fPIC -iquote external/nsync -iquote bazel-out/local-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/local-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/local-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsync//internal -I./external/nsync//platform/posix '-D_POSIX_C_SOURCE=200809L' -pthread -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c external/nsync/internal/counter.c -o bazel-out/local-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/counter.pic.o).\r\nIn file included from external/nsync/internal/counter.c:16:\r\nIn file included from ./external/nsync//platform/c++11/platform.h:29:\r\nIn file included from /Library/Developer/CommandLineTools/usr/include/c++/v1/mutex:189:\r\nIn file included from /Library/Developer/CommandLineTools/usr/include/c++/v1/__mutex_base:17:\r\n/Library/Developer/CommandLineTools/usr/include/c++/v1/__threading_support:156:1: error: unknown type name 'mach_port_t'\r\nmach_port_t __libcpp_thread_get_port();\r\n^\r\n/Library/Developer/CommandLineTools/usr/include/c++/v1/__threading_support:300:1: error: unknown type name 'mach_port_t'\r\nmach_port_t __libcpp_thread_get_port() {\r\n^\r\n/Library/Developer/CommandLineTools/usr/include/c++/v1/__threading_support:301:12: error: use of undeclared identifier 'pthread_mach_thread_np'\r\n    return pthread_mach_thread_np(pthread_self());\r\n           ^\r\n3 errors generated.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 21.210s, Critical Path: 0.45s\r\n\r\n\r\nEdit /Library/Developer/CommandLineTools/usr/include/c++/v1/__threading_support  \r\n with\r\n#ifndef _MACH_PORT_T\r\n#define _MACH_PORT_T\r\n#include <sys/_types.h> /* __darwin_mach_port_t */\r\ntypedef __darwin_mach_port_t mach_port_t;\r\nmach_port_t pthread_mach_thread_np(pthread_t);\r\n#endif /* _MACH_PORT_T */\r\n\r\nexternal/local_config_cc/cc_wrapper.sh -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-march=native' '-std=c++0x' '-march=native' -MD -MF bazel-out/local-opt/bin/external/com_googlesource_code_re2/_objs/re2/external/com_googlesource_code_re2/re2/parse.pic.d '-frandom-seed=bazel-out/local-opt/bin/external/com_googlesource_code_re2/_objs/re2/external/com_googlesource_code_re2/re2/parse.pic.o' -fPIC -iquote external/com_googlesource_code_re2 -iquote bazel-out/local-opt/genfiles/external/com_googlesource_code_re2 -iquote external/bazel_tools -iquote bazel-out/local-opt/genfiles/external/bazel_tools -isystem external/bazel_tools/tools/cpp/gcc3 -pthread -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c external/com_googlesource_code_re2/re2/parse.cc -o bazel-out/local-opt/bin/external/com_googlesource_code_re2/_objs/re2/external/com_googlesource_code_re2/re2/parse.pic.o).\r\nIn file included from external/com_googlesource_code_re2/re2/parse.cc:28:\r\nIn file included from external/com_googlesource_code_re2/util/logging.h:13:\r\nIn file included from /Library/Developer/CommandLineTools/usr/include/c++/v1/ostream:138:\r\nIn file included from /Library/Developer/CommandLineTools/usr/include/c++/v1/ios:216:\r\nIn file included from /Library/Developer/CommandLineTools/usr/include/c++/v1/__locale:18:\r\nIn file included from /Library/Developer/CommandLineTools/usr/include/c++/v1/mutex:189:\r\nIn file included from /Library/Developer/CommandLineTools/usr/include/c++/v1/__mutex_base:17:\r\n/Library/Developer/CommandLineTools/usr/include/c++/v1/__threading_support:24:36: error: unknown type name 'pthread_t'\r\nmach_port_t pthread_mach_thread_np(pthread_t);\r\n                                   ^\r\n1 error generated.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build", "use code shown below  ,build success!\r\n#ifndef _MACH_PORT_T\r\n#define _MACH_PORT_T\r\n#include <sys/_types.h> /* __darwin_mach_port_t */\r\ntypedef __darwin_mach_port_t mach_port_t;\r\n#include <pthread.h>\r\nmach_port_t pthread_mach_thread_np(pthread_t);\r\n#endif /* _MACH_PORT_T */", "@Kangbababa: I only know to follow the exact instructions on Tensorflow source guide. Could you let us know how to use this code or some link where such process is mentioned?", "@rajeshr91  :Edit   /Library/Developer/CommandLineTools/usr/include/c++/v1/__threading_support\r\n file with Mac Text Edit Tool,Add  define head code.", "Same issue when building CPU only config.\r\n\r\nCommand:\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\nError:\r\nWARNING: /Users/betterchen/Documents/build_source/tensorflow/tensorflow/core/BUILD:1772:1: in includes attribute of cc_library rule //tensorflow/core:framework_headers_lib: '../../external/nsync/public' resolves to 'external/nsync/public' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /Users/betterchen/Documents/build_source/tensorflow/tensorflow/tensorflow.bzl:1029:30.\r\nWARNING: /Users/betterchen/Documents/build_source/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.\r\nWARNING: /Users/betterchen/Documents/build_source/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.\r\nINFO: Found 1 target...\r\nERROR: /private/var/tmp/_bazel_betterchen/0097c5c262e7669ec35d623703e38a90/external/nsync/BUILD:397:1: C++ compilation of rule '@nsync//:nsync_cpp' failed (Exit 1).\r\nIn file included from external/nsync/internal/dll.c:16:\r\nIn file included from ./external/nsync//platform/c++11/platform.h:29:\r\nIn file included from /Library/Developer/CommandLineTools/usr/include/c++/v1/mutex:189:\r\nIn file included from /Library/Developer/CommandLineTools/usr/include/c++/v1/__mutex_base:17:\r\n/Library/Developer/CommandLineTools/usr/include/c++/v1/__threading_support:156:1: error: unknown type name 'mach_port_t'\r\nmach_port_t __libcpp_thread_get_port();\r\n^\r\n/Library/Developer/CommandLineTools/usr/include/c++/v1/__threading_support:300:1: error: unknown type name 'mach_port_t'\r\nmach_port_t __libcpp_thread_get_port() {\r\n^\r\n/Library/Developer/CommandLineTools/usr/include/c++/v1/__threading_support:301:12: error: use of undeclared identifier 'pthread_mach_thread_np'\r\n    return pthread_mach_thread_np(pthread_self());\r\n           ^\r\n3 errors generated.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 26.368s, Critical Path: 0.54s", "@Kangbababa I see you built successfully by commenting out the <pthread.h> related section of the fix. But this didn't work for me. Can you please confirm the fix?\r\n\r\nERROR: /private/var/tmp/_bazel_betterchen/0097c5c262e7669ec35d623703e38a90/external/nsync/BUILD:397:1: C++ compilation of rule '@nsync//:nsync_cpp' failed (Exit 1).\r\nIn file included from external/nsync/internal/dll.c:16:\r\nIn file included from ./external/nsync//platform/c++11/platform.h:29:\r\nIn file included from /Library/Developer/CommandLineTools/usr/include/c++/v1/mutex:189:\r\nIn file included from /Library/Developer/CommandLineTools/usr/include/c++/v1/__mutex_base:17:\r\n/Library/Developer/CommandLineTools/usr/include/c++/v1/__threading_support:164:1: error: unknown type name 'mach_port_t'\r\nmach_port_t __libcpp_thread_get_port();\r\n^\r\n/Library/Developer/CommandLineTools/usr/include/c++/v1/__threading_support:308:1: error: unknown type name 'mach_port_t'\r\nmach_port_t __libcpp_thread_get_port() {\r\n^\r\n/Library/Developer/CommandLineTools/usr/include/c++/v1/__threading_support:309:12: error: use of undeclared identifier 'pthread_mach_thread_np'\r\n    return pthread_mach_thread_np(pthread_self());\r\n           ^\r\n/Library/Developer/CommandLineTools/usr/include/c++/v1/__threading_support:11:2: error: unterminated conditional directive\r\n#ifndef _MACH_PORT_T\r\n ^\r\nIn file included from external/nsync/internal/dll.c:16:\r\nIn file included from ./external/nsync//platform/c++11/platform.h:29:\r\nIn file included from /Library/Developer/CommandLineTools/usr/include/c++/v1/mutex:195:\r\n/Library/Developer/CommandLineTools/usr/include/c++/v1/__threading_support:11:2: error: unterminated conditional directive\r\n#ifndef _MACH_PORT_T\r\n ^\r\n5 errors generated.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 25.894s, Critical Path: 0.52s", "Built Successfully with fix:\r\n\r\nInserting following codes\r\n```\r\n#ifndef _MACH_PORT_T\r\n#define _MACH_PORT_T\r\n#include <sys/_types.h> /* __darwin_mach_port_t */\r\ntypedef __darwin_mach_port_t mach_port_t;\r\n#include <pthread.h>\r\nmach_port_t pthread_mach_thread_np(pthread_t);\r\n#endif /* _MACH_PORT_T */\r\n```\r\ninto file:\r\n> /Library/Developer/CommandLineTools/usr/include/c++/v1/__threading_support\r\n", "Has the same problem and @betterchen 's solution worked perfectly.\r\nPlease notice however that the code should be inserted at the **top** of the file, otherwise it wouldn't work.", "I just had the same problem on a completely different project and I could resolve it by defining the `_DARWIN_C_SOURCE` preprocessor macro. This can either be done by using a compiler flag or by including something like\r\n\r\n```\r\n#ifdef __APPLE__\r\n#define _DARWIN_C_SOURCE\r\n#endif\r\n```\r\n\r\nat the top of the affected source file.", "Sorry for the delay in response.\r\nLooks like this was resolved. Closing the issue.\r\n\r\nPlease feel free to crate a pull request to merge this fix into the repository.", "@gunan I still get this at HEAD.", "@betterchen @smarsching Do you know which fix is better?  @smarsching's is certainly simpler.", "Hmm, @betterchen's fix is modifying system files, actually, so that's out.  Not sure what the right file is for @smarsching's fix; need to look at nsync.", "Here's some other project's PR to fix this: https://github.com/apache/arrow/pull/1144/files", "Hmm, nsync doesn't have an issues tab at (https://github.com/google/nsync), and doesn't build on its own.  Seems like the right fix is to nsync given that tensorflow uses nsync's BUILD file, but I'm not sure how to easily test.\r\n\r\nTrying to build nsync standalone gives\r\n\r\n    poisson:nsync% bazel build ...\r\n    ERROR: error loading package 'bazel': Extension file not found. Unable to load file '//bazel:bazel/pkg_path_name.bzl': file doesn't exist or isn't a file.\r\n    ERROR: error loading package 'bazel': Extension file not found. Unable to load file '//bazel:bazel/pkg_path_name.bzl': file doesn't exist or isn't a file.\r\n    INFO: Elapsed time: 0.078s\r\n", "@m3bm3b: You fiddled with nsync's build system last; do you know the right approach?", "From the original message, it seems as though \r\n    #include <mutex>\r\ndoes not work (without additional work) on Sierra.   Is that really true?\r\n\r\nAlas, it does work on the High Sierra system with Xcode 9 that I currently have access to,\r\nso it's hard for me to check whether a fix will work.\r\n", "In that previous comment, I wrote\r\n   #include  &lt;mutex&gt;\r\nbut github helpfully threw away the thing in the angle brackets.", "@m3bm3b Yes, the bug on Sierra seems to affect all standard library headers that internally use the `__threading_support` header. In addition to `<mutex>` this also affects `<condition_variable>` and `<thread>`.\r\n\r\nI just checked and this problem is still present in Sierra, even with the most recent update of the developer tools.", "Based on the reports earlier in this thread, \r\nI've made a change to nsync that defines _DARWIN_C_SOURCE\r\nin the relevant builds in the bazel BUILD file, the cmake CMakeLists.txt file,\r\nand the relevant Makefiles.  \r\n\r\nCould someone running Sierra could try these and report whether they work?\r\n\r\n\r\n", "I just synced the master branch. bazel on Sierra still threw error. Please let me know if anything is missing.\r\n\r\nERROR: /Users/lianjia/repo/tensorflow/tensorflow/compiler/xla/service/cpu/BUILD:145:1: C++ compilation of rule '//tensorflow/compiler/xla/service/cpu:simple_orc_jit' failed (Exit 1).\r\nIn file included from tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:32:\r\n./tensorflow/compiler/xla/service/cpu/cpu_runtime_neon.h:55:30: warning: '__xla_cpu_runtime_ExpV4F32NEON' has C-linkage specified, but returns incomplete type 'xla::cpu::runtime::V4F32NEON' which could be incompatible with C [-Wreturn-type-c-linkage]\r\nxla::cpu::runtime::V4F32NEON __xla_cpu_runtime_ExpV4F32NEON(\r\n                             ^\r\n./tensorflow/compiler/xla/service/cpu/cpu_runtime_neon.h:58:30: warning: '__xla_cpu_runtime_LogV4F32NEON' has C-linkage specified, but returns incomplete type 'xla::cpu::runtime::V4F32NEON' which could be incompatible with C [-Wreturn-type-c-linkage]\r\nxla::cpu::runtime::V4F32NEON __xla_cpu_runtime_LogV4F32NEON(\r\n                             ^\r\ntensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:220:3: error: reinterpret_cast cannot resolve overloaded function 'acos' to type 'void *'\r\n  REGISTER_LIBM_SYMBOL(acos);\r\n  ^~~~~~~~~~~~~~~~~~~~~~~~~~\r\n", "Those errors are not in nsync, but rather in the xla part of tensorflow\nI think it's a known bug that someone is working on.\n\nI assume that nsync (as opposed to tensorflow)\ndoes compile correctly (for example, that it can be compiled and its tests\nrun on Sierra.\nLet me know if that's not the case.\n\n\nOn 26 October 2017 at 10:47, jiangok2006 <notifications@github.com> wrote:\n\n> I just synced the master branch. bazel on Sierra still threw error. Please\n> let me know if anything is missing.\n>\n> ERROR: /Users/lianjia/repo/tensorflow/tensorflow/compiler/xla/service/cpu/BUILD:145:1:\n> C++ compilation of rule '//tensorflow/compiler/xla/service/cpu:simple_orc_jit'\n> failed (Exit 1).\n> In file included from tensorflow/compiler/xla/\n> service/cpu/simple_orc_jit.cc:32:\n> ./tensorflow/compiler/xla/service/cpu/cpu_runtime_neon.h:55:30: warning:\n> '__xla_cpu_runtime_ExpV4F32NEON' has C-linkage specified, but returns\n> incomplete type 'xla::cpu::runtime::V4F32NEON' which could be incompatible\n> with C [-Wreturn-type-c-linkage]\n> xla::cpu::runtime::V4F32NEON __xla_cpu_runtime_ExpV4F32NEON(\n> ^\n> ./tensorflow/compiler/xla/service/cpu/cpu_runtime_neon.h:58:30: warning:\n> '__xla_cpu_runtime_LogV4F32NEON' has C-linkage specified, but returns\n> incomplete type 'xla::cpu::runtime::V4F32NEON' which could be incompatible\n> with C [-Wreturn-type-c-linkage]\n> xla::cpu::runtime::V4F32NEON __xla_cpu_runtime_LogV4F32NEON(\n> ^\n> tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:220:3: error:\n> reinterpret_cast cannot resolve overloaded function 'acos' to type 'void\n>\n>\n>\n> *' REGISTER_LIBM_SYMBOL(acos); ^~~~~~~~~~~~~~~~~~~~~~~~~~\n> tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:217:31: note:\n> expanded from macro 'REGISTER_LIBM_SYMBOL' registry->Register(#name,\n> reinterpret_cast<void*>(name));\n> ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:714:1:\n> note: candidate function\n> acos(_A1 __lcpp_x) _NOEXCEPT {return ::acos((double)__lcpp_x);}\n> ^\n> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:708:46:\n> note: candidate function\n> inline _LIBCPP_INLINE_VISIBILITY long double acos(long double __lcpp_x)\n> _NOEXCEPT {return ::acosl(__lcpp_x);}\n> ^\n> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:707:46:\n> note: candidate function\n> inline _LIBCPP_INLINE_VISIBILITY float acos(float __lcpp_x) _NOEXCEPT\n> {return ::acosf(__lcpp_x);}\n> ^\n> /usr/include/math.h:323:15: note: candidate function\n> extern double acos(double);\n> ^\n> tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:221:3: error:\n> reinterpret_cast cannot resolve overloaded function 'acosh' to type 'void\n>\n>\n>\n> *' REGISTER_LIBM_SYMBOL(acosh); ^~~~~~~~~~~~~~~~~~~~~~~~~~~\n> tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:217:31: note:\n> expanded from macro 'REGISTER_LIBM_SYMBOL' registry->Register(#name,\n> reinterpret_cast<void*>(name));\n> ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:1005:1:\n> note: candidate function\n> acosh(_A1 __lcpp_x) _NOEXCEPT {return ::acosh((double)__lcpp_x);}\n> ^\n> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:1000:46:\n> note: candidate function\n> inline _LIBCPP_INLINE_VISIBILITY long double acosh(long double __lcpp_x)\n> _NOEXCEPT {return ::acoshl(__lcpp_x);}\n> ^\n> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:999:46:\n> note: candidate function\n> inline _LIBCPP_INLINE_VISIBILITY float acosh(float __lcpp_x) _NOEXCEPT\n> {return ::acoshf(__lcpp_x);}\n> ^\n> /usr/include/math.h:351:15: note: candidate function\n> extern double acosh(double);\n> ^\n> tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:222:3: error:\n> reinterpret_cast cannot resolve overloaded function 'asin' to type 'void\n>\n>\n>\n> *' REGISTER_LIBM_SYMBOL(asin); ^~~~~~~~~~~~~~~~~~~~~~~~~~\n> tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:217:31: note:\n> expanded from macro 'REGISTER_LIBM_SYMBOL' registry->Register(#name,\n> reinterpret_cast<void*>(name));\n> ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:726:1:\n> note: candidate function\n> asin(_A1 __lcpp_x) _NOEXCEPT {return ::asin((double)__lcpp_x);}\n> ^\n> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:720:46:\n> note: candidate function\n> inline _LIBCPP_INLINE_VISIBILITY long double asin(long double __lcpp_x)\n> _NOEXCEPT {return ::asinl(__lcpp_x);}\n> ^\n> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:719:46:\n> note: candidate function\n> inline _LIBCPP_INLINE_VISIBILITY float asin(float __lcpp_x) _NOEXCEPT\n> {return ::asinf(__lcpp_x);}\n> ^\n> /usr/include/math.h:327:15: note: candidate function\n> extern double asin(double);\n> ^\n> tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:223:3: error:\n> reinterpret_cast cannot resolve overloaded function 'asinh' to type 'void\n>\n>\n>\n> *' REGISTER_LIBM_SYMBOL(asinh); ^~~~~~~~~~~~~~~~~~~~~~~~~~~\n> tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:217:31: note:\n> expanded from macro 'REGISTER_LIBM_SYMBOL' registry->Register(#name,\n> reinterpret_cast<void*>(name));\n> ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:1017:1:\n> note: candidate function\n> asinh(_A1 __lcpp_x) _NOEXCEPT {return ::asinh((double)__lcpp_x);}\n> ^\n> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:1012:46:\n> note: candidate function\n> inline _LIBCPP_INLINE_VISIBILITY long double asinh(long double __lcpp_x)\n> _NOEXCEPT {return ::asinhl(__lcpp_x);}\n> ^\n> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:1011:46:\n> note: candidate function\n> inline _LIBCPP_INLINE_VISIBILITY float asinh(float __lcpp_x) _NOEXCEPT\n> {return ::asinhf(__lcpp_x);}\n> ^\n> /usr/include/math.h:355:15: note: candidate function\n> extern double asinh(double);\n> ^\n> tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:224:3: error:\n> reinterpret_cast cannot resolve overloaded function 'atan' to type 'void\n>\n>\n>\n> *' REGISTER_LIBM_SYMBOL(atan); ^~~~~~~~~~~~~~~~~~~~~~~~~~\n> tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:217:31: note:\n> expanded from macro 'REGISTER_LIBM_SYMBOL' registry->Register(#name,\n> reinterpret_cast<void*>(name));\n> ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:738:1:\n> note: candidate function\n> atan(_A1 __lcpp_x) _NOEXCEPT {return ::atan((double)__lcpp_x);}\n> ^\n> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:732:46:\n> note: candidate function\n> inline _LIBCPP_INLINE_VISIBILITY long double atan(long double __lcpp_x)\n> _NOEXCEPT {return ::atanl(__lcpp_x);}\n> ^\n> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:731:46:\n> note: candidate function\n> inline _LIBCPP_INLINE_VISIBILITY float atan(float __lcpp_x) _NOEXCEPT\n> {return ::atanf(__lcpp_x);}\n> ^\n> /usr/include/math.h:331:15: note: candidate function\n> extern double atan(double);\n> ^\n> tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:225:3: error:\n> reinterpret_cast cannot resolve overloaded function 'atan2' to type 'void\n>\n>\n>\n> *' REGISTER_LIBM_SYMBOL(atan2); ^~~~~~~~~~~~~~~~~~~~~~~~~~~\n> tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:217:31: note:\n> expanded from macro 'REGISTER_LIBM_SYMBOL' registry->Register(#name,\n> reinterpret_cast<void*>(name));\n> ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:755:1:\n> note: candidate function\n> atan2(_A1 __lcpp_y, _A2 __lcpp_x) _NOEXCEPT\n> ^\n> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:744:46:\n> note: candidate function\n> inline _LIBCPP_INLINE_VISIBILITY long double atan2(long double __lcpp_y,\n> long double __lcpp_x) _NOEXCEPT {return ::atan2l(__lcpp_y, __lcpp_x);}\n> ^\n> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:743:46:\n> note: candidate function\n> inline _LIBCPP_INLINE_VISIBILITY float atan2(float __lcpp_y, float\n> __lcpp_x) _NOEXCEPT {return ::atan2f(__lcpp_y, __lcpp_x);}\n> ^\n> /usr/include/math.h:335:15: note: candidate function\n> extern double atan2(double, double);\n> ^\n> tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:226:3: error:\n> reinterpret_cast cannot resolve overloaded function 'atanh' to type 'void\n>\n>\n>\n> *' REGISTER_LIBM_SYMBOL(atanh); ^~~~~~~~~~~~~~~~~~~~~~~~~~~\n> tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:217:31: note:\n> expanded from macro 'REGISTER_LIBM_SYMBOL' registry->Register(#name,\n> reinterpret_cast<void*>(name));\n> ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:1029:1:\n> note: candidate function\n> atanh(_A1 __lcpp_x) _NOEXCEPT {return ::atanh((double)__lcpp_x);}\n> ^\n> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:1024:46:\n> note: candidate function\n> inline _LIBCPP_INLINE_VISIBILITY long double atanh(long double __lcpp_x)\n> _NOEXCEPT {return ::atanhl(__lcpp_x);}\n> ^\n> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:1023:46:\n> note: candidate function\n> inline _LIBCPP_INLINE_VISIBILITY float atanh(float __lcpp_x) _NOEXCEPT\n> {return ::atanhf(__lcpp_x);}\n> ^\n> /usr/include/math.h:359:15: note: candidate function\n> extern double atanh(double);\n> ^\n> tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:227:3: error:\n> reinterpret_cast cannot resolve overloaded function 'cbrt' to type 'void\n>\n>\n>\n> *' REGISTER_LIBM_SYMBOL(cbrt); ^~~~~~~~~~~~~~~~~~~~~~~~~~\n> tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:217:31: note:\n> expanded from macro 'REGISTER_LIBM_SYMBOL' registry->Register(#name,\n> reinterpret_cast<void*>(name));\n> ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:1041:1:\n> note: candidate function\n> cbrt(_A1 __lcpp_x) _NOEXCEPT {return ::cbrt((double)__lcpp_x);}\n> ^\n> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:1036:46:\n> note: candidate function\n> inline _LIBCPP_INLINE_VISIBILITY long double cbrt(long double __lcpp_x)\n> _NOEXCEPT {return ::cbrtl(__lcpp_x);}\n> ^\n> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:1035:46:\n> note: candidate function\n> inline _LIBCPP_INLINE_VISIBILITY float cbrt(float __lcpp_x) _NOEXCEPT\n> {return ::cbrtf(__lcpp_x);}\n> ^\n> /usr/include/math.h:435:15: note: candidate function\n> extern double cbrt(double);\n> ^\n> tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:228:3: error:\n> reinterpret_cast cannot resolve overloaded function 'ceil' to type 'void\n>\n>\n>\n> *' REGISTER_LIBM_SYMBOL(ceil); ^~~~~~~~~~~~~~~~~~~~~~~~~~\n> tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:217:31: note:\n> expanded from macro 'REGISTER_LIBM_SYMBOL' registry->Register(#name,\n> reinterpret_cast<void*>(name));\n> ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:773:1:\n> note: candidate function\n> ceil(_A1 __lcpp_x) _NOEXCEPT {return ::ceil((double)__lcpp_x);}\n> ^\n> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:767:46:\n> note: candidate function\n> inline _LIBCPP_INLINE_VISIBILITY long double ceil(long double __lcpp_x)\n> _NOEXCEPT {return ::ceill(__lcpp_x);}\n> ^\n> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:766:46:\n> note: candidate function\n> inline _LIBCPP_INLINE_VISIBILITY float ceil(float __lcpp_x) _NOEXCEPT\n> {return ::ceilf(__lcpp_x);}\n> ^\n> /usr/include/math.h:470:15: note: candidate function\n> extern double ceil(double);\n> ^\n> tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:229:3: error:\n> reinterpret_cast cannot resolve overloaded function 'copysign' to type\n> 'void\n>\n>\n>\n> *' REGISTER_LIBM_SYMBOL(copysign); ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n> tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:217:31: note:\n> expanded from macro 'REGISTER_LIBM_SYMBOL' registry->Register(#name,\n> reinterpret_cast<void*>(name));\n> ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:1065:1:\n> note: candidate function\n> copysign(_A1 __lcpp_x, _A2 __lcpp_y) _NOEXCEPT\n> ^\n> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:1052:1:\n> note: candidate function\n> copysign(long double __lcpp_x, long double __lcpp_y) _NOEXCEPT {\n> ^\n> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:1047:40:\n> note: candidate function\n> inline _LIBCPP_INLINE_VISIBILITY float copysign(float __lcpp_x,\n> ^\n> /usr/include/math.h:526:15: note: candidate function\n> extern double copysign(double, double);\n> ^\n> tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:230:3: error:\n> reinterpret_cast cannot resolve overloaded function 'cos' to type 'void\n>\n>\n>\n> *' REGISTER_LIBM_SYMBOL(cos); ^~~~~~~~~~~~~~~~~~~~~~~~~\n> tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:217:31: note:\n> expanded from macro 'REGISTER_LIBM_SYMBOL' registry->Register(#name,\n> reinterpret_cast<void*>(name));\n> ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:785:1:\n> note: candidate function\n> cos(_A1 __lcpp_x) _NOEXCEPT {return ::cos((double)__lcpp_x);}\n> ^\n> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:779:46:\n> note: candidate function\n> inline _LIBCPP_INLINE_VISIBILITY long double cos(long double __lcpp_x)\n> _NOEXCEPT {return ::cosl(__lcpp_x);}\n> ^\n> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:778:46:\n> note: candidate function\n> inline _LIBCPP_INLINE_VISIBILITY float cos(float __lcpp_x) _NOEXCEPT\n> {return ::cosf(__lcpp_x);}\n> ^\n> /usr/include/math.h:339:15: note: candidate function\n> extern double cos(double);\n> ^\n> tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:231:3: error:\n> reinterpret_cast cannot resolve overloaded function 'cosh' to type 'void\n>\n>\n>\n> *' REGISTER_LIBM_SYMBOL(cosh); ^~~~~~~~~~~~~~~~~~~~~~~~~~\n> tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:217:31: note:\n> expanded from macro 'REGISTER_LIBM_SYMBOL' registry->Register(#name,\n> reinterpret_cast<void*>(name));\n> ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:797:1:\n> note: candidate function\n> cosh(_A......\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13220#issuecomment-339743486>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AJsp2c5mtVyiVMIkhka_5rDfu7T4iWqkks5swMXOgaJpZM4Pf1KK>\n> .\n>\n", "I have made sure that the error you're seeing in\n    tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc\non the Mac is known to the person who made the change.\n\nYou may be able to work around it temporarily by turning off XLA\nsupport when you configure tensorflow.   That is, when it asks\n  \"Do you wish to build TensorFlow with XLA JIT support? [y/N]:\"\nrespond \"N\".  Then it might not bother to\nbuild tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc\nBut I have not confirmed that this will work as a workaround.\n\nI believe that the underlying issue is a problem with the standard header\nfiles on a Mac.\nIn particular, the standard header file <math.h> somehow (and I think\nincorrectly)\ndefines an overloaded version of acos() (and many other functions)\nin the top-level namespace.  It's as though though the (correctly)\noverloaded versions of std::acos()\nwere leaking out of the <cmath> header file and into the global namespace,\ninstead of being confined to std::.\nI don't think <math.h> should be doing that,\nbut alas it seems to be true on even the most recent version of MacOS and\nXcode.\n\n\n\nOn 26 October 2017 at 12:20, Mike Burrows <m3b@google.com> wrote:\n\n> Those errors are not in nsync, but rather in the xla part of tensorflow\n> I think it's a known bug that someone is working on.\n>\n> I assume that nsync (as opposed to tensorflow)\n> does compile correctly (for example, that it can be compiled and its tests\n> run on Sierra.\n> Let me know if that's not the case.\n>\n>\n> On 26 October 2017 at 10:47, jiangok2006 <notifications@github.com> wrote:\n>\n>> I just synced the master branch. bazel on Sierra still threw error.\n>> Please let me know if anything is missing.\n>>\n>> ERROR: /Users/lianjia/repo/tensorflow/tensorflow/compiler/xla/service/cpu/BUILD:145:1:\n>> C++ compilation of rule '//tensorflow/compiler/xla/service/cpu:simple_orc_jit'\n>> failed (Exit 1).\n>> In file included from tensorflow/compiler/xla/servic\n>> e/cpu/simple_orc_jit.cc:32:\n>> ./tensorflow/compiler/xla/service/cpu/cpu_runtime_neon.h:55:30: warning:\n>> '__xla_cpu_runtime_ExpV4F32NEON' has C-linkage specified, but returns\n>> incomplete type 'xla::cpu::runtime::V4F32NEON' which could be incompatible\n>> with C [-Wreturn-type-c-linkage]\n>> xla::cpu::runtime::V4F32NEON __xla_cpu_runtime_ExpV4F32NEON(\n>> ^\n>> ./tensorflow/compiler/xla/service/cpu/cpu_runtime_neon.h:58:30: warning:\n>> '__xla_cpu_runtime_LogV4F32NEON' has C-linkage specified, but returns\n>> incomplete type 'xla::cpu::runtime::V4F32NEON' which could be incompatible\n>> with C [-Wreturn-type-c-linkage]\n>> xla::cpu::runtime::V4F32NEON __xla_cpu_runtime_LogV4F32NEON(\n>> ^\n>> tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:220:3: error:\n>> reinterpret_cast cannot resolve overloaded function 'acos' to type 'void\n>>\n>>\n>>\n>> *' REGISTER_LIBM_SYMBOL(acos); ^~~~~~~~~~~~~~~~~~~~~~~~~~\n>> tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:217:31: note:\n>> expanded from macro 'REGISTER_LIBM_SYMBOL' registry->Register(#name,\n>> reinterpret_cast<void*>(name));\n>> ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n>> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:714:1:\n>> note: candidate function\n>> acos(_A1 __lcpp_x) _NOEXCEPT {return ::acos((double)__lcpp_x);}\n>> ^\n>> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:708:46:\n>> note: candidate function\n>> inline _LIBCPP_INLINE_VISIBILITY long double acos(long double __lcpp_x)\n>> _NOEXCEPT {return ::acosl(__lcpp_x);}\n>> ^\n>> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:707:46:\n>> note: candidate function\n>> inline _LIBCPP_INLINE_VISIBILITY float acos(float __lcpp_x) _NOEXCEPT\n>> {return ::acosf(__lcpp_x);}\n>> ^\n>> /usr/include/math.h:323:15: note: candidate function\n>> extern double acos(double);\n>> ^\n>> tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:221:3: error:\n>> reinterpret_cast cannot resolve overloaded function 'acosh' to type 'void\n>>\n>>\n>>\n>> *' REGISTER_LIBM_SYMBOL(acosh); ^~~~~~~~~~~~~~~~~~~~~~~~~~~\n>> tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:217:31: note:\n>> expanded from macro 'REGISTER_LIBM_SYMBOL' registry->Register(#name,\n>> reinterpret_cast<void*>(name));\n>> ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n>> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:1005:1:\n>> note: candidate function\n>> acosh(_A1 __lcpp_x) _NOEXCEPT {return ::acosh((double)__lcpp_x);}\n>> ^\n>> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:1000:46:\n>> note: candidate function\n>> inline _LIBCPP_INLINE_VISIBILITY long double acosh(long double __lcpp_x)\n>> _NOEXCEPT {return ::acoshl(__lcpp_x);}\n>> ^\n>> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:999:46:\n>> note: candidate function\n>> inline _LIBCPP_INLINE_VISIBILITY float acosh(float __lcpp_x) _NOEXCEPT\n>> {return ::acoshf(__lcpp_x);}\n>> ^\n>> /usr/include/math.h:351:15: note: candidate function\n>> extern double acosh(double);\n>> ^\n>> tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:222:3: error:\n>> reinterpret_cast cannot resolve overloaded function 'asin' to type 'void\n>>\n>>\n>>\n>> *' REGISTER_LIBM_SYMBOL(asin); ^~~~~~~~~~~~~~~~~~~~~~~~~~\n>> tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:217:31: note:\n>> expanded from macro 'REGISTER_LIBM_SYMBOL' registry->Register(#name,\n>> reinterpret_cast<void*>(name));\n>> ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n>> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:726:1:\n>> note: candidate function\n>> asin(_A1 __lcpp_x) _NOEXCEPT {return ::asin((double)__lcpp_x);}\n>> ^\n>> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:720:46:\n>> note: candidate function\n>> inline _LIBCPP_INLINE_VISIBILITY long double asin(long double __lcpp_x)\n>> _NOEXCEPT {return ::asinl(__lcpp_x);}\n>> ^\n>> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:719:46:\n>> note: candidate function\n>> inline _LIBCPP_INLINE_VISIBILITY float asin(float __lcpp_x) _NOEXCEPT\n>> {return ::asinf(__lcpp_x);}\n>> ^\n>> /usr/include/math.h:327:15: note: candidate function\n>> extern double asin(double);\n>> ^\n>> tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:223:3: error:\n>> reinterpret_cast cannot resolve overloaded function 'asinh' to type 'void\n>>\n>>\n>>\n>> *' REGISTER_LIBM_SYMBOL(asinh); ^~~~~~~~~~~~~~~~~~~~~~~~~~~\n>> tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:217:31: note:\n>> expanded from macro 'REGISTER_LIBM_SYMBOL' registry->Register(#name,\n>> reinterpret_cast<void*>(name));\n>> ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n>> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:1017:1:\n>> note: candidate function\n>> asinh(_A1 __lcpp_x) _NOEXCEPT {return ::asinh((double)__lcpp_x);}\n>> ^\n>> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:1012:46:\n>> note: candidate function\n>> inline _LIBCPP_INLINE_VISIBILITY long double asinh(long double __lcpp_x)\n>> _NOEXCEPT {return ::asinhl(__lcpp_x);}\n>> ^\n>> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:1011:46:\n>> note: candidate function\n>> inline _LIBCPP_INLINE_VISIBILITY float asinh(float __lcpp_x) _NOEXCEPT\n>> {return ::asinhf(__lcpp_x);}\n>> ^\n>> /usr/include/math.h:355:15: note: candidate function\n>> extern double asinh(double);\n>> ^\n>> tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:224:3: error:\n>> reinterpret_cast cannot resolve overloaded function 'atan' to type 'void\n>>\n>>\n>>\n>> *' REGISTER_LIBM_SYMBOL(atan); ^~~~~~~~~~~~~~~~~~~~~~~~~~\n>> tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:217:31: note:\n>> expanded from macro 'REGISTER_LIBM_SYMBOL' registry->Register(#name,\n>> reinterpret_cast<void*>(name));\n>> ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n>> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:738:1:\n>> note: candidate function\n>> atan(_A1 __lcpp_x) _NOEXCEPT {return ::atan((double)__lcpp_x);}\n>> ^\n>> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:732:46:\n>> note: candidate function\n>> inline _LIBCPP_INLINE_VISIBILITY long double atan(long double __lcpp_x)\n>> _NOEXCEPT {return ::atanl(__lcpp_x);}\n>> ^\n>> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:731:46:\n>> note: candidate function\n>> inline _LIBCPP_INLINE_VISIBILITY float atan(float __lcpp_x) _NOEXCEPT\n>> {return ::atanf(__lcpp_x);}\n>> ^\n>> /usr/include/math.h:331:15: note: candidate function\n>> extern double atan(double);\n>> ^\n>> tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:225:3: error:\n>> reinterpret_cast cannot resolve overloaded function 'atan2' to type 'void\n>>\n>>\n>>\n>> *' REGISTER_LIBM_SYMBOL(atan2); ^~~~~~~~~~~~~~~~~~~~~~~~~~~\n>> tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:217:31: note:\n>> expanded from macro 'REGISTER_LIBM_SYMBOL' registry->Register(#name,\n>> reinterpret_cast<void*>(name));\n>> ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n>> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:755:1:\n>> note: candidate function\n>> atan2(_A1 __lcpp_y, _A2 __lcpp_x) _NOEXCEPT\n>> ^\n>> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:744:46:\n>> note: candidate function\n>> inline _LIBCPP_INLINE_VISIBILITY long double atan2(long double __lcpp_y,\n>> long double __lcpp_x) _NOEXCEPT {return ::atan2l(__lcpp_y, __lcpp_x);}\n>> ^\n>> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:743:46:\n>> note: candidate function\n>> inline _LIBCPP_INLINE_VISIBILITY float atan2(float __lcpp_y, float\n>> __lcpp_x) _NOEXCEPT {return ::atan2f(__lcpp_y, __lcpp_x);}\n>> ^\n>> /usr/include/math.h:335:15: note: candidate function\n>> extern double atan2(double, double);\n>> ^\n>> tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:226:3: error:\n>> reinterpret_cast cannot resolve overloaded function 'atanh' to type 'void\n>>\n>>\n>>\n>> *' REGISTER_LIBM_SYMBOL(atanh); ^~~~~~~~~~~~~~~~~~~~~~~~~~~\n>> tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:217:31: note:\n>> expanded from macro 'REGISTER_LIBM_SYMBOL' registry->Register(#name,\n>> reinterpret_cast<void*>(name));\n>> ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n>> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:1029:1:\n>> note: candidate function\n>> atanh(_A1 __lcpp_x) _NOEXCEPT {return ::atanh((double)__lcpp_x);}\n>> ^\n>> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:1024:46:\n>> note: candidate function\n>> inline _LIBCPP_INLINE_VISIBILITY long double atanh(long double __lcpp_x)\n>> _NOEXCEPT {return ::atanhl(__lcpp_x);}\n>> ^\n>> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:1023:46:\n>> note: candidate function\n>> inline _LIBCPP_INLINE_VISIBILITY float atanh(float __lcpp_x) _NOEXCEPT\n>> {return ::atanhf(__lcpp_x);}\n>> ^\n>> /usr/include/math.h:359:15: note: candidate function\n>> extern double atanh(double);\n>> ^\n>> tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:227:3: error:\n>> reinterpret_cast cannot resolve overloaded function 'cbrt' to type 'void\n>>\n>>\n>>\n>> *' REGISTER_LIBM_SYMBOL(cbrt); ^~~~~~~~~~~~~~~~~~~~~~~~~~\n>> tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:217:31: note:\n>> expanded from macro 'REGISTER_LIBM_SYMBOL' registry->Register(#name,\n>> reinterpret_cast<void*>(name));\n>> ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n>> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:1041:1:\n>> note: candidate function\n>> cbrt(_A1 __lcpp_x) _NOEXCEPT {return ::cbrt((double)__lcpp_x);}\n>> ^\n>> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:1036:46:\n>> note: candidate function\n>> inline _LIBCPP_INLINE_VISIBILITY long double cbrt(long double __lcpp_x)\n>> _NOEXCEPT {return ::cbrtl(__lcpp_x);}\n>> ^\n>> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:1035:46:\n>> note: candidate function\n>> inline _LIBCPP_INLINE_VISIBILITY float cbrt(float __lcpp_x) _NOEXCEPT\n>> {return ::cbrtf(__lcpp_x);}\n>> ^\n>> /usr/include/math.h:435:15: note: candidate function\n>> extern double cbrt(double);\n>> ^\n>> tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:228:3: error:\n>> reinterpret_cast cannot resolve overloaded function 'ceil' to type 'void\n>>\n>>\n>>\n>> *' REGISTER_LIBM_SYMBOL(ceil); ^~~~~~~~~~~~~~~~~~~~~~~~~~\n>> tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:217:31: note:\n>> expanded from macro 'REGISTER_LIBM_SYMBOL' registry->Register(#name,\n>> reinterpret_cast<void*>(name));\n>> ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n>> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:773:1:\n>> note: candidate function\n>> ceil(_A1 __lcpp_x) _NOEXCEPT {return ::ceil((double)__lcpp_x);}\n>> ^\n>> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:767:46:\n>> note: candidate function\n>> inline _LIBCPP_INLINE_VISIBILITY long double ceil(long double __lcpp_x)\n>> _NOEXCEPT {return ::ceill(__lcpp_x);}\n>> ^\n>> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:766:46:\n>> note: candidate function\n>> inline _LIBCPP_INLINE_VISIBILITY float ceil(float __lcpp_x) _NOEXCEPT\n>> {return ::ceilf(__lcpp_x);}\n>> ^\n>> /usr/include/math.h:470:15: note: candidate function\n>> extern double ceil(double);\n>> ^\n>> tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:229:3: error:\n>> reinterpret_cast cannot resolve overloaded function 'copysign' to type\n>> 'void\n>>\n>>\n>>\n>> *' REGISTER_LIBM_SYMBOL(copysign); ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n>> tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:217:31: note:\n>> expanded from macro 'REGISTER_LIBM_SYMBOL' registry->Register(#name,\n>> reinterpret_cast<void*>(name));\n>> ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n>> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:1065:1:\n>> note: candidate function\n>> copysign(_A1 __lcpp_x, _A2 __lcpp_y) _NOEXCEPT\n>> ^\n>> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:1052:1:\n>> note: candidate function\n>> copysign(long double __lcpp_x, long double __lcpp_y) _NOEXCEPT {\n>> ^\n>> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:1047:40:\n>> note: candidate function\n>> inline _LIBCPP_INLINE_VISIBILITY float copysign(float __lcpp_x,\n>> ^\n>> /usr/include/math.h:526:15: note: candidate function\n>> extern double copysign(double, double);\n>> ^\n>> tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:230:3: error:\n>> reinterpret_cast cannot resolve overloaded function 'cos' to type 'void\n>>\n>>\n>>\n>> *' REGISTER_LIBM_SYMBOL(cos); ^~~~~~~~~~~~~~~~~~~~~~~~~\n>> tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:217:31: note:\n>> expanded from macro 'REGISTER_LIBM_SYMBOL' registry->Register(#name,\n>> reinterpret_cast<void*>(name));\n>> ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n>> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:785:1:\n>> note: candidate function\n>> cos(_A1 __lcpp_x) _NOEXCEPT {return ::cos((double)__lcpp_x);}\n>> ^\n>> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:779:46:\n>> note: candidate function\n>> inline _LIBCPP_INLINE_VISIBILITY long double cos(long double __lcpp_x)\n>> _NOEXCEPT {return ::cosl(__lcpp_x);}\n>> ^\n>> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:778:46:\n>> note: candidate function\n>> inline _LIBCPP_INLINE_VISIBILITY float cos(float __lcpp_x) _NOEXCEPT\n>> {return ::cosf(__lcpp_x);}\n>> ^\n>> /usr/include/math.h:339:15: note: candidate function\n>> extern double cos(double);\n>> ^\n>> tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:231:3: error:\n>> reinterpret_cast cannot resolve overloaded function 'cosh' to type 'void\n>>\n>>\n>>\n>> *' REGISTER_LIBM_SYMBOL(cosh); ^~~~~~~~~~~~~~~~~~~~~~~~~~\n>> tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:217:31: note:\n>> expanded from macro 'REGISTER_LIBM_SYMBOL' registry->Register(#name,\n>> reinterpret_cast<void*>(name));\n>> ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n>> /Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:797:1:\n>> note: candidate function\n>> cosh(_A......\n>>\n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/13220#issuecomment-339743486>,\n>> or mute the thread\n>> <https://github.com/notifications/unsubscribe-auth/AJsp2c5mtVyiVMIkhka_5rDfu7T4iWqkks5swMXOgaJpZM4Pf1KK>\n>> .\n>>\n>\n>\n", "I tried to add \r\n\r\n```c\r\n#ifndef _MACH_PORT_T\r\n#define _MACH_PORT_T\r\n#include <sys/_types.h> /* __darwin_mach_port_t */\r\ntypedef __darwin_mach_port_t mach_port_t;\r\n#include <pthread.h>\r\nmach_port_t pthread_mach_thread_np(pthread_t);\r\n#endif /* _MACH_PORT_T */\r\n```\r\n\r\nto the top of `/Library/Developer/CommandLineTools/usr/include/c++/v1/__threading_support` but still got \r\n\r\n```\r\nERROR: /private/var/tmp/_bazel_zhiyzuo/c6ab565d8235ea2aeef2da37c98dc58b/external/nsync/BUILD:397:1: C++ compilation of rule '@nsync//:nsync_cpp' failed (Exit 1).\r\nIn file included from external/nsync/internal/note.c:16:\r\nIn file included from ./external/nsync//platform/c++11/platform.h:29:\r\nIn file included from /Library/Developer/CommandLineTools/usr/include/c++/v1/mutex:189:\r\nIn file included from /Library/Developer/CommandLineTools/usr/include/c++/v1/__mutex_base:17:\r\n/Library/Developer/CommandLineTools/usr/include/c++/v1/__threading_support:15:36: error: unknown type name 'pthread_t'\r\nmach_port_t pthread_mach_thread_np(pthread_t);\r\n                                   ^\r\n1 error generated.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```\r\n\r\nAny advice? -Also a CPU only config.", "zhiyzuo asked:\r\n> Any advice?\r\n\r\nTry the most recent version of nsync.\r\n(I can tell that you're not using the version that has the attempted fix\r\nbecause of the line number reported in the error message.)\r\n", "Thanks for the advice @m3bm3b ! Is that the one in https://github.com/google/nsync? I am not sure if I am doing it right: \r\nI first ran `mkmakefile.sh` and it said `x86_64.macos.clang` already existed so I ran `make` in that folder. No errors pop up. Then I tried installing tensorflow v.1.4.0rc1 again but got the same error. I must be missing something!", "Try the head version of tensorflow.\n\nOn 29 October 2017 at 06:55, Zhiya Zuo <notifications@github.com> wrote:\n\n> Thanks for the advice @m3bm3b <https://github.com/m3bm3b> ! Is that the\n> one in https://github.com/google/nsync? I am not sure if I am doing it\n> right:\n> I first ran mkmakefile.sh and it said x86_64.macos.clang already existed\n> so I ran make in that folder. No errors pop up. Then I tried installing\n> tensorflow v.1.4.0rc1 again but got the same error. I must be missing\n> something!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13220#issuecomment-340263868>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AJsp2VuyKz5nEu_uZI4a6f4ElmkLoJ23ks5sxIPtgaJpZM4Pf1KK>\n> .\n>\n", "I later tried again with `v1.4.0rc1` and it worked. One extra thing I did was installing `grpc` due to some error when building from source: https://github.com/grpc/homebrew-grpc\r\n\r\nThank you so much!\r\n\r\nSum up:\r\n\r\n1. Add the following to `/Library/Developer/CommandLineTools/usr/include/c++/v1/__threading_support`\r\n\r\n```C\r\n#ifndef _MACH_PORT_T\r\n#define _MACH_PORT_T\r\n#include <sys/_types.h> /* __darwin_mach_port_t */\r\ntypedef __darwin_mach_port_t mach_port_t;\r\n#include <pthread.h>\r\nmach_port_t pthread_mach_thread_np(pthread_t);\r\n#endif /* _MACH_PORT_T */\r\n```\r\n\r\n2. Get the most recent `nsync`: https://github.com/google/nsync\r\n\r\n3. Installing `grpc` using `brew`: https://github.com/grpc/homebrew-grpc  ", "Are you saying that step 1 is essential, even if you do everything else.\r\nI ask because, of course, the intent of the new version of nsync was that you should not need to do step 1.\r\n", "I forgot about that and just tried it again without modifying `__threading_support`: it gave me such error with a warning: (Am I not installing `nsync` right: is `make` enough for installation?)\r\n\r\n>WARNING: /Users/zhiyzuo/Documents/tensorflow/tensorflow/core/BUILD:1781:1: in includes attribute of cc_library rule //tensorflow/core:framework_headers_lib: '../../external/nsync/public' resolves to 'external/nsync/public' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /Users/zhiyzuo/Documents/tensorflow/tensorflow/tensorflow.bzl:1044:30.\r\n---\r\n>ERROR: \r\n/private/var/tmp/_bazel_zhiyzuo/c6ab565d8235ea2aeef2da37c98dc58b/external/nsync/BUILD:397:1: C++ compilation of rule '@nsync//:nsync_cpp' failed (Exit 1).\r\nIn file included from external/nsync/internal/sem_wait.c:16:\r\nIn file included from ./external/nsync//platform/c++11/platform.h:29:\r\nIn file included from /Library/Developer/CommandLineTools/usr/include/c++/v1/mutex:189:\r\nIn file included from /Library/Developer/CommandLineTools/usr/include/c++/v1/__mutex_base:17:\r\n/Library/Developer/CommandLineTools/usr/include/c++/v1/__threading_support:156:1: error: unknown type name 'mach_port_t'\r\nmach_port_t __libcpp_thread_get_port();\r\n^\r\n/Library/Developer/CommandLineTools/usr/include/c++/v1/__threading_support:300:1: error: unknown type name 'mach_port_t'\r\nmach_port_t __libcpp_thread_get_port() {\r\n^\r\n/Library/Developer/CommandLineTools/usr/include/c++/v1/__threading_support:301:12: error: use of undeclared identifier 'pthread_mach_thread_np'\r\n    return pthread_mach_thread_np(pthread_self());\r\n           ^\r\n3 errors generated.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.", "> Am I not installing nsync right: is make enough for installation?\r\n\r\nYou're doing a bazel build; there should be no need to invoke make on nsync.\r\nIf you invoke bazel in the tensorflow tree, bazel will fetch and build the version of the nsync package\r\nassociated with the version of the tensorflow package that you've downloaded.\r\n\r\nThe problem is that you're using a version of tensorflow that's using an older version of nsync.\r\nAs I said earlier, this is clear from the line number in the error message you quote.\r\nNotice the \"397\" in the error message: that line number can only have been \r\nproduced by an erlier version of nsync.\r\n\r\nAs I recommended earlier, use the head version of tensorflow.\r\nIf you instead wish to use  v1.4.0-rc1, you must tell bazel to\r\nuse the most recent version of nsync.  To do that,\r\nin tensorflow's workspace.bzl (at line 426 in v1.4.0-rc1)\r\nmodify the stanza that fetches nsync to read as follows:\r\n\r\n    native.http_archive(\r\n      name = \"nsync\",\r\n      urls = [\r\n          \"https://mirror.bazel.build/github.com/google/nsync/archive/839fcc53ff9be58218ed55397deb3f8376a1444e.tar.gz\",\r\n          # \"https://github.com/google/nsync/archive/839fcc53ff9be58218ed55397deb3f8376a1444e.tar.gz\",\r\n      ],\r\n      sha256 = \"124d105edb0313ef2d7f5bb86ec94d9f8de95479e55641c4254ffa8f795e9b37\",\r\n      strip_prefix = \"nsync-839fcc53ff9be58218ed55397deb3f8376a1444e\",\r\n  )\r\n\r\n\r\n----\r\n\r\nThe warning you quote about the cc_header_only_library rule is harmless.\r\nIt represents an unfortunate interaction between tensorflow and bazel.\r\nIt occurs on all platforms, and is not peculiar to the Mac.  \r\nTensorFlow attempts to define a rule that exports just the headers from a library, \r\nbut bazel makes it difficult to construct the include path for such a \r\ncustom rule, based on its dependencies.\r\nIt is unclear how this will be resolved as bazel evolves, but for the moment\r\nthings work, with a warning message.\r\n", "too much build hole!  I build TF1.3 succeed with bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package.  then I makefile with tensorflow/contrib/makefile/build_all_ios.sh is succeed.finally,run bazel build command secondly. build failed\uff01\r\nERROR: /Users/leon/GitHub/tensorflow/tensorflow/contrib/BUILD:137:12: Label '//tensorflow/contrib:makefile/downloads/nsync/bazel/pkg_path_name.bzl' crosses boundary of subpackage 'tensorflow/contrib/makefile/downloads/nsync/bazel' (perhaps you meant to put the colon here: '//tensorflow/contrib/makefile/downloads/nsync/bazel:pkg_path_name.bzl'?).\r\nERROR: /Users/leon/GitHub/tensorflow/tensorflow/contrib/BUILD:137:12: Label '//tensorflow/contrib:makefile/downloads/nsync/bazel/BUILD.bazel' crosses boundary of subpackage 'tensorflow/contrib/makefile/downloads/nsync/bazel' (perhaps you meant to put the colon here: '//tensorflow/contrib/makefile/downloads/nsync/bazel:BUILD.bazel'?).\r\nERROR: /Users/leon/GitHub/tensorflow/tensorflow/python/BUILD:38:1: Target '//tensorflow/contrib:contrib_py' contains an error and its package is in error and referenced by '//tensorflow/python:python'.\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.\r\n", "That's a very different issue, but I hope I have fixed it at head.\r\nI still do not know whether the original issue on Sierra is fixed, because no one has reported\r\neither failure or success at head since I attempted to fix it at head.", "another solution , use python3 on Mac.  built it succeed! ", "I can confirm have successfully built TensorFlow on Sierra from the head with vanilla (cpu) only config. Issues remain with XLA and tensorboard. ", "On the XLA issues, you should configure without XLA on MacOS.   \r\nI suspect that XLA won't have support on MacOS for a while yet.\r\n(That is, the people who support XLA might accept fixes, but I doubt they'll be\r\nworking on MacOS themselves for a while.)\r\n\r\nOn the tensorboard issue, you've probably reported that in its own project.\r\n\r\nI'm going to close this issue, since the nsync-related issues reported originally\r\nseem to be resolved.\r\n", "Thanks for the heads up. So much for targeting all devices I might go down a different road for my apps.\n\nSimon Beaumont\n\nOn 9 Nov 2017, at 21:11, Mike Burrows <notifications@github.com> wrote:\n\nOn the XLA issues, you should configure without XLA on MacOS.\nI suspect that XLA won't have support on MacOS for a while yet.\n(That is, the people who support XLA might accept fixes, but I doubt they'll be\nworking on MacOS themselves for a while.)\n\nOn the tensorboard issue, you've probably reported that in its own project.\n\nI'm going to close this issue, since the nsync-related issues reported originally\nseem to be resolved.\n\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n\n", "@Kangbababa excuse me\uff0chow do you solve the following problem?\r\n\r\n\u201cERROR: /Users/leon/GitHub/tensorflow/tensorflow/contrib/BUILD:137:12: Label '//tensorflow/contrib:makefile/downloads/nsync/bazel/pkg_path_name.bzl' crosses boundary of subpackage 'tensorflow/contrib/makefile/downloads/nsync/bazel' (perhaps you meant to put the colon here: '//tensorflow/contrib/makefile/downloads/nsync/bazel:pkg_path_name.bzl'?).\r\nERROR: /Users/leon/GitHub/tensorflow/tensorflow/contrib/BUILD:137:12: Label '//tensorflow/contrib:makefile/downloads/nsync/bazel/BUILD.bazel' crosses boundary of subpackage 'tensorflow/contrib/makefile/downloads/nsync/bazel' (perhaps you meant to put the colon here: '//tensorflow/contrib/makefile/downloads/nsync/bazel:BUILD.bazel'?).\r\nERROR: /Users/leon/GitHub/tensorflow/tensorflow/python/BUILD:38:1: Target '//tensorflow/contrib:contrib_py' contains an error and its package is in error and referenced by '//tensorflow/python:python'.\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.\u201d", "@jakajacky ,I don't solve this question.\r\nwith another way, I compiled it with python3  environment  instead of python2 environment.\r\n", "@Kangbababa thanks for your reply, we have just solved this problem with #15445, although we do not know the root cause. ", "> \u201cERROR: /Users/leon/GitHub/tensorflow/tensorflow/contrib/BUILD:137:12: Label '//tensorflow/contrib:makefile/downloads/nsync/bazel/pkg_path_name.bzl' crosses boundary of subpackage 'tensorflow/contrib/makefile/downloads/nsync/bazel' (perhaps you meant to put the colon here: '//tensorflow/contrib/makefile/downloads/nsync/bazel:pkg_path_name.bzl'?).\r\n\r\nI would have expected that the issue that caused that particular error message \r\nwould have been fixed at TensorFlow's head (and nsync's head) in early November.\r\n\r\n\r\n>  we do not know the root cause.\r\n\r\nIf I'm understanding it correctly, I believe that the sequence of events is as follows:\r\n\r\n1) An invocation of the makefile build downloaded\r\n     various packages into a directory within the tensorflow source tree.\r\n     Those packages may contain bazel BUILD files because some of those\r\n     packages may be buildable independently with bazel.  \r\n\r\n2) An invocation of bazel (within the same tree that had previously\r\n    used the makefile build) hit a recursive glob(),\r\n    such as the one in the tensorflow/contrib BUILD file.  This forced\r\n    bazel to notice any newly downloaded BUILD files, and parse them\r\n    in their new, unexpected context.\r\n\r\nThis was first fixed in January:   the download script was modified to delete the \r\ndownloaded \"BUILD\" files.  \r\nAlas, bazel also recognizes the name \"BUILD.bazel\" \r\nas a BUILD file (though it's hard to find that fact in bazel's documentation), \r\nand the \"BUILD.bazel\" files were not deleted.   This caused a conflict\r\nwhen nsync was introduced.   That conflict was fixed in early November.\r\n\r\n", "> I'm getting a cpu-only Bazel build failure on OSX. TensorFlow has already been .configure'd with default options.\r\n> \r\n> Message below.\r\n> \r\n> ```\r\n> $ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\n> WARNING: /Users/kevin/projects/tensorflow/tensorflow/core/BUILD:1653:1: in includes attribute of cc_library rule //tensorflow/core:framework_headers_lib: '../../external/nsync/public' resolves to 'external/nsync/public' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /Users/kevin/projects/tensorflow/tensorflow/tensorflow.bzl:913:30.\r\n> WARNING: /Users/kevin/projects/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.\r\n> WARNING: /Users/kevin/projects/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.\r\n> INFO: Found 1 target...\r\n> ERROR: /private/var/tmp/_bazel_kevin/1cea9193b5038a270f5e322e515767d8/external/nsync/BUILD:397:1: C++ compilation of rule '@nsync//:nsync_cpp' failed (Exit 1): cc_wrapper.sh failed: error executing command\r\n>   (cd /private/var/tmp/_bazel_kevin/1cea9193b5038a270f5e322e515767d8/execroot/org_tensorflow && \\\r\n>   exec env - \\\r\n>     PATH=/anaconda/bin:/Users/kevin/bin:/usr/local/Cellar/coreutils/8.28/libexec/gnubin:/usr/local/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin \\\r\n>     PWD=/proc/self/cwd \\\r\n>     PYTHON_BIN_PATH=/anaconda/bin/python \\\r\n>     PYTHON_LIB_PATH=/anaconda/lib/python3.5/site-packages \\\r\n>     TF_NEED_CUDA=0 \\\r\n>     TF_NEED_OPENCL=0 \\\r\n>     TMPDIR=/var/folders/g9/fdv74qn92qs7yw31tlj4q0lw0000gn/T/ \\\r\n>   external/local_config_cc/cc_wrapper.sh -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-march=native' -MD -MF bazel-out/local-py3-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/time_internal.pic.d -fPIC -iquote external/nsync -iquote bazel-out/local-py3-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/local-py3-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/local-py3-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsync//internal -I./external/nsync//platform/posix '-D_POSIX_C_SOURCE=200809L' -pthread -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c external/nsync/internal/time_internal.c -o bazel-out/local-py3-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/time_internal.pic.o).\r\n> In file included from external/nsync/internal/time_internal.c:16:\r\n> In file included from ./external/nsync//platform/c++11/platform.h:29:\r\n> In file included from /Library/Developer/CommandLineTools/usr/include/c++/v1/mutex:189:\r\n> In file included from /Library/Developer/CommandLineTools/usr/include/c++/v1/__mutex_base:17:\r\n> /Library/Developer/CommandLineTools/usr/include/c++/v1/__threading_support:156:1: error: unknown type name 'mach_port_t'\r\n> mach_port_t __libcpp_thread_get_port();\r\n> ^\r\n> /Library/Developer/CommandLineTools/usr/include/c++/v1/__threading_support:300:1: error: unknown type name 'mach_port_t'\r\n> mach_port_t __libcpp_thread_get_port() {\r\n> ^\r\n> /Library/Developer/CommandLineTools/usr/include/c++/v1/__threading_support:301:12: error: use of undeclared identifier 'pthread_mach_thread_np'\r\n>     return pthread_mach_thread_np(pthread_self());\r\n>            ^\r\n> 3 errors generated.\r\n> Target //tensorflow/tools/pip_package:build_pip_package failed to build\r\n> INFO: Elapsed time: 0.830s, Critical Path: 0.26s\r\n> ```\r\n> \r\n> **TensorFlow commit:** [e9d5ee1](https://github.com/tensorflow/tensorflow/commit/e9d5ee1ebffba25cef65f1f354b9e4ca9bcea10c)\r\n> \r\n> **Mac OSX Sierra 10.12.6 (16G29)**\r\n> **Command Line Tools:**\r\n> \r\n> ```\r\n> $ pkgutil --pkg-info=com.apple.pkg.CLTools_Executables\r\n> package-id: com.apple.pkg.CLTools_Executables\r\n> version: 9.0.0.0.1.1504363082\r\n> volume: /\r\n> location: /\r\n> install-time: 1505966986\r\n> groups: com.apple.FindSystemFiles.pkg-group\r\n> ```\r\n> \r\n> **Environment Capture Script:**\r\n> \r\n> ```\r\n> $ cat tf_env.txt\r\n> \r\n> == cat /etc/issue ===============================================\r\n> Darwin MacBook-Pro.localdomain 16.7.0 Darwin Kernel Version 16.7.0: Thu Jun 15 17:36:27 PDT 2017; root:xnu-3789.70.16~2/RELEASE_X86_64 x86_64\r\n> Mac OS X 10.12.6\r\n> \r\n> == are we in docker =============================================\r\n> No\r\n> \r\n> == compiler =====================================================\r\n> Apple LLVM version 9.0.0 (clang-900.0.37)\r\n> Target: x86_64-apple-darwin16.7.0\r\n> Thread model: posix\r\n> InstalledDir: /Library/Developer/CommandLineTools/usr/bin\r\n> \r\n> == uname -a =====================================================\r\n> Darwin MacBook-Pro.localdomain 16.7.0 Darwin Kernel Version 16.7.0: Thu Jun 15 17:36:27 PDT 2017; root:xnu-3789.70.16~2/RELEASE_X86_64 x86_64\r\n> \r\n> == check pips ===================================================\r\n> numpy (1.13.1)\r\n> numpydoc (0.7.0)\r\n> protobuf (3.4.0)\r\n> tensorflow (1.3.0)\r\n> tensorflow-tensorboard (0.1.6)\r\n> \r\n> == check for virtualenv =========================================\r\n> False\r\n> \r\n> == tensorflow import ============================================\r\n> tf.VERSION = 1.3.0\r\n> tf.GIT_VERSION = v1.3.0-rc2-20-g0787eee\r\n> tf.COMPILER_VERSION = v1.3.0-rc2-20-g0787eee\r\n> Sanity check: array([1], dtype=int32)\r\n> Traceback (most recent call last):\r\n>   File \"/Users/kevin/projects/tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 48, in <module>\r\n>     from tensorflow.python.pywrap_tensorflow_internal import *\r\n> ImportError: No module named 'tensorflow.python.pywrap_tensorflow_internal'\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"<string>\", line 1, in <module>\r\n>   File \"/Users/kevin/projects/tensorflow/tensorflow/__init__.py\", line 24, in <module>\r\n>     from tensorflow.python import *\r\n>   File \"/Users/kevin/projects/tensorflow/tensorflow/python/__init__.py\", line 49, in <module>\r\n>     from tensorflow.python import pywrap_tensorflow\r\n>   File \"/Users/kevin/projects/tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 59, in <module>\r\n>     raise ImportError(msg)\r\n> ImportError: Traceback (most recent call last):\r\n>   File \"/Users/kevin/projects/tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 48, in <module>\r\n>     from tensorflow.python.pywrap_tensorflow_internal import *\r\n> ImportError: No module named 'tensorflow.python.pywrap_tensorflow_internal'\r\n> \r\n> \r\n> Failed to load the native TensorFlow runtime.\r\n> \r\n> See https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n> \r\n> for some common reasons and solutions.  Include the entire stack trace\r\n> above this error message when asking for help.\r\n> \r\n> == env ==========================================================\r\n> LD_LIBRARY_PATH is unset\r\n> DYLD_LIBRARY_PATH is unset\r\n> \r\n> == nvidia-smi ===================================================\r\n> tools/tf_env_collect.sh: line 105: nvidia-smi: command not found\r\n> \r\n> == cuda libs  ===================================================\r\n> ```\r\n> \r\n> **Bazel Information:**\r\n> \r\n> ```\r\n> $ bazel info\r\n> bazel-bin: /private/var/tmp/_bazel_kevin/1cea9193b5038a270f5e322e515767d8/execroot/org_tensorflow/bazel-out/local-py3-opt/bin\r\n> bazel-genfiles: /private/var/tmp/_bazel_kevin/1cea9193b5038a270f5e322e515767d8/execroot/org_tensorflow/bazel-out/local-py3-opt/genfiles\r\n> bazel-testlogs: /private/var/tmp/_bazel_kevin/1cea9193b5038a270f5e322e515767d8/execroot/org_tensorflow/bazel-out/local-py3-opt/testlogs\r\n> character-encoding: file.encoding = ISO-8859-1, defaultCharset = ISO-8859-1\r\n> command_log: /private/var/tmp/_bazel_kevin/1cea9193b5038a270f5e322e515767d8/command.log\r\n> committed-heap-size: 1180MB\r\n> execution_root: /private/var/tmp/_bazel_kevin/1cea9193b5038a270f5e322e515767d8/execroot/org_tensorflow\r\n> gc-count: 15\r\n> gc-time: 1146ms\r\n> install_base: /var/tmp/_bazel_kevin/install/ed43083c802b447b0d9313e2450af83f\r\n> java-home: /Library/Java/JavaVirtualMachines/jdk1.8.0_144.jdk/Contents/Home/jre\r\n> java-runtime: Java(TM) SE Runtime Environment (build 1.8.0_144-b01) by Oracle Corporation\r\n> java-vm: Java HotSpot(TM) 64-Bit Server VM (build 25.144-b01, mixed mode) by Oracle Corporation\r\n> max-heap-size: 3817MB\r\n> message_log: /private/var/tmp/_bazel_kevin/1cea9193b5038a270f5e322e515767d8/message.log\r\n> output_base: /private/var/tmp/_bazel_kevin/1cea9193b5038a270f5e322e515767d8\r\n> output_path: /private/var/tmp/_bazel_kevin/1cea9193b5038a270f5e322e515767d8/execroot/tensorflow/bazel-out\r\n> package_path: %workspace%\r\n> release: release 0.5.4-homebrew\r\n> server_pid: 37949\r\n> used-heap-size: 394MB\r\n> workspace: /Users/kevin/projects/tensorflow\r\n> ```\r\n\r\nBuild error using CMake at macOS Sierra #375"]}, {"number": 13219, "title": "Remove github mirror links for all TF workspace dependencies.", "body": "Only use bazel mirror for github mirrors.\r\nFixes #12979", "comments": ["Jenkins, test this please\r\n", "@andrewharp @petewarden the makefile issues, are they flakes or do I need to fix something for those?", "@gunan There's a hack in tensorflow/contrib/makefile/download_dependencies.sh to grep the workspace.bzl file for github links, so it will need to be updated to use the alternate mirrors for all the archives.", "Jenkins, test this please.", "Jenkins, test this please.", "Passing makefile run:\r\nhttp://ci.tensorflow.org/job/tensorflow-pull-requests-makefile/10751/"]}, {"number": 13218, "title": "update protobuf archive checksum", "body": "Fix\r\n#12979 ", "comments": ["Can one of the admins verify this patch?", "Drop the pull request."]}, {"number": 13217, "title": "Fixes #12401: Use traceback instead of inspect for frame information", "body": "#12401 - import tensorflow.contrib.layers takes a very long time\r\n\r\nI learned (while rebasing this diff) that the issue of slow imports of tensorflow.contrib.layers was fixed earlier this week: https://github.com/tensorflow/tensorflow/pull/11830. \ud83c\udf89 Since the diff is ready,  I want to put up the diff for the sake of history, but I understand if at this point the further speedup is minor enough to ignore.\r\n\r\nThe speedup is a factor of 2, while the original fix is a factor of 100.   \r\n\r\n----\r\n\r\nMy performance tests compared three versions:\r\n- original implementation with inspect.stack()\r\n- traceback(limit)\r\n- current_frame + inspect\r\n\r\nI also found that the original implementation was sensitive to the depth of the current stack, and a file importing another file generally increased that call depth by 5.  Which is to say, in practice, my code importing `tensorflow.config` at an import depth of 7 had much worse performance than the baseline test of just importing `tensorflow.config`\r\n\r\n\r\n| Test  | Timing |\r\n| -- | -- |\r\n| python2 - inspect.stack |  7.8 sec |\r\n| python2 - inspect.getframeinfo  |  0.12 sec   |\r\n| python2 - traceback.extract_stack  |  0.04 sec   |\r\n| python3 - inspect.stack |  22.8 sec |\r\n| python3 - inspect.getframeinfo  |  0.14 sec   |\r\n| python3 - traceback.extract_stack  |  0.07 sec   |\r\n\r\n^ tests are the total of 1000 runs with a stack of 200, to give easy to compare numbers.\r\nhttps://gist.github.com/JettJones/c236494013f22723c1822126df944b12\r\n\r\nThis functionality of adding a decorator name from the call stack is covered by the existing unit test:\r\n* testSetsDecoratorNameToFunctionThatCallsMakeDecoratorIfAbsent\r\n\r\nAnd pylint did not report any issues on these changes\r\n", "comments": ["@JettJones, thanks for your PR! By analyzing the history of the files in this pull request, we identified @sguada, @charlesnicholson and @martinwicke to be potential reviewers.", "Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "`I signed it!`  - My company has signed the CLA, and my email address used on the commit is a member of the google group that we specified.  This will be my first time going through the process, so please let me know if some part of that is incorrect.  Thanks!", "@charlesnicholson mind taking a look?", "Jenkins, test this please.", "I don't see your username in our CLA database, presumably because it doesn't crawl all under the company. @gunan please advise.", "@JettJones If you are covered under corporate CLA, you need to:\r\n-Add your corporate email to your github account as a secondary email\r\n-Use your corporate email\r\n-Comment this, and only this to retrigger CLA check: \"I signed it!\"", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please.", "the only failure is `nn_grad_test`, sounds like the test is flaky:\r\n\r\n`Expected: (max_error) < (2e-4), actual: 0.000205815 vs 0.0002`", "Correct, nn_grad_test issues are addressed internally, next push should fix the test.", "@josh11b @drpngx change passing all tests and is ready for review.", "I'm good with the changes, perhaps @charlesnicholson should take a look.", "Pending resolution of the comment I left, this looks good. (I think it's vestigial but if not would indicate behavior regression)", "This looks great, thanks for listening to my nits :)\r\n\r\n@drpngx I think this should be merged ASAP given the speedup. Thanks again for taking the time to write + fix.", "Jenkins, test this please.", "Not clear if it's a transient failure. Jenkins, test this please.", "Jenkins, test this please.", "@caisq Ready to merge.", "@drpngx Will merge when sync'ing to internal is done."]}, {"number": 13216, "title": "[feature request] set some dimension in row_shape to the maximum in each batch in tf.contrib.data.Dataset.dense_to_sparse_batch", "body": "The docstring says:\r\n\r\n**row_shape:** A `tf.TensorShape` or `tf.int64` vector tensor-like\r\n    object representing the equivalent dense shape of a row in the\r\n    resulting `tf.SparseTensor`. Each element of this dataset must\r\n    have the same rank as `row_shape`, and must have size less\r\n    than or equal to `row_shape` in each dimension.\r\n\r\nThe row_shape must be the same for every batch, so if the elements are sequences it's necessary to know the maximum length beforehand. It would be nice if it was possible to set some dimension of row_shape to -1, meaning that this dimension will be the maximum for each batch (just like padded_batch).", "comments": ["Sounds reasonable to me! This would make a great first contribution to the Dataset API, so I'm going to mark this as contributions welcome.", "Hi, @mrry. I'm interested in the issue. Do you have any advice for me?", "Sure, have a look at how the equivalent feature is [implemented in `PaddedBatchDatasetOp`](https://github.com/tensorflow/tensorflow/blob/a6f856b2f7920d4f74d7ca4e71967258423cc9f0/tensorflow/core/kernels/padded_batch_dataset_op.cc#L308). It should be pretty easy to translate this to where the `row_shape` is [calculated in `DenseToSparseBatchDatasetOp`](https://github.com/tensorflow/tensorflow/blob/a6f856b2f7920d4f74d7ca4e71967258423cc9f0/tensorflow/core/kernels/dense_to_sparse_batch_dataset_op.cc#L222). Most of the work would be in adding appropriate tests!", "Thanks for you detailed guide, @mrry. I will take a try, and let you know if I get in trouble.", "@facaiy What is the process of the PR now? Do you need help to pass all tests?", "It's merged now and should be available in TF 1.4.\r\n"]}, {"number": 13215, "title": "how to compute out product with tf", "body": "how to compute out product with tf", "comments": ["please ask this on stackoverflow, this is for bugs :)"]}, {"number": 13214, "title": "Tensorflow 1.3.0 not buildable because bazel fails to download protobuf", "body": "> ____Downloading https://github.com/google/protobuf/archive/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz via codeload.github.com: 2,859,576 bytes\r\n> ____Downloading https://github.com/google/protobuf/archive/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz via codeload.github.com: 3,046,594 bytes\r\n> ____Downloading https://github.com/google/protobuf/archive/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz via codeload.github.com: 3,235,206 bytes\r\n> ____Downloading https://github.com/google/protobuf/archive/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz via codeload.github.com: 3,422,448 bytes\r\n> ____Downloading https://github.com/google/protobuf/archive/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz via codeload.github.com: 3,609,690 bytes\r\n> ____Downloading https://github.com/google/protobuf/archive/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz via codeload.github.com: 3,796,932 bytes\r\n> ____Downloading https://github.com/google/protobuf/archive/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz via codeload.github.com: 3,985,320 bytes\r\n> ____Downloading https://github.com/google/protobuf/archive/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz via codeload.github.com: 4,172,562 bytes\r\n> ____Downloading https://github.com/google/protobuf/archive/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz via codeload.github.com: 4,359,804 bytes\r\n> ERROR: /build/python-tensorflow-cuda-1.3.0/tensorflow/tools/pip_package/BUILD:100:1: no such package '@protobuf//': java.io.IOException: Error downloading [https://github.com/google/protobuf/archive/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz, http://mirror.bazel.build/github.com/google/protobuf/archive/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz] to /build/python-tensorflow-cuda-1.3.0/.cache/bazel/_bazel_pbuilder/f9c4bbbece8e6d872cda536e5e92a13c/external/protobuf/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz: Checksum was e5fdeee6b28cf6c38d61243adff06628baa434a22b5ebb7432d2a7fbabbdb13d but wanted 6d43b9d223ce09e5d4ce8b0060cb8a7513577a35a64c7e3dad10f0703bf3ad93 and referenced by '//tensorflow/tools/pip_package:licenses'.\r\n> ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.\r\n> ____Elapsed time: 12.129s\r\n\r\nIt's not a local issue, reproduced several time with different Internet connection....\r\n\r\nThanks", "comments": ["This is a duplicate of #12979. Please see that issue for more information."]}, {"number": 13213, "title": "GRPC causes training to pause in individual worker (distributed tensorflow, synchronised)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian GNU/Linux 8.9 (jessie)\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.2.0-5-g435cdfc 1.2.1\r\n- **Python version**: 3.6.2\r\n- **CUDA/cuDNN version**: cuda-8.0 / cudnn-5.1.5\r\n- **GPU model and memory**: GeForce GTX Titan X, 12 GB\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n\r\nThe distributed synchronized ( between graph replication, 4 workers, 3 ps ) training works fine until one of the ps tasks reports following error. After that, one of the worker processes just stops, and the rest of the workers may also stop later with same error. \r\n\r\n   ```\r\n 2017-09-21 16:45:55.606842: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2000, 1 -> localhost:2001, 2 -> localhost:2002}\r\n    2017-09-21 16:45:55.606877: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2003, 1 -> localhost:2004, 2 -> localhost:2005, 3 -> localhost:2006}\r\n    2017-09-21 16:45:55.608066: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:2002\r\n    E0921 16:48:52.596846076    3037 parsing.c:801]              ignoring out of order new grpc_chttp2_stream request on server; last grpc_chttp2_stream id=12325, new grpc_chttp2_stream id=12317\r\n    2017-09-21 16:48:57.497244: W tensorflow/core/framework/op_kernel.cc:1158] Out of range: End of sequence\r\n         [[Node: data_source_task_index_0/IteratorGetNext = IteratorGetNext[output_shapes=[[-1,-1], [-1,-1], [-1,-1], [-1,-1], [-1,-1]], output_types=[DT_INT64, DT_INT64, DT_INT64, DT_INT64, DT_INT64], _device=\"/job:ps/replica:0/task:0/cpu:0\"](data_source_task_index_0/Iterator)]]\r\n         [[Node: data_source_task_index_0/cond/Merge_2_S341 = _Recv[client_terminated=false, recv_device=\"/job:ps/replica:0/task:2/cpu:0\", send_device=\"/job:ps/replica:0/task:0/cpu:0\", send_device_incarnation=-6450759800525444137, tensor_name=\"edge_359_data_source_task_index_0/cond/Merge_2\", tensor_type=DT_INT64, _device=\"/job:ps/replica:0/task:2/cpu:0\"]()]]\r\n    E0921 16:49:58.462749643    3036 parsing.c:801]              ignoring out of order new grpc_chttp2_stream request on server; last grpc_chttp2_stream id=24775, new grpc_chttp2_stream id=24769\r\n    E0921 16:49:58.462780714    3036 parsing.c:801]              ignoring out of order new grpc_chttp2_stream request on server; last grpc_chttp2_stream id=24775, new grpc_chttp2_stream id=24773\r\n    E0921 16:49:58.463260203    3036 parsing.c:801]              ignoring out of order new grpc_chttp2_stream request on server; last grpc_chttp2_stream id=24793, new grpc_chttp2_stream id=24777\r\n    E0921 16:49:58.463277333    3036 parsing.c:801]              ignoring out of order new grpc_chttp2_stream request on server; last grpc_chttp2_stream id=24793, new grpc_chttp2_stream id=24779\r\n    E0921 16:49:58.463283953    3036 parsing.c:801]              ignoring out of order new grpc_chttp2_stream request on server; last grpc_chttp2_stream id=24793, new grpc_chttp2_stream id=24781\r\n    E0921 16:49:58.463289625    3036 parsing.c:801]              ignoring out of order new grpc_chttp2_stream request on server; last grpc_chttp2_stream id=24793, new grpc_chttp2_stream id=24783\r\n    E0921 16:49:58.463295275    3036 parsing.c:801]              ignoring out of order new grpc_chttp2_stream request on server; last grpc_chttp2_stream id=24793, new grpc_chttp2_stream id=24785\r\n```\r\n\r\n\r\nFor more detail see the stackoverflow post: \r\nhttps://stackoverflow.com/questions/46322337/frozen-training-in-distributed-tensorflow  ", "comments": ["@mrry could you please take a look.", "Someone mentions [here](https://github.com/grpc/grpc/issues/7785) that: \"I'm also seeing this error in distributed tensorflow when I close and reset a lot of FIFOQueues.\"", "There are a couple of possible issues, but it's difficult to tell without a reproducible example:\r\n\r\n1. According to the linked issue in gRPC, the problem might be due to an old version of gRPC. Try upgrading to a later version of TensorFlow to get the update.\r\n2. It looks like you're using an initializable iterator in order to get end-of-epoch signals. However, `SyncReplicasOptimizer` depends on all of the replicas taking an equal number of steps. (It wasn't designed with the possibility of initializable iterators in mind....)", "(2) Yes I am using initializable  iterator to get local End-of-epoch signal for every worker. In my case the 'epoch' count is a python variable that is local to the worker. It does not interfere with the 'global_step' and the 'local_step' that  'SyncReplicasOptimizer' uses. When some worker is initializing its iterator, other workers need to wait for brief time, but the training runs as expected afterwards. All original assumptions of SyncReplicasOptimizer seems to hold true. The problem seems to arise in grpc.\r\n\r\n(1.) Are you suggesting to update the C++ library or python version of grpc. I updated python package, it does not solve it. I am guessing I should compile tensorflow (1.2) with bazel to use latest grpc.  \r\n\r\nAnd, I will provide you with minimum reproducible code if (1.) does not fix it.", "On (1.), I think you might need to upgrade to a nightly build to get the upgraded version of gRPC. (It doesn't look like the 1.3 branch has the upgraded version.)", "Is [this](http://ci.tensorflow.org/view/Tensorflow%20Jenkins%20Monitored%20builds/job/tensorflow-master-linux-gpu/2634/changes) the relevant one or [this](http://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/119/changes) ? ", "Thank you very much @mrry for the suggestion. Problem went away with this nightly build:\r\nhttp://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3.5,label=gpu-linux/"]}, {"number": 13212, "title": "Changed TanhGrad() operation definition with respect to documentation", "body": "This should resolve #12902 ", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins test this please", "@rmlarsen please review"]}, {"number": 13211, "title": "transform_graph does not allow ~/ in path", "body": "when using ~/ in path, transform_graph returns file not found", "comments": ["would you like to send a PR to fix it? You could get value of '~' as `getenv(\"HOME\")` and then replace `~` with that. That's essentially what bash is doing when you use ~", "I would like to pick this issue for my first contribution.", "@yaroslavvb I'd like to make a PR for fixing the issue. I'm not sure [there](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms) are the related code. Would you mind point out the file which includes the `transform_graph` function?", "Added a PR #15894 for the fix."]}, {"number": 13210, "title": "tensorboard failed  on win 10", "body": "I tried to use tensorboard(win10+python3.5.2+tensorflow-gpu 1.1.0), but failed with the error:\r\n![image](https://user-images.githubusercontent.com/13776012/30694040-dde10e00-9f04-11e7-9b1f-ab55cac9f4f9.png)\r\n(module 'tensorflow' has no attribute 'make_tensor_proto')\r\nbut I tried on win 7 with the same events, and succeed.(win7+python3.5.2+tensorflow-gpu 1.0.1)\r\n![image](https://user-images.githubusercontent.com/13776012/30694521-ec36d8fc-9f06-11e7-96ad-d720974838fb.png)\r\nIs there a bug?\r\n\r\n ", "comments": ["There are a few setting works the best with Windows 10 under specific cuDNN and CUDA library.", "@printdhruv , thank you. but I think it has nothing to do with cuDNN and CUDA. there is no any gpu computation in this case. ", "@micklexqg If I am understanding it correctly then you are mentioning `tensorflow-gpu 1.1.x` which must have cuda support along with specific .dll.\r\nAlso,you are using two different versions of tensorflow-gpu for windows 7 and windows 10!\r\nFrom screenshot it seems tensor flow is not recognizing the some attributes which might be different in two versions of 1.1.0 and 1.0.1\r\nPlease follow template [here](https://github.com/tensorflow/tensorflow/blob/master/ISSUE_TEMPLATE.md) along with your source code in order to get things clear!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly."]}, {"number": 13209, "title": "old build_all_android.sh -X be deleted", "body": "", "comments": ["Can one of the admins verify this patch?", "Sorry, I made a mistake for shell target"]}, {"number": 13208, "title": "Use Tensors instead of TensorArrays for storing AttentionWrapper's alignment_history", "body": "I would like to propose this fix for #13154.\r\n\r\nThis is a breaking change as it replaces `TensorArray`s of the `alignment_history` field with batch major `Tensor`s.\r\n\r\nAdditionally, the helper function used to gather beams in `BeamSearchDecoder` had to be updated to allow keeping the original values dimensions.", "comments": ["This looks like it may kill performance.  I'm closing for now but let's continue discussing other solutions on the original bug report."]}, {"number": 13207, "title": "Failed to compile tensorflow offline with '--fetch=false' after all external dependencies fetched by bazel", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.2\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.3.0 from master branch\r\n- **Python version**: Python 2.7.12\r\n- **Bazel version (if compiling from source)**: 0.5.4\r\n- **CUDA/cuDNN version**: null\r\n- **GPU model and memory**: null\r\n- **Exact command to reproduce**: \r\n1. Fetch all external dependencies by docker image with internet access.\r\n```\r\nbazel fetch //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n2. Complie tensorflow offline without internet access with `--fetch=false`.\r\n```\r\nbazel build --fetch=false --copt=-mavx2 --copt=-mfma --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n### Describe the problem\r\nI fetched all external dependencies successfully by docker image, and then committed and pushed it into our docker hub. \r\n```\r\n#bazel fetch //tensorflow/tools/pip_package:build_pip_package\r\nINFO: All external dependencies fetched successfully.\r\n```\r\nAfter that, I tried to compile tensorflow offline by using the image with `bazel build --fetch=false` since there is no internet access on my server, but it failed with the error \"no such package '@xxx'\", as follows: \r\n```\r\n#bazel build --fetch=false --copt=-mavx2 --copt=-mfma --config=opt //tensorflow/tools/pip_package:build_pip_package\r\nWARNING: /home/admin/src/tensorflow/tensorflow/workspace.bzl:323:3: External repository 'six_archive' is not up-to-date and fetching is disabled. To update, run the build without the '--nofetch' command line option.\r\nWARNING: /home/admin/src/tensorflow/tensorflow/workspace.bzl:173:3: External repository 'eigen_archive' is not up-to-date and fetching is disabled. To update, run the build without the '--nofetch' command line option.\r\nERROR: /home/admin/src/tensorflow/third_party/eigen3/BUILD:20:1: no such package '@eigen_archive//': BUILD file not found on package path and referenced by '//third_party/eigen3:eigen3'.\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.\r\nINFO: Elapsed time: 0.349s\r\n```\r\nIn fact, be sure that the package `eigen_archive` was existing in the container, as below:\r\n```\r\n#ls /root/.cache/bazel/_bazel_root/cb177c55a0e1ff115d6dbd74b4d4974a/external/ | grep eigen_archive\r\neigen_archive\r\n```\r\nAnd the `ps` stdout of bazel process was as follows:\r\n```\r\n#ps aux | grep bazel\r\nroot        664  0.8  0.8 20850492 570628 ?     Ssl  16:39   0:29 bazel(src) -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/root/.cache/bazel/_bazel_root/cb177c55a0e1ff115d6dbd74b4d4974a -Xverify:none -Djava.util.logging.config.file=/root/.cache/bazel/_bazel_root/cb177c55a0e1ff115d6dbd74b4d4974a/javalog.properties -Djava.library.path=/root/.cache/bazel/_bazel_root/install/2211725bdc2c34f807246fe9fb601a7f/_embedded_binaries/ -Dfile.encoding=ISO-8859-1 -jar /root/.cache/bazel/_bazel_root/install/2211725bdc2c34f807246fe9fb601a7f/_embedded_binaries/A-server.jar --max_idle_secs=10800 --connect_timeout_secs=10 --install_base=/root/.cache/bazel/_bazel_root/install/2211725bdc2c34f807246fe9fb601a7f --install_md5=2211725bdc2c34f807246fe9fb601a7f --output_base=/root/.cache/bazel/_bazel_root/cb177c55a0e1ff115d6dbd74b4d4974a --workspace_directory=/home/admin/src/tensorflow --deep_execroot --experimental_oom_more_eagerly_threshold=100 --nofatal_event_bus_exceptions --client_debug=false --product_name=Bazel --option_sources=\r\n```\r\nSo, please help for that and let me know if something wrong with my operation, thanks.\r\n", "comments": ["Is there someone to help to take a look at the issue? Maybe we need to modify `WORKSPACE` file to be able to compile with the two way of offline and online. Also, please let me know if it is a bazel issue, Thanks.", "I don't know for sure whether there's something extra you need here. It sure does sound like this would be the right thing to do from the Bazel args.\r\n\r\n@damienmg for more Bazel expertise. \r\n\r\nI do believe this is not specific to anything TemsorFlow is doing in particular, so could you file an issue over for Bazel, and reference this one? I will close this, if it turns out we can fix this on the TensorFlow side, we can reopen. ", "definitely looks like a bazel bug, can you move the discussion to https://github.com/bazelbuild/bazel/issues/new?", "@damienmg I moved it to bazel.\r\n\r\nThanks everyone."]}, {"number": 13206, "title": "Feature Request - Check parameter types at C++ compile time ", "body": "e.g. Tensor oTensor( DT_INT64, TensorShape( { 4, 3 } ) );\r\n\r\nThe type of parameters **{4, 3}** should be checked at compile time whether is corresponding to the **DataType** of **Tensor(DataType type, const TensorShape& shape);**.\r\n\r\nSee #12501\r\n\r\n", "comments": ["@sskye, would you please consider this request. I suppose we could have a runtime check, or we could have a templatized version that checks types more statically.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "No one seems to be interested about static type checking.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Assignee @skye: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I'm not exactly sure what you're proposing we check at compile time, but it sounds like it would require parsing OpDefs at compile time, which sounds like a major project. Am I misunderstanding?", "I have written in the comment of #12501 why it is better to check types at compile time for C++. To find wrong types at run-time is harder to find.  \r\n\r\n> I have solved the problems. The main problem of Tensorflow usage in C++ environment is not checking the variable types at compile time. Tensorflow checks variable types at runtime, which makes troubleshooting more difficult.\r\nE.g. Tensor hypho2_ix( DT_INT64, TensorShape( { 4, 3 } ) );\r\nThis definition of hypho2_ix is wrong due the numbers 4 and 3 parameters are not int64_t type. But the C++ compiler is not checking the type at compile time. If you using this defined tensor with operations which requires DT_INT64 (int64_t) type then you will have not obvious error. Why check parameter types of Tensor definitions at compile time?\r\nThe correct definition is (C++11):\r\nTensor hypho2_ix( DT_INT64, TensorShape( { 4i64, 3i64 } ) );", "In your example, the numbers 4 and 3 are the dimensions of the TensorShape, not the data of the tensor (which should indeed be int64). For example, Tesnor t(DT_FLOAT, TensorShape({4,3}) makes sense, but specifying TensorShape({4.5, 3.5}) doesn't (since dimensions must be integers). I'm gonna close this for now, please feel free to continue commenting though."]}, {"number": 13205, "title": "Revert \"Revert \"EHN: csv supports missing value (#13008)\" (#13051)\"", "body": "issue: #13007\r\nCF PR: #13008\r\n\r\nThis reverts commit b6f253429c475d1a6f80702feadfff8ff9409156.", "comments": ["Can one of the admins verify this patch?", "@facaiy, thanks for your PR! By analyzing the history of the files in this pull request, we identified @ebrevdo, @keveman and @tensorflower-gardener to be potential reviewers.", "cc @gunan @martinwicke @drpngx who I believe might be interested in this PR.", "The only issue I can see here is that na_value comes before name. I know that's where it belongs, but that's a breaking change for people who are not using named args (as they obviously should).\r\n\r\nSo, in the BUILD file, mark decode_csv \"hidden\", and then write a python wrapper which swaps the last two arguments. \r\n\r\n@josh11b @asimshankar This is an interesting case -- the fault here lies with the generated python wrapper. I suppose that means we need another wrapper around the wrapper for every argument change. It is all very ugly. @annarev this will all be so much better once the API generation is in!", "Thank you for review, @martinwicke . Writing a python wrapper is quite easy for me, however I'm afraid that it brings unnecessary complexity. Hope we could find a more lightweight solution later.", "I agree, this is rather ugly. @annarev is working on a better way to generate our python API from the op definitions -- the new process could fix this. \r\n\r\nPossibly we need to give users more control how Op attrs are turned into python args, maybe by allowing .attr and .attr_after? It's all a little sad, but that's the cost of backwards compatibility.", "Python wrapper has been added, and leave a todo comment as memos.", "Jenkins, test this please.", "Hi, golden file has been updated, and compatibility test passed as well on my local machine.\r\n```\r\nINFO: Elapsed time: 13.524s, Critical Path: 9.72s\r\n//tensorflow/tools/api/tests:api_compatibility_test                      PASSED in 3.6s\r\n\r\nExecuted 1 out of 1 test: 1 test passes.\r\n```", "Jenkins, test this please."]}, {"number": 13204, "title": "TF Record Reader/Writer C API", "body": "@asimshankar Would you be open to adding support for reading/writing TF records using the C API? Similar to the `lib/io` stuff in the Python API.", "comments": ["sorry for drive-by comment -- providing readers lib/io in C_API feels a little weird -- the API comprises methods for managing computational graph. Parsing TF records is a form of computation, so it can be handled within computational graph as tf.TFRecordReader ops rather than a new method\r\n\r\nFor some reasons (performance?) people also added standalone utilities that can do TF-related things without using TF runtime -- the record readers, and checkpoint readers. The lib/io example seems to fall under those \"standalone\" utils", "Thanks for the comment! I think I agree for the most part. My concern though is that there is certain utilities that should be provided by the native library. For example, right now if I want to implement a \"summary_writer\" I would have to implement a writer for the right format and make sure it's compatible with tensorflow. And every other API, for other languages, should do the same. So, given that there is this agreed upon file format, doesn't it makes sense to provide utilities for working with it for other languages? And given that these utilities exist, maybe we could create a separate library (not part our C API), providing an interface for those utilities. What do you think? And please, let me know if there is currently another way to create a summary writer using the c api.\n\nOn Sep 21, 2017, 12:29 PM -0400, Yaroslav Bulatov <notifications@github.com>, wrote:\n> sorry for drive-by comment -- providing readers lib/io in C_API feels a little weird -- the API comprises methods for managing computational graph. Parsing TF records is a form of computation, so it can be handled within computational graph as tf.TFRecordReader ops rather than a new method\n> For some reasons (performance?) people also added standalone utilities that can TF-related things without using TF runtime -- the record readers, and checkpoint readers. The lib/io seems to be one of those \"standalone\" utils\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n", "Closing this since I ended up writing code for the file writers on the Scala side. I still believe though that it would be nice to have that code shared by all languages, given that the file format is fixed.", "cc: @josh11b -- do you think there should be language-agnostic API for checkpoint readers/file readers? Right now record_reader seems to be Python-only -- https://github.com/tensorflow/tensorflow/tree/2b374f7d4ba578764e2b2e9f21da9ba52811a103/tensorflow/python/lib/io", "@yaroslavvb For checkpoint readers, there is already a simple C API [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/checkpoint_reader.h), which is not exposed. I think that exposing it (potentially along with summary reader/writer support), would not be a bad idea. I wrote readers/writers for summary files, but it's tedious and not sustainable (if something changes on the TensorFlow side, I will need to also change my code to imitate those changes, if I want my summaries to stay compatible with TensorBoard, etc.). And the same applies for checkpoint files, even though I haven't written a reader yet.\r\n\r\nFor checkpoint files it's actually interesting to see how the change from V1 to V2 required some not-so-nice changes to the checkpoint reader code. It would be good to not have to replicate those."]}, {"number": 13203, "title": "Brand new windows 10 install, TF doesn't run: ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'", "body": "I'm submitting this as a bug, instead of a support question on SO, because I'm getting this error after a very clean install of the operating system. The error occurs when I attempt to do the most basic validation, but just importing tensorflow!\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo custom code, the error is occurring when I do `import tensorflow as tf` at the repl \r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 10 Home, version 1703, fresh install (the only other thing installed on the computer is Chrome and Anaconda distribution of Python 3.6)\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary. Followed the instruction to the letter, except type in 'pip' instead of 'pip3', as in:\r\n`pip3 install --upgrade tensorflow-gpu` (this is after installing cuda and cudablas)\r\n\r\n- **TensorFlow version (use command below)**:\r\n1.3 (whatever is in the repo as of about 30 minutes ago)\r\n\r\n- **Python version**:  Python 3.6.1 :: Anaconda custom (64-bit)\r\n\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n>nvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2016 NVIDIA Corporation\r\nBuilt on Mon_Jan__9_17:32:33_CST_2017\r\nCuda compilation tools, release 8.0, V8.0.60\r\n\r\n- **GPU model and memory**:\r\nNVIDIA GeForce GTX 960M\r\nApprox. Total Memory: 18313 MB\r\n\r\n- **Exact command to reproduce**:\r\nimport tensorflow as tf\r\n\r\n### Describe the problem\r\nI re-imaged my windows 10 laptop, installed Chrome, installed the latest anaconda python 3.6 and attempted to install tensorflow following the official instructions on the web page (all of this within the last hour or two of filing this ticket).\r\n\r\nInstalled Cuda and Cudablas from nvidia's website, as required by TF instructions.\r\nInstalled tensorflow-gpu\r\n\r\nSaw an exception related to some setuptools file not being found (sorry, didn't keep a record of that)\r\nUpgraded setuptools\r\n\r\nDid a reinstall of tensorflow, but added `--force-reinstall` flag, which claimed to have installed tensorflow without errors:\r\n\r\nSuccessfully installed bleach-1.5.0 html5lib-0.9999999 markdown-2.6.9 numpy-1.13.1 protobuf-3.4.0 setuptools-36.5.0 six-1.11.0 tensorflow-gpu-1.3.0 tensorflow-tensorboard-0.1.6 werkzeug-0.12.2 wheel-0.30.0\r\n\r\nRan the import command and saw these exception trace:\r\n\r\n\r\n### Source code / logs\r\n```\r\n(C:\\Users\\someuser\\Anaconda3) C:\\Users\\someuser>python\r\nPython 3.6.1 |Anaconda custom (64-bit)| (default, May 11 2017, 13:25:24) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\someuser\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\someuser\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 978, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 950, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 648, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 560, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 205, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\someuser\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\someuser\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\someuser\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\someuser\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\someuser\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\someuser\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\someuser\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\someuser\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\someuser\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 978, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 950, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 648, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 560, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 205, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\someuser\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\someuser\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\someuser\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\someuser\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```", "comments": ["This is a duplicate of #13196. Please see that issue for more information.", "Hi @shivaniag , thanks for your quick response. \r\n\r\nWhile I'm still trying to get this issue resolved, I created a ticket on github because following exact instructions from TF's website on a brand new machine, pretty standard,  shouldn't cause exception traces. The fact that I wasn't able to run it means there is a 'bug' some where in the docs or the code.\r\n\r\nRegarding the error, it doesn't look like this is the exact issue as #13196. There someone was apparently installing from source, which is why git shows up in their stack trace. I don't think that happens in my case (a quick ctrl+f, at least, didn't show it).\r\n\r\nThis issue is actually all over tensorflow's bug tracker under different names (now that I look more closely). For example, here is another one #8385, where the suggested solution is to install VC++ redistributable (which I also did, and which did not resolve the issue).", "The TensorFlow GPU worked after uninstalling Tensowflow CPU\r\n\r\n![image](https://user-images.githubusercontent.com/22581263/34695760-35c0e736-f4b3-11e7-89b2-d4d813cbe06d.png)\r\n\r\n  "]}, {"number": 13202, "title": "tf.InteractiveSession leaks sessions", "body": "The following works fine with tf.Session() but will fail to release resources in tf.InteractiveSession\r\n\r\n```\r\nsess = tf.InteractiveSession()\r\n# do stuff  \r\nsess.close()\r\ndel sess\r\n```\r\nThe reason is that interactive session enters a context using `__enter()__` and never quits it, leaving a reference from a DefaultStack object. I found this when debugging why my notebook was hogging all GPU RAM.\r\n\r\nThe two work-arounds:\r\n1. Force C_API to close the session using `sess.__del__()`\r\n2. Get rid of the dangling reference\r\n\r\n```\r\n    sess._default_session.__exit__(None, None, None)\r\n    del sess\r\n    import gc\r\n    gc.collect()\r\n```\r\n\r\nI think a better solution would be to have `sess.close()` call both `TF_CloseSession` and `TF_DeleteSession`, or have a method that will reset all sessions like `session_lib.Reset`", "comments": ["@mrry do you know if anyone's planning to make `Session.Reset` work for local sessions?", "Derek, could you please take a look. Thanks!", "@yaroslavvb I'm confused by your diagnosis, because the `InteractiveSession.close()` method [explicitly calls `__exit__()`](https://github.com/tensorflow/tensorflow/blob/e9d5ee1ebffba25cef65f1f354b9e4ca9bcea10c/tensorflow/python/client/session.py#L1627) on the context managers it enters in the constructor. Where is the dangling reference?\r\n\r\n`Session.reset(\"\")` is [implemented](https://github.com/tensorflow/tensorflow/blob/e9d5ee1ebffba25cef65f1f354b9e4ca9bcea10c/tensorflow/core/common_runtime/direct_session.cc#L183) for local sessions, but it's not clear what you think it should do....", "I'm experiencing the same with the normal `tf.Session` and also while using the C API directly. The following code keeps the memory on the GPU allocated until it exits.\r\n```\r\n#include <tensorflow/c/c_api.h>\r\n#include <unistd.h>  // usleep\r\n\r\n\r\nint main() {\r\n    TF_Graph* graph = TF_NewGraph();\r\n    TF_SessionOptions* options = TF_NewSessionOptions();\r\n    TF_Status* status = TF_NewStatus();\r\n    TF_Session* sess = TF_NewSession(graph, options, status);\r\n    TF_CloseSession(sess, status);\r\n    TF_DeleteSession(sess, status);\r\n    TF_DeleteSessionOptions(options);\r\n    TF_DeleteGraph(graph);\r\n    TF_DeleteStatus(status);\r\n    usleep(5000000);  // gpu resources are still allocated here\r\n}\r\n```\r\n\r\nI'm running Ubuntu 16.04, cudnn 6.0, cuda 8.0, tensorflow-master build is 2 hours old.", "@PhilJd that's a different issue -- the memory allocator is global to the process, so it will keep the memory allocated even if the session gets closed.\r\n\r\n@mrry indeed that method gets called, but somehow session doesn't get garbage collected. I assumed DefaultStack was to blame by looking at `gc.get_referrers(sess)`\r\n\r\nCPython is supposed to call `__del__` immediately after ref-count gets decremented to 0, and somehow it doesn't. Adding `import gc; gc.collect()` after `__exit__` seems to remove the leak as well\r\n\r\nRepro: https://github.com/yaroslavvb/stuff/blob/master/resnet_leak_report.py\r\nWhen I run it, I see\r\n\r\n```\r\nRun 0, GB's in use 2.1\r\nRun 1, GB's in use 3.4\r\nRun 2, GB's in use 5.0\r\n<OOM crash>\r\n\r\nCalling __del__\r\nCalling __del__\r\nCalling __del__\r\n```\r\n\r\nI'll update this issue if I find why CPython doesn't call del", "OK, looks like `__del__` isn't called because there's a cycle and ref count never goes to zero.\r\n`sess._default_session.args[\"default\"]` points back to sess (args are filled in [here](https://github.com/tensorflow/tensorflow/blob/03619fab3f4dd6f28b67418455a953b0fccdd9bf/tensorflow/python/framework/ops.py#L4507)), but there's no such cycle when using `tf.Session`\r\n\r\nRunning `gc.collect` detects cycles and cleans them up.", "Ironically InteractiveSession only causes this problem during interactive use, when you have a notebook open and create new graphs/sessions without restarting the process.\r\n\r\nThe underlying issue is that sess needs to keep a reference to the default context manager (otherwise it gets garbage collected and closed), whereas default context manager must link back to session. This seems hard to fix this issue without big refactoring of context manager logic. Perhaps it should be a docs issue and mentioned in docs of InteractiveSession that `gc.collect` must be called to reclaim resources", "Would it make sense to make `tf.InteractiveSession()` a singleton and add a `warnings.warn('Use tf.Session() instead if you intend to have multiple sessions')` upon the second call? Singletons are typically bad, but maybe very fitting in this particular case.", "I think that would be better than status quo which is \"1. create multiple interactive sessions by accident. 2. run out of GPU memory 3. restart notebook server\"", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Fix in progress...", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @mrry: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "So after implementing a couple of fixes for this, it seems there's no way to stop the leak without breaking an existing use case. The \"fix\" will be a warning, as @carlthome suggested, and we will investigate a more principled fix for TensorFlow 2.0.", "PS, the work-around is to do the following\r\n\r\n```\r\nsess = tf.InteractiveSession()\r\nimport gc; gc.collect()\r\n```\r\nThat will run cyclic garbage collector and remove all the dangling sessions", "Note that we've had to roll back https://github.com/tensorflow/tensorflow/commit/0f508d4de379e800ad7f990de08959bbd6fcabb5 internally, because it has a bug that produces spurious warnings. We will fix it after the weekend, but in the mean time the nightly build will have the spurious messages.", "@mrry maybe you can just do `import gc; gc.collect()` in the first call to `.run` of InteractiveSession", "@yaroslavvb The `import gc; gc.collect()` trick doesn't solve the problem for me.  Not sure why yet.", "Nagging Assignee @mrry: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "The warning was re-added in a80fb2b1cad1bb9c868222b8c25f162d69a509e6 so I'm marking this as closed."]}, {"number": 13201, "title": "Multiple infiniband cards support in tensorflow with RDMA", "body": "It seems that tensorflow with RDMA  doesn't support multiple infiniband cards now.\r\nIt only uses the first device in the infiniband device list.\r\n[source rdma.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/rdma.cc)\r\n```\r\nibv_context* open_default_device() {\r\n  ibv_device** dev_list;\r\n  ibv_device* ib_dev;\r\n  dev_list = ibv_get_device_list(NULL);\r\n  CHECK(dev_list) << \"No InfiniBand device found\";\r\n  ib_dev = dev_list[0];\r\n  CHECK(ib_dev) << \"No InfiniBand device found\";\r\n  ibv_context* context = ibv_open_device(ib_dev);\r\n  CHECK(context) << \"Open context failed for \" << ibv_get_device_name(ib_dev);\r\n  return context;\r\n}\r\n```\r\nThis will fail to communicate when there are different type infiniband cards in one node, and the order of infiniband device list is different in every node.\r\nBesides that, user can't specify one card to use.\r\n\r\nUsually, we will specify the IP:PORT when doing the distributed training.\r\nI think it's better to use the infiniband device which is corresponding to the IP in multiple infiniband cards environment.\r\n```\r\nbazel-bin/inception/imagenet_distributed_train \\\r\n--batch_size=32 \\\r\n--data_dir=/test/ILSVRC2012 \\\r\n--job_name='worker' \\\r\n--task_id=0 \\\r\n--ps_hosts='10.0.20.14:2276' \\\r\n--worker_hosts='10.0.20.15:2276,10.0.20.16:2276' \\\r\n--protocol='grpc+verbs'\r\n```\r\nWhat do you think about this?", "comments": ["@byronyi ", "Thanks Paul. \n\nYou could try the `grpc+gdr` protocol, which will use the same IB interface specified for gRPC.", "Hi @jiens \r\n@noaezra @dariavel worked on a solution for this problem (and more configuration parameters support from the user). PR for the changes should be sent any day now.\r\nWe will update when it merge to the code.", "@byronyi  Thanks for your advice.\r\n@shamoya Good news. Thank you for your working.", "@shamoya, any update on this?", "sure @angersson this was already merged to tensorflow master.\r\nSee #13564 \r\nRelevant description was also added to the [verbs readme page](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/README.md)", "Thanks!\r\n\r\nIt looks like this is resolved, so I'm closing the issue."]}, {"number": 13200, "title": "speech_commands using python speech_recognition as input [working example]", "body": "using your sample code /tensorflow/examples/speech_commands/label_wav.py I tweeked to use speech_recognition  as input that dumps wave data to trained network...\r\n\r\nfeel free to add to examples for others to use..\r\n\r\nmany thanks \r\ncalvin\r\n(ubuntu 16.04, python 2.7)\r\n\r\n<pre>\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport argparse\r\nimport sys\r\n\r\nimport tensorflow as tf\r\nimport speech_recognition as sr\r\nfrom tensorflow.contrib.framework.python.ops import audio_ops as contrib_audio\r\n\r\nFLAGS = None\r\n\r\n\r\ndef load_graph(filename):\r\n  with tf.gfile.FastGFile(filename, 'rb') as f:\r\n    graph_def = tf.GraphDef()\r\n    graph_def.ParseFromString(f.read())\r\n    tf.import_graph_def(graph_def, name='')\r\n\r\n\r\ndef load_labels(filename):\r\n  return [line.rstrip() for line in tf.gfile.GFile(filename)]\r\n\r\n\r\ndef run_graph(wav_data, labels, input_layer_name, output_layer_name,\r\n              num_top_predictions):\r\n  with tf.Session() as sess:\r\n    softmax_tensor = sess.graph.get_tensor_by_name(output_layer_name)\r\n    predictions, = sess.run(softmax_tensor, {input_layer_name: wav_data})\r\n    top_k = predictions.argsort()[-num_top_predictions:][::-1]\r\n    for node_id in top_k:\r\n      human_string = labels[node_id]\r\n      score = predictions[node_id]\r\n      print('%s (score = %.5f)' % (human_string, score))\r\n\r\n    return 0\r\n\r\ndef listen(r,source,labels_list):\r\n    audio = r.listen(source)\r\n\t\r\n    run_graph(audio.get_wav_data(convert_rate = 16000, convert_width = 2), labels_list, FLAGS.input_name,\r\n            FLAGS.output_name, FLAGS.how_many_labels)\r\n\r\ndef main(_):\r\n  print('start')\r\n  labels_list = load_labels(FLAGS.labels)\r\n  load_graph(FLAGS.graph)\r\n  \r\n  r = sr.Recognizer()\r\n  with sr.Microphone() as source:\r\n    print(\"Say something!\")\r\n    while 1:\r\n        listen(r,source,labels_list)\r\n        print(\"------------------\")\r\n  print('emd')\r\n \r\n\r\nif __name__ == '__main__':\r\n  parser = argparse.ArgumentParser()\r\n  parser.add_argument(\r\n      '--graph', type=str, default='', help='Model to use for identification.')\r\n  parser.add_argument(\r\n      '--labels', type=str, default='', help='Path to file containing labels.')\r\n  parser.add_argument(\r\n      '--input_name',\r\n      type=str,\r\n      default='wav_data:0',\r\n      help='Name of WAVE data input node in model.')\r\n  parser.add_argument(\r\n      '--output_name',\r\n      type=str,\r\n      default='labels_softmax:0',\r\n      help='Name of node outputting a prediction in the model.')\r\n  parser.add_argument(\r\n      '--how_many_labels',\r\n      type=int,\r\n      default=3,\r\n      help='Number of results to show.')\r\n\r\n  FLAGS, unparsed = parser.parse_known_args()\r\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n</pre>", "comments": ["Thank you very much for your contribution. Since it's not a bug report nor a feature request, I will close this issue.\r\n"]}]