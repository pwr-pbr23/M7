[{"number": 17179, "title": "Question about freeze the graph", "body": "I found a model which has the checkpoint file inside .pd\r\n\r\n```\r\n## Saving the lastest checkpoint to protobuf file\r\nflow --model cfg/yolo-new.cfg --load -1 --savepb\r\n```\r\n[https://github.com/thtrieu/darkflow](url)\r\n\r\nSo do I still have to freeze the graph?", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 17178, "title": "Failed to build graph_transforms", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nno\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nFedora 24\r\n- **TensorFlow installed from (source or binary)**:\r\nboth... \r\n- **TensorFlow version (use command below)**:\r\nmaster\r\n- **Python version**: \r\n3.6\r\n- **Bazel version (if compiling from source)**:\r\nBuild label: 0.10.0- (@non-git)\r\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Fri Mar 9 06:25:03 +50057 (1517485040703)\r\nBuild timestamp: 1517485040703\r\nBuild timestamp as int: 1517485040703\r\n\r\n- **GCC/Compiler version (if compiling from source)**:\r\ngcc (GCC) 7.3.1 20180130 (Red Hat 7.3.1-2)\r\n\r\n### Describe the problem\r\nI need to use graph_transforms as explained in documentation to be able to load a pb file in OpenCV\r\nSo I launched that command from github clone project:\r\n\r\n```\r\nbazel build tensorflow/tools/graph_transforms:transform_graph\r\n```\r\n\r\nAfter 20 minutes of compilation:\r\n\r\n```\r\nERROR: /home/pafer/Projects/Lab/tensorflow/tensorflow/cc/BUILD:510:1: Executing genrule //tensorflow/cc:remote_fused_graph_ops_genrule failed (Exit 127)\r\nbazel-out/host/bin/tensorflow/cc/ops/remote_fused_graph_ops_gen_cc: symbol lookup error: bazel-out/host/bin/tensorflow/cc/ops/remote_fused_graph_ops_gen_cc: undefined symbol: _ZN6google8protobuf5Arena13CreateMessageIN10tensorflow9AttrValueEEEPT_PS1_\r\nTarget //tensorflow/tools/graph_transforms:transform_graph failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 2304.405s, Critical Path: 88.76s\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nI tried `./configure`, and remove Android build, GCE build and so on. Here the configure output:\r\n\r\n```\r\nbuild --action_env PYTHON_BIN_PATH=\"/usr/bin/python3\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/usr/lib64/python3.6/site-packages\"\r\nbuild --force_python=py3\r\nbuild --host_force_python=py3\r\nbuild --python_path=\"/usr/bin/python3\"\r\nbuild:gcp --define with_gcp_support=true\r\nbuild:hdfs --define with_hdfs_support=true\r\nbuild:s3 --define with_s3_support=true\r\nbuild:kafka --define with_kafka_support=true\r\nbuild:xla --define with_xla_support=true\r\nbuild:gdr --define with_gdr_support=true\r\nbuild:verbs --define with_verbs_support=true\r\nbuild --action_env TF_NEED_OPENCL_SYCL=\"0\"\r\nbuild --action_env TF_NEED_CUDA=\"0\"\r\nbuild --define grpc_no_ares=true\r\nbuild:opt --copt=-march=native\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\nbuild --copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK\r\nbuild --host_copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK\r\n```\r\nNote: why ./configure says \"true\" on kafka, s3, and so on...? I said \"no\" at ./configure prompts\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 17177, "title": "logits or log probabilities?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.4 and 1.5\r\n- **Python version**:  3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: Tried several\r\n- **GPU model and memory**: CPU and GPU, but GPU was nvidia p100\r\n- **Exact command to reproduce**: See \"source code\" below\r\n\r\n### Describe the problem\r\nThere seems to be some confusion about logits vs log probabilities.  In some cases logits seem to mean just log probabilities.  In other cases they seem to mean actual logits, `log(p/(1-p))` which is quite a different thing.  It seems possible to me that this is causing downstream bugs, potentially big ones in edward, though I haven't looked closely.  \r\n\r\nOther people [also seem confused](https://stackoverflow.com/questions/41455101/what-is-the-meaning-of-the-word-logits-in-tensorflow).\r\n\r\n### Source code / logs\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.ops.distributions import util as distribution_util\r\nimport numpy as np\r\n\r\ntf.reset_default_graph()\r\nsess=tf.Session()\r\n\r\ninp = tf.placeholder(dtype=tf.float32)\r\nlogits,probs=distribution_util.get_logits_and_probs(logits=inp)\r\n\r\ncateg=tf.distributions.Categorical(logits=inp)\r\nvals=categ.sample(100000)\r\n\r\nprobsv,valsv=sess.run([probs,vals],feed_dict={inp:np.log([.2,.3,.5])})\r\n\r\nprint(probsv)\r\nprint([np.mean(valsv==x) for x in range(3)])\r\n```\r\n\r\n```\r\n[0.16666667 0.23076922 0.33333334]\r\n[0.20078, 0.29984, 0.49938]\r\n```\r\n\r\nSo in some cases `logits` seems to mean log probabilities, but on other cases\r\nit seem to actually mean logits.  ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Hi!\r\n\r\n`get_logits_and_probs` takes an argument `multidimensional`. which specifies whether to compute `log(p)` vs `log(p/(1-p))`.\r\n\r\nget_logits_and_probs is also called internally by distribution objects, so there isn't a need to call it before passing in arguments (the name isn't exported into the tf namespace for that reason).\r\n\r\nThat being said, I do agree the name is confusing (the function was written with logits in mind, but was reworked to also handle log(prob) in certain cases) , and am happy to iterate on getting a better name for this. There is also potential to clean up some of the names for distributions (e.g. Categorical accepts `logits`, but these are really log probs).", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 17176, "title": "Feature request: Provide API to test whether a copy of TensorFlow has MKL support", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: v1.5.0-9-gc959ec7\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: 0.10.0\r\n- **GCC/Compiler version (if compiling from source)**: c++ (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: n/a\r\n- **Exact command to reproduce**: n/a\r\n\r\n### Describe the problem\r\nI have built a copy of TensorFlow with MKL-DNN support. The MKL convolution operators work much better with the \"channels_first\" (NCHW) data format. On non-MKL, non-GPU builds, the \"channels_first\" data format does not work for me, because a CPU version of average pooling is not implemented for that data format.\r\n\r\nI would like to have my scripts test whether the current copy of TensorFlow has either MKL or GPU support, and if so, to use the \"channels_first\" data format. I can easily test for GPU support with `tf.test.is_built_with_cuda()`. However, there does not seem to be a corresponding API to test for MKL-DNN support. Would it be possible to add one?\r\n", "comments": ["CCing @tfboyd for comment.", "I am adding the ability to test if the program has been built with MKL in the internal repo. It should propagate in a day or two.", "tensorflow/python/framework/test_util.py now has IsMKLEnabled(). Does this solve your problem?\r\n", "Yes, that works nicely, thanks!", "You are welcome."]}, {"number": 17175, "title": "tf.einsum not replicating np.einsum; struggles to read input with whitespace", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Not sure\r\n- **TensorFlow version (use command below)**: v1.5.0-0-g37aa430d84 1.5.0\r\n- **Python version**:  3.5.2\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 8.0.61\r\n- **GPU model and memory**:  NVIDIA Corporation GK210GL [Tesla K80] (rev a1)\r\n- **Exact command to reproduce**: See below.\r\n\r\n\r\n### Problem\r\nUnlike `np.einsum`, I have found that `tf.einsum` struggles to parse input strings with spaces.\r\nEither it throws an error, or it produces varying results for the same syntax (e.g. `ij,jk->ik` gives different results than `ij,jk-> ik`). \r\n\r\nI believe the problem traces to the regular expressions match in line 164 (carried to line 178) of this file: `tensorflow/tensorflow/python/ops/special_math_ops.py`.\r\n\r\n\r\n### Source code\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nimport tensorflow.contrib.eager as tfe\r\ntfe.enable_eager_execution()\r\n\r\nx = tf.range(5,10,  dtype=tf.float64)\r\ny = tf.ones(shape=(2,6), dtype=tf.float64)\r\n\r\n# Each invocation of tf.einsum produces a different result\r\ntf.einsum(\"i,jk->ijk\", x, y)    # shape = (5,2,6)\r\ntf.einsum(\"i,jk-> ijk\", x, y)   # shape = ()\r\ntf.einsum(\"i, jk -> ijk\", x, y) # Error\r\n\r\nx = np.arange(5,10,  dtype=np.float64)\r\ny = np.ones(shape=(2,6), dtype=np.float64)\r\n\r\n# In contrast, np.einsum produces consistent results\r\nnp.einsum(\"i,jk->ijk\", x, y)    # Array 5x2x6\r\nnp.einsum(\"i,jk-> ijk\", x, y)   # Array 5x2x6\r\nnp.einsum(\"i, jk -> ijk\", x, y) # Array 5x2x6\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory", "Ok, I have added some details! Though I don't think they are necessary to replicate the issue.\r\nI wasn't sure about the installation since it is a shared machine.", "Here is an example:\r\n\r\n```\r\nK = 5\r\nd = np.zeros([K,K])\r\nfor s in range(K):\r\n    for q in range(K):\r\n        d[s,q] = q-s\r\nd = tf.constant(d)\r\nd\r\n```\r\n\r\n```\r\n<tf.Tensor: id=179, shape=(5, 5), dtype=float64, numpy=\r\narray([[ 0.,  1.,  2.,  3.,  4.],\r\n       [-1.,  0.,  1.,  2.,  3.],\r\n       [-2., -1.,  0.,  1.,  2.],\r\n       [-3., -2., -1.,  0.,  1.],\r\n       [-4., -3., -2., -1.,  0.]])>\r\n```\r\n----\r\nHere is the correct output (the signature contains no spaces)\r\n```\r\ntf.einsum(\"xy,z->zxy\", xy, tf.ones(shape=[3], dtype=tf.float64) )\r\n```\r\nresults in:\r\n```\r\n<tf.Tensor: id=136, shape=(3, 5, 5), dtype=float64, numpy=\r\narray([[[ 0.,  1.,  2.,  3.,  4.],\r\n        [-1.,  0.,  1.,  2.,  3.],\r\n        [-2., -1.,  0.,  1.,  2.],\r\n        [-3., -2., -1.,  0.,  1.],\r\n        [-4., -3., -2., -1.,  0.]],\r\n\r\n       [[ 0.,  1.,  2.,  3.,  4.],\r\n        [-1.,  0.,  1.,  2.,  3.],\r\n        [-2., -1.,  0.,  1.,  2.],\r\n        [-3., -2., -1.,  0.,  1.],\r\n        [-4., -3., -2., -1.,  0.]],\r\n\r\n       [[ 0.,  1.,  2.,  3.,  4.],\r\n        [-1.,  0.,  1.,  2.,  3.],\r\n        [-2., -1.,  0.,  1.,  2.],\r\n        [-3., -2., -1.,  0.,  1.],\r\n        [-4., -3., -2., -1.,  0.]]])>\r\n```\r\n----\r\nAnd here is a buggy version (with spaces in the signature)\r\n\r\n```\r\ntf.einsum(\"xy,z -> zxy\", xy, tf.ones(shape=[3], dtype=tf.float64) )\r\n```\r\nresults in:\r\n```\r\n<tf.Tensor: id=177, shape=(5, 5, 3), dtype=float64, numpy=\r\narray([[[ 0.,  0.,  0.],\r\n        [ 1.,  1.,  1.],\r\n        [ 2.,  2.,  2.],\r\n        [ 3.,  3.,  3.],\r\n        [ 4.,  4.,  4.]],\r\n\r\n       [[-1., -1., -1.],\r\n        [ 0.,  0.,  0.],\r\n        [ 1.,  1.,  1.],\r\n        [ 2.,  2.,  2.],\r\n        [ 3.,  3.,  3.]],\r\n\r\n       [[-2., -2., -2.],\r\n        [-1., -1., -1.],\r\n        [ 0.,  0.,  0.],\r\n        [ 1.,  1.,  1.],\r\n        [ 2.,  2.,  2.]],\r\n\r\n       [[-3., -3., -3.],\r\n        [-2., -2., -2.],\r\n        [-1., -1., -1.],\r\n        [ 0.,  0.,  0.],\r\n        [ 1.,  1.,  1.]],\r\n\r\n       [[-4., -4., -4.],\r\n        [-3., -3., -3.],\r\n        [-2., -2., -2.],\r\n        [-1., -1., -1.],\r\n        [ 0.,  0.,  0.]]])>\r\n```\r\n\r\n----\r\nAnd\r\n```\r\ntf.einsum(\"xy,z-> zxy\", xy, tf.ones(shape=[3], dtype=tf.float64) )\r\n```\r\nresults in:\r\n```\r\n<tf.Tensor: id=151, shape=(), dtype=float64, numpy=0.0>\r\n```", "@langmore can you please take a look?", "Sorry, I don't know anything about einsum.  Can you track down the original author?", "The problem is the regex at:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/special_math_ops.py#L166\r\n\r\n`match = re.match('([a-z,]+)(->[a-z]*)?', equation)`\r\n\r\nwhich allows the `(->[a-z]*)?` to be optional. Since space is not accepted by the regex, the parsing of the `equation` variable stops once a space character is encountered. If the space appears before the -> character, then the code tries to infer the dimensions of the output, ignoring the equation instructions after the `->` characters.\r\n\r\nChanging the code to:\r\n\r\n`match = re.match('^([a-z,]+)(->[a-z]*)?$', equation)`\r\n\r\nwill resolve the issue, forcing the equation to be fully parsed.\r\n\r\nNote that there is code replication at \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/special_math_ops.py#L405 and the code needs to be fixed there as well.\r\n\r\nJust in case, the parsing logic for the `einsum` implementation in numpy is at https://github.com/numpy/numpy/blob/master/numpy/core/src/multiarray/einsum.c.src#L2640", "@ipeirotis As you are working on this, would it make sense to also support uppercase letters? Numpy allows for uppercase letters in indices, and we use them to denote axes that have a specific semantic meaning. also, it expands the number of axes that can be specified, as they become 52 instead of just 26. Or do you think this requires another issue? I am happy to post one, but as this is already underway, it seems the two could go together.", "@acbellini Sure, added support for uppercase letters as well. Based on my reading of the code, it should work without problems, but let's see if it passes all the tests.", "@ipeirotis should this one be closed?", "@acbellini I do not know what is the policy for closing tickets. \r\n\r\nIf tickets close on merge of the bug fixing code, then it should be closed. \r\n\r\nIf tickets are closed when the merged code gets assigned to a release (or is released), then it may still need to remain open.", "Gotcha, thanks :) So sad it didn't make it in release 1.7 :/ I will custom-compile a little longer :)", "Nagging Assignee @skye: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I believe the policy is to close issues once it's fixed at head. Closing, please reopen if this isn't actually fixed."]}, {"number": 17174, "title": "Merge test local", "body": "", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->"]}, {"number": 17173, "title": "Fix table format in SECURITY.md", "body": "", "comments": []}, {"number": 17172, "title": "Make configure script runnable from external workspace.", "body": "To run from external workspace, you should now be able to invoke\r\nscript like the following. This will generate some TensorFlow specfic\r\nbazel options and import them into your project's .bazelrc.\r\n\r\n$(bazel info output_base)/external/org_tensorflow/configure.py --workspace=$(PWD)", "comments": ["Alternative impl to https://github.com/tensorflow/tensorflow/pull/13817\r\n\r\nBasically, I think configure.py will be kinda a pain to run through \"bazel run\" since then all of the paths in the script will have to take into account that they might be in bazel-bin/ directory.\r\n\r\nSo instead, I would just reccomend having people using TF as an external dep run....\r\n$(bazel info output_base)/external/org_tensorflow/configure.py --workspace=$(PWD)\r\n\r\n...or create a repo rule that runs configure.py automatically."]}, {"number": 17171, "title": "Fix compiler error with cuda-clang", "body": "segment_reduction_ops.h requires cuda_kernel_helper.h to be\r\nincluded in clang because it uses some of the helpers directly in the\r\nheader (e.g. CudaAtomicMax). It works with nvcc, because the usage is\r\nin a template context and nvcc checks that function is available only later\r\nat template instantiation.\r\nHowever, clang does more strict erorr-checking for functions found\r\nduring template instantiation and requires them to also be found either by\r\nADL or at the point of template declaration.", "comments": []}, {"number": 17170, "title": "Provide a way to convert feature columns to tensors", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A\r\n- **TensorFlow installed from (source or binary)**: N/A\r\n- **TensorFlow version (use command below)**: N/A\r\n- **Python version**: N/A\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n\r\nThe only way to convert a list of feature columns to tensors in TensorFlow 1.5 is by passing them to an [`input_layer`](https://www.tensorflow.org/api_docs/python/tf/feature_column/input_layer). This would do the per-column conversion and concatenate the resulting tensors. While useful in some cases, this approach is a bit limiting, as it does not allow to \r\n\r\n* concatenate on an axis different than `axis=1` (think sequence data),\r\n* extract tensors for the individual columns. \r\n\r\nBoth limitations can be mitigated by introducing a new public function converting from `_FeatureColumn` to `Tensor` (or even from a list of feature columns to a list of tensors). In the latter case, it could just be [`_internal_input_layer`](https://github.com/tensorflow/tensorflow/blob/17103a0b8dfdd7abd2b0dcb14f905aba879ff3a4/tensorflow/python/feature_column/feature_column.py#L164) without the concatenation in the end. \r\n\r\nWhat do you think? If this sounds good, I can submit a PR.", "comments": ["@ispirmustafa - any thoughts on this?", "does following work for your use case:\r\n`fc_to_tensor = {fc: input_layer(features, [fc]) for fc in feature_columns}`\r\n\r\nFor sequential inputs we're working on another utility. \r\n  ", "Thanks for the suggestion @ispirmustafa. This is exactly what I'm doing right now. However, I feel that it's a bit of a hack because I'm not really interested in a layer, but rather in a single tensor.\r\n\r\nIs the API for sequential inputs already in `master`? If yes, could you point me to it?", "@superbobry it's not there yet. I'll update the thread whenever it's in master.", "Nagging Assignee @ispirmustafa: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ispirmustafa: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ispirmustafa: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ispirmustafa: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Good news! feature columns for sequential input is in master. Please check out:\r\n[sequence_input_layer](https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/feature_column/sequence_input_layer\r\n)"]}, {"number": 17169, "title": "tf.contrib.layers output_collections inconsistency", "body": "Please correct me if I'm wrong, but there seems to be an inconsistency between the pooling and the convolution layers in `tf.contrib.layers` when it comes to adding variables to the `outputs_collection`, and using `tf.get_variable_scope().reuse_variables()`.\r\n\r\nHere is the relevant part of the model definition.\r\n```\r\ndef build_network(input):\r\n    with tf.variable_scope('scope') as sc:\r\n        ...\r\n        net = slim.max_pool2d(net, [2, 2], scope='pool')\r\n        ...\r\n        end_points = slim.utils.convert_collection_to_dict(end_points_collection)\r\n        ...\r\n        skip = slim.conv2d(end_points[sc.name + '/pool'], out_channels, [1, 1], scope='skip')\r\n        ...\r\n        end_points = slim.utils.convert_collection_to_dict(end_points_collection)\r\n        return net, end_points\r\n```\r\n\r\nAnd here is my test, which fails:\r\n```\r\nnetwork, end_points = build_network(input)\r\ntf.get_variable_scope().reuse_variables()\r\nnetwork, end_points = build_network(input)\r\n```\r\n\r\nThe reason the test fails is that whereas my convolutions all end up with the same names in the `end_points` dictionary both times (e.g. `scope/conv1/conv1_1`), the max_pooling layers end up with different names (e.g. the first time the network is built we get `scope/pool`, and the second time we get `scope_1/pool`). This means that the `end_points['scope/pool']` fails since the dictionary contains `'scope_1/pool'` as a key.\r\n\r\nI've done some breakpoint digging and it turns out that the behavior [here](https://github.com/tensorflow/tensorflow/blob/d100729c309cb22baf1630d9f39cf60516c58cdf/tensorflow/contrib/layers/python/layers/layers.py#L1063) and [here](https://github.com/tensorflow/tensorflow/blob/d100729c309cb22baf1630d9f39cf60516c58cdf/tensorflow/contrib/layers/python/layers/layers.py#L2266) differs due to convolutions using variable scopes and pooling using name scopes.\r\n\r\nNow I can potentially fix my code by using `sc.original_name_scope` instead of `sc.name`, but then the end_points dictionary is still wrong. I could manually fix the dictionary, but this seems hacky. I feel like it could be made such that the behaviour is consistent across the different layers. What do you think?", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  \r\n\r\n We ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "Sorry about not including that, here it is:\r\n\r\nOS Platform and Distribution: Linux Ubuntu 16.04\r\nTensorFlow installed from (source or binary): binary\r\nTensorFlow version (use command below): ('v1.4.0-19-ga52c8d9', '1.4.1')\r\nPython version: Python 3.5.2\r\nCUDA/cuDNN version: CUDA 8, cuDNN 6\r\nGPU model and memory: GeForce GTX 1080, 11172MiB", "@sguada please take a look, thanks.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "If you want to submit a fix with test, we can review it.", "I would be more than glad to!\r\n\r\nCould you please just let me know what you would consider a proper way of fixing this without introducing regressions?\r\n\r\nI feel like the proper solution here is to make name scopes and variable scopes behave the same way w.r.t. to the scope naming when using `reuse_variables`. Anything else would feel like hacking around the problem, but I am aware that this fix might have downsides which I don't see right now.\r\n\r\nWhat do you think?", "Contrib has been deprecated in latest versions please refer to tf/addons for latest updates. Thank you  "]}, {"number": 17168, "title": "Can you document tf metrics use in tf.keras?", "body": "There is no official documentation on how to use tf metrics in tf.keras. \r\nTwo stackoverflow reference but I don't know if it is a API stable solution: \r\nhttps://stackoverflow.com/questions/45947351/how-to-use-tensorflow-metrics-in-keras\r\nhttps://stackoverflow.com/questions/43158719/how-can-i-use-tensorflow-metric-function-within-keras-models", "comments": ["@fchollet @anj-s for comment.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Assignee @fchollet: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @fchollet: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This is generally not supported. But certain TF metrics can work with Keras.\r\n\r\nA metrics function, in Keras, for the time being, is a stateless tensor function that takes as arguments `(y_true, y_pred)` (target tensor and prediction tensor). If you're looking at a stateless TF metric (a metric that does not create weight variables in the background when you call it), then you can wrap it to have this signature, in most cases. Then you can pass the wrapped function to Keras.\r\n\r\nAny stateful metric, in Keras, should be implemented as a metrics layer. [Here is an example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/_impl/keras/metrics_test.py#L81-L119).\r\n\r\nIn general, we recommend using only metrics from `tf.keras.metrics` with tf.keras. Yes, we're aware it's a bad UX to have multiple incompatible metrics modules, and we'll fix it in the future.", "@fchollet So is this the same conclusion comment for https://github.com/keras-team/keras/issues/6050? I think that unifying the API it could be great but it is not a short term solution. Many standard tensorflow metrics are not covered in Keras so I think that we need to document this in some ways also if the documentation will be \"don't try this at home\"."]}, {"number": 17167, "title": "error in validating tensorflow installation", "body": "i installed tensorflow with cuda support on windows 10 and i'm trying to validate it,\r\ni used this script https://gist.github.com/mrry/ee5dbcfdd045fa48a27d56664411d41c and the result is\r\n![11](https://user-images.githubusercontent.com/15125749/36480211-c33bef0a-170b-11e8-89e6-8167389c5812.PNG)\r\n\r\nthen i tried import tensorflow as tf from python console and:\r\n![22](https://user-images.githubusercontent.com/15125749/36480256-ecba34e0-170b-11e8-9860-53b33ce49d21.PNG)\r\n", "comments": ["Hi @mikistra, thanks for reporting the issue. Could you please fill up the template form so we can better assess what is happening and help you? \r\n\r\nIf you're using TensorFlow 1.5 I think there are some updates that need to be made to the self check script. Are you using CUDA 9.0 and cuDNN 6? I'm not sure if the script is currently finding `cudnn64.7.dll`.", "hi, i'm using CUDA 8 and cuDNN 6.... sorry for the template, i missed it, where can i find it?\r\n\r\np.s. for the tensorflow version, i installed it yesterday by pip3 install upgrade command... i'm not able to get version from python console due to the same problem above", "I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you.", "i'm trying to remove tensorflow, cuda, and cuDnn to start the process from the beginning, but i get errors also trying to uninstall tensorflow: \r\n![ca](https://user-images.githubusercontent.com/15125749/36590883-6d9e422e-1890-11e8-9dd2-9e289508df81.PNG)\r\n\r\nif i try to install it again, it says it's already present..... is there a way to remove all tensorflow files manually??\r\n \r\n ", "@mikistra I think the error message is correct. Did you make sure your `pip` is linked to the correct Python installation? For instance, did you mean to use Python 3 and so you should use `pip3 install\\uninstall` rather `pip` and vice-versa? Also you say you installed for GPU, so the PYPI package name would be `tensorflow-gpu`, hence the uninstall command `pip uninstall tensorflow-gpu`.\r\n\r\nPlease go through the [installation docs](https://www.tensorflow.org/install/install_windows) again as well, if there's some detail missing on your installation you'll definitely find it there.\r\n\r\nIt'll be difficult to asses and try help you solve what's occurring without the information asked on the issue template. Please provide all the information using the template linked on @bignamehyp reply or we won't be able to solve it and your issue will need to be closed.\r\n", "@Carmezim  you were right, I feel so dumb. Now i'm going to have a fresh installation of everything and then in case of problem start a thread filling the template correctly. So maybe you can can close this...", "@mikistra No worries, it happens :). Thank you for reporting the issue and following up, glad it's working now. Feel free to close the issue."]}, {"number": 17166, "title": "custom matrix multiplication", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 17165, "title": "cifar 10 can not be downloaded", "body": "Hi, \r\n\r\nI am trying to follow the tutorial from the source code of tensor flow: /model/tutorials/cifar10/\r\n\r\nIn the cifar10.py, the link \r\n\r\n> DATA_URL = 'https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz'\r\n\r\ndoes not work, I guess it is the problem from Canada.\r\n\r\nDo you have a solution for this???\r\n\r\nThanks\r\n", "comments": ["Can you please check the link again? It works for me. ", "+1 not working for the last 12 hours. It seems that they have big outages"]}, {"number": 17164, "title": "Catch / recover from CUDNN_STATUS_BAD_PARAM", "body": "Hi all,\r\n\r\n* Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win7\r\n* TensorFlow installed from (source or binary): source\r\n* TensorFlow version (use command below): 1.3\r\n* GCC/Compiler version (if compiling from source): VC14_64\r\n* CUDA/cuDNN version: 8 / 6\r\n* GPU model and memory: Quadro K2000M\r\n* Bazel version (if compiling from source): N/A\r\n* Exact command to reproduce: N/A\r\n\r\nI am using the C++ API to load and execute trained models.\r\nI sometimes experience a problem regarding the ensemble of CuDNN and tensorflow:\r\nWhen I invoke Session::Run and the input tensor I am providing is too small for the network (which is fully convolutional and thus has no fixed input extents given), CuDNN crashes with CUDNN_STATUS_BAD_PARAM.\r\nIs there any way to catch this as an exception or somehow recover from that failure? Or is there a way to determine whether the given tensor extent will lead to such an error before I actually throw it at the network?\r\n\r\nThanks!", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nExact command to reproduce", "@tensorflowbutler Thanks, I've updated the issue description", "Any thoughts on this? Is more information required?\r\nI think this is a valid issue as crashing is probably not the desired behavior", "@hko-gh can you provide a C++ code sample to reproduce the problem?", "When I tried to arrange a minimal code sample, it surprisingly worked. I guess there was some issue with the self-compiled TF version we were using. Now I am getting an error message and status.ok() == false, which is the expected behavior. Thank you very much for asking for code :)", "Thank you for the update. I'll close the issue for now. Please reopen if you re-encounter the issue and are able to reproduce with a C++ code sample."]}, {"number": 17163, "title": "Fix markdown nit", "body": "Without a leading blank line, it doesn't render properly in https://www.tensorflow.org/programmers_guide/variables#variable_collections.", "comments": []}, {"number": 17162, "title": "Fix build issues when having packed git refs.", "body": "This is a workaround to fix build failure caused by packed git refs.\r\nThe tf.__git_version__ string will be \"unknown\" in this case.", "comments": ["This will fix the build issues seen in https://github.com/tensorflow/tensorflow/issues/10605 and will make it so you dont have to do a nasty workaround of creating a file in .git/head/branch_ref to get the build working.\r\n\r\nIt does not fix tf.__git_version__ in this case, but just will make it show up as \"unknown\"", "@aselle Could you add a review please?"]}, {"number": 17161, "title": "Do not add --host_copt=-march=native on Power PC.", "body": "", "comments": ["I think this should fix https://github.com/tensorflow/tensorflow/issues/17154"]}, {"number": 17160, "title": "[Intel MKL-DNN]: added MKLDNN dilated convolution support", "body": "This PR contains the MKLDNN implementation of dilated convolution forward and backward.\r\nwe supported following operators:\r\nforward dilated conv with and without bias\r\nbackward dilated conv grad input\r\nbackward dilated conv grad filter with and without bias\r\n\r\nTo test MKLDNN dilated convolution, environment variable TF_MKL_TEST = 1 is needed. conv_op_test.py file was modified. dilated conv test will be the tests that check the environment variable TF_MKL_TEST\r\n\r\nTo test it, run as follows:\r\nbazel test --action_env=TF_MKL_TEST=1 --config=mkl -s -c opt //tensorflow/python/kernel_tests:conv_ops_test\r\n", "comments": ["Please resolve the conflicts, thanks.", "Fixed. ", "Thank for the PR! I added test_util.IsMklEnabled() in tensorflow/python/framework/test_util.py that returns true if TensorFlow has been build with MKL support. Please use it instead of TF_MKL_TEST environment variable to guard tests that should be performed only when using MKL.", "On quick glance, it looks good to me except for the TF_MKL_TEST issue. \r\n@rmlarsen Would you mind taking a look at this PR once the test check is fixed?", "Test flags look OK. I'll let @rmlarsen review the rest.", "@tatianashp Thanks for your comments.", "@gunan can we run the MKL enabled kokoro tests for this PR?", "@rmlarsen @tatianashp Thanks for the comments. the failing check seems not relate to this PR.", "@jinghuangintel Thanks for the improvement!"]}, {"number": 17159, "title": "Estimator WarmStartSettings Cause Extreme Training Slowdown.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary(official docker image)\r\n- **TensorFlow version (use command below)**: 1.6.0-rc1\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: None\r\n- **GCC/Compiler version (if compiling from source)**: None\r\n- **CUDA/cuDNN version**: 9.0 / 7.0.5.15-1\r\n- **GPU model and memory**: GeForce GTX 1060 6GB\r\n- **Exact command to reproduce**: \r\n\r\n```py\r\nwarm_start_from = None \r\nif FLAGS.warm_start:\r\n    warm_start_from = tf.estimator.WarmStartSettings(\r\n        ckpt_to_initialize_from=FLAGS.warm_start,\r\n        # NOTE: attempted with and without `vars_to_warm_start` set\r\n        # vars_to_warm_start='^(?!.*(RMSProp|global_step))'\r\n    )\r\n\r\nmy_model = tf.estimator.Estimator(\r\n    model_fn=my_model_fn,\r\n    model_dir=FLAGS.model,\r\n    warm_start_from=warm_start_from\r\n)\r\n```\r\n\r\n### Describe the problem\r\n\r\nI am attempting to fine tune a model trained with RMSProp. I only want to load the model weights from the checkpoint, not the RMSProp state. To achieve this, I am using Tensorflow 1.6-rc1's `WarmStartSettings`.\r\n\r\nWhen I train the model without `WarmStartSettings`, each minibatch takes around 0.5 seconds. However, when I attempt to use that checkpoint with `WarmStartSettings`, each minibatch takes around 1.7 seconds.\r\n\r\nHaving inspected the logs to make sure the weights are not being initialised for every batch, It seems like this is a bug in Tensorflow. It should also be noted that I experienced similar problems in Tensorflow 1.5 when using `init_from_checkpoint` in my `model_fn` (as seen in https://github.com/tensorflow/tensorflow/issues/10155, which led me to discover the `WarmStartSettings` API).\r\n\r\n### Source code / logs\r\n\r\n#### Without WarmStartSettings (train for one epoch)\r\n\r\n```sh\r\n/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\nTrain data shape...\r\n(30000, 500)\r\n(30000, 2048)\r\nEval data shape...\r\n(5000, 500)\r\n(5000, 2048)\r\nTest data shape...\r\n(5000, 500)\r\n(5000, 2048)\r\nINFO:tensorflow:Using default config.\r\nINFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f6243687f90>, '_evaluation_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': 'output/my_model', '_save_summary_steps': 100}\r\nTraining Epoch #1...\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Graph was finalized.\r\n2018-02-20 20:31:08.668914: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-02-20 20:31:08.767997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-02-20 20:31:08.768353: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties: \r\nname: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 5.93GiB freeMemory: 3.84GiB\r\n2018-02-20 20:31:08.768368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0\r\n2018-02-20 20:31:08.919155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3579 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Saving checkpoints for 1 into output/my_model/model.ckpt.\r\nINFO:tensorflow:loss = 0.43950942, step = 1\r\nINFO:tensorflow:global_step/sec: 196.162\r\nINFO:tensorflow:loss = 0.46729112, step = 101 (0.510 sec)\r\nINFO:tensorflow:global_step/sec: 205.419\r\nINFO:tensorflow:loss = 0.35870957, step = 201 (0.487 sec)\r\nINFO:tensorflow:global_step/sec: 206.952\r\nINFO:tensorflow:loss = 0.32010338, step = 301 (0.483 sec)\r\nINFO:tensorflow:global_step/sec: 204.129\r\nINFO:tensorflow:loss = 0.3345171, step = 401 (0.490 sec)\r\nINFO:tensorflow:global_step/sec: 209.188\r\nINFO:tensorflow:loss = 0.34134513, step = 501 (0.478 sec)\r\nINFO:tensorflow:global_step/sec: 205.325\r\nINFO:tensorflow:loss = 0.33101806, step = 601 (0.487 sec)\r\nINFO:tensorflow:global_step/sec: 207.222\r\nINFO:tensorflow:loss = 0.34841973, step = 701 (0.483 sec)\r\nINFO:tensorflow:global_step/sec: 206.455\r\nINFO:tensorflow:loss = 0.33363524, step = 801 (0.484 sec)\r\nINFO:tensorflow:global_step/sec: 208.705\r\nINFO:tensorflow:loss = 0.33115995, step = 901 (0.479 sec)\r\nINFO:tensorflow:global_step/sec: 205.786\r\nINFO:tensorflow:loss = 0.32718512, step = 1001 (0.486 sec)\r\nINFO:tensorflow:global_step/sec: 205.812\r\nINFO:tensorflow:loss = 0.33765212, step = 1101 (0.486 sec)\r\nINFO:tensorflow:global_step/sec: 205.522\r\nINFO:tensorflow:loss = 0.33326942, step = 1201 (0.487 sec)\r\nINFO:tensorflow:global_step/sec: 208.248\r\nINFO:tensorflow:loss = 0.34017226, step = 1301 (0.480 sec)\r\nINFO:tensorflow:global_step/sec: 205.824\r\nINFO:tensorflow:loss = 0.32448825, step = 1401 (0.486 sec)\r\nINFO:tensorflow:Saving checkpoints for 1500 into output/my_model/model.ckpt.\r\nINFO:tensorflow:Loss for final step: 0.30791047.\r\nEvaluating Epoch #5...\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Starting evaluation at 2018-02-20-20:31:17\r\nINFO:tensorflow:Graph was finalized.\r\n2018-02-20 20:31:17.344022: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0\r\n2018-02-20 20:31:17.344148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 42 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nINFO:tensorflow:Restoring parameters from output/my_model/model.ckpt-1500\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Finished evaluation at 2018-02-20-20:31:17\r\nINFO:tensorflow:Saving dict for global step 1500: global_step = 1500, loss = 0.32683796\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Graph was finalized.\r\n2018-02-20 20:31:17.575632: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0\r\n2018-02-20 20:31:17.575748: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 42 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nINFO:tensorflow:Restoring parameters from output/my_model/model.ckpt-1500\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nWARNING:tensorflow:From scripts/my_model.py:85: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\ndim is deprecated, use axis instead\r\n2018-02-20 20:31:17.738903: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0\r\n2018-02-20 20:31:17.739028: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 42 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nR@1: 1.2\r\nR@5: 2.9\r\nR@10: 4.7\r\n```\r\n\r\n#### With WarmStartSettings\r\n\r\n```sh\r\n/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\nTrain data shape...\r\n(30000, 500)\r\n(30000, 2048)\r\nEval data shape...\r\n(5000, 500)\r\n(5000, 2048)\r\nTest data shape...\r\n(5000, 500)\r\n(5000, 2048)\r\nINFO:tensorflow:Using default config.\r\nINFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fccf90c7f90>, '_evaluation_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': 'output/my_model', '_save_summary_steps': 100}\r\nTraining Epoch #1...\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='output/checkpoint/', vars_to_warm_start='^(?!.*(RMSProp|global_step))', var_name_to_vocab_info={}, var_name_to_prev_var_name={})\r\nINFO:tensorflow:Warm-starting from: ('output/checkpoint/',)\r\nINFO:tensorflow:Warm-starting variable: my_model/dense_3/kernel; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable my_model/dense_3/kernel:0 from checkpoint output/checkpoint/ with my_model/dense_3/kernel\r\nINFO:tensorflow:Warm-starting variable: my_model/dense_3/bias; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable my_model/dense_3/bias:0 from checkpoint output/checkpoint/ with my_model/dense_3/bias\r\nINFO:tensorflow:Warm-starting variable: my_model/dense_2/bias; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable my_model/dense_2/bias:0 from checkpoint output/checkpoint/ with my_model/dense_2/bias\r\nINFO:tensorflow:Warm-starting variable: my_model/dense_1/kernel; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable my_model/dense_1/kernel:0 from checkpoint output/checkpoint/ with my_model/dense_1/kernel\r\nINFO:tensorflow:Warm-starting variable: my_model/dense_1/bias; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable my_model/dense_1/bias:0 from checkpoint output/checkpoint/ with my_model/dense_1/bias\r\nINFO:tensorflow:Warm-starting variable: my_model/dense/kernel; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable my_model/dense/kernel:0 from checkpoint output/checkpoint/ with my_model/dense/kernel\r\nINFO:tensorflow:Warm-starting variable: my_model/dense/bias; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable my_model/dense/bias:0 from checkpoint output/checkpoint/ with my_model/dense/bias\r\nINFO:tensorflow:Warm-starting variable: my_model/dense_2/kernel; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable my_model/dense_2/kernel:0 from checkpoint output/checkpoint/ with my_model/dense_2/kernel\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Graph was finalized.\r\n2018-02-20 20:31:44.926116: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-02-20 20:31:45.024835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-02-20 20:31:45.025212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties: \r\nname: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 5.93GiB freeMemory: 3.84GiB\r\n2018-02-20 20:31:45.025242: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0\r\n2018-02-20 20:31:45.174051: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3579 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Saving checkpoints for 1 into output/my_model/model.ckpt.\r\nINFO:tensorflow:loss = 0.32987013, step = 1\r\nINFO:tensorflow:global_step/sec: 56.8537\r\nINFO:tensorflow:loss = 0.3100884, step = 101 (1.759 sec)\r\nINFO:tensorflow:global_step/sec: 57.4584\r\nINFO:tensorflow:loss = 0.32932568, step = 201 (1.740 sec)\r\nINFO:tensorflow:global_step/sec: 58.9717\r\nINFO:tensorflow:loss = 0.30233204, step = 301 (1.696 sec)\r\nINFO:tensorflow:global_step/sec: 57.5253\r\nINFO:tensorflow:loss = 0.3428804, step = 401 (1.738 sec)\r\nINFO:tensorflow:global_step/sec: 57.8845\r\nINFO:tensorflow:loss = 0.331369, step = 501 (1.728 sec)\r\nINFO:tensorflow:global_step/sec: 58.2558\r\nINFO:tensorflow:loss = 0.3059027, step = 601 (1.717 sec)\r\nINFO:tensorflow:global_step/sec: 58.3863\r\nINFO:tensorflow:loss = 0.32942265, step = 701 (1.713 sec)\r\nINFO:tensorflow:global_step/sec: 58.454\r\nINFO:tensorflow:loss = 0.31594634, step = 801 (1.711 sec)\r\nINFO:tensorflow:global_step/sec: 58.8543\r\nINFO:tensorflow:loss = 0.31210855, step = 901 (1.699 sec)\r\nINFO:tensorflow:global_step/sec: 58.7531\r\nINFO:tensorflow:loss = 0.33128193, step = 1001 (1.702 sec)\r\nINFO:tensorflow:global_step/sec: 57.5272\r\nINFO:tensorflow:loss = 0.31882855, step = 1101 (1.739 sec)\r\nINFO:tensorflow:global_step/sec: 58.2324\r\nINFO:tensorflow:loss = 0.32009652, step = 1201 (1.717 sec)\r\nINFO:tensorflow:global_step/sec: 58.8266\r\nINFO:tensorflow:loss = 0.31766868, step = 1301 (1.700 sec)\r\nINFO:tensorflow:global_step/sec: 57.1415\r\nINFO:tensorflow:loss = 0.304159, step = 1401 (1.750 sec)\r\nINFO:tensorflow:Saving checkpoints for 1500 into output/my_model/model.ckpt.\r\nINFO:tensorflow:Loss for final step: 0.30592433.\r\nEvaluating Epoch #5...\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Starting evaluation at 2018-02-20-20:32:12\r\nINFO:tensorflow:Graph was finalized.\r\n2018-02-20 20:32:12.146704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0\r\n2018-02-20 20:32:12.146840: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 45 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nINFO:tensorflow:Restoring parameters from output/my_model/model.ckpt-1500\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Finished evaluation at 2018-02-20-20:32:12\r\nINFO:tensorflow:Saving dict for global step 1500: global_step = 1500, loss = 0.32321918\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Graph was finalized.\r\n2018-02-20 20:32:12.352526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0\r\n2018-02-20 20:32:12.352642: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 45 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nINFO:tensorflow:Restoring parameters from output/my_model/model.ckpt-1500\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nWARNING:tensorflow:From scripts/my_model.py:85: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\ndim is deprecated, use axis instead\r\n2018-02-20 20:32:12.522432: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0\r\n2018-02-20 20:32:12.522561: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 45 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nR@1: 1.7\r\nR@5: 4.3\r\nR@10: 7.4\r\n```\r\n", "comments": ["@zheng-xq @ispirmustafa @martinwicke \r\nWho would be the best person to look into this?\r\nThis can be potentially release blocking.", "@eddie-zhou are you aware of any slowness?\r\nwarmstart-setting changes the initializer-op, it should not impact the global_step/sec. ", "@ispirmustafa is correct, and in various runs I have never encountered any slowness.  Because you're not providing any vocabulary information, the relevant logic is contained entirely in init_from_checkpoint (which you mentioned you saw when directly using it).  And the underlying logic there simply overrides the initialization op, which should only occur once.  Will loop in the author of init_from_checkpoint.  ", "@dacox could you please provide us a single script which reproduces this slowness?", "@vihanjain are you aware of any slowness? ", "@ispirmustafa Sure!\r\n\r\nI've trimmed it down a bit by using a pre-canned estimator, the default optimizer, and not providing any `vars_to_warm_start`.\r\n\r\n**Example Script**\r\n\r\n```py\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport os\r\nimport sys\r\nimport argparse\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\n\r\n\r\nFLAGS = None\r\n\r\n\r\ndef main(unused_args):\r\n    model_dir = os.path.join(FLAGS.model_dir, 'my_model')\r\n\r\n    X_train = np.random.rand(30000, 500)\r\n    y_train = np.random.rand(30000, 2048)\r\n\r\n    print('Train data shape...')\r\n    print(X_train.shape)\r\n    print(y_train.shape)\r\n\r\n    X_dim = X_train.shape[1]\r\n    y_dim = y_train.shape[1]\r\n\r\n    feature_columns = [\r\n        tf.feature_column.numeric_column('x', shape=X_dim)\r\n    ]\r\n\r\n    warm_start_from = None\r\n    if FLAGS.warm_start:\r\n        warm_start_from = tf.estimator.WarmStartSettings(\r\n            ckpt_to_initialize_from=FLAGS.warm_start,\r\n        )\r\n\r\n    model = tf.estimator.DNNRegressor(\r\n        hidden_units=[500, 1000, 2048],\r\n        feature_columns=feature_columns,\r\n        label_dimension=y_dim,\r\n        model_dir=model_dir,\r\n        warm_start_from=warm_start_from\r\n    )\r\n\r\n    train_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n        x={'x': X_train},\r\n        y=y_train,\r\n        batch_size=100,\r\n        num_epochs=5,\r\n        shuffle=True\r\n    )\r\n\r\n    model.train(input_fn=train_input_fn)\r\n\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\r\n        'model_dir',\r\n        nargs='?',\r\n        default='output',\r\n        help='Directory to save resulting model(s).'\r\n    )\r\n    parser.add_argument(\r\n        '--warm_start',\r\n        help='Checkpoint to initialize weights from for fine-tuning.'\r\n    )\r\n\r\n    FLAGS, unparsed = parser.parse_known_args()\r\n\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n```\r\n\r\n**Script Usage**\r\n\r\n```sh\r\n$ mkdir -p output\r\n$ python example.py output/\r\n$ mv output/my_model output/my_checkpoint\r\n$ python example.py --warm_start output/my_checkpoint/ output/\r\n```\r\n**First Epoch**\r\n\r\n```sh\r\n/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\nTrain data shape...\r\n(30000, 500)\r\n(30000, 2048)\r\nINFO:tensorflow:Using default config.\r\nINFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f56c3d6f9d0>, '_eval\r\nuation_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': 'output/my_model', '_save\r\n_summary_steps': 100}\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Graph was finalized.\r\n2018-02-22 23:37:26.110761: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-02-22 23:37:26.207601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-02-22 23:37:26.207969: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties: \r\nname: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 5.93GiB freeMemory: 3.06GiB\r\n2018-02-22 23:37:26.207991: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0\r\n2018-02-22 23:37:26.358043: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2783 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Saving checkpoints for 1 into output/my_model/model.ckpt.\r\nINFO:tensorflow:loss = 71532.54, step = 1\r\nINFO:tensorflow:global_step/sec: 176.009\r\nINFO:tensorflow:loss = 17626.25, step = 101 (0.568 sec)\r\nINFO:tensorflow:global_step/sec: 181.392\r\nINFO:tensorflow:loss = 17433.3, step = 201 (0.551 sec)\r\nINFO:tensorflow:global_step/sec: 182.943\r\nINFO:tensorflow:loss = 17426.316, step = 301 (0.547 sec)\r\nINFO:tensorflow:global_step/sec: 183.914\r\nINFO:tensorflow:loss = 17366.844, step = 401 (0.544 sec)\r\nINFO:tensorflow:global_step/sec: 181.319\r\nINFO:tensorflow:loss = 17432.586, step = 501 (0.552 sec)\r\nINFO:tensorflow:global_step/sec: 183.824\r\nINFO:tensorflow:loss = 17425.293, step = 601 (0.544 sec)\r\nINFO:tensorflow:global_step/sec: 184.213\r\nINFO:tensorflow:loss = 17454.996, step = 701 (0.543 sec)\r\nINFO:tensorflow:global_step/sec: 184.085\r\nINFO:tensorflow:loss = 17382.533, step = 801 (0.543 sec)\r\nINFO:tensorflow:global_step/sec: 183.105\r\nINFO:tensorflow:loss = 17416.637, step = 901 (0.546 sec)\r\nINFO:tensorflow:global_step/sec: 182.509\r\nINFO:tensorflow:loss = 17479.277, step = 1001 (0.548 sec)\r\nINFO:tensorflow:global_step/sec: 183.757\r\nINFO:tensorflow:loss = 17386.068, step = 1101 (0.544 sec)\r\nINFO:tensorflow:global_step/sec: 182.909\r\nINFO:tensorflow:loss = 17392.65, step = 1201 (0.547 sec)\r\nINFO:tensorflow:global_step/sec: 182.233\r\nINFO:tensorflow:loss = 17397.05, step = 1301 (0.549 sec)\r\nINFO:tensorflow:global_step/sec: 183.525\r\nINFO:tensorflow:loss = 17380.441, step = 1401 (0.545 sec)\r\nINFO:tensorflow:Saving checkpoints for 1500 into output/my_model/model.ckpt.\r\nINFO:tensorflow:Loss for final step: 17338.143.\r\n```\r\n\r\n**Second Epoch (with warm-start)**\r\n\r\n```sh\r\n/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\nTrain data shape...\r\n(30000, 500)\r\n(30000, 2048)\r\nINFO:tensorflow:Using default config.\r\nINFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fb55742fa10>, '_evaluation_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': 'output/my_model', '_save_summary_steps': 100}\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='output/my_checkpoint/', vars_to_warm_start='.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})\r\nINFO:tensorflow:Warm-starting from: ('output/my_checkpoint/',)\r\nINFO:tensorflow:Warm-starting variable: dnn/hiddenlayer_1/kernel; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable dnn/hiddenlayer_1/kernel/part_0:0 from checkpoint output/my_checkpoint/ with dnn/hiddenlayer_1/kernel\r\nINFO:tensorflow:Warm-starting variable: dnn/hiddenlayer_0/bias; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable dnn/hiddenlayer_0/bias/part_0:0 from checkpoint output/my_checkpoint/ with dnn/hiddenlayer_0/bias\r\nINFO:tensorflow:Warm-starting variable: dnn/logits/kernel; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable dnn/logits/kernel/part_0:0 from checkpoint output/my_checkpoint/ with dnn/logits/kernel\r\nINFO:tensorflow:Warm-starting variable: dnn/logits/bias; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable dnn/logits/bias/part_0:0 from checkpoint output/my_checkpoint/ with dnn/logits/bias\r\nINFO:tensorflow:Warm-starting variable: dnn/hiddenlayer_2/kernel; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable dnn/hiddenlayer_2/kernel/part_0:0 from checkpoint output/my_checkpoint/ with dnn/hiddenlayer_2/kernel\r\nINFO:tensorflow:Warm-starting variable: dnn/hiddenlayer_2/bias; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable dnn/hiddenlayer_2/bias/part_0:0 from checkpoint output/my_checkpoint/ with dnn/hiddenlayer_2/bias\r\nINFO:tensorflow:Warm-starting variable: dnn/hiddenlayer_0/kernel; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable dnn/hiddenlayer_0/kernel/part_0:0 from checkpoint output/my_checkpoint/ with dnn/hiddenlayer_0/kernel\r\nINFO:tensorflow:Warm-starting variable: dnn/hiddenlayer_1/bias; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable dnn/hiddenlayer_1/bias/part_0:0 from checkpoint output/my_checkpoint/ with dnn/hiddenlayer_1/bias\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Graph was finalized.\r\n2018-02-22 23:37:49.783077: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-02-22 23:37:49.883252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-02-22 23:37:49.883612: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties: \r\nname: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 5.93GiB freeMemory: 3.08GiB\r\n2018-02-22 23:37:49.883628: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0\r\n2018-02-22 23:37:50.032547: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2795 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Saving checkpoints for 1 into output/my_model/model.ckpt.\r\nINFO:tensorflow:loss = 17359.91, step = 1\r\nINFO:tensorflow:global_step/sec: 68.0301\r\nINFO:tensorflow:loss = 17201.738, step = 101 (1.470 sec)\r\nINFO:tensorflow:global_step/sec: 69.0413\r\nINFO:tensorflow:loss = 17168.336, step = 201 (1.448 sec)\r\nINFO:tensorflow:global_step/sec: 69.6031\r\nINFO:tensorflow:loss = 17170.893, step = 301 (1.437 sec)\r\nINFO:tensorflow:global_step/sec: 51.401\r\nINFO:tensorflow:loss = 17156.967, step = 401 (1.947 sec)\r\nINFO:tensorflow:global_step/sec: 64.7499\r\nINFO:tensorflow:loss = 17082.166, step = 501 (1.543 sec)\r\nINFO:tensorflow:global_step/sec: 68.8794\r\nINFO:tensorflow:loss = 17137.012, step = 601 (1.452 sec)\r\nINFO:tensorflow:global_step/sec: 66.9792\r\nINFO:tensorflow:loss = 17190.42, step = 701 (1.493 sec)\r\nINFO:tensorflow:global_step/sec: 69.3956\r\nINFO:tensorflow:loss = 17167.111, step = 801 (1.441 sec)\r\nINFO:tensorflow:global_step/sec: 63.2036\r\nINFO:tensorflow:loss = 17156.027, step = 901 (1.584 sec)\r\nINFO:tensorflow:global_step/sec: 66.5333\r\nINFO:tensorflow:loss = 17144.354, step = 1001 (1.501 sec)\r\nINFO:tensorflow:global_step/sec: 68.6058\r\nINFO:tensorflow:loss = 17067.762, step = 1101 (1.458 sec)\r\nINFO:tensorflow:global_step/sec: 67.3146\r\nINFO:tensorflow:loss = 17135.996, step = 1201 (1.486 sec)\r\nINFO:tensorflow:global_step/sec: 64.1376\r\nINFO:tensorflow:loss = 17125.902, step = 1301 (1.559 sec)\r\nINFO:tensorflow:global_step/sec: 68.5196\r\nINFO:tensorflow:loss = 17054.58, step = 1401 (1.459 sec)\r\nINFO:tensorflow:Saving checkpoints for 1500 into output/my_model/model.ckpt.\r\nINFO:tensorflow:Loss for final step: 17118.121.\r\n```", "@dacox thank you for a simple script!\r\nwe're working on it.", "@dacox Thanks for the script and logs!\r\n\r\nI ran your code on Mac and Ubuntu and I didn't see any slowness between the cold-start run and warm-start run. I ran the code on CPU only. Can you disable GPU for your runs and confirm that you don't see any slowness when everything runs on CPU only?", "@vihanjain I just did a test using CPU only on my laptop(not the same machine, however).\r\nYou're right, I do not see the same slowdown. There is significantly more variance between the batch times when running on my (older) CPU.\r\nWhen I get back on Monday I will try the CPU on the work machine with the GPU. I almost wonder if it's somehow running in CPU mode.", "@vihanjain @ispirmustafa I tried using CPU only on my work desktop(same machine as initial report) and do not see any appreciable difference between runs - it appears it only affects the GPU.\r\nFor the record, each mini-batch on CPU takes `~ 3.5 s`", "The bug was identified and fixed in https://github.com/tensorflow/tensorflow/pull/17312.\r\n\r\nIt should be in TF official release v1.6.0. Please give it a try and let us know if it resolves the training speed issues with your original model.\r\n\r\nThank you @dacox for reporting and for your help in debugging this.", "@vihanjain Thanks for the response! I can confirm that I no longer see the issue using `tensorflow/tensorflow:1.6.0-gpu`.\r\n\r\nCheers!"]}, {"number": 17158, "title": "Feature Request: Better error reporting for tflite conversion", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:MacOS\r\n- **TensorFlow installed from (source or binary)**:source\r\n- **TensorFlow version (use command below)**:1.5.0\r\n- **Python version**: 2.7.10\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: Not using\r\n- **GPU model and memory**: Radeon Pro 455\r\n- **Exact command to reproduce**: In macOS terminal: bazel-bin/tensorflow/contrib/lite/toco/toco \\\r\n  --input_file=../adventures-in-ml-code/tensorflow_word2vec/frozen_graph.pb \\\r\n  --input_format=TENSORFLOW_GRAPHDEF \\\r\n  --output_format=TFLITE \\\r\n  --output_file=../adventures-in-ml-code/tensorflow_word2vec/word2vec.lite \\\r\n  --inference_type=FLOAT \\\r\n  --input_arrays=input \\\r\n  --output_arrays=output \\\r\n  --input_shapes=1 \\\r\n  --output_shapes=1\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\n\r\nOverview\r\nI have been trying to convert a word2vec custom model into tensorflow lite. I am using the code in the tensorflow documentation that was provided as a simple word2vec example.The conversion process up to the freeze_graph.py works fine and I can use that file to run inference no problem in a python script on my desktop. My problem is when I try to convert to lite. \r\n\r\nModel conversation seems to work\r\nWhen I run the command I listed above I get a .tflite file. I can even visualize this file using bazel-bin/tensorflow/contrib/lite/tools/visualize /Users/miperry/Documents/adventures-in-ml-code/tensorflow_word2vec/word2vec.tflite /Users/miperry/Documents/adventures-in-ml-code/tensorflow_word2vec/word2vec_model_viz.html. The model looks fine in the visualizer too.\r\n\r\nUse on mobile doesn't work\r\nWhen I implement my tflite model into an iOS app and try to grab a node with \r\n````\r\nfloat* out = interpreter->typed_tensor<float>();\r\n````\r\nI sometimes get a valid pointer back but other times I get a null pointer depending on the node. \r\n\r\nAfter retraining my model with different shapes of inputs and outputs I finally noticed documentation at the end of the page https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/tf_ops_compatibility.md that states some operations that are present but not ready for custom models. I am using a couple of these operations and I'm guessing this is the reason my lite model isn't working. \r\n\r\nMy reqeust\r\nPlease update the errors available for tflite conversion. I don't understand why it would have been difficult to throw an error on tflite conversion that specifies a node and states that tflite doesn't support its operation. Videos posted by google employees and the introduction to tflite make it sound like this product is ready to make life easier when converting for mobile which is what encouraged me to give it a shot. Now I see that this isn't the case yet. If nothing else I would ask for the sanity of your users that tensorflow documentation more clearly warns about the limitations on mobile. \r\n\r\nTensorflow is a cool concept and the parts that are ready are awesome. Thanks for the great work!\r\n\r\nMy graph code:\r\n````\r\nvalid_dataset = tf.constant(valid_examples, dtype=tf.int32,name=\"input\")\r\n\r\nembeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0),name=\"embeddings\")\r\n\r\nnorm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True),name=\"norm\")\r\nnormalized_embeddings = tf.div(embeddings, norm, 'normalized_embeddings')\r\nvalid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset,name=\"valid_embeddings\")\r\nsimilarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings), transpose_b=False,name=\"similarity\")\r\noutput = tf.reshape(similarity, [-1], name=\"output\")\r\n````\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nOutput from tflite conversion:\r\n````\r\n(adventures-in-ml-code) miperry-macOS:tensorflow miperry$ bazel-bin/tensorflow/contrib/lite/toco/toco   --input_file=../adventures-in-ml-code/tensorflow_word2vec/frozen_graph.pb   --input_format=TENSORFLOW_GRAPHDEF   --output_format=TFLITE   --output_file=../adventures-in-ml-code/tensorflow_word2vec/word2vec.tflite   --inference_type=FLOAT   --input_arrays=input   --output_arrays=output   --input_shapes=1   --output_shapes=1000\r\n2018-02-20 13:06:59.037245: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 14 operators, 21 arrays (0 quantized)\r\n2018-02-20 13:06:59.038319: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 14 operators, 21 arrays (0 quantized)\r\n2018-02-20 13:06:59.072631: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 5 operators, 11 arrays (0 quantized)\r\n2018-02-20 13:06:59.072687: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 5 operators, 11 arrays (0 quantized)\r\n2018-02-20 13:06:59.072731: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:311] Total transient array allocated size: 2401216 bytes, theoretical optimal value: 240000\r\n````", "comments": ["@aselle @andrehentz should be able to triage / comment on whether `toco` should catch/report unsupported operations.", "I'm surprised toco isn't printing out whatever unsupported operation it is finding. The following code should have been triggered:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/tflite/export.cc#L304", "I'm guessing that it was failing on the embedding lookup? I noticed in the docs that this operation wasn't yet supported but it was defined or something like that. \r\n\r\nIf I tried to use the python converter instead of bazel I would get an error about my output size, saying how it should be size 1 when it found 1000. This error was super confusing to me because I didn't find anything in the documentation that says mobile inference was limited to a single output. Also, the flagship tensorflow lite image object recognition example has many output labels (I think 1000). ", "@smoosh911, sorry for the difficulty you are having. We are sorry we weren't clearer that tflite was in developer preview and that means that not-everything is working as well as we would like. I'd like to work to make your case work. Probably the embedding lookup is the difficulty as you say. Could you provide an example of what you are using for vocabulary and valid_dataset?", "@aselle If you could get it working on the basic example provided here https://www.tensorflow.org/tutorials/word2vec then it will work for me.", "Right now string/embedding support is not available. Leaving this bug open to track that.", "Actually, toco supports the embedding_lookup operator, but you should be sure that the input is one dimensional only, or the output dimension will be wrong.", "We've landed quite a few improvements to the converter, including a new conversion backend in the upcoming 2.2 release."]}, {"number": 17157, "title": "LoggingTensorHook to read from runconfig in Estimator", "body": "Fixes https://github.com/tensorflow/tensorflow/issues/17115\r\nBased on the suggestion made in this pull request: https://github.com/tensorflow/tensorflow/pull/17117", "comments": ["Thanks for the contribution. Do you mind also updating the RunConfig pydoc for the log_step_count_steps? I think currently, it says:\r\n\r\n\r\n     log_step_count_steps: The frequency, in number of global steps, that the\r\n          global step/sec will be logged during training\r\n\r\n which does not mention the effect this PR adds. \r\n", "@xiejw Sure, done! Let me know if you think there is a better wording.", "@xiejw Great thanks! Is there anything I need to do to run the CI tests?", "To my best knowledge, one of the admin will handle the rest for you. If you do not see that in one week, feel free to ping the thread and I will help. \r\n\r\nThank you again.", "@xiejw Looks like it hasn't run yet. Is it because there is still an \"awaiting review\" label?", "I reached the yifeif and hope now the labels are correct. ", "@xiejw Looks like there was an infrastructure error. Does that mean the code was broken or things just need to be re-run?", "I kicked multiple runs over the weekend. I believe all errors were transient. Tests look great now. "]}, {"number": 17156, "title": "LookupError: gradient registry has no entry for: Svd", "body": " Hello All,\r\n\r\nI am using singular values of the weights at each convolution layer as a regularizer and adding it in kernel_regularizer=self.l2_reg(). Where l2_reg is the function which return that.\r\n```\r\n\r\n        def l2_reg(weights):\r\n                \"\"\" Reshaping the matrxi in to 2D tensor for enforcing orthogonality\"\"\"\r\n                w = tf.identity(weights)\r\n\r\n                \"\"\"Calculating the Loss Obtained\"\"\"\r\n                s1 = tf.linalg.svd(reg, full_matrices=True,compute_uv=False)\r\n                ortho_loss = tf.nn.l2_loss(s1)\r\n                return ortho_loss\r\n```\r\nError I am getting is:\r\n```\r\nraceback (most recent call last):\r\n  File \"/usr/local/keras-python3/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 551, in gradients\r\n    grad_fn = ops.get_gradient_function(op)\r\n  File \"/usr/local/keras-python3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2134, in get_gradient_function\r\n    return _gradient_registry.lookup(op_type)\r\n  File \"/usr/local/keras-python3/lib/python3.5/site-packages/tensorflow/python/framework/registry.py\", line 93, in lookup\r\n    \"%s registry has no entry for: %s\" % (self._name, name))\r\nLookupError: gradient registry has no entry for: Svd\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"run_dense_net.py\", line 142, in <module>\r\n    model = DenseNet(data_provider=data_provider, **model_params)\r\n  File \"/home/bansa01/densenet_final/tmp_spectral/models/dense_net.py\", line 85, in __init__\r\n    self._build_graph()\r\n  File \"/home/bansa01/densenet_final/tmp_spectral/models/dense_net.py\", line 461, in _build_graph\r\n    cross_entropy + l2_loss_total + reg_loss_total)\r\n  File \"/usr/local/keras-python3/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\", line 343, in minimize\r\n    grad_loss=grad_loss)\r\n  File \"/usr/local/keras-python3/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\", line 414, in compute_gradients\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"/usr/local/keras-python3/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 555, in gradients\r\n    (op.name, op.type))\r\nLookupError: No gradient defined for operation 'Initial_convolution/conv2d/kernel/Regularizer/Svd' (op type: Svd)\r\n```\r\nI Understand that this is due to not able to calculate the gradient for SVD operation, but then how do I remove this issue.\r\nNitin", "comments": ["Please do provide all the information asked for in the issue template  (https://github.com/tensorflow/tensorflow/issues/new), for example which version of TensorFlow are you using?\r\n\r\nSupport for gradients of SVD was added in TensorFlow 1.5 (see https://github.com/tensorflow/tensorflow/issues/6503#issuecomment-335878473 and commit https://github.com/tensorflow/tensorflow/pull/13640/commits/e1d9e4ed05420ab9486bf18ec331e90e59e51982). Is it possible that you're using an older version?", "I have been using tensorflow version 1.4.1.  I do see the commit going in. Do I just need to update my tensorflow?", "Yes, can you try with version 1.5?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 17155, "title": "Memory leak in tf.unique on GPU", "body": "### System 1 information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.4.0\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: 8.0/6.0\r\n- **GPU model and memory**: GeForce GTX TITAN, 6Gb\r\n- **Exact command to reproduce**: n/a\r\n\r\n### System 2 information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.7.0-dev20180219\r\n- **Python version**: 3.6.1\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: 9.0/7.0\r\n- **GPU model and memory**: TITAN X (Pascal), 12Gb\r\n- **Exact command to reproduce**: n/a\r\n\r\n### Describe the problem\r\nThe `tf.unique` op leaks memory when it is placed on GPU. \r\nI wrote a simple script (below) that runs a couple of `tf.unique` ops. I track the memory usage with `psutil.Process(getpid()).memory_info().rss` command. I ran the script on System 1 twice: once on CPU and GPU (specified by `tf.device`), for `num_steps=2000`. Here is the plot of the memory usage:\r\n![mem_usage1](https://user-images.githubusercontent.com/5220571/36442332-583efeb6-166d-11e8-960f-b7bb4990a9a8.png)\r\n\r\nSo, the memory usage of `CPU unique` is pretty flat, but the `GPU unique` is inconclusive. Here is the memory usage on System 1 with `num_steps=10000`:\r\n![mem_usage2](https://user-images.githubusercontent.com/5220571/36442382-88e30238-166d-11e8-8a39-624a80676267.png)\r\n\r\nRelatively recent version of tensorflow (System 2) also has the problem:\r\n![mem_usage3](https://user-images.githubusercontent.com/5220571/36442531-07075100-166e-11e8-84e4-0103c7e4de72.png)\r\n\r\n\r\n### Source code / logs\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport psutil\r\nfrom os import getpid\r\n\r\nval_num = 8*256*256\r\nval_dim = 5\r\nmax_val = 200\r\nnum_steps = 10000\r\n\r\n\r\ndef main():\r\n\r\n    with tf.device(\"/gpu:0\"):\r\n        x = tf.placeholder(tf.int32, [val_num, val_dim])\r\n    \r\n        def tf_unique_row_idxs(inp, max_dim=None, name=''):\r\n            with tf.variable_scope('tf_unique_row_idxs_'+name) as scope:\r\n                if not max_dim:\r\n                    max_dim = inp.get_shape().as_list()[1]\r\n                new_vals = inp[:,0]\r\n                new_vals = tf.cast(new_vals, dtype=tf.int32)\r\n                _, idx = tf.unique(new_vals, out_idx=tf.int32)\r\n                for j in range(1, max_dim):\r\n                    new_vals = inp[:,j]\r\n                    new_vals = tf.cast(new_vals, dtype=tf.int32)\r\n                    val_min = tf.reduce_min(new_vals)\r\n                    val_max = tf.reduce_max(new_vals)\r\n                    idx_shift = val_max - val_min + 1\r\n                    vals = idx*idx_shift + new_vals - val_min\r\n                    uvals, idx = tf.unique(vals, out_idx=tf.int32)\r\n                max_pos = tf.shape(uvals, out_type=tf.int32)[0] + 0\r\n                return idx, max_pos\r\n    \r\n        idxs, max_pos = tf_unique_row_idxs(x)\r\n    \r\n    \r\n    process = psutil.Process(getpid())\r\n\r\n    cur_config=tf.ConfigProto(allow_soft_placement=False,log_device_placement=False)\r\n    sess = tf.Session(config=cur_config)\r\n    sess.run(tf.global_variables_initializer())\r\n    \r\n\r\n    mem_usage = np.zeros([num_steps], dtype=np.float32)\r\n\r\n    np.random.seed(0)\r\n    for i in range(num_steps):\r\n        cur_x = np.random.randint(0, max_val, [val_num, val_dim], dtype=np.int32)\r\n        cur_feed_dict = {x: cur_x}\r\n        cur_max_pos, cur_idxs = sess.run([max_pos, idxs], feed_dict=cur_feed_dict)\r\n        mem_usage[i] = process.memory_info().rss/2**30\r\n        if i%100==0:\r\n            print(i/num_steps)\r\n    \r\n\r\n    np.savetxt('unique_memlog.txt', mem_usage)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nExact command to reproduce", "Can you produce an OOM if you run it long enough or could the plot just be the result of GPU VRAM allocation not being released by the process?", "The memory usage he's showing is CPU memory not GPU VRAM.  Unique also doesn't actually have a GPU implementation - it runs on the CPU even when placed on the GPU.", "I think this is still an issue.", "Any update on this? I am seeing the same issue when tf.unique is being called repeatedly in each iteration. After bunch of iterations, the code goes OOM.\r\n\r\nFor me, even though the tensor is being placed in GPU, the CPU memory keeps increasing and blows the memory at some point. I was using py3.6 and tf-1.4.0", "Is this issue solved? I got CPU RAM memory leak problem while using tf.unique and tf.where as well.", "@alextp @reedwm do you know about the op or can redirect to someone who does?", "@zheng-xq can you take a look?", "This is a stale issue. Please check the issue with latest TensorFlow. If the issue still persists in the newer version of TF, please feel free to reopen it by providing details about the issue and a standalone code to reproduce the issue. Thanks!"]}, {"number": 17154, "title": "Bazel doesn't use the optimization flag specified in configuration", "body": "Hi\r\n\r\nI was compiling TensorFlow on a PowerPC machine and it kept failing with the error \"unrecognized command line option -march=native\" when compiling the file pcre_byte_order.c. This happened even though I specified the default flag -mcpu=native when running ./configure. I also used the --cxxopt=-mcpu=native flag to no avail, and the only way to solve the problem was to remove the --config=opt flag when invoking bazel.", "comments": ["@case540  is already working on the configuration scripts at the moment.\r\nCould you look into this?", "hmm, could you give more specs about your machine. Specifically, what\r\n\r\npython -c \"import platform; print platform.machine()\"\r\n\r\noutputs. If the above outputs 'ppc64le', then --config=opt won't add -march=native flag and will instead add -mcpu=native. ", "Tomorrow I'll run the command and post the output, but the architecture\nshould be ppc64le (I remember seeing it in the name of the whl file). The\nconfigure script did specify -mcpu=native as the optimization flag for\n--config=opt, but -march=native still popped up when compiling one of the\nfiles (and consequently build failed). The problem disappeared when I built\nwithout --config=opt.\n\nIl mer 21 feb 2018, 00:10 Michael Case <notifications@github.com> ha\nscritto:\n\n> hmm, could you give more specs about your machine. Specifically, what\n>\n> python -c \"import platform; print platform.machine()\"\n>\n> outputs. If the above outputs 'ppc64le', then --config=opt won't add\n> -march=native flag and will instead add -mcpu=native.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/17154#issuecomment-367153066>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AGzyMIxvfxL2MW1XRVN2Nn8i31revd6Fks5tW1DegaJpZM4SMRcs>\n> .\n>\n", "We merged a potential fix. Could you test it out when you get the chance?", "I tried to compile from master and everything went smooth (just a few warnings)", "@eann Thank you very much for verifying!\r\n"]}, {"number": 17153, "title": "How to sync worker models of KMeansClustering in distributed tensorflow?", "body": "### System information\r\n-**Have I written custom code**: yes\r\n-**OS Platform and Distribution**: Open SUSE Leap 42.3\r\n-**TensorFlow installed from:** python pip\r\n-**TensorFlow version**: 1.6.0\r\n-**Python version**: 2.7\r\n-**Bazel version**  :N/A\r\n-**CUDA/cuDNN version** : N/A\r\n-**GPU model and memory** : N/A\r\n-**Exact command to reproduce** : N/A\r\n\r\nHi, \r\nI am trying to use distributed tensorflow over KMeansClustering. I have one **parameter server** and two **workers**. **Training data** in both the workers are **different**.  After training, cluster centers in the two workers are different. Is there a function in tensorflow which can be called to sync the models while training so that the cluster   centers are similar if not same.\r\n\r\n**Source Code** \r\n\r\n```\r\ndef startCluster(jobName,taskId):\r\n    parameter_servers = [\"localhost:2222\"]\r\n    workers = [\"localhost:2223\",\r\n               \"localhost:2224\"]\r\n\r\n    cluster = tf.train.ClusterSpec({\"ps\": parameter_servers, \"worker\": workers})\r\n    server = tf.train.Server(\r\n        cluster,\r\n        job_name=jobName,\r\n        task_index=taskId)\r\n\r\n    return cluster,server\r\n\r\ndef trainModelInParallel(cluster,server,taskId):\r\n    k = 4\r\n    n = 1000\r\n    variables = 2\r\n\r\n    points = np.random.uniform(0, 1, [n, variables])\r\n\r\n    # Between-graph replication\r\n    with tf.device(tf.train.replica_device_setter(\r\n            worker_device=\"/job:worker/task:%d\" % taskId,\r\n            cluster=cluster)):\r\n\r\n        input_fn = lambda: tf.train.limit_epochs(tf.convert_to_tensor(\r\n                                    points, dtype=tf.float32), num_epochs=1)\r\n        kmeans = tf.contrib.factorization.KMeansClustering(\r\n            num_clusters=k, use_mini_batch=False,model_dir=defaultModel)\r\n\r\n\r\n    sv = tf.train.Supervisor(is_chief=(taskId == 0),save_model_secs=1)\r\n\r\n    with sv.prepare_or_wait_for_session(server.target) as sess:\r\n\r\n        for _ in xrange(10):\r\n            kmeans.train(input_fn)\r\n            centers = kmeans.cluster_centers()\r\n            print centers[0],centers[1],centers[2],centers[3]\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    if len(sys.argv) == 1:\r\n        print\"Please pass job name and task ID\"\r\n        sys.exit()\r\n\r\n    jobName = sys.argv[1]\r\n    taskId = (sys.argv[2])\r\n    defaultModel += taskId\r\n    cluster, server = startCluster(jobName, int(taskId))\r\n\r\n    if jobName == \"ps\":\r\n        server.join()\r\n    else:\r\n        trainModelInParallel(cluster,server,int(taskId))\r\n\r\n```\r\n**Command Line** \r\nTo execute the above code please enter following commands in 3 different terminals:\r\n```\r\npython kmeansDistributed.py ps 0\r\npython kmeansDistributed.py worker 0\r\npython kmeansDistributed.py worker 1\r\n```\r\n\r\n@ccolby Your inputs will be very helpful.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@tensorflowbutler I have updated my previous comment.", "You may need to add a barrier and do manual sync.\r\n\r\nThis question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 17152, "title": "docker ci allows bad name user", "body": "Our server maintainer creates user name like \"xxx.yyy\", which is invalid for `adduser` and so docker ci failed. I hope tensorflow could provide a way to loose the restriction:\r\n1. use `--force-badname` for `adduser`.\r\n2. or, we can provide `CI_BULID_USER` for `ci_builder.sh`.", "comments": []}, {"number": 17151, "title": "ERROR: Unrecognized option: --python_path", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.12.6\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: commit e5e03ef3148303b3dfed89a1492dedf92b45be25 (HEAD -> master, origin/master, origin/HEAD)\r\n- **Python version**:  Python 2.7.13\r\n- **Bazel version (if compiling from source)**: 0.10.1\r\n- **GCC/Compiler version (if compiling from source)**:  4.2.1\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**: see log\r\n\r\n\r\n### Describe the problem\r\nTF failed to build from source.\r\nWhen building from source, Bazel does not recognize --python_path set during ./configure\r\nPlease see the log below.\r\n\r\n### Source code / logs\r\n**C02PK120FVH6**:tensorflow neitan01$ ./configure\r\nWARNING: current bazel installation is not a release version.\r\nMake sure you are running at least bazel 0.5.4\r\nPlease specify the location of python. [Default is /usr/local/opt/python/bin/python2.7]: \r\n\r\n\r\nFound possible Python library paths:\r\n  /usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages]\r\n\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: y\r\nGoogle Cloud Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Hadoop File System support? [Y/n]: n\r\nNo Hadoop File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: n\r\nNo Amazon S3 File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Apache Kafka Platform support? [y/N]: n\r\nNo Apache Kafka Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: y\r\nXLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with GDR support? [y/N]: y\r\nGDR support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with VERBS support? [y/N]: y\r\nVERBS support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: n\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: n\r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See tools/bazel.rc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\nConfiguration finished\r\n\r\n\r\n**C02PK120FVH6**:tensorflow neitan01$ bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.1 --copt=-msse4.2 -k //tensorflow/tools/pip_package:build_pip_package\r\nKilled non-responsive server process (pid=1017)\r\n.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=181\r\nINFO: Reading options for 'build' from /Users/neitan01/src/tensorflow/tools/bazel.rc:\r\n  'build' options: --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --define=grpc_no_ares=true --spawn_strategy=standalone --genrule_strategy=standalone -c opt\r\nINFO: Reading options for 'build' from /Users/neitan01/src/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/usr/local/opt/python/bin/python2.7 --action_env PYTHON_LIB_PATH=/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages --force_python=py2 --host_force_python=py2 --python_path=/usr/local/opt/python/bin/python2.7 --define with_gcp_support=true --define with_xla_support=true --define with_gdr_support=true --define with_verbs_support=true --action_env TF_NEED_OPENCL_SYCL=0 --action_env TF_NEED_CUDA=0 --define grpc_no_ares=true --copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK --host_copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK\r\n**ERROR: Unrecognized option: --python_path=/usr/local/opt/python/bin/python2.7**\r\n\r\n\r\n**C02PK120FVH6**:tensorflow neitan01$ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=181\r\nINFO: Reading options for 'build' from /Users/neitan01/src/tensorflow/tools/bazel.rc:\r\n  'build' options: --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --define=grpc_no_ares=true --spawn_strategy=standalone --genrule_strategy=standalone -c opt\r\nINFO: Reading options for 'build' from /Users/neitan01/src/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/usr/local/opt/python/bin/python2.7 --action_env PYTHON_LIB_PATH=/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages --force_python=py2 --host_force_python=py2 --python_path=/usr/local/opt/python/bin/python2.7 --define with_gcp_support=true --define with_xla_support=true --define with_gdr_support=true --define with_verbs_support=true --action_env TF_NEED_OPENCL_SYCL=0 --action_env TF_NEED_CUDA=0 --define grpc_no_ares=true --copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK --host_copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK\r\nERROR: Unrecognized option: --python_path=/usr/local/opt/python/bin/python2.7\r\n\r\n\r\n**C02PK120FVH6**:tensorflow neitan01$ bazel version\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Thu Jan 01 00:00:00 1970 (0)\r\nBuild timestamp: Thu Jan 01 00:00:00 1970 (0)\r\nBuild timestamp as int: 0\r\n\r\n\r\n**C02PK120FVH6**:tensorflow neitan01$ brew info bazel\r\n**bazel: stable 0.10.1** (bottled)\r\nGoogle's own build tool\r\nhttps://bazel.build/\r\n/usr/local/Cellar/bazel/0.5.1 (10 files, 138.0MB)\r\n  Poured from bottle on 2017-06-16 at 16:45:38\r\n/usr/local/Cellar/bazel/0.10.1 (12 files, 93.4MB) *\r\n  Poured from bottle on 2018-02-15 at 23:19:51\r\nFrom: https://github.com/Homebrew/homebrew-core/blob/master/Formula/bazel.rb\r\n==> Requirements\r\nRequired: java = 1.8 \u2714, macOS >= 10.10 \u2714\r\n==> Caveats\r\nBash completion has been installed to:\r\n  /usr/local/etc/bash_completion.d\r\n\r\nzsh completions have been installed to:\r\n  /usr/local/share/zsh/site-functions\r\n", "comments": ["My build-environment was setup incorrectly. The bazel command was taken from an old-version of binary in the user space."]}, {"number": 17150, "title": "Problem with Keras sparse_categorical_crossentropy", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary (pip install)\r\n- **TensorFlow version (use command below)**: 1.5.0 (Keras 2.1.2-tf)\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: Can't remember.\r\n- **GPU model and memory**: GTX 1070\r\n- **Exact command to reproduce**: See below.\r\n\r\n### Background\r\n\r\nThis issue seems to be specifically about Keras with TensorFlow so I have posted it here.\r\n\r\nI have a Keras model for doing Machine Translation of human languages. It has an encoder and decoder each of which use the `Embedding` and `GRU` layers from Keras. The output of the decoder is a one-hot encoded array.\r\n\r\nMy data-set is from Europarl so it is very large already and converting the target-data from integer-tokens to one-hot-encoded labels would be extremely wasteful and take many GB of memory.\r\n\r\nOne solution would be to write my own data-generator and only convert integer-tokens to one-hot-labels for a batch at a time. But that's not a very elegant solution.\r\n\r\nThe correct solution is of course to use a sparse version of the crossentropy-loss which automatically converts the integer-tokens to a one-hot-encoded label for comparison to the model's output. Keras' has a built-in loss-function for doing exactly this called `sparse_categorical_crossentropy`. However, it doesn't seem to work as intended.\r\n\r\n### Error\r\n\r\nThe following shows the essential parts of the code.\r\n\r\n    # (Omitted code for building neural network ...)\r\n\r\n    # Output of the decoder-part of the neural network.\r\n    decoder_dense = Dense(num_words,\r\n                          activation='softmax',\r\n                          name='decoder_output')\r\n    decoder_output = decoder_dense(decoder_gru_output)\r\n\r\n    model = Model(inputs=[encoder_input, decoder_input],\r\n                  outputs=[decoder_output])\r\n\r\n    model.compile(optimizer='adam',\r\n                  loss='sparse_categorical_crossentropy')\r\n\r\n    model.fit(x=x_data, y=y_data, batch_size=128, epochs=3)\r\n\r\nEverything runs fine except `model.fit()` at the end which gives this error:\r\n\r\n    ValueError: Error when checking target: expected decoder_output to have 3 dimensions, but got array with shape (20000, 67)\r\n\r\nThis is the shape of the model's output:\r\n\r\n    >>> decoder_output.get_shape()\r\n    TensorShape([Dimension(None), Dimension(None), Dimension(10000)])\r\n\r\nThis is the shape of the target-data, which is a 2-dim array of integer-values:\r\n\r\n    y_data['decoder_output'].shape\r\n    >>> (20000, 67)\r\n\r\nNote that I only allow sequences of length 67 for the decoder's output.\r\n\r\n### Working Solution\r\n\r\nWe can use TensorFlow's implementation of sparse cross-entropy, which seems to work as intended.\r\n\r\nFirst we need to have a linear activation on the output of the decoder:\r\n\r\n    decoder_dense = Dense(num_words,\r\n                          activation='linear', # NOTE: changed from 'softmax'\r\n                          name='decoder_output')\r\n\r\nThen we need a wrapper-function for the loss that is compatible with Keras:\r\n\r\n    def sparse_loss(y_true, y_pred):\r\n        return tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true,\r\n                                                              logits=y_pred)\r\n\r\nThen we need to create a placeholder variable for the batch of target-values. Once again I only allow sequences of length 67 (this is of course a variable in my own code).\r\n\r\n    decoder_target = tf.placeholder(dtype='int32', shape=(None, 67))\r\n\r\n    model_train.compile(optimizer='adam,\r\n                        loss=sparse_loss,\r\n                        target_tensors=[decoder_target])\r\n\r\nThis works fine and we can train it by calling:\r\n\r\n    model.fit(x=x_data, y=y_data, batch_size=128, epochs=3)\r\n\r\nMaybe Keras should use TensorFlow's sparse-cross-entropy more directly, because it seems to handle higher-dim data better?\r\n\r\n### Documentation\r\n\r\nLooking at the implementation of `sparse_categorical_crossentropy` in Keras there is actually some reshaping going on there, but the doc-string doesn't make clear what is assumed of the input/output dims and when/how reshaping is supposed to be done, so it's impossible to know whether it is a bug or a feature I am experiencing, and how to deal with it properly.\r\n\r\nThe doc-string needs to be made more clear by someone who understands the intention of this code.\r\n\r\nFurthermore, the doc-string needs to be \"exported\" somehow to the online docs because it is not shown here: https://keras.io/losses/#sparse_categorical_crossentropy\r\n", "comments": ["Thanks for the report.\r\nCC @fchollet @anj-s for comment.", "Furthermore, I'm not quite sure whether my custom `sparse_loss()` function should reduce the loss-values from a batch-tensor to a single scalar value, e.g. like this:\r\n\r\n    def sparse_loss(y_true, y_pred):\r\n        foo = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true,\r\n                                                             logits=y_pred)\r\n        return tf.reduce_mean(foo)\r\n\r\nKeras' `fit()` function has a parameter `sample_weight` which should calculate the weighted average of the loss across the batch, but I'm not sure it does that when I provide both a custom loss function and `target_tensors`.\r\n\r\nThese are generally quite confusing aspects of the Keras API. I've looked in the source-code but it's really hard to understand because it's so dense and has practically no comments.", "I have a working example of my custom loss-function which is a work-around for the bug in Keras. Search for `sparse_cross_entropy`:\r\n\r\nhttps://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/21_Machine_Translation.ipynb", "@Hvass-Labs : \r\n\r\nyour workaround will be compatible with sample weighting, if you use `tf.reduce_mean(foo, axis=-1)` instead of `tf.reduce_mean(foo)`.\r\n\r\nThanks for the bug report. I verified the issue. Here is a reproducible example:\r\n\r\n```python\r\nimport keras  # also works for tf.keras\r\nimport numpy as np\r\n\r\nmodel = keras.Sequential()\r\nmodel.add(keras.layers.Dense(10, input_shape=(5, 6)))\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy')\r\n\r\nx_data = np.random.random((32, 5, 6))\r\ny_data = np.random.randint(0, 9, size=(32,))\r\n\r\nmodel.fit(x=x_data, y=y_data, batch_size=16, epochs=3)\r\n```\r\n\r\nI will look into a fix.\r\n\r\n\r\n", "For seq2seq at least, I think the suggested workaround with a custom `sparse_loss` function should not return a scalar (i.e. don't use `reduce_mean`). The `weighted_masked_objective` decorator will take care of that. If the loss function returns a scalar, then masking will not work appropriately. I also don't think using `tf.reduce_mean(foo, axis=-1)` will work either as it will change the shape from (samples, time-steps) to (samples) also breaking masking.", "@fchollet I'm not sure if you've actually started on this ticket yet, but I have been reading through the Keras code lately and wouldn't mind taking a stab at addressing this issue if that would be helpful. I'm currently rolling with a similar workaround so I have a vested interest in getting this resolved.", "The Keras objectives documentation says to expand the dimensions of the labels using `y=np.expand_dims(y, -1)`, so I've done this to both my data (X) and my labels and it seems to be running fine. Is this a possible solution or not the same problem?", "@Krumpet you might be on to something here. I think our reproduction example is actually missing a steps dimension for the labels, so `y_data = np.random.randint(0, 9, size=(32,))` should really be `y_data = np.random.randint(0, 9, size=(32,5))`, giving us a reproduction example of:\r\n\r\n```\r\nimport keras\r\nimport numpy as np\r\n\r\nmodel = keras.Sequential()\r\nmodel.add(keras.layers.Dense(10, input_shape=(5, 6)))\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy')\r\n\r\nx_data = np.random.random((32, 5, 6))\r\ny_data = np.random.randint(0, 9, size=(32,5))\r\n\r\nmodel.fit(x=x_data, y=y_data, batch_size=16, epochs=3)\r\n```\r\n\r\nThat gives the same sort of error about the shape:\r\n\r\n```\r\nValueError: Error when checking target: expected dense_1 to have 3 dimensions, but got array with shape (32, 5)\r\n```\r\n\r\nIf we expand the labels dimensions to have shape `(32, 5, 1)` like so:\r\n\r\n```\r\nimport keras\r\nimport numpy as np\r\n\r\nmodel = keras.Sequential()\r\nmodel.add(keras.layers.Dense(10, input_shape=(5, 6)))\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy')\r\n\r\nx_data = np.random.random((32, 5, 6))\r\ny_data = np.random.randint(0, 9, size=(32,5,1))\r\n\r\nmodel.fit(x=x_data, y=y_data, batch_size=16, epochs=3)\r\n```\r\n\r\nThe error does appear to go away (with TensorFlow backend at least, I'm not up and running with Theano presently). I'm going to kick the tires a bit more and make sure this is really working and things like masking still function as we expect. If someone could check the above with Theano, that would be helpful.", "I am convinced this is working. I did a little test to make sure masking is handled correctly, and all looks well:\r\n\r\n```python\r\nfrom tensorflow import keras\r\nimport numpy as np\r\n\r\ndense_layer = keras.layers.Dense(10, activation='softmax')\r\n\r\nmodel = keras.Sequential()\r\nmodel.add(keras.layers.Embedding(11, 6, mask_zero=True, embeddings_initializer=keras.initializers.Constant(value=0.1)))\r\nmodel.add(dense_layer)\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy')\r\n\r\n# Set the weights for our dense layer such that each category has the same\r\n# probability except for 1, which will have a lower probability\r\nnew_weights = []\r\nfor weight in dense_layer.get_weights():\r\n    if weight.shape == (6,10):\r\n        weight = np.full_like(weight, 0.1, dtype='float32')\r\n        weight[:,1] = -2.\r\n    new_weights.append(weight)\r\ndense_layer.set_weights(new_weights)\r\n\r\n# Create random dataset where each sample has a different length with 0 padding\r\n# and none of the labels use category 1\r\nx_data = np.zeros((32, 20))\r\ny_data = np.zeros((32, 20, 1))\r\nfor i in range(32):\r\n    length = np.random.randint(1, 20)\r\n    x_data[i, 0:length] = np.random.randint(2, 10, size=length)\r\n    y_data[i, 0:length] = np.random.randint(2, 10, size=(length, 1))\r\n\r\n# Evaluate to get the cost\r\ncost1 = model.evaluate(x_data, y=y_data)\r\n\r\n# Change the 0s to 1s in the labels. If masking is handled correctly, this\r\n# shouldn't matter as these labels will be ignored anyway. If masking is not\r\n# handled correctly, then the cost should go up as the probability of 1 being\r\n# the correct category is lower than any other category\r\ny_data[y_data == 0] = 1\r\n\r\ncost2 = model.evaluate(x_data, y=y_data)\r\n\r\nassert cost1 == cost2\r\n```\r\n\r\nIt is possible that there needs to be an update to the documentation to make it clearer as to what the shape of the labels should be. It is perhaps a bit counter intuitive that an extra dimension of size 1 needs to be tacked on. Code-wise though, this appears to be working.", "@Hvass-Labs @johntrimble @fchollet any word on a fix here?  I am facing this issue currently with:\r\n\r\n    File \"/home/david/.local/lib/python3.6/site-packages/keras/engine/training.py\", line 787, in _standardize_user_data exception_prefix='target')\r\n    File \"/home/david/.local/lib/python3.6/site-packages/keras/engine/training_utils.py\", line 127, in standardize_input_data 'with shape ' + str(data_shape))\r\n    \r\n    ValueError: Error when checking target: \r\n    expected decoder_output to have 3 dimensions, but got array with shape (2000, 22)\r\n\r\nI plan to one hot encode to finish the work at hand, but this feature is great and would be useful.  ", "@mobiusinversion so I'm not sure this is actually a bug. Check my [example above](https://github.com/tensorflow/tensorflow/issues/17150#issuecomment-399776510). In particular, notice how when I change the labels from having a dimension of `(32,5)` to a dimension of `(32,5,1)` the problem goes away. You can use `expand_dims` to add that extra dimension. Let me know if that solves your problem. ", "@johntrimble that works.  This issue should probably be closed (@Hvass-Labs) in favor of an enhancement, it is a bit of lint that adding the extra dimension is required.  ", "So I was having this problem and decided to look at the code for model.fit. This is the section that's giving us problems (lines 1133 - 1149 of tf 1.12.0)\r\n\r\n```\r\n        for output_shape, loss_fn in zip(self._feed_output_shapes,\r\n                                         self._feed_loss_fns):\r\n          if loss_fn is losses.sparse_categorical_crossentropy:\r\n            if K.image_data_format() == 'channels_first':\r\n              feed_output_shapes.append(\r\n                  (output_shape[0], 1) + output_shape[2:])\r\n            else:\r\n              feed_output_shapes.append(output_shape[:-1] + (1,))\r\n          elif (not hasattr(loss_fn, '__name__') or\r\n                getattr(losses, loss_fn.__name__, None) is None):\r\n            # If `loss_fn` is not a function (e.g. callable class)\r\n            # or if it not in the `losses` module, then\r\n            # it is a user-defined loss and we make no assumptions\r\n            # about it.\r\n            feed_output_shapes.append(None)\r\n          else:\r\n            feed_output_shapes.append(output_shape)\r\n```\r\n\r\n\r\nPretty much if you're using tf.keras.losses.sparse_categorical_crossentropy then it adds an extra dimension to your shapes. This is why if you wrap tf.keras.losses.sparse_categorical_crossentropy with another function then you can bypass this extra dimension. ", "```\r\ndef accuracy_seq2seq(y_true, y_pred):\r\n    y_pred_ = K.cast(K.argmax(y_pred, axis=-1),'int32')\r\n    correct = K.cast(K.equal(y_true, y_pred_), 'int32')\r\n    flattened = K.flatten(correct)\r\n    length = K.shape(flattened)[0]\r\n    acc = tf.reduce_sum(flattened) / length\r\n    return acc\r\n```\r\ni was having problems compiling the model with the default accuracy keras provides, i used this snippet along with the workaround @Hvass-Labs  provided. i can confirm it works on tf 1.12.0", "@Hvass-Labs Which metrics from keras you would recommend for sparse_softmax_... loss function? I tried `sparse_categorical_accuracy` as [here](https://www.dlology.com/blog/how-to-use-keras-sparse_categorical_crossentropy/) but giving data type issue.", "@Hvass-Labs We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.6 which is latest stable version of TF and let us know if the issue still persists in newer versions. please check https://github.com/tensorflow/tensorflow/issues/17150#issuecomment-464426225 .Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/17150\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/17150\">No</a>\n"]}]