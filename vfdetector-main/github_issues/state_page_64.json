[{"number": 31932, "title": "How to create Python extension module that uses TensorFlow C API?", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 1.14.0\r\n- Python version: 3.7.3\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n\r\n\r\nI've written a Python extension module that uses TensorFlow through the C API. I installed the API as described in https://www.tensorflow.org/install/lang_c. On its own, my module works correctly. But if I import my extension module and then also `import tensorflow`, Python crashes with this error.\r\n\r\n```\r\n[libprotobuf ERROR external/protobuf_archive/src/google/protobuf/descriptor_database.cc:58] File already exists in database: tensorflow/core/protobuf/master.proto\r\n[libprotobuf FATAL external/protobuf_archive/src/google/protobuf/descriptor.cc:1370] CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): \r\nlibc++abi.dylib: terminating with uncaught exception of type google::protobuf::FatalException: CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): \r\nAbort trap: 6\r\n```\r\n\r\nI believe this happens because the C API includes its own complete copy of TensorFlow, so now I get two different copies loaded into the same process at the same time.\r\n\r\nWhat is the solution to this? How can I have Python code that uses TensorFlow, and also invokes C code that uses TensorFlow?\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@peastman Please take a look at the [similar issue](http://derivative.ca/Forum/viewtopic.php?f=27&t=19692) and let me know if it helps. Thanks!", "Thanks.  I'm not sure what that has to do with this case?  None of my code does anything with protobuf directly.  I just have a module that links against the libtensorflow that comes with the C API.  Then I open up a Python console and try importing modules.  I can either import the module I wrote or import TensorFlow and either one works.  But if I try to import both, Python immediately crashes.", "Is there any solution to this problem?", "Having the same issue on Mac OS Catalina. Found this solve by downgrading pyarrow, but it does not work for me:\r\nhttps://superuser.com/questions/1491364/attempted-import-of-apache-beam-in-python3-on-mac-throws-fatal-protobuf-error", "I'm still having the same issue. I tried using pyarrow 0.14.0 and the newest 0.15.1. those pyarrow didn't works in my catalina", "reinstalling tensorflow works for me ", "This problem is still present in TensorFlow 2.x.  Nothing has changed since I filed this issue a year and a half ago.  Do not close this issue.", "Please reopen this issue!  It has *not* been fixed.  It is still present in the most recent version of TensorFlow.  Nothing at all has changed.\r\n\r\nWhy was this closed?  Will anyone from the TensorFlow team even so much as post to acknowledge the existence of this issue, which is a showstopper and has forced me to put all TensorFlow support on hold?", "Hi @peastman , I also encountered the same problem when building my c++ project. I found that when I import tensorflow before my lib, everything works normally. Maybe you can try it. It's weird and I'm searching for the reason why this happens.\r\n\r\nSee like this:\r\n\r\n```\r\nroot@f2ddedcd3964:/# ipython\r\nPython 3.7.7 (default, Mar 23 2020, 22:36:06)\r\nType 'copyright', 'credits' or 'license' for more information\r\nIPython 7.13.0 -- An enhanced Interactive Python. Type '?' for help.\r\n\r\nIn [1]: import mylib\r\n\r\nIn [2]: import tensorflow\r\n[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/descriptor_database.cc:118] File already exists in database: tensorflow/core/framework/tensor.proto\r\n[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/descriptor.cc:1367] CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size):\r\nterminate called after throwing an instance of 'google::protobuf::FatalException'\r\n  what():  CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size):\r\nAborted (core dumped)\r\nroot@f2ddedcd3964:/# ipython\r\nPython 3.7.7 (default, Mar 23 2020, 22:36:06)\r\nType 'copyright', 'credits' or 'license' for more information\r\nIPython 7.13.0 -- An enhanced Interactive Python. Type '?' for help.\r\n\r\nIn [1]: import tensorflow\r\n\r\nIn [2]: import mylib\r\n\r\nIn [3]:\r\n```", "Ideally you could use the python versions implementation for your C extension. I.e. don't link TensorFlow but use the C header files. For this to work you'd need to still import tensorflow in your extension. If you want to make it work both if you have only the C library version and a full tensorflow python binding install, then you could check in your extensuion whether to dlopen() (or windows equivalent) the C library... i.e. check if you can find the symbols are already available somehow with dlsym() ... This is definitely in a tricky area where using the python install if possible even for your C extension is preferable."]}, {"number": 31826, "title": " TensorFlow Lite GPU on Android slower than CPU", "body": "**System information**\r\nAndroid 9.0, XiaoMi9, snapdragon855\r\n- TensorFlow installed from (source or binary):\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo\r\n- TensorFlow version (or github SHA if from source):\r\ndependencies {\r\n    ...\r\n    implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'\r\n    implementation 'org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly'\r\n}\r\n\r\nI run a app from https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo, and it worked well, the run time of  default model(mobilenet) on GPU is half of CPU, which is normal I think. And I replace the model with a special model which structure like YOLOv2-Tiny, trained by keras and convert to tflite(Implemented by traditional conv2D and pooling), I found the speed on GPU is slower than CPU float(CPU is less than 500ms but GPU is more than 600ms),  is it normal?\r\nis there anyone help me?\r\n", "comments": ["Have you sloved your problem? I also encountered the same problem. The model I use is YOLOv3 and also trained by keras.", "> Have you sloved your problem? I also encountered the same problem. The model I use is YOLOv3 and also trained by keras.\r\n\r\nNo i'm not solved this problem yet, and i don't have so many Android phones to test this. I tested in a xiaomi 5 with snapdragan 820 SoC and the problem still remained. I think right now the best way is waiting and hope tensorflow lite will fix this problem in the future.", "OMG this issue completely dropped off my radar.  Didn't even know this was assigned to me.  My apologies.\r\n\r\nI'm not familiar with YOLO, so would be great if you can share your YOLOv2-Tiny TFLite model somehow.  Either the entire network is not compatible with TFLite GPU (unsupported ops) and thus only parts of it runs on the GPU and the rest on the CPU.  This can cause in a significant performance loss.\r\n\r\nAlso, if YOLO employs FC layers, performance can suffer, because FC on OpenGL is not that fast, and sometimes slower than CPU.", "> OMG this issue completely dropped off my radar. Didn't even know this was assigned to me. My apologies.\r\n> \r\n> I'm not familiar with YOLO, so would be great if you can share your YOLOv2-Tiny TFLite model somehow. Either the entire network is not compatible with TFLite GPU (unsupported ops) and thus only parts of it runs on the GPU and the rest on the CPU. This can cause in a significant performance loss.\r\n> \r\n> Also, if YOLO employs FC layers, performance can suffer, because FC on OpenGL is not that fast, and sometimes slower than CPU.\r\n\r\nI\u2018m sorry that i don't have this model in my computer right now, but i do remember YOLOv2-Tiny consists of maxpooling and convolution layers, no FCs, so I dont know why. I'll do more tests.", "@EarlHsy Hi\uff0c I encountered another question. I found that the detection results are also different. I checked the outputmap of function \"runForMultipleInputsOutputs\" and found that the values in the outputmap are also totally different. Do you have the same problem,too?"]}, {"number": 31579, "title": "[TF 2.0] allow tf.function input_signature to be specified by annotations", "body": "**System information**\r\n\r\n- TensorFlow version (you are using): 2.0.0-rc0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n`tf.function` has an argument `input_signature` which I have been using to try and make my code a bit safer and ensure I don't keep re-tracing functions. The `input_signature` specifies the tensor type for each of the function arguments. It would be much nicer (I think) to specify these types using python (>=3.5) annotations, where a suitable version of python is available. A very rough example looks like:\r\n\r\n``` python\r\nimport tensorflow as tf\r\n\r\n\r\ndef function(fn):\r\n    input_signature = list(fn.__annotations__.values())\r\n    return tf.function(fn, autograph=False, input_signature=input_signature)\r\n\r\n\r\n@function\r\ndef foo(\r\n    x: tf.TensorSpec(shape=[None], dtype=tf.float64),\r\n    y: tf.TensorSpec(shape=[None], dtype=tf.float64),\r\n):\r\n    return x + 10.0 + y\r\n\r\n\r\nvec32 = tf.random.normal([2], dtype=tf.float32)\r\nvec64 = tf.random.normal([2], dtype=tf.float64)\r\n\r\n\r\n# should pass\r\nfoo(vec64, vec64)\r\nfoo(y=vec64, x=vec64)\r\n\r\n# should fail\r\nfoo(vec32, vec64)\r\n```\r\n\r\nWhich I think is nicer than the current signature:\r\n\r\n``` python\r\n@tf.function(\r\n    autograph=False,\r\n    input_signature=[\r\n        tf.TensorSpec(shape=[None], dtype=tf.float64),\r\n        tf.TensorSpec(shape=[None], dtype=tf.float64),\r\n    ],\r\n)\r\ndef foo(x, y):\r\n    return x + 10.0 + y\r\n```\r\n\r\nI think the main benefit of the annotation approach is that the argument name and type are beside each other, and this syntax is already widely used in python.\r\n\r\nIn order to enable using annotations as the `input_signature` I think there should be an extra boolean argument to `tf.function` called e.g. `use_annotation_input_signature` which defaults to `False`.\r\n\r\nAlso note I have set `autograph=False` here to avoid a warning:\r\n\r\n> Cause: name 'foo_scope' is not defined\r\n\r\nI am guessing a proper implementation inside of `tf.function` would not have this problem.\r\n\r\n**Will this change the current api? How?**\r\n\r\nIt would add an additional argument to `tf.function` which at the default value would not change anything.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nAnyone using python >= 3.5 who would like to specify the tensor types of their functions.\r\n\r\n**Any Other info.**\r\n\r\nNone", "comments": ["the concept of \"None\" == \"Anything\" is dumb and reflects poorly on Tensorflow and Keras team + users", "Hi, I like the general idea of leveraging more standard Python features.  Though one property we want to keep for @tf.function is that preserving the same semantics with or without @tf.function, and since we're not passing tf.TensorSpec instance as an argument, annotating the arguments to be tf.TensorSpec won't be strictly correct.", "Hi @kkimdev. Yes I realised that `TensorSpec` wasn't the right type shortly after writing this and agree it is weird/confusing to annotate the arguments with the wrong type. I can't think of a solution which would work for this now, since the \"spec\" (e.g. can choose any shape based on runtime arguments) is dynamic and the annotations are really meant to be static so that mypy etc can do static analysis.\r\n\r\nWill close this later unless anyone has any ideas on how it could work properly.", "Closing as it has been 2 weeks.", "Why not make it possible to use `tf.Tensor[shape, dtype]` as an annotation? This would also be useful in general to document the input requirements.", "Yes, that would be the ideal format. I think you can use tf.Tensor (untemplated) right now in your regular type annotations, but implementing it as a [generic type](https://www.python.org/dev/peps/pep-0484/#user-defined-generic-types) that captures both dtype and shape would take a bit more programming in the definition of the Tensor classes.", "@Danmou yes that's a good idea - not sure why I didn't do that in the first place. I'll re-open this and try to have a go at implementing it but if anyone has any ideas/would like to help that'd be great.\r\n\r\nEDIT: I can't re-open this, @kkimdev if you think this would be useful at all can you re-open please?", "I've tried to knock up some code to try and figure out how this might work but am finding it quite hard (mainly due to my lack of experience with typing). If there is any feedback on this it would be great:\r\n\r\n``` python\r\nimport tensorflow as tf\r\n\r\nfrom typing import Generic, TypeVar\r\nfrom typing_extensions import Literal\r\n\r\n\r\nShapeType = TypeVar(\"ShapeType\")\r\nDataType = TypeVar(\"DataType\")\r\n\r\nShape = Literal\r\n\r\n\r\nclass Float32:\r\n    dtype = tf.float32\r\n\r\n\r\nclass Float64:\r\n    dtype = tf.float64\r\n\r\n\r\n# TODO(jeff): generate all dtypes\r\n\r\n\r\nclass Tensor(Generic[ShapeType, DataType]):\r\n    @classmethod\r\n    def shape(self):\r\n        return self.__args__[0].__values__\r\n\r\n    @classmethod\r\n    def dtype(self):\r\n        return self.__args__[1].dtype\r\n\r\n    def __add__(self, other):\r\n        return self + other\r\n\r\n\r\ndef function(fn):\r\n    annotation_values = fn.__annotations__.values()\r\n    tensor_specs = [tf.TensorSpec(x.shape(), x.dtype()) for x in annotation_values]\r\n    return tf.function(fn, input_signature=tensor_specs)\r\n\r\n\r\n@function\r\ndef foo(x: Tensor[Shape[None, 2, 3], Float64]):\r\n    return x + 42.0\r\n\r\n\r\nfoo(tf.random.normal([1, 2, 3], dtype=tf.float64))  # OK\r\nfoo(tf.random.normal([2, 2, 3], dtype=tf.float64))  # OK\r\nfoo(tf.random.normal([1, 2, 3, 4], dtype=tf.float64))  # NOT OK\r\nfoo(tf.random.normal([1, 2, 3], dtype=tf.float32))  # NOT OK\r\n```\r\n\r\nwhich seems to pass the correct `input_signature` in this very simple example but has some mypy errors that I don't know how to deal with:\r\n\r\n```\r\n$ mypy types_test.py\r\ntypes_test.py:1: error: No library stub file for module 'tensorflow'\r\ntypes_test.py:1: note: (Stub files are from https://github.com/python/typeshed)\r\ntypes_test.py:44: error: Variable \"types_test.Shape\" is not valid as a type\r\ntypes_test.py:44: error: Invalid type: try using Literal[2] instead?\r\ntypes_test.py:44: error: Invalid type: try using Literal[3] instead?\r\nFound 4 errors in 1 file (checked 1 source file)\r\n```\r\n", "@jeffpollock9 you'll probably need to redefine `__class_getitem__` for the Tensor class. (That's the method that's called when you call `Tensor[...]`).", "Also I don't think defining `Shape` as a `Literal` will work (then undefined dimensions wouldn't work). I don't think it's possible to make mypy handle shapes correctly anyway without extending mypy itself, so I think the easiest solution for now would to simply make `__class_getitem__` for `Tensor` return a `Tensor` with some attributes set for shape and dtype (which will be ignored by mypy but can be used by `tf.function`). Possibly mypy could handle dtypes correctly, but I'd say that's not so important anyway until someone starts type annotating the entire TF library.", "Many thanks for the comments, @Danmou! As far as I can tell [__class_getitem__](https://www.python.org/dev/peps/pep-0560/#class-getitem) was added in [python3.7](https://www.python.org/dev/peps/pep-0537/#features-for-3-7) so have switched to that (was on 3.6 before).\r\n\r\nI'm trying to figure out a better way of making some sort of `Shape` type since you mentioned `Literal` is not a good idea - but I am not sure how. If you don't mind - do you have any ideas? This is where I have got to so far:\r\n\r\n``` python\r\nimport tensorflow as tf\r\n\r\nfrom typing import Generic, TypeVar, get_type_hints\r\nfrom typing_extensions import Literal\r\n\r\nShapeType = TypeVar(\"ShapeType\")\r\nDataType = TypeVar(\"DataType\")\r\n\r\n# TODO(jeff): this shouldn't be Literal\r\nShape = Literal\r\n\r\n\r\nclass Float32:\r\n    dtype = tf.float32\r\n\r\n\r\nclass Float64:\r\n    dtype = tf.float64\r\n\r\n\r\n# TODO(jeff): generate all dtypes\r\n\r\n\r\nclass Tensor(Generic[ShapeType, DataType]):\r\n    def __class_getitem__(cls, item):\r\n        shape = item[0].__args__\r\n        dtype = item[1].dtype\r\n        return shape, dtype\r\n\r\n\r\ndef function(fn):\r\n    type_hints = get_type_hints(fn)\r\n    input_signature = [\r\n        tf.TensorSpec(shape, dtype, name) for name, (shape, dtype) in type_hints.items()\r\n    ]\r\n    return tf.function(fn, input_signature=input_signature)\r\n\r\n\r\n@function\r\ndef foo(x: Tensor[Shape[None, 2, 3], Float64], y: Tensor[Shape[1, 1, 1], Float64]):\r\n    return x + y\r\n\r\n\r\n>>> print(foo.input_signature)\r\n(TensorSpec(shape=(None, 2, 3), dtype=tf.float64, name='x'), TensorSpec(shape=(1, 1, 1), dtype=tf.float64, name='y'))\r\n```\r\n\r\nwith:\r\n``` shell\r\n$ mypy types_test.py\r\ntypes_test.py:1: error: No library stub file for module 'tensorflow'\r\ntypes_test.py:1: note: (Stub files are from https://github.com/python/typeshed)\r\ntypes_test.py:40: error: Variable \"types_test.Shape\" is not valid as a type\r\ntypes_test.py:40: error: Invalid type: try using Literal[2] instead?\r\ntypes_test.py:40: error: Invalid type: try using Literal[3] instead?\r\ntypes_test.py:40: error: Invalid type: try using Literal[1] instead?\r\ntypes_test.py:41: error: Unsupported left operand type for + (\"Tensor[Shape?[None, Any, Any], Float64]\")\r\nFound 6 errors in 1 file (checked 1 source file)\r\n```\r\n\r\nSo indeed there is a problem with using `Shape = Literal` but I am not sure how to make a new type which can hold the list of shape data.\r\n\r\nThanks\r\n", "I think this is going in the right direction. Here's a version of @jeffpollock9 's code that I think mypy is happy with (barring the lack of annotations in TensorFlow).\r\n\r\nSince in the space of types there are no values (`None` is just sugar for `NoneType`), it doesn't seem currently possible to specify something like `MyCustomType[1]` without the use of `Literal`. So the type annotations will look a bit awkward.\r\nPerhaps a future PEP could relax that.\r\n\r\nIt appears that `Literal` is the only special type that allows a variable number of things (and even in that case the values are packed into a sugared tuple). In other words we can't define a type `Shape[*Dim]` and we're forced to either use `Literal[(None, 2, 3)]` or to specialize things by rank, like `Shape1D`, `Shape2D`, etc. I'm also a bit afraid of `Literal` because it has the semantic \"the value can be any one of these (their order is irrelevant)\", but here we really need to say: \"the value is this specific list (in this specific order)\". That's why I used `Literal[(a, b, c)]` and not just `Literal[a, b, c]`.\r\n\r\nLastly, I think we can also get by entirely using inspect to examine these type arguments - see the code.\r\n\r\nHere's the code:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nimport inspect\r\nimport typing\r\nfrom typing import Any, Generic, TypeVar, get_type_hints\r\nfrom typing import NewType\r\nfrom typing_extensions import Literal\r\n\r\nShapeType = TypeVar(\"ShapeType\")\r\nDataType = TypeVar(\"DataType\")\r\n\r\n\r\nclass Shape(Generic[ShapeType]):\r\n  pass\r\n\r\n\r\nclass Float32(object):\r\n    value = tf.float32\r\n\r\n\r\nclass Float64(object):\r\n    value = tf.float64\r\n\r\n\r\n# TODO(jeff): generate all dtypes\r\n\r\n\r\nclass Tensor(Generic[ShapeType, DataType]):\r\n  def __rmul__(self, other: Any):\r\n    pass  # Just appeasing mypy here, the real Tensor has a proper implementation.\r\n\r\n  pass\r\n\r\n\r\ndef function(fn):\r\n    argspec = inspect.getfullargspec(fn)\r\n    if (argspec.varargs is not None or argspec.varkw is not None or argspec.varkw is not None):\r\n      raise NotImplemented('only positional args for now')\r\n\r\n    input_signature = []\r\n    for name in argspec.args:\r\n      if name not in argspec.annotations:\r\n        input_signature.append(None)\r\n        continue\r\n      shape_as_type, dtype = argspec.annotations[name].__args__\r\n      shape = []\r\n      for s in shape_as_type.__args__[0].__values__:\r\n        if s is None:\r\n          shape.append(None)\r\n        else:\r\n          shape.append(int(s))\r\n\r\n      ts = tf.TensorSpec(shape=shape, dtype=dtype.value)\r\n      input_signature.append(ts)\r\n    return tf.function(fn, input_signature=input_signature)\r\n\r\n\r\n@function\r\ndef foo(x: Tensor[Shape[Literal[(None, 2, 3)]], Float64]):\r\n    return 2 * x\r\n\r\nfoo(tf.random.normal([1, 2, 3], dtype=tf.float64))  # OK\r\nfoo(tf.random.normal([2, 2, 3], dtype=tf.float64))  # OK\r\ntry:\r\n  foo(tf.random.normal([1, 2, 3, 4], dtype=tf.float64))  # NOT OK\r\n  assert False\r\nexcept ValueError:\r\n  pass\r\ntry:\r\n  foo(tf.random.normal([1, 2, 3], dtype=tf.float32))  # NOT OK\r\n  assert False\r\nexcept ValueError:\r\n  pass\r\n```", "Going the `Literal`-free path might not be so bad. Here's a version that's very verbose, but the type annotation looks quite nice. I named the dimensions `MNISTHeight` and `MNISTWeight` to show that such boilerplate-y types can have an actual intuitive meaning.\r\n\r\n```\r\n## This is what the gigantic file of type defs would contain\r\n\r\nShape3DDim1 = TypeVar(\"Shape3DDim1\")\r\nShape3DDim2 = TypeVar(\"Shape3DDim2\")\r\nShape3DDim3 = TypeVar(\"Shape3DDim3\")\r\n\r\nclass Shape3D(Generic[Shape3DDim1, Shape3DDim2, Shape3DDim3]):\r\n  pass\r\n\r\nclass Dimension(object):\r\n  value = NotImplemented\r\n\r\nclass Dynamic(Dimension):\r\n  value = None\r\n\r\n## This is what the user would have to define:\r\n\r\nclass MNISTWidth(Dimension):\r\n  value = 2\r\n\r\nclass MNISTHeight(Dimension):\r\n  value = 3\r\n\r\n\r\n@function\r\ndef foo(x: Tensor[Shape3D[Dynamic, MNISTWidth, MNISTHeight], Float64]):\r\n    return 2 * x\r\n\r\n```", "@mdanatg thanks for this! I really like your `Literal`-free code - since it doesn't seem possible to define a `Shape[*Dim]` type I think this is the way to go. The only downside is the big file of typedefs as you mentioned - but I think we could automatically generate a file with up to (say) `Shape10D` and I can't imagine it ever being a limitation.", "I've made a few changes to the code above:\r\n\r\nFirstly, I don't think we need to handle a `None` value in the `input_signature` list as this doesn't work with `tf.function` anyway, i.e. this doesn't work:\r\n\r\n``` python\r\n@tf.function(input_signature=[None, tf.TensorSpec([1, 2], tf.float32)])\r\ndef foo(x, y):\r\n    return x + y\r\n```\r\nwith:\r\n``` python\r\nTypeError: Invalid input_signature [None, TensorSpec(shape=(1, 2), dtype=tf.float32, name=None)]; input_signature must be a possibly nested sequence of TensorSpec objects.\r\n```\r\nso we can remove:\r\n``` python\r\nif name not in argspec.annotations:\r\n    input_signature.append(None)\r\n    continue\r\n```\r\n\r\nSecondly, I had to change:\r\n``` python\r\nfor s in shape_as_type.__args__[0].__values__:\r\n```\r\nto\r\n``` python\r\nfor s in shape_as_type.__args__:\r\n```\r\n\r\nThirdly, for the inner loop over the shapes, should it not be `s.value` instead of `s`?\r\n\r\nso the full code is:\r\n\r\n``` python\r\nimport tensorflow as tf\r\nimport inspect\r\n\r\nfrom typing import Generic, Any, TypeVar\r\n\r\n# TODO: generate all dtypes\r\n# TODO: generate all shapes\r\n\r\nShapeType = TypeVar(\"ShapeType\")\r\nDataType = TypeVar(\"DataType\")\r\n\r\nShape3DDim1 = TypeVar(\"Shape3DDim1\")\r\nShape3DDim2 = TypeVar(\"Shape3DDim2\")\r\nShape3DDim3 = TypeVar(\"Shape3DDim3\")\r\n\r\n\r\nclass Shape3D(Generic[Shape3DDim1, Shape3DDim2, Shape3DDim3]):\r\n    pass\r\n\r\n\r\nclass Dimension(object):\r\n    value = NotImplemented\r\n\r\n\r\nclass Dynamic(Dimension):\r\n    value = None\r\n\r\n\r\nclass Float32(object):\r\n    value = tf.float32\r\n\r\n\r\nclass Float64(object):\r\n    value = tf.float64\r\n\r\n\r\nclass Tensor(Generic[ShapeType, DataType]):\r\n    def __rmul__(self, other: Any):\r\n        pass  # Just appeasing mypy here, the real Tensor has a proper implementation.\r\n\r\n\r\ndef function(fn):\r\n    argspec = inspect.getfullargspec(fn)\r\n    if argspec.varargs is not None or argspec.varkw is not None:\r\n        raise NotImplemented(\"only positional args for now\")\r\n\r\n    input_signature = []\r\n    for name in argspec.args:\r\n        shape_as_type, dtype = argspec.annotations[name].__args__\r\n        shape = []\r\n        for s in shape_as_type.__args__:\r\n            if s.value is None:\r\n                shape.append(None)\r\n            else:\r\n                shape.append(int(s.value))\r\n\r\n        ts = tf.TensorSpec(shape=shape, dtype=dtype.value, name=name)\r\n        input_signature.append(ts)\r\n    return tf.function(fn, input_signature=input_signature)\r\n\r\n\r\n# User code starts here\r\nclass MNISTWidth(Dimension):\r\n    value = 2\r\n\r\n\r\nclass MNISTHeight(Dimension):\r\n    value = 3\r\n\r\n\r\n@function\r\ndef foo(x: Tensor[Shape3D[Dynamic, MNISTWidth, MNISTHeight], Float64]):\r\n    return 2.0 * x\r\n\r\n\r\n# Some ad hoc testing\r\nprint(f\"foo signature: {foo.input_signature}\")\r\nfoo_x_ts = tf.TensorSpec(shape=[None, 2, 3], dtype=tf.float64, name=\"x\")\r\nassert len(foo.input_signature) == 1\r\nassert foo.input_signature[0] == foo_x_ts\r\n\r\n\r\n@function\r\ndef bar():\r\n    return tf.random.normal([1, 2, 3])\r\n\r\n\r\nprint(f\"bar signature: {bar.input_signature}\")\r\nassert bar.input_signature == ()\r\n```\r\n\r\n``` shell\r\n$ python types_test.py \r\nfoo signature: (TensorSpec(shape=(None, 2, 3), dtype=tf.float64, name='x'),)\r\nbar signature: ()\r\n\r\n$ mypy types_test.py\r\ntypes_test.py:1: error: No library stub file for module 'tensorflow'\r\ntypes_test.py:1: note: (Stub files are from https://github.com/python/typeshed)\r\nFound 1 error in 1 file (checked 1 source file)\r\n```\r\n\r\nI also removed the extra check in `argspec.varkw is not None or argspec.varkw is not None` - I guess that was just a typo?", "Yep, your edits all look good! The `Literal`-free version did require them, but I didn't want that to clutter the post.\r\nYou're right about `input_signature`, it only supports `None` in a change that's not submitted yet, sorry! Probably best to raise an error for now. In future version we should be able to leave some args without annotation and their shape/type will be inferred.", "FYI, https://github.com/tensorflow/community/pull/208 aims to establish a home for type definitions such as these. The RFC mentions this ongoing work, but we can include more specific details if ready.", "@mdanatg thanks for this - looks really interesting! I had a couple of evenings to try and add some of this to tensorflow but was struggling to even run the existing tests as TF takes days to build on my laptop. I'm hoping to have some time to try again soon but if there is anything in particular I could contribute please let me know.\r\n\r\n> FYI, [tensorflow/community#208](https://github.com/tensorflow/community/pull/208) aims to establish a home for type definitions such as these. The RFC mentions this ongoing work, but we can include more specific details if ready.\r\n\r\n", "Quick note, DeemMind has created in implementation similar to the ideas in this thread: https://github.com/deepmind/tensor_annotations"]}, {"number": 31545, "title": "tf.shape() for RaggedTensor is raising an exception", "body": "Let suppose the following context:\r\n\r\n```python\r\nragged_tensor = tf.ragged.constant(\r\n    [['All', 'the', 'world', 'is', 'a', 'stage'],\r\n    ['And', 'all', 'the', 'men', 'and', 'women', 'merely', 'players'],\r\n    ['They', 'have', 'their', 'exits', 'and', 'their', 'entrances']]\r\n)\r\n\r\nprint(isinstance(ragged_tensor, tf.RaggedTensor))   # True\r\n\r\ntf.shape(ragged_tensor)[0]   # Should return \"3\"\r\n\r\n>>> TypeError: Failed to convert object of type \r\n<class 'tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor'> to Tensor. \r\nContents: tf.RaggedTensor(\r\n    values=Tensor(\"RaggedConstant/values:0\", shape=(21,), dtype=string), \r\n    row_splits=Tensor(\"RaggedConstant/Const:0\", shape=(4,), dtype=int64)\r\n). \r\nConsider casting elements to a supported type.\r\n\r\n# To obtain the information, one shall call the following:\r\nragged_tensor.bounding_shape()[0]\r\n```\r\n\r\nWe should merge the two functionalities into TF.Shape and raise a warning/exception in case someone tries to evaluate a ragged dimension.", "comments": ["@edloper do you think we should add dispatch support to tf.shape as well, or do you think that'd be more confusing?", "Any news ? @alextp ", "Which of these is the problem that you're trying to address?\r\n\r\n1. You'd like to get the size of some dimension of a ragged tensor.\r\n2. You'd like to do get the size of some dimension of an *optionally* ragged tensor, in a way that works for both tensors and ragged tensors.\r\n\r\nFor problem 1:\r\n\r\n* You can get the size of the outermost dimension with `rt.nrows()`\r\n* You can get the size of inner uniform dimensions with `tf.shape(rt.flat_values)[-n]`.\r\n* You can get the bounding size of any single dimension with: `rt.bounding_shape(axis)`.  For non-ragged dimensions, this will be identical to the actual size.  E.g., `rt.bounding_shape(0) == rt.nrows()`.\r\n\r\nFor problem 2:\r\n\r\n`tf.shape` returns a vector of integers.  There's no backwards-compatibility possibility for changing it to return some other object that will \"raise a warning/exception in case someone tries to evaluate a ragged dimension.\"  And given that we're forced to return a vector of integers, we would have to decide which integer to use for ragged dimensions.  We could use `-1`, but that would lead to some unexpected/unintuitive behavior -- e.g., `tf.reshape(a, tf.shape(b))` would give a surprising result, since `tf.reshape` has a special meaning for `-1`.  So the best solution I can think of would be to introduce some helper function that handles both ragged & dense tensors.  E.g.:\r\n\r\n```\r\ndef nrows(t):\r\n  return t.nrows() if isinstance(t, tf.RaggedTensor) else tf.shape(t)[0]\r\n```\r\n\r\nOr:\r\n\r\n```\r\ndef bounding_shape(t):\r\n  return t.bounding_shape() if isinstance(t, tf.RaggedTensor) else tf.shape(t)\r\n```\r\n\r\n(These would probably need to be helpers in your own code, since they're probably not worth adding as endpoints to TensorFlow.)", "I am having a similar issue:\r\n    \"supported type.\" % (type(values), values))\r\nTypeError: Failed to convert object of type <class 'tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor'> to Tensor. Contents: tf.RaggedTensor(values=Tensor(\"Input1/flat_values:0\", shape=(None,), dtype=int64), row_splits=Tensor(\"Input1/row_splits_0:0\", shape=(None,), dtype=int64)). Consider casting elements to a supported type.\r\n", "@DEKHTIARJonathan,\r\nSorry for the delayed response. We can get the shape of **`Ragged Tensor`** not using **`tf.shape`** but using **`get_shape`** . Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/11f74723513e432440d486709945d95e/gh_31545.ipynb) of the working  code. Thanks!", "@DEKHTIARJonathan, @rmothukuru `ragged_tensor.shape` and `ragged_tensor.get_shape()` are equivalent, and both return the **static shape** of a RaggedTensor.  This shape will contain `None` for any dimensions that are ragged or that have unknown dimension size (because it may depend on the inputs to the graph).  This differs from the `tf.shape()` operation, which returns a `Tensor`, and will never have `None` dimensions.\r\n\r\nFor some use cases, `ragged_tensor.shape` (aka `ragged_tensor.get_shape()` will be sufficient.  If it's not, I listed several alternatives in my comment above from Aug 22, 2019.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "> @DEKHTIARJonathan,\r\n> Sorry for the delayed response. We can get the shape of **`Ragged Tensor`** not using **`tf.shape`** but using **`get_shape`** . Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/11f74723513e432440d486709945d95e/gh_31545.ipynb) of the working code. Thanks!\r\n\r\nThe thing is some network requires to be able to get a ShapeTensor, that wouldn't work here\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nragged_tensor = tf.ragged.constant(\r\n    [['All', 'the', 'world', 'is', 'a', 'stage'],\r\n    ['And', 'all', 'the', 'men', 'and', 'women', 'merely', 'players'],\r\n    ['They', 'have', 'their', 'exits', 'and', 'their', 'entrances']]\r\n)\r\n\r\ntf.shape(ragged_tensor)\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py in convert_to_eager_tensor(value, ctx, dtype)\r\n     96       dtype = dtypes.as_dtype(dtype).as_datatype_enum\r\n     97   ctx.ensure_initialized()\r\n---> 98   return ops.EagerTensor(value, ctx.device_name, dtype)\r\n     99 \r\n    100 \r\n\r\nValueError: TypeError: object of type 'RaggedTensor' has no len()\r\n\r\n```"]}, {"number": 31543, "title": "Interrelations of collections and scope counts are not clear from documentation", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/GraphKeys\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe GraphKeys doc is lacking explanation of ('__variable_store',) and ('__varscope',) keys relation to the others collections e to the variable creation in in a variable scope context.\r\nIn my modelling, since iam building a dnn from scratch, its cond sine-qua-non to estimate memory usage like given in this  for VGGNet \r\nhttp://cs231n.github.io/convolutional-networks/#case\r\n\r\nHowever to me its not clear which tensors summup the computation memory allocations (even if it initializes in the runtime, you should be able to pre-calculate the estimative from the graph builted, before running).\r\n\r\nSo iam able to realize the relations of scopes counting, variable creation and using, will be hard to do such kind of memory estimation function.\r\n\r\nToday iam using ._collections['variables'], i think it subsums the variables used in any session of training.\r\n\r\n### Clear description\r\n\r\nCollections are created in the modelling process with the intent of variable management for some functional reason. The developer must have a clear image of which is the intent of each collection and the inter-relation of them (this is somewhat done in GraphKeys, but the mentioned keys are lacking).\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n\r\n### More Info\r\n\r\nI have seem the RFC and know that the 2.0beta Variable became abs class and the management and implementation is more flexible, but now i dont have time to migrate the code, so i think this is stuff is good to keep-up updated (if it is possible and desirable by TFlow team), more people may be in the same condition .", "comments": ["@Uiuran ,\r\nIf I understand your description, you want the explanation about `('__variable_store',)` and `('__varscope',)`  in the documentation corresponding to `GraphKeys`. \r\n\r\nIf it is so, the documentation about `GraphKeys` (as mentioned by you), is present in this [TF Link](https://www.tensorflow.org/api_docs/python/tf/GraphKeys) and the Code corresponding to `GraphKeys` is present in this [Github Link](https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/framework/ops.py#L6144-L6286).\r\n\r\nIn both the places, I couldn't see the terms mentioned by you `('__variable_store',)` and `('__varscope',)`. \r\n\r\nCan you please be more specific or provide more details so that it would be easy for us to understand and make necessary changes. \r\n\r\nAlso, can you please share the Tensorflow Version you are using.\r\n\r\nThanks!", "('__varscope',) and ('__variable_store',) probably appears in graphs  _collections dictionary, probably  because of the class:\r\nclass _VariableStore(object)\r\nin\r\nhttps://github.com/tensorflow/tensorflow/blob/d61d32a0486953064d172945fa7fb68de8008083/tensorflow/python/ops/variable_scope.py\r\n\r\nSo it appears as a collection in the graph, to the intent of tracking similar variable scopes, but it's not clear why they appear and how they are count from the documentation. In contrast to the other GraphKeys presented so far in the documentation. So they have a role in building graphs that are not well documented", "@Uiuran \r\nIs this still an issue."]}, {"number": 31412, "title": "DeprecationWarning: the imp module is deprecated", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04\r\n- **TensorFlow installed from (source or binary)**: binary (pip)\r\n- **TensorFlow version (use command below)**: tensorflow-gpu==2.0.0b1\r\n- **Python version**: 3.6.8\r\n- **CUDA/cuDNN version**: 10.0.130/7.6.2\r\n- **Exact command to reproduce**:\r\n\r\nCreate `test.py` file with the following content:\r\n`import tensorflow as tf`\r\n\r\nRun pytest (5.0.1) on it:\r\n`pytest test.py`\r\n\r\nResult:\r\n`============================================================================================= warnings summary =============================================================================================\r\nvenv/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py:15\r\n  venv/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py:15: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\r\n    import imp\r\n`\r\n\r\n### Describe the problem\r\nTensorFlow is using \"imp\" which is deprecated since Python 3.4. Are there plans to update it?\r\nThanks!", "comments": ["Confirmed.\r\n\r\nThere are a few more warnings which also show when you try `python test.py` but the `imp` deprecation doesn't show up in that case.\r\n\r\nNote that this file is automatically generated by SWIG and there's a plan to replace SWIG with pybind11: https://github.com/tensorflow/community/pull/67", "@carlosgalvezp Is this still an issue? Can you check with `TF2.0` and let us know whether the issue persists with latest TF version. Thanks!", "@jvishnuvardhan Hi I am using TF2.0 and I still get a DeprecationWarning.\r\n(Using Python 3.7.3)\r\n```\r\n /Users/xxxxxx/.yyyyy/lib/python3.7/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py:15: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\r\n```", "This may help:\r\n   import warnings\r\n   warnings.simplefilter(\"ignore\")", "Hello! Im using TF 2.1 and still the problem while import.  \r\nDeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses ModuleNotFoundError: No module named 'tensorflow.python.tools'; tensorflow.python' is not a package.\r\nAny advice?", "I'm using TF 2.3.1, I still get the same warning.", "@carlosgalvezp \r\nCould you please verify on tf-nightly [2.4.0-dev20201020] and let us know if you still see the warnings.", "Same warning on TF 2.3.0 as well as 2.3.1.", "Same warning on TF 2.4.0 as well.", "Same warning on TF 2.4.1.\r\n\r\n", "I'm seeing the same warning from a different line on TF 2.5.0.\r\n\r\n```\r\n  /usr/local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:22: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\r\n    import imp\r\n```", "same here, TF 2.5.0 native version for apple silicon MacBook \r\n\r\n\r\n```\r\n./../miniforge3/envs/tf2.5/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:22\r\n  /Users/giuseppefilitto/miniforge3/envs/tf2.5/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:22: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\r\n    import imp\r\n\r\n-- Docs: https://docs.pytest.org/en/stable/warnings.html\r\n\r\n```", "This issue still persists. Can this please be fixed.\r\n\r\n`tensorflow = \"2.6.0-rc1\"`\r\n\r\n```\r\n../../../../usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/impl/api.py:22\r\n  /usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/impl/api.py:22: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\r\n    import imp\r\n```\r\n\r\n", "I think the best way forward is to send a PR", "Same here on 2.6.0", "> I think the best way forward is to send a PR\r\n\r\nThis is still true.\r\n\r\nWe don't see this in our CI", "Added a PR #51663 to get the deprecation message fixed.", "This is still happening in TF2.7.0", "Please note that we don't see this in our CI. This points to an issue with your environment instead of the TF code itself. Can you post the output of `pip list` for example, to start the debugging process?"]}, {"number": 31275, "title": "[TF 2.0 API Docs] tf.io.extract_jpeg", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/io/encode_jpeg\r\n\r\n## Description of issue (what needs changing):\r\n- No link to github source code provided\r\n- No `Raises` provided in the description\r\n- No usage example\r\n\r\n### Correct links\r\n\r\nNo link provided\r\n\r\n### Raises listed and defined\r\n\r\nRaises are not defined/listed.\r\n\r\n### Usage example\r\n\r\nThere is no usage example\r\n\r\n### Request visuals, if applicable\r\n\r\nCurrently, no visuals, may not be required.\r\n\r\n\r\n", "comments": ["@dannylwe \r\nIs this still an issue.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 31244, "title": "Make TF-TRT  input and output  semantic same as  TF ", "body": "**System information**\r\n- TensorFlow version (you are using):1.13.1\r\n- Are you willing to contribute it (Yes/No):Do not think have the capability yet. \r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nIn TF-TRT the outputs  have to be specified in create_inference_graph unlike in TF, where outputs can be selected at sess.run ().     Also, in TF not all placeholders need to be assigned at sess.run () if  in the network they have not effect in the output.  That is not the case with TF-TRT. These 2 semantic differences make it very hard for complex TF network to be converted to TF-TRT. \r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\nAny TF-TRT user. \r\n\r\n**Any Other info.**\r\n", "comments": ["@sgambient not sure if I understand your description, are you saying that in `create_inference_graph()` we need to manually provide *all* the outputs using the `outputs` parameter? If that's the case can you use SavedModel as input and set `input_saved_model_dir` to your SavedModel dir? In that way we don't need to specify the inputs/outputs since the SavedModel proto already has everything.", "Also since SavedModel is now the standard format and your request is about TF 1.x with Session (and maybe about GraphDef, based on my understanding), so mark as contributions welcome.", "I tried the SavedModel approach, however  because of the complexity of  the model ( includes multiple dis-jointed sub-graphs), it was not possible  build a TF-TRT engine node for the whole graph.    MetaGraph approach worked for converting one of the sub-graphs into TR-TRT engine node, and the rest of the sub-graphs in pure TF.    Even there , I notices some some disjointed sub-graph in pure TF do not get initialized. ", "Do you have code snippets and model data that can reproduce what you said?"]}, {"number": 31173, "title": "un-deprecate `write_grads` for `fit`", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.0-beta\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n```\r\ntb_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir,\r\n                                                 write_grads=True)\r\n```\r\n\r\n- ignores the `write_grads` \r\n\r\nOne might argue that I can write a custom `train` function then accumulate the gradients myself but some features I'm using cause the custom train function to fail (`tf.feature_columns`) [BUG: [TF 2.0] categorical_column_with_vocabulary_list not usable in custom training loop](https://github.com/tensorflow/tensorflow/issues/30453)\r\n\r\n**Will this change the current api? How?**\r\n\r\nwrites the grads to Tensorboard\r\n\r\n**Who will benefit with this feature?**\r\n\r\nPeople who want to visualize the gradients of their model\r\n\r\n**Any Other info.**\r\n\r\nWhy is it getting deprecated?\r\n", "comments": ["@IanQS thanks for this feature request! Yes, I think we're at a point with 2.0 and TensorBoard in Keras where we can support this now, I'll aim to get this in the within the next week or two", "Hi @omalleyt12, is there any progress on this? Thanks, Deyan", "@omalleyt12 sorry to bother you did you manage to get a solution merged in? ", "Hello, is there any progress or alternative solution for this?\r\n\r\nThanks!", "Is this feature on the roadmap? It's critical for our use case.", "Would really like this for debugging. Is this going to be included in TF2.2?", "This is really wonderful feature. Looking forward to seeing it again!\r\n\r\nP.S. Find a link here, but not very \"keras-ish\"... https://stackoverflow.com/questions/57759635/get-gradients-with-keras-tensorflow-2-0", "Hello, are there any updates? Thank you!", "Any updates?", "This would be a really useful feature to have. Are there any updates?", "I would also like this feature reinstated.", "Need this feature! Pls add this feature back 0.0", "please re-add this feature, it was so useful back in TF 1.x", "I would also like to see this feature for TF2. ", "Is there any update? Will this feature add in the future? Any best alternative for now?", "Any updates? Would love to see this feature back! ", "Any workarounds or updates?", "@omalleyt12 It's now been almost 2 years. If we want to contribute this from the community can you at least provide some info on why it was deprecated in the first place?", "Any updates? or any Any workarounds for \"WARNING:tensorflow:`write_grads` will be ignored in TensorFlow 2.0 for the `TensorBoard` Callback.\"?", "![same](https://user-images.githubusercontent.com/6968573/120903776-ffdc7300-c5fc-11eb-98e7-eb308192655c.png)\r\n\r\nM E T A", "Any updates on this? Still a very useful feature, and TF2.8 still doesn't have it :( \r\n@omalleyt12 ", "Still no updates...", "> @omalleyt12 sorry to bother you did you manage to get a solution merged in? \n\nLooks like they're not working at Google anymore (check the commit history). @gowthamkpr ane @ymodak can we get some help here? "]}, {"number": 31067, "title": "tensorflow polyfill operation", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):Current stable build(1.14)\r\n- Are you willing to contribute it (Yes/No):Maybe\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nAs far as I can tell, there is no easy way to create a polygon mask, given a set of points using pure tensorflow operations during run time. For a given image, with a set of landmark points(let's say 8 points), I would like to create a polygon(more precisely,a convex hull) mask with the points as boundaries, which can be used as an attention mask etc. There are opencv/scipy/etc functions that can easily do this with a single line of code. Unfortunately, there are no equivalent functions in tensorflow and these are tricky to code because of the looping involved. Any ideas on if this is possible would be appreciated. Thanks\r\n\r\n**Will this change the current api? How?**No. This is just a request to support a new operation\r\n\r\n**Who will benefit with this feature?**Tensorflow community/users\r\n\r\n**Any Other info.**\r\n", "comments": ["@capilano Can you provide more details about any use-case for this feature? Thanks!", "@jvishnuvardhan Yes. My particular use case is to create a polygon bounding box (instead of a square) and then I also need to know what pixels are inside the bounding box and which ones are outside. So, given a bunch of landmark points(not necessarily convex), I need to find the outer  boundary and the enclosed pixels. cv2.convex_hull + cv2.fillPoly are the equivalent functions that as far as I know have no equivalents in Tensorflow. The numpy code is very simple, but it involves a for loop and dynamic shapes. ", "I think it's reasonable to have convex hulls and polyfills in tf. I'll mark this as contributions welcome though since I don't think it's on our current roadmap but PRs would be welcome.\r\n\r\n", "Also note that you should be able to use the opencv version right now inside a tf.numpy_function.", "@alextp  Thanks. I am trying to train on TPU's so not sure if I can use a  py_func wrapper. Is there an alternative to this?", "You can run something like py_func with host_call and careful usage of\ntf.device, I think, but I agree it's not ideal.\n\nOn Tue, Jul 30, 2019 at 9:14 PM Ashwin Thyagarajan <notifications@github.com>\nwrote:\n\n> @alextp <https://github.com/alextp> Thanks. I am trying to train on TPU's\n> so not sure if I can use a py_func wrapper. Is there an alternative to this?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/31067?email_source=notifications&email_token=AAABHROEADAAREF73MZY6E3QCEGRJA5CNFSM4IHCMYOKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3GAEDI#issuecomment-516686349>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHROEGJXMMIOAJQBNCNLQCEGRJANCNFSM4IHCMYOA>\n> .\n>\n\n\n-- \n - Alex\n", "That said implementing convex hull and polyfill on TPUs will be somewhat harder than adding a custom C++ kernel for them because for TPUs they'll have to be written in terms of XLA HLOs.\r\n\r\n@sanjoy do you know who would be the best person to ask for help on expressing these operations in term of HLOs?", "@alextp Yeah, many problems. Convex hull  needs to support dynamic shapes by definition(not sure if that works well with XLA/JIT). Also, this is happening inside my training loop, so need gradients defined as well. So,right now I am just abandoning this approach.", "> Convex hull needs to support dynamic shapes by definition(not sure if that works well with XLA/JIT)\r\n\r\nDynamic shapes are not a great fit for XLA.\r\n\r\nBut +CC @blakehechtman in case he has a clever idea.", "@sanjoy I wanted to create a custom C++ op, my initial idea was to just take as input, N points with shape (N,2) and return an array with shape(N,1) where each row is either 0 (if the point is not in the hull) or a scalar representing the position of the point in the hull. This solves the dynamic shapes issue(I think).\r\n\r\nWhat I was(and still am) unsure about is if XLA supports search operations which are needed to compute the hull and even if it does, the depth of the search is not fixed,it depends on the input.\r\n", "> What I was(and still am) unsure about is if XLA supports search operations which are needed to compute the hull and even if it does, the depth of the search is not fixed,it depends on the input.\r\n\r\nXLA does support while loops and conditionals so we might be able to express what you want (though it might end up being too slow).", "@sanjoy I see. and the no of iterations can be dynamic?(only known at run time). Also, can gradients be defined for this op?", ">  the no of iterations can be dynamic?\r\n\r\nYes.\r\n\r\n> Also, can gradients be defined for this op?\r\n\r\nJust in case we were miscommunicating: XLA does not do gradients itself.  Gradients are computed over TF graphs.  So the XLA lowering has no bearing on whether the gradient can be defined or not.", "@sanjoy I was wondering about gradients in general for an op like this which involves a loop.", "> @sanjoy I was wondering about gradients in general for an op like this which involves a loop.\r\n\r\nIf you're asking what the gradient of a `ConvexHull` op should do, then I'm not the right person to answer this.", "@sanjoy Thanks", "Ideally a convexhull op will take a large set of points and return a\nsmaller set of points, in order, right?\n\nIf so, you can represent this op as first computing a set of indices and\nthen calling tf.gather(original_points, convex_hull_indices). Then it's\ndifferentiable.\n\nOn Fri, Aug 2, 2019 at 11:01 PM Ashwin Thyagarajan <notifications@github.com>\nwrote:\n\n> @sanjoy <https://github.com/sanjoy> Thanks\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/31067?email_source=notifications&email_token=AAABHRMCF7A7TWSEXXJ736DQCUNLVA5CNFSM4IHCMYOKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3PHZVA#issuecomment-517897428>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRIDEWESOXEFU5W4LFDQCUNLVANCNFSM4IHCMYOA>\n> .\n>\n\n\n-- \n - Alex\n", "@alextp \r\nYes, convexhull should take a list of points and return a smaller subset of ordered points. \r\nSo,tf.gather will set the gradient to 1 for points in the indices and 0 for the rest,correct?\r\nAlso, there is a polyfill operation after and I think that op needs a gradient as well and I think none of the image transformation ops have gradients and polyfill probably belongs to that class although I think the fastest implementation uses Scanline(which is again a search operation). ", "Hi @alextp, @capilano, do anyone still work on this issue? Otherwise, I can give it a try.", "Please give it a try.\n\nOn Mon, Mar 9, 2020 at 9:38 AM musikisomorphie <notifications@github.com>\nwrote:\n\n> Hi @alextp <https://github.com/alextp>, @capilano\n> <https://github.com/capilano>, do anyone still work on this issue?\n> Otherwise, I can give it a try.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/31067?email_source=notifications&email_token=AAABHRLJQYEFLJG5IDYVRKTRGULPFA5CNFSM4IHCMYOKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEOIAEPY#issuecomment-596640319>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRPTMBP3ISKVIK2GE4LRGULPFANCNFSM4IHCMYOA>\n> .\n>\n\n\n-- \n - Alex\n", "@alextp, I studied convhull and fillpoly code implemented in opencv, I think it is doable for me.\r\n\r\nhttps://github.com/opencv/opencv/blob/45d073f8898c42a95ae54e2e83a827fc26ec336b/modules/imgproc/src/convhull.cpp#L49\r\nhttps://github.com/opencv/opencv/blob/225566da7bab2147d7109f42a0ec9d6039963edb/modules/imgproc/src/drawing.cpp#L1965\r\n\r\nAfter reading the previous discussions, I think it would be better to implement the functions in\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/image_ops.cc\r\nand some test codes in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/image_ops_test.cc\r\n\r\nBecause as a user myself, I used convexhull and polyfill to crop faces based on the face landmarks. Such steps occur during the image processing pipeline before training.\r\n\r\nIs the plan okay for you @alextp ?", "Do keep in mind we also need tfxla implementations of those if we want them\nto run on TPUs.\n\nOn Tue, Mar 10, 2020 at 6:43 AM musikisomorphie <notifications@github.com>\nwrote:\n\n> @alextp <https://github.com/alextp>, I studied convhull and fillpoly code\n> implemented in opencv, I think it is doable for me.\n>\n>\n> https://github.com/opencv/opencv/blob/45d073f8898c42a95ae54e2e83a827fc26ec336b/modules/imgproc/src/convhull.cpp#L49\n>\n> https://github.com/opencv/opencv/blob/225566da7bab2147d7109f42a0ec9d6039963edb/modules/imgproc/src/drawing.cpp#L1965\n>\n> After reading the previous discussions, I think it would be better to\n> implement the functions in\n>\n> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/image_ops.cc\n> and some test codes in\n> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/image_ops_test.cc\n>\n> Because as a user myself, I used convexhull and polyfill to crop faces\n> based on the face landmarks. Such steps occur during the image processing\n> pipeline before training.\n>\n> Is the plan okay for you @alextp <https://github.com/alextp> ?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/31067?email_source=notifications&email_token=AAABHRIOTRLESGLULJBURY3RGY7W5A5CNFSM4IHCMYOKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEOLOROY#issuecomment-597092539>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRPQW5UND5MB67JIANDRGY7W5ANCNFSM4IHCMYOA>\n> .\n>\n\n\n-- \n - Alex\n", "@alextp, Do you have some tfxla implementations related to this problem that I can learn from?", "The kernels in\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/compiler/tf2xla/kernels\nare\na good place to start\n\nOn Tue, Mar 10, 2020 at 10:03 AM musikisomorphie <notifications@github.com>\nwrote:\n\n> Do keep in mind we also need tfxla implementations of those if we want\n> them to run on TPUs.\n> \u2026 <#m_-1118452487377180112_>\n> On Tue, Mar 10, 2020 at 6:43 AM musikisomorphie *@*.***> wrote: @alextp\n> <https://github.com/alextp> https://github.com/alextp, I studied convhull\n> and fillpoly code implemented in opencv, I think it is doable for me.\n> https://github.com/opencv/opencv/blob/45d073f8898c42a95ae54e2e83a827fc26ec336b/modules/imgproc/src/convhull.cpp#L49\n> https://github.com/opencv/opencv/blob/225566da7bab2147d7109f42a0ec9d6039963edb/modules/imgproc/src/drawing.cpp#L1965\n> After reading the previous discussions, I think it would be better to\n> implement the functions in\n> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/image_ops.cc\n> and some test codes in\n> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/image_ops_test.cc\n> Because as a user myself, I used convexhull and polyfill to crop faces\n> based on the face landmarks. Such steps occur during the image processing\n> pipeline before training. Is the plan okay for you @alextp\n> <https://github.com/alextp> https://github.com/alextp ? \u2014 You are\n> receiving this because you were mentioned. Reply to this email directly,\n> view it on GitHub <#31067\n> <https://github.com/tensorflow/tensorflow/issues/31067>?email_source=notifications&email_token=AAABHRIOTRLESGLULJBURY3RGY7W5A5CNFSM4IHCMYOKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEOLOROY#issuecomment-597092539>,\n> or unsubscribe\n> https://github.com/notifications/unsubscribe-auth/AAABHRPQW5UND5MB67JIANDRGY7W5ANCNFSM4IHCMYOA\n> .\n> -- - Alex\n>\n> Do you have some tfxla implementations related to this problem that I can\n> learn from?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/31067?email_source=notifications&email_token=AAABHRMSCV2CNMZC3JYN5BLRGZXG3A5CNFSM4IHCMYOKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEOMI5RQ#issuecomment-597200582>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRMN6CBOFUPBJAK4MGLRGZXG3ANCNFSM4IHCMYOA>\n> .\n>\n\n\n-- \n - Alex\n", "great, thanks for the instruction. I will take a look.", "This doesn't seem to need and API review label anymore (which triggers a review from the API owners).  Please re-add if needed."]}, {"number": 30991, "title": "Name Scope Automatic Handling According to the Model Architectures blocks", "body": "**System information**\r\n- TensorFlow version (you are using):\r\n- Are you willing to contribute it (Yes/No):\r\nYes\r\n**Describe the feature and the current behavior/state.**\r\nTensorflow must be able to give name scopes more logically according to the architecture being built and automatically. Also, must be able to update the names automatically according to the \r\nArchitecture without the dev having to explicity name the things (this should be optional).\r\n\r\nToday it only automatically adds numbers sequentially to name/var scope when it is identical\r\nto any other in the collections and you dont add '/' to it\r\n\r\n**Will this change the current api? How?**\r\nAdd a Name/Var Scope handler. \r\n\r\nThis class/template must be used to implement logic of names according to the architecture block connection. E.G. if a block is just the same from the other in the sequence them it must be named as if is a deep part of the same architecture blocks. If it is a ramification of the block given in a sequence but the same arch them he must be named with another tag.\r\n When duplicating some block to architectonic use, E.G. if you need to frame your data or feature, them each frame must have an coeherent name space block, like the today ordimatlicly done for repeated name space block, but this must be automaticly done for identical built blocks of the model, without the need to explicity code it.\r\nNames must be given as unique ID according to te input-output architecture connection, if it has the same IO from previous Block in the sequence, then each Input-ArchBlock-Output that is IDentical must have the same name with the appropriated tag for deepness in the sequence of blocks.\r\n\r\nSomethings still to be cleared, like if you have a sequence of CNN of deepness, say 100, then many of them will have same IO arch type (name), but they must not be enclosed in a name scope according to IO, since it obviously has the same IO pattern.\r\n\r\n\r\n**Who will benefit with this feature?**\r\n\r\nMuch more self contained and manageable architectonic blocks, idenpendent of its Variable parameters configuration. This will rapid increase how we search for relevant new Neural Networks architecture, since it is a management tools as much as a vizualization tool and a graph construction tool.\r\n\r\n**Any Other info.**\r\n\r\nObs: These Features could be something for the future in Autograph module, if it still dosnt do it", "comments": ["The tracker isnt tagging as feature request as it should.", "First proposal of simple op blocks tracking through ultrametric space ( only some sketches, lemme know if you cant figure anything of this).\r\nhttps://github.com/Uiuran/Biological-Images-Datasets/blob/master/drawing/feature.jpg \r\n\r\nRegarding IO tracking, i think it must be an unique numeric Tensor or a bunch of, in this way we can track relevant IO for computations, the other edges just go for dependency tracking and are not relevant for names(at first glance ...).", "Following the proposal, i would implement an auto-tracking of names and updating to new names with a 3-adic ultrametric space in which each triple ramification inside an name consists of: Ops,Inner-Names Dict and Dependency Edges.\r\n\r\nInner-Names Dict points to new 3-adic trees with the same property\r\n\r\nIn the leaves will always be an Op or None (that is, no Dependency or No Op or No Inner name of any short for this Leave).\r\n\r\nThis can be implemented through linked tables and with Ids being an hashable consisting of an Distance or Ordering Number and a Leave Number.\r\n\r\nThe vision without the dependency edges(eager mode) can be implemented through 2-adic respective.\r\nAlthough it can be ending in undesirable mistaken computations", "ok, subviews of the graph may be useful for redefining archs automatically, as in tf.contrib.graph_editor.subgraph.make_view_from_scope, will only need to hack subview to it copy the subview of the ops as the selected  newscope/copied_ops  and not newscope/oldscope/copied_ops", "@Uiuran Thanks for the suggestion! I think this could be an interesting idea; it would be worth elaborating on the terminology a bit. More examples are always better. Also, have you looked at the name scoping done for instance by Keras?\r\n\r\nI recommend reading the docs on the TF RFC process and c=preparing one according to that process: https://github.com/tensorflow/community/blob/master/governance/TF-RFCs.md", "Iam looking in that direction too, thank you for the hint, update you asap.\n\nOn Wed, Jul 31, 2019 at 10:30 AM Dan Moldovan <notifications@github.com>\nwrote:\n\n> @Uiuran <https://github.com/Uiuran> Thanks for the suggestion! I think\n> this could be an interesting idea; it would be worth elaborating on the\n> terminology a bit. More examples are always better. Also, have you looked\n> at the name scoping done for instance by Keras?\n>\n> I recommend reading the docs on the TF RFC process and c=preparing one\n> according to that process:\n> https://github.com/tensorflow/community/blob/master/governance/TF-RFCs.md\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/30991?email_source=notifications&email_token=AAOOCX6EYT7LTDMDAYVTDGLQCGHZFA5CNFSM4IGQVW62YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3HHIZY#issuecomment-516846695>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAOOCX7N26WUJUOLAN2JYBTQCGHZFANCNFSM4IGQVW6Q>\n> .\n>\n", "Update: \r\nIn my view we fundamentally must look in the following directions\r\n1- Name editable Variable, in the API. For backward compatibility seems that we would need a ner Variable Version.\r\n2- Module that use the collections with ('__varscope',), rename it properly according to the three mentioned above (today it only gives value 1 for the outer-most variable scopes and value 0 for all inner names that was defined by the user). Then one use this lookup engine to change ('__variable_store',) names.", "plus: would not be better to standardize the names to variable or var in these keys  ('__var_store',)?"]}, {"number": 30954, "title": "Potential bugs found with static analysis", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 2.7.12\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): c++ (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Issue 1:**\r\nhttps://github.com/tensorflow/tensorflow/blob/1ee51a3b868a3ccd5f80724f6b9389fd0a9aed07/tensorflow/compiler/tf2xla/functionalize_cond.cc#L445-L447\r\n`dst_copy != nullptr` is checked immediately after `if (dst_copy == nullptr) continue;`. Is one of comparisons supposed to be different, or can the `TF_RET_CHECK` be removed?\r\n\r\n**Issue 2:**\r\nhttps://github.com/tensorflow/tensorflow/blob/9d67841e6c1f852516abf6ff44490b5d5a8331af/tensorflow/contrib/ignite/kernels/dataset/ignite_dataset_iterator.cc#L126-L128\r\nThis code is unreachable, because both branches of `if` above return. Either it should be deleted or some part of `if` modified.\r\n\r\n**Issue 3:**\r\nhttps://github.com/tensorflow/tensorflow/blob/514004a2347058214d1e7b13b9769a2abdd06830/tensorflow/core/profiler/rpc/client/capture_profile.cc#L218-L220\r\nThe condition is always true. My guess is that `==` is intended instead of `!=`.\r\n\r\n**Issue 4:**\r\nhttps://github.com/tensorflow/tensorflow/blob/9590c4c32dd4346ea5c35673336f5912c6072bf2/tensorflow/core/profiler/internal/tfprof_tensor.h#L55\r\nShould this be `void` since nothing gets returned in any branch?\r\n\r\n**Issue 5:**\r\nhttps://github.com/tensorflow/tensorflow/blob/9380a41290e8fb8b9ea85f614472deab56dbc481/tensorflow/stream_executor/stream_executor_pimpl.cc#L112-L125\r\n`ScopedTracer` doesn't obey the rule of 3, but this `SCOPED_TRACE` macro expands to calling its copy constructor. This should be safe in practice because of copy elision, I believe, but undesirable to rely on. Unfortunately, changing to `auto tracer{MakeScopedTracer(this, &LOC##Begin, &LOC##Complete, ##__VA_ARGS__)};` will infer `initializer_list<...>` (is that right?).\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["**Issue 6**\r\nhttps://github.com/tensorflow/tensorflow/blob/4213d5c1bd921f8d5b7b2dc4bbf1eea78d0b5258/tensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc#L282-L293\r\nThe condition in line 284 is always true because the loop ends otherwise.\r\n\r\n**Issue 7**\r\nhttps://github.com/tensorflow/tensorflow/blob/f6822df38cb0b820e2229f07d0a93f215f2dae42/tensorflow/lite/toco/graph_transformations/propagate_fake_quant_num_bits.cc#L208-L214\r\nSecond `if` is always true because if it isn't, we would `continue` in the first one.\r\n\r\n**Issue 8**\r\nhttps://github.com/tensorflow/tensorflow/blob/29ecfbf1e7ab2f073e69770753174667079d10b5/tensorflow/core/kernels/adjust_contrast_op.cc#L311-L313\r\n`mean_plane` is assigned twice, is the first value supposed to be used in the second one?\r\n\r\n**Issue 9**\r\nhttps://github.com/tensorflow/tensorflow/blob/7d18392acd6a00666c2128bb377503637c915668/tensorflow/core/kernels/einsum_op.cc#L165-L166\r\nThe condition in line 166 is always false.\r\n\r\n**Issue 10**\r\nhttps://github.com/tensorflow/tensorflow/blob/e5e4925e7d61cc5e9672d2d936d3ca61549d745d/tensorflow/core/kernels/data/take_dataset_op.cc#L117-L125\r\nCan be simplified to `if`, or needs some condition on `break`."]}, {"number": 30953, "title": "TF-Lite micro low performance", "body": "This issue regards the tflite micro.\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.2\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: STM32F746NG\r\n- TensorFlow installed from (source or binary): github\r\n- TensorFlow version (use command below): 1.14.0-718503b075d\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI've ported a model from Keras to TF and trained it. Then I've converted the model to tflite.\r\n\r\nOne issue is that the tf.lite.Optimize.OPTIMIZE_FOR_SIZE doesn't work as the tflite-micro complains that one of the layers is not compatible with int8 types. Anyway, I've converted the model without quantization and also used CMSIS-NN. The performance is extremely slow. It's 40x times slower on the same CPU compared to X-CUBE-AI API. \r\n\r\n**Describe the expected behavior**\r\nI expected the inference to be much faster especially for the depthwise_conv.cc kernel which supports the cmsis-nn. It seems that using cmsis-nn, makes an insignificant difference.\r\n\r\n**Code to reproduce the issue**\r\nI have a repo here that you can use for validation:\r\nhttps://bitbucket.org/dimtass/stm32f746-tflite-micro-mnist\r\n\r\n**Other info / logs**\r\nAlso, I've written a blog post with the issue here:\r\nhttps://www.stupid-projects.com/machine-learning-on-embedded-part-3/\r\n\r\n**Edit**: I've found out that I haven't enabled the FPU during the g++ compilation. With the correct architecture flags (-mthumb -mcpu=cortex-m7 -mfpu=fpv5-sp-d16 -mfloat-abi=hard), the performance is now 3x times faster, but it's still ~12x times slower than the x-cube-ai.\r\n\r\nThis is the execution time for each layer with comparison to x-cube-ai. Have in mind that the x-cube-ai merges some layers, so I'm using '-' and the sum in the next layer:\r\n\r\nLayer | tflite-micro /soft-float (msec) | tflite-micro /hard-float (msec) | x-cube-ai (msec)\r\n-|-|-|-\r\nDEPTHWISE_CONV_2D | 235 | 69 | -\r\nMAX_POOL_2D | 23 | 7 | 11.2\r\nCONV_2D | 2346 | 733 | -\r\nMAX_POOL_2D | 7 | 2 | 57.19\r\nCONV_2D | 348 | 108 | 8.69\r\nFULLY_CONNECTED | 5 | 3 | -\r\nFULLY_CONNECTED | 0 | 0 | -\r\nSOFTMAX | 0 | 0 | 2\r\nTOTAL TIME | 2964 | 922 | 78.2\r\n\r\nThat's almost 40 times slower on the same MCU. I'm not sure if I'm doing something so terribly wrong or this is the real performance of the API.\r\n\r\nI really hope for some input. Thanks!", "comments": ["I'll also edit the main post, but I've found a stupid mistake that I've made.\r\nI didn't enabled the FPU on the STM32F746. Now that I've enabled it, it's 3x times faster than before, so the inference time dropped from 2964 to 922ms.\r\n\r\nBut it's still much slower that the x-cube-ai implementation. It's now ~12x times slower, which is still too much.\r\n\r\nI've also pushed the changes to the repo.", "I'd like to add my 2 cent's here;\r\nTLDR; I think TF-Lite-Micro is using 64-bit Floats when it should be using 32-bit Floats. \r\n\r\n\r\nBased on the information provided by Dimitris [dimtass] in his articles, when using TF-Lite-Micro each 'Multiply\u2013accumulate operation' (or equivalent) is taking approximately 70 cycles even with the (32-bit) FPU enabled (and approximately 225 cycles without FPU). To me, that seems way too slow for a 32-bit Cortex-M7 with a 32-bit FPU, even if you assume overhead or poor optimization on things like memory access, casting, etc. The fact that going from No FPU to using the FPU only gave a 3x speed up, makes me question what's happening here; specifically, if its working correctly, then a (32-bit) float multiply and addition on the FPU should be way under 70 cycles (even if it's not using the specialized Multiply\u2013accumulate).  \r\n\r\nAdditionally, the fact that the X-Cube-AI framework runs the exact same network (same TF-Lite model and file) on the exact same hardware, and is within the expected performance range says something is very wrong with the TF-Lite run. \r\nNote: The X-Cube-AI averages 5.5 cycles per Multiply\u2013accumulate operation (averaged over a whole prediction), which is 12 times faster than TF-Lite. \r\n\r\nThe simplest explanation is that Double precision Floats (64-bit) are used when it should be (32-bit) Floats. This would force the usage of Software 64-bit Floats because the FPU is only 32-bit, the FPU would speed up some of the individual operations, but still be an order of magnitude slower than the 32-bit Float ran raw on the FPU as it should be. \r\n\r\n\r\nNote: I have not ran this myself, I am making inferences from the numbers and data provided by Dimitris in his Articles.\r\nNote: I'm not a Micro Controller guy, if I made a stupid mistake, or misunderstanding, then let me know so I can learn.\r\nThanks,", "Today morning I thought why not build tflite-micro on my workstation and test it with the python implementation. I thought that since my workstation doesn't have any GPU acceleration and only the CPU is used, then I should get quite similar results.\r\n\r\nAt this point I need to mention that I'm not aware of the internal and the implementation on either C++ or python API, but my guess is that since only the CPU is available, then they should perform about the same.\r\n\r\nI've created this repo here:\r\nhttps://bitbucket.org/dimtass/tflite-micro-python-comparison\r\n\r\nI'm explaining in the README with more detail how to build and run the tests. Don't mind about the cmake files that may refer stm32 at some points as I've just stripped the cmake project I've mention in the first post. So it's the same code with [this repo](https://bitbucket.org/dimtass/stm32f746-tflite-micro-mnist).\r\n\r\nAs I'm explaining in the readme I'm using the exact same model and input data.\r\n\r\nMy results for a 1000 inference run are:\r\n\r\ntflite-micro | python\r\n-|-\r\n2.223457 ms | 0.199918 ms\r\n\r\nSo the python implementation is 11.2 times faster than the C++ code.\r\n\r\nAlthough these are different APIs, I would expect that the performance should be same, as only the CPU is involved. **Unless**, the python API spawns multiple threads in multiple cores. That would explain the difference.\r\n\r\nI've tried this command to run the python benchmark on a single cpu and I got the same result as without it, but tbh I don't know if that command works with python and actually prevents other threads to be spawned to other cores.\r\n```sh\r\ntaskset --cpu-list 1 python3 python_benchmark.py\r\n```", "Well, that would disprove my theory, since your workstation probably has a 64-bit FPU.\r\n\r\nCan you run 10,000 or 100,000 inference runs to get a bigger metric?\r\nA 2ms difference is hard to be sure it's not some other limitation (like HDD, OS, etc). \r\nOr is the time in ms mentioned (2.223457 ms, 0.199918 ms) the average per inference over the 1000 inferences? (making the total over 2 seconds for tflite-micro)\r\n\r\n", "Hi Raukk, yes the result is the average of the 1000 inferences. I've tried more and it seems a constant difference. For the tflite-micro running on the CPU the average time is 2ms, for python is 200us.\r\n\r\nI'm afraid that this might be because the python backend is using all the cores/threads. I'm not sure if this `taskset --cpu-list` can actually stop python to spawn other threads to other cores.\r\n\r\nNext, I want to try with some gcc flags and optimizations for my CPU. Maybe the python backend is using binaries with those optimizations enabled.\r\n\r\nIf I have time I'll try the same test on various SBCs I have around like a RPi3+, nanopi-neo4 and a couple of others. After this week I'll be off for 2 months and I won't have access to my equipment to run further tests :/\r\n\r\nEdit: I guess if I test on my beaglebone black, which has a single core, I'll figure out more things. I'll that first.", "I ran the Python script version (both 1,000 and 100,000 iterations) on an old laptop (Windows, i5) and the average time was 0.55ms on average. CPU was i5-3317U, with clock at 1700 MHz with single core-boost to 2600 MHz or both core 2400 MHz.\r\n\r\nWhen running the Resource Manager shows all 4 CPUs (2 Core with Hyper threading) under heavy load, and CPU frequency at 140% which matches 2400 MHz. \r\nIt reports the python usage as only about %50 but system usage at 80%. \r\nI'd take those values with a grain of salt. \r\n  \r\nI then ran it on my Ryzen 7 (8 cores/16 threads), where CPU 8 got pegged, a few others showed minor load, and the rest were flat lined. Python usage was reported at ~10% with is more than 16th.\r\nThe Ryzen averaged 0.35ms at a sedate ~3.0 GHz, so the speed seems to be tightly coupled to the clock rate.\r\nMy guess would be that standard tensorflow was respecting the single threadedness for the actual math and calculations but was probably also doing everything else in a second thread (data loading, overhead, etc), just a guess.\r\n\r\nWhile the fact it's probably running ~dual threaded may give it some advantage, I'd say it doesn't look like it gave more than double. Which would still leave at least a 5x slowdown. \r\n\r\nI didn't run the TF-Lite-Micro one yet, so, this is only for the python script you gave. \r\n", "I've just verified that the python backend spawns several threads in the background even if it's limited in one cpu, therefore my test is meaningless...\r\n\r\n[python screeshot](https://ibb.co/88CNkRc)\r\n[tflite-micro screenshot](https://ibb.co/2Ykd7CJ)", "@dimtass I wouldn't say it's \"meaningless,\" it just means that you can't draw a 1-to-1 comparison.\r\nSpecifically, I don't believe that would result in a 10x speedup, since your report shows Python using 100%+30%+30%. Even with considering async-loading, pre-processing, or other things that the secondary thread could be doing, I cant see 30%+30% resulting in a 10x performance increase.\r\nIf the CPU it runs on is true single core without hyper threading, then the comparison should be accurate, I might try sticking it in a VM that only has one core, if I have time.\r\n\r\n\r\n@petewarden Has anyone one the Tensorflow-Lite team looked at this (or [Read Dimitris Article](https://www.stupid-projects.com/machine-learning-on-embedded-part-3/) ) ?\r\nI am not a TF-Lite Developer (and never will be), but I am interested in the results of Dimitris experiments, and the performance for TF-Lite looks to be very-very slow for a framework designed around the principles of running efficiently on low end hardware. ", "I've just added more benchmarks for tflite-micro on various SBCs [here](https://www.stupid-projects.com/benchmarking-tensorflow-lite-for-microcontrollers-on-linux-sbcs/).\r\n\r\nI wish there was a #define switch to enable threads to the tflite-micro API :)", "It's been 3 months, has anyone from the tensor flow project even looked at this?", "Apologies for the delay and thanks for your feedback.  I read the article and you made a lot of good points for us to improve on.  Recently we have been digging into performance optimization, and working with ARM to land CMSIS-NN optimized kernels for key ops (conv, depthwise_conv, fully_connected).\r\n\r\nThe version of TFLite Micro you were using had uint8 CMSIS-NN optimizations crafted specifically for our speech example, so you would not have seen any speed-up for other models.  We recently received general-purpose CMSIS-NN kernels for int8 quantized models, which should speed things up significantly.  We realize there is still a long way to go to reach the performance numbers you saw using X-CUBE-AI. Our eventual goal is to get as close as possible to (or exceed where possible) platform-specific optimized libraries like X-CUBE-AI while providing a portable platform. We have also recently been working to improve and consolidate our documentation.\r\n\r\nWe realize there is a long way to go, so thank you for the hard work and clear feedback.", "Hello @dimtass ! Let me know what performance numbers you get with CMSIS-NN using int8 quantization. X-CUBE-AI uses CMSIS-NN kernels as well, so we expect a similar performance. Thanks for showing interest in CMSIS lib!", "Hi @freddan80 there's been quite some time that I haven't follow up the recent changes in tensorflow. Currently, I'm quite busy to setup everything again, but I plan to do so at some point in the near future in order to evaluate TF again. Therefore, I'll update this thread with my findings when it's done. Thanks!", "_I've heavily edited the previous post according to my findings._\r\n\r\nI've updated from v1.14.0 to 2.1.0 and I've documented the results [here](https://www.stupid-projects.com/tensorflow-2-1-0-for-microcontrollers-benchmarks-on-stm32f746/).\r\n\r\nI've found the main issue I had with the very bad performance of tflite compared to x-cube-ai and it was due to the debug flag that was forgotten in the cmake. The performance is now much better but it's still 1.64x times slower compared to x-cube-ai. For me personally, this difference although it's quite high depending the application that is going to be used, it's acceptable especially for evaluating the model on different MCUs.\r\n\r\nAnother thing I've found is that (for me) the v2.1.0 is quite slower compared to v1.14.0. This is the difference I get between the two versions:\r\n\r\nLayer | 1.14.0 | 2.1.0 | msec diff\r\n-- | -- | -- | --\r\nDEPTHWISE_CONV_2D | 18.7 | 24.9 | 6.2\r\nMAX_POOL_2D | 1.99 | 3.27 | 1.28\r\nCONV_2D | 91.08 | 166.29 | 75.21\r\nMAX_POOL_2D | 0.56 | 0.96 | 0.4\r\nCONV_2D | 12.51 | 23.32 | 10.81\r\nFULLY_CONNECTED | 1.48 | 1.48 | 0\r\nFULLY_CONNECTED | 0.03 | 0.03 | 0\r\nSOFTMAX | 0.01 | 0.02 | 0.01\r\nTOTAL TIME= | 126.36 | 220.27 | 93.91\r\n\r\nTherefore, my model is ~94ms slower on v2.1.0.\r\n\r\nI've also managed to make the optimized model load and it seems that from the `tensorflow/lite/core/api/flatbuffer_conversions.cc` I get that the tensor types inside the model are of type `kTfLiteFloat16` and `TensorType_INT8`, therefore the conversion seems to be working and also the model inference is running. For the uncompressed model, I only get `kTfLiteFloat16`.\r\n\r\nNevertheless, the inference result is not precise and actually it's all over the place. For example, this is the result I get from the uncompressed model:\r\n\r\nOutput | Uncompressed | Compressed\r\n-- | -- | --\r\nOut[0] | 0.000000 | 0.093871\r\nOut[1] | 0.000000 | 0.100892\r\nOut[2] | 1.000000 | 0.099631\r\nOut[3] | 0.000000 | 0.106597\r\nOut[4] | 0.000000 | 0.099124\r\nOut[5] | 0.000000 | 0.096398\r\nOut[6] | 0.000000 | 0.099573\r\nOut[7] | 0.000000 | 0.101923\r\nOut[8] | 0.000000 | 0.103691\r\nOut[9] | 0.000000 | 0.098299\r\n\r\nI've tested with both `-mfp16-format=ieee` and `-mfp16-format=alternative` flags during compile and it seems that I don't get any better performance, even with the model's weight optimizations.\r\n\r\nThe repo I've used for my tests is [here](https://github.com/dimtass/stm32f746-tflite-micro-mnist). The master branch is v2.1.0 and there's a branch for v1.14.0.\r\n", "Hello @dimtass! I think the most important issue to sort out here is the quantization of the model. It looks like the data type is float, which will not give you any performance.\r\n\r\nTo get the most out of TFLu and CMSIS-NN, please use int8 data type. Do you have a .tflite file?\r\n\r\nA good place to check that the right function is called is for example here:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/64219fd7dabcc7f5b23cd72d0608378aaabb27ed/tensorflow/lite/micro/kernels/cmsis-nn/depthwise_conv.cc#L201\r\n\r\nIf you print `input->type` here:\r\nhttps://github.com/tensorflow/tensorflow/blob/64219fd7dabcc7f5b23cd72d0608378aaabb27ed/tensorflow/lite/micro/kernels/cmsis-nn/depthwise_conv.cc#L341\r\n\r\nYou'll get the quantization type.\r\n\r\nCheers!", "Hi @freddan80 and thanks for the reply. I've used keras and tensorflow to build the model (you cen find the jupyter notebook [here](https://github.com/dimtass/stm32f746-tflite-micro-mnist/blob/master/jupyter_notebook/MNIST-TensorFlow.ipynb). Also, [this is](https://github.com/dimtass/stm32f746-tflite-micro-mnist/blob/master/jupyter_notebook/mnist-tflite.h5) the keras model that I've used for post-quantization.\r\n\r\nThe script that I've used for the quantization is taken from the tensorflow online documentation [here](https://www.tensorflow.org/lite/convert/quantization) and its the following:\r\n\r\n```py\r\ntflite_mnist_model = 'mnist.tflite'\r\nconverter = tf.lite.TFLiteConverter.from_keras_model_file('mnist-tflite.h5')\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\ntflite_model = converter.convert()\r\nflatbuffer_size = open(tflite_mnist_model, \"wb\").write(tflite_model)\r\n```\r\n\r\nI've also added a printf in the line you suggested and indeed is seems that the type I get from the compressed model is `kTfLiteFloat32` while I was expecting `kTfLiteInt8`. That means that there's something going on with the conversion.\r\n\r\nI've checked that the *.tflite I get after the conversion is much smaller (99.3KB) compared to the one without without the conversion (367KB), therefore I guess that it is processed. Although the fact that the model is much smaller and the input type is wrong and the non-sense results I get from this model when I run the inference, it means that there's something wrong with the conversion.\r\n\r\nDo you have any suggestions on that? Is there any other way or tool to try to optimize an *.h5 model?\r\n\r\nThanks!", "@dimtass Sorry, I don't have much experience with model conversion. Perhaps @petewarden can point to someone.\r\n\r\nCheers!", "Hey @dimtass, according to your code snippet and your Jupyter notebook you only quantize the weights - the inference computation is done in float32. Therefore you see no significant acceleration using `cmsis-nn`. See more [here](https://www.tensorflow.org/lite/performance/post_training_integer_quant).\r\n> At inference, weights are converted from 8-bits of precision to floating point and computed using floating-point kernels. This conversion is done once and cached to reduce latency.\r\n\r\nThe following snippet provides a full int8 quantization (including computation). However, there is currently an issue with int8 input and output see #38285 \r\n\r\n```python\r\ndef representative_dataset():\r\n  for x in x_test_normalized:\r\n    yield [np.array([x], dtype=np.float32)]\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(my_model)\r\n# Set the optimization flag.\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n# Enforce full-int8 quantization (except inputs/outputs which are always float)\r\n# So to ensure that the converted model is fully quantized \r\n# (make the converter throw an error if it encounters an operation it cannot quantize), \r\n# and to use integers for the model's input and output, \r\n# you need to convert the model again using these additional configurations:\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.int8  # or tf.uint8\r\nconverter.inference_output_type = tf.int8  # or tf.uint8\r\n# Provide a representative dataset to ensure we quantize correctly.\r\nconverter.representative_dataset = representative_dataset\r\ntflite_model = converter.convert()\r\nopen('./TFLite-model/LeNET-MNIST_int8ops.tflite', 'wb').write(tflite_model)\r\n``` \r\n\r\n\r\nTry running your code on the STM32F7 again with the updated model. You should see some significant inference time improvement. \r\n\r\nIn my preliminary results I see a speedup of 10x with the fully int8 quantized model compared to the tflite-model with only the weights quantized.", "Hi @lheim I've tried the above snippet and I'm getting an error that `x_test_normalized` is not set, which OK it makes sense because I haven't defined it anywhere. How did you created this normalized set? Thanks!", "@dimtass there is no special requirement from the TF Lite converter side. \r\nYou only need to provide a *representative* dataset which is the same format as your training samples. In my case I used my whole normalized test set, as my network was trained on normalized data. Usually less samples are also fine (~100)."]}, {"number": 30947, "title": "[TF 2.0 nightly] tf.keras.estimator.model_to_estimator with strategy=tf.distribute.MirroredStrategy() -> Method requires being in cross-replica context, use get_replica_context().merge_call()", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 10.14.5\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary from pip\r\n- TensorFlow version (use command below): tf-nightly-2.0-preview==2.0.0.dev20190722\r\n- Python version: 3.6.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n>>> import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\r\nv1.12.1-6737-gb4e5625437 2.0.0-dev20190722\r\n\r\n**Describe the current behavior**\r\nwhen using `strategy = tf.distribute.MirroredStrategy()`, `tf.keras.estimator.model_to_estimator()`\r\n\r\nis crashing during training with \r\n`\r\nRuntimeError: Method requires being in cross-replica context, use get_replica_context().merge_call()`\r\n\r\n**Describe the expected behavior**\r\nShould work as before and without any error messages\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\nfrom absl import logging\r\n\r\nlogging.set_verbosity(logging.INFO)\r\n# Define the estimator's input_fn\r\nSTEPS_PER_EPOCH = 5\r\nBATCH_SIZE = 64\r\nNUM_EPOCHS = 5\r\n\r\n\r\ndef input_fn():\r\n    datasets, ds_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\r\n    mnist_train, mnist_test = datasets['train'], datasets['test']\r\n\r\n    BUFFER_SIZE = 10000\r\n    BATCH_SIZE = 64\r\n\r\n    def scale(image, label):\r\n        image = tf.cast(image, tf.float32)\r\n        image /= 255\r\n    \r\n        return image, label[..., tf.newaxis]\r\n\r\n    train_data = mnist_train.map(scale).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\r\n    return train_data.repeat()\r\n\r\n# Define train & eval specs\r\ntrain_spec = tf.estimator.TrainSpec(input_fn=input_fn,\r\n                                    max_steps=STEPS_PER_EPOCH * NUM_EPOCHS)\r\neval_spec = tf.estimator.EvalSpec(input_fn=input_fn,\r\n                                  steps=STEPS_PER_EPOCH)\r\n\r\ndef make_model():\r\n    return tf.keras.Sequential([\r\n        tf.keras.layers.Conv2D(32, 3, activation='relu',\r\n                               kernel_regularizer=tf.keras.regularizers.l2(0.02),\r\n                               input_shape=(28, 28, 1)),\r\n        tf.keras.layers.MaxPooling2D(),\r\n        tf.keras.layers.Flatten(),\r\n        tf.keras.layers.Dropout(0.1),\r\n        tf.keras.layers.Dense(64, activation='relu'),\r\n        tf.keras.layers.BatchNormalization(),\r\n        tf.keras.layers.Dense(10, activation='softmax')\r\n    ])\r\n\r\nmodel = make_model()\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\n#####\r\n#strategy=None \r\n# crashing\r\nstrategy = tf.distribute.MirroredStrategy()\r\n\r\n# config tf.estimator to use a give strategy\r\ntraining_config = tf.estimator.RunConfig(train_distribute=strategy)\r\n#####\r\n\r\nestimator = tf.keras.estimator.model_to_estimator(\r\n    keras_model = model,\r\n    config=training_config\r\n)\r\n\r\ntf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\nW0723 11:29:42.079696 4746859968 cross_device_ops.py:1207] There is non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\r\nI0723 11:29:42.087183 4746859968 run_config.py:554] Initializing RunConfig with distribution strategies.\r\nI0723 11:29:42.088423 4746859968 estimator_training.py:167] Not using Distribute Coordinator.\r\nW0723 11:29:42.097567 4746859968 estimator.py:1812] Using temporary folder as model directory: /var/folders/l7/00kxfwvs0vbbqxtrp3rpf3yh0000gn/T/tmp8lhzz88_\r\nI0723 11:29:42.098804 4746859968 keras.py:527] Using the Keras model provided.\r\nI0723 11:29:43.420326 4746859968 estimator.py:209] Using config: {'_model_dir': '/var/folders/l7/00kxfwvs0vbbqxtrp3rpf3yh0000gn/T/tmp8lhzz88_', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\r\ngraph_options {\r\n  rewrite_options {\r\n    meta_optimizer_iterations: ONE\r\n  }\r\n}\r\n, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': <tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0xb3aa517b8>, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0xb3aa51a90>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_distribute_coordinator_mode': None}\r\nI0723 11:29:43.421994 4746859968 estimator_training.py:186] Not using Distribute Coordinator.\r\nI0723 11:29:43.422926 4746859968 training.py:612] Running training and evaluation locally (non-distributed).\r\nI0723 11:29:43.423843 4746859968 training.py:700] Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\r\nI0723 11:29:43.429722 4746859968 dataset_builder.py:184] Overwrite dataset info from restored data version.\r\nI0723 11:29:43.434206 4746859968 dataset_info.py:380] Field info.location from disk and from code do not match. Keeping the one from code.\r\nI0723 11:29:43.435199 4746859968 dataset_builder.py:253] Reusing dataset mnist (/Users/tarrade/tensorflow_datasets/mnist/1.0.0)\r\nI0723 11:29:43.435896 4746859968 dataset_builder.py:399] Constructing tf.data.Dataset for split None, from /Users/tarrade/tensorflow_datasets/mnist/1.0.0\r\nW0723 11:29:43.586170 4746859968 dataset_builder.py:439] Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\r\nI0723 11:29:43.751439 4746859968 cross_device_ops.py:425] Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\r\nI0723 11:29:43.764024 123145625903104 estimator.py:1145] Calling model_fn.\r\nI0723 11:29:43.943940 4746859968 cross_device_ops.py:425] Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\r\nI0723 11:29:43.948454 4746859968 cross_device_ops.py:425] Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\r\nI0723 11:29:43.952039 4746859968 cross_device_ops.py:425] Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\r\nI0723 11:29:43.954842 4746859968 cross_device_ops.py:425] Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\r\nI0723 11:29:44.504832 123145625903104 coordinator.py:219] Error reported to Coordinator: Method requires being in cross-replica context, use get_replica_context().merge_call()\r\nTraceback (most recent call last):\r\n  File \"/Users/tarrade/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/training/coordinator.py\", line 297, in stop_on_exception\r\n    yield\r\n  File \"/Users/tarrade/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py\", line 865, in run\r\n    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\r\n  File \"/Users/tarrade/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1146, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"/Users/tarrade/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/keras.py\", line 326, in model_fn\r\n    saver = saver_lib.Saver(var_list=var_list, sharded=True)\r\n  File \"/Users/tarrade/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py\", line 828, in __init__\r\n    self.build()\r\n  File \"/Users/tarrade/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py\", line 840, in build\r\n    self._build(self._filename, build_save=True, build_restore=True)\r\n  File \"/Users/tarrade/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py\", line 878, in _build\r\n    build_restore=build_restore)\r\n  File \"/Users/tarrade/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py\", line 497, in _build_internal\r\n    per_device = self._GroupByDevices(saveables)\r\n  File \"/Users/tarrade/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py\", line 404, in _GroupByDevices\r\n    pydev.canonical_name(spec.tensor.device) for spec in saveable.specs)\r\n  File \"/Users/tarrade/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py\", line 404, in <genexpr>\r\n    pydev.canonical_name(spec.tensor.device) for spec in saveable.specs)\r\n  File \"/Users/tarrade/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/training/saving/saveable_object.py\", line 52, in tensor\r\n    return self._tensor() if callable(self._tensor) else self._tensor\r\n  File \"/Users/tarrade/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/distribute/values.py\", line 1136, in tensor\r\n    return strategy.extended.read_var(sync_on_read_variable)\r\n  File \"/Users/tarrade/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py\", line 722, in read_var\r\n    return replica_local_var._get_cross_replica()  # pylint: disable=protected-access\r\n  File \"/Users/tarrade/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/distribute/values.py\", line 1222, in _get_cross_replica\r\n    self, axis=None)\r\n  File \"/Users/tarrade/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 794, in reduce\r\n    _require_cross_replica_or_default_context_extended(self._extended)\r\n  File \"/Users/tarrade/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 206, in _require_cross_replica_or_default_context_extended\r\n    raise RuntimeError(\"Method requires being in cross-replica context, use \"\r\nRuntimeError: Method requires being in cross-replica context, use get_replica_context().merge_call()\r\n\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-3-f33ca3103e62> in <module>\r\n     64 )\r\n     65 \r\n---> 66 tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n\r\n~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py in train_and_evaluate(estimator, train_spec, eval_spec)\r\n    471         '(with task id 0).  Given task id {}'.format(config.task_id))\r\n    472 \r\n--> 473   return executor.run()\r\n    474 \r\n    475 \r\n\r\n~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py in run(self)\r\n    611         config.task_type != run_config_lib.TaskType.EVALUATOR):\r\n    612       logging.info('Running training and evaluation locally (non-distributed).')\r\n--> 613       return self.run_local()\r\n    614 \r\n    615     # Distributed case.\r\n\r\n~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py in run_local(self)\r\n    712         max_steps=self._train_spec.max_steps,\r\n    713         hooks=train_hooks,\r\n--> 714         saving_listeners=saving_listeners)\r\n    715 \r\n    716     eval_result = listener_for_eval.eval_result or _EvalResult(\r\n\r\n~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)\r\n    365 \r\n    366       saving_listeners = _check_listeners_type(saving_listeners)\r\n--> 367       loss = self._train_model(input_fn, hooks, saving_listeners)\r\n    368       logging.info('Loss for final step: %s.', loss)\r\n    369       return self\r\n\r\n~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)\r\n   1154   def _train_model(self, input_fn, hooks, saving_listeners):\r\n   1155     if self._train_distribution:\r\n-> 1156       return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n   1157     else:\r\n   1158       return self._train_model_default(input_fn, hooks, saving_listeners)\r\n\r\n~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in _train_model_distributed(self, input_fn, hooks, saving_listeners)\r\n   1217       self._config._train_distribute.configure(self._config.session_config)\r\n   1218       return self._actual_train_model_distributed(\r\n-> 1219           self._config._train_distribute, input_fn, hooks, saving_listeners)\r\n   1220     # pylint: enable=protected-access\r\n   1221 \r\n\r\n~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in _actual_train_model_distributed(self, strategy, input_fn, hooks, saving_listeners)\r\n   1297                     labels,  # although this will be None it seems\r\n   1298                     ModeKeys.TRAIN,\r\n-> 1299                     self.config))\r\n   1300           loss = strategy.reduce(\r\n   1301               _get_loss_reduce_op_for_reporting(),\r\n\r\n~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py in call_for_each_replica(self, fn, args, kwargs)\r\n   1769       kwargs = {}\r\n   1770     with self._container_strategy().scope():\r\n-> 1771       return self._call_for_each_replica(fn, args, kwargs)\r\n   1772 \r\n   1773   def _call_for_each_replica(self, fn, args, kwargs):\r\n\r\n~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py in _call_for_each_replica(self, fn, args, kwargs)\r\n    645                           self._container_strategy().__class__.__name__, 5)\r\n    646     return _call_for_each_replica(self._container_strategy(), self._device_map,\r\n--> 647                                   fn, args, kwargs)\r\n    648 \r\n    649   def _configure(self,\r\n\r\n~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py in _call_for_each_replica(distribution, device_map, fn, args, kwargs)\r\n    194     for t in threads:\r\n    195       t.should_run.set()\r\n--> 196     coord.join(threads)\r\n    197 \r\n    198   return values.regroup(device_map, tuple(t.main_result for t in threads))\r\n\r\n~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/training/coordinator.py in join(self, threads, stop_grace_period_secs, ignore_live_threads)\r\n    387       self._registered_threads = set()\r\n    388       if self._exc_info_to_raise:\r\n--> 389         six.reraise(*self._exc_info_to_raise)\r\n    390       elif stragglers:\r\n    391         if ignore_live_threads:\r\n\r\n~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/six.py in reraise(tp, value, tb)\r\n    691             if value.__traceback__ is not tb:\r\n    692                 raise value.with_traceback(tb)\r\n--> 693             raise value\r\n    694         finally:\r\n    695             value = None\r\n\r\n~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/training/coordinator.py in stop_on_exception(self)\r\n    295     \"\"\"\r\n    296     try:\r\n--> 297       yield\r\n    298     except:  # pylint: disable=bare-except\r\n    299       self.request_stop(ex=sys.exc_info())\r\n\r\n~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py in run(self)\r\n    863               self._var_scope, reuse=self.replica_id > 0), \\\r\n    864           variable_scope.variable_creator_scope(self.variable_creator_fn):\r\n--> 865         self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\r\n    866         self.done = True\r\n    867     finally:\r\n\r\n~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in _call_model_fn(self, features, labels, mode, config)\r\n   1144 \r\n   1145     logging.info('Calling model_fn.')\r\n-> 1146     model_fn_results = self._model_fn(features=features, **kwargs)\r\n   1147     logging.info('Done calling model_fn.')\r\n   1148 \r\n\r\n~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/keras.py in model_fn(features, labels, mode)\r\n    324       object_graph = graph_view.ObjectGraphView(model)\r\n    325       var_list = object_graph.frozen_saveable_objects()\r\n--> 326       saver = saver_lib.Saver(var_list=var_list, sharded=True)\r\n    327       saver._object_restore_saver = trackable_util.frozen_saver(model)\r\n    328       scaffold = monitored_session.Scaffold(saver=saver)\r\n\r\n~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py in __init__(self, var_list, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, saver_def, builder, defer_build, allow_empty, write_version, pad_step_number, save_relative_paths, filename)\r\n    826           time.time() + self._keep_checkpoint_every_n_hours * 3600)\r\n    827     elif not defer_build:\r\n--> 828       self.build()\r\n    829     if self.saver_def:\r\n    830       self._check_saver_def()\r\n\r\n~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py in build(self)\r\n    838     if context.executing_eagerly():\r\n    839       raise RuntimeError(\"Use save/restore instead of build in eager mode.\")\r\n--> 840     self._build(self._filename, build_save=True, build_restore=True)\r\n    841 \r\n    842   def _build_eager(self, checkpoint_path, build_save, build_restore):\r\n\r\n~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py in _build(self, checkpoint_path, build_save, build_restore)\r\n    876           filename=checkpoint_path,\r\n    877           build_save=build_save,\r\n--> 878           build_restore=build_restore)\r\n    879     elif self.saver_def and self._name:\r\n    880       # Since self._name is used as a name_scope by builder(), we are\r\n\r\n~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py in _build_internal(self, names_to_saveables, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, filename, build_save, build_restore)\r\n    495       # Add the save ops.\r\n    496       if sharded:\r\n--> 497         per_device = self._GroupByDevices(saveables)\r\n    498         if build_save:\r\n    499           save_tensor = self._AddShardedSaveOps(filename_tensor, per_device)\r\n\r\n~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py in _GroupByDevices(self, saveables)\r\n    402     for saveable in saveables:\r\n    403       canonical_device = set(\r\n--> 404           pydev.canonical_name(spec.tensor.device) for spec in saveable.specs)\r\n    405       if len(canonical_device) != 1:\r\n    406         raise ValueError(\"All tensors of a saveable object must be \"\r\n\r\n~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py in <genexpr>(.0)\r\n    402     for saveable in saveables:\r\n    403       canonical_device = set(\r\n--> 404           pydev.canonical_name(spec.tensor.device) for spec in saveable.specs)\r\n    405       if len(canonical_device) != 1:\r\n    406         raise ValueError(\"All tensors of a saveable object must be \"\r\n\r\n~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/training/saving/saveable_object.py in tensor(self)\r\n     50   @property\r\n     51   def tensor(self):\r\n---> 52     return self._tensor() if callable(self._tensor) else self._tensor\r\n     53 \r\n     54 \r\n\r\n~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/distribute/values.py in tensor()\r\n   1134     def tensor():\r\n   1135       strategy = sync_on_read_variable._distribute_strategy  # pylint: disable=protected-access\r\n-> 1136       return strategy.extended.read_var(sync_on_read_variable)\r\n   1137 \r\n   1138     spec = saver.BaseSaverBuilder.SaveSpec(\r\n\r\n~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py in read_var(self, replica_local_var)\r\n    720     \"\"\"Read the aggregate value of a replica-local variable.\"\"\"\r\n    721     if isinstance(replica_local_var, values.SyncOnReadVariable):\r\n--> 722       return replica_local_var._get_cross_replica()  # pylint: disable=protected-access\r\n    723     assert isinstance(replica_local_var, values.Mirrored)\r\n    724     return array_ops.identity(replica_local_var.get())\r\n\r\n~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/distribute/values.py in _get_cross_replica(self)\r\n   1220       return self._distribute_strategy.reduce(\r\n   1221           reduce_util.ReduceOp.from_variable_aggregation(self.aggregation),\r\n-> 1222           self, axis=None)\r\n   1223 \r\n   1224   def _as_graph_element(self):\r\n\r\n~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py in reduce(self, reduce_op, value, axis)\r\n    792     \"\"\"\r\n    793     # TODO(josh11b): support `value` being a nest.\r\n--> 794     _require_cross_replica_or_default_context_extended(self._extended)\r\n    795     if isinstance(reduce_op, six.string_types):\r\n    796       reduce_op = reduce_util.ReduceOp(reduce_op.upper())\r\n\r\n~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py in _require_cross_replica_or_default_context_extended(extended)\r\n    204     _wrong_strategy_scope(strategy, context)\r\n    205   assert cross_replica is None\r\n--> 206   raise RuntimeError(\"Method requires being in cross-replica context, use \"\r\n    207                      \"get_replica_context().merge_call()\")\r\n    208 \r\n\r\nRuntimeError: Method requires being in cross-replica context, use get_replica_context().merge_call()\r\n\r\n```\r\n", "comments": ["I have tried on colab with TF nightly version  and was able to reproduce the issue.Please, find the [gist](https://colab.research.google.com/drive/1h51lgpv27ro23gwjuz1yv0Sgbz0wQggG) here.Thanks!", "Hi - this looks like a bug related to saving/checkpointing when using a batch norm layer + distribution strategy. While we look into fixing this, it is possible for you to try with these options to unblock yourself:\r\n- use checkpoint_format = \"saver\"\r\n- do not use batch norm layer ", "Hi @guptapriya,\r\n\r\nI try again with Tensorflow 2.1 and the issue is still there.\r\n\r\nI confirm what you were saying:\r\n- if I comment out bacht norm in my keras mode -> it is working\r\n-  if I use checkpoint_format='saver' in with tf.keras.estimator.model_to_estimator\r\n\r\nThanks\r\nCheers\r\nFabien", "Hi @tarrade thanks for confirming. We haven't had the bandwidth to fix this use case as we are focusing on native Keras training workflows. We have opened this for contributions if you or anyone would like to contribute a fix. ", "Hi @guptapriya ,\r\n\r\nThanks for the follow up. I will be happy to contribute but I would like to be sure that this is useful.\r\n\r\nBefore TF 2.0, it was mention many time that tf.estimator will continue. Since TF 2.0 is out it seems that tf.estimator will not continue after 2.x. This was mention many time by Googlers. We can also see that tf.estimator still use v1.comp ops. Keras seems now to have all options that were provided by tf.estimator like distributed training and serving (in the same way as tf.estimator). TFX was until recently the only place were tf.estimator was the only option but now, since the migration to TF 2.1, TFX now provide support for Keras. Given that and if I am not wrong, it doesn't seems like tf.estimator will stay for very long. Maybe I am wrong but this is what I was told since I am discussing about tf.estimator and TF.2.x. I am also migration my code from tf.estimator to Keras to be sure  have all the functionalities I need.", "@tarrade thanks for the follow up. tf.estimator is available in TF 2.x but your assessment is correct that there isn't much investment in improving it going forward, and all efforts are focused on Keras. If you're able to migrate your code to Keras, that is great and I would agree that you do not need to work on this fix. I was only suggesting that in case you could not move to estimator and were blocked by this bug. Thanks! ", "Hi, I'm also facing this problem, but I'm doing custom training loop and not using TF Estimator. I also have batch norm layers in my models.", "I have the same problem in TF1.15, do you have any idea to solve it? @guptapriya ", "@xiankgx @ZhuLingfeng1993 are you guys using model_to_estimator? if not, please open a separate bug with code to repro the problem. Also , have you tried with latest TF versions already? TF 2.1 or nightly? ", "@guptapriya  Thank you for reply. I fix this bug when training mobilenetv2_ssd in tensorflow object detection API with TF1.15, by add `inplace_batchnorm_update: true` in pipeline.config.", "Was able to reproduce this issue in TF v2.5 ,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/962fa7dceb43341f3fed97405513a6ca/untitled16.ipynb)..Thanks !", "Was able to reproduce this issue in TF v2.7 , please find the gist [here](https://colab.research.google.com/gist/kumariko/d96c1a3d6f042f049a73d962008401bc/untitled16.ipynb#scrollTo=zPU3hV5j2fJ5)..Thanks !"]}, {"number": 30914, "title": "CPU.ModifyGraphWithDelegate is disallowed when graph is immutable. Delegate should run on the same thread where it was initialized.Node number 64 (TfLiteGpuDelegate) failed to invoke.ed to invoke.", "body": "<em>\r\n-We are trying to enable GPU using GPU delegate in TensorFlow[ object detection sample project](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android), by taking reference from GPU enabled sample project of [Object  Classification](https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android).-- we are also using Float Model for detection. </em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android Studio installed on Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Realme One\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\ndependencies added:\r\n`implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'\r\nimplementation 'org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly'`\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: OpenGL ES 3.2 available on the phone\r\n\r\nThe project has been compiled in Android Studio using these dependencies:\r\n`dependencies { implementation fileTree(dir: 'libs', include: ['*.jar','*.aar']) implementation 'com.android.support:appcompat-v7:28.0.0' implementation 'com.android.support:design:28.0.0' implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly' implementation 'org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly' } \r\n`\r\n**Describe the current behavior**\r\nWe are facing this error unable to understand the exact issue:\r\n`CPU.ModifyGraphWithDelegate is disallowed when graph is immutable.\r\n       at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)\r\n\r\n   java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: Next operations are not supported by GPU delegate:\r\n   CUSTOM TFLite_Detection_PostProcess: Operation is not supported.\r\n   First 63 operations will run on the GPU, and the remaining 1 on the CPU.TfLiteGpuDelegate Invoke: Delegate should run on the same thread where it was initialized.Node number 64 (TfLiteGpuDelegate) failed to invoke.\r\n   \r\n       at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)\r\n       at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:149)\r\n       at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:275)\r\n       at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:228)\r\n       at com.demo.webview.bt.webviewbtdemo.basic.TFLiteWrapper$5.run(TFLiteWrapper.java:360)\r\n       at android.os.Handler.handleCallback(Handler.java:873)`\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\n`public class TFLiteObjectDetectionAPIModel implements Classifier {\r\n  private static final Logger LOGGER = new Logger();\r\n\r\n  // Only return this many results.\r\n  private static final int NUM_DETECTIONS = 10;\r\n  // Float model\r\n  private static final float IMAGE_MEAN = 128.0f;\r\n  private static final float IMAGE_STD = 128.0f;\r\n  // Number of threads in the java app\r\n  private static final int NUM_THREADS = 4;\r\n  private boolean isModelQuantized;\r\n  // Config values.\r\n  private int inputSize;\r\n  // Pre-allocated buffers.\r\n  private Vector<String> labels = new Vector<String>();\r\n  private int[] intValues;\r\n  // outputLocations: array of shape [Batchsize, NUM_DETECTIONS,4]\r\n  // contains the location of detected boxes\r\n  private float[][][] outputLocations;\r\n  // outputClasses: array of shape [Batchsize, NUM_DETECTIONS]\r\n  // contains the classes of detected boxes\r\n  private float[][] outputClasses;\r\n  // outputScores: array of shape [Batchsize, NUM_DETECTIONS]\r\n  // contains the scores of detected boxes\r\n  private float[][] outputScores;\r\n  // numDetections: array of shape [Batchsize]\r\n  // contains the number of detected boxes\r\n  private float[] numDetections;\r\n\r\n  private ByteBuffer imgData;\r\n\r\n  private Interpreter tflite;\r\n\r\n  /** Options for configuring the Interpreter. */\r\n  private final Interpreter.Options tfliteOptions = new Interpreter.Options();\r\n\r\n  /** The loaded TensorFlow Lite model. */\r\n  private MappedByteBuffer tfliteModel;\r\n\r\n\r\n  private TFLiteObjectDetectionAPIModel() {}\r\n\r\n  /** holds a gpu delegate */\r\n  GpuDelegate gpuDelegate = null;\r\n\r\n  /** Memory-map the model file in Assets. */\r\n  private static MappedByteBuffer loadModelFile(AssetManager assets, String modelFilename)\r\n      throws IOException {\r\n    AssetFileDescriptor fileDescriptor = assets.openFd(modelFilename);\r\n    FileInputStream inputStream = new FileInputStream(fileDescriptor.getFileDescriptor());\r\n    FileChannel fileChannel = inputStream.getChannel();\r\n    long startOffset = fileDescriptor.getStartOffset();\r\n    long declaredLength = fileDescriptor.getDeclaredLength();\r\n\r\n    return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);\r\n  }\r\n\r\n\r\n  private void recreateInterpreter() {\r\n    if (tflite != null) {\r\n      tflite.close();\r\n      tflite = new Interpreter(tfliteModel, tfliteOptions);\r\n    }\r\n  }\r\n\r\n  public void useGpu() {\r\n    if (gpuDelegate == null) {\r\n      gpuDelegate = new GpuDelegate();\r\n      tfliteOptions.addDelegate(gpuDelegate);\r\n      recreateInterpreter();\r\n    }\r\n  }\r\n\r\n  /**\r\n   * Initializes a native TensorFlow session for classifying images.\r\n   *\r\n   * @param assetManager The asset manager to be used to load assets.\r\n   * @param modelFilename The filepath of the model GraphDef protocol buffer.\r\n   * @param labelFilename The filepath of label file for classes.\r\n   * @param inputSize The size of image input\r\n   * @param isQuantized Boolean representing model is quantized or not\r\n   */\r\n  public static Classifier create(\r\n      final AssetManager assetManager,\r\n      final String modelFilename,\r\n      final String labelFilename,\r\n      final int inputSize,\r\n      final boolean isQuantized)\r\n      throws IOException {\r\n    final TFLiteObjectDetectionAPIModel d = new TFLiteObjectDetectionAPIModel();\r\n    //d.setUseNNAPI(true);\r\n    d.useGpu();\r\n    InputStream labelsInput = null;\r\n    String actualFilename = labelFilename.split(\"file:///android_asset/\")[1];\r\n    labelsInput = assetManager.open(actualFilename);\r\n    BufferedReader br = null;\r\n    br = new BufferedReader(new InputStreamReader(labelsInput));\r\n    String line;\r\n    while ((line = br.readLine()) != null) {\r\n      LOGGER.w(line);\r\n      d.labels.add(line);\r\n    }\r\n    br.close();\r\n\r\n    d.inputSize = inputSize;\r\n\r\n    try {\r\n      d.tflite = new Interpreter(loadModelFile(assetManager, modelFilename));\r\n    } catch (Exception e) {\r\n      throw new RuntimeException(e);\r\n    }\r\n\r\n    d.isModelQuantized = isQuantized;\r\n    // Pre-allocate buffers.\r\n    int numBytesPerChannel;\r\n    if (isQuantized) {\r\n      numBytesPerChannel = 1; // Quantized\r\n    } else {\r\n      numBytesPerChannel = 4; // Floating point\r\n    }\r\n    d.imgData = ByteBuffer.allocateDirect(1 * d.inputSize * d.inputSize * 3 * numBytesPerChannel);\r\n    d.imgData.order(ByteOrder.nativeOrder());\r\n    d.intValues = new int[d.inputSize * d.inputSize];\r\n\r\n    d.tflite.setNumThreads(NUM_THREADS);\r\n    d.outputLocations = new float[1][NUM_DETECTIONS][4];\r\n    d.outputClasses = new float[1][NUM_DETECTIONS];\r\n    d.outputScores = new float[1][NUM_DETECTIONS];\r\n    d.numDetections = new float[1];\r\n    return d;\r\n  }\r\n\r\n  @Override\r\n  public List<Recognition> recognizeImage(final Bitmap bitmap) {\r\n    // Log this method so that it can be analyzed with systrace.\r\n    Trace.beginSection(\"recognizeImage\");\r\n\r\n    Trace.beginSection(\"preprocessBitmap\");\r\n    // Preprocess the image data from 0-255 int to normalized float based\r\n    // on the provided parameters.\r\n    bitmap.getPixels(intValues, 0, bitmap.getWidth(), 0, 0, bitmap.getWidth(), bitmap.getHeight());\r\n  //  useGpu();\r\n    imgData.rewind();\r\n    for (int i = 0; i < inputSize; ++i) {\r\n      for (int j = 0; j < inputSize; ++j) {\r\n        int pixelValue = intValues[i * inputSize + j];\r\n        if (isModelQuantized) {\r\n          // Quantized model\r\n          imgData.put((byte) ((pixelValue >> 16) & 0xFF));\r\n          imgData.put((byte) ((pixelValue >> 8) & 0xFF));\r\n          imgData.put((byte) (pixelValue & 0xFF));\r\n        } else { // Float model\r\n          imgData.putFloat((((pixelValue >> 16) & 0xFF) - IMAGE_MEAN) / IMAGE_STD);\r\n          imgData.putFloat((((pixelValue >> 8) & 0xFF) - IMAGE_MEAN) / IMAGE_STD);\r\n          imgData.putFloat(((pixelValue & 0xFF) - IMAGE_MEAN) / IMAGE_STD);\r\n        }\r\n      }\r\n    }\r\n    Trace.endSection(); // preprocessBitmap\r\n\r\n    // Copy the input data into TensorFlow.\r\n    Trace.beginSection(\"feed\");\r\n    outputLocations = new float[1][NUM_DETECTIONS][4];\r\n    outputClasses = new float[1][NUM_DETECTIONS];\r\n    outputScores = new float[1][NUM_DETECTIONS];\r\n    numDetections = new float[1];\r\n\r\n    Object[] inputArray = {imgData};\r\n    Map<Integer, Object> outputMap = new HashMap<>();\r\n    outputMap.put(0, outputLocations);\r\n    outputMap.put(1, outputClasses);\r\n    outputMap.put(2, outputScores);\r\n    outputMap.put(3, numDetections);\r\n    Trace.endSection();\r\n\r\n    // Run the inference call.\r\n    Trace.beginSection(\"run\");\r\n    tflite.runForMultipleInputsOutputs(inputArray, outputMap);\r\n    Trace.endSection();\r\n\r\n    // Show the best detections.\r\n    // after scaling them back to the input size.\r\n    final ArrayList<Recognition> recognitions = new ArrayList<>(NUM_DETECTIONS);\r\n    for (int i = 0; i < NUM_DETECTIONS; ++i) {\r\n      final RectF detection =\r\n          new RectF(\r\n              outputLocations[0][i][1] * inputSize,\r\n              outputLocations[0][i][0] * inputSize,\r\n              outputLocations[0][i][3] * inputSize,\r\n              outputLocations[0][i][2] * inputSize);\r\n      // SSD Mobilenet V1 Model assumes class 0 is background class\r\n      // in label file and class labels start from 1 to number_of_classes+1,\r\n      // while outputClasses correspond to class index from 0 to number_of_classes\r\n      int labelOffset = 1;\r\n      recognitions.add(\r\n          new Recognition(\r\n              \"\" + i,\r\n              labels.get((int) outputClasses[0][i] + labelOffset),\r\n              outputScores[0][i],\r\n              detection));\r\n    }\r\n    Trace.endSection(); // \"recognizeImage\"\r\n   // runInference();\r\n    return recognitions;\r\n  }`\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Please provide help or suggestion to resolve this issue**\r\n\r\n", "comments": ["@Gmrevo \r\n\r\nI'm not clear on what the problem is.  If you get:\r\n\r\n> CPU.ModifyGraphWithDelegate is disallowed when graph is immutable.\r\n\r\nyou shouldn't be able to continue.  However, you seem to be able to progress to the next problem of:\r\n\r\n> java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: Next operations are not supported by GPU delegate:\r\n> CUSTOM TFLite_Detection_PostProcess: Operation is not supported.\r\n> First 63 operations will run on the GPU, and the remaining 1 on the CPU.TfLiteGpuDelegate Invoke: > Delegate should run on the same thread where it was initialized.Node number 64 (TfLiteGpuDelegate) failed to invoke.\r\n\r\nHaving a custom op should be fine; it should be taken over by CPU unless something changed recently.", "Was this issue resolved?\r\nThanks", "I'm was getting the same problem - \"java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: ModifyGraphWithDelegate is disallowed when graph is immutable.\"\r\n\r\nContext: we're creating a tflite depth estimation sample and below is the code for initializing the interpreter with the best options.\r\n\r\n```kotlin\r\n init {\r\n        // Initialize TFLite Interpreter\r\n        val interpreterOptions = Interpreter.Options().apply {\r\n            // Add the GPU Delegate if supported.\r\n            // See -> https://www.tensorflow.org/lite/performance/gpu#android\r\n            if ( CompatibilityList().isDelegateSupportedOnThisDevice ) {\r\n                Logger.logInfo( \"GPU Delegate is supported on this device.\" )\r\n                addDelegate( GpuDelegate( CompatibilityList().bestOptionsForThisDevice ))\r\n            }\r\n            else {\r\n                // Number of threads for computation\r\n                setNumThreads( NUM_THREADS )\r\n            }\r\n            // Add the NNApiDelegate if supported.\r\n            // See -> https://www.tensorflow.org/lite/performance/nnapi#initializing_the_nnapi_delegate\r\n            if ( Build.VERSION.SDK_INT >= Build.VERSION_CODES.P ) {\r\n                Logger.logInfo( \"NNAPI is supported on this device.\" )\r\n                addDelegate( NnApiDelegate() )\r\n            }\r\n        }\r\n        interpreter = Interpreter(FileUtil.loadMappedFile( context, modelFileName ) , interpreterOptions )\r\n        Logger.logInfo( \"TFLite interpreter created.\" )\r\n    }\r\n```\r\n\r\nOur mistake was to add both NNAPI and GPU check in the code, and calling ```addDelegate()``` both the times, you should only use one kind of delegate. look if you're doing the same!\r\nI suggest using GPU delegate if you're sharing your code between IOS and Android, as it can be used on both Android and iOS, and using NNAPI and Hexagon if your writing code only for android.\r\n\r\nFor more details go through this: https://www.tensorflow.org/lite/performance/delegates#delegates_by_platform"]}, {"number": 30730, "title": "Unable to find documentation for framework/ common run time files", "body": "Hello TF team,\r\n\r\nI've been combing the documentation for any reference to OpKernels, Graphs defs, the executor, rendezvous, ect... the important abstractions that seem to be near the core of tensorflow. So far I haven't been able to find any kind of high level overview of how they all come together. I'm working on a feature that involves adding a new device and I don't want to miss any important abstractions in my implementation.\r\n\r\nDoes anyone know where I could find documentation that is concerned with how the tensorflow framework / run time work?\r\nThanks,\r\nScott\r\n", "comments": ["@sthomdev please confirm if you are still facing any issue", "We have this on our todo list: better documentation for tensorflow internals."]}, {"number": 30715, "title": "Low GPU usage of RNN layers under MirroredStrategy", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Ubuntu 14.04\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: `2.0.0b1`\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: 10.0 / \r\n- GPU model and memory: TITAN X\r\n\r\n**Describe the current behavior**\r\n\r\nRNN layers have poor performance and low GPU usage when used with `MirroredStrategy`. By monitoring `nvidia-smi`, part of the execution seems to run sequentially on each GPU.\r\n\r\n**Describe the expected behavior**\r\n\r\nWith `MirroredStrategy`, the model is expected to be run in parallel.\r\n\r\n**Code to reproduce the issue**\r\n\r\nConsider the dummy training code below. It generates examples with random shapes and apply a stack of `LSTMCell` on batches of sequences on 3 GPUs. If you replace the RNN layer by e.g. a stack of Dense layers, the parallelism is visibly improved.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nclass MyModel(tf.keras.layers.Layer):\r\n\r\n    def __init__(self):\r\n        super(MyModel, self).__init__()\r\n        cell = tf.keras.layers.StackedRNNCells(\r\n            [tf.keras.layers.LSTMCell(512) for _ in range(12)])\r\n        self.rnn = tf.keras.layers.RNN(cell)\r\n\r\n    def call(self, inputs):\r\n        return self.rnn(inputs)\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices(\r\n    tf.random.uniform([10000], minval=1, maxval=80, dtype=tf.int32))\r\ndataset = dataset.shuffle(10000)\r\ndataset = dataset.map(lambda t: tf.zeros([t, 512]))\r\ndataset = dataset.padded_batch(\r\n    64, padded_shapes=tf.compat.v1.data.get_output_shapes(dataset))\r\ndataset = dataset.repeat()\r\ndataset = dataset.prefetch(1)\r\n\r\ndevices = [\"/gpu:0\", \"/gpu:1\", \"/gpu:2\"]\r\nstrategy = tf.distribute.MirroredStrategy(devices=devices)\r\n\r\nwith strategy.scope():\r\n    dataset = strategy.experimental_distribute_dataset(dataset)\r\n    model = MyModel()\r\n    optimizer = tf.keras.optimizers.Adam()\r\n\r\ndef step(inputs):\r\n    outputs = model(inputs)\r\n    loss = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM)(\r\n        tf.zeros_like(outputs), outputs)\r\n    variables = model.trainable_variables\r\n    gradients = optimizer.get_gradients(loss, variables)\r\n    optimizer.apply_gradients(zip(gradients, variables))\r\n    return loss\r\n\r\n@tf.function\r\ndef train():\r\n    with strategy.scope():\r\n        for inputs in dataset:\r\n            loss = strategy.experimental_run_v2(step, args=(inputs,))\r\n\r\ntrain()\r\n```\r\n\r\ncc @jkamalu.", "comments": ["I am facing the same problem using LSTM layers in a custom subclassed Keras model. ", "Could you try with latest nightly to see whether the problem is still there?", "I tested `tf-nightly-gpu-2.0-preview==2.0.0.dev20190910` and it's not that much better. With the same code as above running on 3 GPUs, the first one is utilized at 100% but the 2 others are below 30% most of the time.", "@guillaumekln It looks like you are using an older Version of Tensorflow. Many bugs have been fixed in the latest version. Could you please execute your code using Latest Version 2.4.1 or 2.5.0 and let us know if the issue still persists? Thanks!", "I ran the code again with TensorFlow 2.5.0 (the code should be slightly changed: replace `experimental_run_v2` by `run`). I'm still seeing the same issue: the usage of each GPU is very low at around 20%."]}, {"number": 30658, "title": "TPUEstimator function requires `train_batch_size` to be set when `use_tpu` is True", "body": "The [TPUEstimator constructor](https://github.com/JayMody/estimator/blob/e794e634ba52f9e8b04971e74fe24b91857f2228/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py#L2590) requires `train_batch_size` to be set if `use_tpu` is True. \r\n\r\nIn cases when I only want to use a TPU estimator to predict, that means I have to pass in an arbitrary value in for `train_batch_size`. Looking deeper into the code, I can't pinpoint why `train_batch_size` needs to be set when on a TPU, but I'm assuming it's required somewhere deeper in the code. \r\n\r\nIt was very confusing for me, especially since the documentation is conflicting [pull request #37](https://github.com/tensorflow/estimator/pull/37#issue-297115525). \r\n\r\nMaybe an option should be added that if `train_batch_size` is not set, but one of the other two (eval and predict) are, then provide a warning and pass an arbitrary value for `train_batch_size`. Otherwise maybe be more clear with the documentation that `train_batch_size` must always be set when on a TPU, even if you are only predicting or evaluating.", "comments": []}, {"number": 30468, "title": "tensorflow 2.0 variable slice assign_add not supported", "body": "Could anybody tell me in tf 2.0 rc1, why is variable slice assignment (varaible[...].assign function) works well but the assign_add is not supported?\r\ne.g. :\r\n\r\n`var = tf.Variable(tf.ones([2,3,3]))`\r\n`var[:, :, 1].assign(tf.zeros([2,3]))  # this will work`\r\n\r\n`var[:, :, 1].assign_add(tf.zeros([2,3]))  # this will NOT work`\r\n`AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'assign_add'`\r\n", "comments": ["Reproduced this issue with TF Version 2.0.beta.", "@jaingaurav Any update on this issue?", "slice assign_add still not supported in tf 2.0 official release", "I haven't had a chance to make much progress on this issue, but it requires doing similar tricks we do here for `assign`: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_ops.py#L1091. We'll likely have to add other operators beyond just `assign_add`.", "bump", "@ziofil: This is definitely something we want to address as it is a usability issue. We're still trying to allocate time to work on this. In the meantime, contributions are welcome.", "Was able to reproduce the issue with [TF v2.1](https://colab.sandbox.google.com/gist/gadagashwini/c82696421987c70062a38cf2c5d95866/untitled452.ipynb), [TF v2.2.0-rc3](https://colab.research.google.com/gist/amahendrakar/6b30ac0789c5de7eabee8e294c5aeed4/30468-2-2.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/5111609f9c40dbbc51215f02d4e684c2/30468-nightly.ipynb). Please find the attached gist. Thanks!", "what I did is replacing this:\r\n`total_recall = tf.assign_add(total_recall, current_recall)`\r\nwith\r\n`total_recall = tf.compat.v1.assign_add(total_recall, current_recall)`\r\n", "Was able to replicate the issue with TF v2.5,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/110ab4428e1b60348012b20e27b630a6/30468-nightly.ipynb) ..Thanks!", "I could reproduce the issue with TF 2.6 . Please, find the gist [**`here`**](https://colab.research.google.com/gist/kumariko/da64ddd5e26c9b9f6a391e1b275aadc0/30468-nightly.ipynb#scrollTo=JjSEl8bUsizl).Thanks!"]}, {"number": 30293, "title": "[Question] Adding new hardware device support", "body": "Hello, \r\nI'm developing a tensorflow modification to support a new hardware device and I was curious if there had been any update to the procedure since this issue was first raised in: \r\nFeature Request: plug-in support for new devices #4359\r\n\r\nI'm following along with the documentation provided here\r\nhttps://github.com/knuedge/tensorflow/blob/36e0cdf04f294bfd51931d4f78e291590ed0d3ec/tensorflow/g3doc/hardware/adding_support/index.md\r\n\r\nBut it is quite out of date so before I plow ahead I'd like to know that this isn't a solved problem.\r\n\r\nThanks,\r\nScott\r\n", "comments": ["Hey Scott,\r\n\r\nThis issue slipped through my filter.\r\n\r\nWe are working on a new proposal & prototype to get some larger community feedback.  But its not there yet. Could you perhaps tell me more about the target?\r\n\r\nThanks,\r\n\r\nJacques", "Hi Jpienaar\r\nIs there any reference document or discussion about the new proposal?\r\nWe are trying to make our platform works on tensorflow and it involves quite some code change and quite a lot of them involves framework change which is kinda ugly.\r\nAlso any schedule about the proposal and prototype?\r\n\r\nThanks\r\nKevin", "Unfortunately the design/prototype did not get the buy in from other folks and is being redesigned post more exploration work. It may be 3-6 months before the next one. The closest externally visible is the changes we are making on the new TF & XLA integration using MLIR. This is still early though. In the interim the two options are the existing ones of creating a new TF device or a new XLA device.\r\n\r\nBest,\r\n\r\nJacques", "Is it still too early?", "Following this as well", "Any update?", "Any update?", "I'm not speaking for tensorflow community and this is just for your information\r\nRecently tensorflow 2.5 released device plugin\r\nhttps://blog.tensorflow.org/2021/06/pluggabledevice-device-plugins-for-TensorFlow.html\r\n\r\nIMHO, the device plugin along with MLIR/TFRT is the ultimate solution for 3rd party device vendors to hook their chips into TF.", "> I'm not speaking for tensorflow community and this is just for your information\r\n> Recently tensorflow 2.5 released device plugin\r\n> https://blog.tensorflow.org/2021/06/pluggabledevice-device-plugins-for-TensorFlow.html\r\n> \r\n> IMHO, the device plugin along with MLIR/TFRT is the ultimate solution for 3rd party device vendors to hook their chips into TF.\r\n\r\nThis looks awesome, thanks!"]}, {"number": 30276, "title": "Need better documentation for BestExporter", "body": "In the documentation for [BestExporter](https://www.tensorflow.org/api_docs/python/tf/estimator/BestExporter), the example mentioned does not specify how to write a _compare_fn_ .\r\nBy default, it takes the _loss_.\r\nHow to use it if we were to use custom metrics, that are evaluated in the _model_fn_ .", "comments": []}, {"number": 30251, "title": "tf.keras.experimental.export_saved_model in multi-gpu mode doesn't work", "body": "I use TF 2.0. Currently, `tf.keras.experimental.export_saved_model` doesn't work in multi-gpu mode, i.e when model is declarated in `strategy.scope()`. I got the following error:\r\n\r\n```\r\nFile .../tensorflow/python/ops/variables.py\", line 514, in synchronization\r\n    raise NotImplementedError\r\n``` \r\n\r\nI tried the next code:\r\n\r\n```\r\ntf.keras.experimental.export_saved_model(\r\n    model, file_path,\r\n    serving_only=True,\r\n    input_signature=[tf.TensorSpec(shape=[None, None, None, 3], dtype=tf.float32)]\r\n)\r\n```", "comments": ["There is a new standard api to use keras.model.save() to export TF savedmodel. That worked for my case. Could you check if that api is in this release? If so, try it out. The export_saved_model api should be replaced by that.", "@saberkun you mean `tf.keras.models.save_model`? Please, specify method that you mentioned more precisely.\r\n\r\nAlso, I'd like to say that I need `serving_only` and `input_signature`.", "Just `model.save()` is not enough in my case, because I need to specify `serving_only` and `input_signature`", "@seemuch and @k-w-w \r\nThis is also the confusion I had. Can keras team clarify with some HowTo for the serving only case?\r\nIf my model has dropout, what could be expected?\r\n\r\n@Oktai15 \r\nOne workaround should work is: (1) save model with model.save_weights() (2) create a model without ds scope and load_weights(). Then, export_saved_model. (Like the version before this commit: https://github.com/tensorflow/models/commit/a1c47f2845d186e2bc65d3369525e5f7436519c2#diff-e547c86e1d6f441b9e652c8905ae8d62)", "@saberkun I use `tf.keras.experimental.export_saved_model`, because it's the only one function in which I can specify `input_shape` for my model that I defined due to Imperative API (subclassed tf.keras.Model). It would be very sad, if you removed it without alternative.\r\n\r\nYour workaround works, thank you.", "I believe this issue has been fixed in `model.save` by @seemuch (@seemuch do you have the link to the commit with the fix?). "]}, {"number": 30246, "title": "The shared library of C++ API lacks of operation symbols on windows.", "body": "\r\n**System information**\r\n- Windows 10 1809:\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 1.14.0\r\n- Python version: 3.7.3\r\n- Bazel version (if compiling from source):  0.25.2\r\n- GCC/Compiler version (if compiling from source): visual studio 2017\r\n\r\n**Describe the problem**\r\nI tried to build the [label image c++ demo](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/main.cc) and I got lots of linking error which cannot find the tf operation symbols. Such as:\r\nSeverity\tCode\tDescription\tProject\tFile\tLine\tSuppression State\r\nError\tLNK2001\tunresolved external symbol \"public: __cdecl tensorflow::ops::Subtract::Subtract(class tensorflow::Scope const &,class tensorflow::Input,class tensorflow::Input)\" (??0Subtract@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@1@Z)\ttf1.14.0\tD:\\ProgrammingAndStudy\\C++_proj\\tf1.14.0\\tf1.14.0\\main.obj\t1\t\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI build the tensorflow 1.14.0 C++ API through the MSYS2 shell after some configeration. \r\nAlthough I configured using vs2019 here, but it seems useless. Bazel still choosing the vs2017.\r\nexport MSYS_NO_PATHCONV=1\r\nexport MSYS2_ARG_CONV_EXCL=\"*\" \r\nexport PATH=\"/C/Users/yueji/AppData/Local/Programs/Python/Python37\":$PATH\r\nalias python='winpty python.exe'\r\nexport PATH=\"/C/machine learning dlls\":$PATH\r\nset BAZEL_VS=\"C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\"\r\nset BAZEL_VC=\"C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\"\r\n\r\nAnd then, I compiled the tf1.14.0 through\r\nbazel build --config=opt //tensorflow:tensorflow_cc\r\nbazel build --config=opt //tensorflow:tensorflow_framework\r\n\r\nFinally, I got two libs and two dlls. They are located in C:\\Users\\user_name\\_bazel_yueji\\dvw6l3y3\\execroot\\org_tensorflow\\bazel-out\\x64_windows-opt\\bin\\tensorflow\r\ntensorflow.dll.if.lib, tensorflow_framework.dll.if.lib\r\ntensorflow_cc.dll, tensorflow_framework.dll\r\n\r\nI configured my vs projects to make sure the headers are properly included, and linked these two libs. \r\nHowever there are lots of lnk2001 errors are reported.\r\n\r\n**Any other info / logs**\r\nIt's a part of error file.\r\n\r\nSeverity\tCode\tDescription\tProject\tFile\tLine\tSuppression State\r\nError\tLNK2001\tunresolved external symbol \"public: __cdecl tensorflow::ops::Subtract::Subtract(class tensorflow::Scope const &,class tensorflow::Input,class tensorflow::Input)\" (??0Subtract@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@1@Z)\ttf1.14.0\tD:\\ProgrammingAndStudy\\C++_proj\\tf1.14.0\\tf1.14.0\\main.obj\t1\t\r\nError\tLNK2001\tunresolved external symbol \"public: __cdecl tensorflow::ops::Squeeze::Squeeze(class tensorflow::Scope const &,class tensorflow::Input)\" (??0Squeeze@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@@Z)\ttf1.14.0\tD:\\ProgrammingAndStudy\\C++_proj\\tf1.14.0\\tf1.14.0\\main.obj\t1\t\r\nError\tLNK2001\tunresolved external symbol \"public: __cdecl tensorflow::ops::DecodeGif::DecodeGif(class tensorflow::Scope const &,class tensorflow::Input)\" (??0DecodeGif@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@@Z)\ttf1.14.0\tD:\\ProgrammingAndStudy\\C++_proj\\tf1.14.0\\tf1.14.0\\main.obj\t1\t\r\nError\tLNK2001\tunresolved external symbol \"public: __cdecl tensorflow::ops::TopK::TopK(class tensorflow::Scope const &,class tensorflow::Input,class tensorflow::Input)\" (??0TopK@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@1@Z)\ttf1.14.0\tD:\\ProgrammingAndStudy\\C++_proj\\tf1.14.0\\tf1.14.0\\main.obj\t1\t\r\n\r\n", "comments": ["I tried tu use dumpbin tools in msvc commandline tools in powershell:\r\ndumpbin.exe /headers .\\tensorflow_cc.dll.if.lib | Select-String \"ResizeBilinear\"\r\ndumpbin.exe /headers .\\tensorflow_cc.dll.if.lib | Select-String \"Squeeze\"\r\ndumpbin.exe /headers .\\tensorflow_cc.dll.if.lib | Select-String \"DecodeGif\"\r\ndumpbin.exe /headers .\\tensorflow_cc.dll.if.lib | Select-String \"ExpandDims\"\r\nUnfortunately, nothing was returned.\r\n\r\nI also tried to build the library directly: bazel build --config=opt //tensorflow:tensorflow_cc.lib\r\nThe result is the same.", "@EternalSaga looks like similar issue [#24885](https://github.com/tensorflow/tensorflow/issues/24885). Please let us know if that helps you. Thanks! ", "@gadagashwini  Thank you for your advice. But it seems useless for me. I cannot find any solutions about my problem.", "please try to see if  https://github.com/guikarist/tensorflow-windows-build-script.git could help.", "Same issue here. I tried tf_exported_symbol_msvc.ids method and def_fp.write method too. but both failed to solve my case.  Current my builting tensorflow_cc.dll has only c-api, there are no c++ APIs.", "I hope Bazel developmemt team solve this issue by comparing cmake, it has worked well in making c++ api dll for windows.", "So the reason you don't have all the ops symbols is because the generated source code of ops do not include the `__declspec(export)` descriptor for classes like `DecodeGif` on Windows. Check `bazel-bin/tensorflow/cc/ops/image_ops.h`\r\n\r\nBoth CMake and Bazel can parse objects to get global symbols and use it to export symbols in DLL, we use this method to export symbols for tensorflow_cc.dll. <del>But unfortunately, that doesn't work for symbols in C++ Class.</del>\r\n\r\nI tried to solve this by adding `clean_dep(\"//tensorflow/core:ops\"),` at \r\nhttps://github.com/tensorflow/tensorflow/blob/ba5dddcf708e1762b924e4cd7ef07c54b34708f5/tensorflow/tensorflow.bzl#L1812\r\n<del>But it didn't work because the missing symbols are class methods.</del>\r\n<del>\r\nI really don't have any idea to solve this for the shared library of C++ API except adding the `__declspec(export)` descriptor  while generating the source code of ops.</del>", "Wait, I think I should have added `clean_dep(\"//tensorflow/cc:cc_ops\")` instead of `clean_dep(\"//tensorflow/core:ops\")`. Let me try again", "I think I have found a solution.\r\n\r\n@EternalSaga @jeffhwang02 \r\nCan you cherry-pick https://github.com/meteorcloudy/tensorflow/commit/4b8e7bef11040c465453e973eb0c6ad43cfd3c96 and rebuild the `//tensorflow:tensorflow_cc.dll` and `//tensorflow:tensorflow_cc.lib`.\r\n\r\nWith the change in the commit, I can see the symbols in the import library:\r\n```\r\ndumpbin.exe  /headers ./bazel-bin/tensorflow/tensorflow_cc.lib | grep \"ExpandDims\"\r\n  Symbol name  : ??0ExpandDims@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@1@Z (public: __cdecl tensorflow::ops::ExpandDims::ExpandDims(class tensorflow::Scope const &,class tensorflow::Input,class tensorflow::Input))\r\n  Name         : ??0ExpandDims@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@1@Z\r\n```", "Ping @EternalSaga @jeffhwang02\r\nCan you confirm if my change fixed the symbol issue on Windows?\r\nIf so we can merge that change so that more users can benefit from it.", "> I think I have found a solution.\r\n> \r\n> @EternalSaga @jeffhwang02\r\n> Can you cherry-pick [meteorcloudy@4b8e7be](https://github.com/meteorcloudy/tensorflow/commit/4b8e7bef11040c465453e973eb0c6ad43cfd3c96) and rebuild the `//tensorflow:tensorflow_cc.dll` and `//tensorflow:tensorflow_cc.lib`.\r\n> \r\n> With the change in the commit, I can see the symbols in the import library:\r\n> \r\n> ```\r\n> dumpbin.exe  /headers ./bazel-bin/tensorflow/tensorflow_cc.lib | grep \"ExpandDims\"\r\n>   Symbol name  : ??0ExpandDims@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@1@Z (public: __cdecl tensorflow::ops::ExpandDims::ExpandDims(class tensorflow::Scope const &,class tensorflow::Input,class tensorflow::Input))\r\n>   Name         : ??0ExpandDims@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@1@Z\r\n> ```\r\n\r\nDear meteorcloudy.\r\nI tried your solution and could get below error\r\n/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////// \r\nERROR: D:/tf/tf114/tensorflow/tensorflow/BUILD:721:11: in cmd attribute of genrule rule //tensorflow:tensorflow_filtered_def_file: label '//tensorflow:cc_ops_def_file' in $(locations) expression expands to no files\r\n///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////", "@jeffhwang02 Do you have a github branch of your source code?", "> @jeffhwang02 Do you have a github branch of your source code?\r\n\r\n@meteorcloudy \r\nYes I did it in the same path which I built the tensorflow_cc.dll file well.", "@jeffhwang02 \r\nOh I mean, can you show me the actual code so that I can see what went wrong?\r\nAlso which version of Bazel were you using?", "I would just like to say great work @meteorcloudy \r\n\r\nI could not get my application to link because \r\n\r\n\"ExpandDims::ExpandsDims was an unresolved external symbol\"\r\n\r\nI had already used TF_EXPORT on a bunch of other symbols but could not do same thing here ExpandDims::ExpandDims is part of an auto generated file.\r\n\r\nI used the changes to BUILD file in tensorflow sub folder (not the top level one, the one with folders \"c\" \"cc\" \"compiler\"). from last comment 6th JUly 2020.\r\n\r\nWorked straight out of the box.\r\n\r\nTensorFlow 2.3.0. Bazel 3.1.0 VS2019. VS2017 does not work.\r\n\r\nToo many issues to mention trying to get TEnsorFlow to build.\r\n\r\nBut information in this thread is very useful."]}, {"number": 30087, "title": "Multiple duplicate tflite android example projects", "body": "### Description:\r\nThere are two examples for \"tflite image classification on android\" on the Tensorflow repo which achieve the same thing. They both have almost identical code but slight differences(In UI and in code).\r\n\r\n**Example A:** https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android\r\n**Example B:** https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java\r\n\r\nI would like to know which one is supposed to be used, and whether they can be merged so that this confusion is avoided?\r\n\r\n### Example of a difference:\r\n\r\n- In example A we do **not** create a NNAPI Delegate\r\n\r\nhttps://github.com/tensorflow/examples/blob/156a12782a06f085adeb6b352c2763648cece066/lite/examples/image_classification/android/app/src/main/java/org/tensorflow/lite/examples/classification/tflite/Classifier.java#L179\r\n\r\n- But in example B we do create an NNAPI Delegate\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/e8ad5b0552a03e5d065a8f302dd0c0e0ae6b3925/tensorflow/lite/java/demo/app/src/main/java/com/example/android/tflitecamerademo/ImageClassifier.java#L186", "comments": ["Thanks for flagging this, please refer to Example A, we'll likely be removing Example B from the lite repo soon.\r\n\r\nIn practice, using `setUseNNAPI(true)` is equivalent to adding the `NnApiDelegate`.", "@mukulhase \r\nIs this still an issue.", "@miaout17 we should probably remove the OSS version of the image classifier in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo and point users to the examples repo. Please re-assign as appropriate.", "@mukulhase ! Beside NNAPI implementation, Build methods are also different on both examples ( One is gradle.build and another is bazel build). I can see a depreciation warning now on android example [here ](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/android/app)though. will that address this issue? \r\n ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 30069, "title": "Support Sparse Tensors in py_function", "body": "- TensorFlow version (you are using): 2.0.0b0\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently, sparse tensors don't seem to be supported as inputs to `py_function`. Attempting to pass one results in an error like \"`TypeError: Tensors in list passed to 'input' of 'EagerPyFunc' Op have types [<NOT CONVERTIBLE TO TENSOR>] that are invalid.`\". Example:\r\n\r\n    def dataset_map_sparse_test():\r\n        input_data_sparse = tf.SparseTensor(indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4])\r\n        input_data_dense = tf.sparse.to_dense(input_data_sparse)\r\n    \r\n        ds_dense = tf.data.Dataset.from_tensor_slices(input_data_dense)\r\n        ds_sparse = tf.data.Dataset.from_tensor_slices(input_data_sparse)\r\n\r\n        def inner_fn(*input):\r\n            return input\r\n\r\n        def outer_fn(*input_data):\r\n            return tf.py_function(\r\n                inner_fn,\r\n                input_data,\r\n                (tf.int32,))\r\n\r\n        # this works\r\n        ds_dense_mapped = ds_dense.map(outer_fn)\r\n        print(next(iter(ds_dense_mapped)))\r\n\r\n        # this results in the error specified above\r\n        ds_sparse_mapped = ds_sparse.map(outer_fn)\r\n        print(next(iter(ds_sparse_mapped)))\r\n\r\n**Will this change the current api? How?**\r\nYes. The `inp` parameter of `py_function` currently accepts a list of `Tensor` objects; this change would broaden it to accept either `Tensor`s or `SparseTensor`s. (Possibly `_TensorLike`s?)\r\n\r\n**Who will benefit with this feature?**\r\nAnyone wishing to pass sparse tensors to a `py_function`-wrapped function. There are situations where this wrapping is necessary; for example, functions passed to `Dataset.map` cannot perform certain operations unless the function is wrapped using `py_function`.\r\n\r\n**Any Other info.**\r\n", "comments": ["I think now that we have compositetensor and typespec it should be straightforward to implement sparsetensor support in py_function by treating all composites generically.\r\n\r\nI don't have the bandwidth to do this now, so leaving as contributions welcome.", "@alextp I would like to work on this, can I please go ahead?", "Would \"all composites\" include ragged tensors, or should I create a separate feature request for that? Thanks!", "Composites include ragged tensors and a lot of other things as well.", "@pradyumna1 please go ahead!", "I notice that someone had already created issue #26453 (Allow py_function to support functions that return RaggedTensor). Implementing this feature using composite tensors would also satisfy that issue.", "I made changes in file `op_def_library.py` so that CompositeTensor is now allowed. However, there are some missing features of `CompositeTensor` that does not allow adding the node to the computation graph, for example `._as_tf_output()`, which is used in function `_create_c_op`. That function only appears in class `Tensor`. Could you advise on this @alextp?", "@minhtriet, in a similar feature request (#27679), @edloper provided a helpful workaround in the form of a wrapper around `py_function`, which he suggested could be integrated into `_internal_py_func`. Does that help?", "Thank you @novog, I could not read work on it right now, would get back to you in a couple of days", "So, I updated `tf.py_function` in [my fork](https://github.com/minhtriet/tensorflow/blob/master/tensorflow/python/ops/script_ops.py#L373)  \r\n\r\nRerunning the code, the error is\r\n```\r\n2019-12-14 21:38:52.367270: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 4. Tune using inter_op_parallelism_threads for best performance.\r\n(<tf.Tensor: id=20, shape=(4,), dtype=int32, numpy=array([1, 0, 0, 0])>,)\r\n2019-12-14 21:38:53.274875: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at iterator_ops.cc:929 : Invalid argument: {{function_node __inference_Dataset_map_outer_fn_28}} pyfunc_1 returns 3 values, but expects to see 1 values.\r\n\t [[{{node PyFuncStateless}}]]\r\nTraceback (most recent call last):\r\n  File \"C:/Users/cool/Code/tf_contrib_conda/test.py\", line 28, in <module>\r\n    dataset_map_sparse_test()\r\n  File \"C:/Users/cool/Code/tf_contrib_conda/test.py\", line 26, in dataset_map_sparse_test\r\n    print(next(iter(ds_sparse_mapped)))\r\n  File \"C:\\Users\\cool\\Miniconda3\\envs\\tf_contrib_conda\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py\", line 622, in __next__\r\n    return self.next()\r\n  File \"C:\\Users\\cool\\Miniconda3\\envs\\tf_contrib_conda\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py\", line 666, in next\r\n    return self._next_internal()\r\n  File \"C:\\Users\\cool\\Miniconda3\\envs\\tf_contrib_conda\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py\", line 651, in _next_internal\r\n    output_shapes=self._flat_output_shapes)\r\n  File \"C:\\Users\\cool\\Miniconda3\\envs\\tf_contrib_conda\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_dataset_ops.py\", line 2672, in iterator_get_next_sync\r\n    _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __inference_Dataset_map_outer_fn_28}} pyfunc_1 returns 3 values, but expects to see 1 values.\r\n\t [[{{node PyFuncStateless}}]] [Op:IteratorGetNextSync]\r\n```\r\n\r\nMy guess is that the creation of sparsed tensor works, but somehow `next()` does not work because `pyfunc_1` returned 3 values, which I could not debug. Could someone give me a pointer?\r\n\r\n", "This issue likely happens because the sparsetensor isn't being unpacked into its 3 component values.", "Hi @alextp, thank you for the reply. Right now I am using `tensorflow_core.python.util.nest.pack_sequence_as` to produce the output ([here](https://github.com/minhtriet/tensorflow/blob/master/tensorflow/python/ops/script_ops.py#L461)), but somehow `_is_composite_tensor` returns `False`, which I do not understand why\r\n\r\nThe call stack is \r\n\r\n```\r\n_sequence_like, nest.py:149\r\npack_sequence_as, nest.py:471\r\n_get_defun_inputs, func_graph.py:1160\r\n_get_defun_inputs_from_args, func_graph.py:1062\r\nfunc_graph_from_py_func, func_graph.py:837\r\n_create_graph_function, function.py:2041\r\n_maybe_define_function, function.py:2150\r\n_get_concrete_function_internal_garbage_collected, function.py:1848\r\n_get_concrete_function_internal, function.py:1854\r\n__init__, dataset_ops.py:2695\r\n__init__, dataset_ops.py:3416\r\nmap, dataset_ops.py:1211\r\ndataset_map_sparse_test, test.py:25          << This is the test from OP\r\n<module>, test.py:28\r\n```", "That doesn't make sense to me because SparseTensor is a composite: https://github.com/tensorflow/tensorflow/blob/bb45024ae9d3df0127d1c1056b08f25e60ba601c/tensorflow/python/framework/sparse_tensor.py#L48\r\n\r\nSo it's likely something else that is going on here."]}, {"number": 30063, "title": "Tensorflow Bitcast - float32 to int32 to float32?", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: \r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): ('v1.10.1-0-g4dcfddc5d1', '1.10.1')\r\n- Python version: 2.7.12\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CPU\r\n- GPU model and memory: \r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nBased off of the network, found at, \r\nhttps://www.easy-tensorflow.com/tf-tutorials/convolutional-neural-nets-cnns/cnn1\r\nand after some additional modifications, I added layers before and after each convolutional layer that bitcasts the initial tf.float32 to a tf.int32 and back to a tf.float32. This resulted in a average accuracy loss of 5% compared to passing the tensor through the layer unmodified \r\n\r\n**Describe the expected behavior**\r\nBased off of the documentation:\r\n> tf.bitcast ( input, type, name=None )\r\n> Given a tensor _input_, this operation returns a tensor that has the same buffer data as _input_ with datatype _type_.\r\n\r\nI expected that a change and return to the original data type, without modifying the buffer data would not cause that amount of decrease to the network's accuracy.  \r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n`\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nold_v = tf.logging.get_verbosity()\r\ntf.logging.set_verbosity(tf.logging.ERROR)\r\n\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\n\r\n#Image / Class Information\r\nimg_h = img_w = 28  # MNIST images are 28x28\r\nimg_size_flat = img_h * img_w  # 28x28=784, the total number of pixels\r\nn_classes = 10  # Number of classes, one class per digit\r\nn_channels = 1\r\n\r\n #Numpy Number Type\r\nnpNumberType = np.float32\r\n\r\n\r\ndef load_data(mode='train'):\r\n    mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\r\n    if mode == 'train':\r\n        x_train, y_train, x_valid, y_valid = mnist.train.images, mnist.train.labels, \\\r\n                                             mnist.validation.images, mnist.validation.labels\r\n        x_train, _ = reformat(x_train, y_train)\r\n        x_valid, _ = reformat(x_valid, y_valid)\r\n        return x_train, y_train, x_valid, y_valid\r\n    elif mode == 'test':\r\n        x_test, y_test = mnist.test.images, mnist.test.labels\r\n        x_test, _ = reformat(x_test, y_test)\r\n    return x_test, y_test\r\n\r\n\r\ndef reformat(x, y):\r\n    img_size, num_ch, num_class = int(np.sqrt(x.shape[-1])), 1, len(np.unique(np.argmax(y, 1)))\r\n    dataset = x.reshape((-1, img_size, img_size, num_ch)).astype(npNumberType)\r\n    labels = (np.arange(num_class) == y[:, None]).astype(npNumberType)\r\n    return dataset, labels\r\n\r\n\r\ndef randomize(x, y):\r\n    permutation = np.random.permutation(y.shape[0])\r\n    shuffled_x = x[permutation, :, :, :]\r\n    shuffled_y = y[permutation]\r\n    return shuffled_x, shuffled_y\r\n\r\n\r\ndef get_next_batch(x, y, start, end):\r\n    x_batch = x[start:end]\r\n    y_batch = y[start:end]\r\n    return x_batch, y_batch\r\n\r\n\r\nx_train, y_train, x_valid, y_valid = load_data(mode='train')\r\nprint(\"Size of:\")\r\nprint(\"- Training-set:\\t\\t{}\".format(len(y_train)))\r\nprint(\"- Validation-set:\\t{}\".format(len(y_valid)))\r\n\r\ndef padding (x, name):\r\n    dtypecast = tf.bitcast(x, tf.int32)\r\n    return tf.bitcast(dtypecast, tf.float32, name=name)\r\n\r\n#TensorFlow Number Format\r\ntfGraphNumberType = tf.float32\r\nrngseed = 1234\r\n\r\nfor runs in range(1, 2, 1):\r\n    tf.reset_default_graph()\r\n    tf.set_random_seed(rngseed)\r\n    np.random.seed(rngseed)\r\n\r\n    with tf.name_scope('Input'):\r\n        x = tf.placeholder(tfGraphNumberType, shape=[None, img_h, img_w, n_channels], name='X')\r\n        y = tf.placeholder(tfGraphNumberType, shape=[None, n_classes], name='Y')\r\n\r\n    padding0 = padding(x, 'Pad0')\r\n\r\n    # 1st Convolutional Layer\r\n    filter_size1 = 5  # Convolution filters are 5 x 5 pixels.\r\n    num_filters1 = 16  # There are 16 of these filters.\r\n    stride1 = 1  # The stride of the sliding window\r\n\r\n    with tf.variable_scope('conv1'):\r\n        num_in_channel = padding0.get_shape().as_list()[-1]\r\n        W = tf.get_variable('W', dtype=tfGraphNumberType,\r\n                            shape=[filter_size1, filter_size1, num_in_channel, num_filters1],\r\n                            initializer=tf.truncated_normal_initializer(stddev=0.01,seed=rngseed))\r\n        tf.summary.histogram('weight', W)\r\n        b = tf.get_variable('b', dtype=tfGraphNumberType,\r\n                            initializer=tf.constant(0., shape=[num_filters1], dtype=tfGraphNumberType))\r\n        tf.summary.histogram('bias', b)\r\n        layer = tf.nn.conv2d(padding0, W, strides=[1, stride1, stride1, 1], padding=\"SAME\")\r\n        layer += b\r\n        conv1 = tf.nn.relu(layer)\r\n\r\n    shape = conv1.get_shape().as_list()[-1]\r\n    padding1 = padding(conv1, 'Pad1')\r\n    pool1 = tf.nn.max_pool(padding1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\", name='pool1')\r\n\r\n    # 2nd Convolutional Layer\r\n    filter_size2 = 5  # Convolution filters are 5 x 5 pixels.\r\n    num_filters2 = 32  # There are 32 of these filters.\r\n    stride2 = 1  # The stride of the sliding window\r\n\r\n    with tf.variable_scope('conv2'):\r\n        num_in_channel = pool1.get_shape().as_list()[-1]\r\n        W = tf.get_variable('W', dtype=tfGraphNumberType,\r\n                            shape=[filter_size2, filter_size2, num_in_channel, num_filters2],\r\n                            initializer=tf.truncated_normal_initializer(stddev=0.01, seed=rngseed))\r\n        tf.summary.histogram('weight', W)\r\n        b = tf.get_variable('b', dtype=tfGraphNumberType,\r\n                            initializer=tf.constant(0., shape=[num_filters2], dtype=tfGraphNumberType))\r\n        tf.summary.histogram('bias', b)\r\n        layer = tf.nn.conv2d(pool1, W, strides=[1, stride2, stride2, 1], padding=\"SAME\")\r\n        layer += b\r\n        conv2 = tf.nn.relu(layer)\r\n\r\n    padding2 = padding(conv2, 'Pad2')\r\n    pool2 = tf.nn.max_pool(padding2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\", name='pool2')\r\n\r\n    with tf.variable_scope('Flatten_layer'):\r\n        layer_shape = pool2.get_shape()\r\n        num_features = layer_shape[1:4].num_elements()\r\n        layer_flat = tf.reshape(pool2, [-1, num_features])\r\n\r\n    # Fully-connected layer.\r\n    h1 = 128  # Number of neurons in fully-connected layer.\r\n\r\n    with tf.variable_scope('FC1'):\r\n        in_dim = layer_flat.get_shape()[1]\r\n        W = tf.get_variable('W', dtype=tfGraphNumberType,\r\n                            shape=[in_dim, h1],\r\n                            initializer=tf.truncated_normal_initializer(stddev=0.01, seed=1234))\r\n        tf.summary.histogram('weight', W)\r\n        b = tf.get_variable('b', dtype=tfGraphNumberType,\r\n                            initializer=tf.constant(0., shape=[h1], dtype=tfGraphNumberType))\r\n        tf.summary.histogram('bias', b)\r\n\r\n        layer = tf.matmul(layer_flat, W)\r\n        layer += b\r\n        fc1 = tf.nn.relu(layer)\r\n\r\n    with tf.variable_scope('OUT'):\r\n        in_dim = fc1.get_shape()[1]\r\n        W = tf.get_variable('W', dtype=tfGraphNumberType,\r\n                            shape=[in_dim, n_classes],\r\n                            initializer=tf.truncated_normal_initializer(stddev=0.01, seed=rngseed))\r\n        tf.summary.histogram('weight', W)\r\n        b = tf.get_variable('b', dtype=tfGraphNumberType,\r\n                            initializer=tf.constant(0., shape=[n_classes], dtype=tfGraphNumberType))\r\n        tf.summary.histogram('bias', b)\r\n\r\n        output_logits = tf.matmul(fc1, W)\r\n        output_logits += b\r\n\r\n    # Training Variables\r\n    logs_path = \"./logs\"  # path to the folder that we want to save the logs for Tensorboard\r\n    lr = 0.001  # The optimization initial learning rate\r\n    epochs = 5  # Total number of training epochs\r\n    batch_size = 100  # Training batch size\r\n    display_freq = 50  # Frequency of displaying the training results\r\n    tfOutputNumberType = tf.float32\r\n\r\n    with tf.variable_scope('Train'):\r\n        with tf.variable_scope('Loss'):\r\n            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=output_logits), name='loss')\r\n        tf.summary.scalar('loss', loss)\r\n        with tf.variable_scope('Optimizer'):\r\n            optimizer = tf.train.AdamOptimizer(learning_rate=lr, name='Adam-op').minimize(loss)\r\n        with tf.variable_scope('Accuracy'):\r\n            correct_prediction = tf.equal(tf.argmax(output_logits, 1), tf.argmax(y, 1), name='correct_pred')\r\n            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tfOutputNumberType), name='accuracy')\r\n        tf.summary.scalar('accuracy', accuracy)\r\n        with tf.variable_scope('Prediction'):\r\n            cls_prediction = tf.argmax(output_logits, axis=1, name='predictions')\r\n\r\n    # Number of training iterations in each epoch\r\n    num_tr_iter = int(len(y_train) / batch_size)\r\n\r\n    # Initialize the variables\r\n    init = tf.global_variables_initializer()\r\n    # Merge all summaries\r\n    merged = tf.summary.merge_all()\r\n\r\n    sess = tf.InteractiveSession()\r\n    sess.run(init)\r\n    global_step = 0\r\n    summary_writer = tf.summary.FileWriter(logs_path, sess.graph)\r\n\r\n    print('R: {}'.format(runs))\r\n    for epoch in range(epochs):\r\n        #print('Training epoch: {}'.format(epoch + 1))\r\n        x_train, y_train = randomize(x_train, y_train)\r\n        for iteration in range(num_tr_iter):\r\n            global_step += 1\r\n            start = iteration * batch_size\r\n            end = (iteration + 1) * batch_size\r\n            x_batch, y_batch = get_next_batch(x_train, y_train, start, end)\r\n\r\n            # Run optimization op (backprop)\r\n            feed_dict_batch = {x: x_batch, y: y_batch}\r\n            sess.run(optimizer, feed_dict=feed_dict_batch)\r\n\r\n            if iteration % display_freq == 0:\r\n                # Calculate and display the batch loss and accuracy\r\n                loss_batch, acc_batch, summary_tr = sess.run([loss, accuracy, merged],\r\n                                                             feed_dict=feed_dict_batch)\r\n                summary_writer.add_summary(summary_tr, global_step)\r\n\r\n                print(\"I:{0:3d}:\\t L={1:.2f},\\tA={2:.01%}\".\r\n                      format(iteration, loss_batch, acc_batch))\r\n\r\n\r\n        # Run validation after every epoch\r\n        feed_dict_valid = {x: x_valid, y: y_valid}\r\n        loss_valid, acc_valid = sess.run([loss, accuracy], feed_dict=feed_dict_valid)\r\n\r\n        print(\"M:{0}\\tE:{1}\\tF\\tA:{2:.01%}\".\r\n              format(runs, epoch + 1, acc_valid))\r\n\r\n\r\n    # Test the network when training is done\r\n    x_test, y_test = load_data(mode='test')\r\n    feed_dict_test = {x: x_test, y: y_test}\r\n    loss_test, acc_test = sess.run([loss, accuracy], feed_dict=feed_dict_test)\r\n    print('---------------------------------------------------------')\r\n    print(\"Test loss: {0:.2f}, test accuracy: {1:.01%}\".format(loss_test, acc_test))\r\n    print('---------------------------------------------------------')\r\n\r\n    # close the session after you are done with testing\r\n    sess.close()\r\n`\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nI attached a spreadsheet file with the results of training with different datatypes and my expectations. I apoplgize for any mistakes or missing information. Thank you.\r\n[Bitcast Logs.xlsx](https://github.com/tensorflow/tensorflow/files/3318652/Bitcast.Logs.xlsx)\r\n\r\n", "comments": ["I have tried in Colab with TF version 1.10.1 and was able to reproduce the issue.", "Is this because bitcasting through integers generates not differentiable operations?"]}, {"number": 29968, "title": "Many context switches / Many threads even if threading is limited", "body": "**System information**\r\n- Have I written custom code: No (https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/quickstart/beginner.ipynb)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux OpenSuse Leap 15.0\r\n- TensorFlow installed from (source or binary): pip package tensorflow==2.0.0-beta1\r\n- TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213\r\n- Python version: 3.7.3 \r\n- GPU model and memory: CPU only\r\n\r\n**Describe the current behavior**\r\nIf you limit the number of threads with tf.set_inter_op_parallelism_threads(NUM_THREADS) and tf.set_intra_op_parallelism_threads(NUM_THREADS) tensorflow creates a threadpool with more threads than NUM_THREADS and runs maximal NUM_THREADS causing high amount of context switches, which is delaying execution.\r\n\r\n**Describe the expected behavior**\r\nCreate maximal NUM_THREADS and limit context switches.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\n!pip install -q tensorflow==2.0.0-beta1\r\nimport tensorflow as tf\r\nNUM_THREADS=2\r\ntf.config.threading.set_inter_op_parallelism_threads(NUM_THREADS)\r\ntf.config.threading.set_intra_op_parallelism_threads(NUM_THREADS)\r\nmnist = tf.keras.datasets.mnist\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\nmodel = tf.keras.models.Sequential([\r\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n  tf.keras.layers.Dense(128, activation='relu'),\r\n  tf.keras.layers.Dropout(0.2),\r\n  tf.keras.layers.Dense(10, activation='softmax')\r\n])\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\nmodel.fit(x_train, y_train, epochs=100)\r\n```\r\n\r\n\r\n", "comments": ["@fhausmann I tried reproducing the issue above code but I receive the following error\r\n`AttributeError: module 'tensorflow' has no attribute 'set_inter_op_parallelism_threads'`\r\nCan you help us to reproduce the issue. Thanks!", "Sorry, the two lines were incorrect. They should be:\r\ntf.config.threading.set_inter_op_parallelism_threads(NUM_THREADS)\r\ntf.config.threading.set_intra_op_parallelism_threads(NUM_THREADS)\r\nI adjusted the example above.", "I am able to reproduce the issue with Tf 2.0.0.beta1 on colab.", "Adding @tfboyd - should the number of threads be set to 1?", "With TF 2.0 and GPU we have not needed to tough this.  Couple items.\r\n\r\n1. Based on how the issue is written I am not sure of the problem other than guessing.  I do not see any timings for the delay of the execution or how we would verify the problem is fixed or exists.  Or something showing there are more \r\n\r\n2. Maybe this config is not being respected. tf.config \"global\" is new.  With the TF 1.0 method of passing config to the session, I would have tests to black box prove the values are set based on the resulting perf changes.\r\n\r\nAdding penpornk from the CPU group.\r\n@penpornk ", "@fhausmann Could you please clarify how you observed the number of threads actually being used and that a lot of context switches happened? How many threads did you see?\r\n\r\nTensorFlow has two thread pools: inter-op (how many ops can be run concurrently) and intra-op (a thread pool shared between all concurrent ops to execute tasks). The expected number of threads is #inter-threads + #intra-threads. \r\n* If you saw 4 threads instead of 2: This is expected and you should adjust NUM_THREADS.\r\n* If you saw many more than 4 threads: The settings probably somehow didn't get through and I'll try to investigate.", "@penpornk I saw 4 threads running like expected (htop) but both ps and pstree show me 20 threads, if I run the code above.\r\ncat /proc/PID/status shows me then (at approx. epoch 7) the following:\r\nThreads:\t21\r\nvoluntary_ctxt_switches:\t10277\r\nnonvoluntary_ctxt_switches:\t35\r\n\r\n", "@fhausmann Thank you! I was able to reproduce the issue. Will see what's going on.", "FYI @martijnvels", "@fhausmann Is this still an issue? Can you check with `TF2.0` and let us know whether the issue persists with latest TF version. I tried both `!pip install tf-nightly` and `!pip install tensorflow==2.0.0`. \r\n\r\n[Here is the gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/f38f8b7f3c7adbc171814cb863b0de17/untitled539.ipynb) with `!pip install tf-nightly`. Thanks!\r\n\r\nPlease close the issue if it was resolved already. Thanks!", "The issue is not resolved using both,  latest  TF2.0 and nightly. \r\nI still get 16-21 threads and > 38000 voluntary_ctxt_switches (at epoch 20)\r\nI used \r\n```\r\nimport os \r\npid = os.getpid()\r\nwith open(\"/proc/{}/status\".format(pid)) as file:\r\n    print(file.read())\r\n```\r\nafter the execution of model.fit to look into these values.", "Issue still persists in **Tensorflow 2.0.0**\r\nOS: Ubuntu 18.04\r\n\r\nMy Code : \r\n```\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\nNUM_THREADS=2\r\ntf.config.threading.set_inter_op_parallelism_threads(NUM_THREADS)\r\ntf.config.threading.set_intra_op_parallelism_threads(NUM_THREADS)\r\n```\r\n\r\n![Screenshot from 2020-01-09 15-47-02](https://user-images.githubusercontent.com/14270453/72059195-8ea96b00-32f7-11ea-8070-aa7fb13251e6.png)\r\n", "Hi @penpornk \r\nI think it works....\r\nI have tried this version(TF2.0): 99cb9fd68d1b8c6f0857c0775c366234537bacf5  \r\nWith a combination of OMP_NUM_THREADS, intra_threads and inter_threads\r\nFor example:\r\n```\r\nexport OMP_NUM_THREADS=2\r\ntf.config.threading.set_intra_op_parallelism_threads(2)\r\ntf.config.threading.set_inter_op_parallelism_threads(1)\r\n```\r\nFrom the view of HTOP, I may see **more than 2 threads existing**, but actually **only 2 threads are running**. I bet the others threads are sleeping.\r\nAlthough, _i don't know why there are more than 2 threads existing, nevertheless they are sleeping...._ Here may need your help to explain it....\r\n\r\nAs for the OMP_NUM_THREADS and intra_op_parallelism_threads, I think for native TF ops, such as the eigen op, when need parallel computation, may pick the thread for the intra_op_threads pool. For the MKL op, the parallel may be decided by the OMP_NUM_THREADS. _Please correct me, if I am wrong...._", "was this fixed in a tensorflow 2.1 or 2.2?", "@nilskk As to my knowledge it was not fixed up to now (TF 2.2.0 v2.2.0-rc4-8-g2b96f3662b).", "It must have something to do with the way TF was compiled.\r\n\r\nMy pip-installed TF 2.2.0 behaves correctly and as expected: when I use `set_{inter,intra}_op_parallelism_threads(1)` then it will use one busy thread and 100% CPU. When I use my custom-built TF 2.2.0 following the [official build instructions](https://www.tensorflow.org/install/source), then it will use 600% CPU with 6 busy threads (and some other non-busy threads) despite the parallelism settings. My machine is a hexa-core, so it seems like it is auto-detecting instead of using what I specified.", "@jlherren \r\nHmm, from my point ( I was always using the pip version) it was always running only the correct number of threads, but created a lot more (which were all suspended and this slows down performance due to many context switches).\r\n\r\nSo the Issue was never about incorrect number of running, but created threads. So I guess your issue is a different one, but may connected. ", "I'm experiencing something similar while running inference on a previously trained model. I'm using `tensorflow` 2.2.0 installed from PyPI. Using:\r\n\r\n```\r\nexport OMP_NUM_THREADS=2\r\ntf.config.threading.set_intra_op_parallelism_threads(2)\r\ntf.config.threading.set_inter_op_parallelism_threads(1)\r\n```\r\n\r\nhas no effect and I still see very many threads being produced and high CPU usage. I eventualy found something that does have an effect however:\r\n\r\n```\r\nexport TF_NUM_INTEROP_THREADS=2\r\nexport TF_NUM_INTRAOP_THREADS=1\r\n```\r\n\r\nI then see only at most 200% CPU use and the process starting very many fewer threads.", "I've tried:\r\n\r\n```\r\ntf.config.threading.set_intra_op_parallelism_threads(2)\r\ntf.config.threading.set_inter_op_parallelism_threads(2)\r\nos.environ['TF_NUM_INTEROP_THREADS'] = '2'\r\nos.environ['TF_NUM_INTRAOP_THREADS'] = '2'\r\n```\r\n\r\non `tensorflow  2.3.0` (conda-forge) Ubuntu 18.04, and `model.fit()` is trying to use all 80 threads on the server. I'm just getting started in really learning TF, but I can't even use it, since it hogs all shared cpu resources on the server. Maybe this is a sign to learn pytorch instead of TF...", "The TF_NUM_INTEROP_THREADS and TF_NUM_INTRAOP_THREADS must be set before\nthe program starts, or at least before any TF code is run as they are\nevaluated once on the first threadpool creation.\n\nOn Sun, Nov 15, 2020 at 2:02 PM Nick Youngblut <notifications@github.com>\nwrote:\n\n> I've tried:\n>\n> tf.config.threading.set_intra_op_parallelism_threads(2)\n> tf.config.threading.set_inter_op_parallelism_threads(2)\n> os.environ['TF_NUM_INTEROP_THREADS'] = '2'\n> os.environ['TF_NUM_INTRAOP_THREADS'] = '2'\n>\n> on tensorflow 2.3.0 (conda-forge) Ubuntu 18.04, and model.fit() is trying\n> to use all 80 threads on the server. I'm just getting started in really\n> learning TF, but I can't even use it, since it hogs all shared cpu\n> resources on the server. Maybe this is a sign to learn pytorch instead of\n> TF...\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/29968#issuecomment-727619516>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AMNMWW7QU7K4P22YJQJS2LDSQAQTHANCNFSM4HZLJVQA>\n> .\n>\n", ">The TF_NUM_INTEROP_THREADS and TF_NUM_INTRAOP_THREADS must be set before\r\nthe program starts, or at least before any TF code is run as they are\r\nevaluated once on the first threadpool creation.\r\n\r\nI did. My reprex is:\r\n\r\n```\r\nimport os\r\nimport tensorflow as tf\r\n\r\ntf.config.threading.set_intra_op_parallelism_threads(2)\r\ntf.config.threading.set_inter_op_parallelism_threads(2)\r\n\r\nos.environ['TF_NUM_INTEROP_THREADS'] = '2'\r\nos.environ['TF_NUM_INTRAOP_THREADS'] = '2'\r\n\r\nmnist = tf.keras.datasets.mnist\r\n\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\nmodel = tf.keras.models.Sequential([\r\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n  tf.keras.layers.Dense(128, activation='relu'),\r\n  tf.keras.layers.Dropout(0.2),\r\n  tf.keras.layers.Dense(10)\r\n])\r\n\r\npredictions = model(x_train[:1]).numpy()\r\n\r\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss=loss_fn,\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(x_train, y_train, epochs=5)\r\n```\r\n\r\nAt least `torch.set_num_threads(1)` works correctly for me.", "I too am running into this issue as of today, with tensorflow 2.2.0 and beyond. (tried 2.3 and 2.4)", "The following works for me with TensorFlow 2.3.1 :\r\nThe aim was to limit the CPU usage to a maximum of 1 CPU.\r\n```python\r\nimport os\r\nimport tensorflow as tf\r\n\r\nnum_threads = 1\r\n# Maximum number of threads to use for OpenMP parallel regions.\r\nos.environ[\"OMP_NUM_THREADS\"] = \"1\"\r\n# Without setting below 2 environment variables, it didn't work for me. Thanks to @cjw85 \r\nos.environ[\"TF_NUM_INTRAOP_THREADS\"] = \"1\"\r\nos.environ[\"TF_NUM_INTEROP_THREADS\"] = \"1\"\r\n\r\ntf.config.threading.set_inter_op_parallelism_threads(\r\n    num_threads\r\n)\r\ntf.config.threading.set_intra_op_parallelism_threads(\r\n    num_threads\r\n)\r\ntf.config.set_soft_device_placement(True)\r\n```\r\n\r\nAfter using the above, the CPU usage was limited to 1 CPU, also this was tested with my GPU disabled using \r\n```python\r\nos.environ['CUDA_VISIBLE_DEVICES'] = '-1'\r\n```", "With the gist above (tf-nightly) I got: \r\n```\r\nThreads:\t20\r\nvoluntary_ctxt_switches:\t38101\r\nnonvoluntary_ctxt_switches:\t29200\r\n```\r\nAnd limiting by using the enviroment variables:\r\n ```python\r\n# Maximum number of threads to use for OpenMP parallel regions.\r\nos.environ[\"OMP_NUM_THREADS\"] = \"1\"\r\n# Without setting below 2 environment variables, it didn't work for me. Thanks to @cjw85 \r\nos.environ[\"TF_NUM_INTRAOP_THREADS\"] = \"1\"\r\nos.environ[\"TF_NUM_INTEROP_THREADS\"] = \"1\"\r\n```\r\nI got:\r\n```\r\nThreads:\t19\r\nvoluntary_ctxt_switches:\t31513\r\nnonvoluntary_ctxt_switches:\t34667\r\n```\r\n\r\nSo not really an improvement. I didn't checked the number of running threads, but this was always fine for me. \r\nBut I cannot see any big improvement in the total number of threads.\r\n\r\nI think several things get confused in this issue and I want to clarify this a bit:\r\n- Tensorflow sometimes don't honor the setting and **runs** more threads than specified. This never happened to me.\r\n- Tensorflow **creates** more threads than specified runs the correct number, which I think is probably not an issue for most people but can be annoying if a cluster spends a lot of CPU-time in context-switches.", "Was able to reproduce this issue in TF 2.6.0-dev20210526,Please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/9dc8dcd65abf53afcef8a97d355b534e/untitled10.ipynb) ....Thanks !", "Can you cross check in the Linux system level information to check how many threads actually being utilized by passing your PID in below comment `ps -o nlwp <pid>` or `ps -o thcount <pid>`.", "With the following settings:\r\n```python\r\nnum_threads = 1\r\n\r\nos.environ[\"OMP_NUM_THREADS\"] = \"1\"\r\nos.environ[\"TF_NUM_INTRAOP_THREADS\"] = \"1\"\r\nos.environ[\"TF_NUM_INTEROP_THREADS\"] = \"1\"\r\n\r\ntf.config.threading.set_inter_op_parallelism_threads(num_threads)\r\ntf.config.threading.set_intra_op_parallelism_threads(num_threads)\r\n\r\n```\r\nI still get the same results:\r\n\r\ntensorflow==2.5.0\r\n```\r\n$ ps -o nlwp 191200\r\nNLWP\r\n  19\r\n$ ps -o thcount 191200 \r\nTHCNT\r\n   19\r\n```\r\nand after running:\r\n```\r\n$ cat /proc/191200/status\r\nThreads:\t16\r\nvoluntary_ctxt_switches:\t22228\r\nnonvoluntary_ctxt_switches:\t2335\r\n\r\n```\r\n\r\n\r\ntf-nightly==2.6.0.dev20210623\r\n```\r\n$ ps -o nlwp 194592\r\nNLWP\r\n  23\r\n$ ps -o thcount 194592\r\nTHCNT\r\n   23\r\n```\r\nand after running:\r\n```\r\n$ cat /proc/194592/status\r\nThreads:\t16\r\nvoluntary_ctxt_switches:\t22471\r\nnonvoluntary_ctxt_switches:\t2661\r\n```", "Was able to reproduce this issue in TF 2.6 ,Please find the gist [**`here`**](https://colab.research.google.com/gist/kumariko/a705e35660b4008f31679f246e6e5267/untitled10.ipynb#scrollTo=Bf5MdeQ2IluD) . Thanks !"]}, {"number": 29877, "title": "TFRecord guide doesn't show how to serialize and parse tensors", "body": "## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/beta/tutorials/load_data/tf_records#creating_a_tfexample_message\r\nhttps://www.tensorflow.org/tutorials/load_data/tf_records\r\n\r\n## Description of issue (what needs changing):\r\nthe TFRecord docs don't show how to serialize and parse tensors to a TFRecord. It's a minimal example with a bunch of strings, integers, and floats. How do you include tensor features in a tf.train.Example ?\r\n\r\nThe whole tf.train / tf.io / tf.data thing feels scattered and full of unnecessary boilerplate. Why are Examples Features, Feature in train, but then then we have to write a bunch of boilerplate tf.io functions like **_float_value _byte_value** to make the actual features? All I want to do is make a TFRecord with a bunch of tensors of different shapes in each entry. \r\n\r\n### Clear description\r\n\r\nAll of this stuff should be simplified and put into tf.data... Tf.io feels pointless since tf.data is meant to do the same thing. \r\n\r\nWhy can't the tensorflow guide to TFRecord tell us how to write tensor data to a TFRecord and read it with TFData? \r\n\r\nWhy do we need to write so much boilerplate to add features to tf.train.Example?\r\n\r\nWhy is Data IO spread out over four separate modules (train, io, dtypes, and data) ?", "comments": ["You could add `FeatureColumn`s to this list. They can be used as a part of the input data flow to perform some common transformations (although as far as I can tell, only `Estimator`s can currently consume them, which is a shame).", "It's really hard to know the difference between all these different functions...\r\n\r\nFixedLenSequenceFeature, VarLenFeature,\r\nparse_example, parse_single_example, parse_single_sequence_example ? \r\n\r\nExtremely frustrating to implement what ought to be simple, get data, write data, read data ", "https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/train/Example\r\n\r\nZero information. ", "To be fair, the 2.0 beta version of the documentation has a bit more info.\r\n\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/train/Example\r\nhttps://www.tensorflow.org/beta/tutorials/load_data/tf_records\r\n\r\nThe point stands that a guide on how all of these these various systems most easily interface with each other would be quite useful.", "Hopefully https://github.com/tensorflow/community/pull/151 will be a way forward if those could be serialized/deserialized to `tf.train.Example` without a bunch of boilerplate.", "Sounds like feature creep", "more of a general complaint, the entire TF records API needs a user-friendly overhaul.\r\nit's a simple operation, write data, read data.\r\nTensors are well aware of their shapes and datatypes.\r\nPython strings are strings and Python numbers are numbers. I dont understand why I need a page long boilerplate (from the guide) to describe each element 4! times.\r\n\r\nif python pickle can handle arbitrary python objects and python JSON can handle arbitrarily nested data in one line, TF should be able to handle its own objects and other primitives without so much fuss.", "This documentation helps a lot: https://medium.com/mostly-ai/tensorflow-records-what-they-are-and-how-to-use-them-c46bc4bbb564. It could usefully be used as the basis for a tutorial on the official TF site.", "@MikeOfZen, IIUC [tf.data.Snapshot](https://github.com/tensorflow/community/pull/193) will be the fix you're looking for.\r\n\r\nRight @jsimsa?\r\n\r\nBut yes, it looks like we could use a section on `tf.io.parse_tensor` and `tf.io.serialize_tensor` either way.", "@MarkDaoust I don't think that [tf.data.Snapshot](https://github.com/tensorflow/community/pull/193) will provide what is @MikeOfZen asking for. IIUC, he would like to have a simple API to \"save\" and \"load\" arbitrary structure of tensors. \r\n\r\n@MikeOfZen a challenge that Python pickling does not need to solve but TensorFlow ser/de does is how to statically infer shapes and types so that a traced Python function can be executed as a dataflow computation. A simple \"load(file)\" operation does not provide any information into what the shape and type of the data stored in the file is which is needed to be known statically (i.e. reading from a file is not good enough) at the time a Python function with this operation is traced. So even if we had \"save\" and \"load\" as primitives, the \"load\" primitive would still likely need to specify a structure of `tf.TypeSpec`s that describe the shape and type of the loaded data for the sake of static shape inference.\r\n\r\nThe building blocks of \"snapshot\" could be used for implementing \"save\" and \"load\" which is something I can imagine doing as a follow up to \"snapshot\" and will take it into consideration in tf.data 2020 plans.", "@jsimsa  thank you for your response. Yes, you understood it correctly.\r\nI appericiate the diffcutly of integrating this dynamic into the graph tracing. \r\n\r\nI wouldn't mind indicating the tensor types&shapes before a 'load' operation.\r\nor even having a seperate 'descriptor' file (though it can also be a header in a regular data file)\r\nfor the static graph building.\r\n\r\nright now, this information has to be coded anyway into the ser/deser code.\r\nto illustrate. a simple code to serialize an example (with a single tensor) for a TF records file is something like this:\r\n```\r\ndef bytes_feature(value):\r\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\r\n\r\ndef encode_example(mask):\r\n    mask = tf.constant(mask)\r\n    mask = tf.io.serialize_tensor(mask).numpy()\r\n\r\n    feature = {\r\n           'mask'     : bytes_feature(mask),\r\n            }\r\n    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\r\n    return example_proto.SerializeToString()\r\n```\r\nThis is quite convuluted and verbose. and the shape and type are not mentioned here (the type is used in the deserialization as you said) \r\n\r\nEven if the functionality remains as it is, a friendliar API that hides this could be a decent solution.\r\n\r\nBut Anyway, it's great that this is on your mind!", "I second @MikeOfZen. The TensorFlow documentation, at least for this functionality, needs a significant revamp and needs to be much more user-friendly. I have wasted three hours on something that should have taken 30 seconds and still don't have an answer.", "Just finished digging into the darkest corners of `tensorflow` and `tensorflow/datasets` to get me a workable solutions. Everything seems duct taped and half baked in TF. It's so bad that the guys from `tensorflow/datasets` almost built everything again around the protobuf serialization so you don't have to spend much time around protobuffer. `tensorflow/datasets` however introduces other types of pains so unless you publish your dataset publicly and heavy reuse you might be better off without it.\r\n\r\nWhat I don't really understand is why can't all the protobuf magic happen automatically? The tensors as they stand today, without any other funky new objects have everything they need to be put on disk. Unless users do want to go bespoke (which most of us don't) we should get zero exposure to protobuf API. The TF backend should be able to do it automatically. Tensors in, tensors out.\r\n\r\nAs most of us are currently doing it \r\n`tensor`-> `numpy`->`bytes`->`string`->`protobuf`-> `bytes_on_disk` -> `protobuf` -> `funky representation on graph few on this planet understand` -> `string/bytes/weird tensorflow backend artifacts` -> tensors of bytestrings I need to call `.numpy()` on -> numpy objects -> finally my tensor. All that to avoid splitting features into smaller proto fields. \r\n\r\nI just managed to get a working solution based on the protobuf API available in tensorflow. This approach avoids serializing numpy data to bytes and passing the raw bytes to the TFRecord. This also removes the need for special py_function that cannot be serialized and moved on the graph when sending data to the TPU or in a distributed environment. \r\n\r\nThe code is available at https://gist.github.com/vicpara/5c23c78d0f3105af53798272e628d2ad . Hope it helps more people keep sane while dealing with this API.\r\n\r\n", "Are there any updates on this? I'm using TPU which make things more complicated. \r\nIdeally, I want to run operations in TPU, return as tensor which is still on TPU, directly access the gs bucket and write the result as TFRecord. \r\nAll the options I found require the tensor to be transformed to something else (i.e. bytes, numpy, float etc etc), which TPU is not too happy about. \r\nIt would be really great to have a function that just writes tensor to tf record. ", "Hello everyone, I am having some troubles trying to import data into a TFX pipeline, if anyone can point me in the right direction I'd really appreciate it. I have a 3D Ragged (or Sparse if need be) Tensor, where each 2D slice is an example which I'd like to feed into my pipeline. Each 2D tensor is composed of a variable number or rows, but they collectively form one example, not just a batch of examples. What is the best way to encode this structure, so that the ExampleGen component of the tfx pipeline can feed it into the pipeline?\r\nSo far I have tried the instructions in this guide https://medium.com/mostly-ai/tensorflow-records-what-they-are-and-how-to-use-them-c46bc4bbb564 to turn it in a tf.train.SequenceExample, so that each feature has a vector inside the value field, and written the result to a tf.Record. For some reason I am really struggling to unpack it though.\r\n\r\nAny help would be really appreciated!\r\n\r\nMatt\r\n\r\nPs: Please note that the 3rd dim of the tensor is pretty long, say 40k, so writing each example into a tf.Record would generate an equal number of files, which in my opinion is impractical, inefficient and not very elegant, but I'm happy to be wrong.", "there's now `tf.data.experimental.save(tf_dataset, path)`You don't need to `tf.io.serialize` the dataset yourself anymore. The current version (at time of writing), still requires you to know the `output_signature` of the `tf.data.Dataset` you're trying to load, but that's no longer needed in the `nightly` build. The catch is that you don't serialize to a tfrecord, but to a folder. \r\ncurrent:\r\nhttps://www.tensorflow.org/api_docs/python/tf/data/experimental/save\r\nnightly:\r\nhttps://www.tensorflow.org/api_docs/python/tf/data/experimental/save?version=nightly\r\n\r\n```\r\n>>> def gen():\r\n...   for i in range(10):\r\n...     yield i,i\r\n... \r\n>>> ds = tf.data.Dataset.from_generator(gen, output_signature=(tf.TensorSpec(dtype=tf.int32,shape=()),tf.TensorSpec(dtype=tf.int32,shape=())))\r\n>>> tf.data.experimental.save(ds, \"test.tfrecord\")\r\n>>> loaded_ds = tf.data.experimental.load(\"test.tfrecord\") # this should work in the `nightly`\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nTypeError: load() missing 1 required positional argument: 'element_spec'\r\n>>> loaded_ds = tf.data.experimental.load(\"test.tfrecord\", element_spec=(tf.TensorSpec(dtype=tf.int32,shape=()),tf.TensorSpec(dtype=tf.int32,shape=())))\r\n>>> for i in loaded_ds:\r\n...   print(i)\r\n... \r\n(<tf.Tensor: shape=(), dtype=int32, numpy=0>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\r\n(<tf.Tensor: shape=(), dtype=int32, numpy=1>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\r\n(<tf.Tensor: shape=(), dtype=int32, numpy=2>, <tf.Tensor: shape=(), dtype=int32, numpy=2>)\r\n(<tf.Tensor: shape=(), dtype=int32, numpy=3>, <tf.Tensor: shape=(), dtype=int32, numpy=3>)\r\n(<tf.Tensor: shape=(), dtype=int32, numpy=4>, <tf.Tensor: shape=(), dtype=int32, numpy=4>)\r\n(<tf.Tensor: shape=(), dtype=int32, numpy=5>, <tf.Tensor: shape=(), dtype=int32, numpy=5>)\r\n(<tf.Tensor: shape=(), dtype=int32, numpy=6>, <tf.Tensor: shape=(), dtype=int32, numpy=6>)\r\n(<tf.Tensor: shape=(), dtype=int32, numpy=7>, <tf.Tensor: shape=(), dtype=int32, numpy=7>)\r\n(<tf.Tensor: shape=(), dtype=int32, numpy=8>, <tf.Tensor: shape=(), dtype=int32, numpy=8>)\r\n(<tf.Tensor: shape=(), dtype=int32, numpy=9>, <tf.Tensor: shape=(), dtype=int32, numpy=9>)\r\n```"]}, {"number": 29854, "title": "[TF 2.0 API Docs] tf.dynamic_partition", "body": "## System information\r\nTensorflow version: 2.0\r\n\r\n## URL(s) with the issue:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/dynamic_partition\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Raises listed and defined\r\nNo errors have been defined\r\n\r\n### Submit a pull request?\r\nYes\r\n", "comments": ["Hi @lkmandy , The above URL in template is working fine  . Could you please point out the exact issue in URL?", "My point was  **tf.dynamic_partition**  doesn't have a **Raises section** like in the case of **tf.map_fn**  [https://bit.ly/3DZ0DCn](https://bit.ly/3DZ0DCn)\r\n\r\nThere is no section describing possible errors that may occur while using tf.dynamic_partition.", "Hi @Saduf2019 ,Could you please look into this issue!"]}, {"number": 29840, "title": "Bigger Than Memory ops should automatically fallback to RAM and/or Disc in tf-gpu", "body": "**System information**\r\n- TensorFlow version (you are using): 1.14rc \r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nWe built an Atomic Transformer with Keras, and if we try to simulate too many atoms, like a big antibody or enzyme with 50,000 atoms, or a big metabolic network in system biology, it crashes because the GPU memory of the 1070ti fills up and there is no backup plan for when that happens. \r\n\r\nI believe this is an issue for many many advanced uses of TF because we sometimes need to process \"BIG DATA\" larger than the VRAM and Dask is the ONLY available option, and Dask is often extremely slow and still crashes, plus we have to convert tensors into dask arrays, it's a huge hassle. Just imagine TF-GPU intelligently predicted memory usage and used cpu/ram/drive resources as needed.\r\n\r\nObviously, this should print performance warnings in the console, and would be slower, but at least there should be some option for tf-gpu to try to keep working in cases where the workload doesn't fit in VRAM\r\n\r\n**Will this change the current api? How?**\r\nTF-GPU would either automatically scale beyond GPU / TPU memory, or there would be some way to tell it to do this in a python script. Like tf.enable_larger_than_memory() \r\n\r\n**Who will benefit with this feature?**\r\nEveryone who pushes TF-GPU to the breaking point\r\n\r\n**Any Other info.**\r\nUBUNTU 16.04 NVIDIA GTX 1070Ti ", "comments": ["Training or inferencing with neural networks on GPUs when the neural network, batch size, or data is large is a difficult thing. There is more than just data on the GPU during the processing. The neural network, the GPU kernels, the data, and the input and output tensors of each operation all need to fit in the GPU memory. Even when focusing on a single GPU operation, both the input and output tensors need to fit and this can also sometimes be a problem.\r\n\r\nA Python module exists that could possibly help out your case. It is called TensorFlow Large Model Support. TensorFlow Large Model Support (TFLMS) is a Python module that provides an approach to training large models and data that cannot normally be fit in to GPU memory. It takes a computational graph defined by users, and automatically adds swap-in and swap-out nodes for transferring tensors from GPUs to the host and vice versa. During training and inferencing this makes the graph execution operate like operating system memory paging. The system memory is effectively treated as a paging cache for the GPU memory and tensors are swapped back and forth between the GPU memory and CPU memory.\r\n\r\nTFLMS is included in the IBM Watson Machine Learning Community Edition, which is free to use, and can be installed as a conda package on both the x86 and ppc64le platforms. The conda channel URL and other documentation for TFLMS can be found here:\r\nhttps://www.ibm.com/support/knowledgecenter/SS5SF7_1.6.1/navigation/wmlce_getstarted_tflmsv2.html\r\n\r\nMore information about TFLMS, including the papers, videos, and blogs can be found here:\r\nhttps://developer.ibm.com/linuxonpower/2019/06/11/tensorflow-large-model-support-resources/", "@bionicles does setting [per_process_gpu_memory_fraction](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/config.proto#L25) to be >1.0 work for you?\r\n\r\nAlso @jaingaurav ", "Thank you for the advice, we will test these ideas and report back", "@aaroey: We haven't exposed the unified_memory option in the tf.config API namespace yet. Seems like this is needed for 2.0?", "@jaingaurav @aaroey @smatzek @ymodak @gadagashwini \r\n\r\nyes! how do you set this in tf2.0 ? we're maxing out on memory and can't find the per_process_gpu_memory_fraction option since we switched from 1.14 to 2.0.0b1", "sometimes we can do 30,000 atoms, if there were GPU support for bfloat16 and also we could unify memory, then we could push to bigger molecules", "actually, i just tested it, looks like we can indeed use bfloat16 on gpu...", "nevermind. there are issues with it... there's no \"squared difference\" kernel for bfloat16 on gpu", "30k atoms works with tf.float16 though!", "@sanjoy could you help to find an owner of this issue? Thanks", "Assigned to Gaurav since it looks like the core issue is https://github.com/tensorflow/tensorflow/issues/29840#issuecomment-507066169?", "I am from #43049 .\r\nI follow [https://stackoverflow.com/questions/58025069/how-to-enable-cuda-unified-memory-in-tensorflow-v2](url). With the following instructions, I can use unified memory with single gpu training (My machine has 8 GPUs). \r\n\r\n```\r\nfrom tensorflow.compat.v1 import ConfigProto\r\nfrom tensorflow.compat.v1 import InteractiveSession\r\n\r\nconfig = ConfigProto()\r\nconfig.gpu_options.per_process_gpu_memory_fraction = 2\r\nconfig.gpu_options.allow_growth = True\r\nsession = InteractiveSession(config=config)\r\n```\r\nBut for multi-gpu training, it reports the folllowing error. \r\n![image](https://user-images.githubusercontent.com/30710061/92552149-8e041280-f292-11ea-883e-e2e27a278b2e.png)\r\nI use horovod synthetic benchmark. [https://github.com/horovod/horovod/blob/master/examples/tensorflow2_synthetic_benchmark.py](url)\r\nCan you offer some help? Thanks a lot.\r\n", "Does TFLMS supports TPU training?", "The latest TFLMS versions which support TensorFlow 2 do not support TPUs as they are built directly into the memory allocator code for GPUs. The TFLMSv2 version, which is \"version 2\" of the Python graph modification LMS version could possibly be made to work with TPUs, but it only support graph mode with TensorFlow 1.x.\r\n\r\nhttps://github.com/IBM/tensorflow-large-model-support", "> The latest TFLMS versions which support TensorFlow 2 do not support TPUs as they are built directly into the memory allocator code for GPUs. The TFLMSv2 version, which is \"version 2\" of the Python graph modification LMS version could possibly be made to work with TPUs, but it only support graph mode with TensorFlow 1.x.\r\n\r\nAnd does LMS supports `tf-2.3.x`?", "@GF-Huang We open sourced all versions of TensorFlow LMS, including the version for 2.2.0. Unfortunately the dev team that worked on it has no current plans to release versions for newer TensorFlow versions. Having another community member pick it up, work on it, and potentially get it merged in to the main TensorFlow repository would be welcomed. "]}]