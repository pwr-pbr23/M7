[{"number": 22867, "title": "Fix typo in estimator with distribution strategy using hooks", "body": "Fix typo while extracting estimator training hooks using distribution strategy.\r\nCC @yuefengz ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Maybe this PR should be submitted against master, and then cherry-picked to r1.12?", "@byronyi is right.  All PRs should be merged into master, rather than directly into the r1.12 branch.\r\n\r\n@nvcastet please create a new PR for master.", "@tatatodd @yuefengz Can we still merge this PR to 1.12? Estimators got moved to new repo (https://github.com/tensorflow/estimator) in the master branch.", "It got moved, but the code is still in the main repo master branch. I believe it will only be removed after the 2.0 release. \r\n\r\nBut you might also need to submit a PR in tf_estimator repo. ", "@byronyi This code is not in repo anymore. Yes I will submit something for tensorflow_estimator repo too. But we would appreciate if this PR make it to 1.12.", "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py", "@byronyi ok file came back 20h ok with rollback: https://github.com/tensorflow/tensorflow/commit/20f03388ac28fdf5ad33adb87d95346209ef0052#diff-17d893fed7bb36da99ce6c80d933d00b"]}, {"number": 22866, "title": "From <ipython-input-11-8a10d6bf5a6d>:51: calling import_graph_def (from tensorflow.python.framework.importer) with op_dict is deprecated and will be removed in a future version.", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 22865, "title": "ValueError: Invalid tensors 'num_detections,detection_classes,detection_scores,detection_boxes' were found.", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["when i use the post_training_quantize ,return the error : ValueError: Invalid tensors 'num_detections,detection_classes,detection_scores,detection_boxes' were found \r\n\r\nhere is the code:\r\n\r\nimport tensorflow as tf\r\nimport pathlib\r\narchive_dir = pathlib.Path(\"****\")\r\nprint(str(archive_dir))\r\ngraph_def_file = pathlib.Path(archive_dir)/\"*.pb\"\r\ninput_arrays = [\"image_tensor\"]\r\noutput_arrays = [\"num_detections,detection_classes,detection_scores,detection_boxes\"]\r\nconverter = tf.contrib.lite.TocoConverter.from_frozen_graph(\r\n  str(graph_def_file), input_arrays, output_arrays, input_shapes={\"input\":[-1,-1,-1,3]})\r\nconverter.post_training_quantize = True\r\nmobilenet_tflite_file = graph_def_file.parent/\"*.tflite\"\r\nmobilenet_tflite_file.write_bytes(converter.convert())\r\n\r\n\r\nand i use the tensorboard display the graph of the model \r\nhere is the link: [http://dell-sea:6006/#graphs&run=.](url) \r\n\r\n\r\n\r\n", "Please use this \r\n\r\nbazel run tensorflow/tools/graph_transforms:summarize_graph --\r\n--in_graph=YOUR_GRAPH_NAME.pb\r\n\r\nto get the names of your input and output nodes and accordingly move ahead.", "thanks so much , i solved it . but i got the input_shapes:[?,?,?3]. here are the result when i run  the commond:\r\n`./bazel -bin/tensorflow/tools/graph_transforms/summarize_graph --in_graph=/home/caojinrong/ssd_mobilenet_v2_coco_2018_03_29/ssd_mobilenet_v2_coco_2018_03_29/frozen_inference_graph.pb`\r\n\r\ni got the return :\r\n`Found 1 possible inputs: (**name=image_tensor, type=uint8(4), shape=[?,?,?,3]**) \r\nNo variables spotted.\r\nFound 4 possible outputs: **(name=num_detections, op=Identity) (name=detection_classes, op=Identity) (name=detection_scores, op=Identity) (name=detection_boxes, op=Identity**) \r\nFound 16878731 (16.88M) const parameters, 0 (0) variable parameters, and 1548 control_edges\r\nOp types used: 2572 Const, 549 Gather, 465 Identity, 452 Minimum, 371 Reshape, 360 Maximum, 344 Mul, 267 Sub, 261 Add, 211 Cast, 186 Greater, 180 Where, 180 Split, 165 Slice, 144 ConcatV2, 127 StridedSlice, 121 Pack, 116 Shape, 94 Unpack, 92 ZerosLike, 92 Squeeze, 90 NonMaxSuppressionV2, 64 Rsqrt, 55 Conv2D, 47 Relu6, 45 ExpandDims, 40 Fill, 37 Tile, 33 RealDiv, 30 Range, 29 Switch, 26 Enter, 21 DepthwiseConv2dNative, 14 Merge, 12 BiasAdd, 11 TensorArrayV3, 8 NextIteration, 6 Exit, 6 TensorArrayWriteV3, 6 TensorArraySizeV3, 6 TensorArrayGatherV3, 6 Sqrt, 5 TensorArrayReadV3, 5 TensorArrayScatterV3, 3 Equal, 3 Transpose, 3 Assert, 3 Rank, 2 Exp, 2 Less, 2 LoopCond, 1 All, 1 TopKV2, 1 Size, 1 Sigmoid, 1 ResizeBilinear, 1 Placeholder\r\nTo use with tensorflow/tools/benchmark:benchmark_model try these arguments:\r\nbazel run tensorflow/tools/benchmark:benchmark_model -- --graph=/home/caojinrong/ssd_mobilenet_v2_coco_2018_03_29/ssd_mobilenet_v2_coco_2018_03_29/frozen_inference_graph.pb --show_flops --input_layer=image_tensor --input_layer_type=uint8 --input_layer_shape=-1,-1,-1,3 --output_layer=num_detections,detection_classes,detection_scores,detection_boxes\r\n`\r\nso the input shape =[?,?,?,3]\r\n\r\nbut when use the post_training_quantize,if the input_shape write as the return,the relevent code is :\r\n`converter = tf.contrib.lite.TocoConverter.from_frozen_graph(\r\n str(graph_def_file), input_arrays, output_arrays, input_shapes={\"image_tensor\":[-1,-1,-1,3]})\r\nconverter.post_training_quantize = True`\r\n\r\nit got the error:\r\n`Traceback (most recent call last):\r\n  File \"quant.py\", line 9, in <module>\r\n    str(graph_def_file), input_arrays, output_arrays, input_shapes={\"image_tensor\":[-1,-1,-1,3]})\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/lite.py\", line 274, in from_frozen_graph\r\n    _set_tensor_shapes(input_tensors, input_shapes)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/convert_saved_model.py\", line 205, in set_tensor_shapes\r\n    tensor.set_shape(shape)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 540, in set_shape\r\n    shape = tensor_shape.TensorShape(shape)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_shape.py\", line 542, in __init__\r\n    self._dims = [as_dimension(d) for d in dims_iter]\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_shape.py\", line 542, in <listcomp>\r\n    self._dims = [as_dimension(d) for d in dims_iter]\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_shape.py\", line 482, in as_dimension\r\n    return Dimension(value)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_shape.py\", line 42, in __init__\r\n    raise ValueError(\"Dimension %d must be >= 0\" % self._value)\r\nValueError: Dimension -1 must be >= 0\r\n`\r\nit got confused.@ @aejaex ", "For input shape, you must use what is the shape of the array in your input images - if you have used input image size 299 x 299, for example, with 3 channels (RGB), then input shape is (1,299,299,3).\r\n\r\nI don't know why you used (-1,-1,-1,3).. Check your config file for the training model - what did you use for image size there? That will be your input shape.", "@cjr0106: @aejaex is correct. You should use the shape of your input images. Let us know if this is still an issue.", "yeah , i know that is the input image size, i got the training model from [https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md](url),then i use the bazel ,it got the image size as[-1,-1,-1,3], as you say ,it's the config of the training model's problem?", "There are numerous ways to get input image size, which should be prior knowledge to model training. \r\nNevertheless, there is a potential bug in bazel."]}, {"number": 22864, "title": "check_numerics", "body": "", "comments": ["I apologize, but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new). Please provide all the information it asks. Thank you.\r\n"]}, {"number": 22863, "title": "Tensorflow C++ 1.9.0 ~1.11.0 failed call to cuInit: CUresult(-1)  but libcuda.so is ok, where is the problem?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution Linux kernel 3.10.104:\r\n- **TensorFlow installed from source 1.9.0 build with gpu\r\n       configure with open cuda support and build with command as \r\n      \"bazel build -c opt --config=mkl --config=cuda --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.2 //tensorflow:libtensorflow_cc.so\"\r\n- **TensorFlow version 1.9.0:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\nUsing built-in specs.\r\nCOLLECT_GCC=gcc\r\nCOLLECT_LTO_WRAPPER=/usr/libexec/gcc/x86_64-redhat-linux/4.8.5/lto-wrapper\r\nTarget: x86_64-redhat-linux\r\nConfigured with: ../configure --prefix=/usr --mandir=/usr/share/man --infodir=/usr/share/info --with-bugurl=http://bugzilla.redhat.com/bugzilla --enable-bootstrap --enable-shared --enable-threads=posix --enable-checking=release --with-system-zlib --enable-__cxa_atexit --disable-libunwind-exceptions --enable-gnu-unique-object --enable-linker-build-id --with-linker-hash-style=gnu --enable-languages=c,c++,objc,obj-c++,java,fortran,ada,go,lto --enable-plugin --enable-initfini-array --disable-libgcj --with-isl=/builddir/build/BUILD/gcc-4.8.5-20150702/obj-x86_64-redhat-linux/isl-install --with-cloog=/builddir/build/BUILD/gcc-4.8.5-20150702/obj-x86_64-redhat-linux/cloog-install --enable-gnu-indirect-function --with-tune=generic --with-arch_32=x86-64 --build=x86_64-redhat-linux\r\nThread model: posix\r\ngcc version 4.8.5 20150623 (Red Hat 4.8.5-4) (GCC) \r\n\r\n- **CUDA/cuDNN version:/usr/local/cuda-9.0/\r\n- **GPU model and memory**:\r\nWed Oct 10 20:14:44 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 384.81                 Driver Version: 384.81                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla P40           On   | 00000000:04:00.0 Off |                    0 |\r\n| N/A   51C    P0   136W / 250W |  16817MiB / 22912MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla P40           On   | 00000000:06:00.0 Off |                    0 |\r\n| N/A   50C    P0   143W / 250W |  16817MiB / 22912MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  Tesla P40           On   | 00000000:07:00.0 Off |                    0 |\r\n| N/A   49C    P0    93W / 250W |  16817MiB / 22912MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  Tesla P40           On   | 00000000:08:00.0 Off |                    0 |\r\n| N/A   50C    P0   138W / 250W |  16817MiB / 22912MiB |     98%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   4  Tesla P40           On   | 00000000:0C:00.0 Off |                    0 |\r\n| N/A   40C    P0    56W / 250W |   2435MiB / 22912MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   5  Tesla P40           On   | 00000000:0D:00.0 Off |                    0 |\r\n| N/A   26C    P8    10W / 250W |      0MiB / 22912MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   6  Tesla P40           On   | 00000000:0E:00.0 Off |                    0 |\r\n| N/A   25C    P8    10W / 250W |      0MiB / 22912MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   7  Tesla P40           On   | 00000000:0F:00.0 Off |                    0 |\r\n| N/A   28C    P8    10W / 250W |      0MiB / 22912MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n\r\nmy problem is  that when i use c++ api  to inference\uff0cbut get error as follows:\r\n2018-10-10 20:19:47.332240: E tensorflow/stream_executor/cuda/cuda_driver.cc:397] failed call to cuInit: CUresult(-1)\r\n2018-10-10 20:19:47.332371: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: 100-88-66-85\r\n2018-10-10 20:19:47.332402: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: 100-88-66-85\r\n2018-10-10 20:19:47.332545: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 384.81.0\r\n2018-10-10 20:19:47.332661: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 384.81.0\r\n2018-10-10 20:19:47.332691: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 384.81.0\r\n2018-10-10 20:19:47.349695: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n\r\nthe c++ code as follows:\r\n\r\nint main(int argc, char const *argv[]) {\r\n    const char* devices = getenv(\"CUDA_VISIBLE_DEVICES\");\r\n    if(devices!=NULL)\r\n        LOG_INFO(\"devices:%s\", devices);\r\n    std::string model_path = \"wavenet.pb\";\r\n    tensorflow::GraphDef graph;\r\n    tensorflow::Status load_graph_status = ReadBinaryProto(tensorflow::Env::Default(), model_path.c_str(), &graph);\r\n    if (!load_graph_status.ok()) {\r\n        LOG_ERROR(\"Failed to load pb graph:%s\", load_graph_status.error_message().c_str());\r\n        return load_graph_status.code();\r\n    }\r\n    // config\r\n    tensorflow::SessionOptions options;\r\n    int num_threads_1 = 4;\r\n    int num_threads_2 = 4;\r\n    if (num_threads_1 > 0) {\r\n        options.config.set_intra_op_parallelism_threads(num_threads_1);\r\n        options.config.set_inter_op_parallelism_threads(num_threads_2);\r\n\toptions.config.mutable_gpu_options()->set_visible_device_list(\"0\");\r\n    }\r\n    // create session\r\n    std::unique_ptr<tensorflow::Session> session;\r\n    session.reset(tensorflow::NewSession(options));\r\n    tensorflow::Status session_create_status = session->Create(graph);\r\n    if (!session_create_status.ok()) {\r\n        LOG_ERROR(\"Failed to create tensorflow graph: %s\", session_create_status.error_message().c_str());\r\n        return load_graph_status.code();\r\n    }\r\n\r\n    const std::string input_tensor_name = \"Predict/mel:0\";\r\n    const std::string output_tensor_name = \"Predict/wav:0\";\r\n\r\n    int32_t batch_size = 1;\r\n    //int32_t frame_nums = (int)(24000 / 256);\r\n    int32_t frame_nums = 1;\r\n    int32_t dims = 80;\r\n    std::vector<float> spectrum;\r\n    std::vector<float> wave;\r\n    spectrum.resize(frame_nums, 0);\r\n\r\n    LOG_INFO(\"fill data to input tensor begin\");\r\n    tensorflow::Tensor input_tensor(tensorflow::DT_FLOAT, tensorflow::TensorShape({batch_size, frame_nums, dims}));\r\n    auto input_tensor_matrix = input_tensor.tensor<float, 3>();\r\n    for (int32_t x = 0; x < batch_size; ++x) {\r\n        for (int y = 0; y < frame_nums; ++y) {\r\n            for (int z = 0; z < dims; ++z) {\r\n                int32_t index = x * frame_nums * dims + y * frame_nums + z;\r\n                input_tensor_matrix(x, y, z) = spectrum[index];\r\n            }\r\n        }\r\n    }\r\n    LOG_INFO(\"fill data to input tensor end\");\r\n    std::vector<std::pair<std::string, tensorflow::Tensor>> inputs;\r\n    std::vector<tensorflow::Tensor> outputs;\r\n    inputs.push_back(std::pair<std::string, tensorflow::Tensor>(input_tensor_name, input_tensor));\r\n    LOG_INFO(\"session run begin\");\r\n    tensorflow::Status run_status = session->Run(inputs, {output_tensor_name}, {}, &outputs);\r\n    LOG_INFO(\"session run end\");\r\n    if (!run_status.ok()) {\r\n        LOG_ERROR(\"Running model failed: %s\", run_status.error_message().c_str());\r\n        return load_graph_status.code();\r\n    }\r\n    auto output_c = outputs[0].tensor<float, 3>();\r\n    int32_t length = outputs[0].dim_size(2);\r\n    wave.resize(batch_size * 1 * length);\r\n    for (int32_t x = 0; x < batch_size; ++x) {\r\n        for (int y = 0; y < 1; ++y) {\r\n            for (int z = 0; z < length; ++z) {\r\n                int32_t index = x * 1 * length + y * 1 + z;\r\n                wave[index] = output_c(x, y, z);\r\n            }\r\n        }\r\n    }\r\n    session->Close();\r\n    return 0;\r\n}\r\n\r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nExact command to reproduce\nMobile device", "I have solved this problem\uff0cjust export LD_LIBRARY_PATH of nvidia lib before the cuda"]}, {"number": 22862, "title": "Tf keras functional model with tf dataset input", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nno\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nno\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\n1.11\r\n- **Python version**:\r\n3.5.3\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI'm new to tensorflow keras and dataset. Can anyone help me understand why the following code doesn't work?\r\n\r\n    import tensorflow as tf\r\n    import tensorflow.keras as keras\r\n    import numpy as np\r\n    from tensorflow.python.data.ops import dataset_ops\r\n    from tensorflow.python.data.ops import iterator_ops\r\n    from tensorflow.python.keras.utils import multi_gpu_model\r\n    from tensorflow.python.keras import backend as K\r\n\r\n\r\n    data = np.random.random((1000,32))\r\n    labels = np.random.random((1000,10))\r\n    dataset = tf.data.Dataset.from_tensor_slices((data,labels))\r\n    print( dataset)\r\n    print( dataset.output_types)\r\n    print( dataset.output_shapes)\r\n    dataset.batch(10)\r\n    dataset.repeat(100)\r\n\r\n    inputs = keras.Input(shape=(32,))  # Returns a placeholder tensor\r\n\r\n    # A layer instance is callable on a tensor, and returns a tensor.\r\n    x = keras.layers.Dense(64, activation='relu')(inputs)\r\n    x = keras.layers.Dense(64, activation='relu')(x)\r\n    predictions = keras.layers.Dense(10, activation='softmax')(x)\r\n\r\n    # Instantiate the model given inputs and outputs.\r\n    model = keras.Model(inputs=inputs, outputs=predictions)\r\n\r\n    # The compile step specifies the training configuration.\r\n    model.compile(optimizer=tf.train.RMSPropOptimizer(0.001),\r\n              loss='categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\n    # Trains for 5 epochs\r\n    model.fit(dataset, epochs=5, steps_per_epoch=100)\r\n\r\nIt failed with the following error:\r\n\r\n    model.fit(x=dataset, y=None, epochs=5, steps_per_epoch=100)\r\n    File \"/home/wuxinyu/pyEnv/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py\", line 1510, in fit\r\n    validation_split=validation_split)\r\n    File \"/home/wuxinyu/pyEnv/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py\", line 994, in _standardize_user_data\r\n    class_weight, batch_size)\r\n    File \"/home/wuxinyu/pyEnv/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py\", line 1113, in _standardize_weights\r\n    exception_prefix='input')\r\n    File \"/home/wuxinyu/pyEnv/lib/python3.5/site-packages/tensorflow/python/keras/engine/training_utils.py\", line 325, in standardize_input_data\r\n    'with shape ' + str(data_shape))\r\n    ValueError: Error when checking input: expected input_1 to have 2 dimensions, but got array with shape (32,)\r\n\r\n\r\nAccording to tf.keras guide, I should be able to directly pass the dataset to model.fit, as this example shows:\r\n\r\n> # Input tf.data datasets\r\n\r\n> Use the Datasets API to scale to large datasets or multi-device training. Pass a tf.data.Dataset instance to the fit method:\r\n\r\n    # Instantiates a toy dataset instance:\r\n    dataset = tf.data.Dataset.from_tensor_slices((data, labels))\r\n    dataset = dataset.batch(32)\r\n    dataset = dataset.repeat()\r\n\r\n> # Don't forget to specify `steps_per_epoch` when calling `fit` on a dataset.\r\n> model.fit(dataset, epochs=10, steps_per_epoch=30)\r\nHere, the fit method uses the steps_per_epoch argument\u2014this is the number of training steps the model runs before it moves to the next epoch. Since the Dataset yields batches of data, this snippet does not require a batch_size.\r\n\r\n> Datasets can also be used for validation:\r\n\r\n    dataset = tf.data.Dataset.from_tensor_slices((data, labels))\r\n    dataset = dataset.batch(32).repeat()\r\n\r\n    val_dataset = tf.data.Dataset.from_tensor_slices((val_data, val_labels))\r\n    val_dataset = val_dataset.batch(32).repeat()\r\n\r\n    model.fit(dataset, epochs=10, steps_per_epoch=30,\r\n          validation_data=val_dataset,\r\n          validation_steps=3)\r\n\r\nWhat's the problem with my code, and what's the correct way of doing it?", "comments": ["Hello!\r\n\r\nThink you are missing one assignment step in your setup.\r\nIt is a subtle issue and has to do with how tf approaches data pipelines.\r\nIn your case it has to do with the way transforms are chained together.\r\n\r\nFor example in these lines of your code:\r\n\r\n```\r\ndataset.batch(10)\r\ndataset.repeat(100)\r\n```\r\n\r\nYou call the batch and repeat operations yet never assign them.\r\nThis creates a new Dataset object, and then just throws it away.\r\nIt also why you see that dimension error since the dataset that is passed to `fit()` only yields single examples with no batch dimension.\r\n\r\n\r\nYou want to do the same as the examples and assign the new dataset. \r\nThere are a couple of options:\r\n\r\n```\r\ndataset = dataset.batch(10)\r\ndataset = dataset.repeat(100)\r\n```\r\n\r\nor \r\n\r\n```\r\ndataset = dataset.batch(10).repeat(100)\r\n```\r\n\r\nHope this helps!", "It works.  Thank you very much."]}, {"number": 22861, "title": "Variable names created by tf.kera.Model.build() is inconsistent with that by tf.keras.Model.call()", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16.04.4 LTS\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**:  v1.11.0-0-gc19e29306c 1.11.0\r\n- **Python version**: Python 3.5.2\r\n- **Bazel version (if compiling from source)**: No\r\n- **GCC/Compiler version (if compiling from source)**: No\r\n- **CUDA/cuDNN version**: 7.2.1\r\n- **GPU model and memory**: GTX 1060 6GB\r\n- **Exact command to reproduce**: Please see the below\r\n\r\n### Describe the problem\r\ntf.keras.Model makes Variables of its weights, when its build() or call() is called first.\r\n\r\nWhile I found call() makes Variables with the prefix \"MyModel/\", build() make Variables without any prefix.\r\n\r\nThat is inconvenient to manage non-object based checkpoint.\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\n\r\nlayers = keras.layers\r\n\r\nclass Model(keras.Model):\r\n    def __init__(self, name=\"MyModel\"):\r\n        super().__init__(name=name)\r\n        self.conv1 = layers.Conv2D(32, [5,5], activation=tf.nn.relu, name=\"conv1\")\r\n        self.conv2 = layers.Conv2D(64, [5,5], activation=tf.nn.relu, name=\"conv2\")\r\n\r\n    def call(self, images):\r\n        featmap = self.conv1(images)\r\n        featmap = self.conv2(featmap)\r\n        return featmap\r\n\r\n    def inference(self, images):\r\n        return self.__call__(images)\r\n\r\n\r\nmodel = Model()\r\nflags = \"build\"\r\n\r\nif flags == \"build\":\r\n    model.build(input_shape=tf.TensorShape([None, 32, 32, 3]))\r\n\r\n    for w in model.weights:\r\n        print (w.op.name)\r\nelse:\r\n    dummy = tf.zeros([1, 32, 32, 3])\r\n    model(dummy)\r\n\r\n    for w in model.weights:\r\n        print (w.op.name)\r\n```\r\n", "comments": ["Can you please elaborate in brief on following points?\r\nDescribe the current behavior:\r\nDescribe the expected behavior:", "Sorry, I made a mistake. I cannot reproduce my issue. Please close this ticket."]}, {"number": 22860, "title": "Combined the training process and the creation of frozen graph", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->"]}, {"number": 22859, "title": "Feature request: Provide pre-compiled binary file of libtensorflow.so for RaspberryPi", "body": "Thanks to your great works for Raspberry Pi.\r\n\r\nhttps://www.tensorflow.org/install/source_rpi\r\n\r\nI hope to use libtensorflow.so for Raspberry Pi too. Could you please provide pre-compiled binary?\r\n\r\nhttps://www.tensorflow.org/install/lang_c", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "This is feature request. Should I add something?", "@tensorflowbutler \r\n\r\nHave I written custom code: No\r\nOS Platform and Distribution: Raspbian stretch on raspberry Pi\r\nTensorFlow installed from: binary\r\nTensorFlow version: latest\r\nBazel version: None\r\nCUDA/cuDNN version: None\r\nGPU model and memory: None\r\nExact command to reproduce: None\r\nMobile device: false", "perhaps this might help?\r\nhttps://github.com/huaxiaozhong1/tensorflow-lite-apps-on-raspberry-pi-3", "Thanks your comment. But it is tensorflow lite not tensorflow core.", "Unfortunately we don't have any plans to make the tensorflow precompiled lib available for the Raspberry Pi, partly because the process of creating a reusable lib isn't straightforward for the framework (we don't have a good separation between internal headers and public C++ APIs for example). We'd be happy to link to any external projects that provide these, and it's possible that you might be able to reuse the artifacts from inside the pip install, but I don't think that's going to be very stable.", "As you know, tensorflow is not only for python. Go use libtensorflow.so."]}, {"number": 22858, "title": "Fixed extract_volume_patches doc formatting issue", "body": "Fixed broken formatting.\r\nPinging @martinwicke in case changing the documentation requires another API review", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "Closing due to weird CLA issue.", "CLAs look good, thanks!\n\n<!-- ok -->", "Reopening as the CLA issue was fixed.", "@MarkDaoust I thought the summary didn't necessarily have to be in a single line, which is why I submitted it like that initially. But it looks like that's not the case; the documentation generated right now has terribly broken formatting. The newly generated doc with this PR actually is formatted nicely and looks like:\r\n\r\n# tf.extract_volume_patches\r\n\r\n``` python\r\ntf.extract_volume_patches(\r\n    input,\r\n    ksizes,\r\n    strides,\r\n    padding,\r\n    name=None\r\n)\r\n```\r\n\r\n\r\n\r\nDefined in generated file: `tensorflow/python/ops/gen_array_ops.py`.\r\n\r\nExtract `patches` from `input` and put them in the \"depth\" output dimension. 3D extension of `extract_image_patches`.\r\n\r\n#### Args:\r\n\r\n* <b>`input`</b>: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`. 5-D Tensor with shape `[batch, in_planes, in_rows, in_cols, depth]`.\r\n* <b>`ksizes`</b>: A list of `ints` that has length `>= 5`. The size of the sliding window for each dimension of `input`.\r\n* <b>`strides`</b>: A list of `ints` that has length `>= 5`. 1-D of length 5. How far the centers of two consecutive patches are in `input`. Must be: `[1, stride_planes, stride_rows, stride_cols, 1]`.\r\n* <b>`padding`</b>: A `string` from: `\"SAME\", \"VALID\"`. The type of padding algorithm to use.\r\n\r\n    We specify the size-related attributes as: \r\n\r\n    ```python\r\n          ksizes = [1, ksize_planes, ksize_rows, ksize_cols, 1]\r\n          strides = [1, stride_planes, strides_rows, strides_cols, 1]\r\n    ```\r\n* <b>`name`</b>: A name for the operation (optional).\r\n\r\n\r\n#### Returns:\r\n\r\nA `Tensor`. Has the same type as `input`.\r\n", "Great, thanks for clarifying.", "@MarkDaoust Thanks for the merge! Do I have to submit a separate PR to get this pushed into `r1.12` or will that be taken care of internally by the TensorFlow team?", "We only cherry-pick critical bug fixes into the releases at the RC stage.\nThis will be in the next release.\n"]}, {"number": 22857, "title": "tf.estimator.train_and_evaluate auc value is different from tf.estimator.evaluate with same model", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:no \r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**: 'v1.11.0-0-gc19e29306c', '1.11.0'\r\n- **Python version**:2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\n\r\nI use tf.Estimator for training my models.\r\nHere is part of my source code:\r\n\r\n### Source code/log\r\n**tf.estimator.train_and_evaluate**\r\n\r\n\r\n```\r\n for n in range(num_epochs):\r\n    train_spec = tf.estimator.TrainSpec(input_fn=lambda: input_fn(...))\r\n    eval_spec = tf.estimator.EvalSpec(input_fn =lambda: input_fn(...))\r\n    tf.estimator.train_and_evaluate(model, train_spec, eval_spec)\r\n\r\n\r\nINFO:tensorflow:Saving dict for global step 18621: auc = 0.6128701, global_step = 18621, loss = 0.17829555\r\nINFO:tensorflow:Saving dict for global step 37242: auc = 0.6212857, global_step = 37242, loss = 0.17826068\r\nINFO:tensorflow:Saving dict for global step 55863: auc = 0.6261746, global_step = 55863, loss = 0.17617485\r\nINFO:tensorflow:Saving dict for global step 74484: auc = 0.630533, global_step = 74484, loss = 0.17646796\r\nINFO:tensorflow:Saving dict for global step 93105: auc = 0.63444453, global_step = 93105, loss = 0.17552255\r\nINFO:tensorflow:Saving dict for global step 111726: auc = 0.63371396, global_step = 111726, loss = 0.17790207\r\nINFO:tensorflow:Saving dict for global step 130347: auc = 0.62660295, global_step = 130347, loss = 0.17602158\r\nINFO:tensorflow:Saving dict for global step 148968: auc = 0.6290031, global_step = 148968, loss = 0.17708226\r\nINFO:tensorflow:Saving dict for global step 167589: auc = 0.6234657, global_step = 167589, loss = 0.17724389\r\nINFO:tensorflow:Saving dict for global step 186210: auc = 0.62800914, global_step = 186210, loss = 0.17702612\r\n```\r\n\r\n### Source code/log\r\n\r\n**tf.estimator.evaluate**\r\n\r\n```\r\nckpts = [18621, 37242, 55863, 74484, 93105, 111726, 130347, 148968, 167589, 186210]\r\n        for epoch, ckpt in enumerate(ckpts):\r\n            model.evaluate(\r\n                input_fn=lambda: input_fn(...),\r\n                checkpoint_path=FLAGS.model_dir + \"/model.ckpt-\" + str(ckpt))\r\n\r\nINFO:tensorflow:Saving dict for global step 18621: auc = 0.6088399, global_step = 18621, loss = 0.1782806\r\nINFO:tensorflow:Saving dict for global step 37242: auc = 0.61857945, global_step = 37242, loss = 0.17766058\r\nINFO:tensorflow:Saving dict for global step 55863: auc = 0.6241194, global_step = 55863, loss = 0.17692827\r\nINFO:tensorflow:Saving dict for global step 74484: auc = 0.627106, global_step = 74484, loss = 0.17647153\r\nINFO:tensorflow:Saving dict for global step 93105: auc = 0.62898576, global_step = 93105, loss = 0.17620046\r\nINFO:tensorflow:Saving dict for global step 111726: auc = 0.62996054, global_step = 111726, loss = 0.17710653\r\nINFO:tensorflow:Saving dict for global step 130347: auc = 0.63045925, global_step = 130347, loss = 0.17653853\r\nINFO:tensorflow:Saving dict for global step 148968: auc = 0.6306327, global_step = 148968, loss = 0.17709586\r\nINFO:tensorflow:Saving dict for global step 167589: auc = 0.6306394, global_step = 167589, loss = 0.1774284\r\nINFO:tensorflow:Saving dict for global step 186210: auc = 0.6304847, global_step = 186210, loss = 0.17686236\r\n```\r\n\r\nmy question is why same model but different auc value. To validate which is right. I also try auc api by sklearn.The result is that auc value same with tf.estimator.evaluate \r\n\r\n### Source code/log\r\n```\r\n ckpts = [18621, 37242, 55863, 74484, 93105, 111726, 130347, 148968, 167589, 186210]\r\n        for epoch, ckpt in enumerate(ckpts):\r\n            for data_from, data_mode in zip([FLAGS.valid_data], ['test']):\r\n                preds = model.predict(\r\n                    input_fn=lambda: input_fn.(...),\r\n                    checkpoint_path=FLAGS.model_dir + \"/model.ckpt-\" + str(ckpt))\r\n                label_list = []\r\n                pred_list = []\r\n                for i, prob in enumerate(preds):\r\n                    pred_list.append(prob['prob'][0])\r\n                    label_list.append(prob['true_label'])\r\n                auc_local = sklearn.metrics.roc_auc_score(label_list, pred_list)\r\n                print(\"%s sklearn.auc is :\", (epoch, data_mode, auc_local))\r\n\r\n\r\n%s auc is : (0, 'test', 0.608910481886984)\r\n%s auc is : (1, 'test', 0.6186364169158124)\r\n%s auc is : (2, 'test', 0.6242379534016801)\r\n%s auc is : (3, 'test', 0.6272265511554851)\r\n%s auc is : (4, 'test', 0.6291364474305191)\r\n%s auc is : (5, 'test', 0.6300167648655547)\r\n%s auc is : (6, 'test', 0.6305495577793957)\r\n%s auc is : (7, 'test', 0.6307263729168348)\r\n%s auc is : (8, 'test', 0.6307389062228584)\r\n%s auc is : (9, 'test', 0.6306225969185617)\r\n```\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["@wt-huang .I suspect this is a bug? Or is there a problem with my use? Could you give me some advice~~thanks a lot .", "@geyuxiao `tf.estimator.train_and_evaluate` does evaluation during training and is essentially alternating between `tf.estimator.train` and `tf.estimator.evalute`. However the ckpts in `tf.estimator.train`_and `tf.estimator.evalute` may have a slight delay in reporting. \r\n\r\nIn this case, use tf.estimator.evalute and the corresponding auc values if you only care about final evaluation results.", "@wt-huang Thank you very much for your reply. You mentioned that there will be a slight delay in reporting. What is the reason for this delay? Or how to avoid this delay. I hope to get a further answer. Because for me, train_and_evaluate is more efficient, and then evaluate after the training. Look forward to your reply.", "@geyuxiao The delay between tf.estimator.train_and tf.estimator.evalute depends on your model and datasets, among other factors.\r\nThe delay is usually very small so the difference in loss should be minimal and is unlikely to have statistical and practical significance.", "Closing as this resolved, feel free to reopen if problem persists", "There are no errors, but please note the parameters here.\r\n\r\n`eval_spec = tf.estimator.EvalSpec( input_fn=lambda: input_fn.read_fn(validation_data, steps=None )`\r\n\r\nsteps = None represent all test sample data.\r\nIf you don't set it, tf defaults to 100.   \r\nI did not set this parameter before, resulting in 100 samples being evaluated each time, so it is not compatible with the overall sample. So not the delay you mentioned @wt-huang .\r\nAnyway, thank you for your previous reply.", "The same problem occurs for me.\r\nFor `train_and_evaluate`, I have the following codes:\r\n```\r\n    train_input_fn = input_fn_builder(True, cfg, train_examples, label_to_id, id_to_label, vocab)\r\n    dev_examples, _ = processor.get_dev_examples(FLAGS.data_dir)\r\n    eval_input_fn = input_fn_builder(False, cfg, dev_examples, label_to_id, id_to_label, vocab)\r\n    train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=num_train_steps)\r\n    eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn, throttle_secs=60)\r\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n```\r\nand I get the log as follows:\r\n`Saving dict for global step 223632: acc = 0.92796874, global_step = 223632, loss = 52.85372`\r\nFor `estimator.evaluate`, I use the same model trained during `train_and_evaluate`, the codes are as follows:\r\n```\r\n    dev_examples, _ = processor.get_dev_examples(FLAGS.data_dir)\r\n    eval_input_fn = input_fn_builder(False, cfg, dev_examples, label_to_id, id_to_label, vocab)\r\n    result = estimator.evaluate(input_fn=eval_input_fn)\r\n```\r\nBut the log gives a different and poor result:\r\n`Saving dict for global step 223632: acc = 0.90173215, global_step = 223632, loss = 73.50448`\r\nThis really makes me confused. @wt-huang ", "update:\r\nI find the difference is due to the different parameter `steps` in `train_and_evaluate` and `estimator.evaluate`.\r\n`steps` in `train_and_evaluate` has a default value of `100`, but the default value of it in `estimator.evaluate` is `None`.\r\nAfter I set it to `100`, this problem is resolved. But I still do not know the reason, and I wonder whether whether this result is identical to the `estimator.predict` or not.\r\n\r\n", "It seems that `steps` in `estimator.evaluate` controls the proportion of evaluation data, but what exactly does `steps` mean?", "> It seems that `steps` in `estimator.evaluate` controls the proportion of evaluation data, but what exactly does `steps` mean?\r\n\r\nSorry for this stupid question. I know it now.\r\none `step` in this situation means an evaluation for a batch of data.", "> There are no errors, but please note the parameters here.\r\n> \r\n> `eval_spec = tf.estimator.EvalSpec( input_fn=lambda: input_fn.read_fn(validation_data, steps=None )`\r\n> \r\n> steps = None represent all test sample data.\r\n> If you don't set it, tf defaults to 100.\r\n> I did not set this parameter before, resulting in 100 samples being evaluated each time, so it is not compatible with the overall sample. So not the delay you mentioned @wt-huang .\r\n> Anyway, thank you for your previous reply.\r\n\r\nhi , I have a quetion. \"resulting in 100 samples being evaluated each time\" , here \"100 samples \" means 100*batchSize samples or just 100  samples ?", "I had faced a similar case where the train and evaluate results were so different. it turned out that it had to do with batch normalization in my code. ", "@mhajiaghayi Can you please suggest the solution for fixing the batchnormalization issue?", "> I had faced a similar case where the train and evaluate results were so different. it turned out that it had to do with batch normalization in my code.\r\n\r\nI met the same problem, have you fixed it? "]}, {"number": 22856, "title": "[aarch64] make aws sdk work on aarch64", "body": "`bazel build //tensorflow/tools/pip_package:build_pip_package`\r\nrequires AWS SDK by default. but platform part was not built\r\non aarch64", "comments": ["> `bazel build //tensorflow/tools/pip_package:build_pip_package`\r\n> requires AWS SDK by default. but platform part was not built\r\n> on aarch64\r\n\r\nThank you for this changes. Worked perfectly", "Nagging Reviewer @caisq, @gunan: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "How to apply the patch on TF 1.12, we can not find the file \"third_party/aws/BUILD.bazel\"\r\n\r\n", "@wormwang don't remember if AWS is mandatory in 1.12, if no or you don't need AWS, simply skip it. If you really need it, check `third_party/aws.BUILD`."]}, {"number": 22855, "title": "[TF1.10][TRT5.0] TFTRT is still using libnvinfer.so.4 not 5, importing tensorflow.contrib.tensorrt failed", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS 7.0\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NO\r\n- **TensorFlow installed from (source or binary)**: pip\r\n- **TensorFlow version (use command below)**: 1.10\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: NO\r\n- **GCC/Compiler version (if compiling from source)**: NO\r\n- **CUDA/cuDNN version**: CUDA9.0 cuDNN 7.0.5\r\n- **GPU model and memory**: TitanXP\r\n- **Exact command to reproduce**: 'import tensorflow.contrib.tensorrt as trt'\r\n\r\n### Describe the problem\r\nNew TensorRT version 5.0 is released from Nvidia official site. I downloaded it and try to import it inside tensorflow, which is called \"TFTRT\".\r\nBut below error popped out:\r\n```\r\n/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n**** Failed to initialize TensorRT. This is either because the TensorRT installation path is not in LD_LIBRARY_PATH, or because you do not have it installed. If not installed, please go to https://developer.nvidia.com/tensorrt to download and install TensorRT ****\r\nTraceback (most recent call last):\r\n  File \"run_uff.py\", line 4, in <module>\r\n    from tensorflow.contrib.tensorrt import tensorrt as trt\r\n  File \"/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/tensorflow/contrib/tensorrt/__init__.py\", line 34, in <module>\r\n    raise e\r\ntensorflow.python.framework.errors_impl.NotFoundError: libnvinfer.so.4: cannot open shared object file: No such file or directory\r\n\r\n```\r\nSeems the tensorflow is not updated for new TensorRT. Hope @samikama can take a look into this issue.\r\n\r\nThanks,", "comments": ["@oscarriddle, \r\n\r\nTF 1.10 is released long before TRT5.0 is released so it is not possible to have a prebuild pip TF package linking TRT5.0.  If you want, you can rebuild TF and use TRT5.0. \r\n", "But the version TensorFlow-1.12.0 still gives the same error, what's wrong? The offical documents shows that the TensorRT-5.1.5 is compatible for the TensorFlow 1.12.0. It's weird.\r\n\r\nCompatibility\r\n\u2023 TensorRT 5.1.5 has been tested with the following:\r\n\u2023 cuDNN 7.5.0\r\n\u2023 TensorFlow 1.12.0\r\n\u2023 PyTorch 1.0\r\n\u2023 ONNX 1.4.1\r\n\r\n\r\n>>> import tensorflow.contrib.tensorrt as trt\r\n**** Failed to initialize TensorRT. This is either because the TensorRT installation path is not in LD_LIBRARY_PATH, or because you do not have it installed. If not installed, please go to https://developer.nvidia.com/tensorrt to download and install TensorRT ****\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/media/huaxin/tcl1/asr/yujiangling/lib/python3.6/site-packages/tensorflow/contrib/tensorrt/__init__.py\", line 34, in <module>\r\n    raise e\r\n  File \"/media/huaxin/tcl1/asr/yujiangling/lib/python3.6/site-packages/tensorflow/contrib/tensorrt/__init__.py\", line 25, in <module>\r\n    from tensorflow.contrib.tensorrt.python import *\r\n  File \"/media/huaxin/tcl1/asr/yujiangling/lib/python3.6/site-packages/tensorflow/contrib/tensorrt/python/__init__.py\", line 22, in <module>\r\n    from tensorflow.contrib.tensorrt.python.ops import trt_engine_op\r\n  File \"/media/huaxin/tcl1/asr/yujiangling/lib/python3.6/site-packages/tensorflow/contrib/tensorrt/python/ops/trt_engine_op.py\", line 32, in <module>\r\n    resource_loader.get_path_to_datafile(\"_trt_engine_op.so\"))\r\n  File \"/media/huaxin/tcl1/asr/yujiangling/lib/python3.6/site-packages/tensorflow/contrib/util/loader.py\", line 56, in load_op_library\r\n    ret = load_library.load_op_library(path)\r\n  File \"/media/huaxin/tcl1/asr/yujiangling/lib/python3.6/site-packages/tensorflow/python/framework/load_library.py\", line 60, in load_op_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename)\r\ntensorflow.python.framework.errors_impl.NotFoundError: libnvinfer.so.4: cannot open shared object file: No such file or directory"]}, {"number": 22854, "title": "TensorRT INT8 calibration doesn't work with TF r1.12 and TRT 5RC", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:n/a\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a\r\n- **TensorFlow installed from (source or binary)**: source \r\n- **TensorFlow version (use command below)**: r1.12\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: 0.16.1\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**:9/7.1\r\n- **GPU model and memory**:1080ti\r\n- **Exact command to reproduce**:\r\n\r\nFollow workflow from here,  https://devblogs.nvidia.com/tensorrt-integration-speeds-tensorflow-inference/\r\nError at the following piece of code\r\n`trt_graph=trt.calib_graph_to_infer_graph(calibGraph)\r\n`\r\nLog:\r\nFile \"/home/dhingratul/.virtualenvs/tf_trt_source_trt5rc_tf1_12/local/lib/python3.5/site-packages/tensorflow/contrib/tensorrt/python/trt_convert.py\", line 349, in calib_graph_to_infer_graph\r\nfor n in calibration_graph_def.node:\r\nAttributeError: 'Graph' object has no attribute 'node'\r\n\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@dhingratul, Are you passing a graph object or a graphdef object? The method expects a graphdef.", "@samikama This is all what I have tried\r\n1. Create int8 graphdef with `trt.create_inference_graph` and use `trt.calib_graph_to_infer_graph(trt_graph)`\r\n--> FailedPrecondition Error -- Need to run graph with calib data\r\n2. Create int8 graphdef as above, import as graph, run it with calib data, use `trt.calib_graph_to_infer_graph(trt_graph)`\r\n--> 'Graph' object has no attribute 'node' \r\n3. Create int8 graphdef as above, import as graph, run it with calib data, write it out as .pb file with `tf.train.write_graph()`, import graphdef and pass it to `trt.calib_graph_to_infer_graph(trt_graph)`\r\n--> FailedPrecondition Error -- Need to run graph with calib data\r\n\r\nFor running the graph through calibration data, i need to import it as graph, I don't know if there is a right way to get back to graphdef apart from 3", "@dhingratul, are you running calib_to_infer_graph in the same process? if you exit the process between calibration and baking of the calibration? Also you need to pass graph that returned to you from trt.create_inference_graph() in the first step to calib_graph_to_infer_graph() in the third step not the graph from tf.train.write_graph(). You *need* to import it as graph to be able to run. \r\n1. create inference graph\r\n2. run it with calibration data \r\n3. pass the graph_def returned in 1 to calib_graph_to_infer_graph(). you can discard the graph you run in step 2, it is only used for collecting calibration data.", "@samikama I get this workflow, but In order to run calibration on graphdef generated in 1, I need to import the graphdef generated as a graph and then use sess.run to complete calibration. Now I have a graph, not a graphdef, so the only way to pass graphdef to the calib_graph_to_infer_graph() is to export it as .pb and import graphdef. I donot know how to convert a graph to a graphdef to go from step 2 to step3", "@dhingratul, You are trying to pass the graph in step 2 in to the step 3. Please pass the graphdef object that you created in step 1 and used to import into the graph, not the serialized graphdef of the graph in the second step.", "@samikama Based on your recommendation, this is the workflow I have, but i am facing a weird CUBLAS issue now\r\n`2018-10-12 13:15:52.338802: E tensorflow/stream_executor/cuda/cuda_blas.cc:652] failed to run cuBLAS routine cublasSgemm_v2: CUBLAS_STATUS_INTERNAL_ERROR\r\n`\r\n\r\n```\r\nfid = \"model.pb\"\r\noutput_nodenames = 'op1,op2,op3'\r\noutput_node = list(output_nodenames.split(\",\"))\r\ng = load_graph(fid)\r\nwith tf.Session(graph=g) as sess:\r\n\t# Step 1 -- Create Inference graph\r\n    trt_graph = trt.create_inference_graph(\r\n    input_graph_def=tf.get_default_graph().as_graph_def(),\r\n    outputs=output_node,\r\n    max_batch_size=20000,\r\n    max_workspace_size_bytes=1 << 25,\r\n    precision_mode=\"INT8\",  # TRT Engine precision \"FP32\",\"FP16\" or \"INT8\"\r\n    minimum_segment_size=2  # minimum number of nodes in an engine\r\n    )\r\n    # Step 2 -- Calibration\r\n    tf.import_graph_def(\r\n    trt_graph,\r\n    name='', #DEBUG\r\n\t)\r\n    num_samples = 10\r\n    np.random.seed(0)\r\n    ip1_data = np.random.rand(num_samples,700,800,6).astype(np.float32)\r\n    ip1 = tf.get_default_graph().get_tensor_by_name(\"ip1:0\")\r\n\r\n    ip2_data = np.random.rand(4).astype(np.float32)\r\n    ip2 = tf.get_default_graph().get_tensor_by_name(\"ip2:0\")\r\n\r\n    ip3_data = np.random.rand(20000,6).astype(np.float32)\r\n    ip3 = tf.get_default_graph().get_tensor_by_name(\"ip3:0\")\r\n\r\n    ip4_data = np.random.rand(20000,4).astype(np.float32)\r\n    ip4 = tf.get_default_graph().get_tensor_by_name(\"ip4:0\")\r\n    out1 = tf.get_default_graph().get_tensor_by_name(\"op1:0\")\r\n    out2 = tf.get_default_graph().get_tensor_by_name(\"op2:0\")\r\n    out3 = tf.get_default_graph().get_tensor_by_name(\"op3:0\")\r\n\r\n    for i in range(num_samples):\r\n        start_time = timeit.default_timer()\r\n        _ = sess.run([out1, out2, out3], feed_dict={ip1:ip1_data[i], ip2:ip2_data, ip3:ip3_data, ip4:ip4_data})\r\n    # Step 3 -- pass through calib_graph_to_infer_graph\r\n    trt_graph = trt.calib_graph_to_infer_graph(trt_graph)\r\n    with tf.gfile.GFile(\"trt.pb\", \"wb\") as f:\r\n        f.write(trt_graph.SerializeToString())\r\n```\r\n ", "@dhingratul You may want to check whether there are other processes running on GPU. ", "@wt-huang No other processes running on the GPU", "@dhingratul Could you try to install TensorFlow from binary instead? Also use cuDNN 7.3 and Python 3.6. Make sure that cuBLAS library are correctly installed. You can also provide your env by running the script in the issue template.\r\n\r\n", "@wt-huang I have TensorRT tarballs, not .deb hence i don't know how to install tf with pip and provide the correct path to my TRT. Can you expand on this", "@dhingratul One thing I found very useful is to first check if an `TRTEngineOp` has been created:\r\n\r\n```python\r\n    trt_engine_opts = len([1 for n in trt_graph.node if str(n.op) == 'TRTEngineOp'])\r\n    print('TRT Engine Op: {}'.format(trt_engine_opts))\r\n    assert trt_engine_opts > 0, 'No TRT Engine Ops!'\r\n```", "@benjamintanweihao Had that been the case, i would have gotten error like this, https://github.com/tensorflow/tensorflow/issues/21850#issuecomment-442153592", "Hi, any resolutions to this? I am getting the same issue with nvidia docker 19.01-py2", "@dhingratul Is this still an issue?\r\n@gsrujana89  Can you please post a [new issue](https://github.com/tensorflow/tensorflow/issues/new/choose) and provide all the information asked by the template? Thanks!", "I will have to reproduce the issue with the TRT 5GA, it was still an issue until TRT 5RC", "Still having problem of `CUBLAS_STATUS_INTERNAL_ERROR cublasSgemm`", "I have same problem with tf 1.15 tensorrt 6.\r\nCalibrate step cost all of the memory and error as following:\r\n E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:41] DefaultLogger ../rtSafe/safeContext.cpp (105) - Cudnn Error in initializeCommonContext: 4 (Could not initialize cudnn, please check cudnn installation.)", "Hi @dhingratul ! \r\nIt seems you are using older versions(1.x versions) of Tensorflow. Have you checked these threads from  latest versions(TF 2.6/2.7) yet?[link1](https://blog.tensorflow.org/2021/01/leveraging-tensorflow-tensorrt-integration.html),[link2](https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html). Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 22853, "title": "tf.keras.Model overrides 'self' behaviour with lists", "body": "- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 18.04.1 LTS\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nN/A\r\n- **TensorFlow installed from (source or binary)**:\r\npip3 install tensorflow-gpu\r\n- **TensorFlow version (use command below)**:\r\nv1.11.0-0-gc19e29306c 1.11.0\r\n- **Python version**:\r\nPython 3.6.5\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **GCC/Compiler version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\nV9.0.176\r\n- **GPU model and memory**:\r\nGTX 1070, 6GB VRAM\r\n- **Exact command to reproduce**:\r\npython3 codebelow.py\r\n\r\n### Describe the problem\r\nI am not familiar with ListWrapper(), but it is being applied to all list variables created with self when my class inherits from tf.keras.Model. This is bad because it is causing an IndexError when I use it in certain functions, or even by just passing it through my Tensorflow model (I am using eager execution). I don't know whether this is a bug or if it is intended, but if this is supposed to happen, I think documentation should be added to explain this at https://www.tensorflow.org/api_docs/python/tf/keras/models/Model. I have included a smallest working example below.\r\n\r\n### Source code / logs\r\nCode:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nclass my_class(tf.keras.Model):\r\n\r\n    def __init__(self):\r\n        super(my_class, self).__init__()\r\n\r\n        self.x = [0]\r\n        print(self.x)\r\n\r\nmodel = my_class()\r\n```\r\n\r\nOutput:\r\n`ListWrapper([0])`\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nMobile device", "Could you be more specific about the issue it's causing? List wrappers are used to support restore on create for variables.", "With more investigation, `tf.enable_eager_execution` appears to be involved as well. I believe this to be a compatibility issue with `tf.keras.Model` and `tf.enable_eager_execution`. I am unable to pass data contained within a ListWrapper() into my model, as well as in other functions, while both of those are present.\r\nThe following code runs without issue:\r\n```\r\nimport tensorflow as tf\r\n\r\nclass my_class(object):\r\n\r\n    def __init__(self):\r\n        super(my_class, self).__init__()\r\n\r\n        self.model = tf.keras.layers.Dense(3)\r\n        # Specify input size\r\n        self.model.build((32, 32, 1))\r\n    \r\n    def forward(self):\r\n        image = tf.zeros((32, 32, 1))\r\n        self.images = []\r\n        self.images.append(image)\r\n        self.model(self.images)\r\n\r\nmodel = my_class()\r\nmodel.forward()\r\n```\r\nAdding tf.enable_eager_execution() OR tf.keras.Model does not break this program.\r\nHowever, adding tf.enable_eager_execution() AND tf.keras.Model causes an ugly error:\r\n```\r\nimport tensorflow as tf\r\n\r\ntf.enable_eager_execution()\r\n\r\nclass my_class(tf.keras.Model):\r\n\r\n    def __init__(self):\r\n        super(my_class, self).__init__()\r\n\r\n        self.model = tf.keras.layers.Dense(3)\r\n        # Specify input size\r\n        self.model.build((32, 32, 1))\r\n    \r\n    def forward(self):\r\n        image = tf.zeros((32, 32, 1))\r\n        self.images = []\r\n        self.images.append(image)\r\n        self.model(self.images)\r\n\r\nmodel = my_class()\r\nmodel.forward()\r\n```\r\n```\r\n2018-10-12 18:17:23.199858: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-10-12 18:17:23.394365: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties:\r\nname: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.683\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 7.93GiB freeMemory: 5.87GiB\r\n2018-10-12 18:17:23.394407: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\r\n2018-10-12 18:17:23.630373: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-10-12 18:17:23.630403: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0\r\n2018-10-12 18:17:23.630410: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N\r\n2018-10-12 18:17:23.630591: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5641 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 21, in <module>\r\n    model.forward()\r\n  File \"test.py\", line 18, in forward\r\n    self.model(self.images)\r\n  File \"/home/jpatts/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 769, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)  File \"/home/jpatts/.local/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py\", line 936, in call\r\n    inputs = ops.convert_to_tensor(inputs, dtype=self.dtype)\r\n  File \"/home/jpatts/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1048, in convert_to_tensor\r\n    as_ref=False)\r\n  File \"/home/jpatts/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1144, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/home/jpatts/.local/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 971, in _autopacking_conversion_function\r\n    return _autopacking_helper(v, dtype, name or \"packed\")\r\n  File \"/home/jpatts/.local/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 893, in _autopacking_helper\r\n    return gen_array_ops.pack(list_or_tuple, name=name)\r\n  File \"/home/jpatts/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 4713, in pack\r\n    _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: OpKernel 'Pack' has constraint on attr 'T' not in NodeDef '{{node Pack}} = Pack[N=0, axis=0]()', KernelDef: 'op: \"Pack\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } } host_memory_arg: \"values\" host_memory_arg: \"output\"' [Op:Pack] name: packed\r\n```", "Makes sense, presumably we have a PyList_CheckExact or equivalent somewhere in TFE_Py_Execute. I can reproduce and will see about a fix.", "Thank you for the report. The pack error should be fixed now, but please let me know if there are other issues around tracking/ListWrapper.", "@allenlavoie This Bug also occurs in TF version 2.0.0-dev20190328. I just built the docker container and all my lists are ListWrappers.", "@2649 to be clear, the existence of ListWrappers is intended behavior. Them causing problems is a bug. Are they causing issues?", "@allenlavoie I ran into the problem that adding a `ListWrapper` and `TensorShape` results in the error `TypeError: 'TensorShape' object does not support item assignment`. Explicitly converting either of the two objects to a list works fine, but it's a bit confusing behavior. Not sure why `ListWrapper` would try to modify the object you're adding with it.", "@Danmou thanks for the report. I've sent a fix for review; I think we can just be more lazy about re-wrapping the result of operators in ListWrapper.", "Just reporting: as of 2021, using TF version 2.4.1 (stable) and ran into the same issue. My code, while different, uses the same dynamic of feeding options as a list (of dicts, actually). And the code won't recognize. My setting and settings are:\r\nOS: Ubuntu 20.04\r\nTF_version: 2.4.1\r\nEager_Mode: activated by default (i'm using it to debug my custom layers)\r\nIDE: (docker)Jupyter Notebook\r\nContainer: tensorflow/tensorflow:2.4.1-gpu-jupyter\r\n\r\nI was able to model the layer without the options as list, but it was a tedious process, the code became unreadable but works. I need this possibility to configure my layer with a list since I will be feeding all those layers and stuff to a Genetic Algorithm to optimize my topology. Would love a solution.\r\n\r\nA workaround for me was to disable eager_execution by running:\r\n`from tensorflow.python.framework.ops import disable_eager_execution`\r\n`disable_eager_execution`", "@n4ndoz eager execution doesn't change the `self.foo = list(...)` behavior. I'd file another bug with a reproduction.", "@allenlavoie I actually ended up solving this issue refactoring my code, is it good to delete the comment? (I'm actually pretty noob with the GitHub etiquette)"]}, {"number": 22852, "title": "TensorFlow C++ API is significantly slower than Python API in inference", "body": "System information\r\n***Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo\r\n***OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Kernel 4.4.103, LUbuntu 16.04\r\n***TensorFlow installed from (source or binary):\r\nPython API is the official release;  C++ API was complied via Makefile with all available optimization flags (linked as a static library)\r\n***TensorFlow version (use command below):\r\n1.10.1\r\n***Python version:\r\n2.7\r\n***CUDA/cuDNN version:\r\nOnly CPU no GPU\r\n***Bazel version:\r\ndid not use Bazel to build \r\n***GPU model and memory:\r\ndid not use GPU for inference \r\n***Mobile device:\r\nplatform with a RK3399 ARM processor\r\n\r\nSteps to reproduce:\r\n1. Trained a MobileNetSSDv1 model via Google Detection API (Python);\r\n2. Froze the graph via the official tool provided by Detection API with batch_size = 1, and the input tensor size was [1, 150, 150, 3];\r\n3. Used the frozen graph (.pb file) along with TensorFlow Python and C++ API for inference, respectively. All C++ optimization flags on;\r\n4. The inference time with Python API was ~200 ms while C++ API takes ~550 ms. \r\n\r\nAny thoughts would be appreciated!", "comments": ["Is this after ignoring the first few inferences? They are usually very time-consuming.", "Yes, performance was measured after warming up. ", "Could be model/hparam specific, for my model I see equivalent inference times with C++ API and Python", "Does your model run on an ARM processor?  As I understand, the Python API also calls the same machine code with the C++ API.  So it makes sense that the inference time is similar.", "Ah! Missed that part, my model runs on GPU. ", "If you are referring to the [TensorFlow Object Detection API](https://github.com/tensorflow/models/tree/master/research/object_detection), can you file an issue in that repo instead?", "> If you are referring to the [TensorFlow Object Detection API](https://github.com/tensorflow/models/tree/master/research/object_detection), can you file an issue in that repo instead?\r\n\r\n@RothLuo  Can you pls answer ?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 22851, "title": "Ensure all bazel options are incorporated during Intel mkl builds.", "body": "Prior to this fix, mkl devel based container builds would only pickup bazel options defined in `parameterized_docker_build.sh` and ignore anything produced from the top-level `configure.py` script.  As a result default options like AWS/GCP and HDFS support would end up not being included.\r\n\r\nNote that this fix should also remedy the nightly mkl whl build failures that were introduced after recent `.bazelrc` related changes.", "comments": ["@drpngx @gunan Thumbs up from me. Fixes issues caused by https://github.com/tensorflow/tensorflow/pull/21374. As seen here: https://tensorflow-ci.intel.com/job/tensorflow-mkl-build-whl-nightly/125/consoleFull", "We also need these changes reflected under the new dockerfiles (if we have an MKL image there).\r\nChecking with @angersson for that.", "@gunan The way the partials are designed, there isn't an obvious MKL modification necessary. `--config=mkl` could be specified in a .bazelrc file that gets edited on the host and then copied into the container from the host ala \r\n\r\n> COPY .bazelrc /root/.bazelrc\r\n\r\nor it could be included in the script that is run at container launch. Preferences? We have this work item planned for this quarter. ", "@scttl do you understand why it's not picked up by bazel, when it's next to `WORKSPACE` as the documentation says?", "Yes.  Per the [Dockerfile](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/Dockerfile.devel-mkl) the build happens via `RUN tensorflow/tools/ci_build/builds/configured CPU bazel --bazelrc=/root/.bazelrc` so its looking at `/root/.bazelrc` for options.\r\n\r\nThis file gets built from the `COPY .bazelrc /root/.bazelrc` line and the `.bazelrc` being copied into the container is the one on the **host** constructed as part of [this line](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/parameterized_docker_build.sh#L373) in `parameterized_docker_build.sh`.\r\n\r\nThe content of `TF_BAZEL_BUILD_OPTIONS` is being set based on [this line](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/linux/mkl/build-dev-container.sh#L39) and then [this line for avx2](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/linux/mkl/build-dev-container.sh#L73) of `build-dev-container.sh` so the entire contents of `/root/.bazelrc` being passed to the build will look something like:\r\n```\r\nbuild --config=mkl --copt=-march=haswell --copt=-mtune=broadwell --copt=-O3 --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0\r\n```\r\n\r\nand the local in-container `.bazelrc` (and `.tf_configure.bazelrc`) generated as output from [this line](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/Dockerfile.devel-mkl#L117) are completely ignored."]}, {"number": 22850, "title": "Tensorflow 1.11 breaks simple keras estimator: global step does not increment", "body": "\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: running on colab.sandbox.google.com\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.11\r\n- **Python version**: 3.6\r\n\r\n### Describe the problem\r\n\r\nThe below code illustrates a minimal keras model that is then converted to an estimator using tf.keras.estimator.model_to_estimator() and and run using tf.estimator.train_and_evaluate().\r\n\r\nThe code runs properly up until Tensorflow 1.10, however upon upgrading to Tensorflow 1.11 the global step fails to increment, resulting in training never ending. \r\n\r\nYou can reproduce by the running the below code on colab.sandbox.google.com, observe it doesn't increment. Then downgrade to 1.10 and observe it works as expected.\r\n\r\n### Source code / logs\r\n\r\n```python\r\nimport shutil\r\nimport numpy as np\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\n\r\nfrom tensorflow import keras\r\nfrom tensorflow.python.keras.layers import Dense\r\n\r\n#Hyperparameters\r\nunits=64\r\nnum_classes=3\r\nbatch_size=128\r\nmodel_dir='model_files'\r\n\r\n\r\n### 1. Generate Data\r\ndata = np.random.random((1000, 100))\r\nlabels = np.random.randint(3, size=(1000, 1))\r\n\r\n### 2. Input Function\r\ndef input_fn(features, labels, batch_size, mode):\r\n    dataset = tf.data.Dataset.from_tensor_slices((features, labels))\r\n      \r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n      dataset = dataset.repeat()\r\n\r\n    # Shuffle, repeat, and batch the examples.\r\n    return dataset.batch(batch_size)\r\n\r\n### 3. Model Code\r\nmodel = keras.models.Sequential()\r\nmodel.add(Dense(units=units, input_shape=(100,), activation='relu',))\r\nmodel.add(Dense(units=num_classes, activation='softmax'))\r\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\r\n\r\n\r\n### 4. Estimator Code\r\nshutil.rmtree(model_dir, ignore_errors=True) # start fresh each time\r\nestimator = tf.keras.estimator.model_to_estimator(keras_model=model, model_dir=model_dir)\r\n\r\n\r\n# Create TrainSpec\r\ntrain_spec = tf.estimator.TrainSpec(\r\n    input_fn=lambda: input_fn(\r\n        data,\r\n        labels,\r\n        batch_size,\r\n        mode=tf.estimator.ModeKeys.TRAIN),\r\n    max_steps=100\r\n)\r\n\r\n# Create EvalSpec\r\neval_spec = tf.estimator.EvalSpec(\r\n    input_fn=lambda: input_fn(\r\n        data,\r\n        labels,\r\n        batch_size,\r\n        mode=tf.estimator.ModeKeys.EVAL),\r\n    steps=None\r\n)\r\n\r\n# Start training\r\ntf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n```", "comments": ["The same error occurred on examples running within docker image `tensorflow:tensorflow:1.11.0-gpu-py3`", "/cc @lenlen", "I thought this was fixed-- @k-w-w , is this the issue with cloning/iterator that you fixed? When did the fix go out?", "This should be fixed in 1.11.0-rc1, colab runs at 1.11.0. \r\n\r\n@karmel how long will it take for the tensorflow version to be updated?", "Presumably it will happen with the 1.12 release, which is coming out soon. ", "I'm building TensorFlow from source (from r1.12 branch) and can confirm this issue is fixed in 1.12.0-rc0.", "Nagging Assignees @karmel, @ispirmustafa: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing, as this has been fixed."]}, {"number": 22849, "title": "Add validation to axis for tf.nn.softmax", "body": "This fix tries to address the issue raised in #22793 where\r\nan invalid axis (outside of `[-dim, dim)`) still returns\r\nvalue. This behavior is different from most other ops in\r\ntf like `tf.argmax`/etc.\r\n\r\nThis fix add the validation of axis so that an error\r\nwill be returned in case of invalid axis.\r\n\r\nThis fix fixes #22793.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks @hawkinsp for the review. The PR has been updated, now validation does not go through graph any more. Please take a look and let me know if there are any issues.", "@harshini-gadige It looks like all tests passed. Maybe `ready to pull` could be added?"]}, {"number": 22848, "title": "update readme", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "> Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\r\n> \r\n> memo **Please visit https://cla.developers.google.com/ to sign.**\r\n> \r\n> Once you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\r\n> #### What to do if you already signed the CLA\r\n> ##### Individual signers\r\n> \r\n>     * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> \r\n> \r\n> ##### Corporate signers\r\n> \r\n>     * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\r\n> \r\n>     * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> \r\n>     * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\r\n\r\nI signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 22847, "title": "Failed to load the native TensorFlow runtime.", "body": "I'm Try run tensorflow.\r\n\r\nLast version for Bazel, CUDA, pip... using python 3.6 and 3.7 with PyCharm IDE or Windows cmd.\r\n\r\nSystem Win 7 x64, processor AMD + Radeon Graphics GPU.\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Ramon\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Ramon\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Ramon\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Ramon\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Ramon\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed with error code -1073741795\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:/Users/Ramon/.PyCharmCE2018.2/config/scratches/scratch.py\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\Ramon\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Ramon\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Ramon\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Ramon\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Ramon\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Ramon\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Ramon\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Ramon\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed with error code -1073741795\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@Ramonzo Please follow the instructions for [ROCm Tensorflow 1.8 Release](https://gpuopen.com/rocm-tensorflow-1-8-release/). You can also use docker image for installation.", "Closing this, feel fee to reopen if problem persists."]}, {"number": 22846, "title": "TensorFlow Lite for Microcontrollers needs to follow alignment while allocating/access memory", "body": "@petewarden, we have cross compiled TensorFlow Lite for  Microcontrollers for our audio platforms, while running \"micro_speech_test\" our processor throw below LoadStoreAlignmentCause exception.\r\n***WARNING* Unhandled user exception: LoadStoreAlignmentCause (0x57ffe02b)**\r\nIn general our processor expects to be 4 byte aligned while accessing memory.\r\nAfter debugging code we found AllocateMemory() is not following memory aligned, Following patch is fixing the issue,however this will have some penalty for cycles in our hardware.\r\n\r\nOriginal code:\r\nuint8_t* SimpleTensorAllocator::AllocateMemory(size_t size) {\r\n  if ((data_size_ + size) > data_size_max_) {\r\n    // TODO(petewarden): Add error reporting beyond returning null!\r\n    return nullptr;\r\n  }\r\n  uint8_t* result = data_;\r\n  data_ += size;\r\n  data_size_ += size;\r\n  return result;\r\n}\r\n\r\nAfter aligned to 4 bytes:\r\n uint8_t* SimpleTensorAllocator::AllocateMemory(size_t size) {\r\n       int align = 4;\r\n       uint8_t *result = (uint8_t*)((intptr_t)(data_ + align - 1) & (~(align - 1)));\r\n       int aligned_size = result - data_ + size;\r\n  if ((data_size_ + aligned_size) > data_size_max_) {\r\n     // TODO(petewarden): Add error reporting beyond returning null!\r\n     return nullptr;\r\n   }\r\n  data_ += aligned_size;\r\n  data_size_ += aligned_size;\r\n  return result;\r\n }\r\n\r\nIt is good idea to write code for TensorFlow Lite for Microcontrollers follows memory alignment with an argument default to 4bytes and allow overwrite to custom number may be 8 and so on..\r\n\r\n ", "comments": ["@petewarden, Can I also get access to tiny_conv.tflite model file.\r\n\r\n", "@wt-huang ,Does this micro_speech_test runs \"okgoogle\" hotword detection, if so please let me know where to refer input audio file which will have \"OKGOOGLE\" hotword recorded and as well to compare output with golden output file which contains \"OKGOOGLE\"", "@niruyadla: @petewarden is probably the best person to comment on this. Basically you can use`models.py` to generate `tiny_conv` model then convert it to .tflite. In general micro_speech_test should be able to run hotword detection if using the standard speech library. Then you can evaluate the model and make comparisons.", "@petewarden ,I see \"yes\", \"no\" keyword test is released as part of micro speech test and thanks for your push towards tflite for microcontroller. We can run test as specified in your document.\r\nAnd we like to run conv_actions_frozen.tflite quant model on tflite for microcontroller", "@petewarden ,  in order to generate our own Yes/no keyword and to perform frontend processing, we need to have tensorflow/examples/speech_commands/wav_to_features.py file and in tflite master code I could not find the same..\r\n", "Here is the link to [tensorflow/examples/speech_commands/wav_to_features.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/speech_commands/wav_to_features.py) added by @petewarden ", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 22845, "title": "Importing Tensorflow - DLL load failed: the specified procedure could not be found", "body": "I'm creating a basic project in PyCharm 2018.2.4, importing tensor flow (version 1.10.0 installed using pip) , and I get this error:\r\n\r\n`Traceback (most recent call last):\r\n  File \"C:/Users/sandr/Desktop/TensorProj/testing.py\", line 1, in <module>\r\n    import tensorflow\r\n  File \"C:\\Users\\sandr\\.virtualenvs\\TensorProj-uBuGaj5e\\lib\\site-packages\\tensorflow\\__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\sandr\\.virtualenvs\\TensorProj-uBuGaj5e\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 52, in <module>\r\n    from tensorflow.core.framework.graph_pb2 import *\r\n  File \"C:\\Users\\sandr\\.virtualenvs\\TensorProj-uBuGaj5e\\lib\\site-packages\\tensorflow\\core\\framework\\graph_pb2.py\", line 6, in <module>\r\n    from google.protobuf import descriptor as _descriptor\r\n  File \"C:\\Users\\sandr\\.virtualenvs\\TensorProj-uBuGaj5e\\lib\\site-packages\\google\\protobuf\\descriptor.py\", line 47, in <module>\r\n    from google.protobuf.pyext import _message\r\nImportError: DLL load failed: The specified procedure could not be found.`\r\n\r\nTried a whole bunch of different versions, but nothing is working. Could anyone help me out?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "The tensorflow version was 1.10.0. I installed from the command 'pip install --uprade tensorflow' in the PyCharm terminal.\r\n\r\nI am running Windows 10 (Version 10.0.17134 Build 17134). My CPU is an Intel Core i5-5200 CPU, integrated GPU. ", "What is the python version you are using?\r\nAlso can you try doing pip install tensorflow==1.10 (if using python2x)\r\nor pip3 install tensorflow==1.10 (if using python3x)\r\n", "Python version 3.6.0. pip3 install tensorflow for version 1.10 gave me the same error. ", "Please refer to a similar [issue](https://github.com/protocolbuffers/protobuf/issues/5062). Can you please try updating your python version to 3.6.1 and check if the issue still persists?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 22844, "title": "Tf xla unit test", "body": "Changes include \r\n1. Fixes to load/unload/compile test case module as HSACO  object \r\n2. Test case changes to invoke ROCM functionality ", "comments": ["It's not polite to add this many reviewers.", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I think the adding was performed automatically for CODEOWNERS. Clearly a rebase sentido wrong here, but I don't think it was intentional. "]}, {"number": 22842, "title": "Is Tensorflow Distributed Model supporting NCCL 2.0 inter-node AllReduce features?", "body": "Hi, allow me to ask for a question about Tensorflow feature:\r\n\r\nFirstly, I uses the nearly-latest Tensorflow version v1.10 and the system has NCCL 2.0 installed.\r\n\r\nAs is known that NCCL 2.0 has already supported Cross-Node multiple GPU training with fully NCCL management, but I found that Tensorflow benchmark script doesn't support NCCL in Tensorflow distributed mode, and it said NCCL mode is only allowed and designed for in single-node mode, so I don't know whether it is not supported by Tensorflow engine, or just not supported by the Tensorflow benchmark script? If it is just not implemented by the Tensorflow benchmark script, is there documentation or example to enable Distributed Tensorflow to use NCCL 2.0 to manage cross-node Multi GPUs? Thanks.\r\n\r\nThe official Tensorflow benchmark:\r\nhttps://github.com/tensorflow/benchmarks", "comments": ["I will suggest you to checkout uber/horovod, which uses NCCL and MPI for inter-node all reduce.\r\n\r\nAFAIK, we are not using NCCL for inter-node GPU training, and perhaps the reason is simply because that we would also like to support non-NVidia GPU (ROCm support was recently added) for inter-node training. Nevertheless, PRs are always welcomed :)", "Duplicate question of #20447", "Please refer to the discussion on another thread as mentioned below.\r\n> Duplicate question of #20447\r\n\r\n", "@ymodak So is it not a plan for Tensorflow to support this just as byronyi said?", "To be more constructive, please submitting PR to support new features, not duplicated issues."]}, {"number": 22841, "title": "Added bullet points so the values look clearer", "body": "Currently it's these items are all in one line in https://www.tensorflow.org/api_docs/python/tf/losses/Reduction, which is hard to visualize.", "comments": ["Starting the build again.", "This should probably not be merged to 1.11? Is this fix required on master branch", "Sorry I just realized that I submitted the PR to the wrong branch (good catch! @case540 ). Should I close this and submit another one to master branch then? ", "@terrytangyuan Yes, please send this against `master`.", "Thanks. I just opened a new PR against master: https://github.com/tensorflow/tensorflow/pull/23024"]}, {"number": 22840, "title": "Compiling tensorflow 1.10.0 in mingw-w64(msys2 32bit)", "body": "After patching some code(mostly in cmake), I have compiled successfully tensorflow 1.10.0 in mingw-w64(msys2 32bit). Although some unit test not passed, I think some code should be commit to master. So now what should i do?\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: win7 32bit\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.10.0\r\n- **Python version**: 2.7.15\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: gcc with mingw-w64(msys2 32bit)\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\n### Describe the problem\r\nFor some reasons, i need to using tensorflow in mingw-w64. \r\nAfter patching some code(mostly in cmake and little in header files), I have compiled successfully tensorflow source 1.10.0 in win7 mingw-w64(msys2 32bit) with cmake. \r\nAlthough some unit test not passed, I think some code should be commit to master to check. \r\nI think this patchs could make a mingw-w64 port.\r\nBecause i never use github to commit code, so i want to know how should i commit this patchs to master to check?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Has been solved by stackoverflow", "@smalltalkman  Can you please post the link of stackoverflow for this issue? It will help us to guide users who may run into similar issue. Thanks!", "[compiling-tensorflow-1-10-0-in-mingw-w64msys2-32bit](https://stackoverflow.com/questions/52714317/compiling-tensorflow-1-10-0-in-mingw-w64msys2-32bit)\r\n\r\nThe patch for mingw-w64-tensorflow is at [mingw-w64-tensorflow](https://github.com/smalltalkman/mingw-w64-tensorflow). PKGBUILD shows all my compile steps.", "@smalltalkman , i am trying to build the tensorflow using mingw-w64 on  windows 10 ...  i need the tensorflow c++ lib which i wanted to use it in mingw-w64 based projects... can you please help me , how to do the cmake with mingw-w64 to get compile successfully.", "@mammaiap \r\n> @smalltalkman , i am trying to build the tensorflow using mingw-w64 on windows 10 ... i need the tensorflow c++ lib which i wanted to use it in mingw-w64 based projects... can you please help me , how to do the cmake with mingw-w64 to get compile successfully.\r\n\r\nSorry, replied very late.\r\nIf you need `tensorflow c++ lib`, you can refer to the patch in [mingw-w64-tensorflow](https://github.com/smalltalkman/mingw-w64-tensorflow).\r\nAlternatively, reference may be made to [mingw-w64-tensorflow-1.10.0-cmake](https://github.com/smalltalkman/mingw-w64-tensorflow-1.10.0-cmake).\r\n`mingw-w64-tensorflow-1.10.0-cmake` is mainly used to build `python` applications\uff0c currently no perfect `C++` lib.", "@smalltalkman the stackoverflow link page not found. should we run PKGBUILD in msys2 shell?", "@Haffon yes"]}, {"number": 22839, "title": "[XLA] Query whether to enable XLA support on MacOS with no as a default", "body": "This relates to https://groups.google.com/forum/#!topic/xla-dev/ddrQKPB9gbI\r\nWith the current configure script a MacOS user is never prompted whether they want XLA support, where as the intended behaviour is XLA support should be on by default for Linux and off for MacOS.\r\n\r\n@tatatodd  ", "comments": []}, {"number": 22838, "title": "Wav to Spectogram wrong expected height", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No\r\n- **TensorFlow installed from (source or binary)**: pip install tensorflow\r\n- **TensorFlow version (use command below)**:1.11.0\r\n- **Python version**: 2.7.15rc1\r\n- **Bazel version (if compiling from source)**: 0.17.2\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**: bazel run tensorflow/examples/wav_to_spectrogram:wav_to_spectrogram -- --input_wav=/home/lukas/Downloads/music.wav --output_image=/home/lukas/Downloads/spectrogram.png\r\n\r\n\r\n\r\n### Describe the problem\r\nHi I have problem with build this example from tensorflow guide.\r\n bazel run tensorflow/examples/wav_to_spectrogram:wav_to_spectrogram \r\nI get error \"Expected height 18711 but got 18712\" \r\nI tried three differenr wav files and got same error with different height. \"Expected height x but got x + 1\"\r\nWhen I tried with default image spectrogram_test_data/short_test_segment.wav so it works good \r\n\r\n### Source code / logs\r\nbazel run tensorflow/examples/wav_to_spectrogram:wav_to_spectrogram -- --input_wav=/home/lukas/Downloads/music.wav --output_image=/home/lukas/Downloads/spectrogram.png\r\nINFO: Analysed target //tensorflow/examples/wav_to_spectrogram:wav_to_spectrogram (65 packages loaded).\r\nINFO: Found 1 target...\r\nTarget //tensorflow/examples/wav_to_spectrogram:wav_to_spectrogram up-to-date:\r\n  bazel-bin/tensorflow/examples/wav_to_spectrogram/wav_to_spectrogram\r\nINFO: Elapsed time: 149.081s, Critical Path: 4.61s\r\nINFO: 0 processes.\r\nINFO: Build completed successfully, 1 total action\r\nINFO: Running command line: bazel-bin/tensorflow/examples/wav_to_spectrogram/wav_to_spectrogram '--input_wav=/home/lukas/Downloads/music.wav' INFO: Build completed successfully, 1 total action\r\n2018-10-09 17:44:25.818539: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX\r\n2018-10-09 17:44:26.909131: E tensorflow/examples/wav_to_spectrogram/main.cc:61] WavToSpectrogram failed with Invalid argument: Spectrogram size calculation failed: Expected height 18711 but got 18712\r\n   [[Node: spectrogram = AudioSpectrogram[magnitude_squared=false, stride=128, window_size=256, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](wav_decoder)]]\r\n", "comments": ["Were you able to execute the [TensorFlow Spectrogram Example](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/wav_to_spectrogram#tensorflow-spectrogram-example) successfully?", "Yes, i get spectogram for default image  spectrogram_test_data/short_test_segment.wav from repository . Can I sent you the file which is problem ?\r\n\r\nBTW in options example is mistake. window -> window_size", "Thanks for catching the mistake in example. Yes please send a sample audio file. Also did you try writing the spectrogram image by changing window_size and stride?", "I tried change window_size and stride get same error sometimes with +X height, sometimes with -X height\r\nI added music file.\r\n[music.zip](https://github.com/tensorflow/tensorflow/files/2476050/music.zip)\r\n", "The error arises from this line in [spectrogram_op.cc](https://github.com/tensorflow/tensorflow/blob/57d31b5a9ad9ddb9434796b6253decabda61a972/tensorflow/core/kernels/spectrogram_op.cc#L87) . But not sure what should be the correct window_size and stride. Assigning to pete for more insights on this.", "The wav_to_spectrogram tool is only designed to be a debugging tool, so this hasn't made it onto our priority list to fix yet. I'm adding this to contributions welcome.", "Hey, I'd like to take a crack at this! Is that alright?", "I found the root cause. The AudioSpectrogramOp only works for one channel wav files. For a two channel wav file(as the above attached one), the problem would happen, because the 'samples_to_next_step_' in spectrogram.cc won't be initialized correctly when processing the second channel.\r\n\r\nAfter initializing the 'samples_to_next_step_' correctly, the unmatching height problem disappears. But I met another problem about the dimension of the output of the AudioSpectrogramOp.\r\n\r\nThe output dimension of the AudioSpectrogramOp is (channel_count, height, width), but the encode_png op requires a dimension of (height, width, channel_count).  In wave_to_spectrogram.cc, the dimension conversion is done by first expand_dims at the end and then squeeze at axis 0. But this only works when channel_count equals to 1.\r\n\r\nSo my question is how to convert a tensor's dimension from (channel_count, height, width) to (height, width, channel_count)? I believe after doing that conversion the problem should be fixed. Can you give me some help on this, I am new to tensorflow and don't know how to do it in c++. Thanks!", "Hi @rychnal ! The above example on wave_to_spectrogram (is there till 1.15) has been removed from the 2.x version now . Attaching [audio recognition ](https://www.tensorflow.org/tutorials/audio/simple_audio)documentation from the 2.8 version for reference.  Some lite examples on sound classification and speech recognition can be found from [here](https://github.com/tensorflow/examples/tree/master/lite/examples ). Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 22837, "title": "model.save() returning a 'NotImplementedError'", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 1803\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: From pip\r\n- **TensorFlow version (use command below)**: 1.11.0\r\n- **Python version**: 3.6.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n```python\r\n\r\nmodel = tf.keras.models.Sequential()\r\nmodel.add(tf.keras.layers.Flatten())\r\nmodel.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\r\nmodel.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\r\nmodel.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))\r\n\r\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\r\n\r\nmodel.fit(x_train, y_train, epochs=3)\r\n\r\nmodel.save('epic_num_reader.model')\r\n```\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nI'm receiving a `NotImplementedError` when trying to save a new model. The code above written above creates the error. This code is from a tutorial i've been following here [Python Programming (Sentdex)](https://pythonprogramming.net/introduction-deep-learning-python-tensorflow-keras/).\r\n\r\nIt's on a fresh machine with a fresh install of python and tensorflow.\r\n\r\nTalking with some others on a Discord chat, i'm not the first person who's encountered this recently.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nNotImplementedError                       Traceback (most recent call last)\r\n<ipython-input-11-7b7b789f82a6> in <module>\r\n----> 1 model.save('epic_num_reader.model')\r\n\r\nc:\\users\\james\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py in save(self, filepath, overwrite, include_optimizer)\r\n   1356     \"\"\"\r\n   1357     if not self._is_graph_network:\r\n-> 1358       raise NotImplementedError\r\n   1359 \r\n   1360     from tensorflow.python.keras.models import save_model  # pylint: disable=g-import-not-at-top\r\n\r\nNotImplementedError:\r\n```", "comments": ["Please take a look at [tf.train.Saver() method](https://www.tensorflow.org/guide/saved_model#save_variables) and also an [example to save a model](https://github.com/lazyprogrammer/machine_learning_examples/blob/master/ann_class2/tf_with_save.py) .", "Working on a fix for this, in the meantime a workaround is to provide `input_shape` to the first Layer or your Model:\r\n\r\n```python\r\nmodel = tf.keras.models.Sequential()\r\nmodel.add(tf.keras.layers.Flatten(input_shape=...))\r\nmodel.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\r\nmodel.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\r\nmodel.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))\r\n\r\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\r\n\r\nmodel.fit(x_train, y_train, epochs=3)\r\n\r\nmodel.save('epic_num_reader.model')\r\n```", "Just a heads up. I was able to execute the @jam3sn 's code successfully in TF 1.10 but it failed in TF 1.11", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Yep this is still happening with TF 1.11\r\n", "I could run the code successfully in tf-nightly.", "Great, yes sorry meant to update this, the fix for this is in", "This is still broken in 1.12.", "it shows not implemented  error : Currently `save` requires model to be a graph network. Consider using `save_weights`, in order to save the weights of the model.\r\n in 1.12.0 version", "I am also having this problem on version 1.12.0", "I am also getting the same problem in version 1.12.0\r\n\r\n\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-18-b9d746b1e27b> in <module>\r\n----> 1 tf.keras.models.save_model('epic_num_reader.model')\r\n\r\nTypeError: save_model() missing 1 required positional argument: 'filepath'\r\n", "@muhammadmw I found that my problem was caused by subclassing Keras.Model. A model created by subclassing Keras.Model cannot be serialized by using the save_model function. I switched over to the Keras functional Api and I no longer have the problem. ", "this is happening with tf-nightly-2.0-preview-2.0.0.dev20190225", "I am experiencing this in 2.2.4-tf", "So this issue is closed without solution?", "Add save_weights_only = True for ModelCheckpoint or use model.save_weights()", "> @muhammadmw I found that my problem was caused by subclassing Keras.Model. A model created by subclassing Keras.Model cannot be serialized by using the save_model function. I switched over to the Keras functional Api and I no longer have the problem.\r\n\r\nHey So can you please tell me how you fixed the problem as I am trying to save the weights and when I try to load the weights it gives error.\r\nUnable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first, then load the weights."]}]