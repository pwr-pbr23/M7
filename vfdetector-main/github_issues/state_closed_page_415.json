[{"number": 41463, "title": "Moving last set of FileSystem derivatives to new API", "body": "This is part 5 of moving to Transactional FileSystems. This PR prepares extra arguments for API change in FileSystem base class ", "comments": []}, {"number": 41462, "title": "Moving more filesystems to transactions", "body": "This Part 4 of migrating to Transactions API. Preparing more filesystems for migration.", "comments": []}, {"number": 41461, "title": "tf.contrib.framework.assign_from_values in v2", "body": "## Description of issue (what needs changing):\r\nI'm running inference on a model that used tf1, and I've converted most of the code to v2 but I can't seem to find anywhere the proper v2 (or even tf.compat.v1) version of framework.assign_from_values. I'm just getting the error\r\n`AttributeError: module 'tensorflow.compat.v1' has no attribute 'contrib'`\r\n", "comments": ["@sreyahalder,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the dataset you are using. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Have you fixed this bug without installing tensorflow v1.x?\r\nI used code in tensorflow 2. I need to copy a tensor from a graph to another graph to convert into numpy by using `copy_graph.copy_variable_to_graph`. However, it said that `AttributeError: module 'tensorflow.compat.v1' has no attribute 'contrib'`.\r\n\r\n\r\nPlease help me. Thank you a lot. \r\n"]}, {"number": 41460, "title": "tf.keras.preprocessing.image.apply_affine_transform shifts tx as height and ty as width", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 20.04 LTS\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.x\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: GeForce GTX 1070 8GB\r\n\r\n**Describe the current behavior**\r\nWhen shifting the image on x axis using `apply_affine_transform` output is shifted on y axis (and vice versa).\r\n\r\n![00137](https://user-images.githubusercontent.com/11685462/87704058-11761b80-c79c-11ea-8b07-2ab05fe88fa7.jpg)\r\n\r\n```\r\nimport tensorflow as tf\r\nimport cv2\r\nimport matplotlib.pyplot as plt\r\nimg = cv2.imread('00137.jpg')\r\nimg2 = tf.keras.preprocessing.image.apply_affine_transform(img, tx = 100, row_axis=0, col_axis=1)\r\nplt.imshow(img2)\r\n```\r\n\r\n![Screenshot from 2020-07-16 19-14-43](https://user-images.githubusercontent.com/11685462/87704113-29e63600-c79c-11ea-8b33-8644a83ddc4e.png)\r\n\r\nSwitch in row_axis/col_axis is not doing anything.\r\n\r\n**Describe the expected behavior**\r\nSetting `tx` should shift image along `x` axis\r\n\r\n**Standalone code to reproduce the issue**\r\nCode in the example above\r\n", "comments": ["+1 \r\nsomehow confused....", "+1", "Use `tf.keras.layers.experimental.preprocessing.RandomTranslation` instead", "@tanzhenyu I will check it out, but I need non-random translation", "> @tanzhenyu I will check it out, but I need non-random translation\r\n\r\nyou can definitely achieve non-random behavior by setting lower bound equals upper bound.", "@maju116  Any update on this issue?  Please let us know if you need  any additional information.Thanks!", "@maju116 As suggested by @tanzhenyu this was resolved by using keras preprocessing layers (`tf.keras.layers.experimental.preprocessing.RandomTranslation`). [Here](https://colab.research.google.com/gist/jvishnuvardhan/ab12d50bfc732df2c6e45824f28a9cb0/untitled.ipynb) is a gist for your reference. \r\n\r\nIn the example, I have used width_factor as `0.2` which is same as (0.2, 0.2). As mentioned by @tanzhenyu when lower_bound and upper_bound are equal, there there is no random behavior introduced in the operation. \r\n\r\n![car_translated](https://user-images.githubusercontent.com/46058173/120564178-36c54580-c3bf-11eb-8625-9dfd0a1a693a.png)\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41460\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41460\">No</a>\n", "I also ran into this issue on TF 2.5, which is using an outdated version of keras-preprocessing (i.e., v1.1.2). This issue was fixed via this [PR](https://github.com/keras-team/keras-preprocessing/pull/318), but had not made its way into the current TF versions. Though it seems like it will arrive soon based on [this commit](https://github.com/keras-team/keras/commit/373ad97c72ed1ac4b6898e85b2cfd7b016e4b469) from a month ago."]}, {"number": 41459, "title": "TF to s3a:// access needed", "body": "There is funcionality for basic `s3://` scheme on Linux. Then there is work for tensorflow/community#101 to convert all these filesystem to a plugin based form so they can be used on all platforms, on demand.\r\n\r\nRegarding `s3a` and `s3n`, can you file a new issue, feature request type, and assign it to me/mention me? just to keep this issue only for the `s3` not implemented error.\r\n\r\n_Originally posted by @mihaimaruseac in https://github.com/tensorflow/tensorflow/issues/40302#issuecomment-659517610_", "comments": ["@mihaimaruseac : when should we expect this feature?", "Modular filesystems are scheduled to land in TF 2.5.", "Could you please refer this https://github.com/tensorflow/tensorflow/issues/40302 issue and as per the latest comments in the linked issue the problem is solved, could you please confirm the same. Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41458, "title": "tf.random.categorical overflow before OOM on GPU", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 LTS\r\n- TensorFlow version (use command below): 2.2.0 and 2.1.0\r\n- Python version: 3.x\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: V100 32GB\r\n\r\n**Describe the current behavior**\r\n\r\nWhen running the `tf.random.categorical` with a large `num_samples` on a V100 with 32GB we get the following crash:\r\n```bash\r\n2020-07-15 21:58:02.621909: F ./tensorflow/core/util/gpu_launch_config.h:129] Check failed: work_element_count > 0 (-2099511296 vs. 0)\r\nAborted (core dumped)\r\n```\r\nThis looks like an overflow before raising the OOM error.\r\n\r\nHere a minimalist code which reproduces the aborted (core dumped) on the V100 with 32GB:\r\n```python\r\nimport tensorflow as tf\r\nprobs = tf.random.uniform((2 ** 15,), dtype=tf.float64)\r\nprobs = probs / tf.reduce_sum(probs)\r\nlogits = tf.math.log(probs)\r\nsamples = tf.random.categorical(logits[tf.newaxis], 70000, dtype=tf.int64)[0]\r\n```\r\n\r\nHere some tests on different cards:\r\n- V100 32GB, cuda 10.1, tf2.2 and tf2.1: Aborted (core dumped)\r\n- V100 16GB, cuda 10.1, tf2.1: OOM\r\n- TitanV 12GB, cuda 10.2, tf2.2: OOM\r\n- RTX2080TI, 12GB, cuda 10.2, tf2.2: OOM\r\n\r\nThis issue only happens on V100 with 32GB.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe code should raise a Resource Exhausted OOM error instead of being aborted.", "comments": ["@scarrazza,\r\nCurrently I do not have a V100 to test the code. Could you please try limiting GPU growth as shown in [this guide](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) and check if it raises the intended error. Thanks! ", "@amahendrakar, good idea, thanks. \r\n\r\nIf I limit the V100 32GB to use just 16GB then I get the OOM error, as expected.\r\nHowever, if I increase the memory to 18GB, then I get the usual `Aborted (core dumped)`.", "@scarrazza Sorry for the late response. I tried reproducing it with recent `TF2.4rc3` in colab but didn't see OOM or coredump. [Here](https://colab.research.google.com/gist/jvishnuvardhan/db34461fc50a37fd5ee65c2f4808a938/untitled.ipynb) is the gist for reference. \r\n\r\nCan you please check and let us know whether the error still persists in recent TF version? Thanks!", "@jvishnuvardhan I have tested your gist on Colab, however tensorflow-gpu==2.4rc3 doesn't seem to detect the GPU (check the `tf.config.list_physical_devices()` on your notebook), your code is running on CPU. \r\n\r\nI also tried to install 2.4rc3 on other machine with larger GPU memory however also there it doesn't detect the GPU device, so I can't test.", "I tried the code in colab with TF 2.5 version & didn't face the issue reported ,please check this gist [here](https://colab.research.google.com/gist/sushreebarsa/f6ebe38bde51d74a2aa9fd22bb58047f/untitled114.ipynb?authuser=1)..Thanks !", "Closing this issue as it is fixed in latest version of TensorFlow. Please feel free to reopen the issue if you still have a concern. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41458\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41458\">No</a>\n"]}, {"number": 41457, "title": "macos not raising OOM error", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Catalina 10.15\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.x\r\n\r\n**Describe the current behavior**\r\n\r\nWhen allocating a large tensor on macos the code is killed by the kernel instead of raising a OOM error.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe code above should raise a Resource Exhausted OOM error, as it does on Linux.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nHere a minimal example:\r\n```python\r\nimport tensorflow as tf\r\ntf.ones(2**40, dtype=tf.complex128)\r\n```", "comments": ["It is likely macos throws a different error that we don't manage to capture. Thanks for pointing the issue out", "Hi @scarrazza! \r\nWe are checking to see whether you still need help in this issue . Have you tried this [thread ](https://developer.apple.com/metal/tensorflow-plugin/)for Tensorflow installation in Mac OS yet? . Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41457\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41457\">No</a>\n"]}, {"number": 41456, "title": "Add benchmarks for other models.", "body": "Add benchmarks for examples from keras.io and [examples in keras repo](https://github.com/keras-team/keras/tree/master/examples).", "comments": []}, {"number": 41455, "title": "I'm trying to convert a custom model on inception V3 + SSD and running into the same issue. Any workarounds to get the model to tflite format until this is officially fixed?", "body": "I'm trying to convert a custom model on inception V3 + SSD and running into the same issue. Any workarounds to get the model to tflite format until this is officially fixed?\r\n\r\n_Originally posted by @divSivasankaran in https://github.com/tensorflow/tensorflow/issues/18731#issuecomment-398985955_\r\n\r\nHas someone resolved this issue?, I want to convert ssd_inception_v2_coco and  throws  the OperatorType::kMerge Found StridedSlice as non-selected output from Switch, but only Merge supported.", "comments": ["The original issue is a too old issue. We have changed our converter's gear from TOCO to MLIR converter. Could you try the conversion again with the recent version? \r\n\r\nBefore trying conversion, you need to create a model in Python after inserting the code at the beginning in order to upgrade legacy Switch and Merge nodes to V2 control flow ops.\r\ntf.compat.v1.enable_control_flow_v2()\r\nhttps://www.tensorflow.org/api_docs/python/tf/compat/v1/enable_control_flow_v2\r\n\r\nOr you can find alternatives in the here as well:\r\nhttps://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md\r\nhttps://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1_detection_zoo.md\r\n\r\nSome of the above model zip file links contain the converted tflite files.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@IvanChavR \r\n\r\nCan you please confirm if @abattery's workaround is working for you.Thanks!", "Hi, I have just used these sentences wit TF 2:\r\n\r\n`input_arrays = [\"image_tensor\"]\r\noutput_arrays = [\"detection_classes\",\"detection_boxes\",\"detection_scores\", \"num_detections\"]\r\nconverter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(PATH_TO_FROZEN_GRAPH, input_arrays,output_arrays, \r\n                                                                                                        input_shapes={\"image_tensor\":[1,300,300,3]})\r\n    \r\ntf.compat.v1.enable_control_flow_v2()\r\nconverter.experimental_new_converter = True\r\n#converter.post_training_quantize = True\r\n#converter.allow_custom_ops=True\r\ntflite_model = converter.convert()\r\nopen(\"converted_model.tflite\", \"wb\").write(tflite_model)`\r\n\r\nAnd the result is the next:\r\n\r\n> Traceback (most recent call last):\r\n  File \"convertion_resnet.py\", line 80, in <module>\r\n    convertv2()\r\n  File \"convertion_resnet.py\", line 75, in convertv2\r\n    tflite_model = converter.convert()\r\n  File \"/home/ivan/anaconda3/envs/tensor2/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 1084, in convert\r\n    **converter_kwargs)\r\n  File \"/home/ivan/anaconda3/envs/tensor2/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 496, in toco_convert_impl\r\n    enable_mlir_converter=enable_mlir_converter)\r\n  File \"/home/ivan/anaconda3/envs/tensor2/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 227, in toco_convert_protos\r\n    raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: See console for info.\r\n2020-07-25 10:21:33.757076: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:144] Ignored output_format.\r\n2020-07-25 10:21:33.757102: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:147] Ignored drop_control_dependency.\r\nTraceback (most recent call last):\r\n  File \"/home/ivan/anaconda3/envs/tensor2/bin/toco_from_protos\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/home/ivan/anaconda3/envs/tensor2/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 93, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/ivan/anaconda3/envs/tensor2/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/ivan/anaconda3/envs/tensor2/lib/python3.6/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/home/ivan/anaconda3/envs/tensor2/lib/python3.6/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/ivan/anaconda3/envs/tensor2/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 56, in execute\r\n    enable_mlir_converter)\r\nException: Merge of two inputs that differ on more than one predicate {s(Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater:0,else), s(Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/cond/cond/pred_id/_304__cf__307:0,then), s(Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/cond/Greater/_303__cf__306:0,then)} and {s(Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater:0,else), s(Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/cond/cond/pred_id/_304__cf__307:0,else), s(Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/cond/Greater/_303__cf__306:0,else)}\r\n\tfor node {{node Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/cond/cond/Merge}}\r\n\r\n\r\n", "Hi, Is this still an issue? Could you please try in the latest Tensorflow Version and let us know. Thanks", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41455\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41455\">No</a>\n"]}, {"number": 41454, "title": "S3 transfer manager and random_access_file", "body": "@mihaimaruseac \r\nThis PR adds `tf_random_access_file` and `TransferManager` for `s3`.", "comments": []}, {"number": 41453, "title": "After Installing Tensorflow By Using(pip install tensorflow), I got this error while importing.", "body": "ImportError                               Traceback (most recent call last)\r\nD:\\Unity\\Unity\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59 \r\n\r\nD:\\Unity\\Unity\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in <module>\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow_internal = swig_import_helper()\r\n     29     del swig_import_helper\r\n\r\nD:\\Unity\\Unity\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     23             try:\r\n---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n     25             finally:\r\n\r\nD:\\Unity\\Unity\\lib\\imp.py in load_module(name, file, filename, details)\r\n    241         else:\r\n--> 242             return load_dynamic(name, filename, file)\r\n    243     elif type_ == PKG_DIRECTORY:\r\n\r\nD:\\Unity\\Unity\\lib\\imp.py in load_dynamic(name, path, file)\r\n    341             name=name, loader=loader, origin=path)\r\n--> 342         return _load(spec)\r\n    343 \r\n\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-d6579f534729> in <module>\r\n----> 1 import tensorflow\r\n\r\nD:\\Unity\\Unity\\lib\\site-packages\\tensorflow\\__init__.py in <module>\r\n     39 import sys as _sys\r\n     40 \r\n---> 41 from tensorflow.python.tools import module_util as _module_util\r\n     42 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader\r\n     43 \r\n\r\nD:\\Unity\\Unity\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>\r\n     48 import numpy as np\r\n     49 \r\n---> 50 from tensorflow.python import pywrap_tensorflow\r\n     51 \r\n     52 # Protocol buffers\r\n\r\nD:\\Unity\\Unity\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     67 for some common reasons and solutions.  Include the entire stack trace\r\n     68 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 69   raise ImportError(msg)\r\n     70 \r\n     71 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"D:\\Unity\\Unity\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"D:\\Unity\\Unity\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"D:\\Unity\\Unity\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"D:\\Unity\\Unity\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"D:\\Unity\\Unity\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n   * For TF-GPU - See point 1\n   * For TF-CPU - See point 2\n-----------------------------------------------------------------------------------------------\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\nMake sure you are using compatible TF and CUDA versions. Please refer following TF version and CUDA version compatibility table.\n| TF  | CUDA |\n| :-------------: | :-------------: |\n| 2.1.0 - 2.2.0  | 10.1 |\n| 1.13.1 - 2.0  | 10.0  |\n| 1.5.0 - 1.12.0 | 9.0 |\n\n  * If you have above configuration and using _**Windows**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n    * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n  * If you have above configuration and using _**Ubuntu/Linux**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n    * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n  * If error still persists then, apparently your CPU model does not support AVX instruction sets.\n    * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n   * Try Google Colab to use TensorFlow.\n      * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed latest stable TF version. Also you can use ```pip install```  to install any other preferred TF version.\n      * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n      * All you need is a good internet connection and you are all set.\n   * Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*\n", "@SPavanKumar,\r\nYou might be facing this issue because of the following reasons\r\n\r\n- You you running 32-bit Python or 32-bit OS\r\n- You have not installed the [Microsoft Visual C++ Redistributable](https://support.microsoft.com/help/2977003/the-latest-supported-visual-c-downloads) package\r\n- Your CPU does not support AVX instructions. \r\n\r\nPlease take a look at the [system requirements](https://www.tensorflow.org/install/pip#system-requirements) and check if you have the correct dependencies installed.\r\n\r\nAlso, check these similar duplicate issues: #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204.\r\n\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41453\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41453\">No</a>\n"]}, {"number": 41452, "title": "InvalidArgumentError: Only ranks up to 5 supported", "body": "Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS inux release 7.6.1810\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\nTensorFlow installed from (source or binary): binary\r\nTensorFlow version (use command below): 1.14.0\r\nPython version: 3.7.6\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version: V9.1.85\r\nGPU model and memory: Tesla K80 24C\r\n\r\nI am working with Keras to build the following (simplified) model:\r\n```\r\nclass MyModel():\r\n   def __init__(self):\r\n       pass\r\n   \r\n   def build_model(self):\r\n       inp = Input(shape = self.outputs.shape)\r\n        \r\n       x = Dense(128, activation = 'elu')(inp) # We'll start exploring with one hidden layer\r\n       outp = Dense(1, activation = 'elu')(x)\r\n        \r\n       self.model = Model(inputs = inp, outputs = outp)\r\n\r\n       self.model.compile(loss = dice_loss,\r\n                            optimizer = Adam(),\r\n                            metrics = [dice_coef])\r\n\r\n   def run_model(self):\r\n      # For the sake of simplicity\r\n      self.outputs = np.random.rand(8,64,12,86,98,1)      \r\n\r\n      # labels shape = (64, 12, 86, 98) to get (1,64,12,86,98)\r\n      self.labels = np.expand_dims(self.labels, axis = (0,-1))\r\n      # outputs shape (8,64,12,86,98,1) to -> (1,64,12,86,98,8)\r\n      self.outputs = np.swapaxes(self.outputs, 0,-1)\r\n      \r\n      self.build_model()\r\n\r\n      self.model.fit([self.outputs, self.labels],\r\n                                batch_size = 1,\r\n                                epochs = 200)\r\n```\r\nHowever, upon trying to fit the model, as:\r\n```\r\nmodel = MyModel()\r\nmodel.run_model()\r\n```\r\nThe following error appears:\r\n```\r\n  File \"/home/kdqm927/miniconda3/envs/segment/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_v1.py\", line 785, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/home/kdqm927/miniconda3/envs/segment/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\", line 634, in fit\r\n    shuffle=shuffle)\r\n  File \"/home/kdqm927/miniconda3/envs/segment/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_v1.py\", line 2308, in _standardize_user_data\r\n    batch_size=batch_size)\r\n  File \"/home/kdqm927/miniconda3/envs/segment/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_v1.py\", line 2335, in _standardize_tensors\r\n    exception_prefix='input')\r\n  File \"/home/kdqm927/miniconda3/envs/segment/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_utils.py\", line 573, in standardize_input_data\r\n    'with shape ' + str(data_shape))\r\nValueError: Error when checking input: expected input_1 to have 7 dimensions, but got array with shape (1, 64, 12, 86, 98, 8)\r\n```\r\nIf I tweak the code by adding another dimension to self.outputs, following the exception from the previous error:\r\n```\r\nself.model.fit([np.expand_dims(self.outputs, axis = 0), self.labels],\r\n                                batch_size = 1,\r\n                                epochs = 200)\r\n```\r\nThe error I get is: \r\n```\r\nTraceback (most recent call last):\r\n  File \"./file.py\", line 257, in <module>\r\n    s.fit_metamodel('outputs_validation_arr')\r\n  File \"./file.py\", line 157, in fit_metamodel\r\n    callbacks = callbacks)\r\n  File \"/home/.../miniconda3/envs/segment/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_v1.py\", line 785, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/home/.../miniconda3/envs/segment/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\", line 666, in fit\r\n    steps_name='steps_per_epoch')\r\n  File \"/home/.../miniconda3/envs/segment/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\", line 386, in model_iteration\r\n    batch_outs = f(ins_batch)\r\n  File \"/home/.../miniconda3/envs/segment/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\", line 3632, in __call__\r\n    run_metadata=self.run_metadata)\r\n  File \"/home/.../miniconda3/envs/segment/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1472, in __call__\r\n    run_metadata_ptr)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Only ranks up to 5 supported: [1,1,64,12,86,98,128]\r\n\t [[{{node dense/BiasAdd}}]]\r\n```\r\n**EDIT**\r\nTensorFlow version (use command below): 2.2.0\r\n****", "comments": ["@danielfllaneza \r\nI ran the code shared by you, itseems incomplete, please find the error faced in [gist here](https://colab.research.google.com/gist/Saduf2019/756f1fd9af25d0aa7a1994bad8e34818/untitled282.ipynb).\r\n\r\nMeanwhile as suggested in the [link](https://github.com/keras-team/keras/issues/10053#issuecomment-424091976) can you try to reshape\r\n[link](https://stackoverflow.com/questions/41563720/error-when-checking-model-input-expected-convolution2d-input-1-to-have-4-dimens) [link1](https://datascience.stackexchange.com/questions/55975/valueerror-error-when-checking-input-expected-input-1-to-have-4-dimensions-bu) [link2](https://github.com/tensorflow/tensorflow/issues/33074#issuecomment-541574596)\r\n\r\n", "Oh, I forgot mocking labels, for the sake of simplicity we can create self.labels too, so the code would be:\r\n```\r\nclass MyModel():\r\n   def __init__(self):\r\n       pass\r\n   \r\n   def build_model(self):\r\n       inp = Input(shape = self.outputs.shape)\r\n        \r\n       x = Dense(128, activation = 'elu')(inp) # We'll start exploring with one hidden layer\r\n       outp = Dense(1, activation = 'elu')(x)\r\n        \r\n       self.model = Model(inputs = inp, outputs = outp)\r\n\r\n       self.model.compile(loss = dice_loss,\r\n                            optimizer = Adam(),\r\n                            metrics = [dice_coef])\r\n\r\n   def run_model(self):\r\n      # For the sake of simplicity\r\n      self.outputs = np.random.rand(8,64,12,86,98,1)    \r\n      self.labels = np.random.rand(64, 12, 86, 98)\r\n\r\n      # labels shape = (64, 12, 86, 98) to get (1,64,12,86,98)\r\n      self.labels = np.expand_dims(self.labels, axis = (0,-1))\r\n      # outputs shape (8,64,12,86,98,1) to -> (1,64,12,86,98,8)\r\n      self.outputs = np.swapaxes(self.outputs, 0,-1)\r\n      \r\n      self.build_model()\r\n\r\n      self.model.fit([self.outputs, self.labels],\r\n                                batch_size = 1,\r\n                                epochs = 200)\r\n```\r\nAlso, dice_loss and dice_coeff loss should be added, which is defined as follows:\r\n```\r\ndef dice_coef(y_true, y_pred, smooth=1):\r\n    \r\n    y_true_f =  K.cast(K.flatten(y_true), dtype='float32')\r\n    y_pred_f =  K.cast(K.flatten(y_pred), dtype='float32')\r\n\r\n    intersection = y_true_f * y_pred_f\r\n    score = (2. * K.sum(intersection) * smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\r\n\r\n    return score\r\n\r\ndef dice_loss(y_true, y_pred, smooth=1):\r\n    return -dice_coef(y_true, y_pred)\r\n```", "That's not a bug, but an error in your code. \r\nThe input_shape of your model is (None,1,64,12,86,98,8), but the shape of x data you feed into it is (1,64,12,86,98,8). They don't match.\r\n\r\nIn Tensorflow2+ or keras, all the model will leave the first dimension of data's shape for batch-size, for example, one single sample of X has a shape (8,8),  corresponding, the input-shape of model will be (None, 8, 8), where \"None\" stands for \"batch-size\" .", "@danielfllaneza \r\nCan you please follow the links shared, the issue reported is due to error in your code.", "Thanks for your replies.\r\nI have tried the following:\r\n1) [#10053](https://github.com/keras-team/keras/issues/10053)\r\nas suggested by Dref360\r\n```\r\n# method def run_model\r\n(...)\r\nself.outputs = self.outputs.reshape([-1,1,64,12,86,98])\r\n(...)\r\nself.model.fit([self.outputs, self.labels],\r\n                                batch_size = 1,\r\n                                epochs = 200)\r\n\r\n# Error\r\nValueError: Error when checking input: expected input_1 to have 8 dimensions, but got array with shape (1, 1, 64, 12, 86, 98, 8)\r\n```\r\n2) [Stackoverflow](https://stackoverflow.com/questions/41563720/error-when-checking-model-input-expected-convolution2d-input-1-to-have-4-dimens)\r\n```\r\n# method def run_model\r\n(...)\r\nself.outputs = np.expand_dims(self.outputs, axis = 0)\r\n(...)\r\nself.model.fit([self.outputs, self.labels],\r\n                                batch_size = 1,\r\n                                epochs = 200)\r\n\r\n# Error\r\nValueError: Error when checking input: expected input_1 to have 8 dimensions, but got array with shape (1, 1, 64, 12, 86, 98, 8)\r\n```\r\n3) [DataScience Exchange](https://datascience.stackexchange.com/questions/55975/valueerror-error-when-checking-input-expected-input-1-to-have-4-dimensions-bu)\r\n```\r\n# method build model\r\ninp = Input(shape = self.outputs.shape, batch_size = 1)\r\n(...)\r\n#method run model\r\n(...)\r\nself.outputs = np.array([self.outputs])\r\n(...)\r\n# Error\r\nValueError: Error when checking input: expected input_1 to have 8 dimensions, but got array with shape (1, 1, 64, 12, 86, 98, 8)\r\n```\r\n ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41452\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41452\">No</a>\n"]}, {"number": 41450, "title": "use_multiprocessing=True with data generators gives error `CUDA_ERROR_NOT_INITIALIZED: initialization error`", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux 7 (Core)\r\n- TensorFlow installed from (source or binary): Source (conda)\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.7.6\r\n- CUDA/cuDNN version: CUDA 10.1.243/cuDNN 7.6.5\r\n- GPU model and memory: NVIDIA Quadro P4000, 8GB\r\n\r\n**Describe the current behaviour**\r\n\r\nI'm using a custom data loader to train a U-Net and passing `use_multiprocessing=True` to `model.fit`. Training proceeds as expected, but occasionally freezes mid-training with the error `tensorflow/stream_executor/cuda/cuda_driver.cc:219] Failed setting context: CUDA_ERROR_NOT_INITIALIZED: initialization error`. So far, the error has not appeared when I use `use_multiprocessing=False`, which leads me to believe it's caused by multiprocessing.\r\n\r\nI'm not sure I can provide code that reproduces this error on another system, but any possible fixes would be appreciated.\r\n", "comments": ["@tom-andersson,\r\n> I'm not sure I can provide code that reproduces this error on another system, but any possible fixes would be appreciated.\r\n\r\nIs it possible for you to provide a minimal working example to reproduce the issue reported here?\r\n\r\nAlso, please check [this guide](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) to limit GPU memory growth and check if you are still facing the same issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Hi all, \r\nI used multiprocess with apply_async to run multi process with GPUs. I trained model with use_multiprocessing=False and config.gpu_options.allow_growth = True but it returned error:\r\n```\r\n2021-10-06 09:12:50.537049: F tensorflow/stream_executor/cuda/cuda_driver.cc:153] Failed setting context: CUDA_ERROR_NOT_INITIALIZED: initialization error\r\n2021-10-06 09:12:50.541392: F tensorflow/stream_executor/cuda/cuda_driver.cc:153] Failed setting context: CUDA_ERROR_NOT_INITIALIZED: initialization error\r\n2021-10-06 09:12:50.543215: F tensorflow/stream_executor/cuda/cuda_driver.cc:153] Failed setting context: CUDA_ERROR_NOT_INITIALIZED: initialization error\r\n2021-10-06 09:12:50.558302: F tensorflow/stream_executor/cuda/cuda_driver.cc:153] Failed setting context: CUDA_ERROR_NOT_INITIALIZED: initialization error\r\n2021-10-06 09:12:50.656339: F tensorflow/stream_executor/cuda/cuda_driver.cc:153] Failed setting context: CUDA_ERROR_NOT_INITIALIZED: initialization error\r\n```"]}, {"number": 41449, "title": "'ReadVariableOp:value:0 is not found' when converting savedModel to TFLite", "body": "**System information**\r\n- OS Platform and Distribution: OS X 10.14.6\r\n- TensorFlow installed from: Binary (pip3)\r\n- TensorFlow version: 2.2.0\r\n\r\nMy initial issue was very similar to this:\r\nhttps://stackoverflow.com/questions/58499146/how-do-i-convert-tensorflow-2-0-estimator-model-to-tensorflow-lite\r\nso having followed the advice there and at issue #34350 I am running the following code.\r\n\r\n```\r\n    saved_model_obj = tf.saved_model.load(export_dir=args.model_dir)\r\n    concrete_func = saved_model_obj.signatures['serving_default']\r\n\r\n    converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\n\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.experimental_new_converter = True\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                           tf.lite.OpsSet.SELECT_TF_OPS]\r\n\r\n    tflite_model = converter.convert()\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"converter.py\", line 32, in <module>\r\n    main()\r\n  File \"converter.py\", line 26, in main\r\n    tflite_model = converter.convert()\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 472, in convert\r\n    graph=frozen_func.graph)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/lite/python/util.py\", line 218, in run_graph_optimizations\r\n    return tf_optimizer.OptimizeGraph(config, meta_graph)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/grappler/tf_optimizer.py\", line 58, in OptimizeGraph\r\n    graph_id, strip_default_attributes)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Return readvariableop -> ReadVariableOp:value:0 is not found.\r\n        In function output readvariableop:float\r\n```\r\n\r\nThe full output -including the function_optimizer lines- and the saved model are attached.  The saved model was exported from an estimator using estimator.export_saved_model().\r\n\r\n[full_output.txt](https://github.com/tensorflow/tensorflow/files/4931075/full_output.txt)\r\n[saved_model.pb.zip](https://github.com/tensorflow/tensorflow/files/4931060/saved_model.pb.zip)\r\n\r\nMany thanks in advance.\r\n\r\n\r\n\r\n", "comments": ["Could you try conversion with tf-nightly and how about using from_saved_model path directly instead of from_concrete_functions method?", "@abattery Thanks for your reply.  Converting with tf-nightly gives the same error using the code above.  Converting **without** tf-nightly using from_saved_model directly gives\r\n```\r\nValueError: This converter can only convert a single ConcreteFunction. Converting multiple functions is under development.\r\n```\r\nHence why I am using concrete functions as per the stackoverflow link in my original post.\r\nConverting with tf-nightly **and** saved_model_path gives the following error:\r\n```\r\n2020-07-17 08:44:00.859832: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-07-17 08:44:00.876263: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7faa7d76dec0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-07-17 08:44:00.876286: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-07-17 08:44:01.871291: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:118] None of the MLIR optimization passes are enabled (registered 1)\r\nTraceback (most recent call last):\r\n  File \"converter.py\", line 32, in <module>\r\n    main()\r\n  File \"converter.py\", line 25, in main\r\n    converter = tf.lite.TFLiteConverter.from_saved_model(args.model_dir)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 1039, in from_saved_model\r\n    raise ValueError(\"Only support a single signature key.\")\r\nValueError: Only support a single signature key.\r\n```\r\n\r\nI have not seen this error before and can't find much about it.\r\nThanks again\r\n", "@ow4g18 Sorry for the late reply. Could you share your code for from_saved_model path in the tf-nightly version? You need to specify the one concrete function in the conversion.", "@abattery No problem.  My code is below.\r\n```\r\n    converter = tf.lite.TFLiteConverter.from_saved_model(os.path.join(args.model_dir))\r\n    tflite_model = converter.convert()\r\n\r\n    with open(os.path.join(args.model_dir, 'model.tflite'), 'wb') as f:\r\n        f.write(tflite_model)\r\n```\r\nThis gives error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"converter.py\", line 33, in <module>\r\n    main()\r\n  File \"converter.py\", line 26, in main\r\n    converter = tf.lite.TFLiteConverter.from_saved_model(os.path.join(args.model_dir))\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 1075, in from_saved_model\r\n    raise ValueError(\"Only support a single signature key.\")\r\nValueError: Only support a single signature key.\r\n```\r\n\r\nThis seems to be the standard way of converting a model, but I can't find much about this error online.\r\n\r\nThanks!\r\n", "@ow4g18 Thank you for sharing! Could you try conversion with explicitly setting signature_keys and tags arguments?", "@abattery Thanks, it works!  For others looking: I replaced\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(os.path.join(args.model_dir))\r\n```\r\nwith \r\n```\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(os.path.join(args.model_dir),\r\n                                      signature_keys=['serving_default'],\r\n                                      tags=['serve'])\r\n```\r\nThe tags flag may not be necessary as 'serve' seems to be the default anyway, but this solved it.\r\n\r\nThanks again!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41449\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41449\">No</a>\n"]}, {"number": 41447, "title": "Changed the placeholder function in line no. 2588", "body": "As it is tensorflow r1.15 the function should have been tf.placeholder(tf.float32,shape=(1024,1024)) instead of tf.compat.v1.placeholder(tf.float32, shape=(1024, 1024)).", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41447) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41447) for more info**.\n\n<!-- ok -->", "Is there a failure you are seeing?", "In tensorflow1 there is no module named v1 and so the function written\npreviously would not work. (v1 module is for tensorflow2).\nAnd since it is tensorflow1 documentation so the function should just be\ntf.placeholder.\n\nOn Sat, 18 Jul 2020 at 00:20, Mihai Maruseac <notifications@github.com>\nwrote:\n\n> Is there a failure you are seeing?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/41447#issuecomment-660281258>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ANRAI5GDBKZGKNXFHXMMP23R4CMOBANCNFSM4O35NDXA>\n> .\n>\n", "We are not updating documentation on the website on patch releases (no support to do so). So, even if this PR were to land on a patch release, the website documentation won't be fixed.", "Closing as there is nothing we can do with the PR."]}, {"number": 41446, "title": "windows build issues(finally made it)", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10 x64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.4.0\r\n- Python version: 3.8.3 x64\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): 3.4.1\r\n- GCC/Compiler version (if compiling from source): visual studio 2019\r\n- CUDA/cuDNN version: 11.0/8.0.1\r\n- GPU model and memory: RTX2070 GDDR6 8GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nmany problems with windows build, and I made a few fixes\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n1. cudnn_stub  dependency problem ( https://github.com/tensorflow/tensorflow/issues/41057 )\r\nI simply add dependency in that BUILD\r\n\r\n2. then thrust version unmatch error\r\nI delete thrust version - cub version comparison codes from\r\nbazel-out\\x64_windows-opt\\bin\\external\\local_config_cuda\\cuda\\cuda\\include\\thrust\\system\\cuda\\config.h\r\n\r\n3. finally, big old numpy problem\r\nthis has been very long and unsolvable error, I made same fix as before( simply pip uninstall numpy ( uninstall conda numpy ) and pip install numpy )\r\n\r\n\r\nand still there are too many \"ignoring unknown options\" from cl\r\n\r\n\r\nanyway now I succeeded build and have my wheel file\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.", "comments": ["> anyway now I succeeded build and have my wheel file\r\n\r\n@alanpurple,\r\nIs this still an issue? Please feel free to close the issue if resolved. Thanks!", "this is 4 issues, 3 I fixed myself,  1(warnings) I can't fix\r\n\r\n\r\nthis post is about issues and how to fix", "think of this post as kinda PR since I made fixes", "@alanpurple,\r\nIn this case, could you please submit a new PR for the fix. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@amahendrakar \r\n\r\nIs conda numpy problem unsolvable?", "> Is conda numpy problem unsolvable?\r\n\r\nCould you please describe this issue and provide the steps to reproduce it, so that we can look into it? Thanks!", "@amahendrakar \r\n#35977\r\n\r\nthis still exsits.\r\n\r\nFYI, mine is fresh new windows 10 install, anaconda 2020.07 python 3.8, so nothing uncommon", "@alanpurple,\r\nCan we close this issue since it is already being tracked there? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@amahendrakar \r\n\r\nok", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41446\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41446\">No</a>\n", "@alanpurple you mention this:\r\n\r\n> cudnn_stub dependency problem ( #41057 )\r\n> I simply add dependency in that BUILD\r\n\r\nWhat did you add and where? I am having exactly this problem", "@surak I believe this is what he did: \r\nhttps://github.com/tensorflow/tensorflow/pull/41822/commits/d480a3ad713e50057a2b1a611e7cacd6bbca1eb5\r\n\r\nDidn't realize that cudnn_version.h was already defined somewhere in the file. Anyhow, tried it again with fresh source for 2.3.0 and with this change and it builds successfully on Windows now. ", "I still get a directory without the correct file. All I get is\r\n\r\n```sh\r\nIn file included from tensorflow/stream_executor/cuda/cudnn_stub.cc:15:\r\nbazel-out/k8-py2-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cudnn_header/third_party/gpus/cudnn/cudnn.h:59:10: fatal error: cudnn_version.h: No such file or directory\r\n   59 | #include \"cudnn_version.h\"\r\n      |          ^~~~~~~~~~~~~~~~~\r\ncompilation terminated.\r\nSUBCOMMAND: # //tensorflow/core:function_ops_op_lib [action 'Compiling tensorflow/core/ops/function_ops.cc']\r\n(cd /tmp/eb-w9wkEo/tmpSnNZgZ-bazel-build/execroot/org_tensorflow && \\\r\n```\r\n\r\nAnd this directory has \r\n\r\n```bash\r\nls -la bazel-out/k8-py2-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cudnn_header/third_party/gpus/cudnn\r\ntotal 12\r\ndrwxrwxr-x 2 strube1 jusers 4096 Aug 25 19:27 .\r\ndrwxrwxr-x 3 strube1 jusers 4096 Aug 25 19:27 ..\r\nlrwxrwxrwx 1 strube1 jusers  139 Aug 25 19:27 cudnn.h -> /tmp/eb-w9wkEo/tmpSnNZgZ-bazel-build/execroot/org_tensorflow/bazel-out/k8-py2-opt/bin/external/local_config_cuda/cuda/cudnn/include/cudnn.h\r\n```\r\n\r\nI don't really understand Bazel's syntax. Can you help me with that?", "@isleong @alanpurple turns out that it's somewhere else. cudnn 8 has different headers from that cudnn.h and bazel fails to identify the correct version. It's at `third_party/gpus/cuda_configure.bzl` line 1164:\r\n\r\nThis is the problematic part:\r\n```python\r\n    if [int(x) for x in cuda_config.cudnn_version.split(\".\")] < [8, 0]:\r\n      cudnn_headers = [\"cudnn.h\"]\r\n    else:\r\n      cudnn_headers = [\"cudnn_adv_infer.h\",\r\n                       \"cudnn_adv_train.h\",\r\n                       \"cudnn_cnn_infer.h\",\r\n                       \"cudnn_cnn_train.h\",\r\n                       \"cudnn_ops_infer.h\",\r\n                       \"cudnn_ops_train.h\",\r\n                       \"cudnn.h\",\r\n                       \"cudnn_version.h\",\r\n                      ]\r\n```\r\n\r\nThis cudnn version is somehow wrong. \r\n\r\nThis is my patch:\r\n\r\n```diff\r\n--- third_party/gpus/cuda_configure.bzl.orig\t2020-08-26 18:55:08.405838123 +0200\r\n+++ third_party/gpus/cuda_configure.bzl\t\t2020-08-26 19:24:47.466666178 +0200\r\n@@ -1136,10 +1136,11 @@\r\n         out_dir = \"cuda/bin\",\r\n     ))\r\n \r\n-    if [int(x) for x in cuda_config.cudnn_version.split(\".\")] < [8, 0]:\r\n-      cudnn_headers = [\"cudnn.h\"]\r\n-    else:\r\n-      cudnn_headers = [\"cudnn_adv_infer.h\",\r\n+    #if [int(x) for x in cuda_config.cudnn_version.split(\".\")] < [8, 0]:\r\n+    #  cudnn_headers = [\"cudnn.h\"]\r\n+    #else:\r\n+    cudnn_headers = [\"cudnn_adv_infer.h\",\r\n+                       \"cudnn_backend.h\",\r\n                        \"cudnn_adv_train.h\",\r\n                        \"cudnn_cnn_infer.h\",\r\n                        \"cudnn_cnn_train.h\",\r\n```\r\n\r\nThis will break with older cuDNNs, but turns out that because check fails, I really don't care. I only need this version of TensorFlow for the A100 gpus, which only support CUDA 11 and cuDNN 8+ anyway. "]}, {"number": 41445, "title": "Errors in tf.Session().run or tf.reduce_max", "body": "Question:\r\nopt = sess.run(pred_DFS, feed_dict = {x: clinic_factors_val, keep_prob: 1.0, treatment: treat)}) \r\nopt1, Pat_pred = sess.run([output1,pred_DFS], feed_dict = {x: clinic_factors_val), keep_prob: 1.0, treatment: treat)}) \r\nprint(opt1)\r\nprint(opt)\r\nprint(Pat_pred)\r\nThe relationship between pred_DFS and output1: pred_DFS = tf.reduce_max(output1,axis=1). output1 is an intermediate variable.\r\n\r\nWhen pred_DFS is run alone, the result is wrong, but when pred_DFS and output1 are run together, the result of pred_DFS is correct. Why?\r\n\r\nthe result of a run\uff1a\r\nopt1\uff1a[[0.23930925 0.36091098 0.61886203]]\r\nopt\uff1a[0.72315866]\r\nPat_pred\uff1a[0.61886203]\r\nNote that\uff0cthe results are consistent when output1 is run alone and when pred_DFS and output1 are run together", "comments": ["@Zlz-shoulder \r\nPlease share complete stand alone code for us to replicate the issue faced along with tf version, or a colab gist with the issue faced for analysis.", "> @Zlz-shoulder\r\n> Please share complete stand alone code for us to replicate the issue faced along with tf version, or a colab gist with the issue faced for analysis.\r\nI have shared complete stand alone code and the corrsponding data in https://github.com/Zlz-shoulder/test_errors/tree/master\r\nIn order to run \u2018PH+BMC_matched_test.py\u2019, you only need to assign the path of 'PH+BMC_matched_test.csv' to 'clinic_path' in the  \u2018PH+BMC_matched_test.py\u2019 ", "I have run alone pred_DFS or output1 twice, the results maintain unchange. but when pred_DFS and output1 are run together, the value of output1 is the same as the previous operation, but it is not true for pred_DFS.  It is so weird\uff01", "I changed this function and it's right now: pred_DFS = tf.py_func(np.max, [output1,1], tf.float32).  Is \u2018tf.reduce_max\u2019 something wrong?\uff01", "@Zlz-shoulder \r\nI ran the code shared and [face a different error](https://colab.research.google.com/gist/Saduf2019/7d000818b3f287e7fc63e73155b6c894/untitled284.ipynb), please share all dependencies or share a colab gist for us to analyse the issue faced.\r\nPlease share tf version also.", "You must install lifelines! All dependencies are as follows:\r\npandas                    1.0.3            py36h0573a6f_0\r\ntensorboard               1.12.2           py36he6710b0_0 \r\ntensorflow                1.12.0          gpu_py36he68c306_0\r\ntensorflow-base           1.12.0          gpu_py36h8e0ae2d_0 \r\ntensorflow-gpu            1.12.0               h0d30ee6_0\r\nnumpy                     1.18.1           py36h4f9e942_0\r\nnumpy-base                1.18.1           py36hde5b4d6_1\r\nlifelines                 0.24.6                    <pip>\r\nscipy                     1.4.1            py36h0b6359f_0\r\n", "@Zlz-shoulder \r\nI ran the code shared with the dependencies shared, please [find the error faced.](https://colab.research.google.com/gist/Saduf2019/6ff7fed10a67b4776e3e9267704e26a3/untitled283.ipynb), please share a colab gist with the issue reported.", "I fail to log in my Google account, and I also fail to create a new Google account. But I believe this is a simple problem, and you could find many similar errors on the Internet. In addition. I use the Tensorflor of 1.12.0.", "@Zlz-shoulder \r\nI have resolved that error, and a different error occurred , please refer to this [gist here](https://colab.research.google.com/gist/Saduf2019/0977600b4f838c33b711e738da4d238c/untitled283.ipynb).", "Please delete the line 339-340, which are not important. Many thanks.", "@Zlz-shoulder \r\nCan you please mention the lines here, so its easier or delete them and share the gist.", "Please delete the lines 339-340: \r\nwith open(log_path, 'a') as f:\r\n         f.write(line + '\\n')", "@Zlz-shoulder \r\nPLease let us know if this confirms your issue, as per [gist here.](https://colab.research.google.com/gist/Saduf2019/995fc0057b705d6880bc4da600a207e6/untitled283.ipynb)", "Yes, but your results do not have the questions that I encountered. This may be because you use TensorFlow of version 1.1.15 while I use TensorFlow of version 1.1.12.  In my settings, the output values of pred_DFS are different between When pred_DFS is run alone and when pred_DFS and output1 are run together. Why?", "Is there no one responding here?", "@Zlz-shoulder Sorry for the late response. Can you please use the latest version on tensorflow 1.x i.e., tensorflow 1.15 and see if the issue still persists? Thanks!", "You had tested it on tensorflow 1.1.15 and found no errors, so I think that I do not need to test it on tensorflow 1.1.15. However, I found errors using the exactly the same code on tensorflow 1.1.12. In some of the first comments, I have described in detail the errors I encountered about tensorflow 1.1.12. Could you help me answer my question\uff1f", "@Zlz-shoulder This is likely been fixed in 1.15. So, that is the reason why we recommend using tensorflow 1.14 or later. Thanks!", "Many thanks! But I hope that you can help me comfirm whether it is an error in tensorflow 1.12  or an error with my codes. Thanks!", "@Zlz-shoulder This error is with tensorflow 1.12 and is been fixed in future versions. I would recommend you to use tensorflow versions > 1.14. Thanks!", "OK. Many thanks!", "Closing this issue as it has been resloved. Please add additional comments for us to open this issue again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41445\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41445\">No</a>\n"]}, {"number": 41444, "title": " LSTM crash randomly on Windows 10: Failed to call ThenRnnBackward with model config", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.2\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: Cuda 11, cudnn 7\r\n- GPU model and memory: rtx 2070, 8GB\r\n\r\n**Describe the current behavior**\r\nHi, this is a follow up issue for this one : #37942 \r\nThe proposed answers might help, but the core if the issue is still not found. The only known solution so far is to switch the OS, which is not sufficient.\r\n\r\nWhen using LSTM networks on windows 10, model training crashes randomly with an error message like:\r\n```\r\nFailed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 20, 250, 1, 910, 4, 250] \r\n\t [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\r\n\t [[PartitionedCall_3]] [Op:__inference_train_function_13679]\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nI would expect the code to either throw an comprehensible error message on a certain error or run through well.\r\nBut currently it is throwing error randomly, sometimes after a few steps, sometimes after some epochs.\r\n\r\n**Standalone code to reproduce the issue**\r\n@jlebar (thank you!) created a example notebook, which you can find [here](https://gist.github.com/jliebers/995c3c4da4ad2a6f9376d31ee2470ec5)\r\nHowever, I was able to train that net without problem for 15 epochs, while in the example it crashed after three.\r\nSo you can find the code with my repository [here](https://github.com/MichaelJanz/random-LSTM-Crash-TF-Example)\r\n\r\n\r\n**Other info / logs**\r\n\r\nThis is my terminal output:\r\n```\r\n[[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\r\n2020-07-16 08:35:27.443854: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:118] None of the MLIR optimization passes are enabled (registered 1)\r\n2020-07-16 08:35:28.281334: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-07-16 08:35:28.507116: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:118] None of the MLIR optimization passes are enabled (registered 1)\r\n2020-07-16 08:35:28.527048: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-07-16 08:35:29.449350: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:118] None of the MLIR optimization passes are enabled (registered 1)\r\n2020-07-16 08:35:29.482832: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:118] None of the MLIR optimization passes are enabled (registered 1)\r\n2020-07-16 08:35:29.510930: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:118] None of the MLIR optimization passes are enabled (registered 1)\r\n2020-07-16 08:35:30.417794: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:118] None of the MLIR optimization passes are enabled (registered 1)\r\n2020-07-16 08:35:30.459687: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:118] None of the MLIR optimization passes are enabled (registered 1)\r\n2020-07-16 08:35:30.509665: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:118] None of the MLIR optimization passes are enabled (registered 1)\r\n2020-07-16 08:35:30.560799: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:118] None of the MLIR optimization passes are enabled (registered 1)\r\n2020-07-16 08:35:31.524564: E tensorflow/stream_executor/dnn.cc:616] CUDNN_STATUS_EXECUTION_FAILED\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1957): 'cudnnRNNBackwardData( cudnn.handle(), rnn_desc.handle(), model_dims.max_seq_length, output_desc.handles(), output_data.opaque(), output_desc.handles(), output_backprop_data.opaque(), output_h_desc.handle(), output_h_backprop_data.opaque(), output_c_desc.handle(), output_c_backprop_data.opaque(), rnn_desc.params_handle(), params.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), input_desc.handles(), input_backprop_data->opaque(), input_h_desc.handle(), input_h_backprop_data->opaque(), input_c_desc.handle(), input_c_backprop_data->opaque(), workspace.opaque(), workspace.size(), reserve_space_data->opaque(), reserve_space_data->size())'\r\n2020-07-16 08:35:31.525235: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at cudnn_rnn_ops.cc:1922 : Internal: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 20, 250, 1, 910, 4, 250]\r\n2020-07-16 08:35:31.525583: W tensorflow/core/common_runtime/executor.cc:1086] [/job:localhost/replica:0/task:0/device:GPU:0] Executor start aborting: Internal: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 20, 250, 1, 910, 4, 250]\r\n         [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\r\n2020-07-16 08:35:31.526101: W tensorflow/core/common_runtime/executor.cc:1086] [/job:localhost/replica:0/task:0/device:GPU:0] Executor start aborting: Internal: {{function_node __inference___backward_gpu_lstm_with_fallback_8535_8711_specialized_for_PartitionedCall_3_at___inference_train_function_13679}} {{function_node __inference___backward_gpu_lstm_with_fallback_8535_8711_specialized_for_PartitionedCall_3_at___inference_train_function_13679}} Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 20, 250, 1, 910, 4, 250]\r\n         [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\r\n         [[PartitionedCall_3]]\r\n```\r\n", "comments": ["@MichaelJanz \r\n\r\nI have tried in colab with TF version 2.2 and i am seeing the below error (`IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices`).Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/6cd2080103c8e39754885419a815a76c/untitled.ipynb).Thanks!", "Thanks for the answer and for the investigation!\r\n\r\nPlease note, that several causes already have been discussed in the original thread #37942 \r\n\r\nIt seems to be, that the cause for this error lies within Windows 10, so it will perform different on colab. \r\nAlso the error occurs while running `model.fit()`, not during evaluation (not even getting to that point).\r\n\r\nAs I mentioned, alot of possible causes have been discussed in the original thread, but it was closed without a real solution provided. The only real known solution so far is to switch the OS, which is not sufficient.", "+1 on working in colab but not elsewhere\r\n\r\nI'm surprised this hasn't been an issue for more people. Even switching the OS to linux didn't work for me. I keep thinking it's an issue with more the recent nvidia drivers, but haven't been able to prove it yet.", "Hi all, I've ran into the same issue yesterday. I've managed to reproduce the error with the following code. I'm also on Windows 10, RTX 2060 with Cuda 10.1, driver 451.48, Tensorflow 2.1.0.\r\n\r\nI've already tried some of the suggested fixes in the original [issue](https://github.com/tensorflow/tensorflow/issues/37942)\r\n\r\n```\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\nimport tensorflow as tf\r\nphysical_devices = tf.config.list_physical_devices('GPU')\r\ntry:\r\n\ttf.config.experimental.set_memory_growth(physical_devices[0], True)\r\nexcept:\r\n\tprint('Could not set GPU memory growth')\r\n\tpass\r\n\t\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras.callbacks import EarlyStopping\r\n\r\n\r\nN = 1000\r\nT = 2000\r\nC = 5\r\nbatch_size = 1024\r\n\r\nx = np.zeros((T+1, N, C), dtype='float32')\r\n\r\nfor t in range(T+1):\r\n\tif t == 0:\r\n\t\tx[t,:,] = np.random.normal(0., 1., (N, C))\r\n\telse:\r\n\t\tx[t,:,] = 0.95 * x[t-1,:,] + 0.05 * np.random.normal(0., 1., (N, C))\r\n\r\nx = np.swapaxes(x, 0, 1)\r\n\r\nfor i in range(100):\r\n\tkeras.backend.clear_session()\r\n\tinput_layer = keras.layers.Input(shape=(None, C))\r\n\tnn = keras.layers.LSTM(batch_input_shape=(batch_size, T, C), units=32, return_sequences=True)(input_layer)\r\n\tnn = keras.layers.LSTM(units=16, return_sequences=True)(nn)\r\n\tnn = keras.layers.LSTM(units=8)(nn)\r\n\toutput_layer = keras.layers.Dense(1)(nn)\r\n\tmodel = keras.Model(input_layer, output_layer)\r\n\topt = keras.optimizers.Adam(lr=0.001)\r\n\tmodel.compile(loss='mse', optimizer=opt)\r\n\r\n\tc = EarlyStopping(min_delta=0.001, patience=5, restore_best_weights=True)\r\n\tmodel.fit(x[:750,:-1,:], x[:750,-1,1], batch_size=128, callbacks=[c],\r\n\t\tvalidation_data=(x[750:,:-1,:], x[750:,-1,1]), epochs=100)\r\n```\r\n\r\n", "Any progress on this problem?", "Downgrading driver to 431.86 according to this [post](https://forums.developer.nvidia.com/t/cudnn-lstm-is-broken-above-driver-431-60-unexpected-event-status-1-cuda/108800) seems to have fixed the issue for me. Or at least I'm yet to encounter the same crash in half a day of training.", "\"Solution\" for me was to switch to Ubuntu. The networks, that were causing the errors train very well here. Using cuda11, cudnn8 with TF-2.3 with nvidia-450 driver.\r\n\r\nStill for anyone who cannot swap the os, that is no solution", "This is not a permanent solution, but I managed to make it work again by downgrading the NVIDIA driver to the last stable studio driver (431.86) as suggested here: https://forums.developer.nvidia.com/t/cudnn-lstm-is-broken-above-driver-431-60-unexpected-event-status-1-cuda/108800/2\r\n\r\nYou need to first download the corresponding studio driver from NVIDIA, then uninstall the whatever driver version you have now (in my case 442), then install the 431.86 again. This is trickier than it sounds, as NVIDIA utilities only allow you to downgrade to the previous version, and my case I was several versions ahead.\r\n\r\nI ended up using DDU utility as suggested in other forums to wipe the previous driver from my machine, it did the job nicely (no safe mode was necessary).\r\n\r\nAlso, bear in mind that windows will try to automatically update the driver as soon as it gets a chance (creating the problem again). To avoid this you can disable automatic updates for your drivers following these instructions:  https://www.windowscentral.com/how-disable-automatic-driver-updates-windows-10\r\n\r\nBy the way, it wasn't necessary to apply the previous fixes suggested in the post (set batch_size or memory growth), just downgrading the driver did the trick.\r\n\r\nI hope this helps, I wasted several hours trying to make it work!", "I have the same problem and only the (batch_size + memory growth) workaround or downgrading to unsupported 431.86 driver version seem to help. See my full comment on another related issue https://github.com/tensorflow/tensorflow/issues/37942#issuecomment-699468891", "Same problem here. But I too can confirm that a driver update resolved the issue for me on windows completely (was before on 45x now on 466.77)\r\n\r\nIf the author or anybody else could confirm that, I think this issue can be closed, since we now have a good permanent solution.", "@MichaelJanz It looks like you are using an older Version of Tensorflow . Many bugs have been fixed in the latest version. Could you please execute your code using Latest Version 2.5 and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41444\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41444\">No</a>\n"]}, {"number": 41443, "title": "SERLU activation function support", "body": "here, SERLU activation function works better than elu or RELU or leakyRELU\r\n\r\nhttps://www.researchgate.net/figure/Subplot-a-shows-6-different-activation-functions-where-their-main-differences-sit-in_fig1_326646583\r\n\r\nI wonder, if tensorflow can officially support SERLU in next update", "comments": ["/cc @seanpmorgan ", "good stuff I'm still very far from feeling like I can contribute +!", "> /cc @seanpmorgan\r\n\r\nHappy to have this as a PR in TensorFlow Addons if it is deemed not widely applicable or yet proven by TF core team:\r\nhttps://github.com/tensorflow/addons", "@SungmanHong  Due to lack of recent activity we are closing this issue. Please feel free to reopen the issue if necessary.\r\n\r\n As per @seanpmorgan  comment  could you please submit a  Pull Request  in the addons [repo](https://github.com/tensorflow/addons/pulls).Thanks!"]}, {"number": 41442, "title": "model_from_json return deserialize(config, custom_objects=custom_objects), How to remove this error?", "body": "**I am using this code:**\r\n\r\n```\r\nimport numpy as np\r\nimport cv2\r\nfrom keras.preprocessing import image\r\nimport time\r\n\r\n#-----------------------------\r\n#opencv initialization\r\n\r\nface_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\r\n\r\n#-----------------------------\r\n#face expression recognizer initialization\r\nfrom keras.models import model_from_json\r\nmodel = model_from_json(open(\"facial_expression_model_structure.json\", \"r\").read())\r\nmodel.load_weights('facial_expression_model_weights.h5') #load weights\r\n#-----------------------------\r\n```\r\n**I am getting this error:**\r\n\r\n```\r\nFile \"emotion-analysis-from-video.py\", line 15, in <module>\r\n    model = model_from_json(open(\"facial_expression_model_structure.json\", \"r\").read())\r\n  File \"C:\\Users\\DILSHAD\\Desktop\\lfe\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\model_config.py\", line 116, in model_from_json\r\n    return deserialize(config, custom_objects=custom_objects)\r\n  File \"C:\\Users\\DILSHAD\\Desktop\\lfe\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\serialization.py\", line 105, in deserialize\r\n    return deserialize_keras_object(\r\n  File \"C:\\Users\\DILSHAD\\Desktop\\lfe\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\", line 361, in deserialize_keras_object\r\n    (cls, cls_config) = class_and_config_for_serialized_keras_object(\r\n  File \"C:\\Users\\DILSHAD\\Desktop\\lfe\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\", line 325, in class_and_config_for_serialized_keras_object\r\n    for key, item in cls_config.items():\r\nAttributeError: 'list' object has no attribute 'items'\r\n```\r\n**Here is the result of my pip freeze:**\r\n\r\n```\r\nabsl-py==0.9.0\r\nastunparse==1.6.3\r\ncachetools==4.1.1\r\ncertifi==2020.6.20\r\nchardet==3.0.4\r\ncycler==0.10.0\r\ndecorator==4.4.2\r\nefficientnet==1.1.0\r\ngast==0.3.3\r\ngoogle-auth==1.19.1\r\ngoogle-auth-oauthlib==0.4.1\r\ngoogle-pasta==0.2.0\r\ngrpcio==1.30.0\r\nh5py==2.10.0\r\nidna==2.10\r\nimageio==2.9.0\r\nKeras==2.4.3\r\nKeras-Applications==1.0.8\r\nKeras-Preprocessing==1.1.2\r\nkiwisolver==1.2.0\r\nMarkdown==3.2.2\r\nmatplotlib==3.2.2\r\nnetworkx==2.4\r\nnumpy==1.19.0\r\noauthlib==3.1.0\r\nopencv-contrib-python==4.3.0.36\r\nopt-einsum==3.2.1\r\nPillow==7.2.0\r\nprotobuf==3.12.2\r\npyasn1==0.4.8\r\npyasn1-modules==0.2.8\r\npyparsing==2.4.7\r\npython-dateutil==2.8.1\r\nPyWavelets==1.1.1\r\nPyYAML==5.3.1\r\nrequests==2.24.0\r\nrequests-oauthlib==1.3.0\r\nrsa==4.6\r\nscikit-image==0.17.2\r\nscipy==1.4.1\r\nsix==1.15.0\r\ntensorboard==2.2.2\r\ntensorboard-plugin-wit==1.7.0\r\ntensorflow==2.2.0\r\ntensorflow-estimator==2.2.0\r\ntermcolor==1.1.0\r\ntifffile==2020.7.4\r\nurllib3==1.25.9\r\nWerkzeug==1.0.1\r\nwrapt==1.12.1\r\n```\r\nI run this code about a month ago It runs without any error! Now I created a new virtualenv for it and now it is not working fine!!! Any actionable help with be appreciated!!!", "comments": ["@Dilshad737 \r\nI ran the code shared by you and face a different error, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/bbf213e050ab8b2ff175b0af39baf176/untitled281.ipynb), please share all dependencies to replicate the issue or share a colab gist with the error.\r\n\r\nCan you change your imports from keras to tensorflow.keras and confirm if issue persists?\r\n\r\nWith respect to the error faced refer to below issue:\r\n#35934 #38135 [link](https://forum.cogsci.nl/discussion/5318/unexpected-error-attributeerror-list-object-has-no-attribute-copy) [link](https://stackoverflow.com/questions/33949856/why-i-get-list-object-has-no-attribute-items)\r\n", "> @Dilshad737\r\n> I ran the code shared by you and face a different error, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/bbf213e050ab8b2ff175b0af39baf176/untitled281.ipynb), please share all dependencies to replicate the issue or share a colab gist with the error.\r\n> \r\n> Can you change your imports from keras to tensorflow.keras and confirm if issue persists?\r\n> \r\n> With respect to the error faced refer to below issue:\r\n> #35934 #38135 [link](https://forum.cogsci.nl/discussion/5318/unexpected-error-attributeerror-list-object-has-no-attribute-copy) [link](https://stackoverflow.com/questions/33949856/why-i-get-list-object-has-no-attribute-items)\r\n\r\nI ran the code in Google colab all is working fine but when I run it on my local machine it produces the same error I mentioned earlier!!!!\r\n\r\n", "@Dilshad737 \r\nCan you share the steps followed before you faced this issue, please provide all details.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "gestyy.com/ewzP5P", "@Dilshad737 Please use anaconda environment in your local machine and let me know if the error still persists. I am unable to reproduce this issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41442\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41442\">No</a>\n"]}, {"number": 41441, "title": "complex support for tf.math.argmin and tf.math.argmax", "body": "This PR tries to address the issue raised in #41437\r\ntf.math.argmin and tf.math.argmax don't support complex\r\nThe issue was that there aren't kLt, kGt, kLe and kGe ComparisonDirection in opcode kCompare of complex. Besides, MinValue and MaxValue in xla/literal_util.cc didn't support the minimum and maximum value for complex.\r\nThis PR adds complex support for tf.math.argmin and tf.math.argmax.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41441) for more info**.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41441) for more info**.\n\n<!-- ok -->", "In numpy, it looks like np.argmax is defined as the lexi order of the tuple `(real_part, Imag_part)` based on https://github.com/numpy/numpy/issues/10469\r\n\r\nThe following verifies:\r\n```\r\n>>> import numpy as np\r\n>>> np.argmax([1+0.5j, 1-0.5j])\r\n0\r\n>>> np.argmax([1-0.5j, 1+0.5j])\r\n1\r\n```\r\n\r\nShould the behavior match numpy instead? /cc @tensorflow/api-owners ", "+1 to @yongtang 's comment", "+1. In Math there is no comparison between two complex numbers, so if we want to implement that support then we should make sure to follow existing practices", "Argmax returns the indices of the maximum values ignoring NaNs. \r\nIn numpy, np.argmax return the first number that is NAN when there are nans in array, and np.nanargmax returns the indices of the maximum values ignore nan. Currently, the behavior of argmax in tensorflow ignores nans.\r\nShould tensorflow be consistent with numpy at this point?\r\n\r\nNumpy 1.18.1, Python3.8:\r\n```python3\r\nimport numpy as np\r\nprint(np.argmax([complex(3,4), complex(1, np.nan), complex(np.nan, 2)]))\r\n```\r\n**Output**\r\n```\r\n1\r\n```\r\nHowever, the numpy documentation does not seem to mention it, this may be undefined behavior?\r\n\r\nIn addition , when I try to implement complex support for argmax and argmin, I found it is strange that `lhs_value` and `rhs_value` seem to be swapped in `tensorflow/compiler/xla/client/lib/arithmetic.cc:CreateMinMaxComputation`, when the input is complex type. This causes my argmax always choose the last nan if there are nans in tensor...", "> Let's also add some test where we compare several numbers. Scenarios I would include:\r\n> \r\n> 1. Numbers with same real part\r\n> 2. Numbers with same imaginary part\r\n> 3. Numbers all around the complex space (maybe some roots of unity?)\r\n\r\nIt would be nice to have somewhere centrally in the Tensorflow documentation what the expected behavior on complex numbers is and why. That principle would then determined the behavior of all operations. For example this choice of using the real component would suggest preserve the \r\nelementary operations (addition, multiplication) but use the real component norm as norm function as a general principle for complex numbers. But there is at least one paper on 'complex neural networks', like this one: https://arxiv.org/abs/1705.09792. And poking around on stackoverflow some frustration about incomplete support for complex numbers https://stackoverflow.com/questions/43934487/minimizing-loss-in-a-complex-valued-network-in-tensorflow", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41441) for more info**.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41441) for more info**.\n\n<!-- ok -->", "I try to add test cases in `tensorflow\\compiler\\tests\\argminmax_test.py`. However, it seems that there are some problems in the existing code, the following code cannot pass even in the master branch.\r\n\r\n```python3\r\ndef testNaN(self):\r\n    minmax_types = self.all_types & {np.int32, np.int64}\r\n    for dtype in self.float_types:\r\n      for output_type in minmax_types:\r\n        # argmax NAN tests (ignore nan in tensor)\r\n        self._assertOpOutputMatchesExpected(\r\n            math_ops.argmax,\r\n            axis=0,\r\n            output_type=output_type,\r\n            op_input=np.array([np.nan, 6.0, 7.0, -1.0, 4.0, np.nan, -50.0], dtype=dtype),\r\n            expected=output_type(2))\r\n```\r\n\r\nOutput\r\n```\r\nAssertionError:\r\nArrays are not equal\r\n\r\nnot equal lhs = array(6)\r\nnot equal rhs = array(2)\r\nMismatched elements: 1 / 1 (100%)\r\nMax absolute difference: 4\r\nMax relative difference: 2.\r\n x: array(6)\r\n y: array(2)\r\n\r\n```", "@agarwal-ashish any suggestions on what the semantic should be?", "@wangpengmit  Can you please take a look on this PR ? Thanks!", "> I try to add test cases in `tensorflow\\compiler\\tests\\argminmax_test.py`. However, it seems that there are some problems in the existing code, the following code cannot pass even in the master branch.\r\n> \r\n> ```python\r\n> def testNaN(self):\r\n>     minmax_types = self.all_types & {np.int32, np.int64}\r\n>     for dtype in self.float_types:\r\n>       for output_type in minmax_types:\r\n>         # argmax NAN tests (ignore nan in tensor)\r\n>         self._assertOpOutputMatchesExpected(\r\n>             math_ops.argmax,\r\n>             axis=0,\r\n>             output_type=output_type,\r\n>             op_input=np.array([np.nan, 6.0, 7.0, -1.0, 4.0, np.nan, -50.0], dtype=dtype),\r\n>             expected=output_type(2))\r\n> ```\r\n> \r\n> Output\r\n> \r\n> ```\r\n> AssertionError:\r\n> Arrays are not equal\r\n> \r\n> not equal lhs = array(6)\r\n> not equal rhs = array(2)\r\n> Mismatched elements: 1 / 1 (100%)\r\n> Max absolute difference: 4\r\n> Max relative difference: 2.\r\n>  x: array(6)\r\n>  y: array(2)\r\n> ```\r\n\r\nIs it because `all_types` has unsigned integers?", "@jstzwj  Can you please address Ubuntu Sanity errors? Thanks!", "Thanks for making this contribution!\r\n\r\nI think it might be nicer for the complex argmin/argmax to explicitly extract the real/complex components rather than defining minimum and maximum for complex values. I think this should result in the same performance but avoids having to define an ordering of complex elements at the HLO level.", "@jstzwj  Any update on this PR? Please. Thanks!", "I agree with @majnemer - unless we explicitly want to support the same semantics as numpy, it's probably best *not* to implement argmin/argmax for complex types, but instead force the user to be explicit about the semantics.\r\n\r\nDefining argmin/argmax for complex will force us to also define these semantics elsewhere, for all backends (XLA-HLO, Eigen, etc...).  In Eigen we will not implement complex comparison functions like this, since there is no mathematical basis for it - so TF will need to maintain its own internal workaround. ", "@jstzwj Any update on this PR? Please. Thanks!", "@jstzwj Can you please resolve conflicts? Thanks!", "@jstzwj  Any update on this PR? Please. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@jstzwj Any update on this PR? Please. Thanks!", "It has been 28 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 41440, "title": "TypeError: Unsupported return value from function passed to Dataset.map(): OrderedDict in TFF (multiple feature inputs)", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Red Hat 7.7\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): TF 2.2\r\n- Python version: 3.7\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n\r\nI use TFF and want to create an element_spec as follows:\r\n```\r\nOrderedDict([('x', OrderedDict([\r\n('a', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), \r\n('b', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), \r\n('c', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), \r\n('d', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), \r\n('e', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), \r\n('f', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), \r\n('g', TensorSpec(shape=(None,), dtype=tf.int64, name=None))])), \r\n('y', TensorSpec(shape=(None,), dtype=tf.int64, name=None))])\r\n```\r\nTo this end, I create a preprocess function below:\r\n```\r\ndef preprocess(dataset):\r\n\r\n  def batch_format_fn(self):\r\n\r\n    return collections.OrderedDict(\r\n        x=collections.OrderedDict(\r\n\t    a=tf.TensorSpec(shape=[None,], dtype=tf.int64),\r\n\t    b=tf.TensorSpec(shape=[None,], dtype=tf.int64),\r\n            c=tf.TensorSpec(shape=[None,], dtype=tf.int64),\r\n            d=tf.TensorSpec(shape=[None,], dtype=tf.int64),\r\n            e=tf.TensorSpec(shape=[None,], dtype=tf.int64),\r\n            f=tf.TensorSpec(shape=[None,], dtype=tf.int64)),\r\n        y=tf.TensorSpec(shape=[None,], dtype=tf.int64))\r\n  \r\n  return dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER).batch(BATCH_SIZE).map(batch_format_fn)\r\n```\r\nAnd I call this function as follows:\r\n```\r\nsample_dataset = train_dataset.create_tf_dataset_for_client(train_dataset.client_ids[0])\r\npreprocessed_sample_dataset = preprocess(sample_dataset)\r\n```\r\n\r\nHowever, when I run the program, it gives me an error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 3171, in _wrapper_helper\r\n    self._output_structure = structure.type_spec_from_value(ret)\r\n  File \"/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/data/util/structure.py\", line 439, in type_spec_from_value\r\n    return ctor([(k, type_spec_from_value(v)) for k, v in element.items()])\r\n  File \"/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/data/util/structure.py\", line 439, in <listcomp>\r\n    return ctor([(k, type_spec_from_value(v)) for k, v in element.items()])\r\n  File \"/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/data/util/structure.py\", line 439, in type_spec_from_value\r\n    return ctor([(k, type_spec_from_value(v)) for k, v in element.items()])\r\n  File \"/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/data/util/structure.py\", line 439, in <listcomp>\r\n    return ctor([(k, type_spec_from_value(v)) for k, v in element.items()])\r\n  File \"/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/data/util/structure.py\", line 466, in type_spec_from_value\r\n    (element, type(element).__name__))\r\nTypeError: Could not build a TypeSpec for TensorSpec(shape=(None,), dtype=tf.int64, name=None) with type TensorSpec\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"accident_modify_uk_final3b.py\", line 269, in <module>\r\n    preprocessed_sample_dataset = preprocess(sample_dataset)\r\n  File \"accident_modify_uk_final3b.py\", line 267, in preprocess\r\n    return dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER).batch(BATCH_SIZE).map(batch_format_fn)\r\n  File \"/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1621, in map\r\n    return MapDataset(self, map_func, preserve_cardinality=True)\r\n  File \"/homeanaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 3981, in __init__\r\n    use_legacy_function=use_legacy_function)\r\n  File \"/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 3221, in __init__\r\n    self._function = wrapper_fn.get_concrete_function()\r\n  File \"/homeanaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2532, in get_concrete_function\r\n    *args, **kwargs)\r\n  File \"/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2496, in _get_concrete_function_garbage_collected\r\n    graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n  File \"/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2777, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2667, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 981, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 3214, in wrapper_fn\r\n    ret = _wrapper_helper(*args)\r\n  File \"/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 3177, in _wrapper_helper\r\n    sys.exc_info()[2])\r\n  File \"/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/six.py\", line 702, in reraise\r\n    raise value.with_traceback(tb)\r\n  File \"/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 3171, in _wrapper_helper\r\n    self._output_structure = structure.type_spec_from_value(ret)\r\n  File \"/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/data/util/structure.py\", line 439, in type_spec_from_value\r\n    return ctor([(k, type_spec_from_value(v)) for k, v in element.items()])\r\n  File \"/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/data/util/structure.py\", line 439, in <listcomp>\r\n    return ctor([(k, type_spec_from_value(v)) for k, v in element.items()])\r\n  File \"/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/data/util/structure.py\", line 439, in type_spec_from_value\r\n    return ctor([(k, type_spec_from_value(v)) for k, v in element.items()])\r\n  File \"/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/data/util/structure.py\", line 439, in <listcomp>\r\n    return ctor([(k, type_spec_from_value(v)) for k, v in element.items()])\r\n  File \"/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/data/util/structure.py\", line 466, in type_spec_from_value\r\n    (element, type(element).__name__))\r\nTypeError: Unsupported return value from function passed to Dataset.map(): OrderedDict([('x', OrderedDict([('a', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('b', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('c', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('d', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('e', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('f', TensorSpec(shape=(None,), dtype=tf.int64, name=None))])), ('y', TensorSpec(shape=(None,), dtype=tf.int64, name=None))]).\r\n(env1_TF2.1) [@phoenix9 accidents UK]$ \r\n```\r\nWhat is wrong inside the preprocess function()?", "comments": ["@ymsaputra \r\n\r\nRequest you to fill[ issue template.](https://github.com/tensorflow/tensorflow/issues/new/choose)\r\n\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.\r\n\r\nRequest you to share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "I have updated the platform information. I think the problem happens when calling dataset.map(batch_format_fn) using that OrderedDict return.", "I got this answer here: https://stackoverflow.com/questions/62893523/how-to-build-a-model-using-multiple-features-in-tensorflow-federated"]}, {"number": 41438, "title": "Issue with running post quantized TF Lite on ArmNN", "body": "I am have following setup:\r\n\r\nOriginal Model: Retrained Mobilenetv2 with lesser classes (using TF-Keras APIs)\r\nConverted Model: Post quantized- Full integer quantization- uint8 TFLite Model (using TF-Keras APIs)\r\nTensorflow version: 1.15\r\n\r\nI am trying to run this model on a GPU using ArmNN support.\r\n\r\n_I faced the following issue:_\r\n\r\nOptimize graph and load the optimized graph onto a compute device...\r\nWarning: WARNING: Layer of type Convolution2d is not supported on requested backend GpuAcc for input data type QAsymmS8 and output data type QAsymmS8 (reason: in validate src/runtime/CL/functions/CLGEMMConvolutionLayer.cpp:423: Input data type not compatible with Weights), falling back to the next backend.\r\nWarning: ERROR: Layer of type Convolution2d is not supported on any preferred backend [GpuAcc ]\r\n\r\n_I got the following reply from ArmNN git:_\r\n\r\nThat error occurs if the weights are QSymm8 with per channel quantization (that's QSYMM8_PER_CHANNEL) and the input is not QAsymm8. In this case your input is QAsymmS8 so it's not supported right now.\r\nIn order to use the GpuAcc you would have to retrain the model to use QASymm8 instead of QAsymmS8 or not to use per channel quantization on your weights.\r\n\r\n**Link**: https://github.com/ARM-software/armnn/issues/425\r\n\r\nI don't quite understand how I can do the above mentioned using the TFLite Converter. Please help. Thanks in advance", "comments": ["@kratika5 \r\nPlease share a simple stand alone indented code for us to replicate the issue faced or if possible share a colab gist with the error faced.", "Here is a link to Keras model training and conversion to TFLite:\r\nhttps://colab.research.google.com/drive/1lLSimDVdU2oFYH1HH00uYyzRogWkFCuJ?usp=sharing\r\n\r\nFor the second part, as it runs on an embedded platform, I cannot share a direct colab.\r\nI am using this test code provided by ArmNN and I use the TFLite model generated above. \r\nJust for additional information, pretrained quantized models as well as a quantize-aware trained uint8 model (using TF1.15 Slim) work fine with ArmNN. \r\n", "Could you share which datatype of the model ArmNN is complaining?\r\n\"src/runtime/CL/functions/CLGEMMConvolutionLayer.cpp:423: Input data type not compatible with Weights), falling back to the next backend.\"", "The model I have is a uint8 (post quantized model), the error mentioned above says:\r\nInput/Output Datatypes - QAsymmS8\r\n", "@daverim could you guide this?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41438\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41438\">No</a>\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41438\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41438\">No</a>\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41438\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41438\">No</a>\n", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41438\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41438\">No</a>\n"]}, {"number": 41437, "title": "tf.math.argmin and tf.math.argmax don't support complex", "body": "**System information**\r\n- OS Platform and Distribution: Windows10 1909\r\n- TensorFlow installed from : binary\r\n- TensorFlow version: 2.2.0rc2\r\n- Python version: 3.8.0\r\n- CUDA/cuDNN version: None\r\n\r\n**Describe the current behavior**\r\nThe documentation says that input of `argmax` must be one of the following types: float32, float64, int32, uint8, int16, int8, **complex64**, int64, qint8, quint8, qint32, bfloat16, uint16, **complex128** , half, uint32, uint64.\r\nHowever, I encounter problem when I use complex tensors as input.\r\n\r\n**Standalone code to reproduce the issue**\r\nargmin\r\n```python3\r\na = tf.constant([1j], dtype=tf.complex64)\r\nb = tf.math.argmin(a)\r\n```\r\nargmax\r\n```python3\r\na = tf.constant([1j], dtype=tf.complex64)\r\nb = tf.math.argmax(a)\r\n```\r\n**Output**\r\n```\r\n2020-07-16 10:27:30.370246: F tensorflow/compiler/xla/literal_util.cc:289] Unhandled primitive type 15\r\nAborted (core dumped)\r\n```\r\n", "comments": ["- **tf.math.argmax** : *Returns the index with the largest value across axes of a tensor.*\r\n- **tf.math.argmin**  : *Returns the index with the smallest value across axes of a tensor.*\r\n\r\nBut complex numbers can't tell the larger one and the smaller one....\r\nI think that's the reason why they don't support *complex64* and *complex128*\r\n", "> * **tf.math.argmax** : _Returns the index with the largest value across axes of a tensor._\r\n> * **tf.math.argmin**  : _Returns the index with the smallest value across axes of a tensor._\r\n> \r\n> But complex numbers can't tell the larger one and the smaller one....\r\n> I think that's the reason why they don't support _complex64_ and _complex128_\r\n\r\nThough complex numbers cannot be ordered, many numerical computational softwares, such as numpy and matlab, compare the real part of complex numbers.", "Agreed with @jstzwj , I think the library should be consistent with numpy.", "I tried in colab with TF 2.2, 2.3-rc1 and i am seeing the session is crashed.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/87d420209bafe25f94d3f6b11df4ff49/untitled138.ipynb).Thanks!", "Could reproduce the error with **`Tensorflow Version 2.4.1`**. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/e85c132120cad4dafcef3de086e9eb79/untitled138.ipynb). Thanks!", "I tried  the code in colab  with tf-nightly 2.6.0-dev20210527 & didn't face the issue reported.Please check this [gist](https://colab.research.google.com/gist/sushreebarsa/e0a7d88fbf909a57edd1249f371b3e81/untitled11.ipynb)....Thanks !", "Closing this issue as it is fixed in latest version of TensorFlow. Please feel free to reopen the issue if you still have a concern. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41437\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41437\">No</a>\n"]}, {"number": 41436, "title": "Refactor read and blockfetcher in RamFileBlockCache", "body": "@mihaimaruseac \r\nI have refactored `Read` and `BlockFetcher` to make it more compatible with `C API Modular File`.  Now, `Read` and `BlockFetcher` will return total bytes read.", "comments": []}, {"number": 41435, "title": "Fix typo in estimator min bound", "body": "Should be `2.3.0rc0`, not `2.3.0-rc0`.\r\n\r\nThis should fix the error encountered at google/clusterfuzz#1540", "comments": []}, {"number": 41434, "title": "Move some filesystems to new api", "body": "This PR is part of introducing the C++ api layer changes for Transactional FileSystem support. It requires PR #41433 to be merged before being merged. There will be follow up PRs to add other filesystems and re-enable override keywords once base class FileSystem method signature is changed.", "comments": ["@samikama Can you please check build failures. Thanks!", "@mihaimaruseac I modified these changes to workaround compiler flags. I reverted override comments as well since compiler flag is enforcing override anyway. @gbaned  this PR will be able to compile after #41433  is merged. I will open new PRs for other FileSystem derivatives as well.", "Since we commented extra parameters, this PR doesn't depend on #41433  anymore and can be merged at any time."]}, {"number": 41433, "title": "Introducing WrappedFileSystem for transactions", "body": "This is part of a large PR set of introducing Transactional FileSystems. The class will be updated when a follow up PR updates FileSystem class methods. ", "comments": ["Hmm, this fails to build with errors such as\r\n\r\n```\r\n...\r\n./tensorflow/core/platform/file_system.h:422:30: error: 'tensorflow::WrappedFileSystem::Stat' hides overloaded virtual function [-Werror,-Woverloaded-virtual]\r\n  virtual tensorflow::Status Stat(\r\n                             ^\r\n./tensorflow/core/platform/file_system.h:163:30: note: hidden overloaded virtual function 'tensorflow::FileSystem::Stat' declared here: different number of parameters (2 vs 3)\r\n  virtual tensorflow::Status Stat(const string& fname,\r\n                             ^\r\n./tensorflow/core/platform/file_system.h:428:30: error: 'tensorflow::WrappedFileSystem::DeleteFile' hides overloaded virtual function [-Werror,-Woverloaded-virtual]\r\n  virtual tensorflow::Status DeleteFile(\r\n                             ^\r\n./tensorflow/core/platform/file_system.h:167:30: note: hidden overloaded virtual function 'tensorflow::FileSystem::DeleteFile' declared here: different number of parameters (1 vs 2)\r\n  virtual tensorflow::Status DeleteFile(const string& fname) = 0;\r\n                             ^\r\n...\r\n```", "I guess this is because of the compiler flags `-Werror,-Woverloaded-virtual` . We want to hide the overloaded function here. Most of the time it is unintentional but we need to do that in order to be able split a big change set to smaller ones. It doesn't show up in public builds since this flag is not there. There are 2 ways I can think of to get around this compiler flag. First one is to comment arguments of the WrappedFileSystem and uncomment it when base class changes, which need to be applied to all other FileSystem derived classes as well. Or make a huge PR changing all at the same time. Which one do you prefer.", "You are right. Since we cannot change compile flags, let's try the first approach", "@mihaimaruseac Done. I am going to modify other PRs and open new ones with changes in remaining filesystems however we will need to do a big change in the end to uncomment all overrides and commented arguments at once.", "Agree that we will need a big change to change API at one point but that should be easily done/reviewed since it's mostly a string replace everywhere."]}, {"number": 41432, "title": "Mnist Grads Cpp", "body": "@saxenasaurabh ", "comments": ["SGTM\n\nOn Wed, Jul 15, 2020 at 6:58 PM Saurabh Saxena <notifications@github.com>\nwrote:\n\n> *@saxenasaurabh* commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/c/eager/mnist_gradients_test.cc\n> <https://github.com/tensorflow/tensorflow/pull/41432#discussion_r455464234>\n> :\n>\n> > +\n> +namespace tensorflow {\n> +namespace gradients {\n> +namespace internal {\n> +namespace {\n> +\n> +class CppGradients\n> +    : public ::testing::TestWithParam<std::tuple<const char*, bool, bool>> {\n> + protected:\n> +  void SetUp() override {\n> +    TF_SetTracingImplementation(std::get<0>(GetParam()));\n> +  }\n> +};\n> +\n> +// Creates an Identity op.\n> +Status Identity(AbstractContext* ctx,\n>\n> Should we add these in c/experimental/ops and c/experimental/gradients?\n>\n> \u2014\n> You are receiving this because your review was requested.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/41432#discussion_r455464234>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRJLH6J35NGBB5IKNK3R3ZNDPANCNFSM4O3CUXPQ>\n> .\n>\n\n\n-- \n - Alex\n", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41432) for more info**.\n\n<!-- need_author_consent -->", "Hi, It looks like you did a merge in the wrong direction or something.\r\n\r\nCan you rebase your branch onto master? this is not submittable as is.", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41432) for more info**.\n\n<!-- ok -->", "ReluGrad should depend only on the outputs of the relu op\n\nOn Fri, Aug 7, 2020 at 9:24 AM amturati <notifications@github.com> wrote:\n\n> *@amturati* commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/c/experimental/gradients/math_grad.cc\n> <https://github.com/tensorflow/tensorflow/pull/41432#discussion_r467142307>\n> :\n>\n> > +\n> + private:\n> +  long counter;\n> +  std::vector<AbstractTensorHandle*> forward_inputs;\n> +};\n> +\n> +class ReluGradientFunction : public GradientFunction {\n> + public:\n> +  explicit ReluGradientFunction(std::vector<AbstractTensorHandle*> f_inputs)\n> +      : forward_inputs(f_inputs) {}\n> +\n> +  Status Compute(Context* ctx,\n> +                 absl::Span<AbstractTensorHandle* const> grad_inputs,\n> +                 std::vector<AbstractTensorHandle*>* grad_outputs) override {\n> +    AbstractTensorHandle* upstream_grad = grad_inputs[0];\n> +    AbstractTensorHandle* input_features = forward_inputs[0];\n>\n> Unless I'm misunderstanding, I think the ReluGrad Op asks for features\n> which is the input to the Relu Op\n>\n> \u2014\n> You are receiving this because your review was requested.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/41432#discussion_r467142307>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRMLBYYZ5N3ZJB2CWYTR7QTE5ANCNFSM4O3CUXPQ>\n> .\n>\n\n\n-- \n - Alex\n", "Specifically if we use the input we will dramatically increase memory usage\nfor some models. If you have linear+relu+linear+relu style model, the\ngradient of the linear depends on its inputs, which are the outputs of the\nrelu, so if we can make the gradient of relu depend on its outputs we don't\nneed any additional memory (but dpeending on its inputs might close to\ndouble memory usage).\n\nOn Fri, Aug 7, 2020 at 9:38 AM Saurabh Saxena <notifications@github.com>\nwrote:\n\n> *@saxenasaurabh* commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/c/experimental/gradients/math_grad.cc\n> <https://github.com/tensorflow/tensorflow/pull/41432#discussion_r467149084>\n> :\n>\n> > +\n> + private:\n> +  long counter;\n> +  std::vector<AbstractTensorHandle*> forward_inputs;\n> +};\n> +\n> +class ReluGradientFunction : public GradientFunction {\n> + public:\n> +  explicit ReluGradientFunction(std::vector<AbstractTensorHandle*> f_inputs)\n> +      : forward_inputs(f_inputs) {}\n> +\n> +  Status Compute(Context* ctx,\n> +                 absl::Span<AbstractTensorHandle* const> grad_inputs,\n> +                 std::vector<AbstractTensorHandle*>* grad_outputs) override {\n> +    AbstractTensorHandle* upstream_grad = grad_inputs[0];\n> +    AbstractTensorHandle* input_features = forward_inputs[0];\n>\n> Either is correct in this case since all we care about is the sign\n> <https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/core/kernels/relu_op_functor.h;l=51-55;drc=791f4f2b773208b21ece71e1d37204a91720346d>\n> but I think it would be nice to be consistent with the python gradient\n> function which uses the output.\n>\n> \u2014\n> You are receiving this because your review was requested.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/41432#discussion_r467149084>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRPJ257YFYZN7C4WUMDR7QUXVANCNFSM4O3CUXPQ>\n> .\n>\n\n\n-- \n - Alex\n"]}, {"number": 41431, "title": "Adding filesystem_interface methods for transactions", "body": "This PR introduces part of the changes discussed in https://github.com/tensorflow/estimator/pull/245 . This is part 1 of multiple PRs to introduce transaction support to filesystems.", "comments": []}]