[{"number": 37920, "title": "Cannot import ImageDataGenerator from tensorflow.keras GPU version", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04.6 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.7.5\r\n- Bazel version (if compiling from source): None\r\n- GCC/Compiler version (if compiling from source): None\r\n- CUDA/cuDNN version: 10.1/7.3.1\r\n- GPU model and memory: GeForce RTX 2080Ti 10989MiB\r\n\r\n**Describe the current behavior**\r\nI am transfering from TensorFlow 1 to 2 and my codes are all running correct in TensorFlow 1.13 & 1.15. I met the problem when I try to import the ImageDataGenerator: no matter to use `tf.keras.processing.image` or `tf.python.keras.processing.image`.\r\n\r\n**Describe the expected behavior**\r\nIn TensorFlow 1.13 & 1.15 and TensorFlow 2.0.0 CPU version, using `from tensorflow.keras.preprocessing.image import ImageDataGenerator` can import the ImageDataGenerator normally. But for TensorFlow 2.1.0 GPU version, it shows error even I add `.python` right after `tensorflow`. Here \"CPU version\" or \"GPU version\" means the hardware status of the PC I use. \r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n`from tensorflow.keras.preprocessing.image import ImageDataGenerator # option 1`\r\n`from tensorflow.python.keras.preprocessing.image import ImageDataGenerator # option 2`\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nFor now the only way I think it can work is to revise the content in \"__init__.py\" in somewhere within the TensorFlow package. But it will not be a good solution so I wonder if anyone can help me with this. Thanks. \r\n\r\n`Traceback (most recent call last):`\r\n`  File \"<stdin>\", line 1, in <module>`\r\n`  File \"/home/daiwei/.conda/envs/daiwei_tensorflow/lib/python3.7/site-packages/tensorflow_core/python/keras/preprocessing/__init__.py\", line 23, in <module>`\r\n`    import keras_preprocessing`\r\n`ModuleNotFoundError: No module named 'keras_preprocessing'`\r\n", "comments": ["@TMaysGGS, I have tried to reproduce the issue in google colab but i found no error. For your reference link of gist is [here](https://colab.research.google.com/gist/khimraj/b08fb5e8cc4e06e254742bb335aab8c2/cuda.ipynb)", "> @TMaysGGS, I have tried to reproduce the issue in google colab but i found no error. For your reference link of gist is [here](https://colab.research.google.com/gist/khimraj/b08fb5e8cc4e06e254742bb335aab8c2/cuda.ipynb)\r\n\r\nHi, @khimraj ,\r\n\r\nThank you for you reply. I tried it in Colab too and no error occured. Could it be a problem by me wrongly installing the denpendencies or the order I installed all the necessary packages? \r\n\r\nAnd it is also weird that I can import tensorflow.keras in TensorFlow 2.0.0 CPU ver, but I have to import tensorflow.python.keras in TensorFlow 2.1.0 GPU ver. Since for now I do not have a third computer it is hard for me to prove where the problem is located... The functions I need to import are: \r\n`from tensorflow.keras.models import Model`\r\n`from tensorflow.keras.layers import Input, Dense`\r\n`from tensorflow.keras.optimizers import Adam` \r\n`from tensorflow.keras.utils import plot_model, multi_gpu_model`\r\n\r\nand I have to change them to the codes below to successfully import the functions for tf 2.1.0 - gpu: \r\n`from tensorflow.python.keras.models import Model` \r\n`from tensorflow.python.keras.layers import Input, Dense`  \r\n`from tensorflow.python.keras.optimizers import Adam`\r\n`from tensorflow.python.keras.utils.vis_utils import plot_model`\r\n`from tensorflow.python.keras.utils.multi_gpu_utils import multi_gpu_model`\r\n", "The problem seems to be partially solved. The \"No module named 'keras_preprocessing\" error is because when I install TensorFlow 2.1.0 a new version of Keras-Preprocessing package is installed also, and then after I use \"pip uninstall keras-preprocessing=1.xx\" to remove the old version, the new version of Keras-Preprocessing package is somehow damaged or deleted partially, which causes the module not found but can be imported. Thus the error I showed above should not be a bug of TensorFlow. I am sorry to bother. \r\n\r\nBesides, the import restriction that I need to add \".python\" when I try to import packages in tensorflow.keras (tensorflow.python.keras) cannot be reproduced in any other computer. I used one Windows PC with CPU only and another one with GPU Nvidia 1080 Ti, and using \"from tensorflow.keras import xxx\" is always working. So I wonder if this is a bug or something else, though it is not a severe problem. Now I just always use \"tensorflow.python.keras\" for consistency. \r\n\r\nNow I am closing the issue and sorry again for bothering. \r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37920\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37920\">No</a>\n"]}, {"number": 37919, "title": "2.2-rc2 cherry-pick request: Fix crash in Model.fit() if a gradient is None", "body": "Cherrypick of 908cb44d13b7ea24e20f1c39e90c14928b1018fc. Without this, 2.2 will have caused a regression where a crash occurs in Model.fit() if a gradient is None.", "comments": []}, {"number": 37918, "title": "2.2-rc2 cherry-pick request: Fix crash if set_visible_devices() is used with tf.keras.mixed_precision.", "body": "Cherrypick of f748283ee01059be52da5dada6e2157d9f6732ba. Without this, mixed precision no longer works with set_visible_devices().", "comments": []}, {"number": 37917, "title": "Make some \"experimental\" feature flag private", "body": "And also removes all the related api documentation.\r\n\r\nPiperOrigin-RevId: 303006102\r\nChange-Id: Id135e802e6007e98f30e48a8232a51f7150c7aaa", "comments": []}, {"number": 37916, "title": "Fix regression - keras fit class weights are being cast to int", "body": "PiperOrigin-RevId: 303009740\r\nChange-Id: If06ef79162cc3569088feb0669a679954f0ec0e3", "comments": []}, {"number": 37915, "title": "Fix `GetMatchingPaths` with special chars bug.", "body": "After 5659465166daa218168c1f50f1d63c30f9f2bbd9, `GetMatchingPaths` was converted to use RE2 instead of `fnmatch` as that allows non-local filesystems (e.g., GCS, Hadoop, S3) to also be used from Windows. However, this breaks compatibility between `tf.io.gfile.glob` and Python `glob` and that results in tests silently failing or examples being silently skipped during training.\r\n\r\nThe fix is two-pronged. First, to fix #37758 only, we add regexp replacements for `(` and `)` in the pattern, escaping them before matching. After testing and seeing that this works, we then re-enable `fnmatch` on POSIX environments to reduce binary size, just like we did for mobile platforms.\r\n\r\nFixes #37758 (everywhere) and tensorflow/tensorboard#3260 (on posix platforms).\r\n\r\nTested via `bazel run //tensorflow/python:file_io_test` after adding a test for the pattern in #37758.\r\n\r\nWill need to be cherry-picked onto `r2.2` branch.\r\n\r\nPiperOrigin-RevId: 303009914\r\nChange-Id: Ieab047f63e9ba6bb0ec0499e0fa864f6ca6090ff", "comments": []}, {"number": 37914, "title": "[Grappler] Add Einsum to auto_mixed_precision whitelist", "body": "Einsum is used in some of the official NLP models, and without whitelisting it they see no speedup from auto_mixed_precision.\r\nThe op calls into blas gemm routines, and so it should be treated the same as MatMul.\r\n\r\ncc @reedwm @nluehr ", "comments": []}, {"number": 37913, "title": "Performance of Tensorflow distributed training parameter server strategy is much slower for multi gpu multi worker environment", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: \r\n- TensorFlow installed from (source or\r\nbinary): source\r\n- TensorFlow version (use command below): 1.14 \r\n- Python version: - Bazel\r\nversion (if compiling from source):\r\n- GCC/Compiler version (if compiling from\r\nsource): \r\n- CUDA/cuDNN version: Cuda 10.1 cuDNN 7.6.5\r\n- GPU model and memory: V100\r\n\r\n**Describe the current behavior**\r\nI'm doing a performance test on my object detection model on multiple workers, multiple gpus on each worker environment. I'm testing the parameter server strategy and mirror strategy.\r\n\r\nstrategy | workers | gpus per worker | time per 100 iters\r\n-- | -- | -- | --\r\nno distributed training | 1 | 1 | 53s\r\nmirror strategy + NCCL | 1 | 8 | 78~79s\r\nparameter server strategy | 8 | 1 | 65~70s per 800 iters\r\nparameter server strategy | 8 | 8 | 260s per 800 iters\r\n\r\nI noticed that the time spend on the 8 worker * 8 gpus per worker is much slower than the time spend on 8 worker * 1 gpus per worker + the overhead of 1 worker * 8 gpus per worker.\r\n\r\nAfter I profile the code, I noticed that although we aggregate gradient on each worker and send to the parameter server. Each gpu is reading variable from the parameter server directly. Therefore, the parameter server is sending variables 8*8=64 times.\r\n\r\n**Describe the expected behavior**\r\n\r\nI'm wondering if there's a way to change the strategy so that the parameter server will send the variable to each worker and all gpus on the worker load variables from the worker local memory instead.\r\n\r\nIf possible, could you point me to a few code lines that I can modify.", "comments": ["@yaoyaowd \r\ncould you please share a simple stand alone code for us to replicate this issue on our local.\r\nFor performance issue please provide with xprof traces using [link.](https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras#profiler_service)\r\n", "@yaoyaowd\r\ncould you please  update on the above comment", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 37912, "title": "@tf.Module.with_name_scope AttributeError: __enter__", "body": "**System information** \r\nHave I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): No\r\nOS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Linux Mint 19.3 (ubuntu)\r\nTensorFlow installed from (source or\r\nbinary): binary\r\nTensorFlow version (use command below): tf-nightly\r\nPython version: 3.8 and 3.6\r\n\r\n**Describe the current behavior**\r\nExeption is raised, when @tf.Module.with_name_scope is used. The context manager is not implemented proper.\r\n\r\n**Describe the expected behavior**\r\nwork without error\r\n\r\n**Standalone code to reproduce the issue** \r\n```\r\nimport tensorflow as tf\r\n\r\nclass DummyModel(tf.keras.layers.Layer):\r\n    \r\n    def __init__(self, name = \"DummyModel\", **kwargs):  \r\n        super().__init__(name = name, **kwargs)\r\n\r\n    @tf.Module.with_name_scope\r\n    def build(self, inputs_shape):\r\n        super().build(inputs_shape)\r\n    \r\n    @tf.Module.with_name_scope\r\n    def call(self, inputs):\r\n        return inputs\r\n    \r\ndm = DummyModel()\r\n\r\n\r\nprint(dm([1]))\r\n```\r\n\r\n**Other info / logs** \r\n```\r\nAttributeErrorTraceback (most recent call last)\r\n<ipython-input-7-2dd7f6e61139> in <module>\r\n----> 1 import src.test_use_namescope\r\n\r\n~tf/pose3D/src/test_use_namescope.py in <module>\r\n     17 \r\n     18 \r\n---> 19 print(dm([1]))\r\n\r\n~tf/pose3D/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)\r\n    967         # Eager execution on data tensors.\r\n    968         with backend.name_scope(self._name_scope()):\r\n--> 969           self._maybe_build(inputs)\r\n    970           cast_inputs = self._maybe_cast_inputs(inputs)\r\n    971           with base_layer_utils.autocast_context_manager(\r\n\r\n~tf/pose3D/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py in _maybe_build(self, inputs)\r\n   2364         # operations.\r\n   2365         with tf_utils.maybe_init_scope(self):\r\n-> 2366           self.build(input_shapes)  # pylint:disable=not-callable\r\n   2367       # We must set also ensure that the layer is marked as built, and the build\r\n   2368       # shape is stored since user defined build functions may not be calling\r\n\r\n~tf/pose3D/venv/lib/python3.6/site-packages/tensorflow/python/module/module.py in method_with_name_scope(self, *args, **kwargs)\r\n    286     \"\"\"\r\n    287     def method_with_name_scope(self, *args, **kwargs):\r\n--> 288       with self.name_scope:\r\n    289         return method(self, *args, **kwargs)\r\n    290 \r\n\r\nAttributeError: __enter__\r\n```\r\n", "comments": ["inspected the source code from tf.keras.layers.Layer.\r\nit inherits from tf.Module but does not call `supper().__init__` in `__init__` so the needed code in Module is not executed:\r\n```\r\nif tf2.enabled():\r\n      with ops.name_scope_v2(name) as scope_name:\r\n        self._name_scope = ops.name_scope_v2(scope_name)\r\n    else:\r\n      with ops.name_scope(name, skip_on_eager=False) as scope_name:\r\n        self._scope_name = scope_name\r\n```\r\nso i guess calling `supper().__init__` can fix it. ", "I have tried on colab with TF version 2.2.0-rc1 ,Nightly and was able to reproduce the issue.Please, find the gist [here.](https://colab.sandbox.google.com/gist/ravikyram/c06880409d0adbe4ef6424357a64ab7d/untitled747.ipynb) Thanks!", "At this time, the tf.Module name scope decorator conflicts with Keras's own name scoping mechanisms. For this case, the `name` passed in the layer's constructor will already be used in namescoping the methods in question, so you don't need the additional name scope. If you want finer-grained control over name scopes, consider using tf.Module directly.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37912\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37912\">No</a>\n"]}, {"number": 37911, "title": "[2.2  cherrypick] Make tf2.enabled() default to True.", "body": "", "comments": []}, {"number": 37909, "title": "Direct feeding from GPU memory for Java", "body": "**System information**\r\n- TensorFlow version: 1.15.2\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nSuch feature is currently available in C++ API. It is mentioned as experimental but seems to be working. See [#5902](https://github.com/tensorflow/tensorflow/issues/5902) and particularly [direct session test](https://github.com/tensorflow/tensorflow/issues/5902#issuecomment-410792030).\r\n\r\nThis feature is for Java. It adds 2 things: \r\n* allocate tensor in GPU (GPUBFCAllocator)\r\n* directly feed tensors allocated in GPU (makeCallable/runCallable with CallableOptions)\r\n\r\nThe use case is when one does pre-processing of the data (like image rectification and aberration correction) in GPU and then would like to feed the network without copying data back and forth. This might save quite some time for high resolution imagery.\r\n\r\n**Will this change the current api? How?**\r\nAdd methods to Java API and new JNI calls.\r\n\r\n**Who will benefit with this feature?**\r\nOne who uses Java API. This will increase performance of Java applications that employ data preprocessing before feeding graph.\r\n\r\n**Any Other info.**\r\n* My initial approach: https://github.com/okdzhimiev/tensorflow/tree/r1.15\r\n* Small Java Maven project for testing: [tfhello](https://git.elphel.com/oleg/tfhello)\r\n\r\nIntended use:\r\n* Allocate Tensor in GPU memory\r\n* Get the pointer to that memory\r\n* Run a custom CUDA kernel, writing results to the Tensor memory (JCuda)\r\n* Run inference\r\n\r\nCurrently doing testing.\r\nAs I don't casually do C++, please, have a look and let me know if I'm at least on the right track. Any advice will be greatly appreciated.\r\nThanks.\r\n", "comments": ["CC @gunan @sjamesr \r\n\r\nI've CC'ed folks who own the TF Java backend, but I suspect they don't have spare cycles to pre-review changes.  So I would suggest just creating a PR once you have something working.", "It's working. What's the workflow for creating and running unit tests for TF Java? Are tests required for a PR?", "> What's the workflow for creating and running unit tests for TF Java?\r\n\r\nI'm not sure personally, but have you looked into the existing unit tests to see how they work?\r\n\r\n> Are tests required for a PR?\r\n\r\nYes.", "Ok.\r\n\r\nYes, I saw where the tests are. Was just wondering how to run them but it turned out to be straightforward:\r\n```bazel test //tensorflow/java:ZerosTest```\r\n\r\nThanks for replies."]}, {"number": 37908, "title": "Rename all_reduce_sum_gradients to experimental_aggregate_gradients ", "body": "all_reduce_sum_gradients can be misleading since for certain distributed strategy we don't do all reduce.\r\n\r\nThis PR also disables experimental_aggregate_gradients=False for CentralStroage and ParameterServer. They're not supported at this moment.\r\n\r\nFixes #37765\r\n\r\n<!-- Reviewable:start -->\r\n---\r\nThis change is\u2002[<img src=\"https://reviewable.io/review_button.svg\" height=\"34\" align=\"absmiddle\" alt=\"Reviewable\"/>](https://reviewable.io/reviews/tensorflow/tensorflow/37908)\r\n<!-- Reviewable:end -->\r\n", "comments": ["Is this a cherry pick from master? It's strange it keeps getting more commits added.", "@crccw manually added 77e6258f9296c885d5dea4106f2bb891a5f778c6 because it turns out that is also needed to fix #37765.", "It's from my fork. I made a mistake when resolving merge conflict so there're multiple adding commits to start with (and a force push). Later, reedwm@ also has a related change and goldiegadde@ wants us to keep them in one PR.", "Thanks for confirming"]}, {"number": 37907, "title": "[2.2 cherrypick] Make sure that Keras can use tf2.enabled() during initialization.", "body": "", "comments": []}, {"number": 37906, "title": "No module named '_pywrap_tensorflow_internal'", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n_**System information**_\r\n**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):** Windows 10\r\n**- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:** No\r\n**- TensorFlow installed from (source or binary):** python -m pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.12.0-py3-none-any.whl\r\n(doing a regular pip installation does not work for me)\r\n- **TensorFlow version:** 1.12.0\r\n- **Python** version: 3.6.0\r\n- **Installed using virtualenv? pip? conda?**: pip\r\n- **Bazel version (if compiling from source):** ?? I dont think I have this\r\n- **GCC/Compiler version (if compiling from source):** ?? I dont think I have this\r\n- **CUDA/cuDNN version: ??** I dont think I have this\r\n- **GPU model and memory:** None. I was told I could do this without a gpu. \r\n\r\n**Describe the problem**\r\n\r\nI want to run DarkFlow. Specifically I want to run this: \r\n----------------------------------------------\r\nfrom darkflow.net.build import TFNet\r\n\r\noptions = {\"model\": \"cfg/yolo.cfg\", \r\n           \"load\": \"bin/yolo.weights\", \r\n           \"threshold\": 0.1, \r\n           \"gpu\": 0.0}\r\n\r\ntfnet = TFNet(options)\r\n----------------------------------------------\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n1. I installed tensorflow in the aforementioned manner\r\n\r\n2. I Tried to run the aforementioned code:\r\n\r\nC:\\Users\\gcovillo\\darkflow-master>python darkflowEx.py\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\gcovillo\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])\r\n  File \"C:\\Users\\gcovillo\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\imp.py\", line 296, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\gcovillo\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\gcovillo\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\gcovillo\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow_internal\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"darkflowEx.py\", line 2, in <module>\r\n    from darkflow.net.build import TFNet\r\n  File \"C:\\Users\\gcovillo\\darkflow-master\\darkflow\\net\\build.py\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\gcovillo\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\gcovillo\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\gcovillo\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\gcovillo\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])\r\n  File \"C:\\Users\\gcovillo\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\imp.py\", line 296, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\gcovillo\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\gcovillo\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\gcovillo\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow_internal\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errorshefor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nAny Help would be appreciated. I know there is already a closed case with this but I really have no clue what that guy was saying. ", "comments": ["@gcovillo \r\nplease refer to these existing [issues](https://github.com/tensorflow/tensorflow/issues/37863) on the same error faced\r\n\r\n#36167 (comment)\r\n\r\n#36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204 ", "@Saduf2019 Thank you. I thought I had checked most of these but found that I was running 32 bit instead of 64. This has led to an error about DLLS but I have downloaded Microsoft visual studio 2015 already.\r\n\r\nTraceback (most recent call last):\r\n  File \"darkflowEx.py\", line 2, in <module>\r\n    from darkflow.net.build import TFNet\r\n  File \"C:\\Users\\gcovillo\\darkflow-master\\darkflow\\net\\build.py\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\gcovillo\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Users\\gcovillo\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\gcovillo\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\gcovillo\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\gcovillo\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\gcovillo\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\gcovillo\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 30, in <module>\r\n    self_check.preload_check()\r\n  File \"C:\\Users\\gcovillo\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\platform\\self_check.py\", line 60, in preload_check\r\n    % \" or \".join(missing))\r\nImportError: Could not find the DLL(s) 'msvcp140.dll or msvcp140_1.dll'. TensorFlow requires that these DLLs be installed in a directory that is named in your %PATH% environment variable. You may install these DLLs by downloading \"Microsoft C++ Redistributable for Visual Studio 2015, 2017 and 2019\" for your platform from this URL: https://support.microsoft.com/help/2977003/the-latest-supported-visual-c-downloads\r\n\r\n-fixed this by restarting and updating to MSVC 2015-2019", "Traceback (most recent call last):\r\n  File \"darkflowEx.py\", line 2, in <module>\r\n    from darkflow.net.build import TFNet\r\n  File \"C:\\Users\\gcovillo\\darkflow-master\\darkflow\\net\\build.py\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\gcovillo\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Users\\gcovillo\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\gcovillo\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\gcovillo\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\gcovillo\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\gcovillo\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 64, in <module>\r\n    from tensorflow.core.framework.graph_pb2 import *\r\n  File \"C:\\Users\\gcovillo\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\core\\framework\\graph_pb2.py\", line 7, in <module>\r\n    from google.protobuf import descriptor as _descriptor\r\n  File \"C:\\Users\\gcovillo\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\google\\protobuf\\descriptor.py\", line 47, in <module>\r\n    from google.protobuf.pyext import _message\r\nImportError: DLL load failed: The specified procedure could not be found.\r\n\r\n\r\nfixed this by updating python 3.6.0 to 3.6.1", "C:\\Users\\gcovillo\\darkflow-master>python darkflowEx.py\r\n2020-03-26 10:50:43.178108: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n2020-03-26 10:50:43.185108: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\nTraceback (most recent call last):\r\n  File \"darkflowEx.py\", line 2, in <module>\r\n    from darkflow.net.build import TFNet\r\n  File \"C:\\Users\\gcovillo\\darkflow-master\\darkflow\\net\\build.py\", line 5, in <module>\r\n    from .ops import op_create, identity\r\n  File \"C:\\Users\\gcovillo\\darkflow-master\\darkflow\\net\\ops\\__init__.py\", line 1, in <module>\r\n    from .simple import *\r\n  File \"C:\\Users\\gcovillo\\darkflow-master\\darkflow\\net\\ops\\simple.py\", line 1, in <module>\r\n    import tensorflow.contrib.slim as slim\r\nModuleNotFoundError: No module named 'tensorflow.contrib'\r\n\r\nfixed by downgrading tensorflow to 1.5 \r\n pip install \"tensorflow>=1.15,<2.0\"", "  File \"darkflowEx.py\", line 2, in <module>\r\n    from darkflow.net.build import TFNet\r\n  File \"C:\\Users\\gcovillo\\darkflow-master\\darkflow\\net\\build.py\", line 7, in <module>\r\n    from .framework import create_framework\r\n  File \"C:\\Users\\gcovillo\\darkflow-master\\darkflow\\net\\framework.py\", line 1, in <module>\r\n    from . import yolo\r\n  File \"C:\\Users\\gcovillo\\darkflow-master\\darkflow\\net\\yolo\\__init__.py\", line 2, in <module>\r\n    from . import predict\r\n  File \"C:\\Users\\gcovillo\\darkflow-master\\darkflow\\net\\yolo\\predict.py\", line 7, in <module>\r\n    from ...cython_utils.cy_yolo_findboxes import yolo_box_constructor\r\nModuleNotFoundError: No module named 'darkflow.cython_utils.cy_yolo_findboxes'\r\n\r\n-fixed this by:\r\npython setup.py build_ext --inplace in darkflow folder", "WARNING:tensorflow:\r\nThe TensorFlow contrib module will not be included in TensorFlow 2.0.\r\nFor more information, please see:\r\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\r\n  * https://github.com/tensorflow/addons\r\n  * https://github.com/tensorflow/io (for I/O related ops)\r\nIf you depend on functionality not listed there, please file an issue.\r\n\r\nWARNING:tensorflow:From C:\\Users\\gcovillo\\darkflow-master\\darkflow\\net\\build.py:15: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\r\n\r\nWARNING:tensorflow:From C:\\Users\\gcovillo\\darkflow-master\\darkflow\\net\\build.py:16: The name tf.train.AdadeltaOptimizer is deprecated. Please use tf.compat.v1.train.AdadeltaOptimizer instead.\r\n\r\nWARNING:tensorflow:From C:\\Users\\gcovillo\\darkflow-master\\darkflow\\net\\build.py:17: The name tf.train.AdagradOptimizer is deprecated. Please use tf.compat.v1.train.AdagradOptimizer instead.\r\n\r\nWARNING:tensorflow:From C:\\Users\\gcovillo\\darkflow-master\\darkflow\\net\\build.py:18: The name tf.train.AdagradDAOptimizer is deprecated. Please use tf.compat.v1.train.AdagradDAOptimizer instead.\r\n\r\nWARNING:tensorflow:From C:\\Users\\gcovillo\\darkflow-master\\darkflow\\net\\build.py:19: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.\r\n\r\nWARNING:tensorflow:From C:\\Users\\gcovillo\\darkflow-master\\darkflow\\net\\build.py:20: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\r\n\r\nWARNING:tensorflow:From C:\\Users\\gcovillo\\darkflow-master\\darkflow\\net\\build.py:21: The name tf.train.FtrlOptimizer is deprecated. Please use tf.compat.v1.train.FtrlOptimizer instead.\r\n\r\nWARNING:tensorflow:From C:\\Users\\gcovillo\\darkflow-master\\darkflow\\net\\build.py:22: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\r\n\r\nTraceback (most recent call last):\r\n  File \"darkflowEx.py\", line 9, in <module>\r\n    tfnet = TFNet(options)\r\n  File \"C:\\Users\\gcovillo\\darkflow-master\\darkflow\\net\\build.py\", line 58, in __init__\r\n    darknet = Darknet(FLAGS)\r\n  File \"C:\\Users\\gcovillo\\darkflow-master\\darkflow\\dark\\darknet.py\", line 13, in __init__\r\n    self.get_weight_src(FLAGS)\r\n  File \"C:\\Users\\gcovillo\\darkflow-master\\darkflow\\dark\\darknet.py\", line 47, in get_weight_src\r\n    '{} not found'.format(FLAGS.load)\r\nAssertionError: bin/yolo.weights not found\r\n\r\nyay! were getting closer!", "Took this issue to this: \r\n\r\nhttps://github.com/thtrieu/darkflow/issues/1155", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37906\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37906\">No</a>\n"]}, {"number": 37905, "title": "Updated \"tf.keras.losses.cosine_similarity()\" docs", "body": "Fixes : #37896 \r\n@mihaimaruseac Could you plz review this?", "comments": ["This is not true. Cosine similarity will never be `pi` for example, but the docstring rewrite suggests that as a possibility.\r\n\r\nThe scalar product of two unit norm vectors is between -1 and 1. However, the way this similarity is used in optimization requires the output to be -1 and 0.\r\n\r\nAdding @pavithrasv and @fchollet for discussion between changing only documentation so that it is between -1 and 1 or changing code to have output still be between -1 and 0.", "Let's only update docstring to say output is betwene -1 and 1", "@mihaimaruseac Can you plz see if the docstring is fine??", "@mihaimaruseac I've added the statement for values closer to 1 as you've suggested. I had to make some changes after that to make sure lines didn't get too long. All other statements are same as before. Plz review it.", "> I would also modify the doctest below to include a case where output gets close to 1.\r\n\r\nLGTM, but please don't forget about this part", "@mihaimaruseac I've removed the comments. Plz review", "@ManishAradwad Can you please address Ubuntu Sanity errors? Thanks!", "I went through the errors and it looks like it's failing pylint test. But nothing seems wrong in the docstring. Can somebody plz help?", "Downloading the log shows this near the beginning.\r\n\r\n```\r\nFAIL: Found 1 non-whitelisted pylint errors:\r\ntensorflow/python/keras/losses.py:1686: [C0301(line-too-long), ] Line too long (81/80)\r\n```"]}, {"number": 37903, "title": "[Grappler]Fuse Conv2D + BiasAdd + Relu on GPU.", "body": "Author: Dr. Peng Meng from Tencent Cloud TI-ML team.\r\nGrappler:remapper Conv2D + BiasAdd + Relu OP fuse doesn't work on GPU.\r\nTo fuse these three OP,  FindContractionWithBiasAndActivation needs to know the InputProperties:\r\n![image](https://user-images.githubusercontent.com/13826327/77559148-c38c8d80-6ef6-11ea-8d7e-f2f5ee70be70.png)\r\nTF master code  doesn't call graph_properties.InferStatically() before calling FindContractionWithBiasAndActivation,  FindContractionWithBiasAndActivation always return false, so Conv2D + BiasAdd + Relu cannot be fused.\r\n\r\nThis PR fixes this bug. After this fix, the WDSR model inference latency improves 20% on Nvidia T4 GPU.", "comments": ["Thanks a lot for the fix! Could you please update the unit test to make sure we do not regress on this again?", "@rmlarsen Thanks for your review. I have added the unit test. ", "@mpjlu Can you please resolve conflicts? Thanks!", "> @mpjlu Can you please resolve conflicts? Thanks!\r\n\r\nResolved, Thanks!", "@rmlarsen @ezhulenev could you please review this PR again, thanks."]}, {"number": 37902, "title": "Cannot get TensorFlow to work on Mac", "body": "Hello everybody. I have done some programming in the past in C++, HTML, CSS and PHP but I wanted to get into AI. I had done a bit of python in the past (not very much though) and I thought that TensorFlow would be a good place to start. I am on a Mac (something I plan to change in the summer) and I was trying to follow this tutorial to get set up: https://www.youtube.com/watch?v=gWfVwnOyG78\r\n\r\nTo sum up, this is what I did:\r\n1. I installed Anaconda-Navigator.\r\n2. I made a new environment and added TensorFlow and Keras packages (I might need Keras in the future but it's not relevant for now).\r\n3. I opened PyCharm (I already had it on my computer).\r\n\r\nAt this point I ran into issues. In the video linked above at 8:30 he makes a new project. However, my screen options are far different:\r\n\r\n<img width=\"400\" alt=\"Screen Shot 2020-03-25 at 7 05 21 PM\" src=\"https://user-images.githubusercontent.com/47663532/77558085-98407900-6ecb-11ea-823b-c018b2b8ced8.png\">\r\n \r\nSo, I setup the project as shown in the video above. The Project Interpreter is set up as so:\r\n<img width=\"572\" alt=\"Screen Shot 2020-03-25 at 7 08 04 PM\" src=\"https://user-images.githubusercontent.com/47663532/77558481-06853b80-6ecc-11ea-8921-950c36b98c7d.png\">\r\n\r\nI added the TensorFlow packages:\r\n<img width=\"600\" alt=\"Screen Shot 2020-03-25 at 7 09 17 PM\" src=\"https://user-images.githubusercontent.com/47663532/77558589-287ebe00-6ecc-11ea-8f39-2ea5367b4b87.png\">\r\n\r\nIt said that the packages were installed:\r\n<img width=\"374\" alt=\"Screen Shot 2020-03-25 at 7 09 46 PM\" src=\"https://user-images.githubusercontent.com/47663532/77558679-44825f80-6ecc-11ea-95c2-c5195c36f2b7.png\">\r\n\r\nAlso, when I import TensorFlow it looks like it is recognized:\r\n<img width=\"425\" alt=\"Screen Shot 2020-03-25 at 7 12 37 PM\" src=\"https://user-images.githubusercontent.com/47663532/77558937-9dea8e80-6ecc-11ea-9e0c-8fd2cf8e2ea6.png\">\r\n\r\nHowever, when I run the code I get this error:\r\n\r\n<img width=\"616\" alt=\"Screen Shot 2020-03-25 at 7 13 39 PM\" src=\"https://user-images.githubusercontent.com/47663532/77559067-c2466b00-6ecc-11ea-8b17-f46fe498dbb2.png\">\r\n\r\n\r\nI can't seem to understand what I did wrong. I have spent hours at this point and I am so desperate that I would probably pay someone to help me set it up. I would appreciate any help more that you know. Thanks.", "comments": ["There is no `session.run` in TF 2.0 and later", "You should use  parentheses for print()", "Wow. Thanks guys. I didn't think I actually setup the project correctly. You were right about the code, it was for an older version. I tested some code for TF 2.0 and it worked! Thank you so much."]}, {"number": 37901, "title": "[INTEL MKL] Fix a bug within MklFusedBatchNormOp", "body": "For MklFusedBatchNormGradOp, two placeholder output tensors should be initialized with 0 value\r\nHowever the recent MKL DNNL 1.x integration introduced a new bug. \r\n\r\nThe PR fixes this bug and thus addresses the following unit test failure:\r\n       //tensorflow/python/debug:check_numerics_callback_test\r\n\r\nMinor changes: corrects the content of some  improper comments.", "comments": []}, {"number": 37900, "title": "Copying tensors to GPU fails non-deterministically", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Ubuntu 16.04.6 LTS (GNU/Linux 4.4.0-174-generic x86_64)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: No\r\n- TensorFlow installed from (source or\r\nbinary): `pip install tensorflow==2.2.0rc1`\r\n- TensorFlow version (use command below): v2.2.0-rc0-43-gacf4951a2f 2.2.0-rc1\r\n- Python version: Python 3.7.3\r\n- CUDA/cuDNN version: 10.1, V10.1.243\r\n- GPU model and memory: 2x GeForce GTX 1080 8GB \r\n\r\n**Describe the current behavior**\r\nI have a system with two GPUs. Since I need the fix of https://github.com/tensorflow/tensorflow/issues/33929 I have upgraded from 2.1 to 2.2.0rc0 and 2.2.0rc1. In both cases, I sometimes get the following exception when trying to train on a GPU:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/data/personal/username/deployed/project/project/bin/train.py\", line 86, in <module>\r\n    main()\r\n  File \"/data/personal/username/deployed/project/project/bin/train.py\", line 82, in main\r\n    train()  # pragma: no cover\r\n  File \"/data/personal/username/deployed/project/project/bin/train.py\", line 75, in train\r\n    callbacks=callbacks,\r\n  File \"/data/personal/username/deployed/project/project/objects/models/KerasModel.py\", line 190, in train_on_generator\r\n    callbacks=callbacks,\r\n  File \"/home/username/.local/share/virtualenvs/project-1BQtdZDZ/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 65, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n  File \"/home/username/.local/share/virtualenvs/project-1BQtdZDZ/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 783, in fit\r\n    tmp_logs = train_function(iterator)\r\n  File \"/home/username/.local/share/virtualenvs/project-1BQtdZDZ/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 580, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/username/.local/share/virtualenvs/project-1BQtdZDZ/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 644, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/home/username/.local/share/virtualenvs/project-1BQtdZDZ/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2420, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"/home/username/.local/share/virtualenvs/project-1BQtdZDZ/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1665, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"/home/username/.local/share/virtualenvs/project-1BQtdZDZ/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1746, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/home/username/.local/share/virtualenvs/project-1BQtdZDZ/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 598, in call\r\n    ctx=ctx)\r\n  File \"/home/username/.local/share/virtualenvs/project-1BQtdZDZ/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.\r\n  (0) Unknown:  InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run AddV2: Attempted to set tensor for existing mirror. [Op:AddV2]\r\nTraceback (most recent call last):\r\n\r\n  File \"/home/username/.local/share/virtualenvs/project-1BQtdZDZ/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 241, in __call__\r\n    return func(device, token, args)\r\n\r\n  File \"/home/username/.local/share/virtualenvs/project-1BQtdZDZ/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 130, in __call__\r\n    ret = self._func(*args)\r\n\r\n  File \"/home/username/.local/share/virtualenvs/project-1BQtdZDZ/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 309, in wrapper\r\n    return func(*args, **kwargs)\r\n\r\n  File \"/data/personal/username/deployed/project/project/objects/generators/GeneratorRetinaNet.py\", line 85, in _getitem_pre_anchors\r\n    image, node = self.get_prepped_node(idx=idx)\r\n\r\n  File \"/data/personal/username/deployed/project/project/objects/generators/Generator.py\", line 252, in get_prepped_node\r\n    node = self.get_node(idx=idx)\r\n\r\n  File \"/data/personal/username/deployed/project/project/objects/generators/GeneratorObjectDetection.py\", line 97, in get_node\r\n    node = super(GeneratorObjectDetection, self).get_node(idx=idx)\r\n\r\n  File \"/data/personal/username/deployed/project/project/objects/generators/Generator.py\", line 376, in get_node\r\n    return self._dataFetcher.get_node(idx)\r\n\r\n  File \"/data/personal/username/deployed/project/project/objects/generators/DataFetchers/DataFetcher.py\", line 117, in get_node\r\n    nodes = self.get_nodes(node_index, node_index + 1)\r\n\r\n  File \"/home/username/.local/share/virtualenvs/project-1BQtdZDZ/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\", line 997, in binary_op_wrapper\r\n    return func(x, y, name=name)\r\n\r\n  File \"/home/username/.local/share/virtualenvs/project-1BQtdZDZ/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\", line 1276, in _add_dispatch\r\n    return gen_math_ops.add_v2(x, y, name=name)\r\n\r\n  File \"/home/username/.local/share/virtualenvs/project-1BQtdZDZ/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 480, in add_v2\r\n    _ops.raise_from_not_ok_status(e, name)\r\n\r\n  File \"/home/username/.local/share/virtualenvs/project-1BQtdZDZ/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 6653, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n\r\n  File \"<string>\", line 3, in raise_from\r\n\r\ntensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run AddV2: Attempted to set tensor for existing mirror. [Op:AddV2]\r\n```\r\n\r\nThe odd thing about this is, that is is not deterministic behavior. At one moment it may not work. When I kill the process, wait a minute and try again, it sometimes works. \r\nIn general, retrying one or two times will fix the issue, but currently I'm only successful to train about 10% of the time. Is this a known issue?\r\n\r\nNote that in (TF1.13,) TF2.0 and TF2.1, this seemed to work fine for me.\r\n\r\nBefore training, I always use `os.environ['CUDA_VISIBLE_DEVICES'] = '0` or `'1'`, depending on the GPU I want to use. I have tried replacing this with `tf.config.set_visible_devices` to no avail.\r\n\r\nFor completeness, I do not use TPUs or distributed strategies.", "comments": ["@kriskorrel-cw Can you please add minimum repro code snippet preferrably in colab ?", "As I have written (a lot of) custom code, I currently cannot reproduce the issue with a small code snippet.\r\nI have tried with one of the tensorflow tutorials, in which case the problem has not occurred (yet).", "Alright, here are my not-so-scientific findings:\r\n1. The problem appears to be code-specific, which is demonstrated by the fact that running some tensorflow tutorial code seemed to work fine so far.\r\n2. The problem is very non-deterministic. Yesterday I have tried to run the exact same training script multiple times, each time making sure the the program is fully killed and the GPU is freed before I try again. I had to retry for 9 times before I succeeded in running the script. After that, it failed again for 6 times, after which it succeeded again.\r\n3. I am not sure how machine/environment dependent it is. We have two (almost) identical machines, both with two GPUs. On one machine, I can hardly run the train script. On the other, it works about 90% of the time (very rough estimate). \r\n4. Luckily for me, the problems seems to disappear when upgrading from `v2.2.0-rc0-43-gacf4951a2f 2.2.0-rc1` to tf-nightly: `v1.12.1-28047-g983a5e245e 2.2.0-dev20200325`. This might also help in pinpointing the problem. ", "@kriskorrel-cw, Can we close since the problem is disappeared in tf-nightly. ", "> @kriskorrel-cw, Can we close since the problem is disappeared in tf-nightly.\r\n\r\nWould be fine by me", "Hi @kriskorrel-cw , thanks for reporting the issue, and also verifying with tf-nightly. At this point without a code snippet it is hard to pinpoint the issue. \r\nPlease feel free to reopen if the issue reappears or if you have a reproable code snippet.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37900\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37900\">No</a>\n", "I am reopening this is as the issue persists to this day.\r\nSince tensorflow 2.2.0 is now out, I have tried installing this version and retrying whether I can train a model (consistently), and it appears not. However, for the past few weeks, and still today, I can train models perfectly fine with `tf-nightly`. This is true for all `tf-nightly` versions which are tried over the last weeks/months. Are there somehow any differences between the two packages?\r\n\r\nI also had another look at the stack trace which I posted. I find the following part very odd:\r\n```\r\n  File \"/data/personal/username/deployed/project/project/objects/generators/GeneratorRetinaNet.py\", line 85, in _getitem_pre_anchors\r\n    image, node = self.get_prepped_node(idx=idx)\r\n\r\n  File \"/data/personal/username/deployed/project/project/objects/generators/Generator.py\", line 252, in get_prepped_node\r\n    node = self.get_node(idx=idx)\r\n\r\n  File \"/data/personal/username/deployed/project/project/objects/generators/GeneratorObjectDetection.py\", line 97, in get_node\r\n    node = super(GeneratorObjectDetection, self).get_node(idx=idx)\r\n\r\n  File \"/data/personal/username/deployed/project/project/objects/generators/Generator.py\", line 376, in get_node\r\n    return self._dataFetcher.get_node(idx)\r\n\r\n  File \"/data/personal/username/deployed/project/project/objects/generators/DataFetchers/DataFetcher.py\", line 117, in get_node\r\n    nodes = self.get_nodes(node_index, node_index + 1)\r\n```\r\nThis is all custom written code.\r\n\r\nFor some background: We use a `tf.data.Dataset` for generating data and then train using `model.fit(x=my_dataset)`.\r\nBecause of reasons, basically all our data operations/preparations need to be performed with python code, which cannot be easily converted to tensorflow operations nor automatically converted using autograph.\r\nOur tf.data.Dataset therefore basically consists in a `map`, which executes a `tf.py_function`. \r\nThe part of the stack trace above are all methods that are called within this py_function. I don't see, however, how this has anything to do with the GPU. As far as I know, no tf.data.Dataset operations should have anything to do with the GPU.", "Update: as can be seen in the stack trace, `nodes = self.get_nodes(node_index, node_index + 1)` is the last line of custom code that appears. For debugging, I have replaced this line with\r\n```python\r\nprint(node_index, type(node_index), node_index.device)\r\ntest = node_index + 1\r\nnodes = self.get_nodes(node_index, node_index + 1)\r\n```\r\nWhich outputs `tf.Tensor(42584, shape=(), dtype=int64) <class 'tensorflow.python.framework.ops.EagerTensor'> /job:localhost/replica:0/task:0/device:CPU:0` after which a similar stack trace will appear as before, but indeed crashing on the line `test = node_index + 1` instead.\r\n\r\nNote that `node_index` is the input to the py_function and is generated by `tf.data.Dataset.range(dataset_length)`. \r\n\r\nIf I add a line `node_index = node_index.numpy()`, my code now seems to work fine, but I'm not sure whether the issue is \"solved\" with this.\r\nI still have a lot of questions. Why does it work without this statement on tf-nightly, but not on tensorflow 2.2.0 (rc0/rc1/rc2)? Why does this work when I train on a CPU and not when training on a GPU? Why is this tensor being copied to GPU? Or at least I assume it is, based on the stack trace.\r\n\r\nNote that the output of the print statement is the same for both tensorflow versions, and both when I'm training on a CPU or GPU. And for when it does or doesn't throw an exception.", "Getting the same error today.  Going to try tf-nightly for now.  Want to collab @kriskorrel-cw ?", "@mberezo I would be interested to hear whether your setup and cause of the issue is similar to mine. Maybe this could help in writing a minimal reproducing code snippet. I was unable to do this before.", "@kriskorrel-cw, @mberezo,\r\nCould you please provide a minimal sample code to reproduce the issue reported here. Thanks!", "> @kriskorrel-cw, @mberezo,\r\n> Could you please provide a minimal sample code to reproduce the issue reported here. Thanks!\r\n\r\nCurrently, I'm not able to do this. I cannot share my own code, and I have tried twice to replicate the issue with minimal code, but was unable to do so. When I have more time, I will try again with my own codebase, and strip it down as much as possible to get a minimal reproducing snippet.\r\n\r\nI was hoping that you could figure out, or point in a direction based on the stack trace, the situation I described, or the fact the the problem is not present in tf-nightly, but it is in tensorflow.", "I am experiencing this issue as well. I am training with a tf.dataset and using dataset.map() + a tf.py_function() to process the dataset elements. Can't share code. Maybe I'll try tf-nightly.\r\n\r\nThis is the error message:\r\n`tensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run Mul: Attempted to set tensor for existing mirror. [Op:Mul]`\r\n\r\nThis happens when this line is executed inside the map() -> py_func() -> my_func():\r\n\r\n` img = tf.image.convert_image_dtype(img, tf.float32)`", "Update: If I put the problematic line (and related lines) inside a `with tf.device('/device:cpu:0'):` block, then I don't get the error and everything works well. The idea was to prevent a copy from CPU to GPU, and this achieves that idea. Not sure what happened, but it's a quick-fix.", "Update 2: Another part of my pipeline reproduced this issue. Instead of putting it in a with `tf.device()` block as above, I turned OFF \"num_parallel_calls\" in my dataset.map() function. The error went away. It seems there is something about the `num_parallel_calls` attribute that triggers this error.", "@AroMorin Thanks for sharing. Good to know that `num_parallel_calls` is also essential for producing this bug. Your setup seems to be similar to mine. Unfortunately, with that, I'm still not able to reproduce. [This](https://gist.github.com/kriskorrel-cw/72317d7007ee3d3688f884b7a61a4257) is what I tried, btw.\r\n\r\nIs it for you also true that `img = img.numpy()` will 'fix' the issue?", "@kriskorrel-cw The example works with TF 2.2 [gist](https://colab.research.google.com/gist/ymodak/8f56b5df2f4cf1f018a711ad3d3b832a/untitled21.ipynb#scrollTo=SK9SMbuWmrRs)\r\nPlease update the issue when you get a chance and error is reproduced.\r\nThank you.\r\n\r\n@AroMorin  If you can share your example code to reproduce this behavior we can take a look?\r\nThanks!", "@ymodak earlier I said \r\n> @AroMorin Thanks for sharing. Good to know that num_parallel_calls is also essential for producing this bug. Your setup seems to be similar to mine. Unfortunately, with that, I'm still not able to reproduce. This is what I tried, btw.\r\n\r\nHere, I linked to a gist which would work. i.e., no exception could be reproduced with this gist. However, I tested this by running on Google Colab. This, you have also done yourself and found out that there was no problem with it. Today, however, I had the chance to test this code snippet on my own machine, and here it would throw the same exception! So the problem is probably machine/environement-dependent as I already suggested earlier. If you need to know anything about my system, please let me know. Some information is already in the first comment of this issue.\r\n\r\nSo, to reiterate, when running [this exact script](https://gist.github.com/kriskorrel-cw/72317d7007ee3d3688f884b7a61a4257) on my machine, using tf `v2.2.0-rc4-8-g2b96f3662b 2.2.0` I (stochastically) get the following [complete output](https://gist.github.com/kriskorrel-cw/c60035e8f2bcc56337edfd535648f4e6)\r\n\r\n", "Hi @kriskorrel-cw,\r\n\r\nUnfortunately, I don't think we'll be able to productively debug a machine dependent issue that we can't reproduce.", "Hi @AroMorin,\r\n\r\nWere you able to reproduce the issue using tf-nightly?  If yes, do you mind sharing a reproducer?", "> Hi @kriskorrel-cw,\r\n> \r\n> Unfortunately, I don't think we'll be able to productively debug a machine dependent issue that we can't reproduce.\r\n\r\nI understand that. At least there seems to be a workaround (calling `.numpy()` within the py_function) to make it work on tf 2.2.0. Hopefully someone or something will allow to debug this in the future.", "I am also able to reproduce the error with my code in TF 2.2. \r\nI looked up the place in the TF code with the error message, and found this commit to 2.3 that sounds like it might be relevant to the problem https://github.com/tensorflow/tensorflow/commit/6aee08c765b08f1f91663a32e89913336def2ef2\r\n\r\nSo I upgraded to tf-nightly, and I'm no longer seeing the issue.\r\n\r\n@sanjoy when many users report a flaky issue with the same error message, there are things one can do without the repro.  One way could be to find the line with the error message in the code search, and try to ask, \"what are the potential race conditions that would trigger this error message?\" This approach worked for me pretty often.", "I encounter this error in a different context, it was hard to reproduce at first but after some work, i was able to make a [minimum repro code snippet ](https://colab.research.google.com/drive/1RErUzZXUUfg-VNjjczspBex17R2KSfrl?usp=sharing) \r\n(make sure to use a GPU to reproduce the error)\r\nhope it helps\r\n@sanjoy ", "other remarques :\r\n\r\n- when i re-run the script after the error rise, it will work fine\r\n\r\n- when i encountered the error in my project it was non-deterministic. but after making the script above the error seems to accrue all of the time.", "@xnio94 thanks for the reproducer, but it seems to work fine on my machine.\r\n\r\n@jaingaurav any guesses on why we'd get this error: \"AddV2: Attempted to set tensor for existing mirror. [Op:AddV2]\"?\r\n", "@sanjoy: I think the description in 6aee08c that @pshved noted explains it correctly. There are basically some race conditions that we need to more gracefully handle. From what I'm reading the issue is not reproducible after that fix correct?", "Yep @jaingaurav I have never seen this issue after I upgraded to the nightly version of TF, which contained 6aee08c.  I see it sometimes when I forget to switch to the nightly version and when my python env still remains at TF 2.2.  Which seems to indicate that 6aee08c does fix this problem.", "FWIW, I'm getting this issue with the below code. It works the first time and fails on subsequent attempts.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport multiprocessing as mp\r\n\r\ntest = np.random.binomial(1,.3,80000).reshape((1000,80))\r\n\r\ndef run_release_gpu(func):\r\n    def parallel_wrapper(output_dict, *argv, **kwargs):\r\n        ret = func(*argv, **kwargs)\r\n        # print(ret)\r\n        if ret is not None:\r\n            output_dict['ret'] = ret\r\n\r\n    def outer_wrapper(*argv, **kwargs):\r\n        same_process = kwargs.pop('same_process', False)\r\n        if same_process:\r\n            return func(*argv, **kwargs)\r\n\r\n        with mp.Manager() as manager:\r\n            output = manager.dict()\r\n            args = (output,) + argv\r\n            p = mp.Process(target=parallel_wrapper, args=args, kwargs=kwargs)\r\n            p.start()\r\n            p.join()\r\n            ret_val = output.get('ret', None)\r\n\r\n        return ret_val\r\n\r\n    return outer_wrapper\r\n\r\n\r\n@run_release_gpu\r\ndef tf_hamming(a, b):\r\n    l = a.shape[1]\r\n    res = tf.math.divide_no_nan(\r\n        tf.reduce_sum(\r\n            tf.cast(\r\n                tf.not_equal(tf.expand_dims(a, axis=1), tf.expand_dims(b, axis=0)),\r\n                tf.float16)\r\n            , axis=-1),\r\n        l)\r\n    return res\r\n```\r\n\r\nI have the same GPU as OP. \r\npython 3.8.3\r\nUbuntu 20.04 \r\nTensorflow 2.3.1\r\n\r\n```\r\nFile \"/home/.../.local/share/virtualenvs/tf/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/home/.../.local/share/virtualenvs/tf/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\", line 435, in expand_dims_v2\r\n    return gen_array_ops.expand_dims(input, axis, name)\r\n  File \"/home/.../.local/share/virtualenvs/tf/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2313, in expand_dims\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/home/.../.local/share/virtualenvs/tf/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 6843, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run ExpandDims: Dst tensor is not initialized. [Op:ExpandDims]\r\n```", "I have also come across this on TF2.4, I had to keep reducing down the size of various params (vocabs, embedding dims) until the model ran.", "@kriskorrel-cw Based on the comments above, this should have been resolved by `TF2.4.1` or above. Can you please check and let us know whether this is resolved or not.\r\n\r\n@xnio94 If this is still an issue for you, can you share the data to test your standalone code? Thanks! \r\n\r\nPlease feel free to close the issue If this was resolved with recent `TF` versions. Thanks!", "> @kriskorrel-cw Based on the comments above, this should have been resolved by `TF2.4.1` or above. Can you please check and let us know whether this is resolved or not.\r\n> \r\n> @xnio94 If this is still an issue for you, can you share the data to test your standalone code? Thanks!\r\n> \r\n> Please feel free to close the issue If this was resolved with recent `TF` versions. Thanks!\r\n\r\n\"unfortunately\" I currently can't reproduce the issue with TF 2.2.0. I don't know the reason of this.\r\nHowever, if the issue seems to be pinpointed and fixed in TF 2.4.1, I assume that this is correct, and wil now close the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37900\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37900\">No</a>\n", "Having the same issue on TF 2.6.0. Using Laptop RTX 3000"]}, {"number": 37899, "title": "tflite | Build using Intel C++ compiler (Windows)", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 2.2.0-rc1\r\n\r\nIs it possible to build the TensorFlow Lite DLL for Windows 10 using the Intel C++ compiler? Would I need to create a new crosstool? Most info I found online is about Linux or building the pip wheel.\r\n\r\nLinux: https://groups.google.com/forum/#!searchin/bazel-discuss/Use$20or$20support$20Intel$20compiler$20in$20Bazel/bazel-discuss/t-ruQRMis8A/gzF9PWY_AQAJ\r\nPip wheel: https://software.intel.com/en-us/articles/intel-optimization-for-tensorflow-installation-guide\r\n\r\nIt would be great if someone could point me in the right direction. Thanks!", "comments": ["Refer https://docs.bazel.build/versions/master/windows.html#using-bazel-on-windows-1\r\nI guess a possible approach would be to set `BAZEL_VC` or `BAZEL_VS` environment variables to the location of the `icc` installation.", "There is a script \"lite\\tools\\make\\build_lib.sh\" to build on linux. I run it and redict the log into a file. Then analysis and write the dependencies of `libtensorflow-lite.a` into `CMakeLists.txt`. Finally build using CMake, and I get a `static lib`\u3002You can try to build in such a way.", "It seems that this is mostly answered.\r\nhttps://www.tensorflow.org/install/source_windows is basic requirement for Windows build.\r\nOnce you have Bazel and compiler setup for your own, you can just build tflite with\r\n```\r\nbazel build //tensorflow/lite:libtensorflowlite.so\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37899\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37899\">No</a>\n", "@ausk Hello! You are to write their own CmakeList?"]}, {"number": 37898, "title": "TensorFlow Lite vs TensorFlow-TRT vs TesnorRT", "body": "Hello\r\n\r\n\r\nI am doing a benchmark between the Jetson Family (TensorRT, TensorFlow-TRT) and Coral (TensorFlow Lite).\r\n\r\n1.\tCan TensorFlow Lite work with any type of GPU? Or ONLY with GPUs of mobile and embedded devices?\r\nFor example, the NVIDIA Tesla T4 has a precision of INT8 like the Coral TPU.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n2.\tHow does TensorFlow Lite optimize? Does it work like Tensorflow-TRT? \r\nhttps://medium.com/tensorflow/high-performance-inference-with-tensorrt-integration-c4d78795fbfe\r\n\u2022\tElimination of layers whose outputs are not used\r\n\u2022\tElimination of operations which are equivalent to no-op\r\n\u2022\tThe fusion of convolution, bias and ReLU operations\r\n\u2022\tAggregation of operations with sufficiently similar parameters and the same source tensor (for example, the 1x1 convolutions in GoogleNet v5\u2019s inception module)\r\n\u2022\tMerging of concatenation layers by directing layer outputs to the correct eventual destination.\r\n\r\n\r\n \r\n\r\n\r\n\r\nThank you\r\n", "comments": ["Would love to see the benchmark results :)", "1. You might be able to run TFLite GPU on a desktop with OpenCL installed.  One of our core engineers managed to do things on his desktop with an Nvidia graphics card, but it's not tuned for it.  Our workgroup sizes are optimized for mobile GPUs primarily.  Also, we can't currently offer support for TFLite GPU on the desktop; you're on your own.\r\n\r\n2. TFLite GPU does some of those techniques you mention, but doesn't do all of them.  For example, we may do fusion and merging of ops, but don't eliminate layers whose outputs are not used.  It also does unique things that are very specific to GPUs, so you can't have a real apples to apples comparison."]}, {"number": 37897, "title": "ERROR: An error occurred during the fetch of repository 'com_google_protobuf' ", "body": "**The following is my configuration:**\r\n\r\n**System information** : Windows 10 Professional\r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow):  No\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Windows 10 Professional\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below):  pip install tensorflow\r\n- Python version:  N/A\r\n- Bazel version (if compiling from source): 0.27.1\r\n- GCC/Compiler version (if compiling from\r\nsource): \r\n- CUDA/cuDNN version:  V10.0/v7.6.1\r\n- GPU model and memory: GeForce 1050 Ti\r\n\r\nWhile running the code cloneed from [URL](https://github.com/tensorflow/federated/tree/master/tensorflow_federated/python/research/gans/experiments/emnist) with \r\n`bazel run run_experiments`\r\n, I encountered the following issue:\r\n\r\n![image](https://user-images.githubusercontent.com/14258909/77538982-91b9fd80-6edb-11ea-8345-1ef2b5f91f1d.png)\r\n\r\nThe masked information is my user name.\r\nCould yout please give me some adcvices?\r\n\r\n\r\n\r\n\r\n", "comments": ["Hey @AshiakerWang \r\nWhat's the output of \r\n```\r\npip show protobuf\r\n```\r\nIf protobuf isn't installed, try installing with \r\n```\r\npip install protobuf\r\n```\r\nAnd then try running your bazel script.", "@theadityasam Thank for your reply !\r\nThe output of  `pip show protobuf` is \r\n\r\n![image](https://user-images.githubusercontent.com/14258909/77710513-338b3880-7009-11ea-8738-a3b0b0f273c3.png)\r\n\r\nI supposed the 'protobuf' package has already been installed.\r\n\r\nBTW, I re-run the `bazel run run_experiments` just now, and found the problem still exists.\r\n", "@AshiakerWang Please take a look at this [comment](https://github.com/tensorflow/tensorflow/issues/35414#issuecomment-601652629) and let me know if it helps.", "@gowthamkpr Sorry for the late response and thank you for your reply. I have read the [comment](https://github.com/tensorflow/tensorflow/issues/35414#issuecomment-601652629) you provided with me, but unfortunately it seems that I have never installed  the 'Windows Subsystem for Linux' accord to [this](https://superuser.com/questions/1065569/how-to-remove-reset-windows-subsystem-for-linux-on-windows-insider-build-14316).\r\n\r\n![image](https://user-images.githubusercontent.com/14258909/77851418-afed6980-720b-11ea-8633-dfcf2eedb252.png)\r\n\r\n![image](https://user-images.githubusercontent.com/14258909/77851463-e1fecb80-720b-11ea-8b14-368887c9c93e.png)\r\n\r\n\r\n\r\n\r\n\r\n", "@AshiakerWang As mentioned in the comment The problem was the incompatibility of Bazel to bash on Windows as stated at https://docs.bazel.build/versions/master/windows.html.\r\nPlease use linux based system. Thanks!", "@AshiakerWang I have the same issue here. Did you fix it?", "I have the same issue as well on Windows 10 - without 'Windows Subsystem for Linux', and when running bazel on CMD. Did anyone find any other reason for having this issue on Windows 10?", "This is more directly reported here:\r\nhttps://github.com/bazelbuild/rules_docker/issues/1958"]}, {"number": 37896, "title": "tf.keras.losses.cosine_similarity() is not a negative quantity", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/losses/cosine_similarity\r\n\r\n## Description of issue (what needs changing):\r\nDocumentation states that tf.keras.losses.cosine_similarity() \"is a negative quantity between -1 and 0, where 0 indicates orthogonality and values closer to -1 indicate greater similarity.\"\r\nBut it is actually not true. tf.keras.losses.cosine_similarity() can return positive values.\r\n\r\n### Usage example\r\n```\r\n>>> import tensorflow as tf\r\n>>> tf.keras.losses.cosine_similarity([[1., 1.]], [[-1., -1.]])\r\n<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.99999994], dtype=float32)>\r\n```\r\n", "comments": ["I'll start working on this.\r\n ", "Please tag me in PR fixing this"]}, {"number": 37895, "title": "run_eagerly argument bug in tf.Keras.Model compile method", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Windows 10, x64\r\n- TensorFlow installed from (source or\r\nbinary): Binary (pip)\r\n- TensorFlow version (use command below): TF 2.2.0 (2.2.0.dev20200324)\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: 10.1 / 7.6\r\n- GPU model and memory: Bug appears on several computers with different GPU\r\n\r\n**Describe the current behavior**\r\n\r\nThe run_eagerly argument of the compile method of tf.Keras models does not actually set the run_eagerly attribute of the model. This causes the model to not be run eagerly even if run_eagerly=True was given as argument to its compile method.\r\n\r\nThis seems to be caused by a recent modification of the arguments of the compile method, as detailed bellow.\r\n\r\n**Describe the expected behavior**\r\n\r\nCalling compile on a Keras model with the argument run_eagerly=True should set the run_eagerly attribute of the model to True, which causes the model to be run eagerly when calling methods such as fit on it.\r\n\r\n**Standalone code to reproduce the issue** \r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nclass CustomLayer(tf.keras.layers.Layer):\r\n\tdef __init__(self):\r\n\t\tsuper().__init__()\r\n\t\t\r\n\tdef call(self, inputs):\r\n\t\ttf.print('Running eagerly: ', tf.executing_eagerly())\r\n\t\treturn inputs\r\n\r\nif __name__ == \"__main__\" :\r\n\tdata = np.random.random((16, 3)).astype(np.float32)\r\n\r\n\tinputs = tf.keras.Input(shape=(3,))\r\n\toutputs = tf.keras.layers.Dense(3)(inputs)\r\n\toutputs = CustomLayer()(outputs)\r\n\tmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\r\n\t\r\n\tmodel.compile(loss='mse', run_eagerly=True) # does not set model.run_eagerly to True, and model does not run eagerly\r\n\t# model.run_eagerly = True # sets model.run_eagerly to True, and model runs eagerly\r\n\r\n\tprint('run_eagerly: ', model.run_eagerly)\r\n\r\n\tmodel.fit(x=data, y=data)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nLogs corresponding to the standalone code: [log.txt](https://github.com/tensorflow/tensorflow/files/4380372/log.txt)\r\n\r\n**Supplementary info regarding the issue**\r\n\r\ncompile's signature is now\r\n```\r\ndef compile(self,\r\n              optimizer='rmsprop',\r\n              loss=None,\r\n              metrics=None,\r\n              loss_weights=None,\r\n              sample_weight_mode=None,\r\n              weighted_metrics=None,\r\n              run_eagerly=None,\r\n              **kwargs):\r\n```\r\nwhen it used to be (notice that run_eagerly=None has been added to its arguments)\r\n```\r\ndef compile(self,\r\n              optimizer='rmsprop',\r\n              loss=None,\r\n              metrics=None,\r\n              loss_weights=None,\r\n              sample_weight_mode=None,\r\n              weighted_metrics=None,\r\n              **kwargs):\r\n```\r\n\r\nHowever, it still sets _run_eagerly as follows `self._run_eagerly = kwargs.pop('run_eagerly', None)` which can no longer work as run_eagerly is no longer in kwargs.\r\n\r\n", "comments": ["the issue is not seen with 2.2.0-rc1, @Lillypucien can you try that version please ", "I can reproduce the issue with `tf-nightly`. [Here](https://colab.research.google.com/gist/jvishnuvardhan/ae6240266965ecb4803c920e67b64347/untitled40.ipynb) is the colab gist for y/our reference. Thanks ", "The issue was indeed not yet in 2.2.0-rc1, but is still present in the latest tf-nightly as shown by @jvishnuvardhan ", "@ymodak TF 2.0 is not the correct tag for this issue, the bug is present in the last tf 2.2 development version", "fixed the labels, this issue is not seen with 2.2.0-rc2 as well, but seen with latest nightly. ", "This should have been fixed in this commit [ee5a324](https://github.com/tensorflow/tensorflow/commit/6f9a5289f9a59164f17a70029df9e9f39ee5a324). \r\n", "@Lillypucien This was resolved in recent `tf-nightly`. [Here](https://colab.research.google.com/gist/jvishnuvardhan/19d45437d71b437c060057f8bb54b250/untitled40.ipynb) is the gist for your reference.\r\n\r\nCan you please verify once and close the issue if this was resolved for you. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37895\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37895\">No</a>\n"]}, {"number": 37894, "title": "How to save/load the partial model for fine-tuning/transfer-learning in TF2.1?", "body": "I would like to build my own base model and train it with big dataset. After training, I save the base model. I have another customized model and I want to load the weights of first two layers from the base model. How should I achieve it in Tensorflow 2.1.0, thanks.\r\n\r\nSample codes:\r\n\r\n    import os\r\n    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\" \r\n    import tensorflow as tf\r\n    from tensorflow import keras\r\n    from tensorflow.keras import layers\r\n\r\n    class BaseModel():\r\n        def __init__(self):\r\n            inputs = keras.Input(shape=(32, 32, 3))\r\n            x = inputs\r\n            x = layers.Conv2D(32, 3, padding='same', activation=tf.nn.relu)(x)\r\n            x = layers.MaxPool2D()(x)\r\n            x = layers.Conv2D(64, 3, padding='same', activation=tf.nn.relu)(x)\r\n\r\n            x = layers.Flatten()(x)\r\n\r\n            x = layers.Dense(500, activation=tf.nn.relu)(x)\r\n\r\n            outputs = layers.Dense(1000, activation=tf.nn.softmax)(x)\r\n\r\n            self.model = keras.Model(inputs=inputs, outputs=outputs)\r\n\r\n        def __call__(self, inputs):\r\n            return self.model(inputs)\r\n\r\n    bm = BaseModel()  # the model for pretraining\r\n    bm.model.save_weights('base_model') # save the pretrained model\r\n\r\n\r\n    class MyModel():\r\n        def __init__(self):\r\n            inputs = keras.Input(shape=(32, 32, 3))\r\n            x = inputs\r\n            x = layers.Conv2D(32, 3, padding='same', activation=tf.nn.relu)(x)\r\n            x = layers.MaxPool2D()(x)\r\n            x = layers.Conv2D(64, 3, padding='same', activation=tf.nn.relu)(x)\r\n            \r\n            x = layers.Conv2D(128, 3, padding='same', activation=tf.nn.relu)(x)\r\n\r\n            x = layers.Flatten()(x)\r\n\r\n            x = layers.Dense(1000, activation=tf.nn.relu)(x)\r\n\r\n            outputs = layers.Dense(10, activation=tf.nn.softmax)(x)\r\n\r\n            self.model = keras.Model(inputs=inputs, outputs=outputs)\r\n\r\n        def __call__(self, inputs):\r\n            return self.model(inputs)\r\n\r\n\r\n    mm = MyModel()  # the model for my customized applications\r\n    mm.model.load_weights('base_model')  # load the pretrained model with the first two conv layers\r\n\r\n    # further fine-tuning or transfer learning\r\n\r\n", "comments": ["@hgffly I have used the custom model available in TF website demonstrate the idea. Using subclassed models is little different from Keras Sequential and functional model. I have used Subclassed model to demo the idea as follows. [Here](https://colab.research.google.com/gist/jvishnuvardhan/0af2d51513b5e17b7abd4431b0d7ddf7/loading_weights.ipynb) is a colab gist for your reference. \r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\n\r\n## base model\r\nclass ThreeLayerMLP(keras.Model):\r\n\r\n  def __init__(self, name=None):\r\n    super(ThreeLayerMLP, self).__init__(name=name)\r\n    self.dense_1 = layers.Dense(64, activation='relu', name='dense_1')\r\n    self.dense_2 = layers.Dense(64, activation='relu', name='dense_2')\r\n    self.pred_layer = layers.Dense(10, name='predictions')\r\n\r\n  def call(self, inputs):\r\n    x = self.dense_1(inputs)\r\n    x = self.dense_2(x)\r\n    return self.pred_layer(x)\r\n\r\ndef get_model():\r\n  return ThreeLayerMLP(name='3_layer_mlp')\r\n\r\nbase_model = get_model()\r\n\r\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\r\nx_train = x_train.reshape(60000, 784).astype('float32') / 255\r\nx_test = x_test.reshape(10000, 784).astype('float32') / 255\r\n\r\nbase_model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n              optimizer=keras.optimizers.RMSprop())\r\nhistory = base_model.fit(x_train, y_train,\r\n                    batch_size=64,\r\n                    epochs=1)\r\n#saving weights\r\nbase_model.save_weights('./base_model_weights', save_format='tf')\r\n\r\n## custom model\r\nclass MyCustomModel(keras.Model):\r\n\r\n  def __init__(self, name=None):\r\n    super(MyCustomModel, self).__init__(name=name)\r\n    self.dense_1 = layers.Dense(64, activation='relu', name='dense_1')\r\n    self.dense_2 = layers.Dense(64, activation='relu', name='dense_2')\r\n    self.pred_layer = layers.Dense(10, name='predictions')\r\n\r\n  def call(self, inputs):\r\n    x = self.dense_1(inputs)\r\n    x = self.dense_2(x)\r\n    return self.pred_layer(x)\r\n\r\ndef get_custom_model():\r\n  return MyCustomModel(name='my_custom_model')\r\n\r\nmy_custom_model = get_custom_model()\r\n\r\nmy_custom_model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n                  optimizer=keras.optimizers.RMSprop())\r\n\r\n# This initializes the variables used by the optimizers,\r\n# as well as any stateful metric variables\r\nmy_custom_model.train_on_batch(x_train[:1], y_train[:1])\r\n\r\n# Load the state of the old model (to load weights for all layers)\r\n# my_custom_model.load_weights('path_to_my_weights')\r\n\r\nlayer_dict = dict([(layer.name, layer) for layer in base_model.layers])\r\nprint(layer_dict)\r\n\r\n# my_custom_model.trainable = True\r\n# loading the weights from base_model\r\nfor layer in my_custom_model.layers:\r\n  layer_name = layer.name\r\n  #print(layer.name)\r\n  layer.set_weights = layer_dict[layer_name].get_weights()\r\n```\r\n\r\nThere are several ways to access and assign model weights. what I have shown in one of them.\r\n\r\nPlease close the issue. In future please post this kind of support questions in stackoverflow as GitHub is mainly for Bugs and performance related questions. Thanks! ", "Thanks for help, but the codes should be modified as following:\r\nlayer.set_weights(layer_dict[layer_name].get_weights()) "]}, {"number": 37893, "title": "[tflite] make label_image build", "body": "label_image depends on old `//tensorflow/lite/delegates/gpu:gl_delegate`, which no long built.\r\nupdate the dependency to use `//tensorflow/lite/delegates/gpu:delegate`", "comments": []}, {"number": 37892, "title": "Converting unsupported operation: PyFunc", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\npycharm\r\n- TensorFlow version (or github SHA if from source):\r\ntensorflow 1.15.0\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\nThis is my code.\r\nimport tensorflow as tf\r\n\r\npath = 'vgg_freeze_model.pb'\r\ninputs = [\"Placeholder\"]\r\ninput_shapes = {\"Placeholder\": [1, 224, 224, 3]}\r\noutputs = [\"vgg_16/cls_prob\"]\r\n\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(path, inputs, outputs, input_shapes=input_shapes)\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                       tf.lite.OpsSet.SELECT_TF_OPS]\r\nconverter.post_training_quantize = True\r\ntflite_model = converter.convert()\r\nopen(\"vgg16.tflite\", \"wb\").write(tflite_model)\r\n\r\n```\r\n# Copy and paste here the exact command\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n# Copy and paste the output here.\r\n```\r\nThis is my issue\r\n2020-03-25 03:36:10.513606: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: PyFunc\r\n2020-03-25 03:36:10.513648: F tensorflow/lite/toco/import_tensorflow.cc:114] Check failed: attr.value_case() == AttrValue::kType (1 vs. 6)\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\n# Put link here or attach to the issue.\r\n```\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- Producing wrong results and/or decrease in accuracy\r\n- Producing correct results, but the model is slower than expected (model generated from old converter)\r\n\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@taoja12,\r\nCould you please share the `vgg_freeze_model.pb` file you are using in the code? Thanks!", "@amahendrakar    https://pan.baidu.com/s/1UyqLfwA6TkUWXcB-69HupA    retrieve password: u6ay", "@taoja12,\r\nI am unable to access the above link. Could you please upload the file on another alternate website? Thanks!", "> @taoja12,\r\n> I am unable to access the above link. Could you please upload the file on another alternate website? Thanks!\r\n\r\nAny updates regarding this issue? Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Now I have the same problem. How did you solve it\uff0cplease", "@uchihaltachi,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!"]}, {"number": 37891, "title": "help me with tensorflow 2.0 installation", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):windows 10 64 bit\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version: 2.0\r\n- Python version:3.7\r\n- Installed using virtualenv? pip? conda?:pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:10.1\r\n- GPU model and memory: nvidia geforce gtx 1050ti\r\n\r\n\r\n\r\ni tried a lots of tutorials to install tensorflow gpu but failed facing different issues mainly while running the object_detection_tutorial.ipynb is not ruuning..\r\n![Screenshot (22)](https://user-images.githubusercontent.com/62600923/77514129-446a6b80-6e9c-11ea-9bed-f807ac43556d.png)\r\n \r\ni already installed the tensorflow gpu but still it is again installing in jupyter notebook..\r\niam fully confused what to do...\r\nplease anyone help me out.\r\n", "comments": ["Since you are working on your local computer ... go to command prompt and do \"pip install tensorflow==2.0.0\". Your work ll be done. Thank you.\r\n", "What is the error you are getting? It's not visible in the screenshot. Please post the error (using ` ``` ` proper markdown around it) instead of a screenshot", "> What is the error you are getting? It's not visible in the screenshot. Please post the error (using `` ``` `` proper markdown around it) instead of a screenshot\r\n\r\n![Screenshot (23)](https://user-images.githubusercontent.com/62600923/77628595-44d33700-6f6e-11ea-98c5-ddbc19d59fe9.png)\r\nthere is a bash error while executing ..", "@0unstoppable, \r\nLooks like issue is not related to Tenosrflow installation. To confirm Tensorflow installation, try \r\n```\r\nimport tensorflow as tf\r\ntf.__version__\r\n```\r\nFor bash error please refer this\r\nhttps://stackoverflow.com/questions/37335201/jupyter-notebook-couldnt-find-program-bash .Thanks", "Issue not related to TensorFlow, closing.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37891\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37891\">No</a>\n"]}, {"number": 37890, "title": "[tflite] add xnnpack delegate to label_image", "body": "rebase and resubmit #36749 to see if it works.\r\n\r\nThe [XNNPACK Delegate](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/delegates/xnnpack) uses [XNNPACK](https://github.com/google/XNNPACK), a fairly optimized floating point library, to run some inference operators (see the delegate's [README](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/README.md) for currently supported ops). With XNNPACK, I was able to get performance numbers similar to what @Maratyszcza described at [XNNPACK's readme](https://github.com/google/XNNPACK/README.md). I also got good numbers on Pixel 4 and Oppo Reno 3. Numbers on x86 machines are also good.\r\n\r\n- **Single-threaded**, `label_image -m MODEL_NAME -x 1 -c 50 -t 1`\r\n\r\nModel |  Pixel 3a, ms | Pixel 4, ms| Oppo Reno 3, ms |\r\n-- | --: | --: | --:\r\nMobileNet v1 1.0X |  88.02 | 29.95 | 36.23 |\r\nMobileNet v2 1.0X |  55.13 | 19.13 | 21.69 |\r\nMobileNet v3 Large |  44.84 | 15.69 | 17.65 |\r\nMobileNet v3 Small |   14.23 | 5.58 | 5.66|\r\n\r\n- **multi-threaded**, the number of threads is set to be the number of big cores. `label_image -m MODEL_NAME -x 1 -c 50 -t NUMBER_OF_BIG_CORES`\r\n\r\nModel |  Pixel 3a, ms | Pixel 4, ms| Oppo Reno 3, ms|\r\n-- | --: | --: | --:\r\nMobileNet v1 1.0X |  46.00 | 10.60 | 13.23 |\r\nMobileNet v2 1.0X |  28.57 | 6.89 | 7.56 |\r\nMobileNet v3 Large |  24.62 | 6.52 | 6.88 |\r\nMobileNet v3 Small |   8.24 | 2.47 |  2.37 |\r\n", "comments": []}, {"number": 37889, "title": "Bazel Build Error : crosstool_wrapper_driver_is_not_gcc failed(Install Tensorflow1.14 using Bazel)", "body": "I NEED YOUR HEEEEEEEEEEEEEEEEEEEEEEEEEEEELP!!!\r\n\r\nAccording to Nvidia TensorRT page\r\n(https://docs.nvidia.com/deeplearning/sdk/tensorrt-release-notes/tensorrt-6.html), \r\n![Screenshot from 2020-03-25 10-30-32](https://user-images.githubusercontent.com/60951742/77500185-a8d6fc00-6e97-11ea-92a3-ec03f9614c27.png)\r\nso install like this...\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **18.04(llinux-x86-64)**\r\n- TensorFlow installed from : **git clone https://github.com/tensorflow/tensorflow.git**\r\n- TensorFlow version: **1.14.0**\r\n- Python version: **3.6**\r\n- Bazel version (if compiling from source):**0.26.1**\r\n- GCC/Compiler version (if compiling from source):**7.5.0**\r\n- CUDA/cuDNN version: **CUDA : 10.0, cuDNN : 7.6.5**\r\n- GPU model and memory: **GeForce GTX 1070**\r\n- TensorRT : **6.0.1.5**\r\n\r\n\r\n**Describe the problem**\r\nAfter `./configure`,\r\nrun \r\n`bazel build --config=cuda --config=opt //tensorflow/tools/pip_package:build_pip_package  --verbose_failures`.\r\nor \r\n`bazel build --config=opt --config=cuda --config=nonccl //tensorflow/tools/pip_package:build_pip_package --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" --verbose_failures`\r\n\r\nAnd encountered problem like this log...\r\n\r\n```\r\nERROR: /home/gogotis/tensorflow/tensorflow/compiler/tf2tensorrt/BUILD:330:1: C++ compilation of rule '//tensorflow/compiler/tf2tensorrt:trt_conversion' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd /home/gogotis/.cache/bazel/_bazel_gogotis/a070fbb10dc532167de59febc30d733d/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64:/usr/local/cuda-10.0/extras/CUPTI/lib64:/home/gogotis/TensorRT-6.0.1.5/lib \\\r\n    PATH=/usr/local/cuda-10.0/bin:/home/gogotis/.local/bin:/home/gogotis/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/gogotis/bin \\\r\n    PWD=/proc/self/cwd \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/host/bin/tensorflow/compiler/tf2tensorrt/_objs/trt_conversion/convert_nodes.pic.d '-frandom-seed=bazel-out/host/bin/tensorflow/compiler/tf2tensorrt/_objs/trt_conversion/convert_nodes.pic.o' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -DTF_USE_SNAPPY -iquote . -iquote bazel-out/host/bin -iquote external/com_google_absl -iquote bazel-out/host/bin/external/com_google_absl -iquote external/eigen_archive -iquote bazel-out/host/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/host/bin/external/local_config_sycl -iquote external/nsync -iquote bazel-out/host/bin/external/nsync -iquote external/gif_archive -iquote bazel-out/host/bin/external/gif_archive -iquote external/jpeg -iquote bazel-out/host/bin/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/host/bin/external/protobuf_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/host/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/host/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/host/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/host/bin/external/highwayhash -iquote external/zlib_archive -iquote bazel-out/host/bin/external/zlib_archive -iquote external/local_config_cuda -iquote bazel-out/host/bin/external/local_config_cuda -iquote external/double_conversion -iquote bazel-out/host/bin/external/double_conversion -iquote external/local_config_tensorrt -iquote bazel-out/host/bin/external/local_config_tensorrt -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/host/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cudnn_header -isystem external/eigen_archive -isystem bazel-out/host/bin/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/host/bin/external/nsync/public -isystem external/gif_archive/lib -isystem bazel-out/host/bin/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/host/bin/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/host/bin/external/farmhash_archive/src -isystem external/zlib_archive -isystem bazel-out/host/bin/external/zlib_archive -isystem external/local_config_cuda/cuda -isystem bazel-out/host/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include -isystem external/double_conversion -isystem bazel-out/host/bin/external/double_conversion '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -g0 '-march=native' -g0 -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-ftemplate-depth=900' '-DGOOGLE_CUDA=1' '-DGOOGLE_TENSORRT=1' -msse3 -pthread '-DGOOGLE_CUDA=1' '-DGOOGLE_TENSORRT=1' -c tensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc -o bazel-out/host/bin/tensorflow/compiler/tf2tensorrt/_objs/trt_conversion/convert_nodes.pic.o)\r\n```\r\nI read same problem issue...But it can't solve my problem...\r\n\r\n\r\n", "comments": ["Add))\r\n\r\n`./configure` Setting \r\n\r\n```\r\nWARNING: Running Bazel server needs to be killed, because the startup options are different.\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.26.1 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3.6\r\n\r\nFound possible Python library paths:\r\n/usr/local/lib/python3.6/dist-packages\r\n/usr/lib/python3/dist-packages\r\nPlease input the desired Python library path to use. Default is [/usr/local/lib/python3/dist-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]: n\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]:n\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]:n\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]:y\r\n\r\nFound CUDA 10.0 in:\r\n/usr/local/cuda-10.0/lib64\r\n/usr/local/cuda-10.0/include\r\n\r\nFound cuDNN 7 in:\r\n/usr/local/cuda-10.0/lib64\r\n/usr/local/cuda-10.0/include\r\n\r\nFound TensorRT 6 in:\r\n/usr/lib/x86_64-linux-gnu\r\n/usr/include/x86_64-linux-gnu\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 6.1,6.1]: 6.1\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]: n\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]:\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]:n\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]:\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\n```\r\n", "@Gogotis \r\nTensorflow 1.14 supports CUDA 10.0 and cuDNN 7.4.\r\nPlease take a look at [tested build configuration.](https://www.tensorflow.org/install/source#gpu)\r\n\r\nAlso refer to similar issues #33108  #8709 #8790 #32350  #22372 as per error faced", "@Saduf2019 \r\n\r\nThank you for reply!! I will try with that version ;-)\r\n\r\nAnd Can i ask one more??\r\n\r\nAs i mentioned, i want to use TensorRT for TF-TRT.\r\n\r\nIs there any TensorRT version compatibility with that version (CUDA 10.0 cuDNN7.4 Tensorflow 1.14)?? \r\nI saw some build error issue because of TensorRT version.\r\n", "@Gogotis \r\ni found few links for the compatibility [link-1](https://docs.nvidia.com/deeplearning/sdk/tensorrt-support-matrix/index.html) [link-2](https://www.tensorflow.org/install/gpu) [link-3](https://stackoverflow.com/questions/50622525/which-tensorflow-and-cuda-version-combinations-are-compatible), please let us know if this helps resolve the issue\r\n\r\nyou can look up for similar issues example #30753 for reference", "@Gogotis\r\nplease update as per above comment", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37889\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37889\">No</a>\n", "i also meet this problem when i build tf1.15.x with cuda11, cudnn 8, i think that tensorflow 1.x can not compile with cuda 11?"]}]