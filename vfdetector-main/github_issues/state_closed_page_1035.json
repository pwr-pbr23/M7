[{"number": 22272, "title": "failed to build tensorflow wheel file on jetson TX2 board ", "body": "ERROR: /home/nvidia/Desktop/tensorflow/tensorflow/cc/BUILD:422:1: Linking of rule '//tensorflow/cc:ops/math_ops_gen_cc' failed (Exit 1)\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scc_Cops_Smath_Uops_Ugen_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `png_init_filter_functions_neon'\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 1648.891s, Critical Path: 115.03s\r\nFAILED: Build did NOT complete successfully\r\n\r\n\r\nBazel version: 10.0\r\nCUDA: 9.0\r\nCUDNN:7.0\r\n\r\nRequest you to kindly help me find a solution to the problem.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Hi I was able to fix the issue thank you!", "@amoghkashyap  how to fix this issue? Thanks.", "now i meet the same problem, how to fix it?\r\n", "you have to build tensorflow from source using bazel. the versions of CUDA toolkit and CuDNN are very crucial for the tensorflow installation. Also, if you don't have the jetpack, you can install the dependencies with the following instructions:\r\n\r\n\r\n#Add base Jetpack URL to ENV\r\nexport URL=https://developer.download.nvidia.com/devzone/devcenter/mobile/jetpack_l4t/3.2.1/m8u2ki/JetPackL4T_321_b23\r\n\r\n#update and add few usefull packages \r\nsudo apt-get update && apt-get install -y apt-utils bzip2 curl sudo unp && apt-get clean && rm -rf /var/cache/apt\r\n\r\n#install drivers\r\ncurl -sL http://developer.nvidia.com/embedded/dlc/l4t-jetson-tx2-driver-package-28-2 3 | tar xvfj -\r\nchown root /etc/passwd /etc/sudoers /usr/lib/sudo/sudoers.so /etc/sudoers.d/README\r\n/tmp/Linux_for_Tegra/apply_binaries.sh -r / && rm -fr /tmp/*\r\n\r\n#Download the JETPACK libraries for ARM architecture\r\ncurl $URL/cuda-repo-l4t-9-0-local_9.0.252-1_arm64.deb -so /tmp/cuda-repo-l4t_arm64.deb\r\ncurl $URL/libcudnn7_7.0.5.15-1+cuda9.0_arm64.deb -so /tmp/libcudnn_arm64.deb\r\ncurl $URL/libcudnn7-dev_7.0.5.15-1+cuda9.0_arm64.deb -so /tmp/libcudnn-dev_arm64.deb\r\n\r\n#Install L4T, CUDA, cuDNN libraries\r\nsudo dpkg -i /tmp/cuda-repo-l4t_arm64.deb\r\nsudo apt-key add /var/cuda-repo-9-0-local/7fa2af80.pub\r\nsudo apt-get update && apt-get install -y cuda-toolkit-9.0 (default installation dir: /usr/local/cuda)\r\nsudo dpkg -i /tmp/libcudnn_arm64.deb\r\nsudo dpkg -i /tmp/libcudnn-dev_arm64.deb\r\n\r\n#Add LD_LIBRAARY_PATH to ENV\r\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/aarch64-linux-gnu/tegra\r\n\r\n#Re-link libs in /usr/lib/tegra\r\nln -s /usr/lib/aarch64-linux-gnu/tegra/libnvidia-ptxjitcompiler.so.28.2.0 /usr/lib/aarch64-linux-gnu/tegra/libnvidia-ptxjitcompiler.so\r\nln -s /usr/lib/aarch64-linux-gnu/tegra/libnvidia-ptxjitcompiler.so.28.2.0 /usr/lib/aarch64-linux-gnu/tegra/libnvidia-ptxjitcompiler.so.1\r\nln -sf /usr/lib/aarch64-linux-gnu/tegra/libGL.so /usr/lib/aarch64-linux-gnu/libGL.so\r\nln -s /usr/lib/aarch64-linux-gnu/libcuda.so /usr/lib/aarch64-linux-gnu/libcuda.so.1\r\n\r\n#Clone OpenCV Github repo\r\ngit clone https://github.com/jetsonhacks/buildOpenCVTX2.git\r\n\r\n#Run the shell script to build OpenCV\r\n./buildOpenCV.sh -s <file directory>\r\n(This will build OpenCV in the given file directory and install OpenCV in the /usr/local directory.)\r\n\r\n#Remove the source files\r\n./removeOpenCVSources.sh -d <file directory>\r\nwhere the <file directory> contains the OpenCV source.\r\n\r\n#Clean the board \r\nsudo apt-get -y autoremove && apt-get -y autoclean\r\nsudo rm -rf /var/cache/apt\r\n\r\n"]}, {"number": 22271, "title": "make schema_generated.h source file  turn error", "body": "### source  code\r\n```\r\nbool Verify(flatbuffers::Verifier &verifier) const {\r\n    return VerifyTableStart(verifier) &&\r\n           VerifyOffset(verifier, VT_MIN) &&\r\n           verifier.Verify(min()) &&\r\n           VerifyOffset(verifier, VT_MAX) &&\r\n           verifier.Verify(max()) &&\r\n           VerifyOffset(verifier, VT_SCALE) &&\r\n           verifier.Verify(scale()) &&\r\n           VerifyOffset(verifier, VT_ZERO_POINT) &&\r\n           verifier.Verify(zero_point()) &&\r\n           verifier.EndTable();\r\n  }\r\n```\r\n\r\n### error info\r\n```\r\nerroe :   \u201cflatbuffers::Verifier::Verify\u201d: No overload function takes 1 \r\nparameters.tensorflow\\contrib\\lite\\schema\\schema_generated.h\t\r\n````\r\n\r\n\r\n\r\nThe same error occurs in every structure inside this method. \r\n\r\n", "comments": ["@425172555  schema_generated.h is automatically generated. Please fill out the template and provide more information about this error.\r\n\r\nSystem information\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary):\r\nTensorFlow version (use command below):\r\nPython version:\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\nExact command to reproduce:", "Closing this for now, feel free to open a new issue if encountering any errors."]}, {"number": 22270, "title": "tf.nn.softmax_cross_entropy_with_logits_v2() not warning incorrect input shapes when placeholders are provided", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.10.0\r\n- **Python version**: 3.5.2\r\n- **GPU model and memory**: GTX1070\r\n\r\n### Describe the problem\r\nFollowing is a simple snippet to reproduce the problem. If I'm understanding this correctly, the shape of _labels_ is clearly wrong here but tensorflow doesn't warn you and when the same problem is present in my actual code, not even a runtime error will show up. Appreciate it if you guys can help me out on this especially if I mistook this. Otherwise this looks like a devastating issue.\r\n\r\n### Source code / logs\r\na = tf.placeholder(tf.int32, (None))\r\nb = tf.constant([[1.0,2,3],[4,5,6]])\r\nc = tf.nn.softmax_cross_entropy_with_logits_v2(labels=a, logits=b)\r\n", "comments": ["The shape of `a` is unknown, see [tf.Dimension](https://www.tensorflow.org/api_docs/python/tf/Dimension). Could you clarify where is wrong?", "> The shape of `a` is unknown, see [tf.Dimension](https://www.tensorflow.org/api_docs/python/tf/Dimension). Could you clarify where is wrong?\r\n\r\nIf we are to provide placeholders for both the `label` and `logits` arguments, shouldn't they both have the shape (?, # of classes)? If for example in the above snippet `a` has only one dimension then there is no way this is an input with correct shape so shouldn't tensorflow warn about it?", "Okay in my actual code it was `sparse_softmax_cross_entropy_with_logits` that was run so I think there should be runtime error if the inputs for `softmax_cross_entropy_with_logits` are not compatible but the no warning thing with the above snippet is still true which might be not that ideal a situation."]}, {"number": 22269, "title": "TensorFlow compile failure with error tensorflow/tensorflow/compiler/xla/service/gpu/BUILD:649:1: C++ compilation of rule '//tensorflow/compiler/xla/service/gpu:infeed_manager' failed (Exit 1)", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04.1 (GNU/Linux 4.15.0-34-generic x86_64)\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.10.1\r\n- **Python version**: Anaconda 5.2.0 (Python 3.6.5)\r\n- **Bazel version (if compiling from source)**: 0.16.1\r\n- **GCC/Compiler version (if compiling from source)**:7.3.0\r\n- **CUDA/cuDNN version** : 9.2 / 7.2.1\r\n- **GPU model and memory**: NVIDIA 1080Ti x 4 with driver 396.54 installed from ubuntu ppa.\r\n- **Exact command to reproduce**: bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\n### Describe the problem\r\nHi. I have a problem while compiling TensorFlow 1.10. I have always compiled pip package for my computer since back in TF versions like 0.12. But from TF 1.10 I started getting compile error related to XLA (I enabled XLA in ./configure before compiling). I did not get any XLA related error while compiling until TF version 1.9 but the error stated occurring from TF 1.10.\r\n\r\nThe error lines are the following:\r\nTensorFlow fails to compile with error tensorflow/tensorflow/compiler/xla/service/gpu/BUILD:649:1: C++ compilation of rule '//tensorflow/compiler/xla/service/gpu:infeed_manager' failed (Exit 1)", "comments": ["The error lines you copied are simply reports by bazel on roughly where the compilation failure happened.\r\nHere are a few things before we can help you:\r\n- Try a more recent version of the source. Bazel is still pre 1.0, which means it has a lot of backwards incompatible changes. With your bazel version (0.16.1) it is possible the combination will just not work.\r\n- Share the full bazel output using pastebin, or a similar service.", "> The error lines you copied are simply reports by bazel on roughly where the compilation failure happened.\r\n> Here are a few things before we can help you:\r\n> \r\n> * Try a more recent version of the source. Bazel is still pre 1.0, which means it has a lot of backwards incompatible changes. With your bazel version (0.16.1) it is possible the combination will just not work.\r\n> * Share the full bazel output using pastebin, or a similar service.\r\n\r\n\r\n@gunan \r\n\r\nThank you for your response.\r\nIn accordance with your instructions, I have updated the Bazel up-to-date version (0.17.2). Unfortunately, the same error is still occuring.\r\n\r\nHere is the full (as much as I could copy from bash) output.\r\n[https://pastebin.com/j2PXPbBa](https://pastebin.com/j2PXPbBa)\r\n\r\nAlso, here is the full configure that I used for the compilation.\r\n[https://pastebin.com/syJmPGPb](https://pastebin.com/syJmPGPb)\r\n\r\nHave a nice day.", "OK, this gives me much more information, thanks!\r\nHere is the compilation error:\r\n```\r\nERROR: /home/simon/Desktop/tensorflow/tensorflow/compiler/xla/service/gpu/BUILD:649:1: C++ compilation of rule '//tensorflow/compiler/xla/service/gpu:infeed_manager' failed (Exit 1)\r\nIn file included from ./tensorflow/compiler/xla/service/gpu/infeed_manager.h:23:0,\r\n                 from tensorflow/compiler/xla/service/gpu/infeed_manager.cc:16:\r\n./tensorflow/compiler/xla/service/gpu/xfeed_queue.h:68:37: error: 'std::function' has not been declared\r\n```\r\n\r\nLooking at that last part of the error, I found this:\r\nhttps://www.bountysource.com/issues/49075870-compilation-error-loader-resourceloader-h-101-100-error-std-function-has-not-been-declared-loader-resourceloader-h-101-108-error-expected-or-before-token\r\n\r\nLooks like this can be something in latest gcc, @jlebar @Artem-B to weigh in on that.\r\n\r\nOne thing you can try.\r\nIn our configuration script, it asks you if you like to use \"clang\" to compile:\r\n`Do you want to use clang as CUDA compiler?`\r\nCould you pick \"clang\" to compile, and when it asks, you will need to tell it to download and use latest clang.", "```\r\n$ git log --patch master tensorflow/compiler/xla/service/gpu/xfeed_queue.h\r\ncommit fb8d1ca4eaefe58d42c27b6fc676f64f137f4675\r\nAuthor: Ray Kim <msca8h@naver.com>\r\nDate:   Sat Jul 21 21:42:15 2018 +0900\r\n\r\n    fixed build error on gcc-7\r\n\r\ndiff --git a/tensorflow/compiler/xla/service/gpu/xfeed_queue.h b/tensorflow/compiler/xla/service/gpu/xfeed_queue.h\r\nindex 737c7eb025..dd46ff433b 100644\r\n--- a/tensorflow/compiler/xla/service/gpu/xfeed_queue.h\r\n+++ b/tensorflow/compiler/xla/service/gpu/xfeed_queue.h\r\n@@ -17,6 +17,7 @@ limitations under the License.\r\n #define TENSORFLOW_COMPILER_XLA_SERVICE_GPU_XFEED_QUEUE_H_\r\n \r\n #include <deque>\r\n+#include <functional>\r\n #include <vector>\r\n \r\n #include \"tensorflow/core/platform/mutex.h\"\r\n```\r\n\r\nLooks promising.  And looks like this change is not in v1.10.1:\r\n\r\n```\r\n$ git log --pretty=oneline v1.10.1 tensorflow/compiler/xla/service/gpu/xfeed_queue.h\r\neccc1d4c102b1b3b03b98dbc362f799cb540a1da [XLA:GPU] Unify infeed and outfeed queue implementations\r\n```", "Is this still an issue?", "I don't think it's an issue at head.\r\n\r\nIf you'd like to request a backport to  TF 1.10, I guess that's a separate bug?", "@gunan \r\nI tried using clang for the CUDA compiler however the compiling process does not even start with the clang option. I guess this is a separate issue.\r\n\r\n@jlebar @dksb \r\nThe issue is resolved if I cherrypick fb8d1ca4eaefe58d42c27b6fc676f64f137f4675. Or just checkout v1.11.0. Thank you all for the kind helps."]}, {"number": 22268, "title": "Tensorflow compile error with win7", "body": "**Have I written custom code** No\r\n**OS Platform and Distribution** WIN7  Visual Studio 2015\r\n**TensorFlow installed from**  source  r1.10\r\n**Bazel version** N/A\r\n**CUDA/cuDNN version** CUDA9.0  /   CUDNN  7.0\r\n**GPU model and memory**   1080Ti / 11G \r\n**Mobile device**  N/A\r\n**Exact command to reproduce** \r\n**1, cmake** \r\n```\r\ncmake .. -A x64 -DCMAKE_BUILD_TYPE=Debug \r\n-T host=x64 -DSWIG_EXECUTABLE=D:/lib/swigwin-3.0.12/swig.exe \r\n-DPYTHON_EXECUTABLE=C:/Users/tao/AppData/Local/Programs/Python/python35/python.exe \r\n-DPYTHON_LIBRARIES=C:/Users/tao/AppData/Local/Programs/Python/python35/libs/python35.lib \r\n-Dtensorflow_ENABLE_GPU=ON \r\n-Dtensorflow_ENABLE_GRPC_SUPPORT=OFF \r\n-Dtensorflow_BUILD_SHARED_LIB=ON\r\n```\r\n**2. build**\r\n`MSBuild /p:Configuration=Debug /p:Platform=x64 ALL_BUILD.vcxproj`\r\n\r\n### Describe the problem\r\nWhen I compiled the Tensorflow with source code, there exist some errors:\r\n```\r\n\u4e25\u91cd\u6027\t\u4ee3\u7801\t\u8bf4\u660e\t\u9879\u76ee\t\u6587\u4ef6\t\u884c\t\u7981\u6b62\u663e\u793a\u72b6\u6001\r\n\u9519\u8bef\tLNK2019\t\u65e0\u6cd5\u89e3\u6790\u7684\u5916\u90e8\u7b26\u53f7 \"void __cdecl tensorflow::NewRemoteDevices(class tensorflow::Env *,class tensorflow::WorkerCacheInterface *,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class std::function<void __cdecl(class tensorflow::Status const &,class std::vector<class tensorflow::Device *,class std::allocator<class tensorflow::Device *> > *)>)\" (?NewRemoteDevices@tensorflow@@YAXPEAVEnv@1@PEAVWorkerCacheInterface@1@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V?$function@$$A6AXAEBVStatus@tensorflow@@PEAV?$vector@PEAVDevice@tensorflow@@V?$allocator@PEAVDevice@tensorflow@@@std@@@std@@@Z@5@@Z)\uff0c\u8be5\u7b26\u53f7\u5728\u51fd\u6570 \"class tensorflow::Status __cdecl `anonymous namespace'::GetAllRemoteDevices(class std::vector<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class std::allocator<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > > > const &,class tensorflow::WorkerCacheInterface *,class std::unique_ptr<class tensorflow::DeviceMgr,struct std::default_delete<class tensorflow::DeviceMgr> > *)\" (?GetAllRemoteDevices@?A0x02eb28b0@@YA?AVStatus@tensorflow@@AEBV?$vector@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V?$allocator@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@@std@@PEAVWorkerCacheInterface@3@PEAV?$unique_ptr@VDeviceMgr@tensorflow@@U?$default_delete@VDeviceMgr@tensorflow@@@std@@@5@@Z) \u4e2d\u88ab\u5f15\u7528\ttensorflow\tG:\\tensorflow\\tensorflow\\contrib\\cmake\\build\\c_api.cc.obj\t1\t\r\n\u9519\u8bef\tLNK2019\t\u65e0\u6cd5\u89e3\u6790\u7684\u5916\u90e8\u7b26\u53f7 \"public: class tensorflow::Status __cdecl tensorflow::SessionMgr::CreateSession(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class tensorflow::ServerDef const &,bool)\" (?CreateSession@SessionMgr@tensorflow@@QEAA?AVStatus@2@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@AEBVServerDef@2@_N@Z)\uff0c\u8be5\u7b26\u53f7\u5728\u51fd\u6570 \"class tensorflow::Status __cdecl `anonymous namespace'::NewRemoteAwareTFE_Context(struct TFE_ContextOptions const *,struct TFE_Context * *)\" (?NewRemoteAwareTFE_Context@?A0x02eb28b0@@YA?AVStatus@tensorflow@@PEBUTFE_ContextOptions@@PEAPEAUTFE_Context@@@Z) \u4e2d\u88ab\u5f15\u7528\ttensorflow\tG:\\tensorflow\\tensorflow\\contrib\\cmake\\build\\c_api.cc.obj\t1\t\r\n\u9519\u8bef\tLNK2019\t\u65e0\u6cd5\u89e3\u6790\u7684\u5916\u90e8\u7b26\u53f7 \"public: class tensorflow::Status __cdecl tensorflow::SessionMgr::WorkerSessionForSession(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class std::shared_ptr<struct tensorflow::WorkerSession> *)\" (?WorkerSessionForSession@SessionMgr@tensorflow@@QEAA?AVStatus@2@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAV?$shared_ptr@UWorkerSession@tensorflow@@@5@@Z)\uff0c\u8be5\u7b26\u53f7\u5728\u51fd\u6570 \"class tensorflow::Status __cdecl `anonymous namespace'::NewRemoteAwareTFE_Context(struct TFE_ContextOptions const *,struct TFE_Context * *)\" (?NewRemoteAwareTFE_Context@?A0x02eb28b0@@YA?AVStatus@tensorflow@@PEBUTFE_ContextOptions@@PEAPEAUTFE_Context@@@Z) \u4e2d\u88ab\u5f15\u7528\ttensorflow\tG:\\tensorflow\\tensorflow\\contrib\\cmake\\build\\c_api.cc.obj\t1\t\r\n\u9519\u8bef\tLNK2019\t\u65e0\u6cd5\u89e3\u6790\u7684\u5916\u90e8\u7b26\u53f7 \"class tensorflow::eager::EagerClientCache * __cdecl tensorflow::eager::NewGrpcEagerClientCache(class std::shared_ptr<class tensorflow::GrpcChannelCache>)\" (?NewGrpcEagerClientCache@eager@tensorflow@@YAPEAVEagerClientCache@12@V?$shared_ptr@VGrpcChannelCache@tensorflow@@@std@@@Z)\uff0c\u8be5\u7b26\u53f7\u5728\u51fd\u6570 \"class tensorflow::Status __cdecl `anonymous namespace'::NewRemoteAwareTFE_Context(struct TFE_ContextOptions const *,struct TFE_Context * *)\" (?NewRemoteAwareTFE_Context@?A0x02eb28b0@@YA?AVStatus@tensorflow@@PEBUTFE_ContextOptions@@PEAPEAUTFE_Context@@@Z) \u4e2d\u88ab\u5f15\u7528\ttensorflow\tG:\\tensorflow\\tensorflow\\contrib\\cmake\\build\\c_api.cc.obj\t1\t\r\n\u9519\u8bef\tLNK1120\t4 \u4e2a\u65e0\u6cd5\u89e3\u6790\u7684\u5916\u90e8\u547d\u4ee4\ttensorflow\tG:\\tensorflow\\tensorflow\\contrib\\cmake\\build\\Debug\\tensorflow.dll\t1\t\r\n```\r\ncan anyone help me , thx", "comments": ["cmake build is not supported anymore. It may just be broken.\r\nPlease try using bazel to build on windows.", "i try using bazel to build on windows, but tips:\r\n\r\n**Have I written custom code** No\r\n**OS Platform and Distribution** WIN7 Visual Studio 2015\r\n**TensorFlow installed from source**  master\r\n**Bazel version**  0.17.1\r\n**CUDA/cuDNN version** CUDA9.0 / CUDNN 7.2.1\r\n**GPU model and memory** 1080Ti / 11G\r\n**Mobile device** N/A\r\n**Exact command to reproduce**\r\n**1. python ./configure.py**\r\n**2.bazel build --config=opt --config=cuda //tensorflow:libtensorflow.so**\r\n```\r\nG:\\tensorflow>bazel build --config=opt --config=cuda //tensorflow:libtensorflow.\r\nso\r\nStarting local Bazel server and connecting to it...\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeata\r\nble flags, repeats are counted twice and may lead to unexpected behavior.\r\nWARNING: Option 'experimental_shortened_obj_file_path' is deprecated\r\nLoading:\r\nLoading: 0 packages loaded\r\nLoading: 0 packages loaded\r\n    currently loading: tensorflow\r\nERROR: Skipping '//tensorflow:libtensorflow.so': error loading package 'tensorfl\r\now': Encountered error while reading extension file 'cuda/build_defs.bzl': no su\r\nch package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n        File \"G:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1458\r\n                _create_local_cuda_repository(repository_ctx)\r\n        File \"G:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1185, in\r\n_create_local_cuda_repository\r\n                _get_cuda_config(repository_ctx)\r\n        File \"G:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 909, in _\r\nget_cuda_config\r\n                _cudnn_version(repository_ctx, cudnn_install_base..., ...)\r\n        File \"G:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 584, in _\r\ncudnn_version\r\n                find_cuda_define(repository_ctx, cudnn_header_dir, \"c...\", ...)\r\n        File \"G:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 534, in f\r\nind_cuda_define\r\n                auto_configure_fail((\"Error reading %s: %s\" % (str(h...)))\r\n        File \"G:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 315, in a\r\nuto_configure_fail\r\n                fail((\"\\n%sCuda Configuration Error:%...)))\r\n\r\nCuda Configuration Error: Error reading G:/cudnn/include/cudnn.h: java.io.IOExce\r\nption: ERROR: src/main/native/windows/processes-jni.cc(239): CreateProcessW(\"gre\r\np\" --color=never -A1 -E \"#define CUDNN_MAJOR\" G:/cudnn/include/cudnn.h): ???????\r\n????\r\n\r\nWARNING: Target pattern parsing failed.\r\nERROR: error loading package 'tensorflow': Encountered error while reading exten\r\nsion file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Tra\r\nceback (most recent call last):\r\n        File \"G:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1458\r\n                _create_local_cuda_repository(repository_ctx)\r\n        File \"G:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1185, in\r\n_create_local_cuda_repository\r\n                _get_cuda_config(repository_ctx)\r\n        File \"G:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 909, in _\r\nget_cuda_config\r\n                _cudnn_version(repository_ctx, cudnn_install_base..., ...)\r\n        File \"G:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 584, in _\r\ncudnn_version\r\n                find_cuda_define(repository_ctx, cudnn_header_dir, \"c...\", ...)\r\n        File \"G:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 534, in f\r\nind_cuda_define\r\n                auto_configure_fail((\"Error reading %s: %s\" % (str(h...)))\r\n        File \"G:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 315, in a\r\nuto_configure_fail\r\n                fail((\"\\n%sCuda Configuration Error:%...)))\r\n\r\nCuda Configuration Error: Error reading G:/cudnn/include/cudnn.h: java.io.IOExce\r\nption: ERROR: src/main/native/windows/processes-jni.cc(239): CreateProcessW(\"gre\r\np\" --color=never -A1 -E \"#define CUDNN_MAJOR\" G:/cudnn/include/cudnn.h): ???????\r\n????\r\n\r\nINFO: Elapsed time: 3.607s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n```", "The error messages say that cudnn.h cannot be found. You will need to either reinstall cudnn, or if it is already installed make sure the paths set pointing to cudnn are correct.", "\r\n\r\n\r\n> The error messages say that cudnn.h cannot be found. You will need to either reinstall cudnn, or if it is already installed make sure the paths set pointing to cudnn are correct.\r\n\r\n\r\nI am sure there is \u201ccudnn.h\u201d in the corresponding directory, but I still get an error.", "```\r\nCuda Configuration Error: Error reading G:/cudnn/include/cudnn.h: java.io.IOExce\r\nption: ERROR: src/main/native/windows/processes-jni.cc(239): CreateProcessW(\"gre\r\np\" --color=never -A1 -E \"#define CUDNN_MAJOR\" G:/cudnn/include/cudnn.h): ???????\r\n????\r\n```\r\nThis means either cudnn.h is unreadable for bazel, or it is not located at `G:/cudnn/include/cudnn.h`\r\nWhat is the location of your cudnn headers on your machine?\r\nDid you follow these:\r\nhttps://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#installwindows", "> G:/cudnn/include/cudnn.h\r\nNow tips:\r\n```\r\nG:\\tensorflow>bazel build --config = opt --config = cuda //tensorflow:libtensorf\r\nlow.so\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=0 --terminal_columns=80\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=C:/Python27/python.exe\r\nINFO: Reading rc options for 'build' from g:\\tensorflow\\tools\\bazel.rc:\r\n  'build' options: --define framework_shared_object=true --define=use_fast_cpp_p\r\nrotos=true --define=allow_oversize_protos=true --define=grpc_no_ares=true --spaw\r\nn_strategy=standalone --genrule_strategy=standalone -c opt\r\nINFO: Reading rc options for 'build' from g:\\tensorflow\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=C:/Users/tao/AppData/Local/Progr\r\nams/Python/Python35/python.exe --action_env PYTHON_LIB_PATH=C:/Users/tao/AppData\r\n/Local/Programs/Python/Python35/lib/site-packages --python_path=C:/Users/tao/App\r\nData/Local/Programs/Python/Python35/python.exe --action_env TF_NEED_OPENCL_SYCL=\r\n0 --action_env TF_NEED_CUDA=1 --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NV\r\nIDIA GPU Computing Toolkit/CUDA/v9.0 --action_env TF_CUDA_VERSION=9.0 --action_e\r\nnv CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0 --\r\naction_env TF_CUDNN_VERSION=7 --action_env TF_CUDA_COMPUTE_CAPABILITIES=3.5 --ac\r\ntion_env TF_CUDA_CLANG=0 --config=cuda --define grpc_no_ares=true --strip=always\r\n --config monolithic --copt=-w --host_copt=-w --verbose_failures\r\nINFO: Found applicable config definition build:cuda in file g:\\tensorflow\\tools\\\r\nbazel.rc: --crosstool_top=@local_config_cuda//crosstool:toolchain --define=using\r\n_cuda=true --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:monolithic in file g:\\tensorflow\\\r\ntools\\bazel.rc: --define framework_shared_object=false\r\nERROR: Config value = is not defined in any .rc file\r\n```", "Did you run configure script?\r\nAlso, in bazel command line syntax, you are not allowed to have empty spaces around `=`, so you need to provide the options as `--config=opt --config=cuda`", "> Did you run configure script?\r\n> Also, in bazel command line syntax, you are not allowed to have empty spaces around `=`, so you need to provide the options as `--config=opt --config=cuda`\r\n\r\nIt has been complied. but have err:\r\n```\r\nG:\\tensorflow>bazel build --config=opt --config=cuda //tensorflow:libtensorflow.\r\nso\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeata\r\nble flags, repeats are counted twice and may lead to unexpected behavior.\r\nWARNING: Option 'experimental_shortened_obj_file_path' is deprecated\r\nLoading:\r\nLoading: 0 packages loaded\r\nLoading: 0 packages loaded\r\n    currently loading: tensorflow\r\nLoading: 0 packages loaded\r\n    currently loading: tensorflow\r\nLoading: 0 packages loaded\r\n    currently loading: tensorflow\r\nDEBUG: C:/users/tao/_bazel_tao/ic7qrvhc/external/bazel_tools/tools/cpp/lib_cc_co\r\nnfigure.bzl:115:5:\r\nAuto-Configuration Warning: 'BAZEL_VC' is not set, start looking for the latest\r\nVisual C++ installed.\r\nDEBUG: C:/users/tao/_bazel_tao/ic7qrvhc/external/bazel_tools/tools/cpp/lib_cc_co\r\nnfigure.bzl:115:5:\r\nAuto-Configuration Warning: Looking for VS%VERSION%COMNTOOLS environment variabl\r\nes, eg. VS140COMNTOOLS\r\nDEBUG: C:/users/tao/_bazel_tao/ic7qrvhc/external/bazel_tools/tools/cpp/lib_cc_co\r\nnfigure.bzl:115:5:\r\nAuto-Configuration Warning: Visual C++ build tools found at C:\\Program Files (x8\r\n6)\\Microsoft Visual Studio 14.0\\VC\\\r\nDEBUG: C:/users/tao/_bazel_tao/ic7qrvhc/external/bazel_tools/tools/cpp/lib_cc_co\r\nnfigure.bzl:115:5:\r\nAuto-Configuration Warning: 'BAZEL_VC' is not set, start looking for the latest\r\nVisual C++ installed.\r\nDEBUG: C:/users/tao/_bazel_tao/ic7qrvhc/external/bazel_tools/tools/cpp/lib_cc_co\r\nnfigure.bzl:115:5:\r\nAuto-Configuration Warning: Looking for VS%VERSION%COMNTOOLS environment variabl\r\nes, eg. VS140COMNTOOLS\r\nDEBUG: C:/users/tao/_bazel_tao/ic7qrvhc/external/bazel_tools/tools/cpp/lib_cc_co\r\nnfigure.bzl:115:5:\r\nAuto-Configuration Warning: Visual C++ build tools found at C:\\Program Files (x8\r\n6)\\Microsoft Visual Studio 14.0\\VC\\\r\nAnalyzing: target //tensorflow:libtensorflow.so (1 packages loaded)\r\nDEBUG: C:/users/tao/_bazel_tao/ic7qrvhc/external/bazel_tools/tools/cpp/lib_cc_co\r\nnfigure.bzl:115:5:\r\nAuto-Configuration Warning: 'BAZEL_VC' is not set, start looking for the latest\r\nVisual C++ installed.\r\nDEBUG: C:/users/tao/_bazel_tao/ic7qrvhc/external/bazel_tools/tools/cpp/lib_cc_co\r\nnfigure.bzl:115:5:\r\nAuto-Configuration Warning: Looking for VS%VERSION%COMNTOOLS environment variabl\r\nes, eg. VS140COMNTOOLS\r\nDEBUG: C:/users/tao/_bazel_tao/ic7qrvhc/external/bazel_tools/tools/cpp/lib_cc_co\r\nnfigure.bzl:115:5:\r\nAuto-Configuration Warning: Visual C++ build tools found at C:\\Program Files (x8\r\n6)\\Microsoft Visual Studio 14.0\\VC\\\r\nAnalyzing: target //tensorflow:libtensorflow.so (10 packages loaded)\r\nAnalyzing: target //tensorflow:libtensorflow.so (45 packages loaded)\r\nAnalyzing: target //tensorflow:libtensorflow.so (72 packages loaded)\r\nAnalyzing: target //tensorflow:libtensorflow.so (84 packages loaded)\r\nERROR: G:/tensorflow/tensorflow/core/platform/default/build_config/BUILD:188:1:\r\nno such package '@png_archive//': Traceback (most recent call last):\r\n        File \"G:/tensorflow/third_party/repo.bzl\", line 106\r\n                _apply_patch(ctx, ctx.attr.patch_file)\r\n        File \"G:/tensorflow/third_party/repo.bzl\", line 73, in _apply_patch\r\n                _execute_and_check_ret_code(ctx, cmd)\r\n        File \"G:/tensorflow/third_party/repo.bzl\", line 52, in _execute_and_chec\r\nk_ret_code\r\n                fail(\"Non-zero return code({1}) when ...))\r\nNon-zero return code(256) when executing 'C:\\msys64\\usr\\bin\\bash.exe -l -c patch\r\n -p1 -d C:/users/tao/_bazel_tao/ic7qrvhc/external/png_archive -i G:/tensorflow/t\r\nhird_party/png_fix_rpi.patch':\r\nStdout:\r\nStderr: Timed out and referenced by '//tensorflow/core/platform/default/build_co\r\nnfig:png'\r\nERROR: Analysis of target '//tensorflow:libtensorflow.so' failed; build aborted:\r\n Analysis failed\r\nINFO: Elapsed time: 56.491s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (84 packages loaded)\r\nFAILED: Build did NOT complete successfully (84 packages loaded)\r\n```\r\ni can't understand why`no such package '@png_archive//': Traceback (most recent call last):` is produced", "@meteorcloudy to help with that quqestion.\r\nYun, looks like this is happening for a few users now.\r\nOur CI cannot reproduce the problem.\r\nAny idea what is going on?", "Indeed, I couldn't reproduce this, either.\r\nBut @taotaolin can you try to reinstall patch by `C:\\msys64\\usr\\bin\\pacman.exe -S patch`?", "> Indeed, I couldn't reproduce this, either.\r\n> But @taotaolin can you try to reinstall patch by `C:\\msys64\\usr\\bin\\pacman.exe -S patch`?\r\nIt still remains like this\r\n```\r\nC:\\msys64\\usr\\bin\\bash.exe -l -c patch -p1 -d C:/users/tao/_bazel_tao/ic7qrvhc/external/png_archive -i G:/tensorflow/third_party/png_fix_rpi.patch:\r\n```\r\ncan't work but \r\n\r\n```\r\nC:\\msys64\\usr\\bin\\bash.exe -l -c \"patch -p1 -d C:/users/tao/_bazel_tao/ic7qrvhc/external/png_archive -i G:/tensorflow/third_party/png_fix_rpi.patch\"\r\n```\r\nit can work\r\n", "I believe Bazel does pass the arguments as\r\n`C:\\msys64\\usr\\bin\\bash.exe -l -c \"patch -p1 -d C:/users/tao/_bazel_tao/ic7qrvhc/external/png_archive -i G:/tensorflow/third_party/png_fix_rpi.patch\"`\r\n\r\nIt's just missing the quotes when printing the command in terminal.\r\n\r\nCan you try to add `-f` here:\r\nhttps://github.com/tensorflow/tensorflow/blob/f1d42c8967410db1e08c0b6d62dc1fc4844165a8/third_party/repo.bzl#L71\r\nlike:\r\n`[\"patch\", \"-f\", \"-p1\", \"-d\", ctx.path(\".\"), \"-i\", ctx.path(patch_file)], `", "Unfortunately\uff0c It still produces the same mistake", "> I believe Bazel does pass the arguments as\r\n> `C:\\msys64\\usr\\bin\\bash.exe -l -c \"patch -p1 -d C:/users/tao/_bazel_tao/ic7qrvhc/external/png_archive -i G:/tensorflow/third_party/png_fix_rpi.patch\"`\r\n> \r\n> It's just missing the quotes when printing the command in terminal.\r\n> \r\n> Can you try to add `-f` here:\r\n> [tensorflow/third_party/repo.bzl](https://github.com/tensorflow/tensorflow/blob/f1d42c8967410db1e08c0b6d62dc1fc4844165a8/third_party/repo.bzl#L71)\r\n> \r\n> Line 71 in [f1d42c8](/tensorflow/tensorflow/commit/f1d42c8967410db1e08c0b6d62dc1fc4844165a8)\r\n> \r\n>  [\"patch\", \"-p1\", \"-d\", ctx.path(\".\"), \"-i\", ctx.path(patch_file)], \r\n> \r\n> like:\r\n> `[\"patch\", \"-f\", \"-p1\", \"-d\", ctx.path(\".\"), \"-i\", ctx.path(patch_file)],`\r\n\r\nCan provide `tensorflow.dll` and `tensorflow.lib` for windows tensorflow, preferably DEBUG version\uff1fthx.", "@taotaolin , can you run `bazel clean --expunge` and then try to rebuild?", "@taotaolin If that didn't work, can you type `where patch` in your terminal? Just trying to make sure you have the correct patch binary in `PATH`", "> @taotaolin If that didn't work, can you type `where patch` in your terminal? Just trying to make sure you have the correct patch binary in `PATH`\r\n\r\nthank you for your help, It can now be compiled, but it seems that something went wrong, maybe because my Windows language is Chinese \uff0cso error is garbled, I can't understand what went wrong. tips:\r\n```\r\ntensorflow\\bazel-out\\x64_windows-opt\\genfiles\\external\\local_config_cuda\\cuda\\cu\r\nda\\include\\crt\\host_defines.h\r\n\u8133\u5784\u812a\u8292: \u63b3\u7709\u6f5e\u5362\u8126\u811b\u5f55\u9541:       bazel-out/x64_windows-opt/genfiles/external/lo\r\ncal_config_cuda/cuda/cuda/include\\math_constants.h\r\n\u8133\u5784\u812a\u8292: \u63b3\u7709\u6f5e\u5362\u8126\u811b\u5f55\u9541:       bazel-out/x64_windows-opt/genfiles/external/lo\r\ncal_config_cuda/cuda/cuda/include\\crt/func_macro.h\r\n\u8133\u5784\u812a\u8292: \u63b3\u7709\u6f5e\u5362\u8126\u811b\u5f55\u9541:   c:\\users\\tao\\appdata\\local\\temp\\nvcc_inter_files_t\r\nmp_dir\\conv_ops_gpu_2.cu.fatbin.c\r\n\u8133\u5784\u812a\u8292: \u63b3\u7709\u6f5e\u5362\u8126\u811b\u5f55\u9541:    bazel-out/x64_windows-opt/genfiles/external/local\r\n_config_cuda/cuda/cuda/include\\fatBinaryCtl.h\r\n\u8133\u5784\u812a\u8292: \u63b3\u7709\u6f5e\u5362\u8126\u811b\u5f55\u9541:     c:\\users\\tao\\_bazel_tao\\ic7qrvhc\\execroot\\org_te\r\nnsorflow\\bazel-out\\x64_windows-opt\\genfiles\\external\\local_config_cuda\\cuda\\cuda\r\n\\include\\fatbinary.h\r\n```\r\n**external/com_google_absl\\absl/container/inlined_vector.h(651): error C3083: \u9686\u63b3\r\n`global namespace'\u9686\u5364:\u9686\u63b3::\u9686\u5364\u8133\u8d38\u864f\u813f\u788c\u811b\u8def\u6ca1\u6f5e\u811c\u5364\u8134\u8128\u6bdb\u8122\u811f\u812a\u7984\u8130\u8130\u810c\u813f\u8128\r\n\u8125**\r\n```\r\nexternal/com_google_absl\\absl/container/inlined_vector.h(92): note: \u5364\u813f\u812a\u6bdb\u810c\u813f\r\n \u811b\u62e2\u63b3\u6c13 \u9c81\u8121\u812d\u5364\u6f5e\u7089\u8122\u5a92\u9686\u63b3absl::InlinedVector<__int64,6,std::allocator<_Ty>>\r\n::InlinedVector(unsigned __int64,const std::allocator<_Ty> &)\u9686\u5364\u8122\u5364\r\n        with\r\n        [\r\n            _Ty=__int64\r\n        ]\r\n.\\tensorflow/core/util/tensor_format.h(497): note: \u864f\u8126\u5f55\u6ca1\u9732\u812d\u812e\u5a92\u812d\u8137\u5364\u813f\u812a\u6bdb\r\n\u788c\u811b\u6f5e\u7089\u8122\u5a92 \u811b\u62e2\u63b3\u6c13 \u8122\u788c\u810c\u5a92\u7984\u7089\u9686\u63b3absl::InlinedVector<__int64,6,std::allocat\r\nor<_Ty>>::InlinedVector(unsigned __int64,const std::allocator<_Ty> &)\u9686\u5364\u788c\u811b\u812a\r\n\u5a92\u812b\u8119\r\n        with\r\n        [\r\n            _Ty=__int64\r\n        ]\r\n.\\tensorflow/core/util/tensor_format.h(497): note: \u864f\u8126\u5f55\u6ca1\u9732\u812d\u812e\u5a92\u812d\u8137\u5364\u813f\u812a\u6bdb\r\n\u788c\u811b\u810c\u813f \u811b\u62e2\u63b3\u6c13 \u8122\u788c\u810c\u5a92\u7984\u7089\u9686\u63b3absl::InlinedVector<__int64,6,std::allocator<_\r\nTy>>\u9686\u5364\u788c\u811b\u812a\u5a92\u812b\u8119\r\n        with\r\n        [\r\n            _Ty=__int64\r\n        ]\r\n.\\tensorflow/core/util/tensor_format.h(451): note: \u864f\u8126\u5f55\u6ca1\u9732\u812d\u812e\u5a92\u812d\u8137\u5364\u813f\u812a\u6bdb\r\n\u788c\u811b\u810c\u813f \u811b\u62e2\u63b3\u6c13 \u8122\u788c\u810c\u5a92\u7984\u7089\u9686\u63b3absl::Span<const __int64>\u9686\u5364\u788c\u811b\u812a\u5a92\u812b\u8119\r\nTarget //tensorflow:libtensorflow.so failed to build\r\nINFO: Elapsed time: 25.775s, Critical Path: 23.45s\r\nINFO: 14 processes: 14 local.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\n```", "The latest is an issue that was fixed last week. You will need to sync your code to head and retry.\r\nAt this point, this is becoming a technical support issue. It is now clear that this is not a bug in TensorFlow, but rather setup problems in your environment.\r\n\r\nPlease make sure to follow this page to be able to build TF on windows:\r\nhttps://www.tensorflow.org/install/install_sources_windows\r\nAll the solutions to the issues you faced are already documented there.\r\n\r\nClosing this issue.", "@taotaolin Can you sync TF repo to HEAD, this issue should be fixed already.", "> @taotaolin Can you sync TF repo to HEAD, this issue should be fixed already.\r\n\r\nHow should I choose to compile the debug version or the realse version?", "I have same problem like @taotaolin but add -f not help for me\r\n\r\n`bazel run tensorflow/examples/wav_to_spectrogram:wav_to_spectrogram -- --input_wav=C:\\Users\\Lukas\\Downloads\\music.wav --output_image=C:\\Users\\Lukas\\Downloads\\spectogrammmm.png\r\n\r\nStarting local Bazel server and connecting to it...\r\nERROR: C:/users/lukas/documents/source/tensorflow-new/tensorflow/core/platform/default/build_config/BUILD:188:1: no such package '@png_archive//': Traceback (most recent call last):\r\n        File \"C:/users/lukas/documents/source/tensorflow-new/third_party/repo.bzl\", line 106\r\n                _apply_patch(ctx, ctx.attr.patch_file)\r\n        File \"C:/users/lukas/documents/source/tensorflow-new/third_party/repo.bzl\", line 73, in _apply_patch\r\n                _execute_and_check_ret_code(ctx, cmd)\r\n        File \"C:/users/lukas/documents/source/tensorflow-new/third_party/repo.bzl\", line 52, in _execute_and_check_ret_code\r\n                fail(\"Non-zero return code({1}) when ...))\r\nNon-zero return code(127) when executing 'C:\\msys64\\usr\\bin\\bash.exe -l -c patch -p1 -d C:/users/lukas/_bazel_lukas/zizotpgp/external/png_archive -i C:/users/lukas/documents/source/tensorflow-new/third_party/png_fix_rpi.patch':\r\nStdout:\r\nStderr: /usr/bin/bash: patch: p\u253c\u00d6\u251c\u015fkaz nenalezen\r\n and referenced by '//tensorflow/core/platform/default/build_config:png'\r\nERROR: Analysis of target '//tensorflow/examples/wav_to_spectrogram:wav_to_spectrogram' failed; build aborted: Analysis failed\r\nINFO: Elapsed time: 94,156s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (71 packages loaded)\r\nFAILED: Build did NOT complete successfully (71 packages loaded)`\r\nbazel run tensorflow/examples/wav_to_spectrogram:wav_to_spectrogram -- --input_wav=C:\\Users\\Lukas\\Downloads\\music.wav --output_image=C:\\Users\\Lukas\\Downloads\\spectogrammmm.png\r\n\r\nI use bazel 0.17.2\r\nWindwos 10\r\ntensofrlow: master branch with HEAD \r\n\r\nCan you give me any advice? Thanks", "@rychnal \r\nI seem to have this error due to a bad network, it becomes normal after using the proxy."]}, {"number": 22267, "title": "Docs/tutorial link to 'Eager execution', item 1 of 'Research and experimentation' 404 err.", "body": "Documentation; tutorials.\r\nLink to Eager execution tutorial (item 1 of 'Research and experimentation') give a 404 error.\r\n\r\nThe link is currently pointing to:\r\n[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/notebooks/eager_intro.ipynb](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/notebooks/eager_intro.ipynb)\r\n\r\nI believe that it should be pointing to:\r\n[https://github.com/tensorflow/docs/blob/master/site/en/tutorials/eager/eager_basics.ipynb](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/eager/eager_basics.ipynb)", "comments": ["@jmsinusa Thank you for the post. Yes you are right about the pointing of eager_basics.ipynb link, we have moved the notebooks to the tensorflow/docs repository. We have mentioned this update in the Readme file as well. I am closing this issue as you can access the notebooks from the tensorflow/docs repository."]}, {"number": 22266, "title": "How to use the function FusedBatchNorm() in C++?", "body": "The question is that : https://stackoverflow.com/questions/52308509/tensorflow-c-how-to-convert-tensorflowoutput-to-tensor\r\n\r\nPython has the function `covert_to_tensor()`\uff0cbut C++ does not have this function. The `FusedBatchNorm()` need a `Tensor ` input, but my input type is `tensorflow::Input`. So how to solve this problem\uff1fHow to convert `tensorflow::Input` to `Tensor` \uff1f", "comments": ["@EdwardVincentMa : Apologies for late response. Looks like not a bug/performance/feature request therefore question can be better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) for better and faster response. If resolved already, we shall close the issue. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 22265, "title": "Conv2DTranspose supports dilation", "body": "Fix #21598 \r\nI have implemented this function at https://github.com/keras-team/keras/pull/11029 , just pick it to ```tf.keras```.", "comments": ["cc @fchollet Would you mind to have a look? Thanks.", "Nagging Assignee @yifeif: It has been 17 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "https://github.com/tensorflow/tensorflow/commit/8ef3e7c8c053cb6dad530e13c478bbd406ea2c95 added this feature, I'm closing this."]}, {"number": 22264, "title": "Est spec metrics ops check tensor", "body": "This PR corresponds to issue #22066. The Tensor-checking part in `_check_is_tensor_or_operation` is modified to `tensor_util.is_tensor()` to allow `tensor_like` inputs as discussed.\r\n\r\nPlease review @josh11b @alextp ", "comments": ["Thanks for the contribution!"]}, {"number": 22263, "title": "Include 1.11 version numbers and release notes", "body": "", "comments": []}, {"number": 22262, "title": "Clean up docs for abs() and accumulate_n()", "body": "This PR adds some clarifications to the documentation for `tf.abs()` and reformats the docs for `tf.accumulate_n()` into a format more consistent with that of `tf.add_n()`.", "comments": ["@olicht I moved the information about the `shape` and `tensor_dtype` arguments to the \"Arguments\" section at lines 2198-2201 of `math_ops.py`. I can add that information back into the main body of the comment if you'd like.\r\n\r\nBTW, any idea why the CI tests don't seem to be running on this PR?", "Thanks @frreiss! Only user with write access can trigger CI. I can help triggering them after the comments have been addressed.", "Pushed some additional changes to address review comments.", "Nagging Assignee @yifeif: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I'm going to fold this PR into a larger documentation PR I've been working on."]}, {"number": 22261, "title": "Updated docs to point to tfp instead of tf.contrib", "body": "", "comments": ["Nagging Assignee @yifeif: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 22260, "title": "y", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 22259, "title": "`tf.nn.softmax` gives an error at execution time for certain empty inputs", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra (10.13.6)\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.10.0-12-g4dcfddc5d1 1.10.1\r\n- **Python version**: 3.6.4 (Anaconda)\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **GCC/Compiler version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: n/a\r\n- **Exact command to reproduce**: n/a\r\n\r\n### Describe the problem\r\nWhen the softmax axis has length 0 and rank of the tensor is not 2, `tf.nn.softmax` gives an error at execution time:\r\n```\r\n  File \"/Users/dmz/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1612, in _flatten_outer_dims\r\n    output = array_ops.reshape(logits, array_ops.concat([[-1], last_dim_size], 0))\r\n  File \"/Users/dmz/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 6222, in reshape\r\n    _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Reshape cannot infer the missing input size for an empty tensor unless all specified input sizes are non-zero [Op:Reshape]\r\n```\r\nI'd expect it to instead just give back a tensor of the same shape (also empty).\r\n\r\nFor example, `tf.nn.softmax(tf.zeros([0]))` and `tf.nn.softmax(tf.zeros([16,16,0]))` fail, but `tf.nn.softmax(tf.zeros([16,0]))` works (since no reshape needs to be performed).\r\n\r\nThis seems to happen because `_flatten_outer_dims` in `nn_ops.py` uses `-1` as a dimension in a `reshape`. \r\n\r\n### Source code / logs\r\nHere's an interactive session with eager execution that demonstrates the problem. It happens the same way without eager execution.\r\n```\r\nPython 3.6.4 |Anaconda custom (64-bit)| (default, Jan 16 2018, 12:04:33)\r\n[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> tf.enable_eager_execution()\r\n>>> tf.nn.softmax(tf.zeros([0]))\r\n2018-09-13 13:08:55.015409: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/dmz/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/Users/dmz/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1738, in softmax\r\n    return _softmax(logits, gen_nn_ops.softmax, axis, name)\r\n  File \"/Users/dmz/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1679, in _softmax\r\n    logits = _flatten_outer_dims(logits)\r\n  File \"/Users/dmz/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1612, in _flatten_outer_dims\r\n    output = array_ops.reshape(logits, array_ops.concat([[-1], last_dim_size], 0))\r\n  File \"/Users/dmz/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 6222, in reshape\r\n    _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Reshape cannot infer the missing input size for an empty tensor unless all specified input sizes are non-zero [Op:Reshape]\r\n>>> tf.nn.softmax(tf.zeros([16,0]))\r\n<tf.Tensor: id=17, shape=(16, 0), dtype=float32, numpy=array([], shape=(16, 0), dtype=float32)>\r\n>>> tf.nn.softmax(tf.zeros([16,16,0]))\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/dmz/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/Users/dmz/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1738, in softmax\r\n    return _softmax(logits, gen_nn_ops.softmax, axis, name)\r\n  File \"/Users/dmz/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1679, in _softmax\r\n    logits = _flatten_outer_dims(logits)\r\n  File \"/Users/dmz/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1612, in _flatten_outer_dims\r\n    output = array_ops.reshape(logits, array_ops.concat([[-1], last_dim_size], 0))\r\n  File \"/Users/dmz/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 6222, in reshape\r\n    _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Reshape cannot infer the missing input size for an empty tensor unless all specified input sizes are non-zero [Op:Reshape]\r\n>>>\r\n```", "comments": ["@daniel-ziegler I tried with the latest tf-nightly (on Ubuntu 16.04 though), it looks fine. Can you give the tf-nightly a try?\r\n```\r\n# python\r\nPython 2.7.12 (default, Dec  4 2017, 14:50:18) \r\n[GCC 5.4.0 20160609] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> tf.enable_eager_execution()\r\n>>> tf.nn.softmax(tf.zeros([0]))\r\n2018-09-22 12:48:06.009335: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n<tf.Tensor: id=4, shape=(0,), dtype=float32, numpy=array([], dtype=float32)>\r\n>>> tf.nn.softmax(tf.zeros([16,0]))\r\n<tf.Tensor: id=9, shape=(16, 0), dtype=float32, numpy=array([], shape=(16, 0), dtype=float32)>\r\n>>> tf.nn.softmax(tf.zeros([16,16,0]))\r\n<tf.Tensor: id=14, shape=(16, 16, 0), dtype=float32, numpy=array([], shape=(16, 16, 0), dtype=float32)>\r\n>>> tf.VERSION\r\n'1.12.0-dev20180922'\r\n>>> \r\n```", "@daniel-ziegler Hi, have you tried installing \"pip install tf-nightly\" ?", "Just tried it on nightly both with eager and without -- works great. I'll close this issue."]}, {"number": 22258, "title": "Fix Windows GPU Build failure caused by nccl", "body": "This fix is an attempt to fix Window GPU build which was broken:\r\nhttps://source.cloud.google.com/results/invocations/18ee81e7-f798-4ecc-9926-046d82993d80/log\r\n\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["/cc @gunan @yifeif,  FYI @dmitrievanthony", "I have an internal change that is verified to fix the issue across the board.\r\nIt is slightly different, so I am not sure if this one fixes all the issues.\r\n\r\nI will merge that one instead, as that one already passed all presubmits.\r\n\r\n", "Thanks @gunan \ud83d\udc4d "]}, {"number": 22257, "title": "I found that even after following the tensorflow documentation for installing tf gpu, tensorflow and keras was still using CPU only for computation.", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["I solved it as soon as I did a pip install tf-nighly-gpu . I guess this should also be mentioned in the official doc because many GPUs are going wasted because of not knowing about this!", "@jidroid404  Hi, good that you resolved the issue. In order to look into updating the doc, could you please provide the following information.\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary):\r\nTensorFlow version (use command below):\r\nPython version:\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\nExact command to reproduce:\r\n\r\nDescribe the problem : It would be great if you add more info about the issue you are facing. Currently I get to know about the issue by reading the title. Appreciate your clear explanation.", "System information \r\n1.I used a stock example script provided in TensorFlow\r\n2. Windows 10  Home\r\n3. Not on mobile device\r\n3. Installed from binary\r\n4. tf.__version__\r\n'1.11.0-dev20180905'\r\n5.python -V\r\nPython 3.6.6 :: Anaconda, Inc.\r\n6. Not Applicable \r\n7.Not Applicable\r\n8. Cuda_9.0.176.2_windows\r\n    CuDNN : 9.0 \r\n9. Nvidia GPU used 940MX\r\n    Memory :12 GB\r\n10. pip install tf-nightly-gpu\r\n       https://pypi.org/project/tf-nightly-gpu/\r\n\r\n\r\nI had installed CUDA and cuDNN without breaking anything and also set the PATH variable correctly. Still when I was training a model using Keras GPU usage was zero while CPU was maxing out.\r\nAnd evidently so\r\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\nshowed no GPU0\r\nHope this was explainatory and suffices the cause.\r\n", "#12388", "@jidroid404 Installing tensorflow 1.10 would resolve this issue."]}, {"number": 22256, "title": "Option: Use fused_batch_norm instead of batch_norm for layer normalization", "body": "tf.nn.fused_batch_norm uses cudnn for GPU acceleration, tf.nn.batch_normalization does not. This PR adds the option of using fused_batch_norm instead of batch_normalization. It is an opt-in option because fused_batch_norm only improves runtimes on GPUs and does not support double precision tensors.", "comments": ["Thank you for approaching this issue. \r\nHave you tried this change on a specific task? Did it make the layer normalization faster?\r\n", "@AzizCode92 I used it in the attention layer of the MLPerf Transformer benchmark. It made the benchmark run about 25% faster. I don't know how familiar you are with MLPerf, it is a google initiative, the public repository is in https://github.com/mlperf/training/. The Transformer reference code has its own implementation of layer normalization (lines 247-265 in file translation/tensorflow/transformer/model/transformer.py), I changed that to use tf.contrib.layers.layer_norm, which simplified the code, but did not make it any faster. I added the option to use tf.nn.fused_batch_norm and got a 25% jump in throughput on GPUs when running our mixed precision version of Transformer. I have not tested what kind of impact this has on the reference code, which uses purely fp32 arithmetic. ", "Interesting!!! I'm sorry I have no idea about the task you're working on.\r\nI'm applying your patch to the  tf.contrib.rnn.LayerNormBasicLSTMCell where they're applying the layer_norm function. Before I noticed that applying the current code, the training time was too slow. \r\nHave a look at [this issue](https://github.com/tensorflow/tensorflow/issues/21929)\r\nI have also to notice that I combined your modification with this interesting [pull request ](https://github.com/tensorflow/tensorflow/pull/15861).\r\nI'm still training my model and I hope with these two patches I can achieve layerNorm faster and more accurate. \r\nthank you", "I also recommend you to see this [pull request](https://github.com/MycChiu/fast-LayerNorm-TF)\r\n\r\n ", "@AzizCode92 Thanks for telling me about the custom layer norm PR, I'll give that a try. ", "Good, I have worked on it. Unfortunately all the test cases failed!!!\r\nIt is an old repo ( 2 years ago ) but worth spending time to fix it ", "#Update : the latest PR I mentioned is working fine now.\r\nCurrent tensorflow version : 1.10\r\nCuda v.9\r\nand I can confirm it is faster than the normal layer_norm operation we have now  ", "@thorjohnsen could you address @fchollet 's comments? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 22255, "title": "Move ppc64le build/test to cuda 9.2", "body": "CUDA 9.2 is the first version of cuda that nvidia will support on Power9 hardware.\r\n\r\nThe dockerfile is used in the jenkins build of the pp64le whl file and in the CI/CD test.\r\n", "comments": []}, {"number": 22254, "title": "Feature Request: Permit Changes to LSTMStateTuple to be compatible with autodiff", "body": "I would like to make changes to an LSTMStateTuple that (in theory) should be compatible with auto differentiation (I'm taking a vector output by one LSTM and adding it to the cell state `c` of another LSTM at each \"time step\"). However, as far as I can tell, LSTMStateTuple is a named tupled, so making a change to `c` requires creating a new LSTMStateTuple, preventing autodiff from working. I confirmed with Tensorboard summaries that no gradients are flowing through the kernels or biases of my second LSTM. Does a work-around exist or is this capability missing?", "comments": ["Autodiff needs to learn about namedtuples, perhaps?", "Autodiff needs to learn about namedtuples, perhaps?", "Tuples are immutable, so you can't modify them in place", "@ebrevdo , I'm not sure I understand. Yes, tuple are immutable, so they cannot be modified in place. A friend told me that he had no problems creating new `LSTMStateTuple`s, so I'll spend today checking if there's some other problem with my code.", "I think my code actually works as intended. Let me take a closer look and reopen if there's still a problem."]}, {"number": 22253, "title": "Update code owner for s3 and contrib/{kafka,kinesis}", "body": "Add myself so that issues or PRs could be assigned to me.\r\n\r\nNote contrib/{kafka,kinesis} will be moved in 2.0: tensorflow/community#18\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 22252, "title": "Is tensorflow-gpu version same with just tensorflow for cpu?", "body": "I've developed some Keras code using tensorflow with cpu on Windows7 on my pc, and trained it with GPU on linux.\r\n\r\nAfter complete training, I have moved the trained model files to my pc and used them without any problems. But I don't know what happened, it doesn't work on my pc. I guess it is from Keras and tensorflow version issue.\r\n\r\nThe linux's Keras and tensorflow version are Keras (2.1.5), tensorflow-gpu (1.4.1), and my pc's version are Keras (2.1.5), tensorflow (1.4.0.).\r\n\r\nBelow is the error msg.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\WinPython-64bit-3.5.4.1Qt5\\python-3.5.4.amd64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1323, in _do_call\r\n    return fn(*args)\r\n  File \"C:\\WinPython-64bit-3.5.4.1Qt5\\python-3.5.4.amd64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1302, in _run_fn\r\n    status, run_metadata)\r\n  File \"C:\\WinPython-64bit-3.5.4.1Qt5\\python-3.5.4.amd64\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 473, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: indices[22,3] = 13257 is not in [0, 13004)\r\n         [[Node: embedding_1/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/devi\r\nce:CPU:0\"](embedding_1/embeddings/read, embedding_1/Cast)]]\r\n\r\n\r\n```\r\nand \r\n```\r\nFile \"C:\\WinPython-64bit-3.5.4.1Qt5\\python-3.5.4.amd64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1336, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: indices[22,3] = 13257 is not in [0, 13004)\r\n         [[Node: embedding_1/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/devi\r\nce:CPU:0\"](embedding_1/embeddings/read, embedding_1/Cast)]]\r\n```\r\n\r\nand\r\n```\r\nFile \"C:\\WinPython-64bit-3.5.4.1Qt5\\python-3.5.4.amd64\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\r\n    op_def=op_def)\r\n  File \"C:\\WinPython-64bit-3.5.4.1Qt5\\python-3.5.4.amd64\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): indices[22,3] = 13257 is not in [0, 13004)\r\n         [[Node: embedding_1/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/devi\r\nce:CPU:0\"](embedding_1/embeddings/read, embedding_1/Cast)]]\r\n\r\nException ignored in: <bound method BaseSession.__del__ of <tensorflow.python.client.session.Session object at 0x000000001A8DE198>>\r\nTraceback (most recent call last):\r\n  File \"C:\\WinPython-64bit-3.5.4.1Qt5\\python-3.5.4.amd64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 696, in __del__\r\nTypeError: 'NoneType' object is not callable\r\n\r\n```\r\n\r\nThere were several similar questions, but anything were helpful. To use trained model file from Keras (2.1.5) and tensorflow-gpu (1.4.1), which Keras and tensorflow(cpu) version should I use? Of course I use same python version with the linux which is 3.5 on 64bit.\r\n", "comments": ["@bughunter99 If you use Keras (2.1.5) and tensorflow-gpu (1.4.1), use the same versions for pc. Make sure your model doesn't have explicit device allocations to gpu. As an alternative, you can also install tensorflow-gpu on the pc.", "Closing this issue, feel free to reopen if problem persists."]}, {"number": 22251, "title": "error when adding library ", "body": "", "comments": []}, {"number": 22250, "title": "Documentation: \"Creating Custom Estimators\" says that it uses same train_input_fn as \"pre-made Estimator implementation\", but replaces the dataset return value by an iterator.get_next() without explaining", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:na\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:na\r\n- **TensorFlow installed from (source or binary)**:na\r\n- **TensorFlow version (use command below)**:na\r\n- **Python version**:na\r\n- **Bazel version (if compiling from source)**:na\r\n- **GCC/Compiler version (if compiling from source)**:na\r\n- **CUDA/cuDNN version**:na\r\n- **GPU model and memory**:na\r\n- **Exact command to reproduce**:na\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nIn the documentation \"Creating Custom Estimators\" (https://www.tensorflow.org/guide/custom_estimators), under \"Write an Input function\", it is claimed that the function is the same as in \"pre-made Estimator implementation\" (https://www.tensorflow.org/guide/premade_estimators) and in iris_data.py (https://github.com/tensorflow/models/blob/master/samples/core/get_started/iris_data.py). But in both these sources the function is returning the created dataset, while in \"Creating Custom Estimators\" the function returns dataset.make_one_shot_iterator().get_next() instead.\r\n\r\nI haven't been able to understand if both approaches are equivalent, or if both are correct but yield different results, or if one of them is wrong. But in any of these situations, I think the documentation should state why it is using the get next operations instead of the original dataset, since it refers to sources where dataset is used instead.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n\"Creating Custom Estimators\": https://www.tensorflow.org/guide/custom_estimators\r\nClaim: \"_Our custom Estimator implementation uses the same input function as our pre-made Estimator implementation, from iris_data.py. Namely:_\"\r\nFunction:\r\n```\r\ndef train_input_fn(features, labels, batch_size):\r\n\u00a0 \u00a0 \"\"\"An input function for training\"\"\"\r\n\u00a0 \u00a0 # Convert the inputs to a Dataset.\r\n\u00a0 \u00a0 dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\r\n\r\n\u00a0 \u00a0 # Shuffle, repeat, and batch the examples.\r\n\u00a0 \u00a0 dataset = dataset.shuffle(1000).repeat().batch(batch_size)\r\n\r\n\u00a0 \u00a0 # Return the read end of the pipeline.\r\n\u00a0 \u00a0 return dataset.make_one_shot_iterator().get_next()\r\n```\r\n\"Premade Estimators\": https://www.tensorflow.org/guide/premade_estimators\r\nFunction:\r\n```\r\ndef train_input_fn(features, labels, batch_size):\r\n\u00a0 \u00a0 \"\"\"An input function for training\"\"\"\r\n\u00a0 \u00a0 # Convert the inputs to a Dataset.\r\n\u00a0 \u00a0 dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\r\n\r\n\u00a0 \u00a0 # Shuffle, repeat, and batch the examples.\r\n\u00a0 \u00a0 return dataset.shuffle(1000).repeat().batch(batch_size)\r\n```\r\niris_data.py: https://github.com/tensorflow/models/blob/master/samples/core/get_started/iris_data.py\r\nFunction:\r\n```\r\ndef train_input_fn(features, labels, batch_size):\r\n    \"\"\"An input function for training\"\"\"\r\n    # Convert the inputs to a Dataset.\r\n    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\r\n    # Shuffle, repeat, and batch the examples.\r\n    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\r\n    # Return the dataset.\r\n    return dataset\r\n```", "comments": ["Yes, these docs have gotten out of sync. \r\n\r\nThe only reasonable direction to go for these is to update them to be `.ipynb` notebooks, inline all the code and delete those example directories.", "This is done, and those docs no-longer exist."]}, {"number": 22249, "title": "Inconsistency in supported integer types on GPU", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nnot relevant\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux 4.9.0-8-amd64 #1 SMP Debian 4.9.110-3+deb9u4 (2018-08-21) x86_64 GNU/Linux\r\nVERSION_ID=\"9\"\r\nVERSION=\"9 (stretch)\"\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n1.10.1 (from pip binary)\r\n- **TensorFlow version (use command below)**:\r\ntf.VERSION = 1.10.1\r\ntf.GIT_VERSION = v1.10.1-0-g4dcfddc5d1\r\ntf.COMPILER_VERSION = v1.10.1-0-g4dcfddc5d1\r\n- **Python version**:\r\nPython 3.5.3\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2017 NVIDIA Corporation\r\nBuilt on Fri_Nov__3_21:07:56_CDT_2017\r\nCuda compilation tools, release 9.1, V9.1.85\r\n\r\n- **GPU model and memory**:\r\nTesla P100-PCIE-16GB\r\n\r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\nIt appears that the kernel of `tf.reduce_sum` is not registerd for GPU's if the type of the tensor to be summed is `int64` (only registered for `tf.int32`)\r\nMoreover the kernel of `tf.tile` is not registered for the case where the tensor to be tiled is of type `tf.int32` (only registered for `tf.int64`)\r\n\r\nWhy is there this inconsistency?\r\n", "comments": ["tf.tile has int32 implementation (not int64)\r\ntf.reduce has no int* implementations. There are kernels registered on GPU with int32 but they run on CPU since in most common case the tensors are small and launching GPU kernel would be inefficient.\r\nIf you indeed need an int32 op that runs on GPU you are out of luck.\r\n\r\nGenerally, there is an inconsistency since for some ops registering an int32 gpu kernel would lead to many small  ops  being launched  on GPU.", "Nagging Assignee @azaks2: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 22248, "title": "Calculating custom metrics with tf.estimator.DNNRegressor in TensorFlow 1.10", "body": "How to configure a `tf.estimator.DNNRegressor` to report different metrics like **RMSE** and **MAE** while evaluating?\r\n\r\n(One can ask the same question for `tf.estimator.DNNClassifier` and **AUC** metric)\r\n\r\n> **Note:** I know that it must be done in `tf.estimator.EstimatorSpec` of `model_fn()` for a custom `tf.estimator.Estimator`, but I don't know how to apply it for a `tf.estimator.DNNRegressor`.\r\n\r\n[https://stackoverflow.com/q/52300519/2737801](https://stackoverflow.com/q/52300519/2737801)", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n"]}, {"number": 22247, "title": "Use the last char instead of the first in prediction", "body": "1. As it's mentioned the `start_string` could be a string here, in my understanding, the correctly predicted next character should be the last one in `predictions`.\r\n2. As `tf.multinomial` is not used any more here, `temperature` has no effect either.\r\n\r\n- i.e.\r\n  - With `start_string = 'QUEEN'`, the model prediction string is `UEEN:`, so the next char should be `:`\r\n  - In the loop run `for i in range(num_generate):`\r\n    - With `predicted_id = tf.argmax(predictions[0]).numpy()`, output is `QUEENUS:\\nI think so ...`\r\n    - With `predicted_id = tf.argmax(predictions[-1]).numpy()`, output is `QUEEN:\\nThe grand the ...`\r\n", "comments": ["@yashk2810 does this make sense to you?\r\n\r\nI'm a little confused about what this change entails.", "So, I tried out the change and for a single character start_string, the predictions shape is (1, 65). But if the start_string is a word like 'Queen', the shape of the first prediction is (5, 65). Hence we need to use the last character from the prediction to predict the next character, hence `predictions[-1]` makes sense there. \r\n\r\nThis doesn't matter if the input is a single character. So changing to `predictions[-1]` will make the predictions right if the start_string is a word and it won't change anything if the start_string is a character.\r\n\r\nAs far as temperature goes, it makes sense to remove it since tf.multinomial is not being used.\r\n\r\n@leondgarse Can you also remove the comment above the predicted_id line which mentions tf.multinomial and change the text in the markdown above?", "Sorry for the delay, I just replaced the sentence `a multinomial distribution` by `argmax` in comment and the above text.", "Nagging Assignee @yifeif: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 22246, "title": "CPU nodes are swapped out using /grappler/optimizers/gpu_swapping_kernel", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.5\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: Issue occurs on both pip binary and compiled from source\r\n- **TensorFlow version (use command below)**: 'v1.8.0-8-g23c2187' 1.8.0\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**: 0.16.1\r\n- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\n- **CUDA/cuDNN version**: CUDA 9.0, cuDNN 7.2.1.38\r\n- **GPU model and memory**: GeForce GTX 1080, 11178MiB\r\n- **Exact command to reproduce**: Cannot share the exact codebase\r\n\r\n\r\n### Describe the problem\r\n\r\nTensorflow attempts to swap-out a node to the CPU to save memory, even when the node is already on CPU.\r\n\r\nThis happens in `core/grappler/optimizers/memory_optimizers.cc` with the op `_CopyFromGpuToHost`.\r\nBut since this swapping is intended to save GPU memory by swapping temporarily nodes from GPU to the the CPU, the kernel for that op in `core/grappler/optimizers/gpu_swapping_kernels.cc` is only for GPU devices.\r\n\r\nA suggested fix will be to simply check if the node to be swapped out is already in CPU before making it eligible for swap_out/swap_in. Note that on reducing the model size this error disappears as more memory becomes free.\r\n\r\n#### Some more information confirming the issue - \r\nThe op is correctly registered in OpRegistry as shown by `tensorflow::OpRegistry::Global()->DebugString(false)`.\r\nThe OpKernel `CopyFromGpuToHostKernel` is built according to bazel logs.\r\nThe OpKernel is loaded on importing tensorflow in python, verified by adding a print in that file using `__attribute__((constructor)) )`\r\n\r\n### Source code / logs\r\n\r\nThe relevant error is -\r\n\r\ntensorflow.python.framework.errors_impl.NotFoundError: No registered '_CopyFromGpuToHost' OpKernel for CPU devices compatible with node swap_out_context_zoom/GatherNd_2_0 = _CopyFromGpuToHost[T=DT_INT32, _class=[\"loc@context_zoom/GatherNd_2_0\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_context_sents_len_0_10)\r\n\t.  Registered:  device='GPU'\r\n\r\n\t [[Node: swap_out_context_zoom/GatherNd_2_0 = _CopyFromGpuToHost[T=DT_INT32, _class=[\"loc@context_zoom/GatherNd_2_0\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_context_sents_len_0_10)\r\n\r\nI have attached the full log here - \r\nhttps://gist.github.com/akhilkedia/c13ff191e08b4462a3870a0b3a091e93\r\n\r\n", "comments": ["I just noticed that this is probably a duplicate of #22134 . Should I perhaps close this?", "@akhilkedia  Thank you for your post. Yes this is a duplicate of #22134 . Therefore I am going to close this issue. Please stay tuned to #22134 for the fix and feel free to reopen if your issue still persists."]}, {"number": 22245, "title": "Could not open .\\linear_regression.pbtxt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):", "body": "Please go to Stack Overflow for help and support:\r\n\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n------------------------\r\n\r\n### System information\r\nTensorFlow 1.9\r\nWindows 10\r\nPython 3.6 64\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\n```\r\n2018-09-12 19:55:49.880076: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2018-09-12 19:55:49.898198: W C:\\tf_jenkins\\workspace\\rel-win\\M\\windows\\PY\\36\\tensorflow\\core\\util\\tensor_slice_reader.cc:95] Could not open .\\linear_regression.pbtxt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\r\nTraceback (most recent call last):\r\n  File \"C:/Users/vlad/Downloads/Linear_Regression_Model/Freeze_Graph.py\", line 7, in <module>\r\n    initializer_nodes='', variable_names_blacklist=''\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\tools\\freeze_graph.py\", line 244, in freeze_graph\r\n    saved_model_tags.split(\",\"), checkpoint_version=checkpoint_version)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\tools\\freeze_graph.py\", line 119, in freeze_graph_with_def_protos\r\n    reader = pywrap_tensorflow.NewCheckpointReader(input_checkpoint)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 254, in NewCheckpointReader\r\n    return CheckpointReader(compat.as_bytes(filepattern), status)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 516, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.DataLossError: Unable to open table file .\\linear_regression.pbtxt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\r\n\r\n```\r\n\r\n\r\n\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.tools import freeze_graph, optimize_for_inference_lib\r\n\r\nfreeze_graph.freeze_graph(input_graph='linear_regression.pbtxt', input_saver='', input_binary=True, input_checkpoint='linear_regression.pbtxt', output_node_names='y_output',\r\n                          restore_op_name='save/restore_all',\r\n                          filename_tensor_name='save/Const:0', output_graph='frozen_linear_regression.pb', clear_devices= True,\r\n                          initializer_nodes='', variable_names_blacklist=''\r\n                          )\r\ninput_graph_def = tf.GraphDef()\r\nwith tf.gfile.Open('frozen_linear_regression.pb', 'rb') as f:\r\n    data = f.read()\r\n    input_graph_def.ParseFromString(data)\r\n\r\noutput_graph_def = optimize_for_inference_lib.optimize_for_inference(input_graph_def=input_graph_def, input_node_names=['x'],\r\n                                                                     output_node_names=['y_output'],\r\n                                                                     placeholder_type_enum=tf.float32.as_datatype_enum)\r\n\r\nf = tf.gfile.FastGFile(name='optimized_frozen_linear_regression.pb', mode='w')\r\n\r\nf.write(file_content=output_graph_def.SerializeToString())\r\n\r\n\r\n```", "comments": ["Can you help me guys? ", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow-tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n"]}, {"number": 22244, "title": "[Intel-MKL] Enable MKL Relu6 op", "body": "", "comments": ["@agramesh1 ", "@yifeif Can you merge this PR? Thanks.", "Sorry this fell off of my radar. Importing now!", "@yifeif thanks.", "@yifeif Sorry to bug you. The PR seems to be stuck in \"ready to pull\" state. can you take a look at it?", "@yifeif should we pull again?", "@yifeif I have solve the merge conflict, could you help to pull it again? thanks.", "@ezhulenev I have solve the merge conflict, could you help to review it again? thanks.", "@yifeif @drpngx I find feedback/copybara failed, but I cannot check the detail information, can you help me on this?", "i think it's an unrelated failure (`control_flow_gpu_test`) fails with some DMA non-variant copy error. Re-running the tests."]}, {"number": 22243, "title": "bazel build branch r1.10 on windows ERROR _api_implementation.so", "body": "ERROR: C:/users/thinkpad/_bazel_thinkpad/26orbg4z/external/protobuf_archive/BUILD:645:1: C++ compilation of rule '@protobuf_archive//:python/google/protobuf/internal/_api_implementation.so' failed (Exit 1): cl.exe failed: error executing command\r\n\r\nC:\\users\\thinkpad\\_bazel_thinkpad\\26orbg4z\\execroot\\org_tensorflow\\external\\protobuf_archive\\python\\google\\protobuf\\internal\\api_implementation.cc : fatal error C1083: \u65e0\u6cd5\u6253\u5f00\u7f16\u8bd1\u5668\u751f\u6210\u7684\u6587\u4ef6: \u201c\u201d: Invalid argument\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 22363.015s, Critical Path: 4574.16s\r\nINFO: 6185 processes: 6185 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\nHow to solve compilation errors?\r\nThank you", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Versions:Windows 10 64bit\r\nVS2017 with VS2015 workload \r\nPython 3.6.6\r\nBazel 0.15.2\r\nTensorFlow r1.10 CPU version\r\n", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}]