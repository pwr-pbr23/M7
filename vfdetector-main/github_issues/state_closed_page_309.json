[{"number": 44975, "title": "Conversion to TFLite fails with 2.4.0-rc1", "body": "**System information**\r\n- OS - Windows\r\n- TensorFlow installed from binary\r\n- TensorFlow version (or github SHA if from source): v2.4.0-rc1\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nRunning the following set of commands to do the conversion to tf-lite\r\n```\r\ngraph_def = tf.compat.v1.GraphDef()\r\ngraph_def.ParseFromString(open('resnet.pb', 'rb').read())\r\nconcrete_func = wrap_frozen_graph(graph_def, inputs=[\"input:0\"], outputs=[\"output:0\"])\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\nconverter.experimental_new_converter = True\r\nconverter.optimizations = [tf.compat.v1.lite.Optimize.DEFAULT]\r\nconverter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n--------------------------------------------------\r\nFrozen model layers: \r\n--------------------------------------------------\r\n\r\nINFO:absl:Using new converter: If you encounter a problem please file a bug. You can opt-out by setting experimental_new_converter=False\r\n\r\n---------------------------------------------------------------------------\r\nException                                 Traceback (most recent call last)\r\n~\\Anaconda3\\envs\\arcgis_1_8_3_tensorflow_test\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    212                                                  debug_info_str,\r\n--> 213                                                  enable_mlir_converter)\r\n    214       return model_str\r\n\r\n~\\Anaconda3\\envs\\arcgis_1_8_3_tensorflow_test\\lib\\site-packages\\tensorflow\\lite\\python\\wrap_toco.py in wrapped_toco_convert(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n     37       debug_info_str,\r\n---> 38       enable_mlir_converter)\r\n     39 \r\n\r\nException: C:\\Users\\kar11081\\Anaconda3\\envs\\arcgis_1_8_3_tensorflow_test\\lib\\site-packages\\tensorflow\\python\\eager\\lift_to_graph.py:339:0: error: operand #0 does not dominate this use\r\nC:\\Users\\kar11081\\Anaconda3\\envs\\arcgis_1_8_3_tensorflow_test\\lib\\site-packages\\tensorflow\\python\\eager\\wrap_function.py:338:0: note: called from\r\n<ipython-input-5-1fc724505eae>:18:0: note: called from\r\n<ipython-input-9-dd55b0fa94e3>:3:0: note: called from\r\nC:\\Users\\kar11081\\Anaconda3\\envs\\arcgis_1_8_3_tensorflow_test\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3417:0: note: called from\r\nC:\\Users\\kar11081\\Anaconda3\\envs\\arcgis_1_8_3_tensorflow_test\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3337:0: note: called from\r\nC:\\Users\\kar11081\\Anaconda3\\envs\\arcgis_1_8_3_tensorflow_test\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3146:0: note: called from\r\nC:\\Users\\kar11081\\Anaconda3\\envs\\arcgis_1_8_3_tensorflow_test\\lib\\site-packages\\IPython\\core\\async_helpers.py:68:0: note: called from\r\nC:\\Users\\kar11081\\Anaconda3\\envs\\arcgis_1_8_3_tensorflow_test\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2922:0: note: called from\r\nC:\\Users\\kar11081\\Anaconda3\\envs\\arcgis_1_8_3_tensorflow_test\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2877:0: note: called from\r\nC:\\Users\\kar11081\\Anaconda3\\envs\\arcgis_1_8_3_tensorflow_test\\lib\\site-packages\\tensorflow\\python\\eager\\lift_to_graph.py:339:0: note: operand defined here\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nConverterError                            Traceback (most recent call last)\r\n<ipython-input-9-dd55b0fa94e3> in <module>\r\n      6 converter.optimizations = [tf.compat.v1.lite.Optimize.DEFAULT]\r\n      7 converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.OpsSet.SELECT_TF_OPS]\r\n----> 8 tflite_model = converter.convert()\r\n\r\n~\\Anaconda3\\envs\\arcgis_1_8_3_tensorflow_test\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py in convert(self)\r\n   1115         Invalid quantization parameters.\r\n   1116     \"\"\"\r\n-> 1117     return super(TFLiteConverterV2, self).convert()\r\n   1118 \r\n   1119 \r\n\r\n~\\Anaconda3\\envs\\arcgis_1_8_3_tensorflow_test\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py in convert(self)\r\n    941 \r\n    942     return super(TFLiteFrozenGraphConverterV2,\r\n--> 943                  self).convert(graph_def, input_tensors, output_tensors)\r\n    944 \r\n    945 \r\n\r\n~\\Anaconda3\\envs\\arcgis_1_8_3_tensorflow_test\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py in convert(self, graph_def, input_tensors, output_tensors)\r\n    626         input_tensors=input_tensors,\r\n    627         output_tensors=output_tensors,\r\n--> 628         **converter_kwargs)\r\n    629 \r\n    630     calibrate_and_quantize, flags = quant_mode.quantizer_flags()\r\n\r\n~\\Anaconda3\\envs\\arcgis_1_8_3_tensorflow_test\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py in toco_convert_impl(input_data, input_tensors, output_tensors, enable_mlir_converter, *args, **kwargs)\r\n    611       input_data.SerializeToString(),\r\n    612       debug_info_str=debug_info_str,\r\n--> 613       enable_mlir_converter=enable_mlir_converter)\r\n    614   return data\r\n    615 \r\n\r\n~\\Anaconda3\\envs\\arcgis_1_8_3_tensorflow_test\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    214       return model_str\r\n    215     except Exception as e:\r\n--> 216       raise ConverterError(str(e))\r\n    217 \r\n    218   if distutils.spawn.find_executable(_toco_from_proto_bin) is None:\r\n\r\nConverterError: C:\\Users\\kar11081\\Anaconda3\\envs\\arcgis_1_8_3_tensorflow_test\\lib\\site-packages\\tensorflow\\python\\eager\\lift_to_graph.py:339:0: error: operand #0 does not dominate this use\r\nC:\\Users\\kar11081\\Anaconda3\\envs\\arcgis_1_8_3_tensorflow_test\\lib\\site-packages\\tensorflow\\python\\eager\\wrap_function.py:338:0: note: called from\r\n<ipython-input-5-1fc724505eae>:18:0: note: called from\r\n<ipython-input-9-dd55b0fa94e3>:3:0: note: called from\r\nC:\\Users\\kar11081\\Anaconda3\\envs\\arcgis_1_8_3_tensorflow_test\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3417:0: note: called from\r\nC:\\Users\\kar11081\\Anaconda3\\envs\\arcgis_1_8_3_tensorflow_test\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3337:0: note: called from\r\nC:\\Users\\kar11081\\Anaconda3\\envs\\arcgis_1_8_3_tensorflow_test\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3146:0: note: called from\r\nC:\\Users\\kar11081\\Anaconda3\\envs\\arcgis_1_8_3_tensorflow_test\\lib\\site-packages\\IPython\\core\\async_helpers.py:68:0: note: called from\r\nC:\\Users\\kar11081\\Anaconda3\\envs\\arcgis_1_8_3_tensorflow_test\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2922:0: note: called from\r\nC:\\Users\\kar11081\\Anaconda3\\envs\\arcgis_1_8_3_tensorflow_test\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2877:0: note: called from\r\nC:\\Users\\kar11081\\Anaconda3\\envs\\arcgis_1_8_3_tensorflow_test\\lib\\site-packages\\tensorflow\\python\\eager\\lift_to_graph.py:339:0: note: operand defined here\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\n[Link to the model](https://drive.google.com/file/d/1RSmk7zdqmHHBjkE-KPY3BNrW5q0Txv-P/view?usp=sharing)\r\n```\r\n\r\n**Failure details**\r\nThe failure is observed only in the v2.4.0-rc1. The conversion works fine when I switch to nightly version tf-nightly==2.4.0.dev20200901\r\n\r\n", "comments": ["@KarthikDutt \r\nI ran the code on nightly but face a different error, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/ca8c330949823528cbc8200fe28c35a5/untitled463.ipynb). please share all dependencies for us to replicate the issue faced.", "Hello, I have recreated the issue here: \r\nhttps://colab.research.google.com/gist/KarthikDutt/64c0ed9d9ea09a40cd6028fb339d7fe2/untitled463.ipynb\r\n\r\nPlease note, the issue doesn't occur with the nightly  2.4.0.dev20200905 but occurs in v2.4.0-rc1. \r\nIn the gist, you will find code working perfectly when I have nightly build from 2.4.0.dev20200905 and in the subsequent portion of the notebook (after installing v2.4.0-rc1) , the same code fails with the error I reported. ", "@KarthikDutt \r\nPlease share \"resnet.pb\" for us to replicate the issue faced.", "@Saduf2019 - resnet.pb is available here: \r\nhttps://drive.google.com/file/d/1RSmk7zdqmHHBjkE-KPY3BNrW5q0Txv-P/view?usp=sharing\r\n", "@Saduf2019 Any update on this? ", "i ran the code shared, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/acedfe5ba2b4cb18344a15c069791d52/untitled477.ipynb).", "You are using the wrong version of the nightly. If you look at the nightly\nwhich I used it\u2019s a different version. Apparently the conversion was\nhappening fine during the nightlies built for 2.4 but the feature was\neither not propagated to RC versions or purposefully left out.  You will be\nable to reproduce the error with the tf nightly which I have used.\n\nOn Sat, 5 Dec 2020 at 12:32 AM, Saduf2019 <notifications@github.com> wrote:\n\n> i ran the code shared, please find the gist here\n> <https://colab.research.google.com/gist/Saduf2019/acedfe5ba2b4cb18344a15c069791d52/untitled477.ipynb>\n> .\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/44975#issuecomment-738960560>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AFIPWQEVV2MMZ4GD4Y7VABDSTEW3HANCNFSM4TZZSW2Q>\n> .\n>\n", "@KarthikDutt Agree with you. Conversion is successful with `tf-nightly` but not successful with `TF2.4.0rcx`. I tried with most recent TF version `2.4rc4` but that was also not successful. We will look into it. in the meantime, please use `tf-nightly`. Thanks!", "Is my understanding correct that the issue is fixed in the nightly already ?", "@karimnosseir Yes, Conversion is successful in `tf-nighty`. But, those fixes didn't propagate to `2.4` branch (including 2.4rc3 which was released today). Thanks!", "@karimnosseir @jvishnuvardhan Will this be part of 2.4? ", "@KarthikDutt No AFAIK, they should be part of the next release though 2.5", "I am not sure there is any action on the issue. So closing.\r\n\r\nThanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44975\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44975\">No</a>\n", "@karimnosseir @jvishnuvardhan The nightly version 2.4.0.dev20200905 is no longer available. It would be great if you can provide a workaround until this is fixed in 2.5. I can build it locally if you direct me to to the commit which solved this issue. ", "@KarthikDutt i don't know which commit resolved your issue. Can't you use the latest nightly instead ? does it matter which nightly you are using ?", "@karimnosseir I tried with the latest nightly and the bug is not fixed. It was fixed in 2.4.0.dev20200905. As I mentioned before, this fix was somehow not propagated to subsequent nightlies.  Is there any way I can still download and build 2.4.0.dev20200905 on my machine? Any other suggestions on how I can bring in this fix until 2.5 release would be much appreciated. ", "@KarthikDutt I tried latest nightly and works fine for me.\r\nSample of the code\r\n\r\n```\r\nfrom tensorflow.python.util import compat\r\nfrom tensorflow.python.platform import gfile\r\n\r\ngraph_def = tf.compat.v1.GraphDef()\r\nwith gfile.FastGFile('/content/resnet.pb', 'rb') as f:\r\n  data = compat.as_bytes(f.read())\r\n  graph_def.ParseFromString(data)\r\n\r\nconcrete_func = wrap_frozen_graph(graph_def, inputs=[\"input:0\"], outputs=[\"output:0\"])\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\nconverter.experimental_new_converter = True\r\nconverter.optimizations = [tf.compat.v1.lite.Optimize.DEFAULT]\r\nconverter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\n```", "@karimnosseir - You are right. Conversion works fine with the latest nightly on a classification example I had shared before. However, it isn't fixed for a retinaNet model which I have uploaded in the link shared below. Note that the tfnightly version 2.4.0.dev20200905 fixes the issue for retinaNet as well. \r\n[https://drive.google.com/file/d/1HciakM1T5BIJ_ikyBPeMaGzwih9ZGE1K/view?usp=sharing](url)\r\n\r\nTo summarize, part of the issue is fixed in latest nightly but still there are elements from 2.4.0.dev20200905 which are not yet propagated to the latest nightly.  \r\n\r\nHere's the code that I use to convert the retinanet model. \r\n\r\n`graph_def = tf.compat.v1.GraphDef()`\r\n`graph_def.ParseFromString(open(r'retinaNet.pb', 'rb').read())`\r\n`concrete_func = wrap_frozen_graph(graph_def, inputs=[\"input:0\"], outputs=[\"scores:0\",\"box:0\"])`\r\n`converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])`\r\n`converter.experimental_new_converter = True`\r\n`converter.optimizations = [tf.compat.v1.lite.Optimize.DEFAULT]`\r\n`#converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.OpsSet.SELECT_TF_OPS]`\r\n`converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,tf.lite.OpsSet.SELECT_TF_OPS]`\r\n`tflite_model = converter.convert()`\r\n"]}, {"number": 44974, "title": "Error LNK2005 when linking in debug mode.", "body": "**System information**\r\n- OS Platform and Distribution : windows 10\r\n- TensorFlow installed from: source and binary\r\n- TensorFlow version: 2.3.0\r\n- Python version: 3.7\r\n- Installed using virtualenv: using c API\r\n- Bazel version: 3.7.0\r\n- GCC/Compiler version: msvc 2019\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: None\r\n\r\n\r\n\r\n**Describe the problem**\r\nI have a c++ shared library that links both tensorflow and protobuf.\r\n* In release everything works and as expected.\r\n* In debug mode I got a linking issue:\r\n```\r\nlibprotobufd.lib(common.obj) : error LNK2005: \"public: __cdecl google::protobuf::internal::LogMessage::LogMessage(enum google::protobuf::LogLevel,char const *,int)\" (??0LogMessage@internal@protobuf@google@@QEAA@W4LogLevel@23@PEBDH@Z) already defined in tensorflow.lib(tensorflow.dll)\r\n```\r\n\r\nI got this error when using precompiled binaries from: https://www.tensorflow.org/install/lang_c\r\nI tried to compile tensorflow myself in both release and debug mode but still I get the same linking error.\r\n\r\n\r\n**Any other info / logs**\r\n```\r\n00:23:39  libprotobufd.lib(common.obj) : error LNK2005: \"public: __cdecl google::protobuf::internal::LogMessage::LogMessage(enum google::protobuf::LogLevel,char const *,int)\" (??0LogMessage@internal@protobuf@google@@QEAA@W4LogLevel@23@PEBDH@Z) already defined in tensorflow.lib(tensorflow.dll)\r\n\r\n00:23:39  libprotobufd.lib(common.obj) : error LNK2005: \"public: __cdecl google::protobuf::internal::LogMessage::~LogMessage(void)\" (??1LogMessage@internal@protobuf@google@@QEAA@XZ) already defined in tensorflow.lib(tensorflow.dll)\r\n\r\n00:23:39  libprotobufd.lib(common.obj) : error LNK2005: \"public: class google::protobuf::internal::LogMessage & __cdecl google::protobuf::internal::LogMessage::operator<<(char const *)\" (??6LogMessage@internal@protobuf@google@@QEAAAEAV0123@PEBD@Z) already defined in tensorflow.lib(tensorflow.dll)\r\n\r\n00:23:39  libprotobufd.lib(common.obj) : error LNK2005: \"public: void __cdecl google::protobuf::internal::LogFinisher::operator=(class google::protobuf::internal::LogMessage &)\" (??4LogFinisher@internal@protobuf@google@@QEAAXAEAVLogMessage@123@@Z) already defined in tensorflow.lib(tensorflow.dll)\r\n\r\n00:23:39     Creating library lib\\pmi_common_d.lib and object lib\\pmi_common_d.exp\r\n00:23:39  bin\\pmi_common_d.dll : fatal error LNK1169: one or more multiply defined symbols found\r\n```", "comments": ["Did you try msvc docs on [linker error 2005](https://docs.microsoft.com/en-us/cpp/error-messages/tool-errors/linker-tools-error-lnk2005?view=msvc-160) and [force file output](https://docs.microsoft.com/en-us/cpp/build/reference/force-force-file-output?view=msvc-160)?", "Thanks!! That fixed the issue thanks. \r\nBut still the multiple symbols are there, it is just the force flag ignore them. I'm wondering if there is a proper fix for this. \r\n ", "@boussaffawalid Could you please try using latest stable version of **TF 2.6.0** and let us know if the issue still persists? Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44974\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44974\">No</a>\n"]}, {"number": 44973, "title": "When training on the TPU, loss  is in PerReplica dictionary format. Any way to convert it in normal numpy/tensor format?", "body": "![image](https://user-images.githubusercontent.com/18486587/99510460-1dbdef80-29ad-11eb-9b81-ce211054c542.png)\r\n\r\nI am returning the loss, from train_step function like this:\r\n\r\n    def train_step(self, data, model_inputs):\r\n        with tf.GradientTape() as tape:\r\n            predictions = self.model(model_inputs['inputs'], training=True)\r\n            losses = self.loss_func(data,  predictions)\r\n            loss = sum(losses.values()) / self.train_batch_size\r\n        \r\n        gradients = tape.gradient(loss, self.model.trainable_variables)\r\n        self.optimizer.apply_gradients(\r\n            list(zip(gradients, self.model.trainable_variables))\r\n        )\r\n\r\n        self.update_metrics_meter(data, predictions)\r\n        self.update_loss_meter(losses, self.train_batch_size)\r\n        return losses", "comments": ["@coreqode,\r\nIn order to reproduce the issue reported here, could you please provide the TensorFlow version, the complete code and the dataset you are using. Thanks!\r\n", "Thanks for quick reply @amahendrakar .\r\nI am using, colab tpu with tensorflow version 2.3.0. The snippet of the training loop and other functions can be found [here](https://github.com/coreqode/tboard_issue/blob/master/base_module.py)\r\n\r\n I am sorry, dataset is private so can't provide that. Basically, the task is just semantic segmentation of images.", "@coreqode,\r\nThe code snippet you have linked has only the function definitions. But you're not calling the functions anywhere in the code. Could you please provide a minimal code snippet using a dummy dataset so that we can look into this.\r\n\r\nAlso, if this is not a TensorFlow bug or feature request, it is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a larger community that reads questions there. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44973\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44973\">No</a>\n"]}, {"number": 44972, "title": "Moving average not supported in eager mode", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nwhen I enable \"use_moving_average: true\" I encountered this error,\r\n File \"/xxxxxxxx/models/research/object_detection/builders/optimizer_builder.py\", line 150, in build_optimizers_tf_v2\r\n    raise ValueError('Moving average not supported in eager mode.')\r\nValueError: Moving average not supported in eager mode.\r\n\r\n", "comments": ["We need a very, very minimal code example that we coucould copy, paste and run to reproduce this (or a Colab).", "@gneworld \r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44972\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44972\">No</a>\n"]}, {"number": 44971, "title": "Flexbuffers patching causing -Wnull-dereference", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- Tensorflow version (commit SHA if source): 41dc09029c39777b3be83e6cc00853be7c5f0674\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):\r\n\r\n**Describe the problem**\r\ntensorflow/lite/micro/tools/make/flatbuffers_download.sh is patching Flexbuffers. The original code is replaced with:\r\n// Introduce a segfault for an unsupported code path for TFLM.\r\nreturn *(static_cast<double*>(nullptr));\r\nThis is causing a compile issue for us, i.e. it triggers:\r\nerror: indirection of non-volatile null pointer will be deleted, not trap [-Werror,-Wnull-dereference]\r\n\r\nPerhaps we could generate a segfault in another way or maybe just log an error and return 0.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\n", "comments": ["Tagging @advaitjain ", "Sorry about that.\r\n\r\nLogging an error is hard because we're in the external flatbuffers library. Do you have a suggestion for a different way to generate a segfault?\r\n\r\nAlternately, I'd be fine with an ifdef for the arm compiler (as part of the flatbuffer patching) so that you don't run into this issue.", "Documenting for the future when we might wonder why we did what we did.\r\n\r\n#45040 made null-dereference an error for any code that includes flexbuffers.h which resulted in [odd build errors](https://github.com/tensorflow/tensorflow/pull/43566#issuecomment-733250524) because the version of patched flatbuffers that I had locally was changing my build.\r\n\r\nTFLM never really turned on `-Wnull-dereference` and as a test when I did explicitly enable `-Wnull-dereference`, I got a bunch of  (the same) errors in the flatbuffers code and at least one in the TFLM code as well:\r\n```bash\r\ntensorflow/lite/micro/tools/make/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:252:48: error: potential null pointer dereference [-Werror=null-dereference]\r\n  252 |   uoffset_t size() const { return EndianScalar(length_); }\r\nIn file included from tensorflow/lite/micro/tools/make/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:20,\r\n                 from ./tensorflow/lite/micro/micro_allocator.h:21,\r\n                 from tensorflow/lite/micro/micro_allocator.cc:16:\r\ntensorflow/lite/micro/tools/make/downloads/flatbuffers/include/flatbuffers/base.h:392:22: error: null pointer dereference [-Werror=null-dereference]\r\n  392 |   return EndianScalar(*reinterpret_cast<const T *>(p));\r\n      |          ~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n./tensorflow/lite/micro/simple_memory_allocator.h:31:7: error: potential null pointer dereference [-Werror=null-dereference]\r\n   31 | class SimpleMemoryAllocator {\r\n      |       ^~~~~~~~~~~~~~~~~~~~~\r\n./tensorflow/lite/micro/simple_memory_allocator.h:31:7: error: potential null pointer dereference [-Werror=null-dereference]\r\n```\r\n\r\nRealistically, these errors are only a sampling of what we might see.\r\n\r\nBottom-line is that things *should* work out if we push and pop the gcc diagnostics (and that there isn't a short path to enabling `-Wnull-dereference` for the TFLM code.", "Ignore commit 059ef7f. The change from that commit was overwritten with 87b03ee"]}, {"number": 44970, "title": "Why tf.data.Dataset.map() still unknown TensorShape?", "body": "System information\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nOS Platform and Distribution (e.g., Linux Ubuntu 18.04):\r\nTensorFlow installed from (source or binary): binary\r\nTensorFlow version (use command below): tf-nightly-2.5.0\r\nPython version:\r\nBazel version (if compiling from source): 3.7.4\r\nGCC/Compiler version (if compiling from source): 7.3.0\r\nCUDA/cuDNN version: CUDA 11.0 / cuDNN 8.0.4\r\nGPU model and memory: RTX3090 24GB\r\nRAM: 32GB\r\n```\r\ndef parse(img_path, label, resolution, class_num):\r\n    label = tf.one_hot(label, depth=class_num)\r\n    image = tf.io.read_file(img_path)\r\n    image = tf.image.decode_jpeg(image)\r\n    image = tf.image.resize(image, [resolution, resolution])\r\n    image /= 255.\r\n\r\n    return image, label\r\n\r\n\r\ndef make_datasets(image, label, resolution, class_num, batch_size, mode):\r\n\r\n    dataset = tf.data.Dataset.from_tensor_slices((image, label))\r\n    if mode == 'train':\r\n        dataset = dataset.shuffle(buffer_size=len(label))\r\n        dataset = dataset.map(lambda x, y: tf.py_function(func=parse,\r\n                                                          inp=[x, y, resolution, class_num],\r\n                                                          Tout=[tf.float32, tf.float32]))\r\n        dataset = dataset.repeat()\r\n        dataset = dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\r\n\r\n...variables define...\r\n\r\ntrain_dataset = make_datasets(train_images, train_labels, 224, num_classes, batch_size, mode='train')\r\nval_dataset = make_datasets(val_images, val_labels, 224, num_classes, batch_size, mode='val')\r\n\r\ntrain_step = len(train_images) // batch_size\r\nval_step = len(val_images) // batch_size\r\n\r\nmodel = any_model()\r\nmodel.compile(optimizer=optimizers.Adam(lr),\r\n                         loss=losses.CategoricalCrossentropy(from_logits=True),\r\n                         metrics=['accuracy'])\r\n\r\nmodel.fit(train_dataset,\r\n                steps_per_epoch=train_step,\r\n                epochs=epochs,\r\n                validation_data=val_dataset,\r\n                validation_steps=val_step,\r\n                verbose=1)\r\n```\r\nWhen I run it.It excetion:\r\n```\r\nValueError: as_list() is not defined on an unknown TensorShape.\r\n```\r\nI know I can avoid this problem by using **image.set_shape([h, w, 3])**, but I don't want to do it.Beacause I need my code concise and using lambda function.If using **image.set_shape([h, w, 3])** I should write other function.\r\nSo I think it is bug.", "comments": ["Please share a very, very minimal code example that we could simply copy, paste, run (or a Colab)", "```\r\nimport os\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import losses, optimizers, applications\r\n\r\ndef read_data(path):\r\n    image_list = list()\r\n    label_list = list()\r\n    class_list = os.listdir(path)\r\n\r\n    for i, value in enumerate(class_list):\r\n        dirs = os.path.join(path, value)\r\n        for pic in os.listdir(dirs):\r\n            pic_full_path = os.path.join(dirs, pic)\r\n            image_list.append(pic_full_path)\r\n            label_list.append(i)\r\n\r\n    return image_list, label_list\r\n\r\n\r\ndef parse(img_path, label, resolution, class_num):\r\n    label = tf.one_hot(label, depth=class_num)\r\n    image = tf.io.read_file(img_path)\r\n    image = tf.image.decode_jpeg(image)\r\n    image = tf.image.resize(image, [resolution, resolution])\r\n    image /= 255.\r\n\r\n    return image, label\r\n\r\n\r\ndef make_datasets(image, label, resolution, class_num, batch_size, mode):\r\n    dataset = tf.data.Dataset.from_tensor_slices((image, label))\r\n    if mode == 'train':\r\n        dataset = dataset.shuffle(buffer_size=len(label))\r\n        dataset = dataset.map(lambda x, y: tf.py_function(func=parse,\r\n                                                          inp=[x, y, resolution, class_num],\r\n                                                          Tout=[tf.float32, tf.float32]))\r\n        dataset = dataset.repeat()\r\n        dataset = dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\r\n    else:\r\n        dataset = dataset.map(lambda x, y: tf.py_function(func=parse,\r\n                                                          inp=[x, y, resolution, class_num],\r\n                                                          Tout=[tf.float32, tf.float32]))\r\n        dataset = dataset.repeat().batch(batch_size).prefetch(batch_size)\r\n\r\n    return dataset\r\n\r\n\r\ndef main():\r\n    train_dir = r'D:\\Python_Code\\Dataset\\mini_ImageNet\\train'\r\n    val_dir = r'D:\\Python_Code\\Dataset\\mini_ImageNet\\validation'\r\n    epochs = 50\r\n    batch_size = 32\r\n    lr = 1e-4\r\n    num_classes = 100\r\n\r\n    train_images, train_labels = read_data(train_dir)\r\n    val_images, val_labels = read_data(val_dir)\r\n    train_dataset = make_datasets(train_images, train_labels, 224, num_classes, batch_size, mode='train')\r\n    val_dataset = make_datasets(val_images, val_labels, 224, num_classes, batch_size, mode='val')\r\n\r\n    train_step = len(train_images) // batch_size\r\n    val_step = len(val_images) // batch_size\r\n    \r\n    model = applications.ResNet50(weights=None, pooling='avg', classes=num_classes)\r\n    model.summary()\r\n    model.compile(optimizer=optimizers.Adam(lr),\r\n                  loss=losses.CategoricalCrossentropy(from_logits=True),\r\n                  metrics=['accuracy'])\r\n\r\n    model.fit(train_dataset,\r\n              steps_per_epoch=train_step,\r\n              epochs=epochs,\r\n              validation_data=val_dataset,\r\n              validation_steps=val_step,\r\n              verbose=1)\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n\r\n```\r\ntrain_dir  file tree is:\r\n\r\ntrain_dir\r\n\u251c\u2500\u2500 class 1: img 1\u3001img 2  ...            \r\n\u251c\u2500\u2500 class 2: img 1\u3001img 2  ...\r\n ...\r\n", "This will require time for us to run this code as It depend on your input on filesystem and It create overhead to handle the ticket.\n\nCan you reproduce this with dummy inputs or sharing few sample files.", "Was able to reproduce the issue with TF v2.3 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/d6965a146bf83195a651850a2c1f0af0/44970.ipynb). Thanks!", "@amahendrakar Thanks for completing the example.\r\n\r\n@Runist See https://github.com/tensorflow/tensorflow/issues/35108#issuecomment-569781266 \r\n\r\n/cc @jsimsa Probably we could add an info box somewhere in the doc\r\n\r\n", "I tried to simplify the behavior further. See the [gist](https://colab.research.google.com/gist/ymodak/bfb56267184a8731634312542f0950e5/git44970.ipynb) \r\nOn inspecting `train_dataset.element_spec` it has unknown shapes which fails in `model.fit`", "This is expected behavior and @bhack has provided a pointer to an answer that explains why it works this way and how to address this problem.\r\n\r\n@bhack I would be happy to review a PR for updating the `tf.py_function` documentation to call this out. Thanks.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44970\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44970\">No</a>\n", "> I would be happy to review a PR for updating the tf.py_function documentation to call this out. Thanks.\r\n\r\nSee if https://github.com/tensorflow/tensorflow/pull/44994 could be ok", "> @amahendrakar Thanks for completing the example.\r\n> \r\n> @Runist See [#35108 (comment)](https://github.com/tensorflow/tensorflow/issues/35108#issuecomment-569781266)\r\n> \r\n> /cc @jsimsa Probably we could add an info box somewhere in the doc\r\n\r\nI know define other function can work. But it doesn't my want.Because I say I just want to pass other args for map function.So I need to use lambda function.How can we use lambda functions to solve this problem without defining other functions?"]}, {"number": 44969, "title": "2.4.0rc1 not supporting RTX 3090? ", "body": "\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nDocker image `tensorflow/tensorflow:2.4.0rc1-gpu-jupyter`\r\n\r\n\r\n\r\n\r\n**Describe the current behavior**\r\nI checked the cuda version in the container which is cuda 11.1. I also have nvidia driver 455.38 installed on my host machine. I have a custom built of tf 2.3.0 working with this setup. But when I pull the docker image of `tensorflow/tensorflow:2.4.0rc1-gpu-jupyter` it's raising CUDNN errors like below: \r\n\r\n```\r\n2020-11-18 07:54:42.398846: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n2020-11-18 07:54:42.417594: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3693205000 Hz\r\n2020-11-18 07:54:43.206492: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2020-11-18 07:54:44.092906: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2020-11-18 07:54:44.097015: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2020-11-18 07:54:44.422337: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2020-11-18 07:54:44.429661: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n---------------------------------------------------------------------------\r\nUnknownError                              Traceback (most recent call last)\r\n<ipython-input-6-976a0a33b022> in <module>\r\n----> 1 m.predict(np.zeros((1, 256, 256, 3), dtype=np.float32))\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in predict(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\r\n   1627           for step in data_handler.steps():\r\n   1628             callbacks.on_predict_batch_begin(step)\r\n-> 1629             tmp_batch_outputs = self.predict_function(iterator)\r\n   1630             if data_handler.should_sync:\r\n   1631               context.async_wait()\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    826     tracing_count = self.experimental_get_tracing_count()\r\n    827     with trace.Trace(self._name) as tm:\r\n--> 828       result = self._call(*args, **kwds)\r\n    829       compiler = \"xla\" if self._experimental_compile else \"nonXla\"\r\n    830       new_tracing_count = self.experimental_get_tracing_count()\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    893       # If we did not create any variables the trace we have is good enough.\r\n    894       return self._concrete_stateful_fn._call_flat(\r\n--> 895           filtered_flat_args, self._concrete_stateful_fn.captured_inputs)  # pylint: disable=protected-access\r\n    896 \r\n    897     def fn_with_cond(inner_args, inner_kwds, inner_filtered_flat_args):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1917       # No tape is watching; skip to running the function.\r\n   1918       return self._build_call_outputs(self._inference_function.call(\r\n-> 1919           ctx, args, cancellation_manager=cancellation_manager))\r\n   1920     forward_backward = self._select_forward_and_backward_functions(\r\n   1921         args,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager)\r\n    558               inputs=args,\r\n    559               attrs=attrs,\r\n--> 560               ctx=ctx)\r\n    561         else:\r\n    562           outputs = execute.execute_with_cancellation(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     58     ctx.ensure_initialized()\r\n     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n---> 60                                         inputs, attrs, num_outputs)\r\n     61   except core._NotOkStatusException as e:\r\n     62     if name is not None:\r\n\r\nUnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[node efficientnetb0/stem_conv/Conv2D (defined at <ipython-input-6-976a0a33b022>:1) ]] [Op:__inference_predict_function_5650]\r\n\r\nFunction call stack:\r\npredict_function\r\n```\r\n\r\ncode to replicate:\r\n```\r\nfrom tensorflow.python.keras.applications.efficientnet import EfficientNetB0\r\nm = EfficientNetB0(weights=None, input_shape=(256, 256, 3))\r\nimport numpy as np\r\nm.predict(np.zeros((1, 256, 256, 3)))\r\n```\r\n\r\n\r\n**Describe the expected behavior**\r\nShould be able to run on RTX3090 without any issue. \r\n\r\n\r\n\r\n", "comments": ["@ysyyork \r\nThis issue has already been addressed [here](https://github.com/tensorflow/tensorflow/issues/44753#issuecomment-725254982), please verify and let us know.\r\nYou may also check: #44750, #43718", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44969\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44969\">No</a>\n"]}, {"number": 44968, "title": "TFLu: Move Ethos-U custom op out of AllOpsResolver", "body": "Addresses: https://github.com/tensorflow/tensorflow/issues/44628\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@mansnils  Can you please resolve conflicts? Thanks!", "Seems auto-merge is not happening but the changes are now committed so we can close this. Thank you for the PR."]}, {"number": 44967, "title": "Why is the first inference Extremely Slow?", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYES, as custom ops in the network.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No.\r\n- TensorFlow installed from (source or binary): Source, 2.3.1\r\n- TensorFlow version (use command below): 2.3.1\r\n- Python version: 3.5\r\n- Bazel version (if compiling from source): Bazel 3.1\r\n- GCC/Compiler version (if compiling from source): 7\r\n- CUDA/cuDNN version: 10.2\r\n- GPU model and memory: 2080 Ti\r\n\r\n**Describe the current behavior**\r\n\r\nSo I'm using Tensorflow C++ APIs to do inference using `CallableHandle` with `Session`.\r\nI used `RunMetadata` to dump the full trace for the first inference in C++. Part of the timeline showed in Chrome://tracing\r\n\r\nI ran this model freezed using TF 1.15, and it ran in TF 2.3 without obvious issues.\r\n\r\n![Screenshot from 2020-11-17 22-03-05](https://user-images.githubusercontent.com/2466188/99491865-65973500-2921-11eb-8f8e-533c12b1fa7d.png)\r\n\r\nAs you could see, every convolution layer here took EXTREMELY long to do, and it's showing they are also running on GPU.\r\nAfter couple of inferences, the inference time for the model is super short. So this makes me wonder what's happening in the first inference.\r\n\r\n**Describe the expected behavior**\r\n\r\nI know the first inference is usually slower than following ones, but this is extremely slow compared to how long it usually take.\r\nThe first inference here took 200s, while it was expected to run around 0.1s.\r\n\r\nAs one more reference data point, the same model running with TF 1.15 inference stack, the first inference would take around 10s to finish. TF 2.3 seems to be super slow for the same model.\r\n\r\n\r\n**Other info / logs**\r\n\r\nRunMetadata dumped tracing json (had to change to txt suffix):\r\nTruncated, only showing part of the entire timeline.\r\n[github_2.3_profile.txt](https://github.com/tensorflow/tensorflow/files/5558191/github_2.3_profile.txt)\r\n", "comments": ["@golden0080 Can you please attach a minimal code snippet to reproduce the behavior? Thanks!", "@ymodak Seems like this is exactly the issue in https://github.com/tensorflow/tensorflow/issues/40036 and I was able to resolve it after installing `ptxas`."]}, {"number": 44966, "title": "tensorflow.python.framework.errors_impl.UnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n    centos7.4\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):2.2.0\r\n- Python version:3.6.12\r\n- CUDA/cuDNN version:10.1/7.6.5\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@LianShuaiLong,\r\nIn order to reproduce the issue reported here, could you please provide the complete code and the dataset you are using.\r\n\r\nAlso, please go through issue [#24828](https://github.com/tensorflow/tensorflow/issues/24828) with a similar error and let us know if it helps. Thanks!\r\n\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44966\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44966\">No</a>\n"]}, {"number": 44965, "title": "How to run TFLite on specific CPUs?", "body": "@tensorflow/micro\r\n\r\nDear Authors:\r\n     I'm running TFLite on RK3399 with BIG.LITTLE architecture, which contains 2 ARM Cortex A72 cores (BIG cores) and 4 ARM Cortex-A53 cores (LITTLE cores),  and the tflite is installed through source.  I use the example code of [benchmark](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark) and the tflite model is converted from keras model. \r\n     When I run tflite with 2 threads (through the interpreter.SetNumThreads interface), it is expected that it should run on the 2 BIG cores. However, it seems that in some cases it runs on 2 of the LITTLE cores, which will largely degrade performance. So is there any way to run tflite on specific CPUs?", "comments": ["Hi,\r\n\r\nThis issue (pinning a user process to a desired core in BIG.LITTLE architecture) is not particular to TF Lite. In general, the operating system will decide whether to run on the lower energy LITTLE cores, or the higher performance BIG cores. There is no general API for a process to move itself (or fork another process) to a particular core that the OS will respect for the life of the process. This is because the OS must be able to assert control for thermal management, battery management, etc. For benchmarking purposes or other testing activities, one call always use the `taskset` Linux utility to run a given program on a particular core. In the context of the RK3399 board, from what I understand, it appears that your application is not the standard use case for TF Lite, that being \"run ML inference inside a Android/iOS application on a mobile SoC.\" For your case, maybe it is good enough to start some kind of \"server\" process with `taskset` and then have user applications talk to that process in order to make sure inference happens on a BIG core. ", "Answer from one of my group members:\r\n you can assign specific CPUs. You do so by either taskset or CPU shielding. My preference is CPU shielding. Here's how:\r\n\r\n1- install cpuset if not there\r\n2- shield the CPUs that you want to assign (I'm assuming CPUs number 0,1,2,3 (LITTLE cluster)\r\n3- run the tflite model using the benchmark model in the CPUset you have created, with setting num_threads option. Here I set it to 4\r\n\r\n`sudo apt install cpuset\r\nsudo cset shield --cpu 0,1,2,3 --kthread on --threads\r\ncset proc -s user --exec ./benchmark_model -- --graph=/home/mohamed/Downloads/Export/H-128_L-1_A-2_I-512.tflite  --max_profiling_buffer_entries=8092 --num_threads=4 --num_runs=100  --profiling_output_csv_file=output.csv --enable_op_profiling=true --input_layer=input_mask:0,input_ids:0,segment_ids:0 --input_layer_shape=1,64:1,64:1,64 --warmup_runs=1 --min_secs=0.001 --warmup_min_secs=0.001`\r\n    \r\nAnother example if you want to run on only 2 LITTLE cores, say number 0,1( num_threads is set to 2)\r\n`cpu shield --reset\r\nsudo cset shield --cpu 0,1 --kthread on --threads\r\ncset proc -s user --exec ./benchmark_model -- --graph=/home/mohamed/Downloads/Export/H-128_L-1_A-2_I-512.tflite  --max_profiling_buffer_entries=8092 --num_threads=2 --num_runs=100  --profiling_output_csv_file=output.csv --enable_op_profiling=true --input_layer=input_mask:0,input_ids:0,segment_ids:0 --input_layer_shape=1,64:1,64:1,64 --warmup_runs=1 --min_secs=0.001 --warmup_min_secs=0.001`", "Another example: form [https://github.com/adityagupta1089/ComputeLibrary](url)\r\n\r\n![image](https://user-images.githubusercontent.com/70759685/128282036-bc63999b-c6e5-4efd-a028-d4f2de9d3238.png)\r\n"]}, {"number": 44964, "title": "What does TODO(b/XXX) mean?", "body": "There are a lot of TODO comments in the code that include a reference to.... something.\r\n\r\nExamples:\r\n- https://github.com/tensorflow/tensorflow/blob/3061483034ce0196ca7ec0576668c41e5f4424c9/tensorflow/compiler/tests/BUILD#L81\r\n- https://github.com/tensorflow/tensorflow/blob/0650101e05ef1ad56bfaadcd63ab775b25ecdd16/tensorflow/python/keras/saving/saved_model/metric_serialization.py#L44\r\n- https://github.com/tensorflow/tensorflow/blob/1ba764b368daa5a6f34b556f2a44e2e222a18855/tensorflow/python/ops/risc/risc_grad.py#L27\r\n\r\nWhat do these numbers mean?\r\n", "comments": ["https://stackoverflow.com/questions/49810177/what-does-b-xxxx-such-like-b-36865746-mean-in-tensorflow-code-comments", "Got it, thank you!"]}, {"number": 44963, "title": "AttributeError: module 'tensorflow.keras.layers.experimental.preprocessing' has no attribute 'StringLookup'?", "body": "cannot import StringLookup?\r\n\r\nEnvironment:\r\nTF 2.1.0\r\nkeras 2.3.1\r\nWindows 10\r\n\r\nIs there any suggestion please?\r\n", "comments": ["Try TF 2.3.1", "> Try TF 2.3.1\r\n\r\nwhat about keras version for TF 2.3.1?", "Please see https://github.com/keras-team/keras/releases/tag/2.4.0", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44963\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44963\">No</a>\n", "fuck 2.3.1", "my cpu not suppot that!!!", "# Mapping characters to integers\r\ndef label_to_tensor(st):\r\n    f_list = list(characters)\r\n    f_list.sort()\r\n    st_list = []\r\n    for s in st:\r\n        s_index = f_list.index(s)\r\n        st_list.append(s_index)\r\n    return tf.convert_to_tensor(st_list, dtype=tf.float32)", "for \r\nhttps://keras.io/examples/vision/captcha_ocr/", "I want to work at 1.x. (e.g., TF 1.4). How can I do that?", "It is available in tensorflow==2.6.0 "]}, {"number": 44962, "title": "Add missing closing quotes for example in docstring", "body": "Just a docstring fix.", "comments": []}, {"number": 44961, "title": "Quantization op for Reshape is not supported in keras", "body": "Tensorflow version 2.2.0\r\nFor the experiment, I build a simple model and try to quantized aware training with reshape function.\r\n\r\n```\r\ndef test_model(final_depth = 24):\r\n    inputs = layers.Input(shape=(None, None,24))\r\n    input_shape = tf.shape(inputs)\r\n    rs_1 = tf.reshape(inputs, [input_shape[0], input_shape[1] * input_shape[2], input_shape[3], 1])\r\n    conv1 = layers.ReLU(negative_slope = 0.2)(layers.Conv2D(1, (1,3), activation = None, padding = 'same', kernel_initializer = 'glorot_uniform')(rs_1))\r\n    out = tf.reshape(conv1, [input_shape[0], input_shape[1], input_shape[2], input_shape[3]])\r\n    model = tf.keras.Model(inputs = inputs, outputs = out)\r\n    return model\r\n```\r\n\r\nThen I got an error like this:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/root/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow_model_optimization/python/core/quantization/keras/quantize.py\", line 383, in quantize_apply\r\n    model_copy = _clone_model_with_weights(model)\r\n  File \"/root/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow_model_optimization/python/core/quantization/keras/quantize.py\", line 332, in _clone_model_with_weights\r\n    cloned_model = keras.models.clone_model(model_to_clone)\r\n  File \"/root/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/keras/models.py\", line 426, in clone_model\r\n    return _clone_functional_model(\r\n  File \"/root/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/keras/models.py\", line 199, in _clone_functional_model\r\n    network.reconstruct_from_config(model_config,\r\n  File \"/root/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/keras/engine/network.py\", line 2029, in reconstruct_from_config\r\n    process_node(layer, node_data)\r\n  File \"/root/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/keras/engine/network.py\", line 1977, in process_node\r\n    output_tensors = layer(input_tensors, **kwargs)\r\n  File \"/root/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 897, in __call__\r\n    self._maybe_build(inputs)\r\n  File \"/root/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 2416, in _maybe_build\r\n    self.build(input_shapes)  # pylint:disable=not-callable\r\n  File \"/root/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/keras/layers/wrappers.py\", line 58, in build\r\n    self.layer.build(input_shape)\r\n  File \"/root/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/keras/layers/convolutional.py\", line 153, in build\r\n    input_channel = self._get_input_channel(input_shape)\r\n  File \"/root/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/keras/layers/convolutional.py\", line 293, in _get_input_channel\r\n    raise ValueError('The channel dimension of the inputs '\r\nValueError: The channel dimension of the inputs should be defined. Found `None`.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"./tools/train_oppo_template_yuv_24pack.py\", line 227, in <module>\r\n    main()\r\n  File \"./tools/train_oppo_template_yuv_24pack.py\", line 114, in main\r\n    model = tfmot.quantization.keras.quantize_apply(annotated_model)\r\n  File \"/root/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow_model_optimization/python/core/quantization/keras/quantize.py\", line 385, in quantize_apply\r\n    raise ValueError(\r\nValueError: Unable to clone model. This generally happens if you used custom Keras layers or objects in your model. Please specify them via `quantize_scope` for your calls to `quantize_model` and `quantize_apply`.\r\n```\r\n\r\n\r\nIt looks like the reshape operation is not supported. So I try to skip the quantized reshape op just as the other unsupported operations. Stil got the same error\r\n\r\n```\r\ndef annotate(layer):\r\n    if layer._name.startswith('tf_op_layer_Reshape'):\r\n      return layer\r\n    # quantize everything else\r\n    return tfmot.quantization.keras.quantize_annotate_layer(layer)\r\n\r\nannotated_model = tf.keras.models.clone_model(model, clone_function=annotate)\r\nmodel = tfmot.quantization.keras.quantize_apply(annotated_model)\r\n```\r\n\r\n**I did another experiment with the simple model for the reshape function. I directly connect the reshape function to the output (did not skip the quantized reshape op)**\r\n```\r\ndef test_model(final_depth = 24):\r\n    inputs = layers.Input(shape=(None, None,24))\r\n    input_shape = tf.shape(inputs)\r\n   conv1 = layers.ReLU(negative_slope = 0.2)(layers.Conv2D(final_depth, 3, activation = None, padding = 'same', kernel_initializer = 'glorot_uniform')(inputs))\r\n    out = tf.reshape(conv1, [input_shape[0], input_shape[1], input_shape[2], input_shape[3]])\r\n    model = tf.keras.Model(inputs = inputs, outputs = out)\r\n    return model\r\n```\r\nGot the error like this:\r\n```\r\nTraceback (most recent call last):\r\n  File \"./tools/train_oppo_template_yuv_24pack.py\", line 227, in <module>\r\n    main()\r\n  File \"./tools/train_oppo_template_yuv_24pack.py\", line 114, in main\r\n    model = tfmot.quantization.keras.quantize_apply(annotated_model)\r\n  File \"/root/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow_model_optimization/python/core/quantization/keras/quantize.py\", line 420, in quantize_apply\r\n    return keras.models.clone_model(\r\n  File \"/root/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/keras/models.py\", line 426, in clone_model\r\n    return _clone_functional_model(\r\n  File \"/root/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/keras/models.py\", line 195, in _clone_functional_model\r\n    model_config, created_layers = _clone_layers_and_model_config(\r\n  File \"/root/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/keras/models.py\", line 246, in _clone_layers_and_model_config\r\n    config = network.get_network_config(model, serialize_layer_fn=_copy_layer)\r\n  File \"/root/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/keras/engine/network.py\", line 2119, in get_network_config\r\n    layer_config = serialize_layer_fn(layer)\r\n  File \"/root/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/keras/models.py\", line 243, in _copy_layer\r\n    created_layers[layer.name] = layer_fn(layer)\r\n  File \"/root/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow_model_optimization/python/core/quantization/keras/quantize.py\", line 370, in _quantize\r\n    raise RuntimeError(\r\nRuntimeError: Layer tf_op_layer_Reshape/shape:<class 'tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer'> is not supported. You can quantize this layer by passing a `tfmot.quantization.keras.QuantizeConfig` instance to the `quantize_annotate_layer` API.\r\n```\r\n\r\nIf I do skip the quantized reshape op like above, I can run the model. But I found out that the channel dimension after reshape becomes \"None\", Therefore I can't connect reshape layer to any other layer e.g. conv2d\r\n```\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to                     \r\n==================================================================================================\r\ninput_1 (InputLayer)            [(None, None, None,  0                                            \r\n__________________________________________________________________________________________________\r\nquantize_layer (QuantizeLayer)  (None, None, None, 2 3           input_1[0][0]                    \r\n__________________________________________________________________________________________________\r\ntf_op_layer_Shape (TensorFlowOp (4,)                 0           quantize_layer[1][0]             \r\n__________________________________________________________________________________________________\r\nquant_conv2d (QuantizeWrapper)  (None, None, None, 2 5259        quantize_layer[1][0]             \r\n__________________________________________________________________________________________________\r\ntf_op_layer_strided_slice (Tens ()                   0           tf_op_layer_Shape[1][0]          \r\n__________________________________________________________________________________________________\r\ntf_op_layer_strided_slice_1 (Te ()                   0           tf_op_layer_Shape[1][0]          \r\n__________________________________________________________________________________________________\r\ntf_op_layer_strided_slice_2 (Te ()                   0           tf_op_layer_Shape[1][0]          \r\n__________________________________________________________________________________________________\r\ntf_op_layer_strided_slice_3 (Te ()                   0           tf_op_layer_Shape[1][0]          \r\n__________________________________________________________________________________________________\r\nquant_re_lu (QuantizeWrapper)   (None, None, None, 2 3           quant_conv2d[0][0]               \r\n__________________________________________________________________________________________________\r\ntf_op_layer_Reshape/shape (Tens (4,)                 0           tf_op_layer_strided_slice[1][0]  \r\n                                                                 tf_op_layer_strided_slice_1[1][0]\r\n                                                                 tf_op_layer_strided_slice_2[1][0]\r\n                                                                 tf_op_layer_strided_slice_3[1][0]\r\n__________________________________________________________________________________________________\r\ntf_op_layer_Reshape (TensorFlow (None, None, None, N 0           quant_re_lu[0][0]                \r\n                                                                 tf_op_layer_Reshape/shape[1][0]  \r\n==================================================================================================\r\nTotal params: 5,265\r\nTrainable params: 5,208\r\nNon-trainable params: 57\r\n__________________________________________________________________________________________________\r\n```\r\nI think the problem is that quantized reshape is unsupported. Even if I skip quantizing the reshape function, the channel dimension of output become 'None' so I can't connect it to the other conv function\r\n", "comments": ["My tensorflow version is 2.2.0", "@deandx \r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]\r\n\r\nPlease paste the error message (using makrdown formatting around it) instead of screenshotting. Screenshots are not searchable so they don't help in looking for the issue and also don't help other people having the same error from finding about the issue.", "@Saduf2019 \r\nI update the issue\r\n", "@deandx \r\nI ran the code on nightly and do not face any errors as reported, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/f69fc005750d5fffc769eeac40f63d14/untitled463.ipynb).", "@Saduf2019 \r\n    Thanks for your reply. You need to install the tfmot for  tensorflow quantization. \r\n    You can use the command:\r\n**pip install tensorflow-model-optimization**\r\n\r\nyou can run my code after install the tfmot\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nimport tensorflow_model_optimization as tfmot\r\n\r\ndef main():\r\n\r\n  def test_model(final_depth = 24):\r\n    inputs = layers.Input(shape=(None, None,24))\r\n    input_shape = tf.shape(inputs)\r\n    rs_1 = tf.reshape(inputs, [input_shape[0], input_shape[1] * input_shape[2], input_shape[3], 1])\r\n    conv1 = layers.ReLU(negative_slope = 0.2)(layers.Conv2D(1, (1,3), activation = None, padding = 'same', kernel_initializer = 'glorot_uniform')(rs_1))\r\n    out = tf.reshape(conv1, [input_shape[0], input_shape[1], input_shape[2], input_shape[3]])\r\n    model = tf.keras.Model(inputs = inputs, outputs = out)\r\n    return model\r\n\r\n  model = test_model()\r\n\r\n  def annotate(layer):\r\n    if layer._name.startswith('tf_op_layer_Reshape'):\r\n      return layer\r\n    # quantize everything else\r\n    return tfmot.quantization.keras.quantize_annotate_layer(layer)\r\n \r\n  annotated_model = tf.keras.models.clone_model(model, clone_function=annotate)\r\n  model = tfmot.quantization.keras.quantize_apply(annotated_model)\r\n\r\n  model.summary()\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\nThen I got the error as reported", "@deandx \r\nI ran the code shared and face a different error, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/aa01edd5b375de6ac8c07e2c3db43aed/untitled463.ipynb).", "@Saduf2019 1\r\n  The shape layer also not supported.  You can update this function to avoid using the quantized shape function. Thanks!\r\n```\r\ndef annotate(layer):\r\n    if layer._name.startswith('tf_op_layer_Reshape'):\r\n      return layer\r\n    elif layer._name.startswith('tf_op_layer_Shape'):\r\n        return layer\r\n    elif layer._name.startswith('concatenate'):\r\n      return layer\r\n    elif layer._name.startswith('tf_op_layer_strided_slice'):\r\n      return layer\r\n    # quantize everything else\r\n    return tfmot.quantization.keras.quantize_annotate_layer(layer)\r\n```", "While trying to reproduce your issue in Tf Nightly 2.6 facing some different error, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/dda816a5f47b965ee753494e83d94629/44961.ipynb), Thanks!", "@deandx, Sorry for late response. We are checking to see if you still need help on this issue.\r\n\r\nMany bugs have been fixed in the recent versions of TF. Could you please try with TF2.7 and let us know if the issue still persists? \r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44961\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44961\">No</a>\n"]}, {"number": 44960, "title": "Avoid call to cuCtxSynchronize() when moving from graph execution to eager execution", "body": "Hi,\r\n\r\nWhile running TF-2.x on nvidia gpu machine, tensorflow calls `cuCtxSynchronize()` API whenever programs enter eager execution. I.e. when program exits `@tf.function` code block. E.g. please take a look at code block [here](https://github.com/tensorflow/tensorflow/blob/642db2faf55e3ca7acd06ea236e9d47f63190718/tensorflow/stream_executor/cuda/cuda_gpu_executor.cc#L546-L548).\r\n\r\nThis essentially adds a device wide synchronization barrier and blocks until all gpu execution is complete even if some of the work is not related to TF library. I would like to use GPU to do some other work in parallel while TF is executing. Is there any way to achieve this without having to block TF due to this API call? If not, does it make sense to avoid device wide synchronization (which is bit hard restriction) and only synchronize on resources (such as cudaStream, etc.) created by TF? \r\n\r\n", "comments": ["@pranavladkat I'm also curious if such behavior affects C++ inference. Do you happen to see slower inference in C++ or anywhere?", "@pranavladkat \r\n\r\nPlease, fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\nRequest you to share simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "@golden0080 it does affect the overall performance since tf waits for cuda kernels not issued by it. \r\n\r\n@ravikyram sure, I'll create a small example which reproduces this issue and update it in a day or so. \r\n\r\nThanks!\r\n", "I also tried to read some of the `gpu_executor` code, they now works with CUcontext and in 2.3.1's `GpuExecutor::Init`, every executor would create separate CUcontext.\r\nBut I'm still concerning about the synchronization in CUDA, since my upgrade from 1.15 to 2.3.1 comes with various performance issues.", "@pranavladkat \r\n\r\nAny update on the issue please?\r\nThanks!", "@ravikyram sorry for the delay in response as I was away for past week. I spent some time to come up with a small example which reproduces this, however it seems it needs steep efforts to create such example since it requires custom cuda ops which simulates kernel execution in parallel to TF code.\r\n\r\nWhat I'm looking for is to remove a call to `cuCtxSynchronize` API by tensorflow. Since this adds entire device wide synchronization which also impacts non-tf related code. E.g. please take a look at the snapshot of the nvidia profiler below. Here, tf is waiting for all kernels to be finished before continuing to next batch. The kernel being execute is running in parallel to tf code and is performing unrelated work. \r\n\r\n![image](https://user-images.githubusercontent.com/9426781/101099989-98286b00-357a-11eb-9a8f-510da1e13ace.png)\r\n\r\nPlease let me know if you would still need an example code to address this.\r\n\r\nThanks", "Are you able to build TensorFlow locally?  If yes, can you check (by commenting it out) if [this](https://github.com/tensorflow/tensorflow/blob/50ea0bcb081f02d661964a0982f9b0024c55f3e0/tensorflow/core/common_runtime/executor.cc#L1306) is the source of the `cuCtxSynchronize` (this should be a `cudaStreamSynchronize` now).\r\n\r\nWe've been thinking of removing that sync point, but this is a tricky change and will take time.", "Hi Sanjoy,\r\n\r\nThanks for the response! I built tensorflow from source and commented out the lines from the code that you mentioned and I can confirm that the `cuCtxSynchronize` gets triggered from the same lines. After commenting out portion of the code, I do see expected behavior (overlapping cuda kernels) which I would like to have. E.g. please see the nvprof output of same section of the code below.\r\n\r\nIs the `cuCtxSynchronize` has been replaced by `cudaStreamSynchronize` already in latest code or the change is pending so far? If it's not merged yet, do you have any estimates on how long will that could take?\r\n\r\nThanks,\r\nPranav\r\n\r\n![tf_update1](https://user-images.githubusercontent.com/9426781/103822278-a3146680-5024-11eb-82c0-0f3598bcbb3a.png)\r\n ", "> Is the `cuCtxSynchronize` has been replaced by `cudaStreamSynchronize` already in latest code\r\n\r\nYes.", "That's Great! Then this resolves the issue which I raised. \r\n\r\nThanks,\r\nPranav"]}, {"number": 44959, "title": "GeForce 3090 incompatibility with Nightly", "body": "**System information**\r\n- Using a stock example script provided in TensorFlow\r\n- Linux Ubuntu 18.04\r\n- TensorFlow installed from Nightly\r\n- TensorFlow version v1.12.1-45908-g9af48cb079\r\n- Python version 3.8\r\n- CUDA 11.1 cuDNN 8.0.5.39\r\n- GeForce RTX 3090 with 24265MiB\r\n\r\n**Describe the current behavior**\r\nTraining does not run on the GPU, shows \"warning\":\r\n\r\n`Your CUDA software stack is old. We fallback to the NVIDIA driver for some compilation. Update your CUDA version to get the best performance. The ptxas error was: ptxas fatal   : Value 'sm_86' is not defined for option 'gpu-name'`\r\n\r\n**Describe the expected behavior**\r\nTraining should run on the GPU\r\n\r\n**Standalone code to reproduce the issue**\r\nInstall GeForce 3090 then install nightly GPU:\r\n\r\n```\r\nconda create -n tf-n-gpu python=3.8\r\nconda activate tf-n-gpu\r\npip install tf-nightly-gpu\r\npip install matplotlib\r\npip install IPython\r\n```\r\n\r\nRun:\r\nhttps://www.tensorflow.org/tutorials/generative/pix2pix\r\n\r\n", "comments": ["We don't officially support conda here.", "I thought that was the recommend method of installation.\r\n\r\nWhat installation method is supported then?", "https://www.tensorflow.org/install/pip", "I'm having the same issue.\r\n**System Info**\r\n- RTX 3080\r\n- Linux Ubuntu 20.04\r\n- tensorflow 2.4.0rc1-gpu (Docker Image)", "@Harsh188,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!", "Upgrading to tensorflow==2.4.0rc2 fixed this issue for me.", "@3DTOPO,\r\nAny updates regarding this? Please feel free to close the issue if resolved. Thanks!", "Sorry I haven't had time to go back and check. Looks like rc2 may address it so I will try that soon and report back.", "I have the same problem. \r\nRTX 3090\r\ntensorflow-gpu 2.4.0rc3\r\ncuda 11.1.105 (tried 11.0, same error)\r\ncudnn 8.0.5.39\r\nIt showed `Your CUDA software stack is old. We fallback to the NVIDIA driver for some compilation. Update your CUDA version to get the best performance. The ptxas error was: ptxas fatal   : Value 'sm_86' is not defined for option 'gpu-name'` for a few hundred times and started training. The training seemed fine though. ", "I followed the directions in this article: [Install TensorFlow & PyTorch for the RTX 3090, 3080, 3070](https://lambdalabs.com/blog/install-tensorflow-and-pytorch-on-rtx-30-series/). So far it works pretty well and allows me to utilize my 3080 with TF 2.3. ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "This is still a problem", "I have the same problem.\r\nRTX 3080\r\nUbuntu 18.04\r\ntensorflow-gpu 2.4.0(official release)\r\ncuda 11.0\r\ncudnn 8.0.4.30\r\nHave use the same packages mention by tensorflow [link](https://www.tensorflow.org/install/gpu#ubuntu_1804_cuda_110)\r\nIt showed Your CUDA software stack is old. We fallback to the NVIDIA driver for some compilation. Update your CUDA version to get the best performance. The ptxas error was: ptxas fatal : Value 'sm_86' is not defined for option 'gpu-name' for a few hundred times and started training. The training seemed fine though.", "This is still an issue for me.\r\nRTX 3080\r\nUbuntu 20.04\r\nNVIDIA driver 455.38\r\ntensorflow-gpu 2.4.0\r\nTried with both cuda 11.0 and 11.1\r\ncudnn 8.0.4.30", "Same here,\r\nGTX 3090 \r\nTF 2.4.0\r\nCUDA 11.0\r\ncudnn 8.0.4\r\n", "Yeah also the same here.\r\nGTX 3090\r\ntensorflow/tensorflow:2.4.0-gpu docker image", "> Yeah also the same here.\r\n> GTX 3090\r\n> tensorflow/tensorflow:2.4.0-gpu docker image\r\n\r\nIt works with the latest nightly-gpu docker image (https://hub.docker.com/layers/tensorflow/tensorflow/nightly-gpu/images/sha256-f9c8333811c5426be605352f307d665a089343cd699e0b49da128dbde18008ad?context=explore) though.", "I got the same error many times while running the notebook. But finally the training is running on the gpu:\r\n\r\nI additional put this code at the beginning of the notebook:\r\n\r\n```\r\nimport tensorflow as tf\r\ngpu = tf.config.list_physical_devices('GPU')\r\ntf.config.experimental.set_memory_growth(gpu[0], True) #limits gpu memory\r\n\r\n```\r\nsee: https://github.com/tensorflow/tensorflow/issues/45635#event-4131192988\r\n\r\nGPU: RTX 3060 TI\r\nUbuntu-Server-20.04\r\nDriver Version: 455.45.01\r\ncuda_11.0.3_450.51.06\r\ncudnn-11.0-linux-x64-v8.0.2.39\r\nTensorflow: 2.4.0\r\n\r\n\r\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44959\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44959\">No</a>\n", "I am having the same issue and non of the above solutions worked for me ", "@drahmad89,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!"]}, {"number": 44958, "title": "[INTEL MKL] Added cpu docker file with python36.", "body": "", "comments": ["@penpornk can you please review this. Out public PR testing is running with python3.5 which is causing many failures would like to move to py36. Thank you,", "@rsketine Sorry for the delay! Since this is more about the generic TensorFlow build, I think @av8ramit is a better reviewer.", "@av8ramit  can you please review this so that our public CI can start using python3.6."]}, {"number": 44957, "title": "TFLu - refactor apollo3 micro speech", "body": "pertains to #44737\r\n\r\ntesting status (manually verified by inspection):\r\nboard | micro_speech\r\n------|-------------\r\nsparkfun_edge | \u2705\r\napollo3evb | \ud83e\udd14", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@oclyke  Can you please resolve conflicts? Thanks!\r\n", "@oclyke Any update on this PR? Please. Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!\r\n"]}, {"number": 44956, "title": "[Cherrypick:r2.4]Disables //tensorflow/python/kernel_tests/boosted_trees:[training_ops_test,prediction_ops_test,resource_ops_test] on Mac OS because of segfaults", "body": null, "comments": []}, {"number": 44955, "title": "TFLu - refactor apollo3 magic wand", "body": "pertaining to #44737\r\n\r\ntesting status (manually verified by inspection):\r\nboard | magic_wand\r\n------|-----------\r\nsparkfun_edge | \u2705\r\napollo3evb | \ud83e\udd14", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@oclyke  Can you please resolve conflicts? Thanks!\r\n", "@oclyke Any update on this PR? Please. Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!\r\n"]}, {"number": 44954, "title": "TFLu - refactor apollo3 person detection", "body": "pertaining to #44737\r\n\r\ntesting status (manually verified by inspection):\r\nboard | person_detection | person_detection_experimental\r\n-------| -----------------|------------------------------\r\nsparkfun_edge | \u2705 | \ud83e\udd14\r\napollo3evb | \ud83e\udd14 | \ud83e\udd14", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@oclyke Can you please resolve conflicts? Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@oclyke Any update on this PR? Please. Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 44953, "title": "How does classifier predict a class?", "body": "I want to implement the model by myself and thus have to know how does it classify the data. I build a model for 12-class classifier and it predicts fine. But the last conv layer just outputs 12 floating point value and I don't know how it suddenly predicts the right class. Can someone explain for me? Like is it depend on some threshold or it chooses the max value or something? Thanks!\r\n\r\nPS. I've searched in STACKOVERFLOW before but couldn't get any information. I've tried to choose the index with max value but it seemed that model just predicts randomly. So I guess it doesn't work like that. \r\n\r\n**The model**\r\n\r\n> first_conv (Conv2D)          (None, 63, 1, 24)         984       \r\n> _________________________________________________________________\r\n> first_act (Activation)       (None, 63, 1, 24)         0         \r\n> _________________________________________________________________\r\n> dw0 (DepthwiseConv2D)        (None, 61, 1, 24)         96        \r\n> _________________________________________________________________\r\n> pw0 (Conv2D)                 (None, 61, 1, 24)         600       \r\n> _________________________________________________________________\r\n> act0 (Activation)            (None, 61, 1, 24)         0         \r\n> _________________________________________________________________\r\n> dw1 (DepthwiseConv2D)        (None, 59, 1, 24)         96        \r\n> _________________________________________________________________\r\n> pw1 (Conv2D)                 (None, 59, 1, 24)         600       \r\n> _________________________________________________________________\r\n> act1 (Activation)            (None, 59, 1, 24)         0         \r\n> _________________________________________________________________\r\n> dw2 (DepthwiseConv2D)        (None, 57, 1, 24)         96        \r\n> _________________________________________________________________\r\n> pw2 (Conv2D)                 (None, 57, 1, 24)         600       \r\n> _________________________________________________________________\r\n> act2 (Activation)            (None, 57, 1, 24)         0         \r\n> _________________________________________________________________\r\n> dw3 (DepthwiseConv2D)        (None, 55, 1, 24)         96        \r\n> _________________________________________________________________\r\n> pw3 (Conv2D)                 (None, 55, 1, 24)         600       \r\n> _________________________________________________________________\r\n> act3 (Activation)            (None, 55, 1, 24)         0         \r\n> _________________________________________________________________\r\n> pool (AveragePooling2D)      (None, 1, 1, 24)          0         \r\n> _________________________________________________________________\r\n> last_conv (Conv2D)           (None, 1, 1, 12)          300       \r\n> _________________________________________________________________\r\n> tf_op_layer_reshape_3 (Tenso [(None, 12)]              0         ", "comments": [">  I've searched in STACKOVERFLOW before but couldn't get any information. \r\n\r\nThis is a support request so please open a new StackOverflow question.", "I will post on stackoverflow at the same time. Thanks for suggestion."]}, {"number": 44952, "title": "Issue with tf.math.conj function", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0 (also tried on 2.3.1)\r\n- Python version: 3.7.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 10.1 / 7.6.5\r\n- GPU model and memory: RTX2060 8GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nI have been having problems with the tensorflow function tensorflow.math.conj, which is supposed to return the complex conjugate (1+1j -> 1-1j) of a tensor. The error that occurs is the following:\r\n\r\ntensorflow/stream_executor/cuda/cuda_driver.cc:939] could not synchronize on CUDA context: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure :: 0x00007FFF94A28E85 tensorflow::CurrentStackTrace\r\n\r\nHowever, it seems that tf.math.real and tf.math.imag work as expected.\r\n\r\n**Describe the expected behavior**\r\nI expect the function to output the the complex conjugate (1+1j -> 1-1j).\r\n\r\n**Standalone code to reproduce the issue**\r\n`import tensorflow as tf\r\na = tf.constant(1+1.j, dtype=tf.complex64)\r\nb = tf.math.conj(a)'\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\nI am able to perform every other tensorflow operation that I've tried, it's really only this issue that is giving me problems...\r\n", "comments": ["I can't reproduce this on Colab with TF 2.3.1", "hm this must be an issue with the way I installed TF on windows then...", "@azhan137 \r\nI have tried in colab with TF 2.3, nightly version(`2.5.0-dev20201117 `) and i am not seeing any issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/403a23a7985925249f82c5f63437e4ed/untitled522.ipynb).Please, verify once and close the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44952\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44952\">No</a>\n"]}, {"number": 44951, "title": "Disable failing macos tests", "body": "", "comments": ["CP is not needed"]}, {"number": 44949, "title": "Fix small typos", "body": "Fix compatiblity typo in GPU package note and some other small typos.", "comments": []}, {"number": 44948, "title": "Can't load GRUCell weights from TensorFlow 1 to TensorFlow 2", "body": "# Issue \ud83d\udcbb \r\n\r\n## System information\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): `True`\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Windows 10`\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: `False`\r\n- TensorFlow installed from (source or binary): `pip install tensorflow`\r\n- TensorFlow version (use command below): `v2.3.0-rc2-23-gb36436b087 2.3.0`\r\n- Python version: `3.8.3`\r\n- Bazel version (if compiling from source): `None`\r\n- GCC/Compiler version (if compiling from source): `None`\r\n- CUDA/cuDNN version: `None`\r\n- GPU model and memory: `None`\r\n\r\n## Current Behavior\r\n\r\nI migrated a code written with **tensorflow 1** to **tensorflow 2**. In addition I would like to convert the checkpoints of my **tensorflow 1** model too. However, the `GRUCell` seems really different from **tensorflow 1** to **tensorflow 2** implementation, and can't load the saved weights from a `compat.v1.nn.rnn_cell.GRUCell` to a `keras.layers.GRUCell`.\r\nThe error is:\r\n```\r\nW tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: Key gru_cell_1/bias:0 not found in checkpoint\r\n```\r\n\r\nSo the error comes from the weights initialization. By looking closer to the structure of the `GRUCell` from the **1** and **2** versions, I noticed major differences:\r\n\r\n1. The `compat.v1.rnn_cell.GRUCell` has **four weights**:\r\n    * `gru_cell/gates/kernel:0` of shape `(S + H, 2 x H)`, \r\n    * `gru_cell/gates/bias:0` of shape `(2 x H, )`, \r\n    * `gru_cell/candidate/kernel:0` of shape `(S + H, H)`,  \r\n    * `gru_cell/candidate/bias:0` of shape `(H, )`\r\n\r\n2. The `tf.keras.layers.GRUCell` has **three weights**:\r\n    * `gru_cell/kernel:0` of shape `(S, 3 x H)`\r\n    * `gru_cell/recurrent_kernel:0` of shape `(H, 3 x H)`\r\n    * `gru_cell/bias:0` of shape `(2, 3 x H)`\r\n\r\nSo the issue is that **tensorflow** struggles to load weights from the first version to a `keras.layers.GRUCell` - of course, the shape and length of the weights are different. \r\nIs there a function to migrate `compat.v1.nn.rnn_cell.GRUCell` weights to `keras.layers.GRUCell` ? I have not seen in the documentation.\r\n\r\n## To illustrate\r\n\r\n### 1. Weights of `compat.v1.nn.rnn_cell.GRUCell` layer\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nSEQ_LENGTH = 4\r\nHIDDEN_SIZE = 512\r\nBATCH_SIZE = 1\r\ninputs = tf.random.normal([BATCH_SIZE, SEQ_LENGTH])\r\n\r\n# GRU cell\r\ngru = tf.compat.v1.nn.rnn_cell.GRUCell(HIDDEN_SIZE)\r\n# Hidden state\r\nstate = gru.zero_state(BATCH_SIZE, tf.float32)\r\n# Forward\r\noutput, state = gru(inputs, state)\r\n\r\nfor weight in gru.weights:\r\n    print(weight.name, weight.shape)\r\n```\r\n\r\nOutput:\r\n```raw\r\ngru_cell/gates/kernel:0 (516, 1024)\r\ngru_cell/gates/bias:0 (1024,)\r\ngru_cell/candidate/kernel:0 (516, 512)\r\ngru_cell/candidate/bias:0 (512,)\r\n```\r\n\r\n### 2. Weights of `keras.layers.GRUCell` layer\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nSEQ_LENGTH = 4\r\nHIDDEN_SIZE = 512\r\nBATCH_SIZE = 1\r\ninputs = tf.random.normal([BATCH_SIZE , SEQ_LENGTH])\r\n\r\n# GRU cell\r\ngru = tf.keras.layers.GRUCell(HIDDEN_SIZE)\r\n# Hidden state\r\nstate = tf.zeros((BATCH_SIZE, HIDDEN_SIZE), dtype=tf.float32)\r\n# Forward\r\noutput, state = gru(inputs, state)\r\n\r\n# Display the weigths\r\nfor weight in gru.weights:\r\n    print(weight.name, weight.shape)\r\n```\r\n\r\nOutput:\r\n```raw\r\ngru_cell/kernel:0 (4, 1536)\r\ngru_cell/recurrent_kernel:0 (512, 1536)\r\ngru_cell/bias:0 (2, 1536)\r\n```\r\n\r\n## Note\r\n\r\nI tried [`_convert_rnn_weights` tensorflow function](https://github.com/tensorflow/tensorflow/blob/c52ee536c045186476450cf76461d47fb2667e7d/tensorflow/python/keras/saving/hdf5_format.py#L404-L568) to convert the desired weights. It works but only for `CuDNN` weights, so I can't use it in my case.", "comments": ["Was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/f9aabf8ec47e727fbde26a297a4adaf0/44948.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/44cc623d82e2ad35605b21843094e035/44948-tf-nightly.ipynb). Please find the attached gist. Thanks!", "Thanks for reporting the issue.\r\n\r\nIn short, the weights between compat.v1.nn.rnn_cell.GRUCell and keras.layers.GRUCell are not compatible between each other. We don't have a function to convert between them, and if you really want to do it, you will need to do it manually.\r\n\r\nMath wise, if you have the numpy value of the v1 weights, the formula are:\r\nB = batch_size\r\nH = state_size\r\n\r\n1. all_kernel = np.concat([gru_cell/gates/kernel, gru_cell/candidate/kernel], axis=1)  # shape (B+H, 3 * H)\r\n2. kernel = all_kernel[:B]  # shape(B, 3 * H)\r\n3. recurrent_kernel = all_kernel[B:] # shape (H, 3 * H)\r\n4. bias = np.concat([gru_cell/gates/bias, gru_cell/candidate/bias], axis=0)  # shape (B, 3 * H)\r\n5. zero_bias = np.zeros([B, 3 * H])\r\n6. bias = np.concat([bias, zero_bias], axis=0)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44948\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44948\">No</a>\n"]}, {"number": 44947, "title": "How to save a TensorFlow model to use in ML.NET", "body": "I have posted this question [here](https://stackoverflow.com/questions/64794378/correct-pb-file-to-move-tensorflow-model-into-ml-net) on Stack Overflow, but it hasn't received a response so I'll ask here (**please look at the link before commenting** as you will see that the problem is explained in greater detail).\r\n\r\nI have a 1D CNN in TensorFlow and I am trying to save it so I can use it in ML.NET (by use I mean predict only).  Examples of using TF in .NET show that it is saved in a manner similar to the [SavedModel](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/saved_model); however, the saved_model.pb is not correct when comparing it in [Netron](https://netron.app/) (shown in my question on Stack Overflow.\r\n\r\nFor example the following TensorFlow [code](https://www.tensorflow.org/tfx/tutorials/serving/rest_simple), if you examine the model in Netron, it looks like it only has 1 node (and is really confusing).  If you save it as a .h5 and load it into Netron, it looks normal (but ML.NET requires that it be in the SavedModel format such as this [example](https://docs.microsoft.com/en-us/dotnet/machine-learning/tutorials/text-classification-tf)).\r\n\r\nIt should also be noted that there are instances where ML.NET does not require the SavedModel format such as [here](https://github.com/dotnet/machinelearning-samples/tree/master/samples/csharp/getting-started/DeepLearning_ImageClassification_TensorFlow/ImageClassification/assets/inputs/inception) where the inception pb is quite small.  I'm not sure why it's needed sometimes and other times it is not needed.  @Oceania2018 are these meta [graphs](https://github.com/SciSharp/TensorFlow.NET/tree/master/graph) similar to this situation?\r\n\r\nSo my question is simple.  How can I save a TensorFlow model in the correct format so I can pull it into ML.NET to make predictions.\r\n", "comments": ["Can you go through the process follow this example [Image Classcification](https://github.com/SciSharp/SciSharp-Stack-Examples/blob/bc4eea022a3d047f0e96e994713ea398b286947c/src/TensorFlowNET.Examples/ImageProcessing/TransferLearningWithInceptionV3.cs#L503). \r\nIt generates a pb format model file that can be consumed by TensorFlow and ML.NET.", "@Oceania2018 I ran through the sample above and it pulls up and looks correct in Netron.  However, when place the model in the ML.NET TensorFlow prediction [sample](https://github.com/dotnet/machinelearning-samples/tree/master/samples/csharp/getting-started/DeepLearning_ImageClassification_TensorFlow/ImageClassification), then it throws a GraphDef error (I know the Default Parameters are not correct, I was just interested to know if it would actually load the model).\r\n\r\n![image](https://user-images.githubusercontent.com/68339383/99597806-ca03d400-29b5-11eb-9f05-d87a58512e15.png)\r\n\r\nAlso, the sample you provided begins it's transfer learning by loading the InceptionV3.meta file.  I'm not interested in transfer training, I'm just interested in taking my model (.h5) and making predictions within ML.NET.  (And just as a reminder, I'm using a 1D CNN)", "@hjoshuaj Did you get a chance to see https://docs.microsoft.com/en-us/dotnet/machine-learning/tutorials/image-classification ? \r\nFrom this tutorial looks like you need to have a frozen TF model saved model (.pb) for using it in ml net.\r\n", "@ymodak Yes, I know it needs to be in pb format (it is only one line of code to read an h5 into Python and then save it as needed).  However the TF saved model format does not do the pb file correctly for ML.NET and can be verified by looking at the graph on Netron.  Please look through the first link I have posted as this goes into more depth what the problem is.", "@zeahmed You explain how you obtained the correct pb and variables files [here](https://github.com/dotnet/machinelearning-testdata/tree/master/Microsoft.ML.TensorFlow.TestModels/sentiment_model).  Can you update this for TF2?  Basically if I save a model from TensorFlow in the SavedModel format that TF2 uses, it looks really weird (sometimes having only one node) and will not pull into ML.NET correctly since the input and output nodes are used. Do you have a suggestion?", "Closing this issue since the associated SO thread resolves it. Thank you."]}, {"number": 44946, "title": "ValueError: Shapes (None, 1) and (None, 10) are incompatible", "body": "I have 40 rows and 15000 columns \r\nEEG data with attaching lebel  \r\n```\r\nEpoch 1/10\r\nWARNING:tensorflow:Layer dense_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\r\n\r\nIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\r\n\r\nTo change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\r\n\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-33-601f7224d647> in <module>()\r\n----> 1 model.fit(X_train, y_train, epochs=10, batch_size=32)\r\n\r\n10 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n    971           except Exception as e:  # pylint:disable=broad-except\r\n    972             if hasattr(e, \"ag_error_metadata\"):\r\n--> 973               raise e.ag_error_metadata.to_exception(e)\r\n    974             else:\r\n    975               raise\r\n\r\nValueError: in user code:\r\n\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\r\n        return step_function(self, iterator)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:796 step_function  **\r\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\r\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\r\n        return self._call_for_each_replica(fn, args, kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\r\n        return fn(*args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:789 run_step  **\r\n        outputs = model.train_step(data)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:749 train_step\r\n        y, y_pred, sample_weight, regularization_losses=self.losses)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:204 __call__\r\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:149 __call__\r\n        losses = ag_call(y_true, y_pred)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:253 call  **\r\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\r\n        return target(*args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:1535 categorical_crossentropy\r\n        return K.categorical_crossentropy(y_true, y_pred, from_logits=from_logits)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\r\n        return target(*args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:4687 categorical_crossentropy\r\n        target.shape.assert_is_compatible_with(output.shape)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_shape.py:1134 assert_is_compatible_with\r\n        raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\r\n\r\n    ValueError: Shapes (None, 1) and (None, 10) are incompatible\r\n```", "comments": ["@connect2robiul \r\n\r\nPlease, fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nRequest you to share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "> @connect2robiul\r\n> \r\n> Please, fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n> \r\n> Request you to share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!\r\n\r\nhttps://colab.research.google.com/drive/1nLY9ucV9UV6Avp7k4obKaGRZL21i7wLI?usp=sharing ", "@connect2robiul \r\n\r\nCan you help us with sample data to reproduce the issue in our environment.Which TF version you are using?\r\nThanks!", "> @connect2robiul\r\n> \r\n> Can you help us with sample data to reproduce the issue in our environment.Which TF version you are using?\r\n> Thanks!\r\n\r\nI can not share my dataset. I am using default colab TF which is 2.3.0 ", "Looks like you were successful in finding a workaround for this.\r\nI see you are using `tf.data.Dataset.from_tensor_slices()` to convert the `pandas.Dataframe` into a `tf.data` dataset which is the correct approach. \r\nhttps://www.tensorflow.org/tutorials/load_data/pandas_dataframe\r\n`model.fit` accepts input data types such as numpy array, tensor or tf dataset.", "> Looks like you were successful in finding a workaround for this.\r\n> I see you are using `tf.data.Dataset.from_tensor_slices()` to convert the `pandas.Dataframe` into a `tf.data` dataset which is the correct approach.\r\n> https://www.tensorflow.org/tutorials/load_data/pandas_dataframe\r\n> `model.fit` accepts input data types such as numpy array, tensor or tf dataset.\r\n\r\nsame error through I change to to `model.fit(X_train.to_numpy(), y_train.to_numpy(), epochs=10, batch_size=32)`", "@connect2robiul Can you share a minimal example to reproduce this with minimal dummy data?", "> @connect2robiul Can you share a minimal example to reproduce this with minimal dummy data?\r\n\r\nI got a different error on the same line; code link <<https://colab.research.google.com/drive/1UAiDGFQ14EhEzYDnb7yIsJPD0SVL58Rn?usp=sharing>>\r\nAnd it's editable  ", "> @connect2robiul\r\n> \r\n> Can you help us with sample data to reproduce the issue in our environment.Which TF version you are using?\r\n> Thanks!\r\n\r\nI got a different error on the same line; code link <https://colab.research.google.com/drive/1UAiDGFQ14EhEzYDnb7yIsJPD0SVL58Rn?usp=sharing>\r\nAnd it's editable ", "The colab you shared is different from the previously shared where we were dealing with csv data frame and converting it into `tf.data.Datasets`.\r\nHowever in the current colab we may want to change `loss=binary_crossentropy` since the label is in binary and set correct input data (47, 120000) and target data (47,) shapes.\r\n", "> The colab you shared is different from the previously shared where we were dealing with csv data frame and converting it into `tf.data.Datasets`.\r\n> However in the current colab we may want to change `loss=binary_crossentropy` since the label is in binary and set correct input data (47, 120000) and target data (47,) shapes.\r\n\r\nI run the same code here < https://colab.research.google.com/drive/1pYOmiNKCOA4LkIr2MbQlqfHbyvvcWb-U?usp=sharing>. But still has an error on the fit function ", "> > The colab you shared is different from the previously shared where we were dealing with csv data frame and converting it into `tf.data.Datasets`.\r\n> > However in the current colab we may want to change `loss=binary_crossentropy` since the label is in binary and set correct input data (47, 120000) and target data (47,) shapes.\r\n> \r\n> I run the same code here < [https://colab.research.google.com/drive/1pYOmiNKCOA4LkIr2MbQlqfHbyvvcWb-U?usp=sharing>](https://colab.research.google.com/drive/1pYOmiNKCOA4LkIr2MbQlqfHbyvvcWb-U?usp=sharing%3E). But still has an error on the fit function\r\n\r\nFor your last colab example:\r\n```\r\nmodel.add(Dense(11, activation='softmax'))\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44946\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44946\">No</a>\n"]}, {"number": 44945, "title": "[FIX ME!] Matrix computaion in custom-ops got partly correct result.", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):YES\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):UBUNTU16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below):\r\n- Python version:3.6\r\n- Bazel version (if compiling from source): Build label: 0.19.1\r\n- GCC/Compiler version (if compiling from source):gcc version 5.4.0\r\n- CUDA/cuDNN version: 10.1/7.0\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\ntensorflow 1.13.2\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nOnly the first row of the matrix got correct result.\r\n**Describe the expected behavior**\r\nAll row in matrix have the crrect result.\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nyou can clone this [repo](https://github.com/shipleyxie/custom-ops.git) ,By using  `bash build_install.py`  and run following code to reproduce this error.\r\n```shell\r\nimport tensorflow as tf\r\nimport tensorflow_zero_out\r\ninput = [[1.0,2.0,3.0,4.0],[1.0,2.0,3.0,4.0],[1.0,2.0,3.0,4.0],[1.0,2.0,3.0,4.0],[1.0,2.0,3.0,4.0],[1.0,2.0,3.0,4.0],[1.0,2.0,3.0,4.0],[1.0,2.0,3.0,4.0],[1.0,2.0,3.0,4.0]]\r\nfilter = [[0.025], [0.975]]\r\nprint(tensorflow_zero_out.zero_out(input, filter))\r\n```\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```\r\n2020-11-17 22:49:08.034150: I tensorflow_time_two/cc/kernels/time_two_kernels.cc:39] batch_size is 4 data_len is 4\r\n2020-11-17 22:49:08.034178: I tensorflow_time_two/cc/kernels/time_two_kernels.cc:41] weights_tensor[0,0]0.025weights_tensor[0,1]0.975\r\n2020-11-17 22:49:08.034188: I tensorflow_time_two/cc/kernels/time_two_kernels.cc:48] output_tensor[0, 0] = 0.025*1+0.975*0;\r\n2020-11-17 22:49:08.034195: I tensorflow_time_two/cc/kernels/time_two_kernels.cc:51] output_tensor result is 0.025\r\n2020-11-17 22:49:08.034204: I tensorflow_time_two/cc/kernels/time_two_kernels.cc:60] output_tensor[0,1 ] = 0.025*1+0.975*0.025;\r\n2020-11-17 22:49:08.034211: I tensorflow_time_two/cc/kernels/time_two_kernels.cc:63] output_tensor result is 0.049375\r\n2020-11-17 22:49:08.034220: I tensorflow_time_two/cc/kernels/time_two_kernels.cc:60] output_tensor[0,2 ] = 0.025*1+0.975*0.049375;\r\n2020-11-17 22:49:08.034228: I tensorflow_time_two/cc/kernels/time_two_kernels.cc:63] output_tensor result is 0.0731406\r\n2020-11-17 22:49:08.034236: I tensorflow_time_two/cc/kernels/time_two_kernels.cc:60] output_tensor[0,3 ] = 0.025*1+0.975*0.0731406;\r\n2020-11-17 22:49:08.034243: I tensorflow_time_two/cc/kernels/time_two_kernels.cc:63] output_tensor result is 0.0963121\r\n2020-11-17 22:49:08.034252: I tensorflow_time_two/cc/kernels/time_two_kernels.cc:48] output_tensor[1, 0] = 0.025*1+0.975*0;\r\n2020-11-17 22:49:08.034259: I tensorflow_time_two/cc/kernels/time_two_kernels.cc:51] output_tensor result is 0.025\r\n2020-11-17 22:49:08.034267: I tensorflow_time_two/cc/kernels/time_two_kernels.cc:60] output_tensor[1,1 ] = 0.025*1+0.975*0.025;\r\n2020-11-17 22:49:08.034274: I tensorflow_time_two/cc/kernels/time_two_kernels.cc:63] output_tensor result is 0.049375\r\n2020-11-17 22:49:08.034283: I tensorflow_time_two/cc/kernels/time_two_kernels.cc:60] output_tensor[1,2 ] = 0.025*1+0.975*0.049375;\r\n2020-11-17 22:49:08.034290: I tensorflow_time_two/cc/kernels/time_two_kernels.cc:63] output_tensor result is 0.0731406\r\n2020-11-17 22:49:08.034299: I tensorflow_time_two/cc/kernels/time_two_kernels.cc:60] output_tensor[1,3 ] = 0.025*1+0.975*0.0731406;\r\n2020-11-17 22:49:08.034306: I tensorflow_time_two/cc/kernels/time_two_kernels.cc:63] output_tensor result is 0.0963121\r\n2020-11-17 22:49:08.034314: I tensorflow_time_two/cc/kernels/time_two_kernels.cc:48] output_tensor[2, 0] = 0.025*1+0.975*0;\r\n2020-11-17 22:49:08.034321: I tensorflow_time_two/cc/kernels/time_two_kernels.cc:51] output_tensor result is 0.025\r\n2020-11-17 22:49:08.034330: I tensorflow_time_two/cc/kernels/time_two_kernels.cc:60] output_tensor[2,1 ] = 0.025*1+0.975*0.025;\r\n2020-11-17 22:49:08.034337: I tensorflow_time_two/cc/kernels/time_two_kernels.cc:63] output_tensor result is 0.049375\r\n2020-11-17 22:49:08.034345: I tensorflow_time_two/cc/kernels/time_two_kernels.cc:60] output_tensor[2,2 ] = 0.025*1+0.975*0.049375;\r\n2020-11-17 22:49:08.034352: I tensorflow_time_two/cc/kernels/time_two_kernels.cc:63] output_tensor result is 0.0731406\r\n2020-11-17 22:49:08.034362: I tensorflow_time_two/cc/kernels/time_two_kernels.cc:60] output_tensor[2,3 ] = 0.025*1+0.975*0.0731406;\r\n2020-11-17 22:49:08.034370: I tensorflow_time_two/cc/kernels/time_two_kernels.cc:63] output_tensor result is 0.0963121\r\n2020-11-17 22:49:08.034379: I tensorflow_time_two/cc/kernels/time_two_kernels.cc:48] output_tensor[3, 0] = 0.025*1+0.975*0;\r\n2020-11-17 22:49:08.034387: I tensorflow_time_two/cc/kernels/time_two_kernels.cc:51] output_tensor result is 0.025\r\n2020-11-17 22:49:08.034395: I tensorflow_time_two/cc/kernels/time_two_kernels.cc:60] output_tensor[3,1 ] = 0.025*1+0.975*0.025;\r\n2020-11-17 22:49:08.034403: I tensorflow_time_two/cc/kernels/time_two_kernels.cc:63] output_tensor result is 0.049375\r\n2020-11-17 22:49:08.034412: I tensorflow_time_two/cc/kernels/time_two_kernels.cc:60] output_tensor[3,2 ] = 0.025*1+0.975*0.049375;\r\n2020-11-17 22:49:08.034420: I tensorflow_time_two/cc/kernels/time_two_kernels.cc:63] output_tensor result is 0.0731406\r\n2020-11-17 22:49:08.034429: I tensorflow_time_two/cc/kernels/time_two_kernels.cc:60] output_tensor[3,3 ] = 0.025*1+0.975*0.0731406;\r\n2020-11-17 22:49:08.034436: I tensorflow_time_two/cc/kernels/time_two_kernels.cc:63] output_tensor result is 0.0963121\r\n2020-11-17 22:49:08.034449: I tensorflow_time_two/cc/kernels/time_two_kernels.cc:116] debug out_tensor <<<<< 0.0250.0493750.07314060.09631210.0250.0493750.09631210.0250.0493750.07314060.0963121\r\n2020-11-17 22:49:08.034463: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2020-11-17 22:49:08.034493: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\ntf.Tensor(\r\n[[2.5000000e-02 4.9375001e-02 7.3140629e-02 9.6312113e-02]\r\n [1.4873783e+24 4.5570226e-41 1.4873783e+24 4.5570226e-41]\r\n [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\r\n [2.6221498e+20 1.8523601e+28 1.2714611e+31 1.7322379e+19]], shape=(4, 4), dtype=float32)\r\n\r\n```", "comments": ["@shipleyxie,\r\nTensorFlow 1.x in not actively supported. Could you please update TensorFlow to v2.3 and check if you are facing the same issue? Thanks!", "This ERROR is caused by `context->input(0).matrix<T>().data()` .\r\nChange \r\n```C\r\n\r\nZeroOutFunctor<Device, T>()(\r\ncontext->eigen_device<Device>(), input_shape.dim_size(0),\r\ninput_shape.dim_size(1), input_tensor.matrix<T>().data(),\r\nweights_tensor.matrix<T>().data(), output_tensor->matrix<T>().data());\r\n```\r\nto\r\n```C\r\nZeroOutFunctor<Device, T>()(\r\ncontext->eigen_device<Device>(), input_shape.dim_size(0),\r\ninput_shape.dim_size(1), input_tensor,\r\nweights_tensor, output_tensor);\r\n```\r\nwill fix this.\r\nI try to investigate into this, but not getting any further forward with this.\r\nDo you have any suggestions? @amahendrakar ", "> This ERROR is caused by `context->input(0).matrix<T>().data()` .\r\n\r\n@shipleyxie,\r\nCould you please let us know if you are facing this error with TF v1.13 or TF v2.3?\r\n\r\n\r\nAlso while trying to reproduce the issue, I'm facing an error stating `ERROR: tensorflow_custom_ops-0.0.1-cp37-cp37m-linux_x86_64.whl is not a supported wheel on this platform.`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/98835397e8af09ade315c84c014cd18e/44945.ipynb). Thanks!\r\n", "Using TF v1.13.2.\r\n\r\nThere are some diffrence between colab and my environment.\r\nIn the gist as you posted [here](https://colab.research.google.com/gist/amahendrakar/98835397e8af09ade315c84c014cd18e/44945.ipynb), manually  install the build package using `pip install artifacts/tensorflow_custom_ops-0.0.1-cp36-cp36m-linux_x86_64.whl`\r\n is needed after `bash build_install.py`.\r\n\r\nI try to excute the below block of code in colab,\r\n```python\r\nimport tensorflow as tf\r\nimport os\r\nimport tensorflow_zero_out\r\n\r\ntf.enable_eager_execution\r\n\r\nos.environ['TF_CPP_MIN_VLOG_LEVEL']='3'\r\nos.environ['TF_CPP_MIN_LOG_LEVEL']='0'\r\ninput = [[1.0,2.0,3.0,4.0],[1.0,2.0,3.0,4.0],[1.0,2.0,3.0,4.0],[1.0,2.0,3.0,4.0],[1.0,2.0,3.0,4.0],[1.0,2.0,3.0,4.0],[1.0,2.0,3.0,4.0],[1.0,2.0,3.0,4.0],[1.0,2.0,3.0,4.0]]\r\nfilter = [[0.025], [0.975]]\r\nprint(tensorflow_zero_out.zero_out(input, filter))\r\n```\r\n... no printed result. \r\nBut it will show log messages in my python env. I am not familiar with google colab,Could you please reproduce it in your local Python env?", "@shipleyxie,\r\nPlease upgrade your Tensorflow Version (TF 2.x is recommended) as you are using very older version and please let know if your issue persists in the latest version. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44945\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44945\">No</a>\n"]}]