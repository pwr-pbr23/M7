[{"number": 11422, "title": "No clue to inform what 'dimension' arg of argmin or argmax means in API docs", "body": "In documentation of TF API 1.2, `tf.argmin` and `tf.argmax` have `dimension` argument.\r\n\r\n* [tf.argmin](https://www.tensorflow.org/api_docs/python/tf/argmin)\r\n* [tf.argmax](https://www.tensorflow.org/api_docs/python/tf/argmax)\r\n\r\nHowever, there is no any explanation for what it means.", "comments": ["The `dimension` argument is a deprecated synonym for `axis`. You should use `axis` in new code.\r\n\r\n(The appearance of `dimension` in the generated docs suggests that this is something we should avoid in the doc generator. I'll assign this to @MarkDaoust, since he's most familiar with the recent advances in our doc generation technology!)"]}, {"number": 11421, "title": "Daily Pull Request (July 10th 2017)", "body": "", "comments": ["@tensorflow-jenkins test this please.", "@jhseu PTAL", "Barrier ops test is b/35468214.\r\n\r\nJenkins, test this please.", "Nits:\r\n- 5d1b38d is the merge commit, but the commit message indicates something else (probably what was needed in conflict resolution?).\r\n- (Style) Please leave the original pull request title that's set when you create the pull request since it includes the CL#.", "Due to some internal delays, I am going to recreate and land this in a bit."]}, {"number": 11420, "title": "Temporarily disable barrier_ops_test on Mac", "body": "", "comments": []}, {"number": 11419, "title": "KL divergence not in tf.contrib.distributions as of 1.2.0", "body": "Apparently KL divergence is no longer in contrib.distributions.\r\nThis page is broken:\r\nhttps://www.tensorflow.org/api_docs/python/tf/contrib/distributions/kl\r\n\r\nand this file is removed:\r\ntensorflow/contrib/distributions/python/ops/kullback_leibler.py\r\n\r\nbut I couldn't find it being mentioned in the changelog. Did I miss it?", "comments": ["I didn't understand your question correctly. But, If you are referring `API r1.2` than see the link [https://www.tensorflow.org/api_docs/python/tf/contrib/distributions/kl_divergence](https://www.tensorflow.org/api_docs/python/tf/contrib/distributions/kl_divergence)", "Oh I see. So it is just a name change from `kl` to `kl_divergence`", "I also encourage you to review guidelines and follow template [here](https://github.com/tensorflow/tensorflow/blob/master/ISSUE_TEMPLATE.md) before raising an issue."]}, {"number": 11418, "title": "Changed to correct version", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 11417, "title": "Apache License header does not include HTTPS link", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.12.15\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.2\r\n- **Python version**: 2.7.13\r\n\r\n### Describe the problem\r\nVarious code files have `http://www.apache.org/licenses/LICENSE-2.0` instead of `https://www.apache.org/licenses/LICENSE-2.0`. Not sure if this is the result of bazel or simply that needs to be explicitly written.\r\n\r\n### Source code / logs\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/wide_n_deep_tutorial.py\r\n", "comments": ["I'm not sure if it makes a difference. I did a search and the http version of the link seems to be used 400x more, because it's what the Apache 2.0 license recommends. Although in all fairness, it was written in 2004. It would be a big change to refactor these headers and I'm not sure if it's worth doing.", "From a security perspective, there is a difference, though to be fair, it is something that maybe should be updated on Apache's side. If anything, I will change if I see in TensorFlow code when I make a pull request unrelated to HTTPS of the license."]}, {"number": 11416, "title": "Distributed training with synchronized SGD using 'grpc+verbs' sometimes hangs indefinitely", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.2 LTS\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: v1.2.0-1755-gee4259a 1.2.1\r\n- **Python version**: Python 3.5.2\r\n- **Bazel version (if compiling from source)**: 0.5.1\r\n- **CUDA/cuDNN version**: 8.0/5.1\r\n- **GPU model and memory**: GeForce GTX 1080 Ti\r\n- **Exact command to reproduce**: Sorry, the command is not available since this is custom code\r\n- **RDMA driver version**: libibverbs-dev (1.2.1mlnx1-OFED.4.0.1.5.3.40200)\r\n\r\n### Describe the problem\r\nI am training a seq2seq model with synchronized SGD (using `tf.train.SyncReplicasOptimizer` and `tf.train.Supervisor`), over RDMA (using `grpc+verbs` protocol), on 2 workers and 1 parameter server. Each worker has 8 GPUs, and the model on each GPU is the same.\r\n\r\nI can train this model fine with the default `grpc` protocol using the same setting. When I switched to `grpc+verbs`, the behavior becomes unpredictable. Most of the times both workers hang (for at least 12 hours, so not because I didn't wait long enough). Sometimes the chief worker (worker 0) starts training, but worker 1 hangs. (This should not happen since I am programming with synchronized SGD. Worker 0 should be blocked if worker 1 is not ready.) In rare case it can go through and start training. I believe the RDMA driver is correctly installed.\r\n\r\nWhen worker hangs, the CPU utilization stays low but not zero, and the GPU utilization is 0.\r\n\r\n### Source code / logs\r\nI attached below the log from worker 1 [worker1.txt](https://github.com/tensorflow/tensorflow/files/1136607/worker1.txt). In this case, worker 1 hangs, and worker 0 starts training. (There are some custom logs printed.) In the log you can find that worker 1 sent a `RDMA_MESSAGE_TENSOR_REQUEST` to the parameter server, but never got an ACK, and I have checked the log from the parameter server, which showed that it never received a `RDMA_MESSAGE_TENSOR_REQUEST` from worker 1.\r\n\r\nI also attached the full trace [gdb_worker1.txt](https://github.com/tensorflow/tensorflow/files/1136672/gdb_worker1.txt) collected on worker 1 in gdb with `thread apply all bt`, during worker 1 hanging.\r\n\r\nThanks.\r\n\r\n\r\n\r\n", "comments": ["@junshi15 Could you shed some light on this? Thanks!", "Have you tried your code in loopback mode? It helps to make sure that the problem doesn't lie in your network fabrics configuration.", "@byronyi Thanks for the quick reply. I have tried my code over Ethernet, and it worked fine. I think that should indicate the network fabrics is fine. Also, one experiment I tried over RDMA did go through as I described in the issue. So RDMA fabric seems fine. The loopback mode won't traverse the network stack, right?", "Note that RoCE requires a lossless fabric, while gRPC does not. If you could reproduce the error in loopback mode, it would be easier for us to identify the possible issue.", "@on-the-run, can u verify the RoCE configuration with ib_write_bw between the 2 nodes ?\r\nAs @byronyi said, you may need to configure lossless (PFC) or ECN to prevent the packet loss.\r\nib_write_bw is a good stress test to see if RoCE is configured good.\r\nCan u attach the full logs of the PS and worker ? I'm missing a lot of events here in the logs. ", "@shamoya, the RDMA I am using is Infiniband, instead of RoCE. `ib_write_bw` runs fine. The workers and the parameter sever can talk to each other via RDMA. Posting the output from running `ib_write_bw` between one worker and one parameter server below. \r\n```\r\n---------------------------------------------------------------------------------------\r\n                    RDMA_Write BW Test\r\n Dual-port       : OFF          Device         : mlx4_0\r\n Number of qps   : 1            Transport type : IB\r\n Connection type : RC           Using SRQ      : OFF\r\n CQ Moderation   : 100\r\n Mtu             : 2048[B]\r\n Link type       : IB\r\n Max inline data : 0[B]\r\n rdma_cm QPs     : OFF\r\n Data ex. method : Ethernet\r\n---------------------------------------------------------------------------------------\r\n local address: LID 0x15 QPN 0x32da PSN 0xebb028 RKey 0xa0010194 VAddr 0x007f11bbfc2000\r\n remote address: LID 0x12 QPN 0x0337 PSN 0x198251 RKey 0xd001018f VAddr 0x007f728f89f000\r\n---------------------------------------------------------------------------------------\r\n #bytes     #iterations    BW peak[MB/sec]    BW average[MB/sec]   MsgRate[Mpps]\r\n 65536      5000             6071.23            6071.03            0.097136\r\n---------------------------------------------------------------------------------------\r\n```", "@shamoya, about the logs of PS and worker, I didn't turn on VLOG for the log file I posted earlier. Will a log with VLOG(2) enabled be useful?\r\n", "If it's IB then we shouldn't have any packet loss.\r\nVLOG(2) is maybe too much verbose, maybe just change all VLOG in rdma.cc to 0, so we can just see the Rendezvous protocol messages.\r\nHow many model parameters are there ?", "@on-the-run Did you even run an example successfully, say InceptionV3 training on ImageNet dataset?", "@junshi15, Yes. The same code did go through once on RDMA, and it worked well at that time. I didn't try InceptionV3, though.", "that's strange. I have not seen issues like that, Your problem probably depends on the ordering of events, which is random. I am wondering if there is memory corruption somewhere.", "@junshi15, one thing that I observed to happen repeatedly was that after the RDMA channels have been built, worker 1 sent a `RDMA_MESSAGE_TENSOR_REQUEST ` to PS, and never got an ACK. Then worker 1 was blocked there. And, PS never received that `RDMA_MESSAGE_TENSOR_REQUEST `. Does this ring a bell?", "@shamoya, there are about 70 million parameters. Here are full logs:\r\n[ps.txt](https://github.com/tensorflow/tensorflow/files/1137272/ps.txt), \r\n[worker0.txt](https://github.com/tensorflow/tensorflow/files/1137274/worker0.txt), \r\n[worker1.txt](https://github.com/tensorflow/tensorflow/files/1137275/worker1.txt)\r\nIn this case, worker 1 hanged, but worker 0 didn't. Not sure if the log of worker 1 gives enough information. Let me know if you need me to turn on other logs. Thanks.", "does distributed tensorflow using 'grpc+verbs' have unit tests? I didn't find any. I meet `2017-07-11 09:46:46.591411: F tensorflow/contrib/verbs/rdma.cc:95] Check failed: cq_ Failed to create completion queue\r\n` when using 'grpc+verbs' in an ib network.", "@on-the-run RDMA_MESSAGE_TENSOR_REQUEST is a fairly simple message. Here are some possibilities why PS did not receive it.\r\n1) the message was not sent by the worker. Note we only enqueue the message. The actual RDMA write happens [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/rdma.cc#L640-L656), by logging more in that function, we will know if the message is indeed written.\r\n\r\n2) the message was not received at PS. [Here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/rdma.cc#L121-L127) is how we get notification of events. I do not expect any missed events, but maybe there are corner cases...\r\nMy focus would be on (1) since it is easier to verify. If in doubt, You can also check if the worker's tx_message_buf and PS' rx_message_buf addresses are paired, so that messages go to the right buffer.\r\n3) the buffer name is parsed incorrectly. When the message is sent by the worker, we attach a [32-bit number](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/rdma.cc#L641), which is a hash code of the RDMA buffer name, i.e. \"rx_message_buffer\". At the PS, the hash code is [converted back](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/rdma.cc#L140) to RDMA buffer name. Hashing collision is rare but possible. If you have more than 4 billion tensors, then collision is guaranteed.", "@suiyuan2009 It's a good idea to add unit tests. right now there is none. Your error is likely due to hardware config. For example, the IB adapter is down. One situation I have seen is some people have multiple IB adapters, let's say the first one is down, the second is up. The current code opens the first [device](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/rdma.cc#L74), which is down, hence the failure.", "@junshi15 , it's seems user and root have different limit in my vm, I use root to run tf then it works.", "@junshi15, Thanks for the suggestion. For 1, I am pretty sure that the RDMA write from worker succeeded, or at least didn't return non-zero value. The evidence is that this [line](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/rdma.cc#L615) went through, and I did see the code in `ibv_post_send()` (defined in `infiniband/verbs.h`) was executed. I will try to look at 2 and 3.", "@junshi15, some updates about (2) and (3). I am attaching new logs, which give more information on RDMA related events, but fewer lines. In this case, both workers hang.\r\n[ps.txt](https://github.com/tensorflow/tensorflow/files/1142795/ps.txt)\r\n[worker0.txt](https://github.com/tensorflow/tensorflow/files/1142797/worker0.txt)\r\n[worker1.txt](https://github.com/tensorflow/tensorflow/files/1142798/worker1.txt)\r\n\r\nFor (2), I believe the message is not received by PS, but don't know why. In the logs you can find the buffers are correctly paired.\r\n\r\nFor (3), I don't think there is a hashing collision, since not even one tensor had been successfully sent yet in the logs attached.\r\n\r\nCould you shed some light on the following questions? \r\n(1) Which part of the code send the `RDMA_MESSAGE_ACK`? Why did I see `RDMA_MESSAGE_ACK` received in `rx_message_buffer`? \r\n(2) What will happen if all the outstanding `SendNextItem()` end up with doing nothing (which seems to be the case shown in the logs)?  \r\n(3) At line 87 and 88 of [ps.txt](https://github.com/tensorflow/tensorflow/files/1142795/ps.txt), PS received a `RDMA_MESSAGE_ACK` from worker 1. This doesn't make sense since PS had never sent any message to worker 1, nor worker 1 had received any message from PS.", "@on-the-run, Looking at the ps log, something doesn't make sense to me.\r\nThe second call of RecvFromRemoteAsync is for Tensor 140513923548928.\r\nThe thread (coming from RecvFromRemoteAsync) then calls EnqueItem (with a lock) and then SendNextItem which doesn't do Write since local and remove are busy.\r\nBut this tensor should stay at the top of the queue, and the next SendNextItem should send it, but that doesn't happen (same for the third tensor - 140514712069888)\r\n\r\nAfter the RDMA_MESSAGE_ACK is received (which set remote = idle and calls SendNextItem) the SendNextItem takes Tensor 140513093064448, and I don't understand why ? \r\n\r\nIs it possible your print change the locking scheme of the queue ? \r\n\r\n", "@junshi15 @on-the-run \r\nMaybe the imm_data is wrong ?\r\nThe first RDMA message received in worker0 should be RDMA_MESSAGE_TENSOR_REQUEST from the PS (can't be ACK), also (1) from @on-the-run questions indicate such an issue.\r\n\r\n@on-the-run, are you running on x86 ?\r\nMaybe it's an endian issue ? ", "@shamoya, My `tid` actually means thread id. Sorry for the confusion. I think PS finally sent three `RDMA_MESSAGE_TENSOR_REQUEST`, which matches the number of calls of `RecvFromRemoteAsync `. But I did add some VLOG inside lock surrounded code. Will that break something?", "@shamoya, I just noticed this odd behavior, too. It seems that somehow the code got a `RDMA_MESSAGE_TENSOR_REQUEST ` but thought it got a `RDMA_MESSAGE_ACK `. Note that in the log PS also received a `RDMA_MESSAGE_ACK ` from worker 1, but worker 1 only sent a `RDMA_MESSAGE_TENSOR_REQUEST `.\r\n\r\nI am running it on x86_64. How can I check the endian issue? Is endian handled by the Infiniband driver?", "I don't have an idea of the actual issue here, but as far as I know the immediate data is transmitted in network byte-order. You should use `wr.imm_data = htonl(val)` and `imm_data = ntohl(wc.imm_data)` to store and load the immediate data.", "@byronyi @shamoya, Does that mean the code [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/rdma.cc#L597-L612) should be converted with `htonl()`, before sending to the RDMA hardware?", "See [here](https://github.com/linux-rdma/rdma-core/blob/dceee9494bfbadf0cf16b8fef81f3c2cb5355243/librdmacm/examples/udaddy.c#L213) and [there](https://github.com/linux-rdma/rdma-core/blob/dceee9494bfbadf0cf16b8fef81f3c2cb5355243/librdmacm/examples/udaddy.c#L459) for an example.\r\n\r\nAnd yes, but you need to change save/load together, i.e. use `ntohl` [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/rdma.cc#L139).", "@shamoya @byronyi, Yes. It sounds like we found something! Let me try the change and get back. Thanks!", "@on-the-run \r\nsome high-level stuff:\r\nthere are three types of buffers, message/tensor/ack, each of them has tx and rx version. \"tx\" is for sending and rx is for receiving. tensorbuf is for tensor exchange, messagebuf is for message and all acks are on ackbuf. Since there is one one pair of message buffer and one pair of ack buffer, all the messages needs to be sent one by one. So are all the packs.\r\n\r\nWhen a message is sent from A to B, in most cases, B needs to send an [ack](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/rdma.cc#L199), hence if you look at A's log, you have \"sent RDMA_MESSAGE_XXXX\", then \"received RDMA_MESSAGE_ACK\". In B's log, you should have \"received RDMA_MESSAGE_XXX\", then \"sent RDMA_MESSAGE_ACK\"\r\n\r\nit seems all nodes sent \"RDMA_MESSAGE_TENSOR_REQUEST\" and received \"RDMA_MESSAGE_ACK\". I don't see any log from B's perspective. this is odd. A logging issue?\r\n\r\nTo answer your questions:\r\n1) Mostly in Process_CQ(), e.g. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/rdma.cc#L154\r\n\r\n2) buffers are busy. Remember all the messages have to be sent sequentially. A message can be sent only if both local tx-buf and remote rx-buf are idle. I think worker1 was stalled there.\r\n\r\n3) I don't see any \"sent RDMA_MESSAGE_ACK\". logging issue?", "Actually I'm not sure this is the cause, the imm_data is the same one sent from the PS and received by worker0 (986591585). Nothing would have worked if this was the case.\r\n\r\nMaybe the NameHash function acts different between the 2 nodes ? \r\nin PS this hash is for \"rx_message_buffer\" (RdmaMessageBuffer::SendNextItem) but somehow it worker0 the FindBuffer acts differently ? ", "Thanks @junshi15 \r\nIn (1) he meant if the type is recv RDMA_MESSAGE_ACK, then rb (derived from imm_data) must rx_ack_buffer (which isn't the case here for some reason).", "@on-the-run - same TF version / dependent python packages on all sides ?\r\nImportant to mention I don't see this issue in my setup.", "@shamoya, Yes, everything is the same on PS and workers: TF version, CUDA drivers, IB drivers, etc. I launch them with the same docker image.", "@shamoya, I have checked that behaviors of `NameHash()` and `FindBuffer()` are consistent across PS and workers.", "I see.\r\nThen what's happening here is very strange.\r\nI see also my previous claim is wrong since the imm_data is correct and also the RDMABuffer (rx_message_buffer). so the only thing I can think of is the buffer of rx_message_buffer is corrupted/not read well by ParseMessage.\r\n\r\nOk one more idea, RDMA_MESSAGE_ACK is 0 (first in enum).\r\nSo maybe all the message is 0s ? maybe you can print the buffer when u get the CQE ? \r\n\r\nAlso which OFED do u use ? is it MLNX_OFED ? FW version of the ConnectX ? ConnectX2/3/3Pro ? ", "@on-the-run, It is strange I don't see any log about \"RDMA_MESSAGE_ACK sent\" and \"RDMA_MESSAGE_TENSOR_REQUEST received\".\r\nDid you log anything inside [RdmaAckBuffer::SendNextItem()](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/rdma.cc#L628-L637)? \r\n\r\nAlso, I see many occurrences of `recv RDMA message: RDMA_MESSAGE_ACK, imm_data recevied = 986591585`. I assume 986591585 is NameHash('rx_ack_buffer'), can you confirm?", "@junshi15, I did log inside `RdmaAckBuffer::SendNextItem()`. It was jut not triggered. 986591585 is the hash of \"rx_message_buffer\" instead of \"rx_ack_buffer\". That's why I can't understand why the `RDMA_MESSAGE_ACK` was sent to rx_message_buffer. Is it possible that somehow the code misunderstands `RDMA_MESSAGE_TENSOR_REQUEST ` to be `RDMA_MESSAGE_ACK`? Since that assumption can explain the strange log events.", "@shamoya, RDMA driver version: libibverbs-dev (1.2.1mlnx1-OFED.4.0.1.5.3.40200).", "@on-the-run - it's just the version VERBS library, I would like to know also the kernel drivers of the HCA and also the FW version. please attach the output of:\r\n- ofed_info -s\r\n- ibv_devinfo\r\n\r\nAlso what do u think of my guess (all received buffer is actually 0 - and then it's parsed as RDMA_MESSAGE_ACK) ? ", "@shamoya, sorry, I am not so familiar with these stuff.\r\nOutput of `ofed_info -s`:\r\n```\r\nMLNX_OFED_LINUX-4.0-2.0.0.1:\r\n```\r\nOutput of `ibv_devinfo`:\r\n```\r\nhca_id: mlx4_0\r\n        transport:                      InfiniBand (0)\r\n        fw_ver:                         2.40.7000\r\n        node_guid:                      248a:0703:00bd:5740\r\n        sys_image_guid:                 248a:0703:00bd:5743\r\n        vendor_id:                      0x02c9\r\n        vendor_part_id:                 4099\r\n        hw_ver:                         0x1\r\n        board_id:                       MT_1100120019\r\n        phys_port_cnt:                  1\r\n        Device ports:\r\n                port:   1\r\n                        state:                  PORT_ACTIVE (4)\r\n                        max_mtu:                4096 (5)\r\n                        active_mtu:             4096 (5)\r\n                        sm_lid:                 1\r\n                        port_lid:               18\r\n                        port_lmc:               0x00\r\n                        link_layer:             InfiniBand\r\n```\r\n\r\nI think your guess makes sense. I am trying to log all the values of each work completion event. Will get back.", "We are using mlx5_0, if it matters.\r\n\r\n```\r\nhca_id:\tmlx5_0\r\n\ttransport:\t\t\tInfiniBand (0)\r\n\tfw_ver:\t\t\t\t12.1100.6630\r\n\tnode_guid:\t\t\te41d:2d03:0067:c9fc\r\n\tsys_image_guid:\t\t\te41d:2d03:0067:c9fc\r\n\tvendor_id:\t\t\t0x02c9\r\n\tvendor_part_id:\t\t\t4115\r\n\thw_ver:\t\t\t\t0x0\r\n\tboard_id:\t\t\tMT_2180110032\r\n\tphys_port_cnt:\t\t\t1\r\n\tDevice ports:\r\n\t\tport:\t1\r\n\t\t\tstate:\t\t\tPORT_ACTIVE (4)\r\n\t\t\tmax_mtu:\t\t4096 (5)\r\n\t\t\tactive_mtu:\t\t4096 (5)\r\n\t\t\tsm_lid:\t\t\t2\r\n\t\t\tport_lid:\t\t3\r\n\t\t\tport_lmc:\t\t0x00\r\n\t\t\tlink_layer:\t\tInfiniBand\r\n\r\n```", "@byronyi Thanks for raising the endian-ness issue. It will probably improve the portability. \r\n\r\nIf one of the nodes in the cluster is linux (little endian) and another is hp-ux (big endian), for example, then there may be problems. If the cluster is homogeneous, then there should be no issue.", "@junshi15, when parsing a `RdmaMessage` with `ParseMessage()`, does it need to be protected by lock?", "The message and tensor rx_buffers are read-only locally and protected  at the remote end by \"[remote_status_](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/rdma.cc#L644-L645)\". I don't see same protection on [the ack rx_buffer](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/rdma.cc#L628-L637).\r\nI need to think about the reason why ack buffers are treated differently. \r\n", "@shamoya, I have checked the work completion events. It looks fine to me. Most importantly, the `imm_data` is correctly sent. So it is not an endian issue. But the endian should be considered as it suggests here in `infiniband/verbs.h`:\r\n```\r\n struct ibv_wc {\r\n     uint64_t        wr_id;\r\n     enum ibv_wc_status  status;\r\n     enum ibv_wc_opcode  opcode;\r\n     uint32_t        vendor_err;\r\n     uint32_t        byte_len;\r\n     /* When (wc_flags & IBV_WC_WITH_IMM): Immediate data in network byte order.\r\n      * When (wc_flags & IBV_WC_WITH_INV): Stores the invalidated rkey.\r\n      */\r\n     uint32_t        imm_data;\r\n     uint32_t        qp_num;\r\n     uint32_t        src_qp;\r\n     int         wc_flags;\r\n     uint16_t        pkey_index;\r\n     uint16_t        slid;\r\n     uint8_t         sl;\r\n     uint8_t         dlid_path_bits;\r\n};\r\n```\r\nIt looks to me now a memory corruption issue.", "@on-the-run \r\nI refreshed my memory. Here is how the buffers are protected. I will focus on rx buffer which is protected by its remote side (tx side).\r\n\r\nRx_TensorBuffer:\r\nWhen we send a tensor via TensorBuffer, we need to make sure both the Tx_TensorBuffer (at the sender) and the Rx_TensorBuffer (at the receiver) are idle. This is achieved by checking [local_status_ and remote_status_](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/rdma.cc#L778) at the sender under a lock. When the tensor is received, the receiver send a [RDMA_MESSAGE_BUFFER_IDLE message](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/rdma_rendezvous_mgr.cc#L142) to release remote_status_, while local_status_ is released at the [sender](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/rdma.cc#L222).\r\n\r\nRx_MessageBuffer:\r\nSimilar to Rx_TensorBuffer, both local_status_ and remote_status_ flags are [checked](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/rdma.cc#L643). They are turned to [busy](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/rdma.cc#L644-L645) once transmission starts. When the receiver reads the buffer, it sends an ACK to notify the sender to release [remote_status_](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/rdma.cc#L153).\r\n\r\nRx_AckBuffer:\r\nNo protection at all. The sender does not check AckBuffer before sending an Ack. This is OK because (1) all the acks are sent (`ab->SendNextItem()`) by one thread inside Process_CQ().\r\n(2) messages and acks are interleaved. Let's say A wants to send multiple messages to B. After the first message is sent, A has to wait for the ack before sending the next message, i.e. [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/rdma.cc#L148) is the only place tx_message_buffer_.remote_status_ becomes idle. Since A can not send multiple messages consecutively, B can not ack consecutively. No risk of multiple ACKs stepping on each other.\r\n\r\nBut I afraid I may have missed corner cases. This messaging system is very fragile. If one message, even an ACK, is missing, you will likely get a hang.\r\n\r\nTo further debug, I suggest [the following change](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/rdma.h#L55-L60). Switch `RDMA_MESSAGE_ACK`  with `RDMA_MESSAGE_TENSOR_WRITE`. If indeed all message types are zero due to memory corruption, you should see all `RDMA_MESSAGE_TENSOR_WRITE` in your log. ", "@junshi15, Thanks for the explanation. I think memory corruption is confirmed. I switched the order of `RDMA_MESSAGE_ACK ` and `RDMA_MESSAGE_TENSOR_WRITE`. Now the first message received by worker 0 is indeed `RDMA_MESSAGE_TENSOR_WRITE`, which should be the `RDMA_MESSAGE_TENSOR_REQUEST` sent by PS. Worker 0 later failed at the CHECK [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/rdma.cc#L455). Need to figure out why memory is corrupted.", "@on-the-run Thanks for the info. I did not see any obvious reason. it may require a serious debug.", "Hi @on-the-run - can u print the buffer (not the completion buffer) ? \r\nI want to see if it's all 0's.\r\nI'm not sure why we don't see the issues u r seeing .. looks like something severe in your case.", "Hello @shamoya, I am working with @on-the-run on this issue. One important fact we observed is that with only one GPU per server, it will run without a problem. However, with increasing number of GPUs per server, the more likely it will get stuck. With eight GPU, it rarely runs.\r\n\r\nSince our logs show that 1. the (virtual) memory addresses are correctly mapped and exchanged, 2. the NIC reports RDMA WRITE is successful, 3. userspace sees the target buffer remains unchanged (we try initializing rx_message_buffer with 0 or different values, any initial values will remain unchanged), we suspect that the problem is because the mapping from virtual addr to physical addr has been changed.\r\n\r\nWe suspect that this is related to CUDA 8 and Pascal GPU, with which Nvidia has a new memory management feature (https://devblogs.nvidia.com/parallelforall/cuda-8-features-revealed/)\r\n\r\nThis is what may have happened in sequence (our speculation):\r\n1. ibv_reg_mr --> NIC remembers the physical addr\r\n2. CUDA maps the eight GPUs' memory to virtual addr. Something for some reason changes the userspace virtual addr to physical addr mapping. This thing, OS or CUDA, moves the contents and thinks the tensorflow process won't notice this change.\r\n3. RDMA transmission starts, NIC thinks it has written to the physical addr and reports success.\r\n4. However, when the tensorflow process checks the memory contents, the process is actually looking at a different physical addr due to step 2.\r\n\r\nOur current workaround is to call mlock() right after [malloc()](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/rdma.cc#L561).  It tells the OS to pin the physical memory. We have run our program successfully for a few times since the change. We'll continue checking whether it really solves the problem.\r\n\r\nI am not sure which is the right side to blame -- we don't expect CUDA to change the virtual addr mapping, however it seems that the change does happen, especially when CUDA works aggressively (with eight GPU). We expect ibv_reg_mr to pin the memory, however we searched through the ibv_reg_mr implementation in OFED and didn't see it doing that.\r\n\r\n\r\n", "Maybe I have some fundamentally wrong assumptions about how memory registration works in verbs, but shouldn't the IB driver pin the memory pages in kernel space, as verbs user space library will send a command in ibv_cmd_reg_mr?\r\n\r\nThe user space library is not supposed to pin the user pages in user space API, like mlock(2).", "@byronyi, If I am looking at the correct place, I didn't find any memory pinning done inside [`ibv_cmd_reg_mr()`](https://github.com/linux-rdma/rdma-core/blob/master/libibverbs/cmd.c#L358).\r\n\r\nAlso, looking at the manual of [ibv_reg_mr](https://linux.die.net/man/3/ibv_reg_mr), it doesn't mention memory pinning. It would be nice if they can mention whether they pin the memory in the manual.", "It's inside the providers directory, e.g. mlx4_reg_mr", "@byronyi, I believe both `mlx5_reg_mr()` and `mlx4_reg_mr()` call `ibv_cmd_reg_mr()`, but there seems no memory pinning done.", "@byronyi, Personally, I used to believe that too. However, @on-the-run and I both searched in the code and didn't see the memory pinning. In the past, I have done very heavy hacking in the verbs libs, as heavy as rewriting most of the verbs code. Still, I cannot find it. \r\n\r\nHowever, I am not sure what happens after ibv_cmd_reg_mr (I believe this is the last step in userspace. mlx4_* calls ibv_cmd_*) passes the cmd to underlying driver. Mellanox @shamoya should be able to tell us more about this.\r\n", "All the buffers are allocated in [system memory](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/rdma.cc#L296-L299), so CUDA8 should not play a role here. My impression is that `ibv_reg_mr` does pin the memory, although I am not familiar with the source code. @shamoya may be able to explain it.", "@junshi15 I understand that you only allocate main memory. Nevertheless, CUDA starts after your code, and with Pascal GPUs, it maps GPU memory to the same virtual memory address space as the process (see Unified Memory in https://devblogs.nvidia.com/parallelforall/cuda-8-features-revealed/). We do not know what it does with the existing virtual addr and physical addr mapping.\r\n\r\nOtherwise, I don't know how to explain why it works with one GPU, while starts to break with more GPUs..", "Maybe I didn't make it clear, I mean the verbs library is not doing nor supposed to do memory pinning in user space: that job is solely done by the in kernel part of your IB driver. Thus you should search it in the kernel source tree and kernel memory pinning API, something like `get_user_pages`, not the syscall nor user space API like `mlock`.", "@bobzhuyb And I think I have checked with TF people before, and they have made it pretty clear that TF is not designed to utilize UVM nor support this feature. See relevant discussion in [here](https://github.com/tensorflow/tensorflow/issues/3678#issuecomment-238064949). I also tend to believe neither IB driver or CUDA driver should alter memory mappings managed by the other one, as we are not doing peer memory access for NIC and GPU.\r\n\r\nOn the other hand I have worked with TF memory allocation subsystem and they are doing a pretty good job on keeping track of CUDA host/device memory allocation. See relevant details for [host](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/process_state.cc#L241) and [device](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/process_state.cc#L113) memory. And if you adjust those parameters (like the initial memory allocation or `allow_growth=False`), you could force it to allocate all CUDA memory at very early stage (before the whole distributed runtime is even initialized). After that, the virtual memory mappings of these memory regions should not be altered. If you do that, the step 2 of your speculation is rather unlikely to happen.\r\n\r\nBeing said that, the is a possibility that something is wrong in either CUDA or IB driver, but our chances are small. cc @zheng-xq @poxvoculi any comments on this?", "@on-the-run @bobzhuyb, the assumption doesn't make sense to me.\r\nThe memory is being pinned in https://github.com/torvalds/linux/blob/master/drivers/infiniband/core/umem.c#L83 , which is executed in the ibv_reg_mr flow, so mlcok shouldn't have any effect.\r\nAlso as @byronyi (this is what I know as well) said, TF doesn't support UVM.\r\nAre u sure this is the only thing u changed that fixed the bug ? \r\n\r\nI'm having a similar issue with 8 P100 per node (4 nodes), happens with only certain conditions (tf_cnn_script benchmark with \"real\" data) and the pinning issue is not the case.\r\nI think maybe found the issue (checking now and will report soon).", "@byronyi @shamoya Thanks for the clarification. Yes, mlock() was the thing that made it stable in our environment. Good to know that you can reproduce the issue. Please keep us posted.", "Hi @bobzhuyb, \r\nThe mlock solution still doesn't make sense to me.\r\nit shouldn't have any effect, unless you a severe driver issues.\r\nCan u involve RDMA community in this issue in https://github.com/linux-rdma/rdma-core ? \r\n\r\nAnyway my issue still not solved, I opened an issue in https://github.com/tensorflow/tensorflow/issues/11725.\r\n\r\nAny help is appreciated, if u can take a look.\r\n@junshi15 @byronyi if u have time I would be delighted as well.", "Hi @on-the-run,\r\nIs there any way to know if the CUDA remaps the memory mapping of the process ?\r\nAs said, I feel we see a similar issue, but the mlock workaround doesn't help in our case.\r\nWe also feel it's something to do with Pascal memory management (can't say exactly what).\r\nAny clues to what we can check ? \r\nThanks !", "I was using K80 and CUDA-7.5, and was able to reproduce the other issue (#11725). So at least that issue is not only limited to Pascal.", "We don't have a concrete evidence. However, we did a basic test -- when we malloc the receiver buffer, we init the buffer with a special value as a signature (say every byte initialized as 0xab). After the successful receive event triggered by WRITE_WITH_IMM, we immediate check the receiver buffer contents. It remains unchanged in TF process. \r\n\r\nIf you use ibdump at the receiver side, you should see the special signature in the packets. So the data must have arrived. Would you do the same as above to confirm that we are seeing the same (or similar) problem?\r\n\r\nThere are only two potential explanations, \r\n\r\n1. Receiver NIC didn't DMA the data into the receiver buffer -- we don't see why this may happen\r\n2. NIC did write the data to physical memory, but the vaddr mapping has changed\r\n\r\nTo get a concrete evidence, we will need to get the vaddr to physical addr mapping at different stages. Not sure whether the info in /proc/$PID/maps can help us.", "@shamoya, unfortunately I am not an expert on CUDA. Also, my colleague @bobzhuyb has taken this issue over. @bobzhuyb, could you give some input when you have time? Thanks!", "Hi @bobzhuyb, I made a separate RDMA patch #11392 that supports [checksum](https://github.com/tensorflow/tensorflow/pull/11392/commits/9bb2c8eda776f5a4a464cce56f61cbfcf33564a8#diff-baaf62ee09a0ec59d57ba078bc16c21bR445) with VLOG=2. It computes the checksum before sending and after receiving, and the checksum is transmitted out-of-band via gRPC. Maybe you could give it a try and see wether the problem persists.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Closed due to inactivity. Please reopen the issue if you have updates or resolved the issue. Thanks."]}, {"number": 11415, "title": "R1.2", "body": "", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "The version updates for 1.2.1 has already been merged into master: https://github.com/tensorflow/tensorflow/commit/e9f357b16b04ff17599b89a90498d64bc10c3c91"]}, {"number": 11414, "title": "TensorFlow needs a mascot", "body": "Every open source project deserves a mascot. Here's Teensy the TensorFlow Pony, and he's ready to serve:\r\n\r\n![img_20170710_073626](https://user-images.githubusercontent.com/161459/28030840-3a882240-655a-11e7-99ae-4469eec0a58a.jpg)\r\n", "comments": ["sure", "If only she could magically solve my installation problems..", "Awful :D", "The T logo reminds me of the tech museum logo. ", "@petewarden How about this?\r\n![unicorn-938646_1280](https://user-images.githubusercontent.com/17731159/30560602-4c7bad60-9c6d-11e7-813f-f40aa5f3ba03.jpg)\r\n", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rajatmonga: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 11413, "title": "Request genuine consecutive scheme batch generation for RNN Trainning", "body": "The concept of \u201cgenuine consecutive scheme \u201d can be seen at [here(5.4. Batch)](http://www.sciencedirect.com/science/article/pii/S088523081400093X?via%3Dihub).\r\n\r\nMy scenario is as follows:\r\n\r\nI have some files with different sequence lengths.\r\n\r\nFirst, do buckecting to generate file-batches with parameter `batch_size`\r\n\r\nThen, split each  file-batch with parameter `seq_len` to generate trainning sample-batches\r\n\r\nLast, use each sample-batch for one step of trainning.\r\n\r\nFollowing is my test code:\r\n\r\n```\r\n# -*- coding:utf8 -*-\r\n\r\nimport os\r\nimport time\r\nimport random\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.training import bucket_by_sequence_length, batch_sequences_with_states\r\n\r\n\r\ncontext_features = {\r\n    \"length\": tf.FixedLenFeature([], dtype=tf.int64)\r\n}\r\n\r\nsequence_features = {\r\n            \"inputs\": tf.FixedLenSequenceFeature([], dtype=tf.int64),\r\n}\r\n\r\ndef GenerateFakeData():\r\n    FILE_NUM = 100\r\n    DATA_PATH = \"test_dataset\"\r\n    file_path_list, file_len_list = [], []\r\n    for idx in range(FILE_NUM):\r\n        filename = \"{fileno}-of-{idx}\".format(idx=idx+1, fileno=FILE_NUM)\r\n        token_length = random.randint(50, 100)\r\n        ex = tf.train.SequenceExample()\r\n        ex.context.feature[\"length\"].int64_list.value.append(token_length)\r\n        ###########################################\r\n        ex_tokens = ex.feature_lists.feature_list[\"inputs\"]\r\n        for tok in range(token_length):\r\n            ex_tokens.feature.add().int64_list.value.append(tok)\r\n        with tf.python_io.TFRecordWriter(os.path.join(DATA_PATH, filename) + \".tfrecord\") as filew:\r\n            filew.write(ex.SerializeToString())\r\n        file_len_list.append(token_length)\r\n        file_path_list.append(os.path.join(DATA_PATH, filename) + \".tfrecord\")\r\n    with open(\"filelist.txt\", \"w\") as filew:\r\n        for file_name, file_len in zip(file_path_list, file_len_list):\r\n            filew.write(\"{fn}\\t{fl}\\n\".format(fn=os.path.join(file_name), fl=file_len))\r\n\r\ndef LoadFileList(filepath):\r\n    with open(filepath, \"r\") as filer:\r\n        wfilelist, wfilelengthlist = tuple(zip(*[tuple(line.strip().split(\"\\t\")) for line in filer if line.strip() != \"\"]))\r\n        return list(wfilelist), [int(item) for item in wfilelengthlist]\r\n\r\n        \r\ndef InputProducer():\r\n    batch_size = 2\r\n    seq_len = 75\r\n    state_size = 1024\r\n    bucket_boundaries = [60, 70, 80, 90]\r\n    #####################################\r\n    filelist, filelengthlist = LoadFileList(\"filelist.txt\")\r\n    #####################################\r\n    tf_file_queue = tf.train.string_input_producer(\r\n            string_tensor = filelist, \r\n            num_epochs = 1, \r\n            shuffle = False, \r\n            seed = None, \r\n            capacity = 32, \r\n            shared_name = None,\r\n            name = \"tf_file_queue\",\r\n            cancel_op=None\r\n    )\r\n    ######################################\r\n    tf_reader = tf.TFRecordReader()\r\n    tf_key, tf_serialized = tf_reader.read(tf_file_queue)\r\n    tf_context, tf_sequence = tf.parse_single_sequence_example(\r\n            serialized = tf_serialized,\r\n            context_features = context_features,\r\n            sequence_features = sequence_features\r\n    )\r\n    ######################################\r\n    tf_bucket_sequence_length, tf_bucket_outputs = bucket_by_sequence_length(\r\n        input_length = tf.cast(tf_context[\"length\"], dtype=tf.int32), \r\n        tensors = tf_sequence, \r\n        batch_size = batch_size, \r\n        bucket_boundaries = bucket_boundaries, \r\n        num_threads=1, \r\n        capacity=32, \r\n        shapes=None, \r\n        dynamic_pad=True,\r\n        allow_smaller_final_batch=False, \r\n        keep_input=True, \r\n        shared_name=None, \r\n        name=\"bucket_files\"\r\n    )\r\n    #######################################\r\n    tf_bbucket_outputs = {}\r\n    for fkey in tf_bucket_outputs:\r\n        tf_bbucket_outputs[fkey]=tf_bucket_outputs[fkey][0]\r\n    #######################################\r\n    # Solution 1:\r\n    tf_fb_key=time.strftime('%Y-%m-%d-%H-%M-%S',time.localtime(time.time())) + str(random.randint(1,100000000))\r\n    initial_state_values = tf.zeros((state_size,), dtype=tf.float32)\r\n    initial_states = {\"lstm_state\": initial_state_values}\r\n    tf_batch=batch_sequences_with_states(\r\n        input_key = tf_fb_key, \r\n        input_sequences = tf_bbucket_outputs, \r\n        input_context = {}, \r\n        input_length = tf.reduce_max(tf_bucket_sequence_length), \r\n        initial_states=initial_states, \r\n        num_unroll=seq_len, \r\n        batch_size=batch_size, \r\n        num_threads=3, \r\n        capacity=1000, \r\n        allow_small_batch=False, \r\n        pad=True, \r\n        name=None)\r\n    #######################################\r\n    # Solution 2:\r\n    '''\r\n    tf_index_queue=tf.train.range_input_producer(\r\n        limit=tf.reduce_max(tf_bucket_sequence_length),\r\n        num_epochs=1, \r\n        shuffle=False, \r\n        seed=None, \r\n        capacity=32, \r\n        shared_name=None, \r\n        name=None\r\n    )\r\n    tf_index=tf_index_queue.dequeue()\r\n    tf_batch=tf_bbucket_outputs[\"inputs\"][tf_index]\r\n    '''\r\n    #######################################\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        sess.run(tf.local_variables_initializer())\r\n        coord = tf.train.Coordinator()\r\n        threads = tf.train.start_queue_runners(sess, coord)\r\n        try:\r\n            while True:\r\n                #####################################\r\n                # Test Bucketing\r\n                #bucket_sequence_length, bucket_outputs = sess.run([tf_bucket_sequence_length, tf_bucket_outputs])\r\n                #print(bucket_sequence_length)\r\n                #print(bucket_outputs)\r\n                #print(\"#################\")\r\n                #####################################\r\n                # Test Solution 1:\r\n                batch = sess.run(tf_batch)\r\n                print(batch)\r\n                #####################################\r\n                # Test Solution 2:\r\n                #bucket_sequence_length, bucket_outputs, index = sess.run([tf_bucket_sequence_length, tf_bucket_outputs, tf_index])\r\n                #print(bucket_sequence_length)\r\n                #print(bucket_outputs)\r\n                #print(index)\r\n                #print(\"#################\")\r\n        except tf.errors.OutOfRangeError:\r\n            pass\r\n        except tf.errors.InvalidArgumentError:\r\n            pass\r\n        finally:\r\n            coord.request_stop()\r\n        coord.join(threads)\r\n    \r\nif __name__ == \"__main__\":\r\n    #GenerateFakeData()\r\n    InputProducer()\r\n    pass\r\n```\r\n\r\nWith Solution 1, I raised the error as below:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  \r\n    File \"/home/yangming/workspace/tfstudy-3.5.3-tf-1.1.0/BatchSchemas/make_test_dataset.py\", line 157, in <module>\r\n    \r\nInputProducer()\r\n  \r\n    File \"/home/yangming/workspace/tfstudy-3.5.3-tf-1.1.0/BatchSchemas/make_test_dataset.py\", line 107, in InputProducer\r\n    \r\nname=None)\r\n  \r\n    File \"/home/yangming/.pyenv/versions/tfstudy-3.5.3/lib/python3.5/site-packages/tensorflow/contrib/training/python/training/sequence_queueing_state_saver.py\", line 1522, in batch_sequences_with_states\r\n    \r\nallow_small_batch=allow_small_batch)\r\n  \r\n    File \"/home/yangming/.pyenv/versions/tfstudy-3.5.3/lib/python3.5/site-packages/tensorflow/contrib/training/python/training/sequence_queueing_state_saver.py\", line 849, in __init__\r\n    \r\ninitial_states)\r\n  \r\n    File \"/home/yangming/.pyenv/versions/tfstudy-3.5.3/lib/python3.5/site-packages/tensorflow/contrib/training/python/training/sequence_queueing_state_saver.py\", line 332, in _prepare_sequence_inputs\r\n    \r\n\"sequence\", inputs.sequences, ignore_first_dimension=True)\r\n  \r\n    File \"/home/yangming/.pyenv/versions/tfstudy-3.5.3/lib/python3.5/site-packages/tensorflow/contrib/training/python/training/sequence_queueing_state_saver.py\", line 326, in _assert_fully_defined\r\n    \r\nignore_first_dimension else \"\", v.get_shape()))\r\nValueError: Shape for sequence inputs is not fully defined (ignoring first dimension): (?, ?)\r\n```\r\n\r\nSee the document of `batch_sequences_with_states`, I found that \r\n\r\n1. it seems only support only one sequence and don't support multiple sequences .\r\n2. it don't support the situation of `Shape for sequence inputs is not fully defined`, which means `bucket_by_sequence_length` can not be followed with `batch_sequences_with_states`.\r\n\r\nWhat more, I have tried Solution 2, but I failed because of thread synchronization problem between `tf.train.string_input_producer` and `tf.train.range_input_producer`.\r\n\r\nSo, how to relize my request ?\r\n\r\nHope for your help.", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "@jart Thanks for your suggestion. I asked my question on [StackOverflow](https://stackoverflow.com/questions/45048035/how-to-realize-genuine-consecutive-scheme-batch-generation-with-tensorflow), and mailed to some experts. But get no replay after 14 days. Could you give me some suggestion about how to implement my requirement with TF. Thanks."]}, {"number": 11412, "title": "WAV encoding/decoding fixes.", "body": "Hi,\r\n\r\nThis PR contains two WAV related fixes.\r\n\r\nFixes WAV encoding/decoding for multi-channel data:\r\n* `DecodeLin16WaveAsFloatVector` was failing for multi-channel data with \u201cBad bytes per second\u201d due to it expecting the byte rate to be that for a single channel, whereas the header specifies the byte rate for _all_ channels.\r\n* `EncodeAudioAsS16LEWav` was incorrectly encoding the byte rate as that for a single channel instead of for _all_ channels.\r\n\r\nFixes shape function for the `DecodeWav` op:\r\n* The function was returning a non-ok status when either `desired_{samples,channels}` attributes were default; now, we use the unknown-dim instead.\r\n\r\nCheers.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please", "@rryan Thanks!\r\n\r\nI'm slowly working towards adding Python wrappers for the audio_ops (mentioned in #11339); these issues were identified as part of that work and I thought it best to fix them in isolation.", "Ahh sorry I haven't seen that issue yet. I'll comment on there."]}, {"number": 11411, "title": "Fetching data in Distributed Tensorflow has too much latency ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n\r\nhttps://gist.github.com/ahaider3/ae4f6d2d790d963a93b346bb0138a41d\r\n\r\nThe above is a simple benchmark which tests the overhead of distributed TF. It fetches a configurable sized variable from the parameter server and does a matmul on the worker. It also does a matmul from a locally stored variable on the worker. The time difference between these two operations would be the overhead I am measuring. \r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Redhat  \r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\ntensorflow 1.2 \r\n- **Python version**: \r\npython 2.7.7\r\n\r\n\r\n\r\n- **Exact command to reproduce**:\r\n\r\npython matmul_benchmark.py --num_features=256 --batch_size=128 --num_hidden=64 --job_name=ps &\r\n\r\npython matmul_benchmark.py --num_features=256 --batch_size=128 --num_hidden=64 --job_name=worker\r\n\r\nBy increasing batch_size, the timing difference between local/remote computation eventually becomes negligible. \r\nHowever, for small batch sizes the overhead can become 2x/3x:\r\n\r\nFor example, here are two runs for different model/dataset sizes:\r\n\r\n> 128 features, batch size of 32, hidden layer size of 256  returns:\r\nLocal GEMM Time:  0.0002624  Network Fetch GEMM Time: 0.0006798\r\n\r\n> 256 features, batch size of 128, hidden layer size of 128  returns:\r\nLocal GEMM Time: 0.0002995 Network Fetch GEMM Time: 0.0006124\r\n\r\n### Describe the problem\r\nDistributed tensorflow introduces overhead due to its communication stack. By overhead I mean the additional time required for workers to receive data from parameter servers when compared to doing the same computation without fetching any remote data. \r\n\r\nThis is a problem because due to this overhead I have to use 2/3 nodes to just provide performance on-par with non-distributed (single process) tensorflow. The number of nodes required to be on-par with single process TF increases further when I use gpus. \r\n\r\n\r\nFetching small variables provides a constant overhead which limits scaling and efficiency . \r\nThis overhead creates two issues in Distributed Tensorflow:\r\n1) I have to add several workers just to equal the performance of a single process. \r\n2) The overhead of fetching model parameters doesn't scale but the amount of computation does \r\ndecrease as I add more workers. Thus, once I get to a moderately small batch size for each worker I can't scale because the constant overhead of fetching remote model parameters. \r\n\r\nThere have been several issues posted with distributed tensorflow. #6116 is an improvement to large tensor transfer while this problem exists for small tensors. #4498 might have been caused by CPU performance bottleneck and not network. However for my problem, network transfer is definitely the bottleneck. I have tried using RDMA and have seen minimal benefit. \r\n\r\n### Source code / logs\r\nhttps://gist.github.com/ahaider3/ae4f6d2d790d963a93b346bb0138a41d\r\n\r\n\r\n\r\n", "comments": ["This is a known issue (#4498, #6116, #11196) with several workarounds:\r\n\r\n1. Wait until #7466 is merged, which gives roughly 2-3x speedup to gRPC large tensor transport.\r\n2. Use alternative communication protocol, such as `grpc+verbs` or `grpc+mpi`. This requires RDMA capable hardware.\r\n3. Refactor your code and try to use intra-process communication.", "I am mostly interested in the performance of small tensors. I have tried grpc+verbs and haven't seen any significant benefits. I will try with MPI. ", "I see both of your model size and batch size is small (128/256?). What's the time spent on computation each round before fetching model parameters from PS? If it only spends tens to hundreds of microseconds, I don't think the current distributed runtime could reach a performance on par to that of a single process, as there's a fixed overhead on setting up interprocess communication.", "Without fetching model parameters it only takes a few hundred microseconds. Yes, that's the problem I have. My compute time per iteration is low and so going to distributed requires double/triple the time per iteration due to the fixed overhead. \r\n\r\nI didn't see much benefit when I was testing with `grpc+verbs`, but that was with larger tensors. Is there a difference between `grpc+mpi` or `grpc+verbs` when compared to `grpc` besides the actual communication protocol? Are tensors still serialized in the same manner? The actual communication across the network is not the bottleneck from my testing. ", "If you are in doubt, you could try my own GPU Direct patch #11392 which is theoretically of the lowest latency. It does neither memory copy nor serialization. Btw if your computation is not that complicated, you should avoid using GPU; copying data from/to GPU adds nontrivial overhead in your communication pipeline.", "I'm also interested in this issue, with the application of training small feedforward nets (say 200-100-50-4) as quickly as possible. Note this only has ~100KB of parameters. I'd like to run on an algorithmic minibatch size of 8K, split up among as many workers as possible (so 8 workers means each handles 1K examples per iteration). As I add more workers, the batch size per worker gets smaller, which makes connection overhead more significant (per Amdahl's law).\r\n\r\nI'm working on training these on CPU, storing the variables on a parameter server. I've found the connection overhead destroys parallelism, as this issue points out. The particular problem here is not low throughput for transferring large parameters, it is high latency in moving even very small amounts of data. My guess is this comes from connection setup time, and that steps such as serialization and extra copies do not help. \r\n\r\nI have not tried ``gprc+verbs`` or ``grpc+mpi``, and I don't expect them to help significantly as my understanding is these protocols do the same connection establishment and then just move the tensors themselves (tiny amounts of data in my case!) through fast networking. Is this a valid understanding? Is there anywhere (docs or code) that I can learn more about network protocol involved in fetching tensors from parameter server?\r\n\r\nI implemented the same model with data parallelism using MPI (instead of TensorFlow's networking stack) to allreduce the gradients and found hugely better performance (running at 2 nodes results in a ~1.8x speedup instead of a slowdown).\r\n\r\n@byronyi Is their any way to avoid tensor serialization on CPU?", "@eamartin In my patch, tensors are transferred in terms of direct memory access (DMA) of the underlying tensor buffer, so there is no serialization nor memory copy. It is titled \"GPU Direct RDMA\", but certainly works for CPU as well. ", "Thanks for helping our friend @byronyi.", "Can we pause on closing this until the mentioned patch is actually merged? We have not yet confirmed that serialization or memcpy's caused the overhead. Other culprits could be multiple network roundtrips causing extra latency (and killing bandwidth in the small tensor limit).", "Take a look at #10530 and you may find something that interests you. \r\n\r\nLet me know if you have further questions; I am more than willing to help you out.", "I run the script provided by@yaroslavvb in #4498, result for `grpc` is \r\n```\r\nLocal rate:       15962.31 MB per second\r\nDistributed rate: 335.68 MB per second\r\n```\r\nand `grpc+verbs` is\r\n```\r\nLocal rate:       15514.35 MB per second\r\nDistributed rate: 1306.15 MB per second\r\n```\r\nI'm using a 56Gbps ib network.", "I just ran a [similar script](https://gist.github.com/shamoya/731a81a1fe3d12a2b098f8163eaab7dd/) in a 40Gbps RoCE setup for my GDR patch, and here's the result:\r\n\r\n```\r\nAdding data in 100 MB chunks\r\nLocal rate: 5243.48 MB per second\r\nDistributed rate: 2679.18 MB per second\r\n```\r\n\r\nNumbers for `grpc` and `grpc+verbs` are similar to what you got (~300 MB/s and ~1300MB/s).\r\n\r\nTo try out the result in your own environment, do change the host to one of the IB interface address you actually use, as my patch will not work for `localhost` or `127.0.0.1`.", "@suiyuan2009 I am not seeing that benefit from `grpc+rdma` on my ib network. Are your results with an updated patch? I am running with TF 1.2.0.\r\n\r\nHere are tests with different chunk sizes:\r\n\r\nFor 100 MB chunks with grpc:\r\n\r\n> Local rate:       19761.24 MB per second\r\n   Distributed rate: 339.71 MB per second\r\n\r\n\r\nFor 512 KB chunks with grpc:\r\n\r\n> Local rate:       3538.90 MB per second\r\n  Distributed rate: 616.97 MB per second\r\n\r\n\r\nSo decreasing the chunk size by `200x` only increases throughput by `2x`. I think the problem of large tensors is important, but small tensor transfer is also slow. ", "@byronyi Please be sure to add `Fixes #11411` to your commit message in #11392.", "@ahaider3 , I built from official master branch, I'll try @byronyi 's branch. I find there is not much difference between benchmark scripts which run distributed tensorflow on same machine or different machines. The script in #4498 performs bettern on different machines than on a single machine, weird.", "I find that when test with small data(10MB for example), `grpc+rdma` 's performance is very bad, speed decreases from `1300MB/s` to less than `1000MB/s`(300MB/s or 900MB/s, not stable).", "@ahaider3 @suiyuan2009 There is an important patch #10531 got merged after the 1.2 release, so it is expected that the current master is faster than 1.2 for `grpc+verbs` runtime.", "I find `assign` is much slow than `assign_add`, but I think `assign` should be as fast as `assign_add` at least.", "@suiyuan2009 \r\n\r\nI've noticed this issue, too. I'm testing my GDR patch using the [benchmark_grpc_recv.py](https://gist.github.com/yaroslavvb/e196107b5e0afc834652bd3153030c42) script (courtesy @yaroslavvb).\r\n\r\nFor `assign`, 3 measurements in a row:\r\n\r\n```\r\nLocal rate:       6944.22 MB/s\r\nDistributed rate: 2690.64 MB/s\r\n---\r\nLocal rate:       5084.81 MB/s\r\nDistributed rate: 2910.57 MB/s\r\n---\r\nLocal rate:       5864.24 MB/s\r\nDistributed rate: 2588.69 MB/s\r\n```\r\n\r\nFor `assign_add`:\r\n\r\n```\r\nLocal rate:       16558.85 MB/s\r\nDistributed rate: 3248.83 MB/s\r\n---\r\nLocal rate:       9952.02 MB/s\r\nDistributed rate: 3681.21 MB/s\r\n---\r\nLocal rate:       16090.50 MB/s\r\nDistributed rate: 3418.83 MB/s\r\n```\r\n\r\nBut the variance of these measurements (running on the same machine) seems to be just as large as the throughput gap. So I would rather not take it as a serious issue or a performance bug.", "@eamartin As GDR is available in current master #11392, would you mind to try again? It does no tensor serialisation nor memory copies for tensor data.\r\n\r\nSince all three extension protocol still use grpc heavily for the control plane, @ahaider3 you might not see much difference as your computation is way too fast compared to the fixed overhead of setting up each tensor transmission (it is rather a latency issue, not a throughput one). I personally would try to port grpc from HTTP2 to RDMA, but it will be a patch unlikely to be accepted by the grpc project as indicated [here](https://github.com/tensorflow/tensorflow/issues/2916#issuecomment-253924638).\r\n\r\n@jart Any comments?", "https://github.com/caffe2/caffe2/tree/master/caffe2/contrib/gloo\r\nhttps://github.com/facebookincubator/gloo#benchmarking", "I ran the tests again using TF 1.2 with the different extension protocols that I can run on my system. In my case, I can only use `grpc+verbs`.\r\n\r\nI found minimal to no benefit from using this protocol for my small model. I am measuring the throughput of fetching a parameter and then doing a GEMM. \r\n\r\n> 128 features; 32 batch size; 256 hidden layer size\r\ngrpc:  192.81 MB/sec\r\ngrpc+verbs: 179.18 MB/sec\r\n\r\nThis was the average of three runs. I would conclude that these protocols are limited by GRPC because of the latency it introduces, as suggested by @byronyi . \r\n\r\nThere has been interesting work by Uber to make communication all-around more efficient: https://github.com/uber/horovod\r\n\r\n@jart what do you think about uber's method for distributed training. ", "Horovod uses NCCL 2, which supports InfiniBand but not RoCE. See [here](https://github.com/uber/horovod/issues/5).", "I report the same problem as @ahaider3 that small mini-batch on small model is terrible for distributed training using cpu.\r\n\r\nWhen I train a deep&wide model with about 15 feas, embedding_size=32, I tried the 2/3/4 machine of ps+worker and ends up with the same speed as one machine of all in one process. \r\nOnly after increasing embedding_size to 256 give a 2x speed up with 4 machine.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jart: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jart: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I think we should close this issue as it appears to be resolved?", "Thanks for the tip. Closing now that PR is merged. https://github.com/tensorflow/tensorflow/pull/11392"]}, {"number": 11410, "title": "Fix minor docstring formatting issue", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please"]}, {"number": 11409, "title": "MPI path updates", "body": "This commit contains two updates:\r\n- Fixes a problem with uniquely identifying processes\r\n- Simplifies tensor receive call selection and prevents problems with too eagerly freeing resources", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please"]}, {"number": 11408, "title": "[XLA] Add filtering of tests for 3rd party devices on the python suite", "body": "Currently the tests are marked with specific exclusions for devices.\r\n\r\nFor 3rd party devices where we don't want their names and configs in the public repo, this change allows the devices to specify which tests they want to exclude.\r\n\r\nNo change for the CPU/GPU builtin devices.\r\n\r\n", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please", "no problem - will check this method out.  better to use an existing scheme where one exists", "There's a few things I don't get:\r\n\r\nhow do you pass the flag that names the manifest file into the build?  do you do a point edit to the build_defs.bzl file?\r\n\r\nit looks to me (without actually trying it though) that the manifest file is consumed at test run time, which means that it needs to get into the runfiles for each test.  how is this achieved?\r\n\r\n\r\ncheers\r\n", "@hawkinsp \r\n\r\nI think I have answered my questions.  It is possible that my scheme is a bit cleaner than the existing one.   Could they both co-exist?\r\n\r\n", "I'm on PR rotation this week, but as a passerby I think it makes sense to have one mechanism that works for everyone, so if this version is cleaner, it is worth porting that mechanism to use this one too.", "I would like to do something to consolidate the two schemes rather than having two ways to disable tests.\r\n\r\nThe main feature missing from the approach in the PR is the ability to disable individual tests. We use this fairly frequently if there is just one test in a suite that is broken for some reason. This does require some runtime reasoning --- it's not sufficient to disable at the build rule granularity.\r\n\r\nTo inject the extra manifest file into the tests, we have a local modification to the tf_xla_py_test() build rule. It's nothing fancy --- it just adds an extra argument to the py_test() targets it generates. Is there any reason we couldn't do something similar for your plugin tests --- have the plugin description point to a manifest file instead?\r\n", "ok - i'll have another, harder, look at it.\r\n\r\nwhen I looked at the existing scheme before, I made a change similar to the one you are probably referring to, where the plugin conf contains an entry for extra args (where you can push in the reference to the manifest file).   I couldn't see a clean way of inserting a dependency on the manifest file, so that it is pulled into the runfiles of the tests.   also - referring to files in the runfiles for a test seems to be quite ambiguous, as the CWD of the test isn't necessarily always the same (python tests and CC tests have a different CWD, I think)\r\n\r\n\r\n", "For reference here's what we do for an experimental internal backend.\r\n\r\nThe BUILD file in the directory containing the manifest has:\r\n\r\n```\r\nexports_files([\r\n    \"disabled_manifest.txt\",\r\n])\r\n```\r\n\r\nThe build_defs.bzl contains the following logic in `tf_xla_py_test` for the additional backend:\r\n```\r\n      backend_args += [\r\n          ...\r\n          \"--disabled_manifest=path/to/my/disabled_manifest.txt\",\r\n      ]\r\n      backend_data += [\"//path/to/my:disabled_manifest.txt\"]\r\n```\r\n\r\nThis only works for Python tests --- we have a separate mechanism for C++ tests. The C++ mechanism is also based around a manifest file, but we never bothered to opensource that code with the rest of XLA since it didn't seem useful to anyone else. If you would like, I can opensource that code.", "ok.  i didn't realise that I could dynamically add a dependency to a target in any dir (using backend_data).   i will change my code to look very similar to that (where the contents of the backend_args and backend_data are added from the plugin config.\r\n\r\ndon't worry about the C++ tests. i don't run them for my own backend right now because too many are failing.  I looked,briefly, at some of the failures.  I saw some 64 bit assumptions and some things that we don't support currently.  i'm not too worried at the moment.\r\n\r\n", "@vrv @hawkinsp \r\n\r\nI've created a different PR for a method that uses the disabled-manifest\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/11826"]}, {"number": 11407, "title": "how I upgrade my code(build on Tensorflow 1.0.1 python3) to compatible with TF1.2?", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/45007249/how-i-upgrade-my-codebuild-on-tensorflow-1-0-1-python3-to-compatible-with-tf1", "comments": ["Did you tried guidelines listed here [https://www.tensorflow.org/install/migration](https://www.tensorflow.org/install/migration) ?\r\n", "sure\uff0cbut I got a tough issue\r\n\r\nhttps://stackoverflow.com/questions/45015314/tensorflow1-2-caused-the-error-cant-pickle-thread-lock-objects-on-deepcopy-or\r\n", "Hi,\r\n      There is an existing thread for that here! I think it's an old issue started showing up again! See the [https://github.com/tensorflow/tensorflow/issues/11157](https://github.com/tensorflow/tensorflow/issues/11157)\r\n\r\n1. I encourage you to paste the code / online fiddle / repl link which you are trying to migrate?\r\n\r\n2. Also, you can search through `API r.1.2 ` for manually updating existing code base.\r\n\r\n3. `can't pickle _thread.lock objects` error can be experimented to solve with `multiprocessing, multi threading.`"]}, {"number": 11406, "title": "modify SaverDef default version with v2", "body": "SaverDef V1 has been deprecated, so modify default version with V2. ", "comments": ["Can one of the admins verify this patch?", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Can you rebase to master and then apply your changes rather than merging from master? Right now it shows that you are merging in a bunch of changes that are irrelevant to you."]}, {"number": 11405, "title": "Finding position of detected object", "body": "Is it possible to find the position of detected object with accuracy in given image using tensor flow?\ufeff\r\nI am using label_image.py to test my trained data how can I get the position of detected object as I am getting the accuracy of detected object?\r\n\r\nReference:\r\nhttps://stackoverflow.com/questions/44942587/save-image-of-detected-object-from-image-using-tensor-flow?noredirect=1#comment76982154_44942587", "comments": ["_Warning: As this doesn't appear to be a bug with Tensorflow, the devs may ask for this to be moved to Stack Overflow._\r\n\r\nHi @ManojPabani I am back from my break and have replied to you on the thread. As this isn't a bug with Tensorflow I suggest we continue the conversation on the existing thread!\r\n\r\nThanks", "Thanks for helping our friend @jubjamie. Please use StackOverflow for anything that isn't a bug or feature request in the future."]}, {"number": 11404, "title": "Build TF 1.2 error: no such package '@nccl_archive//' OR undeclared inclusion(s) in rule '@nccl_archive//:nccl': this rule is missing dependency declarations for the following files included by 'external/nccl_archive/src/libwrap.cu.cc':", "body": "Hi\uff0cI compiled tensorflow r1.0 successfully with Centos 7.0\uff0ccuda 8.0 and CUDNN 6.5\uff0cgcc 4.8.5.  Recently\uff0cWe want follow the latest version of Tensorflow\uff0cerror below are always occur:\r\n\r\n`ERROR: /home/jiangbo/tensorflow/tensorflow/tools/pip_package/BUILD:76:1: no such package '@nccl_archive//': /home/jiangbo/.cache/bazel/_bazel_jiangbo/0c80707bb0528f07a36b6bc1e1bf9b14/external/nccl_archive/build (Operation not permitted) and referenced by '//tensorflow/tools/pip_package:licenses'.\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.\r\n`\r\nMy bazel version have tried 0.4.5 and 0.5.2, but compile error.  Some method to solve this problem I have tried but no one helps , any body knows this problem, please tell me or let me know you thought.\r\n\r\nAfter I compiled TF r1.2 error, I have checkout to r1.0. it is weird, the same error info again.\r\n", "comments": ["Can you please post the entire build log up to the error? ", "I faced same issue, the full log is bellow:\r\nERROR: /root/.cache/bazel/_bazel_root/6093305914d4a581ed00c0f6c06f975b/external/nccl_archive/BUILD:39:1: undeclared inclusion(s) in rule '@nccl_archive//:nccl':\r\nthis rule is missing dependency declarations for the following files included by 'external/nccl_archive/src/libwrap.cu.cc':\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/limits.h'\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/syslimits.h'\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/stddef.h'\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/stdarg.h'.\r\nIn file included from /opt/cuda-8.0/bin/..//include/host_config.h:173:0,\r\n                 from /opt/cuda-8.0/bin/..//include/cuda_runtime.h:78,\r\n                 from <command-line>:0:\r\n/usr/include/features.h:330:4: warning: #warning _FORTIFY_SOURCE requires compiling with optimization (-O) [-Wcpp]\r\n  warning _FORTIFY_SOURCE requires compiling with optimization (-O)\r\n    ^\r\nIn file included from /opt/cuda-8.0/bin/..//include/host_config.h:173:0,\r\n                 from /opt/cuda-8.0/bin/..//include/cuda_runtime.h:78,\r\n                 from <command-line>:0:\r\n/usr/include/features.h:330:4: warning: #warning _FORTIFY_SOURCE requires compiling with optimization (-O) [-Wcpp]\r\n  warning _FORTIFY_SOURCE requires compiling with optimization (-O)\r\n    ^\r\nIn file included from /opt/cuda-8.0/bin/..//include/host_config.h:173:0,\r\n                 from /opt/cuda-8.0/bin/..//include/cuda_runtime.h:78,\r\n                 from <command-line>:0:\r\n/usr/include/features.h:330:4: warning: #warning _FORTIFY_SOURCE requires compiling with optimization (-O) [-Wcpp]\r\n  warning _FORTIFY_SOURCE requires compiling with optimization (-O)\r\n    ^\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!", "@av8ramit this seems overzealous by our bot -- to do this, we would first have to make sure we have the labels correct.\r\n\r\nCan you disable this behavior until we have the label state machine figured out?", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "@martinwicke disabling the close plugin and state machine changes are in flight", "Using latest TF version, and bazel version 0.8.1 or later, this problem seems to be gone. For more information see #14380"]}, {"number": 11403, "title": "Feature request: explicit state interfaces", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: source \r\n- **TensorFlow version (use command below)**: 1.2.0\r\n- **Python version**: 2.7.12\r\n \r\n### Description:\r\n\r\nThe current API is immensely stateful. Each function and object creation affects and is affected by countless mutable global states. Running a declaration of `Variable` or summary mutates the graph and collections and may return different results based on the current `NameScope` or `VariableScope` or `control_dependencies`. A function running twice can easily return wildly different result, even though it *looks* pure (but it isn't because almost no tensorflow function is pure). Declaring something and then deleting it won't restore the global state, as it may or may not be added somewhere in the graph or the collections or some other hidden objects. Reasoning about what changes what is incredibly difficult.\r\n\r\nWhat I would like to see is a API where all state mutations are explicit. When I declare a variable, it is just a variable, and not added to anything unless I call `add`. The current entangled interfaces can be built on top of that, preserving convenience functions such as `global_variable_initializers` for those who prefer.", "comments": ["I agree with you that APIs which are free of side effects are awesome. I'm not sure how we would do something like that with TensorFlow, since it would be quite a paradigm shift. I'm sure the original designers carefully considered these trade-offs of the current approach years ago. I'm going to close this one out but CC: @mrry in case he wants to weigh in on this feedback."]}, {"number": 11402, "title": "Fix typos", "body": "This PR fixes some typos: `Acccumulate`, `represnting`, and `implictly`.", "comments": ["Can one of the admins verify this patch?"]}, {"number": 11401, "title": "cuDNN version issues", "body": "I am trying to train a ConvNet on my Windows laptop and I got this error.\r\n\r\nLoaded runtime CuDNN library: 5005 (compatibility version 5000) but source was compiled with 5105 (compatibility version 5100).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.\r\n\r\nI am already using cudnn v5.1, so I have no clue why this is happening", "comments": ["What version of Tensorflow are you using? Did you build Tensorflow on the computer?", "Could you fill out the form in the new issue template? Here's a similar recent issue about cudnn: https://github.com/tensorflow/tensorflow/issues/11389", "Maybe there is a old cudnn.dll in your path. Run\r\n```where cudnn64_5.dll```\r\nif there are multiple dll's.\r\n", "There was a old cudnn.dll in my path! Thanks for the help @guschmue "]}, {"number": 11400, "title": "update folder link in docs", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please"]}, {"number": 11399, "title": "Let string_split support splitting utf-8 characters", "body": "### Describe the problem\r\n\r\nType of issue: feature request\r\n\r\nThe [string_split](https://www.tensorflow.org/versions/r0.12/api_docs/python/string_ops/splitting) function has (mostly) good behavior when splitting utf-8 strings by single-character delimiter, but fails to do it properly on null-width delimiter because of its documented behavior:\r\n\r\n> If delimiter is an empty string, each element of the source is split into individual strings, each containing one byte. (This includes splitting multibyte sequences of UTF-8.)\r\n\r\nFor models like seq2seq one needs a split function that can split utf-8 strings into individual characters that can be processed by model as units, also embeddings having properly of being easily joined as utf-8 strings.\r\n\r\nCould tensorflow provide alternative implementation of string_split that is utf-8 - aware?", "comments": ["@ysuematsu Here's a string_split bug / feature request for you.", "Any news on the subject ?", "Added a preliminary utf8 support for `string_split` in PR #12971.", "The PR #12971 is ongoing. Currently the Windows build fails. Don't have a Windows dev machine yet so not much progress but will try to get a Windows dev setup at some point.", "Assigning to @yongtang who has PR #12971 pending.", "Is anybody still working on this? ", "@TTrapper The PR #12971 is still ongoing and mostly complete. However, it does not pass the Windows test case. Unfortunately I don't have a Windows machine (and Visual Studio) and haven't been able to obtain one for sometime. As a result I am not able to debug the failed test case. I will try to see if I could do something about it.", "The PR #12971 is still pending. I was finally able to find a Windows VM image from Microsoft:\r\nhttps://developer.microsoft.com/en-us/windows/downloads/virtual-machines\r\n\r\nAnd it comes with Visual Studio 2017 already. However, when I tried to build TensorFlow with bazel on Windows, it just hangs at the dependency library stage. Will need more time to figure out if bazel is working on Windows or not.", "Thanks for raising this issue @sheerun . Did you find any workaround that you can share please?", "I don't rembember but I think I interlaced every character of input string with some character not present in string, and then used single character delimiter for splitting", "Thanks for the suggestion. I had to do exactly what you've suggested and was searching for a better way. ", "Closing this feature request, as `tf.string_split` has been deprecated in TensorFlow 2.0. The new module [tf.text](https://www.tensorflow.org/tutorials/tensorflow_text/intro) might be of interest, if you require text preprocessing operations."]}, {"number": 11398, "title": "slow cifar10_multi_gpu_train.py stock example and 'Ignoring device specification /device:GPU' warning", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no, using stock example scripts from cifar10\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.2 LTS inside singularity container using docker image tensorflow:latest-gpu\r\n- **TensorFlow installed from (source or binary)**: docker://tensorflow/tensorflow:latest-gpu (unmodified)\r\n- **TensorFlow version (use command below)**: \r\n\r\n```\r\nSingularity tensorflow.img:~> python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n('v1.2.0-5-g435cdfc', '1.2.1')\r\n```\r\n- **Python version**: Python 2.7.12\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: cuda 8\r\n- **GPU model and memory**: 2 x K80, 12GB each\r\n- **Exact command to reproduce**:\r\n\r\n```\r\n$ python $CIFAR10_DIR/cifar10_multi_gpu_train.py --num_gpus=2 \\\r\n                                                 --batch_size=64 \\\r\n                                                 --log_device_placement=false \\\r\n                                                 --max_steps=10000\r\n```\r\n\r\nAs a comparison, this one is faster:\r\n```\r\n$ python $CIFAR10_DIR/cifar10_train.py --batch_size=128 \\\r\n                                       --log_device_placement=false \\\r\n                                       --max_steps=10000\r\n```\r\n\r\n### Describe the problem\r\n\r\nWe have multiGPU systems (8 GPUs with P2P capability) and would like to take advantage of this for faster training but using the stock example cifar10, TensorFlow is even slower when using 2 GPUs than when using a single GPU (`cifar10_train.py`). I tried several batch sizes with no luck. The python process seems CPU bound when using 2 GPUs, so the GPU SMs are far from being busy (~20% usage).\r\n\r\nAlso, I can see the following warnings:\r\n```\r\n2017-07-09 18:44:15.097496: I tensorflow/core/common_runtime/simple_placer.cc:675] Ignoring device specification /device:GPU:1 for node 'tower_1/fifo_queue_Dequeue' because the input edge from 'prefetch_queue/fifo_queue' is a reference connection and already has a device field set to /device:CPU:0\r\n2017-07-09 18:44:15.097562: I tensorflow/core/common_runtime/simple_placer.cc:675] Ignoring device specification /device:GPU:0 for node 'tower_0/fifo_queue_Dequeue' because the input edge from 'prefetch_queue/fifo_queue' is a reference connection and already has a device field set to /device:CPU:0\r\n```\r\n\r\n\r\n### Source code / logs\r\n\r\nTrace from run:\r\n```\r\n2017-07-09 18:44:13.905522: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-07-09 18:44:13.905565: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-07-09 18:44:13.905573: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-07-09 18:44:14.479331: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties:\r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 0000:06:00.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.11GiB\r\n2017-07-09 18:44:14.821091: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x48f93a0 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.\r\n2017-07-09 18:44:14.823728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 1 with properties:\r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 0000:07:00.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.11GiB\r\n2017-07-09 18:44:14.827006: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 1\r\n2017-07-09 18:44:14.827019: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y Y\r\n2017-07-09 18:44:14.827039: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 1:   Y Y\r\n2017-07-09 18:44:14.827053: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:06:00.0)\r\n2017-07-09 18:44:14.827061: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla K80, pci bus id: 0000:07:00.0)\r\n2017-07-09 18:44:15.097496: I tensorflow/core/common_runtime/simple_placer.cc:675] Ignoring device specification /device:GPU:1 for node 'tower_1/fifo_queue_Dequeue' because the input edge from 'prefetch_queue/fifo_queue' is a reference connection and already has a device field set to /device:CPU:0\r\n2017-07-09 18:44:15.097562: I tensorflow/core/common_runtime/simple_placer.cc:675] Ignoring device specification /device:GPU:0 for node 'tower_0/fifo_queue_Dequeue' because the input edge from 'prefetch_queue/fifo_queue' is a reference connection and already has a device field set to /device:CPU:0\r\nFilling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\r\n2017-07-09 18:44:20.249834: step 0, loss = 4.68 (53.2 examples/sec; 2.407 sec/batch)\r\n2017-07-09 18:44:21.645904: step 10, loss = 4.62 (4843.2 examples/sec; 0.026 sec/batch)\r\n2017-07-09 18:44:22.487916: step 20, loss = 4.49 (2954.3 examples/sec; 0.043 sec/batch)\r\n2017-07-09 18:44:23.351235: step 30, loss = 4.30 (2900.9 examples/sec; 0.044 sec/batch)\r\n2017-07-09 18:44:24.264403: step 40, loss = 4.39 (2565.3 examples/sec; 0.050 sec/batch)\r\n```", "comments": ["Can you try to put `'num_gpus',2` manually in the file `cifar10_multi_gpu_train.py` found at ` line 59:` ?\r\n```\r\n\r\ntf.app.flags.DEFINE_integer('num_gpus', 2,\r\n--\r\n\u00a0 | \"\"\"How many GPUs to use.\"\"\")\r\n\r\n\r\n```", "I have the same problem, even after setting the DEFINE_integer flag manually.", "Did You tried various options to set GPU manually listed here ?[https://www.tensorflow.org/tutorials/using_gpu](https://www.tensorflow.org/tutorials/using_gpu) \r\n\r\nYou may try  in various setting of `Python 3.6, cuDNN, native installation for GPU` and see what errors you are getting?\r\n\r\nFor reference pasting an actual code involved behind your errors: \r\n\r\n` A non-primary context 0x48f93a0 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.\r\n`\r\n\r\nSource Code ,\r\n\r\n```\r\nformer_context = CUDADriver::CurrentContextOrDie();\r\n--\r\n\u00a0 | res = cuDevicePrimaryCtxRetain(&new_context, device);\r\n\u00a0 | if (former_context != nullptr) {\r\n\u00a0 | if (former_context == new_context) {\r\n\u00a0 | VLOG(2) << \"The primary context \" << former_context\r\n\u00a0 | << \" exists before initializing the StreamExecutor.\";\r\n\u00a0 | } else {\r\n\u00a0 | LOG(WARNING) << \"A non-primary context \" << former_context\r\n\u00a0 | << \" exists before initializing the StreamExecutor. We \"\r\n\u00a0 | \"haven't verified StreamExecutor works with that.\";\r\n\u00a0 | }\r\n\u00a0 | }\r\n\r\n```\r\n\r\nAnother error \r\n\r\n```\r\nIgnoring device specification /device:GPU:0 for node 'tower_0/fifo_queue_Dequeue' because the input edge from 'prefetch_queue/fifo_queue' is a reference connection and already has a device field set to /device:CPU:0\r\n```\r\n\r\nSource Code,\r\n\r\n```\r\nif (DeviceNameUtils::HasSomeDetails(source_parsed_name) &&\r\n--\r\n\u00a0 | DeviceNameUtils::HasSomeDetails(dest_parsed_name)) {\r\n\u00a0 | // Add a log saying that we are ignoring a specified device\r\n\u00a0 | // for 'dst' if the two names were incompatible.\r\n\u00a0 | if (!DeviceNameUtils::AreCompatibleDevNames(source_parsed_name,\r\n\u00a0 | dest_parsed_name)) {\r\n\u00a0 | LOG(INFO) << \"Ignoring device specification \"\r\n\u00a0 | << DeviceNameUtils::ParsedNameToString(dest_parsed_name)\r\n\u00a0 | << \" for node '\" << dst->name()\r\n\u00a0 | << \"' because the input edge from '\" << src->name()\r\n\u00a0 | << \"' is a reference connection and already has a device \"\r\n\u00a0 | \"field set to \"\r\n\u00a0 | << DeviceNameUtils::ParsedNameToString(source_parsed_name);\r\n\u00a0 | \u00a0\r\n\u00a0 | // Make 'dst' colocated with the source\r\n\u00a0 | dst_root.device_name = source_parsed_name;\r\n\u00a0 | }\r\n\r\n```\r\n", "I tried using Python 3 with docker://tensorflow/tensorflow:nightly-gpu-py3 but the Python version is only 3.5.2 there and the performance with 2 GPUs doesn't seem better than when using 1 GPU. I still have the following warning that you noted:\r\n```\r\n2017-07-18 22:27:02.253830: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x4eba8a0 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.\r\n```\r\nNote: I tried using Default and also Exclusive Process GPU compute modes.\r\nSo no luck so far...", "Hello everyone,\r\n\r\nRegarding the warning:\r\n`tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x4eba8a0 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.`\r\n\r\nI'm having the same warning with 2 GPUs, windows 10, python 2.5.3, all installed through Anaconda. I never thought such warning is problematic more than reporting something \"I don't understand\". I put a question regards it in [what-is-the-reason-of-getting-the-warning-havent-verified-streamexecutor](https://stackoverflow.com/questions/45760388/what-is-the-reason-of-getting-the-warning-havent-verified-streamexecutor). Still waiting.\r\n\r\nI am writing here just to share my similar experience, and hoping that if @thiell has already solved the issue and share his thoughts. \r\n\r\nBest wishes,\r\nAlderasus Ghaliamus", "@poxvoculi, are you able to take a look?", "I think there are two problems being reported, correct me if I misunderstand.\r\n\r\n1. The warning reported above by @alderasus and @thiel.  I see that a change was submitted to cuda_driver.cc on Aug 7 that changed this warning, claiming that the old one included false positive warnings.\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/c67e2c911c9a76942c25a76d37f0568a755010e6\r\n\r\nPerhaps you could retry with a more recent version of TF to see whether you're still getting a warning.\r\n\r\n2. thiel@ is experiencing slower performance with 2 GPUs than with 1 for cifar10_multi_gpu_train.py.  I'm not familiar with this program, but maybe @nealwu is.   Are you seeing both GPUs being used (e.g. using nvidia-smi, or looking at the device assignments in the logged .pbtxt showing the graph)?  If the python part of the process is indeed the bottleneck, can you tell which part, e.g. input preprocessing?\r\n", "@poxvoculi Thanks for looking at these issues. Both GPUs were used but SM usage% was about half lower than with a single GPU, making the whole thing slower I guess. Scaling across multiple GPUs is VERY important these days. I will give it a try if again I can find some time.", "Hi @thiell, I'd recommend taking a look at the [cifar10_estimator](https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10_estimator) example instead. The cifar10_multi_gpu_train.py example is out of date, and based on what I've heard from others, it may not work correctly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Closing this. We recommend looking at the newer [cifar10_estimator](https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10_estimator) example."]}, {"number": 11397, "title": "Third-party Scala API Links in the Documentation", "body": "I added a couple links to my Scala API repository in the documentation pages. I'm not sure if it's in the expected format, but feel free to rephrase if needed.\r\n\r\nIt would be great if we could also add a link in the following page (I could not find it in the documentation sources): https://www.tensorflow.org/api_docs/.\r\n\r\nThanks!\r\n\r\nEDIT: @asimshankar will take care of adding a link to https://www.tensorflow.org/api_docs/, so we're good with that one. Thanks! :)", "comments": ["Can one of the admins verify this patch?", "@asimshankar I made the changes you requested. Thanks for clarifying which part of the documentation refers to officially maintained libraries and third-party ones. :)", "Jenkins, test this please"]}, {"number": 11396, "title": "RecordInput blocks if buffer_size is larger than the amount of files in tfrecords.", "body": "If the `buffer_size` keyword argument in `data_flow_ops.RecordInput(file_pattern, .. buffer_size=buffer_size)` is larger than the amount of files inside of `file_pattern`, the op will block forever.\r\n\r\nThis is slightly related to #11372, another case where `RecordInput` blocks forever.\r\n\r\nNot sure how difficult/possible it would be to check for this or throw an error when this occurs. Feel free to mark this as closed if this is intended behaviour.", "comments": ["Should also be fixed internally.", "Nagging Assignee @ekelsen: It has been 437 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 11395, "title": "Gradients are not registered", "body": "(System information probably is not relevant to the issue, so I moved it below)\r\n\r\n## The Problem\r\n\r\nIt seems that the code, generated by macros `REGISTER_GRADIENT_OP` in [math_grad.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/cc/gradients/math_grad.cc) is never executed:\r\n`static bool unused_ret_val_123 = ::tensorflow::ops::GradOpRegistry::Global()->Register(name, fn);`\r\n\r\nMy `BUILD` file:\r\n```\r\ncc_binary(\r\n    name = \"bitwise_operations\",\r\n    srcs = [\r\n       \"main.cpp\"\r\n    ],\r\n    deps = [\r\n        \"//tensorflow/cc:grad_ops\",\r\n        \"//tensorflow/cc:cc_ops\",\r\n        \"//tensorflow/cc:client_session\",\r\n        \"//tensorflow/core:tensorflow\",\r\n        \"//tensorflow/cc/DimanNe/tensorflow_utils:tensorflow_utils\",\r\n    ],\r\n)\r\n```\r\n\r\n## The cause\r\n\r\n`Bazel` generates such `tensorflow/bazel-out/local-dbg/bin/tensorflow/cc/DimanNe/bitwise_operations/bitwise_operations-2.params` that instructs linker to put `math_grad.pic.o` in separate static library, here is how it looks in the params file containing linking instructions:\r\n```\r\n--start-lib\r\nbazel-out/local-dbg/bin/tensorflow/cc/_objs/real_math_grad/tensorflow/cc/gradients/real_math_grad.pic.o\r\n--end-lib\r\n```\r\nAnd [here1](https://www.google.ru/search?q=global+symbols+in+static+libraries&oq=global+symbols+in+static+libraries&aqs=chrome..69i57.7343j0j7&client=ubuntu&sourceid=chrome&ie=UTF-8#newwindow=1&q=global+initializer+in+static+libraries)/[here2](https://stackoverflow.com/questions/9459980/c-global-variable-not-initialized-when-linked-through-static-libraries-but-ok) you can find a lot of complaints about static global variables not being initialized, being linked as static libraries.\r\n\r\n\r\n## The solution\r\nAdd `alwayslink = 1,` to `math_grad` library in `tensorflow/cc/BUILD` (and actually any *_grad library, since all of them use the same mechanism of registration of gradients).\r\nExactly the same has already been done here - `tensorflow/core/BUILD`.\r\n\r\nIt the solution is OK, I can make a pull-request.\r\n\r\n## System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nKubuntu 17.04\r\n- **TensorFlow installed from (source or binary)**:\r\nCompiled from sources\r\n- **TensorFlow version (use command below)**:\r\n`remotes/origin/r1.2`\r\n- **Python version**: \r\nDo not use it\r\n- **Bazel version (if compiling from source)**:\r\n$ bazel version\r\nBuild label: 0.5.1\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Tue Jun 6 10:34:11 2017 (1496745251)\r\nBuild timestamp: 1496745251\r\nBuild timestamp as int: 1496745251\r\n- **CUDA/cuDNN version**:\r\nno cude\r\n- **GPU model and memory**:\r\n01:00.0 VGA compatible controller: NVIDIA Corporation GK106 [GeForce GTX 650 Ti Boost] (rev a1)\r\n- **Exact command to reproduce**:", "comments": ["Thanks for spotting this. Yes, a PR would be nice.", "Closing this issue as PR is in place"]}, {"number": 11394, "title": "fix broken links, add links check to sanity", "body": "", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please.", "some unrelated fails", "@tensorflow-jenkins test this please", "Jenkins, test this please", "There seems to be a newly introduced broken link in `export.md` and it is fixed now. Could any of the admin let Jenkins rerun the test?", "Jenkins, test this please"]}, {"number": 11393, "title": "fix broken link in adding_an_op.md", "body": "", "comments": ["Can one of the admins verify this patch?", "Close due to https://github.com/tensorflow/tensorflow/pull/11394"]}]