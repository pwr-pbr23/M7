[{"number": 53242, "title": "Perfect shuffle without buffer of size len(dataset)", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.7.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThe new feature will allow perfect shuffle without using a buffer of size len(dataset). When using tf.data.Dataset object for data pipeline, to faster the data process during model training, we use tf.data.Dataset.cache method. But, with large dataset or/and limited RAM capacity, we cannot use perfect shuffle method because it need the same amount of RAM capacity and can cause an Out of Memory error. \r\n\r\n***Exemple of actual behaviour***\r\n\r\n```python\r\nds = tf.data.Dataset.range(100000)  # large dataset\r\nds = ds.map(SomeTransformation)\r\nds = ds.cache() # RAM is 70% use\r\nds = ds.map(SomeTransformationThatUseRandomBehavior)\r\nds = ds.shuffle(buffer_size=len(ds))  # (Perfect) shuffle cannot be done because of an OOM Error \r\n\r\n...\r\n\r\nmodel.fit(ds)\r\n```\r\n\r\n***Exemple of the new feature***\r\n\r\n```python\r\nds = tf.data.Dataset.range(100000)  # large dataset\r\nds = ds.map(SomeTransformation)\r\nds = ds.cache() # RAM is 70% use\r\nds = ds.map(SomeTransformationThatUseRandomBehavior)\r\nds = ds.shuffle(buffer_size=None)  # (Perfect) shuffle will not cause an OOM Error because \r\n                                   # it will no longer need a buffer to work\r\n\r\n...\r\n\r\nmodel.fit(ds)\r\n```\r\n\r\n***Current solution but not perfect***\r\n\r\n```python\r\nds = tf.data.Dataset.range(100000)  # large dataset\r\nds = ds.map(SomeTransformation)\r\nds = ds.cache(\"path/to/file\") # RAM is no longer use\r\nds = ds.map(SomeTransformationThatUseRandomBehavior)\r\nds = ds.shuffle(buffer_size=len(ds))  # (Perfect) shuffle work but take more time\r\n                                      # to read from a cache file than read from a cache  in RAM\r\n\r\n...\r\n\r\nmodel.fit(ds)\r\n```\r\n\r\n**Will this change the current api? How?**\r\ntf.data.Dataset.shuffle now allow buffer_size to be set to None\r\n\r\n**Who will benefit with this feature?**\r\n Anyone who are using large datasets and those who are limited with RAM capacity (can only fit once the data in RAM)\r\n\r\n**Other Info**\r\nAn another shuffle option, if perfect shuffle cannot be done due to some technical constraint, can be to randomly select an element of the dataset. that's mean 2 elements can be draw. ", "comments": ["@DUPASYoann,\r\n\r\nCan you please elaborate you feature request by adding more details and also can you provide an example if possible? Thanks!", "> @sanatmpa1,\r\n> \r\n> Can you please elaborate you feature request by adding more details and also can you provide an example if possible? Thanks!\r\n\r\nEdit done !", "It is normally suggested not to use `cache` for large dataset since it occupies your RAM usage, you can still proceed without using `cache` and then use shuffle which can be performed since your RAM usage will be less.\r\nIf you are concerned about the speed, you can do it in other ways, either you can shuffle the dataset at source(OS level) and then create tf.data and cache or to shuffle in other ways you can split the source data into multiple files and read them in the random order during training using `tf.data.Dataset.list_files`", "> It is normally suggested not to use cache for large dataset since it occupies your RAM usage, you can still proceed without using cache and then use shuffle which can be performed since your RAM usage will be less.\r\n\r\nYeah, I agree, and that what I do with `tf.data.Dataset.cache(filename=filepath)`. In this way, i can use perfect `shuffle` with `reshuffle_each_iteration=True` since cache is no longer on RAM. But perfect `shuffle` can be only ensure if you can store all your dataset in RAM since you have to set the shuffle buffer to len(dataset). \r\n\r\n> you can shuffle the dataset at source(OS level) and then create tf.data and cache\r\n\r\nFor me it's not an ideal solution, because when you use `cache` operation on your dataset, the order will be \"frozen\" and so you will only benefit the shuffle once. (When i say the order will be \"frozen\", I mean that in every epoch (If I consider one epoch equal one entire loop over my dataset) I will have the same order).\r\n\r\n> split the source data into multiple files and read them in the random order during training using tf.data.Dataset.list_files\r\n\r\nThis is the best solution right now. But consider this case : \r\n\r\n#### Actual situation\r\n\r\n```python\r\nds = tf.data.Dataset.list_files(\"path/to/files\")  # few elements but each element are large image\r\nds = ds.map(SomeTransformation)\r\nds = ds.cache() # cache the dataset because the transformation before take long time \r\n                # and we don't have to do it multiple time since the result are the same\r\nds = ds.map(SomeTransformationThatUseRandomBehaviorLikeRandomCropOnImage)\r\nds = ds.shuffle(buffer_size=len(ds))  # Perfect shuffle can only be ensure if we have enough RAM to store all elements\r\n\r\n...\r\n\r\nmodel.fit(ds) # It's important that we have random order every epoch\r\n```\r\n\r\n#### Solution when RAM is not enough\r\n\r\n```python \r\nds = tf.data.Dataset.list_files(\"path/to/files\")  # few elements but elements are large image\r\nds = ds.map(SomeTransformation)\r\n\r\nfor data in ds:\r\n    pickle.dump(data, \"path/for/element{i}\") # cache data with our own method \r\n\r\nnew_ds = tf.data.Dataset.list_files(\"path/off/all/elements/*\", shuffle=True) # we don't need to use tf.data.Dataset.shuffle anymore\r\nnew_ds = new_ds.map(lambda x: tf.numpy_function(read_data_with_pickle_load, [x], [ds.elements_spec.dtype])\r\nnew_ds = new_ds.map(SomeTransformationThatUseRandomBehaviorLikeRandomCropOnImage)\r\n\r\n\r\n...\r\n\r\nmodel.fit(ds) # We still have a random order every epoch\r\n```\r\n\r\nOn further consideration, the new feature can be a shuffle argument on `tf.data.Dataset.cache` like the argument on `tf.data.Dataset.list_files`. When True, It would allow random shuffle of the cached dataset (file or RAM). \r\n\r\n#### With the new feature\r\n\r\n```python\r\nds = tf.data.Dataset.list_files(\"path/to/files\")  # few elements but elements are large image\r\nds = ds.map(SomeTransformation)\r\nds = ds.cache(shuffle=True) # we can now easily have perfect shuffle, and if the dataset is cached in RAM,\r\n                            # it's now only store once\r\nds = ds.map(SomeTransformationThatUseRandomBehaviorLikeRandomCropOnImage)\r\n\r\n...\r\n\r\nmodel.fit(ds) # we would still have our random order every epoch\r\n\u0300```\r\n"]}, {"number": 53238, "title": "nasm Error when compilig for other x86_64 architecture.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution: Ubuntu 21.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: master\r\n- Python version: 3.9\r\n- Installed using virtualenv? pip? conda?: no\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 8\r\n- CUDA/cuDNN version: no\r\n- GPU model and memory: no\r\n- Host processor architecture: Coffee Lake  (i7-8750H)\r\n- Destination architecture: Ice Lake\r\n\r\n\r\n**Describe the problem**\r\nWhen trying to compile TF for Ice Lake, I got the following error when in the `libjpeg_turbo` dependency:\r\n\r\n```\r\n/bin/bash: line 1:  1429 Illegal instruction     (core dumped) bazel-out/k8-opt/bin/external/nasm/nasm -f elf64 -DELF -DPIC -D__x86_64__ -I $(dirname bazel-out/k8-opt/bin/external/libjpeg_turbo/jconfig.h)/ -I $(dirname bazel-out/k8-opt/bin/external/libjpeg_turbo/jconfigint.h)/ -I $(dirname external/libjpeg_turbo/simd/nasm/jsimdcfg.inc.h)/ -I $(dirname external/libjpeg_turbo/simd/x86_64/jccolext-sse2.asm)/ -o $out $(dirname external/libjpeg_turbo/simd/x86_64/jccolext-sse2.asm)/$(basename ${out%.o}.asm)\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n - Clone this repository. I am using master branch\r\n - Set the following env vars:\r\n ```bash\r\nexport BAZEL_VERSION=\r\nexport BUILD_AVX2_CONTAINERS=no\r\nexport BUILD_AVX_CONTAINERS=no\r\nexport BUILD_CLX_CONTAINERS=no\r\nexport BUILD_ICX_CLIENT_CONTAINERS=no\r\nexport BUILD_ICX_SERVER_CONTAINERS=yes\r\nexport BUILD_PY2_CONTAINERS=no\r\nexport BUILD_SKX_CONTAINERS=no\r\nexport BUILD_SSH=no\r\n\r\nexport BUILD_TF_BFLOAT16_CONTAINERS=no\r\nexport BUILD_TF_V2_CONTAINERS=yes\r\n\r\nexport ENABLE_DNNL1=no\r\nexport ENABLE_GCC8=no\r\nexport ENABLE_HOROVOD=no\r\nexport ENABLE_SECURE_BUILD=yes\r\n\r\nexport FINAL_IMAGE_NAME=test/optimized-tensorflow\r\n\r\nexport OPENMPI_DOWNLOAD_URL=\r\nexport OPENMPI_VERSION=\r\nexport HOROVOD_VERSION=\r\nexport INSTALL_HOROVOD_FROM_COMMIT=no\r\nexport IS_NIGHTLY=no\r\nexport RELEASE_CONTAINER=no\r\n\r\nexport TF_BUILD_VERSION_IS_PR=no\r\nexport TF_BUILD_VERSION=master\r\n\r\nexport TF_DOCKER_BUILD_DEVEL_BRANCH_IS_PR=no\r\nexport TF_DOCKER_BUILD_DEVEL_BRANCH=master\r\n\r\nexport TF_REPO=https://github.com/tensorflow/tensorflow\r\n\r\nexport ROOT_CONTAINER_TAG=latest-python3.9\r\nexport ROOT_CONTAINER=tensorflow/build\r\n```\r\n- go to directory `master/tensorflow/tools/ci_build/linux/mkl`\r\n- run: `build-dev-container.sh`\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached\r\n\r\n[compilation.log](https://github.com/tensorflow/tensorflow/files/7620570/compilation.log)\r\n\r\nI also changed the bazel execution line in the dockerfile to limit the number of jobs and provide more info when faling:\r\n```diff\r\n$ git diff Dockerfile.devel-mkl\r\ndiff --git a/tensorflow/tools/ci_build/linux/mkl/Dockerfile.devel-mkl b/tensorflow/tools/ci_build/linux/mkl/Dockerfile.devel-mkl\r\nindex 55f68a73..d1c3a35f 100755\r\n--- a/tensorflow/tools/ci_build/linux/mkl/Dockerfile.devel-mkl\r\n+++ b/tensorflow/tools/ci_build/linux/mkl/Dockerfile.devel-mkl\r\n@@ -68,7 +68,7 @@ RUN echo \"import /root/.mkl.bazelrc\" >>/root/.bazelrc\r\n # Install futures>=0.17.1 for Python2.7 compatibility mode\r\n RUN ${PIP} install future>=0.17.1\r\n \r\n-RUN bazel --bazelrc=/root/.bazelrc build -c opt \\\r\n+RUN bazel --bazelrc=/root/.bazelrc build -c opt --jobs=4 --verbose_failures \\\r\n     tensorflow/tools/pip_package:build_pip_package && \\\r\n     bazel-bin/tensorflow/tools/pip_package/build_pip_package \"${TF_NIGHTLY_FLAG}\" \"${WHL_DIR}\" && \\\r\n     ${PIP} --no-cache-dir install --upgrade \"${WHL_DIR}\"/*.whl && \\\r\n```\r\n\r\nI am not sure if this is the way of doing this kind of compilation of if I need to compile it in a Icelake machine.\r\n\r\n", "comments": ["Please use [TF SIG Build's Docker images](https://github.com/tensorflow/build/tree/master/tf_sig_build_dockerfiles).", "Hi @learning-to-play ,\r\n  Could you elaborate a little bit more ? As far as I can see, the `build-container-dev.sh` is running a `docker build ... -f Dockerfile.devel-mkl`, and in this dockerfile you can see: `FROM ${ROOT_CONTAINER}:${ROOT_CONTAINER_TAG}`, and those environment variables are set to: \r\n```\r\nexport ROOT_CONTAINER_TAG=latest-python3.9\r\nexport ROOT_CONTAINER=tensorflow/build\r\n```\r\n  So it is using the images indicated in the link you provided.", "I found the issue.\r\n\r\nIt was that nasm was compiled with AVX instructions (`BUILD_ICX_SERVER_CONTAINERS=yes`) and the machine where I was building TF has not those instructions.\r\n\r\nIt would be nice if the intermediate tooling used to build TF can be excluded of using those flags and only the final \"product\" is compiled using them. This way it would be possible to compile on any processor generation.\r\n\r\n"]}, {"number": 53236, "title": "Weight Norm - ConverterError: input resource[0] expected type resource != bool", "body": "Weight Norm - ConverterError: input resource[0] expected type resource != bool\r\n### System information\r\n\r\n-   Operating System: **Linux Ubuntu 20.04**\r\n-   TF Version: **2.3, 2.6 or nightly**\r\n-   Python Version: **3.6**\r\n\r\n\r\nI'm trying to convert to TFLite a model with weight normalization. However, an error is thrown during conversion with both TOCO or MLIR.\r\n\r\nModel:\r\n```\r\ndef conv_bn_relu(x, filters, kernel_size, strides, name, dilation_rate=(1, 1), use_bn=True, use_relu=True, use_wn=False, use_leakyrelu=False, padding = 'same'):\r\n    #print('conv_bn_relu')\r\n    if use_wn:\r\n        x = WeightNormalization(Conv2D(filters=filters, \r\n                                       kernel_size=kernel_size, \r\n                                       strides=strides, \r\n                                       padding=padding, \r\n                                       dilation_rate=dilation_rate,\r\n                                       name=name), data_init=False)(x)\r\n    else:\r\n        x = Conv2D(filters=filters, \r\n                   kernel_size=kernel_size, \r\n                   strides=strides, \r\n                   padding=padding, \r\n                   dilation_rate=dilation_rate,\r\n                   name=name)(x)\r\n\r\n    if use_bn:\r\n        x = BatchNormalization(name='%s_bn' % (name))(x)        \r\n    if use_relu:\r\n        x = Activation('relu', name='%s_relu' % (name))(x)\r\n    \r\n    \r\n    if use_leakyrelu:\r\n        x = tf.keras.layers.LeakyReLU(alpha=0.2, name='%s_leakyrelu' % (name))(x)\r\n\r\n    return x\r\n```\r\n\r\nConversion snippet:\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(generator_model)\r\ntflite_model = converter.convert()\r\nopen(_MODEL_FILENAME_, \"wb\").write(tflite_model)\r\n```\r\n\r\nError thrown:\r\n```\r\n~/anaconda3/envs/tf23fix/lib/python3.7/site-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    200       return model_str\r\n    201     except Exception as e:\r\n--> 202       raise ConverterError(str(e))\r\n    203 \r\n    204   if distutils.spawn.find_executable(_toco_from_proto_bin) is None:\r\n\r\nConverterError: input resource[0] expected type resource != bool, the type of functional_3_weight_normalization_cond_assert_equal_1_readvariableop_resource[0]\r\n\tIn {{node functional_3/weight_normalization/cond/AssignVariableOp}}\r\n\r\n```", "comments": ["Can you share full example to reproduce the issue ?\r\n\r\nThanks"]}, {"number": 53234, "title": "Windows heap corruption caused by upgrading protobuf", "body": "### System information\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): \r\n     Windows\r\n\r\n- TensorFlow version:\r\n   At https://github.com/tensorflow/tensorflow/pull/52853\r\n\r\n- Bazel version (if compiling from source):\r\n  3.7.2\r\n- GCC/Compiler version (if compiling from source):\r\n  MSVC\r\n\r\n### Describe the problem\r\nThis is a summary of the Windows heap corruption bug discovered when upgrading protobuf in https://github.com/tensorflow/tensorflow/pull/52853#issuecomment-963081770.\r\n\r\nIn the PR, we tried to upgrade protobuf from 3.9.2 to 3.19.0, but some tf optimizer related tests are failing on Windows with\r\n```\r\nWindows fatal exception: code 0xc0000374\r\n```\r\nThe error code indicates a heap corruption. After some investigation, we discovered the problem is caused by:\r\n- How protobuf is linked in TensorFlow on Windows\r\n   On Windows, most of TF C++ code is linked into `_pywrap_tensorflow_internal.pyd` and some extension code is linked into individual pyd files, like `_pywrap_tf_optimizer.pyd`. The later is also linked to the former. However, the protobuf library is statically linked into both dynamic libraries.\r\n- How protobuf works\r\n  Basically, protobuf [deletes some global string in a proto desctructor](https://cs.opensource.google/protobuf/protobuf/+/master:src/google/protobuf/arenastring.h;l=402) after comparing the address of [some global default string](https://cs.opensource.google/protobuf/protobuf/+/master:src/google/protobuf/message_lite.h;l=135). When there are two protobuf runtimes, there are two default strings with different addresses, which caused some memory are accidentally deleted when mixed together.\r\n\r\n @acozzette also explained why protobuf doesn't work well when linked in multiple places at https://github.com/tensorflow/tensorflow/pull/52853#issuecomment-973138129\r\n\r\n\r\n### Provide the exact sequence of commands / steps that you executed before running into the problem\r\n\r\nTo reproduce the original error in a full build:\r\n```\r\ngit clone https://github.com/meteorcloudy/tensorflow.git\r\ncd tensorflow\r\ngit fetch origin upgrade_protobuf_grpc\r\nconfigure\r\nbazel test --announce_rc --config=opt tensorflow/python/grappler:memory_optimizer_test --test_arg=MemoryOptimizerSwapTest.testNoSwapping\r\n```\r\n\r\nHowever, the full TF build isn't debuggable, I have constructed [some smaller targets](https://github.com/meteorcloudy/tensorflow/commit/55263135a5b5f71cc254838d6dd97852fb8758a9) to imitate the situation.\r\nTo build the minimal reproduce case in a debug build:\r\n```\r\ngit clone https://github.com/meteorcloudy/tensorflow.git\r\ncd tensorflow\r\ngit fetch origin reproduce_win_heap_corruption_2\r\nconfigure\r\nbazel run --announce_rc --config=dbg tensorflow/python/grappler:tf_optimizer_wrapper_bin\r\n```\r\n\r\nWhen debugging in Visual Studio, you should be able to see the following:\r\n![image](https://user-images.githubusercontent.com/4171702/143913840-08256bf0-c657-4c74-b5f6-1424b8568b9b.png)\r\n\r\n### Possible Solution\r\n\r\nTo properly solve this problem, we probably need to migrate TF to [cc_shared_library](https://github.com/bazelbuild/rules_cc/blob/main/examples/experimental_cc_shared_library.bzl), which makes dynamic linking more controllable.\r\n", "comments": ["FYI @r4nt @mihaimaruseac @learning-to-play ", "Hi @sanatmpa1 ! Could you please look at  this Issue?"]}, {"number": 53212, "title": "@org_tensorflow//tensorflow/lite/toco:toco build error", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: Tensorflow2.6.0\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source):  \r\n- CUDA/cuDNN version: CUDA Version 10.0.130,CUDNN_MAJOR7.4.2\r\n**I am trying to build the tensorflow code, When I execute this command\uff1a**\r\n\r\n> bazel build -s --cxxopt=\"-std=c++14\" -c dbg --cxxopt=-msse4 @org_tensorflow//tensorflow/lite/toco:toco --experimental_repo_remote_exec\r\n\r\n**error logs**\r\nERROR: /home/shiyalun/.cache/bazel/_bazel_shiyalun/648af57cc02a336677529154c4ac62f6/external/org_tensorflow/tensorflow/core/platform/default/BUILD:273:11: Compiling tensorflow/core/platform/default/port.cc [for host] failed: (Exit 1): gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 53 argument(s) skipped)\r\n\r\nUse --sandbox_debug to see verbose messages from the sandbox gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 53 argument(s) skipped)\r\n\r\nUse --sandbox_debug to see verbose messages from the sandbox\r\nexternal/org_tensorflow/tensorflow/core/platform/default/port.cc: In function 'tensorflow::port::MemoryInfo tensorflow::port::GetMemoryInfo()':\r\nexternal/org_tensorflow/tensorflow/core/platform/default/port.cc:360:46: error: could not convert '{9223372036854775807l, 9223372036854775807l}' from '<brace-enclosed initializer list>' to 'tensorflow::port::MemoryInfo'\r\n   MemoryInfo mem_info = {INT64_MAX, INT64_MAX};\r\n                                              ^\r\nexternal/org_tensorflow/tensorflow/core/platform/default/port.cc: In function 'tensorflow::port::MemoryBandwidthInfo tensorflow::port::GetMemoryBandwidthInfo()':\r\nexternal/org_tensorflow/tensorflow/core/platform/default/port.cc:373:46: error: could not convert '{9223372036854775807l}' from '<brace-enclosed initializer list>' to 'tensorflow::port::MemoryBandwidthInfo'\r\n   MemoryBandwidthInfo membw_info = {INT64_MAX};\r\n                                              ^\r\nTarget @org_tensorflow//tensorflow/lite/toco:toco failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /home/shiyalun/.cache/bazel/_bazel_shiyalun/648af57cc02a336677529154c4ac62f6/external/org_tensorflow/tensorflow/core/framework/BUILD:1279:31 Middleman _middlemen/@org_Utensorflow_S_Stensorflow_Score_Sframework_Cattr_Uvalue_Uproto_Utext-BazelCppSemantics_build_arch_k8-dbg failed: (Exit 1): gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 53 argument(s) skipped)\r\n\r\nUse --sandbox_debug to see verbose messages from the sandbox gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 53 argument(s) skipped)\r\n\r\nUse --sandbox_debug to see verbose messages from the sandbox\r\nINFO: Elapsed time: 349.821s, Critical Path: 309.18s\r\nINFO: 108 processes: 30 internal, 78 linux-sandbox.\r\nFAILED: Build did NOT complete successfully", "comments": ["Can anyone help me  analysis it? Thanks"]}, {"number": 53209, "title": "GPU with OpenGL implementation for Android", "body": "Hi! I am trying to implement processing from the camera using only GPU with OpenGL. I've built libtensorflowlite_gpu_gl.so\r\n\r\nI've bound buffer like this:\r\n`const void *model_data =\r\nstatic_cast<void *>(env->GetDirectBufferAddress(model_buffer));\r\njlong model_size = env->GetDirectBufferCapacity(model_buffer);\r\n\r\nTfLiteModel *model_ = TfLiteModelCreate(model_data, model_size);\r\nTfLiteInterpreterOptions *options_ = TfLiteInterpreterOptionsCreate();\r\nauto *options = new TfLiteGpuDelegateOptions ();\r\nauto delegate_ = TfLiteGpuDelegateCreate(options);\r\nTfLiteGpuDelegateBindBufferToTensor(delegate_, cam_ssbo_id, 0);\r\nTfLiteInterpreterOptionsAddDelegate(options_, delegate_ );\r\n\r\ninterpreter_ = TfLiteInterpreterCreate(model_, options_);\r\n\r\n\r\n\r\nTfLiteStatus status = TfLiteInterpreterAllocateTensors(interpreter_);`\r\nand then on a new frame, I call\r\nTfLiteStatus status = TfLiteInterpreterInvoke(interpreter_);\r\nIt is working. I am receiving the expected result. But it seems like the invoke is working on CPU instead of GPU. It also freezes the main thread (I am calling all of these methods in a separate thread), and the processing is too slow.\r\nI have tried processing on GPU for my model using standard GPU Delegate it was working fine and two times faster. I was expecting that TfLiteInterpreterInvoke(interpreter_) whould work around the same time for both configurations. Can you help me to understand how I should implement processing on GPU correctly?", "comments": ["@LozariaVinok Could you please refer to the [TF Lite on GPU](https://www.tensorflow.org/lite/performance/gpu_advanced) and let us know if it helps?Thank you!", "I've read these docs. There is no info about using ssbo in input or output.\r\nI've also used samples from #26297 . It helped me to get current results. But maybe this issue is too old.  \r\nIt would be helpful if you can show me the way how to use the current library to process data on GPU from a camera without copying it. Or point me what I could do wrong in my sample", "@srjoglekar246 Do we have code pointers for zero copy?", "broader support for Zero copy interop on the TFLite side is still being designed, and will likely land in Q1 next year.\r\n@impjdi can they use the older GPU delegate surface for this feature? I suppose that one doesnt have a Java interface? :-/", "Yeah, no Java interface that I'm aware of.", "I use tflite in NDK. So, I think I don't need a Java interface, right?"]}, {"number": 53206, "title": "[TFLite] Move AVERAGE_POOL_3D and MAX_POOL_3D to builtin ops", "body": "Hi,\r\n\r\nThis PR moves the custom `AvgPool3D` and `MaxPool3D` ops to bulitin `AVERAGE_POOL_3D` and `MAX_POOL_3D`  ops (`AVG` is changed to `AVERAGE` for coherence with `AVERAGE_POOL_2D`).\r\n\r\nAs discussed previously @jianlijianli we tried to keep the code of the custom ops for backward compatibility, notably in [tensorflow/lite/kernels/BUILD](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/BUILD#L785) and [tensorflow/lite/kernels/custom_ops_register.h](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/custom_ops_register.h#L22). Note though that some of the code in [tensorflow/lite/kernels/pooling3d.cc](https://github.com/tensorflow/tensorflow/pull/53206/files#diff-5591b2bdc4d02753edd1722677e7f3e3645a2531a411c904665bb94f3870ec16) for reading the flatbuffer was removed which could create some backward incompatibilities. We could try to keep it in some way if needed but as the custom ops version of these kernels were not compiled into the official binary I'm more in favour of removing this old code if possible along with the one in `tensorflow/lite/kernels/BUILD` and `tensorflow/lite/kernels/custom_ops_register.h`.\r\n\r\nThibaut", "comments": ["@miaout17 As discussed, please see https://arxiv.org/abs/2007.13224 for a network using MaxPool3D and https://keras.io/examples/vision/3D_image_classification/ example that uses it. It'd be nice to be able to convert the Keras model described in the example to TFLite.\r\n\r\nThanks,\r\nThibaut", "@Tessil Can you please resolve conflicts? Thanks!", "@Tessil Can you please resolve conflicts? Thank you!"]}, {"number": 53205, "title": "STFT --> inverse_STFT not reconstructing the original signal when using no window", "body": "Tensorflow is not reconstructing the original signal when applying the STFT followed by the inverse STFT when using no window. The problems arise when the frames of the STFT overlap: It seems like every frame contributes with a weight of 1 regardless of the number of overlapping frames `N = frame_size / frame_step`. As a result, the central part of the signal is `N` times larger than the original. Here is a minimal code to reproduce the error:\r\n\r\n```python\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nsize = 2048\r\nframe_length = 512\r\nframe_step = 128\r\nwaveform = np.sin(np.arange(size) * 1 / 100)\r\n\r\nstft = tf.signal.stft(waveform, frame_length, frame_step, window_fn=None)\r\ninverse_stft = tf.signal.inverse_stft(stft, frame_length, frame_step, window_fn=None)\r\n\r\nplt.plot(waveform)\r\nplt.plot(inverse_stft)\r\nplt.show()\r\nplt.clf()\r\n```\r\nBroken stft --> istft applied to sinusoidal signal: \r\n![](https://i.stack.imgur.com/BMjqX.png)\r\n\r\nNotice that I'm using no window. If I put the Hann window, the central part works well but the borders are smoothly going to zero, a related but surprisingly different bug that is already documented here: #36616. The implementation of scipy works well under all circumstances.\r\n\r\nI'm using Tensorflow 2.6.0 installed with tensorflow-macos on an Apple M1 MacBookPro. ", "comments": ["Hi @sanatmpa1! Could you please look at this issue. It's replicating in Tf 2.5 ,2.6 and 2.7 . Attaching [Gist ](https://colab.research.google.com/gist/mohantym/7a700af1a33bc4800663adc80c71f5ae/github_53205.ipynb)for reference."]}, {"number": 53202, "title": "Making tensorflow size smaller using selective registration for MacOs", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOs Monterey\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: not relevant\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.7.0\r\n- Python version: 3.9 \r\n- Installed using virtualenv? pip? conda?: not relevant\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): not relevant\r\n- CUDA/cuDNN version: not relevant\r\n- GPU model and memory: not relevant\r\n\r\n\r\n\r\n**Describe the problem**\r\nI am trying to make TF size smaller on MacOs the size is 950mb but after using the below optional flags the size is reduced to 839mb:\r\n--config=nogcp --config=nonccl --config=noaws --config=nohdfs  --define=with_xla_support=false --define=with_ignite_support=false --define=with_kafka_support=false\r\n\r\n1. Is there any more flags that can be used to reduce the size of TF?\r\n\r\n2. Since it is still big 839mb I would like to use the selective registration method to get only the operations I use in my model as described in:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/print_selective_registration_header.py\r\n\r\nI followed the steps mentioned in this link but I am not finding equivalent command for MacOs for the following step:\r\nbazel build -c opt --copt=\"-DSELECTIVE_REGISTRATION\" --copt=\"-DSUPPORT_SELECTIVE_REGISTRATION\" //tensorflow/tools/android/inference_interface:libtensorflow_inference.so \r\n--host_crosstool_top=@bazel_tools//tools/cpp:toolchain\r\n--crosstool_top=//external:android/crosstool --cpu=armeabi-v7a\r\n\r\nMy questions:\r\n1. Is the selective registration supported for android only? I didn't find a directory named MacOs in //tensorflow/tools/?\r\n\r\n2. I need to support both cpu intel and arm so what I should type in --cpu?\r\nI tried --cpu=x86_64 and --cpu=arm64 got:\r\nERROR: /private/var/tmp/_bazel_integrator/0a793f72f57e57f678a00d6fdcdcda5e/external/local_config_cc/BUILD:41:19: in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite '@local_config_cc//:toolchain' does not contain a toolchain for cpu 'x86_64'\r\n\r\n3. What is the equivalent command for MacOs for --crosstool_top=//external:android/crosstool ? \r\nI tried --crosstool_top=//external:macos/crosstool but got error:\r\n no such target '//external:macos/crosstool': target 'macos/crosstool' not declared in package 'external' defined by /Users/integrator/tensorflow/WORKSPACE\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n1. git clone https://github.com/tensorflow/tensorflow.git\r\n2. cd tensorflow\r\n3. git checkout v2.7.0\r\n4. ./configure all is no\r\n5. bazel build tensorflow/python/tools:print_selective_registration_header\r\n6.  bazel-bin/tensorflow/python/tools/print_selective_registration_header --graphs=/Users/integrator/Downloads/vgg_16.ckpt > tensorflow/core/framework/ops_to_register.h\r\n\r\nI used an official trained model from here: http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz\r\nI got runtime warning for this step:\r\n\r\nINFO:tensorflow:Loading proto file /Users/integrator/Downloads/vgg_16.ckpt\r\nI1125 15:27:19.769747 8639198720 selective_registration_header_lib.py:84] Loading proto file /Users/integrator/Downloads/vgg_16.ckpt\r\n/Users/integrator/tensorflow/bazel-bin/tensorflow/python/tools/print_selective_registration_header.runfiles/org_tensorflow/tensorflow/python/tools/selective_registration_header_lib.py:93: RuntimeWarning: Unexpected end-group tag: Not all data was converted\r\n  graph_def = graph_pb2.GraphDef.FromString(file_data)\r\n\r\n7. bazel build -c opt --copt=\"-DSELECTIVE_REGISTRATION\" --copt=\"-DSUPPORT_SELECTIVE_REGISTRATION\" //tensorflow/tools/android/inference_interface:libtensorflow_inference.so \r\n--host_crosstool_top=@bazel_tools//tools/cpp:toolchain\r\n--crosstool_top=//external:android/crosstool --cpu=armeabi-v7a\r\n\r\nGot different errors when running the last step three times in sequence:\r\n1. ERROR: /private/var/tmp/_bazel_integrator/0a793f72f57e57f678a00d6fdcdcda5e/external/highwayhash/BUILD.bazel:8:11: undeclared inclusion(s) in rule '@highwayhash//:sip_hash':\r\nthis rule is missing dependency declarations for the following files included by 'highwayhash/highwayhash/sip_hash.cc':\r\n\r\n2. ERROR: /Users/integrator/tensorflow/tensorflow/core/lib/jpeg/BUILD:47:11: undeclared inclusion(s) in rule '//tensorflow/core/lib/jpeg:portable_jpeg_internal':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/core/lib/jpeg/jpeg_handle.cc':\r\n\r\n3. ERROR:/private/var/tmp/_bazel_integrator/0a793f72f57e57f678a00d6fdcdcda5e/external/com_google_absl/absl/hash/BUILD.bazel:29:11: undeclared inclusion(s) in rule '@com_google_absl//absl/hash:hash':\r\nthis rule is missing dependency declarations for the following files included by 'com_google_absl/absl/hash/internal/hash.cc':\r\n\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Hi @sanatmpa1! Could you please look at this issue?", "@sachinprasadhs any help would be appreciated.\r\nThanks", "Could you please post this issue in the dedicated Apple developer forum https://developer.apple.com/forums/", "Hi @sachinprasadhs\r\n\r\nI don't think it is related to apple developer forum since I am talking about reducing tensorflow size using: \r\n1. optional flags.\r\n2. selective registration method.\r\n\r\nWhy do you think it is related to apple developer forum?\r\n\r\nThanks", "Hi @sachinprasadhs \r\n\r\nCan you explain what do you mean by \"awaiting tensorflower\" status? and is there a real person with user name \"learning-to-play\" or it is a bot?\r\n\r\nThanks"]}, {"number": 53197, "title": "TFLite GPU - uninitialized value used in if", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04 x86_64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Nope\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 2.6.2\r\n- Python version: Nope\r\n- Bazel version (if compiling from source): Cmaked\r\n- GCC/Compiler version (if compiling from source): gcc9\r\n- CUDA/cuDNN version: Nope\r\n- GPU model and memory: Any (RTX3090, 1080Ti,...)\r\n\r\nSeems to me that at \r\nhttps://github.com/tensorflow/tensorflow/blob/08527354a6ad3ee96d02d2581a4dd7742b9c19b9/tensorflow/lite/delegates/gpu/common/tasks/conv_powervr.cc#L392\r\n\r\nthe conv_params_.z_kernel_is_1 variable may be uninitialized, because some ConvPowerVR::GuessBestParams() do not set the value (e.g. 2D conv version)\r\n\r\nValgrind reports:\r\n\r\n==2456592== Conditional jump or move depends on uninitialised value(s)\r\n==2456592== at 0x7839679: std::_Bit_reference::operator=(bool) (stl_bvector.h:92)\r\n==2456592== by 0x7C3A736: std::_Bit_iterator std::__copy_move<false, false, std::random_access_iterator_tag>::__copy_m<bool const*, std::_Bit_iterator>(bool const*, bool const*, std::_Bit_iterator) (stl_algobase.h:342)\r\n==2456592== by 0x7C3A66A: std::_Bit_iterator std::__copy_move_a<false, bool const*, std::_Bit_iterator>(bool const*, bool const*, std::_Bit_iterator) (stl_algobase.h:404)\r\n==2456592== by 0x7C3A5C6: std::_Bit_iterator std::__copy_move_a2<false, bool const*, std::_Bit_iterator>(bool const*, bool const*, std::_Bit_iterator) (stl_algobase.h:440)\r\n==2456592== by 0x7C3A4E8: std::_Bit_iterator std::copy<bool const*, std::_Bit_iterator>(bool const*, bool const*, std::_Bit_iterator) (stl_algobase.h:474)\r\n==2456592== by 0x7C39CF1: void std::vector<bool, std::allocator<bool> >::_M_initialize_range<bool const*>(bool const*, bool const*, std::forward_iterator_tag) (stl_bvector.h:1189)\r\n==2456592== by 0x7C38F47: std::vector<bool, std::allocator<bool> >::vector(std::initializer_list<bool>, std::allocator<bool> const&) (stl_bvector.h:691)\r\n==2456592== by 0x7C29C41: tflite::gpu::ConvPowerVR::GenerateConv[abi:cxx11](tflite::gpu::GpuInfo const&, tflite::gpu::OperationDef const&, bool, tflite::gpu::ConvPowerVR::ConvParams const&)::{lambda(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)#3}::operator()(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const (conv_powervr.cc:392)\r\n==2456592== by 0x7C32AB4: tflite::gpu::ConvPowerVR::GenerateConv[abi:cxx11](tflite::gpu::GpuInfo const&, tflite::gpu::OperationDef const&, bool, tflite::gpu::ConvPowerVR::ConvParams const&) (conv_powervr.cc:713)\r\n==2456592== by 0x7C28198: tflite::gpu::ConvPowerVR::GenerateCode(tflite::gpu::GpuInfo const&) (conv_powervr.cc:249)\r\n==2456592== by 0x7C3830C: tflite::gpu::CreateConvPowerVR(tflite::gpu::GpuInfo const&, tflite::gpu::OperationDef const&, tflite::gpu::Convolution2DAttributes const&, tflite::gpu::StrongShape<(tflite::gpu::Layout)10> const*) (conv_powervr.cc:1449)\r\n==2456592== by 0x7D2801B: tflite::gpu::(anonymous namespace)::SelectConvolutionNVidia(tflite::gpu::Convolution2DAttributes const&, tflite::gpu::StrongShape<(tflite::gpu::Layout)10> const&, tflite::gpu::GpuInfo const&, tflite::gpu::OperationDef const&) (convolution_selector.cc:76)\r\n==2456592== Uninitialised value was created by a stack allocation\r\n==2456592== at 0x7D27F2B: tflite::gpu::(anonymous namespace)::SelectConvolutionNVidia(tflite::gpu::Convolution2DAttributes const&, tflite::gpu::StrongShape<(tflite::gpu::Layout)10> const&, tflite::gpu::GpuInfo const&, tflite::gpu::OperationDef const&) (convolution_selector.cc:71)\r\n\r\nCan anyone with code path knowledge look into it? Thx. The value might be used in initializer of vector only, still perhaps it would be better to either initialize or not insert into vector if not initialized.", "comments": ["@svobora \r\nIn order to expedite the trouble-shooting process here,Could you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose),Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Filled."]}, {"number": 53196, "title": "Restoring mixed placement tf.keras.Model attempts to place CPU variable on GPU.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `gcr.io/deeplearning-platform-release/tf2-gpu.2-5`\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NO\r\n- TensorFlow installed from (source or binary): `gcr.io/deeplearning-platform-release/tf2-gpu.2-5`\r\n- TensorFlow version (use command below): 2.5\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: `NVIDIA-SMI 450.119.04   Driver Version: 450.119.04   CUDA Version: 11.4 `\r\n- GPU model and memory: `V100`\r\n\r\n**Describe the current behavior**\r\n1. A model is created where a embedding layer (list of `Variable`s) is placed on CPU and subsequent `Dense` layers are placed according to a given `strategy`.  The embedding layer variables are large and fit on CPU but do not fit on GPU.\r\n2. this model trains (fwd and bwd pass).\r\n3. this model is saved.\r\n3. then an attempt to restore this model is made, but gives the following error, indicating that restoration tries to assign a CPU variable to GPU:\r\n```\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[2000000,256] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator gpu_host_bfc [Op:AssignVariableOp]\r\n```\r\nresulting in OOM on GPU.\r\n\r\n**Describe the expected behavior**\r\nRestoration of CPU variables should not go through the `gpu_host_bfc` allocator.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n- I ran the following code on a [8xV100 on GCP with 128 GB in GPU memory and 624 GB in RAM](https://cloud.google.com/compute/docs/gpus#other_available_nvidia_gpu_models)\r\n\r\n```\r\n\r\nimport os\r\n\r\nfrom absl import app, flags, logging\r\nimport tensorflow as tf\r\n\r\nFLAGS = flags.FLAGS\r\n\r\nflags.DEFINE_string('workspace', None, '')\r\n\r\n\r\nclass Embeddings(tf.keras.layers.Layer):\r\n  def __init__(self):\r\n    super().__init__()\r\n    with tf.device('cpu:0'):\r\n      self.table = [\r\n        tf.Variable(name=f'shard_{idx}', initial_value=tf.keras.initializers.glorot_normal()([int(2e6), 256]))\r\n        for idx in range(20)\r\n      ]\r\n\r\n  def call(self, inputs):\r\n    with tf.device('cpu:0'):\r\n      looked_up = tf.reduce_sum(tf.nn.embedding_lookup(self.table, inputs), axis=1)\r\n    return looked_up\r\n\r\n\r\nclass M(tf.keras.Model):\r\n  def __init__(self, strategy):\r\n    super().__init__()\r\n    with tf.device('cpu:0'):\r\n      self.embeddings = Embeddings()\r\n    with strategy.scope():\r\n      self.mlp = tf.keras.Sequential([tf.keras.layers.Dense(1)])\r\n\r\n  def call(self, inputs):\r\n    inputs = self.embeddings(inputs)\r\n    out = self.mlp(inputs)\r\n    return out\r\n\r\n\r\nstrategy = tf.distribute.MirroredStrategy()\r\n\r\nmodel = M(strategy)\r\nloss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\r\ndef _replica_loss(labels, logits):\r\n  return tf.nn.compute_average_loss(loss_fn(labels, logits), global_batch_size=10)\r\n\r\nwith tf.device('cpu:0'):\r\n  emb_optimizer = tf.keras.optimizers.Adam()\r\nwith strategy.scope():\r\n  mlp_optimizer = tf.keras.optimizers.Adam()\r\n\r\n\r\ndef split_variables(tv):\r\n  from itertools import filterfalse, tee\r\n  from tensorflow.python.distribute.values import MirroredVariable\r\n  def partition(pred, iterable):\r\n    'Use a predicate to partition entries into false entries and true entries'\r\n    # partition(is_odd, range(10)) --> 0 2 4 6 8   and  1 3 5 7 9\r\n    t1, t2 = tee(iterable)\r\n    return list(filterfalse(pred, t1)), list(filter(pred, t2))\r\n  return partition(lambda v: isinstance(v, MirroredVariable), tv)\r\n\r\n\r\ndef save_or_restore(save_dir: str, step: int , **to_save):\r\n ckpt = tf.train.Checkpoint(**to_save)\r\n manager = tf.train.CheckpointManager(ckpt, save_dir, max_to_keep=10)\r\n latest_checkpoint = tf.train.latest_checkpoint(save_dir)\r\n if latest_checkpoint is not None:\r\n   logging.info(f'Restoring checkpoint: {latest_checkpoint}')\r\n   ckpt.restore(latest_checkpoint)\r\n else:\r\n   manager.save(checkpoint_number=step)\r\n\r\n\r\ndef _train_step(inputs, labels):\r\n  with tf.GradientTape() as tape:\r\n    logits = model(inputs)\r\n    loss = _replica_loss(labels, logits)\r\n\r\n  emb_var, mlp_var = split_variables(model.trainable_variables)\r\n  emb_grad, mlp_grad = tape.gradient(loss, [emb_var, mlp_var])\r\n  mlp_optimizer.apply_gradients(zip(mlp_grad, mlp_var))\r\n\r\n  return loss, emb_var, emb_grad\r\n\r\ndef distribute_step(step_fn):\r\n  @tf.function\r\n  def _step(*step_args):\r\n    loss, emb_var, emb_grad = strategy.run(step_fn, args=step_args)\r\n    loss = strategy.reduce(tf.distribute.ReduceOp.SUM, loss, axis=None)\r\n    emb_grad = strategy.reduce(tf.distribute.ReduceOp.SUM, emb_grad, axis=None)\r\n    emb_optimizer.apply_gradients(zip(emb_grad, emb_var))\r\n    return loss\r\n  return _step\r\n\r\ndef main(_argv):\r\n  save_dir = os.path.join(FLAGS.workspace, 'save_dir')\r\n\r\n  train_step = distribute_step(_train_step)\r\n  ds = tf.data.Dataset.from_tensors((tf.random.uniform([10, 10], minval=0, maxval=10, dtype=tf.int64), tf.random.uniform([10, 1], minval=0, maxval=1, dtype=tf.int64)))\r\n  ds = strategy.experimental_distribute_dataset(ds)\r\n\r\n  for example, label in ds:\r\n    loss = train_step(example, label)\r\n    logging.info(f'loss: {loss}')\r\n    break\r\n\r\n  # Save\r\n  logging.info(f'Saving from {save_dir}')\r\n  save_or_restore(\r\n    save_dir=save_dir,\r\n    step=1,\r\n    model=model,\r\n    cpu_optimizer=emb_optimizer,\r\n    gpu_optimizer=mlp_optimizer,\r\n  )\r\n\r\n  # Restore\r\n  logging.info(f'Restoring from {save_dir}')\r\n  save_or_restore(\r\n    save_dir=save_dir,\r\n    step=1,\r\n    model=model,\r\n    cpu_optimizer=emb_optimizer,\r\n    gpu_optimizer=mlp_optimizer,\r\n  )\r\n\r\n\r\nif __name__ == \"__main__\":\r\n  app.run(main)\r\n```\r\n\r\n- It creates an embedding table of 20 shards of ~2GB `Variable`s\r\n- **It trains and saves successfully**\r\n- **However, it fails to restore**\r\n\r\nwith the following attached full logs [repro_save.txt](https://github.com/tensorflow/tensorflow/files/7600948/repro_save.txt)\r\n", "comments": ["@rllin ,\r\nCan you please take a look at this issues [1](https://github.com/tensorflow/tensorflow/issues/52132) [2](https://github.com/tensorflow/models/issues/8487) and [3](https://stackoverflow.com/questions/46066850/understanding-the-resourceexhaustederror-oom-when-allocating-tensor-with-shape) with the similar error.Also please try to test the code in latest tf v2.7.It helps.Thanks", "they do not look the same.  please take a closer look at my error message.  mine is specifically showing a CPU variable trying to be restored thru a GPU allocator.\r\n\r\nWhen i make the CPU variables smaller and restoration works, they appear in CPU as expected.  So it seems like they trigger GPU allocation but are moved to the CPU.", "Also when using `gcr.io/deeplearning-platform-release/tf2-gpu.2-7`, I get the following error [repro_save_2_7.txt](https://github.com/tensorflow/tensorflow/files/7605694/repro_save_2_7.txt)", "@sanatmpa1 ,\r\nI have tried in colab with TF version 2.5, 2.7 and nightly version and noticed that session is being crashed. Please, find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/53ce7f0dd9d33d6c8d3f0a46ff15be63/untitled127.ipynb). ", "@tilakrayal this will definitely crash in a colab.\r\n\r\nif you look at the code I am creating an embedding layer of 20 variables that take up 40 GB.  You will have to reproduce this on a machine where:\r\n\r\n1. there is more CPU memory (RAM) than per-device GPU memory\r\n2. and ideally you have multiple GPUs\r\n\r\na little more context:\r\n- my goal is to create a large embedding layer on CPU (RAM is more abundant and I only need to do lookups)\r\n- and then keep downstream layers on GPU (MLPs are smaller and thus fit on GPU but can be computed very fast on GPU)", "fwiw this problem is not reproducible if you use `memory_limit` to virtualize a setup where GPU memory < CPU memory.  so it cannot be reproduced in colab unless you connect to a GCP instance that is similar to the one I used where CPU memory > per-device GPU memory:\r\n\r\n> - I ran the following code on a [8xV100 on GCP with 128 GB in GPU memory and 624 GB in RAM](https://cloud.google.com/compute/docs/gpus#other_available_nvidia_gpu_models)\r\n", "hey @sachinprasadhs lmk if there's anything i can clarify, the repro should be very straightforward, but i can provide any necessary context, thanks!", "when i look at memory growth, it looks like when writing is in progress, 250% of the model size is needed in memory is needed to hold buffered data before writing begins.  and 130% is needed during restoration.  what my errors are are due to not having 2.5x or 1.3x the amount of memory as model size when saving/restoring.  it turns out gpu_allocator_bfc is just a poor name and it is still allocating properly to the cpu.\r\n\r\nhowever i still have a problem with how slow the saving is.  it is only writing to gs:// at about 200 MB/s.  This is below expected speeds of 500 MB/s.  Is there any control over multithreading when writing to remote?\r\n\r\n"]}, {"number": 53187, "title": "c++ use LoadSavedModel can't choose gpu?", "body": "\r\n-   **OS Platform and Distribution **: Linux Ubuntu 18.04 in docker\r\n-   **TensorFlow installed from (source or binary)**:\r\n-   **TensorFlow version (use command below)**:\r\n-   **Python version**:\r\n-   **Bazel version (if compiling from source)**:3.1\r\n-   **GCC/Compiler version (if compiling from source)**:r2.4\r\n-   **CUDA/cuDNN version**:CUDA 11.2 Cudnn 8.1\r\n-   **GPU model and memory**: TensorRT7.2.3 tf-trt saved_model\r\n-   **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI want to deploy the same model on two different GPU cards in the same process.  But when I use LoadSavedModel to load this model, it always loads on one card by default.  I check the source code for TensorFlow but can not find the operation in LoadSavedModel for setting device from Nodes which in the GraphDef. Is there a way to deploy the same model on two different GPU cards in the same process\uff1fone model load on card one, another model load on card two.\r\n\r\n### Source code / logs\r\nThis is the load code I wrote\r\n![image](https://user-images.githubusercontent.com/72930044/143239057-eda71e90-8e2e-4fba-aefe-8b939e08da04.png)\r\n\r\ninference is\r\nmodel_bundle.GetSession->Run()\r\n\r\n", "comments": ["@sugar52004 Could you please try with the latest stable `TF v2.7.0` and let us know if it helps? Thank you!", "> @sugar52004 Could you please try with the latest stable `TF v2.7.0` and let us know if it helps? Thank you!\r\n\r\nis cpp api deference\uff1fi also cant see choose gpu option in LoadSavedModel. Are there any examples? thanks\uff01", "@sugar52004 \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Could you please refer to this [link](https://www.tensorflow.org/guide/saved_model#load_a_savedmodel_in_c) ,[link1](https://www.tensorflow.org/guide/create_op),[CPP functions](https://www.tensorflow.org/api_docs/cc) and let us know if it helps?Thanks!", "> @sugar52004 In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Could you please refer to this [link](https://www.tensorflow.org/guide/saved_model#load_a_savedmodel_in_c) ,[link1](https://www.tensorflow.org/guide/create_op),[CPP functions](https://www.tensorflow.org/api_docs/cc) and let us know if it helps?Thanks!\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/r2.4/tensorflow/cc/saved_model/loader.h#L120\r\nthis api,i want to choose gpu for my model in one Process."]}, {"number": 53181, "title": "Error showed when called header files using google\\protobuf 3.9.2 ", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ![image](https://user-images.githubusercontent.com/41325962/143195575-5ea23af8-356e-4ac2-8c33-7f67b825cb2f.png) Traditional Chinese\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): from source \r\n- TensorFlow version (use command below): r2.5\r\n- Python version: 3.8.11\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): use msvc 2019 build tool\r\n- CUDA/cuDNN version: 11.2 8.1.2\r\n- GPU model and memory: \r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nTry to run following code and got some errors\r\n```\r\n#include <tensorflow/cc/ops/io_ops.h>\r\n#include <tensorflow/cc/ops/parsing_ops.h>\r\n#include <tensorflow/cc/ops/array_ops.h>\r\n#include <tensorflow/cc/ops/math_ops.h>\r\n#include <tensorflow/cc/ops/data_flow_ops.h>\r\n\r\n\r\n#include <tensorflow/core/public/session.h>\r\n#include <tensorflow/core/protobuf/meta_graph.pb.h>\r\n#include <fstream>\r\n\r\nusing namespace std;\r\nusing namespace tensorflow;\r\nusing namespace tensorflow::ops;\r\n\r\nint main()\r\n{\r\n\t// set up your input paths\r\n\tconst string pathToGraph = \"D:\\\\python\\\\RSNA\\\\keras2cpp\\\\keras2tf\\\\model_tf2_2\\\\keras_metadata\";\r\n\tconst string checkpointPath = \"D:\\\\python\\\\RSNA\\\\keras2cpp\\\\keras2tf\\\\model_tf2_2\\\\saved_model\";\r\n\r\n\tauto session = NewSession(SessionOptions());\r\n\tif (session == nullptr)\r\n\t{\r\n\t\tthrow runtime_error(\"Could not create Tensorflow session.\");\r\n\t}\r\n\r\n\tStatus status;\r\n\r\n\t// Read in the protobuf graph we exported\r\n\tMetaGraphDef graph_def;\r\n\tstatus = ReadBinaryProto(Env::Default(), pathToGraph, &graph_def);\r\n\tif (!status.ok())\r\n\t{\r\n\t\tthrow runtime_error(\"Error reading graph definition from \" + pathToGraph + \": \" + status.ToString());\r\n\t}\r\n\r\n\t// Add the graph to the session\r\n\tstatus = session->Create(graph_def.graph_def());\r\n\tif (!status.ok())\r\n\t{\r\n\t\tthrow runtime_error(\"Error creating graph: \" + status.ToString());\r\n\t}\r\n\r\n\t// Read weights from the saved checkpoint\r\n\tTensor checkpointPathTensor(DT_STRING, TensorShape());\r\n\tcheckpointPathTensor.scalar<std::string>()() = checkpointPath;\r\n\tstatus = session->Run({ {graph_def.saver_def().filename_tensor_name(), checkpointPathTensor}, }, {},\r\n\t\t{ graph_def.saver_def().restore_op_name() }, nullptr);\r\n\tif (!status.ok())\r\n\t{\r\n\t\tthrow runtime_error(\"Error loading checkpoint from \" + checkpointPath + \": \" + status.ToString());\r\n\t}\r\n\r\n\tcout << 1 << endl;\r\n}\r\n```\r\nerror messages\r\n```\r\nBuild started...\r\n1>------ Build started: Project: TF_savedmodel, Configuration: Release x64 ------\r\n1>Source.cpp\r\n1>D:\\tf_model\\TF_savedmodel\\include\\Eigen\\src\\Core\\arch\\Default\\Half.h(1,1): warning C4819: The file contains a character that cannot be represented in the current code page (950). Save the file in Unicode format to prevent data loss\r\n1>D:\\tf_model\\TF_savedmodel\\include\\Eigen\\src\\Core\\arch\\Default\\BFloat16.h(1,1): warning C4819: The file contains a character that cannot be represented in the current code page (950). Save the file in Unicode format to prevent data loss\r\n1>D:\\tf_model\\TF_savedmodel\\include\\Eigen\\src\\Core\\arch\\Default\\GenericPacketMathFunctions.h(679,1): warning C4819: The file contains a character that cannot be represented in the current code page (950). Save the file in Unicode format to prevent data loss\r\n1>D:\\tf_model\\TF_savedmodel\\include\\Eigen\\src\\Core\\products\\GeneralBlockPanelKernel.h(2066,1): warning C4819: The file contains a character that cannot be represented in the current code page (950). Save the file in Unicode format to prevent data loss\r\n1>D:\\tf_model\\TF_savedmodel\\include\\unsupported\\Eigen\\CXX11\\src\\Tensor\\Tensor.h(76,1): warning C4554: '&': check operator precedence for possible error; use parentheses to clarify precedence\r\n1>D:\\tf_model\\TF_savedmodel\\include\\unsupported\\Eigen\\CXX11\\src\\Tensor\\TensorMap.h(43): message : see reference to class template instantiation 'Eigen::Tensor<T,1,1,int>' being compiled\r\n1>        with\r\n1>        [\r\n1>            T=float\r\n1>        ]\r\n1>D:\\tf_model\\TF_savedmodel\\include\\tensorflow\\core\\framework\\tensor_types.h(105): message : see reference to class template instantiation 'Eigen::TensorMap<Eigen::Tensor<T,1,1,int>,16,Eigen::MakePointer>' being compiled\r\n1>        with\r\n1>        [\r\n1>            T=float\r\n1>        ]\r\n1>D:\\tf_model\\TF_savedmodel\\include\\google\\protobuf\\type.pb.h(106,109): warning C4003: not enough arguments for function-like macro invocation 'min'\r\n1>D:\\tf_model\\TF_savedmodel\\include\\google\\protobuf\\type.pb.h(106,109): error C2589: '(': illegal token on right side of '::'\r\n1>D:\\tf_model\\TF_savedmodel\\include\\google\\protobuf\\type.pb.h(106): error C2062: type 'unknown-type' unexpected\r\n1>D:\\tf_model\\TF_savedmodel\\include\\google\\protobuf\\type.pb.h(106,109): error C3805: 'type': unexpected token, expected either '}' or a ','\r\n1>D:\\tf_model\\TF_savedmodel\\include\\google\\protobuf\\type.pb.h(133,123): warning C4003: not enough arguments for function-like macro invocation 'min'\r\n1>D:\\tf_model\\TF_savedmodel\\include\\google\\protobuf\\type.pb.h(133,123): error C2589: '(': illegal token on right side of '::'\r\n1>D:\\tf_model\\TF_savedmodel\\include\\google\\protobuf\\type.pb.h(133): error C2062: type 'unknown-type' unexpected\r\n1>D:\\tf_model\\TF_savedmodel\\include\\google\\protobuf\\type.pb.h(133,123): error C3805: 'type': unexpected token, expected either '}' or a ','\r\n1>D:\\tf_model\\TF_savedmodel\\include\\google\\protobuf\\type.pb.h(158,94): warning C4003: not enough arguments for function-like macro invocation 'min'\r\n1>D:\\tf_model\\TF_savedmodel\\include\\google\\protobuf\\type.pb.h(158,94): error C2589: '(': illegal token on right side of '::'\r\n1>D:\\tf_model\\TF_savedmodel\\include\\google\\protobuf\\type.pb.h(158): error C2062: type 'unknown-type' unexpected\r\n1>D:\\tf_model\\TF_savedmodel\\include\\google\\protobuf\\type.pb.h(158,94): error C3805: 'type': unexpected token, expected either '}' or a ','\r\n1>D:\\tf_model\\TF_savedmodel\\include\\tensorflow\\core\\framework\\attr_value.pb.h(687,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n1>D:\\tf_model\\TF_savedmodel\\include\\tensorflow\\core\\framework\\node_def.pb.h(97,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n1>D:\\tf_model\\TF_savedmodel\\include\\tensorflow\\core\\framework\\function.pb.h(300,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n1>D:\\tf_model\\TF_savedmodel\\include\\tensorflow\\core\\framework\\function.pb.h(332,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n1>D:\\tf_model\\TF_savedmodel\\include\\tensorflow\\core\\framework\\function.pb.h(587,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n1>D:\\tf_model\\TF_savedmodel\\include\\tensorflow\\core\\framework\\function.pb.h(590,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n1>D:\\tf_model\\TF_savedmodel\\include\\tensorflow\\core\\framework\\function.pb.h(621,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n1>D:\\tf_model\\TF_savedmodel\\include\\tensorflow\\core\\framework\\function.pb.h(624,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n1>D:\\tf_model\\TF_savedmodel\\include\\tensorflow\\core\\util\\tensor_format.h(502,79): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n1>D:\\tf_model\\TF_savedmodel\\include\\tensorflow\\core\\util\\tensor_format.h(524,71): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n1>D:\\tf_model\\TF_savedmodel\\include\\tensorflow\\core\\util\\tensor_format.h(558,77): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n1>D:\\tf_model\\TF_savedmodel\\include\\tensorflow\\core\\framework\\node_def_util.h(147,20): warning C4267: 'return': conversion from 'size_t' to 'int', possible loss of data\r\n1>D:\\tf_model\\TF_savedmodel\\include\\tensorflow\\core\\framework\\step_stats.pb.h(1180,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n1>D:\\tf_model\\TF_savedmodel\\include\\tensorflow\\core\\protobuf\\cluster.pb.h(97,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n1>D:\\tf_model\\TF_savedmodel\\include\\tensorflow\\core\\protobuf\\rewriter_config.pb.h(543,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n1>D:\\tf_model\\TF_savedmodel\\include\\tensorflow\\core\\protobuf\\config.pb.h(1951,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n1>D:\\tf_model\\TF_savedmodel\\include\\tensorflow\\core\\protobuf\\config.pb.h(3797,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n1>D:\\tf_model\\TF_savedmodel\\include\\tensorflow\\core\\protobuf\\config.pb.h(3800,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n1>D:\\tf_model\\TF_savedmodel\\include\\tensorflow\\core\\protobuf\\config.pb.h(3831,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n1>D:\\tf_model\\TF_savedmodel\\include\\tensorflow\\core\\protobuf\\config.pb.h(3834,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n1>D:\\tf_model\\TF_savedmodel\\include\\tensorflow\\core\\framework\\op_kernel.h(159,59): warning C4244: 'return': conversion from 'unsigned __int64' to 'int', possible loss of data\r\n1>D:\\tf_model\\TF_savedmodel\\include\\tensorflow\\core\\framework\\op_kernel.h(166,61): warning C4244: 'return': conversion from 'unsigned __int64' to 'int', possible loss of data\r\n1>D:\\tf_model\\TF_savedmodel\\include\\tensorflow\\core\\framework\\op_kernel.h(307,59): warning C4244: 'return': conversion from 'unsigned __int64' to 'int', possible loss of data\r\n1>D:\\tf_model\\TF_savedmodel\\include\\tensorflow\\core\\framework\\op_kernel.h(315,61): warning C4244: 'return': conversion from 'unsigned __int64' to 'int', possible loss of data\r\n1>D:\\tf_model\\TF_savedmodel\\include\\tensorflow\\core\\framework\\op_kernel.h(730,56): warning C4244: 'return': conversion from 'unsigned __int64' to 'int', possible loss of data\r\n1>D:\\tf_model\\TF_savedmodel\\include\\tensorflow\\core\\framework\\op_kernel.h(735,49): warning C4244: 'return': conversion from 'unsigned __int64' to 'int', possible loss of data\r\n1>D:\\tf_model\\TF_savedmodel\\include\\tensorflow\\core\\lib\\core\\arena.h(46,1): warning C4267: 'argument': conversion from 'size_t' to 'const int', possible loss of data\r\n1>D:\\tf_model\\TF_savedmodel\\include\\tensorflow\\core\\graph\\graph.h(591,34): warning C4244: 'return': conversion from 'const tensorflow::int64' to 'int', possible loss of data\r\n1>D:\\tf_model\\TF_savedmodel\\include\\tensorflow\\core\\graph\\graph.h(596,23): warning C4244: 'return': conversion from 'tensorflow::int64' to 'int', possible loss of data\r\n1>D:\\tf_model\\TF_savedmodel\\include\\tensorflow\\core\\graph\\graph.h(632,28): warning C4267: 'return': conversion from 'size_t' to 'int', possible loss of data\r\n1>D:\\tf_model\\TF_savedmodel\\include\\tensorflow\\core\\graph\\graph.h(641,28): warning C4267: 'return': conversion from 'size_t' to 'int', possible loss of data\r\n1>D:\\tf_model\\TF_savedmodel\\include\\tensorflow\\core\\public\\session_options.h(62,2): warning C4091: '__declspec(dllimport)': ignored on left of 'tensorflow::SessionOptions' when no variable is declared\r\n1>D:\\tf_model\\TF_savedmodel\\include\\tensorflow\\core\\protobuf\\struct.pb.h(874,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n1>D:\\tf_model\\TF_savedmodel\\include\\tensorflow\\core\\protobuf\\saved_object_graph.pb.h(173,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n1>D:\\tf_model\\TF_savedmodel\\include\\tensorflow\\core\\protobuf\\saved_object_graph.pb.h(381,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n1>D:\\tf_model\\TF_savedmodel\\include\\tensorflow\\core\\protobuf\\meta_graph.pb.h(164,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n1>D:\\tf_model\\TF_savedmodel\\include\\tensorflow\\core\\protobuf\\meta_graph.pb.h(167,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n1>D:\\tf_model\\TF_savedmodel\\include\\tensorflow\\core\\protobuf\\meta_graph.pb.h(479,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n1>D:\\tf_model\\TF_savedmodel\\include\\tensorflow\\core\\protobuf\\meta_graph.pb.h(511,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n1>D:\\tf_model\\TF_savedmodel\\include\\tensorflow\\core\\protobuf\\meta_graph.pb.h(2470,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n1>D:\\tf_model\\TF_savedmodel\\include\\tensorflow\\core\\protobuf\\meta_graph.pb.h(2502,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n1>Done building project \"TF_savedmodel.vcxproj\" -- FAILED.\r\n========== Build: 0 succeeded, 1 failed, 0 up-to-date, 0 skipped ==========\r\n```\r\nI'm facing two problem\r\n1. \r\n![image](https://user-images.githubusercontent.com/41325962/143196623-d6063313-4c2d-4214-afac-05c472cb0f0b.png)\r\n2. \r\n![image](https://user-images.githubusercontent.com/41325962/143197092-da67e941-2a85-4231-ad70-0abb812af615.png)\r\n\r\n\r\nI looked at google\\protobuf someone facing the similar problem with 2. [here](https://github.com/protocolbuffers/protobuf/issues/6683) in protobuf 3.9.1.  It seems to be solved in protobuf 3.11 version but the latest tensorflow 2.6.2 still require protobuf 3.9.2 version. I think that maybe just I met the error. Or 2. problem just be caused by 1..\r\nWaiting for help. Thank you. \r\n\r\n\r\n**Describe the expected behavior**\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["If I add \r\n![image](https://user-images.githubusercontent.com/41325962/143198985-3aa9afd8-f676-49c8-813c-73c003cec702.png)\r\nand \r\n![image](https://user-images.githubusercontent.com/41325962/143199127-dfc46e0b-937d-423b-9b6a-255fda5be3ac.png)\r\nto proto_def.inc and proto_undef.inc, I will get another error.\r\n![image](https://user-images.githubusercontent.com/41325962/143199271-16f07dc5-f0b2-4170-bb91-9002d620d995.png)\r\n \r\n", "Hi! @Wistar1308 ! Could you please again by upgrading Tensoflow version to 2.6/2.7 and Protobuf version too as mentioned in the issue? Thanks!", "@mohantym Hi! I tried Tensorflow-2.7 with bazel-3.7.2 python-3.8.11 cuda-11.2 cudnn-8.1.2.\r\nI run the same code and got some error.\r\n![image](https://user-images.githubusercontent.com/41325962/143378993-857ebf39-d4d2-442e-985d-c09e70a17abb.png)\r\nI manually replaced int64 with std::int64_t in following files\r\nio_ops.h\r\nparsing_ops.h\r\nnotification.h\r\ntensor_shape.h\r\narray_ops.h\r\ndata_flow_ops.h\r\n![image](https://user-images.githubusercontent.com/41325962/143380818-ceba3713-136a-4a66-84c4-feabaa01012a.png)\r\nand still got unresolved external symbol error.\r\nWhen I opened google\\protobuf related files still got error about google\\protobuf\r\n![image](https://user-images.githubusercontent.com/41325962/143381038-fdad10f2-53df-4f5d-a648-8727df2fe118.png)\r\n\r\n", "Hi @jvishnuvardhan! Could you please look at this issue?"]}, {"number": 53179, "title": "Tflite LSTM performance drops when running benchmark tool as an app vs compiled binary.", "body": "We've discovered that some of our LSTM models perform unexpectedly ~x2 worse on some Android devices such as the LG Stylo 5. When debugging with the tflite [benchmark tool](https://www.tensorflow.org/lite/performance/measurement), we discovered that this problem only occurs when running tflite inside an APK, and NOT when running the benchmark tool as a compiled binary. It's as if tflite threads scheduling on the cpu is halve. This happens when using even 1 tflite thread and thread affinity doesn't make any difference.\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: LG Stylo 5, LG Stylo 6, Samsung Galaxy A21\r\n- TensorFlow installed from (source or binary): r2.7\r\n- TensorFlow version (use command below): r2.7\r\n- Python version: 3.8\r\n- CUDA/cuDNN version: 11.2 / 8.2.1.32\r\n- GPU model and memory: GTX 1080ti, 11GB VRAM\r\n\r\n\r\n**Reproduce**\r\nHere's a comparison of running the same LSTM model on the LG Stylo 5 (1/2 expected performance running in app) and Samsung Galaxy J3 (no issues):\r\nSamsung Galaxy J3: 4 x 1.4GHz Cortex-A53\r\nLG Stylo 5: 8 x 1.8GHz Cortex-A53\r\nUsing only 1 thread, the Stylo 5 should perform 20%+ better than the J3.\r\nThe app and compiled binary benchmarks are configured identically (100 warmup steps, 1000 bechmark steps).\r\n\r\n```\r\n# Run this to install the benchmark tool, both as a compiled binary, and the app version. Place the LSTM model on the phone.\r\n$ adb push benchmark_model /data/local/tmp\r\n$ adb shell chmod +x /data/local/tmp/benchmark_model\r\n$ adb install -r -d -g android_aarch64_benchmark_model.apk\r\n$ adb push LstmPerfModelTf27Export.tflite /data/local/tmp\r\n```\r\n**Describe the current behavior**\r\nLG Stylo 5:\r\n```\r\n$ adb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/LstmPerfModelTf27Export.tflite --num_threads=1 --warmup_runs=100 --num_runs=1000\r\nSTARTING!\r\nLog parameter values verbosely: [0]\r\nMin num runs: [1000]\r\nNum threads: [1]\r\nMin warmup runs: [100]\r\nGraph: [/data/local/tmp/LstmPerfModelTf27Export.tflite]\r\n#threads used for CPU inference: [1]\r\nLoaded model /data/local/tmp/LstmPerfModelTf27Export.tflite\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\r\nThe input model file size (MB): 9.46564\r\nInitialized session in 35.59ms.\r\nRunning benchmark for at least 100 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\ncount=100 first=74230 curr=5768 min=5583 max=74230 avg=6411.35 std=6817\r\n\r\nRunning benchmark for at least 1000 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\ncount=1000 first=6186 curr=5358 min=5262 max=6256 avg=5396.75 std=117\r\n\r\nInference timings in us: Init: 35590, First inference: 74230, Warmup (avg): 6411.35, Inference (avg): 5396.75\r\nNote: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.\r\nMemory footprint delta from the start of the tool (MB): init=1.57812 overall=10.918\r\n\r\n$ adb shell am start -S -n org.tensorflow.lite.benchmark/.BenchmarkModelActivity --es args '\"--graph=/data/local/tmp/LstmPerfModelTf27Export.tflite --num_threads=1 --warmup_runs=100 --num_runs=1000\"'\r\nStopping: org.tensorflow.lite.benchmark\r\nStarting: Intent { cmp=org.tensorflow.lite.benchmark/.BenchmarkModelActivity (has extras) }\r\n\r\n$ adb logcat | grep \"Average inference\"\r\n11-23 19:01:42.241 17323 17323 E tflite  : Average inference timings in us: Warmup: 10184.5, Init: 3539, Inference: 11459.1\r\n    Overall max resident set size = 10.2305 MB, total malloc-ed size = 0 MB, in-use allocated/mmapped size = 0.0652695 MB\r\n```\r\nThe inference average in benchmark app is doubled!: **11459.1ms** vs **5396.75ms** in benchmark binary.\r\n\r\n\r\n**Describe the expected behavior**\r\nHere's what happens on most phones, even the much slower Samsung Galaxy J3:\r\n```\r\n$ adb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/LstmPerfModelTf27Export.tflite --num_threads=1 --warmup_runs=100 --num_runs=1000\r\nSTARTING!\r\nLog parameter values verbosely: [0]\r\nMin num runs: [1000]\r\nNum threads: [1]\r\nMin warmup runs: [100]\r\nGraph: [/data/local/tmp/LstmPerfModelTf27Export.tflite]\r\n#threads used for CPU inference: [1]\r\nLoaded model /data/local/tmp/LstmPerfModelTf27Export.tflite\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\r\nThe input model file size (MB): 9.46564\r\nInitialized session in 5.456ms.\r\nRunning benchmark for at least 100 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\ncount=100 first=12516 curr=6224 min=6103 max=12516 avg=6375.52 std=651\r\n\r\nRunning benchmark for at least 1000 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\ncount=1000 first=7156 curr=6312 min=6099 max=8715 avg=6319.49 std=232\r\n\r\nInference timings in us: Init: 5456, First inference: 12516, Warmup (avg): 6375.52, Inference (avg): 6319.49\r\nNote: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.\r\nMemory footprint delta from the start of the tool (MB): init=0.828125 overall=9.99219\r\n\r\n$ adb shell am start -S -n org.tensorflow.lite.benchmark/.BenchmarkModelActivity --es args '\"--graph=/data/local/tmp/LstmPerfModelTf27Export.tflite --num_threads=1 --warmup_runs=100 --num_runs=1000\"'\r\nStopping: org.tensorflow.lite.benchmark\r\nStarting: Intent { cmp=org.tensorflow.lite.benchmark/.BenchmarkModelActivity (has extras) }\r\n\r\n$ adb logcat | grep \"Average inference\"\r\n11-23 18:56:24.866 16670 16670 E tflite  : Average inference timings in us: Warmup: 6382.62, Init: 4705, Inference: 6381.92\r\n    Overall max resident set size = 9.83984 MB, total malloc-ed size = 0 MB, in-use allocated/mmapped size = 4095.83 MB\r\n```\r\nBasically no difference between app and compiled binary results: 6381.92ms vs 6319.49ms.\r\n\r\nWe've only observed this problem for LSTM models (using `tf.keras.layers.LSTM`); no performance decline when benchmarking convolutional models such as [`mobilenet_v1_0.25_224.tflite`](https://www.tensorflow.org/lite/guide/hosted_models).\r\nWe've found many phones with this problem: LG Stylo 5 (Qualcomm Snapdragon 450), LG Stylo 6 (MediaTek P35), Samsung Galaxy A21 (MediaTek P35), Samsung Galaxy A12(MediaTek P35).\r\nThese phones have much better CPUs than the Samsung Galaxy J3, yet they perform much worse!\r\n\r\nIs there any way to make the tflite performance running in apps to be the same as running in the benchmark tool compiled binary?\r\n\r\n\r\n\r\n", "comments": ["The tflite LSTM model used in the tests: [LstmPerfModelTf27Export.zip](https://github.com/tensorflow/tensorflow/files/7594203/LstmPerfModelTf27Export.zip)\r\n"]}, {"number": 53165, "title": "cannot build tensorflow/lite/java/demo in Android Studio", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 20.04\r\n- tensorflow: on master `d0af304b7b3cd76a56eae1f5a82910199dd6644d`\r\n- android studio: android-studio-2020.3.1.25-linux\r\n\r\n**Describe the problem - Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nI am following this [tutorial](https://www.tensorflow.org/lite/performance/gpu) about **TensorFlow Lite GPU delegate**. And the related video.\r\n\r\nwhen opening `tensorflow/tensorflow/lite/java/demo` in android studio, it tries to build, but fails with:\r\n\r\n```\r\nUnable to start the daemon process.\r\n\r\nThe project uses Gradle 4.4 which is incompatible with Java 11 or newer.\r\n\r\nPossible solution:\r\n - Upgrade Gradle wrapper to 4.8 version and re-import the project\r\n```\r\n\r\nI then accept the suggestion (`gradle-wrapper.properties` gets modified) but building still fails with\r\n\r\n```\r\nA problem occurred configuring project ':app'.\r\n> java.lang.NullPointerException (no error message)\r\n\r\n* Try:\r\nRun with --info or --debug option to get more log output. Run with --scan to get full insights.\r\n\r\n* Exception is:\r\norg.gradle.api.ProjectConfigurationException: A problem occurred configuring project ':app'.\r\n\tat org.gradle.configuration.project.LifecycleProjectEvaluator.addConfigurationFailure(LifecycleProjectEvaluator.java:109)\r\n\tat org.gradle.configuration.project.LifecycleProjectEvaluator.onAfterEvaluateFailure(LifecycleProjectEvaluator.java:105)\r\n\tat org.gradle.configuration.project.LifecycleProjectEvaluator.notifyAfterEvaluate(LifecycleProjectEvaluator.java:87)\r\n\tat org.gradle.configuration.project.LifecycleProjectEvaluator.doConfigure(LifecycleProjectEvaluator.java:72)\r\n\tat org.gradle.configuration.project.LifecycleProjectEvaluator.access$100(LifecycleProjectEvaluator.java:37)\r\n\tat org.gradle.configuration.project.LifecycleProjectEvaluator$ConfigureProject.run(LifecycleProjectEvaluator.java:125)\r\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:317)\r\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:309)\r\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:185)\r\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:97)\r\n\tat org.gradle.internal.operations.DelegatingBuildOperationExecutor.run(DelegatingBuildOperationExecutor.java:31)\r\n\tat org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:52)\r\n\tat org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:677)\r\n\tat org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:138)\r\n\tat org.gradle.execution.TaskPathProjectEvaluator.configure(TaskPathProjectEvaluator.java:35)\r\n\tat org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:62)\r\n\tat org.gradle.configuration.DefaultBuildConfigurer.configure(DefaultBuildConfigurer.java:41)\r\n\tat org.gradle.initialization.DefaultGradleLauncher$ConfigureBuild.run(DefaultGradleLauncher.java:262)\r\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:317)\r\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:309)\r\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:185)\r\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:97)\r\n\tat org.gradle.internal.operations.DelegatingBuildOperationExecutor.run(DelegatingBuildOperationExecutor.java:31)\r\n\tat org.gradle.initialization.DefaultGradleLauncher.configureBuild(DefaultGradleLauncher.java:175)\r\n\tat org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:132)\r\n\tat org.gradle.initialization.DefaultGradleLauncher.executeTasks(DefaultGradleLauncher.java:115)\r\n\tat org.gradle.internal.invocation.GradleBuildController$1.call(GradleBuildController.java:77)\r\n\tat org.gradle.internal.invocation.GradleBuildController$1.call(GradleBuildController.java:74)\r\n\tat org.gradle.internal.work.DefaultWorkerLeaseService.withLocks(DefaultWorkerLeaseService.java:152)\r\n\tat org.gradle.internal.work.StopShieldingWorkerLeaseService.withLocks(StopShieldingWorkerLeaseService.java:38)\r\n\tat org.gradle.internal.invocation.GradleBuildController.doBuild(GradleBuildController.java:96)\r\n\tat org.gradle.internal.invocation.GradleBuildController.run(GradleBuildController.java:74)\r\n\tat org.gradle.tooling.internal.provider.runner.ClientProvidedPhasedActionRunner.run(ClientProvidedPhasedActionRunner.java:61)\r\n\tat org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35)\r\n\tat org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35)\r\n\tat org.gradle.tooling.internal.provider.ValidatingBuildActionRunner.run(ValidatingBuildActionRunner.java:32)\r\n\tat org.gradle.launcher.exec.RunAsBuildOperationBuildActionRunner$3.run(RunAsBuildOperationBuildActionRunner.java:47)\r\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:317)\r\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:309)\r\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:185)\r\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:97)\r\n\tat org.gradle.internal.operations.DelegatingBuildOperationExecutor.run(DelegatingBuildOperationExecutor.java:31)\r\n\tat org.gradle.launcher.exec.RunAsBuildOperationBuildActionRunner.run(RunAsBuildOperationBuildActionRunner.java:43)\r\n\tat org.gradle.tooling.internal.provider.SubscribableBuildActionRunner.run(SubscribableBuildActionRunner.java:51)\r\n\tat org.gradle.launcher.exec.InProcessBuildActionExecuter$1.transform(InProcessBuildActionExecuter.java:50)\r\n\tat org.gradle.launcher.exec.InProcessBuildActionExecuter$1.transform(InProcessBuildActionExecuter.java:46)\r\n\tat org.gradle.composite.internal.DefaultRootBuildState.run(DefaultRootBuildState.java:65)\r\n\tat org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:46)\r\n\tat org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:32)\r\n\tat org.gradle.launcher.exec.BuildTreeScopeBuildActionExecuter.execute(BuildTreeScopeBuildActionExecuter.java:39)\r\n\tat org.gradle.launcher.exec.BuildTreeScopeBuildActionExecuter.execute(BuildTreeScopeBuildActionExecuter.java:25)\r\n\tat org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:80)\r\n\tat org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:53)\r\n\tat org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:62)\r\n\tat org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:34)\r\n\tat org.gradle.tooling.internal.provider.GradleThreadBuildActionExecuter.execute(GradleThreadBuildActionExecuter.java:36)\r\n\tat org.gradle.tooling.internal.provider.GradleThreadBuildActionExecuter.execute(GradleThreadBuildActionExecuter.java:25)\r\n\tat org.gradle.tooling.internal.provider.ParallelismConfigurationBuildActionExecuter.execute(ParallelismConfigurationBuildActionExecuter.java:43)\r\n\tat org.gradle.tooling.internal.provider.ParallelismConfigurationBuildActionExecuter.execute(ParallelismConfigurationBuildActionExecuter.java:29)\r\n\tat org.gradle.tooling.internal.provider.StartParamsValidatingActionExecuter.execute(StartParamsValidatingActionExecuter.java:59)\r\n\tat org.gradle.tooling.internal.provider.StartParamsValidatingActionExecuter.execute(StartParamsValidatingActionExecuter.java:31)\r\n\tat org.gradle.tooling.internal.provider.SessionFailureReportingActionExecuter.execute(SessionFailureReportingActionExecuter.java:59)\r\n\tat org.gradle.tooling.internal.provider.SessionFailureReportingActionExecuter.execute(SessionFailureReportingActionExecuter.java:44)\r\n\tat org.gradle.tooling.internal.provider.SetupLoggingActionExecuter.execute(SetupLoggingActionExecuter.java:46)\r\n\tat org.gradle.tooling.internal.provider.SetupLoggingActionExecuter.execute(SetupLoggingActionExecuter.java:30)\r\n\tat org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:67)\r\n\tat org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36)\r\n\tat org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:122)\r\n\tat org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:37)\r\n\tat org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:122)\r\n\tat org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26)\r\n\tat org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:122)\r\n\tat org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34)\r\n\tat org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:122)\r\n\tat org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74)\r\n\tat org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72)\r\n\tat org.gradle.util.Swapper.swap(Swapper.java:38)\r\n\tat org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72)\r\n\tat org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:122)\r\n\tat org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55)\r\n\tat org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:122)\r\n\tat org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:62)\r\n\tat org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36)\r\n\tat org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:122)\r\n\tat org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:82)\r\n\tat org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36)\r\n\tat org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:122)\r\n\tat org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50)\r\n\tat org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:295)\r\n\tat org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63)\r\n\tat org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:46)\r\n\tat org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:55)\r\nCaused by: java.lang.NullPointerException\r\n\tat com.google.common.base.Preconditions.checkNotNull(Preconditions.java:782)\r\n\tat com.google.common.base.Splitter.split(Splitter.java:376)\r\n\tat com.android.utils.PathUtils.getClassPathItems(PathUtils.java:84)\r\n\tat com.android.build.gradle.internal.transforms.FixStackFramesTransform.<init>(FixStackFramesTransform.java:151)\r\n\tat com.android.build.gradle.internal.TaskManager.maybeCreateDesugarTask(TaskManager.java:2425)\r\n\tat com.android.build.gradle.internal.TaskManager.createPostCompilationTasks(TaskManager.java:2248)\r\n\tat com.android.build.gradle.internal.ApplicationTaskManager.addCompileTask(ApplicationTaskManager.java:295)\r\n\tat com.android.build.gradle.internal.ApplicationTaskManager.lambda$createTasksForVariantScope$12(ApplicationTaskManager.java:229)\r\n\tat com.android.builder.profile.ThreadRecorder.record(ThreadRecorder.java:81)\r\n\tat com.android.build.gradle.internal.ApplicationTaskManager.createTasksForVariantScope(ApplicationTaskManager.java:225)\r\n\tat com.android.build.gradle.internal.VariantManager.createTasksForVariantData(VariantManager.java:530)\r\n\tat com.android.build.gradle.internal.VariantManager.lambda$createAndroidTasks$1(VariantManager.java:352)\r\n\tat com.android.builder.profile.ThreadRecorder.record(ThreadRecorder.java:81)\r\n\tat com.android.build.gradle.internal.VariantManager.createAndroidTasks(VariantManager.java:348)\r\n\tat com.android.build.gradle.BasePlugin.lambda$createAndroidTasks$6(BasePlugin.java:751)\r\n\tat com.android.builder.profile.ThreadRecorder.record(ThreadRecorder.java:81)\r\n\tat com.android.build.gradle.BasePlugin.createAndroidTasks(BasePlugin.java:746)\r\n\tat com.android.build.gradle.BasePlugin.lambda$null$4(BasePlugin.java:652)\r\n\tat com.android.builder.profile.ThreadRecorder.record(ThreadRecorder.java:81)\r\n\tat com.android.build.gradle.BasePlugin.lambda$createTasks$5(BasePlugin.java:648)\r\n\tat org.gradle.internal.event.BroadcastDispatch$ActionInvocationHandler.dispatch(BroadcastDispatch.java:91)\r\n\tat org.gradle.internal.event.BroadcastDispatch$ActionInvocationHandler.dispatch(BroadcastDispatch.java:80)\r\n\tat org.gradle.internal.event.AbstractBroadcastDispatch.dispatch(AbstractBroadcastDispatch.java:42)\r\n\tat org.gradle.internal.event.BroadcastDispatch$SingletonDispatch.dispatch(BroadcastDispatch.java:230)\r\n\tat org.gradle.internal.event.BroadcastDispatch$SingletonDispatch.dispatch(BroadcastDispatch.java:149)\r\n\tat org.gradle.internal.event.AbstractBroadcastDispatch.dispatch(AbstractBroadcastDispatch.java:58)\r\n\tat org.gradle.internal.event.BroadcastDispatch$CompositeDispatch.dispatch(BroadcastDispatch.java:324)\r\n\tat org.gradle.internal.event.BroadcastDispatch$CompositeDispatch.dispatch(BroadcastDispatch.java:234)\r\n\tat org.gradle.internal.event.ListenerBroadcast.dispatch(ListenerBroadcast.java:140)\r\n\tat org.gradle.internal.event.ListenerBroadcast.dispatch(ListenerBroadcast.java:37)\r\n\tat org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93)\r\n\tat com.sun.proxy.$Proxy36.afterEvaluate(Unknown Source)\r\n\tat org.gradle.configuration.project.LifecycleProjectEvaluator$1.execute(LifecycleProjectEvaluator.java:83)\r\n\tat org.gradle.configuration.project.LifecycleProjectEvaluator$1.execute(LifecycleProjectEvaluator.java:80)\r\n\tat org.gradle.api.internal.project.DefaultProject.stepEvaluationListener(DefaultProject.java:1393)\r\n\tat org.gradle.configuration.project.LifecycleProjectEvaluator.notifyAfterEvaluate(LifecycleProjectEvaluator.java:80)\r\n\t... 89 more\r\n\r\n\r\n\r\n\r\n```", "comments": ["Hi @sachinprasadhs! Could you please look at this issue?"]}, {"number": 53160, "title": "MirroredStrategy: Efficient allreduce is not supported for n IndexedSlices", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Centos 7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.4.4\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): No\r\n- GCC/Compiler version (if compiling from source): No\r\n- CUDA/cuDNN version: cuda11.0+cudnn8\r\n- GPU model and memory: V100+32GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nWith Embedding layer, MirroredStrategy will report a warning: `MirroredStrategy: Efficient allreduce is not supported for n IndexedSlices`.\r\nTrianing speed can scale up with 2-4 GPUs, but as we increase GPU devices, the training speed can not scale up anymore.\r\nBesides, the GPU utilization is quite low if we add more GPUs (can jump from 0 ~ 100%).\r\n\r\nI have search the Internet and find some workaround, e.g. #41898. But even if I change MirroredStrategy with MultiWorkerMirroredStrategy, the GPU utilization is quite low and the training time is nearly the same as MirroredStrategy.\r\n\r\n**Describe the expected behavior**\r\nIn single machine with multiple GPUs, can use MirroredStrategy to scale up training speed with IndexedSlices (Embedding Layer).\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\nBorrow from @ratovarius\r\n```\r\nimport sys\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef build_model_():\r\n\r\n\tinput_a_size = 20\r\n\tinput_b_size = 4\r\n\tnum_classes = 2\r\n\tlen_embedding = 256\r\n\r\n\tinput_a = tf.keras.layers.Input(shape=(input_a_size,), name='input_a', dtype=np.uint8)\r\n\tinput_b = tf.keras.layers.Input(shape=(input_b_size,), name='input_b', dtype=np.float32)\r\n\r\n\tx = tf.keras.layers.Embedding(len_embedding, 100)(input_a)\r\n\tx = tf.keras.layers.Conv1D(128, 4, activation='relu')(x)\r\n\tx = tf.keras.layers.MaxPooling1D(4)(x)\r\n\tx = tf.keras.layers.Flatten()(x)\r\n\tbranch_a = tf.keras.layers.Dense(64, activation='relu')(x)\r\n\r\n\tx = tf.keras.layers.Dense(32, activation='relu')(input_b)\r\n\tbranch_b = tf.keras.layers.Dense(32, activation='relu')(x)\r\n\r\n\tconcat = tf.keras.layers.Concatenate()([\r\n\t\t\t\t                            branch_a,\r\n\t\t\t\t                            branch_b,\r\n\t\t\t\t                           ])\r\n\r\n\tx = tf.keras.layers.Dense(512, activation = 'relu')(concat)\r\n\toutput = tf.keras.layers.Dense(num_classes, name='output', activation='softmax')(x)\r\n\r\n\tmodel = tf.keras.models.Model(inputs=[\r\n\t\t\t\t                          input_a,\r\n\t\t\t\t                          input_b,\r\n\t\t\t\t                         ],\r\n\t\t\t\t                  outputs=[output])\r\n\r\n\treturn model\r\n\r\nstrategy = tf.distribute.MirroredStrategy(['/gpu:0', '/gpu:1'])\r\nwith strategy.scope():\r\n    model = build_model_()\r\n    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\r\n\r\ny_train = True\r\ny_train = tf.keras.utils.to_categorical(y_train, 2)\r\n\r\ndataset = tf.data.Dataset.from_tensors(\r\n    (\r\n        {\"input_a\": [[1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.]], \r\n         \"input_b\": [[1.], [1.], [1.], [1.]],}, \r\n        {\"output\": y_train},\r\n    )\r\n).repeat(1000000).batch(256)\r\n\r\nhistory = model.fit(\r\n    x = dataset,\r\n    epochs=10,\r\n    verbose = 1,\r\n)\r\n\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@SysuJayce Could you please try with the latest stable TF version `2.7.0` and refer to the similar[ issue](https://stackoverflow.com/questions/63034145/warningtensorflowefficient-allreduce-is-not-supported-for-1-indexedslices), [issue1](https://github.com/tensorflow/tensorflow/issues/41898),[ issue2](https://github.com/tensorflow/tensorflow/issues/39545) and let us know if it helps? For further queries please post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\nThank you!", "> @SysuJayce Could you please try with the latest stable TF version `2.7.0` and refer to the similar[ issue](https://stackoverflow.com/questions/63034145/warningtensorflowefficient-allreduce-is-not-supported-for-1-indexedslices), [issue1](https://github.com/tensorflow/tensorflow/issues/41898),[ issue2](https://github.com/tensorflow/tensorflow/issues/39545) and let us know if it helps? For further queries please post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) To know more see; https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999 Thank you!\r\n\r\n@sushreebarsa Hi, I have tried tf2.7.0 and MultiWorkerMirroredStrategy. But as I mentioned above, the GPU utilization is low and the training speed is not scaled up.\r\n\r\nActually, in issue #39545, @dubey and @byronyi have given some suggestion. For instance, convert sparse tensor to dense one. But I don't know how to do this because I do not use sparse tensor myself.\r\n\r\nI dig the tensorflow source code and find that sparse allreduce is used and I guess it is because tensorflow use sparse tensor in Embedding layer.\r\nhttps://github.com/tensorflow/tensorflow/blob/c256c071bb26e1e13b4666d1b3e229e110bc914a/tensorflow/python/distribute/cross_device_ops.py#L929\r\n\r\nDo we need to post distributed training performance issue on keras repo now ?", "Hello, @sushreebarsa @jvishnuvardhan .\r\nIt's been 2 days. Is there any progress? \r\n\r\nI think this is a common use case in NLP, why nobody in TF team cares about it?", "> the GPU utilization is low and the training speed is not scaled up.\r\n\r\nYou are using multi-host training and the performance will be depending on multiple factors, including your network architecture, available bandwidth, system bus topology, etc. For us, It is rather difficult to provide a universally applicable tuning guide for all possible scenario.\r\n\r\nYou could start by pasting your [nccl-test](https://github.com/NVIDIA/nccl-tests) benchmark results, assuming your are using NCCL for multi-host all-reduce.", "> > the GPU utilization is low and the training speed is not scaled up.\r\n> \r\n> You are using multi-host training and the performance will be depending on multiple factors, including your network architecture, available bandwidth, system bus topology, etc. For us, It is rather difficult to provide a universally applicable tuning guide for all possible scenario.\r\n> \r\n> You could start by pasting your [nccl-test](https://github.com/NVIDIA/nccl-tests) benchmark results, assuming your are using NCCL for multi-host all-reduce.\r\n\r\nHi @byronyi, I am currently using single host with multiple GPUs.\r\n\r\nIf I set embedding layer untrainable, everything is fine, the GPU utilization in `MultiWorkerMirroredStrategy` and `MirroredStrategy` can reach nearly 100% all the time.\r\nBut if I set embedding layer trainable, the warning appears and the GPU utilization drops as the number of GPUs increasing.\r\n\r\n\r\nAs the warning saying, `Efficient allreduce is not supported for n IndexedSlices`, is there any way that we can use `MirroredStrategy` to train network with embedding layer since `MultiWorkerMirroredStrategy` is not as performant as `MirroredStrategy` in single machine environment?\r\n\r\n> You could start by pasting your nccl-test benchmark results\r\n\r\nThanks! I'll do this test later.", "If you are using single host, switch to mirrored strategy. Are you using NVLink/NVSwitch or PCIe for your BUS topology? It'd be helpful to set `NCCL_DEBUG=INFO` and paste the NCCL log here.\r\n\r\nBtw, GPU utilization says little about the actual performance. Take a look at your GPU power consumption; if it is stuck on NCCL deadlock, the utilization could reach 100% while the power consumption stays relatively low.", "> If you are using single host, switch to mirrored strategy. Are you using NVLink/NVSwitch or PCIe for your BUS topology? It'd be helpful to set `NCCL_DEBUG=INFO` and paste the NCCL log here.\r\n> \r\n> Btw, GPU utilization says little about the actual performance. Take a look at your GPU power consumption; if it is stuck on NCCL deadlock, the utilization could reach 100% while the power consumption stays relatively low.\r\n\r\nHi @byronyi , I did the test in a machine with P40 GPUs, but in another machine with V100 GPUs, the problem exists too.\r\n\r\nthe system env is:\r\n```\r\nNV_LIBCUBLAS_VERSION=11.2.0.252-1\r\nNVIDIA_VISIBLE_DEVICES=all\r\nNV_NVML_DEV_VERSION=11.0.167-1\r\nNV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.11.4-1+cuda11.0\r\nNV_LIBNCCL_DEV_PACKAGE_VERSION=2.11.4-1\r\nNVIDIA_REQUIRE_CUDA=cuda>=11.0 brand=tesla,driver>=418,driver<419 brand=tesla,driver>=440,driver<441 driver>=450\r\nNV_LIBCUBLAS_DEV_PACKAGE=libcublas-dev-11-0=11.2.0.252-1\r\nNV_NVTX_VERSION=11.0.167-1\r\nNV_ML_REPO_ENABLED=1\r\nNV_CUDA_CUDART_DEV_VERSION=11.0.221-1\r\nNV_LIBCUSPARSE_VERSION=11.1.1.245-1\r\nNV_LIBNPP_VERSION=11.1.0.245-1\r\nNCCL_VERSION=2.11.4-1\r\nNVIDIA_DRIVER_CAPABILITIES=compute,utility\r\nNV_LIBNPP_PACKAGE=libnpp-11-0=11.1.0.245-1\r\nNV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\r\nNV_LIBCUBLAS_DEV_VERSION=11.2.0.252-1\r\nNV_LIBCUBLAS_DEV_PACKAGE_NAME=libcublas-dev-11-0\r\nNV_CUDA_CUDART_VERSION=11.0.221-1\r\nCUDA_VERSION=11.0.3\r\nNV_LIBCUBLAS_PACKAGE=libcublas-11-0=11.2.0.252-1\r\nNV_LIBNPP_DEV_PACKAGE=libnpp-dev-11-0=11.1.0.245-1\r\nNV_LIBCUBLAS_PACKAGE_NAME=libcublas-11-0\r\nNV_LIBNPP_DEV_VERSION=11.1.0.245-1\r\nNV_LIBCUSPARSE_DEV_VERSION=11.1.1.245-1\r\nLIBRARY_PATH=/usr/local/cuda/lib64/stubs\r\nSHLVL=1\r\nNV_CUDA_LIB_VERSION=11.0.3-1\r\nNVARCH=x86_64\r\nNV_CUDA_COMPAT_PACKAGE=cuda-compat-11-0\r\nNV_LIBNCCL_PACKAGE=libnccl2=2.11.4-1+cuda11.0\r\nLD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\nPATH=/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\r\nNV_LIBNCCL_PACKAGE_NAME=libnccl2\r\nNV_LIBNCCL_PACKAGE_VERSION=2.11.4-1\r\n```\r\n\r\nthe topology is:\r\n```\r\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    CPU Affinity    NUMA Affinity\r\nGPU0     X      PIX     PHB     PHB     SYS     SYS     SYS     SYS     0-13,28-41      0\r\nGPU1    PIX      X      PHB     PHB     SYS     SYS     SYS     SYS     0-13,28-41      0\r\nGPU2    PHB     PHB      X      PIX     SYS     SYS     SYS     SYS     0-13,28-41      0\r\nGPU3    PHB     PHB     PIX      X      SYS     SYS     SYS     SYS     0-13,28-41      0\r\nGPU4    SYS     SYS     SYS     SYS      X      PIX     PHB     PHB     14-27,42-55     1\r\nGPU5    SYS     SYS     SYS     SYS     PIX      X      PHB     PHB     14-27,42-55     1\r\nGPU6    SYS     SYS     SYS     SYS     PHB     PHB      X      PIX     14-27,42-55     1\r\nGPU7    SYS     SYS     SYS     SYS     PHB     PHB     PIX      X      14-27,42-55     1\r\n```\r\n\r\nwith GPU 4-7, the NCCL_DEBUG msg is:\r\n```\r\nWARNING:tensorflow:Efficient allreduce is not supported for 2 IndexedSlices\r\nWARNING:tensorflow:Efficient allreduce is not supported for 2 IndexedSlices\r\nWARNING:tensorflow:Efficient allreduce is not supported for 2 IndexedSlices\r\nWARNING:tensorflow:Efficient allreduce is not supported for 2 IndexedSlices\r\ndbe753952c3d:1013:1261 [0] NCCL INFO Bootstrap : Using [0]eth0:172.17.0.23<0>\r\ndbe753952c3d:1013:1261 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\r\ndbe753952c3d:1013:1261 [0] NCCL INFO NET/IB : No device found.\r\ndbe753952c3d:1013:1261 [0] NCCL INFO NET/Socket : Using [0]eth0:172.17.0.23<0>\r\ndbe753952c3d:1013:1261 [0] NCCL INFO Using network Socket\r\nNCCL version 2.7.6+cudaCUDA_MAJOR.CUDA_MINOR\r\ndbe753952c3d:1013:1420 [0] NCCL INFO Channel 00/02 :    0   1   2   3\r\ndbe753952c3d:1013:1421 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/64\r\ndbe753952c3d:1013:1423 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/64\r\ndbe753952c3d:1013:1422 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/64\r\ndbe753952c3d:1013:1420 [0] NCCL INFO Channel 01/02 :    0   1   2   3\r\ndbe753952c3d:1013:1421 [1] NCCL INFO Trees [0] 2/-1/-1->1->0|0->1->2/-1/-1 [1] 2/-1/-1->1->0|0->1->2/-1/-1\r\ndbe753952c3d:1013:1423 [3] NCCL INFO Trees [0] -1/-1/-1->3->2|2->3->-1/-1/-1 [1] -1/-1/-1->3->2|2->3->-1/-1/-1\r\ndbe753952c3d:1013:1422 [2] NCCL INFO Trees [0] 3/-1/-1->2->1|1->2->3/-1/-1 [1] 3/-1/-1->2->1|1->2->3/-1/-1\r\ndbe753952c3d:1013:1423 [3] NCCL INFO Setting affinity for GPU 7 to fffc00,0fffc000\r\ndbe753952c3d:1013:1422 [2] NCCL INFO Setting affinity for GPU 6 to fffc00,0fffc000\r\ndbe753952c3d:1013:1421 [1] NCCL INFO Setting affinity for GPU 5 to fffc00,0fffc000\r\ndbe753952c3d:1013:1420 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/64\r\ndbe753952c3d:1013:1420 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1|-1->0->1/-1/-1 [1] 1/-1/-1->0->-1|-1->0->1/-1/-1\r\ndbe753952c3d:1013:1420 [0] NCCL INFO Setting affinity for GPU 4 to fffc00,0fffc000\r\ndbe753952c3d:1013:1421 [1] NCCL INFO Channel 00 : 1[86000] -> 2[89000] via direct shared memory\r\ndbe753952c3d:1013:1423 [3] NCCL INFO Channel 00 : 3[8a000] -> 0[85000] via direct shared memory\r\ndbe753952c3d:1013:1422 [2] NCCL INFO Channel 00 : 2[89000] -> 3[8a000] via P2P/direct pointer\r\ndbe753952c3d:1013:1420 [0] NCCL INFO Channel 00 : 0[85000] -> 1[86000] via P2P/direct pointer\r\ndbe753952c3d:1013:1423 [3] NCCL INFO Channel 00 : 3[8a000] -> 2[89000] via P2P/direct pointer\r\ndbe753952c3d:1013:1422 [2] NCCL INFO Channel 00 : 2[89000] -> 1[86000] via direct shared memory\r\ndbe753952c3d:1013:1421 [1] NCCL INFO Channel 00 : 1[86000] -> 0[85000] via P2P/direct pointer\r\ndbe753952c3d:1013:1423 [3] NCCL INFO Channel 01 : 3[8a000] -> 0[85000] via direct shared memory\r\ndbe753952c3d:1013:1421 [1] NCCL INFO Channel 01 : 1[86000] -> 2[89000] via direct shared memory\r\ndbe753952c3d:1013:1420 [0] NCCL INFO Channel 01 : 0[85000] -> 1[86000] via P2P/direct pointer\r\ndbe753952c3d:1013:1422 [2] NCCL INFO Channel 01 : 2[89000] -> 3[8a000] via P2P/direct pointer\r\ndbe753952c3d:1013:1423 [3] NCCL INFO Channel 01 : 3[8a000] -> 2[89000] via P2P/direct pointer\r\ndbe753952c3d:1013:1423 [3] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\r\ndbe753952c3d:1013:1422 [2] NCCL INFO Channel 01 : 2[89000] -> 1[86000] via direct shared memory\r\ndbe753952c3d:1013:1421 [1] NCCL INFO Channel 01 : 1[86000] -> 0[85000] via P2P/direct pointer\r\ndbe753952c3d:1013:1422 [2] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\r\ndbe753952c3d:1013:1420 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\r\ndbe753952c3d:1013:1421 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\r\ndbe753952c3d:1013:1420 [0] NCCL INFO comm 0x7d03a00010d0 rank 0 nranks 4 cudaDev 0 busId 85000 - Init COMPLETE\r\ndbe753952c3d:1013:1423 [3] NCCL INFO comm 0x7d038c0010a0 rank 3 nranks 4 cudaDev 3 busId 8a000 - Init COMPLETE\r\ndbe753952c3d:1013:1422 [2] NCCL INFO comm 0x7d03980010a0 rank 2 nranks 4 cudaDev 2 busId 89000 - Init COMPLETE\r\ndbe753952c3d:1013:1421 [1] NCCL INFO comm 0x7d03940010a0 rank 1 nranks 4 cudaDev 1 busId 86000 - Init COMPLETE\r\n```\r\n\r\nthe nccl-test info:\r\n```\r\nroot@dbe753952c3d:/workspace/nccl-tests-master# ./build/all_reduce_perf -b 8 -e 128M -f 2 -g 8\r\n# nThread 1 nGpus 8 minBytes 8 maxBytes 134217728 step: 2(factor) warmup iters: 5 iters: 20 validation: 1 \r\n#\r\n# Using devices\r\n#   Rank  0 Pid   1524 on dbe753952c3d device  0 [0x04] Tesla P40\r\n#   Rank  1 Pid   1524 on dbe753952c3d device  1 [0x05] Tesla P40\r\n#   Rank  2 Pid   1524 on dbe753952c3d device  2 [0x08] Tesla P40\r\n#   Rank  3 Pid   1524 on dbe753952c3d device  3 [0x09] Tesla P40\r\n#   Rank  4 Pid   1524 on dbe753952c3d device  4 [0x85] Tesla P40\r\n#   Rank  5 Pid   1524 on dbe753952c3d device  5 [0x86] Tesla P40\r\n#   Rank  6 Pid   1524 on dbe753952c3d device  6 [0x89] Tesla P40\r\n#   Rank  7 Pid   1524 on dbe753952c3d device  7 [0x8a] Tesla P40\r\n#\r\n#                                                       out-of-place                       in-place          \r\n#       size         count      type   redop     time   algbw   busbw  error     time   algbw   busbw  error\r\n#        (B)    (elements)                       (us)  (GB/s)  (GB/s)            (us)  (GB/s)  (GB/s)       \r\n           8             2     float     sum    131.0    0.00    0.00  1e-07    133.7    0.00    0.00  1e-07\r\n          16             4     float     sum    368.5    0.00    0.00  1e-07    157.5    0.00    0.00  1e-07\r\n          32             8     float     sum    407.3    0.00    0.00  6e-08    45.10    0.00    0.00  6e-08\r\n          64            16     float     sum    40.39    0.00    0.00  6e-08    53.87    0.00    0.00  6e-08\r\n         128            32     float     sum    54.46    0.00    0.00  6e-08    141.0    0.00    0.00  6e-08\r\n         256            64     float     sum    130.5    0.00    0.00  3e-08    134.8    0.00    0.00  3e-08\r\n         512           128     float     sum    138.7    0.00    0.01  3e-08    325.2    0.00    0.00  3e-08\r\n        1024           256     float     sum    51.86    0.02    0.03  1e-07    35.52    0.03    0.05  1e-07\r\n        2048           512     float     sum    72.45    0.03    0.05  2e-07    33.34    0.06    0.11  2e-07\r\n        4096          1024     float     sum    150.1    0.03    0.05  2e-07   1004.7    0.00    0.01  2e-07\r\n        8192          2048     float     sum    149.4    0.05    0.10  2e-07    187.6    0.04    0.08  2e-07\r\n       16384          4096     float     sum    179.0    0.09    0.16  2e-07    171.1    0.10    0.17  2e-07\r\n       32768          8192     float     sum    420.5    0.08    0.14  2e-07    157.6    0.21    0.36  2e-07\r\n       65536         16384     float     sum    168.8    0.39    0.68  2e-07    171.3    0.38    0.67  2e-07\r\n      131072         32768     float     sum    339.6    0.39    0.68  2e-07    352.6    0.37    0.65  2e-07\r\n      262144         65536     float     sum    356.8    0.73    1.29  2e-07    600.6    0.44    0.76  2e-07\r\n      524288        131072     float     sum    568.6    0.92    1.61  2e-07    412.5    1.27    2.22  2e-07\r\n     1048576        262144     float     sum   1064.3    0.99    1.72  2e-07   2835.2    0.37    0.65  2e-07\r\n     2097152        524288     float     sum   2624.7    0.80    1.40  2e-07   2075.9    1.01    1.77  2e-07\r\n     4194304       1048576     float     sum   7038.9    0.60    1.04  2e-07   2286.6    1.83    3.21  2e-07\r\n     8388608       2097152     float     sum   7334.6    1.14    2.00  2e-07   6914.2    1.21    2.12  2e-07\r\n    16777216       4194304     float     sum   9180.5    1.83    3.20  2e-07   9864.8    1.70    2.98  2e-07\r\n    33554432       8388608     float     sum    16978    1.98    3.46  2e-07    15446    2.17    3.80  2e-07\r\n    67108864      16777216     float     sum    29175    2.30    4.03  2e-07    26977    2.49    4.35  2e-07\r\n   134217728      33554432     float     sum    63809    2.10    3.68  2e-07    62694    2.14    3.75  2e-07\r\n# Out of bounds values : 0 OK\r\n# Avg bus bandwidth    : 1.06089 \r\n```", "Your bus topology is suboptimal for all-reduce between 8 GPUs across multiple NUMA nodes. Mind to share all_reduce_perf between GPU 0-3 or 4-7?", "> Your bus topology is suboptimal for all-reduce between 8 GPUs across multiple NUMA nodes. Mind to share all_reduce_perf between GPU 0-3 or 4-7?\r\n\r\nYes, that's why I use GPUs 4-7 to train my network in the above comment.\r\n\r\nBelow is the topo of another machine with V100, but the problem exists too.\r\n```\r\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    CPU Affinity    NUMA Affinity\r\nGPU0     X      NV1     NV1     NV2     NV2     SYS     SYS     SYS     0-19,40-59      0\r\nGPU1    NV1      X      NV2     NV1     SYS     NV2     SYS     SYS     0-19,40-59      0\r\nGPU2    NV1     NV2      X      NV2     SYS     SYS     NV1     SYS     0-19,40-59      0\r\nGPU3    NV2     NV1     NV2      X      SYS     SYS     SYS     NV1     0-19,40-59      0\r\nGPU4    NV2     SYS     SYS     SYS      X      NV1     NV1     NV2     20-39,60-79     1\r\nGPU5    SYS     NV2     SYS     SYS     NV1      X      NV2     NV1     20-39,60-79     1\r\nGPU6    SYS     SYS     NV1     SYS     NV1     NV2      X      NV2     20-39,60-79     1\r\nGPU7    SYS     SYS     SYS     NV1     NV2     NV1     NV2      X      20-39,60-79     1\r\n```", "Your V100 machine has NVLink. \r\n\r\nTo me that seems to be a TF related issue. Mind to share a small model training script to reproduce the issue? Thanks.", "> Your V100 machine has NVLink.\r\n>\r\n> To me that seems to be a TF related issue. Mind to share a small model training script to reproduce the issue? Thanks.\r\n\r\nHi @byronyi , \r\n```\r\nimport os\r\nimport sys\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\ndef build_model():\r\n    input_a = tf.keras.Input(shape=(256,), dtype=tf.int64, name=\"input_a\")\r\n    input_b = tf.keras.Input(shape=(256,), dtype=tf.int64, name=\"input_b\")\r\n\r\n    emb_a = tf.keras.layers.Embedding(\r\n        input_dim=20000,\r\n        output_dim=128,\r\n        input_length=256,\r\n        embeddings_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02),\r\n        trainable=True)\r\n    emb_b = tf.keras.layers.Embedding(\r\n        input_dim=20000,\r\n        output_dim=128,\r\n        input_length=256,\r\n        embeddings_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02),\r\n        trainable=True)\r\n\r\n    x = tf.keras.layers.add([emb_a(input_a), emb_b(input_b)])\r\n    x = tf.keras.layers.Conv1D(filters=128,\r\n                               kernel_size=3,\r\n                               activation=\"relu\")(x)\r\n    x = tf.keras.layers.Conv1D(filters=128,\r\n                               kernel_size=3,\r\n                               activation=\"relu\")(x)\r\n    x = tf.keras.layers.Conv1D(filters=128,\r\n                               kernel_size=3,\r\n                               activation=\"relu\")(x)\r\n    x = tf.keras.layers.Conv1D(filters=128,\r\n                               kernel_size=3,\r\n                               activation=\"relu\")(x)\r\n    x = tf.keras.layers.Conv1D(filters=128,\r\n                               kernel_size=3,\r\n                               activation=\"relu\")(x)\r\n    x = tf.keras.layers.GlobalMaxPooling1D()(x)\r\n    x = tf.keras.layers.Dense(64, activation='relu')(x)\r\n    output = tf.keras.layers.Dense(1, activation='sigmoid', name=\"output\")(x)\r\n\r\n    model = tf.keras.models.Model(\r\n        inputs=[\r\n            input_a,\r\n            input_b,\r\n        ],\r\n        outputs=output\r\n    )\r\n\r\n    return model\r\n\r\n\r\ndef main():\r\n    assert len(sys.argv) == 2\r\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = sys.argv[1]\r\n    gpus = sys.argv[1].split(\",\")\r\n    if len(gpus) == 1:\r\n        strategy = tf.distribute.get_strategy()\r\n    else:\r\n        strategy = tf.distribute.MirroredStrategy()\r\n\r\n    with strategy.scope():\r\n        model = build_model()\r\n        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n        model.summary()\r\n\r\n    batch_size = 1024 * strategy.num_replicas_in_sync\r\n    dataset = tf.data.Dataset.from_tensors(\r\n        (\r\n            {\"input_a\": np.random.randint(0, 20001, (256,), int),\r\n             \"input_b\": np.random.randint(0, 20001, (256,), int)},\r\n            {\"output\": np.random.randint(0, 2, (1,), int)},\r\n        )\r\n    ).repeat(1000000).batch(batch_size)\r\n\r\n    model.fit(\r\n        x=dataset,\r\n        epochs=10,\r\n        verbose=1,\r\n    )\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n\r\n```\r\nIf we save the code in `main.py`, we use an additional arg to specify the GPUs to use, each GPU idx is separated by a comma.\r\n\r\n1. `python3 main.py 1 `\r\n\r\n   > GPU utilization: ~80%\r\n   >\r\n   > Speed: 121s/epoch\r\n\r\n2. `python3 main.py 2,3`\r\n\r\n   > GPU utilization: ~90% for the 1st, ~80% for the 2nd\r\n   >\r\n   > Speed: 34s/epoch\r\n\r\n3. `python3 main.py 4,5,6,7`\r\n\r\n   > GPU utilization: ~95% for the 1st, ~38% for the others\r\n   >\r\n   > Speed: 43s/epoch\r\n\r\n4. `python3 main.py 0,1,2,3,4,5,6,7 `\r\n\r\n   > GPU utilization: 95%-100% for the 1st, 0-70% for the others, and there are always some GPUs have 0 utilization.\r\n   >\r\n   > Speed: 135s/epoch", "So using 2 GPUs the performance is more than 2x compared to using single GPU? That's weird...", "> So using 2 GPUs the performance is more than 2x compared to using single GPU? That's weird...\r\n\r\nYes, it is weird. It is really disappointing that MirroredStrategy can not handle this situation :(\r\n\r\nIf you can't reproduce this issue, ask me to provide anything you need.", "@byronyi @jvishnuvardhan Hi, is there any progress? What should I do to help you solve this problem?", "It's been a month and yet no one comes to solve this problem :("]}, {"number": 53159, "title": "Classify issues using more specific labels", "body": "Hi all,\r\n\r\nbased on the discussion on TF SIG build meeting, raising this issue here.\r\n\r\nIt would be great if TensorFlow issue tracker would have more specific labels so issues could be classified and browsed only based on some specific criteria. An example of labels that would be very helpful for us:\r\n\r\n1. labels specific to dependency issues on Python layer (dependency compatibility issues, underpinning/overpining, ...)\r\n2. labels specific to runtime/buildtime environment issues on software layer ouside of Python libs (python interpreter incompatibilities, CUDA issues, glibc issues, ...)\r\n3. labels specific to runtime/buildtime environment issues on hardware layer (GPU issues, ...)\r\n\r\nWe would like to consume labeled issues - combining efforts on this front would be very helpful for us (Red Hat) to provide better environments to TensorFlow open-source community.\r\n\r\nThanks in advance for any response.", "comments": ["Thanks for adding this as an issue for us to discuss. Can you please review the existing [labels](https://github.com/tensorflow/tensorflow/labels) and give us some specific recommendations. For example, we have used a common convention to create label categories using the syntax <category>:<feature> and have categories that we use in our automation processes (ie., \"cla\" or \"comp\").\r\n\r\nAre you suggesting that we add new labels in a specific category, such as \"type: python\" or \"type: cuda\" or something else? Thanks!\r\n", "@fridex I don't know if you are interested to have these set of labels for TF only or also for the `strict ecosystem` TF+TF SIGs repos. \r\nIf the case is the last one I think that could be useful to have a common label subset in sync on TF+SIGs with some syncing solution e.g.:\r\n\r\n- https://label-sync.com/\r\n- https://github.com/micnncim/action-label-syncer \r\n- etc...", "@bhack or we learn from the Kubernetes ecosystem and their way to have labels declared for the org, or for individual repos, its even generating a markdown documentation of the labels. see https://github.com/kubernetes/test-infra/tree/master/label_sync", "The Kubernetes example is quite clear as they have also the \"orchestration yaml\" in the repository. \n\nGenerally the orchestration isn't always clear in the TF CI infra so sometimes it is a little bit harder to contribute.\n\nI am fine with every transparent approach where the contribution path is clear ", "To keep this updated: we will try to scope a label proposal for this till the next SIG build meeting and update this issue. Thanks a lot for cooperating on this and being open to inputs from the community.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "We have created a set of labels that would help us to consume issues:\r\n\r\n- `env:runtime` - issues that araise on runtime\r\n- `env:buildtime` - issues that arise on buildtime\r\n- `env:hardware` - issues that are caused by hardware and hardware configuration\r\n- `env:python-lib` - issues specific to Python packages that TensorFlow depends on (ex. numpy specific issues)\r\n- `env:native-lib` - issues specific to native dependencies (ex. CUDA. cuDNN, glibc, ...)\r\n  - `env:cudnn`, `env:cuda` - issues specific to CUDA and cuDNN\r\n\r\nThese are just proposed, feel free to rename them based on your preferences (not sure about prefixes). You are probably using some of them already - for example `type:build/install` label. If they match the proposed one, we can reuse already existing ones - just to make sure we can map our expectations to these labels.\r\n\r\nThanks again. Please let us know if there is a need to clarify something or any additional input from our end is needed.", "Maybe the reason for this - we are developing a [Python resolver](https://thoth-station.ninja/) that runs in the cloud. It can resolve application dependencies based on prescribed rules, the resolver is [offered to the community](https://www.youtube.com/watch?v=UCt37waSa6g). See [example rules for TensorFlow](https://github.com/thoth-station/prescriptions/tree/master/prescriptions/te_/tensorflow) for some of the rules to resolve specifically for a TensorFlow application. The database of resolver rules (\"[prescriptions](https://github.com/thoth-station/prescriptions)\") is available publicly - we would like to enhance it based on the issue classification proposed. If issues arise with TensorFlow, we can try to configure the resolver to resolve application dependencies so that users do not encounter known issues. The [following tutorial](https://redhat-scholars.github.io/managing-vulnerabilities-with-thoth/managing-vulnerabilities-with-thoth/index.html) walks through some security aspects of the resolver. We are open to combining efforts."]}, {"number": 53158, "title": "Update documentation of `DataFormatVecPermute` operation", "body": "Resolves #53157 \r\ncc @bixia1 \r\n\r\n- Fixes invalid shape in the example.\r\n- Updated description of supported inputs.\r\n\r\nOnce this updated documentation is reviewed, should I generate `wrappers.go` and `tf_generated_ops.td`?", "comments": ["@rohan100jain  Can you please review this PR ? Thank you!"]}, {"number": 53157, "title": "Outdated documentation for `DataFormatVecPermute`", "body": "This came up while creating a TF-TRT converter for `DataFormatVecPermute` in #52942 \r\ncc @bixia1 \r\n\r\n## URL(s) with the issue:\r\n\r\n - Python: https://www.tensorflow.org/api_docs/python/tf/raw_ops/DataFormatVecPermute\r\n - C++: https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/data-format-vec-permute\r\n - Java: https://www.tensorflow.org/jvm/api_docs/java/org/tensorflow/op/nn/DataFormatVecPermute\r\n\r\n## Description of the issue (what needs changing):\r\n\r\n- Examples imply that an input of shape `(2, 4)` is valid, but that should be `(4, 2)`.\r\n- The format strings can be of size 4 or 5 (e.g `\"NHWC\"`, `\"NDHWC\"`).\r\n- The first dimension of the input shape vector can be `src_format.size()` or `src_format.size() - 2`, in which case it is assumed that non-spatial dimensions are omitted.\r\n\r\nAs an example, here is a valid Python code that is not covered by the documentation:\r\n\r\n```python\r\nimport tensorflow as tf\r\na = tf.constant([1, 2, 3, 4, 5])\r\nprint(tf.raw_ops.DataFormatVecPermute(x=a, src_format=\"NDHWC\", dst_format=\"NCDHW\"))\r\n```\r\nOutput:\r\n```\r\ntf.Tensor([1 5 2 3 4], shape=(5,), dtype=int32)\r\n```\r\n\r\n## Submit a pull request?\r\n\r\nI'm planning to submit a PR.", "comments": ["Hi @Nyrio ! This issue will be closed once the PR #53158  is merged. Thanks!", "Hi @Nyrio ! Is this issue good to close now ? As the above PR seems to be merged now . Thanks!", "@mohantym No, that's a different PR. #53158 is the one that closes this issue.", "Ok @Nyrio! Thanks for the confirmation! "]}, {"number": 53149, "title": "dataset as_numpy_iterator() failing when using RaggedTensor", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 11.6 / docker tensorflow:2.5.0-gpu\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.5.0 and 2.6.0\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nCalling `as_numpy_iterator()` on a `tf.data.Dataset` having `RaggedTensorSpec` will fail with the error:\r\n\r\n```\r\n  File \"/Users/xx/Library/Caches/pypoetry/virtualenvs/sandbox-hasb3I3q-py3.6/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 4685, in to_numpy\r\n    numpy = x._numpy()  # pylint: disable=protected-access\r\nAttributeError: 'RaggedTensor' object has no attribute '_numpy'\r\n```\r\nSample code to reproduce the issue:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ndef gen():\r\n    ragged_tensor = tf.ragged.constant([[1, 2], [3]])\r\n    yield 42, ragged_tensor\r\n\r\ndataset = tf.data.Dataset.from_generator(gen, output_signature=(\r\n                tf.TensorSpec(shape=(), dtype=tf.int32),\r\n                tf.RaggedTensorSpec(shape=(2, None), dtype=tf.int32)))\r\n\r\niterator = dataset.as_numpy_iterator()\r\nprint(iterator.next()) # failing with: AttributeError: 'RaggedTensor' object has no attribute '_numpy'\r\n\r\n```\r\nThis regression has been brought by this [commit](https://github.com/tensorflow/tensorflow/commit/3e98cbc83e73c834d14137b98da0a26d1d8b7034)\r\n\r\n**Describe the expected behavior**\r\n\r\nReturning the numpy iterator as for `Tensor` without failing.\r\nOR: providing a workaround or alternative usage to make it work.\r\n\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["\r\nI was able to reproduce the issue in tf v2.6, v2.7, v2.8 and nightly(2.10.0-dev20220407).Please find the gist of [here](https://colab.research.google.com/gist/chunduriv/3a1bfc4d3bcf1abb2025300e06a214fa/53149.ipynb).", "@kingatlas,\r\n\r\nInstead of using `iterator = dataset.as_numpy_iterator()` , Can you try using this `iterator = iter(dataset)` to iterate through the dataset? You can take a look at the working [gist here](https://colab.sandbox.google.com/gist/sanatmpa1/174830194b1985587d44230372faa66f/untitled122.ipynb#scrollTo=LRqg1is5v2Os). Thanks!", "@sanatmpa1 the `iter(dataset)` is returning a set of tensors, this is not what I want, I want numpy results (as expected with `as_numpy_iterator`)"]}, {"number": 53146, "title": "Python TFlite not using multiple threads for float inference with xnnpack", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Not really.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Raspberry Pi 64 bit.\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Pi Zero 2 W\r\n- TensorFlow installed from (source or binary): Source.\r\n- TensorFlow version (use command below): 365a3b68471f158defc3aea79a25fdaa56be4ac8 (i.e. few days old)\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): cmake 3.16.3\r\n- GCC/Compiler version (if compiling from source): gcc (Debian 8.3.0-6) 8.3.0\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**Describe the current behavior**\r\n\r\nI've trained MobileNet v3 Small using Tensorflow Object Detection API - models [here](https://github.com/tensorflow/tensorflow/files/7576019/models.zip). When I infer with `./linux_aarch64_benchmark_model --graph=models/mobilenetv3_224x224_FLOAT.tflite --num_threads=4 --use_xnnpack=true` I'm happy - around 25ms inference. Running with 1 thread is 57ms, 2 is 35ms, and 3 is 27ms, which is all expected. However, when I run the same tests with the simple script below, I'm getting 56-57ms seconds for any number of threads. Given this is similar to the single threaded performances from the benchmark tool, it'd imply that only a single thread is being used in python. Note that I am seeing `INFO: Created TensorFlow Lite XNNPACK delegate for CPU.` which seems to imply XNNPACK is being used (though I've also seen that log in the benchmark before it later on complaining and not using XNNPACK - I don't see that here, but I can't seem to get more logging.)\r\n\r\nLastly, it's worth noting that if I use a quantized model in Python, it behaves as expected (i.e. faster with more threads, and matches the benchmark tool). Also, while I compiled the pip package with the provided scripts, I tested the wheel from [here](https://wiki.seeedstudio.com/reTerminal_ML_TFLite/), and had the same issue which is at least another data point.\r\n\r\n**Describe the expected behavior**\r\n\r\nI would expect to see similar behaviour with fp32 inference as with quantized i.e. increased performance with more threads. And I would expect it to match the benchmark tool.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```python\r\nimport time\r\n\r\nimport numpy as np\r\nfrom tflite_runtime.interpreter import Interpreter\r\n\r\ninterpreter = Interpreter(\"models/mobilenetv3_224x224_FLOAT.tflite\", num_threads=4)\r\ninterpreter.allocate_tensors()\r\nfor _ in range(10):\r\n    t0 = time.time()\r\n    interpreter.invoke()\r\n    print(int((time.time() - t0) * 1000))\r\n```\r\n\r\n**Other info / logs** NA\r\n", "comments": ["Thx for reporting this issue! We will look into this and get this fixed asap.", "@kodonnell, I noticed that you performed the experiments on Raspberry Pi 64 bit. Could you re-compile your pip with an additional bazel flag \"--define=tflite_with_xnnpack=true\" which explicitly enables the xnnpack-by-default feature, and then re-run your test program?\r\n\r\nBased on the latest [bazel BUILD rule](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/BUILD#L703-L718), I'm afraid xnnpack-by-default isn't enabled on Raspberry Pi 64 bit. There was [an earlier attempt](https://github.com/tensorflow/tensorflow/commit/b36bc5b0d44a4aa6b1d98aefc835737c4bf16b35) to enable this feature on more platforms, but it's [rolled back](https://github.com/tensorflow/tensorflow/commit/31994539956beac283c064effd0885033a33ab4c) due to some issues that were found later internally.", "Thanks @multiverse-tf - as above, I compiled with cmake, which enables xnnpack by default (and, as above, there\u2019s strong evidence to support xnnpack being used). Note I just built aarch64 I.e. independent of raspberry pi.", "@kodonnell, thanks again for clarification! I failed to reproduce the issue on my side with the [Python label image example](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/python) on a Linux machine (where I explicitly turned on XNNPACK-by-default feature).\r\n\r\nTo help debug the issue further (as you did compilation on your own :-)), could you change the tensorflow  lite source code as follows to add more logs:\r\n* Change [TFLITE_LOG](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/core/subgraph.cc#L427) here in tensorflow/lite/core/subgraph.cc to TFLITE_LOG_PROD.\r\n* Log the [`context_->recommended_num_threads`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/interpreter.cc#L326) here and the resulted `delegate_ptr` value (i.e. simply logging delegate_ptr.get() value itself) by using TFLITE_LOG_PROD?\r\n\r\nAfterwards, pls recompile tflite, re-run your test program and share the log? Many thanks!"]}, {"number": 53144, "title": "PyCharm doesn't resolve anything under tensorflow.keras", "body": "**System information**\r\n- OS Platform and Distribution: Windows 10\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 2.7.0\r\n- Python version: 3.9\r\n- Installed using: Conda\r\n- CUDA/cuDNN version: 11.2/8.1\r\n\r\n\r\n**Describe the problem**\r\nI've upgraded tensorflow from 2.5 to 2.7 and now PyCharm doesn't resolve anything under tensorflow.keras.\r\n\r\n![keras-no-autocompletion](https://user-images.githubusercontent.com/25534110/142740868-4f37d8c6-77ff-4ce4-af24-c48d111e933a.png)\r\n\r\nOther modules of tensorflow work, it's only keras that's problematic. I believe this has something to do with the change in TF 2.6 where keras has been split into a separate PIP package.\r\n\r\nPeople have also been reporting this problem to JetBrains (PyCharm developers): https://youtrack.jetbrains.com/issue/PY-50318\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n1. Create the conda environment:\r\n~ conda create --name tensorflow27\r\n2. Activate it:\r\n~ conda activate tensorflow27\r\n3. Install Python:\r\n~ conda install python=3.9\r\n4. Install TensorFlow:\r\n~ pip install tensorflow\r\n5. Create new project in PyCharm\r\n6. Write this code:\r\n```\r\nimport tensorflow as tf\r\ntf.keras.preprocessing.image_dataset_from_directory()\r\n```\r\n7. When you move mouse to image_dataset_from_directory function there will be no autocomplete.\r\n8. Writing _tf.keras._ yields no modules/functions\r\n\r\n\r\n**Any other info / logs**\r\n/\r\n", "comments": ["Hi @sanatmpa1! Could you please look at this issue?", "@markorakita,\r\n\r\nCan you clarify if you mean that autocomplete is not working in Pycharm, or you're not even able to load the modules/functions in tf.keras?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Sorry I thought I replied to this.\r\n\r\nYou can write code and run it and it works, but you don't have autocomplete nor you can go to declaration of any keras function. It's like writing code in notepad :)", "hi, @markorakita\r\nIt's a naming conflict issue.\r\ntry modify the file ```site-packages/tensorflow/__init__.py``` near line 387.\r\n\r\nbefore:\r\n\r\n```\r\n_keras_module = \"keras.api._v2.keras\"\r\nkeras = _LazyLoader(\"keras\", globals(), _keras_module)\r\n_module_dir = _module_util.get_parent_dir_for_name(_keras_module)\r\nif _module_dir:\r\n  _current_module.__path__ = [_module_dir] + _current_module.__path__\r\nsetattr(_current_module, \"keras\", keras)\r\n```\r\n\r\n after:\r\n\r\n```\r\nimport typing as _typing\r\nif _typing.TYPE_CHECKING:\r\n  from keras.api._v2 import keras\r\nelse:\r\n  _keras_module = \"keras.api._v2.keras\"\r\n  keras = _LazyLoader(\"keras\", globals(), _keras_module)\r\n  _module_dir = _module_util.get_parent_dir_for_name(_keras_module)\r\n  if _module_dir:\r\n    _current_module.__path__ = [_module_dir] + _current_module.__path__\r\n  setattr(_current_module, \"keras\", keras)\r\n```\r\nit work for me.\r\n", "@cpuimage,\r\n\r\nThanks for sharing your workaround.\r\n\r\n@markorakita,\r\n\r\nCan you try as per the suggestion by @cpuimage and let us know if it helps in resolving the auto completion problem? ", "@cpuimage Thank you for finding the cause of the problem in the source code!\r\n\r\n@sanatmpa1 I've tried the workaround and it works, but I don't think it's the complete fix since some of the keras classes are still not visible. For example _tensorflow.keras.layers.CenterCrop_ and _tensorflow.keras.layers.Rescaling_ layers cannot be resolved, this gets highlighted in red with the message \"Cannot find reference 'CenterCrop' in \\_\\_init\\_\\_.py\":\r\n`from tensorflow.keras.layers import CenterCrop`\r\n\r\nI hope someone in your team could take a deeper look and apply the proper fix for the next tensorflow update?", "@cpuimage Thanks, they can be found in that path but that is the old path, in TF 2.7 they were moved from experimental to tensorflow.keras.layers (see [API docs](https://www.tensorflow.org/api_docs/python/tf/keras/layers/CenterCrop)) and those new paths should work in IDE. Maybe the same bug that is in `site-packages/tensorflow/__init__.py` is also in `site-packages/tensorflow/keras/__init__.py`\r\n\r\nThese are just the classes that I noticed at a first glance that cannot be resolved, but there might be more, somebody with domain knowledge should really look into this and devise full and proper fix.", "@markorakita    Everything is stirred up, for the time being, only part of it can be solved first, and I hope that someone will solve the problem as soon as possible.", "any updates on the issue? as mentioned now it is like coding in notepad", "I'm using Emacs and I have same issue in TF2.8.\r\nFor now, my solution is: \r\n```\r\nimport typing\r\n\r\nfrom tensorflow import keras\r\n\r\nif typing.TYPE_CHECKING:\r\n    from keras.api._v2 import keras\r\n```\r\nIt works good for my environment.", "Any update on this? It's been a minute and neither this issue nor #53271 have received any updates since Dec", "Maybe @qlzh727 can give us an update? Would really appreciate it!", "same issue here. cpuimage's method worked for me (M1 mac, PyCharm), if I use `import tensorflow as tf` and in the rest of lines such as `tf.keras.layers.Dense(...)`", "le sigh. Ok well if anyone else is using PyCharm/IntelliJ with their own self hosted jupyter server, and need their interpreter to index keras correctly, the file will be under the following path in the file explorer\r\n\r\n`External Libraries` -> `Remote Python X.X.X Docker` (or whatever type of interpreter you have) -> `/usr/local/lib/pythonX.X/dist-packages/tensorflow/__init__.py`\r\n\r\n> If it's not in `/usr/local/lib...` it might be in one of the other folders available from the `Remote Libraries` collection. \r\n\r\nAfter you do this, PyCharm should immediately pick up the change, and start giving Keras autocompletes. \r\n\r\nI have not tried wiping my indexes to see if it persists past an `Invalidate Caches and restart`, but based on my past experiences working with PyCharm docker interpreters at work, PyCharm seems to (annoyingly) not want to clear these cached versions of the files from docker interpreters when you do an `invalidate caches and restart` (it's pretty irritating that this happens), so I'm guessing it will persist.", "Same issue here. ", "I am also having the same issue. After updating the `__init__.py` however, Pycharm does not pick up the changes... I am using `tensorflow-macos==2.8.0` in M1. ", "> I am also having the same issue. After updating the `__init__.py` however, Pycharm does not pick up the changes... I am using `tensorflow-macos==2.8.0` in M1. \n\nYou'll probably need to invalidate caches and restart. Your indexes are likely still from before the change. PyCharm index cache invalidations works differently with native interpreters it seems ", "@vanakema I already did that but no luck... Any other idea? \r\n\r\n<img width=\"567\" alt=\"image\" src=\"https://user-images.githubusercontent.com/32769401/160150987-a6edd25e-7450-4927-89cc-d8b2e3941be8.png\">\r\n", "> @vanakema I already did that but no luck... Any other idea? \n> \n> \n> \n> <img width=\"567\" alt=\"image\" src=\"https://user-images.githubusercontent.com/32769401/160150987-a6edd25e-7450-4927-89cc-d8b2e3941be8.png\">\n> \n> \n\nOh I thought I saw someone else say that for whatever reason that style import doesn't work with the fix. Do you get autocompletes when you write something like `tf.keras.layers` in your actual code?", "Yes, the autocompletes work.", "> Yes, the autocompletes work.\n\nOk awesome! I don't remember if python let's you do this, but you might be able to replace \"tensorflow\" with \"tf\" in those lower imports and maybe it'll work?", "Same issue her - I'm using Tensorflow 2.8.  The Workaround described above worked for me but in addition I needed to do the following in PyCharm:  _File->Invalidate Chaches -> Invalidate and Restart_. Takes a little while tough...", "I can confirm that this issue still exits in latest PyCharm (2021.3.3 professional edition), python 3.10, TF 2.8.  The workaround from cpuimage, then invalidating caches and restarting, did not fix the issue.", "I second what winthropharvey said.  The workaround from cpuimage, then invalidating caches and restarting, did not fix the issue., except for me I have PyCharm 2021.2.4 Community Edition. \r\n", "@winthropharvey @Tolure @nicolasperezdeo \r\n ```site-packages\\tensorflow\\keras``` is removed in tf 2.8.\r\n\r\nThe temporary solution can only be:\r\n\r\nbefore:\r\n```from tensorflow.keras.callbacks import EarlyStopping```\r\nafter:\r\n```from tensorflow.python.keras.callbacks import EarlyStopping```\r\n", "This workaround with the `.python.` addition works to have autocompletion and syntax checking (in some instances but not all).\r\n\r\nHowever, it does not help during runtime:\r\n```python\r\nimport tensorflow.python.keras as keras\r\nl = keras.layers.LayerNormalization()\r\n>>> AttributeError: module 'tensorflow.python.keras.layers' has no attribute 'LayerNormalization'\r\n```\r\n\r\nWhen not using `.python.`:\r\n```python\r\nimport tensorflow.keras as keras\r\nl = keras.layers.LayerNormalization()\r\n# works\r\n```\r\nIt's not a good workaround.\r\n\r\n(TensorFlow 2.8.0)", "@bermeitinger-b This works for me, both on Pycharm and VSCode. Thanks!"]}, {"number": 53140, "title": "issue about XLA compile MirroredStrategy", "body": "System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): I have tested on Ubuntu 18.04.\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.7.0\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: 11.0\r\n- GPU model and memory: Tesla P100\r\n\r\n**Describe the current behavior**\r\nWhen I train my model on multi-gpu with XLA compiling below error is occurred.\r\n\r\n```\r\n021-11-20 12:57:08.333476: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-11-20 12:57:09.302772: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15397 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:3b:00.0, compute capability: 6.0\r\n2021-11-20 12:57:09.303502: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 15397 MB memory:  -> device: 1, name: Tesla P100-PCIE-16GB, pci bus id: 0000:b1:00.0, compute capability: 6.0\r\n2021-11-20 12:57:10.556310: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at xla_ops.cc:241 : INVALID_ARGUMENT: Trying to access resource _AnonymousVar3 (defined @ /home/sdb/wda/tf_xla/lib/python3.7/site-packages/keras/engine/base_layer_utils.py:129) located in device /job:localhost/replica:0/task:0/device:GPU:1 from device /job:localhost/replica:0/task:0/device:GPU:0\r\nTraceback (most recent call last):\r\n  File \"/home/sdb/wda/TF2-jit-compile-on-multi-gpu/xla_tf_function_distributed.py\", line 59, in <module>\r\n    train_step_dist(images, labels)\r\n  File \"/home/sdb/wda/tf_xla/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n  File \"/home/sdb/wda/tf_xla/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Trying to access resource _AnonymousVar3 (defined @ /home/sdb/wda/tf_xla/lib/python3.7/site-packages/keras/engine/base_layer_utils.py:129) located in device /job:localhost/replica:0/task:0/device:GPU:1 from device /job:localhost/replica:0/task:0/device:GPU:0 [Op:__inference_train_step_dist_650]\r\n```\r\n\r\nDescribe the expected behavior\r\nI want to use XLA  compile MirroredStrategy\uff0c because I found that _XLA can now compile MirroredStrategy: the step function passed to`strategy.run` can now be annoted with `jit_compile=True`._ from RELEASE.md from  2.5.0\r\n\r\nStandalone code to reproduce the issue:\r\n\r\n```\r\nimport tensorflow as tf\r\ntf.compat.v1.enable_eager_execution()\r\n\r\n# Size of each input image, 28 x 28 pixels\r\nIMAGE_SIZE = 28 * 28\r\n# Number of distinct number labels, [0..9]\r\nNUM_CLASSES = 10\r\n# Number of examples in each training batch (step)\r\nTRAIN_BATCH_SIZE = 100\r\n# Number of training steps to run\r\nTRAIN_STEPS = 1000\r\n\r\n# Loads MNIST dataset.\r\ntrain, test = tf.keras.datasets.mnist.load_data()\r\ntrain_ds = tf.data.Dataset.from_tensor_slices(train).batch(TRAIN_BATCH_SIZE).repeat()\r\n\r\n\r\n# Casting from raw data to the required datatypes.\r\ndef cast(images, labels):\r\n    images = tf.cast(\r\n        tf.reshape(images, [-1, IMAGE_SIZE]), tf.float32)\r\n    labels = tf.cast(labels, tf.int64)\r\n    return (images, labels)\r\n\r\n\r\nlayer = tf.keras.layers.Dense(NUM_CLASSES)\r\noptimizer = tf.keras.optimizers.Adam()\r\n\r\n\r\n@tf.function(jit_compile=True)\r\ndef compiled_step(images, labels):\r\n    images, labels = cast(images, labels)\r\n\r\n    with tf.GradientTape() as tape:\r\n        predicted_labels = layer(images)\r\n        loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\r\n            logits=predicted_labels, labels=labels\r\n        ))\r\n    gradients = tape.gradient(loss, layer.trainable_variables)\r\n    return loss, predicted_labels, gradients\r\n\r\n@tf.function()\r\ndef train_step(images, labels):\r\n    loss, pred, gradients = compiled_step(images, labels)\r\n    optimizer.apply_gradients(zip(gradients, layer.trainable_variables))\r\n\r\n\r\nstrategy = tf.distribute.MirroredStrategy()\r\n\r\n@tf.function(jit_compile=True)\r\ndef train_step_dist(image, labels):\r\n    strategy.run(train_step, args=(image, labels))\r\n\r\n\r\nfor images, labels in train_ds:\r\n    if optimizer.iterations > TRAIN_STEPS:\r\n        break\r\n    train_step_dist(images, labels)\r\n\r\n```", "comments": ["Hi @xuanhuo! I could not find any problem in [Colab](https://colab.research.google.com/gist/mohantym/1ecfc0a84460bba91e86c300180300e6/github_53140.ipynb#scrollTo=YQLZv6BlEu_r) though . Could you try again with CUDA 11.2 and  CuDNN 8.1  as this [thread ](https://www.tensorflow.org/install/source#gpu)suggests? Thank you!", "@mohantym Thank you very much\uff0cNow\uff0cIf I use one GPU\uff0c It is correct\uff0c but If I use two GPU, It's wrong. so how many gpus did you use?\r\n", "@xuanhuo  ! I used Single GPU in Colab.  @sanatmpa1! Could you please look at this issue ? The issue is not replicating in Colab though .Attaching [Gist ](https://colab.sandbox.google.com/gist/mohantym/1ecfc0a84460bba91e86c300180300e6/github_53140.ipynb#scrollTo=t4WC27ciapV3)for reference.", "Hi, does the issue appear with `jit_compile=True` only or without as well?"]}, {"number": 53136, "title": "[TF-TRT] Migrate saved calibration tables from attribute to asset", "body": "This PR does the following:\r\n\r\n@bixia1 for review\r\n\r\nCC: @tfeher @DEKHTIARJonathan\r\n\r\n- Adds functionality to allow saving and loading of calibration tables via asset files\r\n  - It is implemented by attaching the calibrator to the engine cache, so that the calibrator is loaded in during inference time inside the `TRTEngineCacheResource` object.\r\n  - After calibration, each `TRTEngineOp` has its calibration table saved to its own asset file\r\n- Removes the previous code for saving and loading of calibration tables via attributes\r\n- Currently, calibration tables are saved using attributes which means that the calibration tables count towards the 2GB ProtoBuf size limit. By migrating to asset files, the tables no longer count towards the 2GB limit and can be as large as necessary.\r\n", "comments": ["I updated the PR with the changes. Seems to pass the V1 converter python unit tests now. I also added a new attribute \"is_v2\" since I couldn't find another way to determine if TRTEngineOp was created from V1 converter or from V2 converter.", "Can you use [this](https://github.com/tensorflow/tensorflow/blob/e6c75b6dbf046c280392bf67a5c54223d82ac1ba/tensorflow/core/platform/enable_tf2_utils.h#L25-L27) method to detect whether it is running with TF1 vs TF2?", "@bixia1 Sorry, naive question here: I imported `#include \"tensorflow/core/platform/enable_tf2_utils.h\"` inside `trt_engine_op.cc`, but I'm still unable to use the method `tf2_execution_enabled()`. I'm getting an error like this: \r\n```\r\nin function `tensorflow::tensorrt::TRTEngineOp::TRTEngineOp(tensorflow::OpKernelConstruction*)':\r\ntrt_engine_op.cc:(.text._ZN10tensorflow8tensorrt11TRTEngineOpC2EPNS_20OpKernelConstructionE+0x11e4): undefined reference to `tensorflow::tf2_execution_enabled()'.\r\n```\r\n\r\nAlso, did you want me to use `tf2_execution_enabled()` as a replacement for my `is_v2` attribute?", "@bixia1 Can you please assist on above comments from @vict-guo. Thanks!", "@vict-guo Can you please resolve conflicts? Thanks!", "> tf2_execution_enabled\r\n\r\nHave you added the module dependence? Can you take a look at how this test tensorflow/core/platform/enable_tf2_utils_test.cc is built to see if that helps?", "@gbaned I resolved the conflicts. @bixia1 I'm not sure how much more I'll be able to work on this since I've left Nvidia and don't have access the containers I used to use to build tensorflow with TF-TRT.", "@vict-guo Can you please sign CLA. Thank you!", "Signed the CLA"]}, {"number": 53127, "title": "TFLite Movenet multipose/lightning input error", "body": "Hi, \r\nI am trying to load and run the Movenet multipose model, the loading of model works fine but when I try to run it gives this error:\r\n`java.lang.IllegalArgumentException: Cannot copy to a TensorFlowLite tensor (serving_default_input:0) with 3 bytes from a Java Buffer with 491520 bytes.`\r\n\r\nAs I can understand (and also I looked the model graph in Netron app) the input of the model is `type: uint8[1,1,1,3]` , and this is what the error is also telling, that my converted image doesn't fit the input of the model. So my question is: is this intentionally or the model is not correct ? \r\n\r\nI downloaded model from Tensorflow Hub -> https://tfhub.dev/google/lite-model/movenet/multipose/lightning/tflite/float16/1\r\nAnd here (in the link above) also is specified that the model accepts 'A frame of video or an image, represented as an `int32 tensor of dynamic shape: 1xHxWx3`'. I handled the dynamic shape tensor as suggested in the link.\r\n\r\nThank you!\r\n", "comments": ["@ArtanBerisha1 \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here & let us know which TF version you're using ?Thanks!", "Hi, @sushreebarsa \r\nHere are TF versions: \r\n```\r\n    implementation 'org.tensorflow:tensorflow-lite:2.6.0'\r\n    implementation 'org.tensorflow:tensorflow-lite-gpu:2.6.0'\r\n```\r\n\r\nI am replacing the model in Object Detection Example app(Android App) provided by tensorflow, here is the code snippet that I use to run the inference on Bitmap:\r\n```\r\npublic List<Classifier.Recognition> recognizeImage(final Bitmap bitmap) {\r\n    bitmap.getPixels(intValues, 0, bitmap.getWidth(), 0, 0, bitmap.getWidth(), bitmap.getHeight());\r\n\r\n    imgData.rewind();\r\n    for (int i = 0; i < inputSizeWidth; ++i) {\r\n      for (int j = 0; j < inputSizeHeight; ++j) {\r\n        int pixelValue = intValues[i * inputSizeHeight + j];\r\n        if (isModelQuantized) {\r\n          // Quantized model\r\n          imgData.put((byte) ((pixelValue >> 16) & 0xFF));\r\n          imgData.put((byte) ((pixelValue >> 8) & 0xFF));\r\n          imgData.put((byte) (pixelValue & 0xFF));\r\n        } else { // Float model\r\n          imgData.putFloat(((pixelValue >> 16) & 0xFF));\r\n          imgData.putFloat(((pixelValue >> 8) & 0xFF));\r\n          imgData.putFloat((pixelValue & 0xFF));\r\n        }\r\n      }\r\n    }\r\n    modelOutput = new float[1][6][56];\r\n    xCoordinates = new float[17];\r\n    yCoordinates = new float[17];\r\n    confidences = new float[17];\r\n\r\n    Object[] inputArray = {imgData};\r\n    Map<Integer, Object> outputMap = new HashMap<>();\r\n    outputMap.put(0, modelOutput);\r\n\r\n    tfLite.runForMultipleInputsOutputs(inputArray, outputMap);  // <- this is where I get the error\r\n    \r\n    final ArrayList<Recognition> recognitions = new ArrayList<>(NUM_DETECTIONS);\r\n    for (int i = 0; i < 6; ++i) {\r\n      for(int x = 0; x < 17; x++) {\r\n        yCoordinates[x] = modelOutput[0][i][0 + x];\r\n        xCoordinates[x] = modelOutput[0][i][1 + x];\r\n        confidences[x] = modelOutput[0][i][2 + x];\r\n      }\r\n        recognitions.add(new Recognition(xCoordinates, yCoordinates, confidences));\r\n    }\r\n    return recognitions;\r\n  }\r\n```\r\n\r\nEDIT:\r\nThe model loads successfully, but the error is when I try run the model on the frame. "]}, {"number": 53115, "title": "SSIM index maps", "body": "**System information**\r\n- TensorFlow version (you are using): 2.7\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently tf.image.ssim() returns a global ssim value but it would be useful to have it return a non globally averaged value as per \r\ne.g. https://uk.mathworks.com/help/images/ref/ssim.html that would allow the making of ssim index maps\r\n\r\n**Will this change the current api? How?**\r\n-\r\n\r\n**Who will benefit with this feature?**\r\nanyone doing computer vision\r\n\r\n**Any Other info.**\r\n", "comments": ["Hi @sanatmpa1! Could you look into this feature request?", "Could you please elaborate your feature request and provide some code to show what is implemented and what changes in result you are expecting. Thanks!", "Hi, \r\n\r\nYes so the results I am expecting are like that shown here: https://uk.mathworks.com/help/images/ref/ssim.html for the local ssim map. So it's a SSIM value for each pixel rather than the mean. \r\n\r\nI believe that it comes in the function _ssim_per_channel() where the SSIM values are computed but then it returns the global mean rather than the pixel values. \r\n>  ssim_val = math_ops.reduce_mean(luminance * cs, axes)\r\n>  cs = math_ops.reduce_mean(cs, axes)\r\n>  return ssim_val, cs\r\n\r\nAnd the ssim() function \r\n> ssim_per_channel, _ = _ssim_per_channel(img1, img2, max_val, filter_size,\r\n                                            filter_sigma, k1, k2)\r\n> return math_ops.reduce_mean(ssim_per_channel, [-1])\r\n\r\n\r\n\r\n\r\n\r\n", "Thanks MaggieLieu for tracking this down!\r\n\r\nWe can add a 'return_index_map=False/True' argument to ssim to maintain backward compatibility: when return_index_map is True, returns the index map; when return_idnex_map is False, return the reduced global value.\r\n\r\nSince you have already identified where to change, would you mind contributing a PR?\r\n\r\nPS: Here is a reference where it is explicitly described that the global ssim value is the mean of the local ssim value: https://medium.com/srm-mic/all-about-structural-similarity-index-ssim-theory-code-in-pytorch-6551b455541e . "]}, {"number": 53111, "title": "Segmentation fault when passing tf.string tensor arguments in TFLite", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- TensorFlow installation (pip package or built from source): pip\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): at least TF 2.7 and tf-nightly are affected\r\n\r\n### 2. Code\r\n\r\n```python\r\nimport tensorflow as tf                                                        \r\n                                                                               \r\nclass TestModel(tf.keras.models.Model):                                        \r\n  \r\n  def __init__(self, **kwargs):\r\n    super().__init__(**kwargs)                                                 \r\n    self._hash = tf.lookup.StaticHashTable(\r\n        tf.lookup.KeyValueTensorInitializer(\r\n            tf.constant(['testing', 'this', 'thing']),                         \r\n            tf.constant([1, 2, 3])),\r\n        default_value=-1)                                                      \r\n\r\n  @tf.function\r\n  def test(self, word):\r\n    return self._hash.lookup(word)                                             \r\n\r\n\r\ntest_model = TestModel()                                                       \r\nsignatures = [test_model.test.get_concrete_function(tf.TensorSpec([None], tf.string))] \r\n    \r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions(signatures, test_model) \r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]                           \r\nconverter.target_spec.supported_ops = [                                        \r\n  tf.lite.OpsSet.TFLITE_BUILTINS,\r\n  tf.lite.OpsSet.SELECT_TF_OPS                                                 \r\n]\r\n\r\ntflite_model = converter.convert()                                             \r\ninterpreter = tf.lite.Interpreter(model_content=tflite_model)\r\n\r\n# Causes segmentation fault. Running test_model.test directly works fine.\r\nresult = interpreter.get_signature_runner()(word=tf.constant(['testing', 'that', 'thing']))\r\n```\r\n\r\n### 3. Failure after conversion\r\nConversion raises the following warning but completes.\r\n\r\n```\r\n2021-11-18 22:35:48.110926: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1880] Graph contains the following resource op(s), that use(s) resource type. Currently, the resource type is not natively supported in TFLite. Please consider not using the resource type if there are issues with either TFLite converter or TFLite runtime:\r\nResource ops: HashTableV2, LookupTableFindV2, LookupTableImportV2\r\nDetails:\r\n\ttf.HashTableV2() -> (tensor<!tf_type.resource>) : {container = \"\", device = \"\", key_dtype = !tf_type.string, shared_name = \"13\", use_node_name_sharing = false, value_dtype = i32}\r\n\ttf.LookupTableFindV2(tensor<!tf_type.resource>, tensor<?x!tf_type.string>, tensor<i32>) -> (tensor<*xi32>) : {device = \"\"}\r\n\ttf.LookupTableImportV2(tensor<!tf_type.resource>, tensor<3x!tf_type.string>, tensor<3xi32>) -> () : {device = \"\"}\r\n```\r\n\r\nHowever, trying to run the signature fails with a segmentation fault.\r\n\r\nI thought that static hash table lookups were supported in TFLite since many versions ago. Any possible temporary workarounds until this is fixed would be very much appreciated.", "comments": ["Hi @sanatmpa1! Could you look into this issue. It is replicating in TF 2.5,2.6 and nightly . Attaching [Gist](https://colab.research.google.com/gist/mohantym/251be9d11a652fa6bfafe773e01e24bf/github_53111.ipynb) for Reference.", "@leandro-gracia-gil,\r\n\r\nI tried reproducing the issue, where I can see that session getting crashed after running the code. But I don't see the warning you mentioned above. Here's the [gist](https://colab.sandbox.google.com/gist/sanatmpa1/b2f148e8912019c70e6122de410c630b/github_53111.ipynb) and can you clarify on the same? Thanks!", "@sanatmpa1 \r\n\r\nI'm happy to try providing more info, but I'm not sure what you need. I can confirm you that for me running the exact code I pasted above in TF 2.7 does show that warning among other outputs, and also the keras warning present in the gist. Could it be that the gist is simply less verbose?\r\n\r\nHere's a more detailed output:  (I've omitted the initial lines including hostnames and available CUDA devices and versions)\r\n```\r\nWARNING:tensorflow:Skipping full serialization of Keras layer <__main__.TestModel object at 0x7f4fa1c49790>, because it is not built.\r\n2021-11-25 21:17:45.790533: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\n2021-11-25 21:17:46.010478: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:363] Ignored output_format.\r\n2021-11-25 21:17:46.010513: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:366] Ignored drop_control_dependency.\r\n2021-11-25 21:17:46.011638: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmp4h39x40o\r\n2021-11-25 21:17:46.012182: I tensorflow/cc/saved_model/reader.cc:107] Reading meta graph with tags { serve }\r\n2021-11-25 21:17:46.012213: I tensorflow/cc/saved_model/reader.cc:148] Reading SavedModel debug info (if present) from: /tmp/tmp4h39x40o\r\n2021-11-25 21:17:46.015451: I tensorflow/cc/saved_model/loader.cc:210] Restoring SavedModel bundle.\r\n2021-11-25 21:17:46.026026: I tensorflow/cc/saved_model/loader.cc:194] Running initialization op on SavedModel bundle at path: /tmp/tmp4h39x40o\r\n2021-11-25 21:17:46.042800: I tensorflow/cc/saved_model/loader.cc:283] SavedModel load for tags { serve }; Status: success: OK. Took 31164 microseconds.\r\n2021-11-25 21:17:46.053337: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\n2021-11-25 21:17:46.089494: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1880] Graph contains the following resource op(s), that use(s) resource type. Currently, the resource type is not natively supported in TFLite. Please consider not using the resource type if there are issues with either TFLite converter or TFLite runtime:\r\nResource ops: HashTableV2, LookupTableFindV2, LookupTableImportV2\r\nDetails:\r\n        tf.HashTableV2() -> (tensor<!tf_type.resource>) : {container = \"\", device = \"\", key_dtype = !tf_type.string, shared_name = \"13\", use_node_name_sharing = false, value_dtype = i32}\r\n        tf.LookupTableFindV2(tensor<!tf_type.resource>, tensor<?x!tf_type.string>, tensor<i32>) -> (tensor<*xi32>) : {device = \"\"}\r\n        tf.LookupTableImportV2(tensor<!tf_type.resource>, tensor<3x!tf_type.string>, tensor<3xi32>) -> () : {device = \"\"}\r\n2021-11-25 21:17:46.089591: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1891] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):\r\nFlex ops: FlexHashTableV2, FlexLookupTableFindV2, FlexLookupTableImportV2\r\nDetails:\r\n        tf.HashTableV2() -> (tensor<!tf_type.resource>) : {container = \"\", device = \"\", key_dtype = !tf_type.string, shared_name = \"13\", use_node_name_sharing = false, value_dtype = i32}\r\n        tf.LookupTableFindV2(tensor<!tf_type.resource>, tensor<?x!tf_type.string>, tensor<i32>) -> (tensor<*xi32>) : {device = \"\"}\r\n        tf.LookupTableImportV2(tensor<!tf_type.resource>, tensor<3x!tf_type.string>, tensor<3xi32>) -> () : {device = \"\"}\r\nSee instructions: https://www.tensorflow.org/lite/guide/ops_select\r\n2021-11-25 21:17:46.089635: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1962] Estimated count of arithmetic ops: 0  ops, equivalently 0  MACs\r\n\r\nEstimated count of arithmetic ops: 0  ops, equivalently 0  MACs\r\nINFO: Created TensorFlow Lite delegate for select TF ops.\r\nINFO: TfLiteFlexDelegate delegate: 2 nodes delegated out of 3 nodes with 1 partitions.\r\n\r\nSegmentation fault (core dumped)\r\n```", "Important update: this does not seem to be specifically a StaticHashTable problem, but rather a problem of passing string tensor arguments to exported functions in TFLite!\r\n\r\nThis version of the function causes a segmentation fault just the same:\r\n```python\r\n  @tf.function\r\n  def test(self, word):\r\n    return word\r\n```\r\n\r\nThe segmentation fault happens even when passing empty string tensors. However, it does not happen if changing the argument type in the signature from tf.string to something else like tf.int32."]}, {"number": 53109, "title": " Tensoflow C++ naive build - ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.0  (also tried versions up to to 2.7) \r\n- Python version: 3.9.8\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): gcc 10.2\r\n- CUDA/cuDNN version: 11.5/8.0\r\n- GPU model and memory: NVidia Geforce 940MX\r\n\r\n\r\n\r\n**Describe the problem**\r\nAn error occurred during the fetch of repository 'com_google_protobuf':\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\ngit clone https://github.com/tensorflow/tensorflow.git .\r\ngit checkout r2.0\r\nbazel clean\r\nconfigure\r\nbazel build tensorflow:tensorflow_cc\r\n\r\n\r\n**Any other info / logs**\r\nF:\\workdir\\units\\unit_1_asa\\ai\\project\\c++\\ml\\dl\\software\\tensorflow\\tf>bazel build tensorflow:tensorflow_cc\r\nINFO: Reading 'startup' options from c:\\users\\seth a quarshie\\.bazelrc: --output_user_root=F:/workdir/units/unit_1_asa/ai/project/c++/ml/dl/software/dl-tool/bazel/.bazel\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=126\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=F:/workdir/units/unit_1_asa/ai/project/c++/ml/dl/software/dl-tool/python/python.exe\r\nINFO: Reading rc options for 'build' from f:\\workdir\\units\\unit_1_asa\\ai\\project\\c++\\ml\\dl\\software\\tensorflow\\tf\\.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --copt=-w --config=v2\r\nINFO: Found applicable config definition build:v2 in file f:\\workdir\\units\\unit_1_asa\\ai\\project\\c++\\ml\\dl\\software\\tensorflow\\tf\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Repository com_google_protobuf instantiated at:\r\n  F:/workdir/units/unit_1_asa/ai/project/c++/ml/dl/software/tensorflow/tf/WORKSPACE:19:16: in <toplevel>\r\n  F:/workdir/units/unit_1_asa/ai/project/c++/ml/dl/software/tensorflow/tf/tensorflow/workspace.bzl:432:20: in tf_repositories\r\nRepository rule tf_http_archive defined at:\r\n  F:/workdir/units/unit_1_asa/ai/project/c++/ml/dl/software/tensorflow/tf/third_party/repo.bzl:124:34: in <toplevel>\r\nINFO: Repository 'com_google_protobuf' used the following cache hits instead of downloading the corresponding file.\r\n * Hash 'b9e92f9af8819bbbc514e2902aec860415b70209f31dfc8c4fa72515a5df9d59' for https://storage.googleapis.com/mirror.tensorflow.org/github.com/protocolbuffers/protobuf/archive/310ba5ee72661c081129eb878c1bbcec936b20f0.tar.gz\r\nIf the definition of 'com_google_protobuf' was updated, verify that the hashes were also updated.\r\nERROR: An error occurred during the fetch of repository 'com_google_protobuf':\r\n   Traceback (most recent call last):\r\n        File \"F:/workdir/units/unit_1_asa/ai/project/c++/ml/dl/software/tensorflow/tf/third_party/repo.bzl\", line 104, column 25, in _tf_http_archive\r\n                _apply_patch(ctx, ctx.attr.patch_file)\r\n        File \"F:/workdir/units/unit_1_asa/ai/project/c++/ml/dl/software/tensorflow/tf/third_party/repo.bzl\", line 71, column 32, in _apply_patch\r\n                _execute_and_check_ret_code(ctx, cmd)\r\n        File \"F:/workdir/units/unit_1_asa/ai/project/c++/ml/dl/software/tensorflow/tf/third_party/repo.bzl\", line 52, column 13, in _execute_and_check_ret_code\r\n                fail((\"Non-zero return code({1}) when executing '{0}':\\n\" + \"Stdout: {2}\\n\" +\r\nError in fail: Non-zero return code(256) when executing 'F:\\workdir\\units\\unit_1_asa\\ai\\project\\c++\\ml\\dl\\software\\dl-tool\\msys2\\msys64\\usr\\bin -l -c \"patch\" \"-p1\" \"-d\" \"F:/workdir/units/unit_1_asa/ai/project/c++/ml/dl/software/dl-tool/bazel/.bazel/6iwvpnw5/external/com_google_protobuf\" \"-i\" \"F:/workdir/units/unit_1_asa/ai/project/c++/ml/dl/software/tensorflow/tf/third_party/protobuf/protobuf.patch\"':\r\nStdout:\r\nStderr: java.io.IOException: ERROR: src/main/native/windows/process.cc(202): CreateProcessW(\"F:\\workdir\\units\\unit_1_asa\\ai\\project\\c++\\ml\\dl\\software\\dl-tool\\msys2\\msys64\\usr\\bin\" -l -c \"\\\"patch\\\" \\\"-p1\\\" \\\"-d\\\" \\\"F:/workdir/units/unit_1_asa/ai/project/c++/ml/dl/software/dl-tool/bazel/.bazel/6iwvpnw5/external/com_google_protobuf\\\" \\\"-i\\\" \\\"F:/workdir/units/unit_1_asa/ai/project/c++/ml/dl/software/tensorflow/tf/third_party/protobuf/protobuf.patch\\\"\"): Access is denied.\r\n (error: 5)\r\nERROR: Skipping 'tensorflow:tensorflow_cc': no such package '@com_google_protobuf//': Non-zero return code(256) when executing 'F:\\workdir\\units\\unit_1_asa\\ai\\project\\c++\\ml\\dl\\software\\dl-tool\\msys2\\msys64\\usr\\bin -l -c \"patch\" \"-p1\" \"-d\" \"F:/workdir/units/unit_1_asa/ai/project/c++/ml/dl/software/dl-tool/bazel/.bazel/6iwvpnw5/external/com_google_protobuf\" \"-i\" \"F:/workdir/units/unit_1_asa/ai/project/c++/ml/dl/software/tensorflow/tf/third_party/protobuf/protobuf.patch\"':\r\nStdout:\r\nStderr: java.io.IOException: ERROR: src/main/native/windows/process.cc(202): CreateProcessW(\"F:\\workdir\\units\\unit_1_asa\\ai\\project\\c++\\ml\\dl\\software\\dl-tool\\msys2\\msys64\\usr\\bin\" -l -c \"\\\"patch\\\" \\\"-p1\\\" \\\"-d\\\" \\\"F:/workdir/units/unit_1_asa/ai/project/c++/ml/dl/software/dl-tool/bazel/.bazel/6iwvpnw5/external/com_google_protobuf\\\" \\\"-i\\\" \\\"F:/workdir/units/unit_1_asa/ai/project/c++/ml/dl/software/tensorflow/tf/third_party/protobuf/protobuf.patch\\\"\"): Access is denied.\r\n (error: 5)\r\nWARNING: Target pattern parsing failed.\r\nERROR: no such package '@com_google_protobuf//': Non-zero return code(256) when executing 'F:\\workdir\\units\\unit_1_asa\\ai\\project\\c++\\ml\\dl\\software\\dl-tool\\msys2\\msys64\\usr\\bin -l -c \"patch\" \"-p1\" \"-d\" \"F:/workdir/units/unit_1_asa/ai/project/c++/ml/dl/software/dl-tool/bazel/.bazel/6iwvpnw5/external/com_google_protobuf\" \"-i\" \"F:/workdir/units/unit_1_asa/ai/project/c++/ml/dl/software/tensorflow/tf/third_party/protobuf/protobuf.patch\"':\r\nStdout:\r\nStderr: java.io.IOException: ERROR: src/main/native/windows/process.cc(202): CreateProcessW(\"F:\\workdir\\units\\unit_1_asa\\ai\\project\\c++\\ml\\dl\\software\\dl-tool\\msys2\\msys64\\usr\\bin\" -l -c \"\\\"patch\\\" \\\"-p1\\\" \\\"-d\\\" \\\"F:/workdir/units/unit_1_asa/ai/project/c++/ml/dl/software/dl-tool/bazel/.bazel/6iwvpnw5/external/com_google_protobuf\\\" \\\"-i\\\" \\\"F:/workdir/units/unit_1_asa/ai/project/c++/ml/dl/software/tensorflow/tf/third_party/protobuf/protobuf.patch\\\"\"): Access is denied.\r\n (error: 5)\r\nINFO: Elapsed time: 3.311s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n    currently loading: tensorflow\r\n\r\n\r\n\r\n\r\n", "comments": ["@dzorwulu ,\r\nCan you please take a look at this link [1](https://github.com/tensorflow/tensorflow/issues/45633) , [2](https://github.com/tensorflow/tensorflow/issues/37897) and [3](https://github.com/tensorflow/tensorflow/issues/52153) with similar error.It helps.Thanks!", "Thanks, but none of the solutions of the suggested issues worked.\r\nPlease suggest other  cause of action. ", "***How do I resolve the following download 404 message related to this issue:\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/43d6991c2a4cc2ac374e68c029634f2b59ffdfdf.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nWARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found \r\n\r\n***curl produces the same error:\r\ncurl https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/43d6991c2a4cc2ac374e68c029634f2b59ffdfdf.tar.gz\r\n<?xml version='1.0' encoding='UTF-8'?><Error><Code>NoSuchKey</Code><Message>The specified key does not exist.</Message><Details>No such object: mirror.tensorflow.org/github.com/llvm/llvm-project/archive/43d6991c2a4cc2ac374e68c029634f2b59ffdfdf.tar.gz</Details></Error>\r\n"]}, {"number": 53101, "title": "TensorFlow Lite fails to convert LSTM after upgrading from 2.6.2 to 2.7.0.", "body": "### System information\r\n\r\n-   Have I written custom code: yes\r\n-   OS Platform and Distribution: Linux Ubuntu 16.04 (TensorFlow official docker images)\r\n-   TensorFlow installed from binary\r\n-   TensorFlow version : 2.7.0\r\n-   Python version: 3.8.10\r\n-   Exact command to reproduce:\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import Input, Embedding, LSTM\r\ntf.version.VERSION\r\nmodel_in = Input(shape=(800,))\r\nmodel = Model(model_in, LSTM(8)(Embedding(300, 8,)(model_in)))\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model); tflite_model = converter.convert()\r\n\r\n### Describe the problem\r\nTensorflow fails to convert the above model into TensorFlow lite, opposed to how it worked up to this version.\r\n\r\n### Source code / logs\r\nThe example below shows how it used to work on version 2.6.0 and how it works now. (it was also working fine for 2.6.2)\r\n\r\ndocker run -it --rm --name tf36 tensorflow/tensorflow:2.6.0 python\r\nUnable to find image 'tensorflow/tensorflow:2.6.0' locally\r\n2.6.0: Pulling from tensorflow/tensorflow\r\nfeac53061382: Already exists\r\nbeba0652e867: Already exists\r\nc5060c8118ce: Already exists\r\nbfc0178fb9ad: Already exists\r\n18fb3f957dc0: Already exists\r\ncd5d06d0987e: Already exists\r\n7ed4f7cde30b: Already exists\r\n6bda0595411c: Already exists\r\nDigest: sha256:773d5ce09e4ce003db02740c6a372a8a9f43be2bac23544d8f452bfec5347c53\r\nStatus: Downloaded newer image for tensorflow/tensorflow:2.6.0\r\nPython 3.6.9 (default, Jan 26 2021, 15:33:00)\r\n[GCC 8.4.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> from tensorflow.keras.models import Model\r\n>>> from tensorflow.keras.layers import Input, Embedding, LSTM\r\n>>> tf.version.VERSION\r\n'2.6.0'\r\n>>> model_in = Input(shape=(800,))\r\n>>> model = Model(model_in, LSTM(8)(Embedding(300, 8,)(model_in)))\r\n2021-11-17 18:10:51.093603: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n>>> converter = tf.lite.TFLiteConverter.from_keras_model(model); tflite_model = converter.convert()\r\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\r\n2021-11-17 18:10:59.494222: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\nWARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\r\nINFO:tensorflow:Assets written to: /tmp/tmpg9eczqrl/assets\r\nINFO:tensorflow:Assets written to: /tmp/tmpg9eczqrl/assets\r\n2021-11-17 18:11:04.174113: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:351] Ignored output_format.\r\n2021-11-17 18:11:04.174165: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:354] Ignored drop_control_dependency.\r\n2021-11-17 18:11:04.175758: I tensorflow/cc/saved_model/reader.cc:38] Reading SavedModel from: /tmp/tmpg9eczqrl\r\n2021-11-17 18:11:04.203946: I tensorflow/cc/saved_model/reader.cc:90] Reading meta graph with tags { serve }\r\n2021-11-17 18:11:04.203996: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /tmp/tmpg9eczqrl\r\n2021-11-17 18:11:04.287237: I tensorflow/cc/saved_model/loader.cc:211] Restoring SavedModel bundle.\r\n2021-11-17 18:11:04.364667: I tensorflow/cc/saved_model/loader.cc:195] Running initialization op on SavedModel bundle at path: /tmp/tmpg9eczqrl\r\n2021-11-17 18:11:04.412505: I tensorflow/cc/saved_model/loader.cc:283] SavedModel load for tags { serve }; Status: success: OK. Took 236769 microseconds.\r\n2021-11-17 18:11:04.589284: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\n>>>\r\n\r\n\r\n\r\ndocker run -it --rm --name tf37 tensorflow/tensorflow:2.7.0 python\r\nPython 3.8.10 (default, Sep 28 2021, 16:10:42)\r\n[GCC 9.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> from tensorflow.keras.models import Model\r\n>>> from tensorflow.keras.layers import Input, Embedding, LSTM\r\n>>> tf.version.VERSION\r\n'2.7.0'\r\n>>> model_in = Input(shape=(800,))\r\n>>> model = Model(model_in, LSTM(8)(Embedding(300, 8,)(model_in)))\r\n2021-11-17 18:14:09.592797: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n>>> converter = tf.lite.TFLiteConverter.from_keras_model(model); tflite_model = converter.convert()\r\n2021-11-17 18:14:18.728660: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\nWARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\r\nINFO:tensorflow:Assets written to: /tmp/tmpixyhgbqk/assets\r\nINFO:tensorflow:Assets written to: /tmp/tmpixyhgbqk/assets\r\n2021-11-17 18:14:24.806528: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:363] Ignored output_format.\r\n2021-11-17 18:14:24.806615: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:366] Ignored drop_control_dependency.\r\n2021-11-17 18:14:24.808294: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmpixyhgbqk\r\n2021-11-17 18:14:24.823489: I tensorflow/cc/saved_model/reader.cc:107] Reading meta graph with tags { serve }\r\n2021-11-17 18:14:24.823535: I tensorflow/cc/saved_model/reader.cc:148] Reading SavedModel debug info (if present) from: /tmp/tmpixyhgbqk\r\n2021-11-17 18:14:24.898508: I tensorflow/cc/saved_model/loader.cc:210] Restoring SavedModel bundle.\r\n2021-11-17 18:14:25.002491: I tensorflow/cc/saved_model/loader.cc:194] Running initialization op on SavedModel bundle at path: /tmp/tmpixyhgbqk\r\n2021-11-17 18:14:25.091938: I tensorflow/cc/saved_model/loader.cc:283] SavedModel load for tags { serve }; Status: success: OK. Took 283650 microseconds.\r\n2021-11-17 18:14:25.358785: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\nloc(callsite(callsite(callsite(callsite(\"TensorArrayV2_1@__inference_standard_lstm_654\"(\"/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/save.py\":1315:0) at callsite(\"/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/save.py\":1280:0 at callsite(\"/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py\":1080:0 at callsite(\"/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/convert_phase.py\":216:0 at callsite(\"/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py\":1150:0 at callsite(\"/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py\":1170:0 at callsite(\"/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py\":761:0 at callsite(\"/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py\":775:0 at \"<stdin>\":1:0)))))))) at callsite(\"model/lstm/PartitionedCall@__inference__wrapped_model_927\"(\"/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/save.py\":1315:0) at callsite(\"/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/save.py\":1280:0 at callsite(\"/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py\":1080:0 at callsite(\"/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/convert_phase.py\":216:0 at callsite(\"/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py\":1150:0 at callsite(\"/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py\":1170:0 at callsite(\"/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py\":761:0 at callsite(\"/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py\":775:0 at \"<stdin>\":1:0))))))))) at callsite(\"StatefulPartitionedCall@__inference_signature_wrapper_2825\"(\"/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/save.py\":1315:0) at callsite(\"/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/save.py\":1280:0 at callsite(\"/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py\":1080:0 at callsite(\"/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/convert_phase.py\":216:0 at callsite(\"/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py\":1150:0 at callsite(\"/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py\":1170:0 at callsite(\"/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py\":761:0 at callsite(\"/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py\":775:0 at \"<stdin>\":1:0))))))))) at \"StatefulPartitionedCall\")): error: 'tf.TensorListReserve' op requires element_shape to be static during TF Lite transformation pass\r\nloc(callsite(callsite(callsite(callsite(\"TensorArrayV2_1@__inference_standard_lstm_654\"(\"/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/save.py\":1315:0) at callsite(\"/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/save.py\":1280:0 at callsite(\"/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py\":1080:0 at callsite(\"/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/convert_phase.py\":216:0 at callsite(\"/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py\":1150:0 at callsite(\"/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py\":1170:0 at callsite(\"/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py\":761:0 at callsite(\"/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py\":775:0 at \"<stdin>\":1:0)))))))) at callsite(\"model/lstm/PartitionedCall@__inference__wrapped_model_927\"(\"/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/save.py\":1315:0) at callsite(\"/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/save.py\":1280:0 at callsite(\"/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py\":1080:0 at callsite(\"/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/convert_phase.py\":216:0 at callsite(\"/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py\":1150:0 at callsite(\"/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py\":1170:0 at callsite(\"/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py\":761:0 at callsite(\"/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py\":775:0 at \"<stdin>\":1:0))))))))) at callsite(\"StatefulPartitionedCall@__inference_signature_wrapper_2825\"(\"/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/save.py\":1315:0) at callsite(\"/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/save.py\":1280:0 at callsite(\"/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py\":1080:0 at callsite(\"/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/convert_phase.py\":216:0 at callsite(\"/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py\":1150:0 at callsite(\"/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py\":1170:0 at callsite(\"/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py\":761:0 at callsite(\"/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py\":775:0 at \"<stdin>\":1:0))))))))) at \"StatefulPartitionedCall\")): error: failed to legalize operation 'tf.TensorListReserve' that was explicitly marked illegal\r\nerror: Lowering tensor list ops is failed. Please consider using Select TF ops and disabling `_experimental_lower_tensor_list_ops` flag in the TFLite converter object. For example, converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\\n converter._experimental_lower_tensor_list_ops = False\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py\", line 775, in wrapper\r\n    return self._convert_and_export_metrics(convert_func, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py\", line 761, in _convert_and_export_metrics\r\n    result = convert_func(self, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py\", line 1170, in convert\r\n    saved_model_convert_result = self._convert_as_saved_model()\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py\", line 1152, in _convert_as_saved_model\r\n    return super(TFLiteKerasModelConverterV2,\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py\", line 945, in convert\r\n    result = _toco_convert_impl(\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/convert_phase.py\", line 223, in wrapper\r\n    raise converter_error from None  # Re-throws the exception.\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/convert_phase.py\", line 216, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/convert.py\", line 821, in toco_convert_impl\r\n    data = toco_convert_protos(\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/convert.py\", line 315, in toco_convert_protos\r\n    raise converter_error\r\ntensorflow.lite.python.convert_phase.ConverterError: /usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/save.py:1315:0: error: 'tf.TensorListReserve' op requires element_shape to be static during TF Lite transformation pass\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/save.py:1315:0: error: failed to legalize operation 'tf.TensorListReserve' that was explicitly marked illegal\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: Lowering tensor list ops is failed. Please consider using Select TF ops and disabling `_experimental_lower_tensor_list_ops` flag in the TFLite converter object. For example, converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\\n converter._experimental_lower_tensor_list_ops = False\r\n\r\n>>>", "comments": ["@gaonmaor \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "As I wrote above, this only needed code to simulate it is:\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import Input, Embedding, LSTM\r\ntf.version.VERSION\r\nmodel_in = Input(shape=(800,))\r\nmodel = Model(model_in, LSTM(8)(Embedding(300, 8,)(model_in)))\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model); tflite_model = converter.convert()\r\n\r\nI've also shown how to simulate it using docker and the official TensorFlow images...", "@jvishnuvardhan Was able to replicate the issue on colab using TF v2.7.0 ,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/1dc65d220c1888563bcceff8c7b2623c/53101.ipynb#scrollTo=eZlo7NPqc-Pe) for reference. Thanks!", "@gaonmaor Based on the error trace, i have updated two lines and added at the end of your code as shown below. With that modification, model was converted to tflite format.\r\n\r\n```\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS] \r\nconverter._experimental_lower_tensor_list_ops = False\r\n\r\n\r\n# writing tflite model\r\nimport pathlib\r\n\r\ntflite_models_dir = pathlib.Path(\"/content/my_lstm_model/\")\r\ntflite_models_dir.mkdir(exist_ok=True, parents=True)\r\n\r\n# Save the tflite model:\r\ntflite_model_file = tflite_models_dir/\"myModel.tflite\"\r\ntflite_model_file.write_bytes(tflite_model)\r\n```\r\n\r\nPlease check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/fa8604b6aaf2f13a3e61e03be9691edc/53101.ipynb)", "This is not a solution, I'm intentionally avoiding selected ops, since I'm running with libtensorflowlite alone (no libtensorflow) on the inference machine.\r\n\r\nI do it because I need to deploy a very small image l, and if I want to use selected ops I need to add the entire libtensorflow library to the image, which is huge.\r\n\r\nIn addition this simple model worked just fine up till this version using plain tflite operations. Something went wrong and need to be fixed.", "Looks like some ops (TensorListFromTensor, TensorListGetItem, TensorListReserve, TensorListSetItem, TensorListStack) are not supported by tflite runtime. [Here](https://colab.research.google.com/gist/jvishnuvardhan/73a96e2319a68497a3fdb327bb0605a3/53101.ipynb) is a gist for reference. Thanks!\r\n\r\n```\r\nSome ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select \r\nTF Select ops: TensorListFromTensor, TensorListGetItem, TensorListReserve, TensorListSetItem, TensorListStack\r\nDetails:\r\n\ttf.TensorListFromTensor(tensor<800x?x8xf32>, tensor<2xi32>) -> (tensor<!tf_type.variant<tensor<?x8xf32>>>) : {device = \"\"}\r\n\ttf.TensorListGetItem(tensor<!tf_type.variant<tensor<?x8xf32>>>, tensor<i32>, tensor<2xi32>) -> (tensor<?x8xf32>) : {device = \"\"}\r\n\ttf.TensorListReserve(tensor<2xi32>, tensor<i32>) -> (tensor<!tf_type.variant<tensor<?x8xf32>>>) : {device = \"\"}\r\n\ttf.TensorListSetItem(tensor<!tf_type.variant<tensor<?x8xf32>>>, tensor<i32>, tensor<?x8xf32>) -> (tensor<!tf_type.variant<tensor<?x8xf32>>>) : {device = \"\"}\r\n\ttf.TensorListStack(tensor<!tf_type.variant<tensor<?x8xf32>>>, tensor<2xi32>) -> (tensor<?x?x8xf32>) : {device = \"\", num_elements = -1 : i64}\r\n```", "We can see that it uses unsupported operations based on the original post. As I've said, it used to work fine up until the previous version (2.6.2). You can try running the above code and see it yourself. It is a degradation  which was introduced on version 2.7.0 and that's why we need to follow it to the relevant TensorFlow development group.", "I am seeing the same problem in my own code with 2.7.0 but it works fine with the tensorflow-nightly build for me.", "We are having the same issue tf 2.7.0 when converting to tflite\r\n`TensorListFromTensor, TensorListGetItem, TensorListReserve, TensorListSetItem, TensorListStack`\r\nWe try avoiding selected ops as well as the code runs with tflite alone on the inference machine.  \r\n\r\nI tried tf-nightly as of today and it still has the same problem. Model will not convert without enabling SELECT_TF_OPS. \r\n\r\n> I am seeing the same problem in my own code with 2.7.0 but it works fine with the tensorflow-nightly build for me.\r\n\r\n"]}, {"number": 53090, "title": "Variables don't sync properly between 2 GPUs when SLI is enabled on Windows", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows-10-10.0.19041-SP0\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary (pip)\r\n- TensorFlow version (use command below): v2.7.0-rc1-69-gc256c071bb2 2.7.0\r\n- Python version: 3.7.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:  11.2 / 8.1.0\r\n- GPU model and memory: 2 x NVIDIA GeForce GTX 980 with SLI ON. 4GB memory\r\n\r\n**Describe the current behavior**\r\nWhen SLI is on variables don't sync properly between the 2 GPUs. I first noticed by getting `nan` loss values, but this simple code fails:\r\n```\r\nv = tf.Variable(1.0)\r\nwith tf.device('/gpu:1'):\r\n    print(v)\r\n```\r\nOutput:\r\n```\r\n2021-11-17 03:03:58.146222: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-11-17 03:03:58.788548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2691 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 980, pci bus id: 0000:01:00.0, compute capability: 5.2\r\n2021-11-17 03:03:58.791426: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 2691 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 980, pci bus id: 0000:02:00.0, compute capability: 5.2\r\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.0>\r\n```\r\n\r\nNote that turning SLI off makes the issue disappear, but also the model runs slower.\r\n\r\n**Describe the expected behavior**\r\nVariables should have the same value regardless of the device.\r\nIn the example above, with SLI disabled, the variable has the correct value (1):\r\n```\r\n2021-11-17 03:16:47.448047: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-11-17 03:16:48.068154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2777 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 980, pci bus id: 0000:01:00.0, compute capability: 5.2\r\n2021-11-17 03:16:48.069778: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 2777 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 980, pci bus id: 0000:02:00.0, compute capability: 5.2\r\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>\r\n```\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```\r\nimport tensorflow as tf\r\nv = tf.Variable(1.0)\r\nwith tf.device('/gpu:1'):\r\n    print(v)\r\n```\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@tilakrayal since you tagged this as TF 2.7, I also tested on 2.6 and 2.5 and have the same unintended behavior.\r\n\r\nI have some additional GPU info outputted by tensorflow 2.5 which may be helpful:\r\n```\r\n2021-11-17 19:39:18.708044: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 980 computeCapability: 5.2\r\ncoreClock: 1.342GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 208.91GiB/s\r\n2021-11-17 19:39:18.830701: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties:\r\npciBusID: 0000:02:00.0 name: NVIDIA GeForce GTX 980 computeCapability: 5.2\r\ncoreClock: 1.342GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 208.91GiB/s\r\n2021-11-17 19:39:18.832395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1\r\n2021-11-17 19:39:19.479336: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-11-17 19:39:19.479452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 1\r\n2021-11-17 19:39:19.481625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N Y\r\n2021-11-17 19:39:19.482735: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 1:   Y N\r\n2021-11-17 19:39:19.483528: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2687 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 980, pci bus id: 0000:01:00.0, compute capability: 5.2)\r\n2021-11-17 19:39:19.484916: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 2687 MB memory) -> physical GPU (device: 1, name: NVIDIA GeForce GTX 980, pci bus id: 0000:02:00.0, compute capability: 5.2)\r\n```", "@sanatmpa1 ,\r\nI was able to reproduce the issue in tf v2.6, v2.7 and nightly.Please find the gist [here](https://colab.research.google.com/gist/tilakrayal/62ccfbc9cf11f6f3e52633046bfabe10/untitled117.ipynb).", "Additional update:\r\nTurning off Intel VMX Virtualization Technology on the BIOS (Intel i5 11600K, ASUS Z590 motherboard) makes the issue go away (that means it works properly regardless of having SLI on or off).\r\n\r\nNot sure what the interaction between SLI and Intel Virtualization is, other apps that use GPU work properly with SLI regardless of the virtualization mode.", "When SLI is on and when you test with first GPU `tf.device('/gpu:0')` what is the output you're getting.", "@sachinprasadhs with `tf.device('/gpu:0')` it works properly:\r\n\r\n```import tensorflow as tf\r\nv = tf.Variable(1.0)\r\nwith tf.device('/gpu:0'):\r\n     print(v)\r\n\r\n2021-11-24 18:42:28.808877: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-11-24 18:42:29.476401: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2691 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 980, pci bus id: 0000:01:00.0, compute capability: 5.2\r\n2021-11-24 18:42:29.479514: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 2691 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 980, pci bus id: 0000:02:00.0, compute capability: 5.2\r\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>\r\n```\r\n", "Explicitly setting the device when initializing the variable also fails when going from 1 GPU to another, but works when staying within the same GPU:\r\n\r\n```import tensorflow as tf\r\nwith tf.device('/gpu:0'):\r\n     v = tf.Variable(1.0)\r\n with tf.device('/gpu:0'):\r\n     print(v)\r\n```\r\n```import tensorflow as tf\r\nwith tf.device('/gpu:1'):\r\n     v = tf.Variable(1.0)\r\n with tf.device('/gpu:1'):\r\n     print(v)\r\n```\r\n\r\nIn both cases the output is the expected (1). But for\r\n\r\n```import tensorflow as tf\r\nwith tf.device('/gpu:0'):\r\n     v = tf.Variable(1.0)\r\n with tf.device('/gpu:1'):\r\n     print(v)\r\n```\r\nand\r\n```import tensorflow as tf\r\nwith tf.device('/gpu:1'):\r\n     v = tf.Variable(1.0)\r\n with tf.device('/gpu:0'):\r\n     print(v)\r\n```\r\nIt prints 0.", "Can you try to set the visible devices in the console with `set CUDA_VISIBLE_DEVICES=0,1` and try the output. ", "I tried and the behavior is the same, I get a 0 on the second GPU"]}]