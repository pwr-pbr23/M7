[{"number": 48003, "title": "Multiple inputs with diffrent shapes", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.4\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nWhen working with a graph we usually provide as inputs to the model a feature matrix (Batch, nb_nodes, nb_features) or (Batch, time, nb_nodes, nb_features) and adjacency matrix (batch, nb_nodes, nb_nodes) or (nb_nodes, nb_nodes)\r\n\r\n1. differences of input shapes are different something that isn't allowed in model.fit(), model.predict() or model.evaluate() (for this we can use a custom training and evaluation step that allow us to handle the difference in shape, however, is it possible to still use model.fit() in this kind of situation?)\r\n2. if using tf.data.Dataset.from_tensor_slices it returns `tensorflow.python.framework.errors_impl.InvalidArgumentError: Shapes of all inputs must match:` (is it possible to create tf.data.Dataset with x as multiple inputs with diffrent shapes?)\r\n\r\n**Will this change the current API? How?**\r\n\r\nFlexibility in input shapes\r\n\r\n**Who will benefit from this feature?**\r\n\r\nCreating support for GNN\r\n\r\n**Any Other info.**\r\n", "comments": ["@Mouradost,\r\nInputs with different shapes can be passed to `model.fit` and `model.predict` of `Keras Models` using **`Ragged Tensors`**. Please refer [documentation of Ragged Tensor](https://www.tensorflow.org/guide/ragged_tensor?hl=en#keras) for more details. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 48002, "title": "CUDA_ERROR_UNKNOWN in Docker Rootless Mode.", "body": "## Summary\r\nTensorflow docker image fails to use GPU\r\n\r\n## Configuration\r\n1. Debian 10\r\n2. Nvidia driver 460.32.03\r\n3. 3 RTX 2080, 128 GB of Memory\r\n\r\n## Explanation\r\nI was trying to set up a Tensorflow container in rootless-mode. I had to jump through a few hoops, like disabling cgroups and manually installing slirp4netns (v1.1.4).\r\n\r\nI tried the docker containers tensorflow/tensorflow:latest-gpu and tensorflow/tensorflow:2.4.1-gpu both seem to not work.\r\n\r\nI run\r\n```\r\ndocker run -it --rm --gpus all tensorflow/tensorflow:latest-gpu python\r\n```\r\nand everything starts normally. nvidia-smi reports all three of my gpu's. If I try to test functionality I get unknown error that I don't know how to track down in the container.\r\n\r\n```\r\n>>> import tensorflow as tf\r\n2021-03-23 05:58:32.641242: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n\r\n>>> print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\r\n2021-03-23 05:59:00.670935: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-03-23 05:59:00.673743: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2021-03-23 05:59:00.705979: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\r\n2021-03-23 05:59:00.706237: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: 45f73818aefd\r\n2021-03-23 05:59:00.706366: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: 45f73818aefd\r\n2021-03-23 05:59:00.706791: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 460.32.3\r\n2021-03-23 05:59:00.706968: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 460.32.3\r\n2021-03-23 05:59:00.707088: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 460.32.3\r\nNum GPUs Available:  0\r\n```\r\n\r\n## Extra Information\r\nIf it helps I tried to use these resources.\r\n\r\nhttps://docs.docker.com/engine/install/debian/.\r\nhttps://docs.docker.com/engine/security/rootless/\r\nhttps://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker\r\n\r\n", "comments": ["Docker version = 20.10.5", "@gmaratos \r\nPlease confirm if you are using tensorflow 1.14, as there is no support for yf 1.x , please upgrade to/install tf 2.x and let us know if you face any issues.", "Yes I can confirm on both images\r\n```\r\n$> docker run -it --rm --gpus all tensorflow/tensorflow:latest-gpu python\r\nPython 3.6.9 (default, Oct  8 2020, 12:12:24) \r\n[GCC 8.4.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n2021-03-23 13:52:44.163052: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n>>> tf.__version__\r\n'2.4.1'\r\n\r\n$> docker run -it --rm --gpus all tensorflow/tensorflow:2.4.1-gpu python\r\nPython 3.6.9 (default, Oct  8 2020, 12:12:24) \r\n[GCC 8.4.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n2021-03-23 13:54:20.110771: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n>>> tf.__version__\r\n'2.4.1'\r\n ```", "Please take a look at this related [thread](https://github.com/tensorflow/tensorflow/issues/32623)\r\nAlso see this particular [solution](https://github.com/tensorflow/tensorflow/issues/32623#issuecomment-533936509) posted by the user. Thanks!\r\n", "Ok this fixed the problem. I ran the Device Node Verification script from [here](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#runfile-verifications) on the host.\r\n\r\nNow all three GPU's are available in the container. Unfortunately I was a little too eager and just ran the script first. I cannot confirm if `nvidia-modprobe` also solves the problem or what the device permissions were before I ran it.\r\n\r\nSomehow how I missed that previous issue, thank you so much for your help.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48002\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48002\">No</a>\n"]}, {"number": 48000, "title": "Assert shapes in categorical_accuracy(), resolve local_test.py errors", "body": "Refer this [issue](https://github.com/tensorflow/tensorflow/issues/46953) for details. Refer this [PR](https://github.com/tensorflow/tensorflow/pull/47343) for discussions.", "comments": ["@chenmoneygithub \nThanks for your patience and bearing with me, it means a lot.", "No problem! Thank you for your contribution to Keras!"]}, {"number": 47999, "title": "TensorFlow Lite file Conversion ", "body": "\r\nI have trained a Tfrecord file and converted into tflite file. I have used roboflow instructions as I don't have any knowledge on this domain. As per instructions https://gist.github.com/kishorekethineni/7b170822f27019d2f142be117f9855af I had run all the mentioned scripts exactly and generted tflite file successfully also checked tfilte file for input and output arrays worked fine there. required files: https://drive.google.com/drive/folders/1JnOp1ZXQCKR4RyNbg3BMuogJMOd9YmHg?usp=sharing\r\n\r\n\r\n\r\n### 2. Code\r\n\r\nhttps://colab.research.google.com/drive/1qXn9q6m5ug7EWJsJov6mHaotHhCUY-wG?usp=sharing (GoogleColab)\r\n(can check code here..)\r\n\r\n\r\n!tflite_convert \\\r\n  --input_shape=1,300,300,3 \\\r\n  --input_arrays=normalized_input_image_tensor \\\r\n--output_arrays=TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3 \\\r\n  --allow_custom_ops \\\r\n  --graph_def_file=/content/models/research/fine_tuned_model/tflite/tflite_graph.pb \\\r\n  --output_file=\"/content/models/research/fine_tuned_model/final_model.tflite\"\r\n\r\nhttps://colab.research.google.com/drive/1qXn9q6m5ug7EWJsJov6mHaotHhCUY-wG?usp=sharing (GoogleColab)\r\n\r\nhttps://drive.google.com/drive/folders/1JnOp1ZXQCKR4RyNbg3BMuogJMOd9YmHg?usp=sharing (TensorFlow Lite file)\r\n\r\n\r\n```\r\n\r\n### 3. Failure after conversion\r\nModel was not producing any detection\r\n\r\n\r\n", "comments": ["@MeghnaNatraj ", "@kishorekethineni Are you owning the original roboflow document? Is there any modification you made in the roboflow document if you just follow the roboflow document? Also I would know whether the above roboflow document was usually working or not.", "@abattery  Used the roboflow document as it is just changed file paths and num_steps to 1000 for quick train.", "@abattery  reducing num_steps from 100000 to 1000 will affect the detection accuracy?", "I don't know the details of the roboflow document. I would recommend discussing this with the roboflow colab author. For the accuracy problem, I think you can try some more experiments with the original parameter set.\r\n\r\nClosing this issue for now since this looks like not an issue of the TensorFlow project.\r\n\r\nPlease reopen if you find a TensorFlow specific issue after discussing with the roboflow colab author.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47999\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47999\">No</a>\n"]}, {"number": 47997, "title": "[INTEL MKL] Disable group conv test.", "body": "Disables group conv test when oneDNN is enabled.  Also disables test when oneDNN is enabled via env variables (pending merging of the PR https://github.com/tensorflow/tensorflow/pull/47745). This PR will work even if that PR is not merged.\r\nThis regression was caused by recent commit https://github.com/tensorflow/tensorflow/commit/7b8db6083b34520688dbc71f341f7aeaf156bf17 that enabled it for CPUs.\r\nWe will submit another PR to add group conv support for oneDNN based conv_ops shortly.", "comments": []}, {"number": 47995, "title": "Allow providing custom hash to snapshot", "body": "This exposes a new parameter `hash_code` to `tf.data.experimental.snapshot`.\r\n\r\nUsers may want to specify this parameter to control the behavior of when snapshot enters read vs. write mode.\r\nAs an example: If snapshot is generated outside of training, and the dataset call-chain/graph is guaranteed to be unchanged,\r\nthen a simple hash function can checksum all the input files and generate a hash_code.\r\n\r\nAs long as the input dataset remains the same during training, the hash function can generate the same hash_code, and the\r\ngenerated snapshots would be read. If the input dataset is different, the hash function would generate a new hash_code,\r\nand therefore the SnapshotDataset op would generate new snapshots.\r\n\r\nThis fixes #47994\r\n\r\nThis extends the custom hash param exposed in the C++ API: https://github.com/tensorflow/tensorflow/commit/cbc7f31b7d6aac81158be084201d3b3e8e346907", "comments": ["Being able to override the hashing function does not seem like a good idea. The intended use for `snapshot` is not to be able to read \"snapshot\" generated by different input pipeline graphs and relying on users to correctly determine whether their input pipeline graph have changed and it is therefore safe to read the snapshot is prone to error.\r\n\r\nTo decouple the graph used for reading and writing the materialized data, users should use [save](https://www.tensorflow.org/api_docs/python/tf/data/experimental/save?version=nightly) and [load](https://www.tensorflow.org/api_docs/python/tf/data/experimental/load?version=nightly) instead. \r\n\r\n", "Hi @jsimsa, Thank you for your comment. The capability for adding a custom hash was added by this PR: https://github.com/tensorflow/tensorflow/commit/cbc7f31b7d6aac81158be084201d3b3e8e346907#diff-ddd28df5aaf5fafc521b4012e0fca1801ec50c56dc41a3e2196c98223404b3b4R81\r\n\r\nOur use-case is this: Our Deep-learning jobs always have an infra-portion of the dataset graph, and a user-customizable part of the dataset graph. We would like to always pre-generate snapshots for the infra portion so that all users can benefit from different pre-generated snapshots. However, this strategy does not work well with Estimators as the Estimator expects an entire input_fn. \r\nIt is easier for us if we can pre-generate the snapshots, and then use the same hash for all training jobs that use the same data, regardless of the model training framework(keras vs estimator).\r\n\r\nWould it be ok to add additional warnings for users to only use this when they are certain that the dataset chain and input set have not changed?\r\n\r\nThe issue with using save and load is that the sharding and reading is not as well implemented as `snapshot`, and also checkpoint versioning complicates saving and loading.\r\n", "> Hi @jsimsa, Thank you for your comment. The capability for adding a custom hash was added by this PR: [cbc7f31#diff-ddd28df5aaf5fafc521b4012e0fca1801ec50c56dc41a3e2196c98223404b3b4R81](https://github.com/tensorflow/tensorflow/commit/cbc7f31b7d6aac81158be084201d3b3e8e346907#diff-ddd28df5aaf5fafc521b4012e0fca1801ec50c56dc41a3e2196c98223404b3b4R81)\r\n\r\nThis change introduced the hash as an attribute of the op to avoid the need to recompute the hash repeatedly for efficiency sake. The intended use is not for the hash to be exposed in the user-facing API for the reasons I mentioned in my previous response.\r\n\r\n> Our use-case is this: Our Deep-learning jobs always have an infra-portion of the dataset graph, and a user-customizable part of the dataset graph. We would like to always pre-generate snapshots for the infra portion so that all users can benefit from different pre-generated snapshots. However, this strategy does not work well with Estimators as the Estimator expects an entire input_fn.\r\n> It is easier for us if we can pre-generate the snapshots, and then use the same hash for all training jobs that use the same data, regardless of the model training framework(keras vs estimator).\r\n>\r\n> Would it be ok to add additional warnings for users to only use this when they are certain that the dataset chain and input set have not changed?\r\n> \r\n> The issue with using save and load is that the sharding and reading is not as well implemented as `snapshot`, and also checkpoint versioning complicates saving and loading.\r\n\r\nThe arguments of `save` and `load` mirror the read and write path of `snapshot` and are implemented using the same building blocks as snapshot, so it is not clear what part of `save` and `load` is not as well implemented. For your use case, you could use `save` to pre-generate \"snapshots\" and your user input pipelines would use `load` to read from one of the pre-generated snapshots. \r\n\r\n\r\n", "@jsimsa thank you for your clear explanation. Are you referring to this save and load implementation: https://github.com/tensorflow/tensorflow/blob/827d029baabdcedac973f47076dd2ec9932d6938/tensorflow/core/kernels/data/experimental/io_ops.cc#L190\r\n\r\nThe inconvenience with the load api is that it requires an output tensor shape, whereas the snapshot API does not.", "I am referring to the Python [API](https://www.tensorflow.org/api_docs/python/tf/data/experimental/load?version=nightly) that I also linked in my earlier [response](https://github.com/tensorflow/tensorflow/pull/47995#issuecomment-804527253). The need for providing `element_spec` has been removed recently. TF nightly does not need it and neither will TF 2.5.", "@jsimsa  Thank you for being so responsive! We are on TF 2.4.0. Is this the only backport we need to use the load api without tensor_spec: https://github.com/tensorflow/tensorflow/commit/9132f00c2071753b3da43b81913b7c35d402c610", "yes, it should be", "@ashahab Any update on this PR? Please. Thanks!", "@gbaned I'm working on testing save and load as a viable alternative this. I can get back to this in 1-2 weeks.", "@gbaned @jsimsa  We were able to test `tf.data.experimental.save` and `tf.data.experimental.load` as alternatives to `tf.data.experimental.snapshot`. The issue is that unlike `tf.data.experimental.snapshot`, `tf.data.experimental.save` is not a dataset operation. Therefore, we can't iterate through it to incrementally save the dataset. Our datasets are large and can easily take upwards of 2-4 hours to save. When we were using `tf.data.experimental.snapshot`, we were able to expose the progress of snapshot writes to tensorboard. With `tf.data.experimental.save` there's no such opportunity, unless there's a hook to publish something from the iteration: https://github.com/tensorflow/tensorflow/blob/d96b20aa303b9a90d407cd80b94288626bd8e22e/tensorflow/core/kernels/data/experimental/io_ops.cc#L104\r\n\r\nHow can we solve this issue? If we cannot, we have to revert to using snapshot (with the custom hash id).", "When you talk about incrementally saving a dataset are you referring to checkpointing progress in the presence of pre-emptions?\r\n\r\nHow did monitor progress of `snapshot` through TensorBoard?\r\n\r\nI would rather improve the `save` and `load` API and implementation, instead of exposing implementation details of `snapshot` that should not be exposed.", "Hi @jsimsa What I mean is the tf.data.experimental.snapshot's Writer is a DatasetIterator, which saves the dataset element when GetNextInternal is called. \r\nWe exposed it to tensorboard by emitting a counter to after each call to next(dataset).\r\n\r\nIt is different when tf.data.experimental.save is called, it will iterate through the entire dataset, and therefore we cannot do the same.\r\n", "> It is different when tf.data.experimental.save is called, it will iterate through the entire dataset, and therefore we cannot do the same.\r\n\r\nYou can do the following:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nwriter = tf.summary.create_file_writer(\"/tmp/example\")\r\ncounter = tf.Variable(0, dtype=tf.int64)\r\n\r\ndef log_progress(x):  \r\n  counter.assign_add(1)\r\n  with writer.as_default():\r\n    tf.summary.scalar(\"elements_saved\", counter, counter)  \r\n  return x\r\n\r\nds = ... # your dataset\r\nds = ds.map(log_progress)\r\ntf.data.experimental.save(ds)\r\n``` ", "Thank you @jsimsa . However, what is the reason for not implementing `tf.data.experimental.save` as a DatasetBase operation? Is there a plan to convert it to DatasetBase later?", "What would be the use case for `save` behaving as [tee](https://linuxize.com/post/linux-tee-command/)? There are no plans for this at the moment.", "@jsimsa Thank you for your response.\r\nThe use-case for save behaving like a dataset operation would allow users to iterate through it while saving, and inject stateful functions. Your above example only works for stateless functions, whereas we want to keep state such as \"time when the first operation was done, and time when the current operation is being done\".\r\nThe other reason is that it would make `tf.data.experimental.save` similar to `tf.data.experimental.load` which does require dataset iteration.", "Why does my example only work for stateless functions? \r\n\r\nThe asymmetry between `save` and `load` is intentional and commonplace. Calling `save` on an object saves the contents of an object instance . Calling `load` on an object, creates a new instance of the object by loading its contents.", "@jsimsa \r\nThis is our code that calculates write throughput. I am not sure how we keep track of last_record_time and last_batch_count in your example:\r\n```python\r\nlast_record_time = time.time()\r\nlast_batch_count = 0\r\nbatch_count = 0\r\nfor _ in dataset:\r\n    batch_count += 1\r\n    if batch_count % SNAPSHOT_THROUGHPUT_MEASUREMENT_GRANULARITY == 0:\r\n        current_example_count=batch_count * batch_size,\r\n        last_example_count=last_batch_count * batch_size\r\n        throughput = (current_example_count - last_example_count) / (time.time() - last_record_time)\r\n        with snapshot_summary_writer.as_default():\r\n            tf.summary.scalar(name=SNAPSHOT_THROUGHPUT_SUMMARY_NAME, data=throughput,\r\n                              step=current_example_count, description=SNAPSHOT_THROUGHPUT_SUMMARY_DESCRIPTION)\r\n        last_batch_count = batch_count\r\n        last_record_time = time.time()\r\n```", "Executing the following program:\r\n\r\n```\r\nimport time\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nLOG_EVERY_N_BATCHES=2\r\nBATCH_SIZE=10\r\n\r\nlast_record_time = time.time()\r\nlast_batch_count = 0.0\r\n\r\nbatch_count = tf.Variable(0)\r\n\r\ndataset = tf.data.Dataset.range(200)\r\ndataset = dataset.batch(BATCH_SIZE)\r\n\r\ndef helper(batch_count):\r\n  global last_record_time\r\n  global last_batch_count\r\n  current_example_count=np.float32(batch_count * BATCH_SIZE)\r\n  last_example_count=np.float32(last_batch_count * BATCH_SIZE)\r\n  throughput = (current_example_count - last_example_count) / (time.time() - last_record_time)\r\n  last_batch_count = batch_count\r\n  last_record_time = time.time()\r\n  print(throughput, current_example_count)  # debug print statement\r\n  return throughput, current_example_count\r\n\r\ndef track_progress(x):\r\n  batch_count.assign_add(1)\r\n  if batch_count % LOG_EVERY_N_BATCHES == 0:\r\n    throughput, current_example_count = tf.py_function(func=helper, inp=[batch_count], Tout=(tf.float32, tf.float32))\r\n    # ... your summary writer logic here    \r\n  return x\r\n\r\ndataset = dataset.map(track_progress)\r\n\r\nfor _ in dataset:\r\n  pass\r\n```\r\n\r\nproduces the following example output:\r\n\r\n```\r\n181.52955917136075 20.0\r\n22863.47233578632 40.0\r\n20580.490677134447 60.0\r\n27539.750492449115 80.0\r\n36519.84327383544 100.0\r\n32909.40761082777 120.0\r\n32400.957898802626 140.0\r\n31242.487895716946 160.0\r\n36615.48668703623 180.0\r\n19481.207617278218 200.0\r\n```", "Thank you @jsimsa , this is helpful.\r\nIf we wanted to modify the batch size, do we reset BATCH_SIZE to something else before calling dataset = dataset.map, or is there a cleaner and more tensorflow-friendly way?\r\ni.e:\r\n```python\r\nBATCH_SIZE=10\r\n...\r\ndef track_progress(x):\r\n...\r\nBATCH_SIZE=64\r\ndataset = dataset.map(track_progress)\r\n```", "@jsimsa  we have discovered a bug with snapshots: https://github.com/tensorflow/tensorflow/issues/48903\r\nWe have not yet tested it on `tf.data.experimental.load`, but since the implementations are similar, it is possible that load also runs into a similar race condition as snapshot.", "@jsimsa Can you please assist on above comments from @ashahab. Thanks!", "@jsimsa Any update on this PR? Please. Thanks!", "As per my earlier [comment](https://github.com/tensorflow/tensorflow/pull/47995#issuecomment-804527253), I do not think this PR makes sense.", "Sorry I was out this morning. Yes it is fine to close this one. I have a\npatch pending in 2.5 though for save and load, would like to see that\nreviewed.\n\nOn Fri, Jun 25, 2021 at 12:17 PM Jiri Simsa ***@***.***>\nwrote:\n\n> Closed #47995 <https://github.com/tensorflow/tensorflow/pull/47995>.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/47995#event-4941608641>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAFISPNY2WXHYGVDLXM76WLTUTI3FANCNFSM4ZUDWOVQ>\n> .\n>\n"]}, {"number": 47993, "title": "[Intel MKL] scope_alloctor bug fix with One binary support for v2.5", "body": "Patch for scope_allocator Fix that worked in TF2.3 and TF2.4.1. \r\nModifying the same patch or TF2.5 / Master with some attention to one binary support here.\r\nContext: https://github.com/tensorflow/tensorflow/issues/34213", "comments": ["@penpornk Scope_allocator bug fix for v2.5\r\nFYI @yixinshi \r\n", "@nammbash Thank you for the PR!\r\n\r\nContext for @ezhulenev : This is a TF-oneDNN fix for issue https://github.com/tensorflow/tensorflow/issues/34213#issuecomment-783857373", "@penpornk Accepted your suggestion. Thank you. \r\n For future: We run clang internally on CPP files! Should we run buildifier BUILD file checks also?"]}, {"number": 47992, "title": "[INTEL MKL] Fix failing v1 CI test", "body": "Added check for dtype as MklMatMul supports bfloat16 and float32. whereas the default type is float64. Now with this fix, the correct op names can be compared.", "comments": []}, {"number": 47991, "title": "Python 3.6.12-debug is not supported, but documentation says it is", "body": "This works:\r\n```\r\npyenv shell 3.6.12\r\npip install --upgrade pip\r\npip install tensorflow\r\n```\r\nThis also works:\r\n```\r\npyenv shell 3.8.5-debug\r\npip install --upgrade pip\r\npip install tensorflow\r\n```\r\n\r\nBut this:\r\n```\r\npyenv shell 3.6.12-debug\r\npip install --upgrade pip\r\npip install tensorflow\r\n```\r\nsays \r\n```\r\nERROR: Could not find a version that satisfies the requirement tensorflow\r\nERROR: No matching distribution found for tensorflow\r\n```\r\nAnd, afaik, this is nowhere documented\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): All versions\r\n- Python version: 3.6.12-debug\r\n- CUDA/cuDNN version: Different versions\r\n- GPU model and memory: Different versions\r\n\r\n", "comments": ["@Volker-Weissmann \r\nCould you please verify and update if this is still an issue.", "I tested it and it seems to be fixed.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47991\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47991\">No</a>\n"]}, {"number": 47990, "title": "enh: add stateless flag to numpy_function", "body": "fixes #47777 \r\n\r\n`numpy_function` can now be stateless.\r\n\r\nAdded test to ensure that the op is stateless/stateful depending on the flag.\r\n", "comments": ["Since this changes public API, waiting for review from API owners first.", "The API owners are moving to a rotation based system instead of the meetings. Adding couple API Owners as reviewers.", "I've changed everything as noted, thanks for the remarks", "Thanks!", "Thanks for the review and all the work with the library!", "Looks like a couple of tests are failing - check the logs for Ubuntu CPU for example.", "So one if from the API compatibility test: since we introduce a new option in the function, this indeed changes the API. But I am not more familiar with why this fails.\r\n\r\nFor the other one, it basically tells that the statelessness does not work...\r\n\r\nCan you rerun the tests? ", "For the API compatibility test, the test logs should give you the commands to run to update the tests.", "> For the API compatibility test, the test logs should give you the commands to run to update the tests.\r\n\r\nIt doesn't, as far as I can see (https://source.cloud.google.com/results/invocations/eb308cd4-fcad-4909-8e89-4de24d817093/targets/%2F%2Ftensorflow%2Ftools%2Fapi%2Ftests:api_compatibility_test/tests;group=__main__.ApiCompatibilityTest;test=testAPIBackwardsCompatibility;row=1)\r\n\r\nIt just says that it failed", "Sorry, those details are in the test log, I should have clarified it: https://source.cloud.google.com/results/invocations/eb308cd4-fcad-4909-8e89-4de24d817093/targets/%2F%2Ftensorflow%2Ftools%2Fapi%2Ftests:api_compatibility_test/log (check the tabs at the top: Test | Target Log | Target Details | Artifacts).\r\n\r\nHere's the relevant piece:\r\n\r\n```\r\nERROR:tensorflow:TensorFlow API backwards compatibility test\r\nThis test ensures all changes to the public API of TensorFlow are intended.\r\n\r\nIf this test fails, it means a change has been made to the public API. Backwards\r\nincompatible changes are not allowed. You can run the test as follows to update\r\ntest goldens and package them with your change.\r\n\r\n    $ bazel run tensorflow/tools/api/tests:api_compatibility_test \\\r\n    #     -- --update_goldens True\r\n\r\nYou will need an API approval to make changes to the public TensorFlow API. This\r\nincludes additions to the API.\r\n```", "Ah indeed, sorry, still unfamiliar with the TF CI! Thanks, I'll do that", "@mayou36  Any update on this PR? Please. Thanks!", "Indeed, I've updated the golden rule. Can you run the workflow again?", "this makes numpy_function the same as py_func in v1 was. This change was explicit, because it is almost always a bad idea to do stateful things. I don't see how this is an enhancement right now. Can you explain what your goals are with this change in terms of use-cases?\r\n", "> this makes numpy_function the same as py_func in v1 was. This change was explicit, because it is almost always a bad idea to do stateful things. I don't see how this is an enhancement right now. Can you explain what your goals are with this change in terms of use-cases?\r\n\r\nExactly. This is the goal. `numpy_function` _is_ currently stateful, the goal is to have a flag to allow to make it stateless.\r\n\r\nThere are two functions, `py_function` and `numpy_function`. The former keeps track of the gradients and stuff that happens inside (AFAIU it's an \"eager wrapper\") and can accumulate memory (as explained here: https://github.com/tensorflow/tensorflow/issues/35010#issuecomment-798198842), while the latter does not (AFAIU it's a \"pure Python, no TF\"). Unfortunately, `numpy_function` is always considered stateful, although the internal function actually takes a \"stateful\" flag. Instead of making this always to be true, the proposed change allows to pass this flag through to enable a stateless `numpy_function`.\r\n\r\nThe current problem on this PR is: how to _test_ this. Actually, I would suggest to not test the actual function. Because it is just about passing through a flag and giving the runtime a guarantee. Testing that guarantee would need internal knowledge.\r\n", "Sorry, there's a change that we missed and was caught by the internal checks - could you add a short bullet to https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md, under \"Bug Fixes and Other Changes\", \"TF Core\"? And on this occasion, you might wish to add your name under the Contributors list as well.", "@jonas-eschle Can you please check @mdanatg's comments and keep us posted ? Thanks!", "Thanks, I've done that (not pushed yet), and finally fixed the tests.\r\n\r\nOne thing:\r\n> you might wish to add your name under the Contributors list as well.\r\n\r\nSure, but where exactly? I've tried to find a place (a \"CONTRIBUTORS\" file?) but couldn't find anything.", "@mdanatg  Can you please assist on above comments from @jonas-eschle. Thanks!\r\n", "Sorry, I missed the reply. I was referring to the \"Thanks to our Contributors\" section in the same RELEASE.md file. A word of caution, I think the version number incremented around these days. You may want to rebase the file before modifying it.", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47990) for more info**.\n\n<!-- need_author_consent -->", "Okay, can you please run the CI again? It is ready from my side.\r\nI've added my name, but there was no other name. Is it correct like that? Otherwise, feel free to modify directly\r\n\r\nBTW: I think you can ignore the bot, that was a mistake from my side. Now all should be fine", "@jonas-eschle Can you please resolve conflicts? Thanks!", "Done, ready from my side", "@jonas-eschle Can you please resolve conflicts? Thanks!", "@gbaned resolved them and moved to 2.8 release", "@jonas-eschle Can you please resolve conflicts? Thanks!", "Check out this pull request on&nbsp; <a href=\"https://app.reviewnb.com/tensorflow/tensorflow/pull/47990\"><img align=\"absmiddle\"  alt=\"ReviewNB\" height=\"28\" class=\"BotMessageButtonImage\" src=\"https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png\"/></a> \n\n See visual diffs & provide feedback on Jupyter Notebooks. \n\n---\n\n <i>Powered by <a href='https://www.reviewnb.com/?utm_source=gh'>ReviewNB</a></i>", "Resolved the conflicts and moved it into the 2.8 changelog"]}, {"number": 47988, "title": "TF-TRT Einsum converter for BatchMatMul like equations", "body": "This PR implements TF-TRT Einsum op converter. Currently it accepts inputs describing (Batched) MatMul like operations.\r\n\r\nLet's use the equation \"cebfad,fageb->abcdg\"  to illustrate how the conversion works:\r\n\r\n1. Identify batch dims (ab), contract dims (ef) and free dims (cd, g).\r\n2. Find a transpose that brings the input to a layout where the batch/contract/free dims are contiguous, and batch dims are in the front: cebfad --> ba cd ef,   faged --> ba ef g\r\n3. Reshape the inputs so that the inputs have a single free and contract dim:\r\n   ba cd ef --> ba x y,   where x = cd, and y = ef\r\n   be ef g  --> ba y g\r\n4. Perform the batche matmul: baxy, bayg -> baxg\r\n5. Expand the free dims and transpose the output baxg -> abcdg\r\n\r\nTagging @bixia1 for review and @DEKHTIARJonathan for visibility.", "comments": []}, {"number": 47987, "title": "Fix collections.abc import for Python 3.9", "body": "This PR fixes Python 3.9 compatibility of `keras/feature_column/base_feature_layer.py` by replacing the deprecated `collections` import.\r\n\r\nThis also cleans up `collections.abc` imports in the rest of the Keras code base.", "comments": []}, {"number": 47986, "title": "Do not constant fold FloorDiv(x, 1)", "body": "This PR prevents grappler constant folding of `FloorDiv(x, 1.0)` since `FloorDiv(x, 1.0) == Floor(x) != x`.\r\n\r\nFixes #47970", "comments": ["@lgeiger thanks for the fix!"]}, {"number": 47985, "title": "Invalid format in tf.math.log doc page", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/math/log\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\n<img width=\"1039\" alt=\"Screen Shot 2021-03-23 at 5 12 34 AM\" src=\"https://user-images.githubusercontent.com/8815362/112052367-6291a700-8b96-11eb-8535-c874920d66b4.png\">\r\n\r\nThe example section is broken.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/e13f434909c7007b14f7930370472f26fba06f4e/tensorflow/core/api_def/python_api/api_def_Log.pbtxt#L13-L22\r\n\r\nMaybe the above lines are invalid and markdown-style codeblock lines(```) should be removed like [tensorflow/core/api_def/python_api/api_def_Log1p.pbtxt](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/python_api/api_def_Log1p.pbtxt).", "comments": ["@jeongukjae,\r\nLooks like the issue is fixed in the nightly version of the documentation. \r\n\r\nPlease check [this link](https://www.tensorflow.org/api_docs/python/tf/math/log?version=nightly) for reference. Thanks!", "@amahendrakar Thanks! I can see this issue is fixed in that link."]}, {"number": 47984, "title": "Delete TOSA gather op legalizations temporarily.", "body": "Updating op signature in LLVM, new lowerings to be restored afterward.\r\n\r\nChange-Id: I0f87ef6d4aea4d28305b79bc437a743eeb2b506c\r\nSigned-off-by: Suraj Sudhir <suraj.sudhir@arm.com>", "comments": ["@stellaraccident and @rsuderman - here's the first push to remove the TOSA gather legalizations so we can follow up by updating the LLVM side and then restore newer versions of these. ", "Thanks, suraj. We're in the middle of another LLVM merge and I'll get this landed once we're over that hump (tmw hopefully).", "Thanks for the update! Your schedule is pretty well aligned with the preparation of the next set of changes in the pipeline, so I'm not currently blocked . "]}, {"number": 47983, "title": "ERROR: cannot compute CombinedNonMaxSuppression as input #0(zero-based) was expected to be a float tensor but is a uint8 tensor \t (while executing 'CombinedNonMaxSuppression' via Eager) ERROR: Node number 644 (TfLiteFlexDelegate) failed to invoke.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04/Android 10\r\n- Mobile device if the issue happens on a mobile device: Xiaomi Redmi Note 7\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): tf-nightly-gpu==2.5.0.dev20210322\r\n- Python version: 3.8\r\n\r\n**Describe the current behavior**\r\nI am currently running a benchmark on android and the following error is raised:\r\n```\r\nERROR: cannot compute CombinedNonMaxSuppression as input #0(zero-based) was expected to be a float tensor but is a uint8 tensor (while executing 'CombinedNonMaxSuppression' via Eager)\r\nERROR: Node number 644 (TfLiteFlexDelegate) failed to invoke.\r\n```\r\n\r\nThe thing is I do not have control over the inner operations in the TFLite model. Moreover, I am already casting it into `tf.float32` when building the `tf.keras.Model`:\r\n\r\n```\r\nboxes_to_nms = tf.expand_dims(bboxes, axis=2)\r\nboxes = tf.cast(boxes_to_nms, tf.float32)\r\nscores = tf.cast(scores_to_nms, tf.float32)\r\n\r\nnms_output = tf.image.combined_non_max_suppression(\r\n    boxes=boxes,\r\n    scores=scores,\r\n    ...\r\n    )\r\n```\r\n\r\nDoes someone know how to deal with this ERROR, because I do not find any relevant information online and I feel quite powerless here. Thanks in advance!\r\n\r\n**Standalone code to reproduce the issue**\r\nI am running the benchmark using the native tool which includes TFOps and Flex Delegate that can be downloaded [here](https://storage.googleapis.com/tensorflow-nightly-public/prod/tensorflow/release/lite/tools/nightly/latest/android_arm_benchmark_model_plus_flex).\r\n\r\nMy model can be downloaded [removed].\r\nI run the benchmark on an android divide using the following lines:\r\n```\r\nadb push android_arm_benchmark_model_plus_flex /data/local/tmp/benchmark\r\nadb shell chmod +x /data/local/tmp/benchmark\r\nadb push model.tflite /data/local/tmp/model.tflite\r\nadb shell \"/data/local/tmp/benchmark\"  --graph=\"/data/local/tmp/model.tflite\" --input_layer=input --input_layer_shape=1,800,1024,3\r\n```\r\n\r\n**Other informations**\r\nThe node number in the error is 644 but when I visualize the model using [netron](https://netron.app/), the CombinedNonMaxSuppression has the number 643 which is unexpected.", "comments": ["Could you try your conversion script with the recent TF version? If possible, could you share the reproducible steps for the conversion to us?", "I don't know if you meant TF 2.4.1 or TF 2.5.0.\r\n\r\n**In TF2.4.1**\r\nThe conversion fails and throws `IndexError: _Map_base::at. I read that this would be fixed in TF 2.5. This is why I did the conversion in TF 2.5\r\n\r\n**In TF 2.5.0**\r\nWhen I convert using TF 2.5, the conversion works even though I get the following warning `WARNING:absl:For model outputs containing unsupported operations which cannot be quantized, the 'inference_output_type' attribute will default to the original type.`\r\n\r\n**Conversion**\r\n```\r\nmodel = load_model() # I would rather not share this\r\n\r\n# Create Coco dataset\r\nds = tfds.load(name=\"coco/2017\", split=\"train\", data_dir=\"/data/datasets/tensorflow_datasets/\",)\r\nds = ds.map(lambda obj: tf.image.resize(obj[\"image\"], (800, 1024)))\r\n\r\n# Transform the dataset into a representative dataset as in the TF guide\r\ndef representative_data_gen():\r\n    for input_value in ds.batch(1).take(10):\r\n        # Model has only one input so each data point has one element.\r\n        yield [input_value]\r\n\r\n\r\n# Converter\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\r\n# Set the representative dataset in order to quantize the activations\r\nconverter.representative_dataset = representative_data_gen\r\n\r\n# Ensure that if any ops can't be quantized, the converter throws an error\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.target_spec.supported_types = [tf.int8]\r\n\r\n# Set the input and output tensors to uint8 (APIs added in r2.3)\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\n\r\n# Additional tricks\r\nconverter.experimental_new_converter = True\r\nconverter.experimental_new_quantizer = True\r\nconverter.target_spec.supported_ops = [\r\n    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.\r\n    tf.lite.OpsSet.SELECT_TF_OPS,  # enable TensorFlow ops.\r\n]\r\n\r\ntf_lite_quant_model = converter.convert()\r\n\r\n# saving converted model in TFLite file\r\nwith open(\"model.tflite\", \"wb\") as tf_file:\r\n    tf_file.write(tf_lite_quant_model)\r\n```\r\n\r\n\r\n\r\n\r\n", "converter.experimental_new_quantizer = False \r\n\r\nCould you try the conversion with the old quantizer in TF 2.5?", "@fredrec could you take a look?", "Using `converter.experimental_new_quantizer = False` fixed it. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47983\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47983\">No</a>\n", "@teijeong @liufengdb this issue is another example of the regression introduced by the new quantizer.", "can you provide difference between outputs of models regarding `converter.experimental_new_quantizer` flag? If it's only warning and the converted model works fine, then it would be fine.\r\n\r\nAdding @MeghnaNatraj for more context", "If you can also provide float version of the tflite model, it would help me looking into the issue.", "I couldn't find the corresponding ops in the tflite file for the two \"tf.cast\" operations. Could you find them in when you were reproducing that?"]}, {"number": 47982, "title": "TF-TRT Fix batch dim for unit test with scalar input", "body": "PR #40545 introduced tests with scalar input. For such input we should not access shape.dim_size(0). This PR fixes this error in the converter unit tests. \r\n\r\nTagging @bixia1 for review and @DEKHTIARJonathan for visibility.", "comments": []}, {"number": 47980, "title": " [ROCm] Fix for ROCM CSB breakage - 210322 ", "body": "## 1\r\nThe following commit adds GPU support for int32/int64 support for the Unique/UniqueWithCounts ops, but breaks ROCm build in the process\r\n\r\n02585ac\r\n\r\n```\r\ntensorflow/core/kernels/unique_op_gpu.cu.cc:292:9: error: no matching constructor for initialization of 'gpuprim::TransformInputIterator<int, SegmentIndicatorFunctor<unsigned long, int>, gpuprim::CountingInputIterator<int>>' (aka 'transform_iterator<rocprim::counting_iterator<int, long>, tensorflow::(anonymous namespace)::SegmentIndicatorFunctor<unsigned long, int>, int>')\r\n        segment_indicator_iter(0, {sorted_input_ptr});\r\n        ^                      ~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/kernels/unique_op_gpu.cu.cc:176:12: note: in instantiation of member function 'tensorflow::UniqueOpGPU<unsigned long, int>::ComputeAsync' requested here\r\n  explicit UniqueOpGPU(OpKernelConstruction* context)\r\n           ^\r\ntensorflow/core/kernels/unique_op_gpu.cu.cc:461:27: note: in instantiation of member function 'tensorflow::UniqueOpGPU<unsigned long, int>::UniqueOpGPU' requested here\r\nTF_CALL_REAL_NUMBER_TYPES(REGISTER_UNIQUE_GPU);\r\n```\r\n\r\nThis PR/commit disables ROCm support for newly added ops to get the CSB passing again. We are looking into resolving the build errors, and will file a separate PR to re-enable ROCm functionality for the same. This PR commit also adds the `no_rocm` tag to a couple of unit tests that start failing as a consequence of lack of ROCm support for these ops.\r\n\r\n```\r\n//tensorflow/python/keras/optimizer_v2:adamax_test_gpu                   FAILED in 3 out of 3 in 7.0s\r\n//tensorflow/python/training:adam_test_gpu                               FAILED in 3 out of 3 in 6.3s\r\n```\r\n\r\n## 2 \r\n\r\n The following commit adds a new unit-test which is failing on the ROCm platform\r\n\r\n50f8897\r\n\r\n```\r\n//tensorflow/python/keras/distribute:dataset_creator_model_fit_test_gpu  FAILED in 3 out of 3 in 112.5s\r\n```\r\n\r\nThis commit adds a `no_rocm` tag to temporarily disable that unit-test on ROCm.\r\n\r\n---------------------------------------------------------\r\n\r\n\r\n/cc @cheshire @chsigg\r\n\r\n", "comments": ["@cheshire @chsigg gentle ping.  Need this PR to get ROCm builds passing again.", "@deven-amd  Can you please check build failures. Thanks!", "@gbaned all the build error point to \r\n\r\n```\r\nERROR: /tmpfs/bazel_output/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/external/llvm-project/llvm/BUILD\r\n```\r\n\r\ndon't think they are related to the change in this PR.\r\n\r\ncan you trigger the tests again...maybe whatever was causing them, has been fixed.", "@gbaned , rebased my branch to resolve the merge conflict", "@gbaned , rebased (again) my branch to resolve the merge conflict", "@cheshire @chsigg gentle ping", "rebased PR to resolve merge conflict\r\n\r\n@cheshire @chsigg gentle ping", "rebased PR again to resolve merge conflict\r\n\r\n@cheshire @chsigg gentle ping", "@deven-amd can you please resolve conflicts ", "@rthadur rebased the PR again to resolve the merge conflict.\r\n\r\n@chsigg will need your approval again"]}, {"number": 47979, "title": "ImportError: DLL load failed while importing _pywrap_tensorflow_internal", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): idk\r\n- TensorFlow version: 2.4.1\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): idk\r\n- GCC/Compiler version (if compiling from source): idk\r\n- CUDA/cuDNN version: idk\r\n- GPU model and memory: n/a\r\n\r\n\r\n\r\n**Describe the problem**\r\nI am trying to install and use DeepLabCut on my CPU, which requires Tensorflow. Tensorflow 2.4.1 seems to be installed but whenever I try to import it I get error messages saying \r\n\r\n\"ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\nFailed to load the native TensorFlow runtime.\"\r\n\r\nTo address common fixes I see for this error - I have the most recent update of Visual C++, I am running 64-bit Python on a 64 bit machine, and my machine supports AVX2 (here are my specs https://ark.intel.com/content/www/us/en/ark/products/78937/intel-core-i7-4810mq-processor-6m-cache-up-to-3-80-ghz.html)\r\n\r\nSome people have apparently solved this problem by creating a separate environment, but I am new to Python and have no idea what that means or how to do it.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nimport tensorflow as tf\r\nTraceback (most recent call last):\r\n\r\n  File \"C:\\Users\\pc\\anaconda3\\envs\\DLC-CPU\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n\r\n  File \"C:\\Users\\pc\\anaconda3\\envs\\DLC-CPU\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n\r\n  File \"C:\\Users\\pc\\anaconda3\\envs\\DLC-CPU\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n\r\n  File \"C:\\Users\\pc\\anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n\r\n  File \"C:\\Users\\pc\\anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\n\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-39-64156d691fe5>\", line 1, in <module>\r\n    import tensorflow as tf\r\n\r\n  File \"C:\\Users\\pc\\anaconda3\\envs\\DLC-CPU\\Lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n\r\n  File \"C:\\Users\\pc\\anaconda3\\envs\\DLC-CPU\\Lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n\r\n  File \"C:\\Users\\pc\\anaconda3\\envs\\DLC-CPU\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\pc\\anaconda3\\envs\\DLC-CPU\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\pc\\anaconda3\\envs\\DLC-CPU\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\pc\\anaconda3\\envs\\DLC-CPU\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\pc\\anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\pc\\anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\n\r\n**Any other info / logs**\r\nThis is what happens when I try to import DeepLabCut.\r\n\r\nimport deeplabcut\r\nTraceback (most recent call last):\r\n\r\n  File \"C:\\Users\\pc\\anaconda3\\envs\\DLC-CPU\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n\r\n  File \"C:\\Users\\pc\\anaconda3\\envs\\DLC-CPU\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n\r\n  File \"C:\\Users\\pc\\anaconda3\\envs\\DLC-CPU\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n\r\n  File \"C:\\Users\\pc\\anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n\r\n  File \"C:\\Users\\pc\\anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\n\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-19-cfa4f159dfc5>\", line 1, in <module>\r\n    import deeplabcut\r\n\r\n  File \"C:\\Users\\pc\\anaconda3\\envs\\DLC-CPU\\Lib\\site-packages\\deeplabcut\\__init__.py\", line 14, in <module>\r\n    import tensorflow as tf\r\n\r\n  File \"C:\\Users\\pc\\anaconda3\\envs\\DLC-CPU\\Lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n\r\n  File \"C:\\Users\\pc\\anaconda3\\envs\\DLC-CPU\\Lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n\r\n  File \"C:\\Users\\pc\\anaconda3\\envs\\DLC-CPU\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\pc\\anaconda3\\envs\\DLC-CPU\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\pc\\anaconda3\\envs\\DLC-CPU\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\pc\\anaconda3\\envs\\DLC-CPU\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\pc\\anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\pc\\anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n   * For TF-GPU - See point 1\n   * For TF-CPU - See point 2\n-----------------------------------------------------------------------------------------------\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\nMake sure you are using compatible TF and CUDA versions. Please refer following TF version and CUDA version compatibility table.\n| TF  | CUDA |\n| :-------------: | :-------------: |\n| 2.1.0 - 2.2.0  | 10.1 |\n| 1.13.1 - 2.0  | 10.0  |\n| 1.5.0 - 1.12.0 | 9.0 |\n\n  * If you have above configuration and using _**Windows**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n    * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n  * If you have above configuration and using _**Ubuntu/Linux**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n    * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n  * If error still persists then, apparently your CPU model does not support AVX instruction sets.\n    * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n   * Try Google Colab to use TensorFlow.\n      * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed latest stable TF version. Also you can use ```pip install```  to install any other preferred TF version.\n      * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n      * All you need is a good internet connection and you are all set.\n   * Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*\n", "> Some people have apparently solved this problem by creating a separate environment, but I am new to Python and have no idea what that means or how to do it.\r\n\r\n@HessianHunter,\r\nPlease take a look at [this guide](https://www.tensorflow.org/install/pip#2.-create-a-virtual-environment-recommended) to install TensorFlow in a new virtual environment and check if it helps. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47979\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47979\">No</a>\n"]}, {"number": 47978, "title": "How to Build TensorFlowLiteGPUExperimental for iOS x86 + others", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Big Sur\r\n- TensorFlow version: 2.5.0\r\n- Python version:  3.8.3\r\n- Bazel version (if compiling from source):  3.7.2\r\n- GCC/Compiler version (if compiling from source): 12\r\n\r\n\r\n**Describe the problem**\r\nThere is a great pod called 'TensorFlowLiteGpuExperimental' that has C++ support for GPU delegates on iOS devices. Until now I've been using it as a pod and using the C++ API. It's been working great.\r\n\r\nThe contents of the Pod is a .framework file. Contents of the framework file are attached as screenshots at the end of this post. Most importantly it seems to include the flatbuffers and tensorflow header files, as well as libmetal_delegate.a\r\n\r\nI want to build this framework locally so I can add x86 support so I can use this framework in an iOS simulator. Currently it runs great on iOS devices but is lacking x86 support.\r\n\r\nI think the solution is writing some custom build instructions for Bazel, but after many many attempts I'm still pretty lost.\r\n\r\nI think the solution lies somewhere in this BUILD.apple file\r\nhttps://github.com/tensorflow/tensorflow/blob/b04e4ecfea73a460ecaf80997dc1ca4a01ceeccd/tensorflow/lite/ios/BUILD.apple#L102\r\n\r\nMy current understanding is that I need to come up with my own build section here and then run that? Is that right?\r\n\r\nMy own version of this somehow? `bazel build -c opt --config=ios_fat //tensorflow/lite/ios:TensorFlowLiteC_framework`\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n1. I've tried  these to see if it would work out of the box, but unfortunately it's not working for C++ support. \r\nhttps://github.com/tensorflow/tensorflow/blob/b04e4ecfea73a460ecaf80997dc1ca4a01ceeccd/tensorflow/lite/ios/BUILD.apple\r\n`bazel build -c opt --config=ios_fat //tensorflow/lite/ios:TensorFlowLiteC_framework\r\n`and then separately also doing\r\n`bazel build -c opt --config=ios_fat //tensorflow/lite/ios:TensorFlowLiteCMetal_framework\r\n`\r\n\r\nThe good news is I see this header that I use `metal_delegate.h`, but I think I want to expose all the headers like the TensorFlowLiteGPUExperimental does (see screenshots at end).\r\n\r\nIn my Objective C code on the device itself, I include the headers as follows:\r\n```\r\n#include <tensorflow_lite_gpu/tensorflow/lite/interpreter.h>\r\n#include <tensorflow_lite_gpu/tensorflow/lite/kernels/register.h>\r\n#include <tensorflow_lite_gpu/tensorflow/lite/model.h>\r\n#include <tensorflow_lite_gpu/tensorflow/lite/op_resolver.h>\r\n#include <tensorflow_lite_gpu/tensorflow/lite/string_util.h>\r\n#include <tensorflow_lite_gpu/tensorflow/lite/delegates/gpu/metal_delegate.h>\r\n```\r\n\r\nedit:\r\n@thaink \r\nHere are the contents of the original TensorFlowLite GPU pod that I am attempting to replicate by building\r\n![image](https://user-images.githubusercontent.com/2061485/112009196-8e604e80-8afc-11eb-9896-a5e9b5dfc503.png)\r\n![image](https://user-images.githubusercontent.com/2061485/112009215-928c6c00-8afc-11eb-8abf-6a2c8e4589f6.png)\r\n", "comments": ["Here's my attempt using glob to just include all header files, but then I get no headers at all in the .framework file that outputs\r\n\r\n```\r\n# bazel build -c opt --config=ios_fat //tensorflow/lite/ios:TensorFlowLiteGPU_framework\r\ntflite_ios_framework(\r\n    name = \"TensorFlowLiteGPU_framework\",\r\n    hdrs = glob([\r\n        \"tensorflow/lite/*.h\",\r\n        \":c_api.h\",\r\n        \":common.h\",\r\n        \":xnnpack_delegate.h\",\r\n        \":metal_delegate.h\",\r\n    ]),\r\n    allowlist_symbols_file = \":allowlist_TensorFlowLiteGPU.txt\",\r\n    bundle_name = \"TensorFlowLiteGPU\",\r\n    minimum_os_version = TFL_MINIMUM_OS_VERSION,\r\n    deps = [\r\n        \":tensorflow_lite_c\",\r\n        \"//tensorflow/lite/delegates/gpu:metal_delegate\",\r\n    \r\n    ],\r\n)\r\n```", "Hi Thai,\r\n\r\nDo you mind have a look at this iOS related issue?\r\n\r\nThanks,\r\nTiezhen", "I think it is equivalent to  \"TensorFlowLiteCMetal_framework\". @airman00 Can you try that one?", "Hi @thaink , thanks for the reply\r\n\r\nI tried that but it's missing all the headers that I need\r\n\r\nHere are the headers in the Headers folder from the TensorFlowLiteCMetal_Framework:\r\n![image](https://user-images.githubusercontent.com/2061485/113578377-b7024100-95f0-11eb-9ce6-73e7174a5a6a.png)\r\n\r\nYou can see all the headers that I'd like to have that are included in the gpu_experimental Pod in my original post.\r\n\r\nAny ideas on what I should try next?\r\n", "Those headers are for C++. Would you like to use C++ for iOS?\r\nCurrently, we only export C API for iOS since it can be imported in swift directly.", "As above reason, TensorFlowLiteGPUExperimental was deprecated long ago.\r\n\r\nOur current recommendation as as in https://www.tensorflow.org/lite/performance/gpu#ios_with_xcode", "I solved my problem in a different way.\r\n\r\nI just disabled it from building in the first place on Simulator.\r\n\r\nFor reference:\r\nIn your Target's Build Settings delete the entire entry VALID_ARCHS (if it exists) in the User Defined section at the bottom\r\n![image](https://user-images.githubusercontent.com/2061485/114323902-5fcf0580-9af5-11eb-8d36-ea262115f4c6.png)\r\n\r\nThat might just work. If it doesn't, do this step:\r\na. Add this in EXCLUDE ARCHITECTURES in your Project level Build Settings\r\n![image](https://user-images.githubusercontent.com/2061485/114323792-efc07f80-9af4-11eb-84aa-1276fdf8c5a0.png)\r\n\r\na. If you use Pods, you might need to also do step a for the Pod Project level Build Settings\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47978\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47978\">No</a>\n", "Nice to see you solved it."]}, {"number": 47977, "title": "Will Lamba.compute_output_shape change \"inbound_nodes\" of Lambda layer?", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):2.4.1\r\n- Python version:3.8.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:11.1\r\n- GPU model and memory:Titan V  12G\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nWhen I run x.compute_output_shape((5, 5, 5)),the inbound_nodes will become two.(x is a keras.layers.lambda)\r\n\r\n\r\n**Describe the expected behavior**\r\ncompute_output_shape  should not change  inbound_layers\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nhttps://colab.research.google.com/drive/1p1YhDb3TDXXVuf6tRFQsTIwyNLeCW8rB?usp=sharing\r\n\r\n\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["I am able to replicate the issue on tf 2.4 and nightly, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/6d4c9d10a418f1aa8394e818b9836c38/untitled580.ipynb).", "I see that this worked correctly in TF 2.3 version. See [gist](https://colab.research.google.com/gist/ymodak/ad3f00e9109c80586c85eda364de8783/untitled580.ipynb) for more information.\r\n(For given example; model.summary() with TF 2.4 prints `output shape` as `multiple` for Lambda layer)", "Was able to replicate the issue in TF 2.6.0-dev20210601,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/1d863266c57f1b495d7c47cab9dec21a/untitled175.ipynb)..Thanks !", "Thanks for opening this issue. Development of keras moved to separate repository https://github.com/keras-team/keras/issues\r\n\r\nPlease post this issue on keras-team/keras repo.\r\nTo know more see;\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999\r\nThank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47977\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47977\">No</a>\n"]}, {"number": 47976, "title": "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000002051844F0D0> and will run it as-is. Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: 'arguments' object has no attribute 'posonlyargs' To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert", "body": "I am trying to train my model and then get the Mean Attribute Error between predicted and true values.\r\nBut the predicted value column is empty due to this error.", "comments": ["@shuchitajain,\r\nIn order to reproduce the issue reported here, could you please provide the TensorFlow version, the complete code and the dataset you are using. Thanks!", "Also, you can suppress the warning messages by changing the log level at the start of the program \r\n```\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \r\nimport tensorflow as tf\r\n```\r\n\r\nor you can set the [autograph verbosity](https://www.tensorflow.org/api_docs/python/tf/autograph/set_verbosity\r\n) level using\r\n\r\n```\r\nimport os\r\nimport tensorflow as tf\r\nos.environ['AUTOGRAPH_VERBOSITY'] = 1\r\n```\r\nThanks!", "> or you can set the [autograph verbosity](https://www.tensorflow.org/api_docs/python/tf/autograph/set_verbosity) level using\r\n> \r\n> ```\r\n> import os\r\n> import tensorflow as tf\r\n> os.environ['AUTOGRAPH_VERBOSITY'] = 1\r\n> ```\r\n> \r\n> Thanks!\r\n  \r\n\r\n\r\nHey, actually because of this, my crowd counting model is not developing predicted values for the images. So i don't just wanna suppress the warning, and rather fix it.\r\nI am using tensorflow as backend.\r\n", "Also, I am using Tensorflow 2.4.1.\r\nThe repository that I am working on:\r\nhttps://github.com/Neerajj9/CSRNet-keras/issues\r\n\r\nThe dataset is given in the README", "@shuchitajain,\r\nThe GitHub repository you've linked contains multiple `.ipynb` files. \r\n\r\nIn order to expedite the trouble-shooting process, could you please provide a minimal code snippet? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47976\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47976\">No</a>\n"]}, {"number": 47975, "title": "No visible @interface for 'TFLInterpreter' declares the selector 'copyData:toInputTensorAtIndex:error:'", "body": "Hello, I have a little problem with the tensorflow documentation.\r\n\r\nI would like to use tensorflow lite with Objective C by following this [documentation](https://www.tensorflow.org/lite/guide/inference)\r\n\r\n## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/lite/guide/inference\r\n\r\n## Description of issue (what needs changing):\r\n\r\nApparently this function is not declared:\r\n\r\n`[interpreter copyData:inputData toInputTensorAtIndex:0 error:&error];`\r\n\r\nI got this following error:\r\n\r\n`No visible @interface for 'TFLInterpreter' declares the selector 'copyData:toInputTensorAtIndex:error:'`\r\n\r\nAm I missing something, how can I fill a tensor with objective C ?\r\n", "comments": ["@yyoon could you take a look?", "I think I have it.\r\n\r\nWe need to apply copyData on a TFLTensor object, not the TFLInterpreter like this:\r\n\r\n```\r\nNSMutableData *inputData;  // Should be initialized\r\nTFLTensor *inputTensor = [interpreter inputTensorAtIndex:0 error:&error];\r\n\r\n//Feed data\r\n[inputTensor copyData:inputData error:&error]\r\n```\r\n\r\nAm I correct ?", "@Fedour Thanks for reporting! You are correct on that.\r\nWill update the inference guide doc."]}, {"number": 47974, "title": "Add GPU implem of sparse segment reduction ops", "body": "Adds GPU implementations of the ops: `SparseSegment[Sum|Mean|SqrtN][WithNumSegments]`.\r\n\r\nThis GPU kernel will also be used to implement the corresponding non-sparse and gradient ops in future PRs.\r\n\r\ncc @nluehr @duncanriach ", "comments": ["Btw I haven't been able to work out or reproduce the Windows CI build failure:\r\n\r\n```T:\\tmp\\nvcc_inter_files_tmp_dir\\tmp9pyh0ytr\\segment_reduction_ops_gpu.cu.compute_80.cudafe1.stub.c(5946): error C2719: 'unnamed-parameter': formal parameter with requested alignment of 128 won't be aligned```\r\n\r\nIf you're able to send me the temporary file `segment_reduction_ops_gpu.cu.compute_80.cudafe1.stub.c` (e.g., using `nvcc --keep`) I could try to follow up on it, but otherwise I'm not sure what to do about it.", "Ben,\r\n\r\n> Btw I haven't been able to work out or reproduce the Windows CI build failure:\r\n> \r\n> `T:\\tmp\\nvcc_inter_files_tmp_dir\\tmp9pyh0ytr\\segment_reduction_ops_gpu.cu.compute_80.cudafe1.stub.c(5946): error C2719: 'unnamed-parameter': formal parameter with requested alignment of 128 won't be aligned`\r\n> \r\n> If you're able to send me the temporary file `segment_reduction_ops_gpu.cu.compute_80.cudafe1.stub.c` (e.g., using `nvcc --keep`) I could try to follow up on it, but otherwise I'm not sure what to do about it.\r\n\r\nSorry for not noticing this earlier, this is still breaking Windows and we have to roll back to make the build green.\r\n\r\nCan you update the PR after `#ifdef`ing the offending code on Windows (and thus making the kernel not available on Windows)?  I don't see a better way for now, unless you're willing to reproduce this issue on a Windows VM."]}, {"number": 47972, "title": "Rename PY_ARRAY_UNIQUE_SYMBOL to make it unique", "body": "This makes the symbol unique across all tensorflow parts, thus allowing to link tensoflow and tensorflow/lite into single binary.", "comments": ["Could you elaborate why this change is needed? This question is just for understanding the circumstances around this change.", "Unfortunately, I can not disclosure the whole story, but the basic idea is to link two python modules into single binary file.\r\nAfter this, two PY_ARRAY_UNIQUE_SYMBOLS become not so unique and this make interpreter to segfault."]}, {"number": 47971, "title": "Training becomes increasingly slower when creating models in a loop", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): BINARY\r\n- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1\r\n- Python version: 3.7.6\r\n- CUDA/cuDNN version: NO GPU\r\n- GPU model and memory: NO GPU\r\n\r\n**Describe the current behavior**\r\nWhen creating and training a model multiple times in a loop, the training process gets increasingly slower with each model.\r\n\r\n**Describe the expected behavior**\r\nI expected the training times to be roughly the same.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport time\r\n\r\nn_samples = 300000\r\nn_features = 100\r\nn_targets = 5\r\nbatch_size = 100\r\nn_inducing_points = 100\r\nx = np.array(range(n_samples * n_features), dtype=np.float64).reshape((n_samples, n_features))\r\ny = np.array(range(n_samples * n_targets), dtype=np.float64).reshape((n_samples, n_targets))\r\nfor t_idx in range(10):\r\n    tf.keras.backend.clear_session()\r\n    dataset = [x, y]\r\n    dataset = tf.data.Dataset.from_tensor_slices(tuple(dataset)).shuffle(n_samples).repeat().batch(batch_size=batch_size).prefetch(tf.data.experimental.AUTOTUNE)\r\n    data_iterator = iter(dataset)\r\n\r\n    inputs = tf.keras.Input(shape=(n_features,), name='input')\r\n    outputs = tf.keras.layers.Dense(n_features, name='dense_1', activation=tf.keras.activations.relu)(inputs)\r\n    outputs = tf.keras.layers.Dense(n_features, name='dense_2', activation=tf.keras.activations.relu)(outputs)\r\n    outputs = tf.keras.layers.Dense(n_features, name='dense_3', activation=tf.keras.activations.relu)(outputs)\r\n    outputs = tf.keras.layers.Dense(n_features, name='dense_4', activation=tf.keras.activations.relu)(outputs)\r\n    outputs = tf.keras.layers.Dense(n_targets, name='output', activation=tf.keras.activations.linear)(outputs)\r\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\r\n\r\n    trainable_variables = list(model.trainable_variables)\r\n\r\n    adam_opt = tf.optimizers.Adam(learning_rate=0.001)\r\n\r\n\r\n    @tf.function\r\n    def loss(batch):\r\n        x_, y_ = batch\r\n        y_pred_ = model(x_)\r\n        return tf.keras.losses.MSE(y_pred_, y_)\r\n\r\n\r\n    @tf.function\r\n    def optimization_step():\r\n        batch = next(data_iterator)\r\n        def f(): return loss(batch)\r\n        adam_opt.minimize(f, var_list=trainable_variables)\r\n\r\n    iterations = 50000\r\n    loop_start = time.time()\r\n    optimization_times = []\r\n    for idx in range(iterations):\r\n        optimization_step()\r\n\r\n    loop_end = time.time()\r\n    print(f'Elapsed: {loop_end - loop_start}')\r\n```\r\n\r\nUsing `model.fit()` is not an option, since this code was obtained by stripping down a much more complicated code, where the call to `model.fit()` is not possible.\r\n\r\n**Other info / logs**\r\nOutput obtained with the script above:\r\n```\r\nElapsed: 49.798316955566406\r\nElapsed: 55.18571472167969\r\nElapsed: 58.57510209083557\r\nElapsed: 64.41855955123901\r\nElapsed: 66.76858448982239\r\nElapsed: 68.3305652141571\r\nElapsed: 67.73438382148743\r\nElapsed: 69.73751258850098\r\nElapsed: 73.59102845191956\r\nElapsed: 73.14124798774719\r\n```\r\nAs you can see, training times have a clear upward trend. Moreover, this does not happen on my Linux machine (same python version, same tensorflow version, on a clean virtualenv; Ubuntu 18.04).", "comments": ["@mattia-,\r\nCould you please restart your machine, close all the programs running in the background before running the script and let us know if you are still facing the same issue? Thanks!", "@amahendrakar,\r\nThanks for taking the time to reply!\r\nI can confirm the issue still stands after a clean reboot. Can you reproduce the same behaviour on your machine or is it just me? ", "@rmothukuru,\r\nI did not face any issues while running the code on a **Linux machine** with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.sandbox.google.com/gist/amahendrakar/c3fd3719669e2ac558c3432205aab8df/47971.ipynb). Thanks!", "@rmothukuru,\r\nJust to confirm that this looks like a Windows-only problem. I did not face any issues running my script on Linux either (and neither on Mac or on WSL, for that matter). Thanks!", "@mattia- I ran your code on Win10 with `tf-nightly` and I don't see the issue of slowing down. Please check the output below\r\n\r\n```\r\nElapsed: 51.30133318901062\r\nElapsed: 57.163376331329346\r\nElapsed: 57.61692833900452\r\nElapsed: 55.73898267745972\r\nElapsed: 58.43379831314087\r\nElapsed: 56.97764277458191\r\nElapsed: 57.768574237823486\r\nElapsed: 57.758583784103394\r\nElapsed: 58.231318950653076\r\nElapsed: 58.51157069206238\r\n\r\nProcess finished with exit code 0\r\n```\r\nCan you please verify with `tf-nightly` and let us know what you see. Thanks!\r\n", "Hi @jvishnuvardhan \r\n\r\nI tried to test what you suggested, but I'm having some problems with the use of `tf-nightly`:\r\n```\r\nTraceback (most recent call last):\r\nFile \".\\tmp.py\", line 16, in <module>\r\ndataset = tf.data.Dataset.from_tensor_slices(tuple(dataset)).shuffle(n_samples).repeat().batch(batch_size=batch_size).prefetch(tf.data.experimental.AUTOTUNE)\r\nFile \"C:\\Users\\XXX\\tmp\\venv_tf_nightly\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 685, in from_tensor_slices\r\nreturn TensorSliceDataset(tensors)\r\nFile \"C:\\Users\\XXX\\tmp\\venv_tf_nightly\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 3844, in __init__\r\nelement = structure.normalize_element(element)\r\nFile \"C:\\Users\\XXX\\tmp\\venv_tf_nightly\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py\", line 129, in normalize_element\r\nops.convert_to_tensor(t, name=\"component_%d\" % i, dtype=dtype))\r\nFile \"C:\\Users\\XXX\\tmp\\venv_tf_nightly\\lib\\site-packages\\tensorflow\\python\\profiler\\trace.py\", line 163, in wrapped\r\nreturn func(*args, **kwargs)\r\nFile \"C:\\Users\\XXX\\tmp\\venv_tf_nightly\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1566, in convert_to_tensor\r\nret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\nFile \"C:\\Users\\XXX\\tmp\\venv_tf_nightly\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion_registry.py\", line 52, in _default_conversion_function\r\nreturn constant_op.constant(value, dtype, name=name)\r\nFile \"C:\\Users\\XXX\\tmp\\venv_tf_nightly\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 272, in constant\r\nallow_broadcast=True)\r\nFile \"C:\\Users\\XXX\\tmp\\venv_tf_nightly\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 283, in _constant_impl\r\nreturn _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\r\nFile \"C:\\Users\\XXX\\tmp\\venv_tf_nightly\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 308, in _constant_eager_impl\r\nt = convert_to_eager_tensor(value, ctx, dtype)\r\nFile \"C:\\Users\\XXX\\tmp\\venv_tf_nightly\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 105, in convert_to_eager_tensor\r\nctx.ensure_initialized()\r\nFile \"C:\\Users\\XXX\\tmp\\venv_tf_nightly\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 536, in ensure_initialized\r\nconfig_str = self.config.SerializeToString()\r\nFile \"C:\\Users\\XXX\\tmp\\venv_tf_nightly\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 964, in config\r\nself._initialize_physical_devices()\r\nFile \"C:\\Users\\XXX\\tmp\\venv_tf_nightly\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 1293, in _initialize_physical_devices\r\ndevs = pywrap_tfe.TF_ListPhysicalDevices()\r\ntensorflow.python.framework.errors_impl.InternalError: Cannot dlopen all CUDA libraries.\r\n```\r\n\r\n`tf-nightly` was installed with\r\n```\r\npip install tf-nightly\r\n```\r\n\r\nAny suggestions? Thanks!", "@mattia- which version was installed? can you do `print(tf.__version__)` after `import tensorflow as tf`. \r\nAnother question. Are you trying to use TF with GPU? \r\nthanks!", "@jvishnuvardhan \r\n\r\n> which version was installed? can you do `print(tf.__version__)` after `import tensorflow as tf`\r\n```\r\n>>> import tensorflow as tf\r\n>>> print(tf.__version__)\r\n2.7.0-dev20210630 \r\n```\r\n\r\n\r\n\r\n> Another question. Are you trying to use TF with GPU?\r\n\r\nNope, I'm using (or, at least, trying to use) the plain CPU version.\r\n\r\nThanks!", "@mattia- Do you have GPU in your system? can you try reinstalling TF as `!pip install tf-nightly-cpu`. If you just use `!pip install tf-nightly`, it looks for GPU. If you have GPU, then automatically it installs GPU enabled TF.\r\n\r\nAs far as I know, this is not an issue with TF code but something with the system. So, may be unistall and reinstall TF-cpu and test again. You could also check it in google colab. Thanks", "@jvishnuvardhan \r\n\r\nI managed to install it and test it. Here are the results on my machine:\r\n```\r\n>>> tf.__version__\r\n'2.7.0-dev20210704'\r\n```\r\n```\r\nElapsed: 49.9630708694458\r\nElapsed: 53.02399563789368\r\nElapsed: 53.265995502471924\r\nElapsed: 53.51599383354187\r\nElapsed: 54.79951238632202\r\nElapsed: 55.34299564361572\r\nElapsed: 56.96799683570862\r\nElapsed: 57.81099009513855\r\nElapsed: 59.2099826335907\r\nElapsed: 59.92298483848572\r\n```\r\n\r\nIt looks better than before, but to be honest I can still see the upward trend in the training times.\r\nThanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "My recent runtime (without any other programs running) show that there is a small increase but that is expected. Please check the runtime below\r\n\r\n```\r\nElapsed: 54.97995686531067\r\nElapsed: 52.393901348114014\r\nElapsed: 53.56126165390015\r\nElapsed: 53.747889280319214\r\nElapsed: 54.081284284591675\r\nElapsed: 55.353312253952026\r\nElapsed: 56.64463949203491\r\nElapsed: 55.35281229019165\r\nElapsed: 55.846272230148315\r\nElapsed: 56.97290635108948\r\n```"]}, {"number": 47970, "title": "tf.math.floordiv (//) is broken when inside @tf.function if second argument is 1.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: /\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): /\r\n- GCC/Compiler version (if compiling from source): /\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: Titan Xp\r\n\r\n**Describe the current behavior**\r\n`tf.math.floordiv` produces different values if compiled with `experimental_compile=True` vs not\r\n\r\n**Describe the expected behavior**\r\n`tf.math.floordiv` should return the same values regardless of compilation method.\r\n\r\n**Standalone code to reproduce the issue**\r\n[colab](https://colab.research.google.com/drive/1RgVH3RaLmfcjdzKA5pDtpWhK-23tHmrX#scrollTo=2Iw5kpILGneG)\r\n```python\r\nimport tensorflow as tf\r\n\r\ndef floordiv(x, y):\r\n    # x // y\r\n    return tf.math.floordiv(x, y)\r\n\r\n@tf.function\r\ndef floordiv_tffn(x, y):\r\n    # x // y\r\n    return tf.math.floordiv(x, y)\r\n\r\n@tf.function(experimental_compile=True)\r\ndef floordiv_compiled(x, y):\r\n    # x // y\r\n    return tf.math.floordiv(x, y)\r\n\r\nx, y = tf.constant([0., 0.1, 0.9]), 1.\r\nprint(floordiv(x, y))\r\nprint(floordiv_tffn(x, y))\r\nprint(floordiv_compiled(x, y))\r\n```\r\n```\r\ntf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)\r\ntf.Tensor([0.  0.1 0.9], shape=(3,), dtype=float32)\r\ntf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)\r\n```", "comments": ["I am able to replicate the issue reported on tf 2.3,2.4 and nightly, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/e701dd589462a69a246c57135c1e6920/untitled567.ipynb).", "Opened #47986 with a fix.\r\n\r\nAs a temporary workaround you can use `tf.config.optimizer.set_experimental_options({\"constant_folding\": False})` although that could slightly reduce performance in some cases.", "It works for me if I do `experimental_compile=True` (since I anyways want to do that for performance). I'm wondering though, doesn't experimental compile constant fold too?", "> I'm wondering though, doesn't experimental compile constant fold too?\r\n\r\nexperimental compile does constant folding too, but uses a different code path as far as I know which doesn't have this bug.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47970\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47970\">No</a>\n"]}, {"number": 47969, "title": "[Graph C API] Updating warning messages", "body": "This is a following PR of https://github.com/tensorflow/tensorflow/pull/47635 to print warnings more accurately, which can improve user experience:\r\n1. In some scenarios, warning messages is not printed since config's default value is not considered. It is fixed in this MR.\r\n2. Instead of printing all configs, printing conflicted configs only.\r\n\r\nAn example of warnings:\r\n~~~cpp\r\n2021-03-22 13:08:42.718436: W tensorflow/core/grappler/optimizers/meta_optimizer.cc:609] User's config has been changed based on plugin's config.\r\n2021-03-22 13:08:42.718446: W tensorflow/core/grappler/optimizers/meta_optimizer.cc:610] \r\nConfig of optimizers\t\tUser's config\tPlugin's config\tFinal config(User & Plugin)\r\nlayout_optimizer                1\t\t0\t\t0\r\nremapping                       1\t\t0\t\t0\r\n~~~", "comments": ["@penpornk @ezhulenev Sorry for the inconvenience that I missed some cases in the 3rd PR of Graph C API. This PR only changes warning messages, and does not change the basic logic of plugin optimizer. Please have a review. Thanks.", "@penpornk @ezhulenev Could you please have a review on this PR? I think there is no big change, only some updates on warning messages.", "This PR broke a test and was reverted earlier today (https://github.com/tensorflow/tensorflow/commit/3c462a926fb6875909c64720f67663f4b24a7b7a) and I haven't had a chance to bring it back. So, unfortunately, it will miss the 2.5 branch cut. Sorry about that. :(", "> This PR broke a test and was reverted earlier today ([3c462a9](https://github.com/tensorflow/tensorflow/commit/3c462a926fb6875909c64720f67663f4b24a7b7a)) and I haven't had a chance to bring it back. So, unfortunately, it will miss the 2.5 branch cut. Sorry about that. :(\r\n\r\nIt's fine. This is not a critical PR, and plugin optimizer still works without it. Please let me know if there is needed form my side.  :)"]}, {"number": 47968, "title": "sample_weights not passed to loss from dataset", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Google Colab\r\n- TensorFlow version (use command below): v2.4.1-0-g85c8b2a817f 2.4.1\r\n- Python version: 3\r\n- Bazel version (if compiling from source): No\r\n- GCC/Compiler version (if compiling from source): No\r\n- CUDA/cuDNN version: No\r\n- GPU model and memory: No\r\n\r\n**Describe the current behavior**\r\nI use tf.data.Dataset to feed data into model. Besides the features and labels I need to use weights for each sample (image pixel). I've tried to pass that weighs according to https://www.tensorflow.org/api_docs/python/tf/keras/Model?hl=en#fit but it didn't work.\r\n\r\n**Describe the expected behavior**\r\nThird value returned from Dataset should be passed to loss `sample_weights` argument.\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/153csG3Qv5GiXduEERHkG1Np3pMM958dX?usp=sharing", "comments": ["I am able to replicate the issue reported on tf 2.3,2.4 and nightly, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/1f96bd12950032edcb8c9a160cc25581/untitled571.ipynb).", "It seems i found an answer.\r\nWhen passing a function as loss it wrapped with `LossFunctionWrapper` inside `LossesContainer`.  LossFunctionWrapper intercepts `sample_weights` in __call__ method and applies them on the result of original loss function call.\r\n\r\nThe final loss therefore should be computed correctly while original loss function returns result with a shape broadcastable to sample_weights.\r\n\r\nIssue may be closed.\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47968\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47968\">No</a>\n"]}, {"number": 47967, "title": "Fix typo in comment", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47967) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it! ", "LGTM"]}]