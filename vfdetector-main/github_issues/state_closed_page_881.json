[{"number": 27059, "title": "~\\Anaconda3\\lib\\imp.py......ImportError: DLL load failed: The specified module could not be found - Win10 Pro, Core I3 with AMD Radeon HD7500M Accelerator!", "body": "Hi All,\r\n\r\nFacing error -  _ImportError: DLL load failed: The specified module could not be found!_\r\n\r\nCan anyone provide a 'working' env info for **AMD Radeon HD7500M** GPU alongside the following:\r\n\r\nWin10 Pro, FU 1803\r\nCore I3 3rd Gen (2370M), 4GB\r\nCUDA ver.10.0.130\r\nCUDNN ver 7.3.1 for 10.0_0\r\nAnaconda 2018.12\r\nPy 3.7.1\r\nTensorflow-gpu 1.13.1 - Installed using 'Conda' in an 'Anaconda prompt'!\r\n\r\nThanks!", "comments": ["@hemantjnypc `tensorflow-gpu` only works with NVidia cards. You need `tensorflow-rocm` to use your GPU. Also, your GPU might be too old even for that. Please, see https://rocm.github.io/hardware.html\r\n\r\nYou might have to resort to TensorFlow CPU-only, e.g. `tensorflow-1.13.1`", "Thanks! Shall check!", "Closing this issue since above explanation is correct. Feel free to reopen if facing difficulties any further difficulties. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27059\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27059\">No</a>\n"]}, {"number": 27058, "title": "Is the task type \"master\" for distributed training deprecated?", "body": "Should task type \"master\" be immediately replaced with \"chief\"?\r\nFor example, `CollectiveAllReduceStrategy` only use \"chief\" (not \"master\") and \"worker\":\r\nhttps://github.com/tensorflow/tensorflow/blob/36f817a9f3e7d2339cb53b91ddc508b3e25ab761/tensorflow/python/distribute/multi_worker_util.py#L159-L161\r\n\r\nHowever, in some environment such as Google Cloud ML Engine, environment variable `TF_CONFIG` with \"master\" (not \"chief\"), \"worker\", and \"ps\".\r\n\r\nDo you have any plan to keep compatibility?\r\n", "comments": ["Hi, we don't have plans to support \"master\" job right now. GCP also doesn't have a strong reason to use \"master\". So for now, we'll stick to \"chief\".", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27058\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27058\">No</a>\n", "@yuefengz, this results in a bad experience with AI Platform, since `MultiWorkerMirroredStrategy` gets confused about `master`. I would love to see this documented somewhere."]}, {"number": 27057, "title": "Comment added in keras/layers/core.py", "body": "Comment added for clarification on the question \"why do we use **tensordot** to compute dot product here?\".\r\n#25780", "comments": ["I have added comment for better understanding of the other person, what is your point of view? @pavithrasv "]}, {"number": 27056, "title": "ImportError: cannot import name trt_engine_op when using `tensorrt.create_inference_graph`", "body": "hello, I use `tensorflow-gpu-2.0.0a0`. Here I want to use `tensorrt` to convert a tensorflow `saved_model` to a `output_saved_model` optimized by tensorrt. But when I run code:\r\n\r\n```\r\nimport os\r\nimport sys\r\nfrom tensorflow.python.compiler import tensorrt\r\ntensorrt.create_inference_graph(\r\n  None,\r\n  None,\r\n  max_batch_size=1,\r\n  max_workspace_size_bytes=1<<32,\r\n  precision_mode='FP32',\r\n  minimum_segment_size=3,\r\n  is_dynamic_op=True,\r\n  input_saved_model_dir='/tmp/imagenet_model/resnet/saved_model/1553141893',\r\n  input_saved_model_tags=['serve'],\r\n  output_saved_model_dir='/tmp/imagenet_model/resnet/trt_saved_model/1553141893')\r\n```\r\n\r\nit shows error:\r\n```\r\n**** Failed to import TF-TRT ops. This is because the binary was not built with CUDA or TensorRT enabled. ****\r\nTraceback (most recent call last):\r\n  File \"convert.py\", line 20, in <module>\r\n    output_saved_model_dir='/tmp/imagenet_model/resnet/trt_saved_model/1553141893')\r\n  File \"/xxx/.virtualenvs/tf/lib/python2.7/site-packages/tensorflow/python/compiler/tensorrt/trt_convert.py\", line 880, in create_inference_graph\r\n    use_calibration=use_calibration)\r\n  File \"/xxx/.virtualenvs/tf/lib/python2.7/site-packages/tensorflow/python/compiler/tensorrt/trt_convert.py\", line 637, in __init__\r\n    trt_ops.load_trt_ops()\r\n  File \"/xxx/.virtualenvs/tf/lib/python2.7/site-packages/tensorflow/compiler/tf2tensorrt/python/ops/trt_ops.py\", line 49, in load_trt_ops\r\n    raise e\r\nImportError: cannot import name trt_engine_op\r\n```\r\n\r\nhow to fix it? anyone can give some advises?", "comments": ["As the error says, ```the binary was not built with CUDA or TensorRT enabled```. Please take a look at [compilation steps](https://github.com/tensorflow/tensorrt#compilation) if haven't already."]}, {"number": 27055, "title": "Deeplab TypeError: MonitoredTrainingSession() got an unexpected keyword argument 'summary_dir'", "body": "  When i am running train.py file for pascal voc dataset then fallowing error occures\r\nfile \"train.py\", line 500, in <module>\r\n    tf.app.run()\r\n  File \"/home/suraj/anaconda2/envs/tensor/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"train.py\", line 492, in main\r\n    hooks=[stop_hook]) as sess:\r\nTypeError: MonitoredTrainingSession() got an unexpected keyword argument 'summary_dir'\r\n", "comments": ["What is your tf version? The keyword summary_dir is introduced in tf1.10 so if you version is below that you can solve the problem by upgrading tf version.", "@Telang8100 before raising any issue you should always mention the environ on which you are working that is :- \r\n1) Tensorflow version you are using\r\n2) Your operating system\r\n3) Piece of code causing issue", "Since this issue is related to [DeepLab](https://github.com/tensorflow/models/tree/master/research/deeplab#deeplab-deep-labelling-for-semantic-image-segmentation), It is better suited on ```tensorflow/models``` repo. Please post it on [tensorflow/models](https://github.com/tensorflow/models/issues/new). Thanks!", "\r\n> What is your tf version? The keyword summary_dir is introduced in tf1.10 so if you version is below that you can solve the problem by upgrading tf version.\r\n\r\nThanks..\r\nThe issue is solved by upgrading tensorflow_gpu to 1.10\r\n \r\n"]}, {"number": 27054, "title": "tensorflow 2.0 transfer_learning tutorial: tensorflow_datasets error on local jupyter notebook Anaconda Win10", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Anaconda on Windows 10\r\n- TensorFlow installed from (source or binary): install by pip in anaconda environment\r\n- TensorFlow version (use command below): tensorflow-gpu 2.0 alpha\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: Cuda toolkit 10.0; cuDNN 7.5\r\n- GPU model and memory: GTX 980 Ti 6Gb\r\n\r\n\r\n**Describe the current behavior**\r\nI run the [transfer_learning.ipynb](https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/images/transfer_learning.ipynb) tutorial on/r2 for Tensroflow 2.0 on Jupyter Notebook in Anaconda Windows 10 and I get the error on step:\r\n```\r\nSPLIT_WEIGHTS = (8, 1, 1)\r\nsplits = tfds.Split.TRAIN.subsplit(weighted=SPLIT_WEIGHTS)\r\n\r\n(raw_train, raw_validation, raw_test), metadata = tfds.load(\r\n    'cats_vs_dogs', split=list(splits),\r\n    with_info=True, as_supervised=True)\r\n```\r\n\r\n**Error**: \r\n```\r\n`Downloading / extracting dataset cats_vs_dogs (786.68 MiB) to C:\\Users\\Khoa\\tensorflow_datasets\\cats_vs_dogs\\2.0.0...\r\n\r\nDl Completed...: 0 url [00:00, ? url/s]\r\n\r\nDl Size...: 0 MiB [00:00, ? MiB/s]\r\n\r\n\r\n\r\n0 examples [00:00, ? examples/s]\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-7-2bc776459ab0> in <module>\r\n      4 (raw_train, raw_validation, raw_test), metadata = tfds.load(\r\n      5     'cats_vs_dogs', split=list(splits),\r\n----> 6     with_info=True, as_supervised=True)\r\n\r\nD:\\Anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow_datasets\\core\\api_utils.py in disallow_positional_args_dec(fn, instance, args, kwargs)\r\n     50     _check_no_positional(fn, args, ismethod, allowed=allowed)\r\n     51     _check_required(fn, kwargs)\r\n---> 52     return fn(*args, **kwargs)\r\n     53 \r\n     54   return disallow_positional_args_dec(wrapped)  # pylint: disable=no-value-for-parameter\r\n\r\nD:\\Anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow_datasets\\core\\registered.py in load(name, split, data_dir, batch_size, download, as_supervised, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs)\r\n    251   if download:\r\n    252     download_and_prepare_kwargs = download_and_prepare_kwargs or {}\r\n--> 253     dbuilder.download_and_prepare(**download_and_prepare_kwargs)\r\n    254 \r\n    255   if as_dataset_kwargs is None:\r\n\r\nD:\\Anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow_datasets\\core\\api_utils.py in disallow_positional_args_dec(fn, instance, args, kwargs)\r\n     50     _check_no_positional(fn, args, ismethod, allowed=allowed)\r\n     51     _check_required(fn, kwargs)\r\n---> 52     return fn(*args, **kwargs)\r\n     53 \r\n     54   return disallow_positional_args_dec(wrapped)  # pylint: disable=no-value-for-parameter\r\n\r\nD:\\Anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py in download_and_prepare(self, download_dir, download_config)\r\n    217         self._download_and_prepare(\r\n    218             dl_manager=dl_manager,\r\n--> 219             max_examples_per_split=download_config.max_examples_per_split)\r\n    220 \r\n    221         # NOTE: If modifying the lines below to put additional information in\r\n\r\nD:\\Anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py in _download_and_prepare(self, dl_manager, max_examples_per_split)\r\n    666       self._file_format_adapter.write_from_generator(\r\n    667           make_generator_fn(**split_generator.gen_kwargs),\r\n--> 668           output_files,\r\n    669       )\r\n    670 \r\n\r\nD:\\Anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow_datasets\\core\\file_format_adapter.py in write_from_generator(self, generator_fn, output_files)\r\n    105     wrapped = (\r\n    106         _dict_to_tf_example(d).SerializeToString() for d in generator_fn())\r\n--> 107     _write_tfrecords_from_generator(wrapped, output_files, shuffle=True)\r\n    108 \r\n    109   def dataset_from_filename(self, filename):\r\n\r\nD:\\Anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow_datasets\\core\\file_format_adapter.py in _write_tfrecords_from_generator(generator, output_files, shuffle)\r\n    270     with _close_on_exit(writers) as writers:\r\n    271       logging.info(\"Writing TFRecords\")\r\n--> 272       _round_robin_write(writers, generator)\r\n    273     # Shuffle each shard\r\n    274     if shuffle:\r\n\r\nD:\\Anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow_datasets\\core\\file_format_adapter.py in _round_robin_write(writers, generator)\r\n    283 def _round_robin_write(writers, generator):\r\n    284   \"\"\"Write records from generator round-robin across writers.\"\"\"\r\n--> 285   for i, example in enumerate(tqdm.tqdm(generator, unit=\" examples\")):\r\n    286     writers[i % len(writers)].write(example)\r\n    287 \r\n\r\nD:\\Anaconda\\envs\\tf2\\lib\\site-packages\\tqdm\\_tqdm.py in __iter__(self)\r\n   1020                 \"\"\"), fp_write=getattr(self.fp, 'write', sys.stderr.write))\r\n   1021 \r\n-> 1022             for obj in iterable:\r\n   1023                 yield obj\r\n   1024                 # Update and possibly print the progressbar.\r\n\r\nD:\\Anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow_datasets\\core\\file_format_adapter.py in <genexpr>(.0)\r\n    104   def write_from_generator(self, generator_fn, output_files):\r\n    105     wrapped = (\r\n--> 106         _dict_to_tf_example(d).SerializeToString() for d in generator_fn())\r\n    107     _write_tfrecords_from_generator(wrapped, output_files, shuffle=True)\r\n    108 \r\n\r\nD:\\Anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py in generator_fn()\r\n    636 \r\n    637       def generator_fn():\r\n--> 638         for i, ex in enumerate(self._generate_examples(**kwargs)):\r\n    639           # Use the DatasetInfo FeaturesDict to encode the example. This allows\r\n    640           # the user's function to simply yield raw examples from the source\r\n\r\nD:\\Anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow_datasets\\image\\cats_vs_dogs.py in _generate_examples(self, archive)\r\n    104     if num_skipped != _NUM_CORRUPT_IMAGES:\r\n    105       raise ValueError(\"Expected % corrupt images, but found %d\" % (\r\n--> 106           _NUM_CORRUPT_IMAGES, num_skipped))\r\n    107     logging.warning(\"%d images were corrupted and were skipped\", num_skipped)\r\n\r\nValueError: Expected \u06caorrupt images, but found 0`\r\n```\r\n\r\n\r\n**Describe the expected behavior**\r\nThe code run perfectly on my Jupyter Notebook - Anaconda server Ubuntu 16.04 and also on Colab.\r\nMy Jupyter notebook is 5.7.4 on both windows and ubuntu.\r\n\r\n", "comments": ["I think TensorFlow does not support Google Cloud Storage (GCS) on Windows because of its dependency on `curl` but the datasets download manager code does not seem to depend on any of that, https://github.com/tensorflow/datasets/blob/master/tensorflow_datasets/core/download/downloader.py#L146\r\n\r\nA workaround could be to set that `download` param in `tfds.load` to `False` and download the data manually before hand.", "Thank you @MostafaGazar .\r\nI copy the dataset from my ubuntu server to my win10 PC and add download = False to the function then it works.\r\n```\r\n(raw_train, raw_validation, raw_test), metadata = tfds.load(\r\n    'cats_vs_dogs', split=list(splits),\r\n    download=False,\r\n    with_info=True, as_supervised=True)\r\n```\r\nHope TF dataset team will fix this.\r\n\r\n", "This should be fixed with https://github.com/tensorflow/datasets/pull/403", "I have the same issue in win10 and insist to fix it immediately", "@shaolinkhoa Is this still an issue. It run without an issue on Google Colab. As https://github.com/tensorflow/datasets/pull/403 was already merged, it should work without any problem.\r\nClosing this out since I understand it to be resolved, but please let me know if I'm mistaken. Thanks!"]}, {"number": 27053, "title": "tf.Session and tf.ConfigProto do not work in the new TF2.0", "body": "tf.Session and tf.ConfigProto do not work in the new TF2.0; what other options do I have?\r\n\r\nwith tf.Session(config=tf.ConfigProto(log_device_placement=True)) as session:\r\n        result = session.run(sum_operation)\r\n        print(result)", "comments": ["I patched it with tf.compat.v1 & it still fails.  ", "`tf.ConfigProto` will be split and replaced into other functions ([ref](https://github.com/tensorflow/tensorflow/issues/25446#issuecomment-460516411)). If `tf.compat.v1.ConfigProto` is not working, however, that would be an issue.\r\n\r\n```python\r\nwith tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True)) as session:\r\n  result = session.run(sum_operation)\r\n  print(result)\r\n```\r\n\r\nCan you try running the code snippet above?", "@dynamicwebpaige thank you for that. I\u2019ll try & follow up tomorrow and let you know. Thank you kindly. ", "@joehoeller Is this resolved? If it was resolved, please close the issue. Thanks!", "I think it was resolved. I am closing the issue. But, please let me know if I'm mistaken.", "neither gpu_options.allow_growth nor gpu_options.per_process_gpu_memory_fraction work.\r\nGPU is allocated entirely regardless what values those options are set to.", "Trying to limit the number of cores being used by TF with ConfigProto doesn't work:\r\n\r\n```\r\nconfig = tf.compat.v1.ConfigProto(\r\n        intra_op_parallelism_threads=1,\r\n        inter_op_parallelism_threads=1\r\n    )\r\n\r\nsession = tf.compat.v1.Session(config=config)\r\n```\r\n\r\nBy changing the value of the arguments it does change the number of threads being executed but I couldn't get a single core execution."]}, {"number": 27052, "title": "Unable to import tensorflow in Gpu after successful  installation", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Anaonda \r\n- TensorFlow version: 1.13.1\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: conda \r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 9.0\r\n- GPU model and memory: nvidia getforce Gtx- 4GB \r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-4-fc42d8cf7cab> in <module>\r\n----> 1 import tensorflow as tf\r\n      2 from tensorflow.keras.callbacks import TensorBoard\r\n      3 from tensorflow.keras import models,layers,optimizers\r\n      4 from tensorflow.keras.models import Sequential\r\n      5 from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten,BatchNormalization\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py in <module>\r\n     22 \r\n     23 # pylint: disable=g-bad-import-order\r\n---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n     25 \r\n     26 from tensorflow._api.v1 import app\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>\r\n     61 \r\n     62 # Framework\r\n---> 63 from tensorflow.python.framework.framework_lib import *  # pylint: disable=redefined-builtin\r\n     64 from tensorflow.python.framework.versions import *\r\n     65 from tensorflow.python.framework import config\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\framework_lib.py in <module>\r\n     23 # Classes used when building a Graph.\r\n     24 from tensorflow.python.framework.device import DeviceSpec\r\n---> 25 from tensorflow.python.framework.ops import Graph\r\n     26 from tensorflow.python.framework.ops import Operation\r\n     27 from tensorflow.python.framework.ops import Tensor\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py in <module>\r\n     42 from tensorflow.python.eager import tape\r\n     43 from tensorflow.python.framework import c_api_util\r\n---> 44 from tensorflow.python.framework import composite_tensor\r\n     45 from tensorflow.python.framework import device as pydev\r\n     46 from tensorflow.python.framework import dtypes\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\composite_tensor.py in <module>\r\n     24 \r\n     25 from tensorflow.python import pywrap_tensorflow\r\n---> 26 from tensorflow.python.util import nest\r\n     27 \r\n     28 \r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py in <module>\r\n    104 _is_mapping = _pywrap_tensorflow.IsMapping\r\n    105 _is_attrs = _pywrap_tensorflow.IsAttrs\r\n--> 106 _is_composite_tensor = _pywrap_tensorflow.IsCompositeTensor\r\n    107 \r\n    108 \r\n\r\nAttributeError: module 'tensorflow.python.pywrap_tensorflow' has no attribute 'IsCompositeTensor'\r\n\r\n\r\n**Any other info / logs**\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-4-fc42d8cf7cab> in <module>\r\n----> 1 import tensorflow as tf\r\n      2 from tensorflow.keras.callbacks import TensorBoard\r\n      3 from tensorflow.keras import models,layers,optimizers\r\n      4 from tensorflow.keras.models import Sequential\r\n      5 from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten,BatchNormalization\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py in <module>\r\n     22 \r\n     23 # pylint: disable=g-bad-import-order\r\n---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n     25 \r\n     26 from tensorflow._api.v1 import app\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>\r\n     61 \r\n     62 # Framework\r\n---> 63 from tensorflow.python.framework.framework_lib import *  # pylint: disable=redefined-builtin\r\n     64 from tensorflow.python.framework.versions import *\r\n     65 from tensorflow.python.framework import config\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\framework_lib.py in <module>\r\n     23 # Classes used when building a Graph.\r\n     24 from tensorflow.python.framework.device import DeviceSpec\r\n---> 25 from tensorflow.python.framework.ops import Graph\r\n     26 from tensorflow.python.framework.ops import Operation\r\n     27 from tensorflow.python.framework.ops import Tensor\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py in <module>\r\n     42 from tensorflow.python.eager import tape\r\n     43 from tensorflow.python.framework import c_api_util\r\n---> 44 from tensorflow.python.framework import composite_tensor\r\n     45 from tensorflow.python.framework import device as pydev\r\n     46 from tensorflow.python.framework import dtypes\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\composite_tensor.py in <module>\r\n     24 \r\n     25 from tensorflow.python import pywrap_tensorflow\r\n---> 26 from tensorflow.python.util import nest\r\n     27 \r\n     28 \r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py in <module>\r\n    104 _is_mapping = _pywrap_tensorflow.IsMapping\r\n    105 _is_attrs = _pywrap_tensorflow.IsAttrs\r\n--> 106 _is_composite_tensor = _pywrap_tensorflow.IsCompositeTensor\r\n    107 \r\n    108 \r\n\r\nAttributeError: module 'tensorflow.python.pywrap_tensorflow' has no attribute 'IsCompositeTensor'\r\n", "comments": ["You need to upgrade to [cuda 10.0 for using TF 1.13.1](https://www.tensorflow.org/install/gpu). Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27052\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27052\">No</a>\n"]}, {"number": 27051, "title": "Added float16 type in the attribute of div_no_nan #27044", "body": "added the required type in div_no_nan in its core. #27044 ", "comments": ["@shashvatshahi1998 please resolve conflicts"]}, {"number": 27050, "title": "Immediately build `tf.keras.layers.Layer` if `input_dim` or `input_shape` are specified", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.0alpha0\r\n- Are you willing to contribute it (Yes/No): Y\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently `tf.keras.layers.Layer`(and its subclasses like `tf.keras.layers.Dense`) is lazily built when the first time it sees input tensors since it needs to know the input shape or input dim, but after `input_dim` or `input_shape`  are specified in the constructor, there is no need for that, it should get immediately built because users expect to see variables attached to this layer right after they provide `input_dim` or `input_shape` \r\n\r\n**Will this change the current api? How?**\r\nNo, no need to change the current api.\r\n\r\n**Who will benefit with this feature?**\r\nI believe lots of users benefit with this feature\r\n\r\n**Any Other info.**\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python import keras\r\n\r\nd = keras.layers.Dense(2, input_dim=3)\r\n# this line should raise errors since the input_dim is 3 not 4, but it works in TF 2.0alpha0\r\n# if `tf.keras.layers.Dense` will be immediately built if `input_dim` is specified, then\r\n# errors that (1, 4) can't do matmul with (2, 3) will be rasied, much more clear right?\r\no = d(tf.random.normal(shape=(1, 4)))\r\n```", "comments": ["check this out  #25780, i think this is a bug and will be fixed.", "@zakizhou Can you list your use cases here? eager variable creation will have many side effects. Currently this is the behavior in model level, but not layer level. Having different behavior between 1) input_shape is specified, 2) input_shape is not specified is dangerous and we need to have a strongly consistent semantics in the future.", "@tanzhenyu \r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\npreloaded_weights = [np.random.normal(size=(3, 2))]\r\nd = tf.keras.layers.Dense(2, input_dim=3, use_bias=False)\r\nd.set_weights(preloaded_weights)\r\n# ValueError: You called `set_weights(weights)` on layer \"dense_2\" with a\r\n# weight list of length 1, but the layer was expecting 0 weights. Provided weights: ...\r\n\r\nd.kernel.assign(preloaded_weights)\r\n# AttributeError: 'Dense' object has no attribute 'kernel'\r\n\r\n# So I just simply want to assign a specific weight to `d` before I pass any input to it\r\n# but I can not do it because of lazy building  of `tf.keras.layers.Layer`.\r\n# The only thing I can do is to call `d.build((None, 3))` before assign weights which I\r\n# think not necessary because I already pass `input_dim=3` in the constructor\r\n```", "@zakizhou sounds reasonable. This can be achieved by `build` and I haven't received many requests for it. If you'd like to contribute for it, I'd be happy to review!", "@tanzhenyu one way to implement eager building of `tf.keras.layers.Layer`(and its subclasses) is quite straightforward, just add a check in the constructor whether an `input_dim` is explicitly passed by user or not, if yes, build a `input_shape` from `input_dim`, and add one line `self.build(input_shape)` at the end of constructor, otherwise, still perform lazy building.", "closed"]}, {"number": 27049, "title": "Resuming training from saved checkpoint produces different result than uninterrupted training", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes ([Google Collab](https://colab.research.google.com/drive/1ZRhX4VBEWo4fh4zbXL_p-HDCUGTRelnH))\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Collab\r\n- TensorFlow installed from (source or binary): Google Collab\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: Python 3\r\n\r\n**Describe the current behavior**\r\nLoading a model with `tf.keras.models.load_model`, produced with `tf.keras.callbacks.ModelCheckpoint` , and resuming training produces different results from running the training without save model + restore model in the middle.\r\n\r\n**Describe the expected behavior**\r\nSaving and restoring the model should allow to resume training as if there was no interruption in the first place.\r\n\r\n**Code to reproduce the issue**\r\n[Google Collab](https://colab.research.google.com/drive/1ZRhX4VBEWo4fh4zbXL_p-HDCUGTRelnH)\r\n\r\n**Other info / logs**\r\nNo interruption:\r\n\r\n> Epoch 49/100\r\n>  - 0s - loss: 3.5190 - val_loss: 3.3597\r\n> Epoch 50/100\r\n>  - 0s - loss: 3.4090 - val_loss: 3.2668\r\n> Epoch 51/100\r\n>  - 0s - loss: 3.2637 - val_loss: 3.1623\r\n> Epoch 52/100\r\n>  - 0s - loss: 3.0962 - val_loss: 2.9975\r\n\r\nWith interruption:\r\n\r\n> Epoch 49/50\r\n>  - 0s - loss: 3.5190 - val_loss: 3.3597\r\n> Epoch 50/50\r\n> \r\n> Epoch 00050: saving model to weights.50.ckpt\r\n>  - 0s - loss: 3.4090 - val_loss: 3.2668\r\n> ... `load_model('weights.50.ckpt')` ...\r\n> Epoch 51/100\r\n>  - 0s - loss: 3.2637 - val_loss: 3.3816\r\n> Epoch 52/100\r\n>  - 0s - loss: 3.3175 - val_loss: 3.1457\r\n\r\nThe model does not have any random elements, so it looks like optimizer state is lost.\r\n", "comments": ["Model.fit is taking random slices of the data and batching them together. If you control for that, e.g. make all of the examples identical, can you still reproduce?", "@allenlavoie the model will converge instantly if there's just 1 training sample.\r\nI added `shuffle=False` to `.fit(...)` calls. The issue persisted. (notebook updated).\r\n\r\nAlso, this should not have changed anything at all, since `Has no effect when steps_per_epoch is not None.`, which was `1` in my original sample anyway. Indeed, the losses after this change are the same.", "Hrm. It looks like it's passing include_optimizer to Model.save. @tanzhenyu, any ideas?", "@lostmsu The previous keras optimizer is problematic. We have provided a new set of optimizers under exactly the same name and fully backward compatible for users. However as unfortunate as this is, the new ones did not make to tf 1.13.1. If you do !pip install tf-nightly-gpu-2.0-preview, then you should be able to see identical behavior.\r\n\r\nThat said, you need to change one line from tf.set_random_seed(seed) to tf.random.set_seed(seed)", "@tanzhenyu confirming, seems to be fixed in 2.0-preview", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27049\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27049\">No</a>\n"]}, {"number": 27048, "title": "Minor docs typo/grammar", "body": "", "comments": []}, {"number": 27046, "title": "tf.Module doesn't see variables from keras layers", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0-alpha0\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nUser-defined subclasses of `tf.Module` do not recognize and include variables from keras layer object members.\r\n\r\n**Describe the expected behavior**\r\nSubclassing `tf.Module` should expose the variables (not to mention training_variables) from all sub-computations, including keras layers, so long as these computations are set as members of the class.\r\n\r\n**Code to reproduce the issue**\r\nThis came from a TF2.0 port of the pytorch dynamic graph example [here](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html#pytorch-control-flow-weight-sharing).\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport random\r\n\r\nclass DynamicNet(tf.Module):\r\n    def __init__(self, D_in, H, D_out):\r\n        self.input_linear = tf.keras.layers.Dense(H, input_dim=D_in)\r\n        self.middle_linear = tf.keras.layers.Dense(H, input_dim=H)\r\n        self.output_linear = tf.keras.layers.Dense(D_out, input_dim=H)\r\n\r\n    def __call__(self, x):\r\n        h_relu = tf.maximum(self.input_linear(x), 0)\r\n        for _ in range(random.randint(0, 3)):\r\n            h_relu = tf.maximum(self.middle_linear(h_relu), 0)\r\n        y_pred = self.output_linear(h_relu)\r\n        return y_pred\r\n\r\nN, D_in, H, D_out = 64, 1000, 100, 10\r\n\r\nx = np.random.randn(N, D_in)\r\n\r\nmodel = DynamicNet(D_in, H, D_out)\r\nmodel(x)\r\nassert len(model.variables) > 0\r\n```\r\n(This particular example can be fixed by manually defining a `@property` variables which combines the variables members from each of the Dense layers into one list, but I think this partially defeats the point of using `tf.Module`).\r\n\r\n**Other info / logs**\r\nI'm not completely sure this is a bug/oversight, or if `tf.Module` was never intended to be compatible with keras layers. If that's the case, perhaps this is more appropriately categorized as a feature request. It seems like a natural thing to be able to do based on the the [tf.Module RFC](https://github.com/tensorflow/community/blob/master/rfcs/20190117-tf-module.md).", "comments": ["After `tf.Module` becomes the base class for `tf.keras.layers.Layer` this issue will be fixed, but I don't know when", "Hi all, the `tf.keras` team are working on making Module the base class for Layer. I think @qlzh727 is working on it this quarter.\r\n\r\nAs a temporary hack you can make a reference to the layer variables inside the module:\r\n\r\n```python\r\nclass DenseModule(tf.Module):\r\n  def __init__(self):\r\n    super(DenseModule, self).__init__()\r\n    self._layer = tf.keras.layers.Dense(64)\r\n\r\n  def __call__(self, x):\r\n    y = self._layer(x)\r\n    # Keep a reference to layer variables so Module can find them.\r\n    self._k_variables = self._layer.variables\r\n    return y\r\n```", "Hi @tomhennigan, is there a timeline for it? will it get done in 2.0 release?", "I hope so, I worked with the Keras team last month to remove known blockers (e.g. e866995aff7) and I think it should be possible to do before 2.0 if we don't find more blockers. ", "Yes, the plan is to have keras.layer use module as base class, and we plan to have this feature in 2.0 release. I am actively working on it right now.", "https://github.com/tensorflow/tensorflow/commit/23c8fd4ca3452865ac9ef1359f74cd0039908b59 should fix this issue.", "I've tested this at head and @qlzh727's fix does indeed make your example pass."]}, {"number": 27045, "title": "Cannot remove all warnings", "body": "Run the following code:\r\n\r\n```\r\nimport os, logging\r\n\r\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\r\nlogging.getLogger(\"tensorflow\").setLevel(logging.CRITICAL)\r\nlogging.getLogger(\"tensorflow_hub\").setLevel(logging.CRITICAL)\r\n\r\nimport tensorflow as tf\r\nimport tensorflow_hub as hub\r\n\r\nprint(tf.contrib.util.constant_value(tf.ones([1])))\r\n```\r\nThen you will get the following warnings:\r\n```\r\n\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0322 16:52:19.971889 139848381953792 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\r\n\r\nWARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\r\nFor more information, please see:\r\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\r\n  * https://github.com/tensorflow/addons\r\nIf you depend on functionality not listed there, please file an issue.\r\n\r\n```\r\n\r\nHow can I remove these disgusting messages?", "comments": ["I don't think you can remove the `contrib` going away warning", "users do not like warnings, please...", "@chwang85 Could try the solution provided [here](https://github.com/tensorflow/tensorflow/issues/8340#issuecomment-332212742). It is not showing any warning when I used it. Please let us know how it progresses. Thanks!", "```\r\nimport logging, os\r\n\r\nlogging.disable(logging.WARNING)\r\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\r\n\r\nimport tensorflow as tf\r\n\r\nprint(tf.contrib.util.constant_value(tf.ones([1])))\r\n```\r\nlogging cannot remove the `contrib` warning", "A bit hacky, but this works for me. (Reference: 081d2bd4ec3b1c5bae3bed65fdedb26f1aa99fb7)\r\n\r\n```python\r\nimport types\r\nimport tensorflow as tf\r\nif type(tf.contrib) != types.ModuleType:  # if it is LazyLoader\r\n    tf.contrib._warning = None\r\n```\r\n\r\nOr more simply (importing `types` can be a bit cumbersome):\r\n\r\n```python\r\nimport tensorflow as tf\r\nif type(tf.contrib) != type(tf): tf.contrib._warning = None\r\n```\r\n\r\nIMHO, it should have implemented using logger rather than `print()` so that it can respect logging configurations aforementioned.", "If you want, can you submit a PR to make it using logger instead of `print()`?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "It's caused by abseil instead of tensorflow: abseil/abseil-py#99\r\n\r\nWorkaround: \r\n```\r\nimport absl.logging\r\nlogging.root.removeHandler(absl.logging._absl_handler)\r\nabsl.logging._warn_preinit_stderr = False\r\n```\r\n\r\nhttps://github.com/dhalperi/pybatfish/blob/f8ddd3938148f9a5d9c14c371a099802c564fac3/pybatfish/client/capirca.py#L33-L50", "> It's caused by abseil instead of tensorflow: [abseil/abseil-py#99](https://github.com/abseil/abseil-py/issues/99)\r\n> \r\n> Workaround:\r\n> \r\n> ```\r\n> import absl.logging\r\n> logging.root.removeHandler(absl.logging._absl_handler)\r\n> absl.logging._warn_preinit_stderr = False\r\n> ```\r\n> \r\n> https://github.com/dhalperi/pybatfish/blob/f8ddd3938148f9a5d9c14c371a099802c564fac3/pybatfish/client/capirca.py#L33-L50\r\n\r\nHmm, I tried your suggestions and it seems that this did not work.", "> @chwang85 Could try the solution provided [here](https://github.com/tensorflow/tensorflow/issues/8340#issuecomment-332212742). It is not showing any warning when I used it. Please let us know how it progresses. Thanks!\r\n\r\nThis does not work in TF 1.13.", "This is not the right way to do it but it works, if you are irritated by the warnings while working on your code. Just redirect stderr to /dev/null\r\n\r\n```python\r\nf = open('/dev/null', 'w')\r\nsys.stderr = f\r\n```\r\nFind the original post [here](https://stackoverflow.com/a/6735958).", "@satwikk that's potentially quite dangerous as you won't see any errors as well as warnings", "[https://github.com/tensorflow/tensorflow/issues/8340#issuecomment-332212742](url)"]}, {"number": 27044, "title": "math_ops.div_no_nan support for float16", "body": "**System information**\r\n- OS Platform and Distribution: Ubuntu 18.04\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 1.14.1-dev20190306\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 1.01\r\n- GPU model and memory: M60\r\n\r\n**Describe the current behavior**\r\nmath_ops.div_no_nan only allows `float32` or `float64`, this limits the ability to use `MirroredStrategy` with `float16`", "comments": ["there is no problem I will add the feature in core of math_ops soon.", "I got some issues in my pull request, sorry for that but you can do it on your own go to TensorFlow's/core/ops/math_ops.cc and search for DivNoNan and add required dtypes in its attributes", "The issue seems to have been fixed in:\r\nhttps://github.com/tensorflow/tensorflow/commit/11245e46584b2e610ee7ea8258978bfcef1f5c26", "Excellent, thanks @shashvatshahi1998 and @yongtang! "]}, {"number": 27043, "title": "In Eager mode,  batch norm doesn't support Second order derivative", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux release 7.4.1708 \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):  1.12.0  and  1.10.1\r\n- Python version:  3.6 \r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 9.0, cuDNN - 7\r\n- GPU model and memory:  TITAN Xp\r\n\r\n**Describe the current behavior**\r\n\r\nI wanted to implement an eager mode version of the large margin code from google-research [code](https://github.com/google-research/google-research/tree/master/large_margin).\r\n\r\nIn other words, I computed a second order derivative. If my model/network didn't contain/enable the batch norm layers, it was fine. \r\n\r\nBut if I enabled the batch norm layer, it said that \r\n\"tf.gradients is not supported when eager execution is enabled. Use tf.GradientTape instead.\" however I didn't use tf.gradients and other layers didn't raise such an error.\r\n\r\nThe static/graph code work fine on both cases(with or without batch normalization)\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nimport os\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '2'\r\nos.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\r\nos.environ['CUDA_VISIBLE_DEVICES'] = \"\"\r\n\r\ntf_config = tf.ConfigProto()\r\ntf_config.gpu_options.allow_growth = True\r\ntf.enable_eager_execution(tf_config)\r\n\r\nEPS = 1e-5\r\nMOMENTUM = 0.9\r\n\r\nclass ConfigDict(object):\r\n    def __init__(self):\r\n        self.num_classes = 10\r\n\r\n        # List of tuples specify (kernel_size, number of filters) for each layer.\r\n        self.filter_sizes_conv_layers = [(5, 32), (5, 64)]\r\n        # Dictionary of pooling type (\"max\"/\"average\", size and stride).\r\n        self.pool_params = {\"type\": \"max\", \"size\": 2, \"stride\": 2}\r\n        self.num_units_fc_layers = [512]\r\n        self.dropout_rate = 0\r\n        self.batch_norm = True\r\n        self.activation = tf.nn.relu\r\n    \r\ndef pool2d_layer(inputs, pool_type, pool_size=2, pool_stride=2):\r\n    if pool_type == \"max\":\r\n        # Max pooling layer\r\n        return tf.layers.max_pooling2d(\r\n            inputs, pool_size=[pool_size] * 2, strides=pool_stride)\r\n    \r\n    \r\nclass MNISTNetwork(tf.keras.Model):\r\n    \"\"\"MNIST model. \"\"\"\r\n\r\n    def __init__(self, config):\r\n        super(MNISTNetwork, self).__init__()\r\n        self.num_classes = config.num_classes\r\n        self.var_list = []\r\n        self.init_ops = None\r\n        self.activation = config.activation\r\n        self.filter_sizes_conv_layers = config.filter_sizes_conv_layers\r\n        self.num_units_fc_layers = config.num_units_fc_layers\r\n        self.pool_params = config.pool_params\r\n        self.dropout_rate = config.dropout_rate\r\n        self.batch_norm = config.batch_norm\r\n        self.conv_layers = []\r\n        self.bn_layers = []\r\n        in_channel = 1\r\n        for i, filter_size in enumerate(self.filter_sizes_conv_layers):\r\n            f_size = filter_size[0]\r\n            conv_layer = tf.layers.Conv2D(kernel_size=filter_size[0], filters=filter_size[1], \r\n                                          strides=(1, 1), padding=\"same\",\r\n                                          activation=self.activation, \r\n                                          use_bias=not self.batch_norm)\r\n            self.conv_layers.append(conv_layer)\r\n            batch_norm_layer = tf.keras.layers.BatchNormalization(momentum=MOMENTUM, epsilon=EPS)\r\n            self.bn_layers.append(batch_norm_layer)\r\n            in_channel = filter_size[1]\r\n            \r\n        self.fc_layers = []\r\n        in_shape = 64 * 7 * 7\r\n        for i, num_units in enumerate(self.num_units_fc_layers):\r\n            fc_layer = tf.layers.Dense(num_units, activation=self.activation)\r\n            self.fc_layers.append(fc_layer)\r\n            in_shape = num_units\r\n        self.output_layer = tf.layers.Dense(self.num_classes, activation=None)\r\n\r\n    def __call__(self, images, is_training=False):\r\n        \"\"\"Builds model.\"\"\"\r\n        net = images\r\n        for i in range(len(self.filter_sizes_conv_layers)):\r\n            net = self.conv_layers[i](net)\r\n\r\n            if self.pool_params:\r\n                net = pool2d_layer(net, pool_type=self.pool_params[\"type\"], pool_size=self.pool_params[\"size\"]\r\n                                   , pool_stride=self.pool_params[\"stride\"])\r\n            if self.dropout_rate > 0:\r\n                net = tf.layers.dropout(net, rate=self.dropout_rate, training=is_training)\r\n                \r\n            if self.batch_norm:\r\n                # net = tf.layers.batch_normalization(net, training=is_training, epsilon=EPS, momentum=MOMENTUM)\r\n                net = self.bn_layers[i](net, training=is_training)\r\n            \r\n        net = tf.layers.flatten(net)\r\n\r\n        for i in range(len(self.num_units_fc_layers)):\r\n            net = self.fc_layers[i](net)\r\n        logits = self.output_layer(net)\r\n        return logits\r\n    \r\n\r\nconfig = ConfigDict()\r\n\r\n# enable/disable batch norm\r\nconfig.batch_norm = True\r\n\r\nmodel = MNISTNetwork(config)\r\n\r\nimages = np.random.uniform(0, 1, (3, 28, 28, 1))\r\nimages = tf.convert_to_tensor(images, dtype=np.float32)\r\n# images = tf.Variable(images)\r\nprint(\"data %.5f\" % images.numpy().sum())\r\n\r\nwith tf.GradientTape(persistent=True) as t:\r\n    with tf.GradientTape(persistent=True) as t2:\r\n        logits = model(images, is_training=True)\r\n        m = tf.reduce_sum(logits)\r\n        print(logits.numpy().sum())\r\n        dp_dx = t2.gradient(m, model.variables)\r\n    print(\"first\", dp_dx[0].numpy().sum())\r\n    d2y_dx2 = t.gradient(dp_dx[0], model.variables)\r\n    print(\"second order\", d2y_dx2[0].numpy().sum())\r\n```\r\n\r\n\r\n**Other info / logs**\r\n\r\n```\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-86-44f63511f093> in <module>\r\n     20         dp_dx = t2.gradient(m, model.variables)\r\n     21     print(\"first\", dp_dx[0].numpy().sum())\r\n---> 22     d2y_dx2 = t.gradient(dp_dx[0], model.variables)\r\n     23     print(\"second order\", d2y_dx2[0].numpy().sum())\r\n\r\n~/software/miniconda3/envs/dllib3/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py in gradient(self, target, sources, output_gradients)\r\n    899         nest.flatten(target),\r\n    900         flat_sources,\r\n--> 901         output_gradients=output_gradients)\r\n    902 \r\n    903     if not self._persistent:\r\n\r\n~/software/miniconda3/envs/dllib3/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py in imperative_grad(tape, target, sources, output_gradients)\r\n     62       target,\r\n     63       sources,\r\n---> 64       output_gradients)\r\n\r\n~/software/miniconda3/envs/dllib3/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py in _gradient_function(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads)\r\n    115     return [None] * num_inputs\r\n    116 \r\n--> 117   return grad_fn(mock_op, *out_grads)\r\n    118 \r\n    119 \r\n\r\n~/software/miniconda3/envs/dllib3/lib/python3.6/site-packages/tensorflow/python/ops/nn_grad.py in _FusedBatchNormGradGrad(op, *grad)\r\n    938   grad_initial = [grad_grad_x, grad_grad_scale, grad_grad_offset]\r\n    939   grad_grad_y, grad_x, grad_scale = gradients_impl.gradients(\r\n--> 940       [grad_x, grad_scale, grad_offset], [grad_y, x, scale], grad_initial)\r\n    941   return grad_grad_y, grad_x, grad_scale, None, None\r\n    942 \r\n\r\n~/software/miniconda3/envs/dllib3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py in gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients)\r\n    628   with ops.get_default_graph()._mutation_lock():  # pylint: disable=protected-access\r\n    629     return _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops,\r\n--> 630                             gate_gradients, aggregation_method, stop_gradients)\r\n    631 \r\n    632 \r\n\r\n~/software/miniconda3/envs/dllib3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, src_graph)\r\n    642   \"\"\"Implementation of gradients().\"\"\"\r\n    643   if context.executing_eagerly():\r\n--> 644     raise RuntimeError(\"tf.gradients is not supported when eager execution \"\r\n    645                        \"is enabled. Use tf.GradientTape instead.\")\r\n    646   if src_graph is None:\r\n\r\nRuntimeError: tf.gradients is not supported when eager execution is enabled. Use tf.GradientTape instead.\r\n```", "comments": ["This appears to be working as of the latest `tf-nightly`", "Great! I installed the tf-nightly and this bug has been solved. Perfect!\r\n\r\nThank you! ", "Glad to hear it!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27043\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27043\">No</a>\n"]}, {"number": 27042, "title": "tf.sparse: limited functionality", "body": "1. `tf.sparse.sparse_dense_matmul` only support matrix of rank 2, why cannot it support higher ranks?\r\n2. `tf.sparse.to_dense` cannot support back-propagation at least in and before TF 1.13\r\n3. there is no element-wise multiplication for sparse tensors similar to tf.multiply\r\n4. there is no matmul between two sparse tensors", "comments": ["By the way, before deprecating old components, would you please complete the current functionality? It seems that TF is very good at deprecating widely used functions, but poor at improving its weak points", "Sparse tensors have been added recently and they're under constant work. The deprecation needed for TF2.0 is orthogonal to this, thus unrelated. Let's try to keep things on topic, please.", "See #6391\r\nThe issues about sparse tensors have been there for at least more than 2 years\r\nBut such issues usually cannot get your precious attentions, and such issues are always closed quickly without solving anything.\r\n......", "My bad, I was wrong", "use tf.reshape unroll the batch matix to rank 2 matrix, then use tf.sparse.sparse_dense_matmul"]}, {"number": 27041, "title": "Fix incorrect docstring of compression type in `TFRecordOptions`", "body": "This fix is a followup to https://github.com/tensorflow/tensorflow/pull/26676#discussion_r267883241 (thanks @wchargin \ud83d\udc4d )\r\n\r\nIn PR #26676, the compression type of \"no compression\" was incorrectly specified as `\"NONE\"`. This fix addresses the issue and change it to `\"\"`.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 27040, "title": "Lite: DepthWiseConv Op small refactor", "body": "", "comments": ["@aselle : Gentle Reminder!", "Can one of the admins verify this patch?", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 27039, "title": "Keras added the missing shape case in np_utils", "body": "Missing shape case added in expected_shapes", "comments": []}, {"number": 27038, "title": "Preformance optimized Negate Ops", "body": "", "comments": ["@karimnosseir I have seen generally this method will have performance benefits.\r\nThe `neg_test` testcase bench-marking values are fluctuating.\r\nhowever wrote a small program to prove this change will have benefits, results are:\r\n\r\n> Split loop time in ms: 0.000208\r\n> input:1234567892\r\n> output:-1-2-3-4-5-6-7-8-9-2\r\n> \r\n> non Split loop time in ms: 0.000365\r\n> input:1234567892\r\n> output:-1-2-3-4-5-6-7-8-9-2\r\n\r\nTest code: [loop_split.zip](https://github.com/tensorflow/tensorflow/files/3006826/loop_split.zip)\r\n\r\n\r\n\r\n\r\n", "@karimnosseir Could you please further review this PR?", "Sorry for late reply as i was  out of office.\r\nTo get a better understanding on the benefit please use the benchmark tool to get performance numbers\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark\r\n\r\nThanks", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 27037, "title": "_ref types no longer in 2.0", "body": "", "comments": ["Review pleeaseee @rthadur @dynamicwebpaige ????", "And is there any other place I've missed? @dynamicwebpaige ", "looks like changes are already landed by different PR, thank you for your contribution , i will close this PR"]}, {"number": 27036, "title": "TfLite unpack_test test case refactor", "body": "Test case refactor changes.", "comments": ["@renjie-liu \r\n\r\nconst reference changes for arg parameter handle, please check once again, TIA.", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27036) for more info**.\n\n<!-- need_author_consent -->", "Corrupt PR is replaced with #27371"]}, {"number": 27035, "title": "Tensorflow 1.13.1 import Error on Python 3.7.1", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution : Windows 10 Pro, 64-bit\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version: 1.13.1\r\n- Python version: 3.7.1\r\n- Installed using virtualenv? pip? conda?: pip on a conda env\r\n\r\nFirstly, I reinstalled in my old environment with python 3.5.6, and have faced the same error, then created a new one with 3.7.1 after seeing @gunan  on the #22300 issue .\r\nI checked that i have the : _pywrap_tensorflow_internal.lib on the directory \"..\\Lib\\site-packages\\tensorflow\\python\"\r\ni don't know what causes the error since tf 1.13.1 supports python 3.7\r\n\r\n`Traceback (most recent call last):\r\n  File \"C:\\Users\\EMP\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\EMP\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\EMP\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\EMP\\Anaconda3\\envs\\MachineLearning\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\EMP\\Anaconda3\\envs\\MachineLearning\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\EMP\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\EMP\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\EMP\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\EMP\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\EMP\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\EMP\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\EMP\\Anaconda3\\envs\\MachineLearning\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\EMP\\Anaconda3\\envs\\MachineLearning\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.` \r\n", "comments": ["you could try `pip install --upgrade --no-deps --force-reinstall tensorflow`, it makes tensorflow 1.13.1 work on my win10", "> you could try `pip install --upgrade --no-deps --force-reinstall tensorflow`, it makes tensorflow 1.13.1 work on my win10\r\n\r\nI just tried it and still having the same error.\r\n\r\nI also tried to remove the tf 1.13.1 and installed tf 1.10 but in this case too i get the error ", "I think if you don't need python version 3.7 then you should downgrade the python version to 3.6.8.\r\nBecause on the [pip guide](https://www.tensorflow.org/install/pip) said : `Requires Python 3.4, 3.5, or 3.6]`\r\n\r\nYou can use conda to create a python version 3.6 and install tensorflow by conda. Because Conda has supported tensorflow 1.13.1. Just run this code then conda will do the rest.\r\n`conda create -n py36 python=3.6 pip tensorflow`", "> I think if you don't need python version 3.7 then you should downgrade the python version to 3.6.8.\r\n> Because on the [pip guide](https://www.tensorflow.org/install/pip) said : `Requires Python 3.4, 3.5, or 3.6]`\r\n> \r\n> You can use conda to create a python version 3.6 and install tensorflow by conda. Because Conda has supported tensorflow 1.13.1. Just run this code then conda will do the rest.\r\n> `conda create -n py36 python=3.6 pip tensorflow`\r\n\r\nI installed tf 1.13.1 firstly on my python 3.6 environment, then get the error checking other issues they suggested that it's due to compatibility with python 3.7. Now i face it even on tf 1.10 on python 3.6.", "I suggest you install tensorflow by conda intead of pip. Because currently, Anaconda has supported tensorflow 1.13.1 package.\r\nRun this command on Conda prompt. It will create a new environment which has tensorflow 1.13.1 in it. \r\n`conda create -n py36 python=3.6 pip tensorflow`\r\n", "> I suggest you install tensorflow by conda intead of pip. Because currently, Anaconda has supported tensorflow 1.13.1 package.\r\n> Run this command on Conda prompt. It will create a new environment which has tensorflow 1.13.1 in it.\r\n> `conda create -n py36 python=3.6 pip tensorflow`\r\n\r\ni install usinf pip and not conda, i tried it and it changed anything\r\nthe solution i found temprary is to make a python 3.5.6 and install tensorflow 1.9 waiting for a solution for the issue.", "@ghsama Please uninstall tensorflow and python completely and then reinstall python3.6.5 (that I have) or python3.6.6. Then, install tensorflow1.13.1 or tensorflow1.12 using the instructions provided [here](https://github.com/jvishnuvardhan/Installing-TensorFlow-GPU-on-Windows-10-TF1.12). Please let me know how it progresses. thanks!", "@ghsama If you like conda, you could follow the instructions given by others (above) and install TF. Some time when you have many version install and uninstalled, then there will be lot of incompatibilities arises. Please start fresh by uninstalling python and tensorflow and reinstall them. Thanks!", "I am also plagued by the `ImportError: DLL load failed: ...`  issue.\r\n\r\n**System**\r\nKVM virtual machine with 3 cores and CPU type \"host\"\r\nOS: Win64 (Windows 10)\r\nCPU: i5-6600 (has AVX2)\r\nno dedicated GPU\r\n\r\n**Environment**\r\npython 3.6.8 from anaconda\r\ntensorflow 1.13.1 from anaconda\r\nlibmklml 2019.0.3\r\nprotobuf 3.6.1 from anaconda\r\n\r\nI installed the debugging tools from the Windows SDK and enabled the \"show loader snaps\" flag for python.exe as follows:\r\n`gflags.exe -i python.exe +sls`\r\n\r\nI then executed a python application - which imports tensorflow - under the cdb debugger and looked at what happens when `_pywrap_tensorflow_internal.pyd` is loaded:\r\n`cdb.exe python.exe my_app.py`\r\n```\r\n(1824.11bc): Integer divide-by-zero - code c0000094 (first chance)\r\nFirst chance exceptions are reported before any exception handling.\r\nThis exception may be expected and handled.\r\n*** ERROR: Symbol file could not be found.  Defaulted to export symbols for C:\\...\\app\\lib\\site-packages\\tensorflow\\python\\_pywrap_tensorflow_internal.pyd -\r\n_pywrap_tensorflow_internal+0x2a57cd:\r\n00007ffa`b92557cd f735b9d15105    div     eax,dword ptr [_pywrap_tensorflow_internal!tensorflow::_SequenceExample_default_instance_+0x412c (00007ffa`be77298c)] ds:00007ffa`be77298c=00000000\r\n... (entered \"g\") ...\r\n1824:29e0 @ 1267255250 - LdrpAllocateTls - INFO: TlsVector 00000234F9D5B020 Index 0 : 8 bytes copied from 00007FFAF883C83C to 00000234F9CC2220\r\n(1824.11bc): Integer divide-by-zero - code c0000094 (!!! second chance !!!)\r\n_pywrap_tensorflow_internal+0x2a57cd:\r\n00007ffa`b92557cd f735b9d15105    div     eax,dword ptr [_pywrap_tensorflow_internal!tensorflow::_SequenceExample_default_instance_+0x412c (00007ffa`be77298c)] ds:00007ffa`be77298c=00000000\r\n```\r\n\r\nTo me this looks like a **divide by zero** caused by the protobuf generated code for `SequenceExample`, but I could be wrong.\r\n\r\nIt may be helpful to have a look at the generated code for `SequenceExample`, the `default_instance` constructor in particular.\r\n\r\nPlease contact me for assistance in further analyzing this issue.\r\n", "@lyind Did you try uninstalling and reinstalling tensorflow and python. Some time old files affect the new installation. Thanks!", "@jvishnuvardhan I assemble an embedded python installation in a temporary directory on Windows x64 mainly using default conda (anaconda) python packages. Whenever I run my Makefiles everything is re-downloaded. The regular tensorflow installer is not used. The resulting installation runs fine on another machine (identical binaries/scripts), but not within the virtualized build environment.\r\n\r\nDo you suspect the error may be with the anaconda tensorflow packages?", "@lyind I am not really sure what is the root-cause as we don't support anaconda. Could you also post it in anaconda repo? Thanks!", "In case others face this here is my experience. \r\nI ran into this issue after upgrading my python installation from 3.6.6 I did the upgrade because of tensorboard issue https://github.com/tensorflow/tensorboard/issues/1976. I used conda to upgrade and it appears this causes corruption of many installed pyd modules (including non-tensorflow packagages). I got around all of the failures, one by one,  by reinstalling the packages as suggested above by @YAOYI626 \r\n`pip install --force-reinstall  --upgrade <module>`\r\n", "@lyind Can you try @mrevow suggestion. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!", "> I think if you don't need python version 3.7 then you should downgrade the python version to 3.6.8.\r\n> Because on the [pip guide](https://www.tensorflow.org/install/pip) said : `Requires Python 3.4, 3.5, or 3.6]`\r\n> \r\n> You can use conda to create a python version 3.6 and install tensorflow by conda. Because Conda has supported tensorflow 1.13.1. Just run this code then conda will do the rest.\r\n> `conda create -n py36 python=3.6 pip tensorflow`\r\n\r\nTF 1.13.1 supports Python3.7\r\n"]}, {"number": 27034, "title": "Unexpected log output in Tensorboard", "body": "Env:\r\nTensorflow-GPU 1.13.1\r\nTensorboard 1.13.1\r\nTensorboardX 1.6\r\nInstalled time: 2019/03/22\r\nInstalled from PIP\r\nOS: Windows 10\r\nPython version: 3.7\r\nCUDA: 10\r\n\r\nProblem:\r\nI use command \"tensorboard --logdir ... --host 127.0.0.1\" to run the program, and there are many log output in the command. And not add \"-v\" parameter..\r\n\r\nTensorBoard 1.13.1 at http://127.0.0.1:6006 (Press CTRL+C to quit)\r\nI0322 17:56:22.683215 10068 _internal.py:97] 127.0.0.1 - - [22/Mar/2019 17:56:22] \"GET / HTTP/1.1\" 200 -\r\nI0322 17:56:23.341972 12492 _internal.py:97] 127.0.0.1 - - [22/Mar/2019 17:56:23] \"GET /font-roboto/oMMgfZMQthOryQo9n22dcuvvDin1pK8aKteLpeZ5c0A.woff2 HTTP/1.1\" 200 -\r\nI0322 17:56:24.188966 10068 _internal.py:97] 127.0.0.1 - - [22/Mar/2019 17:56:24] \"GET /tf-interactive-inference-dashboard/editedexample.png HTTP/1.1\" 200 -\r\n...\r\n\r\nThis does not appare in the previous version. I not sure whether it is a bug or a new feature.", "comments": ["@qwesdfok Could you provide a small code to reproduce your bug? Thanks!", "@jvishnuvardhan \r\nFor example,\r\n\r\nfrom tensorboardX import SummaryWriter\r\nimport numpy as np\r\nimport time\r\nwriter = SummaryWriter(\"tb_log\")\r\nstep = 0\r\nwhile True:\r\n    writer.add_scalar(\"loss\", np.random.rand(), step)\r\n    time.sleep(1.0)\r\n    step += 1\r\n\r\nAnd run tensorboard. Open 127.0.0.1:6006 in Chrome browser. The following output will appare in the command line.\r\n\r\ntensorboard --logdir tb_log --host 127.0.0.1\r\nTensorBoard 1.13.1 at http://127.0.0.1:6006 (Press CTRL+C to quit)\r\nI0323 11:05:51.970985  9496 _internal.py:97] 127.0.0.1 - - [23/Mar/2019 11:05:51] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\r\nI0323 11:05:52.261211 11452 _internal.py:97] 127.0.0.1 - - [23/Mar/2019 11:05:52] \"\u001b[37mGET /font-roboto/oMMgfZMQthOryQo9n22dcuvvDin1pK8aKteLpeZ5c0A.woff2 HTTP/1.1\u001b[0m\" 200 -\r\nI0323 11:05:52.833691  9496 _internal.py:97] 127.0.0.1 - - [23/Mar/2019 11:05:52] \"\u001b[37mGET /tf-interactive-inference-dashboard/editedexample.png HTTP/1.1\u001b[0m\" 200 -\r\nI0323 11:05:52.834676 11452 _internal.py:97] 127.0.0.1 - - [23/Mar/2019 11:05:52] \"\u001b[37mGET /tf-interactive-inference-dashboard/distance.png HTTP/1.1\u001b[0m\" 200 -\r\nI0323 11:05:52.836671 18396 _internal.py:97] 127.0.0.1 - - [23/Mar/2019 11:05:52] \"\u001b[37mGET /tf-interactive-inference-dashboard/explorecounterfactuals.png HTTP/1.1\u001b[0m\" 200 -\r\nI0323 11:05:52.839663  8384 _internal.py:97] 127.0.0.1 - - [23/Mar/2019 11:05:52] \"\u001b[37mGET /tf-interactive-inference-dashboard/pdplots.png HTTP/1.1\u001b[0m\" 200 -\r\nI0323 11:05:52.905486 18396 _internal.py:97] 127.0.0.1 - - [23/Mar/2019 11:05:52] \"\u001b[37mGET /data/runs HTTP/1.1\u001b[0m\" 200 -\r\nI0323 11:05:52.905486 11452 _internal.py:97] 127.0.0.1 - - [23/Mar/2019 11:05:52] \"\u001b[37mGET /data/environment HTTP/1.1\u001b[0m\" 200 -\r\nI0323 11:05:52.906484  9496 _internal.py:97] 127.0.0.1 - - [23/Mar/2019 11:05:52] \"\u001b[37mGET /data/experiments HTTP/1.1\u001b[0m\" 200 -\r\nI0323 11:05:52.909475  8384 _internal.py:97] 127.0.0.1 - - [23/Mar/2019 11:05:52] \"\u001b[37mGET /data/plugins_listing HTTP/1.1\u001b[0m\" 200 -\r\nI0323 11:05:52.939399  8384 _internal.py:97] 127.0.0.1 - - [23/Mar/2019 11:05:52] \"\u001b[37mGET /font-roboto/RxZJdnzeo3R5zSexge8UUZBw1xU1rKptJj_0jans920.woff2 HTTP/1.1\u001b[0m\" 200 -\r\nI0323 11:05:52.945382 11452 _internal.py:97] 127.0.0.1 - - [23/Mar/2019 11:05:52] \"\u001b[37mGET /data/runs HTTP/1.1\u001b[0m\" 200 -\r\nI0323 11:05:52.947374  8384 _internal.py:97] 127.0.0.1 - - [23/Mar/2019 11:05:52] \"\u001b[37mGET /data/environment HTTP/1.1\u001b[0m\" 200 -\r\nI0323 11:05:52.947374  9496 _internal.py:97] 127.0.0.1 - - [23/Mar/2019 11:05:52] \"\u001b[37mGET /data/plugins_listing HTTP/1.1\u001b[0m\" 200 -\r\nI0323 11:05:52.949370 11452 _internal.py:97] 127.0.0.1 - - [23/Mar/2019 11:05:52] \"\u001b[37mGET /data/experiments HTTP/1.1\u001b[0m\" 200 -\r\nI0323 11:05:53.057084 11452 _internal.py:97] 127.0.0.1 - - [23/Mar/2019 11:05:53] \"\u001b[37mGET /data/plugin/scalars/tags HTTP/1.1\u001b[0m\" 200 -\r\nI0323 11:05:53.080021 11452 _internal.py:97] 127.0.0.1 - - [23/Mar/2019 11:05:53] \"\u001b[37mGET /data/plugin/scalars/tags HTTP/1.1\u001b[0m\" 200 -\r\nI0323 11:05:53.091987 11452 _internal.py:97] 127.0.0.1 - - [23/Mar/2019 11:05:53] \"\u001b[37mGET /font-roboto/d-6IYplOFocCacKzxwXSOJBw1xU1rKptJj_0jans920.woff2 HTTP/1.1\u001b[0m\" 200 -\r\nI0323 11:05:53.139859 11452 _internal.py:97] 127.0.0.1 - - [23/Mar/2019 11:05:53] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=loss&run=.&experiment= HTTP/1.1\u001b[0m\" 200 -", "Hi @qwesdfok, thanks for raising the issue. I was able to reproduce this and it looks like it is related to https://github.com/tensorflow/tensorboard/issues/2028. `pip freeze` should reveal that you have werkzeug of `0.15.1`. We will address it soon. Thanks!  ", "Duplicate of https://github.com/tensorflow/tensorboard/issues/2028", "Yes, I checked the version of werkzeug and it is 0.15.1. I will downgrade werkzeug, if it is needed. Thanks!", "Closing since this is a dupe, please follow https://github.com/tensorflow/tensorboard/issues/2028", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27034\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27034\">No</a>\n", "Had the same problem with TensorBoard \r\nafter fitting logs created but when used in terminal\r\n`tensorboard --logdir logs/fit`\r\ngot bad link that did not work in Google Chrome\r\n`TensorBoard 1.14.0 at http://DESKTOP-CC81AQ5:6006/ (Press CTRL+C to quit)`\r\nand in IE was nothing with this  python response\r\n`I1119 15:12:22.167826  3004 _internal.py:122] ....... - - [19/Nov/2019 15:12:22] \"?[37mGET / HTTP/1.1?[0m\" 200 -`\r\n\r\nFix this really easy - just **update tensorboard from 1.14 to 2.0** \r\nand left tf version 1.14 (still have problem with tf 2.0 and CUDA 10.0)\r\nand now goog python response\r\n`TensorBoard 2.0.0 at http://localhost:6006/ (Press CTRL+C to quit)`\r\n\r\n"]}, {"number": 27033, "title": "gpu doesn't report error when id is actually out of range in tf.nn.embedding_lookup(embed, ids) ", "body": "- TensorFlow version (use command below):1.13\r\n- Python version:3.6\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\nx = tf.constant(999)\r\n#with tf.device('/cpu:0'):\r\n#    voc_emb = tf.get_variable(name=\"voc_emb\", dtype=tf.float32, shape=[3, 12])\r\n#    embedded_x = tf.nn.embedding_lookup(voc_emb, x)\r\nvoc_emb = tf.get_variable(name=\"voc_emb\", dtype=tf.float32, shape=[3, 12])\r\nembedded_x = tf.nn.embedding_lookup(voc_emb, x)\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\nprint(sess.run(embedded_x))\r\n```\r\n\r\nthe following above I run on cpu and gpu separately, \r\nwhile cpu reports:\r\nInvalidArgumentError (see above for traceback): indices = 999 is not in [0, 3)\r\n\t [[node embedding_lookup (defined at test.py:24) ]]\r\n\r\nbut gpu just give an output like this:\r\n[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r\n", "comments": ["> If back-propagation goes through `embedded_x`, will there be an error?\r\n\r\nJust figured this out, see:\r\nhttps://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup\r\n\r\n> validate_indices: DEPRECATED. If this operation is assigned to CPU, values in indices are always validated to be within range. If assigned to GPU, out-of-bound indices result in safe but unspecified behavior, which may include raising an error.\r\n", "@StarYoung ,\r\n\r\nWe see that this issue is using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to latest stable version and let us know if the issue still persists in newer versions.we will get you the right help.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27033\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27033\">No</a>\n"]}, {"number": 27032, "title": "TfLite tile_test test case refactor", "body": "Test case refactor changes.", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27032) for more info**.\n\n<!-- need_author_consent -->", "Corrupt PR is replaced with #27370"]}, {"number": 27031, "title": "Tensorflow-lite segmentation model does not work with NNAPI on Android 9.0", "body": "\r\n\r\n**System information**\r\n- OS Platform and Distribution : Android, Tensorflow-Lite-Experimental\r\n- Mobile device : Samsung SM-A730F, Android 9 , API 28\r\n- Tensorflow: 1.13\r\n\r\n**Describe the current behavior**\r\n\r\nThe official tensorflow lite segmentation model '[deeplabv3_257_mv_gpu.tflite](https://storage.googleapis.com/download.tensorflow.org/models/tflite/gpu/deeplabv3_257_mv_gpu.tflite)' cannot be run on  Andorid 9.0, using NNAPI. It seems some operators are not supported currently by NNAPI.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe model should run on phone using NNAPI, without any problems.\r\n\r\n**Other info / logs**\r\nThe model gives error when we run it using tensorflow lite interpreter on android.Also it gives similar error on android benchmark tool, when we run it with 'use_nnapi=1' option.\r\n\r\nTF-Lite Benchmark:-\r\n\r\nadb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/deeplabv3_257_mv_gpu.tflite --num_threads=1 --use_nnapi=1\r\nadb: /opt/intel/intelpython27/lib/libcrypto.so.1.0.0: no version information available (required by adb)\r\nSTARTING!\r\nMin num runs: [50]\r\nMin runs duration (seconds): [1]\r\nInter-run delay (seconds): [-1]\r\nNum threads: [1]\r\nBenchmark name: []\r\nOutput prefix: []\r\nMin warmup runs: [1]\r\nMin warmup runs duration (seconds): [0.5]\r\nGraph: [/data/local/tmp/deeplabv3_257_mv_gpu.tflite]\r\nInput layers: []\r\nInput shapes: []\r\nUse nnapi : [1]\r\nAllow fp16 : [0]\r\nLoaded model /data/local/tmp/deeplabv3_257_mv_gpu.tflite\r\nresolved reporter\r\nInitialized session in 325.573ms\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds\r\nOp code 23 is currently not delegated to NNAPI\r\nReturning error since TFLite returned failure nnapi_delegate.cc:736.\r\nFailed to build graph for NNAPI\r\nFailed to invoke!\r\nAborted \r\n\r\nAndroid Log:\r\n\r\nandroid.example.com.tflitecamerademo E/tflite: Op code 23 is currently not delegated to NNAPI\r\n    Returning error since TFLite returned failure nnapi_delegate.cc:751.\r\n    Failed to build graph for NNAPI\r\n", "comments": ["Thanks for flagging. We'll soon be moving to an NNAPI acceleration approach which allows partial subgraph delegation, which should resolve the problem.", "@jdduke Op code 23 is ResizeBilinear (`kTfLiteBuiltinResizeBilinear = 23`), which is supported by NNAPI. That is, this is not an op in TFLite but not NNAPI.", "Also, in the official tflite gpu [classification demo](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo) NNAPI was not giving any speed up (even compared with CPU version) with float model.With quantized model CPU was having better speed than NNAPI.\r\nBenchmark result from app:\r\nCPU: ~ 200 ms\r\nGPU: ~ 90 ms\r\nNNAPI: ~ 300-500 ms", "@anilsathyan7 it seems you are using NNAPI cpu fallback, that is, your device doesn't have real NNAPI  accelerators.", "@jdduke and @anilsathyan7 it seems there is a stalled PR try to delegate ResizeBilinear to NNAPI https://github.com/tensorflow/tensorflow/pull/19928", "We also tried with Huawei Honor Play (Kirin 970 + NPU) with Android 9.0.  The tflite demo app gives  least performance, with both quantized and float models, when we switch to NNAPI !!! (i.e NNAPI < CPU < GPU).", "@miaowang14 I believe the partial delegation should be addressed in the latest Android/TFLite release?", "This should have been fixed. Please re-open if that is not the case.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27031\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27031\">No</a>\n"]}, {"number": 27030, "title": "'tensorflow/core/framework/numeric_types.h' file not found", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- OS Platform and Distribution : cenos7 \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): source code \r\n- TensorFlow version: 1.93\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: un installed virtualenv\r\n- Bazel version (if compiling from source): 0.22.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: no\r\n- GPU model and memory: no\r\n\r\n\r\n**Describe the documentation issue**\r\nIn centos, I downloaded the tensorflow project. I open '/root/tensorflow/tensorflow/core/framework/allocator.h' file,  Tips can not find thoese included files, such as 'tensorflow/core/framework/numeric_types.h' file not found.but those files are already exists. thanks \r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the fields in the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Along with the template, please provide as many details as possible to find the root cause of the issue. Thanks!", "if I change as  #include  \u2018numberic_types.h\u2019 , it works. it maybe the path problem. thanks", "@laiqb If it is resolved, please close the issue. Thanks!", "I think there is a better way, otherwise I must modify many files. ", "@laiqb Could you check whether the bug persists in tf-nightly? Thanks!", "@laiqb I don't see path issue here as all the files are in the correct path. I am not sure what we can do from tensorflow side as all the files are in right location. Thanks!"]}, {"number": 27028, "title": "Added Quantization 8 bit support for Relu", "body": "This is asked by many in the community.", "comments": ["@jianlijianli, I have rebased the code and resolved the merge conflicts, can you please review the PR.\r\n\r\nRegards\r\nAmit", "Thanks for working on this and apologies for the delay. Please see my comment.", "> Could you please update register.cc, operator.cc and operator_test.cc as well, same as in [8d4cdf8](https://github.com/tensorflow/tensorflow/commit/8d4cdf8444389286ec54b7105b07e9059dba4f66) ? Thanks.\r\n\r\n@jianlijianli , have updated the code, as per your suggestion, kindly check.\r\n\r\nRegards\r\nAmit", "@jianlijianli , the implementation is also validated against #28268 (issue) . Can you please check.\r\n\r\nRegards\r\nAmit", "@jianlijianli , i have added one more commit for the support of Relu from tools as well.\r\n\r\nRegards\r\nAmit", "@jianlijianli , could you please have a look and provide your feedback for the same.This is one of the important PRs for me and would like to see this though.\r\n\r\nRegards\r\nAmit", "@jianlijianli i have updated the registery for the Relu kernel with updated version, kindly review.\r\n\r\nRegards\r\nAmit", "> Hi Amit, thanks for making the changes. Apologies if it was misleading, by \"operator.cc\" I mean \"tensorflow/lite/toco/tflite/operator.cc\" not \"tensorflow/lite/tools/optimize/operator_property.cc\". Could you please add the versioning test in \"tensorflow/lite/toco/tflite/operator.cc\", like in here (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/toco/tflite/operator.cc#L278-L286)? Thanks.\r\n> \r\n> Also, thanks for working on operator_property.cc. I added a comment that for ReLU family there is a restriction on input and output scale. This change might require you to update test on \"relu.bin\". Please feel free to leave all the changes related to \"tensorflow/lite/tools/optimize/operator_property.cc\" to a different PR. Thanks!\r\n\r\n@jianlijianli , Thanks for spending time on the PR i have updated the code as per your suggestions, also i have added the TC as well. Kindly check and approve.\r\n\r\nRegards\r\nAmit", "@jianlijianli can you please check and approve the PR.\r\n\r\nRegards\r\nAmit", "@jianlijianli , i have resolved the merge conflict , can you please check and approve.\r\n\r\n\r\nRegards\r\nAmit", "@jianlijianli , sorry to bug you again, i have rebased the code again, would be great if you can spend some time on this PR.\r\n\r\nRegards\r\nAmit", "@jianlijianli , this is a long pending PR, can you please take a decision on this PR.\r\n\r\nRegards\r\nAmit", "@amitsrivastava78 thanks for your patience , it will be reviewed soon.", "Can one of the admins verify this patch?", "@amitsrivastava78  Can you please resolve the conflicts? Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 44 days that this pull-request has stalled. Please create a new pull-request with the requested changes.", "What's the status of this?", "@InCogNiTo124  The support for Quantized relu was added already."]}]